[{"id": "1210.0257", "submitter": "Dimitrios Thilikos", "authors": "Fedor V. Fomin, Daniel Lokshtanov, Saket Saurabh, and Dimitrios M.\n  Thilikos", "title": "Kernels for (connected) Dominating Set on graphs with Excluded\n  Topological subgraphs", "comments": "Final version, accepted to ACM Transactions on Algorithms (TALG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first linear kernels for the (Connected) Dominating Set problems\non H-topological minor free graphs. We prove the existence of polynomial time\nalgorithms that, for a given H-topological-minor-free graph G and a positive\ninteger k, output an H-topological-minor-free graph G' on O(k) vertices such\nthat G has a (connected) dominating set of size k iff G' has one. Our results\nextend the known classes of graphs on which the Dominating Set and Connected\nDominating Set problems admit linear kernels. Prior to our work, it was known\nthat these problems admit linear kernels on graphs excluding a fixed apex graph\nH as a minor. Moreover, for Dominating Set, a kernel of size kc(H), where c(H)\nis a constant depending on the size of H, follows from a more general result on\nthe kernelization of Dominating Set on graphs of bounded degeneracy. Alon and\nGutner explicitly asked whether one can obtain a linear kernel for Dominating\nSet on H-minor-free graphs. We answer this question in the affirmative and in\nfact prove a more general result. For Connected Dominating Set no polynomial\nkernel even on H-minor-free graphs was known prior to our work. On the negative\nside, it is known that Connected Dominating Set on 2-degenerated graphs does\nnot admit a polynomial kernel unless coNP $\\subseteq$ NP/poly. Our\nkernelization algorithm is based on a non-trivial combination of the following\ningredients The structural theorem of Grohe and Marx [STOC 2012] for graphs\nexcluding a fixed graph H as a topological minor; A novel notion of\nprotrusions, different than the one defined in [FOCS 2009]; Our results are\nbased on a generic reduction rule that produces an equivalent instance (in case\nthe input graph is H-minor-free) of the problem, with treewidth $O(\\sqrt{k})$.\nThe application of this rule in a divide-and-conquer fashion, together with the\nnew notion of protrusions, gives us the linear kernels.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2012 23:49:19 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 10:45:45 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 09:17:03 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1210.0260", "submitter": "Saket Saurabh", "authors": "Mark Jones, Daniel Lokshtanov, M. S. Ramanujan, Saket Saurabh and\n  Ond\\v{r}ej Such\\'y", "title": "Parameterized Complexity of Directed Steiner Tree on Sparse Graphs", "comments": "28", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameterized complexity of the directed variant of the\nclassical {\\sc Steiner Tree} problem on various classes of directed sparse\ngraphs. While the parameterized complexity of {\\sc Steiner Tree} parameterized\nby the number of terminals is well understood, not much is known about the\nparameterization by the number of non-terminals in the solution tree. All that\nis known for this parameterization is that both the directed and the undirected\nversions are W[2]-hard on general graphs, and hence unlikely to be fixed\nparameter tractable FPT. The undirected {\\sc Steiner Tree} problem becomes FPT\nwhen restricted to sparse classes of graphs such as planar graphs, but the\ntechniques used to show this result break down on directed planar graphs.\n  In this article we precisely chart the tractability border for {\\sc Directed\nSteiner Tree} (DST) on sparse graphs parameterized by the number of\nnon-terminals in the solution tree. Specifically, we show that the problem is\nfixed parameter tractable on graphs excluding a topological minor, but becomes\nW[2]-hard on graphs of degeneracy 2. On the other hand we show that if the\nsubgraph induced by the terminals is required to be acyclic then the problem\nbecomes FPT on graphs of bounded degeneracy.\n  We further show that our algorithm achieves the best possible running time\ndependence on the solution size and degeneracy of the input graph, under\nstandard complexity theoretic assumptions. Using the ideas developed for DST,\nwe also obtain improved algorithms for {\\sc Dominating Set} on sparse\nundirected graphs. These algorithms are asymptotically optimal.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 00:04:55 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Jones", "Mark", ""], ["Lokshtanov", "Daniel", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""], ["Such\u00fd", "Ond\u0159ej", ""]]}, {"id": "1210.0340", "submitter": "Andrzej Lingas", "authors": "Andrzej Lingas and Mia Persson", "title": "A fast parallel algorithm for minimum-cost small integral flows", "comments": "This is an improved version of a preliminary version which appeared\n  in proc. EUROPAR 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the minimum-cost integral flow problem for small\nvalues of the flow. It reduces the problem to the tests of simple multi-variate\npolynomials over a finite field of characteristic two for non-identity with\nzero. In effect, we show that a minimum-cost flow of value k in a network with\nn vertices, a sink and a source, integral edge capacities and positive integral\nedge costs polynomially bounded in n can be found by a randomized PRAM, with\nerrors of exponentially small probability in n, running in O(k\\log (kn)+\\log^2\n(kn)) time and using 2^{k}(kn)^{O(1)} processors. Thus, in particular, for the\nminimum-cost flow of value O(\\log n), we obtain an RNC^2 algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 10:25:50 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Lingas", "Andrzej", ""], ["Persson", "Mia", ""]]}, {"id": "1210.0374", "submitter": "Marc Schoenauer", "authors": "Thomas Philip Runarsson, Marc Schoenauer (INRIA Saclay - Ile de\n  France, LRI), Mich\\`ele Sebag (LRI)", "title": "Pilot, Rollout and Monte Carlo Tree Search Methods for Job Shop\n  Scheduling", "comments": "Learning and Intelligent OptimizatioN (LION'6) 7219 (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy heuristics may be attuned by looking ahead for each possible choice,\nin an approach called the rollout or Pilot method. These methods may be seen as\nmeta-heuristics that can enhance (any) heuristic solution, by repetitively\nmodifying a master solution: similarly to what is done in game tree search,\nbetter choices are identified using lookahead, based on solutions obtained by\nrepeatedly using a greedy heuristic. This paper first illustrates how the Pilot\nmethod improves upon some simple well known dispatch heuristics for the\njob-shop scheduling problem. The Pilot method is then shown to be a special\ncase of the more recent Monte Carlo Tree Search (MCTS) methods: Unlike the\nPilot method, MCTS methods use random completion of partial solutions to\nidentify promising branches of the tree. The Pilot method and a simple version\nof MCTS, using the $\\varepsilon$-greedy exploration paradigms, are then\ncompared within the same framework, consisting of 300 scheduling problems of\nvarying sizes with fixed-budget of rollouts. Results demonstrate that MCTS\nreaches better or same results as the Pilot methods in this context.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 12:33:25 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Runarsson", "Thomas Philip", "", "INRIA Saclay - Ile de\n  France, LRI"], ["Schoenauer", "Marc", "", "INRIA Saclay - Ile de\n  France, LRI"], ["Sebag", "Mich\u00e8le", "", "LRI"]]}, {"id": "1210.0461", "submitter": "Konstantin Kutzkov", "authors": "Andrea Campagna, Konstantin Kutzkov, Rasmus Pagh", "title": "On Parallelizing Matrix Multiplication by the Column-Row Method", "comments": "To appear in ALENEX 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse matrix multiplication by the column row\nmethod in a distributed setting where the matrix product is not necessarily\nsparse. We present a surprisingly simple method for \"consistent\" parallel\nprocessing of sparse outer products (column-row vector products) over several\nprocessors, in a communication-avoiding setting where each processor has a copy\nof the input. The method is consistent in the sense that a given output entry\nis always assigned to the same processor independently of the specific\nstructure of the outer product. We show guarantees on the work done by each\nprocessor, and achieve linear speedup down to the point where the cost is\ndominated by reading the input. Our method gives a way of distributing (or\nparallelizing) matrix product computations in settings where the main\nbottlenecks are storing the result matrix, and inter-processor communication.\nMotivated by observations on real data that often the absolute values of the\nentries in the product adhere to a power law, we combine our approach with\nfrequent items mining algorithms and show how to obtain a tight approximation\nof the weight of the heaviest entries in the product matrix.\n  As a case study we present the application of our approach to frequent pair\nmining in transactional data streams, a problem that can be phrased in terms of\nsparse ${0,1}$-integer matrix multiplication by the column-row method.\nExperimental evaluation of the proposed method on real-life data supports the\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 16:35:51 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2012 15:28:54 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Campagna", "Andrea", ""], ["Kutzkov", "Konstantin", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1210.0477", "submitter": "Christian Schulz", "authors": "Peter Sanders and Christian Schulz", "title": "Think Locally, Act Globally: Perfectly Balanced Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel local improvement scheme for the perfectly balanced graph\npartitioning problem. This scheme encodes local searches that are not\nrestricted to a balance constraint into a model allowing us to find\ncombinations of these searches maintaining balance by applying a negative cycle\ndetection algorithm. We combine this technique with an algorithm to balance\nunbalanced solutions and integrate it into a parallel multi-level evolutionary\nalgorithm, KaFFPaE, to tackle the problem. Overall, we obtain a system that is\nfast on the one hand and on the other hand is able to improve or reproduce most\nof the best known perfectly balanced partitioning results ever reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 17:30:47 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1210.0481", "submitter": "Todd Veldhuizen", "authors": "Todd L. Veldhuizen", "title": "Leapfrog Triejoin: a worst-case optimal join algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": "LB1201", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen exciting developments in join algorithms. In 2008,\nAtserias, Grohe and Marx (henceforth AGM) proved a tight bound on the maximum\nresult size of a full conjunctive query, given constraints on the input\nrelation sizes. In 2012, Ngo, Porat, R{\\'e} and Rudra (henceforth NPRR) devised\na join algorithm with worst-case running time proportional to the AGM bound.\nOur commercial Datalog system LogicBlox employs a novel join algorithm,\n\\emph{leapfrog triejoin}, which compared conspicuously well to the NPRR\nalgorithm in preliminary benchmarks. This spurred us to analyze the complexity\nof leapfrog triejoin. In this paper we establish that leapfrog triejoin is also\nworst-case optimal, up to a log factor, in the sense of NPRR. We improve on the\nresults of NPRR by proving that leapfrog triejoin achieves worst-case\noptimality for finer-grained classes of database instances, such as those\ndefined by constraints on projection cardinalities. We show that NPRR is\n\\emph{not} worst-case optimal for such classes, giving a counterexample where\nleapfrog triejoin runs in $O(n \\log n)$ time, compared to $\\Theta(n^{1.375})$\ntime for NPRR. On a practical note, leapfrog triejoin can be implemented using\nconventional data structures such as B-trees, and extends naturally to\n$\\exists_1$ queries. We believe our algorithm offers a useful addition to the\nexisting toolbox of join algorithms, being easy to absorb, simple to implement,\nand having a concise optimality proof.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 17:54:13 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2012 14:12:27 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2013 00:10:44 GMT"}, {"version": "v4", "created": "Sat, 7 Sep 2013 13:43:24 GMT"}, {"version": "v5", "created": "Fri, 20 Dec 2013 20:21:03 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Veldhuizen", "Todd L.", ""]]}, {"id": "1210.0508", "submitter": "Vladimir Kolmogorov", "authors": "Rustem Takhanov and Vladimir Kolmogorov", "title": "Inference algorithms for pattern-based CRFs on sequence data", "comments": "Algorithmica accepted version", "journal-ref": "Algorithmica, September 2016, Volume 76, Issue 1, pp 17-46", "doi": "10.1007/s00453-015-0017-7", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Conditional Random Fields (CRFs) with pattern-based potentials\ndefined on a chain. In this model the energy of a string (labeling) $x_1...x_n$\nis the sum of terms over intervals $[i,j]$ where each term is non-zero only if\nthe substring $x_i...x_j$ equals a prespecified pattern $\\alpha$. Such CRFs can\nbe naturally applied to many sequence tagging problems.\n  We present efficient algorithms for the three standard inference tasks in a\nCRF, namely computing (i) the partition function, (ii) marginals, and (iii)\ncomputing the MAP. Their complexities are respectively $O(n L)$, $O(n L\n\\ell_{max})$ and $O(n L \\min\\{|D|,\\log (\\ell_{max}+1)\\})$ where $L$ is the\ncombined length of input patterns, $\\ell_{max}$ is the maximum length of a\npattern, and $D$ is the input alphabet. This improves on the previous\nalgorithms of (Ye et al., 2009) whose complexities are respectively $O(n L\n|D|)$, $O(n |\\Gamma| L^2 \\ell_{max}^2)$ and $O(n L |D|)$, where $|\\Gamma|$ is\nthe number of input patterns.\n  In addition, we give an efficient algorithm for sampling. Finally, we\nconsider the case of non-positive weights. (Komodakis & Paragios, 2009) gave an\n$O(n L)$ algorithm for computing the MAP. We present a modification that has\nthe same worst-case complexity but can beat it in the best case.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 19:13:59 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2012 16:16:58 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2012 00:11:30 GMT"}, {"version": "v4", "created": "Sat, 29 Dec 2012 22:13:01 GMT"}, {"version": "v5", "created": "Fri, 20 Jan 2017 08:00:44 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Takhanov", "Rustem", ""], ["Kolmogorov", "Vladimir", ""]]}, {"id": "1210.0664", "submitter": "David Lee", "authors": "Ashish Goel and David Lee", "title": "Triadic Consensus: A Randomized Algorithm for Voting in a Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical voting rules do not work well in settings with many candidates. If\nthere are just several hundred candidates, then even a simple task such as\nchoosing a top candidate becomes impractical. Motivated by the hope of\ndeveloping group consensus mechanisms over the internet, where the numbers of\ncandidates could easily number in the thousands, we study an urn-based voting\nrule where each participant acts as a voter and a candidate. We prove that when\nparticipants lie in a one-dimensional space, this voting protocol finds a\n$(1-\\epsilon/sqrt{n})$ approximation of the Condorcet winner with high\nprobability while only requiring an expected $O(\\frac{1}{\\epsilon^2}\\log^2\n\\frac{n}{\\epsilon^2})$ comparisons on average per voter. Moreover, this voting\nprotocol is shown to have a quasi-truthful Nash equilibrium: namely, a Nash\nequilibrium exists which may not be truthful, but produces a winner with the\nsame probability distribution as that of the truthful strategy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 06:26:31 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Goel", "Ashish", ""], ["Lee", "David", ""]]}, {"id": "1210.0748", "submitter": "Yongming Luo", "authors": "Yongming Luo, George H. L. Fletcher, Jan Hidders, Yuqing Wu and Paul\n  De Bra", "title": "External memory bisimulation reduction of big graphs", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present, to our knowledge, the first known I/O efficient\nsolutions for computing the k-bisimulation partition of a massive directed\ngraph, and performing maintenance of such a partition upon updates to the\nunderlying graph. Ubiquitous in the theory and application of graph data,\nbisimulation is a robust notion of node equivalence which intuitively groups\ntogether nodes in a graph which share fundamental structural features.\nk-bisimulation is the standard variant of bisimulation where the topological\nfeatures of nodes are only considered within a local neighborhood of radius\n$k\\geqslant 0$.\n  The I/O cost of our partition construction algorithm is bounded by $O(k\\cdot\n\\mathit{sort}(|\\et|) + k\\cdot scan(|\\nt|) + \\mathit{sort}(|\\nt|))$, while our\nmaintenance algorithms are bounded by $O(k\\cdot \\mathit{sort}(|\\et|) + k\\cdot\n\\mathit{sort}(|\\nt|))$. The space complexity bounds are $O(|\\nt|+|\\et|)$ and\n$O(k\\cdot|\\nt|+k\\cdot|\\et|)$, resp. Here, $|\\et|$ and $|\\nt|$ are the number of\ndisk pages occupied by the input graph's edge set and node set, resp., and\n$\\mathit{sort}(n)$ and $\\mathit{scan}(n)$ are the cost of sorting and scanning,\nresp., a file occupying $n$ pages in external memory. Empirical analysis on a\nvariety of massive real-world and synthetic graph datasets shows that our\nalgorithms perform efficiently in practice, scaling gracefully as graphs grow\nin size.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 12:30:15 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2012 09:26:03 GMT"}, {"version": "v3", "created": "Thu, 2 May 2013 08:23:28 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Luo", "Yongming", ""], ["Fletcher", "George H. L.", ""], ["Hidders", "Jan", ""], ["Wu", "Yuqing", ""], ["De Bra", "Paul", ""]]}, {"id": "1210.0864", "submitter": "Ilias Diakonikolas", "authors": "Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun", "title": "Learning mixtures of structured distributions over discrete domains", "comments": "preliminary full version of soda'13 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathfrak{C}$ be a class of probability distributions over the discrete\ndomain $[n] = \\{1,...,n\\}.$ We show that if $\\mathfrak{C}$ satisfies a rather\ngeneral condition -- essentially, that each distribution in $\\mathfrak{C}$ can\nbe well-approximated by a variable-width histogram with few bins -- then there\nis a highly efficient (both in terms of running time and sample complexity)\nalgorithm that can learn any mixture of $k$ unknown distributions from\n$\\mathfrak{C}.$\n  We analyze several natural types of distributions over $[n]$, including\nlog-concave, monotone hazard rate and unimodal distributions, and show that\nthey have the required structural property of being well-approximated by a\nhistogram with few bins. Applying our general algorithm, we obtain\nnear-optimally efficient algorithms for all these mixture learning problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 18:07:13 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Chan", "Siu-on", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1210.1095", "submitter": "Francesco Vezzi", "authors": "Francesco Vezzi, Giuseppe Narzisi and Bud Mishra", "title": "Reevaluating Assembly Evaluations with Feature Response Curves: GAGE and\n  Assemblathons", "comments": "Submitted to PLoS One. Supplementary material available at\n  http://www.nada.kth.se/~vezzi/publications/supplementary.pdf and\n  http://cs.nyu.edu/mishra/PUBLICATIONS/12.supplementaryFRC.pdf", "journal-ref": null, "doi": "10.1371/journal.pone.0052210", "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In just the last decade, a multitude of bio-technologies and software\npipelines have emerged to revolutionize genomics. To further their central\ngoal, they aim to accelerate and improve the quality of de novo whole-genome\nassembly starting from short DNA reads. However, the performance of each of\nthese tools is contingent on the length and quality of the sequencing data, the\nstructure and complexity of the genome sequence, and the resolution and quality\nof long-range information. Furthermore, in the absence of any metric that\ncaptures the most fundamental \"features\" of a high-quality assembly, there is\nno obvious recipe for users to select the most desirable assembler/assembly.\nInternational competitions such as Assemblathons or GAGE tried to identify the\nbest assembler(s) and their features. Some what circuitously, the only\navailable approach to gauge de novo assemblies and assemblers relies solely on\nthe availability of a high-quality fully assembled reference genome sequence.\nStill worse, reference-guided evaluations are often both difficult to analyze,\nleading to conclusions that are difficult to interpret. In this paper, we\ncircumvent many of these issues by relying upon a tool, dubbed FRCbam, which is\ncapable of evaluating de novo assemblies from the read-layouts even when no\nreference exists. We extend the FRCurve approach to cases where lay-out\ninformation may have been obscured, as is true in many deBruijn-graph-based\nalgorithms. As a by-product, FRCurve now expands its applicability to a much\nwider class of assemblers -- thus, identifying higher-quality members of this\ngroup, their inter-relations as well as sensitivity to carefully selected\nfeatures, with or without the support of a reference sequence or layout for the\nreads. The paper concludes by reevaluating several recently conducted assembly\ncompetitions and the datasets that have resulted from them.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 13:02:30 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Vezzi", "Francesco", ""], ["Narzisi", "Giuseppe", ""], ["Mishra", "Bud", ""]]}, {"id": "1210.1193", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler", "title": "Simple, Fast and Deterministic Gossip and Rumor Spreading", "comments": null, "journal-ref": null, "doi": "10.1137/1.9781611973105.51", "report-no": null, "categories": "cs.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study gossip algorithms for the rumor spreading problem which asks each\nnode to deliver a rumor to all nodes in an unknown network. Gossip algorithms\nallow nodes only to call one neighbor per round and have recently attracted\nattention as message efficient, simple and robust solutions to the rumor\nspreading problem.\n  Recently, non-uniform random gossip schemes were devised to allow efficient\nrumor spreading in networks with bottlenecks. In particular, [Censor-Hillel et\nal., STOC'12] gave an O(log^3 n) algorithm to solve the 1-local broadcast\nproblem in which each node wants to exchange rumors locally with its\n1-neighborhood. By repeatedly applying this protocol one can solve the global\nrumor spreading quickly for all networks with small diameter, independently of\nthe conductance.\n  This and all prior gossip algorithms for the rumor spreading problem have\nbeen inherently randomized in their design and analysis. This resulted in a\nparallel research direction trying to reduce and determine the amount of\nrandomness needed for efficient rumor spreading. This has been done via lower\nbounds for restricted models and by designing gossip algorithms with a reduced\nneed for randomness. The general intuition and consensus of these results has\nbeen that randomization plays a important role in effectively spreading rumors.\n  In this paper we improves over this state of the art in several ways by\npresenting a deterministic gossip algorithm that solves the the k-local\nbroadcast problem in 2(k+log n)log n rounds. Besides being the first efficient\ndeterministic solution to the rumor spreading problem this algorithm is\ninteresting in many aspects: It is simpler, more natural, more robust and\nfaster than its randomized pendant and guarantees success with certainty\ninstead of with high probability. Its analysis is furthermore simple,\nself-contained and fundamentally different from prior works.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 19:17:42 GMT"}, {"version": "v2", "created": "Sat, 5 Apr 2014 02:26:18 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Haeupler", "Bernhard", ""]]}, {"id": "1210.1429", "submitter": "Pawel Dlotko PhD", "authors": "Pawe{\\l} D{\\l}otko, Hubert Wagner", "title": "Computing homology and persistent homology using iterated Morse\n  decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach to computing homology (with field\ncoefficients) and persistent homology. We use concepts from discrete Morse\ntheory, to provide an algorithm which can be expressed solely in terms of\nsimple graph theoretical operations. We use iterated Morse decomposition, which\nallows us to sidetrack many problems related to the standard discrete Morse\ntheory. In particular, this approach is provably correct in any dimension.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 13:20:15 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2012 16:01:16 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["D\u0142otko", "Pawe\u0142", ""], ["Wagner", "Hubert", ""]]}, {"id": "1210.1544", "submitter": "Tao Hu", "authors": "Tao Hu and Dmitri B. Chklovskii", "title": "Reconstruction of Sparse Circuits Using Multi-neuronal Excitation\n  (RESCUME)", "comments": "9 pages, 6 figures. Advances in Neural Information Processing Systems\n  (NIPS) 22, 790 (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the central problems in neuroscience is reconstructing synaptic\nconnectivity in neural circuits. Synapses onto a neuron can be probed by\nsequentially stimulating potentially pre-synaptic neurons while monitoring the\nmembrane voltage of the post-synaptic neuron. Reconstructing a large neural\ncircuit using such a \"brute force\" approach is rather time-consuming and\ninefficient because the connectivity in neural circuits is sparse. Instead, we\npropose to measure a post-synaptic neuron's voltage while stimulating\nsequentially random subsets of multiple potentially pre-synaptic neurons. To\nreconstruct these synaptic connections from the recorded voltage we apply a\ndecoding algorithm recently developed for compressive sensing. Compared to the\nbrute force approach, our method promises significant time savings that grow\nwith the size of the circuit. We use computer simulations to find optimal\nstimulation parameters and explore the feasibility of our reconstruction method\nunder realistic experimental conditions including noise and non-linear synaptic\nintegration. Multineuronal stimulation allows reconstructing synaptic\nconnectivity just from the spiking activity of post-synaptic neurons, even when\nsub-threshold voltage is unavailable. By using calcium indicators,\nvoltage-sensitive dyes, or multi-electrode arrays one could monitor activity of\nmultiple postsynaptic neurons simultaneously, thus mapping their synaptic\ninputs in parallel, potentially reconstructing a complete neural circuit.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 19:03:19 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Hu", "Tao", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1210.1765", "submitter": "Travis Gagie", "authors": "Djamal Belazzougui, Travis Gagie and Gonzalo Navarro", "title": "Better Space Bounds for Parameterized Range Majority and Minority", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Karpinski and Nekrich (2008) introduced the problem of parameterized range\nmajority, which asks to preprocess a string of length $n$ such that, given the\nendpoints of a range, one can quickly find all the distinct elements whose\nrelative frequencies in that range are more than a threshold $\\tau$. Subsequent\nauthors have reduced their time and space bounds such that, when $\\tau$ is\ngiven at preprocessing time, we need either $\\Oh{n \\log (1 / \\tau)}$ space and\noptimal $\\Oh{1 / \\tau}$ query time or linear space and $\\Oh{(1 / \\tau) \\log\n\\log \\sigma}$ query time, where $\\sigma$ is the alphabet size. In this paper we\ngive the first linear-space solution with optimal $\\Oh{1 / \\tau}$ query time.\nFor the case when $\\tau$ is given at query time, we significantly improve\nprevious bounds, achieving either $\\Oh{n \\log \\log \\sigma}$ space and optimal\n$\\Oh{1 / \\tau}$ query time or compressed space and $\\Oh{(1 / \\tau) \\log\n\\frac{\\log (1 / \\tau)}{\\log w}}$ query time. Along the way, we consider the\ncomplementary problem of parameterized range minority that was recently\nintroduced by Chan et al.\\ (2012), who achieved linear space and $\\Oh{1 /\n\\tau}$ query time even for variable $\\tau$. We improve their solution to use\neither nearly optimally compressed space with no slowdown, or optimally\ncompressed space with nearly no slowdown. Some of our intermediate results,\nsuch as density-sensitive query time for one-dimensional range counting, may be\nof independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 14:01:58 GMT"}, {"version": "v2", "created": "Sun, 13 Jul 2014 18:35:30 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Gagie", "Travis", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1210.1771", "submitter": "A. Emre Cetin", "authors": "A. Emre Cetin", "title": "In-place associative permutation sort", "comments": "25 pages. arXiv admin note: substantial text overlap with\n  arXiv:1209.0572, arXiv:1209.3668, arXiv:1209.1942, arXiv:1209.4714", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In-place associative integer sorting technique was developed, improved and\nspecialized for distinct integers. The technique is suitable for integer\nsorting. Hence, given a list S of n integers S[0...n-1], the technique sorts\nthe integers in ascending or descending order. It replaces bucket sort,\ndistribution counting sort and address calculation sort family of algorithms\nand requires only constant amount of additional memory for storing counters and\nindices beside the input list. The technique was inspired from one of the\nordinal theories of \"serial order in behavior\" and explained by the analogy\nwith the three main stages in the formation and retrieval of memory in\ncognitive neuroscience: (i) practicing, (ii) storing and (iii) retrieval.\n  In this study in-place associative permutation technique is introduced for\ninteger key sorting problem. Given a list S of n elements S[0...n-1] each have\nan integer key in the range [0,m-1], the technique sorts the elements according\nto their integer keys in O(n) time using only O(1) amount of memory if m<=n. On\nthe other hand, if m>n, it sorts in O(n+m) time for the worst, O(m) time for\nthe average (uniformly distributed keys) and O(n) time for the best case using\nO(1) extra space.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 14:30:37 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Cetin", "A. Emre", ""]]}, {"id": "1210.1890", "submitter": "Konstantin Makarychev", "authors": "Konstantin Makarychev", "title": "Local Search is Better than Random Assignment for Bounded Occurrence\n  Ordering k-CSPs", "comments": "Published at STACS 2013: Konstantin Makarychev. Local Search is\n  Better than Random Assignment for Bounded Occurrence Ordering k-CSPs. STACS\n  2013, pp. 139-147", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the Bounded Occurrence Ordering k-CSP Problem is not\napproximation resistant. We give a very simple local search algorithm that\nalways performs better than the random assignment algorithm. Specifically, the\nexpected value of the solution returned by the algorithm is at least Alg > Avg\n+ a(B,k) (Opt - Avg), where \"Opt\" is the value of the optimal solution; \"Avg\"\nis the expected value of the random solution; and a(B,k)=Omega_k(B^{-(k+O(1))}\nis a parameter depending only on \"k\" (the arity of the CSP) and \"B\" (the\nmaximum number of times each variable is used in constraints). The question\nwhether bounded occurrence ordering k-CSPs are approximation resistant was\nraised by Guruswami and Zhou (APPROX 2012) who recently showed that bounded\noccurrence 3-CSPs and \"monotone\" k-CSPs admit a non-trivial approximation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 23:22:20 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2013 16:28:24 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Makarychev", "Konstantin", ""]]}, {"id": "1210.2381", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan, Mark Rudelson, Adam Smith", "title": "The Power of Linear Reconstruction Attacks", "comments": "30 pages, to appear in ACM-SIAM Symposium on Discrete Algorithms\n  (SODA 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the power of linear reconstruction attacks in statistical data\nprivacy, showing that they can be applied to a much wider range of settings\nthan previously understood. Linear attacks have been studied before (Dinur and\nNissim PODS'03, Dwork, McSherry and Talwar STOC'07, Kasiviswanathan, Rudelson,\nSmith and Ullman STOC'10, De TCC'12, Muthukrishnan and Nikolov STOC'12) but\nhave so far been applied only in settings with releases that are obviously\nlinear.\n  Consider a database curator who manages a database of sensitive information\nbut wants to release statistics about how a sensitive attribute (say, disease)\nin the database relates to some nonsensitive attributes (e.g., postal code,\nage, gender, etc). We show one can mount linear reconstruction attacks based on\nany release that gives: a) the fraction of records that satisfy a given\nnon-degenerate boolean function. Such releases include contingency tables\n(previously studied by Kasiviswanathan et al., STOC'10) as well as more complex\noutputs like the error rate of classifiers such as decision trees; b) any one\nof a large class of M-estimators (that is, the output of empirical risk\nminimization algorithms), including the standard estimators for linear and\nlogistic regression.\n  We make two contributions: first, we show how these types of releases can be\ntransformed into a linear format, making them amenable to existing\npolynomial-time reconstruction algorithms. This is already perhaps surprising,\nsince many of the above releases (like M-estimators) are obtained by solving\nhighly nonlinear formulations. Second, we show how to analyze the resulting\nattacks under various distributional assumptions on the data. Specifically, we\nconsider a setting in which the same statistic (either a) or b) above) is\nreleased about how the sensitive attribute relates to all subsets of size k\n(out of a total of d) nonsensitive boolean attributes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 19:01:53 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Rudelson", "Mark", ""], ["Smith", "Adam", ""]]}, {"id": "1210.2453", "submitter": "EPTCS", "authors": "Alessandro Solimando (University of Genova, Italy), Giorgio Delzanno\n  (University of Genova, Italy), Giovanna Guerrini (University of Genova,\n  Italy)", "title": "Automata-based Static Analysis of XML Document Adaptation", "comments": "In Proceedings GandALF 2012, arXiv:1210.2028", "journal-ref": "EPTCS 96, 2012, pp. 85-98", "doi": "10.4204/EPTCS.96.7", "report-no": null, "categories": "cs.DB cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of an XML document can be optionally specified by means of XML\nSchema, thus enabling the exploitation of structural information for efficient\ndocument handling. Upon schema evolution, or when exchanging documents among\ndifferent collections exploiting related but not identical schemas, the need\nmay arise of adapting a document, known to be valid for a given schema S, to a\ntarget schema S'. The adaptation may require knowledge of the element semantics\nand cannot always be automatically derived. In this paper, we present an\nautomata-based method for the static analysis of user-defined XML document\nadaptations, expressed as sequences of XQuery Update update primitives. The key\nfeature of the method is the use of an automatic inference method for\nextracting the type, expressed as a Hedge Automaton, of a sequence of document\nupdates. The type is computed starting from the original schema S and from\nrewriting rules that formally define the operational semantics of a sequence of\ndocument updates. Type inclusion can then be used as conformance test w.r.t.\nthe type extracted from the target schema S'.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 00:53:46 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Solimando", "Alessandro", "", "University of Genova, Italy"], ["Delzanno", "Giorgio", "", "University of Genova, Italy"], ["Guerrini", "Giovanna", "", "University of Genova,\n  Italy"]]}, {"id": "1210.2515", "submitter": "Peijun Zhu", "authors": "Ting Huang, Peijun Zhu, Zengyou He", "title": "Protein Inference and Protein Quantification: Two Sides of the Same Coin", "comments": "14 Pages, This paper has submitted to RECOMB2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: In mass spectrometry-based shotgun proteomics, protein\nquantification and protein identification are two major computational problems.\nTo quantify the protein abundance, a list of proteins must be firstly inferred\nfrom the sample. Then the relative or absolute protein abundance is estimated\nwith quantification methods, such as spectral counting. Until now, researchers\nhave been dealing with these two processes separately. In fact, they are two\nsides of same coin in the sense that truly present proteins are those proteins\nwith non-zero abundances. Then, one interesting question is if we regard the\nprotein inference problem as a special protein quantification problem, is it\npossible to achieve better protein inference performance?\n  Contribution: In this paper, we investigate the feasibility of using protein\nquantification methods to solve the protein inference problem. Protein\ninference is to determine whether each candidate protein is present in the\nsample or not. Protein quantification is to calculate the abundance of each\nprotein. Naturally, the absent proteins should have zero abundances. Thus, we\nargue that the protein inference problem can be viewed as a special case of\nprotein quantification problem: present proteins are those proteins with\nnon-zero abundances. Based on this idea, our paper tries to use three very\nsimple protein quantification methods to solve the protein inference problem\neffectively.\n  Results: The experimental results on six datasets show that these three\nmethods are competitive with previous protein inference algorithms. This\ndemonstrates that it is plausible to take the protein inference problem as a\nspecial case of protein quantification, which opens the door of devising more\neffective protein inference algorithms from a quantification perspective.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 07:36:26 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Huang", "Ting", ""], ["Zhu", "Peijun", ""], ["He", "Zengyou", ""]]}, {"id": "1210.2584", "submitter": "Mansour Moufid", "authors": "Mansour Moufid", "title": "Sudoku as a special transportation problem", "comments": "6 pages; 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sudoku is a popular combinatorial puzzle. A new method of solving Sudoku is\npresented, which involves formulating a puzzle as a special type of\ntransportation problem. This model allows one to solve puzzles with more than\none solution, keeping the constraints of the problem fixed, and simply changing\na cost matrix between solutions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 12:50:27 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Moufid", "Mansour", ""]]}, {"id": "1210.2587", "submitter": "Michal N\\'an\\'asi", "authors": "Michal N\\'an\\'asi, Tom\\'a\\v{s} Vina\\v{r} and Bro\\v{n}a Brejov\\'a", "title": "Sequence Annotation with HMMs: New Problems and Their Complexity", "comments": "10 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) and their variants were successfully used for\nseveral sequence annotation tasks. Traditionally, inference with HMMs is done\nusing the Viterbi and posterior decoding algorithms. However, recently a\nvariety of different optimization criteria and associated computational\nproblems were proposed. In this paper, we consider three HMM decoding criteria\nand prove their NP hardness. These criteria consider the set of states used to\ngenerate a certain sequence, but abstract from the exact locations of regions\nemitted by individual states. We also illustrate experimentally that these\ncriteria are useful for HIV recombination detection.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 12:55:17 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["N\u00e1n\u00e1si", "Michal", ""], ["Vina\u0159", "Tom\u00e1\u0161", ""], ["Brejov\u00e1", "Bro\u0148a", ""]]}, {"id": "1210.2610", "submitter": "Pierre Lescanne", "authors": "Katarzyna Grygiel, Pierre Lescanne (LIP)", "title": "Counting and generating lambda terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lambda calculus is the basis of functional programming and higher order proof\nassistants. However, little is known about combinatorial properties of lambda\nterms, in particular, about their asymptotic distribution and random\ngeneration. This paper tries to answer questions like: How many terms of a\ngiven size are there? What is a \"typical\" structure of a simply typable term?\nDespite their ostensible simplicity, these questions still remain unanswered,\nwhereas solutions to such problems are essential for testing compilers and\noptimizing programs whose expected efficiency depends on the size of terms. Our\napproach toward the afore-mentioned problems may be later extended to any\nlanguage with bound variables, i.e., with scopes and declarations. This paper\npresents two complementary approaches: one, theoretical, uses complex analysis\nand generating functions, the other, experimental, is based on a generator of\nlambda-terms. Thanks to de Bruijn indices, we provide three families of\nformulas for the number of closed lambda terms of a given size and we give four\nrelations between these numbers which have interesting combinatorial\ninterpretations. As a by-product of the counting formulas, we design an\nalgorithm for generating lambda terms. Performed tests provide us with\nexperimental data, like the average depth of bound variables and the average\nnumber of head lambdas. We also create random generators for various sorts of\nterms. Thereafter, we conduct experiments that answer questions like: What is\nthe ratio of simply typable terms among all terms? (Very small!) How are simply\ntypable lambda terms distributed among all lambda terms? (A typable term almost\nalways starts with an abstraction.) In this paper, abstractions and\napplications have size 1 and variables have size 0.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 14:27:19 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2012 20:35:52 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2013 07:43:27 GMT"}, {"version": "v4", "created": "Mon, 4 Feb 2013 19:43:17 GMT"}, {"version": "v5", "created": "Thu, 11 Apr 2013 11:36:48 GMT"}, {"version": "v6", "created": "Wed, 24 Apr 2013 16:33:33 GMT"}, {"version": "v7", "created": "Thu, 4 Jul 2013 06:16:16 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Grygiel", "Katarzyna", "", "LIP"], ["Lescanne", "Pierre", "", "LIP"]]}, {"id": "1210.2665", "submitter": "Ruchi Chaudhary", "authors": "Ruchi Chaudhary, J. Gordon Burleigh and David Fern\\'andez-Baca", "title": "Inferring Species Trees from Incongruent Multi-Copy Gene Trees Using the\n  Robinson-Foulds Distance", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for inferring species trees from multi-copy gene\ntrees. Our method is based on a generalization of the Robinson-Foulds (RF)\ndistance to multi-labeled trees (mul-trees), i.e., gene trees in which multiple\nleaves can have the same label. Unlike most previous phylogenetic methods using\ngene trees, this method does not assume that gene tree incongruence is caused\nby a single, specific biological process, such as gene duplication and loss,\ndeep coalescence, or lateral gene transfer. We prove that it is NP-hard to\ncompute the RF distance between two mul-trees, but it is easy to calculate the\ngeneralized RF distance between a mul-tree and a singly-labeled tree. Motivated\nby this observation, we formulate the RF supertree problem for mul-trees\n(MulRF), which takes a collection of mul-trees and constructs a species tree\nthat minimizes the total RF distance from the input mul-trees. We present a\nfast heuristic algorithm for the MulRF supertree problem. Simulation\nexperiments demonstrate that the MulRF method produces more accurate species\ntrees than gene tree parsimony methods when incongruence is caused by gene tree\nerror, duplications and losses, and/or lateral gene transfer. Furthermore, the\nMulRF heuristic runs quickly on data sets containing hundreds of trees with up\nto a hundred taxa.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 17:00:27 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Chaudhary", "Ruchi", ""], ["Burleigh", "J. Gordon", ""], ["Fern\u00e1ndez-Baca", "David", ""]]}, {"id": "1210.2698", "submitter": "Mikael Gast", "authors": "Mikael Gast, Mathias Hauptmann, Marek Karpinski", "title": "Improved Approximation Lower Bounds for Vertex Cover on Power Law Graphs\n  and Some Generalizations", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove new explicit inapproximability results for the Vertex Cover Problem\non the Power Law Graphs and some functional generalizations of that class of\ngraphs. Our results depend on special bounded degree amplifier constructions\nfor those classes of graphs and could be also of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 19:19:39 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Gast", "Mikael", ""], ["Hauptmann", "Mathias", ""], ["Karpinski", "Marek", ""]]}, {"id": "1210.2906", "submitter": "Venkatesan Chakaravarthy", "authors": "Venkatesan Chakaravarthy, Arindam Pal, Sambuddha Roy, Yogish Sabharwal", "title": "Scheduling Resources for Executing a Partial Set of Jobs", "comments": "Full version of paper accepted to FSTTCS'2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of choosing a minimum cost set of\nresources for executing a specified set of jobs. Each input job is an interval,\ndetermined by its start-time and end-time. Each resource is also an interval\ndetermined by its start-time and end-time; moreover, every resource has a\ncapacity and a cost associated with it. We consider two versions of this\nproblem. In the partial covering version, we are also given as input a number\nk, specifying the number of jobs that must be performed. The goal is to choose\nk jobs and find a minimum cost set of resources to perform the chosen k jobs\n(at any point of time the capacity of the chosen set of resources should be\nsufficient to execute the jobs active at that time). We present an O(log\nn)-factor approximation algorithm for this problem.\n  We also consider the prize collecting version, wherein every job also has a\npenalty associated with it. The feasible solution consists of a subset of the\njobs, and a set of resources, to perform the chosen subset of jobs. The goal is\nto find a feasible solution that minimizes the sum of the costs of the selected\nresources and the penalties of the jobs that are not selected. We present a\nconstant factor approximation algorithm for this problem\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 13:19:52 GMT"}], "update_date": "2012-10-11", "authors_parsed": [["Chakaravarthy", "Venkatesan", ""], ["Pal", "Arindam", ""], ["Roy", "Sambuddha", ""], ["Sabharwal", "Yogish", ""]]}, {"id": "1210.3135", "submitter": "Xiangrui Meng", "authors": "Xiangrui Meng and Michael W. Mahoney", "title": "Low-distortion Subspace Embeddings in Input-sparsity Time and\n  Applications to Robust Linear Regression", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-distortion embeddings are critical building blocks for developing random\nsampling and random projection algorithms for linear algebra problems. We show\nthat, given a matrix $A \\in \\R^{n \\times d}$ with $n \\gg d$ and a $p \\in [1,\n2)$, with a constant probability, we can construct a low-distortion embedding\nmatrix $\\Pi \\in \\R^{O(\\poly(d)) \\times n}$ that embeds $\\A_p$, the $\\ell_p$\nsubspace spanned by $A$'s columns, into $(\\R^{O(\\poly(d))}, \\| \\cdot \\|_p)$;\nthe distortion of our embeddings is only $O(\\poly(d))$, and we can compute $\\Pi\nA$ in $O(\\nnz(A))$ time, i.e., input-sparsity time. Our result generalizes the\ninput-sparsity time $\\ell_2$ subspace embedding by Clarkson and Woodruff\n[STOC'13]; and for completeness, we present a simpler and improved analysis of\ntheir construction for $\\ell_2$. These input-sparsity time $\\ell_p$ embeddings\nare optimal, up to constants, in terms of their running time; and the improved\nrunning time propagates to applications such as $(1\\pm \\epsilon)$-distortion\n$\\ell_p$ subspace embedding and relative-error $\\ell_p$ regression. For\n$\\ell_2$, we show that a $(1+\\epsilon)$-approximate solution to the $\\ell_2$\nregression problem specified by the matrix $A$ and a vector $b \\in \\R^n$ can be\ncomputed in $O(\\nnz(A) + d^3 \\log(d/\\epsilon) /\\epsilon^2)$ time; and for\n$\\ell_p$, via a subspace-preserving sampling procedure, we show that a $(1\\pm\n\\epsilon)$-distortion embedding of $\\A_p$ into $\\R^{O(\\poly(d))}$ can be\ncomputed in $O(\\nnz(A) \\cdot \\log n)$ time, and we also show that a\n$(1+\\epsilon)$-approximate solution to the $\\ell_p$ regression problem $\\min_{x\n\\in \\R^d} \\|A x - b\\|_p$ can be computed in $O(\\nnz(A) \\cdot \\log n + \\poly(d)\n\\log(1/\\epsilon)/\\epsilon^2)$ time. Moreover, we can improve the embedding\ndimension or equivalently the sample size to $O(d^{3+p/2} \\log(1/\\epsilon) /\n\\epsilon^2)$ without increasing the complexity.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 06:18:59 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2013 05:29:42 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2013 04:30:37 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Meng", "Xiangrui", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1210.3197", "submitter": "Jan Bul\\'anek", "authors": "Martin Babka, Jan Bul\\'anek, Vladim\\'ir \\v{C}un\\'at, Michal Kouck\\'y,\n  Michael Saks", "title": "On Online Labeling with Polynomially Many Labels", "comments": "15 pages, Presented at European Symposium on Algorithms 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online labeling problem with parameters n and m we are presented with\na sequence of n keys from a totally ordered universe U and must assign each\narriving key a label from the label set {1,2,...,m} so that the order of labels\n(strictly) respects the ordering on U. As new keys arrive it may be necessary\nto change the labels of some items; such changes may be done at any time at\nunit cost for each change. The goal is to minimize the total cost. An\nalternative formulation of this problem is the file maintenance problem, in\nwhich the items, instead of being labeled, are maintained in sorted order in an\narray of length m, and we pay unit cost for moving an item.\n  For the case m=cn for constant c>1, there are known algorithms that use at\nmost O(n log(n)^2) relabelings in total [Itai, Konheim, Rodeh, 1981], and it\nwas shown recently that this is asymptotically optimal [Bul\\'anek, Kouck\\'y,\nSaks, 2012]. For the case of m={\\Theta}(n^C) for C>1, algorithms are known that\nuse O(n log n) relabelings. A matching lower bound was claimed in [Dietz,\nSeiferas, Zhang, 2004]. That proof involved two distinct steps: a lower bound\nfor a problem they call prefix bucketing and a reduction from prefix bucketing\nto online labeling. The reduction seems to be incorrect, leaving a (seemingly\nsignificant) gap in the proof. In this paper we close the gap by presenting a\ncorrect reduction to prefix bucketing. Furthermore we give a simplified and\nimproved analysis of the prefix bucketing lower bound. This improvement allows\nus to extend the lower bounds for online labeling to the case where the number\nm of labels is superpolynomial in n. In particular, for superpolynomial m we\nget an asymptotically optimal lower bound {\\Omega}((n log n) / (log log m - log\nlog n)).\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 12:08:39 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Babka", "Martin", ""], ["Bul\u00e1nek", "Jan", ""], ["\u010cun\u00e1t", "Vladim\u00edr", ""], ["Kouck\u00fd", "Michal", ""], ["Saks", "Michael", ""]]}, {"id": "1210.3266", "submitter": "Marco Pellegrini", "authors": "Marco Pellegrini, Filippo Geraci, Miriam Baglioni", "title": "Detecting dense communities in large social and information networks\n  with the Core & Peel algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and characterizing dense subgraphs (tight communities) in social\nand information networks is an important exploratory tool in social network\nanalysis. Several approaches have been proposed that either (i) partition the\nwhole network into clusters, even in low density region, or (ii) are aimed at\nfinding a single densest community (and need to be iterated to find the next\none). As social networks grow larger both approaches (i) and (ii) result in\nalgorithms too slow to be practical, in particular when speed in analyzing the\ndata is required. In this paper we propose an approach that aims at balancing\nefficiency of computation and expressiveness and manageability of the output\ncommunity representation. We define the notion of a partial dense cover (PDC)\nof a graph. Intuitively a PDC of a graph is a collection of sets of nodes that\n(a) each set forms a disjoint dense induced subgraphs and (b) its removal\nleaves the residual graph without dense regions. Exact computation of PDC is an\nNP-complete problem, thus, we propose an efficient heuristic algorithms for\ncomputing a PDC which we christen Core and Peel. Moreover we propose a novel\nbenchmarking technique that allows us to evaluate algorithms for computing PDC\nusing the classical IR concepts of precision and recall even without a golden\nstandard. Tests on 25 social and technological networks from the Stanford Large\nNetwork Dataset Collection confirm that Core and Peel is efficient and attains\nvery high precison and recall.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 15:17:35 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Pellegrini", "Marco", ""], ["Geraci", "Filippo", ""], ["Baglioni", "Miriam", ""]]}, {"id": "1210.3277", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts, Paola Flocchini, Bernard Mans, Nicola Santoro", "title": "Shortest, Fastest, and Foremost Broadcast in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly dynamic networks rarely offer end-to-end connectivity at a given time.\nYet, connectivity in these networks can be established over time and space,\nbased on temporal analogues of multi-hop paths (also called {\\em journeys}).\nAttempting to optimize the selection of the journeys in these networks\nnaturally leads to the study of three cases: shortest (minimum hop), fastest\n(minimum duration), and foremost (earliest arrival) journeys. Efficient\ncentralized algorithms exists to compute all cases, when the full knowledge of\nthe network evolution is given.\n  In this paper, we study the {\\em distributed} counterparts of these problems,\ni.e. shortest, fastest, and foremost broadcast with termination detection\n(TDB), with minimal knowledge on the topology.\n  We show that the feasibility of each of these problems requires distinct\nfeatures on the evolution, through identifying three classes of dynamic graphs\nwherein the problems become gradually feasible: graphs in which the\nre-appearance of edges is {\\em recurrent} (class R), {\\em bounded-recurrent}\n(B), or {\\em periodic} (P), together with specific knowledge that are\nrespectively $n$ (the number of nodes), $\\Delta$ (a bound on the recurrence\ntime), and $p$ (the period). In these classes it is not required that all pairs\nof nodes get in contact -- only that the overall {\\em footprint} of the graph\nis connected over time.\n  Our results, together with the strict inclusion between $P$, $B$, and $R$,\nimplies a feasibility order among the three variants of the problem, i.e.\nTDB[foremost] requires weaker assumptions on the topology dynamics than\nTDB[shortest], which itself requires less than TDB[fastest]. Reversely, these\ndifferences in feasibility imply that the computational powers of $R_n$,\n$B_\\Delta$, and $P_p$ also form a strict hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 15:51:25 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2013 21:21:18 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2013 10:02:14 GMT"}, {"version": "v4", "created": "Fri, 8 Nov 2013 06:58:53 GMT"}, {"version": "v5", "created": "Wed, 27 Aug 2014 12:58:54 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Casteigts", "Arnaud", ""], ["Flocchini", "Paola", ""], ["Mans", "Bernard", ""], ["Santoro", "Nicola", ""]]}, {"id": "1210.3319", "submitter": "Shaun Joseph", "authors": "Shaun N. Joseph and Lisa C. DiPippo", "title": "Pseudo-scheduling: A New Approach to the Broadcast Scheduling Problem", "comments": "8th International Symposium on Algorithms for Sensor Systems,\n  Wireless Ad Hoc Networks and Autonomous Mobile Entities (ALGOSENSORS 2012),\n  13-14 September 2012, Ljubljana, Slovenia. 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The broadcast scheduling problem asks how a multihop network of broadcast\ntransceivers operating on a shared medium may share the medium in such a way\nthat communication over the entire network is possible. This can be naturally\nmodeled as a graph coloring problem via distance-2 coloring (L(1,1)-labeling,\nstrict scheduling). This coloring is difficult to compute and may require a\nnumber of colors quadratic in the graph degree. This paper introduces\npseudo-scheduling, a relaxation of distance-2 coloring. Centralized and\ndecentralized algorithms that compute pseudo-schedules with colors linear in\nthe graph degree are given and proved.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 18:51:02 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Joseph", "Shaun N.", ""], ["DiPippo", "Lisa C.", ""]]}, {"id": "1210.3368", "submitter": "Marc Shapiro", "authors": "Annette Bieniusa (INRIA Rocquencourt), Marek Zawirski (INRIA\n  Rocquencourt, LIP6), Nuno Pregui\\c{c}a (CITI), Marc Shapiro (INRIA\n  Rocquencourt, LIP6), Carlos Baquero (Universidade do Minho Departamento de\n  Inform\\'atica), Valter Balegas (CITI), S\\'ergio Duarte (CITI)", "title": "An optimized conflict-free replicated set", "comments": "No. RR-8083 (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eventual consistency of replicated data supports concurrent updates, reduces\nlatency and improves fault tolerance, but forgoes strong consistency.\nAccordingly, several cloud computing platforms implement eventually-consistent\ndata types. The set is a widespread and useful abstraction, and many replicated\nset designs have been proposed. We present a reasoning abstraction, permutation\nequivalence, that systematizes the characterization of the expected concurrency\nsemantics of concurrent types. Under this framework we present one of the\nexisting conflict-free replicated data types, Observed-Remove Set. Furthermore,\nin order to decrease the size of meta-data, we propose a new optimization to\navoid tombstones. This approach that can be transposed to other data types,\nsuch as maps, graphs or sequences.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 20:16:12 GMT"}], "update_date": "2012-10-15", "authors_parsed": [["Bieniusa", "Annette", "", "INRIA Rocquencourt"], ["Zawirski", "Marek", "", "INRIA\n  Rocquencourt, LIP6"], ["Pregui\u00e7a", "Nuno", "", "CITI"], ["Shapiro", "Marc", "", "INRIA\n  Rocquencourt, LIP6"], ["Baquero", "Carlos", "", "Universidade do Minho Departamento de\n  Inform\u00e1tica"], ["Balegas", "Valter", "", "CITI"], ["Duarte", "S\u00e9rgio", "", "CITI"]]}, {"id": "1210.3371", "submitter": "Stephan Holzer", "authors": "Magnus M. Halldorsson, Stephan Holzer, Pradipta Mitra and Roger\n  Wattenhofer", "title": "The Power of Non-Uniform Wireless Power", "comments": "Appeared in SODA 2013, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a fundamental measure for wireless interference in the SINR model\nknown as (weighted) inductive independence. This measure characterizes the\neffectiveness of using oblivious power --- when the power used by a transmitter\nonly depends on the distance to the receiver --- as a mechanism for improving\nwireless capacity.\n  We prove optimal bounds for inductive independence, implying a number of\nalgorithmic applications. An algorithm is provided that achieves --- due to\nexisting lower bounds --- capacity that is asymptotically best possible using\noblivious power assignments. Improved approximation algorithms are provided for\na number of problems for oblivious power and for power control, including\ndistributed scheduling, connectivity, secondary spectrum auctions, and dynamic\npacket scheduling.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 20:47:30 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 10:22:59 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Holzer", "Stephan", ""], ["Mitra", "Pradipta", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1210.3652", "submitter": "Murphy Choy", "authors": "Murphy Choy, Michelle Cheong", "title": "A Flexible Mixed Integer Programming framework for Nurse Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a nurse-scheduling model is developed using mixed integer\nprogramming model. It is deployed to a general care ward to replace and\nautomate the current manual approach for scheduling. The developed model\ndiffers from other similar studies in that it optimizes both hospitals\nrequirement as well as nurse preferences by allowing flexibility in the\ntransfer of nurses from different duties. The model also incorporated\nadditional policies which are part of the hospitals requirement but not part of\nthe legislations. Hospitals key primary mission is to ensure continuous ward\ncare service with appropriate number of nursing staffs and the right mix of\nnursing skills. The planning and scheduling is done to avoid additional non\nessential cost for hospital. Nurses preferences are taken into considerations\nsuch as the number of night shift and consecutive rest days. We will also\nreformulate problems from another paper which considers the penalty objective\nusing the model but without the flexible components. The models are built using\nAIMMS which solves the problem in very short amount of time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2012 22:33:40 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Choy", "Murphy", ""], ["Cheong", "Michelle", ""]]}, {"id": "1210.3833", "submitter": "Md. Shafiul Alam", "authors": "Md. Shafiul Alam, Asish Mukhopadhyay", "title": "Improved upper and lower bounds for the point placement problem", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The point placement problem is to determine the positions of a set of $n$\ndistinct points, P = {p1, p2, p3, ..., pn}, on a line uniquely, up to\ntranslation and reflection, from the fewest possible distance queries between\npairs of points. Each distance query corresponds to an edge in a graph, called\npoint placement graph ppg, whose vertex set is P. The uniqueness requirement of\nthe placement translates to line rigidity of the ppg. In this paper we show how\nto construct in 2 rounds a line rigid point placement graph of size 9n/7 +\nO(1). This improves the existing best result of 4n/3 + O(1). We also improve\nthe lower bound on 2-round algorithms from 17n/16 to 9n/8.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2012 20:18:36 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Alam", "Md. Shafiul", ""], ["Mukhopadhyay", "Asish", ""]]}, {"id": "1210.3962", "submitter": "Xiaojun Zhou", "authors": "Xiaojun Zhou", "title": "Improved Canonical Dual Algorithms for the Maxcut Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By introducing a quadratic perturbation to the canonical dual of the maxcut\nproblem, we transform the integer programming problem into a concave\nmaximization problem over a convex positive domain under some circumstances,\nwhich can be solved easily by the well-developed optimization methods.\nConsidering that there may exist no critical points in the dual feasible\ndomain, a reduction technique is used gradually to guarantee the feasibility of\nthe reduced solution, and a compensation technique is utilized to strengthen\nthe robustness of the solution. The similar strategy is also applied to the\nmaxcut problem with linear perturbation and its hybrid with quadratic\nperturbation. Experimental results demonstrate the effectiveness of the\nproposed algorithms when compared with other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 09:53:37 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Zhou", "Xiaojun", ""]]}, {"id": "1210.3978", "submitter": "Gregory Gutin", "authors": "J. Crampton, R. Crowston, G. Gutin, M. Jones, and M. S. Ramanujan", "title": "Fixed-Parameter Tractability of Workflow Satisfiability in the Presence\n  of Seniority Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The workflow satisfiability problem is concerned with determining whether it\nis possible to find an allocation of authorized users to the steps in a\nworkflow in such a way that all constraints are satisfied. The problem is\nNP-hard in general, but is known to be fixed-parameter tractable for certain\nclasses of constraints. The known results on fixed-parameter tractability rely\non the symmetry (in some sense) of the constraints. In this paper, we provide\nthe first results that establish fixed-parameter tractability of the\nsatisfiability problem when the constraints are asymmetric. In particular, we\nintroduce the notion of seniority constraints, in which the execution of steps\nis determined, in part, by the relative seniority of the users that perform\nthem. Our results require new techniques, which make use of tree decompositions\nof the graph of the binary relation defining the constraint. Finally, we\nestablish a lower bound for the hardness of the workflow satisfiability\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 11:11:53 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Crampton", "J.", ""], ["Crowston", "R.", ""], ["Gutin", "G.", ""], ["Jones", "M.", ""], ["Ramanujan", "M. S.", ""]]}, {"id": "1210.4053", "submitter": "Omry Tuval", "authors": "Avinatan Hassidim, Haim Kaplan, Omry Tuval", "title": "Joint Cache Partition and Job Assignment on Multi-Core Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicore shared cache processors pose a challenge for designers of embedded\nsystems who try to achieve minimal and predictable execution time of workloads\nconsisting of several jobs. To address this challenge the cache is statically\npartitioned among the cores and the jobs are assigned to the cores so as to\nminimize the makespan. Several heuristic algorithms have been proposed that\njointly decide how to partition the cache among the cores and assign the jobs.\nWe initiate a theoretical study of this problem which we call the joint cache\npartition and job assignment problem.\n  By a careful analysis of the possible cache partitions we obtain a constant\napproximation algorithm for this problem. For some practical special cases we\nobtain a 2-approximation algorithm, and show how to improve the approximation\nfactor even further by allowing the algorithm to use additional cache. We also\nstudy possible improvements that can be obtained by allowing dynamic cache\npartitions and dynamic job assignments.\n  We define a natural special case of the well known scheduling problem on\nunrelated machines in which machines are ordered by \"strength\". Our joint cache\npartition and job assignment problem generalizes this scheduling problem which\nwe think is of independent interest. We give a polynomial time algorithm for\nthis scheduling problem for instances obtained by fixing the cache partition in\na practical case of the joint cache partition and job assignment problem where\njob loads are step functions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 14:47:01 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2012 09:03:28 GMT"}, {"version": "v3", "created": "Thu, 22 Nov 2012 17:16:23 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Hassidim", "Avinatan", ""], ["Kaplan", "Haim", ""], ["Tuval", "Omry", ""]]}, {"id": "1210.4081", "submitter": "Bogdan Savchynskyy", "authors": "Bogdan Savchynskyy and Stefan Schmidt", "title": "Getting Feasible Variable Estimates From Infeasible Ones: MRF Local\n  Polytope Study", "comments": "20 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for construction of approximate feasible primal\nsolutions from dual ones for large-scale optimization problems possessing\ncertain separability properties. Whereas infeasible primal estimates can\ntypically be produced from (sub-)gradients of the dual function, it is often\nnot easy to project them to the primal feasible set, since the projection\nitself has a complexity comparable to the complexity of the initial problem. We\npropose an alternative efficient method to obtain feasibility and show that its\nproperties influencing the convergence to the optimum are similar to the\nproperties of the Euclidean projection. We apply our method to the local\npolytope relaxation of inference problems for Markov Random Fields and\ndemonstrate its superiority over existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 15:55:34 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Savchynskyy", "Bogdan", ""], ["Schmidt", "Stefan", ""]]}, {"id": "1210.4352", "submitter": "George Mertzios", "authors": "George B. Mertzios", "title": "The Recognition of Simple-Triangle Graphs and of Linear-Interval Orders\n  is Polynomial", "comments": "27 pages, 4 figures, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersection graphs of geometric objects have been extensively studied, both\ndue to their interesting structure and their numerous applications; prominent\nexamples include interval graphs and permutation graphs. In this paper we study\na natural graph class that generalizes both interval and permutation graphs,\nnamely \\emph{simple-triangle} graphs. Simple-triangle graphs - also known as\n\\emph{PI} graphs (for Point-Interval) - are the intersection graphs of\ntriangles that are defined by a point on a line $L_{1}$ and an interval on a\nparallel line $L_{2}$. They lie naturally between permutation and trapezoid\ngraphs, which are the intersection graphs of line segments between $L_{1}$ and\n$L_{2}$ and of trapezoids between $L_{1}$ and $L_{2}$, respectively. Although\nvarious efficient recognition algorithms for permutation and trapezoid graphs\nare well known to exist, the recognition of simple-triangle graphs has remained\nan open problem since their introduction by Corneil and Kamula three decades\nago. In this paper we resolve this problem by proving that simple-triangle\ngraphs can be recognized in polynomial time. As a consequence, our algorithm\nalso solves a longstanding open problem in the area of partial orders, namely\nthe recognition of \\emph{linear-interval orders}, i.e. of partial orders\n$P=P_{1}\\cap P_{2}$, where $P_{1}$ is a linear order and $P_{2}$ is an interval\norder. This is one of the first results on recognizing partial orders $P$ that\nare the intersection of orders from two different classes $\\mathcal{P}_{1}$ and\n$\\mathcal{P}_{2}$. In complete contrast to this, partial orders $P$ which are\nthe intersection of orders from the same class $\\mathcal{P}$ have been\nextensively investigated, and in most cases the complexity status of these\nrecognition problems has been already established.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 10:53:54 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2014 14:23:38 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Mertzios", "George B.", ""]]}, {"id": "1210.4367", "submitter": "Bjarke Hammersholt Roune", "authors": "Laurent Evain, Mathias Lederer, Bjarke Hammersholt Roune", "title": "Connect Four and Graph Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the standard decomposition, a way of decomposing a labeled graph\ninto a sum of certain labeled subgraphs. We motivate this graph-theoretic\nconcept by relating it to Connect Four decompositions of standard sets. We\nprove that all standard decompositions can be generated in polynomial time,\nwhich implies that all Connect Four decompositions can be generated in\npolynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 11:31:29 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 17:28:58 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2013 21:43:40 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Evain", "Laurent", ""], ["Lederer", "Mathias", ""], ["Roune", "Bjarke Hammersholt", ""]]}, {"id": "1210.4446", "submitter": "Pradipta Mitra", "authors": "Eyjolfur I. Asgeirsson, Magnus M. Halldorsson and Pradipta Mitra", "title": "Wireless Network Stability in the SINR Model", "comments": "10 pages, appeared in SIROCCO'12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stability of wireless networks under stochastic arrival\nprocesses of packets, and design efficient, distributed algorithms that achieve\nstability in the SINR (Signal to Interference and Noise Ratio) interference\nmodel.\n  Specifically, we make the following contributions. We give a distributed\nalgorithm that achieves $\\Omega(\\frac{1}{\\log^2 n})$-efficiency on all networks\n(where $n$ is the number of links in the network), for all length monotone,\nsub-linear power assignments. For the power control version of the problem, we\ngive a distributed algorithm with $\\Omega(\\frac{1}{\\log n(\\log n + \\log \\log\n\\Delta)})$-efficiency (where $\\Delta$ is the length diversity of the link set).\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 15:09:47 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Asgeirsson", "Eyjolfur I.", ""], ["Halldorsson", "Magnus M.", ""], ["Mitra", "Pradipta", ""]]}, {"id": "1210.4594", "submitter": "Vijay  Vazirani", "authors": "Vijay V. Vazirani", "title": "A Simplification of the MV Matching Algorithm and its Proof", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For all practical purposes, the Micali-Vazirani general graph maximum\nmatching algorithm is still the most efficient known algorithm for the problem.\nThe purpose of this paper is to provide a complete proof of correctness of the\nalgorithm in the simplest possible terms; graph-theoretic machinery developed\nfor this purpose also helps simplify the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 23:18:11 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2012 21:09:54 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2013 18:59:08 GMT"}, {"version": "v4", "created": "Fri, 9 Aug 2013 17:11:30 GMT"}, {"version": "v5", "created": "Fri, 23 Aug 2013 20:52:54 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Vazirani", "Vijay V.", ""]]}, {"id": "1210.4663", "submitter": "Zhijie Wang", "authors": "Jack Wang", "title": "A PRQ Search Method for Probabilistic Objects", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes an PQR search method for probabilistic objects. The\nmain idea of our method is to use a strategy called \\textit{pre-approximation}\nthat can reduce the initial problem to a highly simplified version, implying\nthat it makes the rest of steps easy to tackle. In particular, this strategy\nitself is pretty simple and easy to implement. Furthermore, motivated by the\ncost analysis, we further optimize our solution. The optimizations are mainly\nbased on two insights: (\\romannumeral 1) the number of \\textit{effective\nsubdivision}s is no more than 1; and (\\romannumeral 2) an entity with the\nlarger \\textit{span} is more likely to subdivide a single region. We\ndemonstrate the effectiveness and efficiency of our proposed approaches through\nextensive experiments under various experimental settings.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 08:14:50 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2013 17:57:50 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2013 15:19:49 GMT"}, {"version": "v4", "created": "Sun, 28 Apr 2013 23:13:19 GMT"}, {"version": "v5", "created": "Thu, 4 Jul 2013 07:09:45 GMT"}, {"version": "v6", "created": "Tue, 22 Oct 2013 17:59:49 GMT"}, {"version": "v7", "created": "Tue, 3 Jul 2018 13:38:39 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Jack", ""]]}, {"id": "1210.4673", "submitter": "Guillaume Aupy", "authors": "Guillaume Aupy and Anne Benoit", "title": "Approximation algorithms for energy, reliability and makespan\n  optimization problems", "comments": "Work supported by ANR RESCUE", "journal-ref": null, "doi": null, "report-no": "INRIA Research Report 8107", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of scheduling an application on a\nparallel computational platform. The application is a particular task graph,\neither a linear chain of tasks, or a set of independent tasks. The platform is\nmade of identical processors, whose speed can be dynamically modified. It is\nalso subject to failures: if a processor is slowed down to decrease the energy\nconsumption, it has a higher chance to fail. Therefore, the scheduling problem\nrequires to re-execute or replicate tasks (i.e., execute twice a same task,\neither on the same processor, or on two distinct processors), in order to\nincrease the reliability. It is a tri-criteria problem: the goal is to minimize\nthe energy consumption, while enforcing a bound on the total execution time\n(the makespan), and a constraint on the reliability of each task.\n  Our main contribution is to propose approximation algorithms for these\nparticular classes of task graphs. For linear chains, we design a fully\npolynomial time approximation scheme. However, we show that there exists no\nconstant factor approximation algorithm for independent tasks, unless P=NP, and\nwe are able in this case to propose an approximation algorithm with a\nrelaxation on the makespan constraint.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 09:03:28 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Aupy", "Guillaume", ""], ["Benoit", "Anne", ""]]}, {"id": "1210.4728", "submitter": "Zeev Nutov", "authors": "Guy Kortsarz and Zeev Nutov", "title": "Approximating Source Location and Star Survivable Network Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Source Location (SL) problems the goal is to select a mini-mum cost source\nset $S \\subseteq V$ such that the connectivity (or flow) $\\psi(S,v)$ from $S$\nto any node $v$ is at least the demand $d_v$ of $v$. In many SL problems\n$\\psi(S,v)=d_v$ if $v \\in S$, namely, the demand of nodes selected to $S$ is\ncompletely satisfied. In a node-connectivity variant suggested recently by\nFukunaga, every node $v$ gets a \"bonus\" $p_v \\leq d_v$ if it is selected to\n$S$. Fukunaga showed that for undirected graphs one can achieve ratio $O(k \\ln\nk)$ for his variant, where $k=\\max_{v \\in V}d_v$ is the maximum demand. We\nimprove this by achieving ratio $\\min\\{p^*\\lnk,k\\}\\cdot O(\\ln (k/q^*))$ for a\nmore general version with node capacities, where $p^*=\\max_{v \\in V} p_v$ is\nthe maximum bonus and $q^*=\\min_{v \\in V} q_v$ is the minimum capacity. In\nparticular, for the most natural case $p^*=1$ considered by Fukunaga, we\nimprove the ratio from $O(k \\ln k)$ to $O(\\ln^2k)$. We also get ratio $O(k)$\nfor the edge-connectivity version, for which no ratio that depends on $k$ only\nwas known before. To derive these results, we consider a particular case of the\nSurvivable Network (SN) problem when all edges of positive cost form a star. We\ngive ratio $O(\\min\\{\\ln n,\\ln^2 k\\})$ for this variant, improving over the best\nratio known for the general case $O(k^3 \\ln n)$ of Chuzhoy and Khanna.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 13:08:59 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2013 12:42:40 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 11:44:00 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Kortsarz", "Guy", ""], ["Nutov", "Zeev", ""]]}, {"id": "1210.4811", "submitter": "Yahav Nussbaum", "authors": "Jakub {\\L}\\k{a}cki, Yahav Nussbaum, Piotr Sankowski and Christian\n  Wulff-Nilsen", "title": "Single Source - All Sinks Max Flows in Planar Digraphs", "comments": "25 pages, 4 figures; extended abstract appeared in FOCS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G = (V,E) be a planar n-vertex digraph. Consider the problem of computing\nmax st-flow values in G from a fixed source s to all sinks t in V\\{s}. We show\nhow to solve this problem in near-linear O(n log^3 n) time. Previously, no\nbetter solution was known than running a single-source single-sink max flow\nalgorithm n-1 times, giving a total time bound of O(n^2 log n) with the\nalgorithm of Borradaile and Klein.\n  An important implication is that all-pairs max st-flow values in G can be\ncomputed in near-quadratic time. This is close to optimal as the output size is\nTheta(n^2). We give a quadratic lower bound on the number of distinct max flow\nvalues and an Omega(n^3) lower bound for the total size of all min cut-sets.\nThis distinguishes the problem from the undirected case where the number of\ndistinct max flow values is O(n).\n  Previous to our result, no algorithm which could solve the all-pairs max flow\nvalues problem faster than the time of Theta(n^2) max-flow computations for\nevery planar digraph was known.\n  This result is accompanied with a data structure that reports min cut-sets.\nFor fixed s and all t, after O(n^{3/2} log^{3/2} n) preprocessing time, it can\nreport the set of arcs C crossing a min st-cut in time roughly proportional to\nthe size of C.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 18:17:25 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["\u0141\u0105cki", "Jakub", ""], ["Nussbaum", "Yahav", ""], ["Sankowski", "Piotr", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1210.4822", "submitter": "Amitabh Trehan", "authors": "Shay Kutten, Gopal Pandurangan, David Peleg, Peter Robinson, and\n  Amitabh Trehan", "title": "Sublinear Bounds for Randomized Leader Election", "comments": "Best Paper Award winner at ICDCN 2013, CDCN 2013 14th International\n  Conference on Distributed Computing and Networking. Tata Institute of\n  Fundamental Research, Mumbai, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns {\\em randomized} leader election in synchronous\ndistributed networks. A distributed leader election algorithm is presented for\ncomplete $n$-node networks that runs in O(1) rounds and (with high probability)\nuses only $O(\\sqrt{n}\\log^{3/2} n)$ messages to elect a unique leader (with\nhigh probability). When considering the \"explicit\" variant of leader election\nwhere eventually every node knows the identity of the leader, our algorithm\nyields the asymptotically optimal bounds of O(1) rounds and O(n) messages. This\nalgorithm is then extended to one solving leader election on any connected\nnon-bipartite $n$-node graph $G$ in $O(\\tau(G))$ time and\n$O(\\tau(G)\\sqrt{n}\\log^{3/2} n)$ messages, where $\\tau(G)$ is the mixing time\nof a random walk on $G$. The above result implies highly efficient (sublinear\nrunning time and messages) leader election algorithms for networks with small\nmixing times, such as expanders and hypercubes. In contrast, previous leader\nelection algorithms had at least linear message complexity even in complete\ngraphs. Moreover, super-linear message lower bounds are known for\ntime-efficient {\\em deterministic} leader election algorithms.\n  Finally, we present an almost matching lower bound for randomized leader\nelection, showing that $\\Omega(\\sqrt n)$ messages are needed for any leader\nelection algorithm that succeeds with probability at least $1/e + \\eps$, for\nany small constant $\\eps > 0$.\n  We view our results as a step towards understanding the randomized complexity\nofleader election in distributed networks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 19:03:21 GMT"}, {"version": "v2", "created": "Wed, 15 May 2013 14:18:18 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Kutten", "Shay", ""], ["Pandurangan", "Gopal", ""], ["Peleg", "David", ""], ["Robinson", "Peter", ""], ["Trehan", "Amitabh", ""]]}, {"id": "1210.4874", "submitter": "Hoong Chuin Lau", "authors": "Hoong Chuin Lau, William Yeoh, Pradeep Varakantham, Duc Thien Nguyen,\n  Huaxing Chen", "title": "Dynamic Stochastic Orienteering Problems for Risk-Aware Applications", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-448-458", "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orienteering problems (OPs) are a variant of the well-known prize-collecting\ntraveling salesman problem, where the salesman needs to choose a subset of\ncities to visit within a given deadline. OPs and their extensions with\nstochastic travel times (SOPs) have been used to model vehicle routing problems\nand tourist trip design problems. However, they suffer from two limitations\ntravel times between cities are assumed to be time independent and the route\nprovided is independent of the risk preference (with respect to violating the\ndeadline) of the user. To address these issues, we make the following\ncontributions: We introduce (1) a dynamic SOP (DSOP) model, which is an\nextension of SOPs with dynamic (time-dependent) travel times; (2) a\nrisk-sensitive criterion to allow for different risk preferences; and (3) a\nlocal search algorithm to solve DSOPs with this risk-sensitive criterion. We\nevaluated our algorithms on a real-world dataset for a theme park navigation\nproblem as well as synthetic datasets employed in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:42:27 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lau", "Hoong Chuin", ""], ["Yeoh", "William", ""], ["Varakantham", "Pradeep", ""], ["Nguyen", "Duc Thien", ""], ["Chen", "Huaxing", ""]]}, {"id": "1210.4891", "submitter": "Sergiy Matusevych", "authors": "Sergiy Matusevych, Alex Smola, Amr Ahmed", "title": "Hokusai - Sketching Streams in Real Time", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-594-603", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Hokusai, a real time system which is able to capture frequency\ninformation for streams of arbitrary sequences of symbols. The algorithm uses\nthe CountMin sketch as its basis and exploits the fact that sketching is\nlinear. It provides real time statistics of arbitrary events, e.g. streams of\nqueries as a function of time. We use a factorizing approximation to provide\npoint estimates at arbitrary (time, item) combinations. Queries can be answered\nin constant time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:46:50 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Matusevych", "Sergiy", ""], ["Smola", "Alex", ""], ["Ahmed", "Amr", ""]]}, {"id": "1210.4902", "submitter": "David Sontag", "authors": "David Sontag, Do Kook Choe, Yitao Li", "title": "Efficiently Searching for Frustrated Cycles in MAP Inference", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-795-804", "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual decomposition provides a tractable framework for designing algorithms\nfor finding the most probable (MAP) configuration in graphical models. However,\nfor many real-world inference problems, the typical decomposition has a large\nintegrality gap, due to frustrated cycles. One way to tighten the relaxation is\nto introduce additional constraints that explicitly enforce cycle consistency.\nEarlier work showed that cluster-pursuit algorithms, which iteratively\nintroduce cycle and other higherorder consistency constraints, allows one to\nexactly solve many hard inference problems. However, these algorithms\nexplicitly enumerate a candidate set of clusters, limiting them to triplets or\nother short cycles. We solve the search problem for cycle constraints, giving a\nnearly linear time algorithm for finding the most frustrated cycle of arbitrary\nlength. We show how to use this search algorithm together with the dual\ndecomposition framework and clusterpursuit. The new algorithm exactly solves\nMAP inference problems arising from relational classification and stereo\nvision.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:51:21 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Sontag", "David", ""], ["Choe", "Do Kook", ""], ["Li", "Yitao", ""]]}, {"id": "1210.4906", "submitter": "Bogdan Savchynskyy", "authors": "Bogdan Savchynskyy, Stefan Schmidt, Joerg Kappes, Christoph Schnoerr", "title": "Efficient MRF Energy Minimization via Adaptive Diminishing Smoothing", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-746-755", "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear programming relaxation of an energy minimization\nproblem for Markov Random Fields. The dual objective of this problem can be\ntreated as a concave and unconstrained, but non-smooth function. The idea of\nsmoothing the objective prior to optimization was recently proposed in a series\nof papers. Some of them suggested the idea to decrease the amount of smoothing\n(so called temperature) while getting closer to the optimum. However, no\ntheoretical substantiation was provided. We propose an adaptive smoothing\ndiminishing algorithm based on the duality gap between relaxed primal and dual\nobjectives and demonstrate the efficiency of our approach with a smoothed\nversion of Sequential Tree-Reweighted Message Passing (TRW-S) algorithm. The\nstrategy is applicable to other algorithms as well, avoids adhoc tuning of the\nsmoothing during iterations, and provably guarantees convergence to the\noptimum.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:52:03 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Savchynskyy", "Bogdan", ""], ["Schmidt", "Stefan", ""], ["Kappes", "Joerg", ""], ["Schnoerr", "Christoph", ""]]}, {"id": "1210.5048", "submitter": "Stephanie Wehner", "authors": "Andrew C. Doherty and Stephanie Wehner", "title": "Convergence of SDP hierarchies for polynomial optimization on the\n  hypersphere", "comments": "45 pages, amsmath, comments welcome, for readers in quantum\n  information: contains de Finetti theorem, v2: improved explanations,\n  additional bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math-ph math.MP quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to bound the accuracy of a family of semi-definite programming\nrelaxations for the problem of polynomial optimization on the hypersphere. Our\nmethod is inspired by a set of results from quantum information known as\nquantum de Finetti theorems. In particular, we prove a de Finetti theorem for a\nspecial class of real symmetric matrices to establish the existence of\napproximate representing measures for moment matrix relaxations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 08:07:03 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2013 13:20:59 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Doherty", "Andrew C.", ""], ["Wehner", "Stephanie", ""]]}, {"id": "1210.5118", "submitter": "Matthew Butler", "authors": "Matthew Butler and Dimitar Kazakov", "title": "Creating a level playing field for all symbols in a discretization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time series analysis research there is a strong interest in discrete\nrepresentations of real valued data streams. One approach that emerged over a\ndecade ago and is still considered state-of-the-art is the Symbolic Aggregate\nApproximation algorithm. This discretization algorithm was the first symbolic\napproach that mapped a real-valued time series to a symbolic representation\nthat was guaranteed to lower-bound Euclidean distance. The interest of this\npaper concerns the SAX assumption of data being highly Gaussian and the use of\nthe standard normal curve to choose partitions to discretize the data. Though\nnot necessarily, but generally, and certainly in its canonical form, the SAX\napproach chooses partitions on the standard normal curve that would produce an\nequal probability for each symbol in a finite alphabet to occur. This procedure\nis generally valid as a time series is normalized before the rest of the SAX\nalgorithm is applied. However there exists a caveat to this assumption of\nequi-probability due to the intermediate step of Piecewise Aggregate\nApproximation (PAA). What we will show in this paper is that when PAA is\napplied the distribution of the data is indeed altered, resulting in a\nshrinking standard deviation that is proportional to the number of points used\nto create a segment of the PAA representation and the degree of\nauto-correlation within the series. Data that exhibits statistically\nsignificant auto-correlation is less affected by this shrinking distribution.\nAs the standard deviation of the data contracts, the mean remains the same,\nhowever the distribution is no longer standard normal and therefore the\npartitions based on the standard normal curve are no longer valid for the\nassumption of equal probability.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 13:44:45 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Butler", "Matthew", ""], ["Kazakov", "Dimitar", ""]]}, {"id": "1210.5155", "submitter": "Zsolt Szil\\'agyi", "authors": "Zsolt Szil\\'agyi", "title": "Computation of Jeffrey-Kirwan residue using Gr\\\"obner basis", "comments": "12 pages. Section 6 rewritten", "journal-ref": "Journal of Symbolic Com- putation 79 (2017), 327-341", "doi": "10.1016/j.jsc.2016.02.011", "report-no": null, "categories": "math.AC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jeffrey-Kirwan residue is a powerful tool for computation of intersection\nnumbers or volume of symplectic quotients. In this article, we give an\nalgorithm to compute it using Gr\\\"obner bases. Our result is parallel to that\nof Cattani-Dickenstein for Grothendieck residues.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 15:09:24 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 18:32:48 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Szil\u00e1gyi", "Zsolt", ""]]}, {"id": "1210.5227", "submitter": "Richard Peng", "authors": "Gary Miller, Richard Peng", "title": "Approximate Maximum Flow on Separable Undirected Graphs", "comments": "to appear in SODA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present faster algorithms for approximate maximum flow in undirected\ngraphs with good separator structures, such as bounded genus, minor free, and\ngeometric graphs. Given such a graph with $n$ vertices, $m$ edges along with a\nrecursive $\\sqrt{n}$-vertex separator structure, our algorithm finds an\n$1-\\epsilon$ approximate maximum flow in time $\\tilde{O}(m^{6/5}\n\\poly{\\epsilon^{-1}})$, ignoring poly-logarithmic terms. Similar speedups are\nalso achieved for separable graphs with larger size separators albeit with\nlarger run times. These bounds also apply to image problems in two and three\ndimensions.\n  Key to our algorithm is an intermediate problem that we term grouped $L_2$\nflow, which exists between maximum flows and electrical flows. Our algorithm\nalso makes use of spectral vertex sparsifiers in order to remove vertices while\npreserving the energy dissipation of electrical flows. We also give faster\nspectral vertex sparsification algorithms on well separated graphs, which may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 19:26:00 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Miller", "Gary", ""], ["Peng", "Richard", ""]]}, {"id": "1210.5363", "submitter": "Micha{\\l} Pilipczuk", "authors": "Micha{\\l} Pilipczuk", "title": "Computing cutwidth and pathwidth of semi-complete digraphs via degree\n  orderings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notions of cutwidth and pathwidth of digraphs play a central role in the\ncontainment theory for tournaments, or more generally semi-complete digraphs,\ndeveloped in a recent series of papers by Chudnovsky, Fradkin, Kim, Scott, and\nSeymour [2, 3, 4, 8, 9, 11]. In this work we introduce a new approach to\ncomputing these width measures on semi-complete digraphs, via degree orderings.\nUsing the new technique we are able to reprove the main results of [2, 9] in a\nunified and significantly simplified way, as well as obtain new results. First,\nwe present polynomial-time approximation algorithms for both cutwidth and\npathwidth, faster and simpler than the previously known ones; the most\nsignificant improvement is in case of pathwidth, where instead of previously\nknown O(OPT)-approximation in fixed-parameter tractable time [6] we obtain a\nconstant-factor approximation in polynomial time. Secondly, by exploiting the\nnew set of obstacles for cutwidth and pathwidth, we show that topological\ncontainment and immersion in semi-complete digraphs can be tested in\nsingle-exponential fixed-parameter tractable time. Finally, we present how the\nnew approach can be used to obtain exact fixed-parameter tractable algorithms\nfor cutwidth and pathwidth, with single- exponential running time dependency on\nthe optimal width.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 09:44:21 GMT"}], "update_date": "2012-10-22", "authors_parsed": [["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1210.5664", "submitter": "Reza Bosagh Zadeh", "authors": "Reza Bosagh Zadeh and Gunnar Carlsson", "title": "Characterizing Properties for Q-Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We uniquely characterize two members of the Q-Clustering family in an\naxiomatic framework. We introduce properties that use known tree constructions\nfor the purpose of characterization. To characterize the Max-Sum clustering\nalgorithm, we use the Gomory-Hu construction, and to characterize\nSingle-Linkage, we use the Maximum Spanning Tree. Although at first glance it\nseems these properties are `obviously' all that are necessary to characterize\nMax-Sum and Single-Linkage, we show that this is not the case, by investigating\nhow subsets of properties interact. We conclude by proposing additions to the\ntaxonomy of clustering paradigms currently in use.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2012 22:54:07 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Zadeh", "Reza Bosagh", ""], ["Carlsson", "Gunnar", ""]]}, {"id": "1210.5677", "submitter": "Amit Weinstein", "authors": "Noga Alon and Amit Weinstein", "title": "Local Correction with Constant Error Rate", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Boolean function f of n variables is said to be q-locally correctable if,\ngiven a black-box access to a function g which is \"close\" to an isomorphism\nf_sigma(x)=f_sigma(x_1, ..., x_n) = f(x_sigma(1), ..., x_sigma(n)) of f, we can\ncompute f_sigma(x) for any x in {0,1}^n with good probability using q queries\nto g. It is known that degree d polynomials are O(2^d)-locally correctable, and\nthat most k-juntas are O(k log k)-locally correctable, where the closeness\nparameter, or more precisely the distance between g and f_sigma, is required to\nbe exponentially small (in d and k respectively).\n  In this work we relax the requirement for the closeness parameter by allowing\nthe distance between the functions to be a constant. We first investigate the\nfamily of juntas, and show that almost every k-junta is O(k log^2 k)-locally\ncorrectable for any distance epsilon < 0.001. A similar result is shown for the\nfamily of partially symmetric functions, that is functions which are\nindifferent to any reordering of all but a constant number of their variables.\nFor both families, the algorithms provided here use non-adaptive queries and\nare applicable to most but not all functions of each family (as it is shown to\nbe impossible to locally correct all of them).\n  Our approach utilizes the measure of symmetric influence introduced in the\nrecent analysis of testing partial symmetry of functions.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2012 02:29:56 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Alon", "Noga", ""], ["Weinstein", "Amit", ""]]}, {"id": "1210.5729", "submitter": "Hao-Hsiang Hung", "authors": "Hao-Hsiang Hung", "title": "Survival Network Design of Doubling Dimension Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We investigate the Minimum Weight 2-Edge-Connected Spanning Subgraph (2-ECSS)\nproblem in an arbitrary metric space of doubling dimension and show a\npolynomial time randomized $(1+\\epsilon)$-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2012 14:01:30 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Hung", "Hao-Hsiang", ""]]}, {"id": "1210.5786", "submitter": "Ho-Lin Chen", "authors": "Ho-Lin Chen, Ming-Yang Kao", "title": "Optimizing Tile Concentrations to Minimize Errors and Time for DNA Tile\n  Self-Assembly Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA tile self-assembly has emerged as a rich and promising primitive for\nnano-technology. This paper studies the problems of minimizing assembly time\nand error rate by changing the tile concentrations because changing the tile\nconcentrations is easy to implement in actual lab experiments. We prove that\nsetting the concentration of tile $T_i$ proportional to the square root of\n$N_i$ where $N_i$ is the number of times $T_i$ appears outside the seed\nstructure in the final assembled shape minimizes the rate of growth errors for\nrectilinear tile systems. We also show that the same concentrations minimize\nthe expected assembly time for a feasible class of tile systems. Moreover, for\ngeneral tile systems, given tile concentrations, we can approximate the\nexpected assembly time with high accuracy and probability by running only a\npolynomial number of simulations in the size of the target shape.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 01:14:02 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Chen", "Ho-Lin", ""], ["Kao", "Ming-Yang", ""]]}, {"id": "1210.5955", "submitter": "Ricardo Corr\\^ea", "authors": "Ricardo C. Corr\\^ea, Pablo M. S. Farias, and Cr\\'iston P. de Souza", "title": "Insertion and Sorting in a Sequence of Numbers Minimizing the Maximum\n  Sum of a Contiguous Subsequence", "comments": "This paper has been submitted for journal publication", "journal-ref": null, "doi": "10.1016/j.jda.2013.03.003", "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $A$ be a sequence of $n \\geq 0$ real numbers. A subsequence of $A$ is a\nsequence of contiguous elements of $A$. A \\emph{maximum scoring subsequence} of\n$A$ is a subsequence with largest sum of its elements, which can be found in\nO(n) time by Kadane's dynamic programming algorithm. We consider in this paper\ntwo problems involving maximal scoring subsequences of a sequence. Both of\nthese problems arise in the context of buffer memory minimization in computer\nnetworks. The first one, which is called {\\sc Insertion in a Sequence with\nScores (ISS)}, consists in inserting a given real number $x$ in $A$ in such a\nway to minimize the sum of a maximum scoring subsequence of the resulting\nsequence, which can be easily done in $O(n^2)$ time by successively applying\nKadane's algorithm to compute the maximum scoring subsequence of the resulting\nsequence corresponding to each possible insertion position for $x$. We show in\nthis paper that the ISS problem can be solved in linear time and space with a\nmore specialized algorithm. The second problem we consider in this paper is the\n{\\sc Sorting a Sequence by Scores (SSS)} one, stated as follows: find a\npermutation $A'$ of $A$ that minimizes the sum of a maximum scoring\nsubsequence. We show that the SSS problem is strongly NP-Hard and give a\n2-approximation algorithm for it.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 16:25:18 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 13:05:36 GMT"}], "update_date": "2013-03-21", "authors_parsed": [["Corr\u00eaa", "Ricardo C.", ""], ["Farias", "Pablo M. S.", ""], ["de Souza", "Cr\u00edston P.", ""]]}, {"id": "1210.5968", "submitter": "Tomislav Petrovi\\'c dipl. ing.", "authors": "Tomislav Petrovi\\'c", "title": "A pair of universal sequence-set betting strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the sequence-set betting game, a generalization of An. A.\nMuchnik's non-monotonic betting game. Instead of successively partitioning the\ninfinite binary strings by their value of a bit at a chosen position, as in the\nnon-monotonic game, the player is allowed to partition the strings into any two\nclopen sets with equal measure. We show that, while there is no single\ncomputable sequence-set betting strategy that predicts all non-Martin-L\\\"of\nrandom strings, we can construct two strategies such that every\nnon-Martin-L\\\"of random string is predicted by at least one of them.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 16:46:09 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2012 08:20:29 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2012 12:13:36 GMT"}, {"version": "v4", "created": "Mon, 14 Jan 2013 16:26:07 GMT"}, {"version": "v5", "created": "Thu, 28 Feb 2013 08:15:15 GMT"}, {"version": "v6", "created": "Thu, 14 Mar 2013 13:48:30 GMT"}, {"version": "v7", "created": "Fri, 29 Mar 2013 12:58:16 GMT"}, {"version": "v8", "created": "Tue, 3 Dec 2013 18:33:43 GMT"}, {"version": "v9", "created": "Tue, 22 Dec 2015 15:21:39 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Petrovi\u0107", "Tomislav", ""]]}, {"id": "1210.6118", "submitter": "EPTCS", "authors": "Anton Wijs (Eindhoven University of Technology), Dragan\n  Bo\\v{s}na\\v{c}ki (Eindhoven University of Technology), Stefan Edelkamp\n  (University of Bremen)", "title": "Proceedings First Workshop on GRAPH Inspection and Traversal Engineering", "comments": null, "journal-ref": "EPTCS 99, 2012", "doi": "10.4204/EPTCS.99", "report-no": null, "categories": "cs.DS cs.DM cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are the proceedings of the First Workshop on GRAPH Inspection and\nTraversal Engineering (GRAPHITE 2012), which took place on April 1, 2012 in\nTallinn, Estonia, as a satellite event of the 15th European Joint Conferences\non Theory and Practice of Software (ETAPS 2012).\n  The topic of the GRAPHITE workshop is graph search in all its forms in\ncomputer science. Graph search algorithms tend to have common characteristics,\nsuch as duplicate state detection, independent of their application domain.\nOver the past few years, it has been shown that the scalability of such\nalgorithms can be dramatically improved by using, e.g., external memory, by\nexploiting parallel architectures, such as clusters, multi-core CPUs, and\ngraphics processing units, and by using heuristics to guide the search. The\ngoal of this event is to gather scientists from different communities, such as\nmodel checking, artificial intelligence planning, game playing, and algorithm\nengineering, who do research on graph search algorithms, such that awareness of\neach others' work is increased.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 03:38:33 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Wijs", "Anton", "", "Eindhoven University of Technology"], ["Bo\u0161na\u010dki", "Dragan", "", "Eindhoven University of Technology"], ["Edelkamp", "Stefan", "", "University of Bremen"]]}, {"id": "1210.6176", "submitter": "Emanuele Giaquinta", "authors": "Emanuele Giaquinta and Szymon Grabowski", "title": "New algorithms for binary jumbled pattern matching", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipl.2013.04.013", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pattern $P$ and a text $T$, both strings over a binary alphabet, the\nbinary jumbled string matching problem consists in telling whether any\npermutation of $P$ occurs in $T$. The indexed version of this problem, i.e.,\npreprocessing a string to efficiently answer such permutation queries, is hard\nand has been studied in the last few years. Currently the best bounds for this\nproblem are $O(n^2/\\log^2 n)$ (with O(n) space and O(1) query time) and\n$O(r^2\\log r)$ (with O(|L|) space and $O(\\log|L|)$ query time), where $r$ is\nthe length of the run-length encoding of $T$ and $|L| = O(n)$ is the size of\nthe index. In this paper we present new results for this problem. Our first\nresult is an alternative construction of the index by Badkobeh et al. that\nobtains a trade-off between the space and the time complexity. It has\n$O(r^2\\log k + n/k)$ complexity to build the index, $O(\\log k)$ query time, and\nuses $O(n/k + |L|)$ space, where $k$ is a parameter. The second result is an\n$O(n^2 \\log^2 w / w)$ algorithm (with O(n) space and O(1) query time), based on\nword-level parallelism where $w$ is the word size in bits.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 09:51:03 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 06:09:51 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Giaquinta", "Emanuele", ""], ["Grabowski", "Szymon", ""]]}, {"id": "1210.6286", "submitter": "James Aspnes", "authors": "James Aspnes", "title": "A one-bit swap object using test-and-sets and a max register", "comments": null, "journal-ref": null, "doi": null, "report-no": "YALEU/DCS/TR-1464", "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a linearizable, wait-free implementation of a one-bit swap object\nfrom a single max register and an unbounded array of test-and-set bits. Each\nswap operation takes at most three steps. Using standard randomized\nconstructions, the max register and test-and-set bits can be replaced by\nread-write registers, at the price of raising the cost of a swap operation to\nan expected O(max(log n, min(log t, n))) steps, where t is the number of times\nthe swap object has previously changed its value and n is the number of\nprocesses.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 16:49:56 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Aspnes", "James", ""]]}, {"id": "1210.6287", "submitter": "Parikshit Ram", "authors": "Ryan R. Curtin, Parikshit Ram, Alexander G. Gray", "title": "Fast Exact Max-Kernel Search", "comments": "Under submission in SIAM Data Mining conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide applicability of kernels makes the problem of max-kernel search\nubiquitous and more general than the usual similarity search in metric spaces.\nWe focus on solving this problem efficiently. We begin by characterizing the\ninherent hardness of the max-kernel search problem with a novel notion of\ndirectional concentration. Following that, we present a method to use an $O(n\n\\log n)$ algorithm to index any set of objects (points in $\\Real^\\dims$ or\nabstract objects) directly in the Hilbert space without any explicit feature\nrepresentations of the objects in this space. We present the first provably\n$O(\\log n)$ algorithm for exact max-kernel search using this index. Empirical\nresults for a variety of data sets as well as abstract objects demonstrate up\nto 4 orders of magnitude speedup in some cases. Extensions for approximate\nmax-kernel search are also presented.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 16:51:31 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2012 19:14:20 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Curtin", "Ryan R.", ""], ["Ram", "Parikshit", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1210.6411", "submitter": "EPTCS", "authors": "Andreas Beckmann (Goethe-Universit\\\"at Frankfurt), Jaroslaw Fedorowicz\n  (Goethe-Universit\\\"at Frankfurt), J\\\"org Keller (FernUniversit\\\"at in Hagen),\n  Ulrich Meyer (Goethe-Universit\\\"at Frankfurt)", "title": "A structural analysis of the A5/1 state transition graph", "comments": "In Proceedings GRAPHITE 2012, arXiv:1210.6118", "journal-ref": "EPTCS 99, 2012, pp. 5-19", "doi": "10.4204/EPTCS.99.4", "report-no": null, "categories": "cs.DC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe efficient algorithms to analyze the cycle structure of the graph\ninduced by the state transition function of the A5/1 stream cipher used in GSM\nmobile phones and report on the results of the implementation. The analysis is\nperformed in five steps utilizing HPC clusters, GPGPU and external memory\ncomputation. A great reduction of this huge state transition graph of 2^64\nnodes is achieved by focusing on special nodes in the first step and removing\nleaf nodes that can be detected with limited effort in the second step. This\nstep does not break the overall structure of the graph and keeps at least one\nnode on every cycle. In the third step the nodes of the reduced graph are\nconnected by weighted edges. Since the number of nodes is still huge an\nefficient bitslice approach is presented that is implemented with NVIDIA's CUDA\nframework and executed on several GPUs concurrently. An external memory\nalgorithm based on the STXXL library and its parallel pipelining feature\nfurther reduces the graph in the fourth step. The result is a graph containing\nonly cycles that can be further analyzed in internal memory to count the number\nand size of the cycles. This full analysis which previously would take months\ncan now be completed within a few days and allows to present structural results\nfor the full graph for the first time. The structure of the A5/1 graph deviates\nnotably from the theoretical results for random mappings.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 00:32:55 GMT"}], "update_date": "2012-10-25", "authors_parsed": [["Beckmann", "Andreas", "", "Goethe-Universit\u00e4t Frankfurt"], ["Fedorowicz", "Jaroslaw", "", "Goethe-Universit\u00e4t Frankfurt"], ["Keller", "J\u00f6rg", "", "FernUniversit\u00e4t in Hagen"], ["Meyer", "Ulrich", "", "Goethe-Universit\u00e4t Frankfurt"]]}, {"id": "1210.6465", "submitter": "Carola Winzen", "authors": "Benjamin Doerr and Carola Winzen", "title": "Black-Box Complexity: Breaking the $O(n \\log n)$ Barrier of LeadingOnes", "comments": "12 pages, to appear in the Proc. of Artificial Evolution 2011, LNCS\n  7401, Springer, 2012. For the unrestricted black-box complexity of\n  LeadingOnes there is now a tight $\\Theta(n \\log\\log n)$ bound, cf.\n  http://eccc.hpi-web.de/report/2012/087/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the unrestricted black-box complexity of the $n$-dimensional\nXOR- and permutation-invariant LeadingOnes function class is $O(n \\log (n) /\n\\log \\log n)$. This shows that the recent natural looking $O(n\\log n)$ bound is\nnot tight.\n  The black-box optimization algorithm leading to this bound can be implemented\nin a way that only 3-ary unbiased variation operators are used. Hence our bound\nis also valid for the unbiased black-box complexity recently introduced by\nLehre and Witt (GECCO 2010). The bound also remains valid if we impose the\nadditional restriction that the black-box algorithm does not have access to the\nobjective values but only to their relative order (ranking-based black-box\ncomplexity).\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 09:17:49 GMT"}], "update_date": "2012-10-25", "authors_parsed": [["Doerr", "Benjamin", ""], ["Winzen", "Carola", ""]]}, {"id": "1210.6624", "submitter": "Richard Mayr", "authors": "Lorenzo Clemente and Richard Mayr", "title": "Advanced Automata Minimization", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": "EDI-INF-RR-1414", "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm to reduce the size of nondeterministic\nBuchi word automata, while retaining their language. Additionally, we describe\nmethods to solve PSPACE-complete automata problems like universality,\nequivalence and inclusion for much larger instances (1-3 orders of magnitude)\nthan before. This can be used to scale up applications of automata in formal\nverification tools and decision procedures for logical theories. The algorithm\nis based on new transition pruning techniques. These use criteria based on\ncombinations of backward and forward trace inclusions. Since these relations\nare themselves PSPACE-complete, we describe methods to compute good\napproximations of them in polynomial time. Extensive experiments show that the\naverage-case complexity of our algorithm scales quadratically. The size\nreduction of the automata depends very much on the class of instances, but our\nalgorithm consistently outperforms all previous techniques by a wide margin. We\ntested our algorithm on Buchi automata derived from LTL-formulae, many classes\nof random automata and automata derived from mutual exclusion protocols, and\ncompared its performance to the well-known automata tool GOAL.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 18:43:12 GMT"}], "update_date": "2012-10-25", "authors_parsed": [["Clemente", "Lorenzo", ""], ["Mayr", "Richard", ""]]}, {"id": "1210.6646", "submitter": "Hector J. Garcia", "authors": "Hector J. Garcia, Igor L. Markov and Andrew W. Cross", "title": "Efficient Inner-product Algorithm for Stabilizer States", "comments": "14 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CG cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale quantum computation is likely to require massive quantum error\ncorrection (QEC). QEC codes and circuits are described via the stabilizer\nformalism, which represents stabilizer states by keeping track of the operators\nthat preserve them. Such states are obtained by stabilizer circuits (consisting\nof CNOT, Hadamard and Phase only) and can be represented compactly on\nconventional computers using Omega(n^2) bits, where n is the number of qubits.\nAlthough techniques for the efficient simulation of stabilizer circuits have\nbeen studied extensively, techniques for efficient manipulation of stabilizer\nstates are not currently available. To this end, we design new algorithms for:\n(i) obtaining canonical generators for stabilizer states, (ii) obtaining\ncanonical stabilizer circuits, and (iii) computing the inner product between\nstabilizer states. Our inner-product algorithm takes O(n^3) time in general,\nbut observes quadratic behavior for many practical instances relevant to QECC\n(e.g., GHZ states). We prove that each n-qubit stabilizer state has exactly\n4(2^n - 1) nearest-neighbor stabilizer states, and verify this claim\nexperimentally using our algorithms. We design techniques for representing\narbitrary quantum states using stabilizer frames and generalize our algorithms\nto compute the inner product between two such frames.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 19:59:00 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2013 20:09:04 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2013 23:15:59 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Garcia", "Hector J.", ""], ["Markov", "Igor L.", ""], ["Cross", "Andrew W.", ""]]}, {"id": "1210.6853", "submitter": "Arkadi Nemirovski", "authors": "Aharon Ben-Tal and Arkadi Nemirovski", "title": "On solving large scale polynomial convex problems by randomized\n  first-order algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most attractive recent approaches to processing well-structured\nlarge-scale convex optimization problems is based on smooth convex-concave\nsaddle point reformu-lation of the problem of interest and solving the\nresulting problem by a fast First Order saddle point method utilizing\nsmoothness of the saddle point cost function. In this paper, we demonstrate\nthat when the saddle point cost function is polynomial, the precise gra-dients\nof the cost function required by deterministic First Order saddle point\nalgorithms and becoming prohibitively computationally expensive in the\nextremely large-scale case, can be replaced with incomparably cheaper\ncomputationally unbiased random estimates of the gradients. We show that for\nlarge-scale problems with favourable geometry, this randomization accelerates,\nprogressively as the sizes of the problem grow, the solution process. This\nextends significantly previous results on acceleration by randomization, which,\nto the best of our knowledge, dealt solely with bilinear saddle point problems.\nWe illustrate our theoretical findings by instructive and encouraging numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 14:25:39 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2013 16:51:14 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 18:40:01 GMT"}, {"version": "v4", "created": "Wed, 21 May 2014 18:06:43 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Ben-Tal", "Aharon", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1210.6917", "submitter": "Julia Wolf", "authors": "Eli Ben-Sasson, Noga Ron-Zewi, Madhur Tulsiani, Julia Wolf", "title": "Sampling-based proofs of almost-periodicity results and algorithmic\n  applications", "comments": "28 pages", "journal-ref": null, "doi": "10.1007/978-3-662-43948-7_79", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new combinatorial proofs of known almost-periodicity results for\nsumsets of sets with small doubling in the spirit of Croot and Sisask, whose\nalmost-periodicity lemma has had far-reaching implications in additive\ncombinatorics. We provide an alternative (and L^p-norm free) point of view,\nwhich allows for proofs to easily be converted to probabilistic algorithms that\ndecide membership in almost-periodic sumsets of dense subsets of F_2^n.\n  As an application, we give a new algorithmic version of the quasipolynomial\nBogolyubov-Ruzsa lemma recently proved by Sanders. Together with the results by\nthe last two authors, this implies an algorithmic version of the quadratic\nGoldreich-Levin theorem in which the number of terms in the quadratic Fourier\ndecomposition of a given function is quasipolynomial in the error parameter,\ncompared with an exponential dependence previously proved by the authors. It\nalso improves the running time of the algorithm to have quasipolynomial\ndependence instead of an exponential one.\n  We also give an application to the problem of finding large subspaces in\nsumsets of dense sets. Green showed that the sumset of a dense subset of F_2^n\ncontains a large subspace. Using Fourier analytic methods, Sanders proved that\nsuch a subspace must have dimension bounded below by a constant times the\ndensity times n. We provide an alternative (and L^p norm-free) proof of a\ncomparable bound, which is analogous to a recent result of Croot, Laba and\nSisask in the integers.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 17:24:57 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Ben-Sasson", "Eli", ""], ["Ron-Zewi", "Noga", ""], ["Tulsiani", "Madhur", ""], ["Wolf", "Julia", ""]]}, {"id": "1210.6963", "submitter": "Lane A. Hemaspaandra", "authors": "Lane A. Hemaspaandra, Rahman Lavaee, Curtis Menton", "title": "Schulze and Ranked-Pairs Voting are Fixed-Parameter Tractable to Bribe,\n  Manipulate, and Control", "comments": null, "journal-ref": null, "doi": null, "report-no": "URCS-TR-2012-982", "categories": "cs.GT cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schulze and ranked-pairs elections have received much attention recently, and\nthe former has quickly become a quite widely used election system. For many\ncases these systems have been proven resistant to bribery, control, or\nmanipulation, with ranked pairs being particularly praised for being NP-hard\nfor all three of those. Nonetheless, the present paper shows that with respect\nto the number of candidates, Schulze and ranked-pairs elections are\nfixed-parameter tractable to bribe, control, and manipulate: we obtain uniform,\npolynomial-time algorithms whose degree does not depend on the number of\ncandidates. We also provide such algorithms for some weighted variants of these\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 19:37:24 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2012 03:56:20 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2013 19:34:00 GMT"}, {"version": "v4", "created": "Tue, 16 Jul 2013 20:46:52 GMT"}, {"version": "v5", "created": "Sat, 21 Jun 2014 18:44:21 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Hemaspaandra", "Lane A.", ""], ["Lavaee", "Rahman", ""], ["Menton", "Curtis", ""]]}, {"id": "1210.7605", "submitter": "Zdenek Dvorak", "authors": "Zdenek Dvorak and Ken-ichi Kawarabayashi", "title": "List-coloring embedded graphs", "comments": "14 pages, 0 figures, accepted to SODA'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any fixed surface Sigma of genus g, we give an algorithm to decide\nwhether a graph G of girth at least five embedded in Sigma is colorable from an\nassignment of lists of size three in time O(|V(G)|). Furthermore, we can allow\na subgraph (of any size) with at most s components to be precolored, at the\nexpense of increasing the time complexity of the algorithm to\nO(|V(G)|^{K(g+s)+1}) for some absolute constant K; in both cases, the\nmultiplicative constant hidden in the O-notation depends on g and s. This also\nenables us to find such a coloring when it exists. The idea of the algorithm\ncan be applied to other similar problems, e.g., 5-list-coloring of graphs on\nsurfaces.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 09:56:59 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Dvorak", "Zdenek", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1210.7638", "submitter": "Jack Wang", "authors": "Jack Wang", "title": "Finding Efficient Region in The Plane with Line segments", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathscr O$ be a set of $n$ disjoint obstacles in $\\mathbb{R}^2$,\n$\\mathscr M$ be a moving object. Let $s$ and $l$ denote the starting point and\nmaximum path length of the moving object $\\mathscr M$, respectively. Given a\npoint $p$ in ${R}^2$, we say the point $p$ is achievable for $\\mathscr M$ such\nthat $\\pi(s,p)\\leq l$, where $\\pi(\\cdot)$ denotes the shortest path length in\nthe presence of obstacles. One is to find a region $\\mathscr R$ such that, for\nany point $p\\in \\mathbb{R}^2$, if it is achievable for $\\mathscr M$, then $p\\in\n\\mathscr R$; otherwise, $p\\notin \\mathscr R$. In this paper, we restrict our\nattention to the case of line-segment obstacles. To tackle this problem, we\ndevelop three algorithms. We first present a simpler-version algorithm for the\nsake of intuition. Its basic idea is to reduce our problem to computing the\nunion of a set of circular visibility regions (CVRs). This algorithm takes\n$O(n^3)$ time. By analysing its dominant steps, we break through its bottleneck\nby using the short path map (SPM) technique to obtain those circles\n(unavailable beforehand), yielding an $O(n^2\\log n)$ algorithm. Owing to the\nfinding above, the third algorithm also uses the SPM technique. It however,\ndoes not continue to construct the CVRs. Instead, it directly traverses each\nregion of the SPM to trace the boundaries, the final algorithm obtains $O(n\\log\nn)$ complexity.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 12:27:18 GMT"}, {"version": "v10", "created": "Tue, 3 Jul 2018 15:19:18 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 20:38:43 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2013 21:11:41 GMT"}, {"version": "v4", "created": "Tue, 26 Mar 2013 15:16:28 GMT"}, {"version": "v5", "created": "Fri, 29 Mar 2013 01:02:52 GMT"}, {"version": "v6", "created": "Thu, 4 Jul 2013 10:36:54 GMT"}, {"version": "v7", "created": "Tue, 9 Jul 2013 05:14:32 GMT"}, {"version": "v8", "created": "Tue, 12 Nov 2013 12:30:28 GMT"}, {"version": "v9", "created": "Sat, 30 Jun 2018 11:18:25 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Jack", ""]]}, {"id": "1210.7656", "submitter": "Thomas Vidick", "authors": "Assaf Naor, Oded Regev, Thomas Vidick", "title": "Efficient rounding for the noncommutative Grothendieck inequality", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical Grothendieck inequality has applications to the design of\napproximation algorithms for NP-hard optimization problems. We show that an\nalgorithmic interpretation may also be given for a noncommutative\ngeneralization of the Grothendieck inequality due to Pisier and Haagerup. Our\nmain result, an efficient rounding procedure for this inequality, leads to a\nconstant-factor polynomial time approximation algorithm for an optimization\nproblem which generalizes the Cut Norm problem of Frieze and Kannan, and is\nshown here to have additional applications to robust principle component\nanalysis and the orthogonal Procrustes problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 13:29:52 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Naor", "Assaf", ""], ["Regev", "Oded", ""], ["Vidick", "Thomas", ""]]}, {"id": "1210.7919", "submitter": "G Ramakrishna", "authors": "N.S. Narayanaswamy and G. Ramakrishna", "title": "Tree t-spanners in Outerplanar Graphs via Supply Demand Partition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tree t-spanner of an unweighted graph G is a spanning tree T such that for\nevery two vertices their distance in T is at most t times their distance in G.\nGiven an unweighted graph G and a positive integer t as input, the tree\nt-spanner problem is to compute a tree t-spanner of G if one exists. This\ndecision problem is known to be NP-complete even in the restricted class of\nunweighted planar graphs. We present a linear-time reduction from tree\nt-spanner in outerplanar graphs to the supply-demand tree partition problem.\nBased on this reduction, we obtain a linear-time algorithm to solve tree\nt-spanner in outerplanar graphs. Consequently, we show that the minimum value\nof t for which an input outerplanar graph on n vertices has a tree t-spanner\ncan be found in O(n log n) time.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 07:45:35 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2013 15:50:10 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2013 07:39:50 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Narayanaswamy", "N. S.", ""], ["Ramakrishna", "G.", ""]]}, {"id": "1210.7970", "submitter": "Pascal Lenzner", "authors": "Pascal Lenzner", "title": "Greedy Selfish Network Creation", "comments": "28 pages, 8 figures. An extended abstract of this work was accepted\n  at WINE'12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze greedy equilibria (GE) for the well-known model of\nselfish network creation by Fabrikant et al.[PODC'03]. GE are interesting for\ntwo reasons: (1) they model outcomes found by agents which prefer smooth\nadaptations over radical strategy-changes, (2) GE are outcomes found by agents\nwhich do not have enough computational resources to play optimally. In the\nmodel of Fabrikant et al. agents correspond to Internet Service Providers which\nbuy network links to improve their quality of network usage. It is known that\ncomputing a best response in this model is NP-hard. Hence, poly-time agents are\nlikely not to play optimally. But how good are networks created by such agents?\nWe answer this question for very simple agents. Quite surprisingly, naive\ngreedy play suffices to create remarkably stable networks. Specifically, we\nshow that in the SUM version, where agents attempt to minimize their average\ndistance to all other agents, GE capture Nash equilibria (NE) on trees and that\nany GE is in 3-approximate NE on general networks. For the latter we also\nprovide a lower bound of 3/2 on the approximation ratio. For the MAX version,\nwhere agents attempt to minimize their maximum distance, we show that any\nGE-star is in 2-approximate NE and any GE-tree having larger diameter is in\n6/5-approximate NE. Both bounds are tight. We contrast these positive results\nby providing a linear lower bound on the approximation ratio for the MAX\nversion on general networks in GE. This result implies a locality gap of\n$\\Omega(n)$ for the metric min-max facility location problem, where n is the\nnumber of clients.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 11:35:39 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Lenzner", "Pascal", ""]]}, {"id": "1210.8014", "submitter": "Marc Labadens", "authors": "Marc Labadens, Daniel Pomar\\`ede, Damien Chapon, Romain Teyssier,\n  Fr\\'ed\\'eric Bournaud, Florent Renaud, Nicolas Grandjouan", "title": "Volume Rendering of AMR Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution simulations often rely on the Adaptive Mesh Resolution (AMR)\ntechnique to optimize memory consumption versus attainable precision. While\nthis technique allows for dramatic improvements in terms of computing\nperformance, the analysis and visualization of its data outputs remain\nchallenging. The lack of effective volume renderers for the octree-based AMR\nused by the RAMSES simulation program has led to the development of the\nsolutions presented in this paper. Two custom algorithms are discussed, based\non the splatting and the ray-casting techniques. Their usage is illustrated in\nthe context of the visualization of a high-resolution, 6000-processor\nsimulation of a Milky Way-like galaxy. Performance obtained in terms of memory\nmanagement and parallelism speedup are presented.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 14:04:08 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2013 14:39:37 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Labadens", "Marc", ""], ["Pomar\u00e8de", "Daniel", ""], ["Chapon", "Damien", ""], ["Teyssier", "Romain", ""], ["Bournaud", "Fr\u00e9d\u00e9ric", ""], ["Renaud", "Florent", ""], ["Grandjouan", "Nicolas", ""]]}, {"id": "1210.8099", "submitter": "Lane A. Hemaspaandra", "authors": "Lane A. Hemaspaandra and Ryan Williams", "title": "An Atypical Survey of Typical-Case Heuristic Algorithms", "comments": "This article is currently scheduled to appear in the December 2012\n  issue of SIGACT News", "journal-ref": null, "doi": null, "report-no": "URCS-TR-2012-984", "categories": "cs.CC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristic approaches often do so well that they seem to pretty much always\ngive the right answer. How close can heuristic algorithms get to always giving\nthe right answer, without inducing seismic complexity-theoretic consequences?\nThis article first discusses how a series of results by Berman, Buhrman,\nHartmanis, Homer, Longpr\\'{e}, Ogiwara, Sch\\\"{o}ening, and Watanabe, from the\nearly 1970s through the early 1990s, explicitly or implicitly limited how well\nheuristic algorithms can do on NP-hard problems. In particular, many desirable\nlevels of heuristic success cannot be obtained unless severe, highly unlikely\ncomplexity class collapses occur. Second, we survey work initiated by Goldreich\nand Wigderson, who showed how under plausible assumptions deterministic\nheuristics for randomized computation can achieve a very high frequency of\ncorrectness. Finally, we consider formal ways in which theory can help explain\nthe effectiveness of heuristics that solve NP-hard problems in practice.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 17:50:49 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Hemaspaandra", "Lane A.", ""], ["Williams", "Ryan", ""]]}, {"id": "1210.8123", "submitter": "Fatemeh Rajabi-Alni", "authors": "Fatemeh Rajabi-Alni, Alireza Bagheri", "title": "Limited-Capacity Many-To-Many Point Matching in One Dimension", "comments": "20pages, 13figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two point sets S and T, in a many-to-many matching between S and T each\npoint in S is assigned to one or more points in T and vice versa. A\ngeneralization of the many-to-many matching problem is the limited capacity\nmany-to-many matching problem, where the number of points that can be matched\nto each point, that is the capacity of each point, is limited. In this paper,\nwe provide an O(n^2) time algorithm for the one dimensional minimum-cost\nlimited capacity many-to-many matching problem, where |S| + |T| = n. Our\nalgorithm improves the best previous time complexity of O(k(n^2)), that in\nwhich k is the largest capacity of the points in the union of S and T. In this\nproblem, both S and T lie on the real line and the cost of matching s in S to t\nin T is equal to the distance between s and t.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 19:02:53 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Rajabi-Alni", "Fatemeh", ""], ["Bagheri", "Alireza", ""]]}, {"id": "1210.8303", "submitter": "Loukas Georgiadis", "authors": "Loukas Georgiadis and Robert E. Tarjan", "title": "Dominator Tree Certification and Independent Spanning Trees", "comments": "Rewritten abstract and introduction. Added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does one verify that the output of a complicated program is correct? One\ncan formally prove that the program is correct, but this may be beyond the\npower of existing methods. Alternatively one can check that the output produced\nfor a particular input satisfies the desired input-output relation, by running\na checker on the input-output pair. Then one only needs to prove the\ncorrectness of the checker. But for some problems even such a checker may be\ntoo complicated to formally verify. There is a third alternative: augment the\noriginal program to produce not only an output but also a correctness\ncertificate, with the property that a very simple program (whose correctness is\neasy to prove) can use the certificate to verify that the input-output pair\nsatisfies the desired input-output relation.\n  We consider the following important instance of this general question: How\ndoes one verify that the dominator tree of a flow graph is correct? Existing\nfast algorithms for finding dominators are complicated, and even verifying the\ncorrectness of a dominator tree in the absence of additional information seems\ncomplicated. We define a correctness certificate for a dominator tree, show how\nto use it to easily verify the correctness of the tree, and show how to augment\nfast dominator-finding algorithms so that they produce a correctness\ncertificate. We also relate the dominator certificate problem to the problem of\nfinding independent spanning trees in a flow graph, and we develop algorithms\nto find such trees. All our algorithms run in linear time. Previous algorithms\napply just to the special case of only trivial dominators, and they take at\nleast quadratic time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 11:32:55 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2013 17:58:59 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2013 14:51:42 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "1210.8338", "submitter": "Yonatan Goldhirsh", "authors": "Sourav Chakraborty, Eldar Fischer, Yonatan Goldhirsh, Arie Matsliah", "title": "On the Power of Conditional Samples in Distribution Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we define and examine the power of the {\\em\nconditional-sampling} oracle in the context of distribution-property testing.\nThe conditional-sampling oracle for a discrete distribution $\\mu$ takes as\ninput a subset $S \\subset [n]$ of the domain, and outputs a random sample $i\n\\in S$ drawn according to $\\mu$, conditioned on $S$ (and independently of all\nprior samples). The conditional-sampling oracle is a natural generalization of\nthe ordinary sampling oracle in which $S$ always equals $[n]$.\n  We show that with the conditional-sampling oracle, testing uniformity,\ntesting identity to a known distribution, and testing any label-invariant\nproperty of distributions is easier than with the ordinary sampling oracle. On\nthe other hand, we also show that for some distribution properties the\nsample-complexity remains near-maximal even with conditional sampling.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 14:18:59 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2012 13:26:26 GMT"}, {"version": "v3", "created": "Tue, 8 Apr 2014 14:23:02 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Chakraborty", "Sourav", ""], ["Fischer", "Eldar", ""], ["Goldhirsh", "Yonatan", ""], ["Matsliah", "Arie", ""]]}, {"id": "1210.8386", "submitter": "Travis Gagie", "authors": "Travis Gagie", "title": "Grammar-Based Construction of Indexes for Binary Jumbled Pattern\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how, given a straight-line program with $g$ rules for a binary string\n$B$ of length $n$, in $O(g^{2 / 3} n^{4 / 3})$ time we can build a linear-space\nindex such that, given $m$ and $c$, in O(1) time we can determine whether there\nis a substring of $B$ with length $m$ containing exactly $c$ copies of 1. If we\nuse $O(n \\log n)$ space for the index, then we can list all such substrings\nusing $O(m)$ time per substring.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 16:45:08 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2012 07:52:56 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 18:55:18 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Gagie", "Travis", ""]]}, {"id": "1210.8439", "submitter": "Bernhard Haeupler", "authors": "Mohsen Ghaffari, Bernhard Haeupler", "title": "Near Optimal Leader Election in Multi-Hop Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present distributed randomized leader election protocols for multi-hop\nradio networks that elect a leader in almost the same time $T_{BC}$ required\nfor broadcasting a message. For the setting without collision detection, our\nalgorithm runs with high probability in $O(D \\log \\frac{n}{D} + \\log^3 n)\n\\min\\{\\log\\log n,\\log \\frac{n}{D}\\}$ rounds on any $n$-node network with\ndiameter $D$. Since $T_{BC} = \\Theta(D \\log \\frac{n}{D} + \\log^2 n)$ is a lower\nbound, our upper bound is optimal up to a factor of at most $\\log \\log n$ and\nthe extra $\\log n$ factor on the additive term. This algorithm is furthermore\nthe first $O(n)$ time algorithm for this setting.\n  Our algorithms improve over a 25 year old simulation approach of Bar-Yehuda,\nGoldreich and Itai with a $O(T_{BC} \\log n)$ running time: In 1987 they\ndesigned a fast broadcast protocol and subsequently in 1989 they showed how it\ncan be used to simulate one round of a single-hop network that has collision\ndetection in $T_{BC}$ time. The prime application of this simulation was to\nsimulate Willards single-hop leader election protocol, which elects a leader in\n$O(\\log n)$ rounds with high probability and $O(\\log \\log n)$ rounds in\nexpectation. While it was subsequently shown that Willards bounds are tight, it\nwas unclear whether the simulation approach is optimal. Our results break this\nbarrier and essentially remove the logarithmic slowdown over the broadcast time\n$T_{BC}$ by going away from the simulation approach.\n  We also give a distributed randomized leader election algorithm for the\nsetting with collision detection that runs in $O(D + \\log n \\log \\log n) \\cdot\n\\min\\{\\log \\log n, \\log \\frac{n}{D}\\}$ rounds. This round complexity is optimal\nup to $O(\\log \\log n)$ factors and improves over a deterministic algorithm that\nrequires $\\Theta(n)$ rounds independently of the diameter $D$.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 18:55:48 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 06:18:23 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Haeupler", "Bernhard", ""]]}]