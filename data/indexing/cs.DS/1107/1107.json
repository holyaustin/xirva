[{"id": "1107.0056", "submitter": "Rudini Sampaio", "authors": "Victor Campos, Cl\\'audia Linhares-Sales, Ana Karolinna Maia, Nicolas\n  Martins, Rudini Menezes Sampaio", "title": "Fixed parameter algorithms for restricted coloring problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain polynomial time algorithms to determine the acyclic\nchromatic number, the star chromatic number, the Thue chromatic number, the\nharmonious chromatic number and the clique chromatic number of $P_4$-tidy\ngraphs and $(q,q-4)$-graphs, for every fixed $q$. These classes include\ncographs, $P_4$-sparse and $P_4$-lite graphs. All these coloring problems are\nknown to be NP-hard for general graphs. These algorithms are fixed parameter\ntractable on the parameter $q(G)$, which is the minimum $q$ such that $G$ is a\n$(q,q-4)$-graph. We also prove that every connected $(q,q-4)$-graph with at\nleast $q$ vertices is 2-clique-colorable and that every acyclic coloring of a\ncograph is also nonrepetitive.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:45:34 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2011 14:20:44 GMT"}], "update_date": "2011-09-14", "authors_parsed": [["Campos", "Victor", ""], ["Linhares-Sales", "Cl\u00e1udia", ""], ["Maia", "Ana Karolinna", ""], ["Martins", "Nicolas", ""], ["Sampaio", "Rudini Menezes", ""]]}, {"id": "1107.0088", "submitter": "Marcel de Carli Silva", "authors": "Marcel K. de Carli Silva, Nicholas J. A. Harvey, Cristiane M. Sato", "title": "Sparse Sums of Positive Semidefinite Matrices", "comments": null, "journal-ref": null, "doi": "10.1145/2746241", "report-no": null, "categories": "cs.DM cs.DS cs.NA math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been much interest in \"sparsifying\" sums of rank one\nmatrices: modifying the coefficients such that only a few are nonzero, while\napproximately preserving the matrix that results from the sum. Results of this\nsort have found applications in many different areas, including sparsifying\ngraphs. In this paper we consider the more general problem of sparsifying sums\nof positive semidefinite matrices that have arbitrary rank.\n  We give several algorithms for solving this problem. The first algorithm is\nbased on the method of Batson, Spielman and Srivastava (2009). The second\nalgorithm is based on the matrix multiplicative weights update method of Arora\nand Kale (2007). We also highlight an interesting connection between these two\nalgorithms.\n  Our algorithms have numerous applications. We show how they can be used to\nconstruct graph sparsifiers with auxiliary constraints, sparsifiers of\nhypergraphs, and sparse solutions to semidefinite programs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 00:36:03 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2011 03:50:03 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Silva", "Marcel K. de Carli", ""], ["Harvey", "Nicholas J. A.", ""], ["Sato", "Cristiane M.", ""]]}, {"id": "1107.0098", "submitter": "Alexander Davydov", "authors": "Alexander Y. Davydov", "title": "A Probabilistic Attack on NP-complete Problems", "comments": "16 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the probability theory-based approach, this paper reveals the\nequivalence of an arbitrary NP-complete problem to a problem of checking\nwhether a level set of a specifically constructed harmonic cost function (with\nall diagonal entries of its Hessian matrix equal to zero) intersects with a\nunit hypercube in many-dimensional Euclidean space. This connection suggests\nthe possibility that methods of continuous mathematics can provide crucial\ninsights into the most intriguing open questions in modern complexity theory.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 03:43:19 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2011 01:57:22 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2012 21:14:17 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Davydov", "Alexander Y.", ""]]}, {"id": "1107.0104", "submitter": "Wolfgang Mulzer", "authors": "Wolfgang Mulzer and Daniel Werner", "title": "Approximating Tverberg Points in Linear Time for Any Fixed Dimension", "comments": "14 pages, 2 figures. A preliminary version appeared in SoCG 2012.\n  This version removes an incorrect example at the end of Section 3.1", "journal-ref": "Discrete and Computational Geometry, 50(2), 2013, pp 520-535", "doi": "10.1007/s00454-013-9528-7", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let P be a d-dimensional n-point set. A Tverberg-partition of P is a\npartition of P into r sets P_1, ..., P_r such that the convex hulls conv(P_1),\n..., conv(P_r) have non-empty intersection. A point in the intersection of the\nconv(P_i)'s is called a Tverberg point of depth r for P. A classic result by\nTverberg implies that there always exists a Tverberg partition of size n/(d+1),\nbut it is not known how to find such a partition in polynomial time. Therefore,\napproximate solutions are of interest.\n  We describe a deterministic algorithm that finds a Tverberg partition of size\nn/4(d+1)^3 in time d^{O(log d)} n. This means that for every fixed dimension we\ncan compute an approximate Tverberg point (and hence also an approximate\ncenterpoint) in linear time. Our algorithm is obtained by combining a novel\nlifting approach with a recent result by Miller and Sheehy (2010).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 05:03:16 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 09:06:42 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 20:34:24 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Mulzer", "Wolfgang", ""], ["Werner", "Daniel", ""]]}, {"id": "1107.0234", "submitter": "Miguel Mosteiro", "authors": "Antonio Fern\\'andez Anta and Miguel A. Mosteiro and Jorge Ram\\'on\n  Mu\\~noz", "title": "Unbounded Contention Resolution in Multiple-Access Channels", "comments": "21 pages, 1 figure. To appear in DISC 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A frequent problem in settings where a unique resource must be shared among\nusers is how to resolve the contention that arises when all of them must use\nit, but the resource allows only for one user each time. The application of\nefficient solutions for this problem spans a myriad of settings such as radio\ncommunication networks or databases. For the case where the number of users is\nunknown, recent work has yielded fruitful results for local area networks and\nradio networks, although either a (possibly loose) upper bound on the number of\nusers needs to be known, or the solution is suboptimal, or it is only implicit\nor embedded in other problems, with bounds proved only asymptotically. In this\npaper, under the assumption that collision detection or information on the\nnumber of contenders is not available, we present a novel protocol for\ncontention resolution in radio networks, and we recreate a protocol previously\nused for other problems, tailoring the constants for our needs. In contrast\nwith previous work, both protocols are proved to be optimal up to a small\nconstant factor and with high probability for big enough number of contenders.\nAdditionally, the protocols are evaluated and contrasted with the previous work\nby extensive simulations. The evaluation shows that the complexity bounds\nobtained by the analysis are rather tight, and that both protocols proposed\nhave small and predictable complexity for many system sizes (unlike previous\nproposals).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 14:16:33 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Anta", "Antonio Fern\u00e1ndez", ""], ["Mosteiro", "Miguel A.", ""], ["Mu\u00f1oz", "Jorge Ram\u00f3n", ""]]}, {"id": "1107.0385", "submitter": "Steven Pollack", "authors": "Steven Pollack, Daniel Badali, Jonathan Pollack", "title": "An algorithm for autonomously plotting solution sets in the presence of\n  turning points", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.DS cs.MS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Plotting solution sets for particular equations may be complicated by the\nexistence of turning points. Here we describe an algorithm which not only\novercomes such problematic points, but does so in the most general of settings.\nApplications of the algorithm are highlighted through two examples: the first\nprovides verification, while the second demonstrates a non-trivial application.\nThe latter is followed by a thorough run-time analysis. While both examples\ndeal with bivariate equations, it is discussed how the algorithm may be\ngeneralized for space curves in $\\R^{3}$.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2011 15:07:48 GMT"}], "update_date": "2011-07-05", "authors_parsed": [["Pollack", "Steven", ""], ["Badali", "Daniel", ""], ["Pollack", "Jonathan", ""]]}, {"id": "1107.0634", "submitter": "Christian Reitwiessner", "authors": "Christian Gla{\\ss}er, Christian Reitwie{\\ss}ner, Maximilian Witek", "title": "Applications of Discrepancy Theory in Multiobjective Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a multi-color extension of the Beck-Fiala theorem to show that the\nmultiobjective maximum traveling salesman problem is randomized\n1/2-approximable on directed graphs and randomized 2/3-approximable on\nundirected graphs. Using the same technique we show that the multiobjective\nmaximum satisfiablilty problem is 1/2-approximable.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2011 14:22:07 GMT"}], "update_date": "2011-07-05", "authors_parsed": [["Gla\u00dfer", "Christian", ""], ["Reitwie\u00dfner", "Christian", ""], ["Witek", "Maximilian", ""]]}, {"id": "1107.0789", "submitter": "Lester Mackey", "authors": "Lester Mackey, Ameet Talwalkar, Michael I. Jordan", "title": "Distributed Matrix Completion and Robust Factorization", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If learning methods are to scale to the massive sizes of modern datasets, it\nis essential for the field of machine learning to embrace parallel and\ndistributed computing. Inspired by the recent development of matrix\nfactorization methods with rich theory but poor computational complexity and by\nthe relative ease of mapping matrices onto distributed architectures, we\nintroduce a scalable divide-and-conquer framework for noisy matrix\nfactorization. We present a thorough theoretical analysis of this framework in\nwhich we characterize the statistical errors introduced by the \"divide\" step\nand control their magnitude in the \"conquer\" step, so that the overall\nalgorithm enjoys high-probability estimation guarantees comparable to those of\nits base algorithm. We also present experiments in collaborative filtering and\nvideo background modeling that demonstrate the near-linear to superlinear\nspeed-ups attainable with this approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2011 06:03:44 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2011 00:59:30 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2011 01:38:14 GMT"}, {"version": "v4", "created": "Tue, 1 Nov 2011 05:37:48 GMT"}, {"version": "v5", "created": "Fri, 18 May 2012 09:28:27 GMT"}, {"version": "v6", "created": "Tue, 14 Aug 2012 17:33:30 GMT"}, {"version": "v7", "created": "Mon, 28 Oct 2013 06:02:12 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Mackey", "Lester", ""], ["Talwalkar", "Ameet", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1107.0798", "submitter": "Ondrej Moris", "authors": "Petr Hlineny and Ondrej Moris", "title": "Generalized Maneuvers in Route Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an important practical aspect of the route planning problem in\nreal-world road networks -- maneuvers. Informally, maneuvers represent various\nirregularities of the road network graph such as turn-prohibitions, traffic\nlight delays, round-abouts, forbidden passages and so on. We propose a\ngeneralized model which can handle arbitrarily complex (and even negative)\nmaneuvers, and outline how to enhance Dijkstra's algorithm in order to solve\nroute planning queries in this model without prior adjustments of the\nunderlying road network graph.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2011 07:02:14 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2011 11:24:27 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2011 23:01:24 GMT"}], "update_date": "2011-11-07", "authors_parsed": [["Hlineny", "Petr", ""], ["Moris", "Ondrej", ""]]}, {"id": "1107.0901", "submitter": "Joachim Spoerhase", "authors": "Aparna Das, Emden R. Gansner, Michael Kaufmann, Stephen Kobourov,\n  Joachim Spoerhase, Alexander Wolff", "title": "Approximating Minimum Manhattan Networks in Higher Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimum Manhattan network problem, which is defined as follows.\nGiven a set of points called \\emph{terminals} in $\\R^d$, find a minimum-length\nnetwork such that each pair of terminals is connected by a set of axis-parallel\nline segments whose total length is equal to the pair's Manhattan (that is,\n$L_1$-) distance. The problem is NP-hard in 2D and there is no PTAS for 3D\n(unless ${\\cal P}={\\cal NP}$). Approximation algorithms are known for 2D, but\nnot for 3D.\n  We present, for any fixed dimension $d$ and any $\\eps>0$, an\n$O(n^\\eps)$-approximation algorithm. For 3D, we also give a\n$4(k-1)$-approximation algorithm for the case that the terminals are contained\nin the union of $k \\ge 2$ parallel planes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2011 15:27:34 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2012 13:04:25 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Das", "Aparna", ""], ["Gansner", "Emden R.", ""], ["Kaufmann", "Michael", ""], ["Kobourov", "Stephen", ""], ["Spoerhase", "Joachim", ""], ["Wolff", "Alexander", ""]]}, {"id": "1107.1012", "submitter": "Haitao Wang", "authors": "Danny Z. Chen and Xuehou Tan and Haitao Wang and Gangshan Wu", "title": "Optimal Point Movement for Covering Circular Regions", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ points in a circular region $C$ in the plane, we study the problems\nof moving the $n$ points to its boundary to form a regular $n$-gon such that\nthe maximum (min-max) or the sum (min-sum) of the Euclidean distances traveled\nby the points is minimized. The problems have applications, e.g., in mobile\nsensor barrier coverage of wireless sensor networks. The min-max problem\nfurther has two versions: the decision version and optimization version. For\nthe min-max problem, we present an $O(n\\log^2 n)$ time algorithm for the\ndecision version and an $O(n\\log^3 n)$ time algorithm for the optimization\nversion. The previously best algorithms for the two problem versions take\n$O(n^{3.5})$ time and $O(n^{3.5}\\log n)$ time, respectively. For the min-sum\nproblem, we show that a special case with all points initially lying on the\nboundary of the circular region can be solved in $O(n^2)$ time, improving a\nprevious $O(n^4)$ time solution. For the general min-sum problem, we present a\n3-approximation $O(n^2)$ time algorithm, improving the previous\n$(1+\\pi)$-approximation $O(n^2)$ time algorithm. A by-product of our techniques\nis an algorithm for dynamically maintaining the maximum matching of a circular\nconvex bipartite graph; our algorithm can handle each vertex insertion or\ndeletion on the graph in $O(\\log^2 n)$ time. This result is interesting in its\nown right.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 02:08:59 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Chen", "Danny Z.", ""], ["Tan", "Xuehou", ""], ["Wang", "Haitao", ""], ["Wu", "Gangshan", ""]]}, {"id": "1107.1052", "submitter": "Ren\\'e Sitters", "authors": "Sylvia Boyd, Ren\\'e Sitters, Suzanne van der Ster, and Leen Stougie", "title": "The traveling salesman problem on cubic and subcubic graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Travelling Salesman Problem (TSP) on the metric completion of\ncubic and subcubic graphs, which is known to be NP-hard. The problem is of\ninterest because of its relation to the famous 4/3 conjecture for metric TSP,\nwhich says that the integrality gap, i.e., the worst case ratio between the\noptimal values of the TSP and its linear programming relaxation (the subtour\nelimination relaxation), is 4/3. We present the first algorithm for cubic\ngraphs with approximation ratio 4/3. The proof uses polyhedral techniques in a\nsurprising way, which is of independent interest. In fact we prove\nconstructively that for any cubic graph on $n$ vertices a tour of length 4n/3-2\nexists, which also implies the 4/3 conjecture, as an upper bound, for this\nclass of graph-TSP.\n  Recently, M\\\"omke and Svensson presented a randomized algorithm that gives a\n1.461-approximation for graph-TSP on general graphs and as a side result a\n4/3-approximation algorithm for this problem on subcubic graphs, also settling\nthe 4/3 conjecture for this class of graph-TSP. We will present a way to\nderandomize their algorithm which leads to a smaller running time than the\nobvious derandomization. All of the latter also works for multi-graphs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 08:14:51 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Boyd", "Sylvia", ""], ["Sitters", "Ren\u00e9", ""], ["van der Ster", "Suzanne", ""], ["Stougie", "Leen", ""]]}, {"id": "1107.1076", "submitter": "Antoine Thomas", "authors": "Antoine Thomas, A\\\"ida Ouangraoua, Jean-St\\'ephane Varr\\'e", "title": "Genome Halving by Block Interchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding the minimal number of block interchanges\n(exchange of two intervals) required to transform a duplicated linear genome\ninto a tandem duplicated linear genome. We provide a formula for the distance\nas well as a polynomial time algorithm for the sorting problem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 09:47:25 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Thomas", "Antoine", ""], ["Ouangraoua", "A\u00efda", ""], ["Varr\u00e9", "Jean-St\u00e9phane", ""]]}, {"id": "1107.1177", "submitter": "Stefan Szeider", "authors": "Stefan Szeider", "title": "Not So Easy Problems for Tree Decomposable Graphs", "comments": "Author's self-archived copy", "journal-ref": "Ramanujan Mathematical Society, Lecture Notes Series no. 13, 2010,\n  pp. 179-190", "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider combinatorial problems that can be solved in polynomial time for\ngraphs of bounded treewidth but where the order of the polynomial that bounds\nthe running time is expected to depend on the treewidth bound. First we review\nsome recent results for problems regarding list and equitable colorings,\ngeneral factors, and generalized satisfiability. Second we establish a new\nhardness result for the problem of minimizing the maximum weighted outdegree\nfor orientations of edge-weighted graphs of bounded treewidth.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 16:28:54 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Szeider", "Stefan", ""]]}, {"id": "1107.1199", "submitter": "EPTCS", "authors": "Michal Rutkowski (Department of Computer Science, The University of\n  Warwick)", "title": "Two-Player Reachability-Price Games on Single-Clock Timed Automata", "comments": "In Proceedings QAPL 2011, arXiv:1107.0746", "journal-ref": "EPTCS 57, 2011, pp. 31-46", "doi": "10.4204/EPTCS.57.3", "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two player reachability-price games on single-clock timed automata.\nThe problem is as follows: given a state of the automaton, determine whether\nthe first player can guarantee reaching one of the designated goal locations.\nIf a goal location can be reached then we also want to compute the optimum\nprice of doing so. Our contribution is twofold. First, we develop a theory of\ncost functions, which provide a comprehensive methodology for the analysis of\nthis problem. This theory allows us to establish our second contribution, an\nEXPTIME algorithm for computing the optimum reachability price, which improves\nthe existing 3EXPTIME upper bound.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 17:54:38 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Rutkowski", "Michal", "", "Department of Computer Science, The University of\n  Warwick"]]}, {"id": "1107.1265", "submitter": "Thomas Watson", "authors": "Thomas Watson", "title": "Lift-and-Project Integrality Gaps for the Traveling Salesperson Problem", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the lift-and-project procedures of Lov{\\'a}sz-Schrijver and\nSherali-Adams applied to the standard linear programming relaxation of the\ntraveling salesperson problem with triangle inequality. For the asymmetric TSP\ntour problem, Charikar, Goemans, and Karloff (FOCS 2004) proved that the\nintegrality gap of the standard relaxation is at least 2. We prove that after\none round of the Lov{\\'a}sz-Schrijver or Sherali-Adams procedures, the\nintegrality gap of the asymmetric TSP tour problem is at least 3/2, with a\nsmall caveat on which version of the standard relaxation is used. For the\nsymmetric TSP tour problem, the integrality gap of the standard relaxation is\nknown to be at least 4/3, and Cheung (SIOPT 2005) proved that it remains at\nleast 4/3 after $o(n)$ rounds of the Lov{\\'a}sz-Schrijver procedure, where $n$\nis the number of nodes. For the symmetric TSP path problem, the integrality gap\nof the standard relaxation is known to be at least 3/2, and we prove that it\nremains at least 3/2 after $o(n)$ rounds of the Lov{\\'a}sz-Schrijver procedure,\nby a simple reduction to Cheung's result.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 21:48:07 GMT"}], "update_date": "2011-07-08", "authors_parsed": [["Watson", "Thomas", ""]]}, {"id": "1107.1358", "submitter": "Omri Weinstein", "authors": "Zohar Karnin, Edo Liberty, Shachar Lovett, Roy Schwartz, Omri\n  Weinstein", "title": "On the Furthest Hyperplane Problem and Maximal Margin Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Furthest Hyperplane Problem (FHP), which is an\nunsupervised counterpart of Support Vector Machines. Given a set of n points in\nRd, the objective is to produce the hyperplane (passing through the origin)\nwhich maximizes the separation margin, that is, the minimal distance between\nthe hyperplane and any input point. To the best of our knowledge, this is the\nfirst paper achieving provable results regarding FHP. We provide both lower and\nupper bounds to this NP-hard problem. First, we give a simple randomized\nalgorithm whose running time is n^O(1/{\\theta}^2) where {\\theta} is the optimal\nseparation margin. We show that its exponential dependency on 1/{\\theta}^2 is\ntight, up to sub-polynomial factors, assuming SAT cannot be solved in\nsub-exponential time. Next, we give an efficient approxima- tion algorithm. For\nany {\\alpha} \\in [0, 1], the algorithm produces a hyperplane whose distance\nfrom at least 1 - 5{\\alpha} fraction of the points is at least {\\alpha} times\nthe optimal separation margin. Finally, we show that FHP does not admit a PTAS\nby presenting a gap preserving reduction from a particular version of the PCP\ntheorem.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 11:58:52 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2012 21:40:04 GMT"}], "update_date": "2012-02-06", "authors_parsed": [["Karnin", "Zohar", ""], ["Liberty", "Edo", ""], ["Lovett", "Shachar", ""], ["Schwartz", "Roy", ""], ["Weinstein", "Omri", ""]]}, {"id": "1107.1585", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan, Marcin Pilipczuk, Micha{\\l} Pilipczuk, Jakub Onufry\n  Wojtaszczyk", "title": "On Multiway Cut parameterized above lower bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider two above lower bound parameterizations of the Node\nMultiway Cut problem - above the maximum separating cut and above a natural\nLP-relaxation - and prove them to be fixed-parameter tractable. Our results\nimply O*(4^k) algorithms for Vertex Cover above Maximum Matching and Almost\n2-SAT as well as an O*(2^k) algorithm for Node Multiway Cut with a standard\nparameterization by the solution size, improving previous bounds for these\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 09:29:43 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Wojtaszczyk", "Jakub Onufry", ""]]}, {"id": "1107.1628", "submitter": "Anke van Zuylen", "authors": "Frans Schalekamp, David P. Williamson, Anke van Zuylen", "title": "A Proof of the Boyd-Carr Conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the precise integrality gap for the subtour LP relaxation of the\ntraveling salesman problem is a significant open question, with little progress\nmade in thirty years in the general case of symmetric costs that obey triangle\ninequality. Boyd and Carr [3] observe that we do not even know the worst-case\nupper bound on the ratio of the optimal 2-matching to the subtour LP; they\nconjecture the ratio is at most 10/9. In this paper, we prove the Boyd-Carr\nconjecture. In the case that a fractional 2-matching has no cut edge, we can\nfurther prove that an optimal 2-matching is at most 10/9 times the cost of the\nfractional 2-matching.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 13:25:49 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Schalekamp", "Frans", ""], ["Williamson", "David P.", ""], ["van Zuylen", "Anke", ""]]}, {"id": "1107.1630", "submitter": "Anke van Zuylen", "authors": "Jiawei Qian, Frans Schalekamp, David P. Williamson, Anke van Zuylen", "title": "On the Integrality Gap of the Subtour LP for the 1,2-TSP", "comments": "Changes wrt previous version: upper bound on integrality gap improved\n  to 5/4 (using the same techniques as in the previous version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the integrality gap of the subtour LP relaxation for\nthe traveling salesman problem in the special case when all edge costs are\neither 1 or 2. For the general case of symmetric costs that obey triangle\ninequality, a famous conjecture is that the integrality gap is 4/3. Little\nprogress towards resolving this conjecture has been made in thirty years. We\nconjecture that when all edge costs $c_{ij}\\in \\{1,2\\}$, the integrality gap is\n$10/9$. We show that this conjecture is true when the optimal subtour LP\nsolution has a certain structure. Under a weaker assumption, which is an analog\nof a recent conjecture by Schalekamp, Williamson and van Zuylen, we show that\nthe integrality gap is at most $7/6$. When we do not make any assumptions on\nthe structure of the optimal subtour LP solution, we can show that integrality\ngap is at most $5/4$; this is the first bound on the integrality gap of the\nsubtour LP strictly less than $4/3$ known for an interesting special case of\nthe TSP. We show computationally that the integrality gap is at most $10/9$ for\nall instances with at most 12 cities.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 13:39:09 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2012 14:13:11 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2014 03:48:15 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Qian", "Jiawei", ""], ["Schalekamp", "Frans", ""], ["Williamson", "David P.", ""], ["van Zuylen", "Anke", ""]]}, {"id": "1107.1697", "submitter": "Aiyou Chen", "authors": "Aiyou Chen, Jin Cao, Larry Shepp and Tuan Nguyen", "title": "Distinct counting with a self-learning bitmap", "comments": "Journal of the American Statistical Association (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the number of distinct elements (cardinality) in a dataset is a\nfundamental problem in database management. In recent years, due to many of its\nmodern applications, there has been significant interest to address the\ndistinct counting problem in a data stream setting, where each incoming data\ncan be seen only once and cannot be stored for long periods of time. Many\nprobabilistic approaches based on either sampling or sketching have been\nproposed in the computer science literature, that only require limited\ncomputing and memory resources. However, the performances of these methods are\nnot scale-invariant, in the sense that their relative root mean square\nestimation errors (RRMSE) depend on the unknown cardinalities. This is not\ndesirable in many applications where cardinalities can be very dynamic or\ninhomogeneous and many cardinalities need to be estimated. In this paper, we\ndevelop a novel approach, called self-learning bitmap (S-bitmap) that is\nscale-invariant for cardinalities in a specified range. S-bitmap uses a binary\nvector whose entries are updated from 0 to 1 by an adaptive sampling process\nfor inferring the unknown cardinality, where the sampling rates are reduced\nsequentially as more and more entries change from 0 to 1. We prove rigorously\nthat the S-bitmap estimate is not only unbiased but scale-invariant. We\ndemonstrate that to achieve a small RRMSE value of $\\epsilon$ or less, our\napproach requires significantly less memory and consumes similar or less\noperations than state-of-the-art methods for many common practice cardinality\nscales. Both simulation and experimental studies are reported.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 18:50:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Chen", "Aiyou", ""], ["Cao", "Jin", ""], ["Shepp", "Larry", ""], ["Nguyen", "Tuan", ""]]}, {"id": "1107.1780", "submitter": "Fatemeh Keshavarz-Kohjerdi", "authors": "Fatemeh Keshavarz-Kohjerdi and Alireza Bagheri", "title": "Hamiltonian Paths in Two Classes of Grid Graphs", "comments": "11pages, 7figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give the necessary and sufficient conditions for the\nexistence of Hamiltonian paths in $L-$alphabet and $C-$alphabet grid graphs. We\nalso present a linear-time algorithm for finding Hamiltonian paths in these\ngraphs.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2011 12:44:53 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Keshavarz-Kohjerdi", "Fatemeh", ""], ["Bagheri", "Alireza", ""]]}, {"id": "1107.1958", "submitter": "Ishay Haviv", "authors": "Eden Chlamtac and Ishay Haviv", "title": "Linear Index Coding via Semidefinite Programming", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT math.IT", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In the index coding problem, introduced by Birk and Kol (INFOCOM, 1998), the\ngoal is to broadcast an n bit word to n receivers (one bit per receiver), where\nthe receivers have side information represented by a graph G. The objective is\nto minimize the length of a codeword sent to all receivers which allows each\nreceiver to learn its bit. For linear index coding, the minimum possible length\nis known to be equal to a graph parameter called minrank (Bar-Yossef et al.,\nFOCS, 2006).\n  We show a polynomial time algorithm that, given an n vertex graph G with\nminrank k, finds a linear index code for G of length $\\widetilde{O}(n^{f(k)})$,\nwhere f(k) depends only on k. For example, for k=3 we obtain f(3) ~ 0.2574. Our\nalgorithm employs a semidefinite program (SDP) introduced by Karger, Motwani\nand Sudan (J. ACM, 1998) for graph coloring and its refined analysis due to\nArora, Chlamtac and Charikar (STOC, 2006). Since the SDP we use is not a\nrelaxation of the minimization problem we consider, a crucial component of our\nanalysis is an upper bound on the objective value of the SDP in terms of the\nminrank.\n  At the heart of our analysis lies a combinatorial result which may be of\nindependent interest. Namely, we show an exact expression for the maximum\npossible value of the Lovasz theta-function of a graph with minrank k. This\nyields a tight gap between two classical upper bounds on the Shannon capacity\nof a graph.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 08:20:46 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Chlamtac", "Eden", ""], ["Haviv", "Ishay", ""]]}, {"id": "1107.2000", "submitter": "Marek Karpinski", "authors": "Marek Karpinski, Richard Schmied, Claus Viehmann", "title": "Tight Approximation Bounds for Vertex Cover on Dense k-Partite\n  Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish almost tight upper and lower approximation bounds for the Vertex\nCover problem on dense k-partite hypergraphs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 11:18:27 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Karpinski", "Marek", ""], ["Schmied", "Richard", ""], ["Viehmann", "Claus", ""]]}, {"id": "1107.2001", "submitter": "Marc Thurley", "authors": "Marc Thurley", "title": "An Approximation Algorithm for #k-SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple randomized algorithm that approximates the number of\nsatisfying assignments of Boolean formulas in conjunctive normal form. To the\nbest of our knowledge this is the first algorithm which approximates #k-SAT for\nany k >= 3 within a running time that is not only non-trivial, but also\nsignificantly better than that of the currently fastest exact algorithms for\nthe problem. More precisely, our algorithm is a randomized approximation scheme\nwhose running time depends polynomially on the error tolerance and is mildly\nexponential in the number n of variables of the input formula. For example,\neven stipulating sub-exponentially small error tolerance, the number of\nsolutions to 3-CNF input formulas can be approximated in time O(1.5366^n). For\n4-CNF input the bound increases to O(1.6155^n).\n  We further show how to obtain upper and lower bounds on the number of\nsolutions to a CNF formula in a controllable way. Relaxing the requirements on\nthe quality of the approximation, on k-CNF input we obtain significantly\nreduced running times in comparison to the above bounds.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 11:27:34 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Thurley", "Marc", ""]]}, {"id": "1107.2033", "submitter": "David Williamson", "authors": "Martin Skutella and David P. Williamson", "title": "A note on the generalized min-sum set cover problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the generalized min-sum set cover problem,\nintroduced by Azar, Gamzu, and Yin. Bansal, Gupta, and Krishnaswamy give a\n485-approximation algorithm for the problem. We are able to alter their\nalgorithm and analysis to obtain a 28-approximation algorithm, improving the\nperformance guarantee by an order of magnitude. We use concepts from\n$\\alpha$-point scheduling to obtain our improvements.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 14:03:32 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Skutella", "Martin", ""], ["Williamson", "David P.", ""]]}, {"id": "1107.2105", "submitter": "Dimitrios Letsios", "authors": "Eric Angel, Evripidis Bampis, Fadi Kacem, Dimitrios Letsios", "title": "Speed Scaling on Parallel Processors with Migration", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of scheduling a set of jobs with release dates,\ndeadlines and processing requirements (or works), on parallel speed-scaled\nprocessors so as to minimize the total energy consumption. We consider that\nboth preemption and migration of jobs are allowed. An exact polynomial-time\nalgorithm has been proposed for this problem, which is based on the Ellipsoid\nalgorithm. Here, we formulate the problem as a convex program and we propose a\nsimpler polynomial-time combinatorial algorithm which is based on a reduction\nto the maximum flow problem. Our algorithm runs in $O(nf(n)logP)$ time, where\n$n$ is the number of jobs, $P$ is the range of all possible values of\nprocessors' speeds divided by the desired accuracy and $f(n)$ is the complexity\nof computing a maximum flow in a layered graph with O(n) vertices.\nIndependently, Albers et al. \\cite{AAG11} proposed an $O(n^2f(n))$-time\nalgorithm exploiting the same relation with the maximum flow problem. We extend\nour algorithm to the multiprocessor speed scaling problem with migration where\nthe objective is the minimization of the makespan under a budget of energy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 19:56:53 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2011 08:03:31 GMT"}], "update_date": "2011-07-13", "authors_parsed": [["Angel", "Eric", ""], ["Bampis", "Evripidis", ""], ["Kacem", "Fadi", ""], ["Letsios", "Dimitrios", ""]]}, {"id": "1107.2188", "submitter": "Yajun Wang", "authors": "Tengyu Ma, Bo Tang, Yajun Wang", "title": "The Simulated Greedy Algorithm for Several Submodular Matroid Secretary\n  Problems", "comments": "preliminary version appeared in STACS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the matroid secretary problems with submodular valuation functions.\nIn these problems, the elements arrive in random order. When one element\narrives, we have to make an immediate and irrevocable decision on whether to\naccept it or not. The set of accepted elements must form an {\\em independent\nset} in a predefined matroid. Our objective is to maximize the value of the\naccepted elements. In this paper, we focus on the case that the valuation\nfunction is a non-negative and monotonically non-decreasing submodular\nfunction.\n  We introduce a general algorithm for such {\\em submodular matroid secretary\nproblems}. In particular, we obtain constant competitive algorithms for the\ncases of laminar matroids and transversal matroids. Our algorithms can be\nfurther applied to any independent set system defined by the intersection of a\n{\\em constant} number of laminar matroids, while still achieving constant\ncompetitive ratios. Notice that laminar matroids generalize uniform matroids\nand partition matroids.\n  On the other hand, when the underlying valuation function is linear, our\nalgorithm achieves a competitive ratio of 9.6 for laminar matroids, which\nsignificantly improves the previous result.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 04:35:02 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 20:35:31 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2013 05:49:05 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Ma", "Tengyu", ""], ["Tang", "Bo", ""], ["Wang", "Yajun", ""]]}, {"id": "1107.2221", "submitter": "Fedor Fomin", "authors": "Fedor V. Fomin and Daniel Lokshtanov and Saket Saurabh", "title": "Bidimensionality and Geometric Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use several of the key ideas from Bidimensionality to give a\nnew generic approach to design EPTASs and subexponential time parameterized\nalgorithms for problems on classes of graphs which are not minor closed, but\ninstead exhibit a geometric structure. In particular we present EPTASs and\nsubexponential time parameterized algorithms for Feedback Vertex Set, Vertex\nCover, Connected Vertex Cover, Diamond Hitting Set, on map graphs and unit disk\ngraphs, and for Cycle Packing and Minimum-Vertex Feedback Edge Set on unit disk\ngraphs. Our results are based on the recent decomposition theorems proved by\nFomin et al [SODA 2011], and our algorithms work directly on the input graph.\nThus it is not necessary to compute the geometric representations of the input\ngraph. To the best of our knowledge, these results are previously unknown, with\nthe exception of the EPTAS and a subexponential time parameterized algorithm on\nunit disk graphs for Vertex Cover, which were obtained by Marx [ESA 2005] and\nAlber and Fiala [J. Algorithms 2004], respectively.\n  We proceed to show that our approach can not be extended in its full\ngenerality to more general classes of geometric graphs, such as intersection\ngraphs of unit balls in R^d, d >= 3. Specifically we prove that Feedback Vertex\nSet on unit-ball graphs in R^3 neither admits PTASs unless P=NP, nor\nsubexponential time algorithms unless the Exponential Time Hypothesis fails.\nAdditionally, we show that the decomposition theorems which our approach is\nbased on fail for disk graphs and that therefore any extension of our results\nto disk graphs would require new algorithmic ideas. On the other hand, we prove\nthat our EPTASs and subexponential time algorithms for Vertex Cover and\nConnected Vertex Cover carry over both to disk graphs and to unit-ball graphs\nin R^d for every fixed d.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 09:16:54 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2011 18:03:50 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""]]}, {"id": "1107.2231", "submitter": "Nicolas Broutin", "authors": "Nicolas Broutin, Ralph Neininger and Henning Sulzbach", "title": "Partial match queries in random quadtrees", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering items matching a partially specified\npattern in multidimensional trees (quad trees and k-d trees). We assume the\ntraditional model where the data consist of independent and uniform points in\nthe unit square. For this model, in a structure on $n$ points, it is known that\nthe number of nodes $C_n(\\xi)$ to visit in order to report the items matching\nan independent and uniformly on $[0,1]$ random query $\\xi$ satisfies\n$\\Ec{C_n(\\xi)}\\sim \\kappa n^{\\beta}$, where $\\kappa$ and $\\beta$ are explicit\nconstants. We develop an approach based on the analysis of the cost $C_n(x)$ of\nany fixed query $x\\in [0,1]$, and give precise estimates for the variance and\nlimit distribution of the cost $C_n(x)$. Our results permit to describe a limit\nprocess for the costs $C_n(x)$ as $x$ varies in $[0,1]$; one of the\nconsequences is that $E{\\max_{x\\in [0,1]} C_n(x)} \\sim \\gamma n^\\beta$.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 10:08:58 GMT"}], "update_date": "2011-07-13", "authors_parsed": [["Broutin", "Nicolas", ""], ["Neininger", "Ralph", ""], ["Sulzbach", "Henning", ""]]}, {"id": "1107.2299", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Gordon Wilfong", "title": "iBGP and Constrained Connectivity", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the theoretical study of the problem of minimizing the size of an\niBGP overlay in an Autonomous System (AS) in the Internet subject to a natural\nnotion of correctness derived from the standard \"hot-potato\" routing rules. For\nboth natural versions of the problem (where we measure the size of an overlay\nby either the number of edges or the maximum degree) we prove that it is\nNP-hard to approximate to a factor better than $\\Omega(\\log n)$ and provide\napproximation algorithms with ratio $\\tilde{O}(\\sqrt{n})$. In addition, we give\na slightly worse $\\tilde{O}(n^{2/3})$-approximation based on primal-dual\ntechniques that has the virtue of being both fast and good in practice, which\nwe show via simulations on the actual topologies of five large Autonomous\nSystems.\n  The main technique we use is a reduction to a new connectivity-based network\ndesign problem that we call Constrained Connectivity. In this problem we are\ngiven a graph $G=(V,E)$, and for every pair of vertices $u,v \\in V$ we are\ngiven a set $S(u,v) \\subseteq V$ called the safe set of the pair. The goal is\nto find the smallest subgraph $H$ of $G$ in which every pair of vertices $u,v$\nis connected by a path contained in $S(u,v)$. We show that the iBGP problem can\nbe reduced to the special case of Constrained Connectivity where $G = K_n$ and\nsafe sets are defined geometrically based on the IGP distances in the AS. We\nalso believe that Constrained Connectivity is an interesting problem in its own\nright, so provide stronger hardness results ($2^{\\log^{1-\\epsilon} n}$-hardness\nof approximation) and integrality gaps ($n^{1/3 - \\epsilon}$) for the general\ncase. On the positive side, we show that Constrained Connectivity turns out to\nbe much simpler for some interesting special cases other than iBGP: when safe\nsets are symmetric and hierarchical, we give a polynomial time algorithm that\ncomputes an optimal solution.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 14:31:07 GMT"}], "update_date": "2011-07-13", "authors_parsed": [["Dinitz", "Michael", ""], ["Wilfong", "Gordon", ""]]}, {"id": "1107.2312", "submitter": "Guillaume Moroz", "authors": "Guillaume Moroz (INRIA Lorraine - LORIA), Boris Aronov (NYU-Poly)", "title": "Computing the Distance between Piecewise-Linear Bivariate Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the distance between two\npiecewise-linear bivariate functions $f$ and $g$ defined over a common domain\n$M$. We focus on the distance induced by the $L_2$-norm, that is\n$\\|f-g\\|_2=\\sqrt{\\iint_M (f-g)^2}$. If $f$ is defined by linear interpolation\nover a triangulation of $M$ with $n$ triangles, while $g$ is defined over\nanother such triangulation, the obvious na\\\"ive algorithm requires\n$\\Theta(n^2)$ arithmetic operations to compute this distance. We show that it\nis possible to compute it in $\\O(n\\log^4 n)$ arithmetic operations, by reducing\nthe problem to multi-point evaluation of a certain type of polynomials. We also\npresent an application to terrain matching.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 14:56:31 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2011 09:48:10 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Moroz", "Guillaume", "", "INRIA Lorraine - LORIA"], ["Aronov", "Boris", "", "NYU-Poly"]]}, {"id": "1107.2368", "submitter": "Piyush Srivastava", "authors": "Alistair Sinclair and Piyush Srivastava and Marc Thurley", "title": "Approximation algorithms for two-state anti-ferromagnetic spin systems\n  on bounded degree graphs", "comments": "1 figure. Final Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper (Weitz, 2006), Weitz gave a deterministic fully polynomial\napproximation scheme for count- ing exponentially weighted independent sets\n(equivalently, approximating the partition function of the hard-core model from\nstatistical physics) on graphs of degree at most d, up to the critical activity\nfor the uniqueness of the Gibbs measure on the infinite d-regular tree. More\nrecently Sly (Sly, 2010) showed that this is optimal in the sense that if there\nis an FPRAS for the hard-core partition function on graphs of maximum degree d\nfor activities larger than the critical activity on the infinite d-regular tree\nthen NP = RP. In this paper, we extend Weitz's approach to derive a\ndeterministic fully polynomial approx- imation scheme for the partition\nfunction of the anti-ferromagnetic Ising model with arbitrary field on graphs\nof maximum degree d, up to the corresponding critical point on the d-regular\ntree. The main ingredient of our result is a proof that for two-state\nanti-ferromagnetic spin systems on the d-regular tree, weak spatial mixing\nimplies strong spatial mixing. This in turn uses a message-decay argument which\nextends a similar approach proposed recently for the hard-core model by\nRestrepo et al (Restrepo et al, 2011) to the case of the anti-ferromagnetic\nIsing model with arbitrary field. By a standard correspondence, these results\ntranslate to arbitrary two-state anti-ferromagnetic spin systems with soft\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 18:42:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2011 18:10:05 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2011 05:54:34 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Sinclair", "Alistair", ""], ["Srivastava", "Piyush", ""], ["Thurley", "Marc", ""]]}, {"id": "1107.2379", "submitter": "Lev Reyzin", "authors": "Shalev Ben-David, Lev Reyzin", "title": "Data Stability in Clustering: A Closer Look", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the model introduced by Bilu and Linial (2010), who study\nproblems for which the optimal clustering does not change when distances are\nperturbed. They show that even when a problem is NP-hard, it is sometimes\npossible to obtain efficient algorithms for instances resilient to certain\nmultiplicative perturbations, e.g. on the order of $O(\\sqrt{n})$ for max-cut\nclustering. Awasthi et al. (2010) consider center-based objectives, and Balcan\nand Liang (2011) analyze the $k$-median and min-sum objectives, giving\nefficient algorithms for instances resilient to certain constant multiplicative\nperturbations.\n  Here, we are motivated by the question of to what extent these assumptions\ncan be relaxed while allowing for efficient algorithms. We show there is little\nroom to improve these results by giving NP-hardness lower bounds for both the\n$k$-median and min-sum objectives. On the other hand, we show that constant\nmultiplicative resilience parameters can be so strong as to make the clustering\nproblem trivial, leaving only a narrow range of resilience parameters for which\nclustering is interesting. We also consider a model of additive perturbations\nand give a correspondence between additive and multiplicative notions of\nstability. Our results provide a close examination of the consequences of\nassuming stability in data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 19:27:12 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2011 19:20:42 GMT"}, {"version": "v3", "created": "Mon, 22 Aug 2011 19:04:46 GMT"}, {"version": "v4", "created": "Thu, 3 Nov 2011 15:58:42 GMT"}, {"version": "v5", "created": "Fri, 29 Aug 2014 18:52:16 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Ben-David", "Shalev", ""], ["Reyzin", "Lev", ""]]}, {"id": "1107.2422", "submitter": "Jakub Radoszewski", "authors": "Tomasz Kociumaka, Marcin Kubica, Jakub Radoszewski, Wojciech Rytter\n  and Tomasz Walen", "title": "A Linear Time Algorithm for Seeds Computation", "comments": "full version of a paper submitted to SODA 2012 with simplified\n  algorithms and new combinatorial results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A seed in a word is a relaxed version of a period in which the occurrences of\nthe repeating subword may overlap. We show a linear-time algorithm computing a\nlinear-size representation of all the seeds of a word (the number of seeds\nmight be quadratic). In particular, one can easily derive the shortest seed and\nthe number of seeds from our representation. Thus, we solve an open problem\nstated in the survey by Smyth (2000) and improve upon a previous O(n log n)\nalgorithm by Iliopoulos, Moore, and Park (1996). Our approach is based on\ncombinatorial relations between seeds and subword complexity (used here for the\nfirst time in context of seeds). In the previous papers, the compact\nrepresentation of seeds consisted of two independent parts operating on the\nsuffix tree of the word and the suffix tree of the reverse of the word,\nrespectively. Our second contribution is a simpler representation of all seeds\nwhich avoids dealing with the reversed word.\n  A preliminary version of this work, with a much more complex algorithm\nconstructing the earlier representation of seeds, was presented at the 23rd\nAnnual ACM-SIAM Symposium of Discrete Algorithms (SODA 2012).\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 21:55:22 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 20:09:57 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Kubica", "Marcin", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Walen", "Tomasz", ""]]}, {"id": "1107.2443", "submitter": "Taisuke Izumi", "authors": "Jun Hosoda, Juraj Hromkovic, Taisuke Izumi, Horotaka Ono, Monika\n  Steinova, Koichi Wada", "title": "On the Approximability and Hardness of Minimum Topic Connected Overlay\n  and Its Special Instances", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of designing a scalable overlay network to support\ndecentralized topic-based pub/sub communication, the Minimum Topic-Connected\nOverlay problem (Min-TCO in short) has been investigated: Given a set of t\ntopics and a collection of n users together with the lists of topics they are\ninterested in, the aim is to connect these users to a network by a minimum\nnumber of edges such that every graph induced by users interested in a common\ntopic is connected. It is known that Min-TCO is NP-hard and approximable within\nO(log t) in polynomial time. In this paper, we further investigate the problem\nand some of its special instances. We give various hardness results for\ninstances where the number of topics in which an user is interested in is\nbounded by a constant, and also for the instances where the number of users\ninterested in a common topic is constant. For the latter case, we present a\nfirst constant approximation algorithm. We also present some polynomial-time\nalgorithms for very restricted instances of Min-TCO.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 00:51:53 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Hosoda", "Jun", ""], ["Hromkovic", "Juraj", ""], ["Izumi", "Taisuke", ""], ["Ono", "Horotaka", ""], ["Steinova", "Monika", ""], ["Wada", "Koichi", ""]]}, {"id": "1107.2482", "submitter": "Manjish Pal", "authors": "Anant Jindal, Gazal Kochar, Manjish Pal", "title": "Maximum Matchings via Glauber Dynamics", "comments": "It has been pointed to us independently by Yuval Peres, Jonah\n  Sherman, Piyush Srivastava and other anonymous reviewers that the coupling\n  used in this paper doesn't have the right marginals because of which the\n  mixing time bound doesn't hold, and also the main result presented in the\n  paper. We thank them for reading the paper with interest and promptly\n  pointing out this mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we study the classic problem of computing a maximum cardinality\nmatching in general graphs $G = (V, E)$. The best known algorithm for this\nproblem till date runs in $O(m \\sqrt{n})$ time due to Micali and Vazirani\n\\cite{MV80}. Even for general bipartite graphs this is the best known running\ntime (the algorithm of Karp and Hopcroft \\cite{HK73} also achieves this bound).\nFor regular bipartite graphs one can achieve an $O(m)$ time algorithm which,\nfollowing a series of papers, has been recently improved to $O(n \\log n)$ by\nGoel, Kapralov and Khanna (STOC 2010) \\cite{GKK10}. In this paper we present a\nrandomized algorithm based on the Markov Chain Monte Carlo paradigm which runs\nin $O(m \\log^2 n)$ time, thereby obtaining a significant improvement over\n\\cite{MV80}.\n  We use a Markov chain similar to the \\emph{hard-core model} for Glauber\nDynamics with \\emph{fugacity} parameter $\\lambda$, which is used to sample\nindependent sets in a graph from the Gibbs Distribution \\cite{V99}, to design a\nfaster algorithm for finding maximum matchings in general graphs. Our result\ncrucially relies on the fact that the mixing time of our Markov Chain is\nindependent of $\\lambda$, a significant deviation from the recent series of\nworks \\cite{GGSVY11,MWW09, RSVVY10, S10, W06} which achieve computational\ntransition (for estimating the partition function) on a threshold value of\n$\\lambda$. As a result we are able to design a randomized algorithm which runs\nin $O(m\\log^2 n)$ time that provides a major improvement over the running time\nof the algorithm due to Micali and Vazirani. Using the conductance bound, we\nalso prove that mixing takes $\\Omega(\\frac{m}{k})$ time where $k$ is the size\nof the maximum matching.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 08:03:38 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2011 12:29:48 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Jindal", "Anant", ""], ["Kochar", "Gazal", ""], ["Pal", "Manjish", ""]]}, {"id": "1107.2509", "submitter": "Manuel Moussallam", "authors": "Manuel Moussallam and Laurent Daudet and Ga\\\"el Richard", "title": "Matching Pursuits with Random Sequential Subdictionaries", "comments": "20 pages - accepted 2nd April 2012 at Elsevier Signal Processing", "journal-ref": null, "doi": "10.1016/j.sigpro.2012.03.019", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching pursuits are a class of greedy algorithms commonly used in signal\nprocessing, for solving the sparse approximation problem. They rely on an atom\nselection step that requires the calculation of numerous projections, which can\nbe computationally costly for large dictionaries and burdens their\ncompetitiveness in coding applications. We propose using a non adaptive random\nsequence of subdictionaries in the decomposition process, thus parsing a large\ndictionary in a probabilistic fashion with no additional projection cost nor\nparameter estimation. A theoretical modeling based on order statistics is\nprovided, along with experimental evidence showing that the novel algorithm can\nbe efficiently used on sparse approximation problems. An application to audio\nsignal compression with multiscale time-frequency dictionaries is presented,\nalong with a discussion of the complexity and practical implementations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 10:06:35 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2012 12:20:51 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2012 12:07:01 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Moussallam", "Manuel", ""], ["Daudet", "Laurent", ""], ["Richard", "Ga\u00ebl", ""]]}, {"id": "1107.2554", "submitter": "Julia Chuzhoy", "authors": "Julia Chuzhoy", "title": "Routing in Undirected Graphs with Constant Congestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph G=(V,E), a collection (s_1,t_1),...,(s_k,t_k) of k\nsource-sink pairs, and an integer c, the goal in the Edge Disjoint Paths with\nCongestion problem is to connect maximum possible number of the source-sink\npairs by paths, so that the maximum load on any edge (called edge congestion)\ndoes not exceed c.\n  We show an efficient randomized algorithm to route $\\Omega(OPT/\\poly\\log k)$\nsource-sink pairs with congestion at most 14, where OPT is the maximum number\nof pairs that can be simultaneously routed on edge-disjoint paths. The best\nprevious algorithm that routed $\\Omega(OPT/\\poly\\log n)$ pairs required\ncongestion $\\poly(\\log \\log n)$, and for the setting where the maximum allowed\ncongestion is bounded by a constant c, the best previous algorithms could only\nguarantee the routing of $OPT/n^{O(1/c)}$ pairs.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 14:04:11 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Chuzhoy", "Julia", ""]]}, {"id": "1107.2686", "submitter": "Shayan Oveis Gharan", "authors": "Shayan Oveis Gharan and Luca Trevisan", "title": "A Higher-Order Cheeger's Inequality", "comments": "This paper has been withdrawn by the author since a simpler and more\n  general result is recently posted at http://arxiv.org/abs/1111.1055", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic fact in algebraic graph theory is that the number of connected\ncomponents in an undirected graph is equal to the multiplicity of the\neigenvalue 1 in the normalized adjacency matrix of the graph. In particular,\nthe graph is disconnected if and only if there are at least two eigenvalues\nequal to 1.\n  Cheeger's inequality provides an \"approximate\" version of the latter fact,\nand it states that a graph has a sparse cut (it is \"almost disconnected\") if\nand only if there are at least two eigenvalues that are close to one.\n  It has been conjectured that an analogous characterization holds for higher\nmultiplicities, that is there are $k$ eigenvalues close to 1 if and only if the\nvertex set can be partitioned into $k$ subsets, each defining a sparse cut. In\nthis paper we resolve this conjecture. Our result provides a theoretical\njustification for clustering algorithms that use the top $k$ eigenvector to\nembed the vertices into $\\R^k$, and then apply geometric considerations to the\nembedding.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 22:13:52 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2011 08:10:54 GMT"}], "update_date": "2011-12-09", "authors_parsed": [["Gharan", "Shayan Oveis", ""], ["Trevisan", "Luca", ""]]}, {"id": "1107.2700", "submitter": "Ilias Diakonikolas", "authors": "Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio", "title": "Learning $k$-Modal Distributions via Testing", "comments": "28 pages, full version of SODA'12 paper, to appear in Theory of\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-modal probability distribution over the discrete domain $\\{1,...,n\\}$\nis one whose histogram has at most $k$ \"peaks\" and \"valleys.\" Such\ndistributions are natural generalizations of monotone ($k=0$) and unimodal\n($k=1$) probability distributions, which have been intensively studied in\nprobability theory and statistics.\n  In this paper we consider the problem of \\emph{learning} (i.e., performing\ndensity estimation of) an unknown $k$-modal distribution with respect to the\n$L_1$ distance. The learning algorithm is given access to independent samples\ndrawn from an unknown $k$-modal distribution $p$, and it must output a\nhypothesis distribution $\\widehat{p}$ such that with high probability the total\nvariation distance between $p$ and $\\widehat{p}$ is at most $\\epsilon.$ Our\nmain goal is to obtain \\emph{computationally efficient} algorithms for this\nproblem that use (close to) an information-theoretically optimal number of\nsamples.\n  We give an efficient algorithm for this problem that runs in time\n$\\mathrm{poly}(k,\\log(n),1/\\epsilon)$. For $k \\leq \\tilde{O}(\\log n)$, the\nnumber of samples used by our algorithm is very close (within an\n$\\tilde{O}(\\log(1/\\epsilon))$ factor) to being information-theoretically\noptimal. Prior to this work computationally efficient algorithms were known\nonly for the cases $k=0,1$ \\cite{Birge:87b,Birge:97}.\n  A novel feature of our approach is that our learning algorithm crucially uses\na new algorithm for \\emph{property testing of probability distributions} as a\nkey subroutine. The learning algorithm uses the property tester to efficiently\ndecompose the $k$-modal distribution into $k$ (near-)monotone distributions,\nwhich are easier to learn.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 23:26:53 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2011 06:05:19 GMT"}, {"version": "v3", "created": "Sun, 14 Sep 2014 21:20:37 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1107.2702", "submitter": "Ilias Diakonikolas", "authors": "Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio", "title": "Learning Poisson Binomial Distributions", "comments": "Revised full version. Improved sample complexity bound of O~(1/eps^2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a basic problem in unsupervised learning: learning an unknown\n\\emph{Poisson Binomial Distribution}. A Poisson Binomial Distribution (PBD)\nover $\\{0,1,\\dots,n\\}$ is the distribution of a sum of $n$ independent\nBernoulli random variables which may have arbitrary, potentially non-equal,\nexpectations. These distributions were first studied by S. Poisson in 1837\n\\cite{Poisson:37} and are a natural $n$-parameter generalization of the\nfamiliar Binomial Distribution. Surprisingly, prior to our work this basic\nlearning problem was poorly understood, and known results for it were far from\noptimal.\n  We essentially settle the complexity of the learning problem for this basic\nclass of distributions. As our first main result we give a highly efficient\nalgorithm which learns to $\\eps$-accuracy (with respect to the total variation\ndistance) using $\\tilde{O}(1/\\eps^3)$ samples \\emph{independent of $n$}. The\nrunning time of the algorithm is \\emph{quasilinear} in the size of its input\ndata, i.e., $\\tilde{O}(\\log(n)/\\eps^3)$ bit-operations. (Observe that each draw\nfrom the distribution is a $\\log(n)$-bit string.) Our second main result is a\n{\\em proper} learning algorithm that learns to $\\eps$-accuracy using\n$\\tilde{O}(1/\\eps^2)$ samples, and runs in time $(1/\\eps)^{\\poly (\\log\n(1/\\eps))} \\cdot \\log n$. This is nearly optimal, since any algorithm {for this\nproblem} must use $\\Omega(1/\\eps^2)$ samples. We also give positive and\nnegative results for some extensions of this learning problem to weighted sums\nof independent Bernoulli random variables.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 23:30:39 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2011 06:03:55 GMT"}, {"version": "v3", "created": "Sat, 7 Dec 2013 17:37:52 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 01:45:53 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1107.2722", "submitter": "Luke Mathieson", "authors": "Arnaud Casteigts, Bernard Mans and Luke Mathieson", "title": "On the Feasibility of Maintenance Algorithms in Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near ubiquitous mobile computing has led to intense interest in dynamic graph\ntheory. This provides a new and challenging setting for algorithmics and\ncomplexity theory. For any graph-based problem, the rapid evolution of a\n(possibly disconnected) graph over time naturally leads to the important\ncomplexity question: is it better to calculate a new solution from scratch or\nto adapt the known solution on the prior graph to quickly provide a solution of\nguaranteed quality for the changed graph?\n  In this paper, we demonstrate that the former is the best approach in some\ncases, but that there are cases where the latter is feasible. We prove that,\nunder certain conditions, hard problems cannot even be approximated in any\nreasonable complexity bound --- i.e., even with a large amount of time, having\na solution to a very similar graph does not help in computing a solution to the\ncurrent graph. To achieve this, we formalize the idea as a maintenance\nalgorithm. Using r-Regular Subgraph as the primary example we show that\nW[1]-hardness for the parameterized approximation problem implies the\nnon-existence of a maintenance algorithm for the given approximation ratio.\nConversely we show that Vertex Cover, which is fixed-parameter tractable, has a\n2-approximate maintenance algorithm. The implications of NP-hardness and\nNPO-hardness are also explored.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 03:23:38 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2011 02:03:08 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2012 00:36:29 GMT"}, {"version": "v4", "created": "Fri, 17 Feb 2012 06:33:36 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Casteigts", "Arnaud", ""], ["Mans", "Bernard", ""], ["Mathieson", "Luke", ""]]}, {"id": "1107.2729", "submitter": "Hideo Bannai", "authors": "Keisuke Goto, Shirou Maruyama, Shunsuke Inenaga, Hideo Bannai, Hiroshi\n  Sakamoto, Masayuki Takeda", "title": "Restructuring Compressed Texts without Explicit Decompression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of {\\em restructuring} compressed texts without\nexplicit decompression. We present algorithms which allow conversions from\ncompressed representations of a string $T$ produced by any grammar-based\ncompression algorithm, to representations produced by several specific\ncompression algorithms including LZ77, LZ78, run length encoding, and some\ngrammar based compression algorithms. These are the first algorithms that\nachieve running times polynomial in the size of the compressed input and output\nrepresentations of $T$. Since most of the representations we consider can\nachieve exponential compression, our algorithms are theoretically faster in the\nworst case, than any algorithm which first decompresses the string for the\nconversion.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 05:35:09 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Goto", "Keisuke", ""], ["Maruyama", "Shirou", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Sakamoto", "Hiroshi", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1107.2869", "submitter": "Shahar Dobzinski", "authors": "Ashwinkumar Badanidiyuru, Shahar Dobzinski, Sigal Oren", "title": "Optimization with Demand Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study \\emph{combinatorial procurement auctions}, where a buyer with a\nvaluation function $v$ and budget $B$ wishes to buy a set of items. Each item\n$i$ has a cost $c_i$ and the buyer is interested in a set $S$ that maximizes\n$v(S)$ subject to $\\Sigma_{i\\in S}c_i\\leq B$. Special cases of combinatorial\nprocurement auctions are classical problems from submodular optimization. In\nparticular, when the costs are all equal (\\emph{cardinality constraint}), a\nclassic result by Nemhauser et al shows that the greedy algorithm provides an\n$\\frac e {e-1}$ approximation.\n  Motivated by many papers that utilize demand queries to elicit the\npreferences of agents in economic settings, we develop algorithms that\nguarantee improved approximation ratios in the presence of demand oracles. We\nare able to break the $\\frac e {e-1}$ barrier: we present algorithms that use\nonly polynomially many demand queries and have approximation ratios of $\\frac 9\n8+\\epsilon$ for the general problem and $\\frac 9 8$ for maximization subject to\na cardinality constraint.\n  We also consider the more general class of subadditive valuations. We present\nalgorithms that obtain an approximation ratio of $2+\\epsilon$ for the general\nproblem and 2 for maximization subject to a cardinality constraint. We\nguarantee these approximation ratios even when the valuations are non-monotone.\nWe show that these ratios are essentially optimal, in the sense that for any\nconstant $\\epsilon>0$, obtaining an approximation ratio of $2-\\epsilon$\nrequires exponentially many demand queries.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 17:01:04 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Badanidiyuru", "Ashwinkumar", ""], ["Dobzinski", "Shahar", ""], ["Oren", "Sigal", ""]]}, {"id": "1107.3019", "submitter": "Hideo Bannai", "authors": "Keisuke Goto, Hideo Bannai, Shunsuke Inenaga, Masayuki Takeda", "title": "Computing q-gram Frequencies on Collage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collage systems are a general framework for representing outputs of various\ntext compression algorithms. We consider the all $q$-gram frequency problem on\ncompressed string represented as a collage system, and present an $O((q+h\\log\nn)n)$-time $O(qn)$-space algorithm for calculating the frequencies for all\n$q$-grams that occur in the string. Here, $n$ and $h$ are respectively the size\nand height of the collage system.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 09:22:46 GMT"}], "update_date": "2011-07-18", "authors_parsed": [["Goto", "Keisuke", ""], ["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1107.3022", "submitter": "Hideo Bannai", "authors": "Keisuke Goto, Hideo Bannai, Shunsuke Inenaga, Masayuki Takeda", "title": "Computing q-gram Non-overlapping Frequencies on SLP Compressed Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Length-$q$ substrings, or $q$-grams, can represent important characteristics\nof text data, and determining the frequencies of all $q$-grams contained in the\ndata is an important problem with many applications in the field of data mining\nand machine learning. In this paper, we consider the problem of calculating the\n{\\em non-overlapping frequencies} of all $q$-grams in a text given in\ncompressed form, namely, as a straight line program (SLP). We show that the\nproblem can be solved in $O(q^2n)$ time and $O(qn)$ space where $n$ is the size\nof the SLP. This generalizes and greatly improves previous work (Inenaga &\nBannai, 2009) which solved the problem only for $q=2$ in $O(n^4\\log n)$ time\nand $O(n^3)$ space.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 09:39:57 GMT"}], "update_date": "2011-07-18", "authors_parsed": [["Goto", "Keisuke", ""], ["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1107.3059", "submitter": "Amin Karbasi", "authors": "Amin Karbasi, Stratis Ioannidis, Laurent Massoulie", "title": "From Small-World Networks to Comparison-Based Search", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of content search through comparisons has recently received\nconsiderable attention. In short, a user searching for a target object\nnavigates through a database in the following manner: the user is asked to\nselect the object most similar to her target from a small list of objects. A\nnew object list is then presented to the user based on her earlier selection.\nThis process is repeated until the target is included in the list presented, at\nwhich point the search terminates. This problem is known to be strongly related\nto the small-world network design problem.\n  However, contrary to prior work, which focuses on cases where objects in the\ndatabase are equally popular, we consider here the case where the demand for\nobjects may be heterogeneous. We show that, under heterogeneous demand, the\nsmall-world network design problem is NP-hard. Given the above negative result,\nwe propose a novel mechanism for small-world design and provide an upper bound\non its performance under heterogeneous demand. The above mechanism has a\nnatural equivalent in the context of content search through comparisons, and we\nestablish both an upper bound and a lower bound for the performance of this\nmechanism. These bounds are intuitively appealing, as they depend on the\nentropy of the demand as well as its doubling constant, a quantity capturing\nthe topology of the set of target objects. They also illustrate interesting\nconnections between comparison-based search to classic results from information\ntheory. Finally, we propose an adaptive learning algorithm for content search\nthat meets the performance guarantees achieved by the above mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 12:47:02 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 19:44:00 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2014 07:03:28 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Karbasi", "Amin", ""], ["Ioannidis", "Stratis", ""], ["Massoulie", "Laurent", ""]]}, {"id": "1107.3068", "submitter": "Magnus Wahlstr\\\"om", "authors": "Stefan Kratsch and Magnus Wahlstr\\\"om", "title": "Compression via Matroids: A Randomized Polynomial Kernel for Odd Cycle\n  Transversal", "comments": "Minor changes to agree with SODA 2012 version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Odd Cycle Transversal problem (OCT) asks whether a given graph can be\nmade bipartite by deleting at most $k$ of its vertices. In a breakthrough\nresult Reed, Smith, and Vetta (Operations Research Letters, 2004) gave a\n$\\BigOh(4^kkmn)$ time algorithm for it, the first algorithm with polynomial\nruntime of uniform degree for every fixed $k$. It is known that this implies a\npolynomial-time compression algorithm that turns OCT instances into equivalent\ninstances of size at most $\\BigOh(4^k)$, a so-called kernelization. Since then\nthe existence of a polynomial kernel for OCT, i.e., a kernelization with size\nbounded polynomially in $k$, has turned into one of the main open questions in\nthe study of kernelization.\n  This work provides the first (randomized) polynomial kernelization for OCT.\nWe introduce a novel kernelization approach based on matroid theory, where we\nencode all relevant information about a problem instance into a matroid with a\nrepresentation of size polynomial in $k$. For OCT, the matroid is built to\nallow us to simulate the computation of the iterative compression step of the\nalgorithm of Reed, Smith, and Vetta, applied (for only one round) to an\napproximate odd cycle transversal which it is aiming to shrink to size $k$. The\nprocess is randomized with one-sided error exponentially small in $k$, where\nthe result can contain false positives but no false negatives, and the size\nguarantee is cubic in the size of the approximate solution. Combined with an\n$\\BigOh(\\sqrt{\\log n})$-approximation (Agarwal et al., STOC 2005), we get a\nreduction of the instance to size $\\BigOh(k^{4.5})$, implying a randomized\npolynomial kernelization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 13:54:00 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2011 11:18:06 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Kratsch", "Stefan", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1107.3350", "submitter": "Yang Li Daniel", "authors": "Yang D. Li, Zhenjie Zhang, Marianne Winslett, Yin Yang", "title": "Compressive Mechanism: Utilizing Sparse Representation in Differential\n  Privacy", "comments": "20 pages, 6 figures", "journal-ref": "WPES '11 Proceedings of the 10th annual ACM workshop on Privacy in\n  the electronic society ACM New York, NY, USA (2011), pages 177-182", "doi": "10.1145/2046556.2046581", "report-no": null, "categories": "cs.DS cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy provides the first theoretical foundation with provable\nprivacy guarantee against adversaries with arbitrary prior knowledge. The main\nidea to achieve differential privacy is to inject random noise into statistical\nquery results. Besides correctness, the most important goal in the design of a\ndifferentially private mechanism is to reduce the effect of random noise,\nensuring that the noisy results can still be useful.\n  This paper proposes the \\emph{compressive mechanism}, a novel solution on the\nbasis of state-of-the-art compression technique, called \\emph{compressive\nsensing}. Compressive sensing is a decent theoretical tool for compact synopsis\nconstruction, using random projections. In this paper, we show that the amount\nof noise is significantly reduced from $O(\\sqrt{n})$ to $O(\\log(n))$, when the\nnoise insertion procedure is carried on the synopsis samples instead of the\noriginal database. As an extension, we also apply the proposed compressive\nmechanism to solve the problem of continual release of statistical results.\nExtensive experiments using real datasets justify our accuracy claims.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2011 03:20:58 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Li", "Yang D.", ""], ["Zhang", "Zhenjie", ""], ["Winslett", "Marianne", ""], ["Yang", "Yin", ""]]}, {"id": "1107.3593", "submitter": "Michael Goodrich", "authors": "David Eppstein, Michael T. Goodrich, Pierre Baldi", "title": "Privacy-Enhanced Methods for Comparing Compressed DNA Sequences", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study methods for improving the efficiency and privacy of\ncompressed DNA sequence comparison computations, under various querying\nscenarios. For instance, one scenario involves a querier, Bob, who wants to\ntest if his DNA string, $Q$, is close to a DNA string, $Y$, owned by a data\nowner, Alice, but Bob does not want to reveal $Q$ to Alice and Alice is willing\nto reveal $Y$ to Bob \\emph{only if} it is close to $Q$. We describe a\nprivacy-enhanced method for comparing two compressed DNA sequences, which can\nbe used to achieve the goals of such a scenario. Our method involves a\nreduction to set differencing, and we describe a privacy-enhanced protocol for\nset differencing that achieves absolute privacy for Bob (in the information\ntheoretic sense), and a quantifiable degree of privacy protection for Alice.\nOne of the important features of our protocols, which makes them ideally suited\nto privacy-enhanced DNA sequence comparison problems, is that the communication\ncomplexity of our solutions is proportional to a threshold that bounds the\ncardinality of the set differences that are of interest, rather than the\ncardinality of the sets involved (which correlates to the length of the DNA\nsequences). Moreover, in our protocols, the querier, Bob, can easily compute\nthe set difference only if its cardinality is close to or below a specified\nthreshold.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2011 22:58:37 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Baldi", "Pierre", ""]]}, {"id": "1107.3622", "submitter": "Soubhik Chakraborty", "authors": "Kiran Kumar Sundararajan, Mita Pal, Soubhik Chakraborty and N.C.\n  Mahanti", "title": "K-sort: A new sorting algorithm that beats Heap sort for n <= 70 lakhs!", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sundararajan and Chakraborty (2007) introduced a new version of Quick sort\nremoving the interchanges. Khreisat (2007) found this algorithm to be competing\nwell with some other versions of Quick sort. However, it uses an auxiliary\narray thereby increasing the space complexity. Here, we provide a second\nversion of our new sort where we have removed the auxiliary array. This second\nimproved version of the algorithm, which we call K-sort, is found to sort\nelements faster than Heap sort for an appreciably large array size (n <=\n70,00,000) for uniform U[0, 1] inputs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 04:21:58 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Sundararajan", "Kiran Kumar", ""], ["Pal", "Mita", ""], ["Chakraborty", "Soubhik", ""], ["Mahanti", "N. C.", ""]]}, {"id": "1107.3658", "submitter": "Bart M. P. Jansen", "authors": "Bart M. P. Jansen and Stefan Kratsch", "title": "On Polynomial Kernels for Structural Parameterizations of Odd Cycle\n  Transversal", "comments": "Accepted to IPEC 2011, Saarbrucken", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Odd Cycle Transversal problem (OCT) asks whether a given graph can be\nmade bipartite (i.e., 2-colorable) by deleting at most l vertices. We study\nstructural parameterizations of OCT with respect to their polynomial\nkernelizability, i.e., whether instances can be efficiently reduced to a size\npolynomial in the chosen parameter. It is a major open problem in parameterized\ncomplexity whether Odd Cycle Transversal admits a polynomial kernel when\nparameterized by l. On the positive side, we show a polynomial kernel for OCT\nwhen parameterized by the vertex deletion distance to the class of bipartite\ngraphs of treewidth at most w (for any constant w); this generalizes the\nparameter feedback vertex set number (i.e., the distance to a forest).\nComplementing this, we exclude polynomial kernels for OCT parameterized by the\ndistance to outerplanar graphs, conditioned on the assumption that NP \\not\n\\subseteq coNP/poly. Thus the bipartiteness requirement for the treewidth w\ngraphs is necessary. Further lower bounds are given for parameterization by\ndistance from cluster and co-cluster graphs respectively, as well as for\nWeighted OCT parameterized by the vertex cover number (i.e., the distance from\nan independent set).\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 09:24:48 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1107.3704", "submitter": "Stefan Kratsch", "authors": "Stefan Kratsch", "title": "Co-nondeterminism in compositions: A kernelization lower bound for a\n  Ramsey-type problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, techniques for obtaining lower bounds for kernelization were\none of the most sought after tools in the field of parameterized complexity.\nNow, after a strong influx of techniques, we are in the fortunate situation of\nhaving tools available that are even stronger than what has been required in\ntheir applications so far. Based on a result of Fortnow and Santhanam (JCSS\n2011), Bodlaender et al. (JCSS 2009) showed that, unless NP \\subseteq\ncoNP/poly, the existence of a deterministic polynomial-time composition\nalgorithm, i.e., an algorithm which outputs an instance of bounded parameter\nvalue which is yes if and only if one of t input instances is yes, rules out\nthe existence of polynomial kernels for a problem. Dell and van Melkebeek (STOC\n2010) continued this line of research and, amongst others, were able to rule\nout kernels of size O(k^d-eps) for certain problems, assuming NP !\\subseteq\ncoNP/poly. Their work implies that even the existence of a co-nondeterministic\ncomposition rules out polynomial kernels.\n  In this work we present the first example of how co-nondeterminism can help\nto make a composition algorithm. We study a Ramsey-type problem: Given a graph\nG and an integer k, the question is whether G contains an independent set or a\nclique of size at least k. It was asked by Rod Downey whether this problem\nadmits a polynomial kernelization. We provide a co-nondeterministic composition\nbased on embedding t instances into a single host graph H. The crux is that the\nhost graph H needs to observe a bound of L \\in O(log t) on both its maximum\nindependent set and maximum clique size, while also having a cover of its\nvertex set by independent sets and cliques all of size L; the\nco-nondeterministic composition is build around the search for such graphs.\nThus we show that, unless NP \\subseteq coNP/poly, the problem does not admit a\nkernelization with polynomial size guarantee.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 12:57:55 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2011 13:28:41 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Kratsch", "Stefan", ""]]}, {"id": "1107.3724", "submitter": "Yuri Pirola", "authors": "Yuri Pirola, Gianluca Della Vedova, Stefano Biffani, Alessandra Stella\n  and Paola Bonizzoni", "title": "Haplotype Inference on Pedigrees with Recombinations, Errors, and\n  Missing Genotypes via SAT solvers", "comments": "14 pages, 1 figure, 4 tables, the associated software reHCstar is\n  available at http://www.algolab.eu/reHCstar", "journal-ref": "IEEE/ACM Trans. on Computational Biology and Bioinformatics 9.6\n  (2012) 1582-1594", "doi": "10.1109/TCBB.2012.100", "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum-Recombinant Haplotype Configuration problem (MRHC) has been\nhighly successful in providing a sound combinatorial formulation for the\nimportant problem of genotype phasing on pedigrees. Despite several algorithmic\nadvances and refinements that led to some efficient algorithms, its\napplicability to real datasets has been limited by the absence of some\nimportant characteristics of these data in its formulation, such as mutations,\ngenotyping errors, and missing data.\n  In this work, we propose the Haplotype Configuration with Recombinations and\nErrors problem (HCRE), which generalizes the original MRHC formulation by\nincorporating the two most common characteristics of real data: errors and\nmissing genotypes (including untyped individuals). Although HCRE is\ncomputationally hard, we propose an exact algorithm for the problem based on a\nreduction to the well-known Satisfiability problem. Our reduction exploits\nrecent progresses in the constraint programming literature and, combined with\nthe use of state-of-the-art SAT solvers, provides a practical solution for the\nHCRE problem. Biological soundness of the phasing model and effectiveness (on\nboth accuracy and performance) of the algorithm are experimentally demonstrated\nunder several simulated scenarios and on a real dairy cattle population.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 14:25:10 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Pirola", "Yuri", ""], ["Della Vedova", "Gianluca", ""], ["Biffani", "Stefano", ""], ["Stella", "Alessandra", ""], ["Bonizzoni", "Paola", ""]]}, {"id": "1107.3731", "submitter": "Aaron Roth", "authors": "Anupam Gupta, Aaron Roth, Jonathan Ullman", "title": "Iterative Constructions and Private Data Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of approximately releasing the cut\nfunction of a graph while preserving differential privacy, and give new\nalgorithms (and new analyses of existing algorithms) in both the interactive\nand non-interactive settings.\n  Our algorithms in the interactive setting are achieved by revisiting the\nproblem of releasing differentially private, approximate answers to a large\nnumber of queries on a database. We show that several algorithms for this\nproblem fall into the same basic framework, and are based on the existence of\nobjects which we call iterative database construction algorithms. We give a new\ngeneric framework in which new (efficient) IDC algorithms give rise to new\n(efficient) interactive private query release mechanisms. Our modular analysis\nsimplifies and tightens the analysis of previous algorithms, leading to\nimproved bounds. We then give a new IDC algorithm (and therefore a new private,\ninteractive query release mechanism) based on the Frieze/Kannan low-rank matrix\ndecomposition. This new release mechanism gives an improvement on prior work in\na range of parameters where the size of the database is comparable to the size\nof the data universe (such as releasing all cut queries on dense graphs).\n  We also give a non-interactive algorithm for efficiently releasing private\nsynthetic data for graph cuts with error O(|V|^{1.5}). Our algorithm is based\non randomized response and a non-private implementation of the SDP-based,\nconstant-factor approximation algorithm for cut-norm due to Alon and Naor.\nFinally, we give a reduction based on the IDC framework showing that an\nefficient, private algorithm for computing sufficiently accurate rank-1 matrix\napproximations would lead to an improved efficient algorithm for releasing\nprivate synthetic data for graph cuts. We leave finding such an algorithm as\nour main open problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 14:54:46 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2011 20:56:52 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Gupta", "Anupam", ""], ["Roth", "Aaron", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1107.3793", "submitter": "Tamal Dey", "authors": "Oleksiy Busaryev, Sergio Cabello, Chao Chen, Tamal K. Dey, Yusu Wang", "title": "Annotating Simplices with a Homology Basis and Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $K$ be a simplicial complex and $g$ the rank of its $p$-th homology group\n$H_p(K)$ defined with $Z_2$ coefficients. We show that we can compute a basis\n$H$ of $H_p(K)$ and annotate each $p$-simplex of $K$ with a binary vector of\nlength $g$ with the following property: the annotations, summed over all\n$p$-simplices in any $p$-cycle $z$, provide the coordinate vector of the\nhomology class $[z]$ in the basis $H$. The basis and the annotations for all\nsimplices can be computed in $O(n^{\\omega})$ time, where $n$ is the size of $K$\nand $\\omega<2.376$ is a quantity so that two $n\\times n$ matrices can be\nmultiplied in $O(n^{\\omega})$ time. The pre-computation of annotations permits\nanswering queries about the independence or the triviality of $p$-cycles\nefficiently.\n  Using annotations of edges in 2-complexes, we derive better algorithms for\ncomputing optimal basis and optimal homologous cycles in 1-dimensional\nhomology. Specifically, for computing an optimal basis of $H_1(K)$, we improve\nthe time complexity known for the problem from $O(n^4)$ to\n$O(n^{\\omega}+n^2g^{\\omega-1})$. Here $n$ denotes the size of the 2-skeleton of\n$K$ and $g$ the rank of $H_1(K)$. Computing an optimal cycle homologous to a\ngiven 1-cycle is NP-hard even for surfaces and an algorithm taking\n$2^{O(g)}n\\log n$ time is known for surfaces. We extend this algorithm to work\nwith arbitrary 2-complexes in $O(n^{\\omega})+2^{O(g)}n^2\\log n$ time using\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 17:47:16 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2011 13:12:01 GMT"}, {"version": "v3", "created": "Fri, 18 May 2012 22:15:36 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Busaryev", "Oleksiy", ""], ["Cabello", "Sergio", ""], ["Chen", "Chao", ""], ["Dey", "Tamal K.", ""], ["Wang", "Yusu", ""]]}, {"id": "1107.3876", "submitter": "Luis Rademacher", "authors": "Navin Goyal, Luis Rademacher", "title": "Lower Bounds for the Average and Smoothed Number of Pareto Optima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothed analysis of multiobjective 0-1 linear optimization has drawn\nconsiderable attention recently. The number of Pareto-optimal solutions (i.e.,\nsolutions with the property that no other solution is at least as good in all\nthe coordinates and better in at least one) for multiobjective optimization\nproblems is the central object of study. In this paper, we prove several lower\nbounds for the expected number of Pareto optima. Our basic result is a lower\nbound of \\Omega_d(n^(d-1)) for optimization problems with d objectives and n\nvariables under fairly general conditions on the distributions of the linear\nobjectives. Our proof relates the problem of lower bounding the number of\nPareto optima to results in geometry connected to arrangements of hyperplanes.\nWe use our basic result to derive (1) To our knowledge, the first lower bound\nfor natural multiobjective optimization problems. We illustrate this for the\nmaximum spanning tree problem with randomly chosen edge weights. Our technique\nis sufficiently flexible to yield such lower bounds for other standard\nobjective functions studied in this setting (such as, multiobjective shortest\npath, TSP tour, matching). (2) Smoothed lower bound of min {\\Omega_d(n^(d-1.5)\n\\phi^{(d-log d) (1-\\Theta(1/\\phi))}), 2^{\\Theta(n)}}$ for the 0-1 knapsack\nproblem with d profits for phi-semirandom distributions for a version of the\nknapsack problem. This improves the recent lower bound of Brunsch and Roeglin.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 02:13:01 GMT"}], "update_date": "2011-07-21", "authors_parsed": [["Goyal", "Navin", ""], ["Rademacher", "Luis", ""]]}, {"id": "1107.3977", "submitter": "Nicolas Trotignon", "authors": "Pierre Charbit, Michel Habib, Nicolas Trotignon, Kristina\n  Vu\\v{s}kovi\\'c", "title": "Detecting 2-joins faster", "comments": null, "journal-ref": "P. Charbit, M. Habib, N. Trotignon and K. Vuskovic. Detecting\n  2-joins faster. Journal of Discrete Algorithms, 17:60-66, 2012", "doi": "10.1016/j.jda.2012.11.003", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2-joins are edge cutsets that naturally appear in the decomposition of\nseveral classes of graphs closed under taking induced subgraphs, such as\nbalanced bipartite graphs, even-hole-free graphs, perfect graphs and claw-free\ngraphs. Their detection is needed in several algorithms, and is the slowest\nstep for some of them. The classical method to detect a 2-join takes $O(n^3m)$\ntime where $n$ is the number of vertices of the input graph and $m$ the number\nof its edges. To detect \\emph{non-path} 2-joins (special kinds of 2-joins that\nare needed in all of the known algorithms that use 2-joins), the fastest known\nmethod takes time $O(n^4m)$. Here, we give an $O(n^2m)$-time algorithm for both\nof these problems. A consequence is a speed up of several known algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 13:40:23 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2013 07:27:33 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Charbit", "Pierre", ""], ["Habib", "Michel", ""], ["Trotignon", "Nicolas", ""], ["Vu\u0161kovi\u0107", "Kristina", ""]]}, {"id": "1107.4222", "submitter": "Hakob Aslanyan Hakob Aslanyan", "authors": "Hakob Aslanyan", "title": "Interference minimization in physical model of wireless networks", "comments": null, "journal-ref": "International Journal of Information Theories and Applications,\n  2010. ITHEA2010: Volume 17, Number 3, 221-232", "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interference minimization problem in wireless sensor and ad-hoc networks is\nconsidered. That is to assign a transmission power to each node of a network\nsuch that the network is connected and at the same time the maximum of\naccumulated signal straight on network nodes is minimum. Previous works on\ninterference minimization in wireless networks mainly consider the disk graph\nmodel of network. For disk graph model two approximation algorithms with\n$O(\\sqrt{n})$ and $O((opt\\ln{n})^{2})$ upper bounds of maximum interference are\nknown, where $n$ is the number of nodes and $opt$ is the minimal interference\nof a given network. In current work we consider more general interference\nmodel, the physical interference model, where sender nodes' signal straight on\na given node is a function of a sender/receiver node pair and sender nodes'\ntransmission power. For this model we give a polynomial time approximation\nalgorithm which finds a connected network with at most\n$O((opt\\ln{n})^{2}/\\beta)$ interference, where $\\beta \\geq 1$ is the minimum\nsignal straight necessary on receiver node for successfully receiving a\nmessage.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 10:07:27 GMT"}], "update_date": "2011-07-22", "authors_parsed": [["Aslanyan", "Hakob", ""]]}, {"id": "1107.4223", "submitter": "Hakob Aslanyan Hakob Aslanyan", "authors": "Hakob Aslanyan", "title": "Sorting Algorithms with Restrictions", "comments": null, "journal-ref": "International Conference on Computer Science and Information\n  Technologies. CSIT2005: 125-127, Yerevan, Armenia 2005", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is one of the most used and well investigated algorithmic problem\n[1]. Traditional postulation supposes the sorting data archived, and the\nelementary operation as comparisons of two numbers. In a view of appearance of\nnew processors and applied problems with data streams, sorting changed its\nface. This changes and generalizations are the subject of investigation in the\nresearch below.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 10:11:15 GMT"}], "update_date": "2011-07-22", "authors_parsed": [["Aslanyan", "Hakob", ""]]}, {"id": "1107.4378", "submitter": "Justin Thaler", "authors": "Michael T. Goodrich, Daniel S. Hirschberg, Michael Mitzenmacher,\n  Justin Thaler", "title": "Fully De-Amortized Cuckoo Hashing for Cache-Oblivious Dictionaries and\n  Multimaps", "comments": "27 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dictionary (or map) is a key-value store that requires all keys be unique,\nand a multimap is a key-value store that allows for multiple values to be\nassociated with the same key. We design hashing-based indexing schemes for\ndictionaries and multimaps that achieve worst-case optimal performance for\nlookups and updates, with a small or negligible probability the data structure\nwill require a rehash operation, depending on whether we are working in the the\nexternal-memory (I/O) model or one of the well-known versions of the Random\nAccess Machine (RAM) model. One of the main features of our constructions is\nthat they are \\emph{fully de-amortized}, meaning that their performance bounds\nhold without one having to tune their constructions with certain performance\nparameters, such as the constant factors in the exponents of failure\nprobabilities or, in the case of the external-memory model, the size of blocks\nor cache lines and the size of internal memory (i.e., our external-memory\nalgorithms are cache oblivious). Our solutions are based on a fully\nde-amortized implementation of cuckoo hashing, which may be of independent\ninterest. This hashing scheme uses two cuckoo hash tables, one \"nested\" inside\nthe other, with one serving as a primary structure and the other serving as an\nauxiliary supporting queue/stash structure that is super-sized with respect to\ntraditional auxiliary structures but nevertheless adds negligible storage to\nour scheme. This auxiliary structure allows the success probability for cuckoo\nhashing to be very high, which is useful in cryptographic or data-intensive\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 20:59:47 GMT"}], "update_date": "2011-07-25", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Hirschberg", "Daniel S.", ""], ["Mitzenmacher", "Michael", ""], ["Thaler", "Justin", ""]]}, {"id": "1107.4463", "submitter": "Tao  Ye", "authors": "Wenqi Huang, Tao Ye, Duanbing Chen", "title": "Bottom-Left Placement Theorem for Rectangle Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves a bottom-left placement theorem for the rectangle packing\nproblem, stating that if it is possible to orthogonally place n arbitrarily\ngiven rectangles into a rectangular container without overlapping, then we can\nachieve a feasible packing by successively placing a rectangle onto a\nbottom-left corner in the container. This theorem shows that even for the\nreal-parameter rectangle packing problem, we can solve it after finite times of\nbottom-left placement actions. Based on this theorem, we might develop\nefficient heuristic algorithms for solving the rectangle packing problem.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 09:27:23 GMT"}], "update_date": "2011-07-25", "authors_parsed": [["Huang", "Wenqi", ""], ["Ye", "Tao", ""], ["Chen", "Duanbing", ""]]}, {"id": "1107.4466", "submitter": "Andreas Bjorklund", "authors": "Andreas Bj\\\"orklund", "title": "Counting Perfect Matchings as Fast as Ryser", "comments": "To appear at SIAM-ACM SODA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there is a polynomial space algorithm that counts the number of\nperfect matchings in an $n$-vertex graph in $O^*(2^{n/2})\\subset O(1.415^n)$\ntime. ($O^*(f(n))$ suppresses functions polylogarithmic in $f(n)$).The\npreviously fastest algorithms for the problem was the exponential space\n$O^*(((1+\\sqrt{5})/2)^n) \\subset O(1.619^n)$ time algorithm by Koivisto, and\nfor polynomial space, the $O(1.942^n)$ time algorithm by Nederlof. Our new\nalgorithm's runtime matches up to polynomial factors that of Ryser's 1963\nalgorithm for bipartite graphs. We present our algorithm in the more general\nsetting of computing the hafnian over an arbitrary ring, analogously to Ryser's\nalgorithm for permanent computation.\n  We also give a simple argument why the general exact set cover counting\nproblem over a slightly superpolynomial sized family of subsets of an $n$\nelement ground set cannot be solved in $O^*(2^{(1-\\epsilon_1)n})$ time for any\n$\\epsilon_1>0$ unless there are $O^*(2^{(1-\\epsilon_2)n})$ time algorithms for\ncomputing an $n\\times n$ 0/1 matrix permanent, for some $\\epsilon_2>0$\ndepending only on $\\epsilon_1$.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 09:49:47 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2011 06:30:50 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""]]}, {"id": "1107.4563", "submitter": "Fernando Auil", "authors": "Fernando Auil", "title": "An Algorithm to Generate Square-Free Numbers and to Compute the Moebius\n  Function", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithm that iteratively produces a sequence of natural\nnumbers k_i and functions b_i. The number k_(i+1) arises as the first point of\ndiscontinuity of b_i above k_i. We derive a set of properties of both\nsequences, suggesting that (1) the algorithm produces square-free numbers k_i,\n(2) all the square-free numbers are generated as the output of the algorithm,\nand (3) the value of the Moebius function mu(k_i) can be evaluated as\nb_i(k_(i+1)) - b_i(k_i). The logical equivalence of these properties is\nrigorously proved. The question remains open if one of these properties can be\nderived from the definition of the algorithm. Numerical evidence, limited to\n5x10^6, seems to support this conjecture, and shows a total running time linear\nor quadratic, depending on the implementation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 16:19:28 GMT"}], "update_date": "2011-07-25", "authors_parsed": [["Auil", "Fernando", ""]]}, {"id": "1107.4617", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal Narayan Chaudhury", "title": "Constant-time filtering using shiftable kernels", "comments": "Accepted in IEEE Signal Processing Letters", "journal-ref": "IEEE Signal Processing Letters, vol. 18(11), pp. 651 - 654, 2011", "doi": "10.1109/LSP.2011.2167967", "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently demonstrated in [5] that the non-linear bilateral filter [14]\ncan be efficiently implemented using a constant-time or O(1) algorithm. At the\nheart of this algorithm was the idea of approximating the Gaussian range kernel\nof the bilateral filter using trigonometric functions. In this letter, we\nexplain how the idea in [5] can be extended to few other linear and non-linear\nfilters [14, 17, 2]. While some of these filters have received a lot of\nattention in recent years, they are known to be computationally intensive. To\nextend the idea in [5], we identify a central property of trigonometric\nfunctions, called shiftability, that allows us to exploit the redundancy\ninherent in the filtering operations. In particular, using shiftable kernels,\nwe show how certain complex filtering can be reduced to simply that of\ncomputing the moving sum of a stack of images. Each image in the stack is\nobtained through an elementary pointwise transform of the input image. This has\na two-fold advantage. First, we can use fast recursive algorithms for computing\nthe moving sum [15, 6], and, secondly, we can use parallel computation to\nfurther speed up the computation. We also show how shiftable kernels can also\nbe used to approximate the (non-shiftable) Gaussian kernel that is ubiquitously\nused in image filtering.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 20:18:16 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2011 13:52:16 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2011 15:55:45 GMT"}, {"version": "v4", "created": "Thu, 8 Sep 2011 16:11:25 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chaudhury", "Kunal Narayan", ""]]}, {"id": "1107.4624", "submitter": "Deniz Sarioz", "authors": "Matthew P. Johnson and Deniz Sarioz", "title": "Computing the obstacle number of a plane graph", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An obstacle representation of a plane graph G is V(G) together with a set of\nopaque polygonal obstacles such that G is the visibility graph on V(G)\ndetermined by the obstacles. We investigate the problem of computing an\nobstacle representation of a plane graph (ORPG) with a minimum number of\nobstacles. We call this minimum size the obstacle number of G.\n  First, we show that ORPG is NP-hard by reduction from planar vertex cover,\nresolving a question posed by [8]. Second, we give a reduction from ORPG to\nmaximum degree 3 planar vertex cover. Since this reduction preserves solution\nvalues, it follows that ORPG is fixed parameter tractable (FPT) and admits a\npolynomial-time approximation scheme (PTAS).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 20:43:21 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2011 18:43:00 GMT"}], "update_date": "2011-08-15", "authors_parsed": [["Johnson", "Matthew P.", ""], ["Sarioz", "Deniz", ""]]}, {"id": "1107.4824", "submitter": "Shiva Kintali", "authors": "Shiva Kintali, Nishad Kothari, Akash Kumar", "title": "Approximation Algorithms for Digraph Width Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several problems that are NP-hard on general graphs are efficiently solvable\non graphs with bounded treewidth. Efforts have been made to generalize\ntreewidth and the related notion of pathwidth to digraphs. Directed treewidth,\nDAG-width and Kelly-width are some such notions which generalize treewidth,\nwhereas directed pathwidth generalizes pathwidth. Each of these digraph width\nmeasures have an associated decomposition structure.\n  In this paper, we present approximation algorithms for all these digraph\nwidth parameters. In particular, we give an O(sqrt{logn})-approximation\nalgorithm for directed treewidth, and an O({\\log}^{3/2}{n})-approximation\nalgorithm for directed pathwidth, DAG-width and Kelly-width. Our algorithms\nconstruct the corresponding decompositions whose widths are within the above\nmentioned approximation factors.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 01:58:42 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2013 17:39:02 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Kintali", "Shiva", ""], ["Kothari", "Nishad", ""], ["Kumar", "Akash", ""]]}, {"id": "1107.4890", "submitter": "Jakub Pawlewicz", "authors": "Jakub Pawlewicz", "title": "Counting Square-Free Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main topic of this contribution is the problem of counting square-free\nnumbers not exceeding $n$. Before this work we were able to do it in time\n(Comparing to the Big-O notation, Soft-O ($\\softO$) ignores logarithmic\nfactors) $\\softO(\\sqrt{n})$. Here, the algorithm with time complexity\n$\\softO(n^{2/5})$ and with memory complexity $\\softO(n^{1/5})$ is presented.\nAdditionally, a parallel version is shown, which achieves full scalability.\n  As of now the highest computed value was for $n=10^{17}$. Using our\nimplementation we were able to calculate the value for $n=10^{36}$ on a\ncluster.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 10:55:58 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Pawlewicz", "Jakub", ""]]}, {"id": "1107.4893", "submitter": "Zeev Nutov", "authors": "Nachshon Cohen and Zeev Nutov", "title": "Approximating minimum-power edge-multicovers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph with edge costs, the {\\em power} of a node is themaximum cost\nof an edge incident to it, and the power of a graph is the sum of the powers of\nits nodes. Motivated by applications in wireless networks, we consider the\nfollowing fundamental problem in wireless network design. Given a graph\n$G=(V,E)$ with edge costs and degree bounds $\\{r(v):v \\in V\\}$, the {\\sf\nMinimum-Power Edge-Multi-Cover} ({\\sf MPEMC}) problem is to find a\nminimum-power subgraph $J$ of $G$ such that the degree of every node $v$ in $J$\nis at least $r(v)$. We give two approximation algorithms for {\\sf MPEMC}, with\nratios $O(\\log k)$ and $k+1/2$, where $k=\\max_{v \\in V} r(v)$ is the maximum\ndegree bound. This improves the previous ratios $O(\\log n)$ and $k+1$, and\nimplies ratios $O(\\log k)$ for the {\\sf Minimum-Power $k$-Outconnected\nSubgraph} and $O(\\log k \\log \\frac{n}{n-k})$ for the {\\sf Minimum-Power\n$k$-Connected Subgraph} problems; the latter is the currently best known ratio\nfor the min-cost version of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 11:07:16 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Cohen", "Nachshon", ""], ["Nutov", "Zeev", ""]]}, {"id": "1107.4970", "submitter": "Joachim Spoerhase", "authors": "Martin Fink, Jan-Henrik Haunert, Tamara Mchedlidze, Joachim Spoerhase,\n  Alexander Wolff", "title": "Drawing Graphs with Vertices at Specified Positions and Crossings at\n  Large Angles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-set embeddings and large-angle crossings are two areas of graph drawing\nthat independently have received a lot of attention in the past few years. In\nthis paper, we consider problems in the intersection of these two areas. Given\nthe point-set-embedding scenario, we are interested in how much we gain in\nterms of computational complexity, curve complexity, and generality if we allow\nlarge-angle crossings as compared to the planar case. We investigate two\ndrawing styles where only bends or both bends and edges must be drawn on an\nunderlying grid. We present various results for drawings with one, two, and\nthree bends per edge.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 15:10:12 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Fink", "Martin", ""], ["Haunert", "Jan-Henrik", ""], ["Mchedlidze", "Tamara", ""], ["Spoerhase", "Joachim", ""], ["Wolff", "Alexander", ""]]}, {"id": "1107.5329", "submitter": "Rico Zenklusen", "authors": "Rico Zenklusen", "title": "Matroidal Degree-Bounded Minimum Spanning Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimum spanning tree (MST) problem under the restriction\nthat for every vertex v, the edges of the tree that are adjacent to v satisfy a\ngiven family of constraints. A famous example thereof is the classical\ndegree-constrained MST problem, where for every vertex v, a simple upper bound\non the degree is imposed. Iterative rounding/relaxation algorithms became the\ntool of choice for degree-bounded network design problems. A cornerstone for\nthis development was the work of Singh and Lau, who showed for the\ndegree-bounded MST problem how to find a spanning tree violating each degree\nbound by at most one unit and with cost at most the cost of an optimal solution\nthat respects the degree bounds.\n  However, current iterative rounding approaches face several limits when\ndealing with more general degree constraints. In particular, when several\nconstraints are imposed on the edges adjacent to a vertex v, as for example\nwhen a partition of the edges adjacent to v is given and only a fixed number of\nelements can be chosen out of each set of the partition, current approaches\nmight violate each of the constraints by a constant, instead of violating all\nconstraints together by at most a constant number of edges. Furthermore, it is\nalso not clear how previous iterative rounding approaches can be used for\ndegree constraints where some edges are in a super-constant number of\nconstraints.\n  We extend iterative rounding/relaxation approaches both on a conceptual level\nas well as aspects involving their analysis to address these limitations. This\nleads to an efficient algorithm for the degree-constrained MST problem where\nfor every vertex v, the edges adjacent to v have to be independent in a given\nmatroid. The algorithm returns a spanning tree T of cost at most OPT, such that\nfor every vertex v, it suffices to remove at most 8 edges from T to satisfy the\nmatroidal degree constraint at v.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2011 20:54:16 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Zenklusen", "Rico", ""]]}, {"id": "1107.5349", "submitter": "Luca Pinello", "authors": "Luca Pinello", "title": "Multi Layer Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis presents a new methodology to analyze one-dimensional signals\ntrough a new approach called Multi Layer Analysis, for short MLA. It also\nprovides some new insights on the relationship between one-dimensional signals\nprocessed by MLA and tree kernels, test of randomness and signal processing\ntechniques. The MLA approach has a wide range of application to the fields of\npattern discovery and matching, computational biology and many other areas of\ncomputer science and signal processing. This thesis includes also some\napplications of this approach to real problems in biology and seismology.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2011 22:29:35 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Pinello", "Luca", ""]]}, {"id": "1107.5370", "submitter": "Cristina Fernandes", "authors": "Cristina G. Fernandes and Robin Thomas", "title": "Edge-coloring series-parallel multigraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simpler proof of Seymour's Theorem on edge-coloring series-parallel\nmultigraphs and derive a linear-time algorithm to check whether a given\nseries-parallel multigraph can be colored with a given number of colors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 02:54:20 GMT"}], "update_date": "2011-08-01", "authors_parsed": [["Fernandes", "Cristina G.", ""], ["Thomas", "Robin", ""]]}, {"id": "1107.5372", "submitter": "Weirong Jiang", "authors": "Weirong Jiang and Hoang Le and Viktor K. Prasanna", "title": "Bidirectional Pipelining for Scalable IP Lookup and Packet\n  Classification", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": "Technical Report CENG-2008-3, University of Southern California,\n  2008", "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both IP lookup and packet classification in IP routers can be implemented by\nsome form of tree traversal. SRAM-based Pipelining can improve the throughput\ndramatically. However, previous pipelining schemes result in unbalanced memory\nallocation over the pipeline stages. This has been identified as a major\nchallenge for scalable pipelined solutions. This paper proposes a flexible\nbidirectional linear pipeline architecture based on widely-used dual-port\nSRAMs. A search tree is partitioned, and then mapped onto pipeline stages by a\nbidirectional fine-grained mapping scheme. We introduce the notion of inversion\nfactor and several heuristics to invert subtrees for memory balancing. Due to\nits linear structure, the architecture maintains packet input order, and\nsupports non-blocking route updates. Our experiments show that, the\narchitecture can achieve a perfectly balanced memory distribution over the\npipeline stages, for both trie-based IP lookup and tree-based multi-dimensional\npacket classification. For IP lookup, it can store a full backbone routing\ntable with 154419 entries using 2MB of memory, and sustain a high throughput of\n1.87 billion packets per second (GPPS), i.e. 0.6 Tbps for the minimum size (40\nbytes) packets. The throughput can be improved further to be 2.4 Tbps, by\nemploying caching to exploit the Internet traffic locality.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 03:01:36 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Jiang", "Weirong", ""], ["Le", "Hoang", ""], ["Prasanna", "Viktor K.", ""]]}, {"id": "1107.5550", "submitter": "Michael Molloy", "authors": "Dimitris Achlioptas and Michael Molloy", "title": "The solution space geometry of random linear equations", "comments": "Corrects an error from previous versions. Lemma 35(b) replaces\n  Observation 5 in the journal publication", "journal-ref": "Random Structures and Algorithms 46, 197-231 (2015)", "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider random systems of linear equations over GF(2) in which every\nequation binds k variables. We obtain a precise description of the clustering\nof solutions in such systems. In particular, we prove that with probability\nthat tends to 1 as the number of variables, n, grows: for every pair of\nsolutions \\sigma, \\tau, either there exists a sequence of solutions\n\\sigma,...,\\tau, in which successive elements differ by O(log n) variables, or\nevery sequence of solutions \\sigma,...,\\tau, contains a step requiring the\nsimultaneous change of \\Omega(n) variables. Furthermore, we determine precisely\nwhich pairs of solutions are in each category. Our results are tight and highly\nquantitative in nature. Moreover, our proof highlights the role of unique\nextendability as the driving force behind the success of Low Density Parity\nCheck codes and our techniques also apply to the problem of so-called\npseudo-codewords in such codes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 17:54:52 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 13:22:36 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Achlioptas", "Dimitris", ""], ["Molloy", "Michael", ""]]}]