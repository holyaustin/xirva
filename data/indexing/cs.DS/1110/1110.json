[{"id": "1110.0010", "submitter": "John Jakeman", "authors": "John D. Jakeman, Stephen G. Roberts", "title": "Local and Dimension Adaptive Sparse Grid Interpolation and Quadrature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a locally and dimension-adaptive sparse grid method\nfor interpolation and integration of high-dimensional functions with\ndiscontinuities. The proposed algorithm combines the strengths of the\ngeneralised sparse grid algorithm and hierarchical surplus-guided local\nadaptivity. A high-degree basis is used to obtain a high-order method which,\ngiven sufficient smoothness, performs significantly better than the\npiecewise-linear basis. The underlying generalised sparse grid algorithm\ngreedily selects the dimensions and variable interactions that contribute most\nto the variability of a function. The hierarchical surplus of points within the\nsparse grid is used as an error criterion for local refinement with the aim of\nconcentrating computational effort within rapidly varying or discontinuous\nregions. This approach limits the number of points that are invested in\n`unimportant' dimensions and regions within the high-dimensional domain. We\nshow the utility of the proposed method for non-smooth functions with hundreds\nof variables.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 20:14:32 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Jakeman", "John D.", ""], ["Roberts", "Stephen G.", ""]]}, {"id": "1110.0180", "submitter": "Gleb Novichkov", "authors": "Gleb Novichkov", "title": "An efficient algorithm to find a set of nearest elements in a mesh", "comments": "raw version. to be improved later on", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A linear time algorithm to find a set of nearest elements in a mesh.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2011 12:32:32 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Novichkov", "Gleb", ""]]}, {"id": "1110.0259", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, MohammadTaghi Hajiaghayi and D\\'aniel Marx", "title": "Fixed-Parameter Tractability of Directed Multiway Cut Parameterized by\n  the Size of the Cutset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph $G$, a set of $k$ terminals and an integer $p$, the\n\\textsc{Directed Vertex Multiway Cut} problem asks if there is a set $S$ of at\nmost $p$ (nonterminal) vertices whose removal disconnects each terminal from\nall other terminals. \\textsc{Directed Edge Multiway Cut} is the analogous\nproblem where $S$ is a set of at most $p$ edges. These two problems indeed are\nknown to be equivalent. A natural generalization of the multiway cut is the\n\\emph{multicut} problem, in which we want to disconnect only a set of $k$ given\npairs instead of all pairs. Marx (Theor. Comp. Sci. 2006) showed that in\nundirected graphs multiway cut is fixed-parameter tractable (FPT) parameterized\nby $p$. Marx and Razgon (STOC 2011) showed that undirected multicut is FPT and\ndirected multicut is W[1]-hard parameterized by $p$. We complete the picture\nhere by our main result which is that both \\textsc{Directed Vertex Multiway\nCut} and \\textsc{Directed Edge Multiway Cut} can be solved in time\n$2^{2^{O(p)}}n^{O(1)}$, i.e., FPT parameterized by size $p$ of the cutset of\nthe solution. This answers an open question raised by Marx (Theor. Comp. Sci.\n2006) and Marx and Razgon (STOC 2011). It follows from our result that\n\\textsc{Directed Multicut} is FPT for the case of $k=2$ terminal pairs, which\nanswers another open problem raised in Marx and Razgon (STOC 2011).\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2011 03:32:01 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2013 10:55:10 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1110.0341", "submitter": "Morgan Chopin", "authors": "Cristina Bazgan, Morgan Chopin, Bernard Ries", "title": "The firefighter problem with more than one firefighter on trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the complexity of the firefighter problem and related\nproblems on trees when more than one firefighter is available at each time\nstep, and answer several open questions of Finbow and MacGillivray 2009. More\nprecisely, when $b \\geq 2$ firefighters are allowed at each time step, the\nproblem is NP-complete for trees of maximum degree $b+2$ and polynomial-time\nsolvable for trees of maximum degree $b+2$ when the fire breaks out at a vertex\nof degree at most $b+1$. Moreover we present a polynomial-time algorithm for a\nsubclass of trees, namely $k$-caterpillars.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2011 12:36:58 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Bazgan", "Cristina", ""], ["Chopin", "Morgan", ""], ["Ries", "Bernard", ""]]}, {"id": "1110.0550", "submitter": "Sunil Khatri", "authors": "Pey-Chang Kent Lin, Ayan Mandal, Sunil P Khatri", "title": "Boolean Satisfiability using Noise Based Logic", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel algorithm to solve the Boolean\nSatisfiability (SAT) problem, using noise-based logic (NBL). Contrary to what\nthe name may suggest, NBL is not a random/fuzzy logic system. In fact, it is a\ncompletely deterministic logic system. A key property of NBL is that it allows\nus to apply a superposition of many input vectors to a SAT instance at the same\ntime, circumventing a key restriction and assumption in the traditional\napproach to solving SAT. By exploiting the superposition property of NBL, our\nNBL-based SAT algorithm can determine whether an instance is SAT or not in a\nsingle operation. A satisfying solution can be found by iteratively performing\nSAT check operations up to n times, where n is the number of variables in the\nSAT instance. Although this paper does not focus on the realization of an\nNBL-based SAT engine, such an engine can be conceived using analog circuits\n(wide-band amplifiers, adders and multipliers), FPGAs or ASICs. Additionally,\nwe also discus scalability of our approach, which can apply to NBL in general.\nThe NBL-based SAT engine described in this paper has been simulated in software\nfor validation purposes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 00:32:40 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Lin", "Pey-Chang Kent", ""], ["Mandal", "Ayan", ""], ["Khatri", "Sunil P", ""]]}, {"id": "1110.0583", "submitter": "Ton Kloks", "authors": "Ton Kloks, Sheung-Hung Poon, Chin-Ting Ung and Yue-Li Wang", "title": "Algorithms for the strong chromatic index of Halin graphs,\n  distance-hereditary graphs and maximal outerplanar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there exist linear-time algorithms that compute the strong\nchromatic index of Halin graphs, of maximal outerplanar graphs and of\ndistance-hereditary graphs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 05:43:28 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Kloks", "Ton", ""], ["Poon", "Sheung-Hung", ""], ["Ung", "Chin-Ting", ""], ["Wang", "Yue-Li", ""]]}, {"id": "1110.0620", "submitter": "Shinji Imahori", "authors": "Shinji Imahori, Tomomi Matsui, Ryuhei Miyashiro", "title": "A 2.75-Approximation Algorithm for the Unconstrained Traveling\n  Tournament Problem", "comments": "12 pages, 1 figure", "journal-ref": "Annals of Operations Research, Volume 218 (2014), Issue 1, pp\n  237-247", "doi": "10.1007/s10479-012-1161-y", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 2.75-approximation algorithm is proposed for the unconstrained traveling\ntournament problem, which is a variant of the traveling tournament problem. For\nthe unconstrained traveling tournament problem, this is the first proposal of\nan approximation algorithm with a constant approximation ratio. In addition,\nthe proposed algorithm yields a solution that meets both the no-repeater and\nmirrored constraints. Computational experiments show that the algorithm\ngenerates solutions of good quality.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 09:35:21 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Imahori", "Shinji", ""], ["Matsui", "Tomomi", ""], ["Miyashiro", "Ryuhei", ""]]}, {"id": "1110.0725", "submitter": "Paulo Jesus", "authors": "Paulo Jesus, Carlos Baquero, Paulo S\\'ergio Almeida", "title": "A Survey of Distributed Data Aggregation Algorithms", "comments": "45 pages, Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data aggregation is an important task, allowing the decentralized\ndetermination of meaningful global properties, that can then be used to direct\nthe execution of other applications. The resulting values result from the\ndistributed computation of functions like COUNT, SUM and AVERAGE. Some\napplication examples can found to determine the network size, total storage\ncapacity, average load, majorities and many others.\n  In the last decade, many different approaches have been proposed, with\ndifferent trade-offs in terms of accuracy, reliability, message and time\ncomplexity. Due to the considerable amount and variety of aggregation\nalgorithms, it can be difficult and time consuming to determine which\ntechniques will be more appropriate to use in specific settings, justifying the\nexistence of a survey to aid in this task.\n  This work reviews the state of the art on distributed data aggregation\nalgorithms, providing three main contributions. First, it formally defines the\nconcept of aggregation, characterizing the different types of aggregation\nfunctions. Second, it succinctly describes the main aggregation techniques,\norganizing them in a taxonomy. Finally, it provides some guidelines toward the\nselection and use of the most relevant techniques, summarizing their principal\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 15:24:25 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Jesus", "Paulo", ""], ["Baquero", "Carlos", ""], ["Almeida", "Paulo S\u00e9rgio", ""]]}, {"id": "1110.0728", "submitter": "Katharina Huber", "authors": "K. T. Huber, V. Moulton", "title": "Encoding and Constructing 1-Nested Phylogenetic Networks with Trinets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic networks are a generalization of phylogenetic trees that are\nused in biology to represent reticulate or non-treelike evolution. Recently,\nseveral algorithms have been developed which aim to construct phylogenetic\nnetworks from biological data using {\\em triplets}, i.e. binary phylogenetic\ntrees on 3-element subsets of a given set of species. However, a fundamental\nproblem with this approach is that the triplets displayed by a phylogenetic\nnetwork do not necessary uniquely determine or {\\em encode} the network. Here\nwe propose an alternative approach to encoding and constructing phylogenetic\nnetworks, which uses phylogenetic networks on 3-element subsets of a set, or\n{\\em trinets}, rather than triplets. More specifically, we show that for a\nspecial, well-studied type of phylogenetic network called a 1-nested network,\nthe trinets displayed by a 1-nested network always encode the network. We also\npresent an efficient algorithm for deciding whether a {\\em dense} set of\ntrinets (i.e. one that contains a trinet on every 3-element subset of a set)\ncan be displayed by a 1-nested network or not and, if so, constructs that\nnetwork. In addition, we discuss some potential new directions that this new\napproach opens up for constructing and comparing phylogenetic networks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 15:34:28 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Huber", "K. T.", ""], ["Moulton", "V.", ""]]}, {"id": "1110.0892", "submitter": "Swapnoneel  Roy", "authors": "N. S. Narayanaswamy and Swapnoneel Roy", "title": "On Approximability of Block Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block Sorting is a well studied problem, motivated by its applications in\nOptical Character Recognition (OCR), and Computational Biology. Block Sorting\nhas been shown to be NP-Hard, and two separate polynomial time 2-approximation\nalgorithms have been designed for the problem. But questions like whether a\nbetter approximation algorithm can be designed, and whether the problem is\nAPX-Hard have been open for quite a while now.\n  In this work we answer the latter question by proving Block Sorting to be\nMax-SNP-Hard (APX-Hard). The APX-Hardness result is based on a linear reduction\nof Max-3SAT to Block Sorting. We also provide a new lower bound for the problem\nvia a new parametrized problem k-Block Merging.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 04:12:09 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Narayanaswamy", "N. S.", ""], ["Roy", "Swapnoneel", ""]]}, {"id": "1110.0938", "submitter": "Pradipta Mitra", "authors": "Magnus M. Halldorsson, Pradipta Mitra", "title": "Wireless Connectivity and Capacity", "comments": "to appear in SODA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ wireless transceivers located in a plane, a fundamental problem in\nwireless communications is to construct a strongly connected digraph on them\nsuch that the constituent links can be scheduled in fewest possible time slots,\nassuming the SINR model of interference.\n  In this paper, we provide an algorithm that connects an arbitrary point set\nin $O(\\log n)$ slots, improving on the previous best bound of $O(\\log^2 n)$ due\nto Moscibroda. This is complemented with a super-constant lower bound on our\napproach to connectivity. An important feature is that the algorithms allow for\nbi-directional (half-duplex) communication.\n  One implication of this result is an improved bound of $\\Omega(1/\\log n)$ on\nthe worst-case capacity of wireless networks, matching the best bound known for\nthe extensively studied average-case.\n  We explore the utility of oblivious power assignments, and show that\nessentially all such assignments result in a worst case bound of $\\Omega(n)$\nslots for connectivity. This rules out a recent claim of a $O(\\log n)$ bound\nusing oblivious power. On the other hand, using our result we show that\n$O(\\min(\\log \\Delta, \\log n \\cdot (\\log n + \\log \\log \\Delta)))$ slots suffice,\nwhere $\\Delta$ is the ratio between the largest and the smallest links in a\nminimum spanning tree of the points.\n  Our results extend to the related problem of minimum latency aggregation\nscheduling, where we show that aggregation scheduling with $O(\\log n)$ latency\nis possible, improving upon the previous best known latency of $O(\\log^3 n)$.\nWe also initiate the study of network design problems in the SINR model beyond\nstrong connectivity, obtaining similar bounds for biconnected and $k$-edge\nconnected structures.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 10:14:26 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Mitra", "Pradipta", ""]]}, {"id": "1110.0976", "submitter": "Stefan Kratsch", "authors": "Danny Hermelin and Stefan Kratsch and Karolina So{\\l}tys and Magnus\n  Wahlstr\\\"om and Xi Wu", "title": "Hierarchies of Inefficient Kernelizability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of Bodlaender et al. (ICALP 2008) and Fortnow and Santhanam\n(STOC 2008) allows us to exclude the existence of polynomial kernels for a\nrange of problems under reasonable complexity-theoretical assumptions. However,\nthere are also some issues that are not addressed by this framework, including\nthe existence of Turing kernels such as the \"kernelization\" of Leaf Out\nBranching(k) into a disjunction over n instances of size poly(k). Observing\nthat Turing kernels are preserved by polynomial parametric transformations, we\ndefine a kernelization hardness hierarchy, akin to the M- and W-hierarchy of\nordinary parameterized complexity, by the PPT-closure of problems that seem\nlikely to be fundamentally hard for efficient Turing kernelization. We find\nthat several previously considered problems are complete for our fundamental\nhardness class, including Min Ones d-SAT(k), Binary NDTM Halting(k), Connected\nVertex Cover(k), and Clique(k log n), the clique problem parameterized by k log\nn.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 13:14:58 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hermelin", "Danny", ""], ["Kratsch", "Stefan", ""], ["So\u0142tys", "Karolina", ""], ["Wahlstr\u00f6m", "Magnus", ""], ["Wu", "Xi", ""]]}, {"id": "1110.0990", "submitter": "Marco Molinaro", "authors": "Marco Molinaro and R. Ravi", "title": "The Query-commit Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the query-commit problem we are given a graph where edges have distinct\nprobabilities of existing. It is possible to query the edges of the graph, and\nif the queried edge exists then its endpoints are irrevocably matched. The goal\nis to find a querying strategy which maximizes the expected size of the\nmatching obtained. This stochastic matching setup is motivated by applications\nin kidney exchanges and online dating.\n  In this paper we address the query-commit problem from both theoretical and\nexperimental perspectives. First, we show that a simple class of edges can be\nqueried without compromising the optimality of the strategy. This property is\nthen used to obtain in polynomial time an optimal querying strategy when the\ninput graph is sparse. Next we turn our attentions to the kidney exchange\napplication, focusing on instances modeled over real data from existing\nexchange programs. We prove that, as the number of nodes grows, almost every\ninstance admits a strategy which matches almost all nodes. This result supports\nthe intuition that more exchanges are possible on a larger pool of\npatient/donors and gives theoretical justification for unifying the existing\nexchange programs. Finally, we evaluate experimentally different querying\nstrategies over kidney exchange instances. We show that even very simple\nheuristics perform fairly well, being within 1.5% of an optimal clairvoyant\nstrategy, that knows in advance the edges in the graph. In such a\ntime-sensitive application, this result motivates the use of committing\nstrategies.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 14:08:33 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Molinaro", "Marco", ""], ["Ravi", "R.", ""]]}, {"id": "1110.1064", "submitter": "Ning Tan", "authors": "Prasad Raghavendra and Ning Tan", "title": "Approximating CSPs with Global Cardinality Constraints Using SDP\n  Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with approximating constraint satisfaction problems\n(CSPs) with an additional global cardinality constraints. For example, \\maxcut\nis a boolean CSP where the input is a graph $G = (V,E)$ and the goal is to find\na cut $S \\cup \\bar S = V$ that maximizes the numberof crossing edges,\n$|E(S,\\bar S)|$. The \\maxbisection problem is a variant of \\maxcut with an\nadditional global constraint that each side of the cut has exactly half the\nvertices, i.e., $|S| = |V|/2$. Several other natural optimization problems like\n\\minbisection and approximating Graph Expansion can be formulated as CSPs with\nglobal constraints.\n  In this work, we formulate a general approach towards approximating CSPs with\nglobal constraints using SDP hierarchies. To demonstrate the approach we\npresent the following results:\n  Using the Lasserre hierarchy, we present an algorithm that runs in time\n$O(n^{poly(1/\\epsilon)})$ that given an instance of \\maxbisection with value\n$1-\\epsilon$, finds a bisection with value $1-O(\\sqrt{\\epsilon})$. This\napproximation is near-optimal (up to constant factors in $O()$) under the\nUnique Games Conjecture.\n  By a computer-assisted proof, we show that the same algorithm also achieves a\n0.85-approximation for \\maxbisection, improving on the previous bound of 0.70\n(note that it is \\uniquegames hard to approximate better than a 0.878 factor).\nThe same algorithm also yields a 0.92-approximation for \\maxtwosat with\ncardinality constraints.\n  For every CSP with a global cardinality constraints, we present a generic\nconversion from integrality gap instances for the Lasserre hierarchy to a {\\it\ndictatorship test} whose soundness is at most integrality gap. Dictatorship\ntesting gadgets are central to hardness results for CSPs, and a generic\nconversion of the above nature lies at the core of the tight Unique Games based\nhardness result for CSPs. \\cite{Raghavendra08}\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 18:31:44 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Raghavendra", "Prasad", ""], ["Tan", "Ning", ""]]}, {"id": "1110.1079", "submitter": "Krzysztof Onak", "authors": "Krzysztof Onak and Dana Ron and Michal Rosen and Ronitt Rubinfeld", "title": "A Near-Optimal Sublinear-Time Algorithm for Approximating the Minimum\n  Vertex Cover Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a nearly optimal sublinear-time algorithm for approximating the size\nof a minimum vertex cover in a graph G. The algorithm may query the degree\ndeg(v) of any vertex v of its choice, and for each 1 <= i <= deg(v), it may ask\nfor the i-th neighbor of v. Letting VC_opt(G) denote the minimum size of vertex\ncover in G, the algorithm outputs, with high constant success probability, an\nestimate VC_estimate(G) such that VC_opt(G) <= VC_estimate(G) <= 2 * VC_opt(G)\n+ epsilon*n, where epsilon is a given additive approximation parameter. We\nrefer to such an estimate as a (2,epsilon)-estimate. The query complexity and\nrunning time of the algorithm are ~O(avg_deg * poly(1/epsilon)), where avg_deg\ndenotes the average vertex degree in the graph. The best previously known\nsublinear algorithm, of Yoshida et al. (STOC 2009), has query complexity and\nrunning time O(d^4/epsilon^2), where d is the maximum degree in the graph.\nGiven the lower bound of Omega(avg_deg) (for constant epsilon) for obtaining\nsuch an estimate (with any constant multiplicative factor) due to Parnas and\nRon (TCS 2007), our result is nearly optimal.\n  In the case that the graph is dense, that is, the number of edges is\nTheta(n^2), we consider another model, in which the algorithm may ask, for any\npair of vertices u and v, whether there is an edge between u and v. We show how\nto adapt the algorithm that uses neighbor queries to this model and obtain an\nalgorithm that outputs a (2,epsilon)-estimate of the size of a minimum vertex\ncover whose query complexity and running time are ~O(n) * poly(1/epsilon).\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 19:27:57 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Onak", "Krzysztof", ""], ["Ron", "Dana", ""], ["Rosen", "Michal", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1110.1124", "submitter": "Shiyao Chen", "authors": "Shiyao Chen and Lang Tong and Ting He", "title": "Optimal Deadline Scheduling with Commitment", "comments": "8 pages, 10 figures, 49th Allerton Conference on Communication,\n  Control and Computing, Monticello, IL, September 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online preemptive scheduling problem where jobs with deadlines\narrive sporadically. A commitment requirement is imposed such that the\nscheduler has to either accept or decline a job immediately upon arrival. The\nscheduler's decision to accept an arriving job constitutes a contract with the\ncustomer; if the accepted job is not completed by its deadline as promised, the\nscheduler loses the value of the corresponding job and has to pay an additional\npenalty depending on the amount of unfinished workload. The objective of the\nonline scheduler is to maximize the overall profit, i.e., the total value of\nthe admitted jobs completed before their deadlines less the penalty paid for\nthe admitted jobs that miss their deadlines. We show that the maximum\ncompetitive ratio is $3-2\\sqrt{2}$ and propose a simple online algorithm to\nachieve this competitive ratio. The optimal scheduling includes a threshold\nadmission and a greedy scheduling policies. The proposed algorithm has direct\napplications to the charging of plug-in hybrid electrical vehicles (PHEV) at\ngarages or parking lots.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 00:49:03 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Chen", "Shiyao", ""], ["Tong", "Lang", ""], ["He", "Ting", ""]]}, {"id": "1110.1180", "submitter": "Abhijeet Khopkar", "authors": "Abhijeet Khopkar and Sathish Govindarajan", "title": "On Computing Optimal Locally Gabriel Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delaunay and Gabriel graphs are widely studied geometric proximity\nstructures. Motivated by applications in wireless routing, relaxed versions of\nthese graphs known as \\emph{Locally Delaunay Graphs} ($LDGs$) and \\emph{Locally\nGabriel Graphs} ($LGGs$) were proposed. We propose another generalization of\n$LGGs$ called \\emph{Generalized Locally Gabriel Graphs} ($GLGGs$) in the\ncontext when certain edges are forbidden in the graph. Unlike a Gabriel Graph,\nthere is no unique $LGG$ or $GLGG$ for a given point set because no edge is\nnecessarily included or excluded. This property allows us to choose an\n$LGG/GLGG$ that optimizes a parameter of interest in the graph. We show that\ncomputing an edge maximum $GLGG$ for a given problem instance is NP-hard and\nalso APX-hard. We also show that computing an $LGG$ on a given point set with\ndilation $\\le k$ is NP-hard. Finally, we give an algorithm to verify whether a\ngiven geometric graph $G=(V,E)$ is a valid $LGG$.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 08:29:11 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2012 10:43:32 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Khopkar", "Abhijeet", ""], ["Govindarajan", "Sathish", ""]]}, {"id": "1110.1194", "submitter": "Stavros Nikolopoulos D.", "authors": "Maria Chroni and Stavros D. Nikolopoulos", "title": "Efficient Encoding of Watermark Numbers as Reducible Permutation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a software watermarking environment, several graph theoretic watermark\nmethods use numbers as watermark values, where some of these methods encode the\nwatermark numbers as graph structures. In this paper we extended the class of\nerror correcting graphs by proposing an efficient and easily implemented codec\nsystem for encoding watermark numbers as reducible permutation flow-graphs.\nMore precisely, we first present an efficient algorithm which encodes a\nwatermark number $w$ as self-inverting permutation $\\pi^*$ and, then, an\nalgorithm which encodes the self-inverting permutation $\\pi^*$ as a reducible\npermutation flow-graph $F[\\pi^*]$ by exploiting domination relations on the\nelements of $\\pi^*$ and using an efficient DAG representation of $\\pi^*$. The\nwhole encoding process takes O(n) time and space, where $n$ is the binary size\nof the number $w$ or, equivalently, the number of elements of the permutation\n$\\pi^*$. We also propose efficient decoding algorithms which extract the number\n$w$ from the reducible permutation flow-graph $F[\\pi^*]$ within the same time\nand space complexity. The two main components of our proposed codec system,\ni.e., the self-inverting permutation $\\pi^*$ and the reducible permutation\ngraph $F[\\pi^*]$, incorporate important structural properties which make our\nsystem resilient to attacks.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 09:24:51 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Chroni", "Maria", ""], ["Nikolopoulos", "Stavros D.", ""]]}, {"id": "1110.1320", "submitter": "David Eisenstat", "authors": "David Eisenstat and Philip Klein and Claire Mathieu", "title": "An efficient polynomial-time approximation scheme for Steiner forest in\n  planar graphs", "comments": "added material on balanced branch decompositions; fixed theorem\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $O(n \\log^3 n)$ approximation scheme for Steiner forest in planar\ngraphs, improving on the previous approximation scheme for this problem, which\nruns in $O(n^{f(\\epsilon)})$ time.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 16:51:57 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2011 18:45:35 GMT"}], "update_date": "2011-10-26", "authors_parsed": [["Eisenstat", "David", ""], ["Klein", "Philip", ""], ["Mathieu", "Claire", ""]]}, {"id": "1110.1328", "submitter": "Venu Satuluri", "authors": "Venu Satuluri and Srinivasan Parthasarathy", "title": "Bayesian Locality Sensitive Hashing for Fast Similarity Search", "comments": "13 pages, 5 Tables, 21 figures. Added acknowledgments in v3. A\n  slightly shorter version of this paper without the appendix has been\n  published in the PVLDB journal, 5(5):430-441, 2012.\n  http://vldb.org/pvldb/vol5/p430_venusatuluri_vldb2012.pdf", "journal-ref": "PVLDB 5(5):430-441, 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of objects and an associated similarity measure, the\nall-pairs similarity search problem asks us to find all pairs of objects with\nsimilarity greater than a certain user-specified threshold. Locality-sensitive\nhashing (LSH) based methods have become a very popular approach for this\nproblem. However, most such methods only use LSH for the first phase of\nsimilarity search - i.e. efficient indexing for candidate generation. In this\npaper, we present BayesLSH, a principled Bayesian algorithm for the subsequent\nphase of similarity search - performing candidate pruning and similarity\nestimation using LSH. A simpler variant, BayesLSH-Lite, which calculates\nsimilarities exactly, is also presented. BayesLSH is able to quickly prune away\na large majority of the false positive candidate pairs, leading to significant\nspeedups over baseline approaches. For BayesLSH, we also provide probabilistic\nguarantees on the quality of the output, both in terms of accuracy and recall.\nFinally, the quality of BayesLSH's output can be easily tuned and does not\nrequire any manual setting of the number of hashes to use for similarity\nestimation, unlike standard approaches. For two state-of-the-art candidate\ngeneration algorithms, AllPairs and LSH, BayesLSH enables significant speedups,\ntypically in the range 2x-20x for a wide variety of datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 17:13:48 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2011 17:46:46 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2012 19:34:39 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Satuluri", "Venu", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1110.1358", "submitter": "Richard Peng", "authors": "Hui Han Chin, Aleksander Madry, Gary Miller, Richard Peng", "title": "Runtime Guarantees for Regression Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study theoretical runtime guarantees for a class of optimization problems\nthat occur in a wide variety of inference problems. these problems are\nmotivated by the lasso framework and have applications in machine learning and\ncomputer vision.\n  Our work shows a close connection between these problems and core questions\nin algorithmic graph theory. While this connection demonstrates the\ndifficulties of obtaining runtime guarantees, it also suggests an approach of\nusing techniques originally developed for graph algorithms.\n  We then show that most of these problems can be formulated as a grouped least\nsquares problem, and give efficient algorithms for this formulation. Our\nalgorithms rely on routines for solving quadratic minimization problems, which\nin turn are equivalent to solving linear systems. Finally we present some\nexperimental results on applying our approximation algorithm to image\nprocessing problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 19:24:24 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2012 18:23:58 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Chin", "Hui Han", ""], ["Madry", "Aleksander", ""], ["Miller", "Gary", ""], ["Peng", "Richard", ""]]}, {"id": "1110.1360", "submitter": "Aravindan Vijayaraghavan", "authors": "Aditya Bhaskara, Moses Charikar, Venkatesan Guruswami, Aravindan\n  Vijayaraghavan, Yuan Zhou", "title": "Polynomial integrality gaps for strong SDP relaxations of Densest\n  k-subgraph", "comments": "26 ages, 1 figure. To appear in Symposium on Discrete Algorithms\n  (SODA) 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The densest k-subgraph (DkS) problem (i.e. find a size k subgraph with\nmaximum number of edges), is one of the notorious problems in approximation\nalgorithms. There is a significant gap between known upper and lower bounds for\nDkS: the current best algorithm gives an ~ O(n^{1/4}) approximation, while even\nshowing a small constant factor hardness requires significantly stronger\nassumptions than P != NP. In addition to interest in designing better\nalgorithms, a number of recent results have exploited the conjectured hardness\nof densest k-subgraph and its variants. Thus, understanding the approximability\nof DkS is an important challenge.\n  In this work, we give evidence for the hardness of approximating DkS within\npolynomial factors. Specifically, we expose the limitations of strong\nsemidefinite programs from SDP hierarchies in solving densest k-subgraph. Our\nresults include:\n  * A lower bound of Omega(n^{1/4}/log^3 n) on the integrality gap for\nOmega(log n/log log n) rounds of the Sherali-Adams relaxation for DkS. This\nalso holds for the relaxation obtained from Sherali-Adams with an added SDP\nconstraint. Our gap instances are in fact Erdos-Renyi random graphs.\n  * For every epsilon > 0, a lower bound of n^{2/53-eps} on the integrality gap\nof n^{Omega(eps)} rounds of the Lasserre SDP relaxation for DkS, and an\nn^{Omega_eps(1)} gap for n^{1-eps} rounds. Our construction proceeds via a\nreduction from random instances of a certain Max-CSP over large domains.\n  In the absence of inapproximability results for DkS, our results show that\neven the most powerful SDPs are unable to beat a factor of n^{Omega(1)}, and in\nfact even improving the best known n^{1/4} factor is a barrier for current\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 19:29:01 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Charikar", "Moses", ""], ["Guruswami", "Venkatesan", ""], ["Vijayaraghavan", "Aravindan", ""], ["Zhou", "Yuan", ""]]}, {"id": "1110.1462", "submitter": "Antonio Irpino PhD", "authors": "Antonio Irpino and Rosanna Verde and Francisco de AT De Carvalho", "title": "Dynamic Clustering of Histogram Data Based on Adaptive Squared\n  Wasserstein Distances", "comments": null, "journal-ref": "Expert Systems with Applications, vol. 41, p. 3351-3366, 2014", "doi": "10.1016/j.eswa.2013.12.001", "report-no": null, "categories": "math.ST cs.DS math.PR stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with clustering methods based on adaptive distances for\nhistogram data using a dynamic clustering algorithm. Histogram data describes\nindividuals in terms of empirical distributions. These kind of data can be\nconsidered as complex descriptions of phenomena observed on complex objects:\nimages, groups of individuals, spatial or temporal variant data, results of\nqueries, environmental data, and so on. The Wasserstein distance is used to\ncompare two histograms. The Wasserstein distance between histograms is\nconstituted by two components: the first based on the means, and the second, to\ninternal dispersions (standard deviation, skewness, kurtosis, and so on) of the\nhistograms. To cluster sets of histogram data, we propose to use Dynamic\nClustering Algorithm, (based on adaptive squared Wasserstein distances) that is\na k-means-like algorithm for clustering a set of individuals into $K$ classes\nthat are apriori fixed.\n  The main aim of this research is to provide a tool for clustering histograms,\nemphasizing the different contributions of the histogram variables, and their\ncomponents, to the definition of the clusters. We demonstrate that this can be\nachieved using adaptive distances. Two kind of adaptive distances are\nconsidered: the first takes into account the variability of each component of\neach descriptor for the whole set of individuals; the second takes into account\nthe variability of each component of each descriptor in each cluster. We\nfurnish interpretative tools of the obtained partition based on an extension of\nthe classical measures (indexes) to the use of adaptive distances in the\nclustering criterion function. Applications on synthetic and real-world data\ncorroborate the proposed procedure.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2011 09:11:35 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Irpino", "Antonio", ""], ["Verde", "Rosanna", ""], ["De Carvalho", "Francisco de AT", ""]]}, {"id": "1110.1580", "submitter": "Aleksander M{\\ka}dry", "authors": "Nikhil Bansal, Niv Buchbinder, Aleksander Madry, Joseph (Seffi) Naor", "title": "A Polylogarithmic-Competitive Algorithm for the k-Server Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first polylogarithmic-competitive randomized online algorithm for\nthe $k$-server problem on an arbitrary finite metric space. In particular, our\nalgorithm achieves a competitive ratio of O(log^3 n log^2 k log log n) for any\nmetric space on n points. Our algorithm improves upon the deterministic\n(2k-1)-competitive algorithm of Koutsoupias and Papadimitriou [J.ACM'95]\nwhenever n is sub-exponential in k.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2011 16:39:34 GMT"}], "update_date": "2011-10-10", "authors_parsed": [["Bansal", "Nikhil", "", "Seffi"], ["Buchbinder", "Niv", "", "Seffi"], ["Madry", "Aleksander", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""]]}, {"id": "1110.1693", "submitter": "Ton Kloks", "authors": "Ton Kloks and Chin-Ting Ung and Yue-Li Wang", "title": "On the strong chromatic index and maximum induced matching of\n  tree-cographs and permutation graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there exist linear-time algorithms that compute the strong\nchromatic index and a maximum induced matching of tree-cographs when the\ndecomposition tree is a part of the input. We also show that there exists an\nefficient algorithm for the strong chromatic index of permutation graphs.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2011 04:04:41 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Kloks", "Ton", ""], ["Ung", "Chin-Ting", ""], ["Wang", "Yue-Li", ""]]}, {"id": "1110.1757", "submitter": "Michael Mahoney", "authors": "Patrick O. Perry and Michael W. Mahoney", "title": "Regularized Laplacian Estimation and Fast Eigenvector Approximation", "comments": "13 pages and 3 figures. A more detailed version of a paper appearing\n  in the 2011 NIPS Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Mahoney and Orecchia demonstrated that popular diffusion-based\nprocedures to compute a quick \\emph{approximation} to the first nontrivial\neigenvector of a data graph Laplacian \\emph{exactly} solve certain regularized\nSemi-Definite Programs (SDPs). In this paper, we extend that result by\nproviding a statistical interpretation of their approximation procedure. Our\ninterpretation will be analogous to the manner in which $\\ell_2$-regularized or\n$\\ell_1$-regularized $\\ell_2$-regression (often called Ridge regression and\nLasso regression, respectively) can be interpreted in terms of a Gaussian prior\nor a Laplace prior, respectively, on the coefficient vector of the regression\nproblem. Our framework will imply that the solutions to the Mahoney-Orecchia\nregularized SDP can be interpreted as regularized estimates of the\npseudoinverse of the graph Laplacian. Conversely, it will imply that the\nsolution to this regularized estimation problem can be computed very quickly by\nrunning, e.g., the fast diffusion-based PageRank procedure for computing an\napproximation to the first nontrivial eigenvector of the graph Laplacian.\nEmpirical results are also provided to illustrate the manner in which\napproximate eigenvector computation \\emph{implicitly} performs statistical\nregularization, relative to running the corresponding exact algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2011 18:43:52 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2011 21:31:02 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Perry", "Patrick O.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1110.1785", "submitter": "Flavio Chierichetti", "authors": "Flavio Chierichetti, Jon Kleinberg", "title": "Voting with Limited Information and Many Alternatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.GT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional axiomatic approach to voting is motivated by the problem of\nreconciling differences in subjective preferences. In contrast, a dominant line\nof work in the theory of voting over the past 15 years has considered a\ndifferent kind of scenario, also fundamental to voting, in which there is a\ngenuinely \"best\" outcome that voters would agree on if they only had enough\ninformation. This type of scenario has its roots in the classical Condorcet\nJury Theorem; it includes cases such as jurors in a criminal trial who all want\nto reach the correct verdict but disagree in their inferences from the\navailable evidence, or a corporate board of directors who all want to improve\nthe company's revenue, but who have different information that favors different\noptions.\n  This style of voting leads to a natural set of questions: each voter has a\n{\\em private signal} that provides probabilistic information about which option\nis best, and a central question is whether a simple plurality voting system,\nwhich tabulates votes for different options, can cause the group decision to\narrive at the correct option. We show that plurality voting is powerful enough\nto achieve this: there is a way for voters to map their signals into votes for\noptions in such a way that --- with sufficiently many voters --- the correct\noption receives the greatest number of votes with high probability. We show\nfurther, however, that any process for achieving this is inherently expensive\nin the number of voters it requires: succeeding in identifying the correct\noption with probability at least $1 - \\eta$ requires $\\Omega(n^3 \\epsilon^{-2}\n\\log \\eta^{-1})$ voters, where $n$ is the number of options and $\\epsilon$ is a\ndistributional measure of the minimum difference between the options.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2011 03:22:04 GMT"}], "update_date": "2015-12-19", "authors_parsed": [["Chierichetti", "Flavio", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1110.1842", "submitter": "Antonio Fern\\'andez Anta", "authors": "Sergio Ar\\'evalo and Antonio Fern\\'andez Anta and Damien Imbs and\n  Ernesto Jim\\'enez and Michel Raynal", "title": "Failure Detectors in Homonymous Distributed Systems (with an Application\n  to Consensus)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the consensus problem in homonymous distributed systems\nwhere processes are prone to crash failures and have no initial knowledge of\nthe system membership (\"homonymous\" means that several processes may have the\nsame identifier). New classes of failure detectors suited to these systems are\nfirst defined. Among them, the classes H\\Omega\\ and H\\Sigma\\ are introduced\nthat are the homonymous counterparts of the classes \\Omega\\ and \\Sigma,\nrespectively. (Recall that the pair <\\Omega,\\Sigma> defines the weakest failure\ndetector to solve consensus.) Then, the paper shows how H\\Omega\\ and H\\Sigma\\\ncan be implemented in homonymous systems without membership knowledge (under\ndifferent synchrony requirements). Finally, two algorithms are presented that\nuse these failure detectors to solve consensus in homonymous asynchronous\nsystems where there is no initial knowledge of the membership. One algorithm\nsolves consensus with <H\\Omega,H\\Sigma>, while the other uses only H\\Omega, but\nneeds a majority of correct processes.\n  Observe that the systems with unique identifiers and anonymous systems are\nextreme cases of homonymous systems from which follows that all these results\nalso apply to these systems. Interestingly, the new failure detector class\nH\\Omega\\ can be implemented with partial synchrony, while the analogous class\nA\\Omega\\ defined for anonymous systems can not be implemented (even in\nsynchronous systems). Hence, the paper provides us with the first proof showing\nthat consensus can be solved in anonymous systems with only partial synchrony\n(and a majority of correct processes).\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2011 14:09:44 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2011 10:45:07 GMT"}, {"version": "v3", "created": "Sun, 27 Nov 2011 13:32:28 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Ar\u00e9valo", "Sergio", ""], ["Anta", "Antonio Fern\u00e1ndez", ""], ["Imbs", "Damien", ""], ["Jim\u00e9nez", "Ernesto", ""], ["Raynal", "Michel", ""]]}, {"id": "1110.1894", "submitter": "Paris Siminelakis", "authors": "Dimitris Fotakis and Paris Siminelakis", "title": "On the Efficiency of Influence-and-Exploit Strategies for Revenue\n  Maximization under Positive Externalities", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of revenue maximization in the marketing model for\nsocial networks introduced by (Hartline, Mirrokni, Sundararajan, WWW '08). We\nrestrict our attention to the Uniform Additive Model and mostly focus on\nInfluence-and-Exploit (IE) marketing strategies. We obtain a comprehensive\ncollection of results on the efficiency and the approximability of IE\nstrategies, which also imply a significant improvement on the best known\napproximation ratios for revenue maximization. Specifically, we show that in\nthe Uniform Additive Model, both computing the optimal marketing strategy and\ncomputing the best IE strategy are $\\NP$-hard for undirected social networks.\nWe observe that allowing IE strategies to offer prices smaller than the myopic\nprice in the exploit step leads to a measurable improvement on their\nperformance. Thus, we show that the best IE strategy approximates the maximum\nrevenue within a factor of 0.911 for undirected and of roughly 0.553 for\ndirected networks. Moreover, we present a natural generalization of IE\nstrategies, with more than two pricing classes, and show that they approximate\nthe maximum revenue within a factor of roughly 0.7 for undirected and of\nroughly 0.35 for directed networks. Utilizing a connection between good IE\nstrategies and large cuts in the underlying social network, we obtain\npolynomial-time algorithms that approximate the revenue of the best IE strategy\nwithin a factor of roughly 0.9. Hence, we significantly improve on the best\nknown approximation ratio for revenue maximization to 0.8229 for undirected and\nto 0.5011 for directed networks (from 2/3 and 1/3, respectively, by Hartline et\nal.).\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 00:09:55 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Siminelakis", "Paris", ""]]}, {"id": "1110.1964", "submitter": "Karol Suchan", "authors": "Lukasz Kowalik, Marcin Pilipczuk, Karol Suchan", "title": "Towards optimal kernel for connected vertex cover in planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameterized complexity of the connected version of the vertex\ncover problem, where the solution set has to induce a connected subgraph.\nAlthough this problem does not admit a polynomial kernel for general graphs\n(unless NP is a subset of coNP/poly), for planar graphs Guo and Niedermeier\n[ICALP'08] showed a kernel with at most 14k vertices, subsequently improved by\nWang et al. [MFCS'11] to 4k. The constant 4 here is so small that a natural\nquestion arises: could it be already an optimal value for this problem? In this\npaper we answer this quesion in negative: we show a (11/3)k-vertex kernel for\nConnected Vertex Cover in planar graphs. We believe that this result will\nmotivate further study in search for an optimal kernel.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 09:06:27 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Kowalik", "Lukasz", ""], ["Pilipczuk", "Marcin", ""], ["Suchan", "Karol", ""]]}, {"id": "1110.2207", "submitter": "Sungjin Im", "authors": "Sungjin Im and Viswanath Nagarajan and Ruben van der Zwaan", "title": "Minimum Latency Submodular Cover", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Minimum Latency Submodular Cover problem (MLSC), which consists\nof a metric $(V,d)$ with source $r\\in V$ and $m$ monotone submodular functions\n$f_1, f_2, ..., f_m: 2^V \\rightarrow [0,1]$. The goal is to find a path\noriginating at $r$ that minimizes the total cover time of all functions. This\ngeneralizes well-studied problems, such as Submodular Ranking [AzarG11] and\nGroup Steiner Tree [GKR00]. We give a polynomial time $O(\\log \\frac{1}{\\eps}\n\\cdot \\log^{2+\\delta} |V|)$-approximation algorithm for MLSC, where\n$\\epsilon>0$ is the smallest non-zero marginal increase of any\n$\\{f_i\\}_{i=1}^m$ and $\\delta>0$ is any constant.\n  We also consider the Latency Covering Steiner Tree problem (LCST), which is\nthe special case of \\mlsc where the $f_i$s are multi-coverage functions. This\nis a common generalization of the Latency Group Steiner Tree\n[GuptaNR10a,ChakrabartyS11] and Generalized Min-sum Set Cover [AzarGY09,\nBansalGK10] problems. We obtain an $O(\\log^2|V|)$-approximation algorithm for\nLCST.\n  Finally we study a natural stochastic extension of the Submodular Ranking\nproblem, and obtain an adaptive algorithm with an $O(\\log 1/ \\eps)$\napproximation ratio, which is best possible. This result also generalizes some\npreviously studied stochastic optimization problems, such as Stochastic Set\nCover [GoemansV06] and Shared Filter Evaluation [MunagalaSW07, LiuPRY08].\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 21:45:49 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2011 19:39:07 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2013 21:28:52 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Im", "Sungjin", ""], ["Nagarajan", "Viswanath", ""], ["van der Zwaan", "Ruben", ""]]}, {"id": "1110.2828", "submitter": "Jacob Fox", "authors": "Noga Alon and Jacob Fox", "title": "Testing perfection is hard", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph property P is strongly testable if for every fixed \\epsilon>0 there\nis a one-sided \\epsilon-tester for P whose query complexity is bounded by a\nfunction of \\epsilon. In classifying the strongly testable graph properties,\nthe first author and Shapira showed that any hereditary graph property (such as\nP the family of perfect graphs) is strongly testable. A property is easily\ntestable if it is strongly testable with query complexity bounded by a\npolynomial function of \\epsilon^{-1}, and otherwise it is hard. One of our main\nresults shows that testing perfectness is hard. The proof shows that testing\nperfectness is at least as hard as testing triangle-freeness, which is hard. On\nthe other hand, we show that induced P_3-freeness is easily testable. This\nsettles one of the two exceptional graphs, the other being C_4 (and its\ncomplement), left open in the characterization by the first author and Shapira\nof graphs H for which induced H-freeness is easily testable.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 02:40:44 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Alon", "Noga", ""], ["Fox", "Jacob", ""]]}, {"id": "1110.2893", "submitter": "Hjalte Wedel Vildh{\\o}j", "authors": "Philip Bille, Inge Li Goertz, Hjalte Wedel Vildh{\\o}j, David Kofoed\n  Wind", "title": "String Matching with Variable Length Gaps", "comments": "draft of full version, extended abstract at SPIRE 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider string matching with variable length gaps. Given a string $T$ and\na pattern $P$ consisting of strings separated by variable length gaps\n(arbitrary strings of length in a specified range), the problem is to find all\nending positions of substrings in $T$ that match $P$. This problem is a basic\nprimitive in computational biology applications. Let $m$ and $n$ be the lengths\nof $P$ and $T$, respectively, and let $k$ be the number of strings in $P$. We\npresent a new algorithm achieving time $O(n\\log k + m +\\alpha)$ and space $O(m\n+ A)$, where $A$ is the sum of the lower bounds of the lengths of the gaps in\n$P$ and $\\alpha$ is the total number of occurrences of the strings in $P$\nwithin $T$. Compared to the previous results this bound essentially achieves\nthe best known time and space complexities simultaneously. Consequently, our\nalgorithm obtains the best known bounds for almost all combinations of $m$,\n$n$, $k$, $A$, and $\\alpha$. Our algorithm is surprisingly simple and\nstraightforward to implement. We also present algorithms for finding and\nencoding the positions of all strings in $P$ for every match of the pattern.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 11:13:48 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Bille", "Philip", ""], ["Goertz", "Inge Li", ""], ["Vildh\u00f8j", "Hjalte Wedel", ""], ["Wind", "David Kofoed", ""]]}, {"id": "1110.2897", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis and Anastasios Zouzias and Michael W. Mahoney and\n  Petros Drineas", "title": "Randomized Dimensionality Reduction for k-means Clustering", "comments": "IEEE Transactions on Information Theory, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the topic of dimensionality reduction for $k$-means clustering.\nDimensionality reduction encompasses the union of two approaches: \\emph{feature\nselection} and \\emph{feature extraction}. A feature selection based algorithm\nfor $k$-means clustering selects a small subset of the input features and then\napplies $k$-means clustering on the selected features. A feature extraction\nbased algorithm for $k$-means clustering constructs a small set of new\nartificial features and then applies $k$-means clustering on the constructed\nfeatures. Despite the significance of $k$-means clustering as well as the\nwealth of heuristic methods addressing it, provably accurate feature selection\nmethods for $k$-means clustering are not known. On the other hand, two provably\naccurate feature extraction methods for $k$-means clustering are known in the\nliterature; one is based on random projections and the other is based on the\nsingular value decomposition (SVD).\n  This paper makes further progress towards a better understanding of\ndimensionality reduction for $k$-means clustering. Namely, we present the first\nprovably accurate feature selection method for $k$-means clustering and, in\naddition, we present two feature extraction methods. The first feature\nextraction method is based on random projections and it improves upon the\nexisting results in terms of time complexity and number of features needed to\nbe extracted. The second feature extraction method is based on fast approximate\nSVD factorizations and it also improves upon the existing results in terms of\ntime complexity. The proposed algorithms are randomized and provide\nconstant-factor approximation guarantees with respect to the optimal $k$-means\nobjective value.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 11:24:59 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2013 09:47:52 GMT"}, {"version": "v3", "created": "Tue, 4 Nov 2014 19:40:43 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Boutsidis", "Christos", ""], ["Zouzias", "Anastasios", ""], ["Mahoney", "Michael W.", ""], ["Drineas", "Petros", ""]]}, {"id": "1110.3038", "submitter": "Maria Isabel Herrero", "authors": "Maria Isabel Herrero, Gabriela Jeronimo and Juan Sabia", "title": "Affine solution sets of sparse polynomial systems", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.DS cs.SC math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the equidimensional decomposition of affine varieties\ndefined by sparse polynomial systems. For generic systems with fixed supports,\nwe give combinatorial conditions for the existence of positive dimensional\ncomponents which characterize the equidimensional decomposition of the\nassociated affine variety. This result is applied to design an equidimensional\ndecomposition algorithm for generic sparse systems. For arbitrary sparse\nsystems of n polynomials in n variables with fixed supports, we obtain an upper\nbound for the degree of the affine variety defined and we present an algorithm\nwhich computes finite sets of points representing its equidimensional\ncomponents.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 19:53:30 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2012 20:48:02 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Herrero", "Maria Isabel", ""], ["Jeronimo", "Gabriela", ""], ["Sabia", "Juan", ""]]}, {"id": "1110.3100", "submitter": "Mark Sandler", "authors": "Eyal Even Dar and Mark Sandler", "title": "Telling Two Distributions Apart: a Tight Characterization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distinguishing between two arbitrary black-box\ndistributions defined over the domain [n], given access to $s$ samples from\nboth. It is known that in the worst case O(n^{2/3}) samples is both necessary\nand sufficient, provided that the distributions have L1 difference of at least\n{\\epsilon}. However, it is also known that in many cases fewer samples suffice.\nWe identify a new parameter, that provides an upper bound on how many samples\nneeded, and present an efficient algorithm that requires the number of samples\nindependent of the domain size. Also for a large subclass of distributions we\nprovide a lower bound, that matches our upper bound up to a poly-logarithmic\nfactor.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2011 00:46:23 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Dar", "Eyal Even", ""], ["Sandler", "Mark", ""]]}, {"id": "1110.3225", "submitter": "Anton Dries", "authors": "Anton Dries and Siegfried Nijssen", "title": "Mining Patterns in Networks using Homomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years many algorithms have been developed for finding patterns in\ngraphs and networks. A disadvantage of these algorithms is that they use\nsubgraph isomorphism to determine the support of a graph pattern; subgraph\nisomorphism is a well-known NP complete problem. In this paper, we propose an\nalternative approach which mines tree patterns in networks by using subgraph\nhomomorphism. The advantage of homomorphism is that it can be computed in\npolynomial time, which allows us to develop an algorithm that mines tree\npatterns in arbitrary graphs in incremental polynomial time. Homomorphism\nhowever entails two problems not found when using isomorphism: (1) two patterns\nof different size can be equivalent; (2) patterns of unbounded size can be\nfrequent. In this paper we formalize these problems and study solutions that\neasily fit within our algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2011 14:47:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Dries", "Anton", ""], ["Nijssen", "Siegfried", ""]]}, {"id": "1110.3381", "submitter": "Gianni Franceschini", "authors": "Gianni Franceschini and Roberto Grossi and S. Muthukrishnan", "title": "Partial Data Compression and Text Indexing via Optimal Suffix\n  Multi-Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an input text string T[1,N] drawn from an unbounded alphabet. We\nstudy partial computation in suffix-based problems for Data Compression and\nText Indexing such as\n  (I) retrieve any segment of K<=N consecutive symbols from the Burrows-Wheeler\ntransform of T, and\n  (II) retrieve any chunk of K<=N consecutive entries of the Suffix Array or\nthe Suffix Tree.\n  Prior literature would take O(N log N) comparisons (and time) to solve these\nproblems by solving the total problem of building the entire Burrows-Wheeler\ntransform or Text Index for T, and performing a post-processing to single out\nthe wanted portion.\n  We introduce a novel adaptive approach to partial computational problems\nabove, and solve both the partial problems in O(K log K + N) comparisons and\ntime, improving the best known running times of O(N log N) for K=o(N).\n  These partial-computation problems are intimately related since they share a\ncommon bottleneck: the suffix multi-selection problem, which is to output the\nsuffixes of rank r_1,r_2,...,r_K under the lexicographic order, where\nr_1<r_2<...<r_K, r_i in [1,N]. Special cases of this problem are well known:\nK=N is the suffix sorting problem that is the workhorse in Stringology with\nhundreds of applications, and K=1 is the recently studied suffix selection.\n  We show that suffix multi-selection can be solved in Theta(N log N -\nsum_{j=0}^K Delta_j log Delta_j+N) time and comparisons, where r_0=0,\nr_{K+1}=N+1, and Delta_j=r_{j+1}-r_j for 0<=j<=K. This is asymptotically\noptimal, and also matches the bound in [Dobkin, Munro, JACM 28(3)] for\nmulti-selection on atomic elements (not suffixes). Matching the bound known for\natomic elements for strings is a long running theme and challenge from 70's,\nwhich we achieve for the suffix multi-selection problem. The partial suffix\nproblems as well as the suffix multi-selection problem have many applications.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2011 05:16:18 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Franceschini", "Gianni", ""], ["Grossi", "Roberto", ""], ["Muthukrishnan", "S.", ""]]}, {"id": "1110.3564", "submitter": "Sewoong Oh", "authors": "David R. Karger and Sewoong Oh and Devavrat Shah", "title": "Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems", "comments": "38 pages, 4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing systems, in which numerous tasks are electronically distributed\nto numerous \"information piece-workers\", have emerged as an effective paradigm\nfor human-powered solving of large scale problems in domains such as image\nclassification, data entry, optical character recognition, recommendation, and\nproofreading. Because these low-paid workers can be unreliable, nearly all such\nsystems must devise schemes to increase confidence in their answers, typically\nby assigning each task multiple times and combining the answers in an\nappropriate manner, e.g. majority voting.\n  In this paper, we consider a general model of such crowdsourcing tasks and\npose the problem of minimizing the total price (i.e., number of task\nassignments) that must be paid to achieve a target overall reliability. We give\na new algorithm for deciding which tasks to assign to which workers and for\ninferring correct answers from the workers' answers. We show that our\nalgorithm, inspired by belief propagation and low-rank matrix approximation,\nsignificantly outperforms majority voting and, in fact, is optimal through\ncomparison to an oracle that knows the reliability of every worker. Further, we\ncompare our approach with a more general class of algorithms which can\ndynamically assign tasks. By adaptively deciding which questions to ask to the\nnext arriving worker, one might hope to reduce uncertainty more efficiently. We\nshow that, perhaps surprisingly, the minimum price necessary to achieve a\ntarget reliability scales in the same manner under both adaptive and\nnon-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under\nboth scenarios. This strongly relies on the fact that workers are fleeting and\ncan not be exploited. Therefore, architecturally, our results suggest that\nbuilding a reliable worker-reputation system is essential to fully harnessing\nthe potential of adaptive designs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 02:52:20 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2011 18:49:14 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2012 21:23:24 GMT"}, {"version": "v4", "created": "Tue, 26 Mar 2013 07:28:04 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Karger", "David R.", ""], ["Oh", "Sewoong", ""], ["Shah", "Devavrat", ""]]}, {"id": "1110.3619", "submitter": "Carola Winzen", "authors": "Benjamin Doerr and Carola Winzen", "title": "Playing Mastermind With Constant-Size Memory", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the classic board game of Mastermind with $n$ holes and a constant\nnumber of colors. A result of Chv\\'atal (Combinatorica 3 (1983), 325-329)\nstates that the codebreaker can find the secret code with $\\Theta(n / \\log n)$\nquestions. We show that this bound remains valid if the codebreaker may only\nstore a constant number of guesses and answers. In addition to an intrinsic\ninterest in this question, our result also disproves a conjecture of Droste,\nJansen, and Wegener (Theory of Computing Systems 39 (2006), 525-544) on the\nmemory-restricted black-box complexity of the OneMax function class.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 09:44:13 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Doerr", "Benjamin", ""], ["Winzen", "Carola", ""]]}, {"id": "1110.3850", "submitter": "Eric Price", "authors": "Piotr Indyk, Eric Price, and David P. Woodruff", "title": "On the Power of Adaptivity in Sparse Recovery", "comments": "18 pages; appearing at FOCS 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The goal of (stable) sparse recovery is to recover a $k$-sparse approximation\n$x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is\nto recover $x*$ such that ||x-x*||_p <= C min_{k-sparse x'} ||x-x'||_q for some\nconstant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or\n$p=q=2$, this task can be accomplished using $m=O(k \\log (n/k))$ non-adaptive\nmeasurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].\n  In this paper we show that if one is allowed to perform measurements that are\nadaptive, then the number of measurements can be considerably reduced.\nSpecifically, for $C=1+eps$ and $p=q=2$ we show - A scheme with $m=O((1/eps)k\nlog log (n eps/k))$ measurements that uses $O(log* k \\log \\log (n eps/k))$\nrounds. This is a significant improvement over the best possible non-adaptive\nbound. - A scheme with $m=O((1/eps) k log (k/eps) + k \\log (n/k))$ measurements\nthat uses /two/ rounds. This improves over the best possible non-adaptive\nbound. To the best of our knowledge, these are the first results of this type.\nAs an independent application, we show how to solve the problem of finding a\nduplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using\n$O(log n)$ bits of space and $O(log log n)$ passes, improving over the best\npossible space complexity achievable using a single pass.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 23:35:11 GMT"}], "update_date": "2011-10-19", "authors_parsed": [["Indyk", "Piotr", ""], ["Price", "Eric", ""], ["Woodruff", "David P.", ""]]}, {"id": "1110.4052", "submitter": "Howard Kleiman", "authors": "Howard Kleiman", "title": "The General Traveling Salesman Problem, Version 5", "comments": "This Version 5 corrects some omissions that occurred on the earlier\n  version as well as some corrections. arXiv admin note: substantial text\n  overlap with arXiv:math/0508212", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is example 5 in chapter 5. Let H be an n-cycle. A permutation s is\nH-admissible if Hs = H' where H' is an n-cycle. Here we define a 19 X 19\nmatrix, M, in the following way: We obtain the remainders modulo 100 of each of\nthe smallest 342 odd primes. we obtain the remainders modulo 100 of each of the\nprimes. They are placed in M according to the original value of each prime.\nThus their placement depends on the the original ordinal values of the primes\naccording to size. We use this ordering to place the primes in M. Let H_0 be an\ninitial 19 cycles arbitrarily chosen. We apply a sequence of up to [ln(n)+1]\nH_0 3-cycles to obtain a 19-cycle of smaller value than H_0, call the new\n19-cycle H_1. We follow this procedure to obtain H_1. We call [ln(n)] + 1 a\nchain. We add up the values of the 19-cycles in each chain. This procedure\ncontinues until we cannot obtain a chain the sum of whose values is not\nnegative. COMMENT. I've renamed the document \"Yhe General Traveling Salesman\nProblem, Version 5\". I preciously named it \"The Traveling Salesman, Version 5\".\nAlthough the algorithms work on the GTSP, I thought that more people would\ngoogle it if it was named \"The Traveling Salesman Problem.\" Rhar qas because my\nwork is only available through arxiv.org,\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 17:10:17 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2011 17:58:41 GMT"}, {"version": "v3", "created": "Mon, 6 May 2013 19:52:42 GMT"}, {"version": "v4", "created": "Mon, 17 Jun 2013 20:17:38 GMT"}, {"version": "v5", "created": "Fri, 16 Aug 2013 22:55:35 GMT"}, {"version": "v6", "created": "Thu, 5 Sep 2013 04:40:49 GMT"}, {"version": "v7", "created": "Thu, 26 Sep 2013 18:06:42 GMT"}, {"version": "v8", "created": "Sun, 29 Sep 2013 05:00:26 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Kleiman", "Howard", ""]]}, {"id": "1110.4077", "submitter": "Kitty Meeks", "authors": "Kitty Meeks and Alexander Scott", "title": "The Parameterised Complexity of List Problems on Graphs of Bounded\n  Treewidth", "comments": "Author final version, to appear in Information and Computation.\n  Changes from previous version include improved literature references and\n  restructured proof in Section 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the parameterised complexity of several list problems on graphs,\nwith parameter treewidth or pathwidth. In particular, we show that List Edge\nChromatic Number and List Total Chromatic Number are fixed parameter tractable,\nparameterised by treewidth, whereas List Hamilton Path is W[1]-hard, even\nparameterised by pathwidth. These results resolve two open questions of\nFellows, Fomin, Lokshtanov, Rosamond, Saurabh, Szeider and Thomassen (2011).\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 18:31:30 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2012 16:17:51 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 13:29:25 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Meeks", "Kitty", ""], ["Scott", "Alexander", ""]]}, {"id": "1110.4150", "submitter": "Siddharth Barman", "authors": "Siddharth Barman and Shuchi Chawla", "title": "Traffic-Redundancy Aware Network Design", "comments": "17 pages. To be published in the proceedings of the Twenty-Third\n  Annual ACM-SIAM Symposium on Discrete Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider network design problems for information networks where routers\ncan replicate data but cannot alter it. This functionality allows the network\nto eliminate data-redundancy in traffic, thereby saving on routing costs. We\nconsider two problems within this framework and design approximation\nalgorithms.\n  The first problem we study is the traffic-redundancy aware network design\n(RAND) problem. We are given a weighted graph over a single server and many\nclients. The server owns a number of different data packets and each client\ndesires a subset of the packets; the client demand sets form a laminar set\nsystem. Our goal is to connect every client to the source via a single path,\nsuch that the collective cost of the resulting network is minimized. Here the\ntransportation cost over an edge is its weight times times the number of\ndistinct packets that it carries.\n  The second problem is a facility location problem that we call RAFL. Here the\ngoal is to find an assignment from clients to facilities such that the total\ncost of routing packets from the facilities to clients (along unshared paths),\nplus the total cost of \"producing\" one copy of each desired packet at each\nfacility is minimized.\n  We present a constant factor approximation for the RAFL and an O(log P)\napproximation for RAND, where P is the total number of distinct packets. We\nremark that P is always at most the number of different demand sets desired or\nthe number of clients, and is generally much smaller.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 00:43:25 GMT"}], "update_date": "2011-10-20", "authors_parsed": [["Barman", "Siddharth", ""], ["Chawla", "Shuchi", ""]]}, {"id": "1110.4319", "submitter": "Viswanath Nagarajan", "authors": "Nikhil Bansal, Uriel Feige, Robert Krauthgamer, Konstantin Makarychev,\n  Viswanath Nagarajan, Joseph (Seffi) Naor, Roy Schwartz", "title": "Min-Max Graph Partitioning and Small Set Expansion", "comments": "Full version of paper appearing in FOCS 2011, 29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study graph partitioning problems from a min-max perspective, in which an\ninput graph on n vertices should be partitioned into k parts, and the objective\nis to minimize the maximum number of edges leaving a single part. The two main\nversions we consider are where the k parts need to be of equal-size, and where\nthey must separate a set of k given terminals. We consider a common\ngeneralization of these two problems, and design for it an $O(\\sqrt{\\log n\\log\nk})$-approximation algorithm. This improves over an $O(\\log^2 n)$ approximation\nfor the second version, and roughly $O(k\\log n)$ approximation for the first\nversion that follows from other previous work. We also give an improved\nO(1)-approximation algorithm for graphs that exclude any fixed minor.\n  Our algorithm uses a new procedure for solving the Small-Set Expansion\nproblem. In this problem, we are given a graph G and the goal is to find a\nnon-empty set $S\\subseteq V$ of size $|S| \\leq \\rho n$ with minimum\nedge-expansion. We give an $O(\\sqrt{\\log{n}\\log{(1/\\rho)}})$ bicriteria\napproximation algorithm for the general case of Small-Set Expansion, and O(1)\napproximation algorithm for graphs that exclude any fixed minor.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 15:51:38 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2011 12:01:52 GMT"}], "update_date": "2011-10-21", "authors_parsed": [["Bansal", "Nikhil", "", "Seffi"], ["Feige", "Uriel", "", "Seffi"], ["Krauthgamer", "Robert", "", "Seffi"], ["Makarychev", "Konstantin", "", "Seffi"], ["Nagarajan", "Viswanath", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""], ["Schwartz", "Roy", ""]]}, {"id": "1110.4350", "submitter": "Javad Doliskani", "authors": "Javad Doliskani, Eric Schost", "title": "Taking Roots over High Extensions of Finite Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for computing $m$-th roots over the finite field\n$\\F_q$, where $q = p^n$, with $p$ a prime, and $m$ any positive integer. In the\nparticular case $m=2$, the cost of the new algorithm is an expected\n$O(\\M(n)\\log (p) + \\CC(n)\\log(n))$ operations in $\\F_p$, where $\\M(n)$ and\n$\\CC(n)$ are bounds for the cost of polynomial multiplication and modular\npolynomial composition. Known results give $\\M(n) = O(n\\log (n) \\log\\log (n))$\nand $\\CC(n) = O(n^{1.67})$, so our algorithm is subquadratic in $n$.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 18:33:28 GMT"}], "update_date": "2011-10-20", "authors_parsed": [["Doliskani", "Javad", ""], ["Schost", "Eric", ""]]}, {"id": "1110.4414", "submitter": "Eric Price", "authors": "Eric Price and David P. Woodruff", "title": "(1+eps)-approximate Sparse Recovery", "comments": "21 pages; appeared at FOCS 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The problem central to sparse recovery and compressive sensing is that of\nstable sparse recovery: we want a distribution of matrices A in R^{m\\times n}\nsuch that, for any x \\in R^n and with probability at least 2/3 over A, there is\nan algorithm to recover x* from Ax with\n  ||x* - x||_p <= C min_{k-sparse x'} ||x - x'||_p for some constant C > 1 and\nnorm p. The measurement complexity of this problem is well understood for\nconstant C > 1. However, in a variety of applications it is important to obtain\nC = 1 + eps for a small eps > 0, and this complexity is not well understood. We\nresolve the dependence on eps in the number of measurements required of a\nk-sparse recovery algorithm, up to polylogarithmic factors for the central\ncases of p = 1 and p = 2. Namely, we give new algorithms and lower bounds that\nshow the number of measurements required is (1/eps^{p/2})k polylog(n). For p =\n2, our bound of (1/eps) k log(n/k) is tight up to constant factors. We also\ngive matching bounds when the output is required to be k-sparse, in which case\nwe achieve (1/eps^p) k polylog(n). This shows the distinction between the\ncomplexity of sparse and non-sparse outputs is fundamental.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 22:44:28 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2011 23:48:55 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Price", "Eric", ""], ["Woodruff", "David P.", ""]]}, {"id": "1110.4428", "submitter": "John Iacono", "authors": "John Iacono", "title": "Improved Upper Bounds for Pairing Heaps", "comments": "Preliminary version appeared at the Seventh Scandinavian Workshop on\n  Algorithm Theory (SWAT 2000)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairing heaps are shown to have constant amortized time Insert and Meld, thus\nshowing that pairing heaps have the same amortized runtimes as Fibonacci heaps\nfor all operations but Decrease-key.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 02:43:29 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2013 13:26:38 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2014 00:31:54 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Iacono", "John", ""]]}, {"id": "1110.4437", "submitter": "Haim Avron", "authors": "Haim Avron and Sivan Toledo", "title": "Effective Stiffness: Generalizing Effective Resistance Sampling to\n  Finite Element Matrices", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the notion of effective stiffness and show that it can used to\nbuild sparsifiers, algorithms that sparsify linear systems arising from\nfinite-element discretizations of PDEs. In particular, we show that sampling\n$O(n\\log n)$ elements according to probabilities derived from effective\nstiffnesses yields a high quality preconditioner that can be used to solve the\nlinear system in a small number of iterations. Effective stiffness generalizes\nthe notion of effective resistance, a key ingredient of recent progress in\ndeveloping nearly linear symmetric diagonally dominant (SDD) linear solvers.\nSolving finite elements problems is of considerably more interest than the\nsolution of SDD linear systems, since the finite element method is frequently\nused to numerically solve PDEs arising in scientific and engineering\napplications. Unlike SDD systems, which are relatively easy to solve, there has\nbeen limited success in designing fast solvers for finite element systems, and\nprevious algorithms usually target discretization of limited class of PDEs like\nscalar elliptic or 2D trusses. Our sparsifier is general; it applies to a wide\nrange of finite-element discretizations. A sparsifier does not constitute a\ncomplete linear solver. To construct a solver, one needs additional components\n(e.g., an efficient elimination or multilevel scheme for the sparsified\nsystem). Still, sparsifiers have been a critical tools in efficient SDD\nsolvers, and we believe that our sparsifier will become a key ingredient in\nfuture fast finite-element solvers.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 03:54:53 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2014 17:07:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Avron", "Haim", ""], ["Toledo", "Sivan", ""]]}, {"id": "1110.4477", "submitter": "Tomaso Aste", "authors": "Won-Min Song, T. Di Matteo, Tomaso Aste", "title": "Hierarchical information clustering by means of topologically embedded\n  graphs", "comments": "33 Pages, 18 Figures, 5 Tables", "journal-ref": "PLoS ONE 7 (2012) e31929", "doi": "10.1371/journal.pone.0031929", "report-no": null, "categories": "physics.data-an cs.DS physics.bio-ph q-bio.QM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a graph-theoretic approach to extract clusters and hierarchies\nin complex data-sets in an unsupervised and deterministic manner, without the\nuse of any prior information. This is achieved by building topologically\nembedded networks containing the subset of most significant links and analyzing\nthe network structure. For a planar embedding, this method provides both the\nintra-cluster hierarchy, which describes the way clusters are composed, and the\ninter-cluster hierarchy which describes how clusters gather together. We\ndiscuss performance, robustness and reliability of this method by first\ninvestigating several artificial data-sets, finding that it can outperform\nsignificantly other established approaches. Then we show that our method can\nsuccessfully differentiate meaningful clusters and hierarchies in a variety of\nreal data-sets. In particular, we find that the application to gene expression\npatterns of lymphoma samples uncovers biologically significant groups of genes\nwhich play key-roles in diagnosis, prognosis and treatment of some of the most\nrelevant human lymphoid malignancies.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 09:43:02 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Song", "Won-Min", ""], ["Di Matteo", "T.", ""], ["Aste", "Tomaso", ""]]}, {"id": "1110.4493", "submitter": "Francisco Claude", "authors": "Francisco Claude and Gonzalo Navarro", "title": "Improved Grammar-Based Compressed Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We introduce the first grammar-compressed representation of a sequence that\nsupports searches in time that depends only logarithmically on the size of the\ngrammar. Given a text $T[1..u]$ that is represented by a (context-free) grammar\nof $n$ (terminal and nonterminal) symbols and size $N$ (measured as the sum of\nthe lengths of the right hands of the rules), a basic grammar-based\nrepresentation of $T$ takes $N\\lg n$ bits of space. Our representation requires\n$2N\\lg n + N\\lg u + \\epsilon\\, n\\lg n + o(N\\lg n)$ bits of space, for any\n$0<\\epsilon \\le 1$. It can find the positions of the $occ$ occurrences of a\npattern of length $m$ in $T$ in $O((m^2/\\epsilon)\\lg (\\frac{\\lg u}{\\lg n})\n+occ\\lg n)$ time, and extract any substring of length $\\ell$ of $T$ in time\n$O(\\ell+h\\lg(N/h))$, where $h$ is the height of the grammar tree.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 11:03:07 GMT"}], "update_date": "2011-10-21", "authors_parsed": [["Claude", "Francisco", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1110.4499", "submitter": "Lowell Trott", "authors": "David Eppstein, Michael T. Goodrich, Maarten L\\\"offler, Darren Strash,\n  Lowell Trott", "title": "Category-Based Routing in Social Networks: Membership Dimension and the\n  Small-World Phenomenon (Full)", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic experiment by Milgram shows that individuals can route messages\nalong short paths in social networks, given only simple categorical information\nabout recipients (such as \"he is a prominent lawyer in Boston\" or \"she is a\nFreshman sociology major at Harvard\"). That is, these networks have very short\npaths between pairs of nodes (the so-called small-world phenomenon); moreover,\nparticipants are able to route messages along these paths even though each\nperson is only aware of a small part of the network topology. Some sociologists\nconjecture that participants in such scenarios use a greedy routing strategy in\nwhich they forward messages to acquaintances that have more categories in\ncommon with the recipient than they do, and similar strategies have recently\nbeen proposed for routing messages in dynamic ad-hoc networks of mobile\ndevices. In this paper, we introduce a network property called membership\ndimension, which characterizes the cognitive load required to maintain\nrelationships between participants and categories in a social network. We show\nthat any connected network has a system of categories that will support greedy\nrouting, but that these categories can be made to have small membership\ndimension if and only if the underlying network exhibits the small-world\nphenomenon.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 11:21:43 GMT"}], "update_date": "2011-10-21", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["L\u00f6ffler", "Maarten", ""], ["Strash", "Darren", ""], ["Trott", "Lowell", ""]]}, {"id": "1110.4573", "submitter": "Francis Lazarus", "authors": "Francis Lazarus and Julien Rivaud", "title": "On the homotopy test on surfaces", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G be a graph cellularly embedded in a surface S. Given two closed walks c\nand d in G, we take advantage of the RAM model to describe linear time\nalgorithms to decide if c and d are homotopic in S, either freely or with fixed\nbasepoint. We restrict S to be orientable for the free homotopy test, but allow\nnon-orientable surfaces when the basepoint is fixed. After O(|G|) time\npreprocessing independent of c and d, our algorithms answer the homotopy test\nin O(|c|+|d|) time, where |G|, |c| and |d| are the respective numbers of edges\nof G, c and d. As a byproduct we obtain linear time algorithms for the word\nproblem and the conjugacy problem in surface groups. We present a geometric\napproach based on previous works by Colin de Verdi\\`ere and Erickson.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 16:30:22 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2011 09:22:42 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Lazarus", "Francis", ""], ["Rivaud", "Julien", ""]]}, {"id": "1110.4604", "submitter": "Hyung-Chan An", "authors": "Hyung-Chan An, Robert Kleinberg, David B. Shmoys", "title": "Improving Christofides' Algorithm for the s-t Path TSP", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic (1+sqrt(5))/2-approximation algorithm for the s-t\npath TSP for an arbitrary metric. Given a symmetric metric cost on n vertices\nincluding two prespecified endpoints, the problem is to find a shortest\nHamiltonian path between the two endpoints; Hoogeveen showed that the natural\nvariant of Christofides' algorithm is a 5/3-approximation algorithm for this\nproblem, and this asymptotically tight bound in fact has been the best\napproximation ratio known until now. We modify this algorithm so that it\nchooses the initial spanning tree based on an optimal solution to the Held-Karp\nrelaxation rather than a minimum spanning tree; we prove this simple but\ncrucial modification leads to an improved approximation ratio, surpassing the\n20-year-old barrier set by the natural Christofides' algorithm variant. Our\nalgorithm also proves an upper bound of (1+sqrt(5))/2 on the integrality gap of\nthe path-variant Held-Karp relaxation. The techniques devised in this paper can\nbe applied to other optimization problems as well: these applications include\nimproved approximation algorithms and improved LP integrality gap upper bounds\nfor the prize-collecting s-t path problem and the unit-weight graphical metric\ns-t path TSP.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 18:50:21 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2011 01:45:34 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["An", "Hyung-Chan", ""], ["Kleinberg", "Robert", ""], ["Shmoys", "David B.", ""]]}, {"id": "1110.4623", "submitter": "Jeff A Stuart", "authors": "Jeff A. Stuart and John D. Owens", "title": "Efficient Synchronization Primitives for GPUs", "comments": "13 pages with appendix, several figures, plans to submit to CompSci\n  conference in early 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the design of synchronization\nprimitives---specifically barriers, mutexes, and semaphores---and how they\napply to the GPU. Previous implementations are insufficient due to the\ndiscrepancies in hardware and programming model of the GPU and CPU. We create\nnew implementations in CUDA and analyze the performance of spinning on the GPU,\nas well as a method of sleeping on the GPU, by running a set of memory-system\nbenchmarks on two of the most common GPUs in use, the Tesla- and Fermi-class\nGPUs from NVIDIA. From our results we define higher-level principles that are\nvalid for generic many-core processors, the most important of which is to limit\nthe number of atomic accesses required for a synchronization operation because\natomic accesses are slower than regular memory accesses. We use the results of\nthe benchmarks to critique existing synchronization algorithms and guide our\nnew implementations, and then define an abstraction of GPUs to classify any GPU\nbased on the behavior of the memory system. We use this abstraction to create\nsuitable implementations of the primitives specifically targeting the GPU, and\nanalyze the performance of these algorithms on Tesla and Fermi. We then predict\nperformance on future GPUs based on characteristics of the abstraction. We also\nexamine the roles of spin waiting and sleep waiting in each primitive and how\ntheir performance varies based on the machine abstraction, then give a set of\nguidelines for when each strategy is useful based on the characteristics of the\nGPU and expected contention.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 19:43:58 GMT"}], "update_date": "2011-10-21", "authors_parsed": [["Stuart", "Jeff A.", ""], ["Owens", "John D.", ""]]}, {"id": "1110.4765", "submitter": "D\\'aniel Marx", "authors": "D\\'aniel Marx, Barry O'Sullivan, Igor Razgon", "title": "Finding small separators in linear time via treewidth reduction", "comments": "A subset of the results was presented at STACS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for reducing the treewidth of a graph while preserving\nall of its minimal $s-t$ separators up to a certain fixed size $k$. This\ntechnique allows us to solve $s-t$ Cut and Multicut problems with various\nadditional restrictions (e.g., the vertices being removed from the graph form\nan independent set or induce a connected graph) in linear time for every fixed\nnumber $k$ of removed vertices.\n  Our results have applications for problems that are not directly defined by\nseparators, but the known solution methods depend on some variant of\nseparation. for example, we can solve similarly restricted generalizations of\nBipartization (delete at most $k$ vertices from $G$ to make it bipartite) in\nalmost linear time for every fixed number $k$ of removed vertices. These\nresults answer a number of open questions in the area of parameterized\ncomplexity. Furthermore, our technique turns out to be relevant for $(H,C,K)$-\nand $(H,C,\\le K)$-coloring problems as well, which are cardinality constrained\nvariants of the classical $H$-coloring problem. We make progress in the\nclassification of the parameterized complexity of these problems by identifying\nnew cases that can be solved in almost linear time for every fixed cardinality\nbound.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 11:46:47 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["O'Sullivan", "Barry", ""], ["Razgon", "Igor", ""]]}, {"id": "1110.4860", "submitter": "Jan Vondrak", "authors": "Jan Vondrak", "title": "Symmetry and approximability of submodular maximization problems", "comments": "The conference version of this paper appeared in IEEE FOCS 2009.\n  Unfortunately there was an error in the main theorem of the FOCS 2009 paper.\n  This long version corrects the error (see Theorem 3) and explains why the\n  error does not affect the applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent results on optimization problems involving submodular\nfunctions have made use of the multilinear relaxation of the problem. These\nresults hold typically in the value oracle model, where the objective function\nis accessible via a black box returning f(S) for a given S. We present a\ngeneral approach to deriving inapproximability results in the value oracle\nmodel, based on the notion of symmetry gap. Our main result is that for any\nfixed instance that exhibits a certain symmetry gap in its multilinear\nrelaxation, there is a naturally related class of instances for which a better\napproximation factor than the symmetry gap would require exponentially many\noracle queries. This unifies several known hardness results for submodular\nmaximization, and implies several new ones. In particular, we prove that there\nis no constant-factor approximation for the problem of maximizing a\nnon-negative submodular function over the bases of a matroid. We also provide a\nclosely matching approximation algorithm for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 18:31:38 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2013 00:15:53 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Vondrak", "Jan", ""]]}, {"id": "1110.4882", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "Laszlo A. Vegh", "title": "Strongly polynomial algorithm for a class of minimum-cost flow problems\n  with separable convex objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-studied nonlinear extension of the minimum-cost flow problem is to\nminimize the objective $\\sum_{ij\\in E} C_{ij}(f_{ij})$ over feasible flows $f$,\nwhere on every arc $ij$ of the network, $C_{ij}$ is a convex function. We give\na strongly polynomial algorithm for the case when all $C_{ij}$'s are convex\nquadratic functions, settling an open problem raised e.g. by Hochbaum [1994].\nWe also give strongly polynomial algorithms for computing market equilibria in\nFisher markets with linear utilities and with spending constraint utilities,\nthat can be formulated in this framework (see Shmyrev [2009], Devanur et al.\n[2011]). For the latter class this resolves an open question raised by Vazirani\n[2010]. The running time is $O(m^4\\log m)$ for quadratic costs,\n$O(n^4+n^2(m+n\\log n)\\log n)$ for Fisher's markets with linear utilities and\n$O(mn^3 +m^2(m+n\\log n)\\log m)$ for spending constraint utilities.\n  All these algorithms are presented in a common framework that addresses the\ngeneral problem setting. Whereas it is impossible to give a strongly polynomial\nalgorithm for the general problem even in an approximate sense (see Hochbaum\n[1994]), we show that assuming the existence of certain black-box oracles, one\ncan give an algorithm using a strongly polynomial number of arithmetic\noperations and oracle calls only. The particular algorithms can be derived by\nimplementing these oracles in the respective settings.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 19:51:29 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2011 15:27:39 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2012 15:22:19 GMT"}, {"version": "v4", "created": "Fri, 12 Apr 2013 12:17:27 GMT"}, {"version": "v5", "created": "Mon, 6 Jun 2016 10:17:21 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Vegh", "Laszlo A.", ""]]}, {"id": "1110.5190", "submitter": "Zdenek Dvorak", "authors": "Zdenek Dvorak", "title": "Constant-factor approximation of domination number in sparse graphs", "comments": "10 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-domination number of a graph is the minimum size of a set X such that\nevery vertex of G is in distance at most k from X. We give a linear time\nconstant-factor approximation algorithm for k-domination number in classes of\ngraphs with bounded expansion, which include e.g. proper minor-closed graph\nclasses, classes closed on topological minors or classes of graphs that can be\ndrawn on a fixed surface with bounded number of crossings on each edge.\n  The algorithm is based on the following approximate min-max characterization.\nA subset A of vertices of a graph G is d-independent if the distance between\neach pair of vertices in A is greater than d. Note that the size of the largest\n2k-independent set is a lower bound for the k-domination number. We show that\nevery graph from a fixed class with bounded expansion contains a 2k-independent\nset A and a k-dominating set D such that |D|=O(|A|), and these sets can be\nfound in linear time. For domination number (k=1) the assumptions can be\nrelaxed, and the result holds for all graph classes with arrangeability bounded\nby a constant.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 10:36:50 GMT"}], "update_date": "2011-10-25", "authors_parsed": [["Dvorak", "Zdenek", ""]]}, {"id": "1110.5236", "submitter": "Hjalte Wedel Vildh{\\o}j", "authors": "Philip Bille, Inge Li Goertz, Hjalte Wedel Vildh{\\o}j, S{\\o}ren Vind", "title": "String Indexing for Patterns with Wildcards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of indexing a string $t$ of length $n$ to report the\noccurrences of a query pattern $p$ containing $m$ characters and $j$ wildcards.\nLet $occ$ be the number of occurrences of $p$ in $t$, and $\\sigma$ the size of\nthe alphabet. We obtain the following results.\n  - A linear space index with query time $O(m+\\sigma^j \\log \\log n + occ)$.\nThis significantly improves the previously best known linear space index by Lam\net al. [ISAAC 2007], which requires query time $\\Theta(jn)$ in the worst case.\n  - An index with query time $O(m+j+occ)$ using space $O(\\sigma^{k^2} n \\log^k\n\\log n)$, where $k$ is the maximum number of wildcards allowed in the pattern.\nThis is the first non-trivial bound with this query time.\n  - A time-space trade-off, generalizing the index by Cole et al. [STOC 2004].\n  We also show that these indexes can be generalized to allow variable length\ngaps in the pattern. Our results are obtained using a novel combination of\nwell-known and new techniques, which could be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 13:57:08 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2012 13:35:12 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Bille", "Philip", ""], ["Goertz", "Inge Li", ""], ["Vildh\u00f8j", "Hjalte Wedel", ""], ["Vind", "S\u00f8ren", ""]]}, {"id": "1110.5296", "submitter": "M. Sohel Rahman", "authors": "Shihabur Rahman Chowdhury, Md. Mahbubul Hasan, Sumaiya Iqbal and M.\n  Sohel Rahman", "title": "Computing a Longest Common Palindromic Subsequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\em longest common subsequence (LCS)} problem is a classic and\nwell-studied problem in computer science. Palindrome is a word which reads the\nsame forward as it does backward. The {\\em longest common palindromic\nsubsequence (LCPS)} problem is an interesting variant of the classic LCS\nproblem which finds the longest common subsequence between two given strings\nsuch that the computed subsequence is also a palindrome. In this paper, we\nstudy the LCPS problem and give efficient algorithms to solve this problem. To\nthe best of our knowledge, this is the first attempt to study and solve this\ninteresting problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 18:20:43 GMT"}], "update_date": "2011-10-25", "authors_parsed": [["Chowdhury", "Shihabur Rahman", ""], ["Hasan", "Md. Mahbubul", ""], ["Iqbal", "Sumaiya", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "1110.5355", "submitter": "Jos\\'e Ignacio Alvarez-Hamelin Phd.", "authors": "Jos\\'e Ignacio Alvarez-Hamelin", "title": "Is it possible to find the maximum clique in general graphs?", "comments": "http://hal.archives-ouvertes.fr/hal-00625917/en", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the maximum clique is a known NP-Complete problem and it is also hard\nto approximate. This work proposes two efficient algorithms to obtain it.\nNevertheless, the first one is able to fins the maximum for some special cases,\nwhile the second one has its execution time bounded by the number of cliques\nthat each vertex belongs to.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 21:12:43 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2011 20:20:36 GMT"}, {"version": "v3", "created": "Fri, 17 Feb 2012 21:35:05 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Alvarez-Hamelin", "Jos\u00e9 Ignacio", ""]]}, {"id": "1110.5704", "submitter": "Alexander Sch\\\"onhuth", "authors": "Iman Hajirasouliha, Alexander Sch\\\"onhuth, David Juan, Alfonso\n  Valencia and S.Cenk Sahinalp", "title": "Mirroring co-evolving trees in the light of their topologies", "comments": "13 pages, 2 figures, Iman Hajirasouliha and Alexander Sch\\\"onhuth are\n  joint first authors", "journal-ref": "Bioinformatics, 28(9), 1202-1208, 2012", "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the interaction partners among protein/domain families poses hard\ncomputational problems, in particular in the presence of paralogous proteins.\nAvailable approaches aim to identify interaction partners among protein/domain\nfamilies through maximizing the similarity between trimmed versions of their\nphylogenetic trees. Since maximization of any natural similarity score is\ncomputationally difficult, many approaches employ heuristics to maximize the\ndistance matrices corresponding to the tree topologies in question. In this\npaper we devise an efficient deterministic algorithm which directly maximizes\nthe similarity between two leaf labeled trees with edge lengths, obtaining a\nscore-optimal alignment of the two trees in question.\n  Our algorithm is significantly faster than those methods based on distance\nmatrix comparison: 1 minute on a single processor vs. 730 hours on a\nsupercomputer. Furthermore we have advantages over the current state-of-the-art\nheuristic search approach in terms of precision as well as a recently suggested\noverall performance measure for mirrortree approaches, while incurring only\nacceptable losses in recall.\n  A C implementation of the method demonstrated in this paper is available at\nhttp://compbio.cs.sfu.ca/mirrort.htm\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2011 05:43:01 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Hajirasouliha", "Iman", ""], ["Sch\u00f6nhuth", "Alexander", ""], ["Juan", "David", ""], ["Valencia", "Alfonso", ""], ["Sahinalp", "S. Cenk", ""]]}, {"id": "1110.5753", "submitter": "Thomas Kesselheim", "authors": "Martin Hoefer and Thomas Kesselheim", "title": "Secondary Spectrum Auctions for Symmetric and Submodular Bidders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study truthful auctions for secondary spectrum usage in wireless networks.\nIn this scenario, n communication requests need to be allocated to k available\nchannels that are subject to interference and noise. We present the first\ntruthful mechanisms for secondary spectrum auctions with symmetric or\nsubmodular valuations. Our approach to model interference uses an edge-weighted\nconflict graph, and our algorithms provide asymptotically almost optimal\napproximation bounds for conflict graphs with a small inductive independence\nnumber rho << n. This approach covers a large variety of interference models\nsuch as, e.g., the protocol model or the recently popular physical model of\ninterference. For unweighted conflict graphs and symmetric valuations we use\nLP-rounding to obtain $O(\\rho)$-approximate mechanisms; for weighted conflict\ngraphs we get a factor of O(rho (log n + log k)). For submodular users we\ncombine the convex rounding framework of Dughmi et al [STOC 2011] with\nrandomized meta-rounding to obtain O(rho)-approximate mechanisms for\nmatroid-rank-sum valuations; for weighted conflict graphs we can fully drop the\ndependence on k to get O(rho log n). We conclude with promising initialresults\nfor deterministically truthful mechanisms that allow approximation factors\nbased on rho.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2011 09:58:07 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Hoefer", "Martin", ""], ["Kesselheim", "Thomas", ""]]}, {"id": "1110.5813", "submitter": "Jierui Xie", "authors": "Jierui Xie, Stephen Kelley, Boleslaw K. Szymanski", "title": "Overlapping Community Detection in Networks: the State of the Art and\n  Comparative Study", "comments": "This paper (final version) is accepted in 2012. ACM Computing\n  Surveys, vol. 45, no. 4, 2013 (In press) Contact: jierui.xie@gmail.com", "journal-ref": "ACM Computing Surveys 45(4), Article 43 (August 2013)", "doi": "10.1145/2501654.2501657", "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the state of the art in overlapping community detection\nalgorithms, quality measures, and benchmarks. A thorough comparison of\ndifferent algorithms (a total of fourteen) is provided. In addition to\ncommunity level evaluation, we propose a framework for evaluating algorithms'\nability to detect overlapping nodes, which helps to assess over-detection and\nunder-detection. After considering community level detection performance\nmeasured by Normalized Mutual Information, the Omega index, and node level\ndetection performance measured by F-score, we reached the following\nconclusions. For low overlapping density networks, SLPA, OSLOM, Game and COPRA\noffer better performance than the other tested algorithms. For networks with\nhigh overlapping density and high overlapping diversity, both SLPA and Game\nprovide relatively stable performance. However, test results also suggest that\nthe detection in such networks is still not yet fully resolved. A common\nfeature observed by various algorithms in real-world networks is the relatively\nsmall fraction of overlapping nodes (typically less than 30%), each of which\nbelongs to only 2 or 3 communities.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2011 15:12:38 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2011 03:05:03 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2011 21:27:02 GMT"}, {"version": "v4", "created": "Tue, 3 Jul 2012 19:30:23 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Xie", "Jierui", ""], ["Kelley", "Stephen", ""], ["Szymanski", "Boleslaw K.", ""]]}, {"id": "1110.5915", "submitter": "Gregory Gutin", "authors": "R. Crowston, G. Gutin, M. Jones, and A. Yeo", "title": "Parameterized Complexity of Satisfying Almost All Linear Equations over\n  $\\mathbb{F}_2$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem MaxLin2 can be stated as follows. We are given a system $S$ of\n$m$ equations in variables $x_1,...,x_n$, where each equation is $\\sum_{i \\in\nI_j}x_i = b_j$ is assigned a positive integral weight $w_j$ and $x_i,b_j \\in\n\\mathbb{F}_2$, $I_j \\subseteq \\{1,2,...,n\\}$ for $j=1,...,m$. We are required\nto find an assignment of values to the variables in order to maximize the total\nweight of the satisfied equations.\n  Let $W$ be the total weight of all equations in $S$. We consider the\nfollowing parameterized version of MaxLin2: decide whether there is an\nassignment satisfying equations of total weight at least $W-k$, where $k$ is a\nnonnegative parameter. We prove that this parameterized problem is W[1]-hard\neven if each equation of $S$ has exactly three variables and every variable\nappears in exactly three equations and, moreover, each weight $w_j$ equals 1\nand no two equations have the same left-hand side. We show the tightness of\nthis result by proving that if each equation has at most two variables then the\nparameterized problem is fixed-parameter tractable. We also prove that if no\nvariable appears in more than two equations then we can maximize the total\nweight of satisfied equations in polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2011 20:02:03 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2012 13:01:09 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Crowston", "R.", ""], ["Gutin", "G.", ""], ["Jones", "M.", ""], ["Yeo", "A.", ""]]}, {"id": "1110.6384", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Stefan Szeider", "title": "Backdoors to Acyclic SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor sets, a notion introduced by Williams et al. in 2003, are certain\nsets of key variables of a CNF formula F that make it easy to solve the\nformula; by assigning truth values to the variables in a backdoor set, the\nformula gets reduced to one or several polynomial-time solvable formulas. More\nspecifically, a weak backdoor set of F is a set X of variables such that there\nexits a truth assignment t to X that reduces F to a satisfiable formula F[t]\nthat belongs to a polynomial-time decidable base class C. A strong backdoor set\nis a set X of variables such that for all assignments t to X, the reduced\nformula F[t] belongs to C.\n  We study the problem of finding backdoor sets of size at most k with respect\nto the base class of CNF formulas with acyclic incidence graphs, taking k as\nthe parameter. We show that\n  1. the detection of weak backdoor sets is W[2]-hard in general but\nfixed-parameter tractable for r-CNF formulas, for any fixed r>=3, and\n  2. the detection of strong backdoor sets is fixed-parameter approximable.\n  Result 1 is the the first positive one for a base class that does not have a\ncharacterization with obstructions of bounded size. Result 2 is the first\npositive one for a base class for which strong backdoor sets are more powerful\nthan deletion backdoor sets.\n  Not only SAT, but also #SAT can be solved in polynomial time for CNF formulas\nwith acyclic incidence graphs. Hence Result 2 establishes a new structural\nparameter that makes #SAT fixed-parameter tractable and that is incomparable\nwith known parameters such as treewidth and clique-width.\n  We obtain the algorithms by a combination of an algorithmic version of the\nErd\\\"os-P\\'osa Theorem, Courcelle's model checking for monadic second order\nlogic, and new combinatorial results on how disjoint cycles can interact with\nthe backdoor set.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 16:10:32 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2011 15:09:42 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2012 17:15:41 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Gaspers", "Serge", ""], ["Szeider", "Stefan", ""]]}, {"id": "1110.6387", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Stefan Szeider", "title": "Backdoors to Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A backdoor set is a set of variables of a propositional formula such that\nfixing the truth values of the variables in the backdoor set moves the formula\ninto some polynomial-time decidable class. If we know a small backdoor set we\ncan reduce the question of whether the given formula is satisfiable to the same\nquestion for one or several easy formulas that belong to the tractable class\nunder consideration. In this survey we review parameterized complexity results\nfor problems that arise in the context of backdoor sets, such as the problem of\nfinding a backdoor set of size at most k, parameterized by k. We also discuss\nrecent results on backdoor sets for problems that are beyond NP.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 16:23:45 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2011 14:54:50 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Gaspers", "Serge", ""], ["Szeider", "Stefan", ""]]}, {"id": "1110.6600", "submitter": "Ren\\'e Sitters", "authors": "Rene Sitters", "title": "The generalized work function algorithm is competitive for the\n  generalized 2-server problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized 2-server problem is an online optimization problem where a\nsequence of requests has to be served at minimal cost. Requests arrive one by\none and need to be served instantly by at least one of two servers. We consider\nthe general model where the cost function of the two servers may be different.\nFormally, each server moves in its own metric space and a request consists of\none point in each metric space. It is served by moving one of the two servers\nto its request point. Requests have to be served without knowledge of the\nfuture requests. The objective is to minimize the total traveled distance. The\nspecial case where both servers move on the real line is known as the\nCNN-problem. We show that the generalized work function algorithm is constant\ncompetitive for the generalized 2-server problem.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 11:04:36 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2013 12:40:38 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Sitters", "Rene", ""]]}, {"id": "1110.6739", "submitter": "Paola Bonizzoni", "authors": "Paola Bonizzoni and Chiara Braghin and Riccardo Dondi and Gabriella\n  Trucco", "title": "The Binary Perfect Phylogeny with Persistent characters", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary perfect phylogeny model is too restrictive to model biological\nevents such as back mutations. In this paper we consider a natural\ngeneralization of the model that allows a special type of back mutation. We\ninvestigate the problem of reconstructing a near perfect phylogeny over a\nbinary set of characters where characters are persistent: characters can be\ngained and lost at most once. Based on this notion, we define the problem of\nthe Persistent Perfect Phylogeny (referred as P-PP). We restate the P-PP\nproblem as a special case of the Incomplete Directed Perfect Phylogeny, called\nIncomplete Perfect Phylogeny with Persistent Completion, (refereed as IP-PP),\nwhere the instance is an incomplete binary matrix M having some missing\nentries, denoted by symbol ?, that must be determined (or completed) as 0 or 1\nso that M admits a binary perfect phylogeny. We show that the IP-PP problem can\nbe reduced to a problem over an edge colored graph since the completion of each\ncolumn of the input matrix can be represented by a graph operation. Based on\nthis graph formulation, we develop an exact algorithm for solving the P-PP\nproblem that is exponential in the number of characters and polynomial in the\nnumber of species.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 10:17:52 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2012 14:08:44 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Braghin", "Chiara", ""], ["Dondi", "Riccardo", ""], ["Trucco", "Gabriella", ""]]}, {"id": "1110.6832", "submitter": "Sreeram Kannan", "authors": "Chandra Chekuri, Sreeram Kannan, Adnan Raja, and Pramod Viswanath", "title": "Multicommodity Flows and Cuts in Polymatroidal Networks", "comments": "An extended abstract will appear in Proceedings of the Innovations in\n  Theoretical Computer Science Conference (ITCS), January 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multicommodity flow and cut problems in {\\em polymatroidal}\nnetworks where there are submodular capacity constraints on the edges incident\nto a node. Polymatroidal networks were introduced by Lawler and Martel and\nHassin in the single-commodity setting and are closely related to the\nsubmodular flow model of Edmonds and Giles; the well-known maxflow-mincut\ntheorem holds in this more general setting. Polymatroidal networks for the\nmulticommodity case have not, as far as the authors are aware, been previously\nexplored. Our work is primarily motivated by applications to information flow\nin wireless networks. We also consider the notion of undirected polymatroidal\nnetworks and observe that they provide a natural way to generalize flows and\ncuts in edge and node capacitated undirected networks.\n  We establish poly-logarithmic flow-cut gap results in several scenarios that\nhave been previously considered in the standard network flow models where\ncapacities are on the edges or nodes. Our results have already found\naplications in wireless network information flow and we anticipate more in the\nfuture. On the technical side our key tools are the formulation and analysis of\nthe dual of the flow relaxations via continuous extensions of submodular\nfunctions, in particular the Lov\\'asz extension. For directed graphs we rely on\na simple yet useful reduction from polymatroidal networks to standard networks.\nFor undirected graphs we rely on the interplay between the Lov\\'asz extension\nof a submodular function and line embeddings with low average distortion\nintroduced by Matousek and Rabinovich; this connection is inspired by, and\ngeneralizes, the work of Feige, Hajiaghayi and Lee on node-capacitated\nmulticommodity flows and cuts. The applicability of embeddings to polymatroidal\nnetworks is of independent mathematical interest.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 15:30:19 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Chekuri", "Chandra", ""], ["Kannan", "Sreeram", ""], ["Raja", "Adnan", ""], ["Viswanath", "Pramod", ""]]}]