[{"id": "1412.0036", "submitter": "Aleksandar Nikolov", "authors": "Aleksandar Nikolov", "title": "Randomized Rounding for the Largest Simplex Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum volume $j$-simplex problem asks to compute the $j$-dimensional\nsimplex of maximum volume inside the convex hull of a given set of $n$ points\nin $\\mathbb{Q}^d$. We give a deterministic approximation algorithm for this\nproblem which achieves an approximation ratio of $e^{j/2 + o(j)}$. The problem\nis known to be $\\mathrm{NP}$-hard to approximate within a factor of $c^{j}$ for\nsome constant $c > 1$. Our algorithm also gives a factor $e^{j + o(j)}$\napproximation for the problem of finding the principal $j\\times j$ submatrix of\na rank $d$ positive semidefinite matrix with the largest determinant. We\nachieve our approximation by rounding solutions to a generalization of the\n$D$-optimal design problem, or, equivalently, the dual of an appropriate\nsmallest enclosing ellipsoid problem. Our arguments give a short and simple\nproof of a restricted invertibility principle for determinants.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 21:51:40 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 04:03:22 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Nikolov", "Aleksandar", ""]]}, {"id": "1412.0073", "submitter": "Jingcheng Liu", "authors": "Jingcheng Liu, Pinyan Lu", "title": "FPTAS for #BIS with Degree Bounds on One Side", "comments": "15 pages, to appear in STOC 2015; Improved presentations from\n  previous version;", "journal-ref": null, "doi": "10.1145/2746539.2746598", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the number of independent sets for a bipartite graph (#BIS) plays a\ncrucial role in the study of approximate counting. It has been conjectured that\nthere is no fully polynomial-time (randomized) approximation scheme\n(FPTAS/FPRAS) for #BIS, and it was proved that the problem for instances with a\nmaximum degree of $6$ is already as hard as the general problem. In this paper,\nwe obtain a surprising tractability result for a family of #BIS instances. We\ndesign a very simple deterministic fully polynomial-time approximation scheme\n(FPTAS) for #BIS when the maximum degree for one side is no larger than $5$.\nThere is no restriction for the degrees on the other side, which do not even\nhave to be bounded by a constant. Previously, FPTAS was only known for\ninstances with a maximum degree of $5$ for both sides.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 05:51:23 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 08:11:46 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Liu", "Jingcheng", ""], ["Lu", "Pinyan", ""]]}, {"id": "1412.0193", "submitter": "Sebastian Wild", "authors": "Sebastian Wild, Markus E. Nebel, Conrado Mart\\'inez", "title": "Analysis of Pivot Sampling in Dual-Pivot Quicksort", "comments": "This article is identical (up to typograhical details) to the\n  Algorithmica version available from Springerlink (see DOI). It is an extended\n  and improved version of our corresponding article at the AofA 2014 conference\n  [arXiv:1403.6602]", "journal-ref": null, "doi": "10.1007/s00453-015-0041-7", "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new dual-pivot Quicksort by Vladimir Yaroslavskiy - used in Oracle's Java\nruntime library since version 7 - features intriguing asymmetries. They make a\nbasic variant of this algorithm use less comparisons than classic single-pivot\nQuicksort. In this paper, we extend the analysis to the case where the two\npivots are chosen as fixed order statistics of a random sample. Surprisingly,\ndual-pivot Quicksort then needs more comparisons than a corresponding version\nof classic Quicksort, so it is clear that counting comparisons is not\nsufficient to explain the running time advantages observed for Yaroslavskiy's\nalgorithm in practice. Consequently, we take a more holistic approach and give\nalso the precise leading term of the average number of swaps, the number of\nexecuted Java Bytecode instructions and the number of scanned elements, a new\nsimple cost measure that approximates I/O costs in the memory hierarchy. We\ndetermine optimal order statistics for each of the cost measures. It turns out\nthat the asymmetries in Yaroslavskiy's algorithm render pivots with a\nsystematic skew more efficient than the symmetric choice. Moreover, we finally\nhave a convincing explanation for the success of Yaroslavskiy's algorithm in\npractice: Compared with corresponding versions of classic single-pivot\nQuicksort, dual-pivot Quicksort needs significantly less I/Os, both with and\nwithout pivot sampling.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 07:49:11 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 07:03:08 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Wild", "Sebastian", ""], ["Nebel", "Markus E.", ""], ["Mart\u00ednez", "Conrado", ""]]}, {"id": "1412.0271", "submitter": "Agnes Cseh", "authors": "\\'Agnes Cseh and David F. Manlove", "title": "Stable marriage and roommates problems with restricted edges: complexity\n  and approximability", "comments": "conference version appeared at SAGT 2015", "journal-ref": "Discrete Optimization, 20:62-89, 2016", "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stable marriage and roommates problems, a set of agents is given, each\nof them having a strictly ordered preference list over some or all of the other\nagents. A matching is a set of disjoint pairs of mutually accepted agents. If\nany two agents mutually prefer each other to their partner, then they block the\nmatching, otherwise, the matching is said to be stable. In this paper we\ninvestigate the complexity of finding a solution satisfying additional\nconstraints on restricted pairs of agents. Restricted pairs can be either\nforced or forbidden. A stable solution must contain all of the forced pairs,\nwhile it must contain none of the forbidden pairs.\n  Dias et al. gave a polynomial-time algorithm to decide whether such a\nsolution exists in the presence of restricted edges. If the answer is no, one\nmight look for a solution close to optimal. Since optimality in this context\nmeans that the matching is stable and satisfies all constraints on restricted\npairs, there are two ways of relaxing the constraints by permitting a solution\nto: (1) be blocked by some pairs (as few as possible), or (2) violate some\nconstraints on restricted pairs (again as few as possible).\n  Our main theorems prove that for the (bipartite) stable marriage problem,\ncase (1) leads to NP-hardness and inapproximability results, whilst case (2)\ncan be solved in polynomial time. For the non-bipartite stable roommates\ninstances, case (2) yields an NP-hard problem. In the case of NP-hard problems,\nwe also discuss polynomially solvable special cases, arising from restrictions\non the lengths of the preference lists, or upper bounds on the numbers of\nrestricted pairs.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 19:53:33 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 17:52:09 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2015 17:49:37 GMT"}, {"version": "v4", "created": "Wed, 23 Mar 2016 12:57:26 GMT"}, {"version": "v5", "created": "Sun, 17 Apr 2016 23:56:26 GMT"}, {"version": "v6", "created": "Tue, 31 May 2016 14:53:58 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Cseh", "\u00c1gnes", ""], ["Manlove", "David F.", ""]]}, {"id": "1412.0321", "submitter": "Jiajun Liu", "authors": "Jiajun Liu, Kun Zhao, Philipp Sommer, Shuo Shang, Brano Kusy, Raja\n  Jurdak", "title": "Bounded Quadrant System: Error-bounded Trajectory Compression on the Go", "comments": "International Conference on Data Engineering (ICDE) 2015, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term location tracking, where trajectory compression is commonly used,\nhas gained high interest for many applications in transport, ecology, and\nwearable computing. However, state-of-the-art compression methods involve high\nspace-time complexity or achieve unsatisfactory compression rate, leading to\nrapid exhaustion of memory, computation, storage and energy resources. We\npropose a novel online algorithm for error-bounded trajectory compression\ncalled the Bounded Quadrant System (BQS), which compresses trajectories with\nextremely small costs in space and time using convex-hulls. In this algorithm,\nwe build a virtual coordinate system centered at a start point, and establish a\nrectangular bounding box as well as two bounding lines in each of its\nquadrants. In each quadrant, the points to be assessed are bounded by the\nconvex-hull formed by the box and lines. Various compression error-bounds are\ntherefore derived to quickly draw compression decisions without expensive error\ncomputations. In addition, we also propose a light version of the BQS version\nthat achieves $\\mathcal{O}(1)$ complexity in both time and space for processing\neach point to suit the most constrained computation environments. Furthermore,\nwe briefly demonstrate how this algorithm can be naturally extended to the 3-D\ncase.\n  Using empirical GPS traces from flying foxes, cars and simulation, we\ndemonstrate the effectiveness of our algorithm in significantly reducing the\ntime and space complexity of trajectory compression, while greatly improving\nthe compression rates of the state-of-the-art algorithms (up to 47%). We then\nshow that with this algorithm, the operational time of the target\nresource-constrained hardware platform can be prolonged by up to 41%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 01:14:42 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 14:49:37 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Liu", "Jiajun", ""], ["Zhao", "Kun", ""], ["Sommer", "Philipp", ""], ["Shang", "Shuo", ""], ["Kusy", "Brano", ""], ["Jurdak", "Raja", ""]]}, {"id": "1412.0325", "submitter": "Agnes Cseh", "authors": "Ashwin Arulselvan, \\'Agnes Cseh, Martin Gro{\\ss}, David F. Manlove,\n  Jannik Matuschke", "title": "Matchings with lower quotas: Algorithms and complexity", "comments": "preliminary version has appeared at ISAAC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a natural generalization of the maximum weight many-to-one matching\nproblem. We are given an undirected bipartite graph $G= (A \\cup P, E)$ with\nweights on the edges in $E$, and with lower and upper quotas on the vertices in\n$P$. We seek a maximum weight many-to-one matching satisfying two sets of\nconstraints: vertices in $A$ are incident to at most one matching edge, while\nvertices in $P$ are either unmatched or they are incident to a number of\nmatching edges between their lower and upper quota. This problem, which we call\nmaximum weight many-to-one matching with lower and upper quotas (WMLQ), has\napplications to the assignment of students to projects within university\ncourses, where there are constraints on the minimum and maximum numbers of\nstudents that must be assigned to each project.\n  In this paper, we provide a comprehensive analysis of the complexity of WMLQ\nfrom the viewpoints of classic polynomial time algorithms, fixed-parameter\ntractability, as well as approximability. We draw the line between NP-hard and\npolynomially tractable instances in terms of degree and quota constraints and\nprovide efficient algorithms to solve the tractable ones. We further show that\nthe problem can be solved in polynomial time for instances with bounded\ntreewidth; however, the corresponding runtime is exponential in the treewidth\nwith the maximum upper quota $u_{max}$ as basis, and we prove that this\ndependence is necessary unless FPT = W[1]. The approximability of WMLQ is also\ndiscussed: we present an approximation algorithm for the general case with\nperformance guarantee $u_{\\max}+1$, which is asymptotically best possible\nunless P = NP. Finally, we elaborate on how most of our positive results carry\nover to matchings in arbitrary graphs with lower quotas.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 01:36:23 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 13:26:16 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2015 22:43:05 GMT"}, {"version": "v4", "created": "Sun, 20 Sep 2015 19:04:19 GMT"}, {"version": "v5", "created": "Sun, 27 Mar 2016 23:15:52 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Arulselvan", "Ashwin", ""], ["Cseh", "\u00c1gnes", ""], ["Gro\u00df", "Martin", ""], ["Manlove", "David F.", ""], ["Matuschke", "Jannik", ""]]}, {"id": "1412.0340", "submitter": "Yi-Kai Wang", "authors": "Yi-Kai Wang", "title": "Approximate MAP Estimation for Pairwise Potentials via Baker's Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical models providing mathematical abstractions for several\nsignificant optimization problems in machine learning, combinatorial\noptimization, computer vision and statistical physics have intrinsic\nsimilarities. We propose a unified framework to model these computation tasks\nwhere the structures of these optimization problems are encoded by functions\nattached on the vertices and edges of a graph. We show that computing MAX 2-CSP\nadmits polynomial-time approximation scheme (PTAS) on planar graphs, graphs\nwith bounded local treewidth, $H$-minor-free graphs, geometric graphs with\nbounded density and graphs embeddable with bounded number of crossings per\nedge. This implies computing MAX-CUT, MAX-DICUT and MAX $k$-CUT admits PTASs on\nall these classes of graphs. Our method also gives the first PTAS for computing\nthe ground state of ferromagnetic Edwards-Anderson model without external\nmagnetic field on $d$-dimensional lattice graphs. These results are widely\napplicable in vision, graphics and machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 04:00:23 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 15:44:00 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2015 17:32:38 GMT"}, {"version": "v4", "created": "Tue, 17 Apr 2018 15:49:01 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Wang", "Yi-Kai", ""]]}, {"id": "1412.0348", "submitter": "Arturs Backurs", "authors": "Arturs Backurs, Piotr Indyk", "title": "Edit Distance Cannot Be Computed in Strongly Subquadratic Time (unless\n  SETH is false)", "comments": "STOC'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance (a.k.a. the Levenshtein distance) between two strings is\ndefined as the minimum number of insertions, deletions or substitutions of\nsymbols needed to transform one string into another. The problem of computing\nthe edit distance between two strings is a classical computational task, with a\nwell-known algorithm based on dynamic programming. Unfortunately, all known\nalgorithms for this problem run in nearly quadratic time.\n  In this paper we provide evidence that the near-quadratic running time bounds\nknown for the problem of computing edit distance might be tight. Specifically,\nwe show that, if the edit distance can be computed in time $O(n^{2-\\delta})$\nfor some constant $\\delta>0$, then the satisfiability of conjunctive normal\nform formulas with $N$ variables and $M$ clauses can be solved in time\n$M^{O(1)} 2^{(1-\\epsilon)N}$ for a constant $\\epsilon>0$. The latter result\nwould violate the Strong Exponential Time Hypothesis, which postulates that\nsuch algorithms do not exist.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 04:57:06 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 21:13:21 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 17:11:08 GMT"}, {"version": "v4", "created": "Tue, 15 Aug 2017 18:01:17 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""]]}, {"id": "1412.0538", "submitter": "Elmar Langetepe", "authors": "Elmar Langetepe, Andreas Lenerz and Bernd Br\\\"uggemann", "title": "Strategic deployment in graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conquerors of old (like, e.g., Alexander the Great or Ceasar) had to solve\nthe following deployment problem. Sufficiently strong units had to be stationed\nat locations of strategic importance, and the moving forces had to be strong\nenough to advance to the next location. To the best of our knowledge we are the\nfirst to consider the (off-line) graph version of this problem. While being\nNP-hard for general graphs, for trees the minimum number of agents and an\noptimal deployment can be computed in optimal polynomial time. Moreover, the\noptimal solution for the minimum spanning tree of an arbitrary graph G results\nin a 2-approximation of the optimal solution for G.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 16:42:34 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Langetepe", "Elmar", ""], ["Lenerz", "Andreas", ""], ["Br\u00fcggemann", "Bernd", ""]]}, {"id": "1412.0588", "submitter": "Michael Cohen", "authors": "Michael B. Cohen, Richard Peng", "title": "$\\ell_p$ Row Sampling by Lewis Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple algorithm to efficiently sample the rows of a matrix while\npreserving the p-norms of its product with vectors. Given an $n$-by-$d$ matrix\n$\\boldsymbol{\\mathit{A}}$, we find with high probability and in input sparsity\ntime an $\\boldsymbol{\\mathit{A}}'$ consisting of about $d \\log{d}$ rescaled\nrows of $\\boldsymbol{\\mathit{A}}$ such that $\\| \\boldsymbol{\\mathit{A}}\n\\boldsymbol{\\mathit{x}} \\|_1$ is close to $\\| \\boldsymbol{\\mathit{A}}'\n\\boldsymbol{\\mathit{x}} \\|_1$ for all vectors $\\boldsymbol{\\mathit{x}}$. We\nalso show similar results for all $\\ell_p$ that give nearly optimal sample\nbounds in input sparsity time. Our results are based on sampling by \"Lewis\nweights\", which can be viewed as statistical leverage scores of a reweighted\nmatrix. We also give an elementary proof of the guarantees of this sampling\nprocess for $\\ell_1$.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 18:50:09 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Cohen", "Michael B.", ""], ["Peng", "Richard", ""]]}, {"id": "1412.0639", "submitter": "David J. Rosenbaum", "authors": "David J. Rosenbaum", "title": "Beating the Generator-Enumeration Bound for Solvable-Group Isomorphism", "comments": "22 pages. This is an updated and improved version of the results for\n  solvable groups in arXiv:1205.0642", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the isomorphism problem for groups specified by their\nmultiplication tables. Until recently, the best published bound for the\nworst-case was achieved by the n^(log_p n + O(1)) generator-enumeration\nalgorithm. In previous work with Fabian Wagner, we showed an n^((1 / 2) log_p n\n+ O(log n / log log n)) time algorithm for testing isomorphism of p-groups by\nbuilding graphs with degree bounded by p + O(1) that represent composition\nseries for the groups and applying Luks' algorithm for testing isomorphism of\nbounded degree graphs.\n  In this work, we extend this improvement to the more general class of\nsolvable groups to obtain an n^((1 / 2) log_p n + O(log n / log log n)) time\nalgorithm. In the case of solvable groups, the composition factors can be large\nwhich prevents previous methods from outperforming the generator-enumeration\nalgorithm. Using Hall's theory of Sylow bases, we define a new object that\ngeneralizes the notion of a composition series with small factors but exists\neven when the composition factors are large. By constructing graphs that\nrepresent these objects and running Luks' algorithm, we obtain our algorithm\nfor solvable-group isomorphism. We also extend our algorithm to compute\ncanonical forms of solvable groups while retaining the same complexity.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 20:45:25 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Rosenbaum", "David J.", ""]]}, {"id": "1412.0640", "submitter": "Jaime Marian", "authors": "Tuan L. Hoang, Jaime Marian, Vasily V. Bulatov, Peter Hosemann", "title": "Computationally-efficient stochastic cluster dynamics method for\n  modeling damage accumulation in irradiated materials", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2015.07.061", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An improved version of a recently developed stochastic cluster dynamics (SCD)\nmethod {[}Marian, J. and Bulatov, V. V., {\\it J. Nucl. Mater.} \\textbf{415}\n(2014) 84-95{]} is introduced as an alternative to rate theory (RT) methods for\nsolving coupled ordinary differential equation (ODE) systems for irradiation\ndamage simulations. SCD circumvents by design the curse of dimensionality of\nthe variable space that renders traditional ODE-based RT approaches inefficient\nwhen handling complex defect population comprised of multiple (more than two)\ndefect species. Several improvements introduced here enable efficient and\naccurate simulations of irradiated materials up to realistic (high) damage\ndoses characteristic of next-generation nuclear systems. The first improvement\nis a procedure for efficiently updating the defect reaction-network and event\nselection in the context of a dynamically expanding reaction-network. Next is a\nnovel implementation of the $\\tau$-leaping method that speeds up SCD\nsimulations by advancing the state of the reaction network in large time\nincrements when appropriate. Lastly, a volume rescaling procedure is introduced\nto control the computational complexity of the expanding reaction-network\nthrough occasional reductions of the defect population while maintaining\naccurate statistics. The enhanced SCD method is then applied to model defect\ncluster accumulation in iron thin films subjected to triple ion-beam\n($\\text{Fe}^{3+}$, $\\text{He}^{+}$ and $ $$\\text{H\\ensuremath{{}^{+}}}$$ $)\nirradiations, for which standard RT or spatially-resolved kinetic Monte Carlo\nsimulations are prohibitively expensive.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 20:45:58 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Hoang", "Tuan L.", ""], ["Marian", "Jaime", ""], ["Bulatov", "Vasily V.", ""], ["Hosemann", "Peter", ""]]}, {"id": "1412.0652", "submitter": "Sanjeev Saxena", "authors": "Sanjeev Saxena", "title": "Still Simpler Way of Introducing Interior-Point method for Linear\n  Programming", "comments": "added section on initial solution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Programming is now included in Algorithm undergraduate and\npostgraduate courses for Computer Science majors. It is possible to teach\ninterior-point methods directly with just minimal knowledge of Algebra and\nMatrices.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 11:35:33 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 08:21:05 GMT"}, {"version": "v3", "created": "Wed, 14 Jan 2015 12:26:18 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Saxena", "Sanjeev", ""]]}, {"id": "1412.0681", "submitter": "Grigory Yaroslavtsev", "authors": "Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, Grigory\n  Yaroslavtsev", "title": "Near Optimal LP Rounding Algorithm for Correlation Clustering on\n  Complete and Complete k-partite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new rounding schemes for the standard linear programming relaxation\nof the correlation clustering problem, achieving approximation factors almost\nmatching the integrality gaps:\n  - For complete graphs our appoximation is $2.06 - \\varepsilon$ for a fixed\nconstant $\\varepsilon$, which almost matches the previously known integrality\ngap of $2$.\n  - For complete $k$-partite graphs our approximation is $3$. We also show a\nmatching integrality gap.\n  - For complete graphs with edge weights satisfying triangle inequalities and\nprobability constraints, our approximation is $1.5$, and we show an integrality\ngap of $1.2$.\n  Our results improve a long line of work on approximation algorithms for\ncorrelation clustering in complete graphs, previously culminating in a ratio of\n$2.5$ for the complete case by Ailon, Charikar and Newman (JACM'08). In the\nweighted complete case satisfying triangle inequalities and probability\nconstraints, the same authors give a $2$-approximation; for the bipartite case,\nAilon, Avigdor-Elgrabli, Liberty and van Zuylen give a $4$-approximation\n(SICOMP'12).\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 21:11:40 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 16:39:16 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 21:29:49 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Chawla", "Shuchi", ""], ["Makarychev", "Konstantin", ""], ["Schramm", "Tselil", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1412.0864", "submitter": "Yinglei Song", "authors": "Yinglei Song", "title": "On the Induced Matching Problem in Hamiltonian Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the parameterized complexity and inapproximability of\nthe {\\sc Induced Matching} problem in hamiltonian bipartite graphs. We show\nthat, given a hamiltonian cycle in a hamiltonian bipartite graph, the problem\nis W[1]-hard and cannot be solved in time $n^{o(k^{\\frac{1}{2}})}$ unless\nW[1]=FPT, where $n$ is the number of vertices in the graph. In addition, we\nshow that unless NP=P, the maximum induced matching in a hamiltonian graph\ncannot be approximated within a ratio of $n^{1-\\epsilon}$, where $n$ is the\nnumber of vertices in the graph. For a bipartite hamiltonian graph in $n$\nvertices, it is NP-hard to approximate its maximum induced matching based on a\nhamiltonian cycle of the graph within a ratio of $n^{\\frac{1}{4}-\\epsilon}$,\nwhere $n$ is the number of vertices in the graph and $\\epsilon$ is any positive\nconstant.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 11:17:54 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 02:35:02 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Song", "Yinglei", ""]]}, {"id": "1412.0967", "submitter": "Travis Gagie", "authors": "Djamal Belazzougui, Travis Gagie, Pawe{\\l} Gawrychowski, Juha\n  K\\\"arkk\\\"ainen, Alberto Ord\\'o\\~nez, Simon J. Puglisi, and Yasuo Tabei", "title": "Queries on LZ-Bounded Encodings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a data structure that stores a string $S$ in space similar to\nthat of its Lempel-Ziv encoding and efficiently supports access, rank and\nselect queries. These queries are fundamental for implementing succinct and\ncompressed data structures, such as compressed trees and graphs. We show that\nour data structure can be built in a scalable manner and is both small and fast\nin practice compared to other data structures supporting such queries.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 16:37:37 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Gagie", "Travis", ""], ["Gawrychowski", "Pawe\u0142", ""], ["K\u00e4rkk\u00e4inen", "Juha", ""], ["Ord\u00f3\u00f1ez", "Alberto", ""], ["Puglisi", "Simon J.", ""], ["Tabei", "Yasuo", ""]]}, {"id": "1412.1001", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Zhenyu Liao, Yang Yuan", "title": "Optimization Algorithms for Faster Computational Geometry", "comments": "An abstract of this paper is going to appear in the conference\n  proceedings of ICALP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two fundamental problems in computational geometry: finding the\nmaximum inscribed ball (MaxIB) inside a bounded polyhedron defined by $m$\nhyperplanes, and the minimum enclosing ball (MinEB) of a set of $n$ points,\nboth in $d$-dimensional space. We improve the running time of iterative\nalgorithms on\n  MaxIB from $\\tilde{O}(m d \\alpha^3 / \\varepsilon^3)$ to $\\tilde{O}(md + m\n\\sqrt{d} \\alpha / \\varepsilon)$, a speed-up up to $\\tilde{O}(\\sqrt{d} \\alpha^2\n/ \\varepsilon^2)$, and\n  MinEB from $\\tilde{O}(n d / \\sqrt{\\varepsilon})$ to $\\tilde{O}(nd + n\n\\sqrt{d} / \\sqrt{\\varepsilon})$, a speed-up up to $\\tilde{O}(\\sqrt{d})$.\n  Our improvements are based on a novel saddle-point optimization framework. We\npropose a new algorithm $\\mathtt{L1L2SPSolver}$ for solving a class of\nregularized saddle-point problems, and apply a randomized Hadamard space\nrotation which is a technique borrowed from compressive sensing. Interestingly,\nthe motivation of using Hadamard rotation solely comes from our optimization\nview but not the original geometry problem: indeed, it is not immediately clear\nwhy MaxIB or MinEB, as a geometric problem, should be easier to solve if we\nrotate the space by a unitary matrix. We hope that our optimization perspective\nsheds lights on solving other geometric problems as well.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 18:15:46 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 04:28:03 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 20:43:49 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Liao", "Zhenyu", ""], ["Yuan", "Yang", ""]]}, {"id": "1412.1130", "submitter": "Will Rosenbaum", "authors": "Rafail Ostrovsky, Will Rosenbaum", "title": "It's Not Easy Being Three: The Approximability of Three-Dimensional\n  Stable Matching Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1976, Knuth asked if the stable marriage problem (SMP) can be generalized\nto marriages consisting of 3 genders. In 1988, Alkan showed that the natural\ngeneralization of SMP to 3 genders ($3$GSM) need not admit a stable marriage.\nThree years later, Ng and Hirschberg proved that it is NP-complete to determine\nif given preferences admit a stable marriage. They further prove an analogous\nresult for the $3$ person stable assignment ($3$PSA) problem.\n  In light of Ng and Hirschberg's NP-hardness result for $3$GSM and $3$PSA, we\ninitiate the study of approximate versions of these problems. In particular, we\ndescribe two optimization variants of $3$GSM and $3$PSA: maximally stable\nmarriage/matching (MSM) and maximum stable submarriage/submatching (MSS). We\nshow that both variants are NP-hard to approximate within some fixed constant\nfactor. Conversely, we describe a simple polynomial time algorithm which\ncomputes constant factor approximations for the maximally stable marriage and\nmatching problems. Thus both variants of MSM are APX-complete.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 23:14:41 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Ostrovsky", "Rafail", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1412.1143", "submitter": "Shayan Oveis Gharan", "authors": "Nima Anari and Shayan Oveis Gharan", "title": "The Kadison-Singer Problem for Strongly Rayleigh Measures and\n  Applications to Asymmetric TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marcus, Spielman, and Srivastava in their seminal work \\cite{MSS13} resolved\nthe Kadison-Singer conjecture by proving that for any set of finitely supported\nindependently distributed random vectors $v_1,\\dots, v_n$ which have \"small\"\nexpected squared norm and are in isotropic position (in expectation), there is\na positive probability that the sum $\\sum v_i v_i^\\intercal$ has small spectral\nnorm. Their proof crucially employs real stability of polynomials which is the\nnatural generalization of real-rootedness to multivariate polynomials.\n  Strongly Rayleigh distributions are families of probability distributions\nwhose generating polynomials are real stable \\cite{BBL09}. As independent\ndistributions are just special cases of strongly Rayleigh measures, it is a\nnatural question to see if the main theorem of \\cite{MSS13} can be extended to\nfamilies of vectors assigned to the elements of a strongly Rayleigh\ndistribution.\n  In this paper we answer this question affirmatively; we show that for any\nhomogeneous strongly Rayleigh distribution where the marginal probabilities are\nupper bounded by $\\epsilon_1$ and any isotropic set of vectors assigned to the\nunderlying elements whose norms are at most $\\sqrt{\\epsilon_2}$, there is a set\nin the support of the distribution such that the spectral norm of the sum of\nthe natural quadratic forms of the vectors assigned to the elements of the set\nis at most $O(\\epsilon_1+\\epsilon_2)$. We employ our theorem to provide a\nsufficient condition for the existence of spectrally thin trees. This, together\nwith a recent work of the authors \\cite{AO14}, provides an improved upper bound\non the integrality gap of the natural LP relaxation of the Asymmetric Traveling\nSalesman Problem.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 00:50:46 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 18:15:39 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Anari", "Nima", ""], ["Gharan", "Shayan Oveis", ""]]}, {"id": "1412.1145", "submitter": "Victor Pan", "authors": "Victor Y. Pan", "title": "Matrix Multiplication, Trilinear Decompositions, APA Algorithms, and\n  Summation", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplication (hereafter we use the acronym MM) is among the most\nfundamental operations of modern computations. The efficiency of its\nperformance depends on various factors, in particular vectorization, data\nmovement and arithmetic complexity of the computations, but here we focus just\non the study of the arithmetic cost and the impact of this study on other areas\nof modern computing. In the early 1970s it was expected that the\nstraightforward cubic time algorithm for MM will soon be accelerated to enable\nMM in nearly quadratic arithmetic time, with some far fetched implications.\nWhile pursuing this goal the mainstream research had its focus on the decrease\nof the classical exponent 3 of the complexity of MM towards its lower bound 2,\ndisregarding the growth of the input size required to support this decrease.\nEventually, surprising combinations of novel ideas and sophisticated techniques\nenabled the decrease of the exponent to its benchmark value of about 2.38, but\nthe supporting MM algorithms improved the straightforward one only for the\ninputs of immense sizes. Meanwhile, the communication complexity, rather than\nthe arithmetic complexity, has become the bottleneck of computations in linear\nalgebra. This development may seem to undermine the value of the past and\nfuture research aimed at the decrease of the arithmetic cost of MM, but we feel\nthat the study should be reassessed rather than closed and forgotten. We review\nthe old and new work in this area in the present day context, recall some major\ntechniques introduced in the study of MM, discuss their impact on the modern\ntheory and practice of computations for MM and beyond MM, and link one of these\ntechniques to some simple algorithms for inner product and summation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 01:00:19 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 14:07:37 GMT"}, {"version": "v3", "created": "Thu, 5 Feb 2015 22:26:34 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Pan", "Victor Y.", ""]]}, {"id": "1412.1241", "submitter": "May Szedl\\'ak", "authors": "Komei Fukuda, Bernd G\\\"artner, May Szedl\\'ak", "title": "Combinatorial Redundancy Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting and removing redundant constraints is fundamental in\noptimization. We focus on the case of linear programs (LPs) in dictionary form,\ngiven by $n$ equality constraints in $n+d$ variables, where the variables are\nconstrained to be nonnegative. A variable $x_r$ is called redundant, if after\nremoving $x_r \\geq 0$ the LP still has the same feasible region. The time\nneeded to solve such an LP is denoted by $LP(n,d)$.\n  It is easy to see that solving $n+d$ LPs of the above size is sufficient to\ndetect all redundancies. The currently fastest practical method is the one by\nClarkson: it solves $n+d$ linear programs, but each of them has at most $s$\nvariables, where $s$ is the number of nonredundant constraints.\n  In the first part we show that knowing all of the finitely many dictionaries\nof the LP is sufficient for the purpose of redundancy detection. A dictionary\nis a matrix that can be thought of as an enriched encoding of a vertex in the\nLP. Moreover - and this is the combinatorial aspect - it is enough to know only\nthe signs of the entries, the actual values do not matter. Concretely we show\nthat for any variable $x_r$ one can find a dictionary, such that its sign\npattern is either a redundancy or nonredundancy certificate for $x_r$.\n  In the second part we show that considering only the sign patterns of the\ndictionary, there is an output sensitive algorithm of running time\n$\\mathcal{O}(d \\cdot (n+d) \\cdot s^{d-1} \\cdot LP(s,d) + d \\cdot s^{d} \\cdot\nLP(n,d))$ to detect all redundancies. In the case where all constraints are in\ngeneral position, the running time is $\\mathcal{O}(s \\cdot LP(n,d) + (n+d)\n\\cdot LP(s,d))$, which is essentially the running time of the Clarkson method.\nOur algorithm extends naturally to a more general setting of arrangements of\noriented topological hyperplane arrangements.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 09:09:32 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Fukuda", "Komei", ""], ["G\u00e4rtner", "Bernd", ""], ["Szedl\u00e1k", "May", ""]]}, {"id": "1412.1254", "submitter": "Philip Bille", "authors": "Philip Bille, Pawel Gawrychowski, Inge Li Goertz, Gad M. Landau, and\n  Oren Weimann", "title": "Longest Common Extensions in Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest common extension (LCE) of two indices in a string is the length\nof the longest identical substrings starting at these two indices. The LCE\nproblem asks to preprocess a string into a compact data structure that supports\nfast LCE queries. In this paper we generalize the LCE problem to trees and\nsuggest a few applications of LCE in trees to tries and XML databases. Given a\nlabeled and rooted tree $T$ of size $n$, the goal is to preprocess $T$ into a\ncompact data structure that support the following LCE queries between subpaths\nand subtrees in $T$. Let $v_1$, $v_2$, $w_1$, and $w_2$ be nodes of $T$ such\nthat $w_1$ and $w_2$ are descendants of $v_1$ and $v_2$ respectively.\n\\begin{itemize} \\item $\\LCEPP(v_1, w_1, v_2, w_2)$: (path-path $\\LCE$) return\nthe longest common prefix of the paths $v_1 \\leadsto w_1$ and $v_2 \\leadsto\nw_2$. \\item $\\LCEPT(v_1, w_1, v_2)$: (path-tree $\\LCE$) return maximal\npath-path LCE of the path $v_1 \\leadsto w_1$ and any path from $v_2$ to a\ndescendant leaf. \\item $\\LCETT(v_1, v_2)$: (tree-tree $\\LCE$) return a maximal\npath-path LCE of any pair of paths from $v_1$ and $v_2$ to descendant leaves.\n\\end{itemize} We present the first non-trivial bounds for supporting these\nqueries. For $\\LCEPP$ queries, we present a linear-space solution with\n$O(\\log^{*} n)$ query time. For $\\LCEPT$ queries, we present a linear-space\nsolution with $O((\\log\\log n)^{2})$ query time, and complement this with a\nlower bound showing that any path-tree LCE structure of size $O(n \\polylog(n))$\nmust necessarily use $\\Omega(\\log\\log n)$ time to answer queries. For $\\LCETT$\nqueries, we present a time-space trade-off, that given any parameter $\\tau$, $1\n\\leq \\tau \\leq n$, leads to an $O(n\\tau)$ space and $O(n/\\tau)$ query-time\nsolution. This is complemented with a reduction to the the set intersection\nproblem implying that a fast linear space solution is not likely to exist.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 10:02:47 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 18:06:22 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2015 14:55:22 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Bille", "Philip", ""], ["Gawrychowski", "Pawel", ""], ["Goertz", "Inge Li", ""], ["Landau", "Gad M.", ""], ["Weimann", "Oren", ""]]}, {"id": "1412.1261", "submitter": "Florian Sikora", "authors": "Faisal N. Abu-Khzam, \\'Edouard Bonnet, Florian Sikora", "title": "On the Complexity of Various Parameterizations of Common Induced\n  Subgraph Isomorphism", "comments": "This version introduces new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Maximum Common Induced Subgraph problem (henceforth MCIS), given two\ngraphs $G_1$ and $G_2$, one looks for a graph with the maximum number of\nvertices being both an induced subgraph of $G_1$ and $G_2$. MCIS is among the\nmost studied classical NP-hard problems. It remains NP-hard on many graph\nclasses including forests. In this paper, we study the parameterized complexity\nof MCIS. As a generalization of \\textsc{Clique}, it is W[1]-hard parameterized\nby the size of the solution. Being NP-hard even on forests, most structural\nparameterizations are intractable. One has to go as far as parameterizing by\nthe size of the minimum vertex cover to get some tractability. Indeed, when\nparameterized by $k := \\text{vc}(G_1)+\\text{vc}(G_2)$ the sum of the vertex\ncover number of the two input graphs, the problem was shown to be\nfixed-parameter tractable, with an algorithm running in time $2^{O(k \\log k)}$.\nWe complement this result by showing that, unless the ETH fails, it cannot be\nsolved in time $2^{o(k \\log k)}$. This kind of tight lower bound has been shown\nfor a few problems and parameters but, to the best of our knowledge, not for\nthe vertex cover number. We also show that MCIS does not have a polynomial\nkernel when parameterized by $k$, unless $NP \\subseteq \\mathsf{coNP}/poly$.\nFinally, we study MCIS and its connected variant MCCIS on some special graph\nclasses and with respect to other structural parameters.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 10:28:35 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 18:55:24 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""], ["Bonnet", "\u00c9douard", ""], ["Sikora", "Florian", ""]]}, {"id": "1412.1318", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Monika Henzinger and Giuseppe F. Italiano", "title": "Deterministic Fully Dynamic Data Structures for Vertex Cover and\n  Matching", "comments": "An extended abstract of this paper will appear in SODA' 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first deterministic data structures for maintaining\napproximate minimum vertex cover and maximum matching in a fully dynamic graph\n$G = (V,E)$, with $|V| = n$ and $|E| =m$, in $o(\\sqrt{m}\\,)$ time per update.\nIn particular, for minimum vertex cover we provide deterministic data\nstructures for maintaining a $(2+\\eps)$ approximation in $O(\\log n/\\eps^2)$\namortized time per update.\n  For maximum matching, we show how to maintain a $(3+\\eps)$ approximation in\n$O(\\min(\\sqrt{n}/\\epsilon, m^{1/3}/\\eps^2))$ {\\em amortized} time per update,\nand a $(4+\\eps)$ approximation in $O(m^{1/3}/\\eps^2)$ {\\em worst-case} time per\nupdate. Our data structure for fully dynamic minimum vertex cover is\nessentially near-optimal and settles an open problem by Onak and Rubinfeld from\nSTOC' 2010.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 13:23:11 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Henzinger", "Monika", ""], ["Italiano", "Giuseppe F.", ""]]}, {"id": "1412.1470", "submitter": "Mostafa Haghir Chehreghani", "authors": "Mostafa Haghir Chehreghani and Maurice Bruynooghe", "title": "Mining Rooted Ordered Trees under Subtree Homeomorphism", "comments": "This paper is accepted in the Data Mining and Knowledge Discovery\n  journal\n  (http://www.springer.com/computer/database+management+%26+information+retrieval/journal/10618)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent tree patterns has many applications in different areas such\nas XML data, bioinformatics and World Wide Web. The crucial step in frequent\npattern mining is frequency counting, which involves a matching operator to\nfind occurrences (instances) of a tree pattern in a given collection of trees.\nA widely used matching operator for tree-structured data is subtree\nhomeomorphism, where an edge in the tree pattern is mapped onto an\nancestor-descendant relationship in the given tree. Tree patterns that are\nfrequent under subtree homeomorphism are usually called embedded patterns. In\nthis paper, we present an efficient algorithm for subtree homeomorphism with\napplication to frequent pattern mining. We propose a compact data-structure,\ncalled occ, which stores only information about the rightmost paths of\noccurrences and hence can encode and represent several occurrences of a tree\npattern. We then define efficient join operations on the occ data-structure,\nwhich help us count occurrences of tree patterns according to occurrences of\ntheir proper subtrees. Based on the proposed subtree homeomorphism method, we\ndevelop an effective pattern mining algorithm, called TPMiner. We evaluate the\nefficiency of TPMiner on several real-world and synthetic datasets. Our\nextensive experiments confirm that TPMiner always outperforms well-known\nexisting algorithms, and in several cases the improvement with respect to\nexisting algorithms is significant.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 15:57:00 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 17:43:23 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 20:36:21 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Chehreghani", "Mostafa Haghir", ""], ["Bruynooghe", "Maurice", ""]]}, {"id": "1412.1591", "submitter": "Travis Gagie", "authors": "Travis Gagie and Simon J. Puglisi", "title": "Searching and Indexing Genomic Databases via Kernelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advance of DNA sequencing technologies has yielded databases of\nthousands of genomes. To search and index these databases effectively, it is\nimportant that we take advantage of the similarity between those genomes.\nSeveral authors have recently suggested searching or indexing only one\nreference genome and the parts of the other genomes where they differ. In this\npaper we survey the twenty-year history of this idea and discuss its relation\nto kernelization in parameterized complexity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 09:11:46 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Gagie", "Travis", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1412.1639", "submitter": "Raed Jaberi", "authors": "Martin Dietzfelbinger, Raed Jaberi", "title": "On testing single connectedness in directed graphs and some related\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a directed graph with $n$ vertices and $m$ edges. The graph\n$G$ is called singly-connected if for each pair of vertices $v,w \\in V$ there\nis at most one simple path from $v$ to $w$ in $G$. Buchsbaum and Carlisle\n(1993) gave an algorithm for testing whether $G$ is singly-connected in\n$O(n^{2})$ time. In this paper we describe a refined version of this algorithm\nwith running time $O(s\\cdot t+m)$, where $s$ and $t$ are the number of sources\nand sinks, respectively, in the reduced graph $G^{r}$ obtained by first\ncontracting each strongly connected component of $G$ into one vertex and then\neliminating vertices of indegree or outdegree $1$ by a contraction operation.\nMoreover, we show that the problem of finding a minimum cardinality edge subset\n$C\\subseteq E$ (respectively, vertex subset $F\\subseteq V$) whose removal from\n$G$ leaves a singly-connected graph is NP-hard.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 12:20:41 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 13:11:31 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Dietzfelbinger", "Martin", ""], ["Jaberi", "Raed", ""]]}, {"id": "1412.1763", "submitter": "Zengfeng Huang", "authors": "Zengfeng Huang, Wai Ming Tai and Ke Yi", "title": "Tracking the Frequency Moments at All Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional requirement for a randomized streaming algorithm is just {\\em\none-shot}, i.e., algorithm should be correct (within the stated $\\eps$-error\nbound) at the end of the stream. In this paper, we study the {\\em tracking}\nproblem, where the output should be correct at all times. The standard approach\nfor solving the tracking problem is to run $O(\\log m)$ independent instances of\nthe one-shot algorithm and apply the union bound to all $m$ time instances. In\nthis paper, we study if this standard approach can be improved, for the\nclassical frequency moment problem. We show that for the $F_p$ problem for any\n$1 < p \\le 2$, we actually only need $O(\\log \\log m + \\log n)$ copies to\nachieve the tracking guarantee in the cash register model, where $n$ is the\nuniverse size. Meanwhile, we present a lower bound of $\\Omega(\\log m \\log\\log\nm)$ bits for all linear sketches achieving this guarantee. This shows that our\nupper bound is tight when $n=(\\log m)^{O(1)}$. We also present an\n$\\Omega(\\log^2 m)$ lower bound in the turnstile model, showing that the\nstandard approach by using the union bound is essentially optimal.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 18:49:15 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Huang", "Zengfeng", ""], ["Tai", "Wai Ming", ""], ["Yi", "Ke", ""]]}, {"id": "1412.1787", "submitter": "Michael Bannister", "authors": "Michael J. Bannister and William E. Devanny and David Eppstein", "title": "ERGMs are Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of the exponential random graph\nmodel (ERGM) commonly used in social network analysis. This model represents a\nprobability distribution on graphs by setting the log-likelihood of generating\na graph to be a weighted sum of feature counts. These log-likelihoods must be\nexponentiated and then normalized to produce probabilities, and the normalizing\nconstant is called the \\emph{partition function}. We show that the problem of\ncomputing the partition function is $\\mathsf{\\#P}$-hard, and inapproximable in\npolynomial time to within an exponential ratio, assuming $\\mathsf{P} \\neq\n\\mathsf{NP}$. Furthermore, there is no randomized polynomial time algorithm for\ngenerating random graphs whose distribution is within total variation distance\n$1-o(1)$ of a given ERGM. Our proofs use standard feature types based on the\nsociological theories of assortative mixing and triadic closure.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 19:52:27 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Bannister", "Michael J.", ""], ["Devanny", "William E.", ""], ["Eppstein", "David", ""]]}, {"id": "1412.1792", "submitter": "Anastasios Sidiropoulos", "authors": "Ken-ichi Kawarabayashi, Anastasios Sidiropoulos", "title": "Beyond the Euler characteristic: Approximating the genus of general\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the Euler genus of a graph is a fundamental problem in graph theory\nand topology. It has been shown to be NP-hard by [Thomassen '89] and a\nlinear-time fixed-parameter algorithm has been obtained by [Mohar '99]. Despite\nextensive study, the approximability of the Euler genus remains wide open.\nWhile the existence of an $O(1)$-approximation is not ruled out, the currently\nbest-known upper bound is a trivial $O(n/g)$-approximation that follows from\nbounds on the Euler characteristic.\n  In this paper, we give the first non-trivial approximation algorithm for this\nproblem. Specifically, we present a polynomial-time algorithm which given a\ngraph $G$ of Euler genus $g$ outputs an embedding of $G$ into a surface of\nEuler genus $g^{O(1)}$. Combined with the above $O(n/g)$-approximation, our\nresult also implies a $O(n^{1-\\alpha})$-approximation, for some universal\nconstant $\\alpha>0$.\n  Our approximation algorithm also has implications for the design of\nalgorithms on graphs of small genus. Several of these algorithms require that\nan embedding of the graph into a surface of small genus is given as part of the\ninput. Our result implies that many of these algorithms can be implemented even\nwhen the embedding of the input graph is unknown.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 20:13:36 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Kawarabayashi", "Ken-ichi", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "1412.1870", "submitter": "Yunpeng Li", "authors": "Yunpeng Li", "title": "A New Single-Source Shortest Path Algorithm for Nonnegative Weight Graph", "comments": "11 pages,The algorithm is not good enough to improve the existing\n  work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The single-source shortest path problem is a classical problem in the\nresearch field of graph algorithm. In this paper, a new single-source shortest\npath algorithm for nonnegative weight graph is proposed. The algorithm can\ncompress multi-round Fibonacci heap operations to one round to save running\ntime relative to Dijkstra's algorithm using Fibonacci heap. The time complexity\nof the algorithm is also O(m+nlogn) in the worst case, where m is the number of\nedges and n is the number of nodes. However, the bound can be linear in some\ncase, for example, when edge weights of a graph are all the same and the hop\ncount of the longest shortest path is much less than n.Based on the theoretical\nanalyses, we demonstrate that the algorithm is faster than Dijkstra's algorithm\nusing Fibonacci heap in average situation when n is large enough.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 01:04:51 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 01:34:06 GMT"}, {"version": "v3", "created": "Wed, 17 Dec 2014 14:52:51 GMT"}, {"version": "v4", "created": "Thu, 18 Dec 2014 01:21:56 GMT"}, {"version": "v5", "created": "Mon, 12 Jan 2015 12:24:42 GMT"}, {"version": "v6", "created": "Thu, 26 Mar 2015 10:29:33 GMT"}, {"version": "v7", "created": "Thu, 9 Jul 2015 08:06:22 GMT"}, {"version": "v8", "created": "Thu, 15 Oct 2015 11:52:30 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Li", "Yunpeng", ""]]}, {"id": "1412.2109", "submitter": "Janne H. Korhonen", "authors": "Petteri Kaski, Janne H. Korhonen, Christoph Lenzen, Jukka Suomela", "title": "Algebrisation in Distributed Graph Algorithms: Fast Matrix\n  Multiplication in the Congested Clique", "comments": "This paper has been withdrawn by the authors. This paper has been\n  superseded by arXiv:1503.04963 (merged from arXiv:1412.2109 and\n  arXiv:1412.2667)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While algebrisation constitutes a powerful technique in the design and\nanalysis of centralised algorithms, to date there have been hardly any\napplications of algebraic techniques in the context of distributed graph\nalgorithms. This work is a case study that demonstrates the potential of\nalgebrisation in the distributed context. We will focus on distributed graph\nalgorithms in the congested clique model; the graph problems that we will\nconsider include, e.g., the triangle detection problem and the all-pairs\nshortest path problem (APSP). There is plenty of prior work on combinatorial\nalgorithms in the congested clique model: for example, Dolev et al. (DISC 2012)\ngave an algorithm for triangle detection with a running time of $\\tilde\nO(n^{1/3})$, and Nanongkai (STOC 2014) gave an approximation algorithm for APSP\nwith a running time of $\\tilde O(n^{1/2})$. In this work, we will use algebraic\ntechniques -- in particular, algorithms based on fast matrix multiplication --\nto solve both triangle detection and the unweighted APSP in time\n$O(n^{0.15715})$; for weighted APSP, we give a $(1+o(1))$-approximation with\nthis running time, as well as an exact $\\tilde O(n^{1/3})$ solution.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 19:33:46 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 09:52:27 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Kaski", "Petteri", ""], ["Korhonen", "Janne H.", ""], ["Lenzen", "Christoph", ""], ["Suomela", "Jukka", ""]]}, {"id": "1412.2123", "submitter": "Dawsen Hwang", "authors": "Dawsen Hwang, Patrick Jaillet, Zhengyuan Zhou", "title": "Distributed Multi-Depot Routing without Communications", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider and formulate a class of distributed multi-depot routing\nproblems, where servers are to visit a set of requests, with the aim of\nminimizing the total distance travelled by all servers. These problems fall\ninto two categories: distributed offline routing problems where all the\nrequests that need to be visited are known from the start; distributed online\nrouting problems where the requests come to be known incrementally. A critical\nand novel feature of our formulations is that communications are not allowed\namong the servers, hence posing an interesting and challenging question: what\nperformance can be achieved in comparison to the best possible solution\nobtained from an omniscience planner with perfect communication capabilities?\nThe worst-case (over all possible request-set instances) performance metrics\nare given by the approximation ratio (offline case) and the competitive ratio\n(online case).\n  Our first result indicates that the online and offline problems are\neffectively equivalent: for the same request-set instance, the approximation\nratio and the competitive ratio differ by at most an additive factor of 2,\nirrespective of the release dates in the online case. Therefore, we can\nrestrict our attention to the offline problem. For the offline problem, we show\nthat the approximation ratio given by the Voronoi partition is m (the number of\nservers). For two classes of depot configurations, when the depots form a line\nand when the ratios between the distances of pairs of depots are upper bounded\nby a sublinear function f(m) (i.e., f(m) = o(m)), we give partition schemes\nwith sublinear approximation ratios O(log m) and {\\Theta}(f(m)) respectively.\nWe also discuss several interesting open problems in our formulations: in\nparticular, how our initial results (on the two deliberately chosen classes of\ndepots) shape our conjecture on the open problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 20:14:58 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Hwang", "Dawsen", ""], ["Jaillet", "Patrick", ""], ["Zhou", "Zhengyuan", ""]]}, {"id": "1412.2300", "submitter": "Haitao Wang", "authors": "Aaron M. Andrews and Haitao Wang", "title": "Minimizing the Aggregate Movements for Interval Coverage", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an interval coverage problem. Given $n$ intervals of the same\nlength on a line $L$ and a line segment $B$ on $L$, we want to move the\nintervals along $L$ such that every point of $B$ is covered by at least one\ninterval and the sum of the moving distances of all intervals is minimized. As\na basic geometry problem, it has applications in mobile sensor barrier coverage\nin wireless sensor networks. The previous work solved the problem in $O(n^2)$\ntime. In this paper, by discovering many interesting observations and\ndeveloping new algorithmic techniques, we present an $O(n\\log n)$ time\nalgorithm. We also show an $\\Omega(n\\log n)$ time lower bound for this problem,\nwhich implies the optimality of our algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 00:43:36 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Andrews", "Aaron M.", ""], ["Wang", "Haitao", ""]]}, {"id": "1412.2314", "submitter": "Bo Waggoner", "authors": "Bo Waggoner", "title": "$\\ell_p$ Testing and Learning of Discrete Distributions", "comments": "This is the full version of the paper appearing at ITCS 2015. Two\n  columns. 24 pages, of which 14 appendix", "journal-ref": null, "doi": "10.1145/2688073.2688095", "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic problems of testing uniformity of and learning a discrete\ndistribution, given access to independent samples from it, are examined under\ngeneral $\\ell_p$ metrics. The intuitions and results often contrast with the\nclassic $\\ell_1$ case. For $p > 1$, we can learn and test with a number of\nsamples that is independent of the support size of the distribution: With an\n$\\ell_p$ tolerance $\\epsilon$, $O(\\max\\{ \\sqrt{1/\\epsilon^q}, 1/\\epsilon^2 \\})$\nsamples suffice for testing uniformity and $O(\\max\\{ 1/\\epsilon^q,\n1/\\epsilon^2\\})$ samples suffice for learning, where $q=p/(p-1)$ is the\nconjugate of $p$. As this parallels the intuition that $O(\\sqrt{n})$ and $O(n)$\nsamples suffice for the $\\ell_1$ case, it seems that $1/\\epsilon^q$ acts as an\nupper bound on the \"apparent\" support size.\n  For some $\\ell_p$ metrics, uniformity testing becomes easier over larger\nsupports: a 6-sided die requires fewer trials to test for fairness than a\n2-sided coin, and a card-shuffler requires fewer trials than the die. In fact,\nthis inverse dependence on support size holds if and only if $p > \\frac{4}{3}$.\nThe uniformity testing algorithm simply thresholds the number of \"collisions\"\nor \"coincidences\" and has an optimal sample complexity up to constant factors\nfor all $1 \\leq p \\leq 2$. Another algorithm gives order-optimal sample\ncomplexity for $\\ell_{\\infty}$ uniformity testing. Meanwhile, the most natural\nlearning algorithm is shown to have order-optimal sample complexity for all\n$\\ell_p$ metrics.\n  The author thanks Cl\\'{e}ment Canonne for discussions and contributions to\nthis work.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 03:57:29 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 17:53:34 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 13:34:20 GMT"}, {"version": "v4", "created": "Sat, 21 Mar 2015 17:30:44 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Waggoner", "Bo", ""]]}, {"id": "1412.2341", "submitter": "Virendra Sule", "authors": "Madhav Desai and Virendra Sule", "title": "Generalized cofactors and decomposition of Boolean satisfiability\n  problems", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for decomposing Boolean satisfiability problems while\nextending recent results of \\cite{sul2} on solving Boolean systems of\nequations. Developments in \\cite{sul2} were aimed at the expansion of functions\n$f$ in orthonormal (ON) sets of base functions as a generalization of the\nBoole-Shannon expansion and the derivation of the consistency condition for the\nequation $f=0$ in terms of the expansion co-efficients. In this paper, we\nfurther extend the Boole-Shannon expansion over an arbitrary set of base\nfunctions and derive the consistency condition for $f=1$. The generalization of\nthe Boole-Shannon formula presented in this paper is in terms of\n\\emph{cofactors} as co-efficients with respect to a set of CNFs called a\n\\emph{base} which appear in a given Boolean CNF formula itself. This approach\nresults in a novel parallel algorithm for decomposition of a CNF formula and\ncomputation of all satisfying assignments when they exist by using the given\ndata set of CNFs itself as the base.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 10:53:38 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Desai", "Madhav", ""], ["Sule", "Virendra", ""]]}, {"id": "1412.2379", "submitter": "Quentin Stout", "authors": "Quentin F. Stout", "title": "An Algorithm for $L_\\infty$ Approximation by Step Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is given for determining an optimal $b$-step approximation of\nweighted data, where the error is measured with respect to the $L_\\infty$ norm.\nFor data presorted by the independent variable the algorithm takes $\\Theta(n +\n\\log n \\cdot b(1+\\log n/b))$ time and $\\Theta(n)$ space. This is $\\Theta(n \\log\nn)$ in the worst case and $\\Theta(n)$ when $b = O(n/\\log n \\log\\log n)$. A\nminor change determines an optimal reduced isotonic regression in the same time\nand space bounds, and the algorithm also solves the $k$-center problem for\n1-dimensional weighted data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 17:51:09 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 04:49:47 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Stout", "Quentin F.", ""]]}, {"id": "1412.2437", "submitter": "Yunpeng Li", "authors": "Yunpeng Li (Southeast University)", "title": "A New Exact Algorithm for Traveling Salesman Problem with Time\n  Complexity Interval (O(n^4), O(n^3*2^n))", "comments": "22 pages; Another new faster algorithm is proposed in section 7 of\n  the updated version. arXiv admin note: text overlap with arXiv:1412.1870", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traveling salesman problem is a NP-hard problem. Until now, researchers have\nnot found a polynomial time algorithm for traveling salesman problem. Among the\nexisting algorithms, dynamic programming algorithm can solve the problem in\ntime O(n^2*2^n) where n is the number of nodes in the graph. The branch-and-cut\nalgorithm has been applied to solve the problem with a large number of nodes.\nHowever, branch-and-cut algorithm also has an exponential worst-case running\ntime.\n  In this paper, a new exact algorithm for traveling salesman problem is\nproposed. The algorithm can be used to solve an arbitrary instance of traveling\nsalesman problem in real life and the time complexity interval of the algorithm\nis (O(n^4), O(n^3*2^n)). It means that for some instances, the algorithm can\nfind the optimal solution in polynomial time although the algorithm also has an\nexponential worst-case running time. In other words, the algorithm tells us\nthat not all the instances of traveling salesman problem need exponential time\nto compute the optimal solution. The algorithm of this paper can not only\nassist us to solve traveling salesman problem better, but also can assist us to\ndeepen the comprehension of the relationship between NP-complete and P.\nTherefore, it is considerable in the further research on traveling salesman\nproblem and NP-hard problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 02:53:03 GMT"}, {"version": "v2", "created": "Sat, 20 Dec 2014 08:59:15 GMT"}, {"version": "v3", "created": "Sat, 10 Jan 2015 03:26:48 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2015 11:49:03 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Li", "Yunpeng", "", "Southeast University"]]}, {"id": "1412.2526", "submitter": "Jakub Mare\\v{c}ek", "authors": "Jakub Marecek", "title": "Exploiting Packing Components in General-Purpose Integer Programming\n  Solvers", "comments": null, "journal-ref": "J. D. Pinter and G. Fasano (eds.), Optimized Packings and Their\n  Applications: 207-223. Springer Optimization and its Applications, vol. 105.\n  Springer, 2015", "doi": "10.1007/978-3-319-18899-7_10", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of packing boxes into a large box is often a part of a larger\nproblem. For example in furniture supply chain applications, one needs to\ndecide what trucks to use to transport furniture between production sites and\ndistribution centers and stores, such that the furniture fits inside. Such\nproblems are often formulated and sometimes solved using general-purpose\ninteger programming solvers.\n  This chapter studies the problem of identifying a compact formulation of the\nmulti-dimensional packing component in a general instance of integer linear\nprogramming, reformulating it using the discretisation of\nAllen--Burke--Marecek, and and solving the extended reformulation. Results on\ninstances of up to 10000000 boxes are reported.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 11:55:14 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Marecek", "Jakub", ""]]}, {"id": "1412.2667", "submitter": "Ami Paz", "authors": "Keren Censor-Hillel and Ami Paz", "title": "Computing Exact Distances in the Congested Clique", "comments": "This paper has been withdrawn by the authors. This paper has been\n  superseded by arXiv:1503.04963 (merged from arXiv:1412.2109 and\n  arXiv:1412.2667)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives simple distributed algorithms for the fundamental problem of\ncomputing graph distances in the Congested Clique model. One of the main\ncomponents of our algorithms is fast matrix multiplication, for which we show\nan $O(n^{1/3})$-round algorithm when the multiplication needs to be performed\nover a semi-ring, and an $O(n^{0.157})$-round algorithm when the computation\ncan be performed over a field. We propose to denote by $\\kappa$ the exponent of\nmatrix multiplication in this model, which gives $\\kappa < 0.157$.\n  We show how to compute all-pairs-shortest-paths (APSP) in $O(n^{1/3}\\log{n})$\nrounds in weighted graphs of $n$ nodes, implying also the computation of the\ngraph diameter $D$. In unweighted graphs, APSP can be computed in\n$O(\\min\\{n^{1/3}\\log{D},n^{\\kappa} D\\})$ rounds, and the diameter can be\ncomputed in $O(n^{\\kappa}\\log{D})$ rounds. Furthermore, we show how to compute\nthe girth of a graph in $O(n^{1/3})$ rounds, and provide triangle detection and\n4-cycle detection algorithms that complete in $O(n^{\\kappa})$ rounds.\n  All our algorithms are deterministic. Our triangle detection and 4-cycle\ndetection algorithms improve upon the previously best known algorithms in this\nmodel, and refute a conjecture that $\\tilde \\Omega (n^{1/3})$ rounds are\nrequired for detecting triangles by any deterministic oblivious algorithm. Our\ndistance computation algorithms are exact, and improve upon the previously best\nknown $\\tilde O(n^{1/2})$ algorithm of Nanongkai [STOC 2014] for computing a\n$(2+o(1))$-approximation of APSP.\n  Finally, we give lower bounds that match the above for natural families of\nalgorithms. For the Congested Clique Broadcast model, we derive unconditioned\nlower bounds for matrix multiplication and APSP. The matrix multiplication\nalgorithms and lower bounds are adapted from parallel computations, which is a\nconnection of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 17:08:11 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2015 21:25:56 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Paz", "Ami", ""]]}, {"id": "1412.2787", "submitter": "Thomas Pajor", "authors": "Thomas Pajor, Eduardo Uchoa, Renato F. Werneck", "title": "A Robust and Scalable Algorithm for the Steiner Problem in Graphs", "comments": "33 pages. Presented at the 11th DIMACS Implementation Challenge on\n  Steiner Tree Problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an effective heuristic for the Steiner Problem in Graphs. Its main\nelements are a multistart algorithm coupled with aggressive combination of\nelite solutions, both leveraging recently-proposed fast local searches. We also\npropose a fast implementation of a well-known dual ascent algorithm that not\nonly makes our heuristics more robust (by quickly dealing with easier cases),\nbut can also be used as a building block of an exact (branch-and-bound)\nalgorithm that is quite effective for some inputs. On all graph classes we\nconsider, our heuristic is competitive with (and sometimes more effective than)\nany previous approach with similar running times. It is also scalable: with\nlong runs, we could improve or match the best published results for most open\ninstances in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 21:48:26 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 16:56:32 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Pajor", "Thomas", ""], ["Uchoa", "Eduardo", ""], ["Werneck", "Renato F.", ""]]}, {"id": "1412.2844", "submitter": "Quentin Stout", "authors": "Janis Hardwick and Quentin F. Stout", "title": "Optimal Reduced Isotonic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isotonic regression is a shape-constrained nonparametric regression in which\nthe regression is an increasing step function. For $n$ data points, the number\nof steps in the isotonic regression may be as large as $n$. As a result,\nstandard isotonic regression has been criticized as overfitting the data or\nmaking the representation too complicated. So-called \"reduced\" isotonic\nregression constrains the outcome to be a specified number of steps $b$, $b\n\\leq n$. However, because the previous algorithms for finding the reduced $L_2$\nregression took $\\Theta(n+bm^2)$ time, where $m$ is the number of steps of the\nunconstrained isotonic regression, researchers felt that the algorithms were\ntoo slow and instead used approximations. Other researchers had results that\nwere approximations because they used a greedy top-down approach. Here we give\nan algorithm to find an exact solution in $\\Theta(n+bm)$ time, and a simpler\nalgorithm taking $\\Theta(n+b m \\log m)$ time. These algorithms also determine\noptimal $k$-means clustering of weighted 1-dimensional data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 03:18:48 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Hardwick", "Janis", ""], ["Stout", "Quentin F.", ""]]}, {"id": "1412.2954", "submitter": "Ying Xiao", "authors": "Santosh S. Vempala and Ying Xiao", "title": "Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample\n  Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, general technique for reducing the sample complexity of\nmatrix and tensor decomposition algorithms applied to distributions. We use the\ntechnique to give a polynomial-time algorithm for standard ICA with sample\ncomplexity nearly linear in the dimension, thereby improving substantially on\nprevious bounds. The analysis is based on properties of random polynomials,\nnamely the spacings of an ensemble of polynomials. Our technique also applies\nto other applications of tensor decompositions, including spherical Gaussian\nmixture models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 13:33:04 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 14:31:06 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 19:00:25 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Vempala", "Santosh S.", ""], ["Xiao", "Ying", ""]]}, {"id": "1412.3016", "submitter": "Ali Alatabbi", "authors": "Ali Alatabbi and M. Sohel Rahman and W. F. Smyth", "title": "Computing Covers Using Prefix Tables", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An \\emph{indeterminate string} $x = x[1..n]$ on an alphabet $\\Sigma$ is a\nsequence of nonempty subsets of $\\Sigma$; $x$ is said to be \\emph{regular} if\nevery subset is of size one. A proper substring $u$ of regular $x$ is said to\nbe a \\emph{cover} of $x$ iff for every $i \\in 1..n$, an occurrence of $u$ in\n$x$ includes $x[i]$. The \\emph{cover array} $\\gamma = \\gamma[1..n]$ of $x$ is\nan integer array such that $\\gamma[i]$ is the longest cover of $x[1..i]$.\nFifteen years ago a complex, though nevertheless linear-time, algorithm was\nproposed to compute the cover array of regular $x$ based on prior computation\nof the border array of $x$. In this paper we first describe a linear-time\nalgorithm to compute the cover array of regular string $x$ based on the prefix\ntable of $x$. We then extend this result to indeterminate strings.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 16:38:25 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 11:53:36 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Alatabbi", "Ali", ""], ["Rahman", "M. Sohel", ""], ["Smyth", "W. F.", ""]]}, {"id": "1412.3023", "submitter": "Gregory Gutin", "authors": "F. Barbero, G. Gutin, M. Jones, and B. Sheng", "title": "Parameterized and Approximation Algorithms for the Load Coloring Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $c, k$ be two positive integers and let $G=(V,E)$ be a graph. The\n$(c,k)$-Load Coloring Problem (denoted $(c,k)$-LCP) asks whether there is a\n$c$-coloring $\\varphi: V \\rightarrow [c]$ such that for every $i \\in [c]$,\nthere are at least $k$ edges with both endvertices colored $i$. Gutin and Jones\n(IPL 2014) studied this problem with $c=2$. They showed $(2,k)$-LCP to be fixed\nparameter tractable (FPT) with parameter $k$ by obtaining a kernel with at most\n$7k$ vertices. In this paper, we extend the study to any fixed $c$ by giving\nboth a linear-vertex and a linear-edge kernel. In the particular case of $c=2$,\nwe obtain a kernel with less than $4k$ vertices and less than $8k$ edges. These\nresults imply that for any fixed $c\\ge 2$, $(c,k)$-LCP is FPT and that the\noptimization version of $(c,k)$-LCP (where $k$ is to be maximized) has an\napproximation algorithm with a constant ratio for any fixed $c\\ge 2$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 17:08:13 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 09:41:14 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Barbero", "F.", ""], ["Gutin", "G.", ""], ["Jones", "M.", ""], ["Sheng", "B.", ""]]}, {"id": "1412.3084", "submitter": "Davis Shurbert", "authors": "Michel Alexis, Davis Shurbert, Charles Dunn, Jennifer Nordstrom", "title": "Clique-Relaxed Competitive Graph Coloring", "comments": "11 pages, 4 figures, Willamette Valley REU-RET Consortium for\n  Mathematics Research, Linfield College, Summer 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a variation of the graph coloring game, as studied in [2]. In\nthe original coloring game, two players, Alice and Bob, alternate coloring\nvertices on a graph with legal colors from a fixed color set, where a color\n{\\alpha} is legal for a vertex if said vertex has no neighbors colored\n{\\alpha}. Other variations of the game change this definition of a legal color.\nFor a fixed color set, Alice wins the game if all vertices are colored when the\ngame ends, while Bob wins if there is a point in the game in which a vertex\ncannot be assigned a legal color. The least number of colors needed for Alice\nto have a winning strategy on a graph G is called the game chromatic number of\nG, and is denoted \\c{hi}g(G). A well studied variation is the d-relaxed\ncoloring game [5] in which a legal coloring of a graph G is defined as any\nassignment of colors to V (G) such that the subgraph of G induced by any color\nclass has maximum degree d. We focus on the k-clique-relaxed n-coloring game. A\nk-clique-relaxed n-coloring of a graph G is an n-coloring in which the subgraph\nof G induced by any color class has maximum clique size k or less. In other\nwords, a k-clique-relaxed n-coloring of G is an assignment of n colors to V (G)\nin which there are no monochromatic (k + 1)-cliques.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 20:10:58 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Alexis", "Michel", ""], ["Shurbert", "Davis", ""], ["Dunn", "Charles", ""], ["Nordstrom", "Jennifer", ""]]}, {"id": "1412.3138", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Yuexin Wu, and Jianyang Zeng", "title": "Computational Protein Design Using AND/OR Branch-and-Bound Search", "comments": "RECOMB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of the global minimum energy conformation (GMEC) is an\nimportant and challenging topic in structure-based computational protein\ndesign. In this paper, we propose a new protein design algorithm based on the\nAND/OR branch-and-bound (AOBB) search, which is a variant of the traditional\nbranch-and-bound search algorithm, to solve this combinatorial optimization\nproblem. By integrating with a powerful heuristic function, AOBB is able to\nfully exploit the graph structure of the underlying residue interaction network\nof a backbone template to significantly accelerate the design process. Tests on\nreal protein data show that our new protein design algorithm is able to solve\nmany prob- lems that were previously unsolvable by the traditional exact search\nalgorithms, and for the problems that can be solved with traditional provable\nalgorithms, our new method can provide a large speedup by several orders of\nmagnitude while still guaranteeing to find the global minimum energy\nconformation (GMEC) solution.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 12:23:10 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 14:06:36 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Zhou", "Yichao", ""], ["Wu", "Yuexin", ""], ["Zeng", "Jianyang", ""]]}, {"id": "1412.3290", "submitter": "Marc Pouget", "authors": "R\\'emi Imbach (INRIA Nancy - Grand Est / LORIA), Guillaume Moroz\n  (INRIA Nancy - Grand Est / LORIA), Marc Pouget (INRIA Nancy - Grand Est /\n  LORIA)", "title": "Numeric certified algorithm for the topology of resultant and\n  discriminant curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.SC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal C$ be a real plane algebraic curve defined by the resultant of\ntwo polynomials (resp. by the discriminant of a polynomial). Geometrically such\na curve is the projection of the intersection of the surfaces\n$P(x,y,z)=Q(x,y,z)=0$ (resp. $P(x,y,z)=\\frac{\\partial P}{\\partial\nz}(x,y,z)=0$), and generically its singularities are nodes (resp. nodes and\nordinary cusps). State-of-the-art numerical algorithms compute the topology of\nsmooth curves but usually fail to certify the topology of singular ones. The\nmain challenge is to find practical numerical criteria that guarantee the\nexistence and the uniqueness of a singularity inside a given box $B$, while\nensuring that $B$ does not contain any closed loop of $\\mathcal{C}$. We solve\nthis problem by first providing a square deflation system, based on\nsubresultants, that can be used to certify numerically whether $B$ contains a\nunique singularity $p$ or not. Then we introduce a numeric adaptive separation\ncriterion based on interval arithmetic to ensure that the topology of $\\mathcal\nC$ in $B$ is homeomorphic to the local topology at $p$. Our algorithms are\nimplemented and experiments show their efficiency compared to state-of-the-art\nsymbolic or homotopic methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 12:57:08 GMT"}, {"version": "v2", "created": "Sat, 23 May 2015 10:22:21 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Imbach", "R\u00e9mi", "", "INRIA Nancy - Grand Est / LORIA"], ["Moroz", "Guillaume", "", "INRIA Nancy - Grand Est / LORIA"], ["Pouget", "Marc", "", "INRIA Nancy - Grand Est /\n  LORIA"]]}, {"id": "1412.3333", "submitter": "Patrick Prosser", "authors": "Frod Prefect, Patrick Prosser", "title": "Empirical Algorithmics: draw your own conclusions", "comments": "arXiv admin note: text overlap with arXiv:1207.4616", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an empirical comparisons of algorithms we might compare run times over a\nset of benchmark problems to decide which one is fastest, i.e. an algorithmic\nhorse race. Ideally we would like to download source code for the algorithms,\ncompile and then run on our machine. Sometimes code isn't available to download\nand sometimes resource isn't available to implement all the algorithms we want\nto study. To get round this, published results are rescaled, a technique\nendorsed by DIMACS, and those rescaled results included in a new study. This\ntechnique is frequently used when presenting new algorithms for the maximum\nclique problem. We demonstrate that this is unsafe, and that if carelessly used\nmay allow us to draw conflicting conclusions from our empirical study.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 15:11:05 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Prefect", "Frod", ""], ["Prosser", "Patrick", ""]]}, {"id": "1412.3334", "submitter": "Kei Uchizawa Dr.", "authors": "Takehiro Ito, Yota Otachi, Toshiki Saitoh, Hisayuki Satoh, Akira\n  Suzuki, Kei Uchizawa, Ryuhei Uehara, Katsuhisa Yamanaka and Xiao Zhou", "title": "Computational Complexity of Competitive Diffusion on (Un)weighted Graphs", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.GT cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an undirected graph modeling a social network, where the vertices\nrepresent users, and the edges do connections among them. In the competitive\ndiffusion game, each of a number of players chooses a vertex as a seed to\npropagate his/her opinion, and then it spreads along the edges in the graphs.\nThe objective of every player is to maximize the number of vertices the opinion\ninfects. In this paper, we investigate a computational problem of asking\nwhether a pure Nash equilibrium exists in the competitive diffusion game on\nunweighed and weighted graphs, and present several negative and positive\nresults. We first prove that the problem is W[1]-hard when parameterized by the\nnumber of players even for unweighted graphs. We also show that the problem is\nNP-hard even for series-parallel graphs with positive integer weights, and is\nNP-hard even for forests with arbitrary integer weights. Furthermore, we show\nthat the problem for forest of paths with arbitrary weights is solvable in\npseudo-polynomial time; and it is solvable in quadratic time if a given graph\nis unweighted. We also prove that the problem for chain, cochain, and threshold\ngraphs with arbitrary integer weights is solvable in polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 15:12:10 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Ito", "Takehiro", ""], ["Otachi", "Yota", ""], ["Saitoh", "Toshiki", ""], ["Satoh", "Hisayuki", ""], ["Suzuki", "Akira", ""], ["Uchizawa", "Kei", ""], ["Uehara", "Ryuhei", ""], ["Yamanaka", "Katsuhisa", ""], ["Zhou", "Xiao", ""]]}, {"id": "1412.3359", "submitter": "Qi Duan", "authors": "Qi Duan, Haadi Jafarian, Ehab Al-Shaer and Jinhui Xu", "title": "On DDoS Attack Related Minimum Cut Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two important extensions of the classical minimum cut\nproblem, called {\\em Connectivity Preserving Minimum Cut (CPMC)} problem and\n{\\em Threshold Minimum Cut (TMC)} problem, which have important applications in\nlarge-scale DDoS attacks. In CPMC problem, a minimum cut is sought to separate\na of source from a destination node and meanwhile preserve the connectivity\nbetween the source and its partner node(s). The CPMC problem also has important\napplications in many other areas such as emergency responding, image\nprocessing, pattern recognition, and medical sciences. In TMC problem, a\nminimum cut is sought to isolate a target node from a threshold number of\npartner nodes. TMC problem is an important special case of network inhibition\nproblem and has important applications in network security. We show that the\ngeneral CPMC problem cannot be approximated within $logn$ unless $NP=P$ has\nquasi-polynomial algorithms. We also show that a special case of two group CPMC\nproblem in planar graphs can be solved in polynomial time. The corollary of\nthis result is that the network diversion problem in planar graphs is in $P$, a\npreviously open problem. We show that the threshold minimum node cut (TMNC)\nproblem can be approximated within ratio $O(\\sqrt{n})$ and the threshold\nminimum edge cut problem (TMEC) can be approximated within ratio\n$O(\\log^2{n})$. \\emph{We also answer another long standing open problem: the\nhardness of the network inhibition problem and network interdiction problem. We\nshow that both of them cannot be approximated within any constant ratio. unless\n$NP \\nsubseteq \\cap_{\\delta>0} BPTIME(2^{n^{\\delta}})$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 16:35:32 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 14:33:11 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Duan", "Qi", ""], ["Jafarian", "Haadi", ""], ["Al-Shaer", "Ehab", ""], ["Xu", "Jinhui", ""]]}, {"id": "1412.3445", "submitter": "Stephan Holzer", "authors": "Stephan Holzer, Nathan Pinsker", "title": "Approximation of Distances and Shortest Paths in the Broadcast Congest\n  Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the broadcast version of the CONGEST CLIQUE model of distributed\ncomputing. In this model, in each round, any node in a network of size $n$ can\nsend the same message (i.e. broadcast a message) of limited size to every other\nnode in the network. Nanongkai presented in [STOC'14] a randomized\n$(2+o(1))$-approximation algorithm to compute all pairs shortest paths (APSP)\nin time $\\tilde{O}(\\sqrt{n})$ on weighted graphs, where we use the convention\nthat $\\tilde{\\Omega}(f(n))$ is essentially $\\Omega(f(n)/$polylog$f(n))$ and\n$\\tilde{O}(f(n))$ is essentially $O(f(n) $polylog$f(n))$. We complement this\nresult by proving that any randomized $(2-o(1))$-approximation of APSP and\n$(2-o(1))$-approximation of the diameter of a graph takes $\\tilde\\Omega(n)$\ntime in the worst case. This demonstrates that getting a negligible improvement\nin the approximation factor requires significantly more time. Furthermore this\nbound implies that already computing a $(2-o(1))$-approximation of all pairs\nshortest paths is among the hardest graph-problems in the broadcast-version of\nthe CONGEST CLIQUE model and contrasts a recent $(1+o(1))$-approximation for\nAPSP that runs in time $O(n^{0.15715})$ in the unicast version of the CONGEST\nCLIQUE model. On the positive side we provide a deterministic version of\nNanongkai's $(2+o(1))$-approximation algorithm for APSP. To do so we present a\nfast deterministic construction of small hitting sets. We also show how to\nreplace another randomized part within Nanongkai's algorithm with a\ndeterministic source-detection algorithm designed for the CONGEST model\npresented by Lenzen and Peleg at PODC'13.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 20:59:11 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 19:27:40 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Holzer", "Stephan", ""], ["Pinsker", "Nathan", ""]]}, {"id": "1412.3507", "submitter": "Ilan Cohen", "authors": "Yossi Azar and Ilan Reuven Cohen and Debmalya Panigrahi", "title": "Online Covering with Convex Objectives and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithmic framework for minimizing general convex objectives\n(that are differentiable and monotone non-decreasing) over a set of covering\nconstraints that arrive online. This substantially extends previous work on\nonline covering for linear objectives (Alon {\\em et al.}, STOC 2003) and online\ncovering with offline packing constraints (Azar {\\em et al.}, SODA 2013). To\nthe best of our knowledge, this is the first result in online optimization for\ngeneric non-linear objectives; special cases of such objectives have previously\nbeen considered, particularly for energy minimization.\n  As a specific problem in this genre, we consider the unrelated machine\nscheduling problem with startup costs and arbitrary $\\ell_p$ norms on machine\nloads (including the surprisingly non-trivial $\\ell_1$ norm representing total\nmachine load). This problem was studied earlier for the makespan norm in both\nthe offline (Khuller~{\\em et al.}, SODA 2010; Li and Khuller, SODA 2011) and\nonline settings (Azar {\\em et al.}, SODA 2013). We adapt the two-phase approach\nof obtaining a fractional solution and then rounding it online (used\nsuccessfully to many linear objectives) to the non-linear objective. The\nfractional algorithm uses ideas from our general framework that we described\nabove (but does not fit the framework exactly because of non-positive entries\nin the constraint matrix). The rounding algorithm uses ideas from offline\nrounding of LPs with non-linear objectives (Azar and Epstein, STOC 2005; Kumar\n{\\em et al.}, FOCS 2005). Our competitive ratio is tight up to a logarithmic\nfactor. Finally, for the important special case of total load ($\\ell_1$ norm),\nwe give a different rounding algorithm that obtains a better competitive ratio\nthan the generic rounding algorithm for $\\ell_p$ norms. We show that this\ncompetitive ratio is asymptotically tight.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 00:35:03 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Azar", "Yossi", ""], ["Cohen", "Ilan Reuven", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "1412.3508", "submitter": "Henning Sulzbach", "authors": "Henning Sulzbach", "title": "On martingale tail sums for the path length in random trees", "comments": "Results generalized to broader tree model; convergence of moments in\n  the CLT", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a martingale $(X_n)$ converging almost surely to a random variable $X$,\nthe sequence $(X_n - X)$ is called martingale tail sum. Recently, Neininger\n[Random Structures Algorithms, 46 (2015), 346-361] proved a central limit\ntheorem for the martingale tail sum of R{\\'e}gnier's martingale for the path\nlength in random binary search trees. Gr{\\\"u}bel and Kabluchko [to appear in\nAnnals of Applied Probability, (2016), arXiv 1410.0469] gave an alternative\nproof also conjecturing a corresponding law of the iterated logarithm. We prove\nthe central limit theorem with convergence of higher moments and the law of the\niterated logarithm for a family of trees containing binary search trees,\nrecursive trees and plane-oriented recursive trees.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 00:36:31 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 02:36:02 GMT"}, {"version": "v3", "created": "Tue, 22 Mar 2016 19:34:37 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Sulzbach", "Henning", ""]]}, {"id": "1412.3570", "submitter": "Bruno Grenet", "authors": "Bruno Grenet", "title": "Bounded-degree factors of lacunary multivariate polynomials", "comments": "31 pages; Long version of arXiv:1401.4720 with simplified proofs", "journal-ref": "Journal of Symbolic Computation 75, pages 171-192, 2016", "doi": "10.1016/j.jsc.2015.11.013", "report-no": null, "categories": "cs.SC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new method for computing bounded-degree factors\nof lacunary multivariate polynomials. In particular for polynomials over number\nfields, we give a new algorithm that takes as input a multivariate polynomial f\nin lacunary representation and a degree bound d and computes the irreducible\nfactors of degree at most d of f in time polynomial in the lacunary size of f\nand in d. Our algorithm, which is valid for any field of zero characteristic,\nis based on a new gap theorem that enables reducing the problem to several\ninstances of (a) the univariate case and (b) low-degree multivariate\nfactorization.\n  The reduction algorithms we propose are elementary in that they only\nmanipulate the exponent vectors of the input polynomial. The proof of\ncorrectness and the complexity bounds rely on the Newton polytope of the\npolynomial, where the underlying valued field consists of Puiseux series in a\nsingle variable.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 08:41:03 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 10:45:16 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Grenet", "Bruno", ""]]}, {"id": "1412.3688", "submitter": "Emanuele Giaquinta", "authors": "Emanuele Giaquinta", "title": "Run-Length Encoded Nondeterministic KMP and Suffix Automata", "comments": "Various fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel bit-parallel representation, based on the run-length\nencoding, of the nondeterministic KMP and suffix automata for a string $P$ with\nat least two distinct symbols. Our method is targeted to the case of long\nstrings over small alphabets and complements the method of Cantone et al.\n(2012), which is effective for long strings over large alphabets. Our encoding\nrequires $O((\\sigma + m)\\lceil \\rho / w\\rceil)$ space and allows one to\nsimulate the automata on a string in time $O(\\lceil \\rho / w\\rceil)$ per\ntransition, where $\\sigma$ is the alphabet size, $m$ is the length of $P$,\n$\\rho$ is the length of the run-length encoding of $P$ and $w$ is the machine\nword size in bits. The input string can be given in either unencoded or\nrun-length encoded form.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 15:49:44 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 15:50:27 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Giaquinta", "Emanuele", ""]]}, {"id": "1412.3696", "submitter": "Tomasz Kociumaka", "authors": "Maxime Crochemore, Costas S. Iliopoulos, Tomasz Kociumaka, Jakub\n  Radoszewski, Wojciech Rytter, Tomasz Wale\\'n", "title": "Covering Problems for Partial Words and for Indeterminate Strings", "comments": "full version (simplified and corrected); preliminary version appeared\n  at ISAAC 2014; 14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a shortest solid cover of an\nindeterminate string. An indeterminate string may contain non-solid symbols,\neach of which specifies a subset of the alphabet that could be present at the\ncorresponding position. We also consider covering partial words, which are a\nspecial case of indeterminate strings where each non-solid symbol is a don't\ncare symbol. We prove that indeterminate string covering problem and partial\nword covering problem are NP-complete for binary alphabet and show that both\nproblems are fixed-parameter tractable with respect to $k$, the number of\nnon-solid symbols. For the indeterminate string covering problem we obtain a\n$2^{O(k \\log k)} + n k^{O(1)}$-time algorithm. For the partial word covering\nproblem we obtain a $2^{O(\\sqrt{k}\\log k)} + nk^{O(1)}$-time algorithm. We\nprove that, unless the Exponential Time Hypothesis is false, no\n$2^{o(\\sqrt{k})} n^{O(1)}$-time solution exists for either problem, which shows\nthat our algorithm for this case is close to optimal. We also present an\nalgorithm for both problems which is feasible in practice.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:00:26 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas S.", ""], ["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1412.3721", "submitter": "Sandeep Sen", "authors": "Debjyoti Saharoy and Sandeep Sen", "title": "Approximation Algorithms for Budget Constrained Network Upgradeable\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study budget constrained network upgradeable problems. We are given an\nundirected edge weighted graph $G=(V,E)$ where the weight an edge $e \\in E$ can\nbe upgraded for a cost $c(e)$. Given a budget $B$ for improvement, the goal is\nto find a subset of edges to be upgraded so that the resulting network is\noptimum for $B$. The results obtained in this paper include the following.\n  Maximum Weight Constrained Spanning Tree\n  We present a randomized algorithm for the problem of weight upgradeable\nbudget constrained maximum spanning tree on a general graph. This returns a\nspanning tree $\\mathcal{T}^{'}$ which is feasible within the budget $B$, such\nthat $\\Pr [ l(\\mathcal{T}^{'}) \\geq (1-\\epsilon)\\text{OPT}\\text{ , }\nc(\\mathcal{T}^{'} ) \\leq B] \\ge 1-\\frac{1}{n}$ (where $l$ and $c$ denote the\nlength and cost of the tree respectively), for any fixed $\\epsilon >0$, in time\npolynomial in $|V|=n$, $|E|=m$. Our results extend to the minimization version\nalso. Previously Krumke et. al. \\cite{krumke} presented a$(1+\\frac{1}{\\gamma},\n1+ \\gamma)$ bicriteria approximation algorithm for any fixed $\\gamma >0$ for\nthis problem in general graphs for a more general cost upgrade function. The\nresult in this paper improves their 0/1 cost upgrade model.\n  Longest Path in a DAG We consider the problem of weight improvable longest\npath in a $n$ vertex DAG and give a $O(n^3)$ algorithm for the problem when\nthere is a bound on the number of improvements allowed. We also give a\n$(1-\\epsilon)$-approximation which runs in $O(\\frac{n^4}{\\epsilon})$ time for\nthe budget constrained version. Similar results can be achieved also for the\nproblem of shortest paths in a DAG.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:47:57 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Saharoy", "Debjyoti", ""], ["Sen", "Sandeep", ""]]}, {"id": "1412.3852", "submitter": "Vijaya Ramachandran", "authors": "Matteo Pontecorvi and Vijaya Ramachandran", "title": "Fully Dynamic All Pairs All Shortest Paths", "comments": "This revision incorporates changes that improve the presentation.\n  There is no change to the main result. An extended abstract of this paper\n  will appear in Proc. ISAAC 2015, in a paper by the authors entitled \"Fully\n  Dynamic Betweenness Centrality''", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the all pairs all shortest paths (APASP) problem, which maintains\nall of the multiple shortest paths for every vertex pair in a directed graph\nG=(V,E) with a positive real weight on each edge. We present a fully dynamic\nalgorithm for this problem in which an update supports either weight increases\nor weight decreases on a subset of edges incident to a vertex. Our algorithm\nruns in amortized O(\\vstar^2 \\cdot \\log^3 n) time per update, where n = |V|,\nand \\vstar bounds the number of edges that lie on shortest paths through any\nsingle vertex. Our APASP algorithm leads to the same amortized bound for the\nfully dynamic computation of betweenness centrality (BC), which is a parameter\nwidely used in the analysis of large complex networks.\n  Our method is a generalization and a variant of the fully dynamic algorithm\nof Demetrescu and Italiano [DI04] for unique shortest path, and it builds on\nvery recent work on decremental APASP [NPR14]. Our algorithm matches the fully\ndynamic amortized bound in [DI04] for graphs with unique shortest paths, though\nour method, and especially its analysis, are different.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 22:53:27 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 17:08:28 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2015 18:01:18 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Pontecorvi", "Matteo", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1412.3955", "submitter": "Dimitrios Thilikos", "authors": "Petr A. Golovach, Marcin Kami\\'nski, Spyridon Maniatis, Dimitrios M.\n  Thilikos", "title": "The Parameterized Complexity of Graph Cyclability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cyclability of a graph is the maximum integer $k$ for which every $k$\nvertices lie on a cycle. The algorithmic version of the problem, given a graph\n$G$ and a non-negative integer $k,$ decide whether the cyclability of $G$ is at\nleast $k,$ is {\\sf NP}-hard. We study the parametrized complexity of this\nproblem. We prove that this problem, parameterized by $k,$ is ${\\sf\nco\\mbox{-}W[1]}$-hard and that its does not admit a polynomial kernel on planar\ngraphs, unless ${\\sf NP}\\subseteq{\\sf co}\\mbox{-}{\\sf NP}/{\\sf poly}$. On the\npositive side, we give an {\\sf FPT} algorithm for planar graphs that runs in\ntime $2^{2^{O(k^2\\log k)}}\\cdot n^2$. Our algorithm is based on a series of\ngraph-theoretical results on cyclic linkages in planar graphs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 11:43:40 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 14:43:22 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Golovach", "Petr A.", ""], ["Kami\u0144ski", "Marcin", ""], ["Maniatis", "Spyridon", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1412.3976", "submitter": "Takehiro Ito", "authors": "Takehiro Ito, Hirotaka Ono, Yota Otachi", "title": "Reconfiguration of Cliques in a Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study reconfiguration problems for cliques in a graph, which determine\nwhether there exists a sequence of cliques that transforms a given clique into\nanother one in a step-by-step fashion. As one step of a transformation, we\nconsider three different types of rules, which are defined and studied in\nreconfiguration problems for independent sets. We first prove that all the\nthree rules are equivalent in cliques. We then show that the problems are\nPSPACE-complete for perfect graphs, while we give polynomial-time algorithms\nfor several classes of graphs, such as even-hole-free graphs and cographs. In\nparticular, the shortest variant, which computes the shortest length of a\ndesired sequence, can be solved in polynomial time for chordal graphs,\nbipartite graphs, planar graphs, and bounded treewidth graphs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 12:51:03 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Ito", "Takehiro", ""], ["Ono", "Hirotaka", ""], ["Otachi", "Yota", ""]]}, {"id": "1412.4051", "submitter": "Vahan Mkrtchyan", "authors": "Zola Donovan, Vahan Mkrtchyan, K. Subramani", "title": "Complexity issues in some clustering problems in combinatorial circuits", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern integrated circuit is one of the most complex products that has\nbeen engineered to-date. It continues to grow in complexity as the years\nprogress. As a result, very large-scale integrated (VLSI) circuit design now\ninvolves massive design teams employing state-of-the art computer-aided design\n(CAD) tools. One of the oldest, yet most important CAD problems for VLSI\ncircuits is physical design automation, where one needs to compute the best\nphysical layout of millions to billions of circuit components on a tiny silicon\nsurface \\cite{Lim08}. The process of mapping an electronic design to a chip\ninvolves a number of physical design stages, one of which is clustering. In\nthis paper, we focus on problems in clustering which are critical for more\nsustainable chips. The clustering problem in combinatorial circuits alone is a\nsource of multiple models. In particular, we consider the problem of clustering\ncombinatorial circuits for delay minimization, when logic replication is not\nallowed ({\\sc CN}). The problem of delay minimization when logic replication is\nallowed ({\\sc CA}) has been well studied, and is known to be solvable in\npolynomial-time \\cite{Wong1}. However, unbounded logic replication can be quite\nexpensive. Thus, {\\sc CN} is an important problem. We show that selected\nvariants of {\\sc CN} are {\\bf NP-hard}. We also obtain approximability and\ninapproximability results for these problems. A preliminary version of this\npaper appeared in \\cite{Don15}.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 18:03:48 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 13:19:42 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Donovan", "Zola", ""], ["Mkrtchyan", "Vahan", ""], ["Subramani", "K.", ""]]}, {"id": "1412.4062", "submitter": "Irena Rusu Ph.D.", "authors": "Irena Rusu", "title": "Permutation Reconstruction from MinMax-Betweenness Constraints", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the reconstruction of permutations on {1, 2,\n..., n} from betweenness constraints involving the minimum and the maximum\nelement located between t and t+1, for all t=1, 2, ..., n-1. We propose two\nvariants of the problem (directed and undirected), and focus first on the\ndirected version, for which we draw up general features and design a polynomial\nalgorithm in a particular case. Then, we investigate necessary and sufficient\nconditions for the uniqueness of the reconstruction in both directed and\nundirected versions, using a parameter k whose variation controls the\nstringency of the betweenness constraints. We finally point out open problems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 17:27:15 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Rusu", "Irena", ""]]}, {"id": "1412.4076", "submitter": "Mareike Fischer", "authors": "Steven Kelk and Mareike Fischer", "title": "On the complexity of computing MP distance between binary phylogenetic\n  trees", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the field of phylogenetics there is great interest in distance\nmeasures to quantify the dissimilarity of two trees. Recently, a new distance\nmeasure has been proposed: the Maximum Parsimony (MP) distance. This is based\non the difference of the parsimony scores of a single character on both trees\nunder consideration, and the goal is to find the character which maximizes this\ndifference. Here we show that computation of MP distance on two \\emph{binary}\nphylogenetic trees is NP-hard. This is a highly nontrivial extension of an\nearlier NP-hardness proof for two multifurcating phylogenetic trees, and it is\nparticularly relevant given the prominence of binary trees in the phylogenetics\nliterature. As a corollary to the main hardness result we show that computation\nof MP distance is also hard on binary trees if the number of states available\nis bounded. In fact, via a different reduction we show that it is hard even if\nonly two states are available. Finally, as a first response to this hardness we\ngive a simple Integer Linear Program (ILP) formulation which is capable of\ncomputing the MP distance exactly for small trees (and for larger trees when\nonly a small number of character states are available) and which is used to\ncomputationally verify several auxiliary results required by the hardness\nproofs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 18:24:35 GMT"}, {"version": "v2", "created": "Sun, 18 Jan 2015 09:54:03 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Kelk", "Steven", ""], ["Fischer", "Mareike", ""]]}, {"id": "1412.4088", "submitter": "Daniel Roche", "authors": "Andrew Arnold, Mark Giesbrecht, Daniel S. Roche", "title": "Faster Sparse Multivariate Polynomial Interpolation of Straight-Line\n  Programs", "comments": "33 pages. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Given a straight-line program whose output is a polynomial function of the\ninputs, we present a new algorithm to compute a concise representation of that\nunknown function. Our algorithm can handle any case where the unknown function\nis a multivariate polynomial, with coefficients in an arbitrary finite field,\nand with a reasonable number of nonzero terms but possibly very large degree.\nIt is competitive with previously known sparse interpolation algorithms that\nwork over an arbitrary finite field, and provides an improvement when there are\na large number of variables.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 19:12:15 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 18:44:05 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Arnold", "Andrew", ""], ["Giesbrecht", "Mark", ""], ["Roche", "Daniel S.", ""]]}, {"id": "1412.4273", "submitter": "Maciej Drwal", "authors": "Maciej Drwal, Roman Rischke", "title": "Complexity of interval minmax regret scheduling on parallel identical\n  machines with total completion time criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of scheduling jobs on parallel\nidentical machines, where the processing times of jobs are uncertain: only\ninterval bounds of processing times are known. The optimality criterion of a\nschedule is the total completion time. In order to cope with the uncertainty,\nwe consider the maximum regret objective and we seek a schedule that performs\nwell under all possible instantiations of processing times. Although the\ndeterministic version of the considered problem is solvable in polynomial time,\nthe minmax regret version is known to be weakly NP-hard even for a single\nmachine, and strongly NP-hard for parallel unrelated machines. In this paper,\nwe show that the problem is strongly NP-hard also in the case of parallel\nidentical machines.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 19:18:48 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 09:15:05 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 11:57:03 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Drwal", "Maciej", ""], ["Rischke", "Roman", ""]]}, {"id": "1412.4471", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov", "title": "Online Detection of Repetitions with Backtracking", "comments": "12 pages, 5 figures, accepted to CPM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two algorithms for the following problem: given a\nstring and a rational $e > 1$, detect in the online fashion the earliest\noccurrence of a repetition of exponent $\\ge e$ in the string.\n  1. The first algorithm supports the backtrack operation removing the last\nletter of the input string. This solution runs in $O(n\\log m)$ time and $O(m)$\nspace, where $m$ is the maximal length of a string generated during the\nexecution of a given sequence of $n$ read and backtrack operations.\n  2. The second algorithm works in $O(n\\log\\sigma)$ time and $O(n)$ space,\nwhere $n$ is the length of the input string and $\\sigma$ is the number of\ndistinct letters. This algorithm is relatively simple and requires much less\nmemory than the previously known solution with the same working time and space.\na string generated during the execution of a given sequence of $n$ read and\nbacktrack operations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 06:13:22 GMT"}, {"version": "v2", "created": "Sun, 1 Feb 2015 18:02:46 GMT"}, {"version": "v3", "created": "Fri, 1 May 2015 12:14:47 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Kosolobov", "Dmitry", ""]]}, {"id": "1412.4646", "submitter": "Robert Merca\\c{s}", "authors": "Maxime Crochemore and Robert Mercas", "title": "Fewer runs than word length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work takes another look at the number of runs that a string might contain\nand provides an alternative proof for the bound. We also propose another\nstronger conjecture that states that, for a fixed order on the alphabet, within\nevery factor of a word there are at most as many occurrences of Lyndon roots\ncorresponding to runs in a word as the length of the factor (only first such\noccurrences for each run are considered).\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 15:54:46 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 06:49:32 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2015 08:06:19 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Crochemore", "Maxime", ""], ["Mercas", "Robert", ""]]}, {"id": "1412.4709", "submitter": "Lehilton Pedrosa", "authors": "Fl\\'avio K. Miyazawa, Lehilton L. C. Pedrosa, Rafael C. S. Schouery,\n  Maxim Sviridenko, Yoshiko Wakabayashi", "title": "Polynomial-Time Approximation Schemes for Circle and Other Packing\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an asymptotic approximation scheme (APTAS) for the problem of packing\na set of circles into a minimum number of unit square bins. To obtain rational\nsolutions, we use augmented bins of height $1+\\gamma$, for some arbitrarily\nsmall number $\\gamma > 0$. Our algorithm is polynomial on $\\log 1/\\gamma$, and\nthus $\\gamma$ is part of the problem input. For the special case that $\\gamma$\nis constant, we give a (one dimensional) resource augmentation scheme, that is,\nwe obtain a packing into bins of unit width and height $1+\\gamma$ using no more\nthan the number of bins in an optimal packing. Additionally, we obtain an APTAS\nfor the circle strip packing problem, whose goal is to pack a set of circles\ninto a strip of unit width and minimum height. These are the first\napproximation and resource augmentation schemes for these problems.\n  Our algorithm is based on novel ideas of iteratively separating small and\nlarge items, and may be extended to a wide range of packing problems that\nsatisfy certain conditions. These extensions comprise problems with different\nkinds of items, such as regular polygons, or with bins of different shapes,\nsuch as circles and spheres. As an example, we obtain APTAS's for the problems\nof packing d-dimensional spheres into hypercubes under the $L_p$-norm.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 18:27:32 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Miyazawa", "Fl\u00e1vio K.", ""], ["Pedrosa", "Lehilton L. C.", ""], ["Schouery", "Rafael C. S.", ""], ["Sviridenko", "Maxim", ""], ["Wakabayashi", "Yoshiko", ""]]}, {"id": "1412.4738", "submitter": "EPTCS", "authors": "James Caldwell (University of Wyoming), Philip H\\\"olzenspies\n  (University of Twente), Peter Achten (Radboud University Nijmegen)", "title": "Proceedings 3rd International Workshop on Trends in Functional\n  Programming in Education", "comments": null, "journal-ref": "EPTCS 170, 2014", "doi": "10.4204/EPTCS.170", "report-no": null, "categories": "cs.CY cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of TFPIE is to gather researchers, professors, teachers, and all\nprofessionals interested in functional programming in education. This includes\nthe teaching of functional programming, but also the application of functional\nprogramming as a tool for teaching other topics. The post-workshop review\nprocess received 13 submissions, which were vetted by the program committee,\nassuming scientific journal standards of publication. The six articles in this\nvolume were selected for publication as the result of this process.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 00:47:22 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Caldwell", "James", "", "University of Wyoming"], ["H\u00f6lzenspies", "Philip", "", "University of Twente"], ["Achten", "Peter", "", "Radboud University Nijmegen"]]}, {"id": "1412.4882", "submitter": "EPTCS", "authors": "Prabhakar Ragde (University of Waterloo, Waterloo, Ontario, Canada)", "title": "Simple Balanced Binary Search Trees", "comments": "In Proceedings TFPIE 2014, arXiv:1412.4738", "journal-ref": "EPTCS 170, 2014, pp. 78-87", "doi": "10.4204/EPTCS.170.6", "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient implementations of sets and maps (dictionaries) are important in\ncomputer science, and balanced binary search trees are the basis of the best\npractical implementations. Pedagogically, however, they are often quite\ncomplicated, especially with respect to deletion. I present complete code (with\njustification and analysis not previously available in the literature) for a\npurely-functional implementation based on AA trees, which is the simplest\ntreatment of the subject of which I am aware.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 05:14:44 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Ragde", "Prabhakar", "", "University of Waterloo, Waterloo, Ontario, Canada"]]}, {"id": "1412.5010", "submitter": "Jens Ma{\\ss}berg", "authors": "Jens Ma{\\ss}berg", "title": "The rectilinear Steiner tree problem with given topology and length\n  restrictions", "comments": "14 pages", "journal-ref": "Computing and Combinatorics, Lecture Notes in Computer Science,\n  Volume 9198, 2015, pp 445-456", "doi": "10.1007/978-3-319-21398-9_35", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of embedding the Steiner points of a Steiner tree\nwith given topology into the rectilinear plane. Thereby, the length of the path\nbetween a distinguished terminal and each other terminal must not exceed given\nlength restrictions. We want to minimize the total length of the tree.\n  The problem can be formulated as a linear program and therefore it is\nsolvable in polynomial time. In this paper we analyze the structure of feasible\nembeddings and give a combinatorial polynomial time algorithm for the problem.\nOur algorithm combines a dynamic programming approach and binary search and\nrelies on the total unimodularity of a matrix appearing in a sub-problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 14:21:39 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Ma\u00dfberg", "Jens", ""]]}, {"id": "1412.5075", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "$k$-best enumeration", "comments": "17 pages. A significantly shorter version of this material appears in\n  the Springer Encyclopedia of Algorithms, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey $k$-best enumeration problems and the algorithms for solving them,\nincluding in particular the problems of finding the $k$ shortest paths, $k$\nsmallest spanning trees, and $k$ best matchings in weighted graphs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 16:49:08 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1412.5149", "submitter": "Aubrey Alston", "authors": "Aubrey Alston", "title": "Polynomial-time Method of Determining Subset Sum Solutions", "comments": "This version (now defunct) contains quite a few errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the conditions under which a given set satisfies the stipulations of\nthe subset sum proposition to a set of linear relationships, the question of\nwhether a set satisfies subset sum may be answered in a polynomial number of\nsteps by forming, solving, and constraining a series of linear systems whose\ndimensions and number are both polynomial with respect to the length of the\nset. Given its demonstrated 100% accuracy rate and its demonstrable\njustification, this algorithm may provide basis to reconsider the validity of\nSSP-based and NP-hard-reliant cryptosystems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 20:10:34 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 20:50:53 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 16:28:16 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Alston", "Aubrey", ""]]}, {"id": "1412.5249", "submitter": "Catherine Greenhill", "authors": "Catherine Greenhill", "title": "The switch Markov chain for sampling irregular graphs", "comments": "9 pages, 5 figures, to appear in proceedings of SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of efficiently sampling from a set of(undirected) graphs with a\ngiven degree sequence has many applications. One approach to this problem uses\na simple Markov chain, which we call the switch chain, to perform the sampling.\nThe switch chain is known to be rapidly mixing for regular degree sequences. We\nprove that the switch chain is rapidly mixing for any degree sequence with\nminimum degree at least 1 and with maximum degree $d_{\\max}$ which satisfies\n$3\\leq d_{\\max}\\leq \\frac{1}{4}\\, \\sqrt{M}$, where $M$ is the sum of the\ndegrees. The mixing time bound obtained is only an order of $n$ larger than\nthat established in the regular case, where $n$ is the number of vertices.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 03:33:37 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Greenhill", "Catherine", ""]]}, {"id": "1412.5302", "submitter": "Michael Codish", "authors": "Daniel Bundala, Michael Codish, Lu\\'is Cruz-Filipe, Peter\n  Schneider-Kamp, Jakub Z\\'avodn\\'y", "title": "Optimal-Depth Sorting Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcss.2016.09.004", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve a 40-year-old open problem on the depth optimality of sorting\nnetworks. In 1973, Donald E. Knuth detailed, in Volume 3 of \"The Art of\nComputer Programming\", sorting networks of the smallest depth known at the time\nfor n =< 16 inputs, quoting optimality for n =< 8. In 1989, Parberry proved the\noptimality of the networks with 9 =< n =< 10 inputs. In this article, we\npresent a general technique for obtaining such optimality results, and use it\nto prove the optimality of the remaining open cases of 11 =< n =< 16 inputs. We\nshow how to exploit symmetry to construct a small set of two-layer networks on\nn inputs such that if there is a sorting network on n inputs of a given depth,\nthen there is one whose first layers are in this set. For each network in the\nresulting set, we construct a propositional formula whose satisfiability is\nnecessary for the existence of a sorting network of a given depth. Using an\noff-the-shelf SAT solver we show that the sorting networks listed by Knuth are\noptimal. For n =< 10 inputs, our algorithm is orders of magnitude faster than\nthe prior ones.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 09:36:06 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Bundala", "Daniel", ""], ["Codish", "Michael", ""], ["Cruz-Filipe", "Lu\u00eds", ""], ["Schneider-Kamp", "Peter", ""], ["Z\u00e1vodn\u00fd", "Jakub", ""]]}, {"id": "1412.5381", "submitter": "Heiko R\\\"oglin", "authors": "Tobias Brunsch, Anna Gro{\\ss}wendt, Heiko R\\\"oglin", "title": "Solving Totally Unimodular LPs with the Shadow Vertex Algorithm", "comments": "to be presented at STACS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the shadow vertex simplex algorithm can be used to solve linear\nprograms in strongly polynomial time with respect to the number $n$ of\nvariables, the number $m$ of constraints, and $1/\\delta$, where $\\delta$ is a\nparameter that measures the flatness of the vertices of the polyhedron. This\nextends our recent result that the shadow vertex algorithm finds paths of\npolynomial length (w.r.t. $n$, $m$, and $1/\\delta$) between two given vertices\nof a polyhedron.\n  Our result also complements a recent result due to Eisenbrand and Vempala who\nhave shown that a certain version of the random edge pivot rule solves linear\nprograms with a running time that is strongly polynomial in the number of\nvariables $n$ and $1/\\delta$, but independent of the number $m$ of constraints.\nEven though the running time of our algorithm depends on $m$, it is\nsignificantly faster for the important special case of totally unimodular\nlinear programs, for which $1/\\delta\\le n$ and which have only $O(n^2)$\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 13:33:23 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Brunsch", "Tobias", ""], ["Gro\u00dfwendt", "Anna", ""], ["R\u00f6glin", "Heiko", ""]]}, {"id": "1412.5484", "submitter": "Sheela Devadas", "authors": "Sheela Devadas, Ronitt Rubinfeld", "title": "A Self-Tester for Linear Functions over the Integers with an Elementary\n  Proof of Correctness", "comments": null, "journal-ref": null, "doi": "10.1007/s00224-015-9639-z", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple, self-contained proofs of correctness for algorithms for\nlinearity testing and program checking of linear functions on finite subsets of\nintegers represented as n-bit numbers. In addition we explore a generalization\nof self-testing to homomorphisms on a multidimensional vector space. We show\nthat our self-testing algorithm for the univariate case can be directly\ngeneralized to vector space domains. The number of queries made by our\nalgorithms is independent of domain size.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 17:07:47 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 18:59:50 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 22:31:21 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Devadas", "Sheela", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1412.5498", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Christian Komusiewicz and Rolf Niedermeier and\n  Manuel Sorge and Toby Walsh", "title": "H-Index Manipulation by Merging Articles: Models, Theory, and\n  Experiments", "comments": "Manuscript accepted to Artificial Intelligence", "journal-ref": "Artificial Intelligence, 240:19-35, 2016", "doi": "10.1016/j.artint.2016.08.001", "report-no": null, "categories": "cs.DL cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An author's profile on Google Scholar consists of indexed articles and\nassociated data, such as the number of citations and the H-index. The author is\nallowed to merge articles; this may affect the H-index. We analyze the\n(parameterized) computational complexity of maximizing the H-index using\narticle merges. Herein, to model realistic manipulation scenarios, we define a\ncompatibility graph whose edges correspond to plausible merges. Moreover, we\nconsider several different measures for computing the citation count of a\nmerged article. For the measure used by Google Scholar, we give an algorithm\nthat maximizes the H-index in linear time if the compatibility graph has\nconstant-size connected components. In contrast, if we allow to merge arbitrary\narticles (that is, for compatibility graphs that are cliques), then already\nincreasing the H-index by one is NP-hard. Experiments on Google Scholar\nprofiles of AI researchers show that the H-index can be manipulated\nsubstantially only if one merges articles with highly dissimilar titles.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 17:39:53 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 16:00:58 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2016 06:09:48 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Komusiewicz", "Christian", ""], ["Niedermeier", "Rolf", ""], ["Sorge", "Manuel", ""], ["Walsh", "Toby", ""]]}, {"id": "1412.5721", "submitter": "Edo Liberty", "authors": "Edo Liberty, Ram Sriharsha, Maxim Sviridenko", "title": "An Algorithm for Online K-Means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that one can be competitive with the k-means objective while\noperating online. In this model, the algorithm receives vectors v_1,...,v_n one\nby one in an arbitrary order. For each vector the algorithm outputs a cluster\nidentifier before receiving the next one. Our online algorithm generates ~O(k)\nclusters whose k-means cost is ~O(W*). Here, W* is the optimal k-means cost\nusing k clusters and ~O suppresses poly-logarithmic factors. We also show that,\nexperimentally, it is not much worse than k-means++ while operating in a\nstrictly more constrained computational model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 05:09:32 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 17:30:23 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Liberty", "Edo", ""], ["Sriharsha", "Ram", ""], ["Sviridenko", "Maxim", ""]]}, {"id": "1412.5932", "submitter": "Guillaume Rizk", "authors": "Ga\\\"etan Benoit, Claire Lemaitre, Dominique Lavenier and Guillaume\n  Rizk", "title": "Compression of high throughput sequencing data with probabilistic de\n  Bruijn graph", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Data volumes generated by next-generation sequencing technolo-\ngies is now a major concern, both for storage and transmission. This triggered\nthe need for more efficient methods than general purpose compression tools,\nsuch as the widely used gzip. Most reference-free tools developed for NGS data\ncompression still use general text compression methods and fail to benefit from\nalgorithms already designed specifically for the analysis of NGS data. The goal\nof our new method Leon is to achieve compression of DNA sequences of high\nthroughput sequencing data, without the need of a reference genome, with\ntechniques derived from existing assembly principles, that possibly better\nexploit NGS data redundancy. Results: We propose a novel method, implemented in\nthe software Leon, for compression of DNA sequences issued from high throughput\nsequencing technologies. This is a lossless method that does not need a\nreference genome. Instead, a reference is built de novo from the set of reads\nas a probabilistic de Bruijn Graph, stored in a Bloom filter. Each read is\nencoded as a path in this graph, storing only an anchoring kmer and a list of\nbifurcations indicating which path to follow in the graph. This new method will\nallow to have compressed read files that also already contain its underlying de\nBruijn Graph, thus directly re-usable by many tools relying on this structure.\nLeon achieved encoding of a C. elegans reads set with 0.7 bits/base,\noutperforming state of the art reference-free methods. Availability: Open\nsource, under GNU affero GPL License, available for download at\nhttp://gatb.inria.fr/software/leon/\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 16:37:12 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Benoit", "Ga\u00ebtan", ""], ["Lemaitre", "Claire", ""], ["Lavenier", "Dominique", ""], ["Rizk", "Guillaume", ""]]}, {"id": "1412.6148", "submitter": "Michael Kallitsis", "authors": "Michael Kallitsis and Stilian Stoev and George Michailidis", "title": "Hashing Pursuit for Online Identification of Heavy-Hitters in High-Speed\n  Network Streams", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Denial of Service (DDoS) attacks have become more prominent\nrecently, both in frequency of occurrence, as well as magnitude. Such attacks\nrender key Internet resources unavailable and disrupt its normal operation. It\nis therefore of paramount importance to quickly identify malicious Internet\nactivity. The DDoS threat model includes characteristics such as: (i)\nheavy-hitters that transmit large volumes of traffic towards \"victims\", (ii)\npersistent-hitters that send traffic, not necessarily large, to specific\ndestinations to be used as attack facilitators, (iii) host and port scanning\nfor compiling lists of un-secure servers to be used as attack amplifiers, etc.\nThis conglomeration of problems motivates the development of space/time\nefficient summaries of data traffic streams that can be used to identify\nheavy-hitters associated with the above attack vectors. This paper presents a\nhashing-based framework and fast algorithms that take into account the\nlarge-dimensionality of the incoming network stream and can be employed to\nquickly identify the culprits. The algorithms and data structures proposed\nprovide a synopsis of the network stream that is not taxing to fast-memory, and\ncan be efficiently implemented in hardware due to simple bit-wise operations.\nThe methods are evaluated using real-world Internet data from a large academic\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 17:09:48 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Kallitsis", "Michael", ""], ["Stoev", "Stilian", ""], ["Michailidis", "George", ""]]}, {"id": "1412.6156", "submitter": "Jiaming Xu", "authors": "Bruce Hajek and Yihong Wu and Jiaming Xu", "title": "Achieving Exact Cluster Recovery Threshold via Semidefinite Programming", "comments": "This paper was accepted to IEEE Transactions on Information Theory on\n  January 3, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary symmetric stochastic block model deals with a random graph of $n$\nvertices partitioned into two equal-sized clusters, such that each pair of\nvertices is connected independently with probability $p$ within clusters and\n$q$ across clusters. In the asymptotic regime of $p=a \\log n/n$ and $q=b \\log\nn/n$ for fixed $a,b$ and $n \\to \\infty$, we show that the semidefinite\nprogramming relaxation of the maximum likelihood estimator achieves the optimal\nthreshold for exactly recovering the partition from the graph with probability\ntending to one, resolving a conjecture of Abbe et al. \\cite{Abbe14}.\nFurthermore, we show that the semidefinite programming relaxation also achieves\nthe optimal recovery threshold in the planted dense subgraph model containing a\nsingle cluster of size proportional to $n$.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 05:19:56 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 23:47:57 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Hajek", "Bruce", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1412.6168", "submitter": "Daniel Dadush", "authors": "Nicolas Bonifas and Daniel Dadush", "title": "Short Paths on the Voronoi Graph and the Closest Vector Problem with\n  Preprocessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving on the Voronoi cell based techniques of Micciancio and Voulgaris\n(SIAM J. Comp. 13), and Sommer, Feder and Shalvi (SIAM J. Disc. Math. 09), we\ngive a Las Vegas $\\tilde{O}(2^n)$ expected time and space algorithm for CVPP\n(the preprocessing version of the Closest Vector Problem, CVP). This improves\non the $\\tilde{O}(4^n)$ deterministic runtime of the Micciancio Voulgaris\nalgorithm, henceforth MV, for CVPP (which also solves CVP) at the cost of a\npolynomial amount of randomness (which only affects runtime, not correctness).\nAs in MV, our algorithm proceeds by computing a short path on the Voronoi graph\nof the lattice, where lattice points are adjacent if their Voronoi cells share\na common facet, from the origin to a closest lattice vector.\n  Our main technical contribution is a randomized procedure that given the\nVoronoi relevant vectors of a lattice - the lattice vectors inducing facets of\nthe Voronoi cell - as preprocessing and any \"close enough\" lattice point to the\ntarget, computes a path to a closest lattice vector of expected polynomial\nsize. This improves on the $\\tilde{O}(4^n)$ path length given by the MV\nalgorithm. Furthermore, as in MV, each edge of the path can be computed using a\nsingle iteration over the Voronoi relevant vectors. As a byproduct of our work,\nwe also give an optimal relationship between geometric and path distance on the\nVoronoi graph, which we believe to be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 22:35:04 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Bonifas", "Nicolas", ""], ["Dadush", "Daniel", ""]]}, {"id": "1412.6170", "submitter": "Francesco Lettich", "authors": "Francesco Lettich, Salvatore Orlando and Claudio Silvestri", "title": "Manycore processing of repeated k-NN queries over massive moving objects\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to timely process significant amounts of continuously updated\nspatial data is mandatory for an increasing number of applications. In this\npaper we focus on a specific data-intensive problem concerning the repeated\nprocessing of huge amounts of k nearest neighbours (k-NN) queries over massive\nsets of moving objects, where the spatial extents of queries and the position\nof objects are continuously modified over time. In particular, we propose a\nnovel hybrid CPU/GPU pipeline that significantly accelerate query processing\nthanks to a combination of ad-hoc data structures and non-trivial memory access\npatterns. To the best of our knowledge this is the first work that exploits\nGPUs to efficiently solve repeated k-NN queries over massive sets of\ncontinuously moving objects, even characterized by highly skewed spatial\ndistributions. In comparison with state-of-the-art sequential CPU-based\nimplementations, our method highlights significant speedups in the order of\n10x-20x, depending on the datasets, even when considering cheap GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 22:43:28 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Lettich", "Francesco", ""], ["Orlando", "Salvatore", ""], ["Silvestri", "Claudio", ""]]}, {"id": "1412.6466", "submitter": "Sebastian Krinninger", "authors": "Monika Henzinger, Sebastian Krinninger, Veronika Loitzenbauer", "title": "Finding 2-Edge and 2-Vertex Strongly Connected Components in Quadratic\n  Time", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-47672-7_58", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present faster algorithms for computing the 2-edge and 2-vertex strongly\nconnected components of a directed graph, which are straightforward\ngeneralizations of strongly connected components. While in undirected graphs\nthe 2-edge and 2-vertex connected components can be found in linear time, in\ndirected graphs only rather simple $O(m n)$-time algorithms were known. We use\na hierarchical sparsification technique to obtain algorithms that run in time\n$O(n^2)$. For 2-edge strongly connected components our algorithm gives the\nfirst running time improvement in 20 years. Additionally we present an $O(m^2 /\n\\log{n})$-time algorithm for 2-edge strongly connected components, and thus\nimprove over the $O(m n)$ running time also when $m = O(n)$. Our approach\nextends to k-edge and k-vertex strongly connected components for any constant k\nwith a running time of $O(n^2 \\log^2 n)$ for edges and $O(n^3)$ for vertices.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 18:02:46 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 15:07:50 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Loitzenbauer", "Veronika", ""]]}, {"id": "1412.6595", "submitter": "Neil McGlohon", "authors": "Stacy Patterson, Neil McGlohon, Kirill Dyagilev", "title": "Efficient, Optimal $k$-Leader Selection for Coherent, One-Dimensional\n  Formations", "comments": "7 pages, 5 figures, submitted to ECC15", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of optimal leader selection in consensus networks with\nnoisy relative information. The objective is to identify the set of $k$ leaders\nthat minimizes the formation's deviation from the desired trajectory\nestablished by the leaders. An optimal leader set can be found by an exhaustive\nsearch over all possible leader sets; however, this approach is not scalable to\nlarge networks. In recent years, several works have proposed approximation\nalgorithms to the $k$-leader selection problem, yet the question of whether\nthere exists an efficient, non-combinatorial method to identify the optimal\nleader set remains open. This work takes a first step towards answering this\nquestion. We show that, in one-dimensional weighted graphs, namely path graphs\nand ring graphs, the $k$-leader selection problem can be solved in polynomial\ntime (in both $k$ and the network size $n$). We give an $O(n^3)$ solution for\noptimal $k$-leader selection in path graphs and an $O(kn^3)$ solution for\noptimal $k$-leader selection in ring graphs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:09:31 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Patterson", "Stacy", ""], ["McGlohon", "Neil", ""], ["Dyagilev", "Kirill", ""]]}, {"id": "1412.6686", "submitter": "Joydeep Banerjee", "authors": "Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and\n  Arunabha Sen", "title": "On the Entity Hardening Problem in Multi-layered Interdependent Networks", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power grid and the communication network are highly interdependent on\neach other for their well being. In recent times the research community has\nshown significant interest in modeling such interdependent networks and\nstudying the impact of failures on these networks. Although a number of models\nhave been proposed, many of them are simplistic in nature and fail to capture\nthe complex interdependencies that exist between the entities of these\nnetworks. To overcome the limitations, recently an Implicative Interdependency\nModel that utilizes Boolean Logic, was proposed and a number of problems were\nstudied. In this paper we study the entity hardening problem, where by entity\nhardening we imply the ability of the network operator to ensure that an\nadversary (be it Nature or human) cannot take a network entity from operative\nto inoperative state. Given that the network operator with a limited budget can\nonly harden k entities, the goal of the entity hardening problem is to identify\nthe set of k entities whose hardening will ensure maximum benefit for the\noperator, i.e. maximally reduce the ability of the adversary to degrade the\nnetwork. We show that the problem is solvable in polynomial time for some\ncases, whereas for others it is NP-complete. We provide the optimal solution\nusing ILP, and propose a heuristic approach to solve the problem. We evaluate\nthe efficacy of our heuristic using power and communication network data of\nMaricopa County, Arizona. The experiments show that our heuristic almost always\nproduces near optimal results.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 19:51:03 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Banerjee", "Joydeep", ""], ["Das", "Arun", ""], ["Zhou", "Chenyang", ""], ["Mazumder", "Anisha", ""], ["Sen", "Arunabha", ""]]}, {"id": "1412.6705", "submitter": "Daniel Dadush", "authors": "Daniel Dadush and Nicolai H\\\"ahnle", "title": "On the Shadow Simplex Method for Curved Polyhedra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the simplex method over polyhedra satisfying certain \"discrete\ncurvature\" lower bounds, which enforce that the boundary always meets vertices\nat sharp angles. Motivated by linear programs with totally unimodular\nconstraint matrices, recent results of Bonifas et al (SOCG 2012), Brunsch and\nR\\\"oglin (ICALP 2013), and Eisenbrand and Vempala (2014) have improved our\nunderstanding of such polyhedra.\n  We develop a new type of dual analysis of the shadow simplex method which\nprovides a clean and powerful tool for improving all previously mentioned\nresults. Our methods are inspired by the recent work of Bonifas and the first\nnamed author (SODA 2015), who analyzed a remarkably similar process as part of\nan algorithm for the Closest Vector Problem with Preprocessing.\n  For our first result, we obtain a constructive diameter bound of\n$O(\\frac{n^2}{\\delta} \\ln \\frac{n}{\\delta})$ for $n$-dimensional polyhedra with\ncurvature parameter $\\delta \\in [0,1]$. For the class of polyhedra arising from\ntotally unimodular constraint matrices, this implies a bound of $O(n^3 \\ln n)$.\nFor linear optimization, given an initial feasible vertex, we show that an\noptimal vertex can be found using an expected $O(\\frac{n^3}{\\delta} \\ln\n\\frac{n}{\\delta})$ simplex pivots, each requiring $O(m n)$ time to compute. An\ninitial feasible solution can be found using $O(\\frac{m n^3}{\\delta} \\ln\n\\frac{n}{\\delta})$ pivot steps.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 23:19:09 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Dadush", "Daniel", ""], ["H\u00e4hnle", "Nicolai", ""]]}, {"id": "1412.6755", "submitter": "Tobias M\\\"omke", "authors": "Tobias M\\\"omke", "title": "An Improved Approximation Algorithm for the Traveling Salesman Problem\n  with Relaxed Triangle Inequality", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a complete edge-weighted graph G, we present a polynomial time\nalgorithm to compute a degree-four-bounded spanning Eulerian subgraph of 2G\nthat has at most 1.5 times the weight of an optimal TSP solution of G. Based on\nthis algorithm and a novel use of orientations in graphs, we obtain a (3 beta/4\n+ 3 beta^2/4)-approximation algorithm for TSP with beta-relaxed triangle\ninequality (beta-TSP), where beta >= 1. A graph G is an instance of beta-TSP,\nif it is a complete graph with non-negative edge weights that are restricted as\nfollows. For each triple of vertices u,v,w in V(G), c({u,v}) <= beta (c({u,w})\n+ c({w,v})).\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 10:05:22 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["M\u00f6mke", "Tobias", ""]]}, {"id": "1412.6935", "submitter": "Markus Jalsenius", "authors": "Raphael Clifford, Markus Jalsenius, Benjamin Sach", "title": "Time Bounds for Streaming Problems", "comments": "31 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1207.1885", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give tight cell-probe bounds for the time to compute convolution,\nmultiplication and Hamming distance in a stream. The cell probe model is a\nparticularly strong computational model and subsumes, for example, the popular\nword RAM model.\n  We first consider online convolution where the task is to output the inner\nproduct between a fixed $n$-dimensional vector and a vector of the $n$ most\nrecent values from a stream. One symbol of the stream arrives at a time and the\neach output must be computed before the next symbols arrives.\n  Next we show bounds for online multiplication where the stream consists of\npairs of digits, one from each of two $n$ digit numbers that are to be\nmultiplied. One pair arrives at a time and the task is to output a single new\ndigit from the product before the next pair of digits arrives.\n  Finally we look at the online Hamming distance problem where the Hamming\ndistance is outputted instead of the inner product.\n  For each of these three problems, we give a lower bound of\n$\\Omega(\\frac{\\delta}{w}\\log n)$ time on average per output, where $\\delta$ is\nthe number of bits needed to represent an input symbol and $w$ is the cell or\nword size. We argue that these bound are in fact tight within the cell probe\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 11:35:53 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Clifford", "Raphael", ""], ["Jalsenius", "Markus", ""], ["Sach", "Benjamin", ""]]}, {"id": "1412.7116", "submitter": "Saghar Hosseini", "authors": "Saghar Hosseini, Airlie Chapman, and Mehran Mesbahi", "title": "Online Distributed ADMM on Networks", "comments": "Submitted to The IEEE Transactions on Control of Network Systems,\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines online distributed Alternating Direction Method of\nMultipliers (ADMM). The goal is to distributively optimize a global objective\nfunction over a network of decision makers under linear constraints. The global\nobjective function is composed of convex cost functions associated with each\nagent. The local cost functions, on the other hand, are assumed to have been\ndecomposed into two distinct convex functions, one of which is revealed to the\ndecision makers over time and one known a priori. In addition, the agents must\nachieve consensus on the global variable that relates to the private local\nvariables via linear constraints. In this work, we extend online ADMM to a\ndistributed setting based on dual-averaging and distributed gradient descent.\nWe then propose a performance metric for such online distributed algorithms and\nexplore the performance of the sequence of decisions generated by the algorithm\nas compared with the best fixed decision in hindsight. This performance metric\nis called the social regret. A sub-linear upper bound on the social regret of\nthe proposed algorithm is then obtained that underscores the role of the\nunderlying network topology and certain condition measures associated with the\nlinear constraints. The online distributed ADMM algorithm is then applied to a\nformation acquisition problem demonstrating the application of the proposed\nsetup in distributed robotics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 19:55:56 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 19:59:48 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Hosseini", "Saghar", ""], ["Chapman", "Airlie", ""], ["Mesbahi", "Mehran", ""]]}, {"id": "1412.7215", "submitter": "Saghar Hosseini", "authors": "Saghar Hosseini, Airlie Chapman, and Mehran Mesbahi", "title": "Online Distributed Optimization on Dynamic Networks", "comments": "Submitted to The IEEE Transactions on Automatic Control, 2014", "journal-ref": null, "doi": "10.1109/TAC.2016.2525928", "report-no": null, "categories": "math.OC cs.DS cs.LG cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a distributed optimization scheme over a network of\nagents in the presence of cost uncertainties and over switching communication\ntopologies. Inspired by recent advances in distributed convex optimization, we\npropose a distributed algorithm based on a dual sub-gradient averaging. The\nobjective of this algorithm is to minimize a cost function cooperatively.\nFurthermore, the algorithm changes the weights on the communication links in\nthe network to adapt to varying reliability of neighboring agents. A\nconvergence rate analysis as a function of the underlying network topology is\nthen presented, followed by simulation results for representative classes of\nsensor networks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 23:57:30 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Hosseini", "Saghar", ""], ["Chapman", "Airlie", ""], ["Mesbahi", "Mehran", ""]]}, {"id": "1412.7219", "submitter": "Tuomo Lempi\\\"ainen", "authors": "Mika G\\\"o\\\"os, Tuomo Lempi\\\"ainen, Eugen Czeizler, Pekka Orponen", "title": "Search Methods for Tile Sets in Patterned DNA Self-Assembly", "comments": "1 + 36 pages, 18 figures. arXiv admin note: text overlap with\n  arXiv:0911.2924", "journal-ref": "J. Comput. Syst. Sci. 80 (2014) 297-319", "doi": "10.1016/j.jcss.2013.08.003", "report-no": null, "categories": "cs.ET cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Pattern self-Assembly Tile set Synthesis (PATS) problem, which arises in\nthe theory of structured DNA self-assembly, is to determine a set of coloured\ntiles that, starting from a bordering seed structure, self-assembles to a given\nrectangular colour pattern. The task of finding minimum-size tile sets is known\nto be NP-hard. We explore several complete and incomplete search techniques for\nfinding minimal, or at least small, tile sets and also assess the reliability\nof the solutions obtained according to the kinetic Tile Assembly Model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 00:13:26 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["G\u00f6\u00f6s", "Mika", ""], ["Lempi\u00e4inen", "Tuomo", ""], ["Czeizler", "Eugen", ""], ["Orponen", "Pekka", ""]]}, {"id": "1412.7335", "submitter": "Seyoung Yun", "authors": "Se-Young Yun and Alexandre Proutiere", "title": "Accurate Community Detection in the Stochastic Block Model via Spectral\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of community detection in the Stochastic Block Model\nwith a finite number $K$ of communities of sizes linearly growing with the\nnetwork size $n$. This model consists in a random graph such that each pair of\nvertices is connected independently with probability $p$ within communities and\n$q$ across communities. One observes a realization of this random graph, and\nthe objective is to reconstruct the communities from this observation. We show\nthat under spectral algorithms, the number of misclassified vertices does not\nexceed $s$ with high probability as $n$ grows large, whenever $pn=\\omega(1)$,\n$s=o(n)$ and \\begin{equation*} \\lim\\inf_{n\\to\\infty} {n(\\alpha_1 p+\\alpha_2\nq-(\\alpha_1 + \\alpha_2)p^{\\frac{\\alpha_1}{\\alpha_1 +\n\\alpha_2}}q^{\\frac{\\alpha_2}{\\alpha_1 + \\alpha_2}})\\over \\log (\\frac{n}{s})}\n>1,\\quad\\quad(1) \\end{equation*} where $\\alpha_1$ and $\\alpha_2$ denote the\n(fixed) proportions of vertices in the two smallest communities. In view of\nrecent work by Abbe et al. and Mossel et al., this establishes that the\nproposed spectral algorithms are able to exactly recover communities whenever\nthis is at all possible in the case of networks with two communities with equal\nsizes. We conjecture that condition (1) is actually necessary to obtain less\nthan $s$ misclassified vertices asymptotically, which would establish the\noptimality of spectral method in more general scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 12:12:48 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Yun", "Se-Young", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1412.7366", "submitter": "Stefan Hougardy", "authors": "Judith Brecklinghaus and Stefan Hougardy", "title": "The Approximation Ratio of the Greedy Algorithm for the Metric Traveling\n  Salesman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the approximation ratio of the greedy algorithm for the metric\nTraveling Salesman Problem is $\\Theta(\\log n)$. Moreover, we prove that the\nsame result also holds for graphic, Euclidean, and rectilinear instances of the\nTraveling Salesman Problem. Finally we show that the approximation ratio of the\nClarke-Wright savings heuristic for the metric Traveling Salesman Problem is\n$\\Theta(\\log n)$.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 14:06:44 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Brecklinghaus", "Judith", ""], ["Hougardy", "Stefan", ""]]}, {"id": "1412.7399", "submitter": "Colin Benjamin", "authors": "Namit Anand, Colin Benjamin", "title": "Do quantum strategies always win?", "comments": "12 pages, 3 figures, expanded with material on general quantum\n  unitaries and discussion on gaming the quantum", "journal-ref": "Quantum Information Processing, Volume 14, issue 11, pp 4027-4038\n  (November 2015)", "doi": "10.1007/s11128-015-1105-y", "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.DS physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper, Meyer [David Meyer, Phys. Rev. Lett. 82, 1052 (1999)]\ndescribed the advantages of quantum game theory by looking at the classical\npenny flip game. A player using a quantum strategy can win against a classical\nplayer almost 100\\% of the time. Here we make a slight modification to the\nquantum game, with the two players sharing an entangled state to begin with. We\nthen analyze two different scenarios, first in which quantum player makes\nunitary transformations to his qubit while the classical player uses a pure\nstrategy of either flipping or not flipping the state of his qubit. In this\ncase the quantum player always wins against the classical player. In the second\nscenario we have the quantum player making similar unitary transformations\nwhile the classical player makes use of a mixed strategy wherein he either\nflips or not with some probability \"p\". We show that in the second scenario,\n100\\% win record of a quantum player is drastically reduced and for a\nparticular probability \"p\" the classical player can even win against the\nquantum player. This is of possible relevance to the field of quantum\ncomputation as we show that in this quantum game of preserving versus\ndestroying entanglement a particular classical algorithm can beat the quantum\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 15:17:45 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 15:31:50 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2015 09:04:47 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Anand", "Namit", ""], ["Benjamin", "Colin", ""]]}, {"id": "1412.7558", "submitter": "P{\\aa}l Gr{\\o}n{\\aa}s Drange", "authors": "P{\\aa}l Gr{\\o}n{\\aa}s Drange and Micha{\\l} Pilipczuk", "title": "A Polynomial Kernel for Trivially Perfect Editing", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a kernel with $O(k^7)$ vertices for Trivially Perfect Editing, the\nproblem of adding or removing at most $k$ edges in order to make a given graph\ntrivially perfect. This answers in affirmative an open question posed by Nastos\nand Gao, and by Liu et al. Our general technique implies also the existence of\nkernels of the same size for the related problems Trivially Perfect Completion\nand Trivially Perfect Deletion. Whereas for the former an $O(k^3)$ kernel was\ngiven by Guo, for the latter no polynomial kernel was known.\n  We complement our study of Trivially Perfect Editing by proving that,\ncontrary to Trivially Perfect Completion, it cannot be solved in time $2^{o(k)}\n\\cdot n^{O(1)}$ unless the Exponential Time Hypothesis fails. In this manner we\ncomplete the picture of the parameterized and kernelization complexity of the\nclassic edge modification problems for the class of trivially perfect graphs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 21:53:32 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Drange", "P\u00e5l Gr\u00f8n\u00e5s", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1412.7693", "submitter": "Anupam Gupta", "authors": "Anupam Gupta and Amit Kumar", "title": "Greedy Algorithms for Steiner Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Steiner Forest problem, we are given terminal pairs $\\{s_i, t_i\\}$,\nand need to find the cheapest subgraph which connects each of the terminal\npairs together. In 1991, Agrawal, Klein, and Ravi, and Goemans and Williamson\ngave primal-dual constant-factor approximation algorithms for this problem;\nuntil now, the only constant-factor approximations we know are via linear\nprogramming relaxations.\n  We consider the following greedy algorithm: Given terminal pairs in a metric\nspace, call a terminal \"active\" if its distance to its partner is non-zero.\nPick the two closest active terminals (say $s_i, t_j$), set the distance\nbetween them to zero, and buy a path connecting them. Recompute the metric, and\nrepeat. Our main result is that this algorithm is a constant-factor\napproximation.\n  We also use this algorithm to give new, simpler constructions of cost-sharing\nschemes for Steiner forest. In particular, the first \"group-strict\" cost-shares\nfor this problem implies a very simple combinatorial sampling-based algorithm\nfor stochastic Steiner forest.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 15:48:24 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Gupta", "Anupam", ""], ["Kumar", "Amit", ""]]}, {"id": "1412.7834", "submitter": "Daniel Karapetyan Dr", "authors": "Daniel Karapetyan and Andrei Gagarin and Gregory Gutin", "title": "Pattern Backtracking Algorithm for the Workflow Satisfiability Problem", "comments": "9th International Frontiers of Algorithmics Workshop (FAW 2015), 3-5\n  July 2015, Guilin, Guangxi, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The workflow satisfiability problem (WSP) asks whether there exists an\nassignment of authorised users to the steps in a workflow specification,\nsubject to certain constraints on the assignment. (Such an assignment is called\nvalid.) The problem is NP-hard even when restricted to the large class of\nuser-independent constraints. Since the number of steps $k$ is relatively small\nin practice, it is natural to consider a parametrisation of the WSP by $k$. We\npropose a new fixed-parameter algorithm to solve the WSP with user-independent\nconstraints. The assignments in our method are partitioned into equivalence\nclasses such that the number of classes is exponential in $k$ only. We show\nthat one can decide, in polynomial time, whether there is a valid assignment in\nan equivalence class. By exploiting this property, our algorithm reduces the\nsearch space to the space of equivalence classes, which it browses within a\nbacktracking framework, hence emerging as an efficient yet relatively\nsimple-to-implement or generalise solution method. We empirically evaluate our\nalgorithm against the state-of-the-art methods and show that it clearly wins\nthe competition on the whole range of our test problems and significantly\nextends the domain of practically solvable instances of the WSP.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 16:24:25 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 18:10:45 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 23:16:35 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Karapetyan", "Daniel", ""], ["Gagarin", "Andrei", ""], ["Gutin", "Gregory", ""]]}, {"id": "1412.7860", "submitter": "Christopher Tucker Ph.D.", "authors": "Christopher A. Tucker", "title": "A self-organizing geometric algorithm for autonomous data partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of a geometric algorithm is introduced and methodology of its\noperation is presented for the dynamic partitioning of data spaces.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 20:32:32 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Tucker", "Christopher A.", ""]]}, {"id": "1412.7955", "submitter": "Santosh Vempala", "authors": "Christos H. Papadimitriou and Santosh S. Vempala", "title": "Unsupervised Learning through Prediction in a Model of Cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a primitive called PJOIN, for \"predictive join,\" which combines\nand extends the operations JOIN and LINK, which Valiant proposed as the basis\nof a computational theory of cortex. We show that PJOIN can be implemented in\nValiant's model. We also show that, using PJOIN, certain reasonably complex\nlearning and pattern matching tasks can be performed, in a way that involves\nphenomena which have been observed in cognition and the brain, namely\nmemory-based prediction and downward traffic in the cortical hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 16:41:04 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Papadimitriou", "Christos H.", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1412.7994", "submitter": "Noah Stephens-Davidowitz", "authors": "Divesh Aggarwal and Daniel Dadush and Oded Regev and Noah\n  Stephens-Davidowitz", "title": "Solving the Shortest Vector Problem in $2^n$ Time via Discrete Gaussian\n  Sampling", "comments": null, "journal-ref": "STOC 2015", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a randomized $2^{n+o(n)}$-time and space algorithm for solving the\nShortest Vector Problem (SVP) on n-dimensional Euclidean lattices. This\nimproves on the previous fastest algorithm: the deterministic\n$\\widetilde{O}(4^n)$-time and $\\widetilde{O}(2^n)$-space algorithm of\nMicciancio and Voulgaris (STOC 2010, SIAM J. Comp. 2013).\n  In fact, we give a conceptually simple algorithm that solves the (in our\nopinion, even more interesting) problem of discrete Gaussian sampling (DGS).\nMore specifically, we show how to sample $2^{n/2}$ vectors from the discrete\nGaussian distribution at any parameter in $2^{n+o(n)}$ time and space. (Prior\nwork only solved DGS for very large parameters.) Our SVP result then follows\nfrom a natural reduction from SVP to DGS. We also show that our DGS algorithm\nimplies a $2^{n + o(n)}$-time algorithm that approximates the Closest Vector\nProblem to within a factor of $1.97$.\n  In addition, we give a more refined algorithm for DGS above the so-called\nsmoothing parameter of the lattice, which can generate $2^{n/2}$ discrete\nGaussian samples in just $2^{n/2+o(n)}$ time and space. Among other things,\nthis implies a $2^{n/2+o(n)}$-time and space algorithm for $1.93$-approximate\ndecision SVP.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 21:36:03 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 07:44:03 GMT"}, {"version": "v3", "created": "Sat, 9 May 2015 14:46:21 GMT"}, {"version": "v4", "created": "Wed, 20 May 2015 01:33:50 GMT"}, {"version": "v5", "created": "Tue, 15 Sep 2015 17:47:49 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Dadush", "Daniel", ""], ["Regev", "Oded", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1412.8005", "submitter": "Alex Gavryushkin", "authors": "Alex Gavryushkin, Bakhadyr Khoussainov, Mikhail Kokho, Jiamou Liu", "title": "Dynamic Algorithms for Interval Scheduling on a Single Machine", "comments": null, "journal-ref": "Theoretical Computer Science, Volume 562, 11 January 2015, Pages\n  227-242", "doi": "10.1016/j.tcs.2014.09.046", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate dynamic algorithms for the interval scheduling problem. Our\nalgorithm runs in amortised time $O(\\log n)$ for query operation and $O(d\\log^2\nn)$ for insertion and removal operations, where $n$ and $d$ are the maximal\nnumbers of intervals and pairwise overlapping intervals respectively. We also\nshow that for a monotonic set, that is when no interval properly contains\nanother interval, the amortised complexity is $O(\\log n)$ for both query and\nupdate operations. We compare the two algorithms for the monotonic interval\nsets using experiments.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 00:23:47 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Gavryushkin", "Alex", ""], ["Khoussainov", "Bakhadyr", ""], ["Kokho", "Mikhail", ""], ["Liu", "Jiamou", ""]]}, {"id": "1412.8097", "submitter": "William Hoza", "authors": "William M. Hoza and Leonard J. Schulman", "title": "The Adversarial Noise Threshold for Distributed Protocols", "comments": "23 pages, 2 figures. Fixes mistake in theorem 6 and various typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of implementing distributed protocols, despite\nadversarial channel errors, on synchronous-messaging networks with arbitrary\ntopology.\n  In our first result we show that any $n$-party $T$-round protocol on an\nundirected communication network $G$ can be compiled into a robust simulation\nprotocol on a sparse ($\\mathcal{O}(n)$ edges) subnetwork so that the simulation\ntolerates an adversarial error rate of $\\Omega\\left(\\frac{1}{n}\\right)$; the\nsimulation has a round complexity of $\\mathcal{O}\\left(\\frac{m \\log n}{n}\nT\\right)$, where $m$ is the number of edges in $G$. (So the simulation is\nwork-preserving up to a $\\log$ factor.) The adversary's error rate is within a\nconstant factor of optimal. Given the error rate, the round complexity blowup\nis within a factor of $\\mathcal{O}(k \\log n)$ of optimal, where $k$ is the edge\nconnectivity of $G$. We also determine that the maximum tolerable error rate on\ndirected communication networks is $\\Theta(1/s)$ where $s$ is the number of\nedges in a minimum equivalent digraph.\n  Next we investigate adversarial per-edge error rates, where the adversary is\ngiven an error budget on each edge of the network. We determine the exact limit\nfor tolerable per-edge error rates on an arbitrary directed graph. However, the\nconstruction that approaches this limit has exponential round complexity, so we\ngive another compiler, which transforms $T$-round protocols into\n$\\mathcal{O}(mT)$-round simulations, and prove that for polynomial-query black\nbox compilers, the per-edge error rate tolerated by this last compiler is\nwithin a constant factor of optimal.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 02:02:03 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 21:38:35 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Hoza", "William M.", ""], ["Schulman", "Leonard J.", ""]]}, {"id": "1412.8164", "submitter": "Qin Huang", "authors": "Qin Huang, Xingwu Liu, Xiaoming Sun, and Jialin Zhang", "title": "How to select the largest k elements from evolving data?", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the top-$k$-selection problem, i.e. determine\nthe largest, second largest, ..., and the $k$-th largest elements, in the\ndynamic data model. In this model the order of elements evolves dynamically\nover time. In each time step the algorithm can only probe the changes of data\nby comparing a pair of elements. Previously only two special cases were\nstudied[2]: finding the largest element and the median; and sorting all\nelements. This paper systematically deals with $k\\in [n]$ and solves the\nproblem almost completely. Specifically, we identify a critical point $k^*$\nsuch that the top-$k$-selection problem can be solved error-free with\nprobability $1-o(1)$ if and only if $k=o(k^*)$. A lower bound of the error when\n$k=\\Omega(k^*)$ is also determined, which actually is tight under some\ncondition. On the other hand, it is shown that the top-$k$-set problem, which\nmeans finding the largest $k$ elements without sorting them, can be solved\nerror-free for all $k\\in [n]$. Additionally, we extend the dynamic data model\nand show that most of these results still hold.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 13:26:19 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Huang", "Qin", ""], ["Liu", "Xingwu", ""], ["Sun", "Xiaoming", ""], ["Zhang", "Jialin", ""]]}, {"id": "1412.8225", "submitter": "Jiecao Chen", "authors": "Jiecao Chen, Bo Qin, David P. Woodruff, Qin Zhang", "title": "A Sketching Algorithm for Spectral Graph Sparsification", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of compressing a weighted graph $G$ on $n$ vertices,\nbuilding a \"sketch\" $H$ of $G$, so that given any vector $x \\in \\mathbb{R}^n$,\nthe value $x^T L_G x$ can be approximated up to a multiplicative $1+\\epsilon$\nfactor from only $H$ and $x$, where $L_G$ denotes the Laplacian of $G$. One\nsolution to this problem is to build a spectral sparsifier $H$ of $G$, which,\nusing the result of Batson, Spielman, and Srivastava, consists of $O(n\n\\epsilon^{-2})$ reweighted edges of $G$ and has the property that\nsimultaneously for all $x \\in \\mathbb{R}^n$, $x^T L_H x = (1 \\pm \\epsilon) x^T\nL_G x$. The $O(n \\epsilon^{-2})$ bound is optimal for spectral sparsifiers. We\nshow that if one is interested in only preserving the value of $x^T L_G x$ for\na {\\it fixed} $x \\in \\mathbb{R}^n$ (specified at query time) with high\nprobability, then there is a sketch $H$ using only $\\tilde{O}(n\n\\epsilon^{-1.6})$ bits of space. This is the first data structure achieving a\nsub-quadratic dependence on $\\epsilon$. Our work builds upon recent work of\nAndoni, Krauthgamer, and Woodruff who showed that $\\tilde{O}(n \\epsilon^{-1})$\nbits of space is possible for preserving a fixed {\\it cut query} (i.e., $x\\in\n\\{0,1\\}^n$) with high probability; here we show that even for a general query\nvector $x \\in \\mathbb{R}^n$, a sub-quadratic dependence on $\\epsilon$ is\npossible. Our result for Laplacians is in sharp contrast to sketches for\ngeneral $n \\times n$ positive semidefinite matrices $A$ with $O(\\log n)$ bit\nentries, for which even to preserve the value of $x^T A x$ for a fixed $x \\in\n\\mathbb{R}^n$ (specified at query time) up to a $1+\\epsilon$ factor with\nconstant probability, we show an $\\Omega(n \\epsilon^{-2})$ lower bound.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 23:22:42 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Chen", "Jiecao", ""], ["Qin", "Bo", ""], ["Woodruff", "David P.", ""], ["Zhang", "Qin", ""]]}, {"id": "1412.8246", "submitter": "Shihyen Chen", "authors": "Shihyen Chen, Zhuozhi Wang, Kaizhong Zhang", "title": "Pattern Matching and Local Alignment for RNA Structures", "comments": "7 pages. V2: changed first names initials to full names in metadata.\n  V3: added info of conference proceedings, updated email address", "journal-ref": "Proceedings of the 2002 International Conference on Mathematics\n  and Engineering Techniques in Medicine and Biological Sciences (METMBS),\n  55-61, 2002", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary structure of a ribonucleic acid (RNA) molecule can be represented\nas a sequence of nucleotides (bases) over the alphabet {A, C, G, U}. The\nsecondary or tertiary structure of an RNA is a set of base pairs which form\nbonds between A-U and G-C. For secondary structures, these bonds have been\ntraditionally assumed to be one-to-one and non-crossing. This paper considers\npattern matching as well as local alignment between two RNA structures. For\npattern matching, we present two algorithms, one for obtaining an exact match,\nthe other for approximate match. We then present an algorithm for RNA local\nstructural alignment.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 02:23:39 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 06:40:16 GMT"}, {"version": "v3", "created": "Thu, 1 Jan 2015 06:59:26 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Chen", "Shihyen", ""], ["Wang", "Zhuozhi", ""], ["Zhang", "Kaizhong", ""]]}, {"id": "1412.8296", "submitter": "Yixin Cao", "authors": "Wenjun Li, Jianxin Wang, Jianer Chen, and Yixin Cao", "title": "A $2k$-Vertex Kernel for Maximum Internal Spanning Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the parameterized version of the maximum internal spanning tree\nproblem, which, given an $n$-vertex graph and a parameter $k$, asks for a\nspanning tree with at least $k$ internal vertices. Fomin et al. [J. Comput.\nSystem Sci., 79:1-6] crafted a very ingenious reduction rule, and showed that a\nsimple application of this rule is sufficient to yield a $3k$-vertex kernel.\nHere we propose a novel way to use the same reduction rule, resulting in an\nimproved $2k$-vertex kernel. Our algorithm applies first a greedy procedure\nconsisting of a sequence of local exchange operations, which ends with a\nlocal-optimal spanning tree, and then uses this special tree to find a\nreducible structure. As a corollary of our kernel, we obtain a deterministic\nalgorithm for the problem running in time $4^k \\cdot n^{O(1)}$.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 10:19:48 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Li", "Wenjun", ""], ["Wang", "Jianxin", ""], ["Chen", "Jianer", ""], ["Cao", "Yixin", ""]]}, {"id": "1412.8338", "submitter": "Sanjeev Saxena", "authors": "Neethi K.S. and Sanjeev Saxena", "title": "Maximum Cardinality Neighbourly Sets in Quadrilateral Free Graphs", "comments": null, "journal-ref": "J Comb Optim 33(2): 422-444 (2017)", "doi": "10.1007/s10878-015-9972-9", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighbourly set of a graph is a subset of edges which either share an end\npoint or are joined by an edge of that graph. The maximum cardinality\nneighbourly set problem is known to be NP-complete for general graphs. Mahdian\n(M.Mahdian, On the computational complexity of strong edge coloring, Discrete\nApplied Mathematics, 118:239-248, 2002) proved that it is in polynomial time\nfor quadrilateral-free graphs and proposed an O(n^{11}) algorithm for the same\n(along with a note that by a straightforward but lengthy argument it can be\nproved to be solvable in O(n^5) running time). In this paper we propose an\nO(n^2) time algorithm for finding a maximum cardinality neighbourly set in a\nquadrilateral-free graph.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 13:43:22 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["S.", "Neethi K.", ""], ["Saxena", "Sanjeev", ""]]}, {"id": "1412.8347", "submitter": "Anupam Gupta", "authors": "Niv Buchbinder, Shahar Chen, Anupam Gupta, Viswanath Nagarajan, Joseph\n  (Seffi) Naor", "title": "Online Packing and Covering Framework with Convex Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online fractional covering problems with a convex objective,\nwhere the covering constraints arrive over time. Formally, we want to solve\n$\\min\\,\\{f(x) \\mid Ax\\ge \\mathbf{1},\\, x\\ge 0\\},$ where the objective function\n$f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$ is convex, and the constraint matrix\n$A_{m\\times n}$ is non-negative. The rows of $A$ arrive online over time, and\nwe wish to maintain a feasible solution $x$ at all times while only increasing\ncoordinates of $x$. We also consider \"dual\" packing problems of the form\n$\\max\\,\\{c^\\intercal y - g(\\mu) \\mid A^\\intercal y \\le \\mu,\\, y\\ge 0\\}$, where\n$g$ is a convex function. In the online setting, variables $y$ and columns of\n$A^\\intercal$ arrive over time, and we wish to maintain a non-decreasing\nsolution $(y,\\mu)$.\n  We provide an online primal-dual framework for both classes of problems with\ncompetitive ratio depending on certain \"monotonicity\" and \"smoothness\"\nparameters of $f$; our results match or improve on guarantees for some special\nclasses of functions $f$ considered previously.\n  Using this fractional solver with problem-dependent randomized rounding\nprocedures, we obtain competitive algorithms for the following problems: online\ncovering LPs minimizing $\\ell_p$-norms of arbitrary packing constraints, set\ncover with multiple cost functions, capacity constrained facility location,\ncapacitated multicast problem, set cover with set requests, and profit\nmaximization with non-separable production costs. Some of these results are new\nand others provide a unified view of previous results, with matching or\nslightly worse competitive ratios.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 14:09:31 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Buchbinder", "Niv", "", "Seffi"], ["Chen", "Shahar", "", "Seffi"], ["Gupta", "Anupam", "", "Seffi"], ["Nagarajan", "Viswanath", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""]]}, {"id": "1412.8518", "submitter": "Tim Roughgarden", "authors": "Jason D. Hartline and Tim Roughgarden", "title": "Optimal Platform Design", "comments": "There is some overlap between this paper and the paper \"Optimal\n  Mechanism Design and Money Burning,\" which appeared in the STOC 2008\n  conference and as arXiv:0804.2097. However, the focus of this paper is\n  different, with some of our earlier results omitted and several new results\n  included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An auction house cannot generally provide the optimal auction technology to\nevery client. Instead it provides one or several auction technologies, and\nclients select the most appropriate one. For example, eBay provides ascending\nauctions and \"buy-it-now\" pricing. For each client the offered technology may\nnot be optimal, but it would be too costly for clients to create their own. We\ncall these mechanisms, which emphasize generality rather than optimality,\nplatform mechanisms. A platform mechanism will be adopted by a client if its\nperformance exceeds that of the client's outside option, e.g., hiring (at a\ncost) a consultant to design the optimal mechanism. We ask two related\nquestions. First, for what costs of the outside option will the platform be\nuniversally adopted? Second, what is the structure of good platform mechanisms?\nWe answer these questions using a novel prior-free analysis framework in which\nwe seek mechanisms that are approximately optimal for every prior.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 01:04:14 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Hartline", "Jason D.", ""], ["Roughgarden", "Tim", ""]]}, {"id": "1412.8615", "submitter": "Sumedh Tirodkar", "authors": "Ashish Chiplunkar, Sumedh Tirodkar, Sundar Vishwanathan", "title": "On Randomized Algorithms for Matching in the Online Preemptive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the power of randomized algorithms for the maximum cardinality\nmatching (MCM) and the maximum weight matching (MWM) problems in the online\npreemptive model. In this model, the edges of a graph are revealed one by one\nand the algorithm is required to always maintain a valid matching. On seeing an\nedge, the algorithm has to either accept or reject the edge. If accepted, then\nthe adjacent edges are discarded. The complexity of the problem is settled for\ndeterministic algorithms.\n  Almost nothing is known for randomized algorithms. A lower bound of $1.693$\nis known for MCM with a trivial upper bound of $2$. An upper bound of $5.356$\nis known for MWM. We initiate a systematic study of the same in this paper with\nan aim to isolate and understand the difficulty. We begin with a primal-dual\nanalysis of the deterministic algorithm due to McGregor. All deterministic\nlower bounds are on instances which are trees at every step. For this class of\n(unweighted) graphs we present a randomized algorithm which is\n$\\frac{28}{15}$-competitive. The analysis is a considerable extension of the\n(simple) primal-dual analysis for the deterministic case. The key new technique\nis that the distribution of primal charge to dual variables depends on the\n\"neighborhood\" and needs to be done after having seen the entire input. The\nassignment is asymmetric: in that edges may assign different charges to the two\nend-points. Also the proof depends on a non-trivial structural statement on the\nperformance of the algorithm on the input tree.\n  The other main result of this paper is an extension of the deterministic\nlower bound of Varadaraja to a natural class of randomized algorithms which\ndecide whether to accept a new edge or not using independent random choices.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 12:21:06 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 12:06:58 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Chiplunkar", "Ashish", ""], ["Tirodkar", "Sumedh", ""], ["Vishwanathan", "Sundar", ""]]}, {"id": "1412.8723", "submitter": "Santanu Dey", "authors": "Pelin Damci-Kurt, Santanu S. Dey, Simge Kucukyavuz", "title": "On a Cardinality-Constrained Transportation Problem With Market Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the intersection of the matching polytope with a\ncardinality constraint is integral [8]. We prove a similar result for the\npolytope corresponding to the transportation problem with market choice (TPMC)\n(introduced in [4]) when the demands are in the set $\\{1,2\\}$. This result\ngeneralizes the result regarding the matching polytope and also implies that\nsome special classes of minimum weight perfect matching problem with a\ncardinality constraint on a subset of edges can be solved in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 18:43:50 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Damci-Kurt", "Pelin", ""], ["Dey", "Santanu S.", ""], ["Kucukyavuz", "Simge", ""]]}]