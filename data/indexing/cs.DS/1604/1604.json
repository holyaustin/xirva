[{"id": "1604.00054", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov, Florin Manea, Dirk Nowotka", "title": "Detecting One-variable Patterns", "comments": "16 pages (+13 pages of Appendix), 4 figures, accepted to SPIRE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pattern $p = s_1x_1s_2x_2\\cdots s_{r-1}x_{r-1}s_r$ such that\n$x_1,x_2,\\ldots,x_{r-1}\\in\\{x,\\overset{{}_{\\leftarrow}}{x}\\}$, where $x$ is a\nvariable and $\\overset{{}_{\\leftarrow}}{x}$ its reversal, and\n$s_1,s_2,\\ldots,s_r$ are strings that contain no variables, we describe an\nalgorithm that constructs in $O(rn)$ time a compact representation of all $P$\ninstances of $p$ in an input string of length $n$ over a polynomially bounded\ninteger alphabet, so that one can report those instances in $O(P)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 21:25:42 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 10:24:18 GMT"}, {"version": "v3", "created": "Mon, 9 May 2016 21:35:29 GMT"}, {"version": "v4", "created": "Tue, 28 Feb 2017 17:43:06 GMT"}, {"version": "v5", "created": "Tue, 18 Jul 2017 16:01:01 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Kosolobov", "Dmitry", ""], ["Manea", "Florin", ""], ["Nowotka", "Dirk", ""]]}, {"id": "1604.00111", "submitter": "Hung Ngo", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Dan Suciu", "title": "Computing Join Queries with Functional Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Gottlob, Lee, Valiant, and Valiant (GLVV) presented an output size\nbound for join queries with functional dependencies (FD), based on a linear\nprogram on polymatroids. GLVV bound strictly generalizes the bound of Atserias,\nGrohe and Marx (AGM) for queries with no FD, in which case there are known\nalgorithms running within AGM bound and thus are worst-case optimal.\n  A main result of this paper is an algorithm for computing join queries with\nFDs, running within GLVV bound up to a poly-log factor. In particular, our\nalgorithm is worst-case optimal for any query where the GLVV bound is tight. As\nan unexpected by-product, our algorithm manages to solve a harder problem,\nwhere (some) input relations may have prescribed maximum degree bounds, of\nwhich both functional dependencies and cardinality bounds are special cases.\n  We extend Gottlob et al. framework by replacing all variable subsets with the\nlattice of closed sets (under the given FDs). This gives us new insights into\nthe structure of the worst-case bound and worst-case instances. While it is\nstill open whether GLVV bound is tight in general, we show that it is tight on\ndistributive lattices and some other simple lattices. Distributive lattices\ncapture a strict superset of queries with no FD and with simple FDs. We also\npresent two simpler algorithms which are also worst-case optimal on\ndistributive lattices within a single-$\\log$ factor, but they do not match GLVV\nbound on a general lattice. Our algorithms are designed based on a novel\nprinciple: we turn a proof of a polymatroid-based output size bound into an\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 02:28:40 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 00:21:42 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Suciu", "Dan", ""]]}, {"id": "1604.00310", "submitter": "Ojas Parekh", "authors": "Ojas Parekh", "title": "Iterative Packing for Demand and Hypergraph Matching", "comments": "13 pages. Appeared in the 15th conference on Integer Programming and\n  Combinatorial Optimization (IPCO 2011); available at Springer via\n  http://dx.doi.org/10.1007/978-3-642-20807-2_28", "journal-ref": null, "doi": "10.1007/978-3-642-20807-2_28", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative rounding has enjoyed tremendous success in elegantly resolving open\nquestions regarding the approximability of problems dominated by covering\nconstraints. Although iterative rounding methods have been applied to packing\nproblems, no single method has emerged that matches the effectiveness and\nsimplicity afforded by the covering case. We offer a simple iterative packing\ntechnique that retains features of Jain's seminal approach, including the\nproperty that the magnitude of the fractional value of the element rounded\nduring each iteration has a direct impact on the approximation guarantee. We\napply iterative packing to generalized matching problems including demand\nmatching and $k$-column-sparse column-restricted packing ($k$-CS-PIP) and\nobtain approximation algorithms that essentially settle the integrality gap for\nthese problems. We present a simple deterministic $2k$-approximation for\n$k$-CS-PIP, where an $8k$-approximation was the best deterministic algorithm\npreviously known. The integrality gap in this case is at least $2(k-1+1/k)$. We\nalso give a deterministic $3$-approximation for a generalization of demand\nmatching, settling its natural integrality gap.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 16:08:33 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Parekh", "Ojas", ""]]}, {"id": "1604.00322", "submitter": "Ojas Parekh", "authors": "Ojas Parekh and David Pritchard", "title": "Generalized Hypergraph Matching via Iterated Packing and Local Ratio", "comments": "12 pages. Appeared in the 12th Workshop on Approximation and Online\n  Algorithms (WAOA 2014), available at Springer via\n  http://dx.doi.org/10.1007/978-3-319-18263-6_18", "journal-ref": null, "doi": "10.1007/978-3-319-18263-6_18", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In $k$-hypergraph matching, we are given a collection of sets of size at most\n$k$, each with an associated weight, and we seek a maximum-weight subcollection\nwhose sets are pairwise disjoint. More generally, in $k$-hypergraph\n$b$-matching, instead of disjointness we require that every element appears in\nat most $b$ sets of the subcollection. Our main result is a linear-programming\nbased $(k-1+\\tfrac{1}{k})$-approximation algorithm for $k$-hypergraph\n$b$-matching. This settles the integrality gap when $k$ is one more than a\nprime power, since it matches a previously-known lower bound. When the\nhypergraph is bipartite, we are able to improve the approximation ratio to\n$k-1$, which is also best possible relative to the natural LP. These results\nare obtained using a more careful application of the \\emph{iterated packing}\nmethod.\n  Using the bipartite algorithmic integrality gap upper bound, we show that for\nthe family of combinatorial auctions in which anyone can win at most $t$ items,\nthere is a truthful-in-expectation polynomial-time auction that\n$t$-approximately maximizes social welfare. We also show that our results\ndirectly imply new approximations for a generalization of the recently\nintroduced bounded-color matching problem.\n  We also consider the generalization of $b$-matching to \\emph{demand\nmatching}, where edges have nonuniform demand values. The best known\napproximation algorithm for this problem has ratio $2k$ on $k$-hypergraphs. We\ngive a new algorithm, based on local ratio, that obtains the same approximation\nratio in a much simpler way.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 16:40:23 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Parekh", "Ojas", ""], ["Pritchard", "David", ""]]}, {"id": "1604.00357", "submitter": "Aviad Rubinstein", "authors": "Aviad Rubinstein", "title": "Beyond matroids: Secretary Problem and Prophet Inequality with general\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalizations of the \"Prophet Inequality\" and \"Secretary Problem\",\nwhere the algorithm is restricted to an arbitrary downward-closed set system.\nFor {0,1}-values, we give O(log n)-competitive algorithms for both problems.\nThis is close to the \\Omega(log n / loglog n) lower bound due to Babaioff,\nImmorlica, and Kleinberg. For general values, our results translate to O(log n\nlog r)-competitive algorithms, where r is the cardinality of the largest\nfeasible set. This resolves (up to the O(log r loglog n) factors) an open\nquestion posed to us by Bobby Kleinberg.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 18:53:08 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Rubinstein", "Aviad", ""]]}, {"id": "1604.00391", "submitter": "Gabriel Michau", "authors": "Gabriel Michau, Nelly Pustelnik, Pierre Borgnat, Patrice Abry, Alfredo\n  Nantes, Ashish Bhaskar, Edward Chung", "title": "A Primal-Dual Algorithm for Link Dependent Origin Destination Matrix\n  Estimation", "comments": null, "journal-ref": "2016 IEEE Transactions on Signal and Information Processing over\n  Networks", "doi": "10.1109/TSIPN.2016.2623094", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Origin-Destination Matrix (ODM) estimation is a classical problem in\ntransport engineering aiming to recover flows from every Origin to every\nDestination from measured traffic counts and a priori model information. In\naddition to traffic counts, the present contribution takes advantage of probe\ntrajectories, whose capture is made possible by new measurement technologies.\nIt extends the concept of ODM to that of Link dependent ODM (LODM), keeping the\ninformation about the flow distribution on links and containing inherently the\nODM assignment. Further, an original formulation of LODM estimation, from\ntraffic counts and probe trajectories is presented as an optimisation problem,\nwhere the functional to be minimized consists of five convex functions, each\nmodelling a constraint or property of the transport problem: consistency with\ntraffic counts, consistency with sampled probe trajectories, consistency with\ntraffic conservation (Kirchhoff's law), similarity of flows having close\norigins and destinations, positivity of traffic flows. A primal-dual algorithm\nis devised to minimize the designed functional, as the corresponding objective\nfunctions are not necessarily differentiable. A case study, on a simulated\nnetwork and traffic, validates the feasibility of the procedure and details its\nbenefits for the estimation of an LODM matching real-network constraints and\nobservations.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 15:07:34 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Michau", "Gabriel", ""], ["Pustelnik", "Nelly", ""], ["Borgnat", "Pierre", ""], ["Abry", "Patrice", ""], ["Nantes", "Alfredo", ""], ["Bhaskar", "Ashish", ""], ["Chung", "Edward", ""]]}, {"id": "1604.00446", "submitter": "Abhishek  Sinha", "authors": "Abhishek Sinha, Georgios Paschos, Eytan Modiano", "title": "Throughput-Optimal Multi-hop Broadcast Algorithms", "comments": "To appear in the proceedings of Mobihoc, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we design throughput-optimal dynamic broad- cast algorithms for\nmulti-hop networks with arbitrary topolo- gies. Most of the previous broadcast\nalgorithms route pack- ets along spanning trees, rooted at the source node. For\nlarge dynamic networks, computing and maintaining a set of spanning trees is\nnot efficient, as the network-topology may change frequently. In this paper we\ndesign a class of dynamic algorithms which makes packet-by-packet schedul- ing\nand routing decisions and thus obviates the need for maintaining any global\ntopological structures, such as span- ning trees. Our algorithms may be\nconveniently understood as a non-trivial generalization of the familiar\nback-pressure algorithm which makes unicast packet routing and schedul- ing\ndecisions, based on queue-length information, without maintaining end-to-end\npaths. However, in the broadcast problem, it is hard to define queuing\nstructures due to ab- sence of a work-conservation principle which results from\npacket duplications. We design and prove the optimality of a virtual-queue\nbased algorithm, where a virtual-queue is de- fined for subsets of vertices. We\nthen propose a multi-class broadcast policy which combines the above scheduling\nalgo- rithm with a class-based in-order packet delivery constraint, resulting\nin significant reduction in complexity. Finally, we evaluate performance of the\nproposed algorithms via exten- sive numerical simulations.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 00:54:11 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Sinha", "Abhishek", ""], ["Paschos", "Georgios", ""], ["Modiano", "Eytan", ""]]}, {"id": "1604.00527", "submitter": "Afonso Henrique Sampaio Oliveira", "authors": "Afonso Sampaio and Sebasti\\'an Urrutia and Johan Oppen", "title": "A Decomposition Approach to Solve The Quay Crane Scheduling Problem", "comments": "24 pages, 4 figures, 4 tables. Work presented at EURO 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a decomposition approach to solve the quay crane\nscheduling problem. This is an important maritime transportation problem faced\nin container terminals where quay cranes are used to handle cargo. The\nobjective is to determine a sequence of loading and unloading operations for\neach crane in order to minimize the completion time. We solve a mixed integer\nprogramming formulation for the quay crane scheduling problem, decomposing it\ninto a vehicle routing problem and a corresponding scheduling problem. The\nrouting sub-problem is solved by minimizing the longest crane completion time\nwithout taking crane interference into account. This solution provides a lower\nbound for the makespan of the whole problem and is sent to the scheduling\nsub-problem, where a completion time for each task and the makespan are\ndetermined. This scheme resembles Benders' decomposition and, in particular,\nthe scheme underlying combinatorial Benders' cuts. We evaluate the proposed\napproach by solving instances from the literature and comparing the results\nwith other available methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 16:58:38 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Sampaio", "Afonso", ""], ["Urrutia", "Sebasti\u00e1n", ""], ["Oppen", "Johan", ""]]}, {"id": "1604.00576", "submitter": "Abhishek  Sinha", "authors": "Abhishek Sinha, Leandros Tassiulas, Eytan Modiano", "title": "Throughput-Optimal Broadcast in Wireless Networks with Dynamic Topology", "comments": "To appear in the proceedings of Mobihoc, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of throughput-optimal broadcast- ing in time-varying\nwireless networks, whose underlying topology is restricted to Directed Acyclic\nGraphs (DAG). Previous broadcast algorithms route packets along spanning trees.\nIn large networks with time-varying connectivities, these trees are difficult\nto compute and maintain. In this paper we propose a new online\nthroughput-optimal broadcast algorithm which makes packet-by-packet scheduling\nand routing decisions, obviating the need for maintaining any global\ntopological structures, such as spanning-trees. Our algorithm relies on\nsystem-state information for making transmission decisions and hence, may be\nthought of as a generalization of the well-known back-pressure algorithm which\nmakes point-to-point unicast transmission decisions based on queue-length\ninformation, without requiring knowledge of end-to-end paths. Technically, the\nback-pressure algorithm is derived by stochastically stabilizing the\nnetwork-queues. However, because of packet-duplications associated with\nbroadcast, the work-conservation principle is violated and queuing processes\nare difficult to define in the broadcast problem. To address this fundamental\nissue, we identify certain state-variables which behave like virtual queues in\nthe broadcast setting. By stochastically stabilizing these virtual queues, we\ndevise a throughput-optimal broadcast policy. We also derive new\ncharacterizations of the broadcast-capacity of time-varying wireless DAGs and\nderive an efficient algorithm to compute the capacity exactly under certain\nassumptions of interference model, and a poly-time approximation algorithm for\ncomputing the capacity under less restrictive assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 00:09:19 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Sinha", "Abhishek", ""], ["Tassiulas", "Leandros", ""], ["Modiano", "Eytan", ""]]}, {"id": "1604.00590", "submitter": "Arindam Pal", "authors": "Sushmita Ruj and Arindam Pal", "title": "Preferential Attachment Model with Degree Bound and its Application to\n  Key Predistribution in WSN", "comments": "Published in the proceedings of IEEE International Conference on\n  Advanced Information Networking and Applications (AINA) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preferential attachment models have been widely studied in complex networks,\nbecause they can explain the formation of many networks like social networks,\ncitation networks, power grids, and biological networks, to name a few.\nMotivated by the application of key predistribution in wireless sensor networks\n(WSN), we initiate the study of preferential attachment with degree bound.\n  Our paper has two important contributions to two different areas. The first\nis a contribution in the study of complex networks. We propose preferential\nattachment model with degree bound for the first time. In the normal\npreferential attachment model, the degree distribution follows a power law,\nwith many nodes of low degree and a few nodes of high degree. In our scheme,\nthe nodes can have a maximum degree $d_{\\max}$, where $d_{\\max}$ is an integer\nchosen according to the application. The second is in the security of wireless\nsensor networks. We propose a new key predistribution scheme based on the above\nmodel. The important features of this model are that the network is fully\nconnected, it has fewer keys, has larger size of the giant component and lower\naverage path length compared with traditional key predistribution schemes and\ncomparable resilience to random node attacks.\n  We argue that in many networks like key predistribution and Internet of\nThings, having nodes of very high degree will be a bottle-neck in\ncommunication. Thus, studying preferential attachment model with degree bound\nwill open up new directions in the study of complex networks, and will have\nmany applications in real world scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 03:45:42 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Ruj", "Sushmita", ""], ["Pal", "Arindam", ""]]}, {"id": "1604.00708", "submitter": "Joseph Cheriyan", "authors": "Joe Cheriyan, Zhihan Gao", "title": "Approximating (Unweighted) Tree Augmentation via Lift-and-Project (Part\n  0: $1.8+\\epsilon$ approximation for (Unweighted) TAP)", "comments": "This is a manuscript from July 2014 that was widely circulated but\n  was not posted on the web. It is being posted for the sake of\n  archiving/referencing. The results have been subsumed by arXiv:1507.01309 by\n  the same authors. 15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the unweighted Tree Augmentation Problem (TAP) via the Lasserre (Sum\nof Squares) system. We prove an approximation guarantee of ($1.8+\\epsilon$)\nrelative to an SDP relaxation, which matches the combinatorial approximation\nguarantee of Even, Feldman, Kortsarz and Nutov in ACM TALG (2009), where\n$\\epsilon>0$ is a constant. We generalize the combinatorial analysis of\nintegral solutions of Even, et al., to fractional solutions by identifying some\nproperties of fractional solutions of the Lasserre system via the decomposition\nresult of Rothvo{\\ss} (arXiv:1111.5473, 2011) and Karlin, Mathieu and Nguyen\n(IPCO 2011).\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 00:47:13 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Cheriyan", "Joe", ""], ["Gao", "Zhihan", ""]]}, {"id": "1604.00845", "submitter": "Michael Kapralov", "authors": "Michael Kapralov", "title": "Sparse Fourier Transform in Any Constant Dimension with Nearly-Optimal\n  Sample Complexity in Sublinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a $k$-sparse approximation to the\nFourier transform of a length $N$ signal. Our main result is a randomized\nalgorithm for computing such an approximation (i.e. achieving the\n$\\ell_2/\\ell_2$ sparse recovery guarantees using Fourier measurements) using\n$O_d(k\\log N\\log\\log N)$ samples of the signal in time domain that runs in time\n$O_d(k\\log^{d+3} N)$, where $d\\geq 1$ is the dimensionality of the Fourier\ntransform. The sample complexity matches the lower bound of $\\Omega(k\\log\n(N/k))$ for non-adaptive algorithms due to \\cite{DIPW} for any $k\\leq\nN^{1-\\delta}$ for a constant $\\delta>0$ up to an $O(\\log\\log N)$ factor. Prior\nto our work a result with comparable sample complexity $k\\log N \\log^{O(1)}\\log\nN$ and sublinear runtime was known for the Fourier transform on the line\n\\cite{IKP}, but for any dimension $d\\geq 2$ previously known techniques either\nsuffered from a polylogarithmic factor loss in sample complexity or required\n$\\Omega(N)$ runtime.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 13:19:57 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Kapralov", "Michael", ""]]}, {"id": "1604.00870", "submitter": "Shahrzad Haddadan", "authors": "Shahrzad Haddadan and Peter Winkler", "title": "Mixing Time for Some Adjacent Transposition Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove rapid mixing for certain Markov chains on the set $S_n$ of\npermutations on $1,2,\\dots,n$ in which adjacent transpositions are made with\nprobabilities that depend on the items being transposed. Typically, when in\nstate $\\sigma$, a position $i<n$ is chosen uniformly at random, and $\\sigma(i)$\nand $\\sigma(i{+}1)$ are swapped with probability depending on $\\sigma(i)$ and\n$\\sigma(i{+}1)$. The stationary distributions of such chains appear in various\nfields of theoretical computer science, and rapid mixing established in the\nuniform case.\n  Recently, there has been progress in cases with biased stationary\ndistributions, but there are wide classes of such chains whose mixing time is\nunknown. One case of particular interest is what we call the \"gladiator chain,\"\nin which each number $g$ is assigned a \"strength\" $s_g$ and when $g$ and $g'$\nare adjacent and chosen for possible swapping, $g$ comes out on top with\nprobability $s_g/(s_g + s_{g'})$. We obtain a polynomial-time upper bound on\nmixing time when the gladiators fall into only three strength classes.\n  A preliminary version of this paper appeared as \"Mixing of Permutations by\nBiased Transposition\" in STACS 2017.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 14:23:50 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 18:50:35 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Haddadan", "Shahrzad", ""], ["Winkler", "Peter", ""]]}, {"id": "1604.00922", "submitter": "Nikola Yolov", "authors": "Colin McDiarmid and Nikola Yolov", "title": "Recognition of unipolar and generalised split graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is unipolar if it can be partitioned into a clique and a disjoint\nunion of cliques, and a graph is a generalised split graph if it or its\ncomplement is unipolar. A unipolar partition of a graph can be used to find\nefficiently the clique number, the stability number, the chromatic number, and\nto solve other problems that are hard for general graphs. We present the first\n$O(n^2)$ time algorithm for recognition of $n$-vertex unipolar and generalised\nsplit graphs, improving on previous $O(n^3)$ time algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 15:55:07 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["McDiarmid", "Colin", ""], ["Yolov", "Nikola", ""]]}, {"id": "1604.01088", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "Optimal Parameter Settings for the $(1+(\\lambda, \\lambda))$ Genetic\n  Algorithm", "comments": "Extended version of a paper that appeared at GECCO'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $(1+(\\lambda,\\lambda))$ genetic algorithm is one of the few algorithms\nfor which a super-constant speed-up through the use of crossover could be\nproven. So far, this algorithm has been used with parameters based also on\nintuitive considerations. In this work, we rigorously regard the whole\nparameter space and show that the asymptotic time complexity proven by Doerr\nand Doerr (GECCO 2015) for the intuitive choice is best possible among all\nsettings for population size, mutation probability, and crossover bias.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 23:19:00 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 00:14:24 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "1604.01116", "submitter": "Kasra Khosoussi", "authors": "Kasra Khosoussi, Gaurav S. Sukhatme, Shoudong Huang, Gamini\n  Dissanayake", "title": "Maximizing the Weighted Number of Spanning Trees: Near-$t$-Optimal\n  Graphs", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing well-connected graphs is a fundamental problem that frequently\narises in various contexts across science and engineering. The weighted number\nof spanning trees, as a connectivity measure, emerges in numerous problems and\nplays a key role in, e.g., network reliability under random edge failure,\nestimation over networks and D-optimal experimental designs. This paper tackles\nthe open problem of designing graphs with the maximum weighted number of\nspanning trees under various constraints. We reveal several new structures,\nsuch as the log-submodularity of the weighted number of spanning trees in\nconnected graphs. We then exploit these structures and design a pair of\nefficient approximation algorithms with performance guarantees and\nnear-optimality certificates. Our results can be readily applied to a wide\nverity of applications involving graph synthesis and graph sparsification\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 02:42:48 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 04:04:27 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Khosoussi", "Kasra", ""], ["Sukhatme", "Gaurav S.", ""], ["Huang", "Shoudong", ""], ["Dissanayake", "Gamini", ""]]}, {"id": "1604.01136", "submitter": "Xiaoke Wang", "authors": "Xiaoke Wang, Chuan Wu, Franck Le, Alex Liu, Zongpeng Li and Francis\n  Lau", "title": "Online VNF Scaling in Datacenters", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Function Virtualization (NFV) is a promising technology that promises\nto significantly reduce the operational costs of network services by deploying\nvirtualized network functions (VNFs) to commodity servers in place of dedicated\nhardware middleboxes. The VNFs are typically running on virtual machine\ninstances in a cloud infrastructure, where the virtualization technology\nenables dynamic provisioning of VNF instances, to process the fluctuating\ntraffic that needs to go through the network functions in a network service. In\nthis paper, we target dynamic provisioning of enterprise network services -\nexpressed as one or multiple service chains - in cloud datacenters, and design\nefficient online algorithms without requiring any information on future traffic\nrates. The key is to decide the number of instances of each VNF type to\nprovision at each time, taking into consideration the server resource\ncapacities and traffic rates between adjacent VNFs in a service chain. In the\ncase of a single service chain, we discover an elegant structure of the problem\nand design an efficient randomized algorithm achieving a e/(e-1) competitive\nratio. For multiple concurrent service chains, an online heuristic algorithm is\nproposed, which is O(1)-competitive. We demonstrate the effectiveness of our\nalgorithms using solid theoretical analysis and trace-driven simulations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 05:17:13 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Wang", "Xiaoke", ""], ["Wu", "Chuan", ""], ["Le", "Franck", ""], ["Liu", "Alex", ""], ["Li", "Zongpeng", ""], ["Lau", "Francis", ""]]}, {"id": "1604.01154", "submitter": "Igor Sergeev S.", "authors": "Igor S. Sergeev", "title": "On the complexity of computing prime tables on a Turing machine", "comments": "6 pages", "journal-ref": "Prikladnaya diskretnaya matematika. 2016. 1(31). 86-91 (in\n  Russian)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the complexity of computing the table of primes between $1$ and\n$n$ on a multitape Turing machine is $O(n \\log n)$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 07:05:21 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Sergeev", "Igor S.", ""]]}, {"id": "1604.01168", "submitter": "B\\'alint M\\'ark V\\'as\\'arhelyi", "authors": "B\\'alint V\\'as\\'arhelyi", "title": "An Estimation of the Size of Non-Compact Suffix Trees", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A suffix tree is a data structure used mainly for pattern matching. It is\nknown that the space complexity of simple suffix trees is quadratic in the\nlength of the string. By a slight modification of the simple suffix trees one\ngets the compact suffix trees, which have linear space complexity. The\nmotivation of this paper is the question whether the space complexity of simple\nsuffix trees is quadratic not only in the worst case, but also in expectation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 08:21:53 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 15:26:52 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["V\u00e1s\u00e1rhelyi", "B\u00e1lint", ""]]}, {"id": "1604.01169", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers", "title": "A Fast Incremental BSP Tree Archive for Non-dominated Points", "comments": "This paper has been submitted to the conference EMO'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining an archive of all non-dominated points is a standard task in\nmulti-objective optimization. Sometimes it is sufficient to store all evaluated\npoints and to obtain the non-dominated subset in a post-processing step.\nAlternatively the non-dominated set can be updated on the fly. While keeping\ntrack of many non-dominated points efficiently is easy for two objectives, we\npropose an efficient algorithm based on a binary space partitioning (BSP) tree\nfor the general case of three or more objectives. Our analysis and our\nempirical results demonstrate the superiority of the method over the\nbrute-force baseline method, as well as graceful scaling to large numbers of\nobjectives.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 08:23:16 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 10:29:33 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Glasmachers", "Tobias", ""]]}, {"id": "1604.01317", "submitter": "Michael Sadovsky", "authors": "Sergey Tsarev, Michael Sadovsky", "title": "New Error Tolerant Method to Search Long Repeats in Symbol Sequences", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method to identify all sufficiently long repeating substrings in one or\nseveral symbol sequences is proposed. The method is based on a specific gauge\napplied to symbol sequences that guarantees identification of the repeating\nsubstrings. It allows the matching of substrings to contain a given level of\nerrors. The gauge is based on the development of a heavily sparse dictionary of\nrepeats, thus drastically accelerating the search procedure. Some genomic\napplications illustrate the method.\n  This paper is the extended and detailed version of the presentation at the\nthird International Conference on Algorithms for Computational Biology to be\nheld at Trujillo, Spain, June 21-22, 2016.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 16:18:33 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Tsarev", "Sergey", ""], ["Sadovsky", "Michael", ""]]}, {"id": "1604.01357", "submitter": "Jelani Nelson", "authors": "Kasper Green Larsen, Jelani Nelson, Huy L. Nguyen, Mikkel Thorup", "title": "Heavy hitters via cluster-preserving clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In turnstile $\\ell_p$ $\\varepsilon$-heavy hitters, one maintains a\nhigh-dimensional $x\\in\\mathbb{R}^n$ subject to $\\texttt{update}(i,\\Delta)$\ncausing $x_i\\leftarrow x_i + \\Delta$, where $i\\in[n]$, $\\Delta\\in\\mathbb{R}$.\nUpon receiving a query, the goal is to report a small list $L\\subset[n]$, $|L|\n= O(1/\\varepsilon^p)$, containing every \"heavy hitter\" $i\\in[n]$ with $|x_i|\n\\ge \\varepsilon \\|x_{\\overline{1/\\varepsilon^p}}\\|_p$, where $x_{\\overline{k}}$\ndenotes the vector obtained by zeroing out the largest $k$ entries of $x$ in\nmagnitude.\n  For any $p\\in(0,2]$ the CountSketch solves $\\ell_p$ heavy hitters using\n$O(\\varepsilon^{-p}\\log n)$ words of space with $O(\\log n)$ update time,\n$O(n\\log n)$ query time to output $L$, and whose output after any query is\ncorrect with high probability (whp) $1 - 1/poly(n)$. Unfortunately the query\ntime is very slow. To remedy this, the work [CM05] proposed for $p=1$ in the\nstrict turnstile model, a whp correct algorithm achieving suboptimal space\n$O(\\varepsilon^{-1}\\log^2 n)$, worse update time $O(\\log^2 n)$, but much better\nquery time $O(\\varepsilon^{-1}poly(\\log n))$.\n  We show this tradeoff between space and update time versus query time is\nunnecessary. We provide a new algorithm, ExpanderSketch, which in the most\ngeneral turnstile model achieves optimal $O(\\varepsilon^{-p}\\log n)$ space,\n$O(\\log n)$ update time, and fast $O(\\varepsilon^{-p}poly(\\log n))$ query time,\nand whp correctness. Our main innovation is an efficient reduction from the\nheavy hitters to a clustering problem in which each heavy hitter is encoded as\nsome form of noisy spectral cluster in a much bigger graph, and the goal is to\nidentify every cluster. Since every heavy hitter must be found, correctness\nrequires that every cluster be found. We then develop a \"cluster-preserving\nclustering\" algorithm, partitioning the graph into clusters without destroying\nany original cluster.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 18:33:08 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Larsen", "Kasper Green", ""], ["Nelson", "Jelani", ""], ["Nguyen", "Huy L.", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1604.01421", "submitter": "Bin Fu", "authors": "Bin Fu", "title": "Partial Sublinear Time Approximation and Inapproximation for Maximum\n  Coverage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a randomized approximation algorithm for the classical maximum\ncoverage problem, which given a list of sets $A_1,A_2,\\cdots, A_m$ and integer\nparameter $k$, select $k$ sets $A_{i_1}, A_{i_2},\\cdots, A_{i_k}$ for maximum\nunion $A_{i_1}\\cup A_{i_2}\\cup\\cdots\\cup A_{i_k}$. In our algorithm, each input\nset $A_i$ is a black box that can provide its size $|A_i|$, generate a random\nelement of $A_i$, and answer the membership query $(x\\in A_i?)$ in $O(1)$ time.\nOur algorithm gives $(1-{1\\over e})$-approximation for maximum coverage problem\nin $O(p(m))$ time, which is independent of the sizes of the input sets. No\nexisting $O(p(m)n^{1-\\epsilon})$ time $(1-{1\\over e})$-approximation algorithm\nfor the maximum coverage has been found for any function $p(m)$ that only\ndepends on the number of sets, where $n=\\max(|A_1|,\\cdots,| A_m|)$ (the largest\nsize of input sets). The notion of partial sublinear time algorithm is\nintroduced. For a computational problem with input size controlled by two\nparameters $n$ and $m$, a partial sublinear time algorithm for it runs in a\n$O(p(m)n^{1-\\epsilon})$ time or $O(q(n)m^{1-\\epsilon})$ time. The maximum\ncoverage has a partial sublinear time $O(p(m))$ constant factor approximation.\nOn the other hand, we show that the maximum coverage problem has no partial\nsublinear $O(q(n)m^{1-\\epsilon})$ time constant factor approximation algorithm.\nIt separates the partial sublinear time computation from the conventional\nsublinear time computation by disproving the existence of sublinear time\napproximation algorithm for the maximum coverage problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 20:52:22 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 04:31:44 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Fu", "Bin", ""]]}, {"id": "1604.01429", "submitter": "Jalaj Upadhyay", "authors": "Jalaj Upadhyay", "title": "The Price of Differential Privacy for Low-Rank Factorization", "comments": "76 pages, 2 figures (added results for local model)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study what price one has to pay to release {\\em\ndifferentially private low-rank factorization} of a matrix. We consider various\nsettings that are close to the real world applications of low-rank\nfactorization: (i) the manner in which matrices are updated (row by row or in\nan arbitrary manner), (ii) whether matrices are distributed or not, and (iii)\nhow the output is produced (once at the end of all updates, also known as {\\em\none-shot algorithms} or continually). Even though these settings are well\nstudied without privacy, surprisingly, there are no private algorithm for these\nsettings (except when a matrix is updated row by row). We present the first set\nof differentially private algorithms for all these settings.\n  Our algorithms when private matrix is updated in an arbitrary manner promise\ndifferential privacy with respect to two stronger privacy guarantees than\npreviously studied, use space and time {\\em comparable} to the non-private\nalgorithm, and achieve {\\em optimal accuracy}. To complement our positive\nresults, we also prove that the space required by our algorithms is optimal up\nto logarithmic factors. When data matrices are distributed over multiple\nservers, we give a non-interactive differentially private algorithm with\ncommunication cost independent of dimension. In concise, we give algorithms\nthat incur optimal cost. We also perform experiments to verify that all our\nalgorithms perform well in practice and outperform the best known algorithms\nuntil now for large range of parameters. We give experimental results for total\napproximation error and additive error for varying dimensions, $\\alpha$ and\n$k$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 21:12:01 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 23:30:46 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 12:30:34 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 23:54:14 GMT"}, {"version": "v5", "created": "Wed, 18 Apr 2018 02:00:21 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Upadhyay", "Jalaj", ""]]}, {"id": "1604.01495", "submitter": "Mojgan Pourhassan", "authors": "Mojgan Pourhassan, Feng Shi, Frank Neumann", "title": "Parameterized Analysis of Multi-objective Evolutionary Algorithms and\n  the Weighted Vertex Cover Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rigorous runtime analysis of evolutionary multi-objective optimization for\nthe classical vertex cover problem in the context of parameterized complexity\nanalysis has been presented by Kratsch and Neumann (2013). In this paper, we\nextend the analysis to the weighted vertex cover problem and provide a fixed\nparameter evolutionary algorithm with respect to OPT, the cost of the the\noptimal solution for the problem. Moreover, using a diversity mechanisms, we\npresent a multi-objective evolutionary algorithm that finds a 2-approximation\nin expected polynomial time and introduce a population-based evolutionary\nalgorithm which finds a $(1+\\varepsilon)$-approximation in expected time\n$O(n\\cdot 2^{\\min \\{n,2(1- \\varepsilon)OPT \\}} + n^3)$.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 05:56:26 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Pourhassan", "Mojgan", ""], ["Shi", "Feng", ""], ["Neumann", "Frank", ""]]}, {"id": "1604.01550", "submitter": "R\\'emi Watrigant", "authors": "Jason Crampton, Gregory Gutin, R\\'emi Watrigant", "title": "A Multivariate Approach for Checking Resiliency in Access Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several combinatorial problems were introduced in the area\nof access control. Typically, such problems deal with an authorization policy,\nseen as a relation $UR \\subseteq U \\times R$, where $(u, r) \\in UR$ means that\nuser $u$ is authorized to access resource $r$. Li, Tripunitara and Wang (2009)\nintroduced the Resiliency Checking Problem (RCP), in which we are given an\nauthorization policy, a subset of resources $P \\subseteq R$, as well as\nintegers $s \\ge 0$, $d \\ge 1$ and $t \\geq 1$. It asks whether upon removal of\nany set of at most $s$ users, there still exist $d$ pairwise disjoint sets of\nat most $t$ users such that each set has collectively access to all resources\nin $P$. This problem possesses several parameters which appear to take small\nvalues in practice. We thus analyze the parameterized complexity of RCP with\nrespect to these parameters, by considering all possible combinations of $|P|,\ns, d, t$. In all but one case, we are able to settle whether the problem is in\nFPT, XP, W[2]-hard, para-NP-hard or para-coNP-hard. We also consider the\nrestricted case where $s=0$ for which we determine the complexity for all\npossible combinations of the parameters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 09:20:24 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 11:42:34 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Crampton", "Jason", ""], ["Gutin", "Gregory", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "1604.01697", "submitter": "Alan Roytman", "authors": "Yossi Azar, Ilan Reuven Cohen, Alan Roytman", "title": "Online Lower Bounds via Duality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exploit linear programming duality in the online setting\n(i.e., where input arrives on the fly) from the unique perspective of designing\nlower bounds on the competitive ratio. In particular, we provide a general\ntechnique for obtaining online deterministic and randomized lower bounds (i.e.,\nhardness results) on the competitive ratio for a wide variety of problems. We\nshow the usefulness of our approach by providing new, tight lower bounds for\nthree diverse online problems. The three problems we show tight lower bounds\nfor are the Vector Bin Packing problem, Ad-auctions (and various online\nmatching problems), and the Capital Investment problem. Our methods are\nsufficiently general that they can also be used to reconstruct existing lower\nbounds.\n  Our techniques are in stark contrast to previous works, which exploit linear\nprogramming duality to obtain positive results, often via the useful\nprimal-dual scheme. We design a general recipe with the opposite aim of\nobtaining negative results via duality. The general idea behind our approach is\nto construct a primal linear program based on a collection of input sequences,\nwhere the objective function corresponds to optimizing the competitive ratio.\nWe then obtain the corresponding dual linear program and provide a feasible\nsolution, where the objective function yields a lower bound on the competitive\nratio. Online lower bounds are often achieved by adapting the input sequence\naccording to an online algorithm's behavior and doing an appropriate ad hoc\ncase analysis. Using our unifying techniques, we simultaneously combine these\ncases into one linear program and achieve online lower bounds via a more robust\nanalysis. We are confident that our framework can be successfully applied to\nproduce many more lower bounds for a wide array of online problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 17:24:51 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Azar", "Yossi", ""], ["Cohen", "Ilan Reuven", ""], ["Roytman", "Alan", ""]]}, {"id": "1604.01789", "submitter": "Mohammed Alser", "authors": "Mohammed Alser, Hasan Hassan, Hongyi Xin, O\\u{g}uz Ergin, Onur Mutlu,\n  and Can Alkan", "title": "GateKeeper: A New Hardware Architecture for Accelerating Pre-Alignment\n  in DNA Short Read Mapping", "comments": null, "journal-ref": "Bioinformatics. Nov 1;33(21):3355-3363, 2017", "doi": "10.1093/bioinformatics/btx342", "report-no": null, "categories": "q-bio.GN cs.AR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: High throughput DNA sequencing (HTS) technologies generate an\nexcessive number of small DNA segments -- called short reads -- that cause\nsignificant computational burden. To analyze the entire genome, each of the\nbillions of short reads must be mapped to a reference genome based on the\nsimilarity between a read and \"candidate\" locations in that reference genome.\nThe similarity measurement, called alignment, formulated as an approximate\nstring matching problem, is the computational bottleneck because: (1) it is\nimplemented using quadratic-time dynamic programming algorithms, and (2) the\nmajority of candidate locations in the reference genome do not align with a\ngiven read due to high dissimilarity. Calculating the alignment of such\nincorrect candidate locations consumes an overwhelming majority of a modern\nread mapper's execution time. Therefore, it is crucial to develop a fast and\neffective filter that can detect incorrect candidate locations and eliminate\nthem before invoking computationally costly alignment operations. Results: We\npropose GateKeeper, a new hardware accelerator that functions as a\npre-alignment step that quickly filters out most incorrect candidate locations.\nGateKeeper is the first design to accelerate pre-alignment using\nField-Programmable Gate Arrays (FPGAs), which can perform pre-alignment much\nfaster than software. GateKeeper can be integrated with any mapper that\nperforms sequence alignment for verification. When implemented on a single FPGA\nchip, GateKeeper maintains high accuracy (on average >96%) while providing up\nto 90-fold and 130-fold speedup over the state-of-the-art software\npre-alignment techniques, Adjacency Filter and Shifted Hamming Distance (SHD),\nrespectively. The addition of GateKeeper as a pre-alignment step can reduce the\nverification time of the mrFAST mapper by a factor of 10. Availability:\nhttps://github.com/BilkentCompGen/GateKeeper\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 20:04:56 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 16:57:38 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 00:31:25 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Alser", "Mohammed", ""], ["Hassan", "Hasan", ""], ["Xin", "Hongyi", ""], ["Ergin", "O\u011fuz", ""], ["Mutlu", "Onur", ""], ["Alkan", "Can", ""]]}, {"id": "1604.01839", "submitter": "Barna Saha", "authors": "Arya Mazumdar, Barna Saha", "title": "Clustering Via Crowdsourcing", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, crowdsourcing, aka human aided computation has emerged as an\neffective platform for solving problems that are considered complex for\nmachines alone. Using human is time-consuming and costly due to monetary\ncompensations. Therefore, a crowd based algorithm must judiciously use any\ninformation computed through an automated process, and ask minimum number of\nquestions to the crowd adaptively.\n  One such problem which has received significant attention is {\\em entity\nresolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$\nwhere $G$ is a union of $k$ (again unknown, but typically large $O(n^\\alpha)$,\nfor $\\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \\dots, k$. The goal is\nto retrieve the sets $V_i$s by making minimum number of pair-wise queries $V\n\\times V\\to\\{\\pm1\\}$ to an oracle (the crowd). When the answer to each query is\ncorrect, e.g. via resampling, then this reduces to finding connected components\nin a graph. On the other hand, when crowd answers may be incorrect, it\ncorresponds to clustering over minimum number of noisy inputs. Even, with\nperfect answers, a simple lower and upper bound of $\\Theta(nk)$ on query\ncomplexity can be shown. A major contribution of this paper is to reduce the\nquery complexity to linear or even sublinear in $n$ when mild side information\nis provided by a machine, and even in presence of crowd errors which are not\ncorrectable via resampling. We develop new information theoretic lower bounds\non the query complexity of clustering with side information and errors, and our\nupper bounds closely match with them. Our algorithms are naturally\nparallelizable, and also give near-optimal bounds on the number of adaptive\nrounds required to match the query complexity.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 00:58:23 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Mazumdar", "Arya", ""], ["Saha", "Barna", ""]]}, {"id": "1604.02094", "submitter": "Sebastian Krinninger", "authors": "Ittai Abraham, David Durfee, Ioannis Koutis, Sebastian Krinninger,\n  Richard Peng", "title": "On Fully Dynamic Graph Sparsifiers", "comments": "A preliminary version of this paper appears in the 57th Annual IEEE\n  Symposium on Foundations of Computer Science (FOCS 2016)", "journal-ref": null, "doi": "10.1109/FOCS.2016.44", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of dynamic algorithms for graph sparsification problems\nand obtain fully dynamic algorithms, allowing both edge insertions and edge\ndeletions, that take polylogarithmic time after each update in the graph. Our\nthree main results are as follows. First, we give a fully dynamic algorithm for\nmaintaining a $ (1 \\pm \\epsilon) $-spectral sparsifier with amortized update\ntime $poly(\\log{n}, \\epsilon^{-1})$. Second, we give a fully dynamic algorithm\nfor maintaining a $ (1 \\pm \\epsilon) $-cut sparsifier with \\emph{worst-case}\nupdate time $poly(\\log{n}, \\epsilon^{-1})$. Both sparsifiers have size $ n\n\\cdot poly(\\log{n}, \\epsilon^{-1})$. Third, we apply our dynamic sparsifier\nalgorithm to obtain a fully dynamic algorithm for maintaining a $(1 +\n\\epsilon)$-approximation to the value of the maximum flow in an unweighted,\nundirected, bipartite graph with amortized update time $poly(\\log{n},\n\\epsilon^{-1})$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 18:06:49 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 13:43:37 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Abraham", "Ittai", ""], ["Durfee", "David", ""], ["Koutis", "Ioannis", ""], ["Krinninger", "Sebastian", ""], ["Peng", "Richard", ""]]}, {"id": "1604.02188", "submitter": "Sepideh Mahabadi", "authors": "Piotr Indyk, Robert Kleinberg, Sepideh Mahabadi, Yang Yuan", "title": "Simultaneous Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.SoCG.2016.44", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in computer vision and databases, we introduce and\nstudy the Simultaneous Nearest Neighbor Search (SNN) problem. Given a set of\ndata points, the goal of SNN is to design a data structure that, given a\ncollection of queries, finds a collection of close points that are compatible\nwith each other. Formally, we are given $k$ query points $Q=q_1,\\cdots,q_k$,\nand a compatibility graph $G$ with vertices in $Q$, and the goal is to return\ndata points $p_1,\\cdots,p_k$ that minimize (i) the weighted sum of the\ndistances from $q_i$ to $p_i$ and (ii) the weighted sum, over all edges $(i,j)$\nin the compatibility graph $G$, of the distances between $p_i$ and $p_j$. The\nproblem has several applications, where one wants to return a set of consistent\nanswers to multiple related queries. This generalizes well-studied\ncomputational problems, including NN, Aggregate NN and the 0-extension problem.\n  In this paper we propose and analyze the following general two-step method\nfor designing efficient data structures for SNN. In the first step, for each\nquery point $q_i$ we find its (approximate) nearest neighbor point $\\hat{p}_i$;\nthis can be done efficiently using existing approximate nearest neighbor\nstructures. In the second step, we solve an off-line optimization problem over\nsets $q_1,\\cdots,q_k$ and $\\hat{p}_1,\\cdots,\\hat{p}_k$; this can be done\nefficiently given that $k$ is much smaller than $n$. Even though\n$\\hat{p}_1,\\cdots,\\hat{p}_k$ might not constitute the optimal answers to\nqueries $q_1,\\cdots,q_k$, we show that, for the unweighted case, the resulting\nalgorithm is $O(\\log k/\\log \\log k)$-approximation. Also, we show that the\napproximation factor can be in fact reduced to a constant for compatibility\ngraphs frequently occurring in practice.\n  Finally, we show that the \"empirical approximation factor\" provided by the\nabove approach is very close to 1.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 22:31:38 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Indyk", "Piotr", ""], ["Kleinberg", "Robert", ""], ["Mahabadi", "Sepideh", ""], ["Yuan", "Yang", ""]]}, {"id": "1604.02216", "submitter": "Hao Yu", "authors": "Hao Yu and Michael J. Neely", "title": "A Primal-Dual Type Algorithm with the $O(1/t)$ Convergence Rate for\n  Large Scale Constrained Convex Programs", "comments": "18 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1512.08370", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers large scale constrained convex programs, which are\nusually not solvable by interior point methods or other Newton-type methods due\nto the prohibitive computation and storage complexity for Hessians and matrix\ninversions. Instead, large scale constrained convex programs are often solved\nby gradient based methods or decomposition based methods. The conventional\nprimal-dual subgradient method, aka, Arrow-Hurwicz-Uzawa subgradient method, is\na low complexity algorithm with the $O(1/\\sqrt{t})$ convergence rate, where $t$\nis the number of iterations. If the objective and constraint functions are\nseparable, the Lagrangian dual type method can decompose a large scale convex\nprogram into multiple parallel small scale convex programs. The classical dual\ngradient algorithm is an example of Lagrangian dual type methods and has\nconvergence rate $O(1/\\sqrt{t})$. Recently, a new Lagrangian dual type\nalgorithm with faster $O(1/t)$ convergence is proposed in Yu and Neely (2015).\nHowever, if the objective or constraint functions are not separable, each\niteration of the Lagrangian dual type method in Yu and Neely (2015) requires to\nsolve a large scale unconstrained convex program, which can have huge\ncomplexity. This paper proposes a new primal-dual type algorithm, which only\ninvolves simple gradient updates at each iteration and has the $O(1/t)$\nconvergence rate.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 03:25:32 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Yu", "Hao", ""], ["Neely", "Michael J.", ""]]}, {"id": "1604.02278", "submitter": "Siao-Leu Phouratsamay", "authors": "Siao-Leu Phouratsamay, Safia Kedad-Sidhoum, Fanny Pascual", "title": "Two-level lot-sizing with inventory bounds", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a two-level uncapacitated lot-sizing problem with inventory bounds\nthat occurs in a supply chain composed of a supplier and a retailer. The first\nlevel with the demands is the retailer level and the second one is the supplier\nlevel. The aim is to minimize the cost of the supply chain so as to satisfy the\ndemands when the quantity of item that can be held in inventory at each period\nis limited. The inventory bounds can be imposed at the retailer level, at the\nsupplier level or at both levels. We propose a polynomial dynamic programming\nalgorithm to solve this problem when the inventory bounds are set on the\nretailer level. When the inventory bounds are set on the supplier level, we\nshow that the problem is NP-hard. We give a pseudo-polynomial algorithm which\nsolves this problem when there are inventory bounds on both levels. In the case\nwhere demand lot-splitting is not allowed, i.e. each demand has to be satisfied\nby a single order, we prove that the uncapacitated lot-sizing problem with\ninventory bounds is strongly NP-hard. This implies that the two-level\nlot-sizing problems with inventory bounds are also strongly NP-hard when demand\nlot-splitting is considered.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 08:55:11 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 14:40:23 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Phouratsamay", "Siao-Leu", ""], ["Kedad-Sidhoum", "Safia", ""], ["Pascual", "Fanny", ""]]}, {"id": "1604.02450", "submitter": "Roy Friedman", "authors": "Ran Ben Basat, Gil Einziger, Roy Friedman, Yaron Kassner", "title": "Efficient Summing over Sliding Windows", "comments": "A shorter version appears in SWAT 2016", "journal-ref": null, "doi": "10.4230/LIPIcs.SWAT.2016", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of maintaining statistic aggregates over the\nlast W elements of a data stream. First, the problem of counting the number of\n1's in the last W bits of a binary stream is considered. A lower bound of\n{\\Omega}(1/{\\epsilon} + log W) memory bits for W{\\epsilon}-additive\napproximations is derived. This is followed by an algorithm whose memory\nconsumption is O(1/{\\epsilon} + log W) bits, indicating that the algorithm is\noptimal and that the bound is tight. Next, the more general problem of\nmaintaining a sum of the last W integers, each in the range of {0,1,...,R}, is\naddressed. The paper shows that approximating the sum within an additive error\nof RW{\\epsilon} can also be done using {\\Theta}(1/{\\epsilon} + log W) bits for\n{\\epsilon}={\\Omega}(1/W). For {\\epsilon}=o(1/W), we present a succinct\nalgorithm which uses B(1 + o(1)) bits, where B={\\Theta}(Wlog(1/W{\\epsilon})) is\nthe derived lower bound. We show that all lower bounds generalize to randomized\nalgorithms as well. All algorithms process new elements and answer queries in\nO(1) worst-case time.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 11:42:45 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Friedman", "Roy", ""], ["Kassner", "Yaron", ""]]}, {"id": "1604.02486", "submitter": "Anke van Zuylen", "authors": "Andr\\'as Seb\\H{o} and Anke van Zuylen", "title": "The Salesman's Improved Paths: 3/2+1/34 Integrality Gap and\n  Approximation Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new, strongly polynomial-time algorithm and improved analysis for\nthe metric $s-t$ path TSP. It finds a tour of cost less than 1.53 times the\noptimum of the subtour elimination LP, while known examples show that 1.5 is a\nlower bound for the integrality gap.\n  A key new idea is the deletion of some edges of Christofides' trees, which is\nthen accompanied by novel arguments of the analysis: edge-deletion disconnects\nthe trees, which are then partly reconnected by `parity correction'. We show\nthat the arising `connectivity correction' can be achieved for a minor extra\ncost.\n  On the one hand this algorithm and analysis extend previous tools such as the\nbest-of-many Christofides algorithm. On the other hand, powerful new tools are\nsolicited, such as a flow problem for analyzing the reconnection cost, and the\nconstruction of a set of more and more restrictive spanning trees, each of\nwhich can still be found by the greedy algorithm. We show that these trees can\nreplace the convex combination of spanning trees in the best-of-may\nChristofides algorithm.\n  These new methods lead to improving the integrality ratio and approximation\nguarantee below 1.53, as it is already sketched in the preliminary shortened\nversion of this article that appeared in FOCS 2016. The algorithm and analysis\nhave been significantly simplified in the current article, and details of\nproofs and explanations have been added.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 21:05:05 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 08:31:50 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Seb\u0151", "Andr\u00e1s", ""], ["van Zuylen", "Anke", ""]]}, {"id": "1604.02552", "submitter": "Youhuan Li", "authors": "Youhuan Li, Lei Zou, Huaming Zhang, Dongyan Zhao", "title": "Computing Longest Increasing Subsequence Over Sequential Data Streams", "comments": "20 pages (12+8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a data structure, a quadruple neighbor list\n(QN-list, for short), to support real time queries of all longest increasing\nsubsequence (LIS) and LIS with constraints over sequential data streams. The\nQN-List built by our algorithm requires $O(w)$ space, where $w$ is the time\nwindow size. The running time for building the initial QN-List takes $O(w\\log\nw)$ time. Applying the QN-List, insertion of the new item takes $O(\\log w)$\ntime and deletion of the first item takes $O(w)$ time. To the best of our\nknowledge, this is the first work to support both LIS enumeration and LIS with\nconstraints computation by using a single uniform data structure for real time\nsequential data streams. Our method outperforms the state-of-the-art methods in\nboth time and space cost, not only theoretically, but also empirically.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 11:48:47 GMT"}, {"version": "v2", "created": "Sun, 1 May 2016 08:47:21 GMT"}, {"version": "v3", "created": "Tue, 16 Aug 2016 05:30:00 GMT"}, {"version": "v4", "created": "Tue, 11 Oct 2016 14:40:46 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Li", "Youhuan", ""], ["Zou", "Lei", ""], ["Zhang", "Huaming", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1604.02605", "submitter": "Mohammed El-Kebir", "authors": "Mohammed El-Kebir and Gryte Satas and Layla Oesper and Benjamin J.\n  Raphael", "title": "Multi-State Perfect Phylogeny Mixture Deconvolution and Applications to\n  Cancer Sequencing", "comments": "RECOMB 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of phylogenetic trees from mixed populations has become\nimportant in the study of cancer evolution, as sequencing is often performed on\nbulk tumor tissue containing mixed populations of cells. Recent work has shown\nhow to reconstruct a perfect phylogeny tree from samples that contain mixtures\nof two-state characters, where each character/locus is either mutated or not.\nHowever, most cancers contain more complex mutations, such as copy-number\naberrations, that exhibit more than two states. We formulate the Multi-State\nPerfect Phylogeny Mixture Deconvolution Problem of reconstructing a multi-state\nperfect phylogeny tree given mixtures of the leaves of the tree. We\ncharacterize the solutions of this problem as a restricted class of spanning\ntrees in a graph constructed from the input data, and prove that the problem is\nNP-complete. We derive an algorithm to enumerate such trees in the important\nspecial case of cladisitic characters, where the ordering of the states of each\ncharacter is given. We apply our algorithm to simulated data and to two cancer\ndatasets. On simulated data, we find that for a small number of samples, the\nMulti-State Perfect Phylogeny Mixture Deconvolution Problem often has many\nsolutions, but that this ambiguity declines quickly as the number of samples\nincreases. On real data, we recover copy-neutral loss of heterozygosity,\nsingle-copy amplification and single-copy deletion events, as well as their\ninteractions with single-nucleotide variants.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 20:00:07 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["El-Kebir", "Mohammed", ""], ["Satas", "Gryte", ""], ["Oesper", "Layla", ""], ["Raphael", "Benjamin J.", ""]]}, {"id": "1604.02691", "submitter": "Krasimir Yordzhev", "authors": "Krasimir Yordzhev", "title": "On an algorithm for receiving Sudoku matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the problem to describe an efficient algorithm for\nobtaining $n^2 \\times n^2$ Sudoku matrices. For this purpose, we define the\nconcepts of $n\\times n$ $\\Pi_n$-matrix and disjoint $\\Pi_n$-matrices. The\narticle, using the set-theoretical approach, describes an algorithm for\nobtaining $n^2$-tuples of $n\\times n$ mutually disjoint $\\Pi_n$ matrices. We\nshow that in input $n^2$ mutually disjoint $\\Pi_n$ matrices, it is not\ndifficult to receive a Sudoku matrix.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 14:06:47 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Yordzhev", "Krasimir", ""]]}, {"id": "1604.02711", "submitter": "Loukas Georgiadis", "authors": "Loukas Georgiadis and Giuseppe F. Italiano and Luigi Laura and\n  Federico Santaroni", "title": "An Experimental Study of Dynamic Dominators", "comments": "A preliminary version of this paper appeared in Proc. ESA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent applications of dominator computations, we consider the\nproblem of dynamically maintaining the dominators of flow graphs through a\nsequence of insertions and deletions of edges. Our main theoretical\ncontribution is a simple incremental algorithm that maintains the dominator\ntree of a flow graph with $n$ vertices through a sequence of $k$ edge\ninsertions in $O(m\\min\\{n,k\\}+kn)$ time, where $m$ is the total number of edges\nafter all insertions. Moreover, we can test in constant time if a vertex $u$\ndominates a vertex $v$, for any pair of query vertices $u$ and $v$. Next, we\npresent a new decremental algorithm to update a dominator tree through a\nsequence of edge deletions. Although our new decremental algorithm is not\nasymptotically faster than repeated applications of a static algorithm, i.e.,\nit runs in $O(mk)$ time for $k$ edge deletions, it performs well in practice.\nBy combining our new incremental and decremental algorithms we obtain a fully\ndynamic algorithm that maintains the dominator tree through intermixed sequence\nof insertions and deletions of edges. Finally, we present efficient\nimplementations of our new algorithms as well as of existing algorithms, and\nconduct an extensive experimental study on real-world graphs taken from a\nvariety of application areas.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 15:56:43 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Italiano", "Giuseppe F.", ""], ["Laura", "Luigi", ""], ["Santaroni", "Federico", ""]]}, {"id": "1604.02833", "submitter": "Batya Kenig", "authors": "Nofar Carmeli, Batya Kenig and Benny Kimelfeld", "title": "On the Enumeration of all Minimal Triangulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that enumerates all the minimal triangulations of a\ngraph in incremental polynomial time. Consequently, we get an algorithm for\nenumerating all the proper tree decompositions, in incremental polynomial time,\nwhere \"proper\" means that the tree decomposition cannot be improved by removing\nor splitting a bag.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 08:39:28 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Carmeli", "Nofar", ""], ["Kenig", "Batya", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "1604.02934", "submitter": "Racha El-Hajj", "authors": "Racha El-Hajj (UL, Heudiasyc, Labex MS2T), Duc-Cuong Dang (Heudiasyc,\n  Labex MS2T, UON), Aziz Moukrim (Labex MS2T, Heudiasyc)", "title": "Solving the Team Orienteering Problem with Cutting Planes", "comments": null, "journal-ref": "Computers and Operations Research, Elsevier, 2016, pp.27", "doi": "10.1016/j.cor.2016.04.008", "report-no": null, "categories": "cs.RO cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Team Orienteering Problem (TOP) is an attractive variant of the Vehicle\nRouting Problem (VRP). The aim is to select customers and at the same time\norganize the visits for a vehicle fleet so as to maximize the collected profits\nand subject to a travel time restriction on each vehicle. In this paper, we\ninvestigate the effective use of a linear formulation with polynomial number of\nvariables to solve TOP. Cutting planes are the core components of our solving\nalgorithm. It is first used to solve smaller and intermediate models of the\noriginal problem by considering fewer vehicles. Useful information are then\nretrieved to solve larger models, and eventually reaching the original problem.\nRelatively new and dedicated methods for TOP, such as identification of\nirrelevant arcs and mandatory customers, clique and independent-set cuts based\non the incompatibilities, and profit/customer restriction on subsets of\nvehicles, are introduced. We evaluated our algorithm on the standard benchmark\nof TOP. The results show that the algorithm is competitive and is able to prove\nthe optimality for 12 instances previously unsolved.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 13:08:48 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["El-Hajj", "Racha", "", "UL, Heudiasyc, Labex MS2T"], ["Dang", "Duc-Cuong", "", "Heudiasyc,\n  Labex MS2T, UON"], ["Moukrim", "Aziz", "", "Labex MS2T, Heudiasyc"]]}, {"id": "1604.03008", "submitter": "Ignasi Sau", "authors": "Julien Baste, Christophe Paul, Ignasi Sau, Celine Scornavacca", "title": "Efficient FPT algorithms for (strict) compatibility of unrooted\n  phylogenetic trees", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In phylogenetics, a central problem is to infer the evolutionary\nrelationships between a set of species $X$; these relationships are often\ndepicted via a phylogenetic tree -- a tree having its leaves univocally labeled\nby elements of $X$ and without degree-2 nodes -- called the \"species tree\". One\ncommon approach for reconstructing a species tree consists in first\nconstructing several phylogenetic trees from primary data (e.g. DNA sequences\noriginating from some species in $X$), and then constructing a single\nphylogenetic tree maximizing the \"concordance\" with the input trees. The\nso-obtained tree is our estimation of the species tree and, when the input\ntrees are defined on overlapping -- but not identical -- sets of labels, is\ncalled \"supertree\". In this paper, we focus on two problems that are central\nwhen combining phylogenetic trees into a supertree: the compatibility and the\nstrict compatibility problems for unrooted phylogenetic trees. These problems\nare strongly related, respectively, to the notions of \"containing as a minor\"\nand \"containing as a topological minor\" in the graph community. Both problems\nare known to be fixed-parameter tractable in the number of input trees $k$, by\nusing their expressibility in Monadic Second Order Logic and a reduction to\ngraphs of bounded treewidth. Motivated by the fact that the dependency on $k$\nof these algorithms is prohibitively large, we give the first explicit dynamic\nprogramming algorithms for solving these problems, both running in time\n$2^{O(k^2)} \\cdot n$, where $n$ is the total size of the input.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 15:49:14 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Baste", "Julien", ""], ["Paul", "Christophe", ""], ["Sau", "Ignasi", ""], ["Scornavacca", "Celine", ""]]}, {"id": "1604.03009", "submitter": "Konstantinos Georgiou", "authors": "Konstantinos Georgiou and George Karakostas and Evangelos Kranakis and\n  Danny Krizanc", "title": "Know When to Persist: Deriving Value from a Stream Buffer", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider \\textsc{Persistence}, a new online problem concerning optimizing\nweighted observations in a stream of data when the observer has limited buffer\ncapacity. A stream of weighted items arrive one at a time at the entrance of a\nbuffer with two holding locations. A processor (or observer) can process\n(observe) an item at the buffer location it chooses, deriving this way the\nweight of the observed item as profit. The main constraint is that the\nprocessor can only move {\\em synchronously} with the item stream; as a result,\nmoving from the end of the buffer to the entrance, it crosses paths with the\nitem already there, and will never have the chance to process or even identify\nit. \\textsc{Persistence}\\ is the online problem of scheduling the processor\nmovements through the buffer so that its total derived value is maximized under\nthis constraint.\n  We study the performance of the straight-forward heuristic {\\em Threshold},\ni.e., forcing the processor to \"follow\" an item through the whole buffer only\nif its value is above a threshold. We analyze both the optimal offline and\nThreshold algorithms in case the input stream is a random permutation, or its\nitems are iid valued. We show that in both cases the competitive ratio achieved\nby the Threshold algorithm is at least $2/3$ when the only statistical\nknowledge of the items is the median of all possible values. We generalize our\nresults by showing that Threshold, equipped with some minimal statistical\nadvice about the input, achieves competitive ratios in the whole spectrum\nbetween $2/3$ and $1$, following the variation of a newly defined density-like\nmeasure of the input. This result is a significant improvement over the case of\narbitrary input streams, since in this case we show that no online algorithm\ncan achieve a competitive ratio better than $1/2$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 15:52:04 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Georgiou", "Konstantinos", ""], ["Karakostas", "George", ""], ["Kranakis", "Evangelos", ""], ["Krizanc", "Danny", ""]]}, {"id": "1604.03030", "submitter": "Huacheng Yu", "authors": "Omri Weinstein, Huacheng Yu", "title": "Amortized Dynamic Cell-Probe Lower Bounds from Four-Party Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new technique for proving amortized, randomized\ncell-probe lower bounds on dynamic data structure problems. We introduce a new\nrandomized nondeterministic four-party communication model that enables\n\"accelerated\", error-preserving simulations of dynamic data structures.\n  We use this technique to prove an $\\Omega(n(\\log n/\\log\\log n)^2)$ cell-probe\nlower bound for the dynamic 2D weighted orthogonal range counting problem\n(2D-ORC) with $n/\\mathrm{poly}\\log n$ updates and $n$ queries, that holds even\nfor data structures with $\\exp(-\\tilde{\\Omega}(n))$ success probability. This\nresult not only proves the highest amortized lower bound to date, but is also\ntight in the strongest possible sense, as a matching upper bound can be\nobtained by a deterministic data structure with worst-case operational time.\nThis is the first demonstration of a \"sharp threshold\" phenomenon for dynamic\ndata structures.\n  Our broader motivation is that cell-probe lower bounds for exponentially\nsmall success facilitate reductions from dynamic to static data structures. As\na proof-of-concept, we show that a slightly strengthened version of our lower\nbound would imply an $\\Omega((\\log n /\\log\\log n)^2)$ lower bound for the\nstatic 3D-ORC problem with $O(n\\log^{O(1)}n)$ space. Such result would give a\nnear quadratic improvement over the highest known static cell-probe lower\nbound, and break the long standing $\\Omega(\\log n)$ barrier for static data\nstructures.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 17:02:07 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Weinstein", "Omri", ""], ["Yu", "Huacheng", ""]]}, {"id": "1604.03072", "submitter": "Anand Kumar Narayanan", "authors": "Anand Kumar Narayanan", "title": "Fast Computation of Isomorphisms Between Finite Fields Using Elliptic\n  Curves", "comments": "Springer LNCS (International Workshop on the Arithmetic of Finite\n  Fields) WAIFI 2018 proceedings version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized algorithm to compute isomorphisms between finite\nfields using elliptic curves. To compute an isomorphism between two fields of\ncardinality $q^n$, our algorithm takes $$n^{1+o(1)} \\log^{1+o(1)}q +\n\\max_{\\ell} \\left(\\ell^{n_\\ell + 1+o(1)} \\log^{2+o(1)} q + O(\\ell\n\\log^5q)\\right)$$ time, where $\\ell$ runs through primes dividing $n$ but not\n$q(q-1)$ and $n_\\ell$ denotes the highest power of $\\ell$ dividing $n$. Prior\nto this work, the best known run time dependence on $n$ was quadratic. Our run\ntime dependence on $n$ is at worst quadratic but is subquadratic if $n$ has no\nlarge prime factor. In particular, the $n$ for which our run time is nearly\nlinear in $n$ have natural density at least $3/10$. The crux of our approach is\nfinding a point on an elliptic curve of a prescribed prime power order or\nequivalently finding preimages under the Lang map on elliptic curves over\nfinite fields. We formulate this as an open problem whose resolution would\nsolve the finite field isomorphism problem with run time nearly linear in $n$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 19:11:02 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 02:16:25 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 02:51:44 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Narayanan", "Anand Kumar", ""]]}, {"id": "1604.03132", "submitter": "Robert Patro", "authors": "Nitish Gupta, Komal Sanjeev, Tim Wall, Carl Kingsford, Rob Patro", "title": "Efficient Index Maintenance Under Dynamic Genome Modification", "comments": "paper accepted at the RECOMB-Seq 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient text indexing data structures have enabled large-scale genomic\nsequence analysis and are used to help solve problems ranging from assembly to\nread mapping. However, these data structures typically assume that the\nunderlying reference text is static and will not change over the course of the\nqueries being made. Some progress has been made in exploring how certain text\nindices, like the suffix array, may be updated, rather than rebuilt from\nscratch, when the underlying reference changes. Yet, these update operations\ncan be complex in practice, difficult to implement, and give fairly pessimistic\nworst-case bounds. We present a novel data structure, SkipPatch, for\nmaintaining a k-mer-based index over a dynamically changing genome. SkipPatch\npairs a hash-based k-mer index with an indexable skip list that is used to\nefficiently maintain the set of edits that have been applied to the original\ngenome. SkipPatch is practically fast, significantly outperforming the dynamic\nextended suffix array in terms of update and query speed.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 20:10:48 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Gupta", "Nitish", ""], ["Sanjeev", "Komal", ""], ["Wall", "Tim", ""], ["Kingsford", "Carl", ""], ["Patro", "Rob", ""]]}, {"id": "1604.03228", "submitter": "Jessica McClintock", "authors": "Jessica McClintock, Anthony Wirth", "title": "Efficient Parallel Algorithms for k-Center Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-center problem is one of several classic NP-hard clustering questions.\nFor contemporary massive data sets, RAM-based algorithms become impractical.\nAnd although there exist good sequential algorithms for k-center, they are not\neasily parallelizable.\n  In this paper, we design and implement parallel approximation algorithms for\nthis problem. We observe that Gonzalez's greedy algorithm can be efficiently\nparallelized in several MapReduce rounds; in practice, we find that two rounds\nare sufficient, leading to a 4-approximation. We contrast this with an existing\nparallel algorithm for k-center that runs in a constant number of rounds, and\noffers a 10-approximation. In depth runtime analysis reveals that this scheme\nis often slow, and that its sampling procedure only runs if k is sufficiently\nsmall, relative to the input size. To trade off runtime for approximation\nguarantee, we parameterize this sampling algorithm, and find in our experiments\nthat the algorithm is not only faster, but sometimes more effective. Yet the\nparallel version of Gonzalez is about 100 times faster than both its sequential\nversion and the parallel sampling algorithm, barely compromising solution\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 03:04:11 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["McClintock", "Jessica", ""], ["Wirth", "Anthony", ""]]}, {"id": "1604.03243", "submitter": "Luke Mathieson", "authors": "Giuseppe Lancia, Luke Mathieson, Pablo Moscato", "title": "Separating Sets of Strings by Finding Matching Patterns is Almost Always\n  Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the problem of searching for a set of patterns\nthat separate two given sets of strings. This problem has applications in a\nwide variety of areas, most notably in data mining, computational biology, and\nin understanding the complexity of genetic algorithms. We show that the basic\nproblem of finding a small set of patterns that match one set of strings but do\nnot match any string in a second set is difficult (NP-complete, W[2]-hard when\nparameterized by the size of the pattern set, and APX-hard). We then perform a\ndetailed parameterized analysis of the problem, separating tractable and\nintractable variants. In particular we show that parameterizing by the size of\npattern set and the number of strings, and the size of the alphabet and the\nnumber of strings give FPT results, amongst others.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 04:37:35 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 03:31:41 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 00:50:32 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Lancia", "Giuseppe", ""], ["Mathieson", "Luke", ""], ["Moscato", "Pablo", ""]]}, {"id": "1604.03244", "submitter": "Guangyu Zhou", "authors": "Xiuli Ma, Guangyu Zhou, Jingjing Wang, Jian Peng, Jiawei Han", "title": "Complexes Detection in Biological Networks via Diversified Dense\n  Subgraphs Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein-protein interaction (PPI) networks, providing a comprehensive\nlandscape of protein interacting patterns, enable us to explore biological\nprocesses and cellular components at multiple resolutions. For a biological\nprocess, a number of proteins need to work together to perform the job.\nProteins densely interact with each other, forming large molecular machines or\ncellular building blocks. Identification of such densely interconnected\nclusters or protein complexes from PPI networks enables us to obtain a better\nunderstanding of the hierarchy and organization of biological processes and\ncellular components. Most existing methods apply efficient graph clustering\nalgorithms on PPI networks, often failing to detect possible densely connected\nsubgraphs and overlapped subgraphs. Besides clustering-based methods, dense\nsubgraph enumeration methods have also been used, which aim to find all densely\nconnected protein sets. However, such methods are not practically tractable\neven on a small yeast PPI network, due to high computational complexity. In\nthis paper, we introduce a novel approximate algorithm to efficiently enumerate\nputative protein complexes from biological networks. The key insight of our\nalgorithm is that we do not need to enumerate all dense subgraphs. Instead we\nonly need to find a small subset of subgraphs that cover as many proteins as\npossible. The problem is formulated as finding a diverse set of dense\nsubgraphs, where we develop highly effective pruning techniques to guarantee\nefficiency. To handle large networks, we take a divide-and-conquer approach to\nspeed up the algorithm in a distributed manner. By comparing with existing\nclustering and dense subgraph-based algorithms on several human and yeast PPI\nnetworks, we demonstrate that our method can detect more putative protein\ncomplexes and achieves better prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 04:39:28 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Ma", "Xiuli", ""], ["Zhou", "Guangyu", ""], ["Wang", "Jingjing", ""], ["Peng", "Jian", ""], ["Han", "Jiawei", ""]]}, {"id": "1604.03250", "submitter": "Robert Patro", "authors": "Avi Srivastava, Hirak Sarkar, Laraib Malik, Rob Patro", "title": "Accurate, Fast and Lightweight Clustering of de novo Transcriptomes\n  using Fragment Equivalence Classes", "comments": "paper accepted at the RECOMB-Seq 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: De novo transcriptome assembly of non-model organisms is the\nfirst major step for many RNA-seq analysis tasks. Current methods for de novo\nassembly often report a large number of contiguous sequences (contigs), which\nmay be fractured and incomplete sequences instead of full-length transcripts.\nDealing with a large number of such contigs can slow and complicate downstream\nanalysis.\n  Results :We present a method for clustering contigs from de novo\ntranscriptome assemblies based upon the relationships exposed by multi-mapping\nsequencing fragments. Specifically, we cast the problem of clustering contigs\nas one of clustering a sparse graph that is induced by equivalence classes of\nfragments that map to subsets of the transcriptome. Leveraging recent\ndevelopments in efficient read mapping and transcript quantification, we have\ndeveloped RapClust, a tool implementing this approach that is capable of\naccurately clustering most large de novo transcriptomes in a matter of minutes,\nwhile simultaneously providing accurate estimates of expression for the\nresulting clusters. We compare RapClust against a number of tools commonly used\nfor de novo transcriptome clustering. Using de novo assemblies of organisms for\nwhich reference genomes are available, we assess the accuracy of these\ndifferent methods in terms of the quality of the resulting clusterings, and the\nconcordance of differential expression tests with those based on ground truth\nclusters. We find that RapClust produces clusters of comparable or better\nquality than existing state-of-the-art approaches, and does so substantially\nfaster. RapClust also confers a large benefit in terms of space usage, as it\nproduces only succinct intermediate files - usually on the order of a few\nmegabytes - even when processing hundreds of millions of reads.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 05:23:37 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Srivastava", "Avi", ""], ["Sarkar", "Hirak", ""], ["Malik", "Laraib", ""], ["Patro", "Rob", ""]]}, {"id": "1604.03277", "submitter": "Carola Doerr", "authors": "Benjamin Doerr, Carola Doerr, Timo K\\\"otzing", "title": "The Right Mutation Strength for Multi-Valued Decision Variables", "comments": "an extended abstract of this work is to appear at GECCO 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most common representation in evolutionary computation are bit strings.\nThis is ideal to model binary decision variables, but less useful for variables\ntaking more values. With very little theoretical work existing on how to use\nevolutionary algorithms for such optimization problems, we study the run time\nof simple evolutionary algorithms on some OneMax-like functions defined over\n$\\Omega = \\{0, 1, \\dots, r-1\\}^n$. More precisely, we regard a variety of\nproblem classes requesting the component-wise minimization of the distance to\nan unknown target vector $z \\in \\Omega$. For such problems we see a crucial\ndifference in how we extend the standard-bit mutation operator to these\nmulti-valued domains. While it is natural to select each position of the\nsolution vector to be changed independently with probability $1/n$, there are\nvarious ways to then change such a position. If we change each selected\nposition to a random value different from the original one, we obtain an\nexpected run time of $\\Theta(nr \\log n)$. If we change each selected position\nby either $+1$ or $-1$ (random choice), the optimization time reduces to\n$\\Theta(nr + n\\log n)$. If we use a random mutation strength $i \\in\n\\{0,1,\\ldots,r-1\\}^n$ with probability inversely proportional to $i$ and change\nthe selected position by either $+i$ or $-i$ (random choice), then the\noptimization time becomes $\\Theta(n \\log(r)(\\log(n)+\\log(r)))$, bringing down\nthe dependence on $r$ from linear to polylogarithmic. One of our results\ndepends on a new variant of the lower bounding multiplicative drift theorem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 07:54:10 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""], ["K\u00f6tzing", "Timo", ""]]}, {"id": "1604.03336", "submitter": "Raef Bassily", "authors": "Raef Bassily and Yoav Freund", "title": "Typical Stability", "comments": "New sections, extended discussions, and complete proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a notion of algorithmic stability called typical\nstability. When our goal is to release real-valued queries (statistics)\ncomputed over a dataset, this notion does not require the queries to be of\nbounded sensitivity -- a condition that is generally assumed under differential\nprivacy [DMNS06, Dwork06] when used as a notion of algorithmic stability\n[DFHPRR15a, DFHPRR15b, BNSSSU16] -- nor does it require the samples in the\ndataset to be independent -- a condition that is usually assumed when\ngeneralization-error guarantees are sought. Instead, typical stability requires\nthe output of the query, when computed on a dataset drawn from the underlying\ndistribution, to be concentrated around its expected value with respect to that\ndistribution.\n  We discuss the implications of typical stability on the generalization error\n(i.e., the difference between the value of the query computed on the dataset\nand the expected value of the query with respect to the true data\ndistribution). We show that typical stability can control generalization error\nin adaptive data analysis even when the samples in the dataset are not\nnecessarily independent and when queries to be computed are not necessarily of\nbounded-sensitivity as long as the results of the queries over the dataset\n(i.e., the computed statistics) follow a distribution with a \"light\" tail.\nExamples of such queries include, but not limited to, subgaussian and\nsubexponential queries.\n  We also discuss the composition guarantees of typical stability and prove\ncomposition theorems that characterize the degradation of the parameters of\ntypical stability under $k$-fold adaptive composition. We also give simple\nnoise-addition algorithms that achieve this notion. These algorithms are\nsimilar to their differentially private counterparts, however, the added noise\nis calibrated differently.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 10:52:06 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 00:06:06 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Bassily", "Raef", ""], ["Freund", "Yoav", ""]]}, {"id": "1604.03462", "submitter": "Hiroshi Tsukimoto", "authors": "Hiroshi Tsukimoto", "title": "An algorithm for 3-SAT problems", "comments": "30 pages; typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for 3-SAT problems. First, logical formulas\nare transformed into elementary algebraic formulas. Second, complex\ntrigonometric functions are assigned to the variables in the elementary\nalgebraic formulas, and the sums of the formulas are calculated. The algorithm\noutputs the number of satisfying assignments. The computational complexity of\nthe algorithm is probably polynomial.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 06:49:24 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 09:41:16 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 02:59:03 GMT"}, {"version": "v4", "created": "Sun, 30 Jul 2017 05:16:06 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Tsukimoto", "Hiroshi", ""]]}, {"id": "1604.03482", "submitter": "Somaye Hashemifar", "authors": "Somaye Hashemifar, Qixing Huang and Jinbo XU", "title": "Joint alignment of multiple protein-protein interaction networks via\n  convex optimization", "comments": "Accepted by Recomb 2016, in Journal of Computational Biology 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: High-throughput experimental techniques have been producing more\nand more protein-protein interaction (PPI) data. PPI network alignment greatly\nbenefits the understanding of evolutionary relationship among species, helps\nidentify conserved sub-networks and provides extra information for functional\nannotations. Although a few methods have been developed for multiple PPI\nnetwork alignment, the alignment quality is still far away from perfect and\nthus, new network alignment methods are needed. Result: In this paper, we\npresent a novel method, denoted as ConvexAlign, for joint alignment of multiple\nPPI networks by convex optimization of a scoring function composed of sequence\nsimilarity, topological score and interaction conservation score. In contrast\nto existing methods that generate multiple alignments in a greedy or\nprogressive manner, our convex method optimizes alignments globally and\nenforces consistency among all pairwise alignments, resulting in much better\nalignment quality. Tested on both synthetic and real data, our experimental\nresults show that ConvexAlign outperforms several popular methods in producing\nfunctionally coherent alignments. ConvexAlign even has a larger advantage over\nthe others in aligning real PPI networks. ConvexAlign also finds a few\nconserved complexes among 5 species which cannot be detected by the other\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 17:07:16 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Hashemifar", "Somaye", ""], ["Huang", "Qixing", ""], ["XU", "Jinbo", ""]]}, {"id": "1604.03544", "submitter": "Michael B. Cohen", "authors": "Michael B. Cohen", "title": "Ramanujan Graphs in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent work by Marcus, Spielman and Srivastava proves the existence of\nbipartite Ramanujan (multi)graphs of all degrees and all sizes. However, that\npaper did not provide a polynomial time algorithm to actually compute such\ngraphs. Here, we provide a polynomial time algorithm to compute certain\nexpected characteristic polynomials related to this construction. This leads to\na deterministic polynomial time algorithm to compute bipartite Ramanujan\n(multi)graphs of all degrees and all sizes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 19:59:18 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Cohen", "Michael B.", ""]]}, {"id": "1604.03587", "submitter": "Gianluca Della Vedova", "authors": "Paola Bonizzoni, Gianluca Della Vedova, Yuri Pirola, Marco Previtali,\n  Raffaella Rizzi", "title": "FSG: Fast String Graph Construction for De Novo Assembly of Reads Data", "comments": "Accepted to Journal of Computational Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The string graph for a collection of next-generation reads is a lossless data\nrepresentation that is fundamental for de novo assemblers based on the\noverlap-layout-consensus paradigm. In this paper, we explore a novel approach\nto compute the string graph, based on the FM-index and Burrows-Wheeler\nTransform. We describe a simple algorithm that uses only the FM-index\nrepresentation of the collection of reads to construct the string graph,\nwithout accessing the input reads. Our algorithm has been integrated into the\nSGA assembler as a standalone module to construct the string graph.\n  The new integrated assembler has been assessed on a standard benchmark,\nshowing that FSG is significantly faster than SGA while maintaining a moderate\nuse of main memory, and showing practical advantages in running FSG on multiple\nthreads.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 21:08:48 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 15:24:02 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Della Vedova", "Gianluca", ""], ["Pirola", "Yuri", ""], ["Previtali", "Marco", ""], ["Rizzi", "Raffaella", ""]]}, {"id": "1604.03654", "submitter": "Wolfgang Mulzer", "authors": "Haim Kaplan, Wolfgang Mulzer, Liam Roditty, Paul Seiferth, Micha\n  Sharir", "title": "Dynamic Planar Voronoi Diagrams for General Distance Functions and their\n  Algorithmic Applications", "comments": "61 pages, 11 figures; a preliminary version appeared in SODA 2017", "journal-ref": "Discrete and Computational Geometry (DCG), 64(3), October 2020,\n  pp. 838-904", "doi": "10.1007/s00454-020-00243-7", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new data structure for dynamic nearest neighbor queries in the\nplane with respect to a general family of distance functions. These include\n$L_p$-norms and additively weighted Euclidean distances. Our data structure\nsupports general (convex, pairwise disjoint) sites that have constant\ndescription complexity (e.g., points, line segments, disks, etc.). Our\nstructure uses $O(n \\log^3 n)$ storage, and requires polylogarithmic update and\nquery time, improving an earlier data structure of Agarwal, Efrat and Sharir\nthat required $O(n^\\varepsilon)$ time for an update and $O(\\log n)$ time for a\nquery [SICOMP, 1999]. Our data structure has numerous applications. In all of\nthem, it gives faster algorithms, typically reducing an $O(n^\\varepsilon)$\nfactor in the previous bounds to polylogarithmic. In addition, we give here two\nnew applications: an efficient construction of a spanner in a disk intersection\ngraph, and a data structure for efficient connectivity queries in a dynamic\ndisk graph.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 05:05:13 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 21:51:18 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 17:39:56 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2020 16:04:07 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Kaplan", "Haim", ""], ["Mulzer", "Wolfgang", ""], ["Roditty", "Liam", ""], ["Seiferth", "Paul", ""], ["Sharir", "Micha", ""]]}, {"id": "1604.03655", "submitter": "Haris Aziz", "authors": "Haris Aziz and Simon Mackenzie", "title": "A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of\n  Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the well-studied cake cutting problem in which the goal is to\nfind an envy-free allocation based on queries from $n$ agents. The problem has\nreceived attention in computer science, mathematics, and economics. It has been\na major open problem whether there exists a discrete and bounded envy-free\nprotocol. We resolve the problem by proposing a discrete and bounded envy-free\nprotocol for any number of agents. The maximum number of queries required by\nthe protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do\nnot run our protocol to completion, it can find in at most $n^3{(n^2)}^n$\nqueries a partial allocation of the cake that achieves proportionality (each\nagent gets at least $1/n$ of the value of the whole cake) and envy-freeness.\nFinally we show that an envy-free partial allocation can be computed in at most\n$n^3{(n^2)}^n$ queries such that each agent gets a connected piece that gives\nthe agent at least $1/(3n)$ of the value of the whole cake.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 05:06:29 GMT"}, {"version": "v10", "created": "Wed, 5 Oct 2016 22:30:52 GMT"}, {"version": "v11", "created": "Tue, 27 Dec 2016 23:00:21 GMT"}, {"version": "v12", "created": "Sun, 27 Aug 2017 08:35:26 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 02:50:59 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 21:45:28 GMT"}, {"version": "v4", "created": "Sat, 7 May 2016 13:57:27 GMT"}, {"version": "v5", "created": "Tue, 7 Jun 2016 11:44:24 GMT"}, {"version": "v6", "created": "Mon, 25 Jul 2016 12:36:58 GMT"}, {"version": "v7", "created": "Fri, 29 Jul 2016 22:15:32 GMT"}, {"version": "v8", "created": "Sun, 28 Aug 2016 23:25:26 GMT"}, {"version": "v9", "created": "Thu, 15 Sep 2016 11:07:30 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Aziz", "Haris", ""], ["Mackenzie", "Simon", ""]]}, {"id": "1604.03661", "submitter": "Talya Eden", "authors": "Talya Eden and Dana Ron and C. Seshadhri", "title": "Sublinear Time Estimation of Degree Distribution Moments: The Degeneracy\n  Connection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classic problem of estimating the degree distribution moments\nof an undirected graph. Consider an undirected graph $G=(V,E)$ with $n$\nvertices, and define (for $s > 0$) $\\mu_s = \\frac{1}{n}\\cdot\\sum_{v \\in V}\nd^s_v$. Our aim is to estimate $\\mu_s$ within a multiplicative error of\n$(1+\\epsilon)$ (for a given approximation parameter $\\epsilon>0$) in sublinear\ntime. We consider the sparse graph model that allows access to: uniform random\nvertices, queries for the degree of any vertex, and queries for a neighbor of\nany vertex. For the case of $s=1$ (the average degree),\n$\\widetilde{O}(\\sqrt{n})$ queries suffice for any constant $\\epsilon$ (Feige,\nSICOMP 06 and Goldreich-Ron, RSA 08). Gonen-Ron-Shavitt (SIDMA 11) extended\nthis result to all integral $s > 0$, by designing an algorithms that performs\n$\\widetilde{O}(n^{1-1/(s+1)})$ queries.\n  We design a new, significantly simpler algorithm for this problem. In the\nworst-case, it exactly matches the bounds of Gonen-Ron-Shavitt, and has a much\nsimpler proof. More importantly, the running time of this algorithm is\nconnected to the degeneracy of $G$. This is (essentially) the maximum density\nof an induced subgraph. For the family of graphs with degeneracy at most\n$\\alpha$, it has a query complexity of\n$\\widetilde{O}\\left(\\frac{n^{1-1/s}}{\\mu^{1/s}_s} \\Big(\\alpha^{1/s} +\n\\min\\{\\alpha,\\mu^{1/s}_s\\}\\Big)\\right) =\n\\widetilde{O}(n^{1-1/s}\\alpha/\\mu^{1/s}_s)$. Thus, for the class of bounded\ndegeneracy graphs (which includes all minor closed families and preferential\nattachment graphs), we can estimate the average degree in $\\widetilde{O}(1)$\nqueries, and can estimate the variance of the degree distribution in\n$\\widetilde{O}(\\sqrt{n})$ queries. This is a major improvement over the\nprevious worst-case bounds. Our key insight is in designing an estimator for\n$\\mu_s$ that has low variance when $G$ does not have large dense subgraphs.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 05:36:46 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 13:57:18 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Eden", "Talya", ""], ["Ron", "Dana", ""], ["Seshadhri", "C.", ""]]}, {"id": "1604.03921", "submitter": "Xin He", "authors": "Dayu He and Xin He", "title": "Optimal Monotone Drawings of Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A monotone drawing of a graph G is a straight-line drawing of G such that,\nfor every pair of vertices u,w in G, there exists abpath P_{uw} in G that is\nmonotone in some direction l_{uw}. (Namely, the order of the orthogonal\nprojections of the vertices of P_{uw} on l_{uw} is the same as the order they\nappear in P_{uw}.)\n  The problem of finding monotone drawings for trees has been studied in\nseveral recent papers. The main focus is to reduce the size of the drawing.\nCurrently, the smallest drawing size is O(n^{1.205}) x O(n^{1.205}). In this\npaper, we present an algorithm for constructing monotone drawings of trees on a\ngrid of size at most 12n x 12n. The smaller drawing size is achieved by a new\nsimple Path Draw algorithm, and a procedure that carefully assigns primitive\nvectors to the paths of the input tree T.\n  We also show that there exists a tree T_0 such that any monotone drawing of\nT_0 must use a grid of size Omega(n) x Omega(n). So the size of our monotone\ndrawing of trees is asymptotically optimal.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 19:21:17 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["He", "Dayu", ""], ["He", "Xin", ""]]}, {"id": "1604.04111", "submitter": "Fahad Panolan", "authors": "Daniel Lokshtanov, Fahad Panolan, M. S. Ramanujan, Saket Saurabh", "title": "Lossy Kernelization", "comments": "58 pages. Version 2 contain new results: PSAKS for Cycle Packing and\n  approximate kernel lower bounds for Set Cover and Hitting Set parameterized\n  by universe size", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new framework for analyzing the performance of\npreprocessing algorithms. Our framework builds on the notion of kernelization\nfrom parameterized complexity. However, as opposed to the original notion of\nkernelization, our definitions combine well with approximation algorithms and\nheuristics. The key new definition is that of a polynomial size\n$\\alpha$-approximate kernel. Loosely speaking, a polynomial size\n$\\alpha$-approximate kernel is a polynomial time pre-processing algorithm that\ntakes as input an instance $(I,k)$ to a parameterized problem, and outputs\nanother instance $(I',k')$ to the same problem, such that $|I'|+k' \\leq\nk^{O(1)}$. Additionally, for every $c \\geq 1$, a $c$-approximate solution $s'$\nto the pre-processed instance $(I',k')$ can be turned in polynomial time into a\n$(c \\cdot \\alpha)$-approximate solution $s$ to the original instance $(I,k)$.\n  Our main technical contribution are $\\alpha$-approximate kernels of\npolynomial size for three problems, namely Connected Vertex Cover, Disjoint\nCycle Packing and Disjoint Factors. These problems are known not to admit any\npolynomial size kernels unless $NP \\subseteq coNP/poly$. Our approximate\nkernels simultaneously beat both the lower bounds on the (normal) kernel size,\nand the hardness of approximation lower bounds for all three problems. On the\nnegative side we prove that Longest Path parameterized by the length of the\npath and Set Cover parameterized by the universe size do not admit even an\n$\\alpha$-approximate kernel of polynomial size, for any $\\alpha \\geq 1$, unless\n$NP \\subseteq coNP/poly$. In order to prove this lower bound we need to combine\nin a non-trivial way the techniques used for showing kernelization lower bounds\nwith the methods for showing hardness of approximation\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 11:08:54 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 09:22:20 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Panolan", "Fahad", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1604.04359", "submitter": "Palash Dey", "authors": "Palash Dey, Neeldhara Misra, Y. Narahari", "title": "Complexity of Manipulation with Partial Information in Voting", "comments": "Appeared in IJCAI 2016. Fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CC cs.CY cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coalitional Manipulation problem has been studied extensively in the\nliterature for many voting rules. However, most studies have focused on the\ncomplete information setting, wherein the manipulators know the votes of the\nnon-manipulators. While this assumption is reasonable for purposes of showing\nintractability, it is unrealistic for algorithmic considerations. In most\nreal-world scenarios, it is impractical for the manipulators to have accurate\nknowledge of all the other votes. In this paper, we investigate manipulation\nwith incomplete information. In our framework, the manipulators know a partial\norder for each voter that is consistent with the true preference of that voter.\nIn this setting, we formulate three natural computational notions of\nmanipulation, namely weak, opportunistic, and strong manipulation. We say that\nan extension of a partial order is if there exists a manipulative vote for that\nextension.\n  1. Weak Manipulation (WM): the manipulators seek to vote in a way that makes\ntheir preferred candidate win in at least one extension of the partial votes of\nthe non-manipulators.\n  2. Opportunistic Manipulation (OM): the manipulators seek to vote in a way\nthat makes their preferred candidate win in every viable extension of the\npartial votes of the non-manipulators.\n  3. Strong Manipulation (SM): the manipulators seek to vote in a way that\nmakes their preferred candidate win in every extension of the partial votes of\nthe non-manipulators.\n  We consider several scenarios for which the traditional manipulation problems\nare easy (for instance, Borda with a single manipulator). For many of them, the\ncorresponding manipulative questions that we propose turn out to be\ncomputationally intractable. Our hardness results often hold even when very\nlittle information is missing, or in other words, even when the instances are\nquite close to the complete information setting.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 05:55:07 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 03:30:09 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Dey", "Palash", ""], ["Misra", "Neeldhara", ""], ["Narahari", "Y.", ""]]}, {"id": "1604.04403", "submitter": "Palash Dey", "authors": "Palash Dey and Neeldhara Misra", "title": "Elicitation for Preferences Single Peaked on Trees", "comments": "To appear in IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CC cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiagent systems, we often have a set of agents each of which have a\npreference ordering over a set of items and one would like to know these\npreference orderings for various tasks, for example, data analysis, preference\naggregation, voting etc. However, we often have a large number of items which\nmakes it impractical to ask the agents for their complete preference ordering.\nIn such scenarios, we usually elicit these agents' preferences by asking (a\nhopefully small number of) comparison queries --- asking an agent to compare\ntwo items. Prior works on preference elicitation focus on unrestricted domain\nand the domain of single peaked preferences and show that the preferences in\nsingle peaked domain can be elicited by much less number of queries compared to\nunrestricted domain. We extend this line of research and study preference\nelicitation for single peaked preferences on trees which is a strict superset\nof the domain of single peaked preferences. We show that the query complexity\ncrucially depends on the number of leaves, the path cover number, and the\ndistance from path of the underlying single peaked tree, whereas the other\nnatural parameters like maximum degree, diameter, pathwidth do not play any\ndirect role in determining query complexity. We then investigate the query\ncomplexity for finding a weak Condorcet winner for preferences single peaked on\na tree and show that this task has much less query complexity than preference\nelicitation. Here again we observe that the number of leaves in the underlying\nsingle peaked tree and the path cover number of the tree influence the query\ncomplexity of the problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 08:40:39 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Dey", "Palash", ""], ["Misra", "Neeldhara", ""]]}, {"id": "1604.04471", "submitter": "David Tian", "authors": "Wenhong Tian, Guangchun Luo, Ling Tian, and Aiguo Chen", "title": "On Dynamic Job Ordering and Slot Configurations for Minimizing the\n  Makespan Of Multiple MapReduce Jobs", "comments": "Under Review of IEEE Trans. on Service Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  MapReduce is a popular parallel computing paradigm for Big Data processing in\nclusters and data centers. It is observed that different job execution orders\nand MapReduce slot configurations for a MapReduce workload have significantly\ndifferent performance with regarding to the makespan, total completion time,\nsystem utilization and other performance metrics. There are quite a few\nalgorithms on minimizing makespan of multiple MapReduce jobs. However, these\nalgorithms are heuristic or suboptimal. The best known algorithm for minimizing\nthe makespan is 3-approximation by applying Johnson rule. In this paper, we\npropose an approach called UAAS algorithm to meet the conditions of classical\nJohnson model. Then we can still use Johnson model for an optimal solution. We\nexplain how to adapt to Johnson model and provide a few key features of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 12:54:09 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Tian", "Wenhong", ""], ["Luo", "Guangchun", ""], ["Tian", "Ling", ""], ["Chen", "Aiguo", ""]]}, {"id": "1604.04482", "submitter": "David Tian", "authors": "Guozhong Li, Yaqiu Jiang, Wutong Yang, Chaojie Huang, Wenhong Tian", "title": "Self-Adaptive Consolidation of Virtual Machines For Energy-Efficiency in\n  the Cloud", "comments": "Under Review of GlobalCom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In virtualized data centers, consolidation of Virtual Machines (VMs) on\nminimizing the number of total physical machines (PMs) has been recognized as a\nvery efficient approach. This paper considers the energy-efficient\nconsolidation of VMs in a Cloud Data center. Concentrating on CPU-intensive\napplications, the objective is to schedule all requests non-preemptively,\nsubjecting to constraints of PM capacities and running time interval spans,\nsuch that the total energy consumption of all PMs is minimized (called MinTE\nfor abbreviation). The MinTE problem is NP-complete in general. We propose a\nself-adaptive approached called SAVE. The approach makes decisions of the\nassignment and migration of VMs by probabilistic processes and is based\nexclusively on local information, therefore it is very simple to implement.\nBoth simulation and real environment test show that our proposed method SAVE\ncan reduce energy consumption about 30% against VMWare DRS and 10-20% against\nEcoCloud on average.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 13:05:03 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Li", "Guozhong", ""], ["Jiang", "Yaqiu", ""], ["Yang", "Wutong", ""], ["Huang", "Chaojie", ""], ["Tian", "Wenhong", ""]]}, {"id": "1604.04618", "submitter": "Thomas Steinke", "authors": "Mark Bun, Thomas Steinke, Jonathan Ullman", "title": "Make Up Your Mind: The Price of Online Queries in Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of answering queries about a sensitive dataset\nsubject to differential privacy. The queries may be chosen adversarially from a\nlarger set Q of allowable queries in one of three ways, which we list in order\nfrom easiest to hardest to answer:\n  Offline: The queries are chosen all at once and the differentially private\nmechanism answers the queries in a single batch.\n  Online: The queries are chosen all at once, but the mechanism only receives\nthe queries in a streaming fashion and must answer each query before seeing the\nnext query.\n  Adaptive: The queries are chosen one at a time and the mechanism must answer\neach query before the next query is chosen. In particular, each query may\ndepend on the answers given to previous queries.\n  Many differentially private mechanisms are just as efficient in the adaptive\nmodel as they are in the offline model. Meanwhile, most lower bounds for\ndifferential privacy hold in the offline setting. This suggests that the three\nmodels may be equivalent.\n  We prove that these models are all, in fact, distinct. Specifically, we show\nthat there is a family of statistical queries such that exponentially more\nqueries from this family can be answered in the offline model than in the\nonline model. We also exhibit a family of search queries such that\nexponentially more queries from this family can be answered in the online model\nthan in the adaptive model. We also investigate whether such separations might\nhold for simple queries like threshold queries over the real line.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 19:55:26 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Bun", "Mark", ""], ["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1604.04664", "submitter": "Amariah Becker", "authors": "Amariah Becker", "title": "Capacitated Dominating Set on Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capacitated Domination generalizes the classic Dominating Set problem by\nspecifying for each vertex a required demand and an available capacity for\ncovering demand in its closed neighborhood. The objective is to find a\nminimum-sized set of vertices that can cover all of the graph's demand without\nexceeding any of the capacities. In this paper we look specifically at\ndomination with hard-capacities, where the capacity and cost of a vertex can\ncontribute to the solution at most once. Previous complexity results suggest\nthat this problem cannot be solved (or even closely approximated) efficiently\nin general. In this paper we present a polynomial-time approximation scheme for\nCapacitated Domination in unweighted planar graphs when the maximum capacity\nand maximum demand are bounded. We also show how this result can be extended to\nthe closely-related Capacitated Vertex Cover problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 23:44:38 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Becker", "Amariah", ""]]}, {"id": "1604.04827", "submitter": "Hendrik Molter", "authors": "Ren\\'e van Bevern and Christian Komusiewicz and Hendrik Molter and\n  Rolf Niedermeier and Manuel Sorge and Toby Walsh", "title": "h-Index Manipulation by Undoing Merges", "comments": null, "journal-ref": "Quantitative Science Studies, 1(4): 1529-1552. 2020", "doi": "10.1162/qss_a_00093", "report-no": null, "categories": "cs.DL cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The h-index is an important bibliographic measure used to assess the\nperformance of researchers. Dutiful researchers merge different versions of\ntheir articles in their Google Scholar profile even though this can decrease\ntheir h-index. In this article, we study the manipulation of the h-index by\nundoing such merges. In contrast to manipulation by merging articles (van\nBevern et al. [Artif. Intel. 240:19-35, 2016]) such manipulation is harder to\ndetect. We present numerous results on computational complexity (from\nlinear-time algorithms to parameterized computational hardness results) and\nempirically indicate that at least small improvements of the h-index by\nsplitting merged articles are unfortunately easily achievable.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 04:11:30 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 06:47:32 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 11:28:18 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Komusiewicz", "Christian", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Sorge", "Manuel", ""], ["Walsh", "Toby", ""]]}, {"id": "1604.04829", "submitter": "Raka Jovanovic", "authors": "Raka Jovanovic, Tatsushi Nishi, Stefan Voss", "title": "A heuristic approach for dividing graphs into bi-connected components\n  with a size constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new problem of finding the maximal bi-connected\npartitioning of a graph with a size constraint (MBCPG-SC). With the goal of\nfinding approximate solutions for the MBCPG-SC, a heuristic method is developed\nbased on the open ear decomposition of graphs. Its essential part is an\nadaptation of the breadth first search which makes it possible to grow\nbi-connected subgraphs. The proposed randomized algorithm consists of growing\nseveral subgraphs in parallel. The quality of solutions generated in this way\nis further improved using a local search which exploits neighboring relations\nbetween the subgraphs. In order to evaluate the performance of the method, an\nalgorithm for generating pseudo-random unit disc graphs with known optimal\nsolutions is created. The conducted computational experiments show that the\nproposed method frequently manages to find optimal solutions and has an average\nerror of only a few percent to known optimal solutions. Further, it manages to\nfind high quality approximate solutions for graphs having up to 10.000 nodes in\nreasonable time.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 05:19:05 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Jovanovic", "Raka", ""], ["Nishi", "Tatsushi", ""], ["Voss", "Stefan", ""]]}, {"id": "1604.04893", "submitter": "Fouad Khan", "authors": "Fouad Khan", "title": "An Initial Seed Selection Algorithm for K-means Clustering of\n  Georeferenced Data to Improve Replicability of Cluster Assignments for\n  Mapping Application", "comments": "Applied Soft Computing 12 (2012)", "journal-ref": null, "doi": "10.1016/j.asoc.2012.07.021", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means is one of the most widely used clustering algorithms in various\ndisciplines, especially for large datasets. However the method is known to be\nhighly sensitive to initial seed selection of cluster centers. K-means++ has\nbeen proposed to overcome this problem and has been shown to have better\naccuracy and computational efficiency than k-means. In many clustering problems\nthough -such as when classifying georeferenced data for mapping applications-\nstandardization of clustering methodology, specifically, the ability to arrive\nat the same cluster assignment for every run of the method i.e. replicability\nof the methodology, may be of greater significance than any perceived measure\nof accuracy, especially when the solution is known to be non-unique, as in the\ncase of k-means clustering. Here we propose a simple initial seed selection\nalgorithm for k-means clustering along one attribute that draws initial cluster\nboundaries along the 'deepest valleys' or greatest gaps in dataset. Thus, it\nincorporates a measure to maximize distance between consecutive cluster centers\nwhich augments the conventional k-means optimization for minimum distance\nbetween cluster center and cluster members. Unlike existing initialization\nmethods, no additional parameters or degrees of freedom are introduced to the\nclustering algorithm. This improves the replicability of cluster assignments by\nas much as 100% over k-means and k-means++, virtually reducing the variance\nover different runs to zero, without introducing any additional parameters to\nthe clustering process. Further, the proposed method is more computationally\nefficient than k-means++ and in some cases, more accurate.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 16:25:15 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Khan", "Fouad", ""]]}, {"id": "1604.05023", "submitter": "Yoann Dieudonn\\'e", "authors": "Yoann Dieudonn\\'e and Andrzej Pelc", "title": "Impact of Knowledge on Election Time in Anonymous Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leader election is one of the basic problems in distributed computing. For\nanonymous networks, the task of leader election is formulated as follows: every\nnode v of the network must output a simple path, which is coded as a sequence\nof port numbers, such that all these paths end at a common node, the leader. In\nthis paper, we study deterministic leader election in arbitrary anonymous\nnetworks. It is well known that leader election is impossible in some networks,\nregardless of the allocated amount of time, even if nodes know the map of the\nnetwork. However, even in networks in which it is possible to elect a leader\nknowing the map, the task may be still impossible without any knowledge,\nregardless of the allocated time. On the other hand, for any network in which\nleader election is possible knowing the map, there is a minimum time, called\nthe election index, in which this can be done. Informally, the election index\nof a network is the minimum depth at which views of all nodes are distinct. Our\naim is to establish tradeoffs between the allocated time $\\tau$ and the amount\nof information that has to be given a priori to the nodes to enable leader\nelection in time $\\tau$ in all networks for which leader election in this time\nis at all possible. Following the framework of algorithms with advice, this\ninformation is provided to all nodes at the start by an oracle knowing the\nentire network. The length of this string (its number of bits) is called the\nsize of advice. For a given time $\\tau$ allocated to leader election, we give\nupper and lower bounds on the minimum size of advice sufficient to perform\nleader election in time $\\tau$. We focus on the two sides of the time spectrum\nand give tight (or almost tight) bounds on the minimum size of advice for these\nextremes. We also show that constant advice is not sufficient for leader\nelection in all graphs, regardless of the allocated time.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 07:55:06 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Dieudonn\u00e9", "Yoann", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1604.05103", "submitter": "Ond\\v{r}ej Such\\'y", "authors": "Ond\\v{r}ej Such\\'y", "title": "On Directed Steiner Trees with Multiple Roots", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new Steiner-type problem for directed graphs named\n\\textsc{$q$-Root Steiner Tree}. Here one is given a directed graph $G=(V,A)$\nand two subsets of its vertices, $R$ of size $q$ and $T$, and the task is to\nfind a minimum size subgraph of $G$ that contains a path from each vertex of\n$R$ to each vertex of $T$. The special case of this problem with $q=1$ is the\nwell known \\textsc{Directed Steiner Tree} problem, while the special case with\n$T=R$ is the \\textsc{Strongly Connected Steiner Subgraph} problem.\n  We first show that the problem is W[1]-hard with respect to $|T|$ for any $q\n\\ge 2$. Then we restrict ourselves to instances with $R \\subseteq T$.\nGeneralizing the methods of Feldman and Ruhl [SIAM J. Comput. 2006], we present\nan algorithm for this restriction with running time $O(2^{2q+4|T|}\\cdot\nn^{2q+O(1)})$, i.e., this restriction is FPT with respect to $|T|$ for any\nconstant $q$. We further show that we can, without significantly affecting the\nachievable running time, loosen the restriction to only requiring that in the\nsolution there are a vertex $v$ and a path from each vertex of $R$ to $v$ and\nfrom $v$ to each vertex of~$T$.\n  Finally, we use the methods of Chitnis et al. [SODA 2014] to show that the\nrestricted version can be solved in planar graphs in $O(2^{O(q \\log q+|T|\\log\nq)}\\cdot n^{O(\\sqrt{q})})$ time.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 11:49:34 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Such\u00fd", "Ond\u0159ej", ""]]}, {"id": "1604.05172", "submitter": "Joan Boyar", "authors": "Joan Boyar, Stephan J. Eidenbenz, Lene M. Favrholdt, Michal\n  Kotrb\\v{c}\\'ik, Kim S. Larsen", "title": "Online Dominating Set", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the online dominating set problem and its variants.\nWe believe the paper represents the first systematic study of the effect of two\nlimitations of online algorithms: making irrevocable decisions while not\nknowing the future, and being incremental, i.e., having to maintain solutions\nto all prefixes of the input. This is quantified through competitive analyses\nof online algorithms against two optimal algorithms, both knowing the entire\ninput, but only one having to be incremental. We also consider the competitive\nratio of the weaker of the two optimal algorithms against the other.\n  We consider important graph classes, distinguishing between connected and not\nnecessarily connected graphs. For the classic graph classes of trees,\nbipartite, planar, and general graphs, we obtain tight results in almost all\ncases. We also derive upper and lower bounds for the class of bounded-degree\ngraphs. From these analyses, we get detailed information regarding the\nsignificance of the necessary requirement that online algorithms be\nincremental. In some cases, having to be incremental fully accounts for the\nonline algorithm's disadvantage.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 14:16:19 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 06:46:26 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Boyar", "Joan", ""], ["Eidenbenz", "Stephan J.", ""], ["Favrholdt", "Lene M.", ""], ["Kotrb\u010d\u00edk", "Michal", ""], ["Larsen", "Kim S.", ""]]}, {"id": "1604.05194", "submitter": "Palash Dey", "authors": "Palash Dey and Neeldhara Misra", "title": "Preference Elicitation For Single Crossing Domain", "comments": "To appear in IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliciting the preferences of a set of agents over a set of alternatives is a\nproblem of fundamental importance in social choice theory. Prior work on this\nproblem has studied the query complexity of preference elicitation for the\nunrestricted domain and for the domain of single peaked preferences. In this\npaper, we consider the domain of single crossing preference profiles and study\nthe query complexity of preference elicitation under various settings. We\nconsider two distinct situations: when an ordering of the voters with respect\nto which the profile is single crossing is known versus when it is unknown. We\nalso consider different access models: when the votes can be accessed at\nrandom, as opposed to when they are coming in a pre-defined sequence. In the\nsequential access model, we distinguish two cases when the ordering is known:\nthe first is that sequence in which the votes appear is also a single-crossing\norder, versus when it is not.\n  The main contribution of our work is to provide polynomial time algorithms\nwith low query complexity for preference elicitation in all the above six\ncases. Further, we show that the query complexities of our algorithms are\noptimal up to constant factors for all but one of the above six cases. We then\npresent preference elicitation algorithms for profiles which are close to being\nsingle crossing under various notions of closeness, for example, single\ncrossing width, minimum number of candidates | voters whose deletion makes a\nprofile single crossing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 08:47:18 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Dey", "Palash", ""], ["Misra", "Neeldhara", ""]]}, {"id": "1604.05337", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Monika Henzinger and Giuseppe F. Italiano", "title": "Design of Dynamic Algorithms via Primal-Dual Method", "comments": "A preliminary version of this paper appeared in ICALP 2015 (Track A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a dynamic version of the primal-dual method for optimization\nproblems, and apply it to obtain the following results. (1) For the dynamic\nset-cover problem, we maintain an $O(f^2)$-approximately optimal solution in\n$O(f \\cdot \\log (m+n))$ amortized update time, where $f$ is the maximum\n\"frequency\" of an element, $n$ is the number of sets, and $m$ is the maximum\nnumber of elements in the universe at any point in time. (2) For the dynamic\n$b$-matching problem, we maintain an $O(1)$-approximately optimal solution in\n$O(\\log^3 n)$ amortized update time, where $n$ is the number of nodes in the\ngraph.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 20:15:47 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Henzinger", "Monika", ""], ["Italiano", "Giuseppe F.", ""]]}, {"id": "1604.05448", "submitter": "Cameron Musco", "authors": "Michael B. Cohen and Cameron Musco and Jakub Pachocki", "title": "Online Row Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a small spectral approximation for a tall $n \\times d$ matrix $A$ is\na fundamental numerical primitive. For a number of reasons, one often seeks an\napproximation whose rows are sampled from those of $A$. Row sampling improves\ninterpretability, saves space when $A$ is sparse, and preserves row structure,\nwhich is especially important, for example, when $A$ represents a graph.\n  However, correctly sampling rows from $A$ can be costly when the matrix is\nlarge and cannot be stored and processed in memory. Hence, a number of recent\npublications focus on row sampling in the streaming setting, using little more\nspace than what is required to store the outputted approximation [KL13,\nKLM+14].\n  Inspired by a growing body of work on online algorithms for machine learning\nand data analysis, we extend this work to a more restrictive online setting: we\nread rows of $A$ one by one and immediately decide whether each row should be\nkept in the spectral approximation or discarded, without ever retracting these\ndecisions. We present an extremely simple algorithm that approximates $A$ up to\nmultiplicative error $\\epsilon$ and additive error $\\delta$ using $O(d \\log d\n\\log(\\epsilon||A||_2/\\delta)/\\epsilon^2)$ online samples, with memory overhead\nproportional to the cost of storing the spectral approximation. We also present\nan algorithm that uses $O(d^2$) memory but only requires\n$O(d\\log(\\epsilon||A||_2/\\delta)/\\epsilon^2)$ samples, which we show is\noptimal.\n  Our methods are clean and intuitive, allow for lower memory usage than prior\nwork, and expose new theoretical properties of leverage score based matrix\napproximation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 07:08:56 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Cohen", "Michael B.", ""], ["Musco", "Cameron", ""], ["Pachocki", "Jakub", ""]]}, {"id": "1604.05472", "submitter": "Arpita Biswas", "authors": "Ragavendran Gopalakrishnan, Arpita Biswas, Alefiya Lightwala, Skanda\n  Vasudevan, Partha Dutta, Abhishek Tripathi", "title": "Demand Prediction and Placement Optimization for Electric Vehicle\n  Charging Stations", "comments": "Published in the proceedings of the 25th International Joint\n  Conference on Artificial Intelligence IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective placement of charging stations plays a key role in Electric Vehicle\n(EV) adoption. In the placement problem, given a set of candidate sites, an\noptimal subset needs to be selected with respect to the concerns of both (a)\nthe charging station service provider, such as the demand at the candidate\nsites and the budget for deployment, and (b) the EV user, such as charging\nstation reachability and short waiting times at the station. This work\naddresses these concerns, making the following three novel contributions: (i) a\nsupervised multi-view learning framework using Canonical Correlation Analysis\n(CCA) for demand prediction at candidate sites, using multiple datasets such as\npoints of interest information, traffic density, and the historical usage at\nexisting charging stations; (ii) a mixed-packing-and- covering optimization\nframework that models competing concerns of the service provider and EV users;\n(iii) an iterative heuristic to solve these problems by alternately invoking\nknapsack and set cover algorithms. The performance of the demand prediction\nmodel and the placement optimization heuristic are evaluated using real world\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 08:51:03 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 14:30:23 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Gopalakrishnan", "Ragavendran", ""], ["Biswas", "Arpita", ""], ["Lightwala", "Alefiya", ""], ["Vasudevan", "Skanda", ""], ["Dutta", "Partha", ""], ["Tripathi", "Abhishek", ""]]}, {"id": "1604.05590", "submitter": "Uri Stemmer", "authors": "Kobbi Nissim, Uri Stemmer, Salil Vadhan", "title": "Locating a Small Cluster Privately", "comments": null, "journal-ref": null, "doi": "10.1145/2902251.2902296", "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for locating a small cluster of points with\ndifferential privacy [Dwork, McSherry, Nissim, and Smith, 2006]. Our algorithm\nhas implications to private data exploration, clustering, and removal of\noutliers. Furthermore, we use it to significantly relax the requirements of the\nsample and aggregate technique [Nissim, Raskhodnikova, and Smith, 2007], which\nallows compiling of \"off the shelf\" (non-private) analyses into analyses that\npreserve differential privacy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 14:27:32 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 15:51:54 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""], ["Vadhan", "Salil", ""]]}, {"id": "1604.05603", "submitter": "Wolfgang Dvo\\v{r}\\'ak", "authors": "Wolfgang Dvo\\v{r}\\'ak and Monika Henzinger", "title": "Online Ad Assignment with an Ad Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ad exchanges are becoming an increasingly popular way to sell advertisement\nslots on the internet. An ad exchange is basically a spot market for ad\nimpressions. A publisher who has already signed contracts reserving\nadvertisement impressions on his pages can choose between assigning a new ad\nimpression for a new page view to a contracted advertiser or to sell it at an\nad exchange. This leads to an online revenue maximization problem for the\npublisher. Given a new impression to sell decide whether (a) to assign it to a\ncontracted advertiser and if so to which one or (b) to sell it at the ad\nexchange and if so at which reserve price. We make no assumptions about the\ndistribution of the advertiser valuations that participate in the ad exchange\nand show that there exists a simple primal-dual based online algorithm, whose\nlower bound for the revenue converges to $R_{ADX} + R_A (1 - 1/e)$, where\n$R_{ADX}$ is the revenue that the optimum algorithm achieves from the ad\nexchange and $R_A$ is the revenue that the optimum algorithm achieves from the\ncontracted advertisers.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 14:44:55 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Dvo\u0159\u00e1k", "Wolfgang", ""], ["Henzinger", "Monika", ""]]}, {"id": "1604.05609", "submitter": "Gilad Kutiel", "authors": "Gilad Kutiel", "title": "Approximation Algorithms for the Maximum Carpool Matching Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Carpool Matching problem is a star packing problem in directed\ngraphs. Formally, given a directed graph $G = (V, A)$, a capacity function $ c:\nV \\to N $, and a weight function $w : A \\to R $, a feasible \\emph{carpool\nmatching} is a triple $(P, D, M)$, where $P$ (passengers) and $D$ (drivers)\nform a partition of $V$, and $M$ is a subset of $A \\cap (P \\times D)$, under\nthe constraints that for every vertex $d \\in D$, $din(d) \\leq c(d)$, and for\nevery vertex $p \\in P$, $dout(p) \\leq 1$. In the Maximum Carpool Matching\nproblem we seek for a matching $(P, D, M)$ that maximizes the total weight of\n$M$.\n  The problem arises when designing an online carpool service, such as\nZimride~\\cite{zimride}, that tries to connect between passengers and drivers\nbased on (arbitrary) similarity function. The problem is known to be NP-hard,\neven for uniform weights and without capacity constraints.\n  We present a $3$-approximation algorithm for the problem and\n$2$-approximation algorithm for the unweighted variant of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 18:58:46 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 16:42:59 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Kutiel", "Gilad", ""]]}, {"id": "1604.05765", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Monika Henzinger and Danupon Nanongkai", "title": "New Deterministic Approximation Algorithms for Fully Dynamic Matching", "comments": "To appear in STOC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two deterministic dynamic algorithms for the maximum matching\nproblem. (1) An algorithm that maintains a $(2+\\epsilon)$-approximate maximum\nmatching in general graphs with $O(\\text{poly}(\\log n, 1/\\epsilon))$ update\ntime. (2) An algorithm that maintains an $\\alpha_K$ approximation of the {\\em\nvalue} of the maximum matching with $O(n^{2/K})$ update time in bipartite\ngraphs, for every sufficiently large constant positive integer $K$. Here,\n$1\\leq \\alpha_K < 2$ is a constant determined by the value of $K$. Result (1)\nis the first deterministic algorithm that can maintain an $o(\\log\nn)$-approximate maximum matching with polylogarithmic update time, improving\nthe seminal result of Onak et al. [STOC 2010]. Its approximation guarantee\nalmost matches the guarantee of the best {\\em randomized} polylogarithmic\nupdate time algorithm [Baswana et al. FOCS 2011]. Result (2) achieves a\nbetter-than-two approximation with {\\em arbitrarily small polynomial} update\ntime on bipartite graphs. Previously the best update time for this problem was\n$O(m^{1/4})$ [Bernstein et al. ICALP 2015], where $m$ is the current number of\nedges in the graph.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 22:22:22 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Henzinger", "Monika", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1604.05814", "submitter": "Wei Zhan", "authors": "Jian Li, Wei Zhan", "title": "Almost All Even Yao-Yao Graphs Are Spanners", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is an open problem whether Yao-Yao graphs $\\mathsf{YY}_k$ (also known as\nsparse-Yao graphs) are all spanners when the integer parameter $k$ is large\nenough. In this paper we show that, for any integer $k\\geq 42$, the Yao-Yao\ngraph $\\mathsf{YY}_{2k}$ is a $t_k$-spanner, with stretch factor\n$t_k=6.03+O(k^{-1})$ when $k$ tends to infinity. Our result generalizes the\nbest known result which asserts that all $\\mathsf{YY}_{6k}$ are spanners for\n$k$ large enough [Bauer and Damian, SODA'13]. Our proof is also somewhat\nsimpler.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 04:39:18 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 04:28:09 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Li", "Jian", ""], ["Zhan", "Wei", ""]]}, {"id": "1604.05999", "submitter": "Marcin Pilipczuk", "authors": "Fedor V. Fomin and Daniel Lokshtanov and D\\'aniel Marx and Marcin\n  Pilipczuk and Micha{\\l} Pilipczuk and Saket Saurabh", "title": "Subexponential parameterized algorithms for planar and apex-minor-free\n  graphs via low treewidth pattern covering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the following theorem. Given a planar graph $G$ and an integer $k$,\nit is possible in polynomial time to randomly sample a subset $A$ of vertices\nof $G$ with the following properties: (i) $A$ induces a subgraph of $G$ of\ntreewidth $\\mathcal{O}(\\sqrt{k}\\log k)$, and (ii) for every connected subgraph\n$H$ of $G$ on at most $k$ vertices, the probability that $A$ covers the whole\nvertex set of $H$ is at least $(2^{\\mathcal{O}(\\sqrt{k}\\log^2 k)}\\cdot\nn^{\\mathcal{O}(1)})^{-1}$, where $n$ is the number of vertices of $G$.\n  Together with standard dynamic programming techniques for graphs of bounded\ntreewidth, this result gives a versatile technique for obtaining (randomized)\nsubexponential parameterized algorithms for problems on planar graphs, usually\nwith running time bound $2^{\\mathcal{O}(\\sqrt{k} \\log^2 k)}\nn^{\\mathcal{O}(1)}$. The technique can be applied to problems expressible as\nsearching for a small, connected pattern with a prescribed property in a large\nhost graph, examples of such problems include Directed $k$-Path, Weighted\n$k$-Path, Vertex Cover Local Search, and Subgraph Isomorphism, among others. Up\nto this point, it was open whether these problems can be solved in\nsubexponential parameterized time on planar graphs, because they are not\namenable to the classic technique of bidimensionality. Furthermore, all our\nresults hold in fact on any class of graphs that exclude a fixed apex graph as\na minor, in particular on graphs embeddable in any fixed surface.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 15:19:33 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Marx", "D\u00e1niel", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Saurabh", "Saket", ""]]}, {"id": "1604.06002", "submitter": "Travis Gagie", "authors": "Djamal Belazzougui, Fabio Cunial, Travis Gagie, Nicola Prezza, Mathieu\n  Raffinot", "title": "Practical combinations of repetition-aware data structures", "comments": "arXiv admin note: text overlap with arXiv:1502.05937", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly-repetitive collections of strings are increasingly being amassed by\ngenome sequencing and genetic variation experiments, as well as by storing all\nversions of human-generated files, like webpages and source code. Existing\nindexes for locating all the exact occurrences of a pattern in a\nhighly-repetitive string take advantage of a single measure of repetition.\nHowever, multiple, distinct measures of repetition all grow sublinearly in the\nlength of a highly-repetitive string. In this paper we explore the practical\nadvantages of combining data structures whose size depends on distinct measures\nof repetition. The main ingredient of our structures is the run-length encoded\nBWT (RLBWT), which takes space proportional to the number of runs in the\nBurrows-Wheeler transform of a string. We describe a range of practical\nvariants that combine RLBWT with the set of boundaries of the Lempel-Ziv 77\nfactors of a string, which take space proportional to the number of factors.\nSuch variants use, respectively, the RLBWT of a string and the RLBWT of its\nreverse, or just one RLBWT inside a bidirectional index, or just one RLBWT with\nsupport for unidirectional extraction. We also study the practical advantages\nof combining RLBWT with the compact directed acyclic word graph of a string, a\ndata structure that takes space proportional to the number of one-character\nextensions of maximal repeats. Our approaches are easy to implement, and\nprovide competitive tradeoffs on significant datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 15:30:36 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 14:31:16 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Cunial", "Fabio", ""], ["Gagie", "Travis", ""], ["Prezza", "Nicola", ""], ["Raffinot", "Mathieu", ""]]}, {"id": "1604.06056", "submitter": "O-Joung Kwon", "authors": "Eduard Eiben and Robert Ganian and O-joung Kwon", "title": "A single-exponential fixed-parameter algorithm for Distance-Hereditary\n  Vertex Deletion", "comments": "43 pages, 9 figures (revised journal version; an extended abstract\n  appeared in the proceedings of MFCS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex deletion problems ask whether it is possible to delete at most $k$\nvertices from a graph so that the resulting graph belongs to a specified graph\nclass. Over the past years, the parameterized complexity of vertex deletion to\na plethora of graph classes has been systematically researched. Here we present\nthe first single-exponential fixed-parameter tractable algorithm for vertex\ndeletion to distance-hereditary graphs, a well-studied graph class which is\nparticularly important in the context of vertex deletion due to its connection\nto the graph parameter rank-width. We complement our result with matching\nasymptotic lower bounds based on the exponential time hypothesis. As an\napplication of our algorithm, we show that a vertex deletion set to\ndistance-hereditary graphs can be used as a parameter which allows\nsingle-exponential fixed-parameter tractable algorithms for classical NP-hard\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 18:45:34 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 16:55:52 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 13:54:35 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Kwon", "O-joung", ""]]}, {"id": "1604.06058", "submitter": "Frank Kammer", "authors": "Torben Hagerup and Frank Kammer", "title": "Succinct Choice Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice dictionary is introduced as a data structure that can be\ninitialized with a parameter $n\\in\\mathbb{N}=\\{1,2,\\ldots\\}$ and subsequently\nmaintains an initially empty subset $S$ of $\\{1,\\ldots,n\\}$ under insertion,\ndeletion, membership queries and an operation choice that returns an arbitrary\nelement of $S$. The choice dictionary appears to be fundamental in\nspace-efficient computing. We show that there is a choice dictionary that can\nbe initialized with $n$ and an additional parameter $t\\in\\mathbb{N}$ and\nsubsequently occupies $n+O(n(t/w)^t+\\log n)$ bits of memory and executes each\nof the four operations insert, delete, contains (i.e., a membership query) and\nchoice in $O(t)$ time on a word RAM with a word length of $w=\\Omega(\\log n)$\nbits. In particular, with $w=\\Theta(\\log n)$, we can support insert, delete,\ncontains and choice in constant time using $n+O(n/(\\log n)^t)$ bits for\narbitrary fixed $t$. We extend our results to maintaining several pairwise\ndisjoint subsets of $\\{1,\\ldots,n\\}$.\n  We study additional space-efficient data structures for subsets $S$ of\n$\\{1,\\ldots,n\\}$, including one that supports only insertion and an operation\nextract-choice that returns and deletes an arbitrary element of $S$. All our\nmain data structures can be initialized in constant time and support efficient\niteration over the set $S$, and we can allow changes to $S$ while an iteration\nover $S$ is in progress. We use these abilities crucially in designing the most\nspace-efficient algorithms known for solving a number of graph and other\ncombinatorial problems in linear time. In particular, given an undirected graph\n$G$ with $n$ vertices and $m$ edges, we can output a spanning forest of $G$ in\n$O(n+m)$ time with at most $(1+\\epsilon)n$ bits of working memory for arbitrary\nfixed $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 18:52:00 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 16:06:27 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 17:27:05 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Hagerup", "Torben", ""], ["Kammer", "Frank", ""]]}, {"id": "1604.06067", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Cuckoo Filter: Simplification and Analysis", "comments": "12 pages, 1 figure. To appear at the 15th Scandinavian Symposium and\n  Workshops on Algorithm Theory, June 22-24, 2016, Reykjavik, Iceland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cuckoo filter data structure of Fan, Andersen, Kaminsky, and Mitzenmacher\n(CoNEXT 2014) performs the same approximate set operations as a Bloom filter in\nless memory, with better locality of reference, and adds the ability to delete\nelements as well as to insert them. However, until now it has lacked\ntheoretical guarantees on its performance. We describe a simplified version of\nthe cuckoo filter using fewer hash function calls per query. With this\nsimplification, we provide the first theoretical performance guarantees on\ncuckoo filters, showing that they succeed with high probability whenever their\nfingerprint length is large enough.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 19:14:41 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1604.06181", "submitter": "Zhao Zhang", "authors": "Jiao Zhou, Zhao Zhang, Xiaohui Huang, Ding-Zhu Du", "title": "Approximation Algorithm for Fault-Tolerant Virtual Backbone in Wireless\n  Sensor Networks", "comments": "IEEE/ACM Transactions on Networking, 2017", "journal-ref": null, "doi": "10.1109/TNET.2017.2740328", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To save energy and alleviate interferences in a wireless sensor network, the\nusage of virtual backbone was proposed. Because of accidental damages or energy\ndepletion, it is desirable to construct a fault tolerant virtual backbone,\nwhich can be modeled as a $k$-connected $m$-fold dominating set (abbreviated as\n$(k,m)$-CDS) in a graph. A node set $C\\subseteq V(G)$ is a $(k,m)$-CDS of graph\n$G$ if every node in $V(G)\\backslash C$ is adjacent with at least $m$ nodes in\n$C$ and the subgraph of $G$ induced by $C$ is $k$-connected. In this paper, we\npresent an approximation algorithm for the minimum $(3,m)$-CDS problem with\n$m\\geq3$. The performance ratio is at most $\\gamma$, where\n$\\gamma=\\alpha+8+2\\ln(2\\alpha-6)$ for $\\alpha\\geq4$ and $\\gamma=3\\alpha+2\\ln2$\nfor $\\alpha<4$, and $\\alpha$ is the performance ratio for the minimum\n$(2,m)$-CDS problem. Using currently best known value of $\\alpha$, the\nperformance ratio is $\\ln\\delta+o(\\ln\\delta)$, where $\\delta$ is the maximum\ndegree of the graph, which is asymptotically best possible in view of the\nnon-approximability of the problem. This is the first performance-guaranteed\nalgorithm for the minimum $(3,m)$-CDS problem on a general graph. Furthermore,\napplying our algorithm on a unit disk graph which models a homogeneous wireless\nsensor network, the performance ratio is less than 27, improving previous ratio\n62.3 by a large amount for the $(3,m)$-CDS problem on a unit disk graph.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 05:07:40 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 02:53:43 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhou", "Jiao", ""], ["Zhang", "Zhao", ""], ["Huang", "Xiaohui", ""], ["Du", "Ding-Zhu", ""]]}, {"id": "1604.06264", "submitter": "Jesper Sindahl Nielsen", "authors": "Peyman Afshani and Jesper Sindahl Nielsen", "title": "Data Structure Lower Bounds for Document Indexing Problems", "comments": "Full version of the conference version that appeared at ICALP 2016,\n  25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study data structure problems related to document indexing and pattern\nmatching queries and our main contribution is to show that the pointer machine\nmodel of computation can be extremely useful in proving high and unconditional\nlower bounds that cannot be obtained in any other known model of computation\nwith the current techniques. Often our lower bounds match the known space-query\ntime trade-off curve and in fact for all the problems considered, there is a\nvery good and reasonable match between the our lower bounds and the known upper\nbounds, at least for some choice of input parameters. The problems that we\nconsider are set intersection queries (both the reporting variant and the\nsemi-group counting variant), indexing a set of documents for two-pattern\nqueries, or forbidden- pattern queries, or queries with wild-cards, and\nindexing an input set of gapped-patterns (or two-patterns) to find those\nmatching a document given at the query time.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 11:42:41 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Afshani", "Peyman", ""], ["Nielsen", "Jesper Sindahl", ""]]}, {"id": "1604.06379", "submitter": "Daniel Merkle", "authors": "Christoph Flamm and Daniel Merkle and Peter F. Stadler and Uffe\n  Thorsen", "title": "Automatic Inference of Graph Transformation Rules Using the Cyclic\n  Nature of Chemical Reactions", "comments": "ICGT 2016 : 9th International Conference on Graph Transformation,\n  extended technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph transformation systems have the potential to be realistic models of\nchemistry, provided a comprehensive collection of reaction rules can be\nextracted from the body of chemical knowledge. A first key step for rule\nlearning is the computation of atom-atom mappings, i.e., the atom-wise\ncorrespondence between products and educts of all published chemical reactions.\nThis can be phrased as a maximum common edge subgraph problem with the\nconstraint that transition states must have cyclic structure. We describe a\nsearch tree method well suited for small edit distance and an integer linear\nprogram best suited for general instances and demonstrate that it is feasible\nto compute atom-atom maps at large scales using a manually curated database of\nbiochemical reactions as an example. In this context we address the network\ncompletion problem.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 16:47:11 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Flamm", "Christoph", ""], ["Merkle", "Daniel", ""], ["Stadler", "Peter F.", ""], ["Thorsen", "Uffe", ""]]}, {"id": "1604.06443", "submitter": "Gautam Kamath", "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur\n  Moitra, Alistair Stewart", "title": "Robust Estimators in High Dimensions without the Computational\n  Intractability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional distribution learning in an agnostic setting where\nan adversary is allowed to arbitrarily corrupt an $\\varepsilon$-fraction of the\nsamples. Such questions have a rich history spanning statistics, machine\nlearning and theoretical computer science. Even in the most basic settings, the\nonly known approaches are either computationally inefficient or lose\ndimension-dependent factors in their error guarantees. This raises the\nfollowing question:Is high-dimensional agnostic distribution learning even\npossible, algorithmically?\n  In this work, we obtain the first computationally efficient algorithms with\ndimension-independent error guarantees for agnostically learning several\nfundamental classes of high-dimensional distributions: (1) a single Gaussian,\n(2) a product distribution on the hypercube, (3) mixtures of two product\ndistributions (under a natural balancedness condition), and (4) mixtures of\nspherical Gaussians. Our algorithms achieve error that is independent of the\ndimension, and in many cases scales nearly-linearly with the fraction of\nadversarially corrupted samples. Moreover, we develop a general recipe for\ndetecting and correcting corruptions in high-dimensions, that may be applicable\nto many other problems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 19:54:24 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 02:31:22 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kamath", "Gautam", ""], ["Kane", "Daniel", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""], ["Stewart", "Alistair", ""]]}, {"id": "1604.06452", "submitter": "Tina Novak", "authors": "Tina Novak, Janez Zerovnik", "title": "Weighted domination number of cactus graphs", "comments": "17 pages, figures, submitted to Discussiones Mathematicae Graph\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we write a linear algorithm for calculating the weighted\ndomination number of a vertex-weighted cactus. The algorithm is based on the\nwell known depth first search (DFS) structure. Our algorithm needs less than\n$12n+5b$ additions and $9n+2b$ $\\min$-operations where $n$ is the number of\nvertices and $b$ is the number of blocks in the cactus.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 08:18:01 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Novak", "Tina", ""], ["Zerovnik", "Janez", ""]]}, {"id": "1604.06605", "submitter": "Jouni Sir\\'en", "authors": "Jouni Sir\\'en", "title": "Indexing Variation Graphs", "comments": "Proc. ALENEX 2017. The implementation is available at\n  https://github.com/jltsiren/gcsa2", "journal-ref": null, "doi": "10.1137/1.9781611974768.2", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variation graphs, which represent genetic variation within a population, are\nreplacing sequences as reference genomes. Path indexes are one of the most\nimportant tools for working with variation graphs. They generalize text indexes\nto graphs, allowing one to find the paths matching the query string. We propose\nusing de Bruijn graphs as path indexes, compressing them by merging redundant\nsubgraphs, and encoding them with the Burrows-Wheeler transform. The resulting\nfast, space-efficient, and versatile index is used in the variation graph\ntoolkit vg.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 10:59:53 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 12:33:15 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2016 15:42:42 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2017 12:51:42 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Sir\u00e9n", "Jouni", ""]]}, {"id": "1604.06682", "submitter": "Pierre-David Letourneau", "authors": "Pierre-David Letourneau and Harper Langston and Benoit Meister and\n  Richard Lethin", "title": "A sparse multidimensional FFT for real positive vectors", "comments": "Fixed minor typos. Corrected use of Q^{-1} in Algorithm 3 and theorem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse multidimensional FFT (sMFFT) randomized algorithm for\nreal positive vectors. The algorithm works in any fixed dimension, requires\n(O(R log(R) log(N)) ) samples and runs in O( R log^2(R) log(N)) complexity\n(where N is the total size of the vector in d dimensions and R is the number of\nnonzeros). It is stable to low-level noise and exhibits an exponentially small\nprobability of failure.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 14:39:33 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 19:14:10 GMT"}, {"version": "v3", "created": "Mon, 9 May 2016 15:28:27 GMT"}, {"version": "v4", "created": "Tue, 19 Jul 2016 17:15:07 GMT"}, {"version": "v5", "created": "Fri, 30 Sep 2016 17:57:07 GMT"}, {"version": "v6", "created": "Wed, 7 Dec 2016 15:44:57 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Letourneau", "Pierre-David", ""], ["Langston", "Harper", ""], ["Meister", "Benoit", ""], ["Lethin", "Richard", ""]]}, {"id": "1604.06687", "submitter": "German Tischler", "authors": "German Tischler", "title": "Faster Average Case Low Memory Semi-External Construction of the\n  Burrows-Wheeler Transform", "comments": "accepted for publication on MCS in 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows Wheeler transform has applications in data compression as well as\nfull text indexing. Despite its important applications and various existing\nalgorithmic approaches the construction of the transform for large data sets is\nstill challenging. In this paper we present a new semi external memory\nalgorithm for constructing the Burrows Wheeler transform. It is capable of\nconstructing the transform for an input text of length $n$ over a finite\nalphabet in time $O(n\\log^2\\log n)$ on average, if sufficient internal memory\nis available to hold a fixed fraction of the input text. In the worst case the\nrun-time is $O(n\\log n \\log\\log n)$. The amount of space used by the algorithm\nin external memory is $O(n)$ bits. Based on the serial version we also present\na shared memory parallel algorithm running in time\n$O(\\frac{n}{p}\\max\\{\\log^2\\log n+\\log p\\})$ on average when $p$ processors are\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 14:46:01 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Tischler", "German", ""]]}, {"id": "1604.06697", "submitter": "Armin Wei{\\ss}", "authors": "Stefan Edelkamp and Armin Wei{\\ss}", "title": "BlockQuicksort: How Branch Mispredictions don't affect Quicksort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the work of Kaligosi and Sanders (2006), it is well-known that\nQuicksort -- which is commonly considered as one of the fastest in-place\nsorting algorithms -- suffers in an essential way from branch mispredictions.\nWe present a novel approach to address this problem by partially decoupling\ncontrol from data flow: in order to perform the partitioning, we split the\ninput in blocks of constant size (we propose 128 data elements); then, all\nelements in one block are compared with the pivot and the outcomes of the\ncomparisons are stored in a buffer. In a second pass, the respective elements\nare rearranged. By doing so, we avoid conditional branches based on outcomes of\ncomparisons at all (except for the final Insertionsort). Moreover, we prove\nthat for a static branch predictor the average total number of branch\nmispredictions is at most $\\epsilon n \\log n + O(n)$ for some small $\\epsilon$\ndepending on the block size when sorting $n$ elements.\n  Our experimental results are promising: when sorting random integer data, we\nachieve an increase in speed of 80% over the GCC implementation of C++\nstd::sort. Also for many other types of data and non-random inputs, there is\nstill a significant speedup over std::sort. Only in few special cases like\nsorted or almost sorted inputs, std::sort can beat out implementation.\nMoreover, even on random input permutations, our implementation is even\nslightly faster than an implementation of the highly tuned Super Scalar Sample\nSort, which uses a linear amount of additional space.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 15:07:03 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 20:04:03 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Edelkamp", "Stefan", ""], ["Wei\u00df", "Armin", ""]]}, {"id": "1604.06707", "submitter": "G\\\"unter Rote", "authors": "Felix Herter and G\\\"unter Rote", "title": "Loopless Gray Code Enumeration and the Tower of Bucharest", "comments": "16 pages plus 8 pages of appendix with Python programs, 6 figures. A\n  Python script to extract the program code from the LaTeX file is attached to\n  the sources", "journal-ref": "Theoretical Computer Science 748 (2018), 40-54", "doi": "10.1016/j.tcs.2017.11.017", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new algorithms for generating all n-tuples over an alphabet of m\nletters, changing only one letter at a time (Gray codes). These algorithms are\nbased on the connection with variations of the Towers of Hanoi game. Our\nalgorithms are loopless, in the sense that the next change can be determined in\na constant number of steps, and they can be implemented in hardware. We also\ngive another family of loopless algorithms that is based on the idea of working\nahead and saving the work in a buffer.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 15:19:24 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Herter", "Felix", ""], ["Rote", "G\u00fcnter", ""]]}, {"id": "1604.06918", "submitter": "Kirankumar Shiragur", "authors": "Deeparnab Chakrabarty, Kirankumar Shiragur", "title": "Graph Balancing with Two Edge Types", "comments": "Remark. (added 9th June, 2016.) After posting the version 1 of our\n  paper, it was brought to our notice that our result is not new. Huang and Ott\n  [HO15], and independently, Page and Solis-Oba [PSO16] also obtain the same\n  result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the graph balancing problem the goal is to orient a weighted undirected\ngraph to minimize the maximum weighted in-degree. This special case of makespan\nminimization is NP-hard to approximate to a factor better than 3/2 even when\nthere are only two types of edge weights. In this note we describe a simple 3/2\napproximation for the graph balancing problem with two-edge types, settling\nthis very special case of makespan minimization.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 15:26:32 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 13:53:42 GMT"}, {"version": "v3", "created": "Sun, 11 Sep 2016 00:25:07 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Shiragur", "Kirankumar", ""]]}, {"id": "1604.06968", "submitter": "Kevin A. Lai", "authors": "Kevin A. Lai, Anup B. Rao, Santosh Vempala", "title": "Agnostic Estimation of Mean and Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the mean and covariance of a\ndistribution from iid samples in $\\mathbb{R}^n$, in the presence of an $\\eta$\nfraction of malicious noise; this is in contrast to much recent work where the\nnoise itself is assumed to be from a distribution of known type. The agnostic\nproblem includes many interesting special cases, e.g., learning the parameters\nof a single Gaussian (or finding the best-fit Gaussian) when $\\eta$ fraction of\ndata is adversarially corrupted, agnostically learning a mixture of Gaussians,\nagnostic ICA, etc. We present polynomial-time algorithms to estimate the mean\nand covariance with error guarantees in terms of information-theoretic lower\nbounds. As a corollary, we also obtain an agnostic algorithm for Singular Value\nDecomposition.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 00:23:51 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 19:50:58 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Lai", "Kevin A.", ""], ["Rao", "Anup B.", ""], ["Vempala", "Santosh", ""]]}, {"id": "1604.06997", "submitter": "Manoj Gupta", "authors": "Navin Goyal, Manoj Gupta", "title": "Better Analysis of GREEDY Binary Search Tree on Decomposable Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their seminal paper [Sleator and Tarjan, J.ACM, 1985], the authors\nconjectured that the splay tree is dynamically optimal binary search tree\n(BST). In spite of decades of intensive research, the problem remains open.\nPerhaps a more basic question, which has also attracted much attention, is if\nthere exists any dynamically optimal BST algorithm. One such candidate is\nGREEDY which is a simple and intuitive BST algorithm [Lucas, Rutgers Tech.\nReport, 1988; Munro, ESA, 2000; Demaine, Harmon, Iacono, Kane and Patrascu,\nSODA, 2009]. [Demaine et al., SODA, 2009] showed a novel connection between a\ngeometric problem.\n  Since dynamic optimality conjecture in its most general form remains elusive\ndespite much effort, researchers have studied this problem on special\nsequences. Recently, [Chalermsook, Goswami, Kozma, Mehlhorn and Saranurak,\nFOCS, 2015] studied a type of sequences known as $k$-{\\em decomposable\nsequences} in this context, where $k$ parametrizes easiness of the sequence.\nUsing tools from forbidden submatrix theory, they showed that GREEDY takes\n$n2^{O(k^2)}$ time on this sequence and explicitly raised the question of\nimproving this bound.\n  In this paper, we show that GREEDY takes $O(n \\log{k})$ time on\n$k$-decomposable sequences. In contrast to the previous approach, ours is based\non first principles. One of the main ingredients of our result is a new\nconstruction of a lower bound certificate on the performance of any algorithm.\nThis certificate is constructed using the execution of GREEDY, and is more\nnuanced and possibly more flexible than the previous independent set\ncertificate of Demaine et al. This result, which is applicable to all\nsequences, may be of independent interest and may lead to further progress in\nanalyzing GREEDY on $k$-decomposable as well as general sequences.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 08:48:26 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Goyal", "Navin", ""], ["Gupta", "Manoj", ""]]}, {"id": "1604.07038", "submitter": "Reut Levi", "authors": "Reut Levi and Dana Ron and Ronitt Rubinfeld", "title": "A Local Algorithm for Constructing Spanners in Minor-Free Graphs", "comments": "arXiv admin note: substantial text overlap with arXiv:1402.3609", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing a spanning tree of a graph is one of the most basic tasks in\ngraph theory. We consider this problem in the setting of local algorithms: one\nwants to quickly determine whether a given edge $e$ is in a specific spanning\ntree, without computing the whole spanning tree, but rather by inspecting the\nlocal neighborhood of $e$. The challenge is to maintain consistency. That is,\nto answer queries about different edges according to the same spanning tree.\nSince it is known that this problem cannot be solved without essentially\nviewing all the graph, we consider the relaxed version of finding a spanning\nsubgraph with $(1+\\epsilon)n$ edges (where $n$ is the number of vertices and\n$\\epsilon$ is a given sparsity parameter). It is known that this relaxed\nproblem requires inspecting $\\Omega(\\sqrt{n})$ edges in general graphs, which\nmotivates the study of natural restricted families of graphs. One such family\nis the family of graphs with an excluded minor. For this family there is an\nalgorithm that achieves constant success probability, and inspects\n$(d/\\epsilon)^{poly(h)\\log(1/\\epsilon)}$ edges (for each edge it is queried\non), where $d$ is the maximum degree in the graph and $h$ is the size of the\nexcluded minor. The distances between pairs of vertices in the spanning\nsubgraph $G'$ are at most a factor of $poly(d, 1/\\epsilon, h)$ larger than in\n$G$.\n  In this work, we show that for an input graph that is $H$-minor free for any\n$H$ of size $h$, this task can be performed by inspecting only $poly(d,\n1/\\epsilon, h)$ edges. The distances between pairs of vertices in the spanning\nsubgraph $G'$ are at most a factor of $\\tilde{O}(h\\log(d)/\\epsilon)$ larger\nthan in $G$. Furthermore, the error probability of the new algorithm is\nsignificantly improved to $\\Theta(1/n)$. This algorithm can also be easily\nadapted to yield an efficient algorithm for the distributed setting.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 14:55:43 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Levi", "Reut", ""], ["Ron", "Dana", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1604.07049", "submitter": "Kanstantsin Pashkovich", "authors": "Andreas Emil Feldmann, Jochen K\\\"onemann, Kanstantsin Pashkovich and\n  Laura Sanit\\`a", "title": "Fast Approximation Algorithms for the Generalized Survivable Network\n  Design Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a standard $f$-connectivity network design problem, we are given an\nundirected graph $G=(V,E)$, a cut-requirement function $f:2^V \\rightarrow\n{\\mathbb{N}}$, and non-negative costs $c(e)$ for all $e \\in E$. We are then\nasked to find a minimum-cost vector $x \\in {\\mathbb{N}}^E$ such that\n$x(\\delta(S)) \\geq f(S)$ for all $S \\subseteq V$. We focus on the class of such\nproblems where $f$ is a proper function. This encodes many well-studied NP-hard\nproblems such as the generalized survivable network design problem.\n  In this paper we present the first strongly polynomial time FPTAS for solving\nthe LP relaxation of the standard IP formulation of the $f$-connectivity\nproblem with general proper functions $f$. Implementing Jain's algorithm, this\nyields a strongly polynomial time $(2+\\epsilon)$-approximation for the\ngeneralized survivable network design problem (where we consider rounding up of\nrationals an arithmetic operation).\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 16:40:41 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Feldmann", "Andreas Emil", ""], ["K\u00f6nemann", "Jochen", ""], ["Pashkovich", "Kanstantsin", ""], ["Sanit\u00e0", "Laura", ""]]}, {"id": "1604.07128", "submitter": "Hsiang-Hsuan Liu", "authors": "Wing-Kai Hon, Ton Kloks, Fu-Hong Liu, Hsiang-Hsuan Liu and Tao-Ming\n  Wang", "title": "On the Grundy number of Cameron graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Grundy number of a graph is the maximal number of colors attained by a\nfirst-fit coloring of the graph. The class of Cameron graphs is the Seidel\nswitching class of cographs. In this paper we show that the Grundy number is\ncomputable in polynomial time for Cameron graphs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 04:41:43 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 02:13:05 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Hon", "Wing-Kai", ""], ["Kloks", "Ton", ""], ["Liu", "Fu-Hong", ""], ["Liu", "Hsiang-Hsuan", ""], ["Wang", "Tao-Ming", ""]]}, {"id": "1604.07153", "submitter": "Kim-Manuel Klein", "authors": "Klaus Jansen, Kim-Manuel Klein, Jos\\'e Verschae", "title": "Closing the Gap for Makespan Scheduling via Sparsification Techniques", "comments": "20 pages, ICALP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Makespan scheduling on identical machines is one of the most basic and\nfundamental packing problems studied in the discrete optimization literature.\nIt asks for an assignment of $n$ jobs to a set of $m$ identical machines that\nminimizes the makespan. The problem is strongly NP-hard, and thus we do not\nexpect a $(1+\\epsilon)$-approximation algorithm with a running time that\ndepends polynomially on $1/\\epsilon$. Furthermore, Chen et al. [3] recently\nshowed that a running time of $2^{(1/\\epsilon)^{1-\\delta}}+\\text{poly}(n)$ for\nany $\\delta>0$ would imply that the Exponential Time Hypothesis (ETH) fails. A\nlong sequence of algorithms have been developed that try to obtain low\ndependencies on $1/\\epsilon$, the better of which achieves a running time of\n$2^{\\tilde{O}(1/\\epsilon^2)}+O(n\\log n)$ [11]. In this paper we obtain an\nalgorithm with a running time of $2^{\\tilde{O}(1/\\epsilon)}+O(n\\log n)$, which\nis tight under ETH up to logarithmic factors on the exponent.\n  Our main technical contribution is a new structural result on the\nconfiguration-IP. More precisely, we show the existence of a highly symmetric\nand sparse optimal solution, in which all but a constant number of machines are\nassigned a configuration with small support. This structure can then be\nexploited by integer programming techniques and enumeration. We believe that\nour structural result is of independent interest and should find applications\nto other settings. In particular, we show how the structure can be applied to\nthe minimum makespan problem on related machines and to a larger class of\nobjective functions on parallel machines. For all these cases we obtain an\nefficient PTAS with running time $2^{\\tilde{O}(1/\\epsilon)} + \\text{poly}(n)$.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 07:47:34 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Jansen", "Klaus", ""], ["Klein", "Kim-Manuel", ""], ["Verschae", "Jos\u00e9", ""]]}, {"id": "1604.07166", "submitter": "Liang Li", "authors": "Jun Wan, Yu Xia, Liang Li, Thomas Moscibroda", "title": "Information Cascades on Arbitrary Topologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study information cascades on graphs. In this setting, each\nnode in the graph represents a person. One after another, each person has to\ntake a decision based on a private signal as well as the decisions made by\nearlier neighboring nodes. Such information cascades commonly occur in practice\nand have been studied in complete graphs where everyone can overhear the\ndecisions of every other player. It is known that information cascades can be\nfragile and based on very little information, and that they have a high\nlikelihood of being wrong.\n  Generalizing the problem to arbitrary graphs reveals interesting insights. In\nparticular, we show that in a random graph $G(n,q)$, for the right value of\n$q$, the number of nodes making a wrong decision is logarithmic in $n$. That\nis, in the limit for large $n$, the fraction of players that make a wrong\ndecision tends to zero. This is intriguing because it contrasts to the two\nnatural corner cases: empty graph (everyone decides independently based on his\nprivate signal) and complete graph (all decisions are heard by all nodes). In\nboth of these cases a constant fraction of nodes make a wrong decision in\nexpectation. Thus, our result shows that while both too little and too much\ninformation sharing causes nodes to take wrong decisions, for exactly the right\namount of information sharing, asymptotically everyone can be right. We further\nshow that this result in random graphs is asymptotically optimal for any\ntopology, even if nodes follow a globally optimal algorithmic strategy. Based\non the analysis of random graphs, we explore how topology impacts global\nperformance and construct an optimal deterministic topology among layer graphs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 08:52:37 GMT"}, {"version": "v2", "created": "Sun, 1 May 2016 03:07:55 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Wan", "Jun", ""], ["Xia", "Yu", ""], ["Li", "Liang", ""], ["Moscibroda", "Thomas", ""]]}, {"id": "1604.07273", "submitter": "Ignasi Sau", "authors": "Julien Baste, Marc Noy, Ignasi Sau", "title": "The number of labeled graphs of bounded treewidth", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on counting the number of labeled graphs on $n$ vertices and\ntreewidth at most $k$ (or equivalently, the number of labeled partial\n$k$-trees), which we denote by $T_{n,k}$. So far, only the particular cases\n$T_{n,1}$ and $T_{n,2}$ had been studied. We show that $$ \\left(c \\cdot\n\\frac{k\\cdot 2^k \\cdot n}{\\log k} \\right)^n \\cdot 2^{-\\frac{k(k+3)}{2}} \\cdot\nk^{-2k-2}\\ \\leq\\ T_{n,k}\\ \\leq\\ \\left(k \\cdot 2^k \\cdot n\\right)^n \\cdot\n2^{-\\frac{k(k+1)}{2}} \\cdot k^{-k}, $$ for $k > 1$ and some explicit absolute\nconstant $c > 0$. The upper bound is an immediate consequence of the well-known\nnumber of labeled $k$-trees, while the lower bound is obtained from an explicit\nalgorithmic construction. It follows from this construction that both bounds\nalso apply to graphs of pathwidth and proper-pathwidth at most $k$.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 14:22:58 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Baste", "Julien", ""], ["Noy", "Marc", ""], ["Sau", "Ignasi", ""]]}, {"id": "1604.07286", "submitter": "Kim-Manuel Klein", "authors": "Klaus Jansen, Kim-Manuel Klein", "title": "About the Structure of the Integer Cone and its Application to Bin\n  Packing", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the bin packing problem with $d$ different item sizes and revisit\nthe structure theorem given by Goemans and Rothvo\\ss [6] about solutions of the\ninteger cone. We present new techniques on how solutions can be modified and\ngive a new structure theorem that relies on the set of vertices of the\nunderlying integer polytope. As a result of our new structure theorem, we\nobtain an algorithm for the bin packing problem with running time\n$|V|^{2^{O(d)}} \\cdot enc(I)^{O(1)}$, where $V$ is the set of vertices of the\ninteger knapsack polytope and $enc(I)$ is the encoding length of the bin\npacking instance. The algorithm is fixed parameter tractable, parameterized by\nthe number of vertices of the integer knapsack polytope $|V|$. This shows that\nthe bin packing problem can be solved efficiently when the underlying integer\nknapsack polytope has an easy structure, i.e. has a small number of vertices.\n  Furthermore, we show that the presented bounds of the structure theorem are\nasymptotically tight. We give a construction of bin packing instances using new\nstructural insights and classical number theoretical theorems which yield the\ndesired lower bound.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 14:44:13 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 12:56:10 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Jansen", "Klaus", ""], ["Klein", "Kim-Manuel", ""]]}, {"id": "1604.07359", "submitter": "Katherine Edwards", "authors": "Katherine Edwards, W. Sean Kennedy, Iraj Saniee", "title": "Fast approximation algorithms for $p$-centres in large\n  $\\delta$-hyperbolic graphs", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a quasilinear time algorithm for the $p$-center problem with an\nadditive error less than or equal to 3 times the input graph's hyperbolic\nconstant. Specifically, for the graph $G=(V,E)$ with $n$ vertices, $m$ edges\nand hyperbolic constant $\\delta$, we construct an algorithm for $p$-centers in\ntime $O(p(\\delta+1)(n+m)\\log(n))$ with radius not exceeding $r_p + \\delta$ when\n$p \\leq 2$ and $r_p + 3\\delta$ when $p \\geq 3$, where $r_p$ are the optimal\nradii. Prior work identified $p$-centers with accuracy $r_p+\\delta$ but with\ntime complexity $O((n^3\\log n + n^2m)\\log(diam(G)))$ which is impractical for\nlarge graphs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 18:39:25 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Edwards", "Katherine", ""], ["Kennedy", "W. Sean", ""], ["Saniee", "Iraj", ""]]}, {"id": "1604.07467", "submitter": "Samson Zhou", "authors": "Elena Grigorescu, Morteza Monemizadeh, Samson Zhou", "title": "Estimating Weighted Matchings in $o(n)$ Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the weight of a maximum weighted\nmatching of a weighted graph $G(V,E)$ whose edges are revealed in a streaming\nfashion. We develop a reduction from the maximum weighted matching problem to\nthe maximum cardinality matching problem that only doubles the approximation\nfactor of a streaming algorithm developed for the maximum cardinality matching\nproblem. Our results hold for the insertion-only and the dynamic (i.e,\ninsertion and deletion) edge-arrival streaming models. The previous best-known\nreduction is due to Bury and Schwiegelshohn (ESA 2015) who develop an algorithm\nwhose approximation guarantee scales by a polynomial factor.\n  As an application, we obtain improved estimators for weighted planar graphs\nand, more generally, for weighted bounded-arboricity graphs, by feeding into\nour reduction the recent estimators due to Esfandiari et al. (SODA 2015) and to\nChitnis et al. (SODA 2016). In particular, we obtain a\n$(48+\\epsilon)$-approximation estimator for the weight of a maximum weighted\nmatching in planar graphs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 22:52:38 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 18:41:23 GMT"}, {"version": "v3", "created": "Mon, 5 Sep 2016 04:45:24 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Grigorescu", "Elena", ""], ["Monemizadeh", "Morteza", ""], ["Zhou", "Samson", ""]]}, {"id": "1604.07535", "submitter": "Tsunehiko Kameda", "authors": "Aritra Banik, Binay Bhattacharya, Sandip Das, Tsunehiko Kameda, and\n  Zhao Song", "title": "The $p$-Center Problem in Tree Networks Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two improved algorithms for weighted discrete $p$-center problem\nfor tree networks with $n$ vertices. One of our proposed algorithms runs in\n$O(n \\log n + p \\log^2 n \\log(n/p))$ time. For all values of $p$, our algorithm\nthus runs as fast as or faster than the most efficient $O(n\\log^2 n)$ time\nalgorithm obtained by applying Cole's speed-up technique [cole1987] to the\nalgorithm due to Megiddo and Tamir [megiddo1983], which has remained\nunchallenged for nearly 30 years. Our other algorithm, which is more practical,\nruns in $O(n \\log n + p^2 \\log^2(n/p))$ time, and when $p=O(\\sqrt{n})$ it is\nfaster than Megiddo and Tamir's $O(n \\log^2n \\log\\log n)$ time algorithm\n[megiddo1983].\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 06:07:30 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Banik", "Aritra", ""], ["Bhattacharya", "Binay", ""], ["Das", "Sandip", ""], ["Kameda", "Tsunehiko", ""], ["Song", "Zhao", ""]]}, {"id": "1604.07581", "submitter": "Jakub Radoszewski", "authors": "Tomasz Kociumaka, Solon P. Pissis, and Jakub Radoszewski", "title": "Pattern Matching and Consensus Problems on Weighted Sequences and\n  Profiles", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study pattern matching problems on two major representations of uncertain\nsequences used in molecular biology: weighted sequences (also known as position\nweight matrices, PWM) and profiles (i.e., scoring matrices). In the simple\nversion, in which only the pattern or only the text is uncertain, we obtain\nefficient algorithms with theoretically-provable running times using a\nvariation of the lookahead scoring technique. We also consider a general\nvariant of the pattern matching problems in which both the pattern and the text\nare uncertain. Central to our solution is a special case where the sequences\nhave equal length, called the consensus problem. We propose algorithms for the\nconsensus problem parameterized by the number of strings that match one of the\nsequences. As our basic approach, a careful adaptation of the classic\nmeet-in-the-middle algorithm for the knapsack problem is used. On the lower\nbound side, we prove that our dependence on the parameter is optimal up to\nlower-order terms conditioned on the optimality of the original algorithm for\nthe knapsack problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 09:22:50 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 19:47:55 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""]]}, {"id": "1604.07638", "submitter": "Yixin Bao", "authors": "Yixin Bao, Xiaoke Wang, Zhi Wang, Chuan Wu, Francis C.M. Lau", "title": "Online Influence Maximization in Non-Stationary Social Networks", "comments": "10 pages. To appear in IEEE/ACM IWQoS 2016. Full version", "journal-ref": null, "doi": "10.1109/IWQoS.2016.7590438", "report-no": null, "categories": "cs.SI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks have been popular platforms for information propagation. An\nimportant use case is viral marketing: given a promotion budget, an advertiser\ncan choose some influential users as the seed set and provide them free or\ndiscounted sample products; in this way, the advertiser hopes to increase the\npopularity of the product in the users' friend circles by the world-of-mouth\neffect, and thus maximizes the number of users that information of the\nproduction can reach. There has been a body of literature studying the\ninfluence maximization problem. Nevertheless, the existing studies mostly\ninvestigate the problem on a one-off basis, assuming fixed known influence\nprobabilities among users, or the knowledge of the exact social network\ntopology. In practice, the social network topology and the influence\nprobabilities are typically unknown to the advertiser, which can be varying\nover time, i.e., in cases of newly established, strengthened or weakened social\nties. In this paper, we focus on a dynamic non-stationary social network and\ndesign a randomized algorithm, RSB, based on multi-armed bandit optimization,\nto maximize influence propagation over time. The algorithm produces a sequence\nof online decisions and calibrates its explore-exploit strategy utilizing\noutcomes of previous decisions. It is rigorously proven to achieve an\nupper-bounded regret in reward and applicable to large-scale social networks.\nPractical effectiveness of the algorithm is evaluated using both synthetic and\nreal-world datasets, which demonstrates that our algorithm outperforms previous\nstationary methods under non-stationary conditions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 12:02:55 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Bao", "Yixin", ""], ["Wang", "Xiaoke", ""], ["Wang", "Zhi", ""], ["Wu", "Chuan", ""], ["Lau", "Francis C. M.", ""]]}, {"id": "1604.07661", "submitter": "Danica Greetham", "authors": "Danica Vukadinovi\\'c Greetham, Nathaniel Charlton, Anush Poghosyan", "title": "Total positive influence domination on weighted networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are proposing two greedy and a new linear programming based approximation\nalgorithm for the total positive influence dominating set problem in weighted\nnetworks. Applications of this problem in weighted settings include finding: a\nminimum cost set of nodes to broadcast a message in social networks, such that\neach node has majority of neighbours broadcasting that message; a maximum\ntrusted set in bitcoin network; an optimal set of hosts when running\ndistributed apps etc. Extensive experiments on different generated and real\nnetworks highlight advantages and potential issues for each algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 13:02:05 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 10:17:28 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Greetham", "Danica Vukadinovi\u0107", ""], ["Charlton", "Nathaniel", ""], ["Poghosyan", "Anush", ""]]}, {"id": "1604.07666", "submitter": "Baoyuan Wu", "authors": "Baoyuan Wu, Bernard Ghanem", "title": "$\\ell_p$-Box ADMM: A Versatile Framework for Integer Programming", "comments": "both authors share first-authorship. Integer programming, Lp-box\n  intersection, ADMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the integer programming (IP) problem, which plays a\nfundamental role in many computer vision and machine learning applications. The\nliterature abounds with many seminal works that address this problem, some\nfocusing on continuous approaches (e.g. linear program relaxation) while others\non discrete ones (e.g., min-cut). However, a limited number of them are\ndesigned to handle the general IP form and even these methods cannot adequately\nsatisfy the simultaneous requirements of accuracy, feasibility, and\nscalability. To this end, we propose a novel and versatile framework called\n$\\ell_p$-box ADMM, which is based on two parts. (1) The discrete constraint is\nequivalently replaced by the intersection of a box and a $(n-1)$-dimensional\nsphere (defined through the $\\ell_p$ norm). (2) We infuse this equivalence into\nthe ADMM (Alternating Direction Method of Multipliers) framework to handle\nthese continuous constraints separately and to harness its attractive\nproperties. More importantly, the ADMM update steps can lead to manageable\nsub-problems in the continuous domain. To demonstrate its efficacy, we consider\nan instance of the framework, namely $\\ell_2$-box ADMM applied to binary\nquadratic programming (BQP). Here, the ADMM steps are simple, computationally\nefficient, and theoretically guaranteed to converge to a KKT point. We\ndemonstrate the applicability of $\\ell_2$-box ADMM on three important\napplications: MRF energy minimization, graph matching, and clustering. Results\nclearly show that it significantly outperforms existing generic IP solvers both\nin runtime and objective. It also achieves very competitive performance vs.\nstate-of-the-art methods specific to these applications.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 13:15:17 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 08:24:32 GMT"}, {"version": "v3", "created": "Sun, 19 Jun 2016 12:53:02 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wu", "Baoyuan", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1604.07724", "submitter": "Hendrik Molter", "authors": "Robert Bredereck, Christian Komusiewicz, Stefan Kratsch, Hendrik\n  Molter, Rolf Niedermeier, Manuel Sorge", "title": "Assessing the Computational Complexity of Multi-Layer Subgraph Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-layer graphs consist of several graphs (layers) over the same vertex\nset. They are motivated by real-world problems where entities (vertices) are\nassociated via multiple types of relationships (edges in different layers). We\nchart the border of computational (in)tractability for the class of subgraph\ndetection problems on multi-layer graphs, including fundamental problems such\nas maximum matching, finding certain clique relaxations (motivated by community\ndetection), or path problems. Mostly encountering hardness results, sometimes\neven for two or three layers, we can also spot some islands of tractability.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 15:45:55 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 18:00:29 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Bredereck", "Robert", ""], ["Komusiewicz", "Christian", ""], ["Kratsch", "Stefan", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Sorge", "Manuel", ""]]}, {"id": "1604.08125", "submitter": "Max Klimm", "authors": "Yann Disser, John Fearnley, Martin Gairing, Oliver G\\\"obel, Max Klimm,\n  Daniel Schmand, Alexander Skopalik, Andreas T\\\"onnis", "title": "Hiring Secretaries over Time: The Benefit of Concurrent Employment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic online problem where $n$ applicants arrive over\ntime, one per time step. Upon arrival of each applicant their cost per time\nstep is revealed, and we have to fix the duration of employment, starting\nimmediately. This decision is irrevocable, i.e., we can neither extend a\ncontract nor dismiss a candidate once hired. In every time step, at least one\ncandidate needs to be under contract, and our goal is to minimize the total\nhiring cost, which is the sum of the applicants' costs multiplied with their\nrespective employment durations. We provide a competitive online algorithm for\nthe case that the applicants' costs are drawn independently from a known\ndistribution. Specifically, the algorithm achieves a competitive ratio of 2.965\nfor the case of uniform distributions. For this case, we give an analytical\nlower bound of 2 and a computational lower bound of 2.148. We then adapt our\nalgorithm to stay competitive even in settings with one or more of the\nfollowing restrictions: (i) at most two applicants can be hired concurrently;\n(ii) the distribution of the applicants' costs is unknown; (iii) the total\nnumber $n$ of time steps is unknown. On the other hand, we show that concurrent\nemployment is a necessary feature of competitive algorithms by proving that no\nalgorithm has a competitive ratio better than $\\Omega(\\sqrt{n} / \\log n)$ if\nconcurrent employment is forbidden.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 16:14:28 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 14:30:24 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Disser", "Yann", ""], ["Fearnley", "John", ""], ["Gairing", "Martin", ""], ["G\u00f6bel", "Oliver", ""], ["Klimm", "Max", ""], ["Schmand", "Daniel", ""], ["Skopalik", "Alexander", ""], ["T\u00f6nnis", "Andreas", ""]]}, {"id": "1604.08132", "submitter": "Jochen Koenemann", "authors": "Zachary Friggstad, Jochen Koenemann, Mohammad Shadravan", "title": "A Logarithmic Integrality Gap Bound for Directed Steiner Tree in\n  Quasi-bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that the integrality gap of the natural cut-based LP\nrelaxation for the directed Steiner tree problem is $O(\\log k)$ in\nquasi-bipartite graphs with $k$ terminals. Such instances can be seen to\ngeneralize set cover, so the integrality gap analysis is tight up to a constant\nfactor. A novel aspect of our approach is that we use the primal-dual method; a\ntechnique that is rarely used in designing approximation algorithms for network\ndesign problems in directed graphs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 16:30:43 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Friggstad", "Zachary", ""], ["Koenemann", "Jochen", ""], ["Shadravan", "Mohammad", ""]]}, {"id": "1604.08137", "submitter": "Valmir C. Barbosa", "authors": "Fabiano de S. Oliveira, Valmir C. Barbosa", "title": "On the mediation of program allocation in high-demand environments", "comments": "This version addresses a few minor issues and fixes a derivative", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we challenge the widely accepted premise that, in order to\ncarry out a distributed computation, say on the cloud, users have to inform,\nalong with all the inputs that the algorithm in use requires, the number of\nprocessors to be used. We discuss the complicated nature of deciding the value\nof such parameter, should it be chosen optimally, and propose the alternative\nscenario in which this choice is passed on to the server side for automatic\ndetermination. We show that the allocation problem arising from this\nalternative is NP-hard only weakly, being therefore solvable in\npseudo-polynomial time. In our proposal, one key component on which the\nautomatic determination of the number of processors is based is the cost model.\nThe one we use, which is being increasingly adopted in the wake of the\ncloud-computing movement, posits that each single execution of a program is to\nbe subject to current circumstances on both user and server side, and as such\nbe priced independently of all others. Running through our proposal is thus a\ncritique of the established common sense that sizing a set of processors to\nhandle a submission to some provider is entirely up to the user.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 16:48:45 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 17:43:43 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 17:48:56 GMT"}, {"version": "v4", "created": "Fri, 20 Sep 2019 17:22:17 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Oliveira", "Fabiano de S.", ""], ["Barbosa", "Valmir C.", ""]]}, {"id": "1604.08147", "submitter": "Fritz B\\\"okler", "authors": "Fritz B\\\"okler and Petra Mutzel", "title": "Tree-Deletion Pruning in Label-Correcting Algorithms for the\n  Multiobjective Shortest Path Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we re-evaluate the basic strategies for label correcting\nalgorithms for the multiobjective shortest path (MOSP) problem, i.e., node and\nlabel selection. In contrast to common believe, we show that---when carefully\nimplemented---the node-selection strategy usually beats the label-selection\nstrategy. Moreover, we present a new pruning method which is easy to implement\nand performs very well on real-world road networks. In this study, we test our\nhypotheses on artificial MOSP instances from the literature with up to 15\nobjectives and real-world road networks with up to almost 160,000 nodes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 17:11:09 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["B\u00f6kler", "Fritz", ""], ["Mutzel", "Petra", ""]]}, {"id": "1604.08234", "submitter": "Sebastian Krinninger", "authors": "Krishnendu Chatterjee, Monika Henzinger, Sebastian Krinninger, Danupon\n  Nanongkai", "title": "Polynomial-Time Algorithms for Energy Games with Special Weight\n  Structures", "comments": "This paper appeared in the ESA 2012 special issue of Algorithmica. A\n  preliminary version was presented at the 20th Annual European Symposium on\n  Algorithms (ESA 2012)", "journal-ref": "Algorithmica 70(3): 457-492 (2014)", "doi": "10.1007/s00453-013-9843-7", "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy games belong to a class of turn-based two-player infinite-duration\ngames}played on a weighted directed graph. It is one of the rare and intriguing\ncombinatorial problems that lie in ${\\sf NP} \\cap {\\sf co\\mbox{-}NP}$, but are\nnot known to be in ${\\sf P}$. The existence of polynomial-time algorithms has\nbeen a major open problem for decades and apart from pseudopolynomial\nalgorithms there is no algorithm that solves any non-trivial subclass in\npolynomial time.\n  In this paper, we give several results based on the weight structures of the\ngraph. First, we identify a notion of penalty and present a polynomial-time\nalgorithm when the penalty is large. Our algorithm is the first polynomial-time\nalgorithm on a large class of weighted graphs. It includes several worst-case\ninstances on which previous algorithms, such as value iteration and random\nfacet algorithms, require at least sub-exponential time. Our main technique is\ndeveloping the first non-trivial approximation algorithm and showing how to\nconvert it to an exact algorithm. Moreover, we show that in a practical case in\nverification where weights are clustered around a constant number of values,\nthe energy game problem can be solved in polynomial time. We also show that the\nproblem is still as hard as in general when the clique-width is bounded or the\ngraph is strongly ergodic, suggesting that restricting the graph structure does\nnot necessarily help.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 20:36:13 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 17:03:51 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1604.08342", "submitter": "Gramoz Goranci", "authors": "Yun Kuen Cheung, Gramoz Goranci, Monika Henzinger", "title": "Graph Minors for Preserving Terminal Distances Approximately - Lower and\n  Upper Bounds", "comments": "An extended abstract will appear in Proceedings of ICALP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph where vertices are partitioned into $k$ terminals and\nnon-terminals, the goal is to compress the graph (i.e., reduce the number of\nnon-terminals) using minor operations while preserving terminal distances\napproximately.The distortion of a compressed graph is the maximum\nmultiplicative blow-up of distances between all pairs of terminals. We study\nthe trade-off between the number of non-terminals and the distortion. This\nproblem generalizes the Steiner Point Removal (SPR) problem, in which all\nnon-terminals must be removed.\n  We introduce a novel black-box reduction to convert any lower bound on\ndistortion for the SPR problem into a super-linear lower bound on the number of\nnon-terminals, with the same distortion, for our problem. This allows us to\nshow that there exist graphs such that every minor with distortion less than\n$2~/~2.5~/~3$ must have $\\Omega(k^2)~/~\\Omega(k^{5/4})~/~\\Omega(k^{6/5})$\nnon-terminals, plus more trade-offs in between. The black-box reduction has an\ninteresting consequence: if the tight lower bound on distortion for the SPR\nproblem is super-constant, then allowing any $O(k)$ non-terminals will not help\nimproving the lower bound to a constant.\n  We also build on the existing results on spanners, distance oracles and\nconnected 0-extensions to show a number of upper bounds for general graphs,\nplanar graphs, graphs that exclude a fixed minor and bounded treewidth graphs.\nAmong others, we show that any graph admits a minor with $O(\\log k)$ distortion\nand $O(k^{2})$ non-terminals, and any planar graph admits a minor with\n$1+\\varepsilon$ distortion and $\\widetilde{O}((k/\\varepsilon)^{2})$\nnon-terminals.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 08:31:45 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Cheung", "Yun Kuen", ""], ["Goranci", "Gramoz", ""], ["Henzinger", "Monika", ""]]}, {"id": "1604.08448", "submitter": "Shunji Umetani", "authors": "Shunji Umetani", "title": "Exploiting variable associations to configure efficient local search\n  algorithms in large-scale binary integer programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data mining approach for reducing the search space of local\nsearch algorithms in a class of binary integer programs including the set\ncovering and partitioning problems. The quality of locally optimal solutions\ntypically improves if a larger neighborhood is used, while the computation time\nof searching the neighborhood increases exponentially. To overcome this, we\nextract variable associations from the instance to be solved in order to\nidentify promising pairs of flipping variables in the neighborhood search.\nBased on this, we develop a 4-flip neighborhood local search algorithm that\nincorporates an efficient incremental evaluation of solutions and an adaptive\ncontrol of penalty weights. Computational results show that the proposed method\nimproves the performance of the local search algorithm for large-scale set\ncovering and partitioning problems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 15:04:08 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 15:34:54 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Umetani", "Shunji", ""]]}, {"id": "1604.08491", "submitter": "Shay Solomon", "authors": "Shay Solomon", "title": "Fully Dynamic Maximal Matching in Constant Update Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Baswana, Gupta and Sen [FOCS'11] showed that fully dynamic maximal matching\ncan be maintained in general graphs with logarithmic amortized update time.\nMore specifically, starting from an empty graph on $n$ fixed vertices, they\ndevised a randomized algorithm for maintaining maximal matching over any\nsequence of $t$ edge insertions and deletions with a total runtime of $O(t \\log\nn)$ in expectation and $O(t \\log n + n \\log^2 n)$ with high probability.\nWhether or not this runtime bound can be improved towards $O(t)$ has remained\nan important open problem. Despite significant research efforts, this question\nhas resisted numerous attempts at resolution even for basic graph families such\nas forests.\n  In this paper, we resolve the question in the affirmative, by presenting a\nrandomized algorithm for maintaining maximal matching in general graphs with\n\\emph{constant} amortized update time. The optimal runtime bound $O(t)$ of our\nalgorithm holds both in expectation and with high probability.\n  As an immediate corollary, we can maintain 2-approximate vertex cover with\nconstant amortized update time. This result is essentially the best one can\nhope for (under the unique games conjecture) in the context of dynamic\napproximate vertex cover, culminating a long line of research.\n  Our algorithm builds on Baswana et al.'s algorithm, but is inherently\ndifferent and arguably simpler. As an implication of our simplified approach,\nthe space usage of our algorithm is linear in the (dynamic) graph size, while\nthe space usage of Baswana et al.'s algorithm is always at least $\\Omega(n \\log\nn)$.\n  Finally, we present applications to approximate weighted matchings and to\ndistributed networks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 16:15:17 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Solomon", "Shay", ""]]}, {"id": "1604.08507", "submitter": "Tom\\'as Yany Anich", "authors": "Etienne Callies and Tom\\'as Yany-Anich", "title": "Graph Decompositions Analysis and Comparison for Cohesive Subgraphs\n  Detection", "comments": "10 pages, 14 figures, 3 algorithms pseudocodes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive networks have shown that the determination of dense subgraphs, where\nvertices interact a lot, is necessary in order to visualize groups of common\ninterest, and therefore be able to decompose a big graph into smaller\nstructures. Many decompositions have been built over the years as part of\nresearch in the graph mining field, and the topic is becoming a trend in the\nlast decade because of the increasing size of social networks and databases.\nHere, we analyse some of the decompositions methods and also present a novel\none, the Vertex Triangle k-core. We then compare them and test them against\neach other. Moreover, we establish different kind of measures for comparing the\naccuracy of the decomposition methods. We apply these decompositions to real\nworld graphs, like the Collaboration network of arXiv graph, and found some\ninteresting results.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 16:41:29 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Callies", "Etienne", ""], ["Yany-Anich", "Tom\u00e1s", ""]]}, {"id": "1604.08553", "submitter": "Emanuele Natale", "authors": "Michele Borassi, Emanuele Natale", "title": "KADABRA is an ADaptive Algorithm for Betweenness via Random\n  Approximation", "comments": "Some typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present KADABRA, a new algorithm to approximate betweenness centrality in\ndirected and undirected graphs, which significantly outperforms all previous\napproaches on real-world complex networks. The efficiency of the new algorithm\nrelies on two new theoretical contributions, of independent interest. The first\ncontribution focuses on sampling shortest paths, a subroutine used by most\nalgorithms that approximate betweenness centrality. We show that, on realistic\nrandom graph models, we can perform this task in time $|E|^{\\frac{1}{2}+o(1)}$\nwith high probability, obtaining a significant speedup with respect to the\n$\\Theta(|E|)$ worst-case performance. We experimentally show that this new\ntechnique achieves similar speedups on real-world complex networks, as well.\nThe second contribution is a new rigorous application of the adaptive sampling\ntechnique. This approach decreases the total number of shortest paths that need\nto be sampled to compute all betweenness centralities with a given absolute\nerror, and it also handles more general problems, such as computing the $k$\nmost central nodes. Furthermore, our analysis is general, and it might be\nextended to other settings.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 18:46:52 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 07:29:14 GMT"}, {"version": "v3", "created": "Fri, 17 Jun 2016 13:09:03 GMT"}, {"version": "v4", "created": "Fri, 12 Aug 2016 18:15:23 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Borassi", "Michele", ""], ["Natale", "Emanuele", ""]]}, {"id": "1604.08603", "submitter": "Anthony Labarre", "authors": "Laurent Bulteau and Guillaume Fertin and Anthony Labarre and Romeo\n  Rizzi and Irena Rusu", "title": "Decomposing Cubic Graphs into Connected Subgraphs of Size Three", "comments": "to appear in the proceedings of COCOON 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $S=\\{K_{1,3},K_3,P_4\\}$ be the set of connected graphs of size 3. We\nstudy the problem of partitioning the edge set of a graph $G$ into graphs taken\nfrom any non-empty $S'\\subseteq S$. The problem is known to be NP-complete for\nany possible choice of $S'$ in general graphs. In this paper, we assume that\nthe input graph is cubic, and study the computational complexity of the problem\nof partitioning its edge set for any choice of $S'$. We identify all polynomial\nand NP-complete problems in that setting, and give graph-theoretic\ncharacterisations of $S'$-decomposable cubic graphs in some cases.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 20:11:32 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Bulteau", "Laurent", ""], ["Fertin", "Guillaume", ""], ["Labarre", "Anthony", ""], ["Rizzi", "Romeo", ""], ["Rusu", "Irena", ""]]}, {"id": "1604.08636", "submitter": "Priyam Das", "authors": "Priyam Das", "title": "Recursive Modified Pattern Search on High-dimensional Simplex : A\n  Blackbox Optimization Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel derivative-free pattern search based algorithm for\nBlack-box optimization is proposed over a simplex constrained parameter space.\nAt each iteration, starting from the current solution, new possible set of\nsolutions are found by adding a set of derived step-size vectors to the initial\nstarting point. While deriving these step-size vectors, precautions and\nadjustments are considered so that the set of new possible solution points\nstill remain within the simplex constrained space. Thus, no extra time is spent\nin evaluating the (possibly expensive) objective function at infeasible points\n(points outside the unit-simplex space). While minimizing any objective\nfunction of m parameters, within each iteration, the objective function is\nevaluated at 2m new possible solution points. So, upto 2m parallel threads can\nbe incorporated which makes the computation even faster while optimizing\nexpensive objective functions over high-dimensional parameter space. Once a\nlocal minimum is discovered, in order to find a better solution, a novel\n`re-start' strategy is considered to increase the likelihood of finding a\nbetter solution. Unlike existing pattern search based methods, a sparsity\ncontrol parameter is introduced which can be used to induce sparsity in the\nsolution in case the solution is expected to be sparse in prior. A comparative\nstudy of the performances of the proposed algorithm and other existing\nalgorithms are shown for a few low, moderate and high-dimensional optimization\nproblems. Upto 338 folds improvement in computation time is achieved using the\nproposed algorithm over Genetic algorithm along with better solution. The\nproposed algorithm is used to estimate the simultaneous quantiles of North\nAtlantic Hurricane velocities during 1981-2006 by maximizing a non-closed form\nlikelihood function with (possibly) multiple maximums.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 22:20:09 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 00:03:08 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Das", "Priyam", ""]]}, {"id": "1604.08679", "submitter": "Yi Li", "authors": "Yi Li and David P. Woodruff", "title": "On Approximating Functions of the Singular Values in a Stream", "comments": "fixed a flaw in Section 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any real number $p > 0$, we nearly completely characterize the space\ncomplexity of estimating $\\|A\\|_p^p = \\sum_{i=1}^n \\sigma_i^p$ for $n \\times n$\nmatrices $A$ in which each row and each column has $O(1)$ non-zero entries and\nwhose entries are presented one at a time in a data stream model. Here the\n$\\sigma_i$ are the singular values of $A$, and when $p \\geq 1$, $\\|A\\|_p^p$ is\nthe $p$-th power of the Schatten $p$-norm. We show that when $p$ is not an even\ninteger, to obtain a $(1+\\epsilon)$-approximation to $\\|A\\|_p^p$ with constant\nprobability, any $1$-pass algorithm requires $n^{1-g(\\epsilon)}$ bits of space,\nwhere $g(\\epsilon) \\rightarrow 0$ as $\\epsilon \\rightarrow 0$ and $\\epsilon >\n0$ is a constant independent of $n$. However, when $p$ is an even integer, we\ngive an upper bound of $n^{1-2/p} \\textrm{poly}(\\epsilon^{-1}\\log n)$ bits of\nspace, which holds even in the turnstile data stream model. The latter is\noptimal up to $\\textrm{poly}(\\epsilon^{-1} \\log n)$ factors.\n  Our results considerably strengthen lower bounds in previous work for\narbitrary (not necessarily sparse) matrices $A$: the previous best lower bound\nwas $\\Omega(\\log n)$ for $p\\in (0,1)$, $\\Omega(n^{1/p-1/2}/\\log n)$ for $p\\in\n[1,2)$ and $\\Omega(n^{1-2/p})$ for $p\\in (2,\\infty)$. We note for $p \\in (2,\n\\infty)$, while our lower bound for even integers is the same, for other $p$ in\nthis range our lower bound is $n^{1-g(\\epsilon)}$, which is considerably\nstronger than the previous $n^{1-2/p}$ for small enough constant $\\epsilon >\n0$. We obtain similar near-linear lower bounds for Ky-Fan norms, SVD entropy,\neigenvalue shrinkers, and M-estimators, many of which could have been solvable\nin logarithmic space prior to our work.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 03:35:55 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 07:54:35 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Li", "Yi", ""], ["Woodruff", "David P.", ""]]}, {"id": "1604.08738", "submitter": "Manuel Penschuck", "authors": "Michael Hamann, Ulrich Meyer, Manuel Penschuck, Hung Tran and Dorothea\n  Wagner", "title": "I/O-Efficient Generation of Massive Graphs Following the LFR Benchmark", "comments": "25 pages, 11 figures. We add the sampling of simple graphs using the\n  Configuration Model followed by rewiring steps and experimental results\n  regarding the mixing time of the sampling schemes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LFR is a popular benchmark graph generator used to evaluate community\ndetection algorithms. We present EM-LFR, the first external memory algorithm\nable to generate massive complex networks following the LFR benchmark. Its most\nexpensive component is the generation of random graphs with prescribed degree\nsequences which can be divided into two steps: the graphs are first\nmaterialized deterministically using the Havel-Hakimi algorithm, and then\nrandomized. Our main contributions are EM-HH and EM-ES, two I/O-efficient\nexternal memory algorithms for these two steps. We also propose EM-CM/ES, an\nalternative sampling scheme using the Configuration Model and rewiring steps to\nobtain a random simple graph. In an experimental evaluation we demonstrate\ntheir performance; our implementation is able to handle graphs with more than\n37 billion edges on a single machine, is competitive with a massive parallel\ndistributed algorithm, and is faster than a state-of-the-art internal memory\nimplementation even on instances fitting in main memory. EM-LFR's\nimplementation is capable of generating large graph instances orders of\nmagnitude faster than the original implementation. We give evidence that both\nimplementations yield graphs with matching properties by applying clustering\nalgorithms to generated instances. Similarly, we analyse the evolution of graph\nproperties as EM-ES is executed on networks obtained with EM-CM/ES and find\nthat the alternative approach can accelerate the sampling process.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 08:56:11 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 15:45:31 GMT"}, {"version": "v3", "created": "Wed, 14 Jun 2017 09:23:34 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Hamann", "Michael", ""], ["Meyer", "Ulrich", ""], ["Penschuck", "Manuel", ""], ["Tran", "Hung", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1604.08760", "submitter": "Solon Pissis", "authors": "Yannis Almirantis, Panagiotis Charalampopoulos, Jia Gao, Costas S.\n  Iliopoulos, Manal Mohamed, Solon P. Pissis, and Dimitris Polychronopoulos", "title": "Optimal Computation of Avoided Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deviation of the observed frequency of a word $w$ from its expected\nfrequency in a given sequence $x$ is used to determine whether or not the word\nis avoided. This concept is particularly useful in DNA linguistic analysis. The\nvalue of the standard deviation of $w$, denoted by $std(w)$, effectively\ncharacterises the extent of a word by its edge contrast in the context in which\nit occurs. A word $w$ of length $k>2$ is a $\\rho$-avoided word in $x$ if\n$std(w) \\leq \\rho$, for a given threshold $\\rho < 0$. Notice that such a word\nmay be completely absent from $x$. Hence computing all such words na\\\"{\\i}vely\ncan be a very time-consuming procedure, in particular for large $k$. In this\narticle, we propose an $O(n)$-time and $O(n)$-space algorithm to compute all\n$\\rho$-avoided words of length $k$ in a given sequence $x$ of length $n$ over a\nfixed-sized alphabet. We also present a time-optimal $O(\\sigma n)$-time and\n$O(\\sigma n)$-space algorithm to compute all $\\rho$-avoided words (of any\nlength) in a sequence of length $n$ over an alphabet of size $\\sigma$.\nFurthermore, we provide a tight asymptotic upper bound for the number of\n$\\rho$-avoided words and the expected length of the longest one. We make\navailable an open-source implementation of our algorithm. Experimental results,\nusing both real and synthetic data, show the efficiency of our implementation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 10:12:06 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Almirantis", "Yannis", ""], ["Charalampopoulos", "Panagiotis", ""], ["Gao", "Jia", ""], ["Iliopoulos", "Costas S.", ""], ["Mohamed", "Manal", ""], ["Pissis", "Solon P.", ""], ["Polychronopoulos", "Dimitris", ""]]}, {"id": "1604.08764", "submitter": "Ramanujan M. S.", "authors": "Daniel Lokshtanov and M. S. Ramanujan and Saket Saurabh", "title": "A Linear Time Parameterized Algorithm for Node Unique Label Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization version of the Unique Label Cover problem is at the heart of\nthe Unique Games Conjecture which has played an important role in the proof of\nseveral tight inapproximability results. In recent years, this problem has been\nalso studied extensively from the point of view of parameterized complexity.\nCygan et al. [FOCS 2012] proved that this problem is fixed-parameter tractable\n(FPT) and Wahlstr\\\"om [SODA 2014] gave an FPT algorithm with an improved\nparameter dependence. Subsequently, Iwata, Wahlstr\\\"om and Yoshida [2014]\nproved that the edge version of Unique Label Cover can be solved in linear\nFPT-time. That is, there is an FPT algorithm whose dependence on the input-size\nis linear. However, such an algorithm for the node version of the problem was\nleft as an open problem. In this paper, we resolve this question by presenting\nthe first linear-time FPT algorithm for Node Unique Label Cover.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 10:22:11 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1604.08860", "submitter": "Gilles Didier", "authors": "Gilles Didier and Laurent Tichit", "title": "Designing optimal- and fast-on-average pattern matching algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pattern $w$ and a text $t$, the speed of a pattern matching algorithm\nover $t$ with regard to $w$, is the ratio of the length of $t$ to the number of\ntext accesses performed to search $w$ into $t$. We first propose a general\nmethod for computing the limit of the expected speed of pattern matching\nalgorithms, with regard to $w$, over iid texts. Next, we show how to determine\nthe greatest speed which can be achieved among a large class of algorithms,\naltogether with an algorithm running this speed. Since the complexity of this\ndetermination make it impossible to deal with patterns of length greater than\n4, we propose a polynomial heuristic. Finally, our approaches are compared with\n9 pre-existing pattern matching algorithms from both a theoretical and a\npractical point of view, i.e. both in terms of limit expected speed on iid\ntexts, and in terms of observed average speed on real data. In all cases, the\npre-existing algorithms are outperformed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 14:31:49 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 06:33:40 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 13:35:18 GMT"}, {"version": "v4", "created": "Fri, 25 Nov 2016 22:25:36 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Didier", "Gilles", ""], ["Tichit", "Laurent", ""]]}]