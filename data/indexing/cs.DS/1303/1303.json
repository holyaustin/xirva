[{"id": "1303.0035", "submitter": "Michel J. Mizrahi", "authors": "Min Chih Lin, Michel J. Mizrahi and Jayme L. Szwarcfiter", "title": "An $O^*(1.1939^n)$ time algorithm for minimum weighted dominating\n  induced matching", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Say that an edge of a graph $G$ dominates itself and every other edge\nadjacent to it. An edge dominating set of a graph $G=(V,E)$ is a subset of\nedges $E' \\subseteq E$ which dominates all edges of $G$. In particular, if\nevery edge of $G$ is dominated by exactly one edge of $E'$ then $E'$ is a\ndominating induced matching. It is known that not every graph admits a\ndominating induced matching, while the problem to decide if it does admit it is\nNP-complete. In this paper we consider the problems of finding a minimum\nweighted dominating induced matching, if any, and counting the number of\ndominating induced matchings of a graph with weighted edges. We describe an\nexact algorithm for general graphs that runs in $O^*(1.1939^n)$ time and\npolynomial (linear) space. This improves over any existing exact algorithm for\nthe problems in consideration.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 22:16:51 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2013 04:10:44 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Lin", "Min Chih", ""], ["Mizrahi", "Michel J.", ""], ["Szwarcfiter", "Jayme L.", ""]]}, {"id": "1303.0157", "submitter": "Hong-Han Shuai", "authors": "Hong-Han Shuai", "title": "Scalable Cost-Aware Multi-Way Influence Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Viral marketing is different from other marketing strategies since it\nleverages the influence power in intimate relationship, e.g., close friends,\nfamily members, couples. Due to the development and popularity of social\nnetworking services, such as Facebook, Twitter, and Pinterest, the new notion\nof \"social media marketing\" has appeared in recent years and presents new\nopportunities for enabling large-scale and prevalent viral marketing online. To\nboost the growth of their sales, business is embracing social media in a big\nway. According to USA Today, the sales of software to run corporate social\nnetworks will grow 61\\% a year and be a $6.4$ billion business by 2016.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 12:47:07 GMT"}, {"version": "v10", "created": "Mon, 9 Mar 2015 15:09:46 GMT"}, {"version": "v11", "created": "Fri, 26 Jun 2015 07:45:04 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2013 06:26:44 GMT"}, {"version": "v3", "created": "Sun, 23 Jun 2013 02:13:14 GMT"}, {"version": "v4", "created": "Sat, 22 Feb 2014 07:11:55 GMT"}, {"version": "v5", "created": "Fri, 7 Mar 2014 04:09:52 GMT"}, {"version": "v6", "created": "Sat, 2 Aug 2014 05:00:46 GMT"}, {"version": "v7", "created": "Mon, 23 Feb 2015 22:47:47 GMT"}, {"version": "v8", "created": "Sat, 28 Feb 2015 10:55:17 GMT"}, {"version": "v9", "created": "Tue, 3 Mar 2015 10:39:44 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Shuai", "Hong-Han", ""]]}, {"id": "1303.0270", "submitter": "Flavio Coelho", "authors": "Crysttian Arantes Paix\\~ao, Fl\\'avio Code\\c{c}o Coelho", "title": "Computable Compressed Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The biggest cost of computing with large matrices in any modern computer is\nrelated to memory latency and bandwidth. The average latency of modern RAM\nreads is 150 times greater than a clock step of the processor. Throughput is a\nlittle better but still 25 times slower than the CPU can consume. The\napplication of bitstring compression allows for larger matrices to be moved\nentirely to the cache memory of the computer, which has much better latency and\nbandwidth (average latency of L1 cache is 3 to 4 clock steps). This allows for\nmassive performance gains as well as the ability to simulate much larger models\nefficiently. In this work, we propose a methodology to compress matrices in\nsuch a way that they retain their mathematical properties. Considerable\ncompression of the data is also achieved in the process Thus allowing for the\ncomputation of much larger linear problems within the same memory constraints\nwhen compared with the traditional representation of matrices.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 20:34:41 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Paix\u00e3o", "Crysttian Arantes", ""], ["Coelho", "Fl\u00e1vio Code\u00e7o", ""]]}, {"id": "1303.0328", "submitter": "Ernst W. Mayer", "authors": "Ernst W. Mayer", "title": "Efficient long division via Montgomery multiply", "comments": "23 pages; 8 tables v2: Tweak formatting, pagecount -= 2. v3: Fix\n  incorrect powers of R in formulae [7] and [11] v4: Add Eldridge & Walter ref.\n  v5: Clarify relation between Algos A/A',D and Hensel-div; clarify\n  true-quotient mechanics; Add Haswell timings, refs to Agner Fog timings pdf\n  and GMP asm-timings ref-page. v6: Remove stray +bw in MULL line of Algo D\n  listing; add note re byte-LUT for qinv_0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel right-to-left long division algorithm based on the\nMontgomery modular multiply, consisting of separate highly efficient loops with\nsimply carry structure for computing first the remainder (x mod q) and then the\nquotient floor(x/q). These loops are ideally suited for the case where x\noccupies many more machine words than the divide modulus q, and are strictly\nlinear time in the \"bitsize ratio\" lg(x)/lg(q). For the paradigmatic\nperformance test of multiword dividend and single 64-bit-word divisor,\nexploitation of the inherent data-parallelism of the algorithm effectively\nmitigates the long latency of hardware integer MUL operations, as a result of\nwhich we are able to achieve respective costs for remainder-only and full-DIV\n(remainder and quotient) of 6 and 12.5 cycles per dividend word on the Intel\nCore 2 implementation of the x86_64 architecture, in single-threaded execution\nmode. We further describe a simple \"bit-doubling modular inversion\" scheme,\nwhich allows the entire iterative computation of the mod-inverse required by\nthe Montgomery multiply at arbitrarily large precision to be performed with\ncost less than that of a single Newtonian iteration performed at the full\nprecision of the final result. We also show how the Montgomery-multiply-based\npowering can be efficiently used in Mersenne and Fermat-number trial\nfactorization via direct computation of a modular inverse power of 2, without\nany need for explicit radix-mod scalings.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 01:00:44 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 02:04:48 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2013 18:56:28 GMT"}, {"version": "v4", "created": "Mon, 22 Jul 2013 01:03:34 GMT"}, {"version": "v5", "created": "Mon, 2 Dec 2013 00:22:00 GMT"}, {"version": "v6", "created": "Sun, 21 Aug 2016 00:59:13 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Mayer", "Ernst W.", ""]]}, {"id": "1303.0377", "submitter": "Saswata Shannigrahi", "authors": "Gunjan Kumar, Saswata Shannigrahi", "title": "New Online Algorithm for Dynamic Speed Scaling with Sleep State", "comments": "13 pages", "journal-ref": null, "doi": "10.1016/j.tcs.2015.05.045", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an energy-efficient scheduling problem where $n$\njobs $J_1, J_2, ..., J_n$ need to be executed such that the total energy usage\nof these jobs is minimized while ensuring that each job is finished within it's\ndeadline. We work in an online setting where a job is known only at it's\narrival time, along with it's processing volume and deadline. In such a\nsetting, the currently best-known algorithm by Han et al. \\cite{han} provides a\ncompetitive ratio max $\\{4, 2 + {\\alpha}^{\\alpha}\\}$ of energy usage. In this\npaper, we present a new online algorithm SqOA which provides a competitive\nratio max $\\{4, 2 + (2-1/{\\alpha})^\\alpha 2^{\\alpha-1}\\}$ of energy usage. For\n$\\alpha \\geq 3$, the competitive ratio of our algorithm is better than that of\nany other existing algorithms for this problem.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 11:38:17 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Kumar", "Gunjan", ""], ["Shannigrahi", "Saswata", ""]]}, {"id": "1303.0422", "submitter": "Ahmet Erdem Sariy\\\"uce", "authors": "Ahmet Erdem Sariyuce, Kamer Kaya, Erik Saule, Umit V. Catalyurek", "title": "Incremental Algorithms for Network Management and Analysis based on\n  Closeness Centrality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing networks requires complex algorithms to extract meaningful\ninformation. Centrality metrics have shown to be correlated with the importance\nand loads of the nodes in network traffic. Here, we are interested in the\nproblem of centrality-based network management. The problem has many\napplications such as verifying the robustness of the networks and controlling\nor improving the entity dissemination. It can be defined as finding a small set\nof topological network modifications which yield a desired closeness centrality\nconfiguration. As a fundamental building block to tackle that problem, we\npropose incremental algorithms which efficiently update the closeness\ncentrality values upon changes in network topology, i.e., edge insertions and\ndeletions. Our algorithms are proven to be efficient on many real-life\nnetworks, especially on small-world networks, which have a small diameter and a\nspike-shaped shortest distance distribution. In addition to closeness\ncentrality, they can also be a great arsenal for the shortest-path-based\nmanagement and analysis of the networks. We experimentally validate the\nefficiency of our algorithms on large networks and show that they update the\ncloseness centrality values of the temporal DBLP-coauthorship network of 1.2\nmillion users 460 times faster than it would take to compute them from scratch.\nTo the best of our knowledge, this is the first work which can yield practical\nlarge-scale network management based on closeness centrality values.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 19:49:32 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Sariyuce", "Ahmet Erdem", ""], ["Kaya", "Kamer", ""], ["Saule", "Erik", ""], ["Catalyurek", "Umit V.", ""]]}, {"id": "1303.0609", "submitter": "Petteri Kaski", "authors": "Per Austrin and Petteri Kaski and Mikko Koivisto and Jussi\n  M\\\"a\\\"att\\\"a", "title": "Space--Time Tradeoffs for Subset Sum: An Improved Worst Case Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique of Schroeppel and Shamir (SICOMP, 1981) has long been the most\nefficient way to trade space against time for the SUBSET SUM problem. In the\nrandom-instance setting, however, improved tradeoffs exist. In particular, the\nrecently discovered dissection method of Dinur et al. (CRYPTO 2012) yields a\nsignificantly improved space--time tradeoff curve for instances with strong\nrandomness properties. Our main result is that these strong randomness\nassumptions can be removed, obtaining the same space--time tradeoffs in the\nworst case. We also show that for small space usage the dissection algorithm\ncan be almost fully parallelized. Our strategy for dealing with arbitrary\ninstances is to instead inject the randomness into the dissection process\nitself by working over a carefully selected but random composite modulus, and\nto introduce explicit space--time controls into the algorithm by means of a\n\"bailout mechanism\".\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 05:53:09 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Austrin", "Per", ""], ["Kaski", "Petteri", ""], ["Koivisto", "Mikko", ""], ["M\u00e4\u00e4tt\u00e4", "Jussi", ""]]}, {"id": "1303.0726", "submitter": "Devorah Kletenik", "authors": "Amol Deshpande, Lisa Hellerstein, and Devorah Kletenik", "title": "Approximation Algorithms for Stochastic Boolean Function Evaluation and\n  Stochastic Submodular Set Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Boolean Function Evaluation is the problem of determining the\nvalue of a given Boolean function f on an unknown input x, when each bit of x_i\nof x can only be determined by paying an associated cost c_i. The assumption is\nthat x is drawn from a given product distribution, and the goal is to minimize\nthe expected cost. This problem has been studied in Operations Research, where\nit is known as \"sequential testing\" of Boolean functions. It has also been\nstudied in learning theory in the context of learning with attribute costs. We\nconsider the general problem of developing approximation algorithms for\nStochastic Boolean Function Evaluation. We give a 3-approximation algorithm for\nevaluating Boolean linear threshold formulas. We also present an approximation\nalgorithm for evaluating CDNF formulas (and decision trees) achieving a factor\nof O(log kd), where k is the number of terms in the DNF formula, and d is the\nnumber of clauses in the CNF formula. In addition, we present approximation\nalgorithms for simultaneous evaluation of linear threshold functions, and for\nranking of linear functions.\n  Our function evaluation algorithms are based on reductions to the Stochastic\nSubmodular Set Cover (SSSC) problem. This problem was introduced by Golovin and\nKrause. They presented an approximation algorithm for the problem, called\nAdaptive Greedy. Our main technical contribution is a new approximation\nalgorithm for the SSSC problem, which we call Adaptive Dual Greedy. It is an\nextension of the Dual Greedy algorithm for Submodular Set Cover due to Fujito,\nwhich is a generalization of Hochbaum's algorithm for the classical Set Cover\nProblem. We also give a new bound on the approximation achieved by the Adaptive\nGreedy algorithm of Golovin and Krause.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 15:15:36 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2013 16:50:23 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Deshpande", "Amol", ""], ["Hellerstein", "Lisa", ""], ["Kletenik", "Devorah", ""]]}, {"id": "1303.0728", "submitter": "Carola Doerr", "authors": "Carola Doerr, G. Ramakrishna, Jens M. Schmidt", "title": "Computing Minimum Cycle Bases in Weighted Partial 2-Trees in Linear Time", "comments": "major revision of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a linear time algorithm for computing an implicit linear space\nrepresentation of a minimum cycle basis (MCB) in weighted partial 2-trees,\ni.e., graphs of treewidth two. The implicit representation can be made explicit\nin a running time that is proportional to the size of the MCB.\n  Our algorithm improves the result of Borradaile, Sankowski, and Wulff-Nilsen\n[Min $st$-cut Oracle for Planar Graphs with Near-Linear Preprocessing Time,\nFOCS 2010]---which computes for all planar graphs an implicit $O(n \\log n)$\nspace representation of an MCB in $O(n \\log^5 n)$ time---by a polylog factor\nfor the special case of partial 2-trees. Such an improvement was achieved\npreviously only for outerplanar graphs [Liu and Lu: Minimum Cycle Bases of\nWeighted Outerplanar Graphs, IPL 110:970--974, 2010].\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 15:24:22 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 14:14:09 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Doerr", "Carola", ""], ["Ramakrishna", "G.", ""], ["Schmidt", "Jens M.", ""]]}, {"id": "1303.0821", "submitter": "Carola Wenk", "authors": "Jessica Sherette and Carola Wenk", "title": "Simple Curve Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a curve f and a surface S, how hard is it to find a simple curve f' in\nS that is the most similar to f?\n  We introduce and study this simple curve embedding problem for piecewise\nlinear curves and surfaces in R^2 and R^3, under Hausdorff distance, weak\nFrechet distance, and Frechet distance as similarity measures for curves.\nSurprisingly, while several variants of the problem turn out to have\npolynomial-time solutions, we show that in R^3 the simple curve embedding\nproblem is NP-hard under Frechet distance even if S is a plane, as well as\nunder weak Frechet distance if S is a terrain. Additionally, these results give\ninsight into the difficulty of computing the Frechet distance between surfaces,\nand they imply that the partial Frechet distance between non-planar surfaces is\nNP-hard as well.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 20:49:56 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Sherette", "Jessica", ""], ["Wenk", "Carola", ""]]}, {"id": "1303.0868", "submitter": "Jierui Xie", "authors": "Jierui Xie and Boleslaw K. Szymanski", "title": "LabelRank: A Stabilized Label Propagation Algorithm for Community\n  Detection in Networks", "comments": "Proc. IEEE Network Science Workshop, 2013", "journal-ref": "Proc. IEEE Network Science Workshop, West Point, NY, 2013, pp. 138\n  - 143", "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in big data analysis nowadays is detection of cohesive\ngroups in large-scale networks, including social networks, genetic networks,\ncommunication networks and so. In this paper, we propose LabelRank, an\nefficient algorithm detecting communities through label propagation. A set of\noperators is introduced to control and stabilize the propagation dynamics.\nThese operations resolve the randomness issue in traditional label propagation\nalgorithms (LPA), stabilizing the discovered communities in all runs of the\nsame network. Tests on real-world networks demonstrate that LabelRank\nsignificantly improves the quality of detected communities compared to LPA, as\nwell as other popular algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 21:35:53 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2013 15:44:05 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Xie", "Jierui", ""], ["Szymanski", "Boleslaw K.", ""]]}, {"id": "1303.1016", "submitter": "Joshua Wang", "authors": "Joshua Wang", "title": "Space-Efficient Las Vegas Algorithms for K-SUM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using hashing techniques, this paper develops a family of space-efficient Las\nVegas randomized algorithms for $k$-SUM problems. This family includes an\nalgorithm that can solve 3-SUM in $O(n^2)$ time and $O(\\sqrt{n})$ space. It\nalso establishes a new time-space upper bound for SUBSET-SUM, which can be\nsolved by a Las Vegas algorithm in $O^*(2^{(1-\\sqrt{\\8/9\\beta})n})$ time and\n$O^*(2^{\\beta n})$ space, for any $\\beta \\in [0, \\9/32]$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 12:32:10 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2013 23:10:37 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Wang", "Joshua", ""]]}, {"id": "1303.1209", "submitter": "Eric Price", "authors": "Badih Ghazi, Haitham Hassanieh, Piotr Indyk, Dina Katabi, Eric Price,\n  Lixin Shi", "title": "Sample-Optimal Average-Case Sparse Fourier Transform in Two Dimensions", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present the first sample-optimal sublinear time algorithms for the sparse\nDiscrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our\nalgorithms are analyzed for /average case/ signals. For signals whose spectrum\nis exactly sparse, our algorithms use O(k) samples and run in O(k log k) time,\nwhere k is the expected sparsity of the signal. For signals whose spectrum is\napproximately sparse, our algorithm uses O(k log n) samples and runs in O(k\nlog^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of\nsamples used by our algorithms matches the known lower bounds for the\nrespective signal models.\n  By a known reduction, our algorithms give similar results for the\none-dimensional sparse Discrete Fourier Transform when n is a power of a small\ncomposite number (e.g., n = 6^t).\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 22:28:21 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Ghazi", "Badih", ""], ["Hassanieh", "Haitham", ""], ["Indyk", "Piotr", ""], ["Katabi", "Dina", ""], ["Price", "Eric", ""], ["Shi", "Lixin", ""]]}, {"id": "1303.1559", "submitter": "Paolo Giulio Franciosa", "authors": "G. Ausiello, P. G. Franciosa, G. F. Italiano and A. Ribichini", "title": "On Resilient Graph Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and investigate a new notion of resilience in graph spanners.\nLet $S$ be a spanner of a graph $G$. Roughly speaking, we say that a spanner\n$S$ is resilient if all its point-to-point distances are resilient to edge\nfailures. Namely, whenever any edge in $G$ fails, then as a consequence of this\nfailure all distances do not degrade in $S$ substantially more than in $G$\n(i.e., the relative distance increases in $S$ are very close to those in the\nunderlying graph $G$). In this paper we show that sparse resilient spanners\nexist, and that they can be computed efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 22:17:18 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2013 10:20:53 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2013 12:57:48 GMT"}, {"version": "v4", "created": "Thu, 29 May 2014 16:32:34 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Ausiello", "G.", ""], ["Franciosa", "P. G.", ""], ["Italiano", "G. F.", ""], ["Ribichini", "A.", ""]]}, {"id": "1303.1640", "submitter": "Ignaz Rutter", "authors": "Patrizio Angelini, Thomas Bl\\\"asius, Ignaz Rutter", "title": "Testing Mutual Duality of Planar Graphs", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the problem \\mpd, which asks for two planar graphs\n$G_1$ and $G_2$ whether $G_1$ can be embedded such that its dual is isomorphic\nto $G_2$. Our algorithmic main result is an NP-completeness proof for the\ngeneral case and a linear-time algorithm for biconnected graphs. To shed light\nonto the combinatorial structure of the duals of a planar graph, we consider\nthe \\emph{common dual relation} $\\sim$, where $G_1 \\sim G_2$ if and only if\nthey have a common dual. While $\\sim$ is generally not transitive, we show that\nthe restriction to biconnected graphs is an equivalence relation. In this case,\nbeing dual to each other carries over to the equivalence classes, i.e., two\ngraphs are dual to each other if and only if any two elements of their\nrespective equivalence classes are dual to each other.\n  To achieve the efficient testing algorithm for \\mpd on biconnected graphs, we\ndevise a succinct representation of the equivalence class of a biconnected\nplanar graph. It is similar to SPQR-trees and represents exactly the graphs\nthat are contained in the equivalence class. The testing algorithm then works\nby testing in linear time whether two such representations are isomorphic. We\nnote that a special case of \\mpd is testing whether a graph $G$ is self-dual.\nOur algorithm handles the case where $G$ is biconnected and our NP-hardness\nproof extends to testing self-duality of general planar graphs and also to\ntesting map self-duality, where a graph $G$ is map self-dual if it admits a\nplanar embedding $\\mathcal G$ such that $G^\\star$ is isomorphic to $G$, and\nadditionally the embedding induced by $\\mathcal G$ on $G^\\star$ is $\\mathcal\nG$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 10:35:12 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Angelini", "Patrizio", ""], ["Bl\u00e4sius", "Thomas", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1303.1643", "submitter": "R Subashini", "authors": "N.S. Narayanaswamy and R. Subashini", "title": "$d$-COS-R is FPT via Interval Deletion", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A binary matrix $M$ has the Consecutive Ones Property (COP) if there exists a\npermutation of columns that arranges the ones consecutively in all the rows.\nGiven a matrix, the $d$-COS-R problem is to determine if there exists a set of\nat most $d$ rows whose deletion results in a matrix with COP. We consider the\nparameterized complexity of this problem with respect to the number $d$ of rows\nto be deleted as the parameter. The closely related Interval Deletion problem\nhas recently shown to be FPT [Y. Cao and D. Marx, Interval Deletion is\nFixed-Parameter Tractable, arXiv:1211.5933 [cs.DS],2012]. In this work, we\ndescribe a recursive depth-bounded search tree algorithm in which the problems\nat the leaf-level are solved as instances of Interval Deletion. The running\ntime of the algorithm is dominated by the running time of Interval Deletion,\nand therefore we show that $d$-COS-R is fixed-parameter tractable and has a\nrun-time of $O^*(10^d)$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 10:44:42 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Narayanaswamy", "N. S.", ""], ["Subashini", "R.", ""]]}, {"id": "1303.1671", "submitter": "R.Krithika", "authors": "R.Krithika and N.S.Narayanaswamy", "title": "Another Disjoint Compression Algorithm for OCT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an elegant O*(2^k) algorithm for the disjoint compression problem\nfor Odd Cycle Transversal based on a reduction to Above Guarantee Vertex Cover.\nWe believe that this algorithm refines the understanding of the Odd Cycle\nTransversal algorithm by Reed, Smith and Vetta.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 12:58:35 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Krithika", "R.", ""], ["Narayanaswamy", "N. S.", ""]]}, {"id": "1303.1741", "submitter": "Emilio Ferrara", "authors": "Pasquale De Meo, Emilio Ferrara, Giacomo Fiumara, Alessandro Provetti", "title": "Enhancing community detection using a network weighting strategy", "comments": "28 pages, 2 figures", "journal-ref": "Information Sciences, 222:648-668, 2013", "doi": "10.1016/j.ins.2012.08.001", "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A community within a network is a group of vertices densely connected to each\nother but less connected to the vertices outside. The problem of detecting\ncommunities in large networks plays a key role in a wide range of research\nareas, e.g. Computer Science, Biology and Sociology. Most of the existing\nalgorithms to find communities count on the topological features of the network\nand often do not scale well on large, real-life instances.\n  In this article we propose a strategy to enhance existing community detection\nalgorithms by adding a pre-processing step in which edges are weighted\naccording to their centrality w.r.t. the network topology. In our approach, the\ncentrality of an edge reflects its contribute to making arbitrary graph\ntranversals, i.e., spreading messages over the network, as short as possible.\nOur strategy is able to effectively complements information about network\ntopology and it can be used as an additional tool to enhance community\ndetection. The computation of edge centralities is carried out by performing\nmultiple random walks of bounded length on the network. Our method makes the\ncomputation of edge centralities feasible also on large-scale networks. It has\nbeen tested in conjunction with three state-of-the-art community detection\nalgorithms, namely the Louvain method, COPRA and OSLOM. Experimental results\nshow that our method raises the accuracy of existing algorithms both on\nsynthetic and real-life datasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 16:43:30 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["De Meo", "Pasquale", ""], ["Ferrara", "Emilio", ""], ["Fiumara", "Giacomo", ""], ["Provetti", "Alessandro", ""]]}, {"id": "1303.1747", "submitter": "Emilio Ferrara", "authors": "Pasquale De Meo, Emilio Ferrara, Giacomo Fiumara, Angela Ricciardello", "title": "A Novel Measure of Edge Centrality in Social Networks", "comments": "28 pages, 5 figures", "journal-ref": "Knowledge-based Systems, 30:136-150, 2012", "doi": "10.1016/j.knosys.2012.01.007", "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of assigning centrality values to nodes and edges in graphs has\nbeen widely investigated during last years. Recently, a novel measure of node\ncentrality has been proposed, called k-path centrality index, which is based on\nthe propagation of messages inside a network along paths consisting of at most\nk edges. On the other hand, the importance of computing the centrality of edges\nhas been put into evidence since 1970's by Anthonisse and, subsequently by\nGirvan and Newman. In this work we propose the generalization of the concept of\nk-path centrality by defining the k-path edge centrality, a measure of\ncentrality introduced to compute the importance of edges. We provide an\nefficient algorithm, running in O(k m), being m the number of edges in the\ngraph. Thus, our technique is feasible for large scale network analysis.\nFinally, the performance of our algorithm is analyzed, discussing the results\nobtained against large online social network datasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 16:54:34 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["De Meo", "Pasquale", ""], ["Ferrara", "Emilio", ""], ["Fiumara", "Giacomo", ""], ["Ricciardello", "Angela", ""]]}, {"id": "1303.1786", "submitter": "Friedrich Slivovsky", "authors": "Robert Ganian, Friedrich Slivovsky, Stefan Szeider", "title": "Meta-Kernelization with Structural Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-kernelization theorems are general results that provide polynomial\nkernels for large classes of parameterized problems. The known\nmeta-kernelization theorems, in particular the results of Bodlaender et al.\n(FOCS'09) and of Fomin et al. (FOCS'10), apply to optimization problems\nparameterized by solution size. We present the first meta-kernelization\ntheorems that use a structural parameters of the input and not the solution\nsize. Let C be a graph class. We define the C-cover number of a graph to be a\nthe smallest number of modules the vertex set can be partitioned into, such\nthat each module induces a subgraph that belongs to the class C. We show that\neach graph problem that can be expressed in Monadic Second Order (MSO) logic\nhas a polynomial kernel with a linear number of vertices when parameterized by\nthe C-cover number for any fixed class C of bounded rank-width (or\nequivalently, of bounded clique-width, or bounded Boolean width). Many graph\nproblems such as Independent Dominating Set, c-Coloring, and c-Domatic Number\nare covered by this meta-kernelization result. Our second result applies to MSO\nexpressible optimization problems, such as Minimum Vertex Cover, Minimum\nDominating Set, and Maximum Clique. We show that these problems admit a\npolynomial annotated kernel with a linear number of vertices.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 19:24:06 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2013 18:59:26 GMT"}], "update_date": "2013-04-22", "authors_parsed": [["Ganian", "Robert", ""], ["Slivovsky", "Friedrich", ""], ["Szeider", "Stefan", ""]]}, {"id": "1303.1840", "submitter": "Nathan Lindzey C", "authors": "Nathan Lindzey and Ross McConnell", "title": "On Finding Lekkerkerker-Boland Subgraphs", "comments": "Submitted to WG 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lekkerkerker and Boland characterized the minimal forbidden induced subgraphs\nfor the class of interval graphs. We give a linear-time algorithm to find one\nin any graph that is not an interval graph. Tucker characterized the minimal\nforbidden submatrices of matrices that do not have the consecutive-ones\nproperty. We give a linear-time algorithm to find one in any matrix that does\nnot have the consecutive-ones property.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 22:23:26 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Lindzey", "Nathan", ""], ["McConnell", "Ross", ""]]}, {"id": "1303.1849", "submitter": "Alex Gittens", "authors": "Alex Gittens and Michael W. Mahoney", "title": "Revisiting the Nystrom Method for Improved Large-Scale Machine Learning", "comments": "60 pages, 15 color figures; updated proof of Frobenius norm bounds,\n  added comparison to projection-based low-rank approximations, and an analysis\n  of the power method applied to SPSD sketches", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconsider randomized algorithms for the low-rank approximation of\nsymmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel\nmatrices that arise in data analysis and machine learning applications. Our\nmain results consist of an empirical evaluation of the performance quality and\nrunning time of sampling and projection methods on a diverse suite of SPSD\nmatrices. Our results highlight complementary aspects of sampling versus\nprojection methods; they characterize the effects of common data preprocessing\nsteps on the performance of these algorithms; and they point to important\ndifferences between uniform sampling and nonuniform sampling methods based on\nleverage scores. In addition, our empirical results illustrate that existing\ntheory is so weak that it does not provide even a qualitative guide to\npractice. Thus, we complement our empirical results with a suite of worst-case\ntheoretical bounds for both random sampling and random projection methods.\nThese bounds are qualitatively superior to existing bounds---e.g. improved\nadditive-error bounds for spectral and Frobenius norm error and relative-error\nbounds for trace norm error---and they point to future directions to make these\nalgorithms useful in even larger-scale machine learning applications.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 23:16:16 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 20:07:19 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Gittens", "Alex", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1303.1872", "submitter": "Xiaodong Wang", "authors": "Lei Wang, Xiaodong Wang, Yingjie Wu, and Daxin Zhu", "title": "An Efficient Dynamic Programming Algorithm for the Generalized LCS\n  Problem with Multiple Substring Exclusion Constrains", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.7183", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a generalized longest common subsequence problem\nwith multiple substring exclusion constrains. For the two input sequences $X$\nand $Y$ of lengths $n$ and $m$, and a set of $d$ constrains $P=\\{P_1,...,P_d\\}$\nof total length $r$, the problem is to find a common subsequence $Z$ of $X$ and\n$Y$ excluding each of constrain string in $P$ as a substring and the length of\n$Z$ is maximized. The problem was declared to be NP-hard\\cite{1}, but we\nfinally found that this is not true. A new dynamic programming solution for\nthis problem is presented in this paper. The correctness of the new algorithm\nis proved. The time complexity of our algorithm is $O(nmr)$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 02:15:59 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Wang", "Lei", ""], ["Wang", "Xiaodong", ""], ["Wu", "Yingjie", ""], ["Zhu", "Daxin", ""]]}, {"id": "1303.1912", "submitter": "Nicole Megow", "authors": "Nicole Megow and Andreas Wiese", "title": "Competitive-Ratio Approximation Schemes for Minimizing the Makespan in\n  the Online-List Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online scheduling on multiple machines for jobs arriving\none-by-one with the objective of minimizing the makespan. For any number of\nidentical parallel or uniformly related machines, we provide a\ncompetitive-ratio approximation scheme that computes an online algorithm whose\ncompetitive ratio is arbitrarily close to the best possible competitive ratio.\nWe also determine this value up to any desired accuracy. This is the first\napplication of competitive-ratio approximation schemes in the online-list\nmodel. The result proves the applicability of the concept in different online\nmodels. We expect that it fosters further research on other online problems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 08:52:20 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Megow", "Nicole", ""], ["Wiese", "Andreas", ""]]}, {"id": "1303.2033", "submitter": "Vilnis Liepins", "authors": "Vilnis Liepins", "title": "Extended Fourier analysis of signals", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This summary of the doctoral thesis is created to emphasize the close\nconnection of the proposed spectral analysis method with the Discrete Fourier\nTransform (DFT), the most extensively studied and frequently used approach in\nthe history of signal processing. It is shown that in a typical application\ncase, where uniform data readings are transformed to the same number of\nuniformly spaced frequencies, the results of the classical DFT and proposed\napproach coincide. The difference in performance appears when the length of the\nDFT is selected to be greater than the length of the data. The DFT solves the\nunknown data problem by padding readings with zeros up to the length of the\nDFT, while the proposed Extended DFT (EDFT) deals with this situation in a\ndifferent way, it uses the Fourier integral transform as a target and optimizes\nthe transform basis in the extended frequency range without putting such\nrestrictions on the time domain. Consequently, the Inverse DFT (IDFT) applied\nto the result of EDFT returns not only known readings, but also the\nextrapolated data, where classical DFT is able to give back just zeros, and\nhigher resolution are achieved at frequencies where the data has been\nsuccessfully extended. It has been demonstrated that EDFT able to process data\nwith missing readings or gaps inside or even nonuniformly distributed data.\nThus, EDFT significantly extends the usability of the DFT-based methods, where\npreviously these approaches have been considered as not applicable. The EDFT\nfounds the solution in an iterative way and requires repeated calculations to\nget the adaptive basis, and this makes it numerical complexity much higher\ncompared to DFT. This disadvantage was a serious problem in the 1990s, when the\nmethod has been proposed. Fortunately, since then the power of computers has\nincreased so much that nowadays EDFT application could be considered as a real\nalternative.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 15:47:28 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 14:58:43 GMT"}, {"version": "v3", "created": "Fri, 31 May 2013 13:15:18 GMT"}, {"version": "v4", "created": "Tue, 15 Oct 2013 08:47:43 GMT"}, {"version": "v5", "created": "Thu, 23 Jan 2014 08:39:21 GMT"}, {"version": "v6", "created": "Fri, 12 Sep 2014 11:43:40 GMT"}, {"version": "v7", "created": "Mon, 23 Feb 2015 06:29:25 GMT"}, {"version": "v8", "created": "Thu, 28 Sep 2017 07:26:38 GMT"}, {"version": "v9", "created": "Tue, 12 Mar 2019 09:20:17 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Liepins", "Vilnis", ""]]}, {"id": "1303.2285", "submitter": "Oded Green", "authors": "Oded Green, Lior David, Ami Galperin, Yitzhak Birk", "title": "Efficient Parallel Computation of the Estimated Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of a signal's estimated covariance matrix is an important\nbuilding block in signal processing, e.g., for spectral estimation. Each matrix\nelement is a sum of products of elements in the input matrix taken over a\nsliding window. Any given product contributes to multiple output elements,\nthereby complicating parallelization. We present a novel algorithm that attains\nvery high parallelism without repeating multiplications or requiring inter-core\nsynchronization. Key to this is the assignment to each core of distinct\ndiagonal segments of the output matrix, selected such that no multiplications\nneed to be repeated yet only one core writes to any given output-matrix\nelement, and exploitation of a shared memory (including L1 cache) that obviates\nthe need for a corresponding awkward partitioning of the memory among cores.\nImplementation on Plurality's HyperCore shared-memory many-core architecture\ndemonstrates linear speedup of up to 64 cores and speedups of ~85X for 128\ncores. On an x86 system we demonstrate that the new algorithm has consider\nparallel speedups but also show that a sequential implementation of the new\nalgorithm outperforms the parallel implementation of the baseline approach. On\na quad-core x86 system, the new algorithm is 20X faster than sequential\nbaseline and 5X than parallel implementation of the baseline.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 02:48:08 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Green", "Oded", ""], ["David", "Lior", ""], ["Galperin", "Ami", ""], ["Birk", "Yitzhak", ""]]}, {"id": "1303.2416", "submitter": "Alexandra Keenan", "authors": "Alexandra Keenan, Robert Schweller, Michael Sherman, Xingsi Zhong", "title": "Fast Arithmetic in Algorithmic Self-Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the time complexity of computing the sum and\nproduct of two $n$-bit numbers within the tile self-assembly model. The\n(abstract) tile assembly model is a mathematical model of self-assembly in\nwhich system components are square tiles with different glue types assigned to\ntile edges. Assembly is driven by the attachment of singleton tiles to a\ngrowing seed assembly when the net force of glue attraction for a tile exceeds\nsome fixed threshold. Within this frame work, we examine the time complexity of\ncomputing the sum or product of 2 n-bit numbers, where the input numbers are\nencoded in an initial seed assembly, and the output is encoded in the final,\nterminal assembly of the system. We show that the problems of addition and\nmultiplication have worst case lower bounds of $\\Omega(\\sqrt{n})$ in 2D\nassembly, and $\\Omega(\\sqrt[3]{n})$ in 3D assembly. In the case of addition, we\ndesign algorithms for both 2D and 3D that meet this bound with worst case run\ntimes of $O(\\sqrt{n})$ and $O(\\sqrt[3]{n})$ respectively, which beats the\nprevious best known upper bound of O(n). Further, we consider average case\ncomplexity of addition over uniformly distributed n-bit strings and show how to\nachieve $O(\\log n)$ average case time with a simultaneous $O(\\sqrt{n})$ worst\ncase run time in 2D. For multiplication, we present an $O(n^{5/6})$ time\nmultiplication algorithm which works in 3D, which beats the previous best known\nupper bound of O(n). As additional evidence for the speed of our algorithms, we\nimplement our addition algorithms, along with the simpler O(n) time addition\nalgorithm, into a probabilistic run-time simulator and compare the timing\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 03:09:38 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2013 17:43:46 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Keenan", "Alexandra", ""], ["Schweller", "Robert", ""], ["Sherman", "Michael", ""], ["Zhong", "Xingsi", ""]]}, {"id": "1303.2514", "submitter": "Wojciech Wawrzyniak", "authors": "Wojciech Wawrzyniak", "title": "A local constant-factor approximation algorithm for MDS problem in\n  anonymous network", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In research on distributed local algorithms it is commonly assumed that each\nvertex has a unique identifier in the entire graph. However, it turns out that\nin case of certain classes of graphs (for example not lift-closed bounded\ndegree graphs) identifiers are unnecessary and only a port ordering is needed.\nOne of the open issues was whether identifiers are essential in planar graphs.\nIn this paper, we answer this question and we propose an algorithm which\nreturns constant approximation of the MDS problem in CONGEST model. The\nalgorithm doesn't use any additional information about the structure of the\ngraph and the nodes don't have unique identifiers. We hope that this paper will\nbe very helpful as a hint for further comparisons of the unique identifier\nmodel and the model with only a port numbering in other classes of graphs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 13:32:14 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2013 21:41:59 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Wawrzyniak", "Wojciech", ""]]}, {"id": "1303.2546", "submitter": "Nikolay Lavnikevich", "authors": "Nikolay Lavnikevich", "title": "On the Complexity of Maximum Clique Algorithms: usage of coloring\n  heuristics leads to the 2^(n\\5) algorithm running time lower bound", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum Clique Problem(MCP) is one of the 21 original NP--complete problems\nenumerated by Karp in 1972. In recent years a large number of exact methods to\nsolve MCP have been appeared(Babel, Wood, Kumlander, Fahle, Li, Tomita and\netc). Most of them are branch and bound algorithms that use branching rule\nintroduced by Balas and Yu and based on coloring heuristics to establish an\nupper bound on the clique number. They differ from each other primarily in\nvertex preordering and vertex coloring methods. Current methods of worst case\nrunning time analysis for branch and bound algorithms do not allow to provide\ntight upper bounds. This motivates the study of lower bounds for such\nalgorithms. We prove 2^(n\\5) lower bound for group of MCP algorithms based on\nusage of coloring heuristics.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 15:18:47 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Lavnikevich", "Nikolay", ""]]}, {"id": "1303.2730", "submitter": "Luca Trevisan", "authors": "Luca Trevisan", "title": "Is Cheeger-type Approximation Possible for Nonuniform Sparsest Cut?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the {\\em nonuniform sparsest cut} problem, given two undirected graphs $G$\nand $H$ over the same set of vertices $V$, we want to find a cut $(S,V-S)$ that\nminimizes the ratio between the fraction of $G$-edges that are cut and the\nfraction of $H$-edges that are cut. The ratio (which is at most 1 in an optimal\nsolution) is called the {\\em sparsity} of the cut. In the {\\em uniform sparsest\ncut} problem, $H$ is a clique over $V$. If $G$ is regular, it is possible to\nfind a solution to the uniform sparsest cut of cost $O(\\sqrt{opt})$ in nearly\nlinear time. Is such an approximation, which we call \"Cheege-type\"\napproximation, achievable in the non-uniform case?\n  We show that the answer is negative, assuming the Unique Games Conjecture,\nfor general H. Furthermore, the Leighton-Rao linear programming relaxation and\nthe spectral relaxation fail to find such an approximation even if $H$ is a\nclique over a subset of vertices. Using semidefinite programming, however, we\ncan find Cheeger-type approximations in polynomial time whenever the adjacency\nmatrix of $H$ has rank 1. (This includes the cases in which $H$ is a clique\nover a subset of vertices.)\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 00:16:54 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Trevisan", "Luca", ""]]}, {"id": "1303.2772", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Further analysis of the binary Euclidean algorithm", "comments": "An old Technical Report which no longer seems to be available from\n  the Oxford University website", "journal-ref": "Technical Report PRG TR-7-99, Oxford University Computing\n  Laboratory, November 1999, 18 pp", "doi": null, "report-no": "PRG TR-7-99", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary Euclidean algorithm is a variant of the classical Euclidean\nalgorithm. It avoids multiplications and divisions, except by powers of two, so\nis potentially faster than the classical algorithm on a binary machine. We\ndescribe the binary algorithm and consider its average case behaviour. In\nparticular, we correct some errors in the literature, discuss some results of\nVall\\'ee, and describe a numerical computation which supports a conjecture of\nVall\\'ee.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 03:57:58 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1303.2860", "submitter": "Moritz M\\\"uhlenthaler", "authors": "Moritz M\\\"uhlenthaler, Rolf Wanka", "title": "Fairness in Academic Course Timetabling", "comments": "appeared in PATAT 2012, pp. 114-130", "journal-ref": null, "doi": "10.1007/s10479-014-1553-2", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of creating fair course timetables in the setting of\na university. Our motivation is to improve the overall satisfaction of\nindividuals concerned (students, teachers, etc.) by providing a fair timetable\nto them. The central idea is that undesirable arrangements in the course\ntimetable, i.e., violations of soft constraints, should be distributed in a\nfair way among the individuals. We propose two formulations for the fair course\ntimetabling problem that are based on max-min fairness and Jain's fairness\nindex, respectively. Furthermore, we present and experimentally evaluate an\noptimization algorithm based on simulated annealing for solving max-min fair\ncourse timetabling problems. The new contribution is concerned with measuring\nthe energy difference between two timetables, i.e., how much worse a timetable\nis compared to another timetable with respect to max-min fairness. We introduce\nthree different energy difference measures and evaluate their impact on the\noverall algorithm performance. The second proposed problem formulation focuses\non the tradeoff between fairness and the total amount of soft constraint\nviolations. Our experimental evaluation shows that the known best solutions to\nthe ITC2007 curriculum-based course timetabling instances are quite fair with\nrespect to Jain's fairness index. However, the experiments also show that the\nfairness can be improved further for only a rather small increase in the total\namount of soft constraint violations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 12:46:54 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["M\u00fchlenthaler", "Moritz", ""], ["Wanka", "Rolf", ""]]}, {"id": "1303.2920", "submitter": "Neal E. Young", "authors": "Neal E. Young", "title": "Approximating 1-dimensional TSP Requires Omega(n log n) Comparisons", "comments": "Superseded by \"On the complexity of approximating Euclidean traveling\n  salesman tours and minimum spanning trees\", by Das et al; Algorithmica\n  19:447-460 (1997)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a short proof that any comparison-based n^(1-epsilon)-approximation\nalgorithm for the 1-dimensional Traveling Salesman Problem (TSP) requires\nOmega(n log n) comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 15:38:48 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2013 23:29:25 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Young", "Neal E.", ""]]}, {"id": "1303.2963", "submitter": "Tobias M\\\"omke", "authors": "Tobias M\\\"omke", "title": "A Competitive Ratio Approximation Scheme for the k-Server Problem in\n  Fixed Finite Metrics", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to restrict the analysis of a class of online problems that\nincludes the $k$-server problem in finite metrics such that we only have to\nconsider finite sequences of request. When applying the restrictions, both the\noptimal offline solutions and the best possible deterministic or randomized\nonline solutions only differ by at most an arbitrarily small constant factor\nfrom the corresponding solutions without restrictions. Furthermore, we show how\nto obtain an algorithm with best possible deterministic or randomized\ncompetitive ratio for the restricted setup. Thus, for each fixed finite metrics\nour result qualifies as a competitive ratio approximation scheme as defined by\nG\\\"unther et al.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 17:38:35 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["M\u00f6mke", "Tobias", ""]]}, {"id": "1303.2981", "submitter": "Ventsislav Chonev", "authors": "Ventsislav Chonev, Jo\\\"el Ouaknine and James Worrell", "title": "On the Complexity of the Orbit Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider higher-dimensional versions of Kannan and Lipton's Orbit\nProblem---determining whether a target vector space V may be reached from a\nstarting point x under repeated applications of a linear transformation A.\nAnswering two questions posed by Kannan and Lipton in the 1980s, we show that\nwhen V has dimension one, this problem is solvable in polynomial time, and when\nV has dimension two or three, the problem is in NP^{RP}.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 18:35:29 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 18:26:18 GMT"}, {"version": "v3", "created": "Wed, 22 Jun 2016 12:16:33 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Chonev", "Ventsislav", ""], ["Ouaknine", "Jo\u00ebl", ""], ["Worrell", "James", ""]]}, {"id": "1303.3018", "submitter": "Zhenliang Zhang", "authors": "Zhenliang Zhang, Edwin K. P. Chong, Ali Pezeshki, and William Moran", "title": "String Submodular Functions with Curvature Constraints", "comments": "to appear in IEEE Transaction on Automatic Control", "journal-ref": null, "doi": "10.1109/TAC.2015.2440566", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of objectively choosing a string of actions to optimize an\nobjective function that is string submodular has been considered in [1]. There\nit is shown that the greedy strategy, consisting of a string of actions that\nonly locally maximizes the step-wise gain in the objective function achieves at\nleast a (1-e^{-1})-approximation to the optimal strategy. This paper improves\nthis approximation by introducing additional constraints on curvatures, namely,\ntotal backward curvature, total forward curvature, and elemental forward\ncurvature. We show that if the objective function has total backward curvature\n\\sigma, then the greedy strategy achieves at least a\n\\frac{1}{\\sigma}(1-e^{-\\sigma})-approximation of the optimal strategy. If the\nobjective function has total forward curvature \\epsilon, then the greedy\nstrategy achieves at least a (1-\\epsilon)-approximation of the optimal\nstrategy. Moreover, we consider a generalization of the diminishing-return\nproperty by defining the elemental forward curvature. We also consider the\nproblem of maximizing the objective function subject to general a\nstring-matroid constraint. We investigate an applications of string submodular\nfunctions with curvature constraints.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 20:29:21 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2013 23:25:37 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 20:20:57 GMT"}, {"version": "v4", "created": "Mon, 25 May 2015 18:46:14 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhang", "Zhenliang", ""], ["Chong", "Edwin K. P.", ""], ["Pezeshki", "Ali", ""], ["Moran", "William", ""]]}, {"id": "1303.3319", "submitter": "Anhui Tan", "authors": "Anhui Tan", "title": "A new type of judgement theorems for attribute characters in information\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research of attribute characters in information system which contains\ncore, necessary, unnecessary is a basic and important issue in attribute\nreduct. Many methods for the judgement of attribute characters are based on the\nrelationship between the objects and attributes. In this paper, a new type of\njudgement theorems which are absolutely based on the relationship among\nattributes is proposed for the judgement of attribute characters. The method is\nthrough comparing the two new attribute sets $E(a)$ and $N(a)$ with respect to\nthe designated attribute $a$ which is proposed in this paper. We conclude that\nwhich type of the attribute $a$ belongs to is determined by the relationship\nbetween $E(a)$ and $N(a)$ in essence. Secondly, more concise and clear results\nare given about the judgment of the attribute characters through analyzing the\nproperties of refinement and precise-refinement between $E(a)$ and $N(a)$ in\ntopology. In addition, the relationship among attributes are discussed which is\nuseful for constructing a reduct in the last section of this paper. In the\nlast, we propose a reduct algorithm based on $E(a)$, and this algorithm is an\nextended application of the analysis of attribute characters above.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 00:35:54 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Tan", "Anhui", ""]]}, {"id": "1303.3386", "submitter": "Noa Avigdor-Elgrabli", "authors": "Noa Avigdor-elgrabli, Yuval Rabani", "title": "An Optimal Randomized Online Algorithm for Reordering Buffer Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $O(\\log\\log k)$-competitive randomized online algorithm for\nreordering buffer management, where $k$ is the buffer size. Our bound matches\nthe lower bound of Adamaszek et al. (STOC 2011). Our algorithm has two stages\nwhich are executed online in parallel. The first stage computes\ndeterministically a feasible fractional solution to an LP relaxation for\nreordering buffer management. The second stage \"rounds\" using randomness the\nfractional solution. The first stage is based on the online primal-dual schema,\ncombined with a dual fitting argument. As multiplicative weights steps and dual\nfitting steps are interleaved and in some sense conflicting, combining them is\nchallenging. We also note that we apply the primal-dual schema to a relaxation\nwith mixed packing and covering constraints. We pay the $O(\\log\\log k)$\ncompetitive factor for the gap between the computed LP solution and the optimal\nLP solution. The second stage gives an online algorithm that converts the LP\nsolution to an integral solution, while increasing the cost by an O(1) factor.\nThis stage generalizes recent results that gave a similar approximation factor\nfor rounding the LP solution, albeit using an offline rounding algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 09:47:49 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Avigdor-elgrabli", "Noa", ""], ["Rabani", "Yuval", ""]]}, {"id": "1303.3422", "submitter": "Christophe Osswald", "authors": "Christophe Osswald (STIC, Lab-STICC)", "title": "Controling the number of focal elements", "comments": "Belief 2012, Compi\\`egne : France (2012)", "journal-ref": null, "doi": "10.1007/978-3-642-29461-7", "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic belief assignment can have up to 2^n focal elements, and combining\nthem with a simple conjunctive operator will need O(2^2n) operations. This\narticle proposes some techniques to limit the size of the focal sets of the\nbbas to be combined while preserving a large part of the information they\ncarry. The first section revisits some well-known definitions with an\nalgorithmic point of vue. The second section proposes a matrix way of building\nthe least committed isopignistic, and extends it to some other bodies of\nevidence. The third section adapts the k-means algorithm for an unsupervized\nclustering of the focal elements of a given bba.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 12:11:07 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Osswald", "Christophe", "", "STIC, Lab-STICC"]]}, {"id": "1303.3445", "submitter": "Mourad Gouicem", "authors": "Mourad Gouicem", "title": "New modular multiplication and division algorithms based on continued\n  fraction expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply results on number systems based on continued fraction\nexpansions to modular arithmetic. We provide two new algorithms in order to\ncompute modular multiplication and modular division. The presented algorithms\nare based on the Euclidean algorithm and are of quadratic complexity.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 13:44:44 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Gouicem", "Mourad", ""]]}, {"id": "1303.3564", "submitter": "Elaheh Fata", "authors": "Elaheh Fata, Stephen L. Smith and Shreyas Sundaram", "title": "Distributed Dominating Sets on Grids", "comments": "10 pages, 9 figures, accepted in ACC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a distributed algorithm for finding near optimal\ndominating sets on grids. The basis for this algorithm is an existing\ncentralized algorithm that constructs dominating sets on grids. The size of the\ndominating set provided by this centralized algorithm is upper-bounded by\n$\\lceil\\frac{(m+2)(n+2)}{5}\\rceil$ for $m\\times n$ grids and its difference\nfrom the optimal domination number of the grid is upper-bounded by five. Both\nthe centralized and distributed algorithms are generalized for the $k$-distance\ndominating set problem, where all grid vertices are within distance $k$ of the\nvertices in the dominating set.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 19:33:26 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Fata", "Elaheh", ""], ["Smith", "Stephen L.", ""], ["Sundaram", "Shreyas", ""]]}, {"id": "1303.3660", "submitter": "Prithwish Basu", "authors": "Philippe Nain, Don Towsley, Matthew P. Johnson, Prithwish Basu, Amotz\n  Bar-Noy, and Feng Yu", "title": "Computing Traversal Times on Dynamic Markovian Paths", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In source routing, a complete path is chosen for a packet to travel from\nsource to destination. While computing the time to traverse such a path may be\nstraightforward in a fixed, static graph, doing so becomes much more\nchallenging in dynamic graphs, in which the state of an edge in one time slot\n(i.e., its presence or absence) is random, and may depend on its state in the\nprevious time step. The traversal time is due to both time spent waiting for\nedges to appear and time spent crossing them once they become available. We\ncompute the expected traversal time (ETT) for a dynamic path in a number of\nspecial cases of stochastic edge dynamics models, and for three edge failure\nmodels, culminating in a surprisingly challenging yet realistic setting in\nwhich the initial configuration of edge states for the entire path is known. We\nshow that the ETT for this \"initial configuration\" setting can be computed in\nquadratic time, by an algorithm based on probability generating functions. We\nalso give several linear-time upper and lower bounds on the ETT.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 02:10:06 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Nain", "Philippe", ""], ["Towsley", "Don", ""], ["Johnson", "Matthew P.", ""], ["Basu", "Prithwish", ""], ["Bar-Noy", "Amotz", ""], ["Yu", "Feng", ""]]}, {"id": "1303.3692", "submitter": "Gang Liao", "authors": "Gang Liao, Qi Sun, Longfei Ma, Sha Ding and Wen Xie", "title": "Ultra-fast Multiple Genome Sequence Matching Using GPU", "comments": "The 2013 International Conference on High Performance Computing &\n  Simulation (ACM/IEEE HPCS 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a contrastive evaluation of massively parallel implementations\nof suffix tree and suffix array to accelerate genome sequence matching are\nproposed based on Intel Core i7 3770K quad-core and NVIDIA GeForce GTX680 GPU.\nBesides suffix array only held approximately 20%~30% of the space relative to\nsuffix tree, the coalesced binary search and tile optimization make suffix\narray clearly outperform suffix tree using GPU. Consequently, the experimental\nresults show that multiple genome sequence matching based on suffix array is\nmore than 99 times speedup than that of CPU serial implementation. There is no\ndoubt that massively parallel matching algorithm based on suffix array is an\nefficient approach to high-performance bioinformatics applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 07:00:14 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 03:38:17 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 19:52:18 GMT"}, {"version": "v4", "created": "Sun, 3 May 2015 20:03:29 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Liao", "Gang", ""], ["Sun", "Qi", ""], ["Ma", "Longfei", ""], ["Ding", "Sha", ""], ["Xie", "Wen", ""]]}, {"id": "1303.3777", "submitter": "Andrey Grinshpun", "authors": "A. Grinshpun, P. Phalitnonkiat, S. Rubin, A. Tarfulea", "title": "Alternating Traps in Muller and Parity Games", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Muller games are played by two players moving a token along a graph; the\nwinner is determined by the set of vertices that occur infinitely often. The\ncentral algorithmic problem is to compute the winning regions for the players.\nDifferent classes and representations of Muller games lead to problems of\nvarying computational complexity. One such class are parity games; these are of\nparticular significance in computational complexity, as they remain one of the\nfew combinatorial problems known to be in NP and co-NP but not known to be in\nP. We show that winning regions for a Muller game can be determined from the\nalternating structure of its traps. To every Muller game we then associate a\nnatural number that we call its trap-depth; this parameter measures how\ncomplicated the trap structure is. We present algorithms for parity games that\nrun in polynomial time for graphs of bounded trap depth, and in general run in\ntime exponential in the trap depth.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 18:11:16 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 07:32:01 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Grinshpun", "A.", ""], ["Phalitnonkiat", "P.", ""], ["Rubin", "S.", ""], ["Tarfulea", "A.", ""]]}, {"id": "1303.3931", "submitter": "Rob Gysel", "authors": "Rob Gysel", "title": "Potential Maximal Clique Algorithms for Perfect Phylogeny Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kloks, Kratsch, and Spinrad showed how treewidth and minimum-fill, NP-hard\ncombinatorial optimization problems related to minimal triangulations, are\nbroken into subproblems by block subgraphs defined by minimal separators. These\nideas were expanded on by Bouchitt\\'e and Todinca, who used potential maximal\ncliques to solve these problems using a dynamic programming approach in time\npolynomial in the number of minimal separators of a graph. It is known that\nsolutions to the perfect phylogeny problem, maximum compatibility problem, and\nunique perfect phylogeny problem are characterized by minimal triangulations of\nthe partition intersection graph. In this paper, we show that techniques\nsimilar to those proposed by Bouchitt\\'e and Todinca can be used to solve the\nperfect phylogeny problem with missing data, the two- state maximum\ncompatibility problem with missing data, and the unique perfect phylogeny\nproblem with missing data in time polynomial in the number of minimal\nseparators of the partition intersection graph.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 00:16:47 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Gysel", "Rob", ""]]}, {"id": "1303.3945", "submitter": "Hideo Bannai", "authors": "Toshiya Tanaka, Tomohiro I, Shunsuke Inenaga, Hideo Bannai, Masayuki\n  Takeda", "title": "Computing convolution on grammar-compressed text", "comments": "DCC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolution between a text string $S$ of length $N$ and a pattern string\n$P$ of length $m$ can be computed in $O(N \\log m)$ time by FFT. It is known\nthat various types of approximate string matching problems are reducible to\nconvolution. In this paper, we assume that the input text string is given in a\ncompressed form, as a \\emph{straight-line program (SLP)}, which is a context\nfree grammar in the Chomsky normal form that derives a single string. Given an\nSLP $\\mathcal{S}$ of size $n$ describing a text $S$ of length $N$, and an\nuncompressed pattern $P$ of length $m$, we present a simple $O(nm \\log m)$-time\nalgorithm to compute the convolution between $S$ and $P$. We then show that\nthis can be improved to $O(\\min\\{nm, N-\\alpha\\} \\log m)$ time, where $\\alpha\n\\geq 0$ is a value that represents the amount of redundancy that the SLP\ncaptures with respect to the length-$m$ substrings. The key of the improvement\nis our new algorithm that computes the convolution between a trie of size $r$\nand a pattern string $P$ of length $m$ in $O(r \\log m)$ time.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 05:07:24 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Tanaka", "Toshiya", ""], ["I", "Tomohiro", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1303.4031", "submitter": "Fatemeh Rajabi-Alni", "authors": "Fatemeh Rajabi-Alni", "title": "Two exact algorithms for the generalized assignment problem", "comments": "13 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1302.4426", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let A={a_1,a_2,...,a_s} and B={b_1,b_2,...,b_t} be two sets of objects with\ns+r=n, the generalized assignment problem assigns each element a_i in A to at\nleast alpha_i and at most alpha '_i elements in B, and each element b_j in B to\nat least beta_j and at most beta '_j elements in A for all 1 <= i <= s and 1 <=\nj <= t. In this paper, we present an O(n^4) time and O(n) space algorithm for\nthis problem using the well known Hungarian algorithm. We also present an\nO(n^3) algorithm for a special case of the generalized assignment, called the\nlimited-capacity assignment problem, where alpha_i,beta_j=1 for all i,j.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2013 02:57:16 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Rajabi-Alni", "Fatemeh", ""]]}, {"id": "1303.4177", "submitter": "Igor Sergeev", "authors": "Igor S. Sergeev", "title": "A relation between additive and multiplicative complexity of Boolean\n  functions", "comments": "4 pages, in English; 4 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present note we prove an asymptotically tight relation between\nadditive and multiplicative complexity of Boolean functions with respect to\nimplementation by circuits over the basis {+,*,1}.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 08:02:55 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Sergeev", "Igor S.", ""]]}, {"id": "1303.4244", "submitter": "Vytas Zacharovas", "authors": "Michael Fuchs, Hsien-Kuei Hwang and Vytas Zacharovas", "title": "An analytic approach to the asymptotic variance of trie statistics and\n  related structures", "comments": "51 pages, the expressions of all Fourier coefficients are largely\n  simplified in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop analytic tools for the asymptotics of general trie statistics,\nwhich are particularly advantageous for clarifying the asymptotic variance.\nMany concrete examples are discussed for which new Fourier expansions are\ngiven. The tools are also useful for other splitting processes with an\nunderlying binomial distribution. We specially highlight Philippe Flajolet's\ncontribution in the analysis of these random structures.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 13:23:33 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 22:06:56 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2013 21:09:26 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Fuchs", "Michael", ""], ["Hwang", "Hsien-Kuei", ""], ["Zacharovas", "Vytas", ""]]}, {"id": "1303.4324", "submitter": "Xavier Labouze", "authors": "Xavier Labouze", "title": "About Inverse 3-SAT", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Inverse 3-SAT problem is known to be coNP Complete. This article shows a\nnew interesting way to solve directly the problem by using closure under\nresolution and partial assignment properties. An algorithm is proposed which\nlets solve the (co)Inverse 3-SAT problem.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 17:23:58 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2013 22:00:22 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2013 17:18:57 GMT"}, {"version": "v4", "created": "Mon, 26 Aug 2013 13:15:45 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Labouze", "Xavier", ""]]}, {"id": "1303.4349", "submitter": "Roland Glantz", "authors": "Roland Glantz and Henning Meyerhenke", "title": "Finding all Convex Cuts of a Plane Graph in Polynomial Time", "comments": "23 pages. Submitted to Journal of Discrete Algorithms (JDA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convexity is a notion that has been defined for subsets of $\\RR^n$ and for\nsubsets of general graphs. A convex cut of a graph $G=(V, E)$ is a\n$2$-partition $V_1 \\dot{\\cup} V_2=V$ such that both $V_1$ and $V_2$ are convex,\n\\ie shortest paths between vertices in $V_i$ never leave $V_i$, $i \\in \\{1,\n2\\}$. Finding convex cuts is $\\mathcal{NP}$-hard for general graphs. To\ncharacterize convex cuts, we employ the Djokovic relation, a reflexive and\nsymmetric relation on the edges of a graph that is based on shortest paths\nbetween the edges' end vertices.\n  It is known for a long time that, if $G$ is bipartite and the Djokovic\nrelation is transitive on $G$, \\ie $G$ is a partial cube, then the cut-sets of\n$G$'s convex cuts are precisely the equivalence classes of the Djokovic\nrelation. In particular, any edge of $G$ is contained in the cut-set of exactly\none convex cut. We first characterize a class of plane graphs that we call {\\em\nwell-arranged}. These graphs are not necessarily partial cubes, but any edge of\na well-arranged graph is contained in the cut-set(s) of at least one convex\ncut. We also present an algorithm that uses the Djokovic relation for computing\nall convex cuts of a (not necessarily plane) bipartite graph in $\\bigO(|E|^3)$\ntime. Specifically, a cut-set is the cut-set of a convex cut if and only if the\nDjokovic relation holds for any pair of edges in the cut-set.\n  We then characterize the cut-sets of the convex cuts of a general graph $H$\nusing two binary relations on edges: (i) the Djokovic relation on the edges of\na subdivision of $H$, where any edge of $H$ is subdivided into exactly two\nedges and (ii) a relation on the edges of $H$ itself that is not the Djokovic\nrelation. Finally, we use this characterization to present the first algorithm\nfor finding all convex cuts of a plane graph in polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 18:21:45 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 10:15:17 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2014 11:14:28 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Glantz", "Roland", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1303.4376", "submitter": "Dominique Rossin", "authors": "Adeline Pierrot (LIAFA), Dominique Rossin (LIX)", "title": "2-stack pushall sortable permutations", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 60's, Knuth introduced stack-sorting and serial compositions of\nstacks. In particular, one significant question arise out of the work of Knuth:\nhow to decide efficiently if a given permutation is sortable with 2 stacks in\nseries? Whether this problem is polynomial or NP-complete is still unanswered\nyet. In this article we introduce 2-stack pushall permutations which form a\nsubclass of 2-stack sortable permutations and show that these two classes are\nclosely related. Moreover, we give an optimal O(n^2) algorithm to decide if a\ngiven permutation of size n is 2-stack pushall sortable and describe all its\nsortings. This result is a step to the solve the general 2-stack sorting\nproblem in polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 19:44:45 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Pierrot", "Adeline", "", "LIAFA"], ["Rossin", "Dominique", "", "LIX"]]}, {"id": "1303.4632", "submitter": "Paulo Shakarian", "authors": "Paulo Shakarian and V.S. Subrahmanian", "title": "Geospatial Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  There are numerous applications which require the ability to take certain\nactions (e.g. distribute money, medicines, people etc.) over a geographic\nregion. A disaster relief organization must allocate people and supplies to\nparts of a region after a disaster. A public health organization must allocate\nlimited vaccine to people across a region. In both cases, the organization is\ntrying to optimize something (e.g. minimize expected number of people with a\ndisease). We introduce \"geospatial optimization problems\" (GOPs) where an\norganization has limited resources and budget to take actions in a geographic\narea. The actions result in one or more properties changing for one or more\nlocations. There are also certain constraints on the combinations of actions\nthat can be taken. We study two types of GOPs - goal-based and\nbenefit-maximizing (GBGOP and BMGOP respectively). A GBGOP ensures that certain\nproperties must be true at specified locations after the actions are taken\nwhile a BMGOP optimizes a linear benefit function. We show both problems to be\nNP-hard (with membership in NP for the associated decision problems).\nAdditionally, we prove limits on approximation for both problems. We present\ninteger programs for both GOPs that provide exact solutions. We also correctly\nreduce the number of variables in for the GBGOP integer constraints. For BMGOP,\nwe present the BMGOP-Compute algorithm that runs in PTIME and provides a\nreasonable approximation guarantee in most cases.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2013 15:02:55 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Shakarian", "Paulo", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "1303.4897", "submitter": "Guyslain Naves", "authors": "Chandra Chekuri and Guyslain Naves and F. Bruce Shepherd", "title": "Maximum Edge-Disjoint Paths in $k$-sums of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the approximability of the maximum edge-disjoint paths problem\n(MEDP) in undirected graphs, and in particular, the integrality gap of the\nnatural multicommodity flow based relaxation for it. The integrality gap is\nknown to be $\\Omega(\\sqrt{n})$ even for planar graphs due to a simple\ntopological obstruction and a major focus, following earlier work, has been\nunderstanding the gap if some constant congestion is allowed.\n  In this context, it is natural to ask for which classes of graphs does a\nconstant-factor constant-congestion property hold. It is easy to deduce that\nfor given constant bounds on the approximation and congestion, the class of\n\"nice\" graphs is nor-closed. Is the converse true? Does every proper\nminor-closed family of graphs exhibit a constant factor, constant congestion\nbound relative to the LP relaxation? We conjecture that the answer is yes.\n  One stumbling block has been that such bounds were not known for bounded\ntreewidth graphs (or even treewidth 3). In this paper we give a polytime\nalgorithm which takes a fractional routing solution in a graph of bounded\ntreewidth and is able to integrally route a constant fraction of the LP\nsolution's value. Note that we do not incur any edge congestion. Previously\nthis was not known even for series parallel graphs which have treewidth 2. The\nalgorithm is based on a more general argument that applies to $k$-sums of\ngraphs in some graph family, as long as the graph family has a constant factor,\nconstant congestion bound. We then use this to show that such bounds hold for\nthe class of $k$-sums of bounded genus graphs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 10:30:17 GMT"}], "update_date": "2013-03-21", "authors_parsed": [["Chekuri", "Chandra", ""], ["Naves", "Guyslain", ""], ["Shepherd", "F. Bruce", ""]]}, {"id": "1303.5197", "submitter": "Yoann Isaac", "authors": "Yoann Isaac, Quentin Barth\\'elemy, Jamal Atif, C\\'edric Gouy-Pailler,\n  Mich\\`ele Sebag", "title": "Multi-dimensional sparse structured signal approximation using split\n  Bregman iterations", "comments": "5 pages, ICASSP 2013 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on the sparse approximation of signals using overcomplete\nrepresentations, such that it preserves the (prior) structure of\nmulti-dimensional signals. The underlying optimization problem is tackled using\na multi-dimensional split Bregman optimization approach. An extensive empirical\nevaluation shows how the proposed approach compares to the state of the art\ndepending on the signal features.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 09:13:23 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2013 14:45:23 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 12:54:19 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Isaac", "Yoann", ""], ["Barth\u00e9lemy", "Quentin", ""], ["Atif", "Jamal", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Sebag", "Mich\u00e8le", ""]]}, {"id": "1303.5217", "submitter": "Martin  Aum\\\"uller", "authors": "Martin Aum\\\"uller and Martin Dietzfelbinger", "title": "Optimal Partitioning for Dual-Pivot Quicksort", "comments": "Accepted for publication in ACM Transactions on Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual-pivot quicksort refers to variants of classical quicksort where in the\npartitioning step two pivots are used to split the input into three segments.\nThis can be done in different ways, giving rise to different algorithms.\nRecently, a dual-pivot algorithm proposed by Yaroslavskiy received much\nattention, because a variant of it replaced the well-engineered quicksort\nalgorithm in Sun's Java 7 runtime library. Nebel and Wild (ESA 2012) analyzed\nthis algorithm and showed that on average it uses 1.9n ln n + O(n) comparisons\nto sort an input of size n, beating standard quicksort, which uses 2n ln n +\nO(n) comparisons. We introduce a model that captures all dual-pivot algorithms,\ngive a unified analysis, and identify new dual-pivot algorithms that minimize\nthe average number of key comparisons among all possible algorithms up to a\nlinear term. This minimum is 1.8n ln n + O(n). For the case that the pivots are\nchosen from a small sample, we include a comparison of dual-pivot quicksort and\nclassical quicksort. Specifically, we show that dual-pivot quicksort benefits\nfrom a skewed choice of pivots. We experimentally evaluate our algorithms and\ncompare them to Yaroslavskiy's algorithm and the recently described three-pivot\nquicksort algorithm of Kushagra et al. (ALENEX 2014).\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 10:20:51 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2013 13:58:47 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 13:21:52 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Dietzfelbinger", "Martin", ""]]}, {"id": "1303.5313", "submitter": "Todd Veldhuizen", "authors": "Todd L. Veldhuizen", "title": "Incremental Maintenance for Leapfrog Triejoin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an incremental maintenance algorithm for leapfrog triejoin. The\nalgorithm maintains rules in time proportional (modulo log factors) to the edit\ndistance between leapfrog triejoin traces.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 15:56:54 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Veldhuizen", "Todd L.", ""]]}, {"id": "1303.5319", "submitter": "Josh Lockhart", "authors": "J. Lockhart, C. Di Franco, M. Paternostro", "title": "Glued trees algorithm under phase damping", "comments": "6 pages, 7 figures, RevTeX4", "journal-ref": "Phys. Lett. A 378, 338 (2014)", "doi": "10.1016/j.physleta.2013.11.034", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behaviour of the glued trees algorithm described by Childs et\nal. in [STOC `03, Proc. 35th ACM Symposium on Theory of Computing (2004) 59]\nunder decoherence. We consider a discrete time reformulation of the continuous\ntime quantum walk protocol and apply a phase damping channel to the coin state,\ninvestigating the effect of such a mechanism on the probability of the walker\nappearing on the target vertex of the graph. We pay particular attention to any\npotential advantage coming from the use of weak decoherence for the spreading\nof the walk across the glued trees graph.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 16:19:45 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2014 15:25:33 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Lockhart", "J.", ""], ["Di Franco", "C.", ""], ["Paternostro", "M.", ""]]}, {"id": "1303.5479", "submitter": "Mikkel Thorup", "authors": "Mikkel Thorup", "title": "Bottom-k and Priority Sampling, Set Similarity and Subset Sums with\n  Minimal Independence", "comments": "A short version appeared at STOC'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider bottom-k sampling for a set X, picking a sample S_k(X) consisting\nof the k elements that are smallest according to a given hash function h. With\nthis sample we can estimate the relative size f=|Y|/|X| of any subset Y as\n|S_k(X) intersect Y|/k. A standard application is the estimation of the Jaccard\nsimilarity f=|A intersect B|/|A union B| between sets A and B. Given the\nbottom-k samples from A and B, we construct the bottom-k sample of their union\nas S_k(A union B)=S_k(S_k(A) union S_k(B)), and then the similarity is\nestimated as |S_k(A union B) intersect S_k(A) intersect S_k(B)|/k.\n  We show here that even if the hash function is only 2-independent, the\nexpected relative error is O(1/sqrt(fk)). For fk=Omega(1) this is within a\nconstant factor of the expected relative error with truly random hashing.\n  For comparison, consider the classic approach of kxmin-wise where we use k\nhash independent functions h_1,...,h_k, storing the smallest element with each\nhash function. For kxmin-wise there is an at least constant bias with constant\nindependence, and it is not reduced with larger k. Recently Feigenblat et al.\nshowed that bottom-k circumvents the bias if the hash function is 8-independent\nand k is sufficiently large. We get down to 2-independence for any k. Our\nresult is based on a simply union bound, transferring generic concentration\nbounds for the hashing scheme to the bottom-k sample, e.g., getting stronger\nprobability error bounds with higher independence.\n  For weighted sets, we consider priority sampling which adapts efficiently to\nthe concrete input weights, e.g., benefiting strongly from heavy-tailed input.\nThis time, the analysis is much more involved, but again we show that generic\nconcentration bounds can be applied.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 22:18:08 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2013 19:17:12 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Thorup", "Mikkel", ""]]}, {"id": "1303.5481", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica", "title": "Novel O(H(N)+N/H(N)) Algorithmic Techniques for Several Types of Queries\n  and Updates on Rooted Trees and Lists", "comments": null, "journal-ref": "Metalurgia International, vol. 17, no. 12, pp. 216-223, 2012.\n  (ISSN: 1582-2214)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present novel algorithmic techniques with a O(H(N)+N/H(N))\ntime complexity for performing several types of queries and updates on general\nrooted trees, binary search trees and lists of size N. For rooted trees we\nintroduce a new compressed super-node tree representation which can be used for\nefficiently addressing a wide range of applications. For binary search trees we\ndiscuss the idea of globally rebuilding the entire tree in a fully balanced\nmanner whenever the height of the tree exceeds the value of a conveniently\nchosen function of the number of tree nodes. In the end of the paper we\nintroduce the H-list data structure which supports concatenation, split and\nseveral types of queries. Note that when choosing H(N)=sqrt(N) we obtain\nO(H(N)+N/H(N))=O(sqrt(N)).\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 22:41:52 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Andreica", "Mugurel Ionut", ""]]}, {"id": "1303.5800", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Eliana-Dina Tirsa", "title": "Line-Constrained Geometric Server Placement", "comments": null, "journal-ref": "Metalurgia International, vol. 16, no. 11, pp. 106-110, 2011.\n  (ISSN: 1582-2214)", "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present new algorithmic solutions for several constrained\ngeometric server placement problems. We consider the problems of computing the\n1-center and obnoxious 1-center of a set of line segments, constrained to lie\non a line segment, and the problem of computing the K-median of a set of\npoints, constrained to lie on a line. The presented algorithms have\napplications in many types of distributed systems, as well as in various fields\nwhich make use of distributed systems for running some of their applications\n(like chemistry, metallurgy, physics, etc.).\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 00:29:24 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Tirsa", "Eliana-Dina", ""]]}, {"id": "1303.5803", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Nicolae Tapus", "title": "Efficient Online Algorithmic Strategies for Several Two-Player Games\n  with Different or Identical Player Roles", "comments": null, "journal-ref": "Acta Universitatis Apulensis - Mathematics-Informatics, no. 25,\n  pp. 77-97, 2011. (ISSN: 1582-5329)", "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce novel algorithmic strategies for effciently\nplaying two-player games in which the players have different or identical\nplayer roles. In the case of identical roles, the players compete for the same\nobjective (that of winning the game). The case with different player roles\nassumes that one of the players asks questions in order to identify a secret\npattern and the other one answers them. The purpose of the first player is to\nask as few questions as possible (or that the questions and their number\nsatisfy some previously known constraints) and the purpose of the secret player\nis to answer the questions in a way that will maximize the number of questions\nasked by the first player (or in a way which forces the first player to break\nthe constraints of the game). We consider both previously known games (or\nextensions of theirs) and new types of games, introduced in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 00:41:29 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Tapus", "Nicolae", ""]]}, {"id": "1303.5862", "submitter": "Volkmar Sauerland", "authors": "Mourad El Ouali and Volkmar Sauerland", "title": "Improved Approximation Algorithm for the Number of Queries Necessary to\n  Identify a Permutation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past three decades, deductive games have become interesting from the\nalgorithmic point of view. Deductive games are two players zero sum games of\nimperfect information. The first player, called \"codemaker\", chooses a secret\ncode and the second player, called \"codebreaker\", tries to break the secret\ncode by making as few guesses as possible, exploiting information that is given\nby the codemaker after each guess. A well known deductive game is the famous\nMastermind game. In this paper, we consider the so called Black-Peg variant of\nMastermind, where the only information concerning a guess is the number of\npositions in which the guess coincides with the secret code. More precisely, we\ndeal with a special version of the Black-Peg game with n holes and k >= n\ncolors where no repetition of colors is allowed. We present a strategy that\nidentifies the secret code in O(n log n) queries. Our algorithm improves the\nprevious result of Ker-I Ko and Shia-Chung Teng (1985) by almost a factor of 2\nfor the case k = n. To our knowledge there is no previous work dealing with the\ncase k > n.\n  Keywords: Mastermind; combinatorial problems; permutations; algorithms\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 16:42:20 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2013 18:22:54 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Ouali", "Mourad El", ""], ["Sauerland", "Volkmar", ""]]}, {"id": "1303.6071", "submitter": "Jian Li", "authors": "Jian Li and Tianlin Shi", "title": "A Fully Polynomial-Time Approximation Scheme for Approximating a Sum of\n  Random Variables", "comments": "11 pages, new title, proofs polished, several typos revised. Also\n  added a section about the bit complexity", "journal-ref": null, "doi": "10.1016/j.orl.2014.02.004", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ independent random variables $X_1, X_2, ..., X_n$ and an integer\n$C$, we study the fundamental problem of computing the probability that the sum\n$X=X_1+X_2+...+X_n$ is at most $C$. We assume that each random variable $X_i$\nis implicitly given by an oracle which, given an input value $k$, returns the\nprobability $X_i\\leq k$. We give the first deterministic fully polynomial-time\napproximation scheme (FPTAS) to estimate the probability up to a relative error\nof $1\\pm \\epsilon$. Our algorithm is based on the idea developed for\napproximately counting knapsack solutions in [Gopalan et al. FOCS11].\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 10:09:00 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 08:30:56 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Li", "Jian", ""], ["Shi", "Tianlin", ""]]}, {"id": "1303.6200", "submitter": "Xujin Chen", "authors": "Zhigang Cao, Xujin Chen, Changjun Wang", "title": "How to Schedule the Marketing of Products with Negative Externalities", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In marketing products with negative externalities, a schedule which specifies\nan order of consumer purchase decisions is crucial, since in the social network\nof consumers, the decision of each consumer is negatively affected by the\nchoices of her neighbors. In this paper, we study the problems of finding a\nmarketing schedule for two asymmetric products with negative externalites. The\ngoals are two-fold: maximizing the sale of one product and ensuring regret-free\npurchase decisions. We show that the maximization is NP-hard, and provide\nefficient algorithms with satisfactory performance guarantees. Two of these\nalgorithms give regret-proof schedules, i.e. they reach Nash equilibria where\nno consumers regret their previous decisions. Our work is the first attempt to\naddress these marketing problems from an algorithmic point of view.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 16:32:20 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Cao", "Zhigang", ""], ["Chen", "Xujin", ""], ["Wang", "Changjun", ""]]}, {"id": "1303.6437", "submitter": "Richard Schmied", "authors": "Marek Karpinski, Michael Lampis and Richard Schmied", "title": "New Inapproximability Bounds for TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the approximability of the metric Traveling Salesman\nProblem (TSP) and prove new explicit inapproximability bounds for that problem.\nThe best up to now known hardness of approximation bounds were 185/184 for the\nsymmetric case (due to Lampis) and 117/116 for the asymmetric case (due to\nPapadimitriou and Vempala). We construct here two new bounded occurrence CSP\nreductions which improve these bounds to 123/122 and 75/74, respectively. The\nlatter bound is the first improvement in more than a decade for the case of the\nasymmetric TSP. One of our main tools, which may be of independent interest, is\na new construction of a bounded degree wheel amplifier used in the proof of our\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 11:23:04 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2013 20:14:43 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Karpinski", "Marek", ""], ["Lampis", "Michael", ""], ["Schmied", "Richard", ""]]}, {"id": "1303.6481", "submitter": "Simon Gog", "authors": "Simon Gog, Alistair Moffat, J. Shane Culpepper, Andrew Turpin, and\n  Anthony Wirth", "title": "Large-Scale Pattern Search Using Reduced-Space On-Disk Suffix Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suffix array is an efficient data structure for in-memory pattern search.\nSuffix arrays can also be used for external-memory pattern search, via\ntwo-level structures that use an internal index to identify the correct block\nof suffix pointers. In this paper we describe a new two-level suffix\narray-based index structure that requires significantly less disk space than\nprevious approaches. Key to the saving is the use of disk blocks that are based\non prefixes rather than the more usual uniform-sampling approach, allowing\nreductions between blocks and subparts of other blocks. We also describe a new\nin-memory structure based on a condensed BWT string, and show that it allows\ncommon patterns to be resolved without access to the text. Experiments using 64\nGB of English web text and a laptop computer with just 4 GB of main memory\ndemonstrate the speed and versatility of the new approach. For this data the\nindex is around one- third the size of previous two-level mechanisms; and the\nmemory footprint of as little as 1% of the text size means that queries can be\nprocessed more quickly than is possible with a compact FM-INDEX.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 13:28:31 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Gog", "Simon", ""], ["Moffat", "Alistair", ""], ["Culpepper", "J. Shane", ""], ["Turpin", "Andrew", ""], ["Wirth", "Anthony", ""]]}, {"id": "1303.6609", "submitter": "Jagan Sankaranarayanan", "authors": "Jeff LeFevre, Jagan Sankaranarayanan, Hakan Hacigumus, Junichi\n  Tatemura, Neoklis Polyzotis, Michael J. Carey", "title": "Exploiting Opportunistic Physical Design in Large-scale Data Analytics", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Large-scale systems, such as MapReduce and Hadoop, perform aggressive\nmaterialization of intermediate job results in order to support fault\ntolerance. When jobs correspond to exploratory queries submitted by data\nanalysts, these materializations yield a large set of materialized views that\ntypically capture common computation among successive queries from the same\nanalyst, or even across queries of different analysts who test similar\nhypotheses. We propose to treat these views as an opportunistic physical design\nand use them for the purpose of query optimization. We develop a novel\nquery-rewrite algorithm that addresses the two main challenges in this context:\nhow to search the large space of rewrites, and how to reason about views that\ncontain UDFs (a common feature in large-scale data analytics). The algorithm,\nwhich provably finds the minimum-cost rewrite, is inspired by nearest-neighbor\nsearches in non-metric spaces. We present an extensive experimental study on\nreal-world datasets with a prototype data-analytics system based on Hive. The\nresults demonstrate that our approach can result in dramatic performance\nimprovements on complex data-analysis queries, reducing total execution time by\nan average of 61% and up to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 19:08:55 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2013 17:35:09 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["LeFevre", "Jeff", ""], ["Sankaranarayanan", "Jagan", ""], ["Hacigumus", "Hakan", ""], ["Tatemura", "Junichi", ""], ["Polyzotis", "Neoklis", ""], ["Carey", "Michael J.", ""]]}, {"id": "1303.6761", "submitter": "Jin-Kao Hao", "authors": "Qinghua Wu and Jin-Kao Hao", "title": "Improved Lower Bounds for Sum Coloring via Clique Decomposition", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G = (V,E)$ with a set $V$ of vertices and a set\n$E$ of edges, the minimum sum coloring problem (MSCP) is to find a legal vertex\ncoloring of $G$, using colors represented by natural numbers $1, 2, . . .$ such\nthat the total sum of the colors assigned to the vertices is minimized. This\npaper describes an approach based on the decomposition of the original graph\ninto disjoint cliques for computing lower bounds for the MSCP. Basically, the\nproposed approach identifies and removes at each extraction iteration a maximum\nnumber of cliques of the same size (the largest possible) from the graph.\nComputational experiments show that this approach is able to improve on the\ncurrent best lower bounds for 14 benchmark instances, and to prove optimality\nfor the first time for 4 instances. We also report lower bounds for 24 more\ninstances for which no such bounds are available in the literature. These new\nlower bounds are useful to estimate the quality of the upper bounds obtained\nwith various heuristic approaches.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 08:35:16 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Wu", "Qinghua", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "1303.6775", "submitter": "Jinlong Tu", "authors": "Jinlong Tu, Lian Lu, Minghua Chen, Ramesh K. Sitaraman", "title": "Dynamic Provisioning in Next-Generation Data Centers with On-site Power\n  Production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The critical need for clean and economical sources of energy is transforming\ndata centers that are primarily energy consumers to also energy producers. We\nfocus on minimizing the operating costs of next-generation data centers that\ncan jointly optimize the energy supply from on-site generators and the power\ngrid, and the energy demand from servers as well as power conditioning and\ncooling systems. We formulate the cost minimization problem and present an\noffline optimal algorithm. For \"on-grid\" data centers that use only the grid,\nwe devise a deterministic online algorithm that achieves the best possible\ncompetitive ratio of $2-\\alpha_{s}$, where $\\alpha_{s}$ is a normalized\nlook-ahead window size. For \"hybrid\" data centers that have on-site power\ngeneration in addition to the grid, we develop an online algorithm that\nachieves a competitive ratio of at most \\textmd{\\normalsize {\\small\n$\\frac{P_{\\max} (2-\\alpha_{s})}{c_{o}+c_{m}/L}\n[1+2\\frac{P_{\\max}-c_{o}}{P_{\\max}(1+\\alpha_{g})}]$}}, where $\\alpha_{s}$ and\n$\\alpha_{g}$ are normalized look-ahead window sizes, $P_{\\max}$ is the maximum\ngrid power price, and $L$, $c_{o}$, and $c_{m}$ are parameters of an on-site\ngenerator.\n  Using extensive workload traces from Akamai with the corresponding grid power\nprices, we simulate our offline and online algorithms in a realistic setting.\nOur offline (resp., online) algorithm achieves a cost reduction of 25.8%\n(resp., 20.7%) for a hybrid data center and 12.3% (resp., 7.3%) for an on-grid\ndata center. The cost reductions are quite significant and make a strong case\nfor a joint optimization of energy supply and energy demand in a data center. A\nhybrid data center provides about 13% additional cost reduction over an on-grid\ndata center representing the additional cost benefits that on-site power\ngeneration provides over using the grid alone.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 10:01:40 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2013 05:57:04 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Tu", "Jinlong", ""], ["Lu", "Lian", ""], ["Chen", "Minghua", ""], ["Sitaraman", "Ramesh K.", ""]]}, {"id": "1303.6785", "submitter": "Gennaro Cordasco PhD", "authors": "Ferdinando Cicalese, Gennaro Cordasco, Luisa Gargano, M. Milanic and\n  Ugo Vaccaro", "title": "Latency-Bounded Target Set Selection in Social Networks", "comments": "An extended version of this paper will appear in Theoretical Computer\n  Science, Elsevier. See also Proceedings of Computability in Europe 2013 (CiE\n  2013), The Nature of Computation: Logic, Algorithms, Applications, Lectures\n  Notes in Computer Science, Springer", "journal-ref": null, "doi": "10.1016/j.tcs.2014.02.027", "report-no": null, "categories": "cs.DS cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in sociology, economy and medicine, we study\nvariants of the Target Set Selection problem, first proposed by Kempe,\nKleinberg and Tardos. In our scenario one is given a graph $G=(V,E)$, integer\nvalues $t(v)$ for each vertex $v$ (\\emph{thresholds}), and the objective is to\ndetermine a small set of vertices (\\emph{target set}) that activates a given\nnumber (or a given subset) of vertices of $G$ \\emph{within} a prescribed number\nof rounds. The activation process in $G$ proceeds as follows: initially, at\nround 0, all vertices in the target set are activated; subsequently at each\nround $r\\geq 1$ every vertex of $G$ becomes activated if at least $t(v)$ of its\nneighbors are already active by round $r-1$. It is known that the problem of\nfinding a minimum cardinality Target Set that eventually activates the whole\ngraph $G$ is hard to approximate to a factor better than\n$O(2^{\\log^{1-\\epsilon}|V|})$. In this paper we give \\emph{exact} polynomial\ntime algorithms to find minimum cardinality Target Sets in graphs of bounded\nclique-width, and \\emph{exact} linear time algorithms for trees.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 10:52:32 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 18:13:22 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Cordasco", "Gennaro", ""], ["Gargano", "Luisa", ""], ["Milanic", "M.", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1303.6867", "submitter": "Bang Ye Wu", "authors": "Bang Ye Wu and Li-Hsuan Chen", "title": "Parameterized algorithms for the 2-clustering problem with minimum sum\n  and minimum sum of squares objective functions", "comments": "journal version", "journal-ref": null, "doi": "10.1007/s00453-014-9874-8", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the {\\sc Min-Sum 2-Clustering} problem, we are given a graph and a\nparameter $k$, and the goal is to determine if there exists a 2-partition of\nthe vertex set such that the total conflict number is at most $k$, where the\nconflict number of a vertex is the number of its non-neighbors in the same\ncluster and neighbors in the different cluster. The problem is equivalent to\n{\\sc 2-Cluster Editing} and {\\sc 2-Correlation Clustering} with an additional\nmultiplicative factor two in the cost function. In this paper we show an\nalgorithm for {\\sc Min-Sum 2-Clustering} with time complexity $O(n\\cdot\n2.619^{r/(1-4r/n)}+n^3)$, where $n$ is the number of vertices and $r=k/n$.\nParticularly, the time complexity is $O^*(2.619^{k/n})$ for $k\\in o(n^2)$ and\npolynomial for $k\\in O(n\\log n)$, which implies that the problem can be solved\nin subexponential time for $k\\in o(n^2)$. We also design a parameterized\nalgorithm for a variant in which the cost is the sum of the squared\nconflict-numbers. For $k\\in o(n^3)$, the algorithm runs in subexponential\n$O(n^3\\cdot 5.171^{\\theta})$ time, where $\\theta=\\sqrt{k/n}$.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 15:57:58 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 04:35:17 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Wu", "Bang Ye", ""], ["Chen", "Li-Hsuan", ""]]}, {"id": "1303.6872", "submitter": "Jakub Radoszewski", "authors": "Maxime Crochemore, Costas S. Iliopoulos, Tomasz Kociumaka, Marcin\n  Kubica, Alessio Langiu, Solon P. Pissis, Jakub Radoszewski, Wojciech Rytter,\n  Tomasz Walen", "title": "Order-Preserving Suffix Trees and Their Algorithmic Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Kubica et al. (Inf. Process. Let., 2013) and Kim et al. (submitted\nto Theor. Comp. Sci.) introduced order-preserving pattern matching. In this\nproblem we are looking for consecutive substrings of the text that have the\nsame \"shape\" as a given pattern. These results include a linear-time\norder-preserving pattern matching algorithm for polynomially-bounded alphabet\nand an extension of this result to pattern matching with multiple patterns. We\nmake one step forward in the analysis and give an\n$O(\\frac{n\\log{n}}{\\log\\log{n}})$ time randomized algorithm constructing suffix\ntrees in the order-preserving setting. We show a number of applications of\norder-preserving suffix trees to identify patterns and repetitions in time\nseries.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 16:13:03 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas S.", ""], ["Kociumaka", "Tomasz", ""], ["Kubica", "Marcin", ""], ["Langiu", "Alessio", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Walen", "Tomasz", ""]]}, {"id": "1303.6907", "submitter": "Florian Sikora", "authors": "Cristina Bazgan, Morgan Chopin, Andr\\'e Nichterlein, Florian Sikora", "title": "Parameterized Approximability of Maximizing the Spread of Influence in\n  Networks", "comments": null, "journal-ref": "Journal of Discrete Algorithms (27), 2014, 54--65", "doi": "10.1016/j.jda.2014.05.001", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of maximizing the spread of influence\nthrough a social network. Given a graph with a threshold value~$thr(v)$\nattached to each vertex~$v$, the spread of influence is modeled as follows: A\nvertex~$v$ becomes \"active\" (influenced) if at least $thr(v)$ of its neighbors\nare active. In the corresponding optimization problem the objective is then to\nfind a fixed number of vertices to activate such that the number of activated\nvertices at the end of the propagation process is maximum. We show that this\nproblem is strongly inapproximable in fpt-time with respect to (w.r.t.)\nparameter $k$ even for very restrictive thresholds. In the case that the\nthreshold of each vertex equals its degree, we prove that the problem is\ninapproximable in polynomial time and it becomes $r(n)$-approximable in\nfpt-time w.r.t. parameter $k$ for any strictly increasing function $r$.\n  Moreover, we show that the decision version is W[1]-hard w.r.t. parameter $k$\nbut becomes fixed-parameter tractable on bounded degree graphs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 17:41:51 GMT"}, {"version": "v2", "created": "Sun, 17 Aug 2014 09:13:01 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Bazgan", "Cristina", ""], ["Chopin", "Morgan", ""], ["Nichterlein", "Andr\u00e9", ""], ["Sikora", "Florian", ""]]}, {"id": "1303.7217", "submitter": "XiangYang Li", "authors": "Sanjiv Kapoor and XiangYang Li", "title": "Efficient Construction of Spanners in $d$-Dimensions", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of efficiently constructing $k$-vertex\nfault-tolerant geometric $t$-spanners in $\\dspace$ (for $k \\ge 0$ and $t >1$).\nVertex fault-tolerant spanners were introduced by Levcopoulus et. al in 1998.\nFor $k=0$, we present an $O(n \\log n)$ method using the algebraic computation\ntree model to find a $t$-spanner with degree bound O(1) and weight\n$O(\\weight(MST))$. This resolves an open problem. For $k \\ge 1$, we present an\nefficient method that, given $n$ points in $\\dspace$, constructs $k$-vertex\nfault-tolerant $t$-spanners with the maximum degree bound O(k) and weight bound\n$O(k^2 \\weight(MST))$ in time $O(n \\log n)$. Our method achieves the best\npossible bounds on degree, total edge length, and the time complexity, and\nsolves the open problem of efficient construction of (fault-tolerant)\n$t$-spanners in $\\dspace$ in time $O(n \\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 19:23:25 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Kapoor", "Sanjiv", ""], ["Li", "XiangYang", ""]]}, {"id": "1303.7462", "submitter": "James Smith", "authors": "James Smith", "title": "Concur: An Algorithm for Merging Concurrent Changes without Conflicts", "comments": "The tone is amateurish and there are far too many mistakes. There\n  will be two new papers covering the same material in a more professional\n  manner uploaded to arxiv.org in the near future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose you and I are both editing a document. You make and change and I make\na change, concurrently. Now if we want to still be seeing the same document\nthen I need to apply your change after mine and you mine after yours. But we\ncan't just apply them willy-nilly. I must amend yours somehow and you mine. If\nmy change is written D, yours d, and the amended changes D.d and d.D, we get\n*D*D.d=*d*d.D as long as applying is written * and we don't care about what\nwe're applying the changes to. We start by proving this identity for single\nchanges and finish by proving it for many.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 19:15:04 GMT"}, {"version": "v2", "created": "Mon, 13 May 2013 20:44:22 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 07:59:32 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2016 18:35:17 GMT"}, {"version": "v5", "created": "Sat, 20 Feb 2016 15:26:20 GMT"}, {"version": "v6", "created": "Mon, 11 Jul 2016 21:25:06 GMT"}, {"version": "v7", "created": "Tue, 19 Jul 2016 19:04:54 GMT"}, {"version": "v8", "created": "Thu, 4 Jul 2019 11:22:58 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Smith", "James", ""]]}]