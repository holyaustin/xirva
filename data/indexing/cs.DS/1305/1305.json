[{"id": "1305.0069", "submitter": "Martin Fink", "authors": "Martin Fink and Sergey Pupyrev", "title": "Ordering Metro Lines by Block Crossings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem that arises in drawings of transportation networks is to minimize\nthe number of crossings between different transportation lines. While this can\nbe done efficiently under specific constraints, not all solutions are visually\nequivalent. We suggest merging crossings into block crossings, that is,\ncrossings of two neighboring groups of consecutive lines. Unfortunately,\nminimizing the total number of block crossings is NP-hard even for very simple\ngraphs. We give approximation algorithms for special classes of graphs and an\nasymptotically worst-case optimal algorithm for block crossings on general\ngraphs. That is, we bound the number of block crossings that our algorithm\nneeds and construct worst-case instances on which the number of block crossings\nthat is necessary in any solution is asymptotically the same as our bound.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 02:14:19 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2013 13:22:52 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Fink", "Martin", ""], ["Pupyrev", "Sergey", ""]]}, {"id": "1305.0087", "submitter": "Jiyan Yang", "authors": "Jiyan Yang and Xiangrui Meng and Michael W. Mahoney", "title": "Quantile Regression for Large-scale Applications", "comments": "35 pages; long version of a paper appearing in the 2013 ICML. Version\n  to appear in the SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a method to estimate the quantiles of the conditional\ndistribution of a response variable, and as such it permits a much more\naccurate portrayal of the relationship between the response variable and\nobserved covariates than methods such as Least-squares or Least Absolute\nDeviations regression. It can be expressed as a linear program, and, with\nappropriate preprocessing, interior-point methods can be used to find a\nsolution for moderately large problems. Dealing with very large problems,\n\\emph(e.g.), involving data up to and beyond the terabyte regime, remains a\nchallenge. Here, we present a randomized algorithm that runs in nearly linear\ntime in the size of the input and that, with constant probability, computes a\n$(1+\\epsilon)$ approximate solution to an arbitrary quantile regression\nproblem. As a key step, our algorithm computes a low-distortion\nsubspace-preserving embedding with respect to the loss function of quantile\nregression. Our empirical evaluation illustrates that our algorithm is\ncompetitive with the best previous work on small to medium-sized problems, and\nthat in addition it can be implemented in MapReduce-like environments and\napplied to terabyte-sized problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 05:21:03 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 18:18:50 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2014 00:33:30 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Yang", "Jiyan", ""], ["Meng", "Xiangrui", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1305.0110", "submitter": "David Eppstein", "authors": "David Eppstein, Michael T. Goodrich and Daniel S. Hirschberg", "title": "Combinatorial Pair Testing: Distinguishing Workers from Slackers", "comments": "12 pages. Extended version of a paper to appear at the Algorithms and\n  Data Structures Symposium (WADS 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize a problem we call combinatorial pair testing (CPT), which has\napplications to the identification of uncooperative or unproductive\nparticipants in pair programming, massively distributed computing, and\ncrowdsourcing environments. We give efficient adaptive and nonadaptive CPT\nalgorithms and we show that our methods use an optimal number of testing rounds\nto within constant factors. We also provide an empirical evaluation of some of\nour methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 07:10:01 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Hirschberg", "Daniel S.", ""]]}, {"id": "1305.0159", "submitter": "Anthony J Cox", "authors": "Lilian Janin and Giovanna Rosone and Anthony J. Cox", "title": "Adaptive reference-free compression of sequence quality scores", "comments": "Accepted paper for HiTSeq 2013, to appear in Bioinformatics.\n  Bioinformatics should be considered the original place of publication of this\n  work, please cite accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation:\n  Rapid technological progress in DNA sequencing has stimulated interest in\ncompressing the vast datasets that are now routinely produced. Relatively\nlittle attention has been paid to compressing the quality scores that are\nassigned to each sequence, even though these scores may be harder to compress\nthan the sequences themselves. By aggregating a set of reads into a compressed\nindex, we find that the majority of bases can be predicted from the sequence of\nbases that are adjacent to them and hence are likely to be less informative for\nvariant calling or other applications. The quality scores for such bases are\naggressively compressed, leaving a relatively small number at full resolution.\nSince our approach relies directly on redundancy present in the reads, it does\nnot need a reference sequence and is therefore applicable to data from\nmetagenomics and de novo experiments as well as to resequencing data.\n  Results:\n  We show that a conservative smoothing strategy affecting 75% of the quality\nscores above Q2 leads to an overall quality score compression of 1 bit per\nvalue with a negligible effect on variant calling. A compression of 0.68 bit\nper quality value is achieved using a more aggressive smoothing strategy, again\nwith a very small effect on variant calling.\n  Availability:\n  Code to construct the BWT and LCP-array on large genomic data sets is part of\nthe BEETL library, available as a github respository at\nhttp://git@github.com:BEETL/BEETL.git .\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 12:51:10 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Janin", "Lilian", ""], ["Rosone", "Giovanna", ""], ["Cox", "Anthony J.", ""]]}, {"id": "1305.0160", "submitter": "Anthony J Cox", "authors": "Markus J. Bauer and Anthony J. Cox and Giovanna Rosone and Marinella\n  Sciortino", "title": "Lightweight LCP Construction for Next-Generation Sequencing Datasets", "comments": "Springer LNCS (Lecture Notes in Computer Science) should be\n  considered as the original place of publication, please cite accordingly. The\n  final version of this manuscript is available at\n  http://link.springer.com/chapter/10.1007/978-3-642-33122-0_26", "journal-ref": "Lecture Notes in Computer Science Volume 7534, 2012, pp 326-337", "doi": "10.1007/978-3-642-33122-0_26", "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of \"next-generation\" DNA sequencing (NGS) technologies has meant\nthat collections of hundreds of millions of DNA sequences are now commonplace\nin bioinformatics. Knowing the longest common prefix array (LCP) of such a\ncollection would facilitate the rapid computation of maximal exact matches,\nshortest unique substrings and shortest absent words. CPU-efficient algorithms\nfor computing the LCP of a string have been described in the literature, but\nrequire the presence in RAM of large data structures. This prevents such\nmethods from being feasible for NGS datasets.\n  In this paper we propose the first lightweight method that simultaneously\ncomputes, via sequential scans, the LCP and BWT of very large collections of\nsequences. Computational results on collections as large as 800 million\n100-mers demonstrate that our algorithm scales to the vast sequence collections\nencountered in human whole genome sequencing experiments.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 12:51:45 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Bauer", "Markus J.", ""], ["Cox", "Anthony J.", ""], ["Rosone", "Giovanna", ""], ["Sciortino", "Marinella", ""]]}, {"id": "1305.0433", "submitter": "Ioan Todinca", "authors": "Mathieu Chapelle (1), Mathieu Liedloff (2), Ioan Todinca (2), and\n  Yngve Villanger (3) ((1) University Paris Est, France, (2) University of\n  Orleans, France, (3) University of Bergen, Norway)", "title": "TREEWIDTH and PATHWIDTH parameterized by vertex cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the number of vertices, Vertex Cover is the largest of the classical\ngraph parameters and has more and more frequently been used as a separate\nparameter in parameterized problems, including problems that are not directly\nrelated to the Vertex Cover. Here we consider the TREEWIDTH and PATHWIDTH\nproblems parameterized by k, the size of a minimum vertex cover of the input\ngraph. We show that the PATHWIDTH and TREEWIDTH can be computed in O*(3^k)\ntime. This complements recent polynomial kernel results for TREEWIDTH and\nPATHWIDTH parameterized by the Vertex Cover.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 13:49:35 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Chapelle", "Mathieu", ""], ["Liedloff", "Mathieu", ""], ["Todinca", "Ioan", ""], ["Villanger", "Yngve", ""]]}, {"id": "1305.0512", "submitter": "Christopher Whidden", "authors": "Chris Whidden and Robert G. Beiko and Norbert Zeh", "title": "Fixed-Parameter and Approximation Algorithms for Maximum Agreement\n  Forests of Multifurcating Trees", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient algorithms for computing a maximum agreement forest\n(MAF) of a pair of multifurcating (nonbinary) rooted trees. Our algorithms\nmatch the running times of the currently best algorithms for the binary case.\nThe size of an MAF corresponds to the subtree prune-and-regraft (SPR) distance\nof the two trees and is intimately connected to their hybridization number.\nThese distance measures are essential tools for understanding reticulate\nevolution, such as lateral gene transfer, recombination, and hybridization.\nMultifurcating trees arise naturally as a result of statistical uncertainty in\ncurrent tree construction methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 16:57:48 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Whidden", "Chris", ""], ["Beiko", "Robert G.", ""], ["Zeh", "Norbert", ""]]}, {"id": "1305.0526", "submitter": "Sushant Sachdeva", "authors": "Sushant Sachdeva and Nisheeth K. Vishnoi", "title": "Matrix Inversion Is As Easy As Exponentiation", "comments": "This paper appears in the monograph 'Faster Algorithms via\n  Approximation Theory' written by the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the inverse of a positive-definite matrix can be approximated\nby a weighted-sum of a small number of matrix exponentials. Combining this with\na previous result [OSV12], we establish an equivalence between matrix inversion\nand exponentiation up to polylogarithmic factors. In particular, this\nconnection justifies the use of Laplacian solvers for designing fast\nsemi-definite programming based algorithms for certain graph problems. The\nproof relies on the Euler-Maclaurin formula and certain bounds derived from the\nRiemann zeta function.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 18:06:54 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 17:38:03 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Sachdeva", "Sushant", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1305.0597", "submitter": "Balasubramanian Sivan", "authors": "Shuchi Chawla, Jason D. Hartline, David Malec, Balasubramanian Sivan", "title": "Prior-Independent Mechanisms for Scheduling", "comments": "This paper will appear in Proceedings of the ACM Symposium on Theory\n  of Computing 2013 (STOC'13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the makespan minimization problem with unrelated selfish machines\nunder the assumption that job sizes are stochastic. We design simple truthful\nmechanisms that under various distributional assumptions provide constant and\nsublogarithmic approximations to expected makespan. Our mechanisms are\nprior-independent in that they do not rely on knowledge of the job size\ndistributions. Prior-independent approximation mechanisms have been previously\nstudied for the objective of revenue maximization [Dhangwatnotai, Roughgarden\nand Yan'10, Devanur, Hartline, Karlin and Nguyen'11, Roughgarden, Talgam-Cohen\nand Yan'12]. In contrast to our results, in prior-free settings no truthful\nanonymous deterministic mechanism for the makespan objective can provide a\nsublinear approximation [Ashlagi, Dobzinski and Lavi'09].\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 23:36:16 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Chawla", "Shuchi", ""], ["Hartline", "Jason D.", ""], ["Malec", "David", ""], ["Sivan", "Balasubramanian", ""]]}, {"id": "1305.0649", "submitter": "Christian Komusiewicz", "authors": "Laurent Bulteau, Christian Komusiewicz", "title": "Minimum Common String Partition Parameterized by Partition Size is\n  Fixed-Parameter Tractable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NP-hard Minimum Common String Partition problem asks whether two strings\n$x$ and $y$ can each be partitioned into at most $k$ substrings, called blocks,\nsuch that both partitions use exactly the same blocks in a different order. We\npresent the first fixed-parameter algorithm for Minimum Common String Partition\nusing only parameter $k$.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 09:26:21 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2013 09:30:17 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Bulteau", "Laurent", ""], ["Komusiewicz", "Christian", ""]]}, {"id": "1305.0669", "submitter": "Kim S. Larsen", "authors": "Joan Boyar, Sushmita Gupta, Kim S. Larsen", "title": "Relative Interval Analysis of Paging Algorithms on Access Graphs", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access graphs, which have been used previously in connection with competitive\nanalysis and relative worst order analysis to model locality of reference in\npaging, are considered in connection with relative interval analysis. The\nalgorithms LRU, FIFO, FWF, and FAR are compared using the path, star, and cycle\naccess graphs. In this model, some of the expected results are obtained.\nHowever, although LRU is found to be strictly better than FIFO on paths, it has\nworse performance on stars, cycles, and complete graphs, in this model. We\nsolve an open question from [Dorrigiv, Lopez-Ortiz, Munro, 2009], obtaining\ntight bounds on the relationship between LRU and FIFO with relative interval\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 10:57:32 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Boyar", "Joan", ""], ["Gupta", "Sushmita", ""], ["Larsen", "Kim S.", ""]]}, {"id": "1305.0674", "submitter": "Johannes Fischer", "authors": "Julian Arz and Johannes Fischer", "title": "LZ-Compressed String Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to compress string dictionaries using the Lempel-Ziv (LZ78) data\ncompression algorithm. Our approach is validated experimentally on dictionaries\nof up to 1.5 GB of uncompressed text. We achieve compression ratios often\noutperforming the existing alternatives, especially on dictionaries containing\nmany repeated substrings. Our query times remain competitive.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 11:29:39 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Arz", "Julian", ""], ["Fischer", "Johannes", ""]]}, {"id": "1305.0757", "submitter": "Tanja Hartmann", "authors": "Michael Hamann and Tanja Hartmann and Dorothea Wagner", "title": "Hierarchies of Predominantly Connected Communities", "comments": "to appear (WADS 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider communities whose vertices are predominantly connected, i.e., the\nvertices in each community are stronger connected to other community members of\nthe same community than to vertices outside the community. Flake et al.\nintroduced a hierarchical clustering algorithm that finds such predominantly\nconnected communities of different coarseness depending on an input parameter.\nWe present a simple and efficient method for constructing a clustering\nhierarchy according to Flake et al. that supersedes the necessity of choosing\nfeasible parameter values and guarantees the completeness of the resulting\nhierarchy, i.e., the hierarchy contains all clusterings that can be constructed\nby the original algorithm for any parameter value. However, predominantly\nconnected communities are not organized in a single hierarchy. Thus, we develop\na framework that, after precomputing at most $2(n-1)$ maximum flows, admits a\nlinear time construction of a clustering $\\C(S)$ of predominantly connected\ncommunities that contains a given community $S$ and is maximum in the sense\nthat any further clustering of predominantly connected communities that also\ncontains $S$ is hierarchically nested in $\\C(S)$. We further generalize this\nconstruction yielding a clustering with similar properties for $k$ given\ncommunities in $O(kn)$ time. This admits the analysis of a network's structure\nwith respect to various communities in different hierarchies.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 15:51:15 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Hamann", "Michael", ""], ["Hartmann", "Tanja", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1305.0870", "submitter": "Sameer Pawar", "authors": "Sameer Pawar and Kannan Ramchandran", "title": "Computing a k-sparse n-length Discrete Fourier Transform using at most\n  4k samples and O(k log k) complexity", "comments": "36 pages, 15 figures. To be presented at ISIT-2013, Istanbul Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an $n$-length input signal $\\mbf{x}$, it is well known that its\nDiscrete Fourier Transform (DFT), $\\mbf{X}$, can be computed in $O(n \\log n)$\ncomplexity using a Fast Fourier Transform (FFT). If the spectrum $\\mbf{X}$ is\nexactly $k$-sparse (where $k<<n$), can we do better? We show that\nasymptotically in $k$ and $n$, when $k$ is sub-linear in $n$ (precisely, $k\n\\propto n^{\\delta}$ where $0 < \\delta <1$), and the support of the non-zero DFT\ncoefficients is uniformly random, we can exploit this sparsity in two\nfundamental ways (i) {\\bf {sample complexity}}: we need only $M=rk$\ndeterministically chosen samples of the input signal $\\mbf{x}$ (where $r < 4$\nwhen $0 < \\delta < 0.99$); and (ii) {\\bf {computational complexity}}: we can\nreliably compute the DFT $\\mbf{X}$ using $O(k \\log k)$ operations, where the\nconstants in the big Oh are small and are related to the constants involved in\ncomputing a small number of DFTs of length approximately equal to the sparsity\nparameter $k$. Our algorithm succeeds with high probability, with the\nprobability of failure vanishing to zero asymptotically in the number of\nsamples acquired, $M$.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2013 02:54:59 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 06:41:20 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Pawar", "Sameer", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1305.1021", "submitter": "J\\=anis Iraids", "authors": "Andris Ambainis, Kaspars Balodis, J\\=anis Iraids, Raitis Ozols, Juris\n  Smotrovs", "title": "Parameterized Quantum Query Complexity of Graph Collision", "comments": "12 pages, 5 figures, submitted to ICALP workshop \"Workshop on Quantum\n  and Classical Complexity\" in 5/5/2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present three new quantum algorithms in the quantum query model for\n\\textsc{graph-collision} problem: \\begin{itemize} \\item an algorithm based on\ntree decomposition that uses $O\\left(\\sqrt{n}t^{\\sfrac{1}{6}}\\right)$ queries\nwhere $t$ is the treewidth of the graph; \\item an algorithm constructed on a\nspan program that improves a result by Gavinsky and Ito. The algorithm uses\n$O(\\sqrt{n}+\\sqrt{\\alpha^{**}})$ queries, where $\\alpha^{**}(G)$ is a graph\nparameter defined by \\[\\alpha^{**}(G):=\\min_{VC\\text{-- vertex cover\nof}G}{\\max_{\\substack{I\\subseteq VC\\\\I\\text{-- independent set}}}{\\sum_{v\\in\nI}{\\deg{v}}}};\\] \\item an algorithm for a subclass of circulant graphs that\nuses $O(\\sqrt{n})$ queries. \\end{itemize} We also present an example of a\npossibly difficult graph $G$ for which all the known graphs fail to solve graph\ncollision in $O(\\sqrt{n} \\log^c n)$ queries.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 15:40:13 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Ambainis", "Andris", ""], ["Balodis", "Kaspars", ""], ["Iraids", "J\u0101nis", ""], ["Ozols", "Raitis", ""], ["Smotrovs", "Juris", ""]]}, {"id": "1305.1157", "submitter": "Timo Bingmann", "authors": "Timo Bingmann and Peter Sanders", "title": "Parallel String Sample Sort", "comments": "34 pages, 7 figures and 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss how string sorting algorithms can be parallelized on modern\nmulti-core shared memory machines. As a synthesis of the best sequential string\nsorting algorithms and successful parallel sorting algorithms for atomic\nobjects, we propose string sample sort. The algorithm makes effective use of\nthe memory hierarchy, uses additional word level parallelism, and largely\navoids branch mispredictions. Additionally, we parallelize variants of multikey\nquicksort and radix sort that are also useful in certain situations.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 12:03:56 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Bingmann", "Timo", ""], ["Sanders", "Peter", ""]]}, {"id": "1305.1222", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff", "title": "A Note on (Parallel) Depth- and Breadth-First Search by Arc Elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note recapitulates an algorithmic observation for ordered Depth-First\nSearch (DFS) in directed graphs that immediately leads to a parallel algorithm\nwith linear speed-up for a range of processors for non-sparse graphs. The note\nextends the approach to ordered Breadth-First Search (BFS). With $p$\nprocessors, both DFS and BFS algorithms run in $O(m/p+n)$ time steps on a\nshared-memory parallel machine allowing concurrent reading of locations, e.g.,\na CREW PRAM, and have linear speed-up for $p\\leq m/n$. Both algorithms need $n$\nsynchronization steps.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 15:31:17 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2013 11:45:24 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2013 17:00:34 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1305.1295", "submitter": "Philipp Woelfel", "authors": "Martin Dietzfelbinger and Philipp Woelfel", "title": "Tight Lower Bounds for Greedy Routing in Higher-Dimensional Small-World\n  Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Kleinberg's celebrated small world graph model (Kleinberg, 2000),\nin which a D-dimensional grid {0,...,n-1}^D is augmented with a constant number\nof additional unidirectional edges leaving each node. These long range edges\nare determined at random according to a probability distribution (the\naugmenting distribution), which is the same for each node. Kleinberg suggested\nusing the inverse D-th power distribution, in which node v is the long range\ncontact of node u with a probability proportional to ||u-v||^(-D). He showed\nthat such an augmenting distribution allows to route a message efficiently in\nthe resulting random graph: The greedy algorithm, where in each intermediate\nnode the message travels over a link that brings the message closest to the\ntarget w.r.t. the Manhattan distance, finds a path of expected length O(log^2\nn) between any two nodes. In this paper we prove that greedy routing does not\nperform asymptotically better for any uniform and isotropic augmenting\ndistribution, i.e., the probability that node u has a particular long range\ncontact v is independent of the labels of u and v and only a function of\n||u-v||.\n  In order to obtain the result, we introduce a novel proof technique: We\ndefine a budget game, in which a token travels over a game board, while the\nplayer manages a \"probability budget\". In each round, the player bets part of\nher remaining probability budget on step sizes. A step size is chosen at random\naccording to a probability distribution of the player's bet. The token then\nmakes progress as determined by the chosen step size, while some of the\nplayer's bet is removed from her probability budget. We prove a tight lower\nbound for such a budget game, and then obtain a lower bound for greedy routing\nin the D-dimensional grid by a reduction.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 18:19:17 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Dietzfelbinger", "Martin", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1305.1327", "submitter": "Kevin Zatloukal", "authors": "Kevin C. Zatloukal", "title": "Classical and Quantum Algorithms for Testing Equivalence of Group\n  Extensions", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While efficient algorithms are known for solving many important problems\nrelated to groups, no efficient algorithm is known for determining whether two\narbitrary groups are isomorphic. The particular case of 2-nilpotent groups, a\nspecial type of central extension, is widely believed to contain the essential\nhard cases. However, looking specifically at central extensions, the natural\nformulation of being \"the same\" is not isomorphism but rather \"equivalence,\"\nwhich requires an isomorphism to preserves the structure of the extension. In\nthis paper, we show that equivalence of central extensions can be computed\nefficiently on a classical computer when the groups are small enough to be\ngiven by their multiplication tables. However, in the model of black box\ngroups, which allows the groups to be much larger, we show that equivalence can\nbe computed efficiently on a quantum computer but not a classical one (under\ncommon complexity assumptions). Our quantum algorithm demonstrates a new\napplication of the hidden subgroup problem for general abelian groups.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 20:46:10 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Zatloukal", "Kevin C.", ""]]}, {"id": "1305.1330", "submitter": "Quan Geng", "authors": "Quan Geng and Pramod Viswanath", "title": "Optimal Noise Adding Mechanisms for Approximate Differential Privacy", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the (nearly) optimal mechanisms in $(\\epsilon,\\delta)$-approximate\ndifferential privacy for integer-valued query functions and vector-valued\n(histogram-like) query functions under a utility-maximization/cost-minimization\nframework. We characterize the tradeoff between $\\epsilon$ and $\\delta$ in\nutility and privacy analysis for histogram-like query functions ($\\ell^1$\nsensitivity), and show that the $(\\epsilon,\\delta)$-differential privacy is a\nframework not much more general than the $(\\epsilon,0)$-differential privacy\nand $(0,\\delta)$-differential privacy in the context of $\\ell^1$ and $\\ell^2$\ncost functions, i.e., minimum expected noise magnitude and noise power. In the\nsame context of $\\ell^1$ and $\\ell^2$ cost functions, we show the\nnear-optimality of uniform noise mechanism and discrete Laplacian mechanism in\nthe high privacy regime (as $(\\epsilon,\\delta) \\to (0,0)$). We conclude that in\n$(\\epsilon,\\delta)$-differential privacy, the optimal noise magnitude and noise\npower are $\\Theta(\\min(\\frac{1}{\\epsilon},\\frac{1}{\\delta}))$ and\n$\\Theta(\\min(\\frac{1}{\\epsilon^2},\\frac{1}{\\delta^2}))$, respectively, in the\nhigh privacy regime.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 20:56:24 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2013 17:51:50 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2013 19:11:43 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Geng", "Quan", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1305.1347", "submitter": "David Witmer", "authors": "Anupam Gupta, Kunal Talwar, David Witmer", "title": "Sparsest Cut on Bounded Treewidth Graphs: Algorithms and Hardness\n  Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a 2-approximation algorithm for Non-Uniform Sparsest Cut that runs in\ntime $n^{O(k)}$, where $k$ is the treewidth of the graph. This improves on the\nprevious $2^{2^k}$-approximation in time $\\poly(n) 2^{O(k)}$ due to\nChlamt\\'a\\v{c} et al.\n  To complement this algorithm, we show the following hardness results: If the\nNon-Uniform Sparsest Cut problem has a $\\rho$-approximation for series-parallel\ngraphs (where $\\rho \\geq 1$), then the Max Cut problem has an algorithm with\napproximation factor arbitrarily close to $1/\\rho$. Hence, even for such\nrestricted graphs (which have treewidth 2), the Sparsest Cut problem is NP-hard\nto approximate better than $17/16 - \\epsilon$ for $\\epsilon > 0$; assuming the\nUnique Games Conjecture the hardness becomes $1/\\alpha_{GW} - \\epsilon$. For\ngraphs with large (but constant) treewidth, we show a hardness result of $2 -\n\\epsilon$ assuming the Unique Games Conjecture.\n  Our algorithm rounds a linear program based on (a subset of) the\nSherali-Adams lift of the standard Sparsest Cut LP. We show that even for\ntreewidth-2 graphs, the LP has an integrality gap close to 2 even after\npolynomially many rounds of Sherali-Adams. Hence our approach cannot be\nimproved even on such restricted graphs without using a stronger relaxation.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 22:39:09 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Gupta", "Anupam", ""], ["Talwar", "Kunal", ""], ["Witmer", "David", ""]]}, {"id": "1305.1407", "submitter": "Aaron Schild", "authors": "Feng Pan and Aaron Schild", "title": "Interdiction Problems on Planar Graphs", "comments": "25 pages, 9 figures. Extended abstract in APPROX-RANDOM 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interdiction problems are leader-follower games in which the leader is\nallowed to delete a certain number of edges from the graph in order to\nmaximally impede the follower, who is trying to solve an optimization problem\non the impeded graph. We introduce approximation algorithms and strong\nNP-completeness results for interdiction problems on planar graphs. We give a\nmultiplicative $(1 + \\epsilon)$-approximation for the maximum matching\ninterdiction problem on weighted planar graphs. The algorithm runs in\npseudo-polynomial time for each fixed $\\epsilon > 0$. We also show that\nweighted maximum matching interdiction, budget-constrained flow improvement,\ndirected shortest path interdiction, and minimum perfect matching interdiction\nare strongly NP-complete on planar graphs. To our knowledge, our\nbudget-constrained flow improvement result is the first planar NP-completeness\nproof that uses a one-vertex crossing gadget.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 04:53:45 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 04:09:07 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Pan", "Feng", ""], ["Schild", "Aaron", ""]]}, {"id": "1305.1535", "submitter": "Alexander Shen", "authors": "Andrei Rumyantsev, Alexander Shen", "title": "Probabilistic Constructions of Computable Objects and a Computable\n  Version of Lov\\'asz Local Lemma", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1012.0557", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonconstructive proof can be used to prove the existence of an object with\nsome properties without providing an explicit example of such an object. A\nspecial case is a probabilistic proof where we show that an object with\nrequired properties appears with some positive probability in some random\nprocess. Can we use such arguments to prove the existence of a computable\ninfinite object? Sometimes yes: following [8], we show how the notion of a\nlayerwise computable mapping can be used to prove a computable version of\nLov\\'asz local lemma. (A survey of Moser-Tardos proof is included to make the\npaper self-contained.)\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 14:23:30 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2013 12:05:40 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Rumyantsev", "Andrei", ""], ["Shen", "Alexander", ""]]}, {"id": "1305.1681", "submitter": "Yury Makarychev", "authors": "Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan", "title": "Bilu-Linial Stable Instances of Max Cut and Minimum Multiway Cut", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the notion of stability proposed by Bilu and Linial. We obtain\nan exact polynomial-time algorithm for $\\gamma$-stable Max Cut instances with\n$\\gamma \\geq c\\sqrt{\\log n}\\log\\log n$ for some absolute constant $c > 0$. Our\nalgorithm is robust: it never returns an incorrect answer; if the instance is\n$\\gamma$-stable, it finds the maximum cut, otherwise, it either finds the\nmaximum cut or certifies that the instance is not $\\gamma$-stable. We prove\nthat there is no robust polynomial-time algorithm for $\\gamma$-stable instances\nof Max Cut when $\\gamma < \\alpha_{SC}(n/2)$, where $\\alpha_{SC}$ is the best\napproximation factor for Sparsest Cut with non-uniform demands.\n  Our algorithm is based on semidefinite programming. We show that the standard\nSDP relaxation for Max Cut (with $\\ell_2^2$ triangle inequalities) is integral\nif $\\gamma \\geq D_{\\ell_2^2\\to \\ell_1}(n)$, where $D_{\\ell_2^2\\to \\ell_1}(n)$\nis the least distortion with which every $n$ point metric space of negative\ntype embeds into $\\ell_1$. On the negative side, we show that the SDP\nrelaxation is not integral when $\\gamma < D_{\\ell_2^2\\to \\ell_1}(n/2)$.\nMoreover, there is no tractable convex relaxation for $\\gamma$-stable instances\nof Max Cut when $\\gamma < \\alpha_{SC}(n/2)$. That suggests that solving\n$\\gamma$-stable instances with $\\gamma =o(\\sqrt{\\log n})$ might be difficult or\nimpossible.\n  Our results significantly improve previously known results. The best\npreviously known algorithm for $\\gamma$-stable instances of Max Cut required\nthat $\\gamma \\geq c\\sqrt{n}$ (for some $c > 0$) [Bilu, Daniely, Linial, and\nSaks]. No hardness results were known for the problem. Additionally, we present\nan algorithm for 4-stable instances of Minimum Multiway Cut. We also study a\nrelaxed notion of weak stability.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 23:54:03 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2013 13:31:34 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2013 23:25:58 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1305.1694", "submitter": "Yajun Wang", "authors": "Yajun Wang and Sam Chiu-wai Wong", "title": "Online Vertex Cover and Matching: Beating the Greedy Algorithm", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explicitly study the online vertex cover problem, which is\na natural generalization of the well-studied ski-rental problem. In the online\nvertex cover problem, we are required to maintain a monotone vertex cover in a\ngraph whose vertices arrive online. When a vertex arrives, all its incident\nedges to previously arrived vertices are revealed to the algorithm. For\nbipartite graphs with the left vertices offline (i.e. all of the left vertices\narrive first before any right vertex), there are algorithms achieving the\noptimal competitive ratio of $\\frac{1}{1-1/e}\\approx 1.582$.\n  Our first result is a new optimal water-filling algorithm for this case. One\nmajor ingredient of our result is a new charging-based analysis, which can be\ngeneralized to attack the online fractional vertex cover problem in general\ngraphs. The main contribution of this paper is a 1.901-competitive algorithm\nfor this problem. When the underlying graph is bipartite, our fractional\nsolution can be rounded to an integral solution. In other words, we can obtain\na vertex cover with expected size at most 1.901 of the optimal vertex cover in\nbipartite graphs.\n  The next major result is a primal-dual analysis of our algorithm for the\nonline fractional vertex cover problem in general graphs, which implies the\ndual result of a 0.526-competitive algorithm for online fractional matching in\ngeneral graphs. Notice that both problems admit a well-known 2-competitive\ngreedy algorithm. Our result in this paper is the first successful attempt to\nbeat the greedy algorithm for these two problems.\n  On the hardness side, we show that no randomized online algorithm can achieve\na competitive ratio better than 1.753 and 0.625 for the online fractional\nvertex cover problem and the online fractional matching problem respectively,\neven for bipartite graphs.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 02:18:28 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Wang", "Yajun", ""], ["Wong", "Sam Chiu-wai", ""]]}, {"id": "1305.1744", "submitter": "Joong Chae Na", "authors": "Joong Chae Na, Heejin Park, Maxime Crochemore, Jan Holub, Costas S.\n  Iliopoulos, Laurent Mouchard, and Kunsoo Park", "title": "Suffix Tree of Alignment: An Efficient Index for Similar Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an index data structure for similar strings. The generalized\nsuffix tree can be a solution for this. The generalized suffix tree of two\nstrings $A$ and $B$ is a compacted trie representing all suffixes in $A$ and\n$B$. It has $|A|+|B|$ leaves and can be constructed in $O(|A|+|B|)$ time.\nHowever, if the two strings are similar, the generalized suffix tree is not\nefficient because it does not exploit the similarity which is usually\nrepresented as an alignment of $A$ and $B$.\n  In this paper we propose a space/time-efficient suffix tree of alignment\nwhich wisely exploits the similarity in an alignment. Our suffix tree for an\nalignment of $A$ and $B$ has $|A| + l_d + l_1$ leaves where $l_d$ is the sum of\nthe lengths of all parts of $B$ different from $A$ and $l_1$ is the sum of the\nlengths of some common parts of $A$ and $B$. We did not compromise the pattern\nsearch to reduce the space. Our suffix tree can be searched for a pattern $P$\nin $O(|P|+occ)$ time where $occ$ is the number of occurrences of $P$ in $A$ and\n$B$. We also present an efficient algorithm to construct the suffix tree of\nalignment. When the suffix tree is constructed from scratch, the algorithm\nrequires $O(|A| + l_d + l_1 + l_2)$ time where $l_2$ is the sum of the lengths\nof other common substrings of $A$ and $B$. When the suffix tree of $A$ is\nalready given, it requires $O(l_d + l_1 + l_2)$ time.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 08:36:07 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Na", "Joong Chae", ""], ["Park", "Heejin", ""], ["Crochemore", "Maxime", ""], ["Holub", "Jan", ""], ["Iliopoulos", "Costas S.", ""], ["Mouchard", "Laurent", ""], ["Park", "Kunsoo", ""]]}, {"id": "1305.1880", "submitter": "Hebert P\\'erez-Ros\\'es PhD", "authors": "Fran\\c{c}ois Bertault, Mirka Miller, Hebert P\\'erez-Ros\\'es, Ramiro\n  Feria-Puron, Elaheh Vaezpour", "title": "A Heuristic for Magic and Antimagic Graph Labellings", "comments": null, "journal-ref": "Proceedings of the VII Spanish Congress on Metaheuristics, and\n  Evolutive and Bioinspired Algorithms (MAEB 2010). V.Campos, A.Duarte,\n  M.Gallego, F.Cortazar, R.Marti (eds). Ibergarceta Publicaciones, S.L.,\n  Madrid. pp. 677--684", "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph labellings have been a very fruitful area of research in the last four\ndecades. However, despite the staggering number of papers published in the\nfield (over 1000), few general results are available, and most papers deal with\nparticular classes of graphs and methods. Here we approach the problem from the\ncomputational viewpoint, and in a quite general way. We present the existence\nproblem of a particular labelling as a combinatorial optimization problem, then\nwe discuss the possible strategies to solve it, and finally we present a\nheuristic for finding different classes of labellings, like vertex-, edge-, or\nface-magic, and $(a, d)$-antimagic $(v, e, f)$-labellings. The algorithm has\nbeen implemented in C++ and MATLAB, and with its aid we have been able to\nderive new results for some classes of graphs, in particular, vertex-antimagic\nedge labellings for small graphs of the type $P_2^r \\times P_3^s$, for which no\ngeneral construction is known so far.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 16:51:03 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Bertault", "Fran\u00e7ois", ""], ["Miller", "Mirka", ""], ["P\u00e9rez-Ros\u00e9s", "Hebert", ""], ["Feria-Puron", "Ramiro", ""], ["Vaezpour", "Elaheh", ""]]}, {"id": "1305.1922", "submitter": "Aaron Sidford", "authors": "Yin Tat Lee and Aaron Sidford", "title": "Efficient Accelerated Coordinate Descent Methods and Faster Algorithms\n  for Solving Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to accelerate randomized coordinate descent methods\nand achieve faster convergence rates without paying per-iteration costs in\nasymptotic running time. In particular, we show how to generalize and\nefficiently implement a method proposed by Nesterov, giving faster asymptotic\nrunning times for various algorithms that use standard coordinate descent as a\nblack box. In addition to providing a proof of convergence for this new general\nmethod, we show that it is numerically stable, efficiently implementable, and\nin certain regimes, asymptotically optimal.\n  To highlight the computational power of this algorithm, we show how it can\nused to create faster linear system solvers in several regimes:\n  - We show how this method achieves a faster asymptotic runtime than conjugate\ngradient for solving a broad class of symmetric positive definite systems of\nequations.\n  - We improve the best known asymptotic convergence guarantees for Kaczmarz\nmethods, a popular technique for image reconstruction and solving\noverdetermined systems of equations, by accelerating a randomized algorithm of\nStrohmer and Vershynin.\n  - We achieve the best known running time for solving Symmetric Diagonally\nDominant (SDD) system of equations in the unit-cost RAM model, obtaining an O(m\nlog^{3/2} n (log log n)^{1/2} log (log n / eps)) asymptotic running time by\naccelerating a recent solver by Kelner et al.\n  Beyond the independent interest of these solvers, we believe they highlight\nthe versatility of the approach of this paper and we hope that they will open\nthe door for further algorithmic improvements in the future.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 19:41:00 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""]]}, {"id": "1305.1961", "submitter": "Jonathan Yedidia Dr.", "authors": "Nate Derbinsky, Jos\\'e Bento, Veit Elser, and Jonathan S. Yedidia", "title": "An Improved Three-Weight Message-Passing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS math.OC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe how the powerful \"Divide and Concur\" algorithm for constraint\nsatisfaction can be derived as a special case of a message-passing version of\nthe Alternating Direction Method of Multipliers (ADMM) algorithm for convex\noptimization, and introduce an improved message-passing algorithm based on\nADMM/DC by introducing three distinct weights for messages, with \"certain\" and\n\"no opinion\" weights, as well as the standard weight used in ADMM/DC. The\n\"certain\" messages allow our improved algorithm to implement constraint\npropagation as a special case, while the \"no opinion\" messages speed\nconvergence for some problems by making the algorithm focus only on active\nconstraints. We describe how our three-weight version of ADMM/DC can give\ngreatly improved performance for non-convex problems such as circle packing and\nsolving large Sudoku puzzles, while retaining the exact performance of ADMM for\nconvex problems. We also describe the advantages of our algorithm compared to\nother message-passing algorithms based upon belief propagation.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 21:24:14 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Derbinsky", "Nate", ""], ["Bento", "Jos\u00e9", ""], ["Elser", "Veit", ""], ["Yedidia", "Jonathan S.", ""]]}, {"id": "1305.2108", "submitter": "Shahin Kamali", "authors": "Sushmita Gupta and Shahin Kamali and Alejandro L\\'opez-Ortiz", "title": "On Advice Complexity of the k-server Problem under Sparse Metrics", "comments": "16 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the k-server problem under the advice model of computation when\nthe underlying metric space is sparse. On one side, we show that an advice of\nsize {\\Omega}(n) is required to obtain a 1-competitive algorithm for sequences\nof size n, even for the 2-server problem on a path metric of size N >= 5.\nThrough another lower bound argument, we show that at least (n/2)(log {\\alpha}\n- 1.22) bits of advice is required to obtain an optimal solution for metric\nspaces of treewidth {\\alpha}, where 4 <= {\\alpha} < 2k. On the other side, we\nintroduce {\\Theta}(1)-competitive algorithms for a wide range of sparse graphs,\nwhich require advice of (almost) linear size. Namely, we show that for graphs\nof size N and treewidth {\\alpha}, there is an online algorithm which receives\n$O(n (log {\\alpha} + log log N))$ bits of advice and optimally serves a\nsequence of length n. With a different argument, we show that if a graph admits\na system of {\\mu} collective tree (q,r)-spanners, then there is a\n(q+r)-competitive algorithm which receives O(n (log {\\mu} + log log N)) bits of\nadvice. Among other results, this gives a 3-competitive algorithm for planar\ngraphs, provided with O(n log log N) bits of advice.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 14:40:06 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Gupta", "Sushmita", ""], ["Kamali", "Shahin", ""], ["L\u00f3pez-Ortiz", "Alejandro", ""]]}, {"id": "1305.2540", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov, Mikhail Rubinchik, Arseny M. Shur", "title": "Finding Distinct Subpalindromes Online", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit an online algorithm finding all distinct palindromes inside a\ngiven string in time $\\Theta(n\\log|\\Sigma|)$ over an ordered alphabet and in\ntime $\\Theta(n|\\Sigma|)$ over an unordered alphabet. Using a reduction from a\ndictionary-like data structure, we prove the optimality of this algorithm in\nthe comparison-based computation model.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 20:33:09 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kosolobov", "Dmitry", ""], ["Rubinchik", "Mikhail", ""], ["Shur", "Arseny M.", ""]]}, {"id": "1305.2545", "submitter": "Aleksandrs Slivkins", "authors": "Ashwinkumar Badanidiyuru, Robert Kleinberg and Aleksandrs Slivkins", "title": "Bandits with Knapsacks", "comments": "An extended abstract of this work has appeared in the 54th IEEE\n  Symposium on Foundations of Computer Science (FOCS 2013). 55 pages. Compared\n  to the initial \"full version\" from May'13, this version has a significantly\n  revised presentation and reflects the current status of the follow-up work.\n  Also, this version contains a stronger regret bound in one of the main\n  results", "journal-ref": null, "doi": "10.1109/FOCS.2013.30", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandit problems are the predominant theoretical model of\nexploration-exploitation tradeoffs in learning, and they have countless\napplications ranging from medical trials, to communication networks, to Web\nsearch and advertising. In many of these application domains the learner may be\nconstrained by one or more supply (or budget) limits, in addition to the\ncustomary limitation on the time horizon. The literature lacks a general model\nencompassing these sorts of problems. We introduce such a model, called\n\"bandits with knapsacks\", that combines aspects of stochastic integer\nprogramming with online learning. A distinctive feature of our problem, in\ncomparison to the existing regret-minimization literature, is that the optimal\npolicy for a given latent distribution may significantly outperform the policy\nthat plays the optimal fixed arm. Consequently, achieving sublinear regret in\nthe bandits-with-knapsacks problem is significantly more challenging than in\nconventional bandit problems.\n  We present two algorithms whose reward is close to the information-theoretic\noptimum: one is based on a novel \"balanced exploration\" paradigm, while the\nother is a primal-dual algorithm that uses multiplicative updates. Further, we\nprove that the regret achieved by both algorithms is optimal up to\npolylogarithmic factors. We illustrate the generality of the problem by\npresenting applications in a number of different domains including electronic\ncommerce, routing, and scheduling. As one example of a concrete application, we\nconsider the problem of dynamic posted pricing with limited supply and obtain\nthe first algorithm whose regret, with respect to the optimal dynamic policy,\nis sublinear in the supply.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 21:50:46 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2013 21:13:51 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2013 13:52:46 GMT"}, {"version": "v4", "created": "Mon, 20 Oct 2014 19:24:09 GMT"}, {"version": "v5", "created": "Wed, 24 Jun 2015 16:43:30 GMT"}, {"version": "v6", "created": "Fri, 31 Jul 2015 18:36:51 GMT"}, {"version": "v7", "created": "Sat, 17 Jun 2017 18:59:24 GMT"}, {"version": "v8", "created": "Tue, 5 Sep 2017 14:00:33 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Badanidiyuru", "Ashwinkumar", ""], ["Kleinberg", "Robert", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1305.2636", "submitter": "Mark Korenblit", "authors": "Mark Korenblit", "title": "Full Square Rhomboids and Their Algebraic Expressions", "comments": "13 pages, 10 figures. arXiv admin note: substantial text overlap with\n  arXiv:1211.1661", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates relationship between algebraic expressions and graphs.\nWe consider a digraph called a full square rhomboid that is an example of\nnon-series-parallel graphs. Our intention is to simplify the expressions of\nfull square rhomboids and eventually find their shortest representations. With\nthat end in view, we describe two decomposition methods for generating\nexpressions of full square rhomboids and carry out their comparative analysis.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 21:30:50 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Korenblit", "Mark", ""]]}, {"id": "1305.2645", "submitter": "Mark Korenblit", "authors": "Mark Korenblit and Vadim E. Levit", "title": "On the Optimal Representation of Algebraic Expressions of Fibonacci\n  Graphs", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates relationship between algebraic expressions and graphs.\nWe consider a digraph called a Fibonacci graph which gives a generic example of\nnon-series-parallel graphs. Our intention in this paper is to simplify the\nexpressions of Fibonacci graphs and eventually find their shortest\nrepresentations. With that end in view, we describe the optimal decomposition\nmethod for generating Fibonacci graph expressions that is conjectured to\nprovide these representations. Proof (or disproof) of this conjecture is\npresented as an open problem.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 23:26:56 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Korenblit", "Mark", ""], ["Levit", "Vadim E.", ""]]}, {"id": "1305.2647", "submitter": "Mark Korenblit", "authors": "Mark Korenblit and Vadim E. Levit", "title": "Fibonacci Graphs and their Expressions", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates relationship between algebraic expressions and graphs.\nWe consider a digraph called a Fibonacci graph which gives a generic example of\nnon-series-parallel graphs. Our intention in this paper is to simplify the\nexpressions of Fibonacci graphs and eventually find their shortest\nrepresentations. With that end in view, we describe the number of methods for\ngenerating Fibonacci graph expressions and carry out their comparative\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 00:06:38 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Korenblit", "Mark", ""], ["Levit", "Vadim E.", ""]]}, {"id": "1305.2743", "submitter": "Sylvain Guillemot", "authors": "Sylvain Guillemot and D\\'aniel Marx", "title": "A faster FPT algorithm for Bipartite Contraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\textsc{Bipartite Contraction} problem is to decide, given a graph $G$\nand a parameter $k$, whether we can can obtain a bipartite graph from $G$ by at\nmost $k$ edge contractions. The fixed-parameter tractability of the problem was\nshown by [Heggernes et al. 2011], with an algorithm whose running time has\ndouble-exponential dependence on $k$. We present a new randomized FPT algorithm\nfor the problem, which is both conceptually simpler and achieves an improved\n$2^{O(k^2)} n m$ running time, i.e., avoiding the double-exponential dependence\non $k$. The algorithm can be derandomized using standard techniques.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 11:25:01 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2013 12:35:14 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2013 14:46:45 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Guillemot", "Sylvain", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1305.2772", "submitter": "Marc Gille", "authors": "Marc Gill\\'e", "title": "OBDD-Based Representation of Interval Graphs", "comments": "29 pages, accepted for 39th International Workshop on Graph-Theoretic\n  Concepts 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G = (V,E)$ can be described by the characteristic function of the\nedge set $\\chi_E$ which maps a pair of binary encoded nodes to 1 iff the nodes\nare adjacent. Using \\emph{Ordered Binary Decision Diagrams} (OBDDs) to store\n$\\chi_E$ can lead to a (hopefully) compact representation. Given the OBDD as an\ninput, symbolic/implicit OBDD-based graph algorithms can solve optimization\nproblems by mainly using functional operations, e.g. quantification or binary\nsynthesis. While the OBDD representation size can not be small in general, it\ncan be provable small for special graph classes and then also lead to fast\nalgorithms. In this paper, we show that the OBDD size of unit interval graphs\nis $O(\\ | V \\ | /\\log \\ | V \\ |)$ and the OBDD size of interval graphs is $O(\\\n| V \\ | \\log \\ | V \\ |)$ which both improve a known result from Nunkesser and\nWoelfel (2009). Furthermore, we can show that using our variable order and node\nlabeling for interval graphs the worst-case OBDD size is $\\Omega(\\ | V \\ | \\log\n\\ | V \\ |)$. We use the structure of the adjacency matrices to prove these\nbounds. This method may be of independent interest and can be applied to other\ngraph classes. We also develop a maximum matching algorithm on unit interval\ngraphs using $O(\\log \\ | V \\ |)$ operations and a coloring algorithm for unit\nand general intervals graphs using $O(\\log^2 \\ | V \\ |)$ operations and\nevaluate the algorithms empirically.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 13:26:25 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Gill\u00e9", "Marc", ""]]}, {"id": "1305.2777", "submitter": "S{\\o}ren Vind", "authors": "Philip Bille, Patrick Hagge Cording, Inge Li G{\\o}rtz, Benjamin Sach,\n  Hjalte Wedel Vildh{\\o}j, S{\\o}ren Vind", "title": "Fingerprints in Compressed Strings", "comments": "An extended abstract of this paper will appear at WADS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Karp-Rabin fingerprint of a string is a type of hash value that due to\nits strong properties has been used in many string algorithms. In this paper we\nshow how to construct a data structure for a string $S$ of size $N$ compressed\nby a context-free grammar of size $n$ that answers fingerprint queries. That\nis, given indices $i$ and $j$, the answer to a query is the fingerprint of the\nsubstring $S[i,j]$. We present the first O(n) space data structures that answer\nfingerprint queries without decompressing any characters. For Straight Line\nPrograms (SLP) we get $O(\\log N)$ query time, and for Linear SLPs (an SLP\nderivative that captures LZ78 compression and its variations) we get $O(\\log\n\\log N)$ query time. Hence, our data structures has the same time and space\ncomplexity as for random access in SLPs. We utilize the fingerprint data\nstructures to solve the longest common extension problem in query time $O(\\log\nN \\log \\lce)$ and $O(\\log \\lce \\log\\log \\lce + \\log\\log N)$ for SLPs and Linear\nSLPs, respectively. Here, $\\lce$ denotes the length of the LCE.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 13:50:11 GMT"}, {"version": "v2", "created": "Thu, 16 May 2013 12:32:19 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Bille", "Philip", ""], ["Cording", "Patrick Hagge", ""], ["G\u00f8rtz", "Inge Li", ""], ["Sach", "Benjamin", ""], ["Vildh\u00f8j", "Hjalte Wedel", ""], ["Vind", "S\u00f8ren", ""]]}, {"id": "1305.2835", "submitter": "Kostas Tsichlas", "authors": "Andreas Kosmatopoulos and Kostas Tsichlas", "title": "Dynamic Top-$k$ Dominating Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{S}$ be a dataset of $n$ 2-dimensional points. The top-$k$\ndominating query aims to report the $k$ points that dominate the most points in\n$\\mathcal{S}$. A point $p$ dominates a point $q$ iff all coordinates of $p$ are\nsmaller than or equal to those of $q$ and at least one of them is strictly\nsmaller. The top-$k$ dominating query combines the dominance concept of maxima\nqueries with the ranking function of top-$k$ queries and can be used as an\nimportant tool in multi-criteria decision making systems. In this work, we\npropose novel algorithms for answering semi-dynamic (insertions only) and fully\ndynamic (insertions and deletions) top-$k$ dominating queries. To the best of\nour knowledge, this is the first work towards handling (semi-)dynamic top-$k$\ndominating queries that offers algorithms with asymptotic guarantees regarding\ntheir time and space cost.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 16:30:11 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kosmatopoulos", "Andreas", ""], ["Tsichlas", "Kostas", ""]]}, {"id": "1305.3102", "submitter": "Bart M. P. Jansen", "authors": "Michael R. Fellows and Bart M. P. Jansen", "title": "FPT is Characterized by Useful Obstruction Sets", "comments": "Extended abstract with appendix, as accepted to WG 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graph problems were first shown to be fixed-parameter tractable using\nthe results of Robertson and Seymour on graph minors. We show that the\ncombination of finite, computable, obstruction sets and efficient order tests\nis not just one way of obtaining strongly uniform FPT algorithms, but that all\nof FPT may be captured in this way. Our new characterization of FPT has a\nstrong connection to the theory of kernelization, as we prove that problems\nwith polynomial kernels can be characterized by obstruction sets whose elements\nhave polynomial size. Consequently we investigate the interplay between the\nsizes of problem kernels and the sizes of the elements of such obstruction\nsets, obtaining several examples of how results in one area yield new insights\nin the other. We show how exponential-size minor-minimal obstructions for\npathwidth k form the crucial ingredient in a novel OR-cross-composition for\nk-Pathwidth, complementing the trivial AND-composition that is known for this\nproblem. In the other direction, we show that OR-cross-compositions into a\nparameterized problem can be used to rule out the existence of efficiently\ngenerated quasi-orders on its instances that characterize the NO-instances by\npolynomial-size obstructions.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 10:43:00 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Fellows", "Michael R.", ""], ["Jansen", "Bart M. P.", ""]]}, {"id": "1305.3164", "submitter": "Travis Gagie", "authors": "Travis Gagie, Pawe{\\l} Gawrychowski and Yakov Nekrich", "title": "Heaviest Induced Ancestors and Longest Common Substrings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we have two trees on the same set of leaves, in which nodes are\nweighted such that children are heavier than their parents. We say a node from\nthe first tree and a node from the second tree are induced together if they\nhave a common leaf descendant. In this paper we describe data structures that\nefficiently support the following heaviest-induced-ancestor query: given a node\nfrom the first tree and a node from the second tree, find an induced pair of\ntheir ancestors with maximum combined weight. Our solutions are based on a\ngeometric interpretation that enables us to find heaviest induced ancestors\nusing range queries. We then show how to use these results to build an\nLZ-compressed index with which we can quickly find with high probability a\nlongest substring common to the indexed string and a given pattern.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 14:24:42 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2013 23:33:37 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Gagie", "Travis", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1305.3207", "submitter": "Ilias Diakonikolas", "authors": "Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun", "title": "Efficient Density Estimation via Piecewise Polynomial Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a highly efficient \"semi-agnostic\" algorithm for learning univariate\nprobability distributions that are well approximated by piecewise polynomial\ndensity functions. Let $p$ be an arbitrary distribution over an interval $I$\nwhich is $\\tau$-close (in total variation distance) to an unknown probability\ndistribution $q$ that is defined by an unknown partition of $I$ into $t$\nintervals and $t$ unknown degree-$d$ polynomials specifying $q$ over each of\nthe intervals. We give an algorithm that draws $\\tilde{O}(t\\new{(d+1)}/\\eps^2)$\nsamples from $p$, runs in time $\\poly(t,d,1/\\eps)$, and with high probability\noutputs a piecewise polynomial hypothesis distribution $h$ that is\n$(O(\\tau)+\\eps)$-close (in total variation distance) to $p$. This sample\ncomplexity is essentially optimal; we show that even for $\\tau=0$, any\nalgorithm that learns an unknown $t$-piecewise degree-$d$ probability\ndistribution over $I$ to accuracy $\\eps$ must use $\\Omega({\\frac {t(d+1)}\n{\\poly(1 + \\log(d+1))}} \\cdot {\\frac 1 {\\eps^2}})$ samples from the\ndistribution, regardless of its running time. Our algorithm combines tools from\napproximation theory, uniform convergence, linear programming, and dynamic\nprogramming.\n  We apply this general algorithm to obtain a wide range of results for many\nnatural problems in density estimation over both continuous and discrete\ndomains. These include state-of-the-art results for learning mixtures of\nlog-concave distributions; mixtures of $t$-modal distributions; mixtures of\nMonotone Hazard Rate distributions; mixtures of Poisson Binomial Distributions;\nmixtures of Gaussians; and mixtures of $k$-monotone densities. Our general\ntechnique yields computationally efficient algorithms for all these problems,\nin many cases with provably optimal sample complexities (up to logarithmic\nfactors) in all parameters.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 16:54:10 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Chan", "Siu-On", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1305.3314", "submitter": "Shiri Chechik", "authors": "Shiri Chechik", "title": "Approximate Distance Oracle with Constant Query Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approximate distance oracle is a succinct data structure that provides\nfast answers to distance queries between any two nodes. In this paper we\nconsider approximate distance oracles for general undirected graphs with\nnon-negative edge weights with constant query time. We present a distance\noracle of size O(k n^{1+1/k}), with 2k-1 stretch and O(1) query time. This\nimproves the O(log{k}) query time of Wulff-Nilsen's distance oracle [SODA '13],\nwhich in turn improved the O(k) query time of Thorup and Zwick's distance\noracle [J. ACM '05].\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 22:09:01 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Chechik", "Shiri", ""]]}, {"id": "1305.3584", "submitter": "David Kordalewski David Kordalewski", "authors": "David Kordalewski", "title": "New Greedy Heuristics For Set Cover and Set Packing", "comments": "49 pages, 8 figures, submitted for M.Sc. degree", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Set Cover problem (SCP) and Set Packing problem (SPP) are standard\nNP-hard combinatorial optimization problems. Their decision problem versions\nare shown to be NP-Complete in Karp's 1972 paper. We specify a rough guide to\nconstructing approximation heuristics that may have widespread applications and\napply it to devise greedy approximation algorithms for SCP and SPP, where the\nselection heuristic is a variation of that in the standard greedy approximation\nalgorithm. Our technique involves assigning to each input set a valuation and\nthen selecting, in each round, the set whose valuation is highest. We prove\nthat the technique we use for determining a valuation of the input sets yields\na unique value for all Set Cover instances. For both SCP and SPP we give\nexperimental evidence that the valuations we specify are unique and can be\ncomputed to high precision quickly by an iterative algorithm. Others have\nexperimented with testing the observed approximation ratio of various\nalgorithms over a variety of randomly generated instances, and we have\nextensive experimental evidence to show the quality of the new algorithm\nrelative to greedy heuristics in common use. Our algorithms are somewhat more\ncomputationally intensive than the standard heuristics, though they are still\npractical for large instances. We discuss some ways to speed up our algorithms\nthat do not significantly distort their effectiveness in practice on random\ninstances.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 18:44:39 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Kordalewski", "David", ""]]}, {"id": "1305.3616", "submitter": "Manuel Gomez Rodriguez", "authors": "Manuel Gomez Rodriguez, Jure Leskovec, Bernhard Schoelkopf", "title": "Modeling Information Propagation with Survival Theory", "comments": "To appear at ICML '13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks provide a skeleton for the spread of contagions, like, information,\nideas, behaviors and diseases. Many times networks over which contagions\ndiffuse are unobserved and need to be inferred. Here we apply survival theory\nto develop general additive and multiplicative risk models under which the\nnetwork inference problems can be solved efficiently by exploiting their\nconvexity. Our additive risk model generalizes several existing network\ninference models. We show all these models are particular cases of our more\ngeneral model. Our multiplicative model allows for modeling scenarios in which\na node can either increase or decrease the risk of activation of another node,\nin contrast with previous approaches, which consider only positive risk\nincrements. We evaluate the performance of our network inference algorithms on\nlarge synthetic and real cascade datasets, and show that our models are able to\npredict the length and duration of cascades in real data.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 20:01:06 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Rodriguez", "Manuel Gomez", ""], ["Leskovec", "Jure", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1305.3688", "submitter": "Jianhang Gao", "authors": "Jianhang Gao, Qing Zhao, Ananthram Swami", "title": "The Thinnest Path Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and study the thinnest path problem in wireless ad hoc networks.\nThe objective is to find a path from a source to its destination that results\nin the minimum number of nodes overhearing the message by a judicious choice of\nrelaying nodes and their corresponding transmission power. We adopt a directed\nhypergraph model of the problem and establish the NP-completeness of the\nproblem in 2-D networks. We then develop two polynomial-time approximation\nalgorithms that offer $\\sqrt{\\frac{n}{2}}$ and $\\frac{n}{2\\sqrt{n-1}}$\napproximation ratios for general directed hypergraphs (which can model\nnon-isomorphic signal propagation in space) and constant approximation ratios\nfor ring hypergraphs (which result from isomorphic signal propagation). We also\nconsider the thinnest path problem in 1-D networks and 1-D networks embedded in\n2-D field of eavesdroppers with arbitrary unknown locations (the so-called\n1.5-D networks). We propose a linear-complexity algorithm based on nested\nbackward induction that obtains the optimal solution to both 1-D and 1.5-D\nnetworks. This algorithm does not require the knowledge of eavesdropper\nlocations and achieves the best performance offered by any algorithm that\nassumes complete location information of the eavesdroppers.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 06:17:51 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2013 21:55:50 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Gao", "Jianhang", ""], ["Zhao", "Qing", ""], ["Swami", "Ananthram", ""]]}, {"id": "1305.3735", "submitter": "Andr\\'e Nichterlein", "authors": "Sepp Hartung, Christian Komusiewicz, Andr\\'e Nichterlein, Ondrej\n  Such\\'y", "title": "On Structural Parameterizations for the 2-Club Problem", "comments": "An extended abstract of this paper appeared in Proceedings of the\n  39th International Conference on Current Trends in Theory and Practice of\n  Computer Science (SOFSEM'13), Jan. 2013, volume 7741 of LNCS, pages 233-243,\n  Springer, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NP-hard 2-Club problem is, given an undirected graph G=(V,E) and l\\in N,\nto decide whether there is a vertex set S\\subseteq V of size at least l such\nthat the induced subgraph G[S] has diameter at most two. We make progress\ntowards a systematic classification of the complexity of 2-Club with respect to\na hierarchy of prominent structural graph parameters. First, we present the\nfollowing tight NP-hardness results: 2-Club is NP-hard on graphs that become\nbipartite by deleting one vertex, on graphs that can be covered by three\ncliques, and on graphs with domination number two and diameter three. Then, we\nconsider the parameter h-index of the input graph. This parameter is motivated\nby real-world instances and the fact that 2-Club is fixed-parameter tractable\nwith respect to the larger parameter maximum degree. We present an algorithm\nthat solves 2-Club in |V|^{f(k)} time with k being the h-index. By showing\nW[1]-hardness for this parameter, we provide evidence that the above algorithm\ncannot be improved to a fixed-parameter algorithm. Furthermore, the reduction\nused for this hardness result can be modified to show that 2-Club is NP-hard if\nthe input graph has constant degeneracy. Finally, we show that 2-Club is\nfixed-parameter tractable with respect to distance to cographs.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 09:34:03 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Hartung", "Sepp", ""], ["Komusiewicz", "Christian", ""], ["Nichterlein", "Andr\u00e9", ""], ["Such\u00fd", "Ondrej", ""]]}, {"id": "1305.3827", "submitter": "Zahra Jafargholi", "authors": "Zahra Jafargholi and Emanuele Viola", "title": "3SUM, 3XOR, Triangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that if one can solve 3SUM on a set of size n in time n^{1+\\e} then\none can list t triangles in a graph with m edges in time\nO(m^{1+\\e}t^{1/3-\\e/3}). This is a reversal of Patrascu's reduction from 3SUM\nto listing triangles (STOC '10). Our result builds on and extends works by the\nPaghs (PODS '06) and by Vassilevska and Williams (FOCS '10). We make our\nreductions deterministic using tools from pseudorandomness.\n  We then re-execute both Patrascu's reduction and our reversal for the variant\n3XOR of 3SUM where integer summation is replaced by bit-wise xor. As a\ncorollary we obtain that if 3XOR is solvable in linear time but 3SUM requires\nquadratic randomized time, or vice versa, then the randomized time complexity\nof listing m triangles in a graph with $m$ edges is m^{4/3} up to a factor\nm^\\alpha for any \\alpha > 0.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 14:50:07 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Jafargholi", "Zahra", ""], ["Viola", "Emanuele", ""]]}, {"id": "1305.3828", "submitter": "Francesco Silvestri", "authors": "Lorenzo De Stefani and Francesco Silvestri", "title": "Exploiting non-constant safe memory in resilient algorithms and data\n  structures", "comments": "To appear in Theoretical Computer Science, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Faulty RAM model by Finocchi and Italiano (2008) by adding a\nsafe memory of arbitrary size $S$, and we then derive tradeoffs between the\nperformance of resilient algorithmic techniques and the size of the safe\nmemory. Let $\\delta$ and $\\alpha$ denote, respectively, the maximum amount of\nfaults which can happen during the execution of an algorithm and the actual\nnumber of occurred faults, with $\\alpha \\leq \\delta$. We propose a resilient\nalgorithm for sorting $n$ entries which requires $O\\left(n\\log n+\\alpha\n(\\delta/S + \\log S)\\right)$ time and uses $\\Theta(S)$ safe memory words. Our\nalgorithm outperforms previous resilient sorting algorithms which do not\nexploit the available safe memory and require $O\\left(n\\log n+\n\\alpha\\delta\\right)$ time. Finally, we exploit our sorting algorithm for\nderiving a resilient priority queue. Our implementation uses $\\Theta(S)$ safe\nmemory words and $\\Theta(n)$ faulty memory words for storing $n$ keys, and\nrequires $O\\left(\\log n + \\delta/S\\right)$ amortized time for each insert and\ndeletemin operation. Our resilient priority queue improves the $O\\left(\\log n +\n\\delta\\right)$ amortized time required by the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 14:50:18 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 19:55:14 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["De Stefani", "Lorenzo", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1305.3869", "submitter": "Anna Blasiak", "authors": "Anna Blasiak", "title": "Multicut Lower Bounds via Network Coding", "comments": "6 pages, Netcod 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new technique to certify lower bounds on the multicut size\nusing network coding. In directed networks the network coding rate is not a\nlower bound on the multicut, but we identify a class of networks on which the\nrate is equal to the size of the minimum multicut and show this class is closed\nunder the strong graph product. We then show that the famous construction of\nSaks et al. that gives a $\\Theta(k)$ gap between the multicut and the\nmulticommodity flow rate is contained in this class. This allows us to apply\nour result to strengthen their multicut lower bound, determine the exact value\nof the minimum multicut, and give an optimal network coding solution with rate\nmatching the multicut.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 16:54:22 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Blasiak", "Anna", ""]]}, {"id": "1305.4000", "submitter": "Matt Weinberg", "authors": "Yang Cai, Constantinos Daskalakis, S. Matthew Weinberg", "title": "Reducing Revenue to Welfare Maximization: Approximation Algorithms and\n  other Generalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown in [http://arxiv.org/abs/1207.5518] that revenue\noptimization can be computationally efficiently reduced to welfare optimization\nin all multi-dimensional Bayesian auction problems with arbitrary (possibly\ncombinatorial) feasibility constraints and independent additive bidders with\narbitrary (possibly combinatorial) demand constraints. This reduction provides\na poly-time solution to the optimal mechanism design problem in all auction\nsettings where welfare optimization can be solved efficiently, but it is\nfragile to approximation and cannot provide solutions to settings where welfare\nmaximization can only be tractably approximated. In this paper, we extend the\nreduction to accommodate approximation algorithms, providing an approximation\npreserving reduction from (truthful) revenue maximization to (not necessarily\ntruthful) welfare maximization. The mechanisms output by our reduction choose\nallocations via black-box calls to welfare approximation on randomly selected\ninputs, thereby generalizing also our earlier structural results on optimal\nmulti-dimensional mechanisms to approximately optimal mechanisms. Unlike\n[http://arxiv.org/abs/1207.5518], our results here are obtained through novel\nuses of the Ellipsoid algorithm and other optimization techniques over {\\em\nnon-convex regions}.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 07:19:17 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Cai", "Yang", ""], ["Daskalakis", "Constantinos", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1305.4002", "submitter": "Matt Weinberg", "authors": "Yang Cai, Constantinos Daskalakis, S. Matthew Weinberg", "title": "Understanding Incentives: Mechanism Design becomes Algorithm Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a computationally efficient black-box reduction from mechanism\ndesign to algorithm design in very general settings. Specifically, we give an\napproximation-preserving reduction from truthfully maximizing \\emph{any}\nobjective under \\emph{arbitrary} feasibility constraints with \\emph{arbitrary}\nbidder types to (not necessarily truthfully) maximizing the same objective plus\nvirtual welfare (under the same feasibility constraints). Our reduction is\nbased on a fundamentally new approach: we describe a mechanism's behavior\nindirectly only in terms of the expected value it awards bidders for certain\nbehavior, and never directly access the allocation rule at all.\n  Applying our new approach to revenue, we exhibit settings where our reduction\nholds \\emph{both ways}. That is, we also provide an approximation-sensitive\nreduction from (non-truthfully) maximizing virtual welfare to (truthfully)\nmaximizing revenue, and therefore the two problems are computationally\nequivalent. With this equivalence in hand, we show that both problems are\nNP-hard to approximate within any polynomial factor, even for a single monotone\nsubmodular bidder.\n  We further demonstrate the applicability of our reduction by providing a\ntruthful mechanism maximizing fractional max-min fairness. This is the first\ninstance of a truthful mechanism that optimizes a non-linear objective.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 07:28:53 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Cai", "Yang", ""], ["Daskalakis", "Constantinos", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1305.4130", "submitter": "Andrew Gelfand", "authors": "Andrew Gelfand, Jinwoo Shin, Michael Chertkov", "title": "Belief Propagation for Linear Programming", "comments": "To appear in ISIT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief Propagation (BP) is a popular, distributed heuristic for performing\nMAP computations in Graphical Models. BP can be interpreted, from a variational\nperspective, as minimizing the Bethe Free Energy (BFE). BP can also be used to\nsolve a special class of Linear Programming (LP) problems. For this class of\nproblems, MAP inference can be stated as an integer LP with an LP relaxation\nthat coincides with minimization of the BFE at ``zero temperature\". We\ngeneralize these prior results and establish a tight characterization of the LP\nproblems that can be formulated as an equivalent LP relaxation of MAP\ninference. Moreover, we suggest an efficient, iterative annealing BP algorithm\nfor solving this broader class of LP problems. We demonstrate the algorithm's\nperformance on a set of weighted matching problems by using it as a cutting\nplane method to solve a sequence of LPs tightened by adding ``blossom''\ninequalities.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 16:40:43 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Gelfand", "Andrew", ""], ["Shin", "Jinwoo", ""], ["Chertkov", "Michael", ""]]}, {"id": "1305.4308", "submitter": "Alina Ene", "authors": "Alina Ene, Nitish Korula, Ali Vakilian", "title": "Connected Domatic Packings in Node-capacitated Graphs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of vertices in a graph is a dominating set if every vertex outside the\nset has a neighbor in the set. A dominating set is connected if the subgraph\ninduced by its vertices is connected. The connected domatic partition problem\nasks for a partition of the nodes into connected dominating sets. The connected\ndomatic number of a graph is the size of a largest connected domatic partition\nand it is a well-studied graph parameter with applications in the design of\nwireless networks. In this note, we consider the fractional counterpart of the\nconnected domatic partition problem in \\emph{node-capacitated} graphs. Let $n$\nbe the number of nodes in the graph and let $k$ be the minimum capacity of a\nnode separator in $G$. Fractionally we can pack at most $k$ connected\ndominating sets subject to the capacities on the nodes, and our algorithms\nconstruct packings whose sizes are proportional to $k$. Some of our main\ncontributions are the following: \\begin{itemize} \\item An algorithm for\nconstructing a fractional connected domatic packing of size $\\Omega(k)$ for\nnode-capacitated planar and minor-closed families of graphs. \\item An algorithm\nfor constructing a fractional connected domatic packing of size $\\Omega(k /\n\\ln{n})$ for node-capacitated general graphs. \\end{itemize}\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 00:10:34 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2013 22:08:08 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Ene", "Alina", ""], ["Korula", "Nitish", ""], ["Vakilian", "Ali", ""]]}, {"id": "1305.4389", "submitter": "Igor Sergeev", "authors": "Igor S. Sergeev", "title": "Implementation of linear maps with circulant matrices via modulo 2\n  rectifier circuits of bounded depth", "comments": "3 pages, in English; 4 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present note we show that for any positive integer k an arbitrary\nBoolean circulant matrix can be implemented via modulo 2 rectifier circuit of\ndepth 2k-1 and complexity O(n^{1+1/k}), and also via circuit of depth 2k and\ncomplexity O(n^{1+1/k} log^{-1/k} n).\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 18:29:11 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Sergeev", "Igor S.", ""]]}, {"id": "1305.4454", "submitter": "Shantanav Chakraborty", "authors": "Shantanav Chakraborty, Subhashish Banerjee, Satyabrata Adhikari and\n  Atul Kumar", "title": "Entanglement in the Grover's Search Algorithm", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum Algorithms have long captured the imagination of computer scientists\nand physicists primarily because of the speed up achieved by them over their\nclassical counterparts using principles of quantum mechanics. Entanglement is\nbelieved to be the primary phenomena behind this speed up. However their\nprecise role in quantum algorithms is yet unclear. In this article, we explore\nthe nature of entanglement in the Grover's search algorithm. This algorithm\nenables searching of elements from an unstructured database quadratically\nfaster than the best known classical algorithm. Geometric measure of\nentanglement has been used to quantify and analyse entanglement across\niterations of the algorithm. We reveal how the entanglement varies with\nincrease in the number of qubits and also with the number of marked or solution\nstates. Numerically, it is seen that the behaviour of the maximum value of\nentanglement is monotonous with the number of qubits. Also, for a given value\nof the number of qubits, a change in the marked states alters the amount of\nentanglement. The amount of entanglement in the final state of the algorithm\nhas been shown to depend solely on the nature of the marked states. Explicit\nanalytical expressions are given showing the variation of entanglement with the\nnumber of iterations and the global maximum value of entanglement attained\nacross all iterations of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 07:34:36 GMT"}, {"version": "v2", "created": "Thu, 30 May 2013 09:26:19 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Chakraborty", "Shantanav", ""], ["Banerjee", "Subhashish", ""], ["Adhikari", "Satyabrata", ""], ["Kumar", "Atul", ""]]}, {"id": "1305.4581", "submitter": "Nisheeth Vishnoi", "authors": "Subhash A. Khot and Nisheeth K. Vishnoi", "title": "The Unique Games Conjecture, Integrality Gap for Cut Problems and\n  Embeddability of Negative Type Metrics into $\\ell_1$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we disprove a conjecture of Goemans and Linial; namely, that\nevery negative type metric embeds into $\\ell_1$ with constant distortion. We\nshow that for an arbitrarily small constant $\\delta> 0$, for all large enough\n$n$, there is an $n$-point negative type metric which requires distortion at\nleast $(\\log\\log n)^{1/6-\\delta}$ to embed into $\\ell_1.$\n  Surprisingly, our construction is inspired by the Unique Games Conjecture\n(UGC) of Khot, establishing a previously unsuspected connection between\nprobabilistically checkable proof systems (PCPs) and the theory of metric\nembeddings. We first prove that the UGC implies a super-constant hardness\nresult for the (non-uniform) Sparsest Cut problem. Though this hardness result\nrelies on the UGC, we demonstrate, nevertheless, that the corresponding PCP\nreduction can be used to construct an \"integrality gap instance\" for Sparsest\nCut. Towards this, we first construct an integrality gap instance for a natural\nSDP relaxation of Unique Games. Then we \"simulate\" the PCP reduction and\n\"translate\" the integrality gap instance of Unique Games to an integrality gap\ninstance of Sparsest Cut. This enables us to prove a $(\\log \\log\nn)^{1/6-\\delta}$ integrality gap for Sparsest Cut, which is known to be\nequivalent to the metric embedding lower bound.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 17:24:54 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Khot", "Subhash A.", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1305.4675", "submitter": "Amitabh Trehan", "authors": "Amitabh Trehan", "title": "Algorithms for Self-Healing Networks", "comments": "Ph.D. Dissertation. University of New Mexico Computer Science, May\n  2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern networks are \\emph{reconfigurable}, in the sense that the\ntopology of the network can be changed by the nodes in the network. For\nexample, peer-to-peer, wireless and ad-hoc networks are reconfigurable. More\ngenerally, many social networks, such as a company's organizational chart;\ninfrastructure networks, such as an airline's transportation network; and\nbiological networks, such as the human brain, are also reconfigurable. Modern\nreconfigurable networks have a complexity unprecedented in the history of\nengineering, resembling more a dynamic and evolving living animal rather than a\nstructure of steel designed from a blueprint. Unfortunately, our mathematical\nand algorithmic tools have not yet developed enough to handle this complexity\nand fully exploit the flexibility of these networks.\n  We believe that it is no longer possible to build networks that are scalable\nand never have node failures. Instead, these networks should be able to admit\nsmall, and maybe, periodic failures and still recover like skin heals from a\ncut. This process, where the network can recover itself by maintaining key\ninvariants in response to attack by a powerful adversary is what we call\n\\emph{self-healing}.\n  Here, we present several fast and provably good distributed algorithms for\nself-healing in reconfigurable dynamic networks. Each of these algorithms have\ndifferent properties, a different set of gaurantees and limitations. We also\ndiscuss future directions and theoretical questions we would like to answer.\n%in the final dissertation that this document is proposed to lead to.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 23:05:46 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Trehan", "Amitabh", ""]]}, {"id": "1305.4696", "submitter": "Rotem Oshman", "authors": "Mark Braverman, Faith Ellen, Rotem Oshman, Toniann Pitassi, Vinod\n  Vaikuntanathan", "title": "Tight Bounds for Set Disjointness in the Message Passing Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multiparty message-passing model of communication, there are $k$\nplayers. Each player has a private input, and they communicate by sending\nmessages to one another over private channels. While this model has been used\nextensively in distributed computing and in multiparty computation, lower\nbounds on communication complexity in this model and related models have been\nsomewhat scarce. In recent work \\cite{phillips12,woodruff12,woodruff13}, strong\nlower bounds of the form $\\Omega(n \\cdot k)$ were obtained for several\nfunctions in the message-passing model; however, a lower bound on the classical\nSet Disjointness problem remained elusive.\n  In this paper, we prove tight lower bounds of the form $\\Omega(n \\cdot k)$\nfor the Set Disjointness problem in the message passing model. Our bounds are\nobtained by developing information complexity tools in the message-passing\nmodel, and then proving an information complexity lower bound for Set\nDisjointness. As a corollary, we show a tight lower bound for the task\nallocation problem \\cite{DruckerKuhnOshman} via a reduction from Set\nDisjointness.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 03:02:02 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Braverman", "Mark", ""], ["Ellen", "Faith", ""], ["Oshman", "Rotem", ""], ["Pitassi", "Toniann", ""], ["Vaikuntanathan", "Vinod", ""]]}, {"id": "1305.4747", "submitter": "Mathieu Raffinot", "authors": "Fabien de Montgolfier, Mathieu Raffinot, Irena Rusu", "title": "Easy identification of generalized common and conserved nested intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explain how to easily compute gene clusters, formalized by\nclassical or generalized nested common or conserved intervals, between a set of\nK genomes represented as K permutations. A b-nested common (resp. conserved)\ninterval I of size |I| is either an interval of size 1 or a common (resp.\nconserved) interval that contains another b-nested common (resp. conserved)\ninterval of size at least |I|-b. When b=1, this corresponds to the classical\nnotion of nested interval. We exhibit two simple algorithms to output all\nb-nested common or conserved intervals between K permutations in O(Kn+nocc)\ntime, where nocc is the total number of such intervals. We also explain how to\ncount all b-nested intervals in O(Kn) time. New properties of the family of\nconserved intervals are proposed to do so.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 08:04:55 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 14:04:21 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["de Montgolfier", "Fabien", ""], ["Raffinot", "Mathieu", ""], ["Rusu", "Irena", ""]]}, {"id": "1305.4760", "submitter": "Nick Jones", "authors": "Binh-Minh Bui-Xuan and Nick S. Jones", "title": "How modular structure can simplify tasks on networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By considering the task of finding the shortest walk through a network we\nfind an algorithm for which the run time is not as O(2^n), with n being the\nnumber of nodes, but instead scales with the number of nodes in a coarsened\nnetwork. This coarsened network has a number of nodes related to the number of\ndense regions in the original graph. Since we exploit a form of local community\ndetection as a preprocessing, this work gives support to the project of\ndeveloping heuristic algorithms for detecting dense regions in networks:\npreprocessing of this kind can accelerate optimization tasks on networks. Our\nwork also suggests a class of empirical conjectures for how structural features\nof efficient networked systems might scale with system size.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 09:17:16 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Bui-Xuan", "Binh-Minh", ""], ["Jones", "Nick S.", ""]]}, {"id": "1305.4874", "submitter": "Sergiu Hart", "authors": "Sergiu Hart and Noam Nisan", "title": "The Query Complexity of Correlated Equilibria", "comments": null, "journal-ref": "Games and Economic Behavior 108 (2018), 401-410", "doi": "10.1016/j.geb.2016.11.003", "report-no": "Center for Rationality DP-647", "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the complexity of finding a correlated equilibrium of an\n$n$-player game in a model that allows the algorithm to make queries on\nplayers' payoffs at pure strategy profiles. Randomized regret-based dynamics\nare known to yield an approximate correlated equilibrium efficiently, namely,\nin time that is polynomial in the number of players $n$. Here we show that both\nrandomization and approximation are necessary: no efficient deterministic\nalgorithm can reach even an approximate correlated equilibrium, and no\nefficient randomized algorithm can reach an exact correlated equilibrium. The\nresults are obtained by bounding from below the number of payoff queries that\nare needed.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 16:33:32 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 09:51:50 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Hart", "Sergiu", ""], ["Nisan", "Noam", ""]]}, {"id": "1305.4883", "submitter": "Marcos Kiwi", "authors": "Marcos Kiwi and Cristina G. Fernandes", "title": "Repetition-free longest common subsequence of random sequences", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A repetition free Longest Common Subsequence (LCS) of two sequences x and y\nis an LCS of x and y where each symbol may appear at most once. Let R denote\nthe length of a repetition free LCS of two sequences of n symbols each one\nchosen randomly, uniformly, and independently over a k-ary alphabet. We study\nthe asymptotic, in n and k, behavior of R and establish that there are three\ndistinct regimes, depending on the relative speed of growth of n and k. For\neach regime we establish the limiting behavior of R. In fact, we do more, since\nwe actually establish tail bounds for large deviations of R from its limiting\nbehavior.\n  Our study is motivated by the so called exemplar model proposed by Sankoff\n(1999) and the related similarity measure introduced by Adi et al. (2007). A\nnatural question that arises in this context, which as we show is related to\nlong standing open problems in the area of probabilistic combinatorics, is to\nunderstand the asymptotic, in n and k, behavior of parameter R.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 17:04:21 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Kiwi", "Marcos", ""], ["Fernandes", "Cristina G.", ""]]}, {"id": "1305.4905", "submitter": "Xunrui Yin", "authors": "Xunrui Yin, Yan Wang, Zongpeng Li, Xin Wang, Xiangyang Xue", "title": "A Graph Minor Perspective to Multicast Network Coding", "comments": "26 pages, 12 (sub-)figures, partially presented in INFOCOM 2013,\n  submitted to Trans. IT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Coding encourages information coding across a communication network.\nWhile the necessity, benefit and complexity of network coding are sensitive to\nthe underlying graph structure of a network, existing theory on network coding\noften treats the network topology as a black box, focusing on algebraic or\ninformation theoretic aspects of the problem. This work aims at an in-depth\nexamination of the relation between algebraic coding and network topologies. We\nmathematically establish a series of results along the direction of: if network\ncoding is necessary/beneficial, or if a particular finite field is required for\ncoding, then the network must have a corresponding hidden structure embedded in\nits underlying topology, and such embedding is computationally efficient to\nverify. Specifically, we first formulate a meta-conjecture, the NC-Minor\nConjecture, that articulates such a connection between graph theory and network\ncoding, in the language of graph minors. We next prove that the NC-Minor\nConjecture is almost equivalent to the Hadwiger Conjecture, which connects\ngraph minors with graph coloring. Such equivalence implies the existence of\n$K_4$, $K_5$, $K_6$, and $K_{O(q/\\log{q})}$ minors, for networks requiring\n$\\mathbb{F}_3$, $\\mathbb{F}_4$, $\\mathbb{F}_5$ and $\\mathbb{F}_q$,\nrespectively. We finally prove that network coding can make a difference from\nrouting only if the network contains a $K_4$ minor, and this minor containment\nresult is tight. Practical implications of the above results are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 18:26:42 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Yin", "Xunrui", ""], ["Wang", "Yan", ""], ["Li", "Zongpeng", ""], ["Wang", "Xin", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1305.4914", "submitter": "Marek Cygan", "authors": "Marek Cygan and Fabrizio Grandoni and Danny Hermelin", "title": "Tight Kernel Bounds for Problems on Graphs with Small Degeneracy", "comments": "Full version of ESA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider kernelization for problems on d-degenerate graphs,\ni.e. graphs such that any subgraph contains a vertex of degree at most $d$.\nThis graph class generalizes many classes of graphs for which effective\nkernelization is known to exist, e.g. planar graphs, H-minor free graphs, and\nH-topological-minor free graphs. We show that for several natural problems on\nd-degenerate graphs the best known kernelization upper bounds are essentially\ntight.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 18:54:23 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2013 20:41:49 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Cygan", "Marek", ""], ["Grandoni", "Fabrizio", ""], ["Hermelin", "Danny", ""]]}, {"id": "1305.5266", "submitter": "Kerstin Daechert", "authors": "Kerstin Daechert and Kathrin Klamroth", "title": "A linear bound on the number of scalarizations needed to solve discrete\n  tricriteria optimization problems", "comments": "32 pages, 8 figures, Journal of Global Optimization, 2014", "journal-ref": null, "doi": "10.1007/s10898-014-0205-z", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General multi-objective optimization problems are often solved by a sequence\nof parametric single objective problems, so-called scalarizations. If the set\nof nondominated points is finite, and if an appropriate scalarization is\nemployed, the entire nondominated set can be generated in this way. In the\nbicriteria case it is well known that this can be realized by an adaptive\napproach which, given an appropriate initial search space, requires the\nsolution of a number of subproblems which is at most two times the number of\nnondominated points. For higher dimensional problems, no linear methods were\nknown up to now. We present a new procedure for finding the entire nondominated\nset of tricriteria optimization problems for which the number of scalarized\nsubproblems to be solved is at most three times the number of nondominated\npoints of the underlying problem. The approach includes an iterative update of\nthe search space that, given a (sub-)set of nondominated points, describes the\narea in which additional nondominated points may be located. In particular, we\nshow that the number of boxes, into which the search space is decomposed,\ndepends linearly on the number of nondominated points.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 20:46:45 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 10:35:52 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 10:49:53 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Daechert", "Kerstin", ""], ["Klamroth", "Kathrin", ""]]}, {"id": "1305.5339", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Pawel Zielinski", "title": "Combinatorial optimization problems with uncertain costs and the OWA\n  criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a class of combinatorial optimization problems with uncertain\ncosts is discussed. The uncertainty is modeled by specifying a discrete\nscenario set containing $K$ distinct cost scenarios. The Ordered Weighted\nAveraging (OWA for short) aggregation operator is applied to choose a solution.\nThe well-known criteria such as: the maximum, minimum, average, Hurwicz and\nmedian are special cases of OWA. By using OWA, the traditional min-max approach\nto combinatorial optimization problems with uncertain costs, often regarded as\ntoo conservative, can be generalized. The computational complexity and\napproximability of the problem of minimizing OWA for the considered class of\nproblems are investigated and some new positive and negative results in this\narea are provided. These results remain valid for many important problems, such\nas network or resource allocation problems.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 07:32:09 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 08:15:08 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1305.5500", "submitter": "Madhur Tulsiani", "authors": "Subhash Khot, Madhur Tulsiani, Pratik Worah", "title": "A Characterization of Approximation Resistance", "comments": "62 pages. The previous version of this paper gave a characterization\n  of a modified notion called \"Strong Approximation Resistance\". We now present\n  a characterization of approximation resistance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A predicate f:{-1,1}^k -> {0,1} with \\rho(f) = \\frac{|f^{-1}(1)|}{2^k} is\ncalled {\\it approximation resistant} if given a near-satisfiable instance of\nCSP(f), it is computationally hard to find an assignment that satisfies at\nleast \\rho(f)+\\Omega(1) fraction of the constraints.\n  We present a complete characterization of approximation resistant predicates\nunder the Unique Games Conjecture. We also present characterizations in the\n{\\it mixed} linear and semi-definite programming hierarchy and the\nSherali-Adams linear programming hierarchy. In the former case, the\ncharacterization coincides with the one based on UGC. Each of the two\ncharacterizations is in terms of existence of a probability measure with\ncertain symmetry properties on a natural convex polytope associated with the\npredicate.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 17:52:58 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2013 05:24:35 GMT"}], "update_date": "2013-10-24", "authors_parsed": [["Khot", "Subhash", ""], ["Tulsiani", "Madhur", ""], ["Worah", "Pratik", ""]]}, {"id": "1305.5520", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari, Fabian Kuhn", "title": "Distributed Minimum Cut Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing approximate minimum edge cuts by\ndistributed algorithms. We use a standard synchronous message passing model\nwhere in each round, $O(\\log n)$ bits can be transmitted over each edge (a.k.a.\nthe CONGEST model). We present a distributed algorithm that, for any weighted\ngraph and any $\\epsilon \\in (0, 1)$, with high probability finds a cut of size\nat most $O(\\epsilon^{-1}\\lambda)$ in $O(D) + \\tilde{O}(n^{1/2 + \\epsilon})$\nrounds, where $\\lambda$ is the size of the minimum cut. This algorithm is based\non a simple approach for analyzing random edge sampling, which we call the\nrandom layering technique. In addition, we also present another distributed\nalgorithm, which is based on a centralized algorithm due to Matula [SODA '93],\nthat with high probability computes a cut of size at most $(2+\\epsilon)\\lambda$\nin $\\tilde{O}((D+\\sqrt{n})/\\epsilon^5)$ rounds for any $\\epsilon>0$.\n  The time complexities of both of these algorithms almost match the\n$\\tilde{\\Omega}(D + \\sqrt{n})$ lower bound of Das Sarma et al. [STOC '11], thus\nleading to an answer to an open question raised by Elkin [SIGACT-News '04] and\nDas Sarma et al. [STOC '11].\n  Furthermore, we also strengthen the lower bound of Das Sarma et al. by\nextending it to unweighted graphs. We show that the same lower bound also holds\nfor unweighted multigraphs (or equivalently for weighted graphs in which\n$O(w\\log n)$ bits can be transmitted in each round over an edge of weight $w$),\neven if the diameter is $D=O(\\log n)$. For unweighted simple graphs, we show\nthat even for networks of diameter $\\tilde{O}(\\frac{1}{\\lambda}\\cdot\n\\sqrt{\\frac{n}{\\alpha\\lambda}})$, finding an $\\alpha$-approximate minimum cut\nin networks of edge connectivity $\\lambda$ or computing an\n$\\alpha$-approximation of the edge connectivity requires $\\tilde{\\Omega}(D +\n\\sqrt{\\frac{n}{\\alpha\\lambda}})$ rounds.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 19:13:15 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 19:16:07 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1305.5551", "submitter": "Valery Kirzhner", "authors": "Alexander Bolshoy and Valery Kirzhner", "title": "Algorithms of an optimal integer tree labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we label the vertices of a tree by positive integers. The weight of\nan edge is defined by a monotonically increasing function of the absolute value\nof the difference of the labels of its endpoints. We define the total cost of\nthe labeling to be the sum of weight of all the edges.The problem we consider\nis that of determining for a given tree G and given a labeling of the leaves of\nG the minimum total cost labellings of G. In this paper we present an algorithm\nthat works for any cost function satisfies the condition of monotony mentioned\nabove. In a case of the function defined as the absolute value of the\ndifference of the labels the fast algorithm is presented.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 20:09:50 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Bolshoy", "Alexander", ""], ["Kirzhner", "Valery", ""]]}, {"id": "1305.5580", "submitter": "Qin Zhang", "authors": "David P. Woodruff and Qin Zhang", "title": "Subspace Embeddings and $\\ell_p$-Regression Using Exponential Random\n  Variables", "comments": "Corrected some technical issues in Sec. 4.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oblivious low-distortion subspace embeddings are a crucial building block for\nnumerical linear algebra problems. We show for any real $p, 1 \\leq p < \\infty$,\ngiven a matrix $M \\in \\mathbb{R}^{n \\times d}$ with $n \\gg d$, with constant\nprobability we can choose a matrix $\\Pi$ with $\\max(1, n^{1-2/p}) \\poly(d)$\nrows and $n$ columns so that simultaneously for all $x \\in \\mathbb{R}^d$,\n$\\|Mx\\|_p \\leq \\|\\Pi Mx\\|_{\\infty} \\leq \\poly(d) \\|Mx\\|_p.$ Importantly, $\\Pi\nM$ can be computed in the optimal $O(\\nnz(M))$ time, where $\\nnz(M)$ is the\nnumber of non-zero entries of $M$. This generalizes all previous oblivious\nsubspace embeddings which required $p \\in [1,2]$ due to their use of $p$-stable\nrandom variables. Using our matrices $\\Pi$, we also improve the best known\ndistortion of oblivious subspace embeddings of $\\ell_1$ into $\\ell_1$ with\n$\\tilde{O}(d)$ target dimension in $O(\\nnz(M))$ time from $\\tilde{O}(d^3)$ to\n$\\tilde{O}(d^2)$, which can further be improved to $\\tilde{O}(d^{3/2})\n\\log^{1/2} n$ if $d = \\Omega(\\log n)$, answering a question of Meng and Mahoney\n(STOC, 2013).\n  We apply our results to $\\ell_p$-regression, obtaining a\n$(1+\\eps)$-approximation in $O(\\nnz(M)\\log n) + \\poly(d/\\eps)$ time, improving\nthe best known $\\poly(d/\\eps)$ factors for every $p \\in [1, \\infty) \\setminus\n\\{2\\}$. If one is just interested in a $\\poly(d)$ rather than a\n$(1+\\eps)$-approximation to $\\ell_p$-regression, a corollary of our results is\nthat for all $p \\in [1, \\infty)$ we can solve the $\\ell_p$-regression problem\nwithout using general convex programming, that is, since our subspace embeds\ninto $\\ell_{\\infty}$ it suffices to solve a linear programming problem.\nFinally, we give the first protocols for the distributed $\\ell_p$-regression\nproblem for every $p \\geq 1$ which are nearly optimal in communication and\ncomputation.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 23:08:55 GMT"}, {"version": "v2", "created": "Mon, 17 Mar 2014 23:53:12 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Woodruff", "David P.", ""], ["Zhang", "Qin", ""]]}, {"id": "1305.5617", "submitter": "Tomasz Popiel", "authors": "Alice C. Niemeyer, Tomasz Popiel and Cheryl E. Praeger", "title": "Straight-line programs with memory and matrix Bruhat decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate that straight-line programs designed for algebraic computations\nshould be accompanied by a comprehensive complexity analysis that takes into\naccount both the number of fundamental algebraic operations needed, as well as\nmemory requirements arising during evaluation. We introduce an approach for\nformalising this idea and, as illustration, construct and analyse straight-line\nprograms for the Bruhat decomposition of $d \\times d$ matrices with determinant\n1 over a finite field of order $q$ that have length $O(d^2 \\log(q))$ and\nrequire storing only $O(\\log(q))$ matrices during evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 05:18:37 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Niemeyer", "Alice C.", ""], ["Popiel", "Tomasz", ""], ["Praeger", "Cheryl E.", ""]]}, {"id": "1305.5662", "submitter": "J\\'anos Tapolcai", "authors": "J\\'anos Tapolcai, G\\'abor R\\'etv\\'ari, Attila K\\H{o}r\\\"osi", "title": "Memory size bounds of prefix DAGs", "comments": "3 pages, G. R\\'etv\\'ari, J. Tapolcai, A. K\\H{o}r\\\"osi, A. Majd\\'an,\n  Z. Heszberger, \"Compressing IP Forwarding Tables: Towards Entropy Bounds and\n  Beyond\", In ACM Sigcomm, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report an entropy bound on the memory size is given for a compression\nmethod of leaf-labeled trees. The compression converts the tree into a Directed\nAcyclic Graph (DAG) by merging isomorphic subtrees.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 09:22:11 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Tapolcai", "J\u00e1nos", ""], ["R\u00e9tv\u00e1ri", "G\u00e1bor", ""], ["K\u0151r\u00f6si", "Attila", ""]]}, {"id": "1305.5757", "submitter": "Fang  Wei-Kleiner", "authors": "Fang Wei-Kleiner", "title": "Tree Decomposition based Steiner Tree Computation over Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we present an exact algorithm for the Steiner tree problem.\nThe algorithm is based on certain pre-computed index structures. Our algorithm\noffers a practical solution for the Steiner tree problems on graphs of large\nsize and bounded number of terminals.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 14:48:17 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Wei-Kleiner", "Fang", ""]]}, {"id": "1305.5823", "submitter": "Anna Harutyunyan", "authors": "Glencora Borradaile and Anna Harutyunyan", "title": "Maximum st-flow in directed planar graphs via shortest paths", "comments": "20 pages, 4 figures. Short version to be published in proceedings of\n  IWOCA'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum cuts have been closely related to shortest paths in planar graphs via\nplanar duality - so long as the graphs are undirected. Even maximum flows are\nclosely related to shortest paths for the same reason - so long as the source\nand the sink are on a common face. In this paper, we give a correspondence\nbetween maximum flows and shortest paths via duality in directed planar graphs\nwith no constraints on the source and sink. We believe this a promising avenue\nfor developing algorithms that are more practical than the current\nasymptotically best algorithms for maximum st-flow.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 18:42:49 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Borradaile", "Glencora", ""], ["Harutyunyan", "Anna", ""]]}, {"id": "1305.5946", "submitter": "Tao Qin Dr.", "authors": "Weihao Kong and Jian Li and Tao Qin and Tie-Yan Liu", "title": "Optimal Groupon Allocations", "comments": "Web and Internet Economics (WINE) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-buying websites represented by Groupon.com are very popular in\nelectronic commerce and online shopping nowadays. They have multiple slots to\nprovide deals with significant discounts to their visitors every day. The\ncurrent user traffic allocation mostly relies on human decisions. We study the\nproblem of automatically allocating the user traffic of a group-buying website\nto different deals to maximize the total revenue and refer to it as the\nGroup-buying Allocation Problem (\\GAP). The key challenge of \\GAP\\ is how to\nhandle the tipping point (lower bound) and the purchase limit (upper bound) of\neach deal. We formulate \\GAP\\ as a knapsack-like problem with variable-sized\nitems and majorization constraints. Our main results for \\GAP\\ can be\nsummarized as follows. (1) We first show that for a special case of \\GAP, in\nwhich the lower bound equals the upper bound for each deal, there is a simple\ndynamic programming-based algorithm that can find an optimal allocation in\npseudo-polynomial time. (2) The general case of \\GAP\\ is much more difficult\nthan the special case. To solve the problem, we first discover several\nstructural properties of the optimal allocation, and then design a two-layer\ndynamic programming-based algorithm leveraging those properties. This algorithm\ncan find an optimal allocation in pseudo-polynomial time. (3) We convert the\ntwo-layer dynamic programming based algorithm to a fully polynomial time\napproximation scheme (FPTAS), using the technique developed in\n\\cite{ibarra1975fast}, combined with some careful modifications of the dynamic\nprograms. Besides these results, we further investigate some natural\ngeneralizations of \\GAP, and propose effective algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2013 16:22:57 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2013 06:07:13 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 12:11:41 GMT"}, {"version": "v4", "created": "Tue, 17 Dec 2013 05:36:41 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Kong", "Weihao", ""], ["Li", "Jian", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1305.5976", "submitter": "Xinwen Jiang", "authors": "Xinwen Jiang", "title": "A Polynomial Time Algorithm for the Hamilton Circuit Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a so-called Multistage graph Simple Path (MSP)\nproblem and show that the Hamilton Circuit (HC) problem can be polynomially\nreducible to the MSP problem. To solve the MSP problem, we propose a polynomial\nalgorithm and prove its NP-completeness. Our result implies NP=P.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2013 00:40:00 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Jiang", "Xinwen", ""]]}, {"id": "1305.5998", "submitter": "Yannis Moysoglou", "authors": "Stavros G. Kolliopoulos and Yannis Moysoglou", "title": "Integrality gaps for strengthened LP relaxations of Capacitated and\n  Lower-Bounded Facility Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The metric uncapacitated facility location problem (UFL) enjoys a special\nstature in approximation algorithms as a testbed for various techniques. Two\ngeneralizations of UFL are capacitated facility location (CFL) and\nlower-bounded facility location (LBFL). In the former, every facility has a\ncapacity which is the maximum demand that can be assigned to it, while in the\nlatter, every open facility is required to serve a given minimum amount of\ndemand. Both CFL and LBFL are approximable within a constant factor but their\nrespective natural LP relaxations have an unbounded integrality gap. According\nto Shmoys and Williamson, the existence of a relaxation-based algorithm for CFL\nis one of the top 10 open problems in approximation algorithms.\n  In this paper we give the first results on this problem. We provide\nsubstantial evidence against the existence of a good LP relaxation for CFL by\nshowing unbounded integrality gaps for two families of strengthened\nformulations.\n  The first family we consider is the hierarchy of LPs resulting from repeated\napplications of the lift-and-project Lov\\'{a}sz-Schrijver procedure starting\nfrom the standard relaxation. We show that the LP relaxation for CFL resulting\nafter $\\Omega(n)$ rounds, where $n$ is the number of facilities in the\ninstance, has unbounded integrality gap. Note that the Lov\\'{a}sz-Schrijver\nprocedure is known to yield an exact formulation for CFL in at most $n$ rounds.\n  We also introduce the family of proper relaxations which generalizes to its\nlogical extreme the classic star relaxation, an equivalent form of the natural\nLP. We characterize the integrality gap of proper relaxations for both LBFL and\nCFL and show a threshold phenomenon under which it decreases from unbounded to\n1.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2013 07:29:08 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 12:54:48 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Kolliopoulos", "Stavros G.", ""], ["Moysoglou", "Yannis", ""]]}, {"id": "1305.6095", "submitter": "Hideo Bannai", "authors": "Jun'ichi Yamamoto, Tomohiro I, Hideo Bannai, Shunsuke Inenaga,\n  Masayuki Takeda", "title": "Faster Compact On-Line Lempel-Ziv Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new on-line algorithm for computing the Lempel-Ziv factorization\nof a string that runs in $O(N\\log N)$ time and uses only $O(N\\log\\sigma)$ bits\nof working space, where $N$ is the length of the string and $\\sigma$ is the\nsize of the alphabet. This is a notable improvement compared to the performance\nof previous on-line algorithms using the same order of working space but\nrunning in either $O(N\\log^3N)$ time (Okanohara & Sadakane 2009) or\n$O(N\\log^2N)$ time (Starikovskaya 2012). The key to our new algorithm is in the\nutilization of an elegant but less popular index structure called Directed\nAcyclic Word Graphs, or DAWGs (Blumer et al. 1985). We also present an\nopportunistic variant of our algorithm, which, given the run length encoding of\nsize $m$ of a string of length $N$, computes the Lempel-Ziv factorization\non-line, in $O\\left(m \\cdot \\min \\left\\{\\frac{(\\log\\log m)(\\log \\log\nN)}{\\log\\log\\log N}, \\sqrt{\\frac{\\log m}{\\log \\log m}} \\right\\}\\right)$ time\nand $O(m\\log N)$ bits of space, which is faster and more space efficient when\nthe string is run-length compressible.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 02:35:15 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Yamamoto", "Jun'ichi", ""], ["I", "Tomohiro", ""], ["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1305.6349", "submitter": "Vance Faber", "authors": "Vance Faber", "title": "Global communication algorithms for Cayley graphs", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss several combinatorial problems that arise when one looks at\ncomputational algorithms for highly symmetric networks of processors. More\nspecifically, we are interested in minimal times associated with four\ncommunication tasks (defined more precisely below): universal broadcast, every\nprocessor has a vector that it wishes to broadcast to all the others; universal\naccumulation, every processor wishes to receive the sum of all the vectors\nbeing sent to it by all the other processors; universal exchange, every\nprocessor wishes to exchange a vector with each other processor; and global\nsummation, every processor wants the sum of the vectors in all the processors\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 01:24:52 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 02:46:08 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2013 03:40:58 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Faber", "Vance", ""]]}, {"id": "1305.6376", "submitter": "Frank Vanderzwet", "authors": "Frank Vanderzwet", "title": "Fractional Pebbling Game Lower Bounds", "comments": "Graduate Research Paper for University of Toronto. Completion of\n  paper assisted by Professor Stephen Cook and Professor Toniann Pitassi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional pebbling is a generalization of black-white pebbling introduced\nrecently. In this reasearch paper we solve an open problem by proving a tight\nlower bound on the pebble weight required to fractionally pebble a balanced\nd-ary tree of height h. This bound has close ties with branching programs and\nthe separation of P from NL.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 06:06:53 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Vanderzwet", "Frank", ""]]}, {"id": "1305.6390", "submitter": "He Huang", "authors": "Yu-e Sun, He Huang, Xiang-Yang Li, Zhili Chen, Wei Yang, Hongli Xu,\n  Liusheng Huang", "title": "Near-Optimal Truthful Auction Mechanisms in Secondary Spectrum Markets", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study spectrum auction problem where each request from\nsecondary users has spatial, temporal, and spectral features. With the requests\nof secondary users and the reserve price of the primary user, our goal is to\ndesign truthful mechanisms that will either maximize the social efficiency or\nmaximize the revenue of the primary user. As the optimal conflict-free spectrum\nallocation problem is NP-hard, in this work, we design near optimal spectrum\nallocation mechanisms separately based on the following techniques:\nderandomized allocation from integer programming formulation, its linear\nprogramming (LP) relaxation, and the dual of the LP. We theoretically prove\nthat 1) our near optimal allocation methods are bid monotone, which implys\ntruthful auction mechanisms; and 2) our near optimal allocation methods can\nachieve a social efficiency or a revenue that is at least $1-\\frac{1}{e}$ times\nof the optimal respectively. At last, we conduct extensive simulations to study\nthe performances (social efficiency, revenue) of the proposed methods, and the\nsimulation results corroborate our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 07:33:18 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Sun", "Yu-e", ""], ["Huang", "He", ""], ["Li", "Xiang-Yang", ""], ["Chen", "Zhili", ""], ["Yang", "Wei", ""], ["Xu", "Hongli", ""], ["Huang", "Liusheng", ""]]}, {"id": "1305.6432", "submitter": "Ali Dehghan", "authors": "Arash Ahadi and Ali Dehghan", "title": "The Complexity of the Proper Orientation Number", "comments": "10 pages, 2 figures. Submitted to Information Processing Letters", "journal-ref": null, "doi": "10.1016/j.ipl.2013.07.017", "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph orientation is a well-studied area of graph theory. A proper\norientation of a graph $G = (V,E)$ is an orientation $D$ of $E(G)$ such that\nfor every two adjacent vertices $ v $ and $ u $, $ d^{-}_{D}(v) \\neq\nd^{-}_{D}(u)$ where $d_{D}^{-}(v)$ is the number of edges with head $v$ in $D$.\nThe proper orientation number of $G$ is defined as $ \\overrightarrow{\\chi} (G)\n=\\displaystyle \\min_{D\\in \\Gamma} \\displaystyle\\max_{v\\in V(G)} d^{-}_{D}(v) $\nwhere $\\Gamma$ is the set of proper orientations of $G$. We have $ \\chi(G)-1\n\\leq \\overrightarrow{\\chi} (G)\\leq \\Delta(G) $. We show that, it is $\n\\mathbf{NP} $-complete to decide whether $\\overrightarrow{\\chi}(G)=2$, for a\ngiven planar graph $G$. Also, we prove that there is a polynomial time\nalgorithm for determining the proper orientation number of 3-regular graphs. In\nsharp contrast, we will prove that this problem is $ \\mathbf{NP} $-hard for\n4-regular graphs.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 09:44:45 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Ahadi", "Arash", ""], ["Dehghan", "Ali", ""]]}, {"id": "1305.6555", "submitter": "Sandor P. Fekete", "authors": "Michael A. Bender and Martin Farach-Colton and S\\'andor P. Fekete and\n  Jeremy T. Fineman and Seth Gilbert", "title": "Reallocation Problems in Scheduling", "comments": "9 oages, 1 table; extended abstract version to appear in SPAA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional on-line problems, such as scheduling, requests arrive over\ntime, demanding available resources. As each request arrives, some resources\nmay have to be irrevocably committed to servicing that request. In many\nsituations, however, it may be possible or even necessary to reallocate\npreviously allocated resources in order to satisfy a new request. This\nreallocation has a cost. This paper shows how to service the requests while\nminimizing the reallocation cost. We focus on the classic problem of scheduling\njobs on a multiprocessor system. Each unit-size job has a time window in which\nit can be executed. Jobs are dynamically added and removed from the system. We\nprovide an algorithm that maintains a valid schedule, as long as a sufficiently\nfeasible schedule exists. The algorithm reschedules only a total number of\nO(min{log^* n, log^* Delta}) jobs for each job that is inserted or deleted from\nthe system, where n is the number of active jobs and Delta is the size of the\nlargest window.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 16:34:44 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Bender", "Michael A.", ""], ["Farach-Colton", "Martin", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Fineman", "Jeremy T.", ""], ["Gilbert", "Seth", ""]]}, {"id": "1305.6577", "submitter": "Chandra Chekuri", "authors": "Chandra Chekuri and Julia Chuzhoy", "title": "Polynomial Bounds for the Grid-Minor Theorem", "comments": "Preliminary version of this paper appeared in Proceedings of ACM\n  STOC, 2014. This is a full version that has been submitted to a journal and\n  then revised based on reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key results in Robertson and Seymour's seminal work on graph\nminors is the Grid-Minor Theorem (also called the Excluded Grid Theorem). The\ntheorem states that for every grid $H$, every graph whose treewidth is large\nenough relative to $|V(H)|$ contains $H$ as a minor. This theorem has found\nmany applications in graph theory and algorithms. Let $f(k)$ denote the largest\nvalue such that every graph of treewidth $k$ contains a grid minor of size\n$(f(k)\\times f(k))$. The best previous quantitative bound, due to recent work\nof Kawarabayashi and Kobayashi, and Leaf and Seymour, shows that\n$f(k)=\\Omega(\\sqrt{\\log k/\\log \\log k})$. In contrast, the best known upper\nbound implies that $f(k) = O(\\sqrt{k/\\log k})$. In this paper we obtain the\nfirst polynomial relationship between treewidth and grid minor size by showing\nthat $f(k)=\\Omega(k^{\\delta})$ for some fixed constant $\\delta > 0$, and\ndescribe a randomized algorithm, whose running time is polynomial in $|V(G)|$\nand $k$, that with high probability finds a model of such a grid minor in $G$.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 18:27:20 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2013 16:50:19 GMT"}, {"version": "v3", "created": "Mon, 21 Jul 2014 21:38:19 GMT"}, {"version": "v4", "created": "Tue, 23 Sep 2014 21:33:18 GMT"}, {"version": "v5", "created": "Tue, 9 Aug 2016 21:33:21 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Chekuri", "Chandra", ""], ["Chuzhoy", "Julia", ""]]}, {"id": "1305.7050", "submitter": "Dennis Guck", "authors": "Dennis Guck, Hassan Hatefi, Holger Hermanns, Joost-Pieter Katoen, Mark\n  Timmer", "title": "Modelling, Reduction and Analysis of Markov Automata (extended version)", "comments": "Technical report accompanying the QEST 2013 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov automata (MA) constitute an expressive continuous-time compositional\nmodelling formalism. They appear as semantic backbones for engineering\nframeworks including dynamic fault trees, Generalised Stochastic Petri Nets,\nand AADL. Their expressive power has thus far precluded them from effective\nanalysis by probabilistic (and statistical) model checkers, stochastic game\nsolvers, or analysis tools for Petri net-like formalisms. This paper presents\nthe foundations and underlying algorithms for efficient MA modelling, reduction\nusing static analysis, and most importantly, quantitative analysis. We also\ndiscuss implementation pragmatics of supporting tools and present several case\nstudies demonstrating feasibility and usability of MA in practice.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 10:09:33 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Guck", "Dennis", ""], ["Hatefi", "Hassan", ""], ["Hermanns", "Holger", ""], ["Katoen", "Joost-Pieter", ""], ["Timmer", "Mark", ""]]}, {"id": "1305.7146", "submitter": "Michele Coscia", "authors": "Michele Coscia, Giulio Rossetti, Diego Pennacchioli, Damiano\n  Ceccarelli, Fosca Giannotti", "title": "\"You Know Because I Know\": a Multidimensional Network Approach to Human\n  Resources Problem", "comments": null, "journal-ref": "Advances in Social Network Analysis and Mining (ASONAM) 2013", "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding talents, often among the people already hired, is an endemic\nchallenge for organizations. The social networking revolution, with online\ntools like Linkedin, made possible to make explicit and accessible what we\nperceived, but not used, for thousands of years: the exact position and ranking\nof a person in a network of professional and personal connections. To search\nand mine where and how an employee is positioned on a global skill network will\nenable organizations to find unpredictable sources of knowledge, innovation and\nknow-how. This data richness and hidden knowledge demands for a\nmultidimensional and multiskill approach to the network ranking problem.\nMultidimensional networks are networks with multiple kinds of relations. To the\nbest of our knowledge, no network-based ranking algorithm is able to handle\nmultidimensional networks and multiple rankings over multiple attributes at the\nsame time. In this paper we propose such an algorithm, whose aim is to address\nthe node multi-ranking problem in multidimensional networks. We test our\nalgorithm over several real world networks, extracted from DBLP and the Enron\nemail corpus, and we show its usefulness in providing less trivial and more\nflexible rankings than the current state of the art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 15:51:01 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Coscia", "Michele", ""], ["Rossetti", "Giulio", ""], ["Pennacchioli", "Diego", ""], ["Ceccarelli", "Damiano", ""], ["Giannotti", "Fosca", ""]]}, {"id": "1305.7448", "submitter": "Hans Bodlaender", "authors": "Stefan Fafianie, Hans L. Bodlaender and Jesper Nederlof", "title": "Speeding-up Dynamic Programming with Representative Sets - An\n  Experimental Evaluation of Algorithms for Steiner Tree on Tree Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming on tree decompositions is a frequently used approach to\nsolve otherwise intractable problems on instances of small treewidth. In recent\nwork by Bodlaender et al., it was shown that for many connectivity problems,\nthere exist algorithms that use time, linear in the number of vertices, and\nsingle exponential in the width of the tree decomposition that is used. The\ncentral idea is that it suffices to compute representative sets, and these can\nbe computed efficiently with help of Gaussian elimination.\n  In this paper, we give an experimental evaluation of this technique for the\nSteiner Tree problem. A comparison of the classic dynamic programming algorithm\nand the improved dynamic programming algorithm that employs the table reduction\nshows that the new approach gives significant improvements on the running time\nof the algorithm and the size of the tables computed by the dynamic programming\nalgorithm, and thus that the rank based approach from Bodlaender et al. does\nnot only give significant theoretical improvements but also is a viable\napproach in a practical setting, and showcases the potential of exploiting the\nidea of representative sets for speeding up dynamic programming algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 15:17:38 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Fafianie", "Stefan", ""], ["Bodlaender", "Hans L.", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1305.7476", "submitter": "Uwe Aickelin", "authors": "Feng Gu, Julie Greensmith, Uwe Aickelin", "title": "Theoretical formulation and analysis of the deterministic dendritic cell\n  algorithm", "comments": null, "journal-ref": "Biosystems 111 (2), 127-135, 2013", "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the emerging algorithms in the field of Artificial Immune Systems\n(AIS), the Dendritic Cell Algorithm (DCA) has been successfully applied to a\nnumber of challenging real-world problems. However, one criticism is the lack\nof a formal definition, which could result in ambiguity for understanding the\nalgorithm. Moreover, previous investigations have mainly focused on its\nempirical aspects. Therefore, it is necessary to provide a formal definition of\nthe algorithm, as well as to perform runtime analyses to revealits theoretical\naspects. In this paper, we define the deterministic version of the DCA, named\nthe dDCA, using set theory and mathematical functions. Runtime analyses of the\nstandard algorithm and the one with additional segmentation are performed. Our\nanalysis suggests that the standard dDCA has a runtime complexity of O(n2) for\nthe worst-case scenario, where n is the number of input data instances. The\nintroduction of segmentation changes the algorithm's worst case runtime\ncomplexity to O(max(nN; nz)), for DC population size N with size of each\nsegment z. Finally, two runtime variables of the algorithm are formulated based\non the input data, to understand its runtime behaviour as guidelines for\nfurther development.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 16:20:40 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Gu", "Feng", ""], ["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""]]}]