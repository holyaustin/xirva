[{"id": "1106.0365", "submitter": "Eric Price", "authors": "Khanh Do Ba, Piotr Indyk, Eric Price, and David P. Woodruff", "title": "Lower Bounds for Sparse Recovery", "comments": "11 pages. Appeared at SODA 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We consider the following k-sparse recovery problem: design an m x n matrix\nA, such that for any signal x, given Ax we can efficiently recover x'\nsatisfying\n  ||x-x'||_1 <= C min_{k-sparse} x\"} ||x-x\"||_1.\n  It is known that there exist matrices A with this property that have only O(k\nlog (n/k)) rows.\n  In this paper we show that this bound is tight. Our bound holds even for the\nmore general /randomized/ version of the problem, where A is a random variable\nand the recovery algorithm is required to work for any fixed x with constant\nprobability (over A).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 05:20:14 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2011 02:16:05 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Ba", "Khanh Do", ""], ["Indyk", "Piotr", ""], ["Price", "Eric", ""], ["Woodruff", "David P.", ""]]}, {"id": "1106.0423", "submitter": "Girish Varma", "authors": "Vincenzo Bonifaci, Kurt Mehlhorn, Girish Varma", "title": "Physarum Can Compute Shortest Paths", "comments": "Accepted in SODA 2012", "journal-ref": "Journal of Theoretical Biology, 309:121-133, 2012", "doi": "10.1016/j.jtbi.2012.06.017", "report-no": null, "categories": "cs.DS cs.CE cs.ET cs.SY math.DS math.OC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physarum Polycephalum is a slime mold that is apparently able to solve\nshortest path problems.\n  A mathematical model has been proposed by biologists to describe the feedback\nmechanism used by the slime mold to adapt its tubular channels while foraging\ntwo food sources s0 and s1. We prove that, under this model, the mass of the\nmold will eventually converge to the shortest s0 - s1 path of the network that\nthe mold lies on, independently of the structure of the network or of the\ninitial mass distribution.\n  This matches the experimental observations by the biologists and can be seen\nas an example of a \"natural algorithm\", that is, an algorithm developed by\nevolution over millions of years.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 13:08:09 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2011 16:45:00 GMT"}, {"version": "v3", "created": "Wed, 12 Oct 2011 17:54:08 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Bonifaci", "Vincenzo", ""], ["Mehlhorn", "Kurt", ""], ["Varma", "Girish", ""]]}, {"id": "1106.0436", "submitter": "Venkatesan Guruswami", "authors": "Venkatesan Guruswami", "title": "Linear-algebraic list decoding of folded Reed-Solomon codes", "comments": "16 pages. Extended abstract in Proc. of IEEE Conference on\n  Computational Complexity (CCC), 2011", "journal-ref": null, "doi": "10.1109/CCC.2011.22", "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Folded Reed-Solomon codes are an explicit family of codes that achieve the\noptimal trade-off between rate and error-correction capability: specifically,\nfor any $\\eps > 0$, the author and Rudra (2006,08) presented an $n^{O(1/\\eps)}$\ntime algorithm to list decode appropriate folded RS codes of rate $R$ from a\nfraction $1-R-\\eps$ of errors. The algorithm is based on multivariate\npolynomial interpolation and root-finding over extension fields. It was noted\nby Vadhan that interpolating a linear polynomial suffices if one settles for a\nsmaller decoding radius (but still enough for a statement of the above form).\nHere we give a simple linear-algebra based analysis of this variant that\neliminates the need for the computationally expensive root-finding step over\nextension fields (and indeed any mention of extension fields). The entire list\ndecoding algorithm is linear-algebraic, solving one linear system for the\ninterpolation step, and another linear system to find a small subspace of\ncandidate solutions. Except for the step of pruning this subspace, the\nalgorithm can be implemented to run in {\\em quadratic} time. The theoretical\ndrawback of folded RS codes are that both the decoding complexity and proven\nworst-case list-size bound are $n^{\\Omega(1/\\eps)}$. By combining the above\nidea with a pseudorandom subset of all polynomials as messages, we get a Monte\nCarlo construction achieving a list size bound of $O(1/\\eps^2)$ which is quite\nclose to the existential $O(1/\\eps)$ bound (however, the decoding complexity\nremains $n^{\\Omega(1/\\eps)}$). Our work highlights that constructing an\nexplicit {\\em subspace-evasive} subset that has small intersection with\nlow-dimensional subspaces could lead to explicit codes with better\nlist-decoding guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 14:18:27 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Guruswami", "Venkatesan", ""]]}, {"id": "1106.0461", "submitter": "James King", "authors": "Luc Devroye and James King", "title": "Random hyperplane search trees in high dimensions", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set S of n \\geq d points in general position in R^d, a random\nhyperplane split is obtained by sampling d points uniformly at random without\nreplacement from S and splitting based on their affine hull. A random\nhyperplane search tree is a binary space partition tree obtained by recursive\napplication of random hyperplane splits. We investigate the structural\ndistributions of such random trees with a particular focus on the growth with\nd. A blessing of dimensionality arises--as d increases, random hyperplane\nsplits more closely resemble perfectly balanced splits; in turn, random\nhyperplane search trees more closely resemble perfectly balanced binary search\ntrees.\n  We prove that, for any fixed dimension d, a random hyperplane search tree\nstoring n points has height at most (1 + O(1/sqrt(d))) log_2 n and average\nelement depth at most (1 + O(1/d)) log_2 n with high probability as n\n\\rightarrow \\infty. Further, we show that these bounds are asymptotically\noptimal with respect to d.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 16:16:35 GMT"}], "update_date": "2011-06-03", "authors_parsed": [["Devroye", "Luc", ""], ["King", "James", ""]]}, {"id": "1106.0519", "submitter": "Yang Cai", "authors": "Yang Cai, Constantinos Daskalakis", "title": "Extreme-Value Theorems for Optimal Multidimensional Pricing", "comments": "58 pages, 2 figure, Appeared in FOCS 2011 and accepted to Games and\n  Economics Behavior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a near-optimal, computationally efficient algorithm for the\nunit-demand pricing problem, where a seller wants to price n items to optimize\nrevenue against a unit-demand buyer whose values for the items are\nindependently drawn from known distributions. For any chosen accuracy eps>0 and\nitem values bounded in [0,1], our algorithm achieves revenue that is optimal up\nto an additive error of at most eps, in polynomial time. For values sampled\nfrom Monotone Hazard Rate (MHR) distributions, we achieve a (1-eps)-fraction of\nthe optimal revenue in polynomial time, while for values sampled from regular\ndistributions the same revenue guarantees are achieved in quasi-polynomial\ntime.\n  Our algorithm for bounded distributions applies probabilistic techniques to\nunderstand the statistical properties of revenue distributions, obtaining a\nreduction in the search space of the algorithm via dynamic programming.\nAdapting this approach to MHR and regular distributions requires the proof of\nnovel extreme value theorems for such distributions.\n  As a byproduct, our techniques establish structural properties of\napproximately-optimal and near-optimal solutions. We show that, for values\nindependently distributed according to MHR distributions, pricing all items at\nthe same price achieves a constant fraction of the optimal revenue. Moreover,\nfor all eps >0, g(1/eps) distinct prices suffice to obtain a (1-eps)-fraction\nof the optimal revenue, where g(1/eps) is quadratic in 1/eps and independent of\nn. Similarly, for all eps>0 and n>0, at most g(1/(eps log n)) distinct prices\nsuffice if the values are independently distributed according to regular\ndistributions, where g() is a polynomial function. Finally, when the values are\ni.i.d. from some MHR distribution, we show that, if n is a sufficiently large\nfunction of 1/eps, a single price suffices to achieve a (1-eps)-fraction of the\noptimal revenue.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 21:37:16 GMT"}, {"version": "v2", "created": "Sun, 26 Oct 2014 20:31:08 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Cai", "Yang", ""], ["Daskalakis", "Constantinos", ""]]}, {"id": "1106.0683", "submitter": "Matthew Groff S.", "authors": "Matt Groff", "title": "Towards P = NP via k-SAT: A k-SAT Algorithm Using Linear Algebra on\n  Finite Fields", "comments": "28 pages, 25 figures, 1 picture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of P vs. NP is very serious, and solutions to the problem can\nhelp save lives. This article is an attempt at solving the problem using a\ncomputer algorithm. It is presented in a fashion that will hopefully allow for\neasy understanding for many people and scientists from many diverse fields.\n  In technical terms, a novel method for solving k-SAT is explained. This\nmethod is primarily based on linear algebra and finite fields. Evidence is\ngiven that this method may require rougly O(n^3) time and space for\ndeterministic models. More specifically the algorithm runs in time O(P\nV(n+V)^2) with mistaking satisfiable Boolean expressions as unsatisfiable with\nan approximate probablity 1 / \\Theta(V(n+V)^2)^P, where n is the number of\nclauses and V is the number of variables. It's concluded that significant\nevidence exists that P=NP.\n  There is a forum devoted to this paper at http://482527.ForumRomanum.com. All\nare invited to correspond here and help with the analysis of the algorithm.\nSource code for the associated algorithm can be found at\nhttps://sourceforge.net/p/la3sat.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 15:04:59 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2011 19:52:20 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Groff", "Matt", ""]]}, {"id": "1106.0874", "submitter": "Brad Shutters", "authors": "Brad Shutters, David Fern\\'andez-Baca", "title": "A Simple Characterization of the Minimal Obstruction Sets for\n  Three-State Perfect Phylogenies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lam, Gusfield, and Sridhar (2009) showed that a set of three-state characters\nhas a perfect phylogeny if and only if every subset of three characters has a\nperfect phylogeny. They also gave a complete characterization of the sets of\nthree three-state characters that do not have a perfect phylogeny. However, it\nis not clear from their characterization how to find a subset of three\ncharacters that does not have a perfect phylogeny without testing all triples\nof characters. In this note, we build upon their result by giving a simple\ncharacterization of when a set of three-state characters does not have a\nperfect phylogeny that can be inferred from testing all pairs of characters.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2011 05:10:38 GMT"}], "update_date": "2011-06-07", "authors_parsed": [["Shutters", "Brad", ""], ["Fern\u00e1ndez-Baca", "David", ""]]}, {"id": "1106.1049", "submitter": "Gregory Gutin", "authors": "Gregory Gutin and Anders Yeo", "title": "Hypercontractive Inequality for Pseudo-Boolean Functions of Bounded\n  Fourier Width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f:\\ \\{-1,1\\}^n\\rightarrow \\mathbb{R}$ is called pseudo-Boolean.\nIt is well-known that each pseudo-Boolean function $f$ can be written as\n$f(x)=\\sum_{I\\in {\\cal F}}\\hat{f}(I)\\chi_I(x),$ where ${\\cal F}\\subseteq \\{I:\\\nI\\subseteq [n]\\}$, $[n]=\\{1,2,...,n\\}$, and $\\chi_I(x)=\\prod_{i\\in I}x_i$ and\n$\\hat{f}(I)$ are non-zero reals. The degree of $f$ is $\\max \\{|I|:\\ I\\in {\\cal\nF}\\}$ and the width of $f$ is the minimum integer $\\rho$ such that every $i\\in\n[n]$ appears in at most $\\rho$ sets in $\\cal F$. For $i\\in [n]$, let\n$\\mathbf{x}_i$ be a random variable taking values 1 or -1 uniformly and\nindependently from all other variables $\\mathbf{x}_j$, $j\\neq i.$ Let\n$\\mathbf{x}=(\\mathbf{x}_1,...,\\mathbf{x}_n)$. The $p$-norm of $f$ is\n$||f||_p=(\\mathbb E[|f(\\mathbf{x})|^p])^{1/p}$ for any $p\\ge 1$. It is\nwell-known that $||f||_q\\ge ||f||_p$ whenever $q> p\\ge 1$. However, the higher\nnorm can be bounded by the lower norm times a coefficient not directly\ndepending on $f$: if $f$ is of degree $d$ and $q> p>1$ then $ ||f||_q\\le\n(\\frac{q-1}{p-1})^{d/2}||f||_p.$ This inequality is called the Hypercontractive\nInequality. We show that one can replace $d$ by $\\rho$ in the Hypercontractive\nInequality for each $q> p\\ge 2$ as follows: $ ||f||_q\\le\n((2r)!\\rho^{r-1})^{1/(2r)}||f||_p,$ where $r=\\lceil q/2\\rceil$. For the case\n$q=4$ and $p=2$, which is important in many applications, we prove a stronger\ninequality: $ ||f||_4\\le (2\\rho+1)^{1/4}||f||_2.$\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2011 12:47:37 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2011 14:34:27 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2012 13:07:33 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Gutin", "Gregory", ""], ["Yeo", "Anders", ""]]}, {"id": "1106.2122", "submitter": "Praveen Manjunatha", "authors": "M. Praveen and Kamal Lodaya", "title": "Parameterized complexity results for 1-safe Petri nets", "comments": "Full version of the paper appearing in CONCUR 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We associate a graph with a 1-safe Petri net and study the parameterized\ncomplexity of various problems with parameters derived from the graph. With\ntreewidth as the parameter, we give W[1]-hardness results for many problems\nabout 1-safe Petri nets. As a corollary, this proves a conjecture of Downey et.\nal. about the hardness of some graph pebbling problems. We consider the\nparameter benefit depth (that is known to be helpful in getting better\nalgorithms for general Petri nets) and again give W[1]-hardness results for\nvarious problems on 1-safe Petri nets. We also consider the stronger parameter\nvertex cover number. Combining the well known automata-theoretic method and a\npowerful fixed parameter tractability (FPT) result about Integer Linear\nProgramming, we give a FPT algorithm for model checking Monadic Second Order\n(MSO) formulas on 1-safe Petri nets, with parameters vertex cover number and\nthe size of the formula.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 17:11:53 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Praveen", "M.", ""], ["Lodaya", "Kamal", ""]]}, {"id": "1106.2263", "submitter": "David Miguel Antunes", "authors": "David Miguel Antunes, David Martins de Matos, Jos\\'e Gaspar", "title": "A Library for Implementing the Multiple Hypothesis Tracking Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multiple Hypothesis Tracking (MHT) algorithm is known to produce good\nresults in difficult multi-target tracking situations. However, its\nimplementation is not trivial, and is associated with a significant programming\neffort, code size and long implementation time. We propose a library which\naddresses these problems by providing a domain independent implementation of\nthe most complex MHT operations. We also address the problem of applying\nclustering in domain independent manner.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2011 22:32:32 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Antunes", "David Miguel", ""], ["de Matos", "David Martins", ""], ["Gaspar", "Jos\u00e9", ""]]}, {"id": "1106.2294", "submitter": "Kewen Liao", "authors": "Kewen Liao and Hong Shen", "title": "Unconstrained and Constrained Fault-Tolerant Resource Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First, we study the Unconstrained Fault-Tolerant Resource Allocation (UFTRA)\nproblem (a.k.a. FTFA problem in \\cite{shihongftfa}). In the problem, we are\ngiven a set of sites equipped with an unconstrained number of facilities as\nresources, and a set of clients with set $\\mathcal{R}$ as corresponding\nconnection requirements, where every facility belonging to the same site has an\nidentical opening (operating) cost and every client-facility pair has a\nconnection cost. The objective is to allocate facilities from sites to satisfy\n$\\mathcal{R}$ at a minimum total cost. Next, we introduce the Constrained\nFault-Tolerant Resource Allocation (CFTRA) problem. It differs from UFTRA in\nthat the number of resources available at each site $i$ is limited by $R_{i}$.\nBoth problems are practical extensions of the classical Fault-Tolerant Facility\nLocation (FTFL) problem \\cite{Jain00FTFL}. For instance, their solutions\nprovide optimal resource allocation (w.r.t. enterprises) and leasing (w.r.t.\nclients) strategies for the contemporary cloud platforms.\n  In this paper, we consider the metric version of the problems. For UFTRA with\nuniform $\\mathcal{R}$, we present a star-greedy algorithm. The algorithm\nachieves the approximation ratio of 1.5186 after combining with the cost\nscaling and greedy augmentation techniques similar to\n\\cite{Charikar051.7281.853,Mahdian021.52}, which significantly improves the\nresult of \\cite{shihongftfa} using a phase-greedy algorithm. We also study the\ncapacitated extension of UFTRA and give a factor of 2.89. For CFTRA with\nuniform $\\mathcal{R}$, we slightly modify the algorithm to achieve\n1.5186-approximation. For a more general version of CFTRA, we show that it is\nreducible to FTFL using linear programming.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2011 08:59:06 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Liao", "Kewen", ""], ["Shen", "Hong", ""]]}, {"id": "1106.2301", "submitter": "Sergey Yakhontov V", "authors": "Sergey V. Yakhontov", "title": "A simple algorithm for the evaluation of the hypergeometric series using\n  quasi-linear time and linear space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple algorithm with quasi-linear time complexity and linear space\ncomplexity for the evaluation of the hypergeometric series with rational\ncoefficients is constructed. It is shown that this algorithm is suitable in\npractical informatics for constructive analogues of often used constants of\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2011 10:33:05 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2012 10:09:25 GMT"}], "update_date": "2012-08-08", "authors_parsed": [["Yakhontov", "Sergey V.", ""]]}, {"id": "1106.2351", "submitter": "Aleksandar Ilic", "authors": "Aleksandar Ilic and Andreja Ilic", "title": "On vertex covers and matching number of trapezoid graphs", "comments": "9 pages, 1 figure, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intersection graph of a collection of trapezoids with corner points lying\non two parallel lines is called a trapezoid graph. Using binary indexed tree\ndata structure, we improve algorithms for calculating the size and the number\nof minimum vertex covers (or independent sets), as well as the total number of\nvertex covers, and reduce the time complexity from $O (n^2)$ to $O (n \\log n)$,\nwhere $n$ is the number of trapezoids. Furthermore, we present the family of\ncounterexamples for recently proposed algorithm with time complexity $O (n^2)$\nfor calculating the maximum cardinality matching in trapezoid graphs.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2011 22:11:56 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Ilic", "Aleksandar", ""], ["Ilic", "Andreja", ""]]}, {"id": "1106.2587", "submitter": "Christopher Hoobin", "authors": "Christopher Hoobin, Simon J. Puglisi and Justin Zobel", "title": "Relative Lempel-Ziv Factorization for Efficient Storage and Retrieval of\n  Web Collections", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  265-273 (2011)", "doi": null, "report-no": "vol5no3/p265_christopherhoobin_vldb2012", "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression techniques that support fast random access are a core component\nof any information system. Current state-of-the-art methods group documents\ninto fixed-sized blocks and compress each block with a general-purpose adaptive\nalgorithm such as GZIP. Random access to a specific document then requires\ndecompression of a block. The choice of block size is critical: it trades\nbetween compression effectiveness and document retrieval times. In this paper\nwe present a scalable compression method for large document collections that\nallows fast random access. We build a representative sample of the collection\nand use it as a dictionary in a LZ77-like encoding of the rest of the\ncollection, relative to the dictionary. We demonstrate on large collections,\nthat using a dictionary as small as 0.1% of the collection size, our algorithm\nis dramatically faster than previous methods, and in general gives much better\ncompression.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 00:53:40 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2011 03:26:13 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hoobin", "Christopher", ""], ["Puglisi", "Simon J.", ""], ["Zobel", "Justin", ""]]}, {"id": "1106.2603", "submitter": "Sam Ma", "authors": "Chengxi Ye, Zhanshan Sam Ma, Charles H. Cannon, Mihai Pop, Douglas W.\n  Yu", "title": "SparseAssembler: de novo Assembly with the Sparse de Bruijn Graph", "comments": "Corresponding author: Douglas W. Yu, dougwyu@gmail.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  de Bruijn graph-based algorithms are one of the two most widely used\napproaches for de novo genome assembly. A major limitation of this approach is\nthe large computational memory space requirement to construct the de Bruijn\ngraph, which scales with k-mer length and total diversity (N) of unique k-mers\nin the genome expressed in base pairs or roughly (2k+8)N bits. This limitation\nis particularly important with large-scale genome analysis and for sequencing\ncenters that simultaneously process multiple genomes. We present a sparse de\nBruijn graph structure, based on which we developed SparseAssembler that\ngreatly reduces memory space requirements. The structure also allows us to\nintroduce a novel method for the removal of substitution errors introduced\nduring sequencing. The sparse de Bruijn graph structure skips g intermediate\nk-mers, therefore reducing the theoretical memory space requirement to\n~(2k/g+8)N. We have found that a practical value of g=16 consumes approximately\n10% of the memory required by standard de Bruijn graph-based algorithms but\nyields comparable results. A high error rate could potentially derail the\nSparseAssembler. Therefore, we developed a sparse de Bruijn graph-based\ndenoising algorithm that can remove more than 99% of substitution errors from\ndatasets with a \\leq 2% error rate. Given that substitution error rates for the\ncurrent generation of sequencers is lower than 1%, our denoising procedure is\nsufficiently effective to safeguard the performance of our algorithm. Finally,\nwe also introduce a novel Dijkstra-like breadth-first search algorithm for the\nsparse de Bruijn graph structure to circumvent residual errors and resolve\npolymorphisms.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 04:06:06 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Ye", "Chengxi", ""], ["Ma", "Zhanshan Sam", ""], ["Cannon", "Charles H.", ""], ["Pop", "Mihai", ""], ["Yu", "Douglas W.", ""]]}, {"id": "1106.2619", "submitter": "Chandan K. Dubey", "authors": "Chandan Dubey, Thomas Holenstein", "title": "Approximating the Closest Vector Problem Using an Approximate Shortest\n  Vector Oracle", "comments": "10 pages, Published in APPROX 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial time Turing reduction from the\n$\\gamma^2\\sqrt{n}$-approximate closest vector problem on a lattice of dimension\n$n$ to a $\\gamma$-approximate oracle for the shortest vector problem. This is\nan improvement over a reduction by Kannan, which achieved $\\gamma^2n^{3/2}$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 06:30:29 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2011 10:14:34 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Dubey", "Chandan", ""], ["Holenstein", "Thomas", ""]]}, {"id": "1106.2694", "submitter": "Antonios Symvonis", "authors": "Evmorfia N. Argyriou, Michael A. Bekos, Michael Kaufmann, Antonios\n  Symvonis", "title": "Geometric Simultaneous RAC Drawings of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and study \"geometric simultaneous RAC drawing\nproblems\", i.e., a combination of problems on geometric RAC drawings and\ngeometric simultaneous graph drawings. To the best of our knowledge, this is\nthe first time where such a combination is attempted.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 12:42:31 GMT"}], "update_date": "2011-06-15", "authors_parsed": [["Argyriou", "Evmorfia N.", ""], ["Bekos", "Michael A.", ""], ["Kaufmann", "Michael", ""], ["Symvonis", "Antonios", ""]]}, {"id": "1106.2720", "submitter": "Massimiliano Sala Prof.", "authors": "Edoardo Ballico and Michele Elia and Massimiliano Sala", "title": "Complexity of multivariate polynomial evaluation", "comments": null, "journal-ref": "Journal of Symbolic Computation, 2013, vol. 50, p. 255-262", "doi": "10.1016/j.jsc.2012.07.005", "report-no": null, "categories": "math.AC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to evaluate multivariate polynomials over a finite field\nand discuss its multiplicative complexity.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 14:08:15 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Ballico", "Edoardo", ""], ["Elia", "Michele", ""], ["Sala", "Massimiliano", ""]]}, {"id": "1106.3037", "submitter": "Aleksandar Ilic", "authors": "Aleksandar Ilic", "title": "Efficient algorithm for the vertex connectivity of trapezoid graphs", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intersection graph of a collection of trapezoids with corner points lying\non two parallel lines is called a trapezoid graph. These graphs and their\ngeneralizations were applied in various fields, including modeling channel\nrouting problems in VLSI design and identifying the optimal chain of\nnon-overlapping fragments in bioinformatics. Using modified binary indexed tree\ndata structure, we design an algorithm for calculating the vertex connectivity\nof trapezoid graph $G$ with time complexity $O (n \\log n)$, where $n$ is the\nnumber of trapezoids. Furthermore, we establish sufficient and necessary\ncondition for a trapezoid graph $G$ to be bipartite and characterize trees that\ncan be represented as trapezoid graphs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 17:48:52 GMT"}], "update_date": "2011-06-16", "authors_parsed": [["Ilic", "Aleksandar", ""]]}, {"id": "1106.3126", "submitter": "Yuichi Yoshida", "authors": "Yuichi Yoshida", "title": "Testing List H-Homomorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $H$ be an undirected graph. In the List $H$-Homomorphism Problem, given\nan undirected graph $G$ with a list constraint $L(v) \\subseteq V(H)$ for each\nvariable $v \\in V(G)$, the objective is to find a list $H$-homomorphism $f:V(G)\n\\to V(H)$, that is, $f(v) \\in L(v)$ for every $v \\in V(G)$ and $(f(u),f(v)) \\in\nE(H)$ whenever $(u,v) \\in E(G)$.\n  We consider the following problem: given a map $f:V(G) \\to V(H)$ as an oracle\naccess, the objective is to decide with high probability whether $f$ is a list\n$H$-homomorphism or \\textit{far} from any list $H$-homomorphisms. The\nefficiency of an algorithm is measured by the number of accesses to $f$.\n  In this paper, we classify graphs $H$ with respect to the query complexity\nfor testing list $H$-homomorphisms and show the following trichotomy holds: (i)\nList $H$-homomorphisms are testable with a constant number of queries if and\nonly if $H$ is a reflexive complete graph or an irreflexive complete bipartite\ngraph. (ii) List $H$-homomorphisms are testable with a sublinear number of\nqueries if and only if $H$ is a bi-arc graph. (iii) Testing list\n$H$-homomorphisms requires a linear number of queries if $H$ is not a bi-arc\ngraph.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2011 01:21:34 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Yoshida", "Yuichi", ""]]}, {"id": "1106.3325", "submitter": "Daniel Wilkerson", "authors": "Daniel Shawcross Wilkerson, Simon Fredrick Vicente Goldsmith, Ryan\n  Barrett, Erick Armbrust, Robert Johnson, Alfred Fuller", "title": "Distributed Transactions for Google App Engine: Optimistic Distributed\n  Transactions built upon Local Multi-Version Concurrency Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively scalable web applications encounter a fundamental tension in\ncomputing between \"performance\" and \"correctness\": performance is often\naddressed by using a large and therefore distributed machine where programs are\nmulti-threaded and interruptible, whereas correctness requires data invariants\nto be maintained with certainty. A solution to this problem is \"transactions\"\n[Gray-Reuter].\n  Some distributed systems such as Google App Engine\n[http://code.google.com/appengine/docs/] provide transaction semantics but only\nfor functions that access one of a set of predefined local regions of the\ndatabase: a \"Local Transaction\" (LT)\n[http://code.google.com/appengine/docs/python/datastore/transactions.html]. To\naddress this problem we give a \"Distributed Transaction\" (DT) algorithm which\nprovides transaction semantics for functions that operate on any set of objects\ndistributed across the machine. Our algorithm is in an \"optimistic\"\n[http://en.wikipedia.org/wiki/Optimistic_concurrency_control] style. We assume\nSequential [Time-]Consistency\n[http://en.wikipedia.org/wiki/Sequential_consistency] for Local Transactions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 00:00:22 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Wilkerson", "Daniel Shawcross", ""], ["Goldsmith", "Simon Fredrick Vicente", ""], ["Barrett", "Ryan", ""], ["Armbrust", "Erick", ""], ["Johnson", "Robert", ""], ["Fuller", "Alfred", ""]]}, {"id": "1106.3527", "submitter": "Stefan Szeider", "authors": "Gregory Gutin, Eun Jung Kim, Arezou Soleimanfallah, Stefan Szeider,\n  Anders Yeo", "title": "Parameterized Complexity Results for General Factors in Bipartite Graphs\n  with an Application to Constraint Programming", "comments": "Full version of a paper that appeared in preliminary form in the\n  proceedings of IPEC'10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NP-hard general factor problem asks, given a graph and for each vertex a\nlist of integers, whether the graph has a spanning subgraph where each vertex\nhas a degree that belongs to its assigned list. The problem remains NP-hard\neven if the given graph is bipartite with partition U+V, and each vertex in U\nis assigned the list {1}; this subproblem appears in the context of constraint\nprogramming as the consistency problem for the extended global cardinality\nconstraint. We show that this subproblem is fixed-parameter tractable when\nparameterized by the size of the second partite set V. More generally, we show\nthat the general factor problem for bipartite graphs, parameterized by |V|, is\nfixed-parameter tractable as long as all vertices in U are assigned lists of\nlength 1, but becomes W[1]-hard if vertices in U are assigned lists of length\nat most 2. We establish fixed-parameter tractability by reducing the problem\ninstance to a bounded number of acyclic instances, each of which can be solved\nin polynomial time by dynamic programming.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 16:19:57 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gutin", "Gregory", ""], ["Kim", "Eun Jung", ""], ["Soleimanfallah", "Arezou", ""], ["Szeider", "Stefan", ""], ["Yeo", "Anders", ""]]}, {"id": "1106.3628", "submitter": "Haim Kaplan", "authors": "Haim Kaplan and Micha Sharir", "title": "Finding the Maximal Empty Rectangle Containing a Query Point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a set of $n$ points in an axis-parallel rectangle $B$ in the\nplane. We present an $O(n\\alpha(n)\\log^4 n)$-time algorithm to preprocess $P$\ninto a data structure of size $O(n\\alpha(n)\\log^3 n)$, such that, given a query\npoint $q$, we can find, in $O(\\log^4 n)$ time, the largest-area axis-parallel\nrectangle that is contained in $B$, contains $q$, and its interior contains no\npoint of $P$. This is a significant improvement over the previous solution of\nAugustine {\\em et al.} \\cite{qmex}, which uses slightly superquadratic\npreprocessing and storage.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2011 08:56:46 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Kaplan", "Haim", ""], ["Sharir", "Micha", ""]]}, {"id": "1106.3680", "submitter": "Michael Huber", "authors": "Michael Huber", "title": "Efficient Two-Stage Group Testing Algorithms for DNA Screening", "comments": "12 pages; accepted for ICALP 2011 Group Testing Workshop: Algorithms\n  and Data Structures for selection, identification and encoding (ICALP 2011\n  GT, Zurich)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CE cs.DS cs.IT math.CO math.IT q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group testing algorithms are very useful tools for DNA library screening.\nBuilding on recent work by Levenshtein (2003) and Tonchev (2008), we construct\nin this paper new infinite classes of combinatorial structures, the existence\nof which are essential for attaining the minimum number of individual tests at\nthe second stage of a two-stage disjunctive testing algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2011 19:07:29 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Huber", "Michael", ""]]}, {"id": "1106.3951", "submitter": "Venkatesan Guruswami", "authors": "Venkatesan Guruswami, Carol Wang", "title": "Optimal rate list decoding via derivative codes", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical family of $[n,k]_q$ Reed-Solomon codes over a field $\\F_q$\nconsist of the evaluations of polynomials $f \\in \\F_q[X]$ of degree $< k$ at\n$n$ distinct field elements. In this work, we consider a closely related family\nof codes, called (order $m$) {\\em derivative codes} and defined over fields of\nlarge characteristic, which consist of the evaluations of $f$ as well as its\nfirst $m-1$ formal derivatives at $n$ distinct field elements. For large enough\n$m$, we show that these codes can be list-decoded in polynomial time from an\nerror fraction approaching $1-R$, where $R=k/(nm)$ is the rate of the code.\nThis gives an alternate construction to folded Reed-Solomon codes for achieving\nthe optimal trade-off between rate and list error-correction radius. Our\ndecoding algorithm is linear-algebraic, and involves solving a linear system to\ninterpolate a multivariate polynomial, and then solving another structured\nlinear system to retrieve the list of candidate polynomials $f$. The algorithm\nfor derivative codes offers some advantages compared to a similar one for\nfolded Reed-Solomon codes in terms of efficient unique decoding in the presence\nof side information.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 15:51:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Wang", "Carol", ""]]}, {"id": "1106.4141", "submitter": "Stefan Kratsch", "authors": "Hans L. Bodlaender and Bart M. P. Jansen and Stefan Kratsch", "title": "Kernel Bounds for Path and Cycle Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectivity problems like k-Path and k-Disjoint Paths relate to many\nimportant milestones in parameterized complexity, namely the Graph Minors\nProject, color coding, and the recent development of techniques for obtaining\nkernelization lower bounds. This work explores the existence of polynomial\nkernels for various path and cycle problems, by considering nonstandard\nparameterizations. We show polynomial kernels when the parameters are a given\nvertex cover, a modulator to a cluster graph, or a (promised) max leaf number.\nWe obtain lower bounds via cross-composition, e.g., for Hamiltonian Cycle and\nrelated problems when parameterized by a modulator to an outerplanar graph.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 09:14:42 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2011 10:51:54 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Jansen", "Bart M. P.", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1106.4267", "submitter": "Gilles Brassard", "authors": "Gilles Brassard, Frederic Dupuis, Sebastien Gambs, Alain Tapp", "title": "An optimal quantum algorithm to approximate the mean and its application\n  for approximating the median of a set of points over an arbitrary distance", "comments": "Ten pages, no figures, three algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe two quantum algorithms to approximate the mean value of a\nblack-box function. The first algorithm is novel and asymptotically optimal\nwhile the second is a variation on an earlier algorithm due to Aharonov. Both\nalgorithms have their own strengths and caveats and may be relevant in\ndifferent contexts. We then propose a new algorithm for approximating the\nmedian of a set of points over an arbitrary distance function.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 17:14:46 GMT"}], "update_date": "2011-06-22", "authors_parsed": [["Brassard", "Gilles", ""], ["Dupuis", "Frederic", ""], ["Gambs", "Sebastien", ""], ["Tapp", "Alain", ""]]}, {"id": "1106.4412", "submitter": "Markus Jalsenius", "authors": "Raphael Clifford, Markus Jalsenius, Ely Porat, Benjamin Sach", "title": "Space Lower Bounds for Online Pattern Matching", "comments": "10 pages, 7 figures, CPM 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present space lower bounds for online pattern matching under a number of\ndifferent distance measures. Given a pattern of length m and a text that\narrives one character at a time, the online pattern matching problem is to\nreport the distance between the pattern and a sliding window of the text as\nsoon as the new character arrives. We require that the correct answer is given\nat each position with constant probability. We give Omega(m) bit space lower\nbounds for L_1, L_2, L_\\infty, Hamming, edit and swap distances as well as for\nany algorithm that computes the cross-correlation/convolution. We then show a\ndichotomy between distance functions that have wildcard-like properties and\nthose that do not. In the former case which includes, as an example, pattern\nmatching with character classes, we give Omega(m) bit space lower bounds. For\nother distance functions, we show that there exist space bounds of Omega(log m)\nand O(log^2 m) bits. Finally we discuss space lower bounds for non-binary\ninputs and show how in some cases they can be improved.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 10:39:48 GMT"}], "update_date": "2011-06-23", "authors_parsed": [["Clifford", "Raphael", ""], ["Jalsenius", "Markus", ""], ["Porat", "Ely", ""], ["Sach", "Benjamin", ""]]}, {"id": "1106.4454", "submitter": "Gregory Gutin", "authors": "Robert Crowston, Gregory Gutin, Mark Jones and Anders Yeo", "title": "Parameterized Eulerian Strong Component Arc Deletion Problem on\n  Tournaments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem {\\sc Min-DESC}, we are given a digraph $D$ and an integer $k$,\nand asked if there exists a set $A'$ of at most $k$ arcs in $D$, such that if\nwe remove the arcs of $A'$, in the resulting digraph every strong component is\nEulerian. {\\sc Min-DESC} is NP-hard; Cechl\\'{a}rov\\'{a} and Schlotter (IPEC\n2010) asked if the problem is fixed-parameter tractable when parameterized by\n$k$. We consider the subproblem of{\\sc Min-DESC} when $D$ is a tournament. We\nshow that this problem is fixed-parameter tractable with respect to $k$.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 14:13:10 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2011 10:02:19 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Crowston", "Robert", ""], ["Gutin", "Gregory", ""], ["Jones", "Mark", ""], ["Yeo", "Anders", ""]]}, {"id": "1106.4475", "submitter": "Eirini Spyropoulou", "authors": "Eirini Spyropoulou and Tijl De Bie", "title": "Interesting Multi-Relational Patterns", "comments": "Accepted at ICDM'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining patterns from multi-relational data is a problem attracting increasing\ninterest within the data mining community. Traditional data mining approaches\nare typically developed for highly simplified types of data, such as an\nattribute-value table or a binary database, such that those methods are not\ndirectly applicable to multi-relational data. Nevertheless, multi-relational\ndata is a more truthful and therefore often also a more powerful representation\nof reality. Mining patterns of a suitably expressive syntax directly from this\nrepresentation, is thus a research problem of great importance. In this paper\nwe introduce a novel approach to mining patterns in multi-relational data. We\npropose a new syntax for multi-relational patterns as complete connected\nsubgraphs in a representation of the database as a K-partite graph. We show how\nthis pattern syntax is generally applicable to multirelational data, while it\nreduces to well-known tiles [7] when the data is a simple binary or\nattribute-value table. We propose RMiner, an efficient algorithm to mine such\npatterns, and we introduce a method for quantifying their interestingness when\ncontrasted with prior information of the data miner. Finally, we illustrate the\nusefulness of our approach by discussing results on real-world and synthetic\ndatabases.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 15:12:30 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2011 09:57:38 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Spyropoulou", "Eirini", ""], ["De Bie", "Tijl", ""]]}, {"id": "1106.4489", "submitter": "Antonio Fern\\'andez Anta", "authors": "Jose Luis Lopez-Presa and Antonio Fernandez Anta", "title": "Fast Isomorphism Testing of Graphs with Regularly-Connected Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graph Isomorphism problem has both theoretical and practical interest. In\nthis paper we present an algorithm, called conauto-1.2, that efficiently tests\nwhether two graphs are isomorphic, and finds an isomorphism if they are. This\nalgorithm is an improved version of the algorithm conauto, which has been shown\nto be very fast for random graphs and several families of hard graphs. In this\npaper we establish a new theorem that allows, at very low cost, the easy\ndiscovery of many automorphisms. This result is especially suited for graphs\nwith regularly connected components, and can be applied in any isomorphism\ntesting and canonical labeling algorithm to drastically improve its\nperformance. In particular, algorithm conauto-1.2 is obtained by the\napplication of this result to conauto. The resulting algorithm preserves all\nthe nice features of conauto, but drastically improves the testing of graphs\nwith regularly connected components. We run extensive experiments, which show\nthat the most popular algorithms (namely, nauty and bliss) can not compete with\nconauto-1.2 for these graph families.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 15:59:40 GMT"}], "update_date": "2011-06-23", "authors_parsed": [["Lopez-Presa", "Jose Luis", ""], ["Anta", "Antonio Fernandez", ""]]}, {"id": "1106.4587", "submitter": "Krzysztof Onak", "authors": "Alan Edelman and Avinatan Hassidim and Huy N. Nguyen and Krzysztof\n  Onak", "title": "An Efficient Partitioning Oracle for Bounded-Treewidth Graphs", "comments": "Full version of a paper to appear in RANDOM 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioning oracles were introduced by Hassidim et al. (FOCS 2009) as a\ngeneric tool for constant-time algorithms. For any epsilon > 0, a partitioning\noracle provides query access to a fixed partition of the input bounded-degree\nminor-free graph, in which every component has size poly(1/epsilon), and the\nnumber of edges removed is at most epsilon*n, where n is the number of vertices\nin the graph.\n  However, the oracle of Hassidimet al. makes an exponential number of queries\nto the input graph to answer every query about the partition. In this paper, we\nconstruct an efficient partitioning oracle for graphs with constant treewidth.\nThe oracle makes only O(poly(1/epsilon)) queries to the input graph to answer\neach query about the partition.\n  Examples of bounded-treewidth graph classes include k-outerplanar graphs for\nfixed k, series-parallel graphs, cactus graphs, and pseudoforests. Our oracle\nyields poly(1/epsilon)-time property testing algorithms for membership in these\nclasses of graphs. Another application of the oracle is a poly(1/epsilon)-time\nalgorithm that approximates the maximum matching size, the minimum vertex cover\nsize, and the minimum dominating set size up to an additive epsilon*n in graphs\nwith bounded treewidth. Finally, the oracle can be used to test in\npoly(1/epsilon) time whether the input bounded-treewidth graph is k-colorable\nor perfect.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 21:57:02 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Edelman", "Alan", ""], ["Hassidim", "Avinatan", ""], ["Nguyen", "Huy N.", ""], ["Onak", "Krzysztof", ""]]}, {"id": "1106.4649", "submitter": "Gonzalo Navarro", "authors": "Gonzalo Navarro and Yakov Nekrich and Lu\\'is M. S. Russo", "title": "Space-Efficient Data-Analysis Queries on Grids", "comments": "20 pages, 2 figures, submitting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider various data-analysis queries on two-dimensional points. We give\nnew space/time tradeoffs over previous work on geometric queries such as\ndominance and rectangle visibility, and on semigroup and group queries such as\nsum, average, variance, minimum and maximum. We also introduce new solutions to\nqueries less frequently considered in the literature such as two-dimensional\nquantiles, majorities, successor/predecessor, mode, and various top-$k$\nqueries, considering static and dynamic scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 08:15:24 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2012 21:41:24 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Nekrich", "Yakov", ""], ["Russo", "Lu\u00eds M. S.", ""]]}, {"id": "1106.4677", "submitter": "Oren Ben-Zwi", "authors": "Oren Ben-Zwi and Ilan Newman", "title": "Optimal Bi-Valued Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate \\emph{bi-valued} auctions in the digital good setting and\nconstruct an explicit polynomial time deterministic auction. We prove an\nunconditional tight lower bound which holds even for random superpolynomial\nauctions. The analysis of the construction uses the adoption of the finer lens\nof \\emph{general competitiveness} which considers additive losses on top of\nmultiplicative ones. The result implies that general competitiveness is the\nright notion to use in this setting, as this optimal auction is uncompetitive\nwith respect to competitive measures which do not consider additive losses.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 10:34:33 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Ben-Zwi", "Oren", ""], ["Newman", "Ilan", ""]]}, {"id": "1106.4719", "submitter": "Marc Thurley", "authors": "Lukas Moll, Siamak Tazari, Marc Thurley", "title": "Computing hypergraph width measures exactly", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph width measures are a class of hypergraph invariants important in\nstudying the complexity of constraint satisfaction problems (CSPs). We present\na general exact exponential algorithm for a large variety of these measures. A\nconnection between these and tree decompositions is established. This enables\nus to almost seamlessly adapt the combinatorial and algorithmic results known\nfor tree decompositions of graphs to the case of hypergraphs and obtain fast\nexact algorithms.\n  As a consequence, we provide algorithms which, given a hypergraph H on n\nvertices and m hyperedges, compute the generalized hypertree-width of H in time\nO*(2^n) and compute the fractional hypertree-width of H in time\nO(m*1.734601^n).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 13:51:33 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Moll", "Lukas", ""], ["Tazari", "Siamak", ""], ["Thurley", "Marc", ""]]}, {"id": "1106.5076", "submitter": "Patrick Nicholson", "authors": "Meng He and J. Ian Munro and Patrick K. Nicholson", "title": "Dynamic Range Selection in Linear Space", "comments": "11 pages (lncs fullpage). This is a corrected version of the\n  preliminary version of the paper that appeared in ISAAC 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $S$ of $n$ points in the plane, we consider the problem of\nanswering range selection queries on $S$: that is, given an arbitrary $x$-range\n$Q$ and an integer $k > 0$, return the $k$-th smallest $y$-coordinate from the\nset of points that have $x$-coordinates in $Q$. We present a linear space data\nstructure that maintains a dynamic set of $n$ points in the plane with real\ncoordinates, and supports range selection queries in $O((\\lg n / \\lg \\lg n)^2)$\ntime, as well as insertions and deletions in $O((\\lg n / \\lg \\lg n)^2)$\namortized time. The space usage of this data structure is an $\\Theta(\\lg n /\n\\lg \\lg n)$ factor improvement over the previous best result, while maintaining\nasymptotically matching query and update times. We also present a succinct data\nstructure that supports range selection queries on a dynamic array of $n$\nvalues drawn from a bounded universe.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 22:27:37 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2011 18:04:37 GMT"}, {"version": "v3", "created": "Wed, 8 May 2013 17:46:10 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["He", "Meng", ""], ["Munro", "J. Ian", ""], ["Nicholson", "Patrick K.", ""]]}, {"id": "1106.5736", "submitter": "Erik Demaine", "authors": "Erik D. Demaine, Martin L. Demaine, Sarah Eisenstat, Anna Lubiw,\n  Andrew Winslow", "title": "Algorithms for Solving Rubik's Cubes", "comments": "34 pages, 9 figures. A short version of this paper is to appear at\n  the 19th Annual European Symposium on Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rubik's Cube is perhaps the world's most famous and iconic puzzle,\nwell-known to have a rich underlying mathematical structure (group theory). In\nthis paper, we show that the Rubik's Cube also has a rich underlying\nalgorithmic structure. Specifically, we show that the n x n x n Rubik's Cube,\nas well as the n x n x 1 variant, has a \"God's Number\" (diameter of the\nconfiguration space) of Theta(n^2/log n). The upper bound comes from\neffectively parallelizing standard Theta(n^2) solution algorithms, while the\nlower bound follows from a counting argument. The upper bound gives an\nasymptotically optimal algorithm for solving a general Rubik's Cube in the\nworst case. Given a specific starting state, we show how to find the shortest\nsolution in an n x O(1) x O(1) Rubik's Cube. Finally, we show that finding this\noptimal solution becomes NP-hard in an n x n x 1 Rubik's Cube when the\npositions and colors of some of the cubies are ignored (not used in determining\nwhether the cube is solved).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 17:35:09 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["Eisenstat", "Sarah", ""], ["Lubiw", "Anna", ""], ["Winslow", "Andrew", ""]]}, {"id": "1106.5845", "submitter": "Taisuke Izumi", "authors": "Taisuke Izumi, Tomoko Izumi, Hirotaka Ono, Koichi Wada", "title": "Minimum Certificate Dispersal with Tree Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an n-vertex graph G=(V,E) and a set R \\subseteq {{x,y} | x,y \\in V} of\nrequests, we consider to assign a set of edges to each vertex in G so that for\nevery request {u, v} in R the union of the edge sets assigned to u and v\ncontains a path from u to v. The Minimum Certificate Dispersal Problem (MCD) is\ndefined as one to find an assignment that minimizes the sum of the cardinality\nof the edge set assigned to each vertex. This problem has been shown to be\nLOGAPX-complete for the most general setting, and APX-hard and 2-approximable\nin polynomial time for dense request sets, where R forms a clique. In this\npaper, we investigate the complexity of MCD with sparse (tree) structures. We\nfirst show that MCD is APX-hard when R is a tree, even a star. We then explore\nthe problem from the viewpoint of the maximum degree \\Delta of the tree: MCD\nfor tree request set with constant \\Delta is solvable in polynomial time, while\nthat with \\Delta=\\Omega(n) is 2.56-approximable in polynomial time but hard to\napproximate within 1.01 unless P=NP. As for the structure of G itself, we show\nthat the problem can be solved in polynomial time if G is a tree.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 05:58:37 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Izumi", "Taisuke", ""], ["Izumi", "Tomoko", ""], ["Ono", "Hirotaka", ""], ["Wada", "Koichi", ""]]}, {"id": "1106.5906", "submitter": "An Zeng", "authors": "An Zeng, Linyuan L\\\"u, Tao Zhou", "title": "Reconstructing directed networks for better synchronization", "comments": "7 pages, 5 figures", "journal-ref": "New J. Phys. 14, 083006 (2012)", "doi": "10.1088/1367-2630/14/8/083006", "report-no": null, "categories": "physics.soc-ph cs.DS nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we studied the strategies to enhance synchronization on\ndirected networks by manipulating a fixed number of links. We proposed a\ncentrality-based reconstructing (CBR) method, where the node centrality is\nmeasured by the well-known PageRank algorithm. Extensive numerical simulation\non many modeled networks demonstrated that the CBR method is more effective in\nfacilitating synchronization than the degree-based reconstructing method and\nrandom reconstructing method for adding or removing links. The reason is that\nCBR method can effectively narrow the incoming degree distribution and\nreinforce the hierarchical structure of the network. Furthermore, we apply the\nCBR method to links rewiring procedure where at each step one link is removed\nand one new link is added. The CBR method helps to decide which links should be\nremoved or added. After several steps, the resulted networks are very close to\nthe optimal structure from the evolutionary optimization algorithm. The\nnumerical simulations on the Kuramoto model further demonstrate that our method\nhas advantage in shortening the convergence time to synchronization on directed\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 11:20:34 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Zeng", "An", ""], ["L\u00fc", "Linyuan", ""], ["Zhou", "Tao", ""]]}, {"id": "1106.5930", "submitter": "Stefka Bouyuklieva", "authors": "Stefka Bouyuklieva and Iliya Bouyukliev", "title": "An Algorithm for Classification of Binary Self-Dual Codes", "comments": "The title is changed", "journal-ref": "IEEE Transactions on Information Theory, vol. 58, pp. 3933--3940,\n  2012", "doi": "10.1109/TIT.2012.2190134", "report-no": null, "categories": "math.CO cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient algorithm for classification of binary self-dual codes is\npresented. As an application, a complete classification of the self-dual codes\nof length 38 is given.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 13:30:12 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2012 10:36:31 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Bouyuklieva", "Stefka", ""], ["Bouyukliev", "Iliya", ""]]}, {"id": "1106.5971", "submitter": "Aur\\'elien Garivier", "authors": "Aur\\'elien Garivier", "title": "Perfect Simulation Of Processes With Long Memory: A `Coupling Into And\n  From The Past' Algorithm", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new algorithm for the perfect simulation of variable length\nMarkov chains and random systems with perfect connections. This algorithm,\nwhich generalizes Propp and Wilson's simulation scheme, is based on the idea of\ncoupling into and from the past. It improves on existing algorithms by relaxing\nthe conditions on the kernel and by accelerating convergence, even in the\nsimple case of finite order Markov chains. Although chains of variable or\ninfinite order have been widely investigated for decades, their use in applied\nprobability, from information theory to bio-informatics and linguistics, has\nrecently led to considerable renewed interest.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 15:27:31 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2012 10:13:30 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2013 09:09:47 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2013 14:26:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Garivier", "Aur\u00e9lien", ""]]}, {"id": "1106.6037", "submitter": "Arnaud Labourel", "authors": "J\\'er\\'emie Chalopin (LIF), Shantanu Das (LIF), Arnaud Labourel (LIF),\n  Euripides Markou", "title": "Black Hole Search with Finite Automata Scattered in a Synchronous Torus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of locating a black hole in synchronous anonymous\nnetworks using finite state agents. A black hole is a harmful node in the\nnetwork that destroys any agent visiting that node without leaving any trace.\nThe objective is to locate the black hole without destroying too many agents.\nThis is difficult to achieve when the agents are initially scattered in the\nnetwork and are unaware of the location of each other. Previous studies for\nblack hole search used more powerful models where the agents had non-constant\nmemory, were labelled with distinct identifiers and could either write messages\non the nodes of the network or mark the edges of the network. In contrast, we\nsolve the problem using a small team of finite-state agents each carrying a\nconstant number of identical tokens that could be placed on the nodes of the\nnetwork. Thus, all resources used in our algorithms are independent of the\nnetwork size. We restrict our attention to oriented torus networks and first\nshow that no finite team of finite state agents can solve the problem in such\nnetworks, when the tokens are not movable. In case the agents are equipped with\nmovable tokens, we determine lower bounds on the number of agents and tokens\nrequired for solving the problem in torus networks of arbitrary size. Further,\nwe present a deterministic solution to the black hole search problem for\noriented torus networks, using the minimum number of agents and tokens.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 19:43:23 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2011 14:04:27 GMT"}], "update_date": "2011-07-29", "authors_parsed": [["Chalopin", "J\u00e9r\u00e9mie", "", "LIF"], ["Das", "Shantanu", "", "LIF"], ["Labourel", "Arnaud", "", "LIF"], ["Markou", "Euripides", ""]]}, {"id": "1106.6136", "submitter": "Joan Boyar F", "authors": "Joan Boyar, Kim S. Larsen, Abyayananda Maiti", "title": "A Comparison of Performance Measures via Online Search", "comments": "IMADA-preprint 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though competitive analysis has been a very useful performance measure for\nthe quality of online algorithms, it is recognized that it sometimes fails to\ndistinguish between algorithms of different quality in practice. A number of\nalternative measures have been proposed, but, with a few exceptions, these have\ngenerally been applied only to the online problem they were developed in\nconnection with. Recently, a systematic study of performance measures for\nonline algorithms was initiated [Boyar, Irani, Larsen: Eleventh International\nAlgorithms and Data Structures Symposium 2009], first focusing on a simple\nserver problem. We continue this work by studying a fundamentally different\nonline problem, online search, and the Reservation Price Policies in\nparticular. The purpose of this line of work is to learn more about the\napplicability of various performance measures in different situations and the\nproperties that the different measures emphasize. We investigate the following\nanalysis techniques: Competitive, Relative Worst Order, Bijective, Average,\nRelative Interval, Random Order, and Max/Max. In addition to drawing\nconclusions on this work, we also investigate the measures' sensitivity to\nintegral vs. real-valued domains, and as a part of this work, generalize some\nof the known performance measures. Finally, we have established the first\noptimality proof for Relative Interval Analysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 07:35:00 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Boyar", "Joan", ""], ["Larsen", "Kim S.", ""], ["Maiti", "Abyayananda", ""]]}, {"id": "1106.6261", "submitter": "Yakov Nekrich Yakov Nekrich", "authors": "Yakov Nekrich", "title": "External Memory Orthogonal Range Reporting with Fast Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe data structures for orthogonal range reporting in\nexternal memory that support fast update operations. The query costs either\nmatch the query costs of the best previously known data structures or differ by\na small multiplicative factor.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 15:11:57 GMT"}], "update_date": "2011-07-01", "authors_parsed": [["Nekrich", "Yakov", ""]]}, {"id": "1106.6336", "submitter": "Pawe{\\l} Pszona", "authors": "Michael T. Goodrich and Pawel Pszona", "title": "External-Memory Network Analysis Algorithms for Naturally Sparse Graphs", "comments": "23 pages, 2 figures. To appear at the 19th Annual European Symposium\n  on Algorithms (ESA 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a number of network-analysis algorithms in the\nexternal-memory model. We focus on methods for large naturally sparse graphs,\nthat is, n-vertex graphs that have O(n) edges and are structured so that this\nsparsity property holds for any subgraph of such a graph. We give efficient\nexternal-memory algorithms for the following problems for such graphs: -\nFinding an approximate d-degeneracy ordering; - Finding a cycle of length\nexactly c; - Enumerating all maximal cliques. Such problems are of interest,\nfor example, in the analysis of social networks, where they are used to study\nnetwork cohesion.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 18:43:43 GMT"}], "update_date": "2011-07-01", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Pszona", "Pawel", ""]]}, {"id": "1106.6342", "submitter": "Sebastian Deorowicz", "authors": "Sebastian Deorowicz", "title": "Quadratic-time Algorithm for the String Constrained LCS Problem", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding a longest common subsequence of two main sequences\nwith some constraint that must be a substring of the result (STR-IC-LCS) was\nformulated recently. It is a variant of the constrained longest common\nsubsequence problem. As the known algorithms for the STR-IC-LCS problem are\ncubic-time, the presented quadratic-time algorithm is significantly faster.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 19:04:18 GMT"}], "update_date": "2011-07-01", "authors_parsed": [["Deorowicz", "Sebastian", ""]]}]