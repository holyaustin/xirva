[{"id": "1011.0078", "submitter": "Marek Karpinski", "authors": "Jean Cardinal, Marek Karpinski, Richard Schmied and Claus Viehmann", "title": "Approximating Subdense Instances of Covering Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approximability of subdense instances of various covering problems\non graphs, defined as instances in which the minimum or average degree is\nOmega(n/psi(n)) for some function psi(n)=omega(1) of the instance size. We\ndesign new approximation algorithms as well as new polynomial time\napproximation schemes (PTASs) for those problems and establish first\napproximation hardness results for them. Interestingly, in some cases we were\nable to prove optimality of the underlying approximation ratios, under usual\ncomplexity-theoretic assumptions. Our results for the Vertex Cover problem\ndepend on an improved recursive sampling method which could be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sat, 30 Oct 2010 15:11:30 GMT"}, {"version": "v2", "created": "Tue, 9 Nov 2010 16:21:22 GMT"}], "update_date": "2010-11-10", "authors_parsed": [["Cardinal", "Jean", ""], ["Karpinski", "Marek", ""], ["Schmied", "Richard", ""], ["Viehmann", "Claus", ""]]}, {"id": "1011.0108", "submitter": "Nir Ailon", "authors": "Nir Ailon", "title": "An Active Learning Algorithm for Ranking from Pairwise Preferences with\n  an Almost Optimal Query Complexity", "comments": "Fixed a tiny error in theorem 3.1 statement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning to rank from pairwise preferences, and solve\na long-standing open problem that has led to development of many heuristics but\nno provable results for our particular problem. Given a set $V$ of $n$\nelements, we wish to linearly order them given pairwise preference labels. A\npairwise preference label is obtained as a response, typically from a human, to\nthe question \"which if preferred, u or v?$ for two elements $u,v\\in V$. We\nassume possible non-transitivity paradoxes which may arise naturally due to\nhuman mistakes or irrationality. The goal is to linearly order the elements\nfrom the most preferred to the least preferred, while disagreeing with as few\npairwise preference labels as possible. Our performance is measured by two\nparameters: The loss and the query complexity (number of pairwise preference\nlabels we obtain). This is a typical learning problem, with the exception that\nthe space from which the pairwise preferences is drawn is finite, consisting of\n${n\\choose 2}$ possibilities only. We present an active learning algorithm for\nthis problem, with query bounds significantly beating general (non active)\nbounds for the same error guarantee, while almost achieving the information\ntheoretical lower bound. Our main construct is a decomposition of the input\ns.t. (i) each block incurs high loss at optimum, and (ii) the optimal solution\nrespecting the decomposition is not much worse than the true opt. The\ndecomposition is done by adapting a recent result by Kenyon and Schudy for a\nrelated combinatorial optimization problem to the query efficient setting. We\nthus settle an open problem posed by learning-to-rank theoreticians and\npractitioners: What is a provably correct way to sample preference labels? To\nfurther show the power and practicality of our solution, we show how to use it\nin concert with an SVM relaxation.\n", "versions": [{"version": "v1", "created": "Sat, 30 Oct 2010 21:47:19 GMT"}, {"version": "v10", "created": "Wed, 11 May 2011 12:59:24 GMT"}, {"version": "v11", "created": "Tue, 17 May 2011 11:38:44 GMT"}, {"version": "v2", "created": "Thu, 4 Nov 2010 18:15:58 GMT"}, {"version": "v3", "created": "Fri, 5 Nov 2010 08:32:11 GMT"}, {"version": "v4", "created": "Wed, 13 Apr 2011 07:11:55 GMT"}, {"version": "v5", "created": "Wed, 20 Apr 2011 20:43:02 GMT"}, {"version": "v6", "created": "Sat, 23 Apr 2011 20:32:28 GMT"}, {"version": "v7", "created": "Wed, 27 Apr 2011 20:14:43 GMT"}, {"version": "v8", "created": "Thu, 5 May 2011 19:45:14 GMT"}, {"version": "v9", "created": "Sat, 7 May 2011 18:30:11 GMT"}], "update_date": "2011-05-18", "authors_parsed": [["Ailon", "Nir", ""]]}, {"id": "1011.0148", "submitter": "LeRoy Johnson Professor", "authors": "L. F. Johnson", "title": "Golden and Alternating, fast simple O(lg n) algorithms for Fibonacci", "comments": "11 pages, 1 table This paper illustrates the importance of\n  considering storage size when running experiments and that the results of\n  expermential comparsion of algorithms in a well known paper are at best\n  unreliable because of register overflow. The two presented algorithms are\n  concise but readable", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two very fast and simple O(lg n) algorithms for individual Fibonacci numbers\nare given and compared to competing algorithms. A simple O(lg n) recursion is\nderived that can also be applied to Lucas. A formula is given to estimate the\nlargest n, where F_n does not overflow the implementation's data type. The\ndanger of timing runs on input that is too large for the computer\nrepresentation leads to false research results.\n", "versions": [{"version": "v1", "created": "Sun, 31 Oct 2010 12:15:27 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Johnson", "L. F.", ""]]}, {"id": "1011.0344", "submitter": "Michael Sagraloff", "authors": "Michael Sagraloff", "title": "On the Complexity of Real Root Isolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.NA cs.SC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to isolate the real roots of a square-free\npolynomial $F=\\sum_{i=0}^n A_i x^i$ with real coefficients. It is assumed that\neach coefficient of $F$ can be approximated to any specified error bound. The\npresented method is exact, complete and deterministic. Due to its similarities\nto the Descartes method, we also consider it practical and easy to implement.\nCompared to previous approaches, our new method achieves a significantly better\nbit complexity. It is further shown that the hardness of isolating the real\nroots of $F$ is exclusively determined by the geometry of the roots and not by\nthe complexity or the size of the coefficients. For the special case where $F$\nhas integer coefficients of maximal bitsize $\\tau$, our bound on the bit\ncomplexity writes as $\\tilde{O}(n^3\\tau^2)$ which improves the best bounds\nknown for existing practical algorithms by a factor of $n=deg F$. The crucial\nidea underlying the new approach is to run an approximate version of the\nDescartes method, where, in each subdivision step, we only consider\napproximations of the intermediate results to a certain precision. We give an\nupper bound on the maximal precision that is needed for isolating the roots of\n$F$. For integer polynomials, this bound is by a factor $n$ lower than that of\nthe precision needed when using exact arithmetic explaining the improved bound\non the bit complexity.\n", "versions": [{"version": "v1", "created": "Mon, 1 Nov 2010 15:29:23 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2011 13:26:06 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Sagraloff", "Michael", ""]]}, {"id": "1011.0351", "submitter": "Anant Godbole", "authors": "Michael S. Donders and Anant P. Godbole", "title": "$t$-Covering Arrays Generated by a Tiling Probability Model", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $t-\\a$ covering array is an $m\\times n$ matrix, with entries from an\nalphabet of size $\\alpha$, such that for any choice of $t$ rows, and any\nordered string of $t$ letters of the alphabet, there exists a column such that\nthe \"values\" of the rows in that column match those of the string of letters.\nWe use the Lov\\'asz Local Lemma in conjunction with a new tiling-based\nprobability model to improve the upper bound on the smallest number of columns\n$N=N(m,t,\\alpha)$ of a $t-\\a$ covering array.\n", "versions": [{"version": "v1", "created": "Mon, 1 Nov 2010 15:40:49 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Donders", "Michael S.", ""], ["Godbole", "Anant P.", ""]]}, {"id": "1011.0413", "submitter": "Michael Mahoney", "authors": "Jacob Bien and Ya Xu and Michael W. Mahoney", "title": "CUR from a Sparse Optimization Viewpoint", "comments": "9 pages; in NIPS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CUR decomposition provides an approximation of a matrix $X$ that has low\nreconstruction error and that is sparse in the sense that the resulting\napproximation lies in the span of only a few columns of $X$. In this regard, it\nappears to be similar to many sparse PCA methods. However, CUR takes a\nrandomized algorithmic approach, whereas most sparse PCA methods are framed as\nconvex optimization problems. In this paper, we try to understand CUR from a\nsparse optimization viewpoint. We show that CUR is implicitly optimizing a\nsparse regression objective and, furthermore, cannot be directly cast as a\nsparse PCA method. We also observe that the sparsity attained by CUR possesses\nan interesting structure, which leads us to formulate a sparse PCA method that\nachieves a CUR-like sparsity.\n", "versions": [{"version": "v1", "created": "Mon, 1 Nov 2010 19:07:15 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Bien", "Jacob", ""], ["Xu", "Ya", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1011.0468", "submitter": "Charalampos Tsourakakis", "authors": "Mihail N. Kolountzakis, Gary L. Miller, Richard Peng, Charalampos E.\n  Tsourakakis", "title": "Efficient Triangle Counting in Large Graphs via Degree-based Vertex\n  Partitioning", "comments": "1) 12 pages 2) To appear in the 7th Workshop on Algorithms and Models\n  for the Web Graph (WAW 2010)", "journal-ref": null, "doi": "10.1007/978-3-642-18009-5_3", "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of triangles is a computationally expensive graph statistic which\nis frequently used in complex network analysis (e.g., transitivity ratio), in\nvarious random graph models (e.g., exponential random graph model) and in\nimportant real world applications such as spam detection, uncovering of the\nhidden thematic structure of the Web and link recommendation. Counting\ntriangles in graphs with millions and billions of edges requires algorithms\nwhich run fast, use small amount of space, provide accurate estimates of the\nnumber of triangles and preferably are parallelizable.\n  In this paper we present an efficient triangle counting algorithm which can\nbe adapted to the semistreaming model. The key idea of our algorithm is to\ncombine the sampling algorithm of Tsourakakis et al. and the partitioning of\nthe set of vertices into a high degree and a low degree subset respectively as\nin the Alon, Yuster and Zwick work treating each set appropriately. We obtain a\nrunning time $O \\left(m + \\frac{m^{3/2} \\Delta \\log{n}}{t \\epsilon^2} \\right)$\nand an $\\epsilon$ approximation (multiplicative error), where $n$ is the number\nof vertices, $m$ the number of edges and $\\Delta$ the maximum number of\ntriangles an edge is contained.\n  Furthermore, we show how this algorithm can be adapted to the semistreaming\nmodel with space usage $O\\left(m^{1/2}\\log{n} + \\frac{m^{3/2} \\Delta \\log{n}}{t\n\\epsilon^2} \\right)$ and a constant number of passes (three) over the graph\nstream. We apply our methods in various networks with several millions of edges\nand we obtain excellent results. Finally, we propose a random projection based\nmethod for triangle counting and provide a sufficient condition to obtain an\nestimate with low variance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Nov 2010 23:16:05 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Kolountzakis", "Mihail N.", ""], ["Miller", "Gary L.", ""], ["Peng", "Richard", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1011.0495", "submitter": "EPTCS", "authors": "Michael J. Dinneen (University of Auckland), Yun-Bum Kim (University\n  of Auckland), Radu Nicolescu (University of Auckland)", "title": "Edge- and Node-Disjoint Paths in P Systems", "comments": "In Proceedings MeCBIC 2010, arXiv:1011.0051", "journal-ref": "EPTCS 40, 2010, pp. 121-141", "doi": "10.4204/EPTCS.40.9", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we continue our development of algorithms used for topological\nnetwork discovery. We present native P system versions of two fundamental\nproblems in graph theory: finding the maximum number of edge- and node-disjoint\npaths between a source node and target node. We start from the standard\ndepth-first-search maximum flow algorithms, but our approach is totally\ndistributed, when initially no structural information is available and each P\nsystem cell has to even learn its immediate neighbors. For the node-disjoint\nversion, our P system rules are designed to enforce node weight capacities (of\none), in addition to edge capacities (of one), which are not readily available\nin the standard network flow algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 01:29:56 GMT"}], "update_date": "2010-11-03", "authors_parsed": [["Dinneen", "Michael J.", "", "University of Auckland"], ["Kim", "Yun-Bum", "", "University\n  of Auckland"], ["Nicolescu", "Radu", "", "University of Auckland"]]}, {"id": "1011.0531", "submitter": "Andrej Bogdanov", "authors": "Andrej Bogdanov and Fan Li", "title": "A better tester for bipartiteness?", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alon and Krivelevich (SIAM J. Discrete Math. 15(2): 211-227 (2002)) show that\nif a graph is {\\epsilon}-far from bipartite, then the subgraph induced by a\nrandom subset of O(1/{\\epsilon}) vertices is bipartite with high probability.\nWe conjecture that the induced subgraph is {\\Omega}~({\\epsilon})-far from\nbipartite with high probability. Gonen and Ron (RANDOM 2007) proved this\nconjecture in the case when the degrees of all vertices are at most\nO({\\epsilon}n). We give a more general proof that works for any d-regular (or\nalmost d-regular) graph for arbitrary degree d. Assuming this conjecture, we\nprove that bipartiteness is testable with one-sided error in time\nO(1/{\\epsilon}^c), where c is a constant strictly smaller than two, improving\nupon the tester of Alon and Krivelevich. As it is known that non-adaptive\ntesters for bipartiteness require {\\Omega}(1/{\\epsilon}^2) queries (Bogdanov\nand Trevisan, CCC 2004), our result shows, assuming the conjecture, that\nadaptivity helps in testing bipartiteness.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 07:48:15 GMT"}], "update_date": "2010-11-03", "authors_parsed": [["Bogdanov", "Andrej", ""], ["Li", "Fan", ""]]}, {"id": "1011.0597", "submitter": "Shashank Srikant", "authors": "Shashank Srikant", "title": "Parallelization of Weighted Sequence Comparison by using EBWT", "comments": "This paper has been withdrawn by the authors. Extended Burrows\n  Wheeler transform; CUDA; Molecular Weighted Sequence; arXiv Admin note:\n  Author list truncated due to authorship dispute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Extended Burrows Wheeler transform (EBWT) helps to find the distance\nbetween two sequences. Implementation of an existing algorithm takes\nconsiderable amount of time for small size sequences. In this paper, we give a\nparallel implementation of this algorithm using NVIDIA Compute Unified Device\nArchitecture (CUDA). We have obtained, on an average, a 2X improvement in the\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 12:25:16 GMT"}, {"version": "v2", "created": "Wed, 26 Jan 2011 18:44:05 GMT"}, {"version": "v3", "created": "Sat, 29 Jan 2011 21:56:30 GMT"}], "update_date": "2011-02-02", "authors_parsed": [["Srikant", "Shashank", ""]]}, {"id": "1011.0972", "submitter": "Guillaume Cheze", "authors": "Guillaume Ch\\`eze (IMT)", "title": "A recombination algorithm for the decomposition of multivariate rational\n  functions", "comments": null, "journal-ref": "Mathematics of Computation 82 (2013) 1793--1812", "doi": null, "report-no": null, "categories": "cs.SC cs.DS math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how we can compute in a deterministic way the\ndecomposition of a multivariate rational function with a recombination\nstrategy. The key point of our recombination strategy is the used of Darboux\npolynomials. We study the complexity of this strategy and we show that this\nmethod improves the previous ones. In appendix, we explain how the strategy\nproposed recently by J. Berthomieu and G. Lecerf for the sparse factorization\ncan be used in the decomposition setting. Then we deduce a decomposition\nalgorithm in the sparse bivariate case and we give its complexity\n", "versions": [{"version": "v1", "created": "Wed, 3 Nov 2010 19:01:48 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Ch\u00e8ze", "Guillaume", "", "IMT"]]}, {"id": "1011.1157", "submitter": "Laurent Bulteau", "authors": "Laurent Bulteau, Guillaume Fertin, Irena Rusu", "title": "Sorting by Transpositions is Difficult", "comments": null, "journal-ref": null, "doi": "10.1137/110851390", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In comparative genomics, a transposition is an operation that exchanges two\nconsecutive sequences of genes in a genome. The transposition distance, that\nis, the minimum number of transpositions needed to transform a genome into\nanother, is, according to numerous studies, a relevant evolutionary distance.\nThe problem of computing this distance when genomes are represented by\npermutations, called the Sorting by Transpositions problem, has been introduced\nby Bafna and Pevzner in 1995. It has naturally been the focus of a number of\nstudies, but the computational complexity of this problem has remained\nundetermined for 15 years. In this paper, we answer this long-standing open\nquestion by proving that the Sorting by Transpositions problem is NP-hard. As a\ncorollary of our result, we also prove that the following problem is NP-hard:\ngiven a permutation pi, is it possible to sort pi using db(pi)/3 permutations,\nwhere db(pi) is the number of breakpoints of pi?\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 13:41:12 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Bulteau", "Laurent", ""], ["Fertin", "Guillaume", ""], ["Rusu", "Irena", ""]]}, {"id": "1011.1161", "submitter": "Kamesh Munagala", "authors": "Sudipto Guha and Kamesh Munagala and Martin Pal", "title": "Multiarmed Bandit Problems with Delayed Feedback", "comments": "The results and presentation in this paper are subsumed by the\n  article \"Approximation algorithms for Bayesian multi-armed bandit problems\"\n  arXiv:1306.3525", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we initiate the study of optimization of bandit type problems\nin scenarios where the feedback of a play is not immediately known. This arises\nnaturally in allocation problems which have been studied extensively in the\nliterature, albeit in the absence of delays in the feedback. We study this\nproblem in the Bayesian setting. In presence of delays, no solution with\nprovable guarantees is known to exist with sub-exponential running time.\n  We show that bandit problems with delayed feedback that arise in allocation\nsettings can be forced to have significant structure, with a slight loss in\noptimality. This structure gives us the ability to reason about the\nrelationship of single arm policies to the entangled optimum policy, and\neventually leads to a O(1) approximation for a significantly general class of\npriors. The structural insights we develop are of key interest and carry over\nto the setting where the feedback of an action is available instantaneously,\nand we improve all previous results in this setting as well.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 14:00:41 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2011 13:53:42 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2013 15:10:04 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Guha", "Sudipto", ""], ["Munagala", "Kamesh", ""], ["Pal", "Martin", ""]]}, {"id": "1011.1168", "submitter": "Ola Svensson", "authors": "Ola Svensson", "title": "Santa Claus Schedules Jobs on Unrelated Machines", "comments": "22 pages, 1 figure; corrected typos and changed some notation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the classic results in scheduling theory is the 2-approximation\nalgorithm by Lenstra, Shmoys, and Tardos for the problem of scheduling jobs to\nminimize makespan on unrelated machines, i.e., job j requires time p_{ij} if\nprocessed on machine i. More than two decades after its introduction it is\nstill the algorithm of choice even in the restricted model where processing\ntimes are of the form p_{ij} in {p_j, \\infty}. This problem, also known as the\nrestricted assignment problem, is NP-hard to approximate within a factor less\nthan 1.5 which is also the best known lower bound for the general version.\n  Our main result is a polynomial time algorithm that estimates the optimal\nmakespan of the restricted assignment problem within a factor 33/17 + \\epsilon\n\\approx 1.9412 + \\epsilon, where \\epsilon > 0 is an arbitrarily small constant.\nThe result is obtained by upper bounding the integrality gap of a certain\nstrong linear program, known as configuration LP, that was previously\nsuccessfully used for the related Santa Claus problem. Similar to the strongest\nanalysis for that problem our proof is based on a local search algorithm that\nwill eventually find a schedule of the mentioned approximation guarantee, but\nis not known to converge in polynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 14:22:11 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2011 13:38:25 GMT"}], "update_date": "2011-03-22", "authors_parsed": [["Svensson", "Ola", ""]]}, {"id": "1011.1202", "submitter": "Prudence W.H. Wong", "authors": "Alexandru Popa and Prudence W.H. Wong and Fencol C.C. Yung", "title": "Hardness and Approximation of The Asynchronous Border Minimization\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a combinatorial problem arising from microarrays synthesis. The\nsynthesis is done by a light-directed chemical process. The objective is to\nminimize unintended illumination that may contaminate the quality of\nexperiments. Unintended illumination is measured by a notion called border\nlength and the problem is called Border Minimization Problem (BMP). The\nobjective of the BMP is to place a set of probe sequences in the array and find\nan embedding (deposition of nucleotides/residues to the array cells) such that\nthe sum of border length is minimized. A variant of the problem, called P-BMP,\nis that the placement is given and the concern is simply to find the embedding.\nApproximation algorithms have been previously proposed for the problem but it\nis unknown whether the problem is NP-hard or not. In this paper, we give a\nthorough study of different variations of BMP by giving NP-hardness proofs and\nimproved approximation algorithms. We show that P-BMP, 1D-BMP, and BMP are all\nNP-hard. Contrast with the previous result that 1D-P-BMP is polynomial time\nsolvable, the interesting implications include (i) the array dimension (1D or\n2D) differentiates the complexity of P-BMP; (ii) for 1D array, whether\nplacement is given differentiates the complexity of BMP; (iii) BMP is NP-hard\nregardless of the dimension of the array. Another contribution of the paper is\nimproving the approximation for BMP from $O(n^{1/2} \\log^2 n)$ to $O(n^{1/4}\n\\log^2 n)$, where $n$ is the total number of sequences.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 16:25:26 GMT"}], "update_date": "2010-11-05", "authors_parsed": [["Popa", "Alexandru", ""], ["Wong", "Prudence W. H.", ""], ["Yung", "Fencol C. C.", ""]]}, {"id": "1011.1263", "submitter": "Alexandr Andoni", "authors": "Alexandr Andoni, Robert Krauthgamer, Krzysztof Onak", "title": "Streaming Algorithms from Precision Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique introduced by Indyk and Woodruff [STOC 2005] has inspired several\nrecent advances in data-stream algorithms. We show that a number of these\nresults follow easily from the application of a single probabilistic method\ncalled Precision Sampling. Using this method, we obtain simple data-stream\nalgorithms that maintain a randomized sketch of an input vector\n$x=(x_1,...x_n)$, which is useful for the following applications. 1) Estimating\nthe $F_k$-moment of $x$, for $k>2$. 2) Estimating the $\\ell_p$-norm of $x$, for\n$p\\in[1,2]$, with small update time. 3) Estimating cascaded norms\n$\\ell_p(\\ell_q)$ for all $p,q>0$. 4) $\\ell_1$ sampling, where the goal is to\nproduce an element $i$ with probability (approximately) $|x_i|/\\|x\\|_1$. It\nextends to similarly defined $\\ell_p$-sampling, for $p\\in [1,2]$.\n  For all these applications the algorithm is essentially the same: scale the\nvector x entry-wise by a well-chosen random vector, and run a heavy-hitter\nestimation algorithm on the resulting vector. Our sketch is a linear function\nof x, thereby allowing general updates to the vector x.\n  Precision Sampling itself addresses the problem of estimating a sum\n$\\sum_{i=1}^n a_i$ from weak estimates of each real $a_i\\in[0,1]$. More\nprecisely, the estimator first chooses a desired precision $u_i\\in(0,1]$ for\neach $i\\in[n]$, and then it receives an estimate of every $a_i$ within additive\n$u_i$. Its goal is to provide a good approximation to $\\sum a_i$ while keeping\na tab on the \"approximation cost\" $\\sum_i (1/u_i)$. Here we refine previous\nwork [Andoni, Krauthgamer, and Onak, FOCS 2010] which shows that as long as\n$\\sum a_i=\\Omega(1)$, a good multiplicative approximation can be achieved using\ntotal precision of only $O(n\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 20:13:20 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2011 02:26:14 GMT"}], "update_date": "2011-04-26", "authors_parsed": [["Andoni", "Alexandr", ""], ["Krauthgamer", "Robert", ""], ["Onak", "Krzysztof", ""]]}, {"id": "1011.1296", "submitter": "Jonathan Ullman", "authors": "Anupam Gupta, Moritz Hardt, Aaron Roth, Jonathan Ullman", "title": "Privately Releasing Conjunctions and the Statistical Query Barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we would like to know all answers to a set of statistical queries C\non a data set up to small error, but we can only access the data itself using\nstatistical queries. A trivial solution is to exhaustively ask all queries in\nC. Can we do any better?\n  + We show that the number of statistical queries necessary and sufficient for\nthis task is---up to polynomial factors---equal to the agnostic learning\ncomplexity of C in Kearns' statistical query (SQ) model. This gives a complete\nanswer to the question when running time is not a concern.\n  + We then show that the problem can be solved efficiently (allowing arbitrary\nerror on a small fraction of queries) whenever the answers to C can be\ndescribed by a submodular function. This includes many natural concept classes,\nsuch as graph cuts and Boolean disjunctions and conjunctions.\n  While interesting from a learning theoretic point of view, our main\napplications are in privacy-preserving data analysis:\n  Here, our second result leads to the first algorithm that efficiently\nreleases differentially private answers to of all Boolean conjunctions with 1%\naverage error. This presents significant progress on a key open problem in\nprivacy-preserving data analysis.\n  Our first result on the other hand gives unconditional lower bounds on any\ndifferentially private algorithm that admits a (potentially\nnon-privacy-preserving) implementation using only statistical queries. Not only\nour algorithms, but also most known private algorithms can be implemented using\nonly statistical queries, and hence are constrained by these lower bounds. Our\nresult therefore isolates the complexity of agnostic learning in the SQ-model\nas a new barrier in the design of differentially private algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 23:59:08 GMT"}, {"version": "v2", "created": "Sun, 14 Nov 2010 15:36:08 GMT"}, {"version": "v3", "created": "Wed, 15 Dec 2010 00:31:57 GMT"}, {"version": "v4", "created": "Thu, 27 Oct 2011 16:50:37 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Gupta", "Anupam", ""], ["Hardt", "Moritz", ""], ["Roth", "Aaron", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1011.1337", "submitter": "Peter Becker", "authors": "Peter Becker", "title": "Optimal Binary Search Trees with Near Minimal Height", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we have n keys, n access probabilities for the keys, and n+1 access\nprobabilities for the gaps between the keys. Let h_min(n) be the minimal height\nof a binary search tree for n keys. We consider the problem to construct an\noptimal binary search tree with near minimal height, i.e.\\ with height h <=\nh_min(n) + Delta for some fixed Delta. It is shown, that for any fixed Delta\noptimal binary search trees with near minimal height can be constructed in time\nO(n^2). This is as fast as in the unrestricted case.\n  So far, the best known algorithms for the construction of height-restricted\noptimal binary search trees have running time O(L n^2), whereby L is the\nmaximal permitted height. Compared to these algorithms our algorithm is at\nleast faster by a factor of log n, because L is lower bounded by log n.\n", "versions": [{"version": "v1", "created": "Fri, 5 Nov 2010 08:16:42 GMT"}], "update_date": "2010-11-08", "authors_parsed": [["Becker", "Peter", ""]]}, {"id": "1011.1443", "submitter": "Robin Kothari", "authors": "Andrew M. Childs and Robin Kothari", "title": "Quantum query complexity of minor-closed graph properties", "comments": "v1: 25 pages, 2 figures. v2: 26 pages", "journal-ref": "Proc. 28th Symposium on Theoretical Aspects of Computer Science\n  (STACS 2011), Leibniz International Proceedings in Informatics 9, pp. 661-672", "doi": "10.4230/LIPIcs.STACS.2011.661", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the quantum query complexity of minor-closed graph properties, which\ninclude such problems as determining whether an $n$-vertex graph is planar, is\na forest, or does not contain a path of a given length. We show that most\nminor-closed properties---those that cannot be characterized by a finite set of\nforbidden subgraphs---have quantum query complexity \\Theta(n^{3/2}). To\nestablish this, we prove an adversary lower bound using a detailed analysis of\nthe structure of minor-closed properties with respect to forbidden topological\nminors and forbidden subgraphs. On the other hand, we show that minor-closed\nproperties (and more generally, sparse graph properties) that can be\ncharacterized by finitely many forbidden subgraphs can be solved strictly\nfaster, in o(n^{3/2}) queries. Our algorithms are a novel application of the\nquantum walk search framework and give improved upper bounds for several\nsubgraph-finding problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Nov 2010 16:03:10 GMT"}, {"version": "v2", "created": "Thu, 19 May 2011 17:53:02 GMT"}], "update_date": "2011-05-20", "authors_parsed": [["Childs", "Andrew M.", ""], ["Kothari", "Robin", ""]]}, {"id": "1011.1595", "submitter": "Joel Tropp", "authors": "Joel A. Tropp", "title": "Improved analysis of the subsampled randomized Hadamard transform", "comments": "8 pages. To appear, Advances in Adaptive Data Analysis, special issue\n  \"Sparse Representation of Data and Images.\" v2--v4 include minor corrections", "journal-ref": "Adv. Adapt. Data Anal., Vol. 3, num. 1-2, special issue, \"Sparse\n  Representation of Data and Images,\" pp. 115-126, 2011", "doi": null, "report-no": null, "categories": "math.NA cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an improved analysis of a structured dimension-reduction\nmap called the subsampled randomized Hadamard transform. This argument\ndemonstrates that the map preserves the Euclidean geometry of an entire\nsubspace of vectors. The new proof is much simpler than previous approaches,\nand it offers---for the first time---optimal constants in the estimate on the\nnumber of dimensions required for the embedding.\n", "versions": [{"version": "v1", "created": "Sat, 6 Nov 2010 22:15:18 GMT"}, {"version": "v2", "created": "Tue, 9 Nov 2010 18:22:42 GMT"}, {"version": "v3", "created": "Fri, 12 Nov 2010 03:44:51 GMT"}, {"version": "v4", "created": "Sun, 17 Jul 2011 19:12:44 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Tropp", "Joel A.", ""]]}, {"id": "1011.1708", "submitter": "Kunihiko Sadakane", "authors": "Jesper Jansson, Kunihiko Sadakane, Wing-Kin Sung", "title": "CRAM: Compressed Random Access Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new data structure called the \\emph{Compressed Random Access\nMemory} (CRAM) that can store a dynamic string $T$ of characters, e.g.,\nrepresenting the memory of a computer, in compressed form while achieving\nasymptotically almost-optimal bounds (in terms of empirical entropy) on the\ncompression ratio. It allows short substrings of $T$ to be decompressed and\nretrieved efficiently and, significantly, characters at arbitrary positions of\n$T$ to be modified quickly during execution \\emph{without decompressing the\nentire string}. This can be regarded as a new type of data compression that can\nupdate a compressed file directly. Moreover, at the cost of slightly increasing\nthe time spent per operation, the CRAM can be extended to also support\ninsertions and deletions. Our key observation that the empirical entropy of a\nstring does not change much after a small change to the string, as well as our\nsimple yet efficient method for maintaining an array of variable-length blocks\nunder length modifications, may be useful for many other applications as well.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 04:10:42 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2012 09:08:29 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Jansson", "Jesper", ""], ["Sadakane", "Kunihiko", ""], ["Sung", "Wing-Kin", ""]]}, {"id": "1011.1754", "submitter": "Jop Bri\\\"et", "authors": "Jop Briet, Fernando Mario de Oliveira Filho, Frank Vallentin", "title": "Grothendieck inequalities for semidefinite programs with rank constraint", "comments": "22 pages", "journal-ref": "Theory of Computing 10 (2014), 77-105", "doi": "10.4086/toc.2014.v010a004", "report-no": null, "categories": "math.OC cs.DS math.CO math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grothendieck inequalities are fundamental inequalities which are frequently\nused in many areas of mathematics and computer science. They can be interpreted\nas upper bounds for the integrality gap between two optimization problems: a\ndifficult semidefinite program with rank-1 constraint and its easy semidefinite\nrelaxation where the rank constrained is dropped. For instance, the integrality\ngap of the Goemans-Williamson approximation algorithm for MAX CUT can be seen\nas a Grothendieck inequality. In this paper we consider Grothendieck\ninequalities for ranks greater than 1 and we give two applications:\napproximating ground states in the n-vector model in statistical mechanics and\nXOR games in quantum information theory.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 10:23:26 GMT"}, {"version": "v2", "created": "Fri, 4 May 2012 13:12:03 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Briet", "Jop", ""], ["Filho", "Fernando Mario de Oliveira", ""], ["Vallentin", "Frank", ""]]}, {"id": "1011.1827", "submitter": "D\\'aniel Marx", "authors": "Martin Grohe and Ken-ichi Kawarabayashi and D\\'aniel Marx and Paul\n  Wollan", "title": "Finding topological subgraphs is fixed-parameter tractable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for every fixed undirected graph $H$, there is a $O(|V(G)|^3)$\ntime algorithm that tests, given a graph $G$, if $G$ contains $H$ as a\ntopological subgraph (that is, a subdivision of $H$ is subgraph of $G$). This\nshows that topological subgraph testing is fixed-parameter tractable, resolving\na longstanding open question of Downey and Fellows from 1992. As a corollary,\nfor every $H$ we obtain an $O(|V(G)|^3)$ time algorithm that tests if there is\nan immersion of $H$ into a given graph $G$. This answers another open question\nraised by Downey and Fellows in 1992.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 15:23:20 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Grohe", "Martin", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Marx", "D\u00e1niel", ""], ["Wollan", "Paul", ""]]}, {"id": "1011.1868", "submitter": "Mahmoud Fouz", "authors": "Benjamin Doerr, Mahmoud Fouz", "title": "Asymptotically Optimal Randomized Rumor Spreading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new protocol solving the fundamental problem of disseminating a\npiece of information to all members of a group of n players. It builds upon the\nclassical randomized rumor spreading protocol and several extensions. The main\nachievements are the following:\n  Our protocol spreads the rumor to all other nodes in the asymptotically\noptimal time of (1 + o(1)) \\log_2 n. The whole process can be implemented in a\nway such that only O(n f(n)) calls are made, where f(n)= \\omega(1) can be\narbitrary.\n  In contrast to other protocols suggested in the literature, our algorithm\nonly uses push operations, i.e., only informed nodes take active actions in the\nnetwork. To the best of our knowledge, this is the first randomized push\nalgorithm that achieves an asymptotically optimal running time.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 18:43:27 GMT"}, {"version": "v2", "created": "Wed, 17 Nov 2010 16:03:43 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Doerr", "Benjamin", ""], ["Fouz", "Mahmoud", ""]]}, {"id": "1011.2136", "submitter": "Isolde Adler", "authors": "Isolde Adler, Philipp Klaus Krause", "title": "A lower bound for the tree-width of planar graphs with vital linkages", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disjoint paths problem asks, given an graph G and k + 1 pairs of\nterminals (s_0,t_0), ...,(s_k,t_k), whether there are k+1 pairwise disjoint\npaths P_0, ...,P_k, such that P_i connects s_i to t_i. Robertson and Seymour\nhave proven that the problem can be solved in polynomial time if k is fixed.\nNevertheless, the constants involved are huge, and the algorithm is far from\nimplementable. The algorithm uses a bound on the tree-width of graphs with\nvital linkages, and deletion of irrelevant vertices. We give single exponential\nlower bounds both for the tree-width of planar graphs with vital linkages, and\nfor the size of the grid necessary for finding irrelevant vertices.\n", "versions": [{"version": "v1", "created": "Tue, 9 Nov 2010 16:29:47 GMT"}], "update_date": "2010-11-10", "authors_parsed": [["Adler", "Isolde", ""], ["Krause", "Philipp Klaus", ""]]}, {"id": "1011.2187", "submitter": "Benjmain Moseley", "authors": "Kyle Fox and Benjamin Moseley", "title": "Online Scheduling on Identical Machines using SRPT", "comments": "Accepted for publication at SODA. This version fixes an error in a\n  preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its optimality on a single machine for the problem of minimizing\naverage flow time, Shortest-Remaining-Processing-Time (\\srpt) appears to be the\nmost natural algorithm to consider for the problem of minimizing average flow\ntime on multiple identical machines. It is known that $\\srpt$ achieves the best\npossible competitive ratio on multiple machines up to a constant factor. Using\nresource augmentation, $\\srpt$ is known to achieve total flow time at most that\nof the optimal solution when given machines of speed $2- \\frac{1}{m}$. Further,\nit is known that $\\srpt$'s competitive ratio improves as the speed increases;\n$\\srpt$ is $s$-speed $\\frac{1}{s}$-competitive when $s \\geq 2- \\frac{1}{m}$.\n  However, a gap has persisted in our understanding of $\\srpt$. Before this\nwork, the performance of $\\srpt$ was not known when $\\srpt$ is given\n$(1+\\eps)$-speed when $0 < \\eps < 1-\\frac{1}{m}$, even though it has been\nthought that $\\srpt$ is $(1+\\eps)$-speed $O(1)$-competitive for over a decade.\nResolving this question was suggested in Open Problem 2.9 from the survey\n\"Online Scheduling\" by Pruhs, Sgall, and Torng \\cite{PruhsST}, and we answer\nthe question in this paper. We show that $\\srpt$ is \\emph{scalable} on $m$\nidentical machines. That is, we show $\\srpt$ is $(1+\\eps)$-speed\n$O(\\frac{1}{\\eps})$-competitive for $\\eps >0$. We complement this by showing\nthat $\\srpt$ is $(1+\\eps)$-speed $O(\\frac{1}{\\eps^2})$-competitive for the\nobjective of minimizing the $\\ell_k$-norms of flow time on $m$ identical\nmachines. Both of our results rely on new potential functions that capture the\nstructure of \\srpt. Our results, combined with previous work, show that $\\srpt$\nis the best possible online algorithm in essentially every aspect when\nmigration is permissible.\n", "versions": [{"version": "v1", "created": "Tue, 9 Nov 2010 20:18:56 GMT"}], "update_date": "2010-11-10", "authors_parsed": [["Fox", "Kyle", ""], ["Moseley", "Benjamin", ""]]}, {"id": "1011.2249", "submitter": "Ankur Moitra", "authors": "Ankur Moitra and Ryan O'Donnell", "title": "Pareto Optimal Solutions for Smoothed Analysts", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an optimization problem with $n$ binary variables and $d+1$ linear\nobjective functions. Each valid solution $x \\in \\{0,1\\}^n$ gives rise to an\nobjective vector in $\\R^{d+1}$, and one often wants to enumerate the Pareto\noptima among them. In the worst case there may be exponentially many Pareto\noptima; however, it was recently shown that in (a generalization of) the\nsmoothed analysis framework, the expected number is polynomial in $n$.\nUnfortunately, the bound obtained had a rather bad dependence on $d$; roughly\n$n^{d^d}$. In this paper we show a significantly improved bound of $n^{2d}$.\n  Our proof is based on analyzing two algorithms. The first algorithm, on input\na Pareto optimal $x$, outputs a \"testimony\" containing clues about $x$'s\nobjective vector, $x$'s coordinates, and the region of space $B$ in which $x$'s\nobjective vector lies. The second algorithm can be regarded as a {\\em\nspeculative} execution of the first -- it can uniquely reconstruct $x$ from the\ntestimony's clues and just \\emph{some} of the probability space's outcomes. The\nremainder of the probability space's outcomes are just enough to bound the\nprobability that $x$'s objective vector falls into the region $B$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Nov 2010 01:31:44 GMT"}], "update_date": "2010-11-11", "authors_parsed": [["Moitra", "Ankur", ""], ["O'Donnell", "Ryan", ""]]}, {"id": "1011.2348", "submitter": "Olivier Fercoq", "authors": "Olivier Fercoq, Marianne Akian, Mustapha Bouhtou, St\\'ephane Gaubert", "title": "Ergodic Control and Polyhedral approaches to PageRank Optimization", "comments": "39 pages", "journal-ref": "IEEE-TAC, 58(1), pp. 134--148 (2013)", "doi": "10.1109/TAC.2012.2226103", "report-no": null, "categories": "math.OC cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general class of PageRank optimization problems which consist in\nfinding an optimal outlink strategy for a web site subject to design\nconstraints. We consider both a continuous problem, in which one can choose the\nintensity of a link, and a discrete one, in which in each page, there are\nobligatory links, facultative links and forbidden links. We show that the\ncontinuous problem, as well as its discrete variant when there are no\nconstraints coupling different pages, can both be modeled by constrained Markov\ndecision processes with ergodic reward, in which the webmaster determines the\ntransition probabilities of websurfers. Although the number of actions turns\nout to be exponential, we show that an associated polytope of transition\nmeasures has a concise representation, from which we deduce that the continuous\nproblem is solvable in polynomial time, and that the same is true for the\ndiscrete problem when there are no coupling constraints. We also provide\nefficient algorithms, adapted to very large networks. Then, we investigate the\nqualitative features of optimal outlink strategies, and identify in particular\nassumptions under which there exists a \"master\" page to which all controlled\npages should point. We report numerical results on fragments of the real web\ngraph.\n", "versions": [{"version": "v1", "created": "Wed, 10 Nov 2010 11:56:42 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2011 08:33:31 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Fercoq", "Olivier", ""], ["Akian", "Marianne", ""], ["Bouhtou", "Mustapha", ""], ["Gaubert", "St\u00e9phane", ""]]}, {"id": "1011.2480", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich", "title": "Spin-the-bottle Sort and Annealing Sort: Oblivious Sorting via\n  Round-robin Random Comparisons", "comments": "Full version of a paper appearing in ANALCO 2011, in conjunction with\n  SODA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sorting algorithms based on randomized round-robin comparisons.\nSpecifically, we study Spin-the-bottle sort, where comparisons are\nunrestricted, and Annealing sort, where comparisons are restricted to a\ndistance bounded by a \\emph{temperature} parameter. Both algorithms are simple,\nrandomized, data-oblivious sorting algorithms, which are useful in\nprivacy-preserving computations, but, as we show, Annealing sort is much more\nefficient. We show that there is an input permutation that causes\nSpin-the-bottle sort to require $\\Omega(n^2\\log n)$ expected time in order to\nsucceed, and that in $O(n^2\\log n)$ time this algorithm succeeds with high\nprobability for any input. We also show there is an implementation of Annealing\nsort that runs in $O(n\\log n)$ time and succeeds with very high probability.\n", "versions": [{"version": "v1", "created": "Wed, 10 Nov 2010 20:22:42 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Goodrich", "Michael T.", ""]]}, {"id": "1011.2571", "submitter": "Vladimir Braverman", "authors": "Vladimir Braverman and Rafail Ostrovsky", "title": "Recursive Sketching For Frequency Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a ground-breaking paper, Indyk and Woodruff (STOC 05) showed how to\ncompute $F_k$ (for $k>2$) in space complexity $O(\\mbox{\\em poly-log}(n,m)\\cdot\nn^{1-\\frac2k})$, which is optimal up to (large) poly-logarithmic factors in $n$\nand $m$, where $m$ is the length of the stream and $n$ is the upper bound on\nthe number of distinct elements in a stream. The best known lower bound for\nlarge moments is $\\Omega(\\log(n)n^{1-\\frac2k})$. A follow-up work of\nBhuvanagiri, Ganguly, Kesh and Saha (SODA 2006) reduced the poly-logarithmic\nfactors of Indyk and Woodruff to $O(\\log^2(m)\\cdot (\\log n+ \\log m)\\cdot\nn^{1-{2\\over k}})$. Further reduction of poly-log factors has been an elusive\ngoal since 2006, when Indyk and Woodruff method seemed to hit a natural\n\"barrier.\" Using our simple recursive sketch, we provide a different yet simple\napproach to obtain a $O(\\log(m)\\log(nm)\\cdot (\\log\\log n)^4\\cdot n^{1-{2\\over\nk}})$ algorithm for constant $\\epsilon$ (our bound is, in fact, somewhat\nstronger, where the $(\\log\\log n)$ term can be replaced by any constant number\nof $\\log $ iterations instead of just two or three, thus approaching $log^*n$.\nOur bound also works for non-constant $\\epsilon$ (for details see the body of\nthe paper). Further, our algorithm requires only $4$-wise independence, in\ncontrast to existing methods that use pseudo-random generators for computing\nlarge frequency moments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Nov 2010 06:07:18 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Braverman", "Vladimir", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "1011.2590", "submitter": "Vladimir Braverman", "authors": "Vladimir Braverman and Rafail Ostrovsky and Yuval Rabani", "title": "Rademacher Chaos, Random Eulerian Graphs and The Sparse\n  Johnson-Lindenstrauss Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated dimension reduction lemma of Johnson and Lindenstrauss has\nnumerous computational and other applications. Due to its application in\npractice, speeding up the computation of a Johnson-Lindenstrauss style\ndimension reduction is an important question. Recently, Dasgupta, Kumar, and\nSarlos (STOC 2010) constructed such a transform that uses a sparse matrix. This\nis motivated by the desire to speed up the computation when applied to sparse\ninput vectors, a scenario that comes up in applications. The sparsity of their\nconstruction was further improved by Kane and Nelson (ArXiv 2010).\n  We improve the previous bound on the number of non-zero entries per column of\nKane and Nelson from $O(1/\\epsilon \\log(1/\\delta)\\log(k/\\delta))$ (where the\ntarget dimension is $k$, the distortion is $1\\pm \\epsilon$, and the failure\nprobability is $\\delta$) to $$ O\\left({1\\over\\epsilon}\n\\left({\\log(1/\\delta)\\log\\log\\log(1/\\delta) \\over\n\\log\\log(1/\\delta)}\\right)^2\\right). $$\n  We also improve the amount of randomness needed to generate the matrix. Our\nresults are obtained by connecting the moments of an order 2 Rademacher chaos\nto the combinatorial properties of random Eulerian multigraphs. Estimating the\nchance that a random multigraph is composed of a given number of node-disjoint\nEulerian components leads to a new tail bound on the chaos. Our estimates may\nbe of independent interest, and as this part of the argument is decoupled from\nthe analysis of the coefficients of the chaos, we believe that our methods can\nbe useful in the analysis of other chaoses.\n", "versions": [{"version": "v1", "created": "Thu, 11 Nov 2010 08:13:15 GMT"}], "update_date": "2010-11-12", "authors_parsed": [["Braverman", "Vladimir", ""], ["Ostrovsky", "Rafail", ""], ["Rabani", "Yuval", ""]]}, {"id": "1011.2807", "submitter": "Jijie Wang", "authors": "Jijie Wang, Lei Lin, Ting Huang, Jingjing Wang and Zengyou He", "title": "Efficient K-Nearest Neighbor Join Algorithms for High Dimensional Sparse\n  Data", "comments": "12 pages, This paper has been submitted to PAKDD2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The K-Nearest Neighbor (KNN) join is an expensive but important operation in\nmany data mining algorithms. Several recent applications need to perform KNN\njoin for high dimensional sparse data. Unfortunately, all existing KNN join\nalgorithms are designed for low dimensional data. To fulfill this void, we\ninvestigate the KNN join problem for high dimensional sparse data.\n  In this paper, we propose three KNN join algorithms: a brute force (BF)\nalgorithm, an inverted index-based(IIB) algorithm and an improved inverted\nindex-based(IIIB) algorithm. Extensive experiments on both synthetic and\nreal-world datasets were conducted to demonstrate the effectiveness of our\nalgorithms for high dimensional sparse data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Nov 2010 01:35:39 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Wang", "Jijie", ""], ["Lin", "Lei", ""], ["Huang", "Ting", ""], ["Wang", "Jingjing", ""], ["He", "Zengyou", ""]]}, {"id": "1011.2843", "submitter": "Piotr Sankowski", "authors": "Giuseppe F. Italiano and Piotr Sankowski", "title": "Improved Minimum Cuts and Maximum Flows in Undirected Planar Graphs", "comments": "This paper is being merged with the paper by Christian Wulff-Nilsen\n  \"Min st-Cut of a Planar Graph in O(n loglog n) Time\"\n  http://arxiv.org/abs/1007.3609", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study minimum cut and maximum flow problems on planar\ngraphs, both in static and in dynamic settings. First, we present an algorithm\nthat given an undirected planar graph computes the minimum cut between any two\ngiven vertices in O(n log log n) time. Second, we show how to achieve the same\nO(n log log n) bound for the problem of computing maximum flows in undirected\nplanar graphs. To the best of our knowledge, these are the first algorithms for\nthose two problems that break the O(n log n) barrier, which has been standing\nfor more than 25 years. Third, we present a fully dynamic algorithm that is\nable to maintain information about minimum cuts and maximum flows in a plane\ngraph (i.e., a planar graph with a fixed embedding): our algorithm is able to\ninsert edges, delete edges and answer min-cut and max-flow queries between any\npair of vertices in O(n^(2/3) log^3 n) time per operation. This result is based\non a new dynamic shortest path algorithm for planar graphs which may be of\nindependent interest. We remark that this is the first known non-trivial\nalgorithm for min-cut and max-flow problems in a dynamic setting.\n", "versions": [{"version": "v1", "created": "Fri, 12 Nov 2010 08:45:01 GMT"}, {"version": "v2", "created": "Mon, 22 Nov 2010 15:11:33 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["Italiano", "Giuseppe F.", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1011.3049", "submitter": "Danupon Nanongkai", "authors": "Atish Das Sarma, Stephan Holzer, Liah Kor, Amos Korman, Danupon\n  Nanongkai, Gopal Pandurangan, David Peleg, Roger Wattenhofer", "title": "Distributed Verification and Hardness of Distributed Approximation", "comments": "Submitted to Journal (special issue of STOC 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the {\\em verification} problem in distributed networks, stated as\nfollows. Let $H$ be a subgraph of a network $G$ where each vertex of $G$ knows\nwhich edges incident on it are in $H$. We would like to verify whether $H$ has\nsome properties, e.g., if it is a tree or if it is connected. We would like to\nperform this verification in a decentralized fashion via a distributed\nalgorithm. The time complexity of verification is measured as the number of\nrounds of distributed communication. In this paper we initiate a systematic\nstudy of distributed verification, and give almost tight lower bounds on the\nrunning time of distributed verification algorithms for many fundamental\nproblems such as connectivity, spanning connected subgraph, and $s-t$ cut\nverification. We then show applications of these results in deriving strong\nunconditional time lower bounds on the {\\em hardness of distributed\napproximation} for many classical optimization problems including minimum\nspanning tree, shortest paths, and minimum cut. Many of these results are the\nfirst non-trivial lower bounds for both exact and approximate distributed\ncomputation and they resolve previous open questions. Moreover, our\nunconditional lower bound of approximating minimum spanning tree (MST) subsumes\nand improves upon the previous hardness of approximation bound of Elkin [STOC\n2004] as well as the lower bound for (exact) MST computation of Peleg and\nRubinovich [FOCS 1999]. Our result implies that there can be no distributed\napproximation algorithm for MST that is significantly faster than the current\nexact algorithm, for {\\em any} approximation factor. Our lower bound proofs\nshow an interesting connection between communication complexity and distributed\ncomputing which turns out to be useful in establishing the time complexity of\nexact and approximate distributed computation of many problems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Nov 2010 21:06:13 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2011 00:02:26 GMT"}, {"version": "v3", "created": "Sat, 15 Oct 2011 17:01:07 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Sarma", "Atish Das", ""], ["Holzer", "Stephan", ""], ["Kor", "Liah", ""], ["Korman", "Amos", ""], ["Nanongkai", "Danupon", ""], ["Pandurangan", "Gopal", ""], ["Peleg", "David", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1011.3182", "submitter": "Gopal Pandurangan", "authors": "Tim Jacobs and Gopal Pandurangan", "title": "Stochastic Analysis of a Churn-Tolerant Structured Peer-to-Peer Scheme", "comments": null, "journal-ref": "Peer-to-Peer Networking and Applications, 6(1), 1-14, 2013", "doi": "10.1007/s12083-012-0124-z", "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyze a simple and general scheme to build a churn\n(fault)-tolerant structured Peer-to-Peer (P2P) network. Our scheme shows how to\n\"convert\" a static network into a dynamic distributed hash table(DHT)-based P2P\nnetwork such that all the good properties of the static network are guaranteed\nwith high probability (w.h.p). Applying our scheme to a cube-connected cycles\nnetwork, for example, yields a $O(\\log N)$ degree connected network, in which\nevery search succeeds in $O(\\log N)$ hops w.h.p., using $O(\\log N)$ messages,\nwhere $N$ is the expected stable network size. Our scheme has an constant\nstorage overhead (the number of nodes responsible for servicing a data item)\nand an $O(\\log N)$ overhead (messages and time) per insertion and essentially\nno overhead for deletions. All these bounds are essentially optimal. While DHT\nschemes with similar guarantees are already known in the literature, this work\nis new in the following aspects:\n  (1) It presents a rigorous mathematical analysis of the scheme under a\ngeneral stochastic model of churn and shows the above guarantees;\n  (2) The theoretical analysis is complemented by a simulation-based analysis\nthat validates the asymptotic bounds even in moderately sized networks and also\nstudies performance under changing stable network size;\n  (3) The presented scheme seems especially suitable for maintaining dynamic\nstructures under churn efficiently. In particular, we show that a spanning tree\nof low diameter can be efficiently maintained in constant time and logarithmic\nnumber of messages per insertion or deletion w.h.p.\n  Keywords: P2P Network, DHT Scheme, Churn, Dynamic Spanning Tree, Stochastic\nAnalysis.\n", "versions": [{"version": "v1", "created": "Sun, 14 Nov 2010 04:05:49 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 21:24:13 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Jacobs", "Tim", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1011.3373", "submitter": "Torsten Ueckerdt", "authors": "Torsten Ueckerdt", "title": "CAT-generation of ideals", "comments": "This paper has been withdrawn by the author due to a crucial error", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of generating all ideals of a poset. It is a long\nstanding open problem, whether or not the ideals of any poset can be generated\nin constant amortized time, CAT for short. We refine the tree traversal, a\nmethod introduced by Pruesse and Ruskey in 1993, to obtain a CAT-generator for\ntwo large classes of posets: posets of interval dimension at most two and so\ncalled locally planar posets. This includes all posets for which a\nCAT-generator was known before. Posets of interval dimension at most two\ngeneralize both, interval orders and 2-dimensional posets. Locally planar\nposets generalize for example posets with a planar cover graph.\n  We apply our results to CAT-generate all c-orientations of a planar graph. As\na special case this is a CAT-generator for many combinatorial objects like\ndomino and lozenge tilings, planar spanning trees, planar bipartite perfect\nmatchings, Schnyder woods, and others.\n", "versions": [{"version": "v1", "created": "Mon, 15 Nov 2010 13:14:43 GMT"}, {"version": "v2", "created": "Tue, 16 Nov 2010 10:01:00 GMT"}, {"version": "v3", "created": "Wed, 14 Dec 2011 20:31:55 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Ueckerdt", "Torsten", ""]]}, {"id": "1011.3441", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui", "title": "Worst case efficient single and multiple string matching in the Word-RAM\n  model", "comments": "Full version of an extended abstract presented at IWOCA 2010\n  conference", "journal-ref": null, "doi": "10.1007/978-3-642-19222-7_10", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore worst-case solutions for the problems of single and\nmultiple matching on strings in the word RAM model with word length w. In the\nfirst problem, we have to build a data structure based on a pattern p of length\nm over an alphabet of size sigma such that we can answer to the following\nquery: given a text T of length n, where each character is encoded using\nlog(sigma) bits return the positions of all the occurrences of p in T (in the\nfollowing we refer by occ to the number of reported occurrences). For the\nmulti-pattern matching problem we have a set S of d patterns of total length m\nand a query on a text T consists in finding all positions of all occurrences in\nT of the patterns in S. As each character of the text is encoded using log\nsigma bits and we can read w bits in constant time in the RAM model, we assume\nthat we can read up to (w/log sigma) consecutive characters of the text in one\ntime step. This implies that the fastest possible query time for both problems\nis O((n(log sigma/w)+occ). In this paper we present several different results\nfor both problems which come close to that best possible query time. We first\npresent two different linear space data structures for the first and second\nproblem: the first one answers to single pattern matching queries in time\nO(n(1/m+log sigma/w)+occ) while the second one answers to multiple pattern\nmatching queries to O(n((log d+log y+log log d)/y+log sigma/w)+occ) where y is\nthe length of the shortest pattern in the case of multiple pattern-matching. We\nthen show how a simple application of the four russian technique permits to get\ndata structures with query times independent of the length of the shortest\npattern (the length of the only pattern in case of single string matching) at\nthe expense of using more space.\n", "versions": [{"version": "v1", "created": "Mon, 15 Nov 2010 16:37:43 GMT"}, {"version": "v2", "created": "Fri, 14 Jan 2011 13:01:03 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Belazzougui", "Djamal", ""]]}, {"id": "1011.3480", "submitter": "Travis Gagie", "authors": "Travis Gagie and Juha K\\\"arkk\\\"ainen", "title": "Counting Colours in Compressed Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we are asked to preprocess a string \\(s [1..n]\\) such that later,\ngiven a substring's endpoints, we can quickly count how many distinct\ncharacters it contains. In this paper we give a data structure for this problem\nthat takes \\(n H_0 (s) + \\Oh{n} + \\oh{n H_0 (s)}\\) bits, where \\(H_0 (s)\\) is\nthe 0th-order empirical entropy of $s$, and answers queries in $\\Oh{\\log^{1 +\n\\epsilon} n}$ time for any constant \\(\\epsilon > 0\\). We also show how our data\nstructure can be made partially dynamic.\n", "versions": [{"version": "v1", "created": "Mon, 15 Nov 2010 19:30:19 GMT"}], "update_date": "2010-11-16", "authors_parsed": [["Gagie", "Travis", ""], ["K\u00e4rkk\u00e4inen", "Juha", ""]]}, {"id": "1011.3491", "submitter": "Travis Gagie", "authors": "Travis Gagie and Kalle Karhu and Juha K\\\"arkk\\\"ainen and Veli\n  M\\\"akinen and Leena Salmela", "title": "Pattern Kits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we have just performed searches in a self-index for two patterns $A$\nand $B$ and now we want to search for their concatenation \\A B); how can we\nbest make use of our previous computations? In this paper we consider this\nproblem and, more generally, how we can store a dynamic library of patterns\nthat we can easily manipulate in interesting ways. We give a space- and\ntime-efficient data structure for this problem that is compatible with many of\nthe best self-indexes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Nov 2010 20:21:33 GMT"}, {"version": "v2", "created": "Sat, 2 Apr 2011 12:44:53 GMT"}], "update_date": "2011-04-05", "authors_parsed": [["Gagie", "Travis", ""], ["Karhu", "Kalle", ""], ["K\u00e4rkk\u00e4inen", "Juha", ""], ["M\u00e4kinen", "Veli", ""], ["Salmela", "Leena", ""]]}, {"id": "1011.3493", "submitter": "David Doty", "authors": "Ho-Lin Chen, David Doty, and Shinnosuke Seki", "title": "Program Size and Temperature in Self-Assembly", "comments": "The previous version contained more sections, but we have split that\n  paper into two. The other half will be posted as a separate paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Winfree's abstract Tile Assembly Model (aTAM) is a model of molecular\nself-assembly of DNA complexes known as tiles, which float freely in solution\nand attach one at a time to a growing \"seed\" assembly based on specific binding\nsites on their four sides. We show that there is a polynomial-time algorithm\nthat, given an n x n square, finds the minimal tile system (i.e., the system\nwith the smallest number of distinct tile types) that uniquely self-assembles\nthe square, answering an open question of Adleman, Cheng, Goel, Huang, Kempe,\nMoisset de Espanes, and Rothemund (\"Combinatorial Optimization Problems in\nSelf-Assembly\", STOC 2002). Our investigation leading to this algorithm reveals\nother positive and negative results about the relationship between the size of\na tile system and its \"temperature\" (the binding strength threshold required\nfor a tile to attach).\n", "versions": [{"version": "v1", "created": "Mon, 15 Nov 2010 20:22:58 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2011 17:25:06 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Chen", "Ho-Lin", ""], ["Doty", "David", ""], ["Seki", "Shinnosuke", ""]]}, {"id": "1011.3534", "submitter": "Matt Challacombe", "authors": "Matt Challacombe and Nicolas Bock", "title": "Fast Multiplication of Matrices with Decay", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR 10-07458", "categories": "cs.DS cond-mat.mtrl-sci cs.MS cs.NA", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  A fast algorithm for the approximate multiplication of matrices with decay is\nintroduced; the Sparse Approximate Matrix Multiply (SpAMM) reduces complexity\nin the product space, a different approach from current methods that economize\nwithin the matrix space through truncation or rank reduction. Matrix truncation\n(element dropping) is compared to SpAMM for quantum chemical matrices with\napproximate exponential and algebraic decay. For matched errors in the\nelectronic total energy, SpAMM is found to require fewer to far fewer floating\npoint operations relative to dropping. The challenges and opportunities\nafforded by this new approach are discussed, including the potential for high\nperformance implementations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Nov 2010 21:59:11 GMT"}], "update_date": "2010-11-17", "authors_parsed": [["Challacombe", "Matt", ""], ["Bock", "Nicolas", ""]]}, {"id": "1011.3701", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Robert Krauthgamer", "title": "Directed Spanners via Flow-Based Linear Programs", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine directed spanners through flow-based linear programming\nrelaxations. We design an $\\~O(n^{2/3})$-approximation algorithm for the\ndirected $k$-spanner problem that works for all $k\\geq 1$, which is the first\nsublinear approximation for arbitrary edge-lengths. Even in the more restricted\nsetting of unit edge-lengths, our algorithm improves over the previous\n$\\~O(n^{1-1/k})$ approximation of Bhattacharyya et al. when $k\\ge 4$. For the\nspecial case of $k=3$ we design a different algorithm achieving an\n$\\~O(\\sqrt{n})$-approximation, improving the previous $\\~O(n^{2/3})$. Both of\nour algorithms easily extend to the fault-tolerant setting, which has recently\nattracted attention but not from an approximation viewpoint. We also prove a\nnearly matching integrality gap of $\\Omega(n^{\\frac13 - \\epsilon})$ for any\nconstant $\\epsilon > 0$.\n  A virtue of all our algorithms is that they are relatively simple.\nTechnically, we introduce a new yet natural flow-based relaxation, and show how\nto approximately solve it even when its size is not polynomial. The main\nchallenge is to design a rounding scheme that \"coordinates\" the choices of\nflow-paths between the many demand pairs while using few edges overall. We\nachieve this, roughly speaking, by randomization at the level of vertices.\n", "versions": [{"version": "v1", "created": "Tue, 16 Nov 2010 14:14:17 GMT"}, {"version": "v2", "created": "Sun, 21 Nov 2010 13:29:24 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["Dinitz", "Michael", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1011.3770", "submitter": "Deeparnab Chakrabarty", "authors": "Anand Bhalgat and Deeparnab Chakrabarty and Sanjeev Khanna", "title": "Optimal Lower Bounds for Universal and Differentially Private Steiner\n  Tree and TSP", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a metric space on n points, an {\\alpha}-approximate universal algorithm\nfor the Steiner tree problem outputs a distribution over rooted spanning trees\nsuch that for any subset X of vertices containing the root, the expected cost\nof the induced subtree is within an {\\alpha} factor of the optimal Steiner tree\ncost for X. An {\\alpha}-approximate differentially private algorithm for the\nSteiner tree problem takes as input a subset X of vertices, and outputs a tree\ndistribution that induces a solution within an {\\alpha} factor of the optimal\nas before, and satisfies the additional property that for any set X' that\ndiffers in a single vertex from X, the tree distributions for X and X' are\n\"close\" to each other. Universal and differentially private algorithms for TSP\nare defined similarly. An {\\alpha}-approximate universal algorithm for the\nSteiner tree problem or TSP is also an {\\alpha}-approximate differentially\nprivate algorithm. It is known that both problems admit O(logn)-approximate\nuniversal algorithms, and hence O(log n)-approximate differentially private\nalgorithms as well. We prove an {\\Omega}(logn) lower bound on the approximation\nratio achievable for the universal Steiner tree problem and the universal TSP,\nmatching the known upper bounds. Our lower bound for the Steiner tree problem\nholds even when the algorithm is allowed to output a more general solution of a\ndistribution on paths to the root.\n", "versions": [{"version": "v1", "created": "Tue, 16 Nov 2010 17:49:27 GMT"}, {"version": "v2", "created": "Wed, 17 Nov 2010 20:05:00 GMT"}], "update_date": "2010-11-18", "authors_parsed": [["Bhalgat", "Anand", ""], ["Chakrabarty", "Deeparnab", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "1011.3843", "submitter": "Uri Levy", "authors": "Uri Levy", "title": "Magnetic Towers of Hanoi and their Optimal Solutions", "comments": "39 pages, 8 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Magnetic Tower of Hanoi puzzle - a modified \"base 3\" version of the\nclassical Tower of Hanoi puzzle as described in earlier papers, is actually a\nsmall set of independent sister-puzzles, depending on the \"pre-coloring\"\ncombination of the tower's posts. Starting with Red facing up on a Source post,\nworking through an Intermediate - colored or Neutral post, and ending Blue\nfacing up on a Destination post, we identify the different pre-coloring\ncombinations in (S,I,D) order. The Tower's pre-coloring combinations are\n{[(R,B,B) / (R,R,B)] ; [(R,B,N) / (N,R,B)] ; [(N,B,N) / (N,R,N)] ; [R,N,B] ;\n[(R,N,N) / (N,N,B)] ; [N,N,N]}. In this paper we investigate these\nsister-puzzles, identify the algorithm that optimally solves each pre-colored\npuzzle, and prove its Optimality. As it turns out, five of the six algorithms,\nchallenging on their own, are part of the algorithm solving the \"natural\", Free\nMagnetic Tower of Hanoi puzzle [N,N,N]. We start by showing that the N-disk\nColored Tower [(R,B,B) / (R,R,B)] is solved by (3^N - 1)/2 moves. Defining\n\"Algorithm Duration\" as the ratio of number of algorithm-moves solving the\npuzzle to the number of algorithm-moves solving the Colored Tower, we find the\nDuration-Limits for all sister-puzzles. In the order of the list above they are\n{[1] ; [10/11] ; [10/11] ; [8/11] ; [7/11] ; [20/33]}. Thus, the Duration-Limit\nof the Optimal Algorithm solving the Free Magnetic Tower of Hanoi puzzle is\n20/33 or 606 0/00. On the road to optimally solve this colorful Magnetic\npuzzle, we hit other \"forward-moving\" puzzle-solving algorithms. Overall we\nlooked at 10 pairs of integer sequences. Of the twenty integer sequences, five\nare listed in the On-line Encyclopedia of Integer Sequences, the other fifteen\n- not yet. The large set of different solutions is a clear indication to the\nfreedom-of-wondering that makes this Magnetic Tower of Hanoi puzzle so\ncolorful.\n", "versions": [{"version": "v1", "created": "Tue, 16 Nov 2010 22:16:52 GMT"}], "update_date": "2010-11-18", "authors_parsed": [["Levy", "Uri", ""]]}, {"id": "1011.3944", "submitter": "Dmitry Gusev", "authors": "V. F. Romanov", "title": "Non-Orthodox Combinatorial Models Based on Discordant Structures", "comments": "19 pages; typeset in LaTeX, some typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method for compact representation of sets of\nn-dimensional binary sequences in a form of compact triplets structures (CTS),\nsupposing both logic and arithmetic interpretations of data. Suitable\nillustration of CTS application is the unique graph-combinatorial model for the\nclassic intractable 3-Satisfiability problem and a polynomial algorithm for the\nmodel synthesis. The method used for Boolean formulas analysis and\nclassification by means of the model is defined as a bijective mapping\nprinciple for sets of components of discordant structures to a basic set. The\nstatistic computer-aided experiment showed efficiency of the algorithm in a\nlarge scale of problem dimension parameters, including those that make\nenumeration procedures of no use. The formulated principle expands resources of\nconstructive approach to investigation of intractable problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Nov 2010 11:19:31 GMT"}, {"version": "v2", "created": "Wed, 12 Jan 2011 11:27:44 GMT"}], "update_date": "2011-01-13", "authors_parsed": [["Romanov", "V. F.", ""]]}, {"id": "1011.4071", "submitter": "Jure Leskovec", "authors": "L. Backstrom, J. Leskovec", "title": "Supervised Random Walks: Predicting and Recommending Links in Social\n  Networks", "comments": null, "journal-ref": "Proceedings of the Fourth ACM International Conference on Web\n  Search and Data Mining (WSDM '11), February, 2011", "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DS physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the occurrence of links is a fundamental problem in networks. In\nthe link prediction problem we are given a snapshot of a network and would like\nto infer which interactions among existing members are likely to occur in the\nnear future or which existing interactions are we missing. Although this\nproblem has been extensively studied, the challenge of how to effectively\ncombine the information from the network structure with rich node and edge\nattribute data remains largely open.\n  We develop an algorithm based on Supervised Random Walks that naturally\ncombines the information from the network structure with node and edge level\nattributes. We achieve this by using these attributes to guide a random walk on\nthe graph. We formulate a supervised learning task where the goal is to learn a\nfunction that assigns strengths to edges in the network such that a random\nwalker is more likely to visit the nodes to which new links will be created in\nthe future. We develop an efficient training algorithm to directly learn the\nedge strength estimation function.\n  Our experiments on the Facebook social graph and large collaboration networks\nshow that our approach outperforms state-of-the-art unsupervised approaches as\nwell as approaches that are based on feature extraction.\n", "versions": [{"version": "v1", "created": "Wed, 17 Nov 2010 21:01:46 GMT"}], "update_date": "2010-11-19", "authors_parsed": [["Backstrom", "L.", ""], ["Leskovec", "J.", ""]]}, {"id": "1011.4401", "submitter": "Manjish Pal", "authors": "Manjish Pal", "title": "Combinatorial Geometry of Graph Partitioning - I", "comments": "Extension of results in authors' previous paper, CoRR abs/0907.1369", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\sc $c$-Balanced Separator} problem is a graph-partitioning problem in\nwhich given a graph $G$, one aims to find a cut of minimum size such that both\nthe sides of the cut have at least $cn$ vertices. In this paper, we present new\ndirections of progress in the {\\sc $c$-Balanced Separator} problem. More\nspecifically, we propose a family of mathematical programs, that depend upon a\nparameter $p > 0$, and is an extension of the uniform version of the SDPs\nproposed by Goemans and Linial for this problem. In fact for the case, when\n$p=1$, if one can solve this program in polynomial time then simply using the\nGoemans-Williamson's randomized rounding algorithm for {\\sc Max Cut}\n\\cite{WG95} will give an $O(1)$-factor approximation algorithm for {\\sc\n$c$-Balanced Separator} improving the best known approximation factor of\n$O(\\sqrt{\\log n})$ due to Arora, Rao and Vazirani \\cite{ARV}. This family of\nprograms is not convex but one can transform them into so called\n\\emph{\\textbf{concave programs}} in which one optimizes a concave function over\na convex feasible set. It is well known that the optima of such programs lie at\none of the extreme points of the feasible set \\cite{TTT85}. Our main\ncontribution is a combinatorial characterization of some extreme points of the\nfeasible set of the mathematical program, for $p=1$ case, which to the best of\nour knowledge is the first of its kind. We further demonstrate how this\ncharacterization can be used to solve the program in a restricted setting.\nNon-convex programs have recently been investigated by Bhaskara and\nVijayaraghvan \\cite{BV11} in which they design algorithms for approximating\nMatrix $p$-norms although their algorithmic techniques are analytical in\nnature.\n", "versions": [{"version": "v1", "created": "Fri, 19 Nov 2010 11:27:53 GMT"}], "update_date": "2010-11-22", "authors_parsed": [["Pal", "Manjish", ""]]}, {"id": "1011.4465", "submitter": "Gernot Veit Batz", "authors": "Gernot Veit Batz, Robert Geisberger, Dennis Luxen and Peter Sanders", "title": "Compressed Transmission of Route Descriptions", "comments": "7 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two methods to compress the description of a route in a road\nnetwork, i.e., of a path in a directed graph. The first method represents a\npath by a sequence of via edges. The subpaths between the via edges have to be\nunique shortest paths. Instead of via edges also via nodes can be used, though\nthis requires some simple preprocessing. The second method uses contraction\nhierarchies to replace subpaths of the original path by shortcuts. The two\nmethods can be combined with each other. Also, we propose the application to\nmobile server based routing: We compute the route on a server which has access\nto the latest information about congestions for example. Then we transmit the\ncomputed route to the car using some mobile radio communication. There, we\napply the compression to save costs and transmission time. If the compression\nworks well, we can transmit routes even when the bandwidth is low. Although we\nhave not evaluated our ideas with realistic data yet, they are quite promising.\n", "versions": [{"version": "v1", "created": "Fri, 19 Nov 2010 16:36:30 GMT"}], "update_date": "2010-11-22", "authors_parsed": [["Batz", "Gernot Veit", ""], ["Geisberger", "Robert", ""], ["Luxen", "Dennis", ""], ["Sanders", "Peter", ""]]}, {"id": "1011.4532", "submitter": "Travis Gagie", "authors": "Travis Gagie and Gonzalo Navarro and Simon J. Puglisi", "title": "New Algorithms on Wavelet Trees and Applications to Information\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelet trees are widely used in the representation of sequences,\npermutations, text collections, binary relations, discrete points, and other\nsuccinct data structures. We show, however, that this still falls short of\nexploiting all of the virtues of this versatile data structure. In particular\nwe show how to use wavelet trees to solve fundamental algorithmic problems such\nas {\\em range quantile} queries, {\\em range next value} queries, and {\\em range\nintersection} queries. We explore several applications of these queries in\nInformation Retrieval, in particular {\\em document retrieval} in hierarchical\nand temporal documents, and in the representation of {\\em inverted lists}.\n", "versions": [{"version": "v1", "created": "Fri, 19 Nov 2010 22:00:50 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["Gagie", "Travis", ""], ["Navarro", "Gonzalo", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1011.4632", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis, Anastasios Zouzias, Petros Drineas", "title": "Random Projections for $k$-means Clustering", "comments": "Neural Information Processing Systems (NIPS) 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the topic of dimensionality reduction for $k$-means\nclustering. We prove that any set of $n$ points in $d$ dimensions (rows in a\nmatrix $A \\in \\RR^{n \\times d}$) can be projected into $t = \\Omega(k / \\eps^2)$\ndimensions, for any $\\eps \\in (0,1/3)$, in $O(n d \\lceil \\eps^{-2} k/ \\log(d)\n\\rceil )$ time, such that with constant probability the optimal $k$-partition\nof the point set is preserved within a factor of $2+\\eps$. The projection is\ndone by post-multiplying $A$ with a $d \\times t$ random matrix $R$ having\nentries $+1/\\sqrt{t}$ or $-1/\\sqrt{t}$ with equal probability. A numerical\nimplementation of our technique and experiments on a large face images dataset\nverify the speed and the accuracy of our theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 21 Nov 2010 02:37:10 GMT"}], "update_date": "2011-05-05", "authors_parsed": [["Boutsidis", "Christos", ""], ["Zouzias", "Anastasios", ""], ["Drineas", "Petros", ""]]}, {"id": "1011.4955", "submitter": "Steve Oudot", "authors": "David Arthur and Steve Y. Oudot", "title": "Reverse Nearest Neighbors Search in High Dimensions using\n  Locality-Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": "INRIA RR-7084", "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of finding reverse nearest neighbors efficiently.\nAlthough provably good solutions exist for this problem in low or fixed\ndimensions, to this date the methods proposed in high dimensions are mostly\nheuristic. We introduce a method that is both provably correct and efficient in\nall dimensions, based on a reduction of the problem to one instance of\n$\\e$-nearest neighbor search plus a controlled number of instances of {\\em\nexhaustive $r$-\\pleb}, a variant of {\\em Point Location among Equal Balls}\nwhere all the $r$-balls centered at the data points that contain the query\npoint are sought for, not just one. The former problem has been extensively\nstudied and elegantly solved in high dimensions using Locality-Sensitive\nHashing (LSH) techniques. By contrast, the latter problem has a complexity that\nis still not fully understood. We revisit the analysis of the LSH scheme for\nexhaustive $r$-\\pleb using a somewhat refined notion of locality-sensitive\nfamily of hash function, which brings out a meaningful output-sensitive term in\nthe complexity of the problem. Our analysis, combined with a non-isometric\nlifting of the data, enables us to answer exhaustive $r$-\\pleb queries (and\ndown the road reverse nearest neighbors queries) efficiently. Along the way, we\nobtain a simple algorithm for answering exact nearest neighbor queries, whose\ncomplexity is parametrized by some {\\em condition number} measuring the\ninherent difficulty of a given instance of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 22 Nov 2010 21:29:23 GMT"}], "update_date": "2010-11-24", "authors_parsed": [["Arthur", "David", ""], ["Oudot", "Steve Y.", ""]]}, {"id": "1011.4957", "submitter": "Andreas Wiese", "authors": "Jos\\'e Verschae and Andreas Wiese", "title": "On the Configuration-LP for Scheduling on Unrelated Machines", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "Report-no: 025-2010", "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important open problems in machine scheduling is the problem\nof scheduling a set of jobs on unrelated machines to minimize the makespan. The\nbest known approximation algorithm for this problem guarantees an approximation\nfactor of 2. It is known to be NP-hard to approximate with a better ratio than\n3/2. Closing this gap has been open for over 20 years. The best known\napproximation factors are achieved by LP-based algorithms. The strongest known\nlinear program formulation for the problem is the configuration-LP. We show\nthat the configuration-LP has an integrality gap of 2 even for the special case\nof unrelated graph balancing, where each job can be assigned to at most two\nmachines. In particular, our result implies that a large family of cuts does\nnot help to diminish the integrality gap of the canonical assignment-LP. Also,\nwe present cases of the problem which can be approximated with a better factor\nthan 2. They constitute valuable insights for constructing an NP-hardness\nreduction which improves the known lower bound. Very recently Svensson studied\nthe restricted assignment case, where each job can only be assigned to a given\nset of machines on which it has the same processing time. He shows that in this\nsetting the configuration-LP has an integrality gap of 33/17<2. Hence, our\nresult imply that the unrelated graph balancing case is significantly more\ncomplex than the restricted assignment case. Then we turn to another objective\nfunction: maximizing the minimum machine load. For the case that every job can\nbe assigned to at most two machines we give a purely combinatorial\n2-approximation algorithm which is best possible, unless P=NP. This improves on\nthe computationally costly LP-based (2+eps)-approximation algorithm by\nChakrabarty et al.\n", "versions": [{"version": "v1", "created": "Mon, 22 Nov 2010 21:30:29 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Verschae", "Jos\u00e9", ""], ["Wiese", "Andreas", ""]]}, {"id": "1011.5200", "submitter": "Mikkel Thorup", "authors": "Mihai Patrascu and Mikkel Thorup", "title": "The Power of Simple Tabulation Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms are often enjoyed for their simplicity, but the hash\nfunctions used to yield the desired theoretical guarantees are often neither\nsimple nor practical. Here we show that the simplest possible tabulation\nhashing provides unexpectedly strong guarantees.\n  The scheme itself dates back to Carter and Wegman (STOC'77). Keys are viewed\nas consisting of c characters. We initialize c tables T_1, ..., T_c mapping\ncharacters to random hash codes. A key x=(x_1, ..., x_q) is hashed to T_1[x_1]\nxor ... xor T_c[x_c].\n  While this scheme is not even 4-independent, we show that it provides many of\nthe guarantees that are normally obtained via higher independence, e.g.,\nChernoff-type concentration, min-wise hashing for estimating set intersection,\nand cuckoo hashing.\n", "versions": [{"version": "v1", "created": "Tue, 23 Nov 2010 19:18:30 GMT"}, {"version": "v2", "created": "Sat, 7 May 2011 21:56:43 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Patrascu", "Mihai", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1011.5425", "submitter": "Sebastiano Vigna", "authors": "Paolo Boldi, Marco Rosa, Massimo Santini, Sebastiano Vigna", "title": "Layered Label Propagation: A MultiResolution Coordinate-Free Ordering\n  for Compressing Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We continue the line of research on graph compression started with WebGraph,\nbut we move our focus to the compression of social networks in a proper sense\n(e.g., LiveJournal): the approaches that have been used for a long time to\ncompress web graphs rely on a specific ordering of the nodes (lexicographical\nURL ordering) whose extension to general social networks is not trivial. In\nthis paper, we propose a solution that mixes clusterings and orders, and devise\na new algorithm, called Layered Label Propagation, that builds on previous work\non scalable clustering and can be used to reorder very large graphs (billions\nof nodes). Our implementation uses overdecomposition to perform aggressively on\nmulti-core architecture, making it possible to reorder graphs of more than 600\nmillions nodes in a few hours. Experiments performed on a wide array of web\ngraphs and social networks show that combining the order produced by the\nproposed algorithm with the WebGraph compression framework provides a major\nincrease in compression with respect to all currently known techniques, both on\nweb graphs and on social networks. These improvements make it possible to\nanalyse in main memory significantly larger graphs.\n", "versions": [{"version": "v1", "created": "Wed, 24 Nov 2010 16:49:34 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2011 07:59:18 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Boldi", "Paolo", ""], ["Rosa", "Marco", ""], ["Santini", "Massimo", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1011.5447", "submitter": "Rastislav Lenhardt", "authors": "Rastislav Lenhardt", "title": "Proof of Concept: Fast Solutions to NP-problems by Using SAT and Integer\n  Programming Solvers", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, the power of the state-of-the-art SAT and Integer\nProgramming solvers has dramatically increased. They implement many new\ntechniques and heuristics and since any NP problem can be converted to SAT or\nILP instance, we could take advantage of these techniques in general by\nconverting the instance of NP problem to SAT formula or Integer program. A\nproblem we consider, in this proof of concept, is finding a largest clique in a\ngraph. We ran several experiments on large random graphs and compared 3\napproaches: Optimised backtrack solution, Translation to SAT and Translation to\nInteger program. The last one was the fastest one.\n", "versions": [{"version": "v1", "created": "Wed, 24 Nov 2010 18:19:29 GMT"}], "update_date": "2010-11-25", "authors_parsed": [["Lenhardt", "Rastislav", ""]]}, {"id": "1011.5549", "submitter": "Christian Sommer", "authors": "Shay Mozes and Christian Sommer", "title": "Exact Distance Oracles for Planar Graphs", "comments": "To appear in the proceedings of the 23rd ACM-SIAM Symposium on\n  Discrete Algorithms, SODA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new and improved data structures that answer exact node-to-node\ndistance queries in planar graphs. Such data structures are also known as\ndistance oracles. For any directed planar graph on n nodes with non-negative\nlengths we obtain the following:\n  * Given a desired space allocation $S\\in[n\\lg\\lg n,n^2]$, we show how to\nconstruct in $\\tilde O(S)$ time a data structure of size $O(S)$ that answers\ndistance queries in $\\tilde O(n/\\sqrt S)$ time per query.\n  As a consequence, we obtain an improvement over the fastest algorithm for\nk-many distances in planar graphs whenever $k\\in[\\sqrt n,n)$.\n  * We provide a linear-space exact distance oracle for planar graphs with\nquery time $O(n^{1/2+eps})$ for any constant eps>0. This is the first such data\nstructure with provable sublinear query time.\n  * For edge lengths at least one, we provide an exact distance oracle of space\n$\\tilde O(n)$ such that for any pair of nodes at distance D the query time is\n$\\tilde O(min {D,\\sqrt n})$. Comparable query performance had been observed\nexperimentally but has never been explained theoretically.\n  Our data structures are based on the following new tool: given a\nnon-self-crossing cycle C with $c = O(\\sqrt n)$ nodes, we can preprocess G in\n$\\tilde O(n)$ time to produce a data structure of size $O(n \\lg\\lg c)$ that can\nanswer the following queries in $\\tilde O(c)$ time: for a query node u, output\nthe distance from u to all the nodes of C. This data structure builds on and\nextends a related data structure of Klein (SODA'05), which reports distances to\nthe boundary of a face, rather than a cycle.\n  The best distance oracles for planar graphs until the current work are due to\nCabello (SODA'06), Djidjev (WG'96), and Fakcharoenphol and Rao (FOCS'01). For\n$\\sigma\\in(1,4/3)$ and space $S=n^\\sigma$, we essentially improve the query\ntime from $n^2/S$ to $\\sqrt{n^2/S}$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Nov 2010 03:30:26 GMT"}, {"version": "v2", "created": "Wed, 15 Dec 2010 04:08:08 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2011 15:50:24 GMT"}, {"version": "v4", "created": "Thu, 10 Nov 2011 16:28:35 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["Mozes", "Shay", ""], ["Sommer", "Christian", ""]]}, {"id": "1011.5599", "submitter": "Sebastiano Vigna", "authors": "Paolo Boldi, Marco Rosa, Sebastiano Vigna", "title": "HyperANF: Approximating the Neighbourhood Function of Very Large Graphs\n  on a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neighbourhood function N(t) of a graph G gives, for each t, the number of\npairs of nodes <x, y> such that y is reachable from x in less that t hops. The\nneighbourhood function provides a wealth of information about the graph (e.g.,\nit easily allows one to compute its diameter), but it is very expensive to\ncompute it exactly. Recently, the ANF algorithm (approximate neighbourhood\nfunction) has been proposed with the purpose of approximating NG(t) on large\ngraphs. We describe a breakthrough improvement over ANF in terms of speed and\nscalability. Our algorithm, called HyperANF, uses the new HyperLogLog counters\nand combines them efficiently through broadword programming; our implementation\nuses overdecomposition to exploit multi-core parallelism. With HyperANF, for\nthe first time we can compute in a few hours the neighbourhood function of\ngraphs with billions of nodes with a small error and good confidence using a\nstandard workstation. Then, we turn to the study of the distribution of the\nshortest paths between reachable nodes (that can be efficiently approximated by\nmeans of HyperANF), and discover the surprising fact that its index of\ndispersion provides a clear-cut characterisation of proper social networks vs.\nweb graphs. We thus propose the spid (Shortest-Paths Index of Dispersion) of a\ngraph as a new, informative statistics that is able to discriminate between the\nabove two types of graphs. We believe this is the first proposal of a\nsignificant new non-local structural index for complex networks whose\ncomputation is highly scalable.\n", "versions": [{"version": "v1", "created": "Thu, 25 Nov 2010 11:35:38 GMT"}, {"version": "v2", "created": "Wed, 26 Jan 2011 11:38:49 GMT"}], "update_date": "2011-01-27", "authors_parsed": [["Boldi", "Paolo", ""], ["Rosa", "Marco", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1011.5666", "submitter": "Daniel Dadush", "authors": "Daniel Dadush and Chris Peikert and Santosh Vempala", "title": "Enumerative Lattice Algorithms in Any Norm via M-Ellipsoid Coverings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a novel algorithm for enumerating lattice points in any convex body,\nand give applications to several classic lattice problems, including the\nShortest and Closest Vector Problems (SVP and CVP, respectively) and Integer\nProgramming (IP). Our enumeration technique relies on a classical concept from\nasymptotic convex geometry known as the M-ellipsoid, and uses as a crucial\nsubroutine the recent algorithm of Micciancio and Voulgaris (STOC 2010) for\nlattice problems in the l_2 norm. As a main technical contribution, which may\nbe of independent interest, we build on the techniques of Klartag (Geometric\nand Functional Analysis, 2006) to give an expected 2^O(n)-time algorithm for\ncomputing an M-ellipsoid for any n-dimensional convex body.\n  As applications, we give deterministic 2^{O(n)}-time and -space algorithms\nfor solving exact SVP, and exact CVP when the target point is sufficiently\nclose to the lattice, on n-dimensional lattices in any (semi-)norm given an\nM-ellipsoid of the unit ball. In many norms of interest, including all l_p\nnorms, an M-ellipsoid is computable in deterministic poly(n) time, in which\ncase these algorithms are fully deterministic. Here our approach may be seen as\na derandomization of the \"AKS sieve\" for exact SVP and CVP (Ajtai, Kumar, and\nSivakumar; STOC 2001 and CCC 2002).\n  As a further application of our SVP algorithm, we derive an expected\nO(f*(n))^n-time algorithm for Integer Programming, where f*(n) denotes the\noptimal bound in the so-called \"flatness theorem,\" which satisfies f*(n) =\nO(n^{4/3} \\polylog(n)) and is conjectured to be f*(n)=\\Theta(n). Our runtime\nimproves upon the previous best of O(n^{2})^{n} by Hildebrand and Koppe (2010).\n", "versions": [{"version": "v1", "created": "Thu, 25 Nov 2010 18:24:17 GMT"}, {"version": "v2", "created": "Mon, 29 Nov 2010 23:14:37 GMT"}, {"version": "v3", "created": "Sun, 5 Dec 2010 23:49:29 GMT"}, {"version": "v4", "created": "Sun, 12 Jun 2011 20:40:54 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Dadush", "Daniel", ""], ["Peikert", "Chris", ""], ["Vempala", "Santosh", ""]]}, {"id": "1011.6100", "submitter": "Grigory Yaroslavtsev", "authors": "Piotr Berman, Arnab Bhattacharyya, Elena Grigorescu, Sofya\n  Raskhodnikova, David Woodruff, Grigory Yaroslavtsev", "title": "Steiner Transitive-Closure Spanners of d-Dimensional Posets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph G and an integer k >= 1, a\nk-transitive-closure-spanner (k-TCspanner) of G is a directed graph H that has\n(1) the same transitive-closure as G and (2) diameter at most k. In some\napplications, the shortcut paths added to the graph in order to obtain small\ndiameter can use Steiner vertices, that is, vertices not in the original graph\nG. The resulting spanner is called a Steiner transitive-closure spanner\n(Steiner TC-spanner).\n  Motivated by applications to property reconstruction and access control\nhierarchies, we concentrate on Steiner TC-spanners of directed acyclic graphs\nor, equivalently, partially ordered sets. In these applications, the goal is to\nfind a sparsest Steiner k-TC-spanner of a poset G for a given k and G. The\nfocus of this paper is the relationship between the dimension of a poset and\nthe size of its sparsest Steiner TCspanner. The dimension of a poset G is the\nsmallest d such that G can be embedded into a d-dimensional directed hypergrid\nvia an order-preserving embedding.\n  We present a nearly tight lower bound on the size of Steiner 2-TC-spanners of\nd-dimensional directed hypergrids. It implies better lower bounds on the\ncomplexity of local reconstructors of monotone functions and functions with low\nLipschitz constant. The proof of the lower bound constructs a dual solution to\na linear programming relaxation of the Steiner 2-TC-spanner problem. We also\nshow that one can efficiently construct a Steiner 2-TC-spanner, of size\nmatching the lower bound, for any low-dimensional poset. Finally, we present a\nlower bound on the size of Steiner k-TC-spanners of d-dimensional posets that\nshows that the best-known construction, due to De Santis et al., cannot be\nimproved significantly.\n", "versions": [{"version": "v1", "created": "Sun, 28 Nov 2010 22:40:46 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Berman", "Piotr", ""], ["Bhattacharyya", "Arnab", ""], ["Grigorescu", "Elena", ""], ["Raskhodnikova", "Sofya", ""], ["Woodruff", "David", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1011.6181", "submitter": "Raphael Yuster", "authors": "Raphael Yuster", "title": "Computing the diameter polynomially faster than APSP", "comments": "revised to handle negative weights; faster algorithm for positive\n  weights; added observation regarding the unweighted case", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new randomized algorithm for computing the diameter of a\nweighted directed graph. The algorithm runs in\n$\\Ot(M^{\\w/(\\w+1)}n^{(\\w^2+3)/(\\w+1)})$ time, where $\\w < 2.376$ is the\nexponent of fast matrix multiplication, $n$ is the number of vertices of the\ngraph, and the edge weights are integers in $\\{-M,...,0,...,M\\}$. For bounded\ninteger weights the running time is $O(n^{2.561})$ and if $\\w=2+o(1)$ it is\n$\\Ot(n^{7/3})$. This is the first algorithm that computes the diameter of an\ninteger weighted directed graph polynomially faster than any known All-Pairs\nShortest Paths (APSP) algorithm. For bounded integer weights, the fastest\nalgorithm for APSP runs in $O(n^{2.575})$ time for the present value of $\\w$\nand runs in $\\Ot(n^{2.5})$ time if $\\w=2+o(1)$.\n  For directed graphs with {\\em positive} integer weights in $\\{1,...,M\\}$ we\nobtain a deterministic algorithm that computes the diameter in $\\Ot(Mn^\\w)$\ntime. This extends a simple $\\Ot(n^\\w)$ algorithm for computing the diameter of\nan {\\em unweighted} directed graph to the positive integer weighted setting and\nis the first algorithm in this setting whose time complexity matches that of\nthe fastest known Diameter algorithm for {\\em undirected} graphs.\n  The diameter algorithms are consequences of a more general result. We\nconstruct algorithms that for any given integer $d$, report all ordered pairs\nof vertices having distance {\\em at most} $d$. The diameter can therefore be\ncomputed using binary search for the smallest $d$ for which all pairs are\nreported.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 10:26:27 GMT"}, {"version": "v2", "created": "Thu, 13 Jan 2011 16:58:34 GMT"}], "update_date": "2011-01-14", "authors_parsed": [["Yuster", "Raphael", ""]]}, {"id": "1011.6187", "submitter": "Jens M. Schmidt", "authors": "Jens M. Schmidt", "title": "Contractions, Removals and How to Certify 3-Connectivity in Linear Time", "comments": "preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known as an existence result that every 3-connected graph G=(V,E)\non more than 4 vertices admits a sequence of contractions and a sequence of\nremoval operations to K_4 such that every intermediate graph is 3-connected. We\nshow that both sequences can be computed in optimal time, improving the\npreviously best known running times of O(|V|^2) to O(|V|+|E|). This settles\nalso the open question of finding a linear time 3-connectivity test that is\ncertifying and extends to a certifying 3-edge-connectivity test in the same\ntime. The certificates used are easy to verify in time O(|E|).\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 10:46:27 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Schmidt", "Jens M.", ""]]}, {"id": "1011.6239", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and Geevarghese Philip and Marcin Pilipczuk and Micha{\\l}\n  Pilipczuk and Jakub Onufry Wojtaszczyk", "title": "Dominating Set is Fixed Parameter Tractable in Claw-free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the dominating set problem parameterized by solution size is\nfixed-parameter tractable (FPT) in graphs that do not contain the claw\n(K(1,3)), the complete bipartite graph on four vertices where the two parts\nhave one and three vertices, respectively) as an induced subgraph. We present\nan algorithm that uses 2^O(k^2)n^O(1) time and polynomial space to decide\nwhether a claw-free graph on n vertices has a dominating set of size at most k.\nNote that this parameterization of dominating set is W[2]-hard on the set of\nall graphs, and thus is unlikely to have an FPT algorithm for graphs in\ngeneral. The most general class of graphs for which an FPT algorithm was\npreviously known for this parameterization of dominating set is the class of\nK(i,j)-free graphs, which exclude, for some fixed i,j, the complete bipartite\ngraph K(i,j) as a subgraph. For i,i >= 2, the class of claw-free graphs and any\nclass of K(i,j)-free graphs are not comparable with respect to set inclusion.\nWe thus extend the range of graphs over which this parameterization of\ndominating set is known to be fixed-parameter tractable. We also show that, in\nsome sense, it is the presence of the claw that makes this parameterization of\nthe dominating set problem hard. More precisely, we show that for any t ?>= 4,\nthe dominating set problem parameterized by the solution size is W[2]-hard in\ngraphs that exclude the t-claw K(1,t) as an induced subgraph. Our arguments\nalso imply that the related connected dominating set and dominating clique\nproblems are W[2]-hard in these graph classes. Finally, we show that for any t,\nthe clique problem parameterized by solution size, which is W[1]-hard on\ngeneral graphs, is FPT in t-claw-free graphs. Our results add to the small and\ngrowing collection of FPT results for graph classes defined by excluded\nsubgraphs, rather than by excluded minors.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 14:28:40 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2011 13:19:14 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Cygan", "Marek", ""], ["Philip", "Geevarghese", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Wojtaszczyk", "Jakub Onufry", ""]]}, {"id": "1011.6267", "submitter": "Igor Razgon", "authors": "Igor Razgon", "title": "Computing multiway cut within the given excess over the largest minimum\n  isolating cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(G,T)$ be an instance of the (vertex) multiway cut problem where $G$ is\na graph and $T$ is a set of terminals. For $t \\in T$, a set of nonterminal\nvertices separating $t$ from $T \\setminus \\{T\\}$ is called an \\emph{isolating\ncut} of $t$. The largest among all the smallest isolating cuts is a natural\nlower bound for a multiway cut of $(G,T)$. Denote this lower bound by $m$ and\nlet $k$ be an integer.\n  In this paper we propose an $O(kn^{k+3})$ algorithm that computes a multiway\ncut of $(G,T)$ of size at most $m+k$ or reports that there is no such multiway\ncut. The core of the proposed algorithm is the following combinatorial result.\nLet $G$ be a graph and let $X,Y$ be two disjoint subsets of vertices of $G$.\nLet $m$ be the smallest size of a vertex $X-Y$ separator. Then, for the given\ninteger $k$, the number of \\emph{important} $X-Y$ separators \\cite{MarxTCS} of\nsize at most $m+k$ is at most $\\sum_{i=0}^k{n \\choose i}$.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 15:35:51 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Razgon", "Igor", ""]]}, {"id": "1011.6397", "submitter": "Raghu Meka", "authors": "Raghu Meka", "title": "Almost Optimal Explicit Johnson-Lindenstrauss Transformations", "comments": "Updated references to prior work and minor formatting changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Johnson-Lindenstrauss lemma is a fundamental result in probability with\nseveral applications in the design and analysis of algorithms in high\ndimensional geometry. Most known constructions of linear embeddings that\nsatisfy the Johnson-Lindenstrauss property involve randomness. We address the\nquestion of explicitly constructing such embedding families and provide a\nconstruction with an almost optimal use of randomness: we use\nO(log(n/delta)log(log(n/delta)/epsilon)) random bits for embedding n dimensions\nto O(log(1/delta)/epsilon^2) dimensions with error probability at most delta,\nand distortion at most epsilon.\n  In particular, for delta = 1/poly(n) and fixed epsilon, we use O(log n loglog\nn) random bits. Previous constructions required at least O(log^2 n) random bits\nto get polynomially small error.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 21:42:10 GMT"}, {"version": "v2", "created": "Fri, 10 Dec 2010 20:23:02 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Meka", "Raghu", ""]]}, {"id": "1011.6543", "submitter": "Bartosz Meglicki", "authors": "Bartosz Meglicki", "title": "Generating functions partitioning algorithm for computing power indices\n  in weighted voting games", "comments": "15 pages, algorithm pessimistic complexity O(n 2^(n/2)),\n  pseudopolynomial complexity O(nq), calculates Banzhaf indices of all players,\n  #P-complete problem. Minor errors corrected. Explicit explanation of general\n  (non-integer) case without pseudopolynomial complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper new algorithm for calculating power indices is described. The\ncomplexity class of the problem is #P-complete and even calculating power index\nof the biggest player is NP-hard task. Constructed algorithm is a mix of ideas\nof two algorithms: Klinz & Woeginger partitioning algorithm and Mann & Shapley\ngenerating functions algorithm. Time and space complexities of the algorithm\nare analysed and compared with other known algorithms for the problem.\nConstructed algorithm has pessimistic time complexity O(n 2^(n/2))and\npseudopolynomial complexity O(nq), where q is quota of the voting game. This\npaper also solves open problem stated by H. Aziz and M. Paterson - existence of\nthe algorithm for calculating Banzhaf power indices of all players with time\ncomplexity lower than O(n^2 2^(n/2)). Not only is the answer positive but this\ncan be done keeping the pseudopolynomial complexity of generating functions\nalgorithm in case weights are integers. New open problems are stated.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 13:38:46 GMT"}, {"version": "v2", "created": "Sat, 22 Jan 2011 13:55:18 GMT"}], "update_date": "2011-01-25", "authors_parsed": [["Meglicki", "Bartosz", ""]]}, {"id": "1011.6664", "submitter": "Raymond Hemmecke", "authors": "Raymond Hemmecke, Silvia Lindner and Milan Studen\\'y", "title": "Learning restricted Bayesian network structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are basic graphical models, used widely both in statistics\nand artificial intelligence. These statistical models of conditional\nindependence structure are described by acyclic directed graphs whose nodes\ncorrespond to (random) variables in consideration. A quite important topic is\nthe learning of Bayesian network structures, which is determining the best\nfitting statistical model on the basis of given data. Although there are\nlearning methods based on statistical conditional independence tests,\ncontemporary methods are mainly based on maximization of a suitable quality\ncriterion that evaluates how good the graph explains the occurrence of the\nobserved data. This leads to a nonlinear combinatorial optimization problem\nthat is in general NP-hard to solve. In this paper we deal with the complexity\nof learning restricted Bayesian network structures, that is, we wish to find\nnetwork structures of highest score within a given subset of all possible\nnetwork structures. For this, we introduce a new unique algebraic\nrepresentative for these structures, called the characteristic imset. We show\nthat these imsets are always 0-1-vectors and that they have many nice\nproperties that allow us to simplify long proofs for some known results and to\neasily establish new complexity results for learning restricted Bayes network\nstructures.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 20:20:55 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["Hemmecke", "Raymond", ""], ["Lindner", "Silvia", ""], ["Studen\u00fd", "Milan", ""]]}]