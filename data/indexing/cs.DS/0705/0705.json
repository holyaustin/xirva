[{"id": "0705.0204", "submitter": "Tshilidzi Marwala", "authors": "Lukasz A. Machowski, and Tshilidzi Marwala", "title": "Using Images to create a Hierarchical Grid Spatial Index", "comments": "In Proceedings of the IEEE International Conference on Systems, Man\n  and Cybernetics, Taiwan, 2006, pp. 1974-1979", "journal-ref": null, "doi": "10.1109/ICSMC.2006.385020", "report-no": null, "categories": "cs.DS", "license": null, "abstract": "  This paper presents a hybrid approach to spatial indexing of two dimensional\ndata. It sheds new light on the age old problem by thinking of the traditional\nalgorithms as working with images. Inspiration is drawn from an analogous\nsituation that is found in machine and human vision. Image processing\ntechniques are used to assist in the spatial indexing of the data. A fixed grid\napproach is used and bins with too many records are sub-divided hierarchically.\nSearch queries are pre-computed for bins that do not contain any data records.\nThis has the effect of dividing the search space up into non rectangular\nregions which are based on the spatial properties of the data. The bucketing\nquad tree can be considered as an image with a resolution of two by two for\neach layer. The results show that this method performs better than the quad\ntree if there are more divisions per layer. This confirms our suspicions that\nthe algorithm works better if it gets to look at the data with higher\nresolution images. An elegant class structure is developed where the\nimplementation of concrete spatial indexes for a particular data type merely\nrelies on rendering the data onto an image.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2007 05:37:32 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Machowski", "Lukasz A.", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "0705.0253", "submitter": "Jian Li", "authors": "Mordecai Golin and Li Jian", "title": "More Efficient Algorithms and Analyses for Unequal Letter Cost\n  Prefix-Free Coding", "comments": "29 pages;9 figures;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": null, "abstract": "  There is a large literature devoted to the problem of finding an optimal\n(min-cost) prefix-free code with an unequal letter-cost encoding alphabet of\nsize. While there is no known polynomial time algorithm for solving it\noptimally there are many good heuristics that all provide additive errors to\noptimal. The additive error in these algorithms usually depends linearly upon\nthe largest encoding letter size.\n  This paper was motivated by the problem of finding optimal codes when the\nencoding alphabet is infinite. Because the largest letter cost is infinite, the\nprevious analyses could give infinite error bounds. We provide a new algorithm\nthat works with infinite encoding alphabets. When restricted to the finite\nalphabet case, our algorithm often provides better error bounds than the best\nprevious ones known.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2007 11:23:52 GMT"}, {"version": "v2", "created": "Thu, 3 May 2007 09:00:23 GMT"}], "update_date": "2007-07-13", "authors_parsed": [["Golin", "Mordecai", ""], ["Jian", "Li", ""]]}, {"id": "0705.0413", "submitter": "David Eppstein", "authors": "David Eppstein, Marc van Kreveld, Elena Mumford, and Bettina Speckmann", "title": "Edges and Switches, Tunnels and Bridges", "comments": "15 pages, 11 figures. To appear in 10th Worksh. Algorithms and Data\n  Structures, Halifax, Nova Scotia, 2007. This version includes three pages of\n  appendices that will not be included in the conference proceedings version", "journal-ref": "Computational Geometry Theory & Applications 42(8): 790-802, 2009", "doi": "10.1016/j.comgeo.2008.05.005", "report-no": null, "categories": "cs.DS cs.CG", "license": null, "abstract": "  Edge casing is a well-known method to improve the readability of drawings of\nnon-planar graphs. A cased drawing orders the edges of each edge crossing and\ninterrupts the lower edge in an appropriate neighborhood of the crossing.\nCertain orders will lead to a more readable drawing than others. We formulate\nseveral optimization criteria that try to capture the concept of a \"good\" cased\ndrawing. Further, we address the algorithmic question of how to turn a given\ndrawing into an optimal cased drawing. For many of the resulting optimization\nproblems, we either find polynomial time algorithms or NP-hardness results.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2007 06:33:04 GMT"}], "update_date": "2009-07-09", "authors_parsed": [["Eppstein", "David", ""], ["van Kreveld", "Marc", ""], ["Mumford", "Elena", ""], ["Speckmann", "Bettina", ""]]}, {"id": "0705.0552", "submitter": "Rajeev Raman", "authors": "Rajeev Raman, Venkatesh Raman, Srinivasa Rao Satti", "title": "Succinct Indexable Dictionaries with Applications to Encoding $k$-ary\n  Trees, Prefix Sums and Multisets", "comments": "Final version of SODA 2002 paper; supersedes Leicester Tech report\n  2002/16", "journal-ref": "ACM Transactions on Algorithms vol 3 (2007), Article 43, 25pp", "doi": "10.1145/1290672.1290680", "report-no": null, "categories": "cs.DS cs.DM cs.IT math.IT", "license": null, "abstract": "  We consider the {\\it indexable dictionary} problem, which consists of storing\na set $S \\subseteq \\{0,...,m-1\\}$ for some integer $m$, while supporting the\noperations of $\\Rank(x)$, which returns the number of elements in $S$ that are\nless than $x$ if $x \\in S$, and -1 otherwise; and $\\Select(i)$ which returns\nthe $i$-th smallest element in $S$. We give a data structure that supports both\noperations in O(1) time on the RAM model and requires ${\\cal B}(n,m) + o(n) +\nO(\\lg \\lg m)$ bits to store a set of size $n$, where ${\\cal B}(n,m) = \\ceil{\\lg\n{m \\choose n}}$ is the minimum number of bits required to store any $n$-element\nsubset from a universe of size $m$. Previous dictionaries taking this space\nonly supported (yes/no) membership queries in O(1) time. In the cell probe\nmodel we can remove the $O(\\lg \\lg m)$ additive term in the space bound,\nanswering a question raised by Fich and Miltersen, and Pagh.\n  We present extensions and applications of our indexable dictionary data\nstructure, including:\n  An information-theoretically optimal representation of a $k$-ary cardinal\ntree that supports standard operations in constant time,\n  A representation of a multiset of size $n$ from $\\{0,...,m-1\\}$ in ${\\cal\nB}(n,m+n) + o(n)$ bits that supports (appropriate generalizations of) $\\Rank$\nand $\\Select$ operations in constant time, and\n  A representation of a sequence of $n$ non-negative integers summing up to $m$\nin ${\\cal B}(n,m+n) + o(n)$ bits that supports prefix sum queries in constant\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2007 07:47:05 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Raman", "Rajeev", ""], ["Raman", "Venkatesh", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "0705.0561", "submitter": "Jingchao Chen", "authors": "Jing-Chao Chen", "title": "Iterative Rounding for the Closest String Problem", "comments": "This paper has been published in abstract Booklet of CiE09", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The closest string problem is an NP-hard problem, whose task is to find a\nstring that minimizes maximum Hamming distance to a given set of strings. This\ncan be reduced to an integer program (IP). However, to date, there exists no\nknown polynomial-time algorithm for IP. In 2004, Meneses et al. introduced a\nbranch-and-bound (B & B) method for solving the IP problem. Their algorithm is\nnot always efficient and has the exponential time complexity. In the paper, we\nattempt to solve efficiently the IP problem by a greedy iterative rounding\ntechnique. The proposed algorithm is polynomial time and much faster than the\nexisting B & B IP for the CSP. If the number of strings is limited to 3, the\nalgorithm is provably at most 1 away from the optimum. The empirical results\nshow that in many cases we can find an exact solution. Even though we fail to\nfind an exact solution, the solution found is very close to exact solution.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2007 03:01:42 GMT"}, {"version": "v2", "created": "Wed, 11 May 2011 00:18:55 GMT"}], "update_date": "2011-05-12", "authors_parsed": [["Chen", "Jing-Chao", ""]]}, {"id": "0705.0588", "submitter": "Edgar Graaf de", "authors": "Edgar H. de Graaf, Joost N. Kok, Walter A. Kosters", "title": "Clustering Co-occurrence of Maximal Frequent Patterns in Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": null, "abstract": "  One way of getting a better view of data is using frequent patterns. In this\npaper frequent patterns are subsets that occur a minimal number of times in a\nstream of itemsets. However, the discovery of frequent patterns in streams has\nalways been problematic. Because streams are potentially endless it is in\nprinciple impossible to say if a pattern is often occurring or not. Furthermore\nthe number of patterns can be huge and a good overview of the structure of the\nstream is lost quickly. The proposed approach will use clustering to facilitate\nthe analysis of the structure of the stream.\n  A clustering on the co-occurrence of patterns will give the user an improved\nview on the structure of the stream. Some patterns might occur so much together\nthat they should form a combined pattern. In this way the patterns in the\nclustering will be the largest frequent patterns: maximal frequent patterns.\n  Our approach to decide if patterns occur often together will be based on a\nmethod of clustering when only the distance between pairs is known. The number\nof maximal frequent patterns is much smaller and combined with clustering\nmethods these patterns provide a good view on the structure of the stream.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2007 10:36:53 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["de Graaf", "Edgar H.", ""], ["Kok", "Joost N.", ""], ["Kosters", "Walter A.", ""]]}, {"id": "0705.0593", "submitter": "Edgar Graaf de", "authors": "Edgar H. de Graaf, Joost N. Kok, Walter A. Kosters", "title": "Clustering with Lattices in the Analysis of Graph Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": null, "abstract": "  Mining frequent subgraphs is an area of research where we have a given set of\ngraphs (each graph can be seen as a transaction), and we search for (connected)\nsubgraphs contained in many of these graphs. In this work we will discuss\ntechniques used in our framework Lattice2SAR for mining and analysing frequent\nsubgraph data and their corresponding lattice information. Lattice information\nis provided by the graph mining algorithm gSpan; it contains all\nsupergraph-subgraph relations of the frequent subgraph patterns -- and their\nsupports.\n  Lattice2SAR is in particular used in the analysis of frequent graph patterns\nwhere the graphs are molecules and the frequent subgraphs are fragments. In the\nanalysis of fragments one is interested in the molecules where patterns occur.\nThis data can be very extensive and in this paper we focus on a technique of\nmaking it better available by using the lattice information in our clustering.\nNow we can reduce the number of times the highly compressed occurrence data\nneeds to be accessed by the user. The user does not have to browse all the\noccurrence data in search of patterns occurring in the same molecules. Instead\none can directly see which frequent subgraphs are of interest.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2007 10:52:28 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["de Graaf", "Edgar H.", ""], ["Kok", "Joost N.", ""], ["Kosters", "Walter A.", ""]]}, {"id": "0705.0933", "submitter": "Max Neunh\\\"offer", "authors": "Max Neunhoeffer, Cheryl E. Praeger", "title": "Computing Minimal Polynomials of Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA cs.DS", "license": null, "abstract": "  We present and analyse a Monte-Carlo algorithm to compute the minimal\npolynomial of an $n\\times n$ matrix over a finite field that requires $O(n^3)$\nfield operations and O(n) random vectors, and is well suited for successful\npractical implementation. The algorithm, and its complexity analysis, use\nstandard algorithms for polynomial and matrix operations. We compare features\nof the algorithm with several other algorithms in the literature. In addition\nwe present a deterministic verification procedure which is similarly efficient\nin most cases but has a worst-case complexity of $O(n^4)$. Finally, we report\nthe results of practical experiments with an implementation of our algorithms\nin comparison with the current algorithms in the {\\sf GAP} library.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2007 15:48:12 GMT"}, {"version": "v2", "created": "Mon, 7 Apr 2008 12:18:34 GMT"}], "update_date": "2008-04-07", "authors_parsed": [["Neunhoeffer", "Max", ""], ["Praeger", "Cheryl E.", ""]]}, {"id": "0705.1025", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Recognizing Partial Cubes in Quadratic Time", "comments": "25 pages, five figures. This version significantly expands previous\n  versions, including a new report on an implementation of the algorithm and\n  experiments with it", "journal-ref": "Journal of Graph Algorithms and Applications 15(2) 269-293, 2011", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to test whether a graph with n vertices and m edges is a partial\ncube, and if so how to find a distance-preserving embedding of the graph into a\nhypercube, in the near-optimal time bound O(n^2), improving previous O(nm)-time\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2007 17:59:08 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2011 22:39:16 GMT"}], "update_date": "2011-07-21", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "0705.1033", "submitter": "Kebin Wang", "authors": "Michael A. Bender, Bradley C. Kuszmaul, Shang-Hua Teng, Kebin Wang", "title": "Optimal Cache-Oblivious Mesh Layouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mesh is a graph that divides physical space into regularly-shaped regions.\nMeshes computations form the basis of many applications, e.g. finite-element\nmethods, image rendering, and collision detection. In one important mesh\nprimitive, called a mesh update, each mesh vertex stores a value and repeatedly\nupdates this value based on the values stored in all neighboring vertices. The\nperformance of a mesh update depends on the layout of the mesh in memory.\n  This paper shows how to find a memory layout that guarantees that the mesh\nupdate has asymptotically optimal memory performance for any set of memory\nparameters. Such a memory layout is called cache-oblivious. Formally, for a\n$d$-dimensional mesh $G$, block size $B$, and cache size $M$ (where\n$M=\\Omega(B^d)$), the mesh update of $G$ uses $O(1+|G|/B)$ memory transfers.\nThe paper also shows how the mesh-update performance degrades for smaller\ncaches, where $M=o(B^d)$.\n  The paper then gives two algorithms for finding cache-oblivious mesh layouts.\nThe first layout algorithm runs in time $O(|G|\\log^2|G|)$ both in expectation\nand with high probability on a RAM. It uses $O(1+|G|\\log^2(|G|/M)/B)$ memory\ntransfers in expectation and $O(1+(|G|/B)(\\log^2(|G|/M) + \\log|G|))$ memory\ntransfers with high probability in the cache-oblivious and disk-access machine\n(DAM) models. The layout is obtained by finding a fully balanced decomposition\ntree of $G$ and then performing an in-order traversal of the leaves of the\ntree. The second algorithm runs faster by almost a $\\log|G|/\\log\\log|G|$ factor\nin all three memory models, both in expectation and with high probability. The\nlayout obtained by finding a relax-balanced decomposition tree of $G$ and then\nperforming an in-order traversal of the leaves of the tree.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2007 05:59:55 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2009 18:45:25 GMT"}], "update_date": "2009-10-05", "authors_parsed": [["Bender", "Michael A.", ""], ["Kuszmaul", "Bradley C.", ""], ["Teng", "Shang-Hua", ""], ["Wang", "Kebin", ""]]}, {"id": "0705.1364", "submitter": "Mustaq Ahmed", "authors": "Mustaq Ahmed and Anna Lubiw", "title": "An Approximation Algorithm for Shortest Descending Paths", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "CS-2007-14", "categories": "cs.CG cs.DS", "license": null, "abstract": "  A path from s to t on a polyhedral terrain is descending if the height of a\npoint p never increases while we move p along the path from s to t. No\nefficient algorithm is known to find a shortest descending path (SDP) from s to\nt in a polyhedral terrain. We give a simple approximation algorithm that solves\nthe SDP problem on general terrains. Our algorithm discretizes the terrain with\nO(n^2 X / e) Steiner points so that after an O(n^2 X / e * log(n X /e))-time\npreprocessing phase for a given vertex s, we can determine a (1+e)-approximate\nSDP from s to any point v in O(n) time if v is either a vertex of the terrain\nor a Steiner point, and in O(n X /e) time otherwise. Here n is the size of the\nterrain, and X is a parameter of the geometry of the terrain.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2007 22:02:28 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Ahmed", "Mustaq", ""], ["Lubiw", "Anna", ""]]}, {"id": "0705.1521", "submitter": "Frank Gurski", "authors": "Frank Gurski", "title": "A note on module-composed graphs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": null, "abstract": "  In this paper we consider module-composed graphs, i.e. graphs which can be\ndefined by a sequence of one-vertex insertions v_1,...,v_n, such that the\nneighbourhood of vertex v_i, 2<= i<= n, forms a module (a homogeneous set) of\nthe graph defined by vertices v_1,..., v_{i-1}.\n  We show that module-composed graphs are HHDS-free and thus homogeneously\norderable, weakly chordal, and perfect. Every bipartite distance hereditary\ngraph, every (co-2C_4,P_4)-free graph and thus every trivially perfect graph is\nmodule-composed. We give an O(|V_G|(|V_G|+|E_G|)) time algorithm to decide\nwhether a given graph G is module-composed and construct a corresponding\nmodule-sequence.\n  For the case of bipartite graphs, module-composed graphs are exactly distance\nhereditary graphs, which implies simple linear time algorithms for their\nrecognition and construction of a corresponding module-sequence.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2007 18:08:22 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2007 16:30:53 GMT"}], "update_date": "2007-07-23", "authors_parsed": [["Gurski", "Frank", ""]]}, {"id": "0705.1750", "submitter": "Peng Cui", "authors": "Peng Cui", "title": "A Tighter Analysis of Setcover Greedy Algorithm for Test Set", "comments": "12 pages, 3 figures, Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Setcover greedy algorithm is a natural approximation algorithm for test set\nproblem. This paper gives a precise and tighter analysis of performance\nguarantee of this algorithm. The author improves the performance guarantee\n$2\\ln n$ which derives from set cover problem to $1.1354\\ln n$ by applying the\npotential function technique. In addition, the author gives a nontrivial lower\nbound $1.0004609\\ln n$ of performance guarantee of this algorithm. This lower\nbound, together with the matching bound of information content heuristic,\nconfirms the fact information content heuristic is slightly better than\nsetcover greedy algorithm in worst case.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2007 04:18:36 GMT"}, {"version": "v2", "created": "Thu, 17 May 2007 09:32:21 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2009 02:58:29 GMT"}, {"version": "v4", "created": "Sat, 4 Apr 2009 02:46:12 GMT"}, {"version": "v5", "created": "Sat, 29 Jan 2011 04:49:11 GMT"}, {"version": "v6", "created": "Sat, 5 Mar 2011 00:17:44 GMT"}], "update_date": "2011-03-08", "authors_parsed": [["Cui", "Peng", ""]]}, {"id": "0705.1876", "submitter": "Grzegorz Malewicz", "authors": "Grzegorz Malewicz", "title": "Scheduling Dags under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": null, "abstract": "  This paper introduces a parallel scheduling problem where a directed acyclic\ngraph modeling $t$ tasks and their dependencies needs to be executed on $n$\nunreliable workers. Worker $i$ executes task $j$ correctly with probability\n$p_{i,j}$. The goal is to find a regimen $\\Sigma$, that dictates how workers\nget assigned to tasks (possibly in parallel and redundantly) throughout\nexecution, so as to minimize the expected completion time. This fundamental\nparallel scheduling problem arises in grid computing and project management\nfields, and has several applications.\n  We show a polynomial time algorithm for the problem restricted to the case\nwhen dag width is at most a constant and the number of workers is also at most\na constant. These two restrictions may appear to be too severe. However, they\nare fundamentally required. Specifically, we demonstrate that the problem is\nNP-hard with constant number of workers when dag width can grow, and is also\nNP-hard with constant dag width when the number of workers can grow. When both\ndag width and the number of workers are unconstrained, then the problem is\ninapproximable within factor less than 5/4, unless P=NP.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2007 06:54:42 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Malewicz", "Grzegorz", ""]]}, {"id": "0705.1970", "submitter": "Nikolaos Laoutaris", "authors": "Nikolaos Laoutaris", "title": "A Closed-Form Method for LRU Replacement under Generalized Power-Law\n  Demand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": null, "abstract": "  We consider the well known \\emph{Least Recently Used} (LRU) replacement\nalgorithm and analyze it under the independent reference model and generalized\npower-law demand. For this extensive family of demand distributions we derive a\nclosed-form expression for the per object steady-state hit ratio. To the best\nof our knowledge, this is the first analytic derivation of the per object hit\nratio of LRU that can be obtained in constant time without requiring laborious\nnumeric computations or simulation. Since most applications of replacement\nalgorithms include (at least) some scenarios under i.i.d. requests, our method\nhas substantial practical value, especially when having to analyze multiple\ncaches, where existing numeric methods and simulation become too time\nconsuming.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2007 16:04:48 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Laoutaris", "Nikolaos", ""]]}, {"id": "0705.1986", "submitter": "Andrei Paun", "authors": "Andrei Paun", "title": "On the Hopcroft's minimization algorithm", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": null, "abstract": "  We show that the absolute worst case time complexity for Hopcroft's\nminimization algorithm applied to unary languages is reached only for de Bruijn\nwords. A previous paper by Berstel and Carton gave the example of de Bruijn\nwords as a language that requires O(n log n) steps by carefully choosing the\nsplitting sets and processing these sets in a FIFO mode. We refine the previous\nresult by showing that the Berstel/Carton example is actually the absolute\nworst case time complexity in the case of unary languages. We also show that a\nLIFO implementation will not achieve the same worst time complexity for the\ncase of unary languages. Lastly, we show that the same result is valid also for\nthe cover automata and a modification of the Hopcroft's algorithm, modification\nused in minimization of cover automata.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2007 17:15:53 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Paun", "Andrei", ""]]}, {"id": "0705.2125", "submitter": "Ching-Lueh Chang", "authors": "Ching-Lueh Chang, Yuh-Dauh Lyuu", "title": "Parallelized approximation algorithms for minimum routing cost spanning\n  trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": null, "abstract": "  We parallelize several previously proposed algorithms for the minimum routing\ncost spanning tree problem and some related problems.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2007 17:48:42 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2007 20:10:22 GMT"}], "update_date": "2007-07-04", "authors_parsed": [["Chang", "Ching-Lueh", ""], ["Lyuu", "Yuh-Dauh", ""]]}, {"id": "0705.2503", "submitter": "Peng Cui", "authors": "Peng Cui", "title": "Improved Approximability Result for Test Set with Small Redundancy", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": null, "abstract": "  Test set with redundancy is one of the focuses in recent bioinformatics\nresearch. Set cover greedy algorithm (SGA for short) is a commonly used\nalgorithm for test set with redundancy. This paper proves that the\napproximation ratio of SGA can be $(2-\\frac{1}{2r})\\ln n+{3/2}\\ln r+O(\\ln\\ln\nn)$ by using the potential function technique. This result is better than the\napproximation ratio $2\\ln n$ which directly derives from set multicover, when\n$r=o(\\frac{\\ln n}{\\ln\\ln n})$, and is an extension of the approximability\nresults for plain test set.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2007 09:53:20 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2007 09:11:18 GMT"}, {"version": "v3", "created": "Tue, 11 Sep 2007 09:21:21 GMT"}, {"version": "v4", "created": "Thu, 27 Sep 2007 14:58:21 GMT"}], "update_date": "2007-09-27", "authors_parsed": [["Cui", "Peng", ""]]}, {"id": "0705.2876", "submitter": "Phillip Bradford", "authors": "Phillip G. Bradford and Daniel A. Ray", "title": "An online algorithm for generating fractal hash chains applied to\n  digital chains of custody", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": null, "abstract": "  This paper gives an online algorithm for generating Jakobsson's fractal hash\nchains. Our new algorithm compliments Jakobsson's fractal hash chain algorithm\nfor preimage traversal since his algorithm assumes the entire hash chain is\nprecomputed and a particular list of Ceiling(log n) hash elements or pebbles\nare saved. Our online algorithm for hash chain traversal incrementally\ngenerates a hash chain of n hash elements without knowledge of n before it\nstarts. For any n, our algorithm stores only the Ceiling(log n) pebbles which\nare precisely the inputs for Jakobsson's amortized hash chain preimage\ntraversal algorithm. This compact representation is useful to generate,\ntraverse, and store a number of large digital hash chains on a small and\nconstrained device. We also give an application using both Jakobsson's and our\nnew algorithm applied to digital chains of custody for validating dynamically\nchanging forensics data.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2007 17:14:38 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Bradford", "Phillip G.", ""], ["Ray", "Daniel A.", ""]]}, {"id": "0705.4171", "submitter": "Eva Borbely", "authors": "Eva Borbely", "title": "Grover search algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": null, "abstract": "  A quantum algorithm is a set of instructions for a quantum computer, however,\nunlike algorithms in classical computer science their results cannot be\nguaranteed. A quantum system can undergo two types of operation, measurement\nand quantum state transformation, operations themselves must be unitary\n(reversible). Most quantum algorithms involve a series of quantum state\ntransformations followed by a measurement. Currently very few quantum\nalgorithms are known and no general design methodology exists for their\nconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2007 09:42:46 GMT"}], "update_date": "2007-05-30", "authors_parsed": [["Borbely", "Eva", ""]]}, {"id": "0705.4320", "submitter": "William Hung", "authors": "William N. N. Hung, Changjian Gao, Xiaoyu Song, Dan Hammerstrom", "title": "Defect-Tolerant CMOL Cell Assignment via Satisfiability", "comments": "To appear in Nanoelectronic Devices for Defense and Security\n  (NANO-DDS), Crystal City, Virginia, June 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": null, "abstract": "  We present a CAD framework for CMOL, a hybrid CMOS/ molecular circuit\narchitecture. Our framework first transforms any logically synthesized circuit\nbased on AND/OR/NOT gates to a NOR gate circuit, and then maps the NOR gates to\nCMOL. We encode the CMOL cell assignment problem as boolean conditions. The\nboolean constraint is satisfiable if and only if there is a way to map all the\nNOR gates to the CMOL cells. We further investigate various types of static\ndefects for the CMOL architecture, and propose a reconfiguration technique that\ncan deal with these defects through our CAD framework. This is the first\nautomated framework for CMOL cell assignment, and the first to model several\ndifferent CMOL static defects. Empirical results show that our approach is\nefficient and scalable.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2007 23:46:38 GMT"}], "update_date": "2007-05-31", "authors_parsed": [["Hung", "William N. N.", ""], ["Gao", "Changjian", ""], ["Song", "Xiaoyu", ""], ["Hammerstrom", "Dan", ""]]}, {"id": "0705.4606", "submitter": "Marco Pellegrini", "authors": "Filippo Geraci and Marco Pellegrini", "title": "Dynamic User-Defined Similarity Searching in Semi-Structured Text\n  Retrieval", "comments": "Submitted to Spire 2007", "journal-ref": null, "doi": null, "report-no": "IIT TR-07/2007", "categories": "cs.IR cs.DS", "license": null, "abstract": "  Modern text retrieval systems often provide a similarity search utility, that\nallows the user to find efficiently a fixed number k of documents in the data\nset that are most similar to a given query (here a query is either a simple\nsequence of keywords or the identifier of a full document found in previous\nsearches that is considered of interest). We consider the case of a textual\ndatabase made of semi-structured documents. Each field, in turns, is modelled\nwith a specific vector space. The problem is more complex when we also allow\neach such vector space to have an associated user-defined dynamic weight that\ninfluences its contribution to the overall dynamic aggregated and weighted\nsimilarity. This dynamic problem has been tackled in a recent paper by\nSingitham et al. in in VLDB 2004. Their proposed solution, which we take as\nbaseline, is a variant of the cluster-pruning technique that has the potential\nfor scaling to very large corpora of documents, and is far more efficient than\nthe naive exhaustive search. We devise an alternative way of embedding weights\nin the data structure, coupled with a non-trivial application of a clustering\nalgorithm based on the furthest point first heuristic for the metric k-center\nproblem. The validity of our approach is demonstrated experimentally by showing\nsignificant performance improvements over the scheme proposed in Singitham et\nal. in VLDB 2004. We improve significantly tradeoffs between query time and\noutput quality with respect to the baseline method in Singitham et al. in in\nVLDB 2004, and also with respect to a novel method by Chierichetti et al. to\nappear in ACM PODS 2007. We also speed up the pre-processing time by a factor\nat least thirty.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2007 13:46:39 GMT"}], "update_date": "2007-06-01", "authors_parsed": [["Geraci", "Filippo", ""], ["Pellegrini", "Marco", ""]]}, {"id": "0705.4618", "submitter": "Roberto Bagnara", "authors": "Roberto Bagnara, Patricia M. Hill, Enea Zaffanella", "title": "An Improved Tight Closure Algorithm for Integer Octagonal Constraints", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LO", "license": null, "abstract": "  Integer octagonal constraints (a.k.a. ``Unit Two Variables Per Inequality''\nor ``UTVPI integer constraints'') constitute an interesting class of\nconstraints for the representation and solution of integer problems in the\nfields of constraint programming and formal analysis and verification of\nsoftware and hardware systems, since they couple algorithms having polynomial\ncomplexity with a relatively good expressive power. The main algorithms\nrequired for the manipulation of such constraints are the satisfiability check\nand the computation of the inferential closure of a set of constraints. The\nlatter is called `tight' closure to mark the difference with the (incomplete)\nclosure algorithm that does not exploit the integrality of the variables. In\nthis paper we present and fully justify an O(n^3) algorithm to compute the\ntight closure of a set of UTVPI integer constraints.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2007 14:32:46 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2007 08:17:11 GMT"}], "update_date": "2007-06-01", "authors_parsed": [["Bagnara", "Roberto", ""], ["Hill", "Patricia M.", ""], ["Zaffanella", "Enea", ""]]}, {"id": "0705.4673", "submitter": "B\\'ela Csaba", "authors": "B\\'ela Csaba (Anal. and Stoch. Res. Group, HAS), Andr\\'as S. Pluh\\'ar\n  (Dept. of Comp. Sci., Univ. of Szeged)", "title": "A randomized algorithm for the on-line weighted bipartite matching\n  problem", "comments": "to be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": null, "abstract": "  We study the on-line minimum weighted bipartite matching problem in arbitrary\nmetric spaces. Here, $n$ not necessary disjoint points of a metric space $M$\nare given, and are to be matched on-line with $n$ points of $M$ revealed one by\none. The cost of a matching is the sum of the distances of the matched points,\nand the goal is to find or approximate its minimum. The competitive ratio of\nthe deterministic problem is known to be $\\Theta(n)$. It was conjectured that a\nrandomized algorithm may perform better against an oblivious adversary, namely\nwith an expected competitive ratio $\\Theta(\\log n)$. We prove a slightly weaker\nresult by showing a $o(\\log^3 n)$ upper bound on the expected competitive\nratio. As an application the same upper bound holds for the notoriously hard\nfire station problem, where $M$ is the real line.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2007 18:35:21 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2007 20:24:22 GMT"}], "update_date": "2007-06-06", "authors_parsed": [["Csaba", "B\u00e9la", "", "Anal. and Stoch. Res. Group, HAS"], ["Pluh\u00e1r", "Andr\u00e1s S.", "", "Dept. of Comp. Sci., Univ. of Szeged"]]}]