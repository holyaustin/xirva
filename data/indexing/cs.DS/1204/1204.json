[{"id": "1204.0062", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis and Alex Gittens", "title": "Improved matrix algorithms via the Subsampled Randomized Hadamard\n  Transform", "comments": "to appear in SIAM Journal on Matrix Analysis and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent randomized linear algebra algorithms rely upon fast dimension\nreduction methods. A popular choice is the Subsampled Randomized Hadamard\nTransform (SRHT). In this article, we address the efficacy, in the Frobenius\nand spectral norms, of an SRHT-based low-rank matrix approximation technique\nintroduced by Woolfe, Liberty, Rohklin, and Tygert. We establish a slightly\nbetter Frobenius norm error bound than currently available, and a much sharper\nspectral norm error bound (in the presence of reasonable decay of the singular\nvalues). Along the way, we produce several results on matrix operations with\nSRHTs (such as approximate matrix multiplication) that may be of independent\ninterest. Our approach builds upon Tropp's in \"Improved analysis of the\nSubsampled Randomized Hadamard Transform\".\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2012 02:49:46 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2012 22:07:54 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2013 05:34:10 GMT"}, {"version": "v4", "created": "Fri, 21 Jun 2013 21:11:59 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Boutsidis", "Christos", ""], ["Gittens", "Alex", ""]]}, {"id": "1204.0136", "submitter": "Satyen Kale", "authors": "Elad Hazan, Satyen Kale, Shai Shalev-Shwartz", "title": "Near-Optimal Algorithms for Online Matrix Prediction", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several online prediction problems of recent interest the comparison class\nis composed of matrices with bounded entries. For example, in the online\nmax-cut problem, the comparison class is matrices which represent cuts of a\ngiven graph and in online gambling the comparison class is matrices which\nrepresent permutations over n teams. Another important example is online\ncollaborative filtering in which a widely used comparison class is the set of\nmatrices with a small trace norm. In this paper we isolate a property of\nmatrices, which we call (beta,tau)-decomposability, and derive an efficient\nonline learning algorithm, that enjoys a regret bound of O*(sqrt(beta tau T))\nfor all problems in which the comparison class is composed of\n(beta,tau)-decomposable matrices. By analyzing the decomposability of cut\nmatrices, triangular matrices, and low trace-norm matrices, we derive near\noptimal regret bounds for online max-cut, online gambling, and online\ncollaborative filtering. In particular, this resolves (in the affirmative) an\nopen problem posed by Abernethy (2010); Kleinberg et al (2010). Finally, we\nderive lower bounds for the three problems and show that our upper bounds are\noptimal up to logarithmic factors. In particular, our lower bound for the\nonline collaborative filtering problem resolves another open problem posed by\nShamir and Srebro (2011).\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2012 21:15:28 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Hazan", "Elad", ""], ["Kale", "Satyen", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1204.0219", "submitter": "Moti Medina", "authors": "Iftah Gamzu, Moti Medina", "title": "Improved Approximation for Orienting Mixed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instance of the maximum mixed graph orientation problem consists of a\nmixed graph and a collection of source-target vertex pairs. The objective is to\norient the undirected edges of the graph so as to maximize the number of pairs\nthat admit a directed source-target path. This problem has recently arisen in\nthe study of biological networks, and it also has applications in communication\nnetworks.\n  In this paper, we identify an interesting local-to-global orientation\nproperty. This property enables us to modify the best known algorithms for\nmaximum mixed graph orientation and some of its special structured instances,\ndue to Elberfeld et al. (CPM '11), and obtain improved approximation ratios. We\nfurther proceed by developing an algorithm that achieves an even better\napproximation guarantee for the general setting of the problem. Finally, we\nstudy several well-motivated variants of this orientation problem.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 15:14:26 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Gamzu", "Iftah", ""], ["Medina", "Moti", ""]]}, {"id": "1204.0220", "submitter": "Youssef Bassil", "authors": "Youssef Bassil, Aziz Barbar", "title": "Sequential & Parallel Algorithms for Big-Integer Numbers Subtraction", "comments": "Global Journal of Computer Science and Technology, Vol. 9, Issue 5,\n  January 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many emerging computer applications require the processing of large numbers,\nlarger than what a CPU can handle. In fact, the top of the line PCs can only\nmanipulate numbers not longer than 32 bits or 64 bits. This is due to the size\nof the registers and the data-path inside the CPU. As a result, performing\narithmetic operations such as subtraction on big-integer numbers is to some\nextend limited. Different algorithms were designed in an attempt to solve this\nproblem; they all operate on big-integer numbers by first converting them into\na binary representation then performing bitwise operations on single bits. Such\nalgorithms are of complexity O(n) where n is the total number of bits in each\noperand. This paper proposes two new algorithms for performing arithmetic\nsubtraction on big-integer numbers. The two algorithms are different in that\none is sequential while the other is parallel. The similarity between them is\nthat both follow the same concept of dividing the big-integer inputs into\nseveral blocks or tokens of 60 bits (18 digits) each; thus reducing the input\nsize n in O(n) by a factor of 60. Subtraction of corresponding tokens, one from\neach operand, is performed as humans perform subtraction, using a pencil and a\npaper in the decimal system. Both algorithms are to be implemented using MS\nC#.NET 2005 and tested over a multiple processor system. Further studies can be\ndone on other arithmetic operations such as addition and multiplication.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 15:31:31 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bassil", "Youssef", ""], ["Barbar", "Aziz", ""]]}, {"id": "1204.0232", "submitter": "Youssef Bassil", "authors": "Youssef Bassil, Aziz Barbar", "title": "Sequential and Parallel Algorithms for the Addition of Big-Integer\n  Numbers", "comments": null, "journal-ref": "International Journal of Computational Science, vol. 4, no. 1, pp.\n  52-69, 2010", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's PCs can directly manipulate numbers not longer than 64 bits because\nthe size of the CPU registers and the data-path are limited. Consequently,\narithmetic operations such as addition, can only be performed on numbers of\nthat length. To solve the problem of computation on big-integer numbers,\ndifferent algorithms were developed. However, these algorithms are considerably\nslow because they operate on individual bits; and are only designed to run over\nsingle-processor computers. In this paper, two algorithms for handling\narithmetic addition on big-integer numbers are presented. The first algorithm\nis sequential while the second is parallel. Both algorithms, unlike existing\nones, perform addition on blocks or tokens of 60 bits (18 digits), and thus\nboosting the execution time by a factor of 60.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 16:12:07 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bassil", "Youssef", ""], ["Barbar", "Aziz", ""]]}, {"id": "1204.0816", "submitter": "Shiva Kintali", "authors": "Shiva Kintali and Asaf Shapira", "title": "A Note on the Balanced ST-Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that every YES instance of Balanced ST-Connectivity has a balanced\npath of polynomial length.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 21:39:39 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Kintali", "Shiva", ""], ["Shapira", "Asaf", ""]]}, {"id": "1204.0824", "submitter": "C. Seshadhri", "authors": "Kenneth L. Clarkson, Wolfgang Mulzer, C. Seshadhri", "title": "Self-improving Algorithms for Coordinate-wise Maxima", "comments": "To appear in Symposium of Computational Geometry 2012 (17 pages, 2\n  figures)", "journal-ref": "SIAM Journal on Computing (SICOMP), 43(2), 2014, pp. 617-653", "doi": "10.1137/12089702X", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the coordinate-wise maxima of a planar point set is a classic and\nwell-studied problem in computational geometry. We give an algorithm for this\nproblem in the \\emph{self-improving setting}. We have $n$ (unknown) independent\ndistributions $\\cD_1, \\cD_2, ..., \\cD_n$ of planar points. An input pointset\n$(p_1, p_2, ..., p_n)$ is generated by taking an independent sample $p_i$ from\neach $\\cD_i$, so the input distribution $\\cD$ is the product $\\prod_i \\cD_i$. A\nself-improving algorithm repeatedly gets input sets from the distribution $\\cD$\n(which is \\emph{a priori} unknown) and tries to optimize its running time for\n$\\cD$. Our algorithm uses the first few inputs to learn salient features of the\ndistribution, and then becomes an optimal algorithm for distribution $\\cD$. Let\n$\\OPT_\\cD$ denote the expected depth of an \\emph{optimal} linear comparison\ntree computing the maxima for distribution $\\cD$. Our algorithm eventually has\nan expected running time of $O(\\text{OPT}_\\cD + n)$, even though it did not\nknow $\\cD$ to begin with.\n  Our result requires new tools to understand linear comparison trees for\ncomputing maxima. We show how to convert general linear comparison trees to\nvery restricted versions, which can then be related to the running time of our\nalgorithm. An interesting feature of our algorithm is an interleaved search,\nwhere the algorithm tries to determine the likeliest point to be maximal with\nminimal computation. This allows the running time to be truly optimal for the\ndistribution $\\cD$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 22:42:57 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Clarkson", "Kenneth L.", ""], ["Mulzer", "Wolfgang", ""], ["Seshadhri", "C.", ""]]}, {"id": "1204.0849", "submitter": "C. Seshadhri", "authors": "Deeparnab Chakrabarty and C. Seshadhri", "title": "Optimal bounds for monotonicity and Lipschitz testing over hypercubes\n  and hypergrids", "comments": "Cleaner proof and much better presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of monotonicity testing over the hypergrid and its special case,\nthe hypercube, is a classic, well-studied, yet unsolved question in property\ntesting. We are given query access to $f:[k]^n \\mapsto \\R$ (for some ordered\nrange $\\R$). The hypergrid/cube has a natural partial order given by\ncoordinate-wise ordering, denoted by $\\prec$. A function is \\emph{monotone} if\nfor all pairs $x \\prec y$, $f(x) \\leq f(y)$. The distance to monotonicity,\n$\\eps_f$, is the minimum fraction of values of $f$ that need to be changed to\nmake $f$ monotone.\n  For $k=2$ (the boolean hypercube), the usual tester is the \\emph{edge\ntester}, which checks monotonicity on adjacent pairs of domain points. It is\nknown that the edge tester using $O(\\eps^{-1}n\\log|\\R|)$ samples can\ndistinguish a monotone function from one where $\\eps_f > \\eps$. On the other\nhand, the best lower bound for monotonicity testing over the hypercube is\n$\\min(|\\R|^2,n)$. This leaves a quadratic gap in our knowledge, since $|\\R|$\ncan be $2^n$. We resolve this long standing open problem and prove that\n$O(n/\\eps)$ samples suffice for the edge tester. For hypergrids, known testers\nrequire $O(\\eps^{-1}n\\log k\\log |\\R|)$ samples, while the best known\n(non-adaptive) lower bound is $\\Omega(\\eps^{-1} n\\log k)$. We give a\n(non-adaptive) monotonicity tester for hypergrids running in $O(\\eps^{-1} n\\log\nk)$ time.\n  Our techniques lead to optimal property testers (with the same running time)\nfor the natural \\emph{Lipschitz property} on hypercubes and hypergrids. (A\n$c$-Lipschitz function is one where $|f(x) - f(y)| \\leq c\\|x-y\\|_1$.) In fact,\nwe give a general unified proof for $O(\\eps^{-1}n\\log k)$-query testers for a\nclass of \"bounded-derivative\" properties, a class containing both monotonicity\nand Lipschitz.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 02:15:55 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2012 21:55:18 GMT"}, {"version": "v3", "created": "Wed, 2 Apr 2014 21:57:38 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Seshadhri", "C.", ""]]}, {"id": "1204.0897", "submitter": "Olaf Maurer", "authors": "Elisabeth G\\\"unther, Olaf Maurer, Nicole Megow, Andreas Wiese", "title": "A New Approach to Online Scheduling: Approximating the Optimal\n  Competitive Ratio", "comments": "24 pages; short version to appear in SODA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to competitive analysis in online scheduling by\nintroducing the novel concept of competitive-ratio approximation schemes. Such\na scheme algorithmically constructs an online algorithm with a competitive\nratio arbitrarily close to the best possible competitive ratio for any online\nalgorithm. We study the problem of scheduling jobs online to minimize the\nweighted sum of completion times on parallel, related, and unrelated machines,\nand we derive both deterministic and randomized algorithms which are almost\nbest possible among all online algorithms of the respective settings. We also\ngeneralize our techniques to arbitrary monomial cost functions and apply them\nto the makespan objective. Our method relies on an abstract characterization of\nonline algorithms combined with various simplifications and transformations. We\nalso contribute algorithmic means to compute the actual value of the best\npossi- ble competitive ratio up to an arbitrary accuracy. This strongly\ncontrasts all previous manually obtained competitiveness results for algorithms\nand, most importantly, it reduces the search for the optimal com- petitive\nratio to a question that a computer can answer. We believe that our concept can\nalso be applied to many other problems and yields a new perspective on online\nalgorithms in general.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 09:17:22 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2012 14:43:08 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["G\u00fcnther", "Elisabeth", ""], ["Maurer", "Olaf", ""], ["Megow", "Nicole", ""], ["Wiese", "Andreas", ""]]}, {"id": "1204.0944", "submitter": "Tom Gur", "authors": "Tom Gur and Omer Tamuz", "title": "Testing Booleanity and the Uncertainty Principle", "comments": "15 pages", "journal-ref": "Chicago Journal of Theoretical Computer Science 2013, Article 14", "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let f:{-1,1}^n -> R be a real function on the hypercube, given by its\ndiscrete Fourier expansion, or, equivalently, represented as a multilinear\npolynomial. We say that it is Boolean if its image is in {-1,1}.\n  We show that every function on the hypercube with a sparse Fourier expansion\nmust either be Boolean or far from Boolean. In particular, we show that a\nmultilinear polynomial with at most k terms must either be Boolean, or output\nvalues different than -1 or 1 for a fraction of at least 2/(k+2)^2 of its\ndomain.\n  It follows that given oracle access to f, together with the guarantee that\nits representation as a multilinear polynomial has at most k terms, one can\ntest Booleanity using O(k^2) queries. We show an \\Omega(k) queries lower bound\nfor this problem.\n  Our proof crucially uses Hirschman's entropic version of Heisenberg's\nuncertainty principle.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 13:09:30 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 05:45:40 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Gur", "Tom", ""], ["Tamuz", "Omer", ""]]}, {"id": "1204.0982", "submitter": "Mikael Gast", "authors": "Mikael Gast and Mathias Hauptmann", "title": "Approximability of the Vertex Cover Problem in Power Law Graphs", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we construct an approximation algorithm for the Minimum Vertex\nCover Problem (Min-VC) with an expected approximation ratio of 2-f(beta) for\nrandom Power Law Graphs (PLG) in the (alpha,beta)-model of Aiello et. al.,\nwhere f(beta) is a strictly positive function of the parameter beta. We obtain\nthis result by combining the Nemhauser and Trotter approach for Min-VC with a\nnew deterministic rounding procedure which achieves an approximation ratio of\n3/2 on a subset of low degree vertices for which the expected contribution to\nthe cost of the associated linear program is sufficiently large.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 16:16:37 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2012 08:04:48 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Gast", "Mikael", ""], ["Hauptmann", "Mathias", ""]]}, {"id": "1204.1002", "submitter": "Erwan Le Martelot", "authors": "Erwan Le Martelot and Chris Hankin", "title": "Fast Multi-Scale Detection of Relevant Communities", "comments": "19 pages, 3 figures, 1 table, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, networks are almost ubiquitous. In the past decade, community\ndetection received an increasing interest as a way to uncover the structure of\nnetworks by grouping nodes into communities more densely connected internally\nthan externally. Yet most of the effective methods available do not consider\nthe potential levels of organisation, or scales, a network may encompass and\nare therefore limited. In this paper we present a method compatible with global\nand local criteria that enables fast multi-scale community detection. The\nmethod is derived in two algorithms, one for each type of criterion, and\nimplemented with 6 known criteria. Uncovering communities at various scales is\na computationally expensive task. Therefore this work puts a strong emphasis on\nthe reduction of computational complexity. Some heuristics are introduced for\nspeed-up purposes. Experiments demonstrate the efficiency and accuracy of our\nmethod with respect to each algorithm and criterion by testing them against\nlarge generated multi-scale networks. This study also offers a comparison\nbetween criteria and between the global and local approaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 17:06:13 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2012 11:18:29 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2012 16:35:23 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Martelot", "Erwan Le", ""], ["Hankin", "Chris", ""]]}, {"id": "1204.1025", "submitter": "Jan Vondrak", "authors": "Michael Kapralov, Ian Post, Jan Vondrak", "title": "Online submodular welfare maximization: Greedy is optimal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that no online algorithm (even randomized, against an oblivious\nadversary) is better than 1/2-competitive for welfare maximization with\ncoverage valuations, unless $NP = RP$. Since the Greedy algorithm is known to\nbe 1/2-competitive for monotone submodular valuations, of which coverage is a\nspecial case, this proves that Greedy provides the optimal competitive ratio.\nOn the other hand, we prove that Greedy in a stochastic setting with\ni.i.d.items and valuations satisfying diminishing returns is\n$(1-1/e)$-competitive, which is optimal even for coverage valuations, unless\n$NP=RP$. For online budget-additive allocation, we prove that no algorithm can\nbe 0.612-competitive with respect to a natural LP which has been used\npreviously for this problem.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 18:43:31 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2013 00:08:18 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Kapralov", "Michael", ""], ["Post", "Ian", ""], ["Vondrak", "Jan", ""]]}, {"id": "1204.1082", "submitter": "Ben Baumer", "authors": "Amotz Bar-Noy and Ben Baumer and Dror Rawitz", "title": "Set It and Forget It: Approximating the Set Once Strip Cover Problem", "comments": "briefly announced at SPAA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Set Once Strip Cover problem, in which n wireless sensors are\ndeployed over a one-dimensional region. Each sensor has a fixed battery that\ndrains in inverse proportion to a radius that can be set just once, but\nactivated at any time. The problem is to find an assignment of radii and\nactivation times that maximizes the length of time during which the entire\nregion is covered. We show that this problem is NP-hard. Second, we show that\nRoundRobin, the algorithm in which the sensors simply take turns covering the\nentire region, has a tight approximation guarantee of 3/2 in both Set Once\nStrip Cover and the more general Strip Cover problem, in which each radius may\nbe set finitely-many times. Moreover, we show that the more general class of\nduty cycle algorithms, in which groups of sensors take turns covering the\nentire region, can do no better. Finally, we give an optimal O(n^2 log n)-time\nalgorithm for the related Set Radius Strip Cover problem, in which all sensors\nmust be activated immediately.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 21:31:00 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2013 21:22:01 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Bar-Noy", "Amotz", ""], ["Baumer", "Ben", ""], ["Rawitz", "Dror", ""]]}, {"id": "1204.1098", "submitter": "C. Seshadhri", "authors": "Michael Saks and C. Seshadhri", "title": "Space efficient streaming algorithms for the distance to monotonicity\n  and asymmetric edit distance", "comments": "Final SODA 2013 version. Fixed bugs. We get a \\delta n-additive\n  approximation for edit distance, not multiplicative as said in the earlier\n  tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating the length of the longest increasing sequence (LIS) of an array\nis a well-studied problem. We study this problem in the data stream model,\nwhere the algorithm is allowed to make a single left-to-right pass through the\narray and the key resource to be minimized is the amount of additional memory\nused. We present an algorithm which, for any $\\delta > 0$, given streaming\naccess to an array of length $n$ provides a $(1+\\delta)$-multiplicative\napproximation to the \\emph{distance to monotonicity} ($n$ minus the length of\nthe LIS), and uses only $O((\\log^2 n)/\\delta)$ space. The previous best known\napproximation using polylogarithmic space was a multiplicative 2-factor. Our\nalgorithm can be used to estimate the length of the LIS to within an additive\n$\\delta n$ for any $\\delta >0$ while previous algorithms could only achieve\nadditive error $n(1/2-o(1))$.\n  Our algorithm is very simple, being just 3 lines of pseudocode, and has a\nsmall update time. It is essentially a polylogarithmic space approximate\nimplementation of a classic dynamic program that computes the LIS.\n  We also give a streaming algorithm for approximating $LCS(x,y)$, the length\nof the longest common subsequence between strings $x$ and $y$, each of length\n$n$. Our algorithm works in the asymmetric setting (inspired by \\cite{AKO10}),\nin which we have random access to $y$ and streaming access to $x$, and runs in\nsmall space provided that no single symbol appears very often in $y$. More\nprecisely, it gives an additive-$\\delta n$ approximation to $LCS(x,y)$ (and\nhence also to $E(x,y) = n-LCS(x,y)$, the edit distance between $x$ and $y$ when\ninsertions and deletions, but not substitutions, are allowed), with space\ncomplexity $O(k(\\log^2 n)/\\delta)$, where $k$ is the maximum number of times\nany one symbol appears in $y$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 00:41:08 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2013 21:15:54 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Saks", "Michael", ""], ["Seshadhri", "C.", ""]]}, {"id": "1204.1111", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall", "title": "Faster Algorithms for Rectangular Matrix Multiplication", "comments": "37 pages; v2: some additions in the acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let {\\alpha} be the maximal value such that the product of an n x n^{\\alpha}\nmatrix by an n^{\\alpha} x n matrix can be computed with n^{2+o(1)} arithmetic\noperations. In this paper we show that \\alpha>0.30298, which improves the\nprevious record \\alpha>0.29462 by Coppersmith (Journal of Complexity, 1997).\nMore generally, we construct a new algorithm for multiplying an n x n^k matrix\nby an n^k x n matrix, for any value k\\neq 1. The complexity of this algorithm\nis better than all known algorithms for rectangular matrix multiplication. In\nthe case of square matrix multiplication (i.e., for k=1), we recover exactly\nthe complexity of the algorithm by Coppersmith and Winograd (Journal of\nSymbolic Computation, 1990).\n  These new upper bounds can be used to improve the time complexity of several\nknown algorithms that rely on rectangular matrix multiplication. For example,\nwe directly obtain a O(n^{2.5302})-time algorithm for the all-pairs shortest\npaths problem over directed graphs with small integer weights, improving over\nthe O(n^{2.575})-time algorithm by Zwick (JACM 2002), and also improve the time\ncomplexity of sparse square matrix multiplication.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 02:14:37 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2012 06:47:16 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1204.1136", "submitter": "Adrian Kosowski", "authors": "Adrian Kosowski (INRIA Bordeaux - Sud-Ouest)", "title": "Faster Walks in Graphs: A $\\tilde O(n^2)$ Time-Space Trade-off for\n  Undirected s-t Connectivity", "comments": "Version 3 makes use of the Metropolis-Hastings walk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we make use of the Metropolis-type walks due to Nonaka et al.\n(2010) to provide a faster solution to the $S$-$T$-connectivity problem in\nundirected graphs (USTCON). As our main result, we propose a family of\nrandomized algorithms for USTCON which achieves a time-space product of $S\\cdot\nT = \\tilde O(n^2)$ in graphs with $n$ nodes and $m$ edges (where the $\\tilde\nO$-notation disregards poly-logarithmic terms). This improves the previously\nbest trade-off of $\\tilde O(n m)$, due to Feige (1995). Our algorithm consists\nin deploying several short Metropolis-type walks, starting from landmark nodes\ndistributed using the scheme of Broder et al. (1994) on a modified input graph.\nIn particular, we obtain an algorithm running in time $\\tilde O(n+m)$ which is,\nin general, more space-efficient than both BFS and DFS. We close the paper by\nshowing how to fine-tune the Metropolis-type walk so as to match the\nperformance parameters (e.g., average hitting time) of the unbiased random walk\nfor any graph, while preserving a worst-case bound of $\\tilde O(n^2)$ on cover\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 07:47:57 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2012 20:23:39 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2012 06:51:10 GMT"}], "update_date": "2012-07-11", "authors_parsed": [["Kosowski", "Adrian", "", "INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1204.1215", "submitter": "Travis Gagie", "authors": "Travis Gagie", "title": "On the Value of Multiple Read/Write Streams for Data Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study whether, when restricted to using polylogarithmic memory and\npolylogarithmic passes, we can achieve qualitatively better data compression\nwith multiple read/write streams than we can with only one. We first show how\nwe can achieve universal compression using only one pass over one stream. We\nthen show that one stream is not sufficient for us to achieve good\ngrammar-based compression. Finally, we show that two streams are necessary and\nsufficient for us to achieve entropy-only bounds.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 13:07:38 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Gagie", "Travis", ""]]}, {"id": "1204.1369", "submitter": "Taso Viglas", "authors": "Martin Olsen, Anastasios Viglas, Ilia Zvedeniouk", "title": "An approximation algorithm for the link building problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of maximizing the PageRank of a given\ntarget node in a graph by adding $k$ new links. We consider the case that the\nnew links must point to the given target node (backlinks). Previous work shows\nthat this problem has no fully polynomial time approximation schemes unless\n$P=NP$. We present a polynomial time algorithm yielding a PageRank value within\na constant factor from the optimal. We also consider the naive algorithm where\nwe choose backlinks from nodes with high PageRank values compared to the\noutdegree and show that the naive algorithm performs much worse on certain\ngraphs compared to the constant factor approximation scheme.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 22:28:02 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Olsen", "Martin", ""], ["Viglas", "Anastasios", ""], ["Zvedeniouk", "Ilia", ""]]}, {"id": "1204.1373", "submitter": "Paulo Jesus", "authors": "Miguel Borges, Paulo Jesus, Carlos Baquero, Paulo S\\'ergio Almeida", "title": "Spectra: Robust Estimation of Distribution Functions in Networks", "comments": "Full version of the paper published at 12th IFIP International\n  Conference on Distributed Applications and Interoperable Systems (DAIS),\n  Stockholm (Sweden), June 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed aggregation allows the derivation of a given global aggregate\nproperty from many individual local values in nodes of an interconnected\nnetwork system. Simple aggregates such as minima/maxima, counts, sums and\naverages have been thoroughly studied in the past and are important tools for\ndistributed algorithms and network coordination. Nonetheless, this kind of\naggregates may not be comprehensive enough to characterize biased data\ndistributions or when in presence of outliers, making the case for richer\nestimates of the values on the network. This work presents Spectra, a\ndistributed algorithm for the estimation of distribution functions over large\nscale networks. The estimate is available at all nodes and the technique\ndepicts important properties, namely: robust when exposed to high levels of\nmessage loss, fast convergence speed and fine precision in the estimate. It can\nalso dynamically cope with changes of the sampled local property, not requiring\nalgorithm restarts, and is highly resilient to node churn. The proposed\napproach is experimentally evaluated and contrasted to a competing state of the\nart distribution aggregation technique.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 22:53:35 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Borges", "Miguel", ""], ["Jesus", "Paulo", ""], ["Baquero", "Carlos", ""], ["Almeida", "Paulo S\u00e9rgio", ""]]}, {"id": "1204.1417", "submitter": "Christophe Paul", "authors": "Eun Jung Kim and Christophe Paul and Geevarghese Philip", "title": "A single-exponential FPT algorithm for the $K_4$-minor cover problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an input graph G and an integer k, the parameterized K_4-minor cover\nproblem asks whether there is a set S of at most k vertices whose deletion\nresults in a K_4-minor-free graph, or equivalently in a graph of treewidth at\nmost 2. This problem is inspired by two well-studied parameterized vertex\ndeletion problems, Vertex Cover and Feedback Vertex Set, which can also be\nexpressed as Treewidth-t Vertex Deletion problems: t=0 for Vertex Cover and t=1\nfor Feedback Vertex Set. While a single-exponential FPT algorithm has been\nknown for a long time for \\textsc{Vertex Cover}, such an algorithm for Feedback\nVertex Set was devised comparatively recently. While it is known to be unlikely\nthat Treewidth-t Vertex Deletion can be solved in time c^{o(k)}.n^{O(1)}, it\nwas open whether the K_4-minor cover problem could be solved in\nsingle-exponential FPT time, i.e. in c^k.n^{O(1)} time. This paper answers this\nquestion in the affirmative.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 06:37:59 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Kim", "Eun Jung", ""], ["Paul", "Christophe", ""], ["Philip", "Geevarghese", ""]]}, {"id": "1204.1467", "submitter": "Ali Soltan Mohammadi", "authors": "Ali Soltan Mohammadi and L. Asadzadeh and D. D. Rezaee", "title": "Learning Fuzzy {\\beta}-Certain and {\\beta}-Possible rules from\n  incomplete quantitative data by rough sets", "comments": "hi thanks for attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rough-set theory proposed by Pawlak, has been widely used in dealing with\ndata classification problems. The original rough-set model is, however, quite\nsensitive to noisy data. Tzung thus proposed deals with the problem of\nproducing a set of fuzzy certain and fuzzy possible rules from quantitative\ndata with a predefined tolerance degree of uncertainty and misclassification.\nThis model allowed, which combines the variable precision rough-set model and\nthe fuzzy set theory, is thus proposed to solve this problem. This paper thus\ndeals with the problem of producing a set of fuzzy certain and fuzzy possible\nrules from incomplete quantitative data with a predefined tolerance degree of\nuncertainty and misclassification. A new method, incomplete quantitative data\nfor rough-set model and the fuzzy set theory, is thus proposed to solve this\nproblem. It first transforms each quantitative value into a fuzzy set of\nlinguistic terms using membership functions and then finding incomplete\nquantitative data with lower and the fuzzy upper approximations. It second\ncalculates the fuzzy {\\beta}-lower and the fuzzy {\\beta}-upper approximations.\nThe certain and possible rules are then generated based on these fuzzy\napproximations. These rules can then be used to classify unknown objects.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 12:42:24 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Mohammadi", "Ali Soltan", ""], ["Asadzadeh", "L.", ""], ["Rezaee", "D. D.", ""]]}, {"id": "1204.1616", "submitter": "Marek Cygan", "authors": "Marek Cygan and Harold N. Gabow and Piotr Sankowski", "title": "Algorithmic Applications of Baur-Strassen's Theorem: Shortest Cycles,\n  Diameter and Matchings", "comments": "To appear in FOCS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a directed or an undirected graph with integral edge weights from\nthe set [-W, W], that does not contain negative weight cycles. In this paper,\nwe introduce a general framework for solving problems on such graphs using\nmatrix multiplication. The framework is based on the usage of Baur-Strassen's\ntheorem and of Strojohann's determinant algorithm. It allows us to give new and\nsimple solutions to the following problems:\n  * Finding Shortest Cycles -- We give a simple \\tilde{O}(Wn^{\\omega}) time\nalgorithm for finding shortest cycles in undirected and directed graphs. For\ndirected graphs (and undirected graphs with non-negative weights) this matches\nthe time bounds obtained in 2011 by Roditty and Vassilevska-Williams. On the\nother hand, no algorithm working in \\tilde{O}(Wn^{\\omega}) time was previously\nknown for undirected graphs with negative weights. Furthermore our algorithm\nfor a given directed or undirected graph detects whether it contains a negative\nweight cycle within the same running time.\n  * Computing Diameter and Radius -- We give a simple \\tilde{O}(Wn^{\\omega})\ntime algorithm for computing a diameter and radius of an undirected or directed\ngraphs. To the best of our knowledge no algorithm with this running time was\nknown for undirected graphs with negative weights.\n  * Finding Minimum Weight Perfect Matchings -- We present an\n\\tilde{O}(Wn^{\\omega}) time algorithm for finding minimum weight perfect\nmatchings in undirected graphs. This resolves an open problem posted by\nSankowski in 2006, who presented such an algorithm but only in the case of\nbipartite graphs.\n  In order to solve minimum weight perfect matching problem we develop a novel\ncombinatorial interpretation of the dual solution which sheds new light on this\nproblem. Such a combinatorial interpretation was not know previously, and is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 09:54:25 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2012 19:05:56 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Cygan", "Marek", ""], ["Gabow", "Harold N.", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1204.1939", "submitter": "Colin Cooper", "authors": "Petra Berenbrink, Colin Cooper, Tom Friedetzky", "title": "Random walks which prefer unvisited edges. Exploring high girth even\n  degree expanders in linear time", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a modified random walk which uses unvisited edges whenever\npossible, and makes a simple random walk otherwise. We call such a walk an\nedge-process. We assume there is a rule A, which tells the walk which unvisited\nedge to use whenever there is a choice. In the simplest case, A is a uniform\nrandom choice over unvisited edges incident with the current walk position.\nHowever we do not exclude arbitrary choices of rule A. For example, the rule\ncould be determined on-line by an adversary, or could vary from vertex to\nvertex.\n  For even degree expander graphs, of bounded maximum degree, we have the\nfollowing result. Let G be an n vertex even degree expander graph, for which\nevery vertex is in at least one vertex induced cycle of length L. Any\nedge-process on G has cover time (n+ (n log n)/L). This result is independent\nof the rule A used to select the order of the unvisited edges, which can be\nchosen on-line by an adversary.\n  As an example, With high probability, random r-regular graphs, (r at least 4,\neven), are expanders for which L = Omega(log n). Thus, for almost all such\ngraphs, the vertex cover time of the edge-process is Theta(n). This improves\nthe vertex cover time of such graphs by a factor of log n, compared to the\nOmega(n log n) cover time of any weighted random walk.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 17:52:17 GMT"}, {"version": "v2", "created": "Sat, 5 May 2012 08:36:13 GMT"}, {"version": "v3", "created": "Sun, 27 May 2012 11:31:11 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Berenbrink", "Petra", ""], ["Cooper", "Colin", ""], ["Friedetzky", "Tom", ""]]}, {"id": "1204.1956", "submitter": "Rong Ge", "authors": "Sanjeev Arora, Rong Ge, Ankur Moitra", "title": "Learning Topic Models - Going beyond SVD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic Modeling is an approach used for automatic comprehension and\nclassification of data in a variety of settings, and perhaps the canonical\napplication is in uncovering thematic structure in a corpus of documents. A\nnumber of foundational works both in machine learning and in theory have\nsuggested a probabilistic model for documents, whereby documents arise as a\nconvex combination of (i.e. distribution on) a small number of topic vectors,\neach topic vector being a distribution on words (i.e. a vector of\nword-frequencies). Similar models have since been used in a variety of\napplication areas; the Latent Dirichlet Allocation or LDA model of Blei et al.\nis especially popular.\n  Theoretical studies of topic modeling focus on learning the model's\nparameters assuming the data is actually generated from it. Existing approaches\nfor the most part rely on Singular Value Decomposition(SVD), and consequently\nhave one of two limitations: these works need to either assume that each\ndocument contains only one topic, or else can only recover the span of the\ntopic vectors instead of the topic vectors themselves.\n  This paper formally justifies Nonnegative Matrix Factorization(NMF) as a main\ntool in this context, which is an analog of SVD where all vectors are\nnonnegative. Using this tool we give the first polynomial-time algorithm for\nlearning topic models without the above two limitations. The algorithm uses a\nfairly mild assumption about the underlying topic matrix called separability,\nwhich is usually found to hold in real-life data. A compelling feature of our\nalgorithm is that it generalizes to models that incorporate topic-topic\ncorrelations, such as the Correlated Topic Model and the Pachinko Allocation\nModel.\n  We hope that this paper will motivate further theoretical results that use\nNMF as a replacement for SVD - just as NMF has come to replace SVD in many\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 19:33:47 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2012 01:08:52 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Moitra", "Ankur", ""]]}, {"id": "1204.1957", "submitter": "Patrick Nicholson", "authors": "J. Ian Munro, Patrick K. Nicholson", "title": "Succinct Posets", "comments": "12 pages lncs format + short appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an algorithm for compressing a partially ordered set, or\n\\emph{poset}, so that it occupies space matching the information theory lower\nbound (to within lower order terms), in the worst case. Using this algorithm,\nwe design a succinct data structure for representing a poset that, given two\nelements, can report whether one precedes the other in constant time. This is\nequivalent to succinctly representing the transitive closure graph of the\nposet, and we note that the same method can also be used to succinctly\nrepresent the transitive reduction graph. For an $n$ element poset, the data\nstructure occupies $n^2/4 + o(n^2)$ bits, in the worst case, which is roughly\nhalf the space occupied by an upper triangular matrix. Furthermore, a slight\nextension to this data structure yields a succinct oracle for reachability in\narbitrary directed graphs. Thus, using roughly a quarter of the space required\nto represent an arbitrary directed graph, reachability queries can be supported\nin constant time.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 19:38:58 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2012 19:43:32 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Munro", "J. Ian", ""], ["Nicholson", "Patrick K.", ""]]}, {"id": "1204.1958", "submitter": "Frank Ruskey", "authors": "Qingxuan Yang, John Ellis, Khalegh Mamakani, Frank Ruskey", "title": "Parallel and sequential in-place permuting and perfect shuffling using\n  involutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that any permutation of ${1,2,...,N}$ can be written as the product\nof two involutions. As a consequence, any permutation of the elements of an\narray can be performed in-place in parallel in time O(1). In the case where the\npermutation is the $k$-way perfect shuffle we develop two methods for\nefficiently computing such a pair of involutions.\n  The first method works whenever $N$ is a power of $k$; in this case the time\nis O(N) and space $O(\\log^2 N)$. The second method applies to the general case\nwhere $N$ is a multiple of $k$; here the time is $O(N \\log N)$ and the space is\n$O(\\log^2 N)$. If $k=2$ the space usage of the first method can be reduced to\n$O(\\log N)$ on a machine that has a SADD (population count) instruction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 19:39:27 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2013 06:52:53 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yang", "Qingxuan", ""], ["Ellis", "John", ""], ["Mamakani", "Khalegh", ""], ["Ruskey", "Frank", ""]]}, {"id": "1204.2021", "submitter": "Shayan Oveis Gharan", "authors": "Shayan Oveis Gharan and Luca Trevisan", "title": "Approximating the Expansion Profile and Almost Optimal Local Graph\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral partitioning is a simple, nearly-linear time, algorithm to find\nsparse cuts, and the Cheeger inequalities provide a worst-case guarantee for\nthe quality of the approximation found by the algorithm. Local graph\npartitioning algorithms [ST08,ACL06,AP09] run in time that is nearly linear in\nthe size of the output set, and their approximation guarantee is worse than the\nguarantee provided by the Cheeger inequalities by a polylogarithmic\n$\\log^{\\Omega(1)} n$ factor. It has been a long standing open problem to design\na local graph clustering algorithm with an approximation guarantee close to the\nguarantee of the Cheeger inequalities and with a running time nearly linear in\nthe size of the output.\n  In this paper we solve this problem; we design an algorithm with the same\nguarantee (up to a constant factor) as the Cheeger inequality, that runs in\ntime slightly super linear in the size of the output. This is the first\nsublinear (in the size of the input) time algorithm with almost the same\nguarantee as the Cheeger's inequality. As a byproduct of our results, we prove\na bicriteria approximation algorithm for the expansion profile of any graph.\nLet $\\phi(\\gamma) = \\min_{\\mu(S) \\leq \\gamma}\\phi(S)$. There is a polynomial\ntime algorithm that, for any $\\gamma,\\epsilon>0$, finds a set $S$ of measure\n$\\mu(S)\\leq 2\\gamma^{1+\\epsilon}$, and expansion $\\phi(S)\\leq\n\\sqrt{2\\phi(\\gamma)/\\epsilon}$. Our proof techniques also provide a simpler\nproof of the structural result of Arora, Barak, Steurer [ABS10], that can be\napplied to irregular graphs.\n  Our main technical tool is that for any set $S$ of vertices of a graph, a\nlazy $t$-step random walk started from a randomly chosen vertex of $S$, will\nremain entirely inside $S$ with probability at least $(1-\\phi(S)/2)^t$. This\nitself provides a new lower bound to the uniform mixing time of any finite\nstates reversible markov chain.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 01:06:25 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2012 19:52:10 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2012 21:44:37 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Gharan", "Shayan Oveis", ""], ["Trevisan", "Luca", ""]]}, {"id": "1204.2031", "submitter": "Amitabh Basu", "authors": "Amitabh Basu, Jesus De Loera, Mark Junod", "title": "On Chubanov's method for Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the method recently proposed by S. Chubanov for the linear\nfeasibility problem. We present new, concise proofs and interpretations of some\nof his results. We then show how our proofs can be used to find strongly\npolynomial time algorithms for special classes of linear feasibility problems.\nUnder certain conditions, these results provide new proofs of classical results\nobtained by Tardos, and Vavasis and Ye.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 02:50:13 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Basu", "Amitabh", ""], ["De Loera", "Jesus", ""], ["Junod", "Mark", ""]]}, {"id": "1204.2033", "submitter": "Paul Tune PT", "authors": "Paul Tune", "title": "Computing Constrained Cramer Rao Bounds", "comments": "Tech report version. Journal version was submitted to IEEE\n  Transactions on Signal Processing. This version includes derivation of the\n  iterations of the constrained majorization-minimization algoirthm, subject to\n  parameter equality constraints. 3 figures", "journal-ref": null, "doi": "10.1109/TSP.2012.2204258", "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of computing submatrices of the Cram\\'er-Rao bound\n(CRB), which lower bounds the variance of any unbiased estimator of a vector\nparameter $\\vth$. We explore iterative methods that avoid direct inversion of\nthe Fisher information matrix, which can be computationally expensive when the\ndimension of $\\vth$ is large. The computation of the bound is related to the\nquadratic matrix program, where there are highly efficient methods for solving\nit. We present several methods, and show that algorithms in prior work are\nspecial instances of existing optimization algorithms. Some of these methods\nconverge to the bound monotonically, but in particular, algorithms converging\nnon-monotonically are much faster. We then extend the work to encompass the\ncomputation of the CRB when the Fisher information matrix is singular and when\nthe parameter $\\vth$ is subject to constraints. As an application, we consider\nthe design of a data streaming algorithm for network measurement.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 02:56:57 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Tune", "Paul", ""]]}, {"id": "1204.2034", "submitter": "Pablo P\\'erez-Lantero", "authors": "J. Barbay and G. Navarro and P. P\\'erez-Lantero", "title": "Adaptive Techniques to find Optimal Planar Boxes", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $P$ of $n$ planar points, two axes and a real-valued score\nfunction $f()$ on subsets of $P$, the Optimal Planar Box problem consists in\nfinding a box (i.e. axis-aligned rectangle) $H$ maximizing $f(H\\cap P)$. We\nconsider the case where $f()$ is monotone decomposable, i.e. there exists a\ncomposition function $g()$ monotone in its two arguments such that\n$f(A)=g(f(A_1),f(A_2))$ for every subset $A\\subseteq P$ and every partition\n$\\{A_1,A_2\\}$ of $A$. In this context we propose a solution for the Optimal\nPlanar Box problem which performs in the worst case $O(n^2\\lg n)$ score\ncompositions and coordinate comparisons, and much less on other classes of\ninstances defined by various measures of difficulty. A side result of its own\ninterest is a fully dynamic \\textit{MCS Splay tree} data structure supporting\ninsertions and deletions with the \\emph{dynamic finger} property, improving\nupon previous results [Cort\\'es et al., J.Alg. 2009].\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 02:58:52 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Barbay", "J.", ""], ["Navarro", "G.", ""], ["P\u00e9rez-Lantero", "P.", ""]]}, {"id": "1204.2131", "submitter": "Michael Rink", "authors": "Michael Rink", "title": "On Thresholds for the Appearance of 2-cores in Mixed Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study thresholds for the appearance of a 2-core in random hypergraphs that\nare a mixture of a constant number of random uniform hypergraphs each with a\nlinear number of edges but with different edge sizes. For the case of two\noverlapping hypergraphs we give a solution for the optimal (expected) number of\nedges of each size such that the 2-core threshold for the resulting mixed\nhypergraph is maximized. We show that for adequate edge sizes this threshold\nexceeds the maximum 2-core threshold for any random uniform hypergraph, which\ncan be used to improve the space utilization of several data structures that\nrely on this parameter.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 12:56:42 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Rink", "Michael", ""]]}, {"id": "1204.2136", "submitter": "Or Sheffet", "authors": "Jeremiah Blocki and Avrim Blum and Anupam Datta and Or Sheffet", "title": "The Johnson-Lindenstrauss Transform Itself Preserves Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves that an \"old dog\", namely -- the classical\nJohnson-Lindenstrauss transform, \"performs new tricks\" -- it gives a novel way\nof preserving differential privacy. We show that if we take two databases, $D$\nand $D'$, such that (i) $D'-D$ is a rank-1 matrix of bounded norm and (ii) all\nsingular values of $D$ and $D'$ are sufficiently large, then multiplying either\n$D$ or $D'$ with a vector of iid normal Gaussians yields two statistically\nclose distributions in the sense of differential privacy. Furthermore, a small,\ndeterministic and \\emph{public} alteration of the input is enough to assert\nthat all singular values of $D$ are large.\n  We apply the Johnson-Lindenstrauss transform to the task of approximating\ncut-queries: the number of edges crossing a $(S,\\bar S)$-cut in a graph. We\nshow that the JL transform allows us to \\emph{publish a sanitized graph} that\npreserves edge differential privacy (where two graphs are neighbors if they\ndiffer on a single edge) while adding only $O(|S|/\\epsilon)$ random noise to\nany given query (w.h.p). Comparing the additive noise of our algorithm to\nexisting algorithms for answering cut-queries in a differentially private\nmanner, we outperform all others on small cuts ($|S| = o(n)$).\n  We also apply our technique to the task of estimating the variance of a given\nmatrix in any given direction. The JL transform allows us to \\emph{publish a\nsanitized covariance matrix} that preserves differential privacy w.r.t bounded\nchanges (each row in the matrix can change by at most a norm-1 vector) while\nadding random noise of magnitude independent of the size of the matrix (w.h.p).\nIn contrast, existing algorithms introduce an error which depends on the matrix\ndimensions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 13:11:47 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2012 01:29:07 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Blocki", "Jeremiah", ""], ["Blum", "Avrim", ""], ["Datta", "Anupam", ""], ["Sheffet", "Or", ""]]}, {"id": "1204.2581", "submitter": "Sheng Gao", "authors": "Sheng Gao and Ludovic Denoyer and Patrick Gallinari", "title": "Modeling Relational Data via Latent Factor Blockmodel", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of modeling relational data, which\nappear in many applications such as social network analysis, recommender\nsystems and bioinformatics. Previous studies either consider latent feature\nbased models but disregarding local structure in the network, or focus\nexclusively on capturing local structure of objects based on latent blockmodels\nwithout coupling with latent characteristics of objects. To combine the\nbenefits of the previous work, we propose a novel model that can simultaneously\nincorporate the effect of latent features and covariates if any, as well as the\neffect of latent structure that may exist in the data. To achieve this, we\nmodel the relation graph as a function of both latent feature factors and\nlatent cluster memberships of objects to collectively discover globally\npredictive intrinsic properties of objects and capture latent block structure\nin the network to improve prediction performance. We also develop an\noptimization transfer algorithm based on the generalized EM-style strategy to\nlearn the latent factors. We prove the efficacy of our proposed model through\nthe link prediction task and cluster analysis task, and extensive experiments\non the synthetic data and several real world datasets suggest that our proposed\nLFBM model outperforms the other state of the art approaches in the evaluated\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 22:14:05 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Gao", "Sheng", ""], ["Denoyer", "Ludovic", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1204.2606", "submitter": "Aleksandra Korolova", "authors": "Krishnaram Kenthapadi, Aleksandra Korolova, Ilya Mironov, Nina Mishra", "title": "Privacy via the Johnson-Lindenstrauss Transform", "comments": "24 pages", "journal-ref": "Journal of Privacy and Confidentiality, Volume 5, Issue 1, Pages\n  39-71, 2013", "doi": "10.29012/jpc.v5i1.625", "report-no": null, "categories": "cs.DS cs.CY cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that party A collects private information about its users, where each\nuser's data is represented as a bit vector. Suppose that party B has a\nproprietary data mining algorithm that requires estimating the distance between\nusers, such as clustering or nearest neighbors. We ask if it is possible for\nparty A to publish some information about each user so that B can estimate the\ndistance between users without being able to infer any private bit of a user.\nOur method involves projecting each user's representation into a random,\nlower-dimensional space via a sparse Johnson-Lindenstrauss transform and then\nadding Gaussian noise to each entry of the lower-dimensional representation. We\nshow that the method preserves differential privacy---where the more privacy is\ndesired, the larger the variance of the Gaussian noise. Further, we show how to\napproximate the true distances between users via only the lower-dimensional,\nperturbed data. Finally, we consider other perturbation methods such as\nrandomized response and draw comparisons to sketch-based methods. While the\ngoal of releasing user-specific data to third parties is more broad than\npreserving distances, this work shows that distance computations with privacy\nis an achievable goal.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 03:06:58 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Kenthapadi", "Krishnaram", ""], ["Korolova", "Aleksandra", ""], ["Mironov", "Ilya", ""], ["Mishra", "Nina", ""]]}, {"id": "1204.2844", "submitter": "Julia Chuzhoy", "authors": "Julia Chuzhoy", "title": "On Vertex Sparsifiers with Steiner Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G=(V,E)$ with edge capacities $c_e\\geq 1$ for\n$e\\in E$ and a subset $T$ of $k$ vertices called terminals, we say that a graph\n$H$ is a quality-$q$ cut sparsifier for $G$ iff $T\\subseteq V(H)$, and for any\npartition $(A,B)$ of $T$, the values of the minimum cuts separating $A$ and $B$\nin graphs $G$ and $H$ are within a factor $q$ from each other. We say that $H$\nis a quality-$q$ flow sparsifier for $G$ iff $T\\subseteq V(H)$, and for any set\n$D$ of demands over the terminals, the values of the minimum edge congestion\nincurred by fractionally routing the demands in $D$ in graphs $G$ and $H$ are\nwithin a factor $q$ from each other.\n  So far vertex sparsifiers have been studied in a restricted setting where the\nsparsifier $H$ is not allowed to contain any non-terminal vertices, that is\n$V(H)=T$. For this setting, efficient algorithms are known for constructing\nquality-$O(\\log k/\\log\\log k)$ cut and flow vertex sparsifiers, as well as a\nlower bound of $\\tilde{\\Omega}(\\sqrt{\\log k})$ on the quality of any flow or\ncut sparsifier.\n  We study flow and cut sparsifiers in the more general setting where Steiner\nvertices are allowed, that is, we no longer require that $V(H)=T$. We show\nalgorithms to construct constant-quality cut sparsifiers of size $O(C^3)$ in\ntime $\\poly(n)\\cdot 2^C$, and constant-quality flow sparsifiers of size\n$C^{O(\\log\\log C)}$ in time $n^{O(\\log C)}\\cdot 2^C$, where $C$ is the total\ncapacity of the edges incident on the terminals.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 21:37:48 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Chuzhoy", "Julia", ""]]}, {"id": "1204.2903", "submitter": "Thomas Bl\\\"asius", "authors": "Thomas Bl\\\"asius and Ignaz Rutter", "title": "Disconnectivity and Relative Positions in Simultaneous Embeddings", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem Simultaneous Embedding with Fixed Edges (SEFE) asks for two\nplanar graph $G^1 = (V^1, E^1)$ and $G^2 = (V^2, E^2)$ sharing a common\nsubgraph $G = G^1 \\cap G^2$ whether they admit planar drawings such that the\ncommon graph is drawn the same in both. Previous results on this problem\nrequire $G$, $G^1$ and $G^2$ to be connected. This paper is a first step\ntowards solving instances where these graphs are disconnected.\n  First, we show that an instance of the general SEFE-problem can be reduced in\nlinear time to an equivalent instance where $V^1 = V^2$ and $G^1$ and $G^2$ are\nconnected. This shows that it can be assumed without loss of generality that\nboth input graphs are connected. Second, we consider instances where $G$ is\ndisconnected. We show that SEFE can be solved in linear time if $G$ is a family\nof disjoint cycles by introducing the CC-tree, which represents all\nsimultaneous embeddings. We extend these results (including the CC-tree) to the\ncase where $G$ consists of arbitrary connected components, each with a fixed\nembedding.\n  Note that previous results require $G$ to be connected and thus do not need\nto care about relative positions of connected components. By contrast, we\nassume the embedding of each connected component to be fixed and thus focus on\nthese relative positions. As SEFE requires to deal with both, embeddings of\nconnected components and their relative positions, this complements previous\nwork.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 07:29:15 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 08:35:18 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1204.2933", "submitter": "Chung Keung Poon", "authors": "Stanley P.Y. Fung, Chung Keung Poon and Feifeng Zheng", "title": "Improved Randomized Online Scheduling of Intervals and Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online preemptive scheduling of intervals and jobs (with\nrestarts). Each interval or job has an arrival time, a deadline, a length and a\nweight. The objective is to maximize the total weight of completed intervals or\njobs. While the deterministic case for intervals was settled a long time ago,\nthe randomized case remains open. In this paper we first give a 2-competitive\nrandomized algorithm for the case of equal length intervals. The algorithm is\nbarely random in the sense that it randomly chooses between two deterministic\nalgorithms at the beginning and then sticks with it thereafter. Then we extend\nthe algorithm to cover several other cases of interval scheduling including\nmonotone instances, C-benevolent instances and D-benevolent instances, giving\nthe same competitive ratio. These algorithms are surprisingly simple but have\nthe best competitive ratio against all previous (fully or barely) randomized\nalgorithms. Next we extend the idea to give a 3-competitive algorithm for equal\nlength jobs. Finally, we prove a lower bound of 2 on the competitive ratio of\nall barely random algorithms that choose between two deterministic algorithms\nfor scheduling equal length intervals (and hence jobs).\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 09:43:27 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Fung", "Stanley P. Y.", ""], ["Poon", "Chung Keung", ""], ["Zheng", "Feifeng", ""]]}, {"id": "1204.2955", "submitter": "Meng  Zhang", "authors": "Meng Zhang, Yi Zhang, Jijun Tang", "title": "Label-Guided Graph Exploration with Adjustable Ratio of Labels", "comments": "20 pages, 7 figures. Accepted by International Journal of Foundations\n  of Computer Science", "journal-ref": null, "doi": null, "report-no": "MengZhang-2012-J-01", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph exploration problem is to visit all the nodes of a connected graph\nby a mobile entity, e.g., a robot. The robot has no a priori knowledge of the\ntopology of the graph or of its size. Cohen et al. \\cite{Ilcinkas08} introduced\nlabel guided graph exploration which allows the system designer to add short\nlabels to the graph nodes in a preprocessing stage; these labels can guide the\nrobot in the exploration of the graph. In this paper, we address the problem of\nadjustable 1-bit label guided graph exploration. We focus on the labeling\nschemes that not only enable a robot to explore the graph but also allow the\nsystem designer to adjust the ratio of the number of different labels. This\nflexibility is necessary when maintaining different labels may have different\ncosts or when the ratio is pre-specified. We present 1-bit labeling (two\ncolors, namely black and white) schemes for this problem along with a labeling\nalgorithm for generating the required labels. Given an $n$-node graph and a\nrational number $\\rho$, we can design a 1-bit labeling scheme such that\n$n/b\\geq \\rho$ where $b$ is the number of nodes labeled black. The robot uses\n$O(\\rho\\log\\Delta)$ bits of memory for exploring all graphs of maximum degree\n$\\Delta$. The exploration is completed in time\n$O(n\\Delta^{\\frac{16\\rho+7}{3}}/\\rho+\\Delta^{\\frac{40\\rho+10}{3}})$. Moreover,\nour labeling scheme can work on graphs containing loops and multiple edges,\nwhile that of Cohen et al. focuses on simple graphs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 11:19:06 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2012 01:45:30 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Zhang", "Meng", ""], ["Zhang", "Yi", ""], ["Tang", "Jijun", ""]]}, {"id": "1204.3113", "submitter": "\\'Alvaro Franco", "authors": "Carlos Eduardo Ferreira, \\'Alvaro Junio Pereira Franco", "title": "Algorithms for Junctions in Directed Acyclic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pair of distinct vertices u, v in a graph G, we say that s is a\njunction of u, v if there are in G internally vertex disjoint directed paths\nfrom s to u and from s to v. We show how to characterize junctions in directed\nacyclic graphs. We also consider the two problems in the following and derive\nefficient algorithms to solve them. Given a directed acyclic graph G and a\nvertex s in G, how can we find all pairs of vertices of G such that s is a\njunction of them? And given a directed acyclic graph G and k pairs of vertices\nof G, how can we preprocess G such that all junctions of k given pairs of\nvertices could be listed quickly? All junctions of k pairs problem arises in an\napplication in Anthropology and we apply our algorithm to find such junctions\non kinship networks of some brazilian indian ethnic groups.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 21:43:31 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Ferreira", "Carlos Eduardo", ""], ["Franco", "\u00c1lvaro Junio Pereira", ""]]}, {"id": "1204.3240", "submitter": "Ond\\v{r}ej Leng\\'al", "authors": "Ond\\v{r}ej Leng\\'al", "title": "An Efficient Finite Tree Automata Library", "comments": "Master's thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous computer systems use dynamic control and data structures of\nunbounded size. These data structures have often the character of trees or they\ncan be encoded as trees with some additional pointers. This is exploited by\nsome currently intensively studied techniques of formal verification that\nrepresent an infinite number of states using a finite tree automaton. However,\ncurrently there is no tree automata library implementation that would provide\nan efficient and flexible support for such methods. Thus the aim of this\nMaster's Thesis is to provide such a library. The present paper first describes\nthe theoretical background of finite tree automata and regular tree languages.\nThen it surveys the current implementations of tree automata libraries and\nstudies various verification techniques, outlining requirements for the\nlibrary. Representation of a finite tree automaton and algorithms that perform\nstandard language operations on this representation are proposed in the next\npart, which is followed by description of library implementation. Through a\nseries of experiments it is shown that the library can compete with other\navailable tree automata libraries, in certain areas being even significantly\nsuperior to them.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2012 07:24:37 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Leng\u00e1l", "Ond\u0159ej", ""]]}, {"id": "1204.3337", "submitter": "Mark Iwen", "authors": "Mark A. Iwen and Mauro Maggioni", "title": "Approximation of Points on Low-Dimensional Manifolds Via Random Linear\n  Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the approximate reconstruction of points, x \\in R^D,\nwhich are close to a given compact d-dimensional submanifold, M, of R^D using a\nsmall number of linear measurements of x. In particular, it is shown that a\nnumber of measurements of x which is independent of the extrinsic dimension D\nsuffices for highly accurate reconstruction of a given x with high probability.\nFurthermore, it is also proven that all vectors, x, which are sufficiently\nclose to M can be reconstructed with uniform approximation guarantees when the\nnumber of linear measurements of x depends logarithmically on D. Finally, the\nproofs of these facts are constructive: A practical algorithm for\nmanifold-based signal recovery is presented in the process of proving the two\nmain results mentioned above.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 00:26:07 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Iwen", "Mark A.", ""], ["Maggioni", "Mauro", ""]]}, {"id": "1204.3413", "submitter": "Yonatan Goldhirsh", "authors": "Eldar Fischer, Yonatan Goldhirsh and Oded Lachish", "title": "Testing Formula Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the query complexity of testing for properties defined by read once\nformulas, as instances of {\\em massively parametrized properties}, and prove\nseveral testability and non-testability results. First we prove the testability\nof any property accepted by a Boolean read-once formula involving any bounded\narity gates, with a number of queries exponential in $\\epsilon$, doubly\nexponential in the arity, and independent of all other parameters. When the\ngates are limited to being monotone, we prove that there is an {\\em estimation}\nalgorithm, that outputs an approximation of the distance of the input from\nsatisfying the property. For formulas only involving And/Or gates, we provide a\nmore efficient test whose query complexity is only quasipolynomial in\n$\\epsilon$. On the other hand, we show that such testability results do not\nhold in general for formulas over non-Boolean alphabets; specifically we\nconstruct a property defined by a read-once arity $2$ (non-Boolean) formula\nover an alphabet of size $4$, such that any $1/4$-test for it requires a number\nof queries depending on the formula size. We also present such a formula over\nan alphabet of size $5$ that additionally satisfies a strong monotonicity\ncondition.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 09:14:54 GMT"}, {"version": "v2", "created": "Thu, 27 Mar 2014 06:39:05 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Fischer", "Eldar", ""], ["Goldhirsh", "Yonatan", ""], ["Lachish", "Oded", ""]]}, {"id": "1204.3488", "submitter": "Vinicius Gusmao Pereira de Sa", "authors": "Guilherme D. da Fonseca, Celina M. H. de Figueiredo, Vin\\'icius G. P.\n  de S\\'a and Raphael Machado", "title": "Efficient sub-5 approximations for minimum dominating sets in unit disk\n  graphs", "comments": "An extended abstract of this paper appeared in the proceedings of the\n  10th Workshop on Approximation and Online Algorithms (WAOA 2012), Lecture\n  Notes in Computer Science 7846 (2013), pp. 82-92", "journal-ref": null, "doi": "10.1016/j.tcs.2014.01.023", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unit disk graph is the intersection graph of n congruent disks in the\nplane. Dominating sets in unit disk graphs are widely studied due to their\napplication in wireless ad-hoc networks. Because the minimum dominating set\nproblem for unit disk graphs is NP-hard, numerous approximation algorithms have\nbeen proposed in the literature, including some PTAS. However, since the\nproposal of a linear-time 5-approximation algorithm in 1995, the lack of\nefficient algorithms attaining better approximation factors has aroused\nattention. We introduce a linear-time O(n+m) approximation algorithm that takes\nthe usual adjacency representation of the graph as input and outputs a\n44/9-approximation. This approximation factor is also attained by a second\nalgorithm, which takes the geometric representation of the graph as input and\nruns in O(n log n) time regardless of the number of edges. Additionally, we\npropose a 43/9-approximation which can be obtained in O(n^2 m) time given only\nthe graph's adjacency representation. It is noteworthy that the dominating sets\nobtained by our algorithms are also independent sets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 13:59:38 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2012 22:36:23 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2014 20:31:02 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["da Fonseca", "Guilherme D.", ""], ["de Figueiredo", "Celina M. H.", ""], ["de S\u00e1", "Vin\u00edcius G. P.", ""], ["Machado", "Raphael", ""]]}, {"id": "1204.3514", "submitter": "Avrim Blum", "authors": "Maria-Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour", "title": "Distributed Learning, Communication Complexity and Privacy", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of PAC-learning from distributed data and analyze\nfundamental communication complexity questions involved. We provide general\nupper and lower bounds on the amount of communication needed to learn well,\nshowing that in addition to VC-dimension and covering number, quantities such\nas the teaching-dimension and mistake-bound of a class play an important role.\nWe also present tight results for a number of common concept classes including\nconjunctions, parity functions, and decision lists. For linear separators, we\nshow that for non-concentrated distributions, we can use a version of the\nPerceptron algorithm to learn with much less communication than the number of\nupdates given by the usual margin bound. We also show how boosting can be\nperformed in a generic manner in the distributed setting to achieve\ncommunication with only logarithmic dependence on 1/epsilon for any concept\nclass, and demonstrate how recent work on agnostic learning from\nclass-conditional queries can be used to achieve low communication in agnostic\nsettings as well. We additionally present an analysis of privacy, considering\nboth differential privacy and a notion of distributional privacy that is\nespecially appealing in this context.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 15:10:32 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2012 21:42:21 GMT"}, {"version": "v3", "created": "Fri, 25 May 2012 15:53:51 GMT"}], "update_date": "2012-05-28", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Blum", "Avrim", ""], ["Fine", "Shai", ""], ["Mansour", "Yishay", ""]]}, {"id": "1204.3569", "submitter": "Federico Librino", "authors": "Federico Librino, Marco Levorato and Michele Zorzi", "title": "An Algorithmic Solution for Computing Circle Intersection Areas and its\n  Applications to Wireless Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel iterative algorithm for the efficient computation of the intersection\nareas of an arbitrary number of circles is presented. The algorithm, based on a\ntrellis-structure, hinges on two geometric results which allow the\nexistence-check and the computation of the area of the intersection regions\ngenerated by more than three circles by simple algebraic manipulations of the\nintersection areas of a smaller number of circles. The presented algorithm is a\npowerful tool for the performance analysis of wireless networks, and finds many\napplications, ranging from sensor to cellular networks. As an example of\npractical application, an insightful study of the uplink outage probability of\nin a wireless network with cooperative access points as a function of the\ntransmission power and access point density is presented.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 16:55:50 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2012 16:33:01 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Librino", "Federico", ""], ["Levorato", "Marco", ""], ["Zorzi", "Michele", ""]]}, {"id": "1204.3581", "submitter": "Giuseppe Ottaviano", "authors": "Roberto Grossi, Giuseppe Ottaviano", "title": "The Wavelet Trie: Maintaining an Indexed Sequence of Strings in\n  Compressed Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An indexed sequence of strings is a data structure for storing a string\nsequence that supports random access, searching, range counting and analytics\noperations, both for exact matches and prefix search. String sequences lie at\nthe core of column-oriented databases, log processing, and other storage and\nquery tasks. In these applications each string can appear several times and the\norder of the strings in the sequence is relevant. The prefix structure of the\nstrings is relevant as well: common prefixes are sought in strings to extract\ninteresting features from the sequence. Moreover, space-efficiency is highly\ndesirable as it translates directly into higher performance, since more data\ncan fit in fast memory.\n  We introduce and study the problem of compressed indexed sequence of strings,\nrepresenting indexed sequences of strings in nearly-optimal compressed space,\nboth in the static and dynamic settings, while preserving provably good\nperformance for the supported operations.\n  We present a new data structure for this problem, the Wavelet Trie, which\ncombines the classical Patricia Trie with the Wavelet Tree, a succinct data\nstructure for storing a compressed sequence. The resulting Wavelet Trie\nsmoothly adapts to a sequence of strings that changes over time. It improves on\nthe state-of-the-art compressed data structures by supporting a dynamic\nalphabet (i.e. the set of distinct strings) and prefix queries, both crucial\nrequirements in the aforementioned applications, and on traditional indexes by\nreducing space occupancy to close to the entropy of the sequence.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 17:50:35 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Grossi", "Roberto", ""], ["Ottaviano", "Giuseppe", ""]]}, {"id": "1204.3873", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira, Amit Singer, Daniel A. Spielman", "title": "A Cheeger Inequality for the Graph Connection Laplacian", "comments": "To appear in the SIAM Journal on Matrix Analysis and Applications\n  (SIMAX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SP cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The O(d) Synchronization problem consists of estimating a set of unknown\northogonal transformations O_i from noisy measurements of a subset of the\npairwise ratios O_iO_j^{-1}. We formulate and prove a Cheeger-type inequality\nthat relates a measure of how well it is possible to solve the O(d)\nsynchronization problem with the spectra of an operator, the graph Connection\nLaplacian. We also show how this inequality provides a worst case performance\nguarantee for a spectral method to solve this problem.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 18:37:46 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 18:41:09 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Singer", "Amit", ""], ["Spielman", "Daniel A.", ""]]}, {"id": "1204.4047", "submitter": "Kim S. Larsen", "authors": "Joan Boyar, Sushmita Gupta, Kim S. Larsen", "title": "Access Graphs Results for LRU versus FIFO under Relative Worst Order\n  Analysis", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access graphs, which have been used previously in connection with competitive\nanalysis to model locality of reference in paging, are considered in connection\nwith relative worst order analysis. In this model, FWF is shown to be strictly\nworse than both LRU and FIFO on any access graph. LRU is shown to be strictly\nbetter than FIFO on paths and cycles, but they are incomparable on some\nfamilies of graphs which grow with the length of the sequences.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 11:19:01 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2012 19:01:20 GMT"}], "update_date": "2012-08-23", "authors_parsed": [["Boyar", "Joan", ""], ["Gupta", "Sushmita", ""], ["Larsen", "Kim S.", ""]]}, {"id": "1204.4106", "submitter": "Colin Cooper", "authors": "Colin Cooper, Robert Elsasser, Hirotaka Ono, Tomasz Radzik", "title": "Coalescing random walks and voting on connected graphs", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a coalescing random walk, a set of particles make independent random walks\non a graph. Whenever one or more particles meet at a vertex, they unite to form\na single particle, which then continues the random walk through the graph.\nCoalescing random walks can be used to achieve consensus in distributed\nnetworks, and is the basis of the self-stabilizing mutual exclusion algorithm\nof Israeli and Jalfon.\n  Let G=(V,E), be an undirected, connected n vertex graph with m edges. Let\nC(n) be the expected time for all particles to coalesce, when initially one\nparticle is located at each vertex of an n vertex graph.\n  We study the problem of bounding the coalescence time C(n) for general\nclasses of graphs. Our main result is that C(n)= O(1/(1-lambda_2))*((log n)^4\n+n/A)), where lambda_2 is the absolute value of the second largest eigenvalue\nof the transition matrix of the random walk, A= (sum d^2(v))/(d^2 n), d(v) is\nthe degree of vertex v, and d is the average node degree. The parameter A is an\nindicator of the variability of node degrees. Thus 1 <= A =O(n), with A=1 for\nregular graphs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 18:52:27 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 20:56:04 GMT"}, {"version": "v3", "created": "Sun, 25 Dec 2016 19:22:09 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Cooper", "Colin", ""], ["Elsasser", "Robert", ""], ["Ono", "Hirotaka", ""], ["Radzik", "Tomasz", ""]]}, {"id": "1204.4140", "submitter": "Chul-Ho Lee", "authors": "Chul-Ho Lee, Xin Xu, and Do Young Eun", "title": "Beyond Random Walk and Metropolis-Hastings Samplers: Why You Should Not\n  Backtrack for Unbiased Graph Sampling", "comments": "A short (double-column, 12-page) version of this paper will appear in\n  ACM SIGMETRICS/Performance 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.NI cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sampling via crawling has been actively considered as a generic and\nimportant tool for collecting uniform node samples so as to consistently\nestimate and uncover various characteristics of complex networks. The so-called\nsimple random walk with re-weighting (SRW-rw) and Metropolis-Hastings (MH)\nalgorithm have been popular in the literature for such unbiased graph sampling.\nHowever, an unavoidable downside of their core random walks -- slow diffusion\nover the space, can cause poor estimation accuracy. In this paper, we propose\nnon-backtracking random walk with re-weighting (NBRW-rw) and MH algorithm with\ndelayed acceptance (MHDA) which are theoretically guaranteed to achieve, at\nalmost no additional cost, not only unbiased graph sampling but also higher\nefficiency (smaller asymptotic variance of the resulting unbiased estimators)\nthan the SRW-rw and the MH algorithm, respectively. In particular, a remarkable\nfeature of the MHDA is its applicability for any non-uniform node sampling like\nthe MH algorithm, but ensuring better sampling efficiency than the MH\nalgorithm. We also provide simulation results to confirm our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 17:11:23 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Lee", "Chul-Ho", ""], ["Xu", "Xin", ""], ["Eun", "Do Young", ""]]}, {"id": "1204.4159", "submitter": "Seth Pettie", "authors": "Glencora Borradaile and Seth Pettie and Christian Wulff-Nilsen", "title": "Connectivity Oracles for Planar Graphs", "comments": "extended abstract to appear in SWAT 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider dynamic subgraph connectivity problems for planar graphs. In this\nmodel there is a fixed underlying planar graph, where each edge and vertex is\neither \"off\" (failed) or \"on\" (recovered). We wish to answer connectivity\nqueries with respect to the \"on\" subgraph. The model has two natural variants,\none in which there are $d$ edge/vertex failures that precede all connectivity\nqueries, and one in which failures/recoveries and queries are intermixed.\n  We present a $d$-failure connectivity oracle for planar graphs that processes\nany $d$ edge/vertex failures in $sort(d,n)$ time so that connectivity queries\ncan be answered in $pred(d,n)$ time. (Here $sort$ and $pred$ are the time for\ninteger sorting and integer predecessor search over a subset of $[n]$ of size\n$d$.) Our algorithm has two discrete parts. The first is an algorithm tailored\nto triconnected planar graphs. It makes use of Barnette's theorem, which states\nthat every triconnected planar graph contains a degree-3 spanning tree. The\nsecond part is a generic reduction from general (planar) graphs to triconnected\n(planar) graphs. Our algorithm is, moreover, provably optimal. An implication\nof Patrascu and Thorup's lower bound on predecessor search is that no\n$d$-failure connectivity oracle (even on trees) can beat $pred(d,n)$ query\ntime.\n  We extend our algorithms to the subgraph connectivity model where edge/vertex\nfailures (but no recoveries) are intermixed with connectivity queries. In\ntriconnected planar graphs each failure and query is handled in $O(\\log n)$\ntime (amortized), whereas in general planar graphs both bounds become $O(\\log^2\nn)$.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 18:46:35 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 18:27:35 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Borradaile", "Glencora", ""], ["Pettie", "Seth", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1204.4209", "submitter": "Venkatesan Guruswami", "authors": "Venkatesan Guruswami and Chaoping Xing", "title": "Folded Codes from Function Field Towers and Improved Optimal Rate List\n  Decoding", "comments": "Conference version appears at STOC 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.AG math.IT math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new construction of algebraic codes which are efficiently list\ndecodable from a fraction $1-R-\\eps$ of adversarial errors where $R$ is the\nrate of the code, for any desired positive constant $\\eps$. The worst-case list\nsize output by the algorithm is $O(1/\\eps)$, matching the existential bound for\nrandom codes up to constant factors. Further, the alphabet size of the codes is\na constant depending only on $\\eps$ - it can be made\n$\\exp(\\tilde{O}(1/\\eps^2))$ which is not much worse than the lower bound of\n$\\exp(\\Omega(1/\\eps))$. The parameters we achieve are thus quite close to the\nexistential bounds in all three aspects - error-correction radius, alphabet\nsize, and list-size - simultaneously. Our code construction is Monte Carlo and\nhas the claimed list decoding property with high probability. Once the code is\n(efficiently) sampled, the encoding/decoding algorithms are deterministic with\na running time $O_\\eps(N^c)$ for an absolute constant $c$, where $N$ is the\ncode's block length.\n  Our construction is based on a linear-algebraic approach to list decoding\nfolded codes from towers of function fields, and combining it with a special\nform of subspace-evasive sets. Instantiating this with the explicit\n\"asymptotically good\" Garcia-Stichtenoth tower of function fields yields the\nabove parameters. To illustrate the method in a simpler setting, we also\npresent a construction based on Hermitian function fields, which offers similar\nguarantees with a list and alphabet size polylogarithmic in the block length\n$N$. Along the way, we shed light on how to use automorphisms of certain\nfunction fields to enable list decoding of the folded version of the associated\nalgebraic-geometric codes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 21:02:36 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Xing", "Chaoping", ""]]}, {"id": "1204.4230", "submitter": "Daniel Lokshtanov", "authors": "Fedor Fomin, Daniel Lokshtanov, Neeldhara Misra and Saket Saurabh", "title": "Planar F-Deletion: Approximation, Kernelization and Optimal FPT\n  Algorithms", "comments": "Added Kernelization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let F be a finite set of graphs. In the F-Deletion problem, we are given an\nn-vertex graph G and an integer k as input, and asked whether at most k\nvertices can be deleted from G such that the resulting graph does not contain a\ngraph from F as a minor. F-Deletion is a generic problem and by selecting\ndifferent sets of forbidden minors F, one can obtain various fundamental\nproblems such as Vertex Cover, Feedback Vertex Set or Treewidth t-Deletion.\n  In this paper we obtain a number of generic algorithmic results about Planar\nF-Deletion, when F contains at least one planar graph. The highlights of our\nwork are - A constant factor approximation algorithm for the optimization\nversion of Planar F-Deletion. - A randomized linear time and single exponential\nparameterized algorithm, that is, an algorithm running in time O(2^{O(k)} n),\nfor the parameterized version of Planar F-deletion where all graphs in F are\nconnected. The algorithm can be made deterministic at the cost of making the\npolynomial factor in the running time n*log^2 n rather than linear. - A\npolynomial kernel for parameterized Planar F-deletion\n  These algorithms unify, generalize, and improve a multitude of results in the\nliterature. Our main results have several direct applications, but also the\nmethods we develop on the way have applicability beyond the scope of this\npaper. Our results -- constant factor approximation, polynomial kernelization\nand FPT algorithms -- are stringed together by a common theme of polynomial\ntime preprocessing.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 01:23:04 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 18:14:18 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Fomin", "Fedor", ""], ["Lokshtanov", "Daniel", ""], ["Misra", "Neeldhara", ""], ["Saurabh", "Saket", ""]]}, {"id": "1204.4368", "submitter": "Gregory Gutin", "authors": "G. Gutin, G. Muciaccia and A. Yeo", "title": "(Non-)existence of Polynomial Kernels for the Test Cover Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input of the Test Cover problem consists of a set $V$ of vertices, and a\ncollection ${\\cal E}=\\{E_1,..., E_m\\}$ of distinct subsets of $V$, called\ntests. A test $E_q$ separates a pair $v_i,v_j$ of vertices if $|\\{v_i,v_j\\}\\cap\nE_q|=1.$ A subcollection ${\\cal T}\\subseteq {\\cal E}$ is a test cover if each\npair $v_i,v_j$ of distinct vertices is separated by a test in ${\\cal T}$. The\nobjective is to find a test cover of minimum cardinality, if one exists. This\nproblem is NP-hard.\n  We consider two parameterizations the Test Cover problem with parameter $k$:\n(a) decide whether there is a test cover with at most $k$ tests, (b) decide\nwhether there is a test cover with at most $|V|-k$ tests. Both\nparameterizations are known to be fixed-parameter tractable. We prove that none\nhave a polynomial size kernel unless $NP\\subseteq coNP/poly$. Our proofs use\nthe cross-composition method recently introduced by Bodlaender et al. (2011)\nand parametric duality introduced by Chen et al. (2005). The result for the\nparameterization (a) was an open problem (private communications with Henning\nFernau and Jiong Guo, Jan.-Feb. 2012). We also show that the parameterization\n(a) admits a polynomial size kernel if the size of each test is upper-bounded\nby a constant.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 15:05:32 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Gutin", "G.", ""], ["Muciaccia", "G.", ""], ["Yeo", "A.", ""]]}, {"id": "1204.4431", "submitter": "Martin  Aum\\\"uller", "authors": "Martin Aum\\\"uller, Martin Dietzfelbinger, Philipp Woelfel", "title": "Explicit and Efficient Hash Families Suffice for Cuckoo Hashing with a\n  Stash", "comments": "18 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that for cuckoo hashing with a stash as proposed by Kirsch,\nMitzenmacher, and Wieder (2008) families of very simple hash functions can be\nused, maintaining the favorable performance guarantees: with stash size $s$ the\nprobability of a rehash is $O(1/n^{s+1})$, and the evaluation time is $O(s)$.\nInstead of the full randomness needed for the analysis of Kirsch et al. and of\nKutzelnigg (2010) (resp. $\\Theta(\\log n)$-wise independence for standard cuckoo\nhashing) the new approach even works with 2-wise independent hash families as\nbuilding blocks. Both construction and analysis build upon the work of\nDietzfelbinger and Woelfel (2003). The analysis, which can also be applied to\nthe fully random case, utilizes a graph counting argument and is much simpler\nthan previous proofs. As a byproduct, an algorithm for simulating uniform\nhashing is obtained. While it requires about twice as much space as the most\nspace efficient solutions, it is attractive because of its simple and direct\nstructure.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 18:40:58 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Dietzfelbinger", "Martin", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1204.4509", "submitter": "Yakov Nekrich", "authors": "Yakov Nekrich and Gonzalo Navarro", "title": "Sorted Range Reporting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a variant of the orthogonal range reporting problem\nwhen all points should be reported in the sorted order of their\n$x$-coordinates. We show that reporting two-dimensional points with this\nadditional condition can be organized (almost) as efficiently as the standard\nrange reporting.\n  Moreover, our results generalize and improve the previously known results for\nthe orthogonal range successor problem and can be used to obtain better\nsolutions for some stringology problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 00:54:14 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2012 02:18:05 GMT"}], "update_date": "2012-04-26", "authors_parsed": [["Nekrich", "Yakov", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1204.4526", "submitter": "Justin Ward", "authors": "Yuval Filmus and Justin Ward", "title": "A Tight Combinatorial Algorithm for Submodular Maximization Subject to a\n  Matroid Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimal, combinatorial 1-1/e approximation algorithm for\nmonotone submodular optimization over a matroid constraint. Compared to the\ncontinuous greedy algorithm (Calinescu, Chekuri, Pal and Vondrak, 2008), our\nalgorithm is extremely simple and requires no rounding. It consists of the\ngreedy algorithm followed by local search. Both phases are run not on the\nactual objective function, but on a related non-oblivious potential function,\nwhich is also monotone submodular. Our algorithm runs in randomized time\nO(n^8u), where n is the rank of the given matroid and u is the size of its\nground set. We additionally obtain a 1-1/e-eps approximation algorithm running\nin randomized time O (eps^-3n^4u). For matroids in which n = o(u), this\nimproves on the runtime of the continuous greedy algorithm. The improvement is\ndue primarily to the time required by the pipage rounding phase, which we avoid\naltogether. Furthermore, the independence of our algorithm from pipage rounding\ntechniques suggests that our general approach may be helpful in contexts such\nas monotone submodular maximization subject to multiple matroid constraints.\n  Our approach generalizes to the case where the monotone submodular function\nhas restricted curvature. For any curvature c, we adapt our algorithm to\nproduce a (1-e^-c)/c approximation. This result complements results of Vondrak\n(2008), who has shown that the continuous greedy algorithm produces a\n(1-e^-c)/c approximation when the objective function has curvature c. He has\nalso proved that achieving any better approximation ratio is impossible in the\nvalue oracle model.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 03:42:03 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2012 06:39:48 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2013 17:56:02 GMT"}, {"version": "v4", "created": "Tue, 19 Nov 2013 17:01:33 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Filmus", "Yuval", ""], ["Ward", "Justin", ""]]}, {"id": "1204.4562", "submitter": "Wajeb Gharibi", "authors": "Wajeb gharibi, Yong Xia", "title": "A Tight Linearization Strategy for Zero-One Quadratic Programming\n  Problems", "comments": "6 pages", "journal-ref": "International Journal of Computer Science Issues, IJCSI Volume 9,\n  Issue 2, March 2012", "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new approach to linearizing zero-one quadratic\nminimization problem which has many applications in computer science and\ncommunications. Our algorithm is based on the observation that the quadratic\nterm of zero-one variables has two equivalent piece-wise formulations, convex\nand concave cases. The convex piece-wise objective function and/or constraints\nplay a great role in deducing small linearization. Further tight strategies are\nalso discussed.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 08:29:44 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["gharibi", "Wajeb", ""], ["Xia", "Yong", ""]]}, {"id": "1204.4666", "submitter": "Lap Chi Lau", "authors": "Tsz Chiu Kwok and Lap Chi Lau", "title": "Finding Small Sparse Cuts Locally by Random Walk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a small sparse cut in an undirected graph.\nGiven an undirected graph G=(V,E) and a parameter k <= |E|, the small sparsest\ncut problem is to find a subset of vertices S with minimum conductance among\nall sets with volume at most k. Using ideas developed in local graph\npartitioning algorithms, we obtain the following bicriteria approximation\nalgorithms for the small sparsest cut problem:\n  - If there is a subset U with conductance \\phi and vol(U) <= k, then there is\na polynomial time algorithm to find a set S with conductance\nO(\\sqrt{\\phi/\\epsilon}) and vol(S) <= k^{1+\\epsilon} for any \\epsilon > 1/k.\n  - If there is a subset U with conductance \\phi and vol(U) <= k, then there is\na polynomial time algorithm to find a set S with conductance O(\\sqrt{\\phi\nln(k)/\\epsilon}) and vol(S) <= (1+\\epsilon)k for any \\epsilon > 2ln(k)/k.\n  These algorithms can be implemented locally using truncated random walk, with\nrunning time almost linear to the output size. This provides a local graph\npartitioning algorithm with a better conductance guarantee when k is sublinear.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 16:16:01 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Kwok", "Tsz Chiu", ""], ["Lau", "Lap Chi", ""]]}, {"id": "1204.4688", "submitter": "David Witmer", "authors": "Ryan O'Donnell and David Witmer", "title": "Markov chain methods for small-set expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a finite irreducible Markov chain with invariant distribution $\\pi$.\nWe use the inner product induced by $\\pi$ and the associated heat operator to\nsimplify and generalize some results related to graph partitioning and the\nsmall-set expansion problem. For example, Steurer showed a tight connection\nbetween the number of small eigenvalues of a graph's Laplacian and the\nexpansion of small sets in that graph. We give a simplified proof which\ngeneralizes to the nonregular, directed case. This result implies an\napproximation algorithm for an \"analytic\" version of the Small-Set Expansion\nProblem, which, in turn, immediately gives an approximation algorithm for\nSmall-Set Expansion. We also give a simpler proof of a lower bound on the\nprobability that a random walk stays within a set; this result was used in some\nrecent works on finding small sparse cuts.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 17:52:57 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 20:29:46 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2013 18:31:41 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["O'Donnell", "Ryan", ""], ["Witmer", "David", ""]]}, {"id": "1204.4714", "submitter": "Joseph Simons", "authors": "Maarten L\\\"offler, Joe Simons, Darren Strash", "title": "Dynamic Planar Point Location with Sub-Logarithmic Local Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study planar point location in a collection of disjoint fat regions, and\ninvestigate the complexity of \\emph {local updates}: replacing any region by a\ndifferent region that is \"similar\" to the original region. (i.e., the size\ndiffers by at most a constant factor, and distance between the two regions is a\nconstant times that size). We show that it is possible to create a linear size\ndata structure that allows for insertions, deletions, and queries in\nlogarithmic time, and allows for local updates in sub-logarithmic time on a\npointer machine.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 19:47:45 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 23:21:09 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["L\u00f6ffler", "Maarten", ""], ["Simons", "Joe", ""], ["Strash", "Darren", ""]]}, {"id": "1204.4765", "submitter": "Julius D'souza", "authors": "Julius D'souza", "title": "String Trees", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A string-like compact data structure for unlabelled rooted trees is given\nusing 2n bits.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2012 00:36:28 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["D'souza", "Julius", ""]]}, {"id": "1204.4829", "submitter": "Wajeb Gharibi", "authors": "Wajeb Gharibi", "title": "Improved Balas and Mazzola Linearization for Quadratic 0-1 Programs with\n  Application in a New Cutting Plane Algorithm", "comments": null, "journal-ref": "Int. J. Communications, Network and System Sciences, 2012", "doi": "10.4236/ijcns.2012", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balas and Mazzola linearization (BML) is widely used in devising cutting\nplane algorithms for quadratic 0-1 programs. In this article, we improve BML by\nfirst strengthening the primal formulation of BML and then considering the dual\nformulation. Additionally, a new cutting plane algorithm is proposed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2012 17:21:39 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Gharibi", "Wajeb", ""]]}, {"id": "1204.4835", "submitter": "Rajeev Raman", "authors": "Arash Farzan, J. Ian Munro, Rajeev Raman", "title": "Succinct Indices for Range Queries with applications to Orthogonal Range\n  Maxima", "comments": "To appear in ICALP 2012", "journal-ref": null, "doi": null, "report-no": "Leicester CS-TR-12-001", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of preprocessing $N$ points in 2D, each endowed with\na priority, to answer the following queries: given a axis-parallel rectangle,\ndetermine the point with the largest priority in the rectangle. Using the ideas\nof the \\emph{effective entropy} of range maxima queries and \\emph{succinct\nindices} for range maxima queries, we obtain a structure that uses O(N) words\nand answers the above query in $O(\\log N \\log \\log N)$ time. This is a direct\nimprovement of Chazelle's result from FOCS 1985 for this problem -- Chazelle\nrequired $O(N/\\epsilon)$ words to answer queries in $O((\\log N)^{1+\\epsilon})$\ntime for any constant $\\epsilon > 0$.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2012 19:36:06 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Farzan", "Arash", ""], ["Munro", "J. Ian", ""], ["Raman", "Rajeev", ""]]}, {"id": "1204.4880", "submitter": "Yngve Villanger", "authors": "Fedor V. Fomin and Saket Saurabh and Yngve Villanger", "title": "A Polynomial kernel for Proper Interval Vertex Deletion", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the problem of deleting at most k vertices to obtain a\nproper interval graph (Proper Interval Vertex Deletion) is fixed parameter\ntractable. However, whether the problem admits a polynomial kernel or not was\nopen. Here, we answers this question in affirmative by obtaining a polynomial\nkernel for Proper Interval Vertex Deletion. This resolves an open question of\nvan Bevern, Komusiewicz, Moser, and Niedermeier.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2012 11:24:42 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Saurabh", "Saket", ""], ["Villanger", "Yngve", ""]]}, {"id": "1204.4997", "submitter": "Thomas Bl\\\"asius", "authors": "Thomas Bl\\\"asius, Ignaz Rutter, Dorothea Wagner", "title": "Optimal Orthogonal Graph Drawing with Convex Bend Costs", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the quality of orthogonal planar drawings is quantified by\neither the total number of bends, or the maximum number of bends per edge.\nHowever, this neglects that in typical applications, edges have varying\nimportance. Moreover, as bend minimization over all planar embeddings is\nNP-hard, most approaches focus on a fixed planar embedding.\n  We consider the problem OptimalFlexDraw that is defined as follows. Given a\nplanar graph G on n vertices with maximum degree 4 and for each edge e a cost\nfunction cost_e : N_0 --> R defining costs depending on the number of bends on\ne, compute an orthogonal drawing of G of minimum cost. Note that this optimizes\nover all planar embeddings of the input graphs, and the cost functions allow\nfine-grained control on the bends of edges.\n  In this generality OptimalFlexDraw is NP-hard. We show that it can be solved\nefficiently if 1) the cost function of each edge is convex and 2) the first\nbend on each edge does not cause any cost (which is a condition similar to the\npositive flexibility for the decision problem FlexDraw). Moreover, we show the\nexistence of an optimal solution with at most three bends per edge except for a\nsingle edge per block (maximal biconnected component) with up to four bends.\nFor biconnected graphs we obtain a running time of O(n T_flow(n)), where\nT_flow(n) denotes the time necessary to compute a minimum-cost flow in a planar\nflow network with multiple sources and sinks. For connected graphs that are not\nbiconnected we need an additional factor of O(n).\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 08:25:37 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Rutter", "Ignaz", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1204.5023", "submitter": "Niraj Singh", "authors": "Niraj Kumar Singh, Mita Pal and Soubhik Chakraborty", "title": "The Parameterized Complexity Analysis of Partition Sort for Negative\n  Binomial Distribution Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper makes a study on Partition sort algorithm for negative\nbinomial inputs. Comparing the results with those for binomial inputs in our\nprevious work, we find that this algorithm is sensitive to parameters of both\ndistributions. But the main effects as well as the interaction effects\ninvolving these parameters and the input size are more significant for negative\nbinomial case.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 10:53:16 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Singh", "Niraj Kumar", ""], ["Pal", "Mita", ""], ["Chakraborty", "Soubhik", ""]]}, {"id": "1204.5083", "submitter": "Niraj Singh", "authors": "Niraj Kumar Singh and Soubhik Chakraborty", "title": "Smart Sort: Design and Analysis of a Fast, Efficient and Robust\n  Comparison Based Internal Sort Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart Sort algorithm is a \"smart\" fusion of heap construction procedures (of\nHeap sort algorithm) into the conventional \"Partition\" function (of Quick sort\nalgorithm) resulting in a robust version of Quick sort algorithm. We have also\nperformed empirical analysis of average case behavior of our proposed algorithm\nalong with the necessary theoretical analysis for best and worst cases. Its\nperformance was checked against some standard probability distributions, both\nuniform and non-uniform, like Binomial, Poisson, Discrete & Continuous Uniform,\nExponential, and Standard Normal. The analysis exhibited the desired robustness\ncoupled with excellent performance of our algorithm. Although this paper\nassumes the static partition ratios, its dynamic version is expected to yield\nstill better results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 15:22:43 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Singh", "Niraj Kumar", ""], ["Chakraborty", "Soubhik", ""]]}, {"id": "1204.5113", "submitter": "Pim van 't Hof", "authors": "Petr A. Golovach, Pim van 't Hof, Daniel Paulusma", "title": "Obtaining Planarity by Contracting Few Edges", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Planar Contraction problem is to test whether a given graph can be made\nplanar by using at most k edge contractions. This problem is known to be\nNP-complete. We show that it is fixed-parameter tractable when parameterized by\nk.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 17:12:35 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Golovach", "Petr A.", ""], ["Hof", "Pim van 't", ""], ["Paulusma", "Daniel", ""]]}, {"id": "1204.5224", "submitter": "Martin Lackner", "authors": "Marie-Louise Bruner, Martin Lackner", "title": "A Fast Algorithm for Permutation Pattern Matching Based on Alternating\n  Runs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NP-complete Permutation Pattern Matching problem asks whether a\n$k$-permutation $P$ is contained in a $n$-permutation $T$ as a pattern. This is\nthe case if there exists an order-preserving embedding of $P$ into $T$. In this\npaper, we present a fixed-parameter algorithm solving this problem with a\nworst-case runtime of $\\mathcal{O}(1.79^{\\mathsf{run}(T)}\\cdot n\\cdot k)$,\nwhere $\\mathsf{run}(T)$ denotes the number of alternating runs of $T$. This\nalgorithm is particularly well-suited for instances where $T$ has few runs,\ni.e., few ups and downs. Moreover, since $\\mathsf{run}(T)<n$, this can be seen\nas a $\\mathcal{O}(1.79^{n}\\cdot n\\cdot k)$ algorithm which is the first to beat\nthe exponential $2^n$ runtime of brute-force search. Furthermore, we prove that\nunder standard complexity theoretic assumptions such a fixed-parameter\ntractability result is not possible for $\\mathsf{run}(P)$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 22:46:21 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 16:47:42 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bruner", "Marie-Louise", ""], ["Lackner", "Martin", ""]]}, {"id": "1204.5229", "submitter": "Nimrod Talmon", "authors": "Tsvi Kopelowitz and Nimrod Talmon", "title": "Selection in the Presence of Memory Faults, with Applications to\n  In-place Resilient Sorting", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection problem, where one wishes to locate the $k^{th}$ smallest\nelement in an unsorted array of size $n$, is one of the basic problems studied\nin computer science. The main focus of this work is designing algorithms for\nsolving the selection problem in the presence of memory faults. These can\nhappen as the result of cosmic rays, alpha particles, or hardware failures.\n  Specifically, the computational model assumed here is a faulty variant of the\nRAM model (abbreviated as FRAM), which was introduced by Finocchi and Italiano.\nIn this model, the content of memory cells might get corrupted adversarially\nduring the execution, and the algorithm is given an upper bound $\\delta$ on the\nnumber of corruptions that may occur.\n  The main contribution of this work is a deterministic resilient selection\nalgorithm with optimal O(n) worst-case running time. Interestingly, the running\ntime does not depend on the number of faults, and the algorithm does not need\nto know $\\delta$.\n  The aforementioned resilient selection algorithm can be used to improve the\ncomplexity bounds for resilient $k$-d trees developed by Gieseke, Moruz and\nVahrenhold. Specifically, the time complexity for constructing a $k$-d tree is\nimproved from $O(n\\log^2 n + \\delta^2)$ to $O(n \\log n)$.\n  Besides the deterministic algorithm, a randomized resilient selection\nalgorithm is developed, which is simpler than the deterministic one, and has\n$O(n + \\alpha)$ expected time complexity and O(1) space complexity (i.e., is\nin-place). This algorithm is used to develop the first resilient sorting\nalgorithm that is in-place and achieves optimal $O(n\\log n + \\alpha\\delta)$\nexpected running time.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 23:08:38 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2012 20:17:11 GMT"}], "update_date": "2012-08-30", "authors_parsed": [["Kopelowitz", "Tsvi", ""], ["Talmon", "Nimrod", ""]]}, {"id": "1204.5335", "submitter": "Marek Karpinski", "authors": "Marek Karpinski, Andrzej Rucinski, Edyta Szymanska", "title": "Approximate Counting of Matchings in Sparse Uniform Hypergraphs", "comments": "arXiv admin note: substantial text overlap with arXiv:1202.5885", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a fully polynomial randomized approximation scheme\n(FPRAS) for the number of matchings in k-uniform hypergraphs whose intersection\ngraphs contain few claws. Our method gives a generalization of the canonical\npath method of Jerrum and Sinclair to hypergraphs satisfying a local\nrestriction. Our proof method depends on an application of the Euler tour\ntechnique for the canonical paths of the underlying Markov chains. On the other\nhand, we prove that it is NP-hard to approximate the number of matchings even\nfor the class of k-uniform, 2-regular and linear hypergraphs, for all k >= 6,\nwithout the above restriction.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 11:07:42 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Karpinski", "Marek", ""], ["Rucinski", "Andrzej", ""], ["Szymanska", "Edyta", ""]]}, {"id": "1204.5436", "submitter": "Eric Braude", "authors": "Eric Braude", "title": "Weakest Preconditions and Cumulative Subgoal Fulfillment: A Comparison", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contrast the use of weakest preconditions for the correct construction of\nprocedures with the cumulative subgoal fulfillment (CSF) approach. An example\nof Cohen and Monin is used for this purpose. The CSF construction process is\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 17:20:37 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Braude", "Eric", ""]]}, {"id": "1204.5442", "submitter": "Qin Xin", "authors": "Qin Xin", "title": "Faster Treasure Hunt and Better Strongly Universal Exploration Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the explicit deterministic treasure hunt\nproblem in a $n$-vertex network. This problem was firstly introduced by Ta-Shma\nand Zwick in \\cite{TZ07} [SODA'07]. Note also it is a variant of the well known\nrendezvous problem in which one of the robot (the treasure) is always\nstationary. In this paper, we propose an $O(n^{c(1+\\frac{1}{\\lambda})})$-time\nalgorithm for the treasure hunt problem, which significantly improves the\ncurrently best known result of running time $O(n^{2c})$ in \\cite{TZ07}, where\n$c$ is a constant induced from the construction of an universal exploration\nsequence in \\cite{R05,TZ07}, and $\\lambda \\gg 1$ is an arbitrary large, but\nfixed, integer constant. The treasure hunt problem also motivates the study of\nstrongly universal exploration sequences. In this paper, we also propose a much\nbetter explicit construction for strongly universal exploration sequences\ncompared to the one in \\cite{TZ07}.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 17:33:56 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Xin", "Qin", ""]]}, {"id": "1204.5489", "submitter": "Konstantinos Georgiou", "authors": "Eden Chlamtac and Zac Friggstad and Konstantinos Georgiou", "title": "Understanding Set Cover: Sub-exponential Time Approximations and\n  Lift-and-Project Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Cygan, Kowalik, and Wykurz [IPL 2009] gave sub-exponential-time\napproximation algorithms for the Set-Cover problem with approximation ratios\nbetter than ln(n). In light of this result, it is natural to ask whether such\nimprovements can be achieved using lift-and-project methods. We present a\nsimpler combinatorial algorithm which has nearly the same time-approximation\ntradeoff as the algorithm of Cygan et al., and which lends itself naturally to\na lift-and-project based approach.\n  At a high level, our approach is similar to the recent work of Karlin,\nMathieu, and Nguyen [IPCO 2011], who examined a known PTAS for Knapsack\n(similar to our combinatorial Set-Cover algorithm) and its connection to\nhierarchies of LP and SDP relaxations for Knapsack. For Set-Cover, we show\nthat, indeed, using the trick of \"lifting the objective function\", we can match\nthe performance of our combinatorial algorithm using the LP hierarchy of Lovasz\nand Schrijver. We also show that this trick is essential: even in the stronger\nLP hierarchy of Sherali and Adams, the integrality gap remains at least (1-eps)\nln(n) at level Omega(n) (when the objective function is not lifted).\n  As shown by Aleknovich, Arora, and Tourlakis [STOC 2005], Set-Cover\nrelaxations stemming from SDP hierarchies (specifically, LS+) have similarly\nlarge integrality gaps. This stands in contrast to Knapsack, where Karlin et\nal. showed that the (much stronger) Lasserre SDP hierarchy reduces the\nintegrality gap to (1+eps) at level O(1). For completeness, we show that LS+\nalso reduces the integrality gap for Knapsack to (1+eps). This result may be of\nindependent interest, as our LS+ based rounding and analysis are rather\ndifferent from those of Karlin et al., and to the best of our knowledge this is\nthe first explicit demonstration of such a reduction in the integrality gap of\nLS+ relaxations after few rounds.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 20:12:25 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2012 13:29:01 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Chlamtac", "Eden", ""], ["Friggstad", "Zac", ""], ["Georgiou", "Konstantinos", ""]]}, {"id": "1204.5500", "submitter": "Peter Lofgren", "authors": "Peter Lofgren", "title": "On the Complexity of the Monte Carlo Method for Incremental PageRank", "comments": null, "journal-ref": "Information Processing Letters, Volume 114, Issue 3, March 2014,\n  Pages 104-106", "doi": "10.1016/j.ipl.2013.11.006", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note extends the analysis of incremental PageRank in [B. Bahmani, A.\nChowdhury, and A. Goel. Fast Incremental and Personalized PageRank. VLDB 2011].\nIn that work, the authors prove a running time of $O(\\frac{nR}{\\epsilon^2}\n\\ln(m))$ to keep PageRank updated over $m$ edge arrivals in a graph with $n$\nnodes when the algorithm stores $R$ random walks per node and the PageRank\nteleport probability is $\\epsilon$. To prove this running time, they assume\nthat edges arrive in a random order, and leave it to future work to extend\ntheir running time guarantees to adversarial edge arrival. In this note, we\nshow that the random edge order assumption is necessary by exhibiting a graph\nand adversarial edge arrival order in which the running time is $\\Omega \\left(R\nn m^{\\lg{\\frac{3}{2}(1-\\epsilon)}}\\right)$. More generally, for any integer $d\n\\geq 2$, we construct a graph and adversarial edge order in which the running\ntime is $\\Omega \\left(R n m^{\\log_d(H_d (1-\\epsilon))}\\right)$, where $H_d$ is\nthe $d$th harmonic number.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 21:35:32 GMT"}, {"version": "v2", "created": "Sat, 12 Apr 2014 00:52:42 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Lofgren", "Peter", ""]]}, {"id": "1204.5524", "submitter": "Hideo Bannai", "authors": "Jun'ichi Yamamoto, Hideo Bannai, Shunsuke Inenaga, Masayuki Takeda", "title": "Time and Space Efficient Lempel-Ziv Factorization based on Run Length\n  Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for calculating the Lempel-Ziv factorization of a\nstring, based on run length encoding (RLE). We present a conceptually simple\noff-line algorithm based on a variant of suffix arrays, as well as an on-line\nalgorithm based on a variant of directed acyclic word graphs (DAWGs). Both\nalgorithms run in $O(N+n\\log n)$ time and O(n) extra space, where N is the size\nof the string, $n\\leq N$ is the number of RLE factors. The time dependency on N\nis only in the conversion of the string to RLE, which can be computed very\nefficiently in O(N) time and O(1) extra space (excluding the output). When the\nstring is compressible via RLE, i.e., $n = o(N)$, our algorithms are, to the\nbest of our knowledge, the first algorithms which require only o(N) extra space\nwhile running in $o(N\\log N)$ time.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 01:07:13 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2012 09:28:10 GMT"}, {"version": "v3", "created": "Mon, 27 May 2013 02:34:42 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yamamoto", "Jun'ichi", ""], ["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1204.5613", "submitter": "Paul Bonsma", "authors": "Paul Bonsma", "title": "Rerouting shortest paths in planar graphs", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rerouting sequence is a sequence of shortest st-paths such that consecutive\npaths differ in one vertex. We study the the Shortest Path Rerouting Problem,\nwhich asks, given two shortest st-paths P and Q in a graph G, whether a\nrerouting sequence exists from P to Q. This problem is PSPACE-hard in general,\nbut we show that it can be solved in polynomial time if G is planar. To this\nend, we introduce a dynamic programming method for reconfiguration problems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 10:59:41 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Bonsma", "Paul", ""]]}, {"id": "1204.5717", "submitter": "Jingjin Yu", "authors": "Jingjin Yu and Steven M. LaValle", "title": "Multi-agent Path Planning and Network Flow", "comments": "Corrected an inaccuracy on time optimal solution for average arrival\n  time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper connects multi-agent path planning on graphs (roadmaps) to network\nflow problems, showing that the former can be reduced to the latter, therefore\nenabling the application of combinatorial network flow algorithms, as well as\ngeneral linear program techniques, to multi-agent path planning problems on\ngraphs. Exploiting this connection, we show that when the goals are permutation\ninvariant, the problem always has a feasible solution path set with a longest\nfinish time of no more than $n + V - 1$ steps, in which $n$ is the number of\nagents and $V$ is the number of vertices of the underlying graph. We then give\na complete algorithm that finds such a solution in $O(nVE)$ time, with $E$\nbeing the number of edges of the graph. Taking a further step, we study time\nand distance optimality of the feasible solutions, show that they have a\npairwise Pareto optimal structure, and again provide efficient algorithms for\noptimizing two of these practical objectives.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 17:46:58 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2012 02:37:53 GMT"}, {"version": "v3", "created": "Thu, 3 May 2012 17:08:17 GMT"}, {"version": "v4", "created": "Mon, 7 Jan 2013 04:42:26 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yu", "Jingjin", ""], ["LaValle", "Steven M.", ""]]}, {"id": "1204.5801", "submitter": "J\\'er\\'emy Barbay", "authors": "J\\'er\\'emy Barbay", "title": "Optimal Prefix Free Code in Linear Time", "comments": "The algorithm TopDown is incorrect, and it is not clear how to\n  correct it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an algorithm computing an optimal prefix free code from $N$\nunsorted positive integer weights in time linear in the number of machine words\nholding those weights. This algorithm takes advantage of common non-algebraic\ninstructions, and of specific results on optimal prefix free codes. This result\nimproves over the state of the art complexities of $O(N\\lg N)$ in the algebraic\ndecision tree model and $O(N\\lg\\lg N)$ in the RAM model for the computation of\nHuffman's codes, a landmark in compression and coding since 1952.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 01:27:44 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2012 22:00:44 GMT"}, {"version": "v3", "created": "Sat, 13 Jul 2013 15:43:18 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2014 10:22:39 GMT"}, {"version": "v5", "created": "Tue, 13 Dec 2016 16:56:03 GMT"}, {"version": "v6", "created": "Wed, 1 Mar 2017 10:10:15 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Barbay", "J\u00e9r\u00e9my", ""]]}, {"id": "1204.5810", "submitter": "Marco Molinaro", "authors": "Marco Molinaro and R. Ravi", "title": "Geometry of Online Packing Linear Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider packing LP's with $m$ rows where all constraint coefficients are\nnormalized to be in the unit interval. The n columns arrive in random order and\nthe goal is to set the corresponding decision variables irrevocably when they\narrive so as to obtain a feasible solution maximizing the expected reward.\nPrevious (1 - \\epsilon)-competitive algorithms require the right-hand side of\nthe LP to be Omega((m/\\epsilon^2) log (n/\\epsilon)), a bound that worsens with\nthe number of columns and rows. However, the dependence on the number of\ncolumns is not required in the single-row case and known lower bounds for the\ngeneral case are also independent of n.\n  Our goal is to understand whether the dependence on n is required in the\nmulti-row case, making it fundamentally harder than the single-row version. We\nrefute this by exhibiting an algorithm which is (1 - \\epsilon)-competitive as\nlong as the right-hand sides are Omega((m^2/\\epsilon^2) log (m/\\epsilon)). Our\ntechniques refine previous PAC-learning based approaches which interpret the\nonline decisions as linear classifications of the columns based on sampled dual\nprices. The key ingredient of our improvement comes from a non-standard\ncovering argument together with the realization that only when the columns of\nthe LP belong to few 1-d subspaces we can obtain small such covers; bounding\nthe size of the cover constructed also relies on the geometry of linear\nclassifiers. General packing LP's are handled by perturbing the input columns,\nwhich can be seen as making the learning problem more robust.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 02:06:44 GMT"}], "update_date": "2012-04-27", "authors_parsed": [["Molinaro", "Marco", ""], ["Ravi", "R.", ""]]}, {"id": "1204.5823", "submitter": "Siddharth Barman", "authors": "Siddharth Barman, Shuchi Chawla, Seeun Umboh", "title": "A Bicriteria Approximation for the Reordering Buffer Problem", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the reordering buffer problem (RBP), a server is asked to process a\nsequence of requests lying in a metric space. To process a request the server\nmust move to the corresponding point in the metric. The requests can be\nprocessed slightly out of order; in particular, the server has a buffer of\ncapacity k which can store up to k requests as it reads in the sequence. The\ngoal is to reorder the requests in such a manner that the buffer constraint is\nsatisfied and the total travel cost of the server is minimized. The RBP arises\nin many applications that require scheduling with a limited buffer capacity,\nsuch as scheduling a disk arm in storage systems, switching colors in paint\nshops of a car manufacturing plant, and rendering 3D images in computer\ngraphics.\n  We study the offline version of RBP and develop bicriteria approximations.\nWhen the underlying metric is a tree, we obtain a solution of cost no more than\n9OPT using a buffer of capacity 4k + 1 where OPT is the cost of an optimal\nsolution with buffer capacity k. Constant factor approximations were known\npreviously only for the uniform metric (Avigdor-Elgrabli et al., 2012). Via\nrandomized tree embeddings, this implies an O(log n) approximation to cost and\nO(1) approximation to buffer size for general metrics. Previously the best\nknown algorithm for arbitrary metrics by Englert et al. (2007) provided an\nO(log^2 k log n) approximation without violating the buffer constraint.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 03:33:41 GMT"}], "update_date": "2012-04-27", "authors_parsed": [["Barman", "Siddharth", ""], ["Chawla", "Shuchi", ""], ["Umboh", "Seeun", ""]]}, {"id": "1204.5834", "submitter": "Antonio Blanca", "authors": "Antonio Blanca and Milena Mihail", "title": "Efficient Generation \\epsilon-close to G(n,p) and Generalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an efficient algorithm to generate a graph from a distribution\n$\\epsilon$-close to $G(n,p)$, in the sense of total variation distance. In\nparticular, if $p$ is represented with $O(\\log n)$-bit accuracy, then, with\nhigh probability, the running time is linear in the expected number of edges of\nthe output graph (up to poly-logarithmic factors). All our running times\ninclude the complexity of the arithmetic involved in the corresponding\nalgorithms. Previous standard methods for exact $G(n,p)$ sampling (see e.g.\nBatagelj and Brandes, 2005) achieve similar running times, however, under the\nassumption that performing real number arithmetic with arbitrary accuracy takes\nconstant time. We note that the actual accuracy required by these methods is\nO(n)-bit per step, which results in quadratic running times.\n  The main idea of our $G(n,p)$ generation algorithm is a Metropolis Markov\nchain to sample $\\epsilon$-close from the binomial distribution. This is a new\nmethod for sampling from the binomial distribution: it is of separate interest\nand may find other useful applications. Our analysis accounts for all necessary\nbit-accuracy and arithmetic, and our running times are comparable to known\nmethods for exact binomial sampling.\n  We further obtain efficient generation algorithms for random graphs with\ngiven arbitrary degree distributions, Inhomogeneous Random Graphs when the\nkernel function is the inner product, and Stochastic Kronecker Graphs. To the\nbest our knowledge, our work can be viewed as the first effort to simulate\nefficient generation of graphs from classical random graph models, while taking\ninto account implementational considerations as fundamental computational\naspects, and quantifying the tradeoff between accuracy and running time in a\nway that can be useful in practice.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 06:06:52 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2012 07:16:49 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Blanca", "Antonio", ""], ["Mihail", "Milena", ""]]}, {"id": "1204.5853", "submitter": "Thomas Bl\\\"asius", "authors": "Thomas Bl\\\"asius and Stephen G. Kobourov and Ignaz Rutter", "title": "Simultaneous Embedding of Planar Graphs", "comments": "survey, 35 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous embedding is concerned with simultaneously representing a series\nof graphs sharing some or all vertices. This forms the basis for the\nvisualization of dynamic graphs and thus is an important field of research.\nRecently there has been a great deal of work investigating simultaneous\nembedding problems both from a theoretical and a practical point of view. We\nsurvey recent work on this topic.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 07:50:05 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2012 16:05:15 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 07:55:46 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Kobourov", "Stephen G.", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1204.5929", "submitter": "Fabrizio Luccio", "authors": "Fabrizio Luccio and Linda Pagli", "title": "Chain Rotations: a New Look at Tree Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As well known the rotation distance D(S,T) between two binary trees S, T of n\nvertices is the minimum number of rotations of pairs of vertices to transform S\ninto T. We introduce the new operation of chain rotation on a tree, involving\ntwo chains of vertices, that requires changing exactly three pointers in the\ndata structure as for a standard rotation, and define the corresponding chain\ndistance C(S,T). As for D(S,T), no polynomial time algorithm to compute C(S,T)\nis known. We prove a constructive upper bound and an analytical lower bound on\nC(S,T) based on the number of maximal chains in the two trees. In terms of n we\nprove the general upper bound C(S,T)<= n-1 and we show that there are pairs of\ntrees for which this bound is tight. No similar result is known for D(S,T)\nwhere the best upper and lower bounds are 2n-6 and 5n/3-4 respectively.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 14:05:22 GMT"}], "update_date": "2012-04-27", "authors_parsed": [["Luccio", "Fabrizio", ""], ["Pagli", "Linda", ""]]}, {"id": "1204.5952", "submitter": "Sergei Kozyrev", "authors": "S. Albeverio, S.V. Kozyrev", "title": "Clustering by hypergraphs and dimensionality of cluster systems", "comments": "15 pages", "journal-ref": "p-Adic Numbers, Ultrametric Analysis and Applications, 4 (2012)\n  no. 3, 167--178", "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we discuss the clustering procedure in the case where\ninstead of a single metric we have a family of metrics. In this case we can\nobtain a partially ordered graph of clusters which is not necessarily a tree.\nWe discuss a structure of a hypergraph above this graph. We propose two\ndefinitions of dimension for hyperedges of this hypergraph and show that for\nthe multidimensional p-adic case both dimensions are reduced to the number of\np-adic parameters.\n  We discuss the application of the hypergraph clustering procedure to the\nconstruction of phylogenetic graphs in biology. In this case the dimension of a\nhyperedge will describe the number of sources of genetic diversity.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 14:57:59 GMT"}], "update_date": "2012-08-01", "authors_parsed": [["Albeverio", "S.", ""], ["Kozyrev", "S. V.", ""]]}, {"id": "1204.6070", "submitter": "Pavel Klav\\'ik", "authors": "James Abello, Pavel Klav\\'ik, Jan Kratochv\\'il, Tom\\'a\\v{s}\n  Vysko\\v{c}il", "title": "MSOL Restricted Contractibility to Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of graph planarization via edge\ncontraction. The problem CONTRACT asks whether there exists a set $S$ of at\nmost $k$ edges that when contracted produces a planar graph. We work with a\nmore general problem called $P$-RESTRICTEDCONTRACT in which $S$, in addition,\nis required to satisfy a fixed MSOL formula $P(S,G)$. We give an FPT algorithm\nin time $O(n^2 f(k))$ which solves $P$-RESTRICTEDCONTRACT, where $P(S,G)$ is\n(i) inclusion-closed and (ii) inert contraction-closed (where inert edges are\nthe edges non-incident to any inclusion minimal solution $S$).\n  As a specific example, we can solve the $\\ell$-subgraph contractibility\nproblem in which the edges of a set $S$ are required to form disjoint connected\nsubgraphs of size at most $\\ell$. This problem can be solved in time $O(n^2\nf'(k,\\ell))$ using the general algorithm. We also show that for $\\ell \\ge 2$\nthe problem is NP-complete.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 22:46:55 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 15:08:42 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 09:21:14 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Abello", "James", ""], ["Klav\u00edk", "Pavel", ""], ["Kratochv\u00edl", "Jan", ""], ["Vysko\u010dil", "Tom\u00e1\u0161", ""]]}, {"id": "1204.6233", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Stefan Szeider", "title": "Strong Backdoors to Bounded Treewidth SAT", "comments": "arXiv admin note: substantial text overlap with arXiv:1202.4331", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are various approaches to exploiting \"hidden structure\" in instances of\nhard combinatorial problems to allow faster algorithms than for general\nunstructured or random instances. For SAT and its counting version #SAT, hidden\nstructure has been exploited in terms of decomposability and strong backdoor\nsets. Decomposability can be considered in terms of the treewidth of a graph\nthat is associated with the given CNF formula, for instance by considering\nclauses and variables as vertices of the graph, and making a variable adjacent\nwith all the clauses it appears in. On the other hand, a strong backdoor set of\na CNF formula is a set of variables such that each possible partial assignment\nto this set moves the formula into a fixed class for which (#)SAT can be solved\nin polynomial time.\n  In this paper we combine the two above approaches. In particular, we study\nthe algorithmic question of finding a small strong backdoor set into the class\nW_t of CNF formulas whose associated graphs have treewidth at most t. The main\nresults are positive:\n  (1) There is a cubic-time algorithm that, given a CNF formula F and two\nconstants k,t\\ge 0, either finds a strong W_t-backdoor set of size at most 2^k,\nor concludes that F has no strong W_t-backdoor set of size at most k.\n  (2) There is a cubic-time algorithm that, given a CNF formula F, computes the\nnumber of satisfying assignments of F or concludes that sb_t(F)>k, for any pair\nof constants k,t\\ge 0. Here, sb_t(F) denotes the size of a smallest strong\nW_t-backdoor set of F.\n  The significance of our results lies in the fact that they allow us to\nexploit algorithmically a hidden structure in formulas that is not accessible\nby any one of the two approaches (decomposability, backdoors) alone. Already a\nbackdoor size 1 on top of treewidth 1 (i.e., sb_1(F)=1) entails formulas of\narbitrarily large treewidth and arbitrarily large cycle cutsets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 14:51:23 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Gaspers", "Serge", ""], ["Szeider", "Stefan", ""]]}, {"id": "1204.6391", "submitter": "Pavel Klav\\'ik", "authors": "Pavel Klav\\'ik and Jan Kratochv\\'il and Tomasz Krawczyk and Bartosz\n  Walczak", "title": "Extending partial representations of function graphs and permutation\n  graphs", "comments": "Submitted to ESA 2012, track A", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function graphs are graphs representable by intersections of continuous\nreal-valued functions on the interval [0,1] and are known to be exactly the\ncomplements of comparability graphs. As such they are recognizable in\npolynomial time. Function graphs generalize permutation graphs, which arise\nwhen all functions considered are linear.\n  We focus on the problem of extending partial representations, which\ngeneralizes the recognition problem. We observe that for permutation graphs an\neasy extension of Golumbic's comparability graph recognition algorithm can be\nexploited. This approach fails for function graphs. Nevertheless, we present a\npolynomial-time algorithm for extending a partial representation of a graph by\nfunctions defined on the entire interval [0,1] provided for some of the\nvertices. On the other hand, we show that if a partial representation consists\nof functions defined on subintervals of [0,1], then the problem of extending\nthis representation to functions on the entire interval [0,1] becomes\nNP-complete.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2012 10:10:06 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Klav\u00edk", "Pavel", ""], ["Kratochv\u00edl", "Jan", ""], ["Krawczyk", "Tomasz", ""], ["Walczak", "Bartosz", ""]]}, {"id": "1204.6422", "submitter": "Panagiotis Cheilaris", "authors": "Panagiotis Cheilaris and Shakhar Smorodinsky", "title": "Conflict-free coloring with respect to a subset of intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a hypergraph H = (V, E), a coloring of its vertices is said to be\nconflict-free if for every hyperedge S \\in E there is at least one vertex in S\nwhose color is distinct from the colors of all other vertices in S. The\ndiscrete interval hypergraph Hn is the hypergraph with vertex set {1,...,n} and\nhyperedge set the family of all subsets of consecutive integers in {1,...,n}.\nWe provide a polynomial time algorithm for conflict-free coloring any\nsubhypergraph of Hn, we show that the algorithm has approximation ratio 2, and\nwe prove that our analysis is tight, i.e., there is a subhypergraph for which\nthe algorithm computes a solution which uses twice the number of colors of the\noptimal solution. We also show that the problem of deciding whether a given\nsubhypergraph of Hn can be colored with at most k colors has a quasipolynomial\ntime algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2012 16:58:51 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Cheilaris", "Panagiotis", ""], ["Smorodinsky", "Shakhar", ""]]}, {"id": "1204.6508", "submitter": "Neeraj Sharma", "authors": "Neeraj Sharma, Sandeep Sen", "title": "Efficient cache oblivious algorithms for randomized divide-and-conquer\n  on the multicore model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present randomized algorithms for sorting and convex hull\nthat achieves optimal performance (for speed-up and cache misses) on the\nmulticore model with private cache model. Our algorithms are cache oblivious\nand generalize the randomized divide and conquer strategy given by Reischuk and\nReif and Sen. Although the approach yielded optimal speed-up in the PRAM model,\nwe require additional techniques to optimize cache-misses in an oblivious\nsetting. Under a mild assumption on input and number of processors our\nalgorithm will have optimal time and cache misses with high probability.\nAlthough similar results have been obtained recently for sorting, we feel that\nour approach is simpler and general and we apply it to obtain an optimal\nparallel algorithm for 3D convex hulls with similar bounds. We also present a\nsimple randomized processor allocation technique without the explicit knowledge\nof the number of processors that is likely to find additional applications in\nresource oblivious environments.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 19:31:01 GMT"}, {"version": "v2", "created": "Sun, 27 May 2012 19:41:29 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["Sharma", "Neeraj", ""], ["Sen", "Sandeep", ""]]}, {"id": "1204.6675", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim", "title": "On the Locality of Some NP-Complete Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distributed message-passing {LOCAL} model. In this model a\ncommunication network is represented by a graph where vertices host processors,\nand communication is performed over the edges. Computation proceeds in\nsynchronous rounds. The running time of an algorithm is the number of rounds\nfrom the beginning until all vertices terminate. Local computation is free. An\nalgorithm is called {local} if it terminates within a constant number of\nrounds. The question of what problems can be computed locally was raised by\nNaor and Stockmayer \\cite{NS93} in their seminal paper in STOC'93. Since then\nthe quest for problems with local algorithms, and for problems that cannot be\ncomputed locally, has become a central research direction in the field of\ndistributed algorithms \\cite{KMW04,KMW10,LOW08,PR01}.\n  We devise the first local algorithm for an {NP-complete} problem.\nSpecifically, our randomized algorithm computes, with high probability, an\nO(n^{1/2 + epsilon} \\cdot chi)-coloring within O(1) rounds, where epsilon > 0\nis an arbitrarily small constant, and chi is the chromatic number of the input\ngraph. (This problem was shown to be NP-complete in \\cite{Z07}.) On our way to\nthis result we devise a constant-time algorithm for computing (O(1), O(n^{1/2 +\nepsilon}))-network-decompositions. Network-decompositions were introduced by\nAwerbuch et al. \\cite{AGLP89}, and are very useful for solving various\ndistributed problems. The best previously-known algorithm for\nnetwork-decomposition has a polylogarithmic running time (but is applicable for\na wider range of parameters) \\cite{LS93}. We also devise a Delta^{1 +\nepsilon}-coloring algorithm for graphs with sufficiently large maximum degree\nDelta that runs within O(1) rounds. It improves the best previously-known\nresult for this family of graphs, which is O(\\log-star n) \\cite{SW10}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 15:59:47 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Barenboim", "Leonid", ""]]}]