[{"id": "1908.00089", "submitter": "John Saunders", "authors": "Dina Barak-Pelleg, Daniel Berend, J.C. Saunders", "title": "A Model of Random Industrial SAT", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most studied models of SAT is random SAT. In this model, instances\nare composed from clauses chosen uniformly randomly and independently of each\nother. This model may be unsatisfactory in that it fails to describe various\nfeatures of SAT instances, arising in real-world applications. Various\nmodifications have been suggested to define models of industrial SAT. Here, we\nfocus mainly on the aspect of community structure. Namely, here the set of\nvariables consists of a number of disjoint communities, and clauses tend to\nconsist of variables from the same community. Thus, we suggest a model of\nrandom industrial SAT, in which the central generalization with respect to\nrandom SAT is the additional community structure. There has been a lot of work\non the satisfiability threshold of random $k$-SAT, starting with the\ncalculation of the threshold of $2$-SAT, up to the recent result that the\nthreshold exists for sufficiently large $k$. In this paper, we endeavor to\nstudy the satisfiability threshold for the proposed model of random industrial\nSAT. Our main result is that the threshold in this model tends to be smaller\nthan its counterpart for random SAT. Moreover, under some conditions, this\nthreshold even vanishes.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 20:41:22 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 21:58:54 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Barak-Pelleg", "Dina", ""], ["Berend", "Daniel", ""], ["Saunders", "J. C.", ""]]}, {"id": "1908.00140", "submitter": "Max Reuter", "authors": "Max Reuter, Gheorghe-Teodor Bercea", "title": "Sublinear Subwindow Search", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient approximation algorithm for subwindow search that\nruns in sublinear time and memory. Applied to object localization, this\nalgorithm significantly reduces running time and memory usage while maintaining\ncompetitive accuracy scores compared to the state-of-the-art. The algorithm's\naccuracy also scales with both the size and the spatial coherence\n(nearby-element similarity) of the matrix. It is thus well-suited for real-time\napplications and against many matrices in general.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 23:21:52 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Reuter", "Max", ""], ["Bercea", "Gheorghe-Teodor", ""]]}, {"id": "1908.00142", "submitter": "Ahmad Khaled Zarabie", "authors": "Ahmad Khaled Zarabie, Sanjoy Das", "title": "An L0-Norm Constrained Non-Negative Matrix Factorization Algorithm for\n  the Simultaneous Disaggregation of Fixed and Shiftable Loads", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy disaggregation refers to the decomposition of energy use time series\ndata into its constituent loads. This paper decomposes daily use data of a\nhousehold unit into fixed loads and one or more classes of shiftable loads. The\nlatter is characterized by ON OFF duty cycles. A novel algorithm based on\nnonnegative matrix factorization NMF for energy disaggregation is proposed,\nwhere fixed loads are represented in terms of real-valued basis vectors,\nwhereas shiftable loads are divided into binary signals. This binary\ndecomposition approach directly applies L0 norm constraints on individual\nshiftable loads. The new approach obviates the need for more computationally\nintensive methods e.g. spectral decomposition or mean field annealing that have\nbeen used in earlier research for these constraints. A probabilistic framework\nfor the proposed approach has been addressed. The proposed approach s\neffectiveness has been demonstrated with real consumer energy data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 23:29:08 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Zarabie", "Ahmad Khaled", ""], ["Das", "Sanjoy", ""]]}, {"id": "1908.00161", "submitter": "Haris Aziz", "authors": "Haris Aziz and Xin Huang and Nicholas Mattei and Erel Segal-Halevi", "title": "The Constrained Round Robin Algorithm for Fair and Efficient Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-agent resource allocation setting that models the\nassignment of papers to reviewers. A recurring issue in allocation problems is\nthe compatibility of welfare/efficiency and fairness. Given an oracle to find a\nwelfare-achieving allocation, we embed such an oracle into a flexible algorithm\ncalled the Constrained Round Robin (CRR) algorithm, that achieves the required\nwelfare level. Our algorithm also allows the system designer to lower the\nwelfare requirements in order to achieve a higher degree of fairness. If the\nwelfare requirement is lowered enough, a strengthening of envy-freeness up to\none item is guaranteed. Hence, our algorithm can be viewed as a computationally\nefficient way to interpolate between welfare and approximate envy-freeness in\nallocation problems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 00:58:13 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Aziz", "Haris", ""], ["Huang", "Xin", ""], ["Mattei", "Nicholas", ""], ["Segal-Halevi", "Erel", ""]]}, {"id": "1908.00204", "submitter": "Sheldon Tan", "authors": "Shaoyi Peng and Sheldon X.-D. Tan", "title": "GLU3.0: Fast GPU-based Parallel Sparse LU Factorization for Circuit\n  Simulation", "comments": "10 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LU factorization for sparse matrices is the most important computing step for\nmany engineering and scientific computing problems such as circuit simulation.\nBut parallelizing LU factorization with the Graphic Processing Units (GPU)\nstill remains a challenging problem due to high data dependency and irregular\nmemory accesses. Recently GPU-based hybrid right-looking sparse LU solver,\ncalled GLU (1.0 and 2.0), has been proposed to exploit the fine grain level\nparallelism of GPU. However, a new type of data dependency (called double-U\ndependency) introduced by GLU slows down the preprocessing step. Furthermore,\nGLU uses fixed GPU thread allocation strategy, which limits the parallelism. In\nthis article, we propose a new GPU-based sparse LU factorization method, called\n{\\it GLU3.0}, which solves the aforementioned problems. First, it introduces a\nmuch more efficient data dependency detection algorithm. Second, we observe\nthat the potential parallelism is different as the matrix factorization goes\non. We then develop three different modes of GPU kernel which adapt to\ndifferent stages to accommodate the computing task changes in the\nfactorization. Experimental results on circuit matrices from University of\nFlorida Sparse Matrix Collection (UFL) show that GLU3.0 delivers 2-3 orders of\nmagnitude speedup over GLU2.0 for the data dependency detection. Furthermore,\nGLU3.0 achieve 13.0 $\\times$ (arithmetic mean) or 6.7$\\times$ (geometric mean)\nspeedup over GLU2.0 and 7.1$\\times$ (arithmetic mean) or 4.8 $\\times$\n(geometric mean) over the recently proposed enhanced GLU2.0 sparse LU solver on\nthe same set of circuit matrices.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 04:28:49 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 00:46:42 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 21:48:20 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Peng", "Shaoyi", ""], ["Tan", "Sheldon X. -D.", ""]]}, {"id": "1908.00227", "submitter": "Shayan Oveis Gharan", "authors": "Anna Karlin and Nathan Klein and Shayan Oveis Gharan", "title": "An Improved Approximation Algorithm for TSP in the Half Integral Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a $1.49993$-approximation algorithm for the metric traveling\nsalesperson problem (TSP) for instances in which an optimal solution to the\nsubtour linear programming relaxation is half-integral. These instances\nreceived significant attention over the last decade due to a conjecture of\nSchalekamp, Williamson and van Zuylen stating that half-integral LP solutions\nhave the largest integrality gap over all fractional solutions. So, if the\nconjecture of Schalekamp et al. holds true, our result shows that the\nintegrality gap of the subtour polytope is bounded away from $3/2$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 06:17:10 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Karlin", "Anna", ""], ["Klein", "Nathan", ""], ["Gharan", "Shayan Oveis", ""]]}, {"id": "1908.00236", "submitter": "Hoa Vu", "authors": "Hsin-Hao Su, Hoa T. Vu", "title": "Distributed Data Summarization in Well-Connected Networks", "comments": "Conference version to appear at DISC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed algorithms for some fundamental problems in data\nsummarization. Given a communication graph $G$ of $n$ nodes each of which may\nhold a value initially, we focus on computing $\\sum_{i=1}^N g(f_i)$, where\n$f_i$ is the number of occurrences of value $i$ and $g$ is some fixed function.\nThis includes important statistics such as the number of distinct elements,\nfrequency moments, and the empirical entropy of the data.\n  In the CONGEST model, a simple adaptation from streaming lower bounds shows\nthat it requires $\\tilde{\\Omega}(D+ n)$ rounds, where $D$ is the diameter of\nthe graph, to compute some of these statistics exactly. However, these lower\nbounds do not hold for graphs that are well-connected. We give an algorithm\nthat computes $\\sum_{i=1}^{N} g(f_i)$ exactly in $\\tau_G \\cdot 2^{O(\\sqrt{\\log\nn})}$ rounds where $\\tau_G$ is the mixing time of $G$. This also has\napplications in computing the top $k$ most frequent elements.\n  We demonstrate that there is a high similarity between the GOSSIP model and\nthe CONGEST model in well-connected graphs. In particular, we show that each\nround of the GOSSIP model can be simulated almost-perfectly in\n$\\tilde{O}(\\tau_G $ rounds of the CONGEST model. To this end, we develop a new\nalgorithm for the GOSSIP model that $1\\pm \\epsilon$ approximates the $p$-th\nfrequency moment $F_p = \\sum_{i=1}^N f_i^p$ in $\\tilde{O}(\\epsilon^{-2}\nn^{1-k/p})$ rounds, for $p \\geq2$, when the number of distinct elements $F_0$\nis at most $O\\left(n^{1/(k-1)}\\right)$. This result can be translated back to\nthe CONGEST model with a factor $\\tilde{O}(\\tau_G)$ blow-up in the number of\nrounds.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 07:05:02 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 17:30:23 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Su", "Hsin-Hao", ""], ["Vu", "Hoa T.", ""]]}, {"id": "1908.00265", "submitter": "David B. Blumenthal", "authors": "David B. Blumenthal", "title": "New Techniques for Graph Edit Distance Computation", "comments": "Ph.D. Thesis, Free University of Bozen-Bolzano", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Due to their capacity to encode rich structural information, labeled graphs\nare often used for modeling various kinds of objects such as images, molecules,\nand chemical compounds. If pattern recognition problems such as clustering and\nclassification are to be solved on these domains, a (dis-)similarity measure\nfor labeled graphs has to be defined. A widely used measure is the graph edit\ndistance (GED), which, intuitively, is defined as the minimum amount of\ndistortion that has to be applied to a source graph in order to transform it\ninto a target graph. The main advantage of GED is its flexibility and\nsensitivity to small differences between the input graphs. Its main drawback is\nthat it is hard to compute.\n  In this thesis, new results and techniques for several aspects of computing\nGED are presented. Firstly, theoretical aspects are discussed: competing\ndefinitions of GED are harmonized, the problem of computing GED is\ncharacterized in terms of complexity, and several reductions from GED to the\nquadratic assignment problem (QAP) are presented. Secondly, solvers for the\nlinear sum assignment problem with error-correction (LSAPE) are discussed.\nLSAPE is a generalization of the well-known linear sum assignment problem\n(LSAP), and has to be solved as a subproblem by many GED algorithms. In\nparticular, a new solver is presented that efficiently reduces LSAPE to LSAP.\nThirdly, exact algorithms for computing GED are presented in a systematic way,\nand improvements of existing algorithms as well as a new mixed integer\nprogramming (MIP) based approach are introduced. Fourthly, a detailed overview\nof heuristic algorithms that approximate GED via upper and lower bounds is\nprovided, and eight new heuristics are described. Finally, a new easily\nextensible C++ library for exactly or approximately computing GED is presented.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 08:33:48 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Blumenthal", "David B.", ""]]}, {"id": "1908.00351", "submitter": "Aur\\'elien Ooms", "authors": "Jean Cardinal and Aur\\'elien Ooms", "title": "Sparse Regression via Range Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse regression problem, also known as best subset selection problem,\ncan be cast as follows: Given a set $S$ of $n$ points in $\\mathbb{R}^d$, a\npoint $y\\in \\mathbb{R}^d$, and an integer $2 \\leq k \\leq d$, find an affine\ncombination of at most $k$ points of $S$ that is nearest to $y$. We describe a\n$O(n^{k-1} \\log^{d-k+2} n)$-time randomized $(1+\\varepsilon)$-approximation\nalgorithm for this problem with \\(d\\) and \\(\\varepsilon\\) constant. This is the\nfirst algorithm for this problem running in time $o(n^k)$. Its running time is\nsimilar to the query time of a data structure recently proposed by Har-Peled,\nIndyk, and Mahabadi (ICALP'18), while not requiring any preprocessing. Up to\npolylogarithmic factors, it matches a conditional lower bound relying on a\nconjecture about affine degeneracy testing. In the special case where $k = d =\nO(1)$, we also provide a simple $O_\\delta(n^{d-1+\\delta})$-time deterministic\nexact algorithm, for any \\(\\delta > 0\\). Finally, we show how to adapt the\napproximation algorithm for the sparse linear regression and sparse convex\nregression problems with the same running time, up to polylogarithmic factors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 12:16:16 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 16:59:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Cardinal", "Jean", ""], ["Ooms", "Aur\u00e9lien", ""]]}, {"id": "1908.00352", "submitter": "Guido Br\\\"uckner", "authors": "Guido Br\\\"uckner, Markus Himmel, and Ignaz Rutter", "title": "An SPQR-Tree-Like Embedding Representation for Upward Planarity", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SPQR-tree is a data structure that compactly represents all planar\nembeddings of a biconnected planar graph. It plays a key role in constrained\nplanarity testing. We develop a similar data structure, called the UP-tree,\nthat compactly represents all upward planar embeddings of a biconnected\nsingle-source directed graph. We demonstrate the usefulness of the UP-tree by\nsolving the upward planar embedding extension problem for biconnected\nsingle-source directed graphs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 12:19:55 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Br\u00fcckner", "Guido", ""], ["Himmel", "Markus", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1908.00432", "submitter": "Georgia Avarikioti", "authors": "Georgia Avarikioti, Kenan Besic, Yuyi Wang, Roger Wattenhofer", "title": "Online Payment Network Design", "comments": "To appear in 3rd International Workshop on Cryptocurrencies and\n  Blockchain Technology (CBT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payment channels allow transactions between participants of the blockchain to\nbe executed securely off-chain, and thus provide a promising solution for the\nscalability problem of popular blockchains. We study the online network design\nproblem for payment channels, assuming a central coordinator. We focus on a\nsingle channel, where the coordinator desires to maximize the number of\naccepted transactions under given capital constraints. Despite the simplicity\nof the problem, we present a flurry of impossibility results, both for\ndeterministic and randomized algorithms against adaptive as well as oblivious\nadversaries.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:29:43 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Avarikioti", "Georgia", ""], ["Besic", "Kenan", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1908.00491", "submitter": "Konrad Dabrowski", "authors": "Konrad K. Dabrowski, Carl Feghali, Matthew Johnson, Giacomo Paesani,\n  Dani\\\"el Paulusma, Pawe{\\l} Rz\\k{a}\\.zewski", "title": "On Cycle Transversals and Their Connected Variants in the Absence of a\n  Small Linear Forest", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is $H$-free if it contains no induced subgraph isomorphic to $H$. We\nprove new complexity results for the two classical cycle transversal problems\nFeedback Vertex Set and Odd Cycle Transversal by showing that they can be\nsolved in polynomial time on $(sP_1+P_3)$-free graphs for every integer $s\\geq\n1$. We show the same result for the variants Connected Feedback Vertex Set and\nConnected Odd Cycle Transversal. We also prove that the latter two problems are\npolynomial-time solvable on cographs; this was already known for Feedback\nVertex Set and Odd Cycle Transversal. We complement these results by proving\nthat Odd Cycle Transversal and Connected Odd Cycle Transversal are NP-complete\non $(P_2+P_5,P_6)$-free graphs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:36:43 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Feghali", "Carl", ""], ["Johnson", "Matthew", ""], ["Paesani", "Giacomo", ""], ["Paulusma", "Dani\u00ebl", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "1908.00563", "submitter": "Sebastian Wild", "authors": "J. Ian Munro, Richard Peng, Sebastian Wild, Lingyi Zhang", "title": "Dynamic Optimality Refuted -- For Tournament Heaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a separation between offline and online algorithms for finger-based\ntournament heaps undergoing key modifications. These heaps are implemented by\nbinary trees with keys stored on leaves, and intermediate nodes tracking the\nmin of their respective subtrees. They represent a natural starting point for\nstudying self-adjusting heaps due to the need to access the root-to-leaf path\nupon modifications. We combine previous studies on the competitive ratios of\nunordered binary search trees by [Fredman WADS2011] and on order-by-next\nrequest by [Mart\\'inez-Roura TCS2000] and [Munro ESA2000] to show that for any\nnumber of fingers, tournament heaps cannot handle a sequence of modify-key\noperations with competitive ratio in $o(\\sqrt{\\log{n}})$. Critical to this\nanalysis is the characterization of the modifications that a heap can undergo\nupon an access. There are $\\exp(\\Theta(n \\log{n}))$ valid heaps on $n$ keys,\nbut only $\\exp(\\Theta(n))$ binary search trees. We parameterize the\nmodification power through the well-studied concept of fingers: additional\npointers the data structure can manipulate arbitrarily. Here we demonstrate\nthat fingers can be significantly more powerful than servers moving on a static\ntree by showing that access to $k$ fingers allow an offline algorithm to handle\nany access sequence with amortized cost $O(\\log_{k}(n) + 2^{\\lg^{*}n})$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 18:08:19 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Munro", "J. Ian", ""], ["Peng", "Richard", ""], ["Wild", "Sebastian", ""], ["Zhang", "Lingyi", ""]]}, {"id": "1908.00633", "submitter": "Conner DiPaolo", "authors": "Conner DiPaolo, Weiqing Gu", "title": "A Randomized Algorithm for Preconditioner Selection", "comments": "20(+1) pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of choosing a preconditioner $\\boldsymbol{M}$ to use when solving a\nlinear system $\\boldsymbol{Ax}=\\boldsymbol{b}$ with iterative methods is\ndifficult. For instance, even if one has access to a collection\n$\\boldsymbol{M}_1,\\boldsymbol{M}_2,\\ldots,\\boldsymbol{M}_n$ of candidate\npreconditioners, it is currently unclear how to practically choose the\n$\\boldsymbol{M}_i$ which minimizes the number of iterations of an iterative\nalgorithm to achieve a suitable approximation to $\\boldsymbol{x}$. This paper\nmakes progress on this sub-problem by showing that the preconditioner stability\n$\\|\\boldsymbol{I}-\\boldsymbol{M}^{-1}\\boldsymbol{A}\\|_\\mathsf{F}$, known to\nforecast preconditioner quality, can be computed in the time it takes to run a\nconstant number of iterations of conjugate gradients through use of sketching\nmethods. This is in spite of folklore which suggests the quantity is\nimpractical to compute, and a proof we give that ensures the quantity could not\npossibly be approximated in a useful amount of time by a deterministic\nalgorithm. Using our estimator, we provide a method which can provably select\nthe minimal stability preconditioner among $n$ candidates using floating point\noperations commensurate with running on the order of $n\\log n$ steps of the\nconjugate gradients algorithm. Our method can also advise the practitioner to\nuse no preconditioner at all if none of the candidates appears useful. The\nalgorithm is extremely easy to implement and trivially parallelizable. In one\nof our experiments, we use our preconditioner selection algorithm to create to\nthe best of our knowledge the first preconditioned method for kernel regression\nreported to never use more iterations than the non-preconditioned analog in\nstandard tests.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 21:11:30 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["DiPaolo", "Conner", ""], ["Gu", "Weiqing", ""]]}, {"id": "1908.00653", "submitter": "Victoria Crawford", "authors": "Victoria G. Crawford, Alan Kuhnle, My T. Thai", "title": "Submodular Cost Submodular Cover with an Approximate Oracle", "comments": "International Conference on Machine Learning. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the Submodular Cost Submodular Cover problem, which is\nto minimize the submodular cost required to ensure that the submodular benefit\nfunction exceeds a given threshold. Existing approximation ratios for the\ngreedy algorithm assume a value oracle to the benefit function. However, access\nto a value oracle is not a realistic assumption for many applications of this\nproblem, where the benefit function is difficult to compute. We present two\nincomparable approximation ratios for this problem with an approximate value\noracle and demonstrate that the ratios take on empirically relevant values\nthrough a case study with the Influence Threshold problem in online social\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 22:55:50 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Crawford", "Victoria G.", ""], ["Kuhnle", "Alan", ""], ["Thai", "My T.", ""]]}, {"id": "1908.00784", "submitter": "Decheng Wei", "authors": "Decheng Wei, Changhong Lu", "title": "Independent Double Roman Domination on Block Graphs", "comments": "11 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$, $f:V \\rightarrow \\{0,1,2 \\}$ is the Italian\ndominating function of $G$ if $f$ satisfies $\\sum_{u \\in N(v)}f(u) \\geq 2$ when\n$f(v)=0$. Denote $w(f)=\\sum_{v \\in V}f(v)$ as the weight of $f$. Let\n$V_i=\\{v:f(v)=i\\},i=0,1,2$, we call $f$ the independent Italian dominating\nfunction if $V_1 \\cup V_2$ is an independent set. The independent Italian\ndomination number of $G$ is the minimum weight of independent Italian\ndominating function $f$, denoted by $i_{I}(G)$. We equivalently transform the\nindependent domination problem of the connected block graph $G$ to the induced\nindependent domination problem of its block-cutpoint graph $T$, then a linear\ntime algorithm is given to find $i_{I}(G)$ of any connected block graph $G$\nbased on dynamic programming.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 10:03:31 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Wei", "Decheng", ""], ["Lu", "Changhong", ""]]}, {"id": "1908.00814", "submitter": "Pengcheng Lin", "authors": "Wan-Lei Zhao, Hui Wang, Peng-Cheng Lin, and Chong-Wah Ngo", "title": "On the Merge of k-NN Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-nearest neighbor graph is a fundamental data structure in many disciplines\nsuch as information retrieval, data-mining, pattern recognition, and machine\nlearning, etc. In the literature, considerable research has been focusing on\nhow to efficiently build an approximate k-nearest neighbor graph (k-NN graph)\nfor a fixed dataset. Unfortunately, a closely related issue of how to merge two\nexisting k-NN graphs has been overlooked. In this paper, we address the issue\nof k-NN graph merging in two different scenarios. In the first scenario, a\nsymmetric merge algorithm is proposed to combine two approximate k-NN graphs.\nThe algorithm facilitates large-scale processing by the efficient merging of\nk-NN graphs that are produced in parallel. In the second scenario, a joint\nmerge algorithm is proposed to expand an existing k-NN graph with a raw\ndataset. The algorithm enables the incremental construction of a hierarchical\napproximate k-NN graph. Superior performance is attained when leveraging the\nhierarchy for NN search of various data types, dimensionality, and distance\nmeasures.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 11:46:42 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 08:16:48 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 01:45:23 GMT"}, {"version": "v4", "created": "Fri, 13 Mar 2020 06:37:05 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2020 04:07:35 GMT"}, {"version": "v6", "created": "Thu, 29 Jul 2021 11:07:40 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhao", "Wan-Lei", ""], ["Wang", "Hui", ""], ["Lin", "Peng-Cheng", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "1908.00848", "submitter": "Jean Cardinal", "authors": "Prosenjit Bose, Jean Cardinal, John Iacono, Grigorios Koumoutsos,\n  Stefan Langerman", "title": "Competitive Online Search Trees on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of adaptive data structures for searching elements of\na tree-structured space. We use a natural generalization of the rotation-based\nonline binary search tree model in which the underlying search space is the set\nof vertices of a tree. This model is based on a simple structure for\ndecomposing graphs, previously known under several names including elimination\ntrees, vertex rankings, and tubings. The model is equivalent to the classical\nbinary search tree model exactly when the underlying tree is a path. We\ndescribe an online $O(\\log \\log n)$-competitive search tree data structure in\nthis model, matching the best known competitive ratio of binary search trees.\nOur method is inspired by Tango trees, an online binary search tree algorithm,\nbut critically needs several new notions including one which we call\nSteiner-closed search trees, which may be of independent interest. Moreover our\ntechnique is based on a novel use of two levels of decomposition, first from\nsearch space to a set of Steiner-closed trees, and secondly from these trees\ninto paths.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 13:33:47 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Bose", "Prosenjit", ""], ["Cardinal", "Jean", ""], ["Iacono", "John", ""], ["Koumoutsos", "Grigorios", ""], ["Langerman", "Stefan", ""]]}, {"id": "1908.01029", "submitter": "Victoria Crawford", "authors": "Victoria G. Crawford", "title": "An Efficient Evolutionary Algorithm for Minimum Cost Submodular Cover", "comments": "To appear at IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the Minimum Cost Submodular Cover problem is studied, which is\nto minimize a modular cost function such that the monotone submodular benefit\nfunction is above a threshold. For this problem, an evolutionary algorithm EASC\nis introduced that achieves a constant, bicriteria approximation in expected\npolynomial time; this is the first polynomial-time evolutionary approximation\nalgorithm for Minimum Cost Submodular Cover. To achieve this running time,\nideas motivated by submodularity and monotonicity are incorporated into the\nevolutionary process, which likely will extend to other submodular optimization\nproblems. In a practical application, EASC is demonstrated to outperform the\ngreedy algorithm and converge faster than competing evolutionary algorithms for\nthis problem.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 19:46:33 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Crawford", "Victoria G.", ""]]}, {"id": "1908.01034", "submitter": "Vasilis Kontonis", "authors": "Vasilis Kontonis, Christos Tzamos, Manolis Zampetakis", "title": "Efficient Truncated Statistics with Unknown Truncation", "comments": "to appear at 60th Annual IEEE Symposium on Foundations of Computer\n  Science (FOCS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the parameters of a Gaussian distribution\nwhen samples are only shown if they fall in some (unknown) subset $S \\subseteq\n\\R^d$. This core problem in truncated statistics has long history going back to\nGalton, Lee, Pearson and Fisher. Recent work by Daskalakis et al. (FOCS'18),\nprovides the first efficient algorithm that works for arbitrary sets in high\ndimension when the set is known, but leaves as an open problem the more\nchallenging and relevant case of unknown truncation set.\n  Our main result is a computationally and sample efficient algorithm for\nestimating the parameters of the Gaussian under arbitrary unknown truncation\nsets whose performance decays with a natural measure of complexity of the set,\nnamely its Gaussian surface area. Notably, this algorithm works for large\nfamilies of sets including intersections of halfspaces, polynomial threshold\nfunctions and general convex sets. We show that our algorithm closely captures\nthe tradeoff between the complexity of the set and the number of samples needed\nto learn the parameters by exhibiting a set with small Gaussian surface area\nfor which it is information theoretically impossible to learn the true Gaussian\nwith few samples.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 20:05:52 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1908.01087", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Nick Duffield", "title": "Adaptive Shrinkage Estimation for Streaming Graphs", "comments": "This paper is accepted at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a natural representation of complex systems across the sciences,\nand higher-order dependencies are central to the understanding and modeling of\nthese systems. However, in many practical applications such as online social\nnetworks, networks are massive, dynamic, and naturally streaming, where\npairwise interactions among vertices become available one at a time in some\narbitrary order. The massive size and streaming nature of these networks allow\nonly partial observation, since it is infeasible to analyze the entire network.\nUnder such scenarios, it is challenging to study the higher-order structural\nand connectivity patterns of streaming networks. In this work, we consider the\nfundamental problem of estimating the higher-order dependencies using adaptive\nsampling. We propose a novel adaptive, single-pass sampling framework and\nunbiased estimators for higher-order network analysis of large streaming\nnetworks. Our algorithms exploit adaptive techniques to identify edges that are\nhighly informative for efficiently estimating the higher-order structure of\nstreaming networks from small sample data. We also introduce a novel\nJames-Stein shrinkage estimator to reduce the estimation error. Our approach is\nfully analytic, computationally efficient, and can be incrementally updated in\na streaming setting. Numerical experiments on large networks show that our\napproach is superior to baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 23:02:16 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 20:06:30 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 23:44:44 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 22:59:24 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""]]}, {"id": "1908.01167", "submitter": "Amar Ranjan Dash", "authors": "Amar Ranjan Dash, Sandipta Kumar Sahu, B Kewal", "title": "An Optimized Disk Scheduling Algorithm With Bad-Sector Management", "comments": "21 pages, 21 figures, 3 table, International Journal of Computer\n  Science, Engineering and Applications (IJCSEA)", "journal-ref": "International Journal of Computer Science, Engineering and\n  Applications (IJCSEA), AIRCC, 2019, Vol. 9(3), pp 1-21, DOI\n  :10.5012/ijcsea.2019.9301", "doi": "10.5012/ijcsea.2019.9301", "report-no": null, "categories": "cs.OS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high performance computing, researchers try to optimize the CPU Scheduling\nalgorithms, for faster and efficient working of computers. But a process needs\nboth CPU bound and I/O bound for completion of its execution. With\nmodernization of computers the speed of processor, hard-disk, and I/O devices\nincreases gradually. Still the data access speed of hard-disk is much less than\nthe speed of the processor. So when processor receives a data from secondary\nmemory it executes immediately and again it have to wait for receiving another\ndata. So the slowness of the hard-disk becomes a bottleneck in the performance\nof processor. Researchers try to develop and optimize the traditional disk\nscheduling algorithms for faster data transfer to and from secondary data\nstorage devices. In this paper we try to evolve an optimized scheduling\nalgorithm by reducing the seek time, the rotational latency, and the data\ntransfer time in runtime. This algorithm has the feature to manage the\nbad-sectors of the hard-disk. It also attempts to reduce power consumption and\nheat reduction by minimizing bad sector reading time.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 13:16:51 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Dash", "Amar Ranjan", ""], ["Sahu", "Sandipta Kumar", ""], ["Kewal", "B", ""]]}, {"id": "1908.01181", "submitter": "Clemens Thielen", "authors": "Cristina Bazgan and Stefan Ruzika and Clemens Thielen and Daniel\n  Vanderpooten", "title": "The Power of the Weighted Sum Scalarization for Approximating\n  Multiobjective Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We determine the power of the weighted sum scalarization with respect to the\ncomputation of approximations for general multiobjective minimization and\nmaximization problems. Additionally, we introduce a new multi-factor notion of\napproximation that is specifically tailored to the multiobjective case and its\ninherent trade-offs between different objectives.\n  For minimization problems, we provide an efficient algorithm that computes an\napproximation of a multiobjective problem by using an exact or approximate\nalgorithm for its weighted sum scalarization. In case that an exact algorithm\nfor the weighted sum scalarization is used, this algorithm comes arbitrarily\nclose to the best approximation quality that is obtainable by supported\nsolutions - both with respect to the common notion of approximation and with\nrespect to the new multi-factor notion. Moreover, the algorithm yields the\ncurrently best approximation results for several well-known multiobjective\nminimization problems. For maximization problems, however, we show that a\npolynomial approximation guarantee can, in general, not be obtained in more\nthan one of the objective functions simultaneously by supported solutions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 14:16:40 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 12:14:16 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 11:05:33 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Bazgan", "Cristina", ""], ["Ruzika", "Stefan", ""], ["Thielen", "Clemens", ""], ["Vanderpooten", "Daniel", ""]]}, {"id": "1908.01223", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Faster algorithms for cograph edge modification problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Cograph Deletion (resp., Cograph Editing) problem the input is a graph\n$G$ and an integer $k$, and the goal is to decide whether there is a set of\nedges of size at most $k$ whose removal from $G$ (resp., removal and addition\nto $G$) results in a graph that does not contain an induced path with four\nvertices. In this paper we give algorithms for Cograph Deletion and Cograph\nEditing whose running times are $O^*(2.303^k)$ and $O^*(4.329^k)$,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 19:42:32 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 14:35:57 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2019 16:57:18 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1908.01230", "submitter": "Victoria Crawford", "authors": "Victoria G. Crawford", "title": "Faster Guarantees of Evolutionary Algorithms for Maximization of\n  Monotone Submodular Functions", "comments": "To be presented as a full paper at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the monotone submodular maximization problem (SM) is studied.\nSM is to find a subset of size $\\kappa$ from a universe of size $n$ that\nmaximizes a monotone submodular objective function $f$.\n  We show using a novel analysis that the Pareto optimization algorithm\nachieves a worst-case ratio of $(1-\\epsilon)(1-1/e)$ in expectation for every\ncardinality constraint $\\kappa < P$, where $P\\leq n+1$ is an input, in\n$O(nP\\ln(1/\\epsilon))$ queries of $f$.\n  In addition, a novel evolutionary algorithm called the biased Pareto\noptimization algorithm, is proposed that achieves a worst-case ratio of\n$(1-\\epsilon)(1-1/e)$ in expectation for every cardinality constraint $\\kappa <\nP$ in $O(n\\ln(P)\\ln(1/\\epsilon))$ queries of $f$. Further, the biased Pareto\noptimization algorithm can be modified in order to achieve a worst-case ratio\nof $(1-\\epsilon)(1-1/e)$ in expectation for cardinality constraint $\\kappa$ in\n$O(n\\ln(1/\\epsilon))$ queries of $f$.\n  An empirical evaluation corroborates our theoretical analysis of the\nalgorithms, as the algorithms exceed the stochastic greedy solution value at\nroughly when one would expect based upon our analysis.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 20:28:16 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 22:28:37 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 18:53:20 GMT"}, {"version": "v4", "created": "Wed, 7 Oct 2020 18:06:15 GMT"}, {"version": "v5", "created": "Mon, 5 Jul 2021 18:02:23 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Crawford", "Victoria G.", ""]]}, {"id": "1908.01241", "submitter": "Christina Lee Yu", "authors": "Devavrat Shah, Christina Lee Yu", "title": "Iterative Collaborative Filtering for Sparse Noisy Tensor Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the task of tensor estimation, i.e. estimating a low-rank 3-order $n\n\\times n \\times n$ tensor from noisy observations of randomly chosen entries in\nthe sparse regime. We introduce a generalization of the collaborative filtering\nalgorithm for sparse tensor estimation and argue that it achieves sample\ncomplexity that nearly matches the conjectured computationally efficient lower\nbound on the sample complexity. Our algorithm uses the matrix obtained from the\nflattened tensor to compute similarity, and estimates the tensor entries using\na nearest neighbor estimator. We prove that the algorithm recovers the tensor\nwith maximum entry-wise error and mean-squared-error (MSE) decaying to $0$ as\nlong as each entry is observed independently with probability $p =\n\\Omega(n^{-3/2 + \\kappa})$ for any arbitrarily small $\\kappa> 0$. Our analysis\nsheds insight into the conjectured sample complexity lower bound, showing that\nit matches the connectivity threshold of the graph used by our algorithm for\nestimating similarity between coordinates.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 22:27:26 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 04:28:39 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Shah", "Devavrat", ""], ["Yu", "Christina Lee", ""]]}, {"id": "1908.01263", "submitter": "Travis Gagie", "authors": "Taher Mun, Alan Kuhnle, Christina Boucher, Travis Gagie, Ben Langmead\n  and Giovanni Manzini", "title": "Matching reads to many genomes with the $r$-index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $r$-index is a tool for compressed indexing of genomic databases for\nexact pattern matching, which can be used to completely align reads that\nperfectly match some part of a genome in the database or to find seeds for\nreads that do not. This paper shows how to download and install the programs\nri-buildfasta and ri-align; how to call ri-buildfasta on a FASTA file to build\nan $r$-index for that file; and how to query that index with ri-align.\n  Availability: The source code for these programs is released under GPLv3 and\navailable at https://github.com/alshai/r-index .\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 03:31:16 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Mun", "Taher", ""], ["Kuhnle", "Alan", ""], ["Boucher", "Christina", ""], ["Gagie", "Travis", ""], ["Langmead", "Ben", ""], ["Manzini", "Giovanni", ""]]}, {"id": "1908.01317", "submitter": "Sergio Cabello", "authors": "Sergio Cabello", "title": "Computing the inverse geodesic length in planar graphs and graphs of\n  bounded treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse geodesic length of a graph $G$ is the sum of the inverse of the\ndistances between all pairs of distinct vertices of $G$. In some domains it is\nknown as the Harary index or the global efficiency of the graph. We show that,\nif $G$ is planar and has $n$ vertices, then the inverse geodesic length of $G$\ncan be computed in roughly $O(n^{9/5})$ time. We also show that, if $G$ has $n$\nvertices and treewidth at most $k$, then the inverse geodesic length of $G$ can\nbe computed in $O(n \\log^{O(k)}n)$ time. In both cases we use techniques\ndeveloped for computing the sum of the distances, which does not have\n``inverse'' component, together with batched evaluations of rational functions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 11:01:39 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Cabello", "Sergio", ""]]}, {"id": "1908.01441", "submitter": "Kazuo Misue", "authors": "Kazuo Misue and Katsuya Akasaka", "title": "Graph Drawing with Morphing Partial Edges", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A partial edge drawing (PED) of a graph is a variation of a node-link\ndiagram. PED draws a link, which is a partial visual representation of an edge,\nand reduces visual clutter of the node-link diagram. However, more time is\nrequired to read a PED to infer undrawn parts. The authors propose a morphing\nedge drawing (MED), which is a PED that changes with time. In MED, links morph\nbetween partial and complete drawings; thus, a reduced load for estimation of\nundrawn parts in a PED is expected. Herein, a formalization of MED is shown\nbased on a formalization of PED. Then, requirements for the scheduling of\nmorphing are specified. The requirements inhibit morphing from crossing and\nshorten the overall time for morphing the edges. Moreover, an algorithm for a\nscheduling method implemented by the authors is illustrated and the\neffectiveness of PED from a reading time viewpoint is shown through an\nexperimental evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 01:56:23 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 05:06:19 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 12:16:00 GMT"}, {"version": "v4", "created": "Thu, 22 Aug 2019 05:28:23 GMT"}, {"version": "v5", "created": "Wed, 2 Oct 2019 02:04:42 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Misue", "Kazuo", ""], ["Akasaka", "Katsuya", ""]]}, {"id": "1908.01562", "submitter": "Radu Stefan Mincu", "authors": "Radu Stefan Mincu", "title": "Heuristic Algorithm for Generalized Function Matching", "comments": "The paper was accepted for publication in the proceedings of KES 2019\n  23rd International Conference on Knowledge-Based and Intelligent Information\n  & Engineering Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of generalized function matching can be defined as follows: given\na pattern $p=p_1 \\cdots p_m$ and a text $t=t_1 \\cdots t_n$, find a mapping\n$f:\\Sigma_p\\rightarrow\\Sigma_t^{*}$ and all text locations $i$ such that\n$f(p_1) f(p_2) \\cdots f(p_m) = t_i \\cdots t_j$, a substring of $t$.\n  By modifying the restrictions of the matching function $f$, one can obtain\ndifferent matching problems, many of which have important applications. When\n$f:\\Sigma_p\\rightarrow\\Sigma_t$ we are faced with problems found in the\nwell-established field of combinatorial pattern matching. If the single\ncharacter constraint is lifted and $f:\\Sigma_p\\rightarrow\\Sigma_t^{*}$, we\nobtain generalized function matching as introduced by Amir and Nor (JDA 2007).\nIf we further constrain $f$ to be injective, then we arrive at generalized\nparametrized matching as defined by Clifford et al. (SPIRE 2009).\n  There are a number of important applications for pattern matching in\ncomputational biology, text editors and data compression, to name a few.\nTherefore, many efficient algorithms have been developed for a wide variety of\nspecific problems including finding tandem repeats in DNA sequences, optimizing\nembedded systems by reusing code etc.\n  In this work we present a heuristic algorithm illustrating a practical\napproach to tackling a variant of generalized function matching where\n$f:\\Sigma_p\\rightarrow\\Sigma_t^{+}$ and demonstrate its performance on\nhuman-produced text as well as random strings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 11:24:11 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Mincu", "Radu Stefan", ""]]}, {"id": "1908.01624", "submitter": "Marc Hartung", "authors": "Marc Hartung and Florian Schintke", "title": "Learned Clause Minimization in Parallel SAT Solvers", "comments": "accepted at Pragmatics of SAT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned clauses minimization (LCM) let to performance improvements of modern\nSAT solvers especially in solving hard SAT instances. Despite the success of\nLCM approaches in sequential solvers, they are not widely incorporated in\nparallel SAT solvers. In this paper we explore the potential of LCM for\nparallel SAT solvers by defining multiple LCM approaches based on clause\nvivification, comparing their runtime in different SAT solvers and discussing\nreasons for performance gains and losses. Results show that LCM only boosts\nperformance of parallel SAT solvers on a fraction of SAT instances. More\ncommonly applying LCM decreases performance. Only certain LCM approaches are\nable to improve the overall performance of parallel SAT solvers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 13:53:24 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Hartung", "Marc", ""], ["Schintke", "Florian", ""]]}, {"id": "1908.01664", "submitter": "Oluwole Ajala", "authors": "Oluwole Ajala, Miznah Alshammary, Mai Alzamel, Jia Gao, Costas\n  Iliopoulos, Jakub Radoszewski, Wojciech Rytter and Bruce Watson", "title": "On the cyclic regularities of strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularities in strings are often related to periods and covers, which have\nextensively been studied, and algorithms for their efficient computation have\nbroad application. In this paper we concentrate on computing cyclic\nregularities of strings, in particular, we propose several efficient algorithms\nfor computing: (i) cyclic periodicity; (ii) all cyclic periodicity; (iii)\nmaximal local cyclic periodicity; (iv) cyclic covers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 14:54:19 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Ajala", "Oluwole", ""], ["Alshammary", "Miznah", ""], ["Alzamel", "Mai", ""], ["Gao", "Jia", ""], ["Iliopoulos", "Costas", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Watson", "Bruce", ""]]}, {"id": "1908.01812", "submitter": "Javiel Rojas-Ledesma", "authors": "Gonzalo Navarro and Juan L. Reutter and Javiel Rojas-Ledesma", "title": "Optimal Joins using Compact Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worst-case optimal join algorithms have gained a lot of attention in the\ndatabase literature. We now count with several algorithms that are optimal in\nthe worst case, and many of them have been implemented and validated in\npractice. However, the implementation of these algorithms often requires an\nenhanced indexing structure: to achieve optimality we either need to build\ncompletely new indexes, or we must populate the database with several\ninstantiations of indexes such as B$+$-trees. Either way, this means spending\nan extra amount of storage space that may be non-negligible.\n  We show that optimal algorithms can be obtained directly from a\nrepresentation that regards the relations as point sets in variable-dimensional\ngrids, without the need of extra storage. Our representation is a compact quad\ntree for the static indexes, and a dynamic quadtree sharing subtrees (which we\ndub a qdag) for intermediate results. We develop a compositional algorithm to\nprocess full join queries under this representation, and show that the running\ntime of this algorithm is worst-case optimal in data complexity. Remarkably, we\ncan extend our framework to evaluate more expressive queries from relational\nalgebra by introducing a lazy version of qdags (lqdags). Once again, we can\nshow that the running time of our algorithms is worst-case optimal.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 19:25:57 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 15:28:33 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Reutter", "Juan L.", ""], ["Rojas-Ledesma", "Javiel", ""]]}, {"id": "1908.01956", "submitter": "Xiaorui Sun", "authors": "David Durfee, Laxman Dhulipala, Janardhan Kulkarni, Richard Peng,\n  Saurabh Sawlani, Xiaorui Sun", "title": "Parallel Batch-Dynamic Graphs: Algorithms and Lower Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of dynamically maintaining graph\nproperties under batches of edge insertions and deletions in the massively\nparallel model of computation. In this setting, the graph is stored on a number\nof machines, each having space strongly sublinear with respect to the number of\nvertices, that is, $n^\\epsilon$ for some constant $0 < \\epsilon < 1$. Our goal\nis to handle batches of updates and queries where the data for each batch fits\nonto one machine in constant rounds of parallel computation, as well as to\nreduce the total communication between the machines. This objective corresponds\nto the gradual buildup of databases over time, while the goal of obtaining\nconstant rounds of communication for problems in the static setting has been\nelusive for problems as simple as undirected graph connectivity.\n  We give an algorithm for dynamic graph connectivity in this setting with\nconstant communication rounds and communication cost almost linear in terms of\nthe batch size. Our techniques combine a new graph contraction technique, an\nindependent random sample extractor from correlated samples, as well as\ndistributed data structures supporting parallel updates and queries in batches.\n  We also illustrate the power of dynamic algorithms in the MPC model by\nshowing that the batched version of the adaptive connectivity problem is\n$\\mathsf{P}$-complete in the centralized setting, but sub-linear sized batches\ncan be handled in a constant number of rounds. Due to the wide applicability of\nour approaches, we believe it represents a practically-motivated workaround to\nthe current difficulties in designing more efficient massively parallel static\ngraph algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 04:56:46 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Durfee", "David", ""], ["Dhulipala", "Laxman", ""], ["Kulkarni", "Janardhan", ""], ["Peng", "Richard", ""], ["Sawlani", "Saurabh", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1908.02129", "submitter": "Sascha Gritzbach", "authors": "Sascha Gritzbach, Torsten Ueckerdt, Dorothea Wagner, Franziska Wegner,\n  Matthias Wolf", "title": "Engineering Negative Cycle Canceling for Wind Farm Cabling", "comments": "A preliminary version of this work was published in the Proceedings\n  of the 27th Annual European Symposium on Algorithms (ESA'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wind farm turbines convert wind energy into electrical energy. The\ngeneration of each turbine is transmitted, possibly via other turbines, to a\nsubstation that is connected to the power grid. On every possible\ninterconnection there can be at most one of various different cable types. Each\ntype comes with a cost per unit length and with a capacity. Designing a\ncost-minimal cable layout for a wind farm to feed all turbine production into\nthe power grid is called the Wind Farm Cabling Problem (WCP).\n  We consider a formulation of WCP as a flow problem on a graph where the cost\nof a flow on an edge is modeled by a step function originating from the cable\ntypes. Recently, we presented a proof-of-concept for a negative cycle\ncanceling-based algorithm for WCP [14]. We extend key steps of that heuristic\nand build a theoretical foundation that explains how this heuristic tackles the\nproblems arising from the special structure of WCP.\n  A thorough experimental evaluation identifies the best setup of the algorithm\nand compares it to existing methods from the literature such as Mixed-integer\nLinear Programming (MILP) and Simulated Annealing (SA). The heuristic runs in a\nrange of half a millisecond to approximately one and a half minutes on\ninstances with up to 500 turbines. It provides solutions of similar quality\ncompared to both competitors with running times of one hour and one day. When\ncomparing the solution quality after a running time of two seconds, our\nalgorithm outperforms the MILP- and SA-approaches, which allows it to be\napplied in interactive wind farm planning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:20:17 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Gritzbach", "Sascha", ""], ["Ueckerdt", "Torsten", ""], ["Wagner", "Dorothea", ""], ["Wegner", "Franziska", ""], ["Wolf", "Matthias", ""]]}, {"id": "1908.02132", "submitter": "Thomas Lidbetter Dr", "authors": "Spyros Angelopoulos and Thomas Lidbetter", "title": "Competitive Search in a Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic problem in which a Searcher must locate a hidden point,\nalso called the Hider in a network, starting from a root point. The network may\nbe either bounded or unbounded, thus generalizing well-known settings such as\nlinear and star search. We distinguish between pathwise search, in which the\nSearcher follows a continuous unit-speed path until the Hider is reached, and\nexpanding search, in which, at any point in time, the Searcher may restart from\nany previously reached point. The former has been the usual paradigm for\nstudying search games, whereas the latter is a more recent paradigm that can\nmodel real-life settings such as hunting for a fugitive, demining a field, or\nsearch-and-rescue operations. We seek both deterministic and randomized search\nstrategies that minimize the competitive ratio, namely the worst-case ratio of\nthe Hider's discovery time, divided by the shortest path to it from the root.\nConcerning expanding search, we show that a simple search strategy that applies\na \"waterfilling\" principle has optimal deterministic competitive ratio; in\ncontrast, we show that the optimal randomized competitive ratio is attained by\nfairly complex strategies even in a very simple network of three arcs.\nMotivated by this observation, we present and analyze an expanding search\nstrategy that is a 5/4 approximation of the randomized competitive ratio. Our\napproach is also applicable to pathwise search, for which we give a strategy\nthat is a 5 approximation of the randomized competitive ratio, and which\nimproves upon strategies derived from previous work.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:24:18 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["Lidbetter", "Thomas", ""]]}, {"id": "1908.02174", "submitter": "Mohamed Yosri Sayadi", "authors": "Mohamed Yosri Sayadi", "title": "On the maximum number of minimal connected dominating sets in convex\n  bipartite graphs", "comments": "10 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:1602.07504 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The enumeration of minimal connected dominating sets is known to be\nnotoriously hard for general graphs. Currently, it is only known that the sets\ncan be enumerated slightly faster than $\\mathcal{O}^{*}(2^n)$ and the algorithm\nis highly nontrivial. Moreover, it seems that it is hard to use bipartiteness\nas a structural aide when constructing enumeration algorithms. Hence, to the\nbest of our knowledge, there is no known input-sensitive algorithm for\nenumerating minimal dominating sets, or one of their related sets, in bipartite\ngraphs better than that of general graphs. In this paper, we provide the first\ninput-sensitive enumeration algorithm for some non trivial subclass of\nbipartite graphs, namely the convex graphs. We present an algorithm to\nenumerate all minimal connected dominating sets of convex bipartite graphs in\ntime $\\mathcal{O}(1.7254^{n})$ where $n$ is the number of vertices of the input\ngraph. Our algorithm implies a corresponding upper bound for the number of\nminimal connected dominating sets for this graph class. We complement the\nresult by providing a convex bipartite graph, which have at least $3^{(n-2)/3}$\nminimal connected dominating sets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 14:13:50 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Sayadi", "Mohamed Yosri", ""]]}, {"id": "1908.02211", "submitter": "Diego D\\'iaz-Dom\\'inguez", "authors": "Diego Diaz-Dom\\'inguez", "title": "An Index for Sequencing Reads Based on The Colored de Bruijn Graph", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we show how to transform a colored de Bruijn graph (dBG)\ninto a practical index for processing massive sets of sequencing reads. Similar\nto previous works, we encode an instance of a colored dBG of the set using BOSS\nand a color matrix C. To reduce the space requirements, we devise an algorithm\nthat produces a smaller and more sparse version of C. The novelties in this\nalgorithm are (i) an incomplete coloring of the graph and (ii) a greedy\ncoloring approach that tries to reuse the same colors for different strings\nwhen possible. We also propose two algorithms that work on top of the index;\none is for reconstructing reads, and the other is for contig assembly.\nExperimental results show that our data structure uses about half the space of\nthe plain representation of the set (1 Byte per DNA symbol) and that more than\n99% of the reads can be reconstructed just from the index.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 15:25:54 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 13:06:54 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Diaz-Dom\u00ednguez", "Diego", ""]]}, {"id": "1908.02213", "submitter": "Tillmann Miltzow", "authors": "Michael G. Dobbins, Andreas Holmsen, Tillmann Miltzow", "title": "A Universality Theorem for Nested Polytopes", "comments": "20 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a nutshell, we show that polynomials and nested polytopes are topological,\nalgebraic and algorithmically equivalent. Given two polytops $A\\subseteq B$ and\na number $k$, the Nested Polytope Problem (NPP) asks, if there exists a\npolytope $X$ on $k$ vertices such that $A\\subseteq X \\subseteq B$. The polytope\n$A$ is given by a set of vertices and the polytope $B$ is given by the defining\nhyperplanes. We show a universality theorem for NPP. Given an instance $I$ of\nthe NPP, we define the solutions set of $I$ as $$ V'(I) = \\{(x_1,\\ldots,x_k)\\in\n\\mathbb{R}^{k\\cdot n} : A\\subseteq \\text{conv}(x_1,\\ldots,x_k) \\subseteq B\\}.$$\nAs there are many symmetries, induced by permutations of the vertices, we will\nconsider the \\emph{normalized} solution space $V(I)$. Let $F$ be a finite set\nof polynomials, with bounded solution space. Then there is an instance $I$ of\nthe NPP, which has a rationally-equivalent normalized solution space $V(I)$.\nTwo sets $V$ and $W$ are rationally equivalent if there exists a homeomorphism\n$f : V \\rightarrow W$ such that both $f$ and $f^{-1}$ are given by rational\nfunctions. A function $f:V\\rightarrow W$ is a homeomorphism, if it is\ncontinuous, invertible and its inverse is continuous as well. As a corollary,\nwe show that NPP is $\\exists \\mathbb{R}$-complete. This implies that unless\n$\\exists \\mathbb{R} =$ NP, the NPP is not contained in the complexity class NP.\nNote that those results already follow from a recent paper by Shitov. Our proof\nis geometric and arguably easier.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 15:31:43 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Dobbins", "Michael G.", ""], ["Holmsen", "Andreas", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "1908.02530", "submitter": "Patrick Prosser", "authors": "Benjamin Bumpus, Patrick Prosser, James Trimble", "title": "A Constraint Model for the Tree Decomposition of a Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a constraint model for the problem of producing a tree\ndecomposition of a graph. The inputs to the model are a simple graph G, the\nnumber of nodes in the desired tree decomposition and the maximum cardinality\nof each node in that decomposition. Via a sequence of decision problems, the\nmodel allows us to find the tree width of a graph whilst delivering a tree\ndecomposition of that width, i.e. a witness.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 11:18:27 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Bumpus", "Benjamin", ""], ["Prosser", "Patrick", ""], ["Trimble", "James", ""]]}, {"id": "1908.02645", "submitter": "Melanie Schmidt", "authors": "Melanie Schmidt and Christian Sohler", "title": "Fully dynamic hierarchical diameter k-clustering and k-center", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop dynamic data structures for maintaining a hierarchical k-center\nclustering when the points come from a discrete space $\\{1,\\ldots,\\Delta\\}^d$.\nOur first data structure is for the low dimensional setting, i.e., d is a\nconstant, and processes insertions, deletions and cluster representative\nqueries in $\\log^{O(1)} (\\Delta n)$ time, where $n$ is the current size of the\npoint set. For the high dimensional case and an integer parameter $\\ell > 1$,\nwe provide a randomized data structure that maintains an $O(d\n\\ell)$-approximation. The amortized expected insertion time is $O(d^2 \\ell \\log\nn \\log \\Delta)$. The amortized expected deletion time is $O(d^2 n^{1/\\ell}\n\\log^2 n \\log \\Delta)$. At any point of time, with probability at least\n$1-1/n$, the data structure can correctly answer all queries for cluster\nrepresentatives in $O(d \\ell \\log n \\log \\Delta)$ time per query.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 14:04:23 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Schmidt", "Melanie", ""], ["Sohler", "Christian", ""]]}, {"id": "1908.02741", "submitter": "Wei Quan Lim", "authors": "Seth Gilbert, Wei Quan Lim", "title": "Parallel Finger Search Structures", "comments": "Full version of a paper published in DISC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two versions of a parallel finger structure FS on p\nprocessors that supports searches, insertions and deletions, and has a finger\nat each end. This is to our knowledge the first implementation of a parallel\nsearch structure that is work-optimal with respect to the finger bound and yet\nhas very good parallelism (within a factor of O( (log p)^2 ) of optimal). We\nutilize an extended implicit batching framework that transparently facilitates\nthe use of FS by any parallel program P that is modelled by a dynamically\ngenerated DAG D where each node is either a unit-time instruction or a call to\nFS.\n  The total work done by either version of FS is bounded by the finger bound\nF[L] (for some linearization L of D ), i.e. each operation on an item with\nfinger distance r takes O( log r + 1 ) amortized work; it is cheaper for items\ncloser to a finger. Running P using the simpler version takes O( ( T[1] + F[L]\n) / p + T[inf] + d * ( (log p)^2 + log n ) ) time on a greedy scheduler, where\nT[1],T[inf] are the size and span of D respectively, and n is the maximum\nnumber of items in FS, and d is the maximum number of calls to FS along any\npath in D. Using the faster version, this is reduced to O( ( T[1] + F[L] ) / p\n+ T[inf] + d * (log p)^2 + s[L] ) time, where s[L] is the weighted span of D\nwhere each call to FS is weighted by its cost according to F[L]. We also sketch\nhow to extend FS to support a fixed number of movable fingers.\n  The data structures in our paper fit into the dynamic multithreading\nparadigm, and their performance bounds are directly composable with other data\nstructures given in the same paradigm. Also, the results can be translated to\npractical implementations using work-stealing schedulers.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 17:35:50 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 02:59:54 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 08:49:53 GMT"}, {"version": "v4", "created": "Thu, 10 Oct 2019 17:58:10 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gilbert", "Seth", ""], ["Lim", "Wei Quan", ""]]}, {"id": "1908.03022", "submitter": "Merav Parter", "authors": "Merav Parter", "title": "Small Cuts and Connectivity Certificates: A Fault Tolerant Approach", "comments": "DISC 2019 (new version fixes minor typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit classical connectivity problems in the CONGEST model of\ndistributed computing. By using techniques from fault tolerant network design,\nwe show improved constructions, some of which are even \"local\" (i.e., with\n$\\widetilde{O}(1)$ rounds) for problems that are closely related to hard global\nproblems (i.e., with a lower bound of $\\Omega(Diam+\\sqrt{n})$ rounds).\n  Our main results are:\n  (1) For $D$-diameter unweighted graphs with constant edge connectivity, we\nshow an exact distributed deterministic computation of the minimum cut in\n$poly(D)$ rounds. This resolves one the open problems recently raised in Daga,\nHenzinger, Nanongkai and Saranurak, STOC'19.\n  (2) For $D$-diameter unweighted graphs, we present a deterministic algorithm\nthat computes of all edge connectivities up to constant in $poly(D)\\cdot\n2^{O(\\sqrt{\\log n\\log\\log n})}$ rounds.\n  (3) Computation of sparse $\\lambda$ connectivity certificates in\n$\\widetilde{O}(\\lambda)$ rounds. Previous constructions where known only for\n$\\lambda \\leq 3$ and required $O(D)$ rounds. This resolves the problem raised\nby Dori PODC'18.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 11:35:44 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 13:53:51 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Parter", "Merav", ""]]}, {"id": "1908.03042", "submitter": "Patrizio Angelini", "authors": "Patrizio Angelini, Michael A. Bekos, Michael Kaufmann, Thomas Schneck", "title": "Efficient Generation of Different Topological Representations of Graphs\n  Beyond-Planarity", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond-planarity focuses on combinatorial properties of classes of non-planar\ngraphs that allow for representations satisfying certain local geometric or\ntopological constraints on their edge crossings. Beside the study of a specific\ngraph class for its maximum edge density, another parameter that is often\nconsidered in the literature is the size of the largest complete or complete\nbipartite graph belonging to it.\n  Overcoming the limitations of standard combinatorial arguments, we present a\ntechnique to systematically generate all non-isomorphic topological\nrepresentations of complete and complete bipartite graphs, taking into account\nthe constraints of the specific class. As a proof of concept, we apply our\ntechnique to various beyond-planarity classes and achieve new tight bounds for\nthe aforementioned parameter.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 12:49:36 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 17:19:11 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Angelini", "Patrizio", ""], ["Bekos", "Michael A.", ""], ["Kaufmann", "Michael", ""], ["Schneck", "Thomas", ""]]}, {"id": "1908.03252", "submitter": "Aditya Aniruddha Shrotri", "authors": "Supratik Chakraborty, Aditya A. Shrotri, Moshe Y. Vardi", "title": "On Symbolic Approaches for Computing the Matrix Permanent", "comments": "To appear in proceedings of the 25th International Conference on\n  Principles and Practice of Constraint Programming (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the number of perfect matchings in bipartite graphs, or equivalently\ncomputing the permanent of 0-1 matrices, is an important combinatorial problem\nthat has been extensively studied by theoreticians and practitioners alike. The\npermanent is #P-Complete; hence it is unlikely that a polynomial-time algorithm\nexists for the problem. Researchers have therefore focused on finding tractable\nsubclasses of matrices for permanent computation. One such subclass that has\nreceived much attention is that of sparse matrices i.e. matrices with few\nentries set to 1, the rest being 0. For this subclass, improved theoretical\nupper bounds and practically efficient algorithms have been developed. In this\npaper, we ask whether it is possible to go beyond sparse matrices in our quest\nfor developing scalable techniques for the permanent, and answer this question\naffirmatively. Our key insight is to represent permanent computation\nsymbolically using Algebraic Decision Diagrams (ADDs). ADD-based techniques\nnaturally use dynamic programming, and hence avoid redundant computation\nthrough memoization. This permits exploiting the hidden structure in a large\nclass of matrices that have so far remained beyond the reach of permanent\ncomputation techniques. The availability of sophisticated libraries\nimplementing ADDs also makes the task of engineering practical solutions\nrelatively straightforward. While a complete characterization of matrices\nadmitting a compact ADD representation remains open, we provide strong\nexperimental evidence of the effectiveness of our approach for computing the\npermanent, not just for sparse matrices, but also for dense matrices and for\nmatrices with \"similar\" rows.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 20:01:59 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Chakraborty", "Supratik", ""], ["Shrotri", "Aditya A.", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1908.03341", "submitter": "Cyril Gavoille", "authors": "Marthe Bonamy (LaBRI), Cyril Gavoille (LaBRI), Michal Pilipczuk", "title": "Shorter Labeling Schemes for Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An \\emph{adjacency labeling scheme} for a given class of graphs is an\nalgorithm that for every graph $G$ from the class, assigns bit strings (labels)\nto vertices of $G$ so that for any two vertices $u,v$, whether $u$ and $v$ are\nadjacent can be determined by a fixed procedure that examines only their\nlabels. It is known that planar graphs with $n$ vertices admit a labeling\nscheme with labels of bit length $(2+o(1))\\log{n}$. In this work we improve\nthis bound by designing a labeling scheme with labels of bit length\n$(\\frac{4}{3}+o(1))\\log{n}$. In graph-theoretical terms, this implies an\nexplicit construction of a graph on $n^{4/3+o(1)}$ vertices that contains all\nplanar graphs on $n$ vertices as induced subgraphs, improving the previous best\nupper bound of $n^{2+o(1)}$. Our scheme generalizes to graphs of bounded Euler\ngenus with the same label length up to a second-order term. All the labels of\nthe input graph can be computed in polynomial time, while adjacency can be\ndecided from the labels in constant time.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 07:08:23 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 13:25:26 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Bonamy", "Marthe", "", "LaBRI"], ["Gavoille", "Cyril", "", "LaBRI"], ["Pilipczuk", "Michal", ""]]}, {"id": "1908.03389", "submitter": "Tesshu Hanaka", "authors": "Hiroshi Eto and Tesshu Hanaka and Yasuaki Kobayashi and Yusuke\n  Kobayashi", "title": "Parameterized Algorithms for Maximum Cut with Connectivity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two variants of \\textsc{Maximum Cut}, which we call\n\\textsc{Connected Maximum Cut} and \\textsc{Maximum Minimal Cut}, in this paper.\nIn these problems, given an unweighted graph, the goal is to compute a maximum\ncut satisfying some connectivity requirements. Both problems are known to be\nNP-complete even on planar graphs whereas \\textsc{Maximum Cut} on planar graphs\nis solvable in polynomial time. We first show that these problems are\nNP-complete even on planar bipartite graphs and split graphs. Then we give\nparameterized algorithms using graph parameters such as clique-width,\ntree-width, and twin-cover number. Finally, we obtain FPT algorithms with\nrespect to the solution size.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 09:56:48 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Eto", "Hiroshi", ""], ["Hanaka", "Tesshu", ""], ["Kobayashi", "Yasuaki", ""], ["Kobayashi", "Yusuke", ""]]}, {"id": "1908.03473", "submitter": "David Bader", "authors": "David A. Bader and Paul Burkhardt", "title": "A Linear Time Algorithm for Finding Minimum Spanning Tree Replacement\n  Edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected, weighted graph, the minimum spanning tree (MST) is a\ntree that connects all of the vertices of the graph with minimum sum of edge\nweights. In real world applications, network designers often seek to quickly\nfind a replacement edge for each edge in the MST. For example, when a traffic\naccident closes a road in a transportation network, or a line goes down in a\ncommunication network, the replacement edge may reconnect the MST at lowest\ncost. In the paper, we consider the case of finding the lowest cost replacement\nedge for each edge of the MST. A previous algorithm by Tarjan takes $O(m\n\\alpha(m, n))$ time, where $\\alpha(m, n)$ is the inverse Ackermann's function.\nGiven the MST and sorted non-tree edges, our algorithm is the first that runs\nin $O(m+n)$ time and $O(m+n)$ space to find all replacement edges. Moreover, it\nis easy to implement and our experimental study demonstrates fast performance\non several types of graphs. Additionally, since the most vital edge is the tree\nedge whose removal causes the highest cost, our algorithm finds it in linear\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 14:31:09 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 20:28:06 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 18:19:50 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Bader", "David A.", ""], ["Burkhardt", "Paul", ""]]}, {"id": "1908.03500", "submitter": "Julian Portmann", "authors": "Mohsen Ghaffari, Julian Portmann", "title": "Improved Network Decompositions using Small Messages with Applications\n  on MIS, Neighborhood Covers, and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network decompositions, as introduced by Awerbuch, Luby, Goldberg, and\nPlotkin [FOCS'89], are one of the key algorithmic tools in distributed graph\nalgorithms. We present an improved deterministic distributed algorithm for\nconstructing network decompositions of power graphs using small messages, which\nimproves upon the algorithm of Ghaffari and Kuhn [DISC'18]. In addition, we\nprovide a randomized distributed network decomposition algorithm, based on our\ndeterministic algorithm, with failure probability exponentially small in the\ninput size that works with small messages as well. Compared to the previous\nalgorithm of Elkin and Neiman [PODC'16], our algorithm achieves a better\nsuccess probability at the expense of its round complexity, while giving a\nnetwork decomposition of the same quality. As a consequence of the randomized\nalgorithm for network decomposition, we get a faster randomized algorithm for\ncomputing a Maximal Independent Set, improving on a result of Ghaffari\n[SODA'19]. Other implications of our improved deterministic network\ndecomposition algorithm are: a faster deterministic distributed algorithms for\nconstructing spanners and approximations of distributed set cover, improving\nresults of Ghaffari, and Kuhn [DISC'18] and Deurer, Kuhn, and Maus [PODC'19];\nand faster a deterministic distributed algorithm for constructing neighborhood\ncovers, resolving an open question of Elkin [SODA'04].\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 15:42:16 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Portmann", "Julian", ""]]}, {"id": "1908.03519", "submitter": "Mahmood Ettehad", "authors": "Mahmood Ettehad, Nick Duffield, and Gregory Berkolaiko", "title": "Optimizing Consistent Merging and Pruning of Subgraphs in Network\n  Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A communication network can be modeled as a directed connected graph with\nedge weights that characterize performance metrics such as loss and delay.\nNetwork tomography aims to infer these edge weights from their pathwise\nversions measured on a set of intersecting paths between a subset of boundary\nvertices, and even the underlying graph when this is not known. Recent work has\nestablished conditions under which the underlying directed graph can be\nrecovered exactly the pairwise Path Correlation Data, namely, the set of\nweights of intersection of each pair of directed paths to and from each\nendpoint. Algorithmically, this enables us to consistently fused tree-based\nview of the set of network paths to and from each endpoint to reconstruct the\nunderlying network.\n  However, in practice the PCD is not consistently determined by path\nmeasurements. Statistical fluctuations give rise to inconsistent inferred\nweight of edges from measurement based on different endpoints, as do\noperational constraints on synchronization, and deviations from the underlying\npacket transmission model. Furthermore, ad hoc solutions to eliminate noise,\nsuch as pruning small weight inferred links, are hard to apply in a consistent\nmanner that preserves known end-to-end metric values.\n  This paper takes a unified approach to the problem of inconsistent weight\nestimation. We formulate two type of inconsistency: \\textsl{intrinsic}, when\nthe weight set is internally inconsistent, and \\textsl{extrinsic}, when they\nare inconsistent with a set of known end-to-end path metrics. In both cases we\nmap inconsistent weight to consistent PCD within a least-squares framework. We\nevaluate the performance of this mapping in composition with tree-based\ninference algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 16:19:21 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ettehad", "Mahmood", ""], ["Duffield", "Nick", ""], ["Berkolaiko", "Gregory", ""]]}, {"id": "1908.03586", "submitter": "Fabrizio Frati", "authors": "Manuel Borrazzo and Fabrizio Frati", "title": "On the Planar Edge-Length Ratio of Planar Graphs", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edge-length ratio of a straight-line drawing of a graph is the ratio\nbetween the lengths of the longest and of the shortest edge in the drawing. The\nplanar edge-length ratio of a planar graph is the minimum edge-length ratio of\nany planar straight-line drawing of the graph.\n  In this paper, we study the planar edge-length ratio of planar graphs. We\nprove that there exist $n$-vertex planar graphs whose planar edge-length ratio\nis in $\\Omega(n)$; this bound is tight. We also prove upper bounds on the\nplanar edge-length ratio of several families of planar graphs, including\nseries-parallel graphs and bipartite planar graphs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 18:05:49 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 15:49:01 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 19:45:24 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Borrazzo", "Manuel", ""], ["Frati", "Fabrizio", ""]]}, {"id": "1908.03600", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Kernel for Kt-free edge deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $K_t$-free edge deletion problem, the input is a graph $G$ and an\ninteger $k$, and the goal is to decide whether there is a set of at most $k$\nedges of $G$ whose removal results a graph with no clique of size $t$. In this\npaper we give a kernel to this problem with $O(k^{t-1})$ vertices and edges.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 19:18:15 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1908.03724", "submitter": "Noah Stephens-Davidowitz", "authors": "Divesh Aggarwal, Jianwei Li, Phong Q. Nguyen, Noah Stephens-Davidowitz", "title": "Slide Reduction, Revisited---Filling the Gaps in SVP Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to generalize Gama and Nguyen's slide reduction algorithm [STOC\n'08] for solving the approximate Shortest Vector Problem over lattices (SVP).\nAs a result, we show the fastest provably correct algorithm for\n$\\delta$-approximate SVP for all approximation factors $n^{1/2+\\varepsilon}\n\\leq \\delta \\leq n^{O(1)}$. This is the range of approximation factors most\nrelevant for cryptography.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 10:43:45 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Li", "Jianwei", ""], ["Nguyen", "Phong Q.", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1908.03773", "submitter": "Vincent Delecroix", "authors": "Vincent Delecroix, Carlos Matheus, Carlos Gustavo Moreira", "title": "Approximation of the Lagrange and Markov spectra", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The (classical) Lagrange spectrum is a closed subset of the positive real\nnumbers defined in terms of diophantine approximation. Its structure is quite\ninvolved. This article describes a polynomial time algorithm to approximate it\nin Hausdorff distance. It also extends to approximate the Markov spectrum\nrelated to infimum of binary quadratic forms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 15:12:14 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 18:24:32 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Delecroix", "Vincent", ""], ["Matheus", "Carlos", ""], ["Moreira", "Carlos Gustavo", ""]]}, {"id": "1908.03870", "submitter": "Christian Komusiewicz", "authors": "Guillaume Fertin and Christian Komusiewicz", "title": "Graph Motif Problems Parameterized by Dual", "comments": "A preliminary version of this work appeared in Proceedings of the\n  27th Annual Symposium on Combinatorial Pattern Matching (CPM '16), volume 54\n  of LIPIcs, pages 7:1--7:12. This version contains all missing proofs and\n  several further improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a vertex-colored graph, where $C$ is the set of colors used\nto color $V$. The Graph Motif (or GM) problem takes as input $G$, a multiset\n$M$ of colors built from $C$, and asks whether there is a subset $S\\subseteq V$\nsuch that (i) $G[S]$ is connected and (ii) the multiset of colors obtained from\n$S$ equals $M$. The Colorful Graph Motif (or CGM) problem is the special case\nof GM in which $M$ is a set, and the List-Colored Graph Motif (or LGM) problem\nis the extension of GM in which each vertex $v$ of $V$ may choose its color\nfrom a list $\\mathcal{L}(v)\\subseteq C$ of colors.\n  We study the three problems GM, CGM, and LGM, parameterized by the dual\nparameter $\\ell:=|V|-|M|$. For general graphs, we show that, assuming the\nstrong exponential time hypothesis, CGM has no $(2-\\epsilon)^\\ell\\cdot\n|V|^{\\mathcal{O}(1)}$-time algorithm, which implies that a previous algorithm,\nrunning in $\\mathcal{O}(2^\\ell\\cdot |E|)$ time is optimal [Betzler et al.,\nIEEE/ACM TCBB 2011]. We also prove that LGM is W[1]-hard with respect to $\\ell$\neven if we restrict ourselves to lists of at most two colors. If we constrain\nthe input graph to be a tree, then we show that GM can be solved in\n$\\mathcal{O}(3^\\ell\\cdot |V|)$ time but admits no polynomial-size problem\nkernel, while CGM can be solved in $\\mathcal{O}(\\sqrt{2}^{\\ell} + |V|)$ time\nand admits a polynomial-size problem kernel.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 08:34:12 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Fertin", "Guillaume", ""], ["Komusiewicz", "Christian", ""]]}, {"id": "1908.03903", "submitter": "Tongyang Li", "authors": "Shouvanik Chakrabarti, Andrew M. Childs, Shih-Han Hung, Tongyang Li,\n  Chunhao Wang, Xiaodi Wu", "title": "Quantum algorithm for estimating volumes of convex bodies", "comments": "56 pages, 7 figures. v2: Quantum query complexity improved to\n  $\\tilde{O}(n^{3}+n^{2.5}/\\epsilon)$ and number of additional arithmetic\n  operations improved to $\\tilde{O}(n^{5}+n^{4.5}/\\epsilon)$", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the volume of a convex body is a central problem in convex\ngeometry and can be viewed as a continuous version of counting. We present a\nquantum algorithm that estimates the volume of an $n$-dimensional convex body\nwithin multiplicative error $\\epsilon$ using\n$\\tilde{O}(n^{3}+n^{2.5}/\\epsilon)$ queries to a membership oracle and\n$\\tilde{O}(n^{5}+n^{4.5}/\\epsilon)$ additional arithmetic operations. For\ncomparison, the best known classical algorithm uses\n$\\tilde{O}(n^{4}+n^{3}/\\epsilon^{2})$ queries and\n$\\tilde{O}(n^{6}+n^{5}/\\epsilon^{2})$ additional arithmetic operations. To the\nbest of our knowledge, this is the first quantum speedup for volume estimation.\nOur algorithm is based on a refined framework for speeding up simulated\nannealing algorithms that might be of independent interest. This framework\napplies in the setting of \"Chebyshev cooling\", where the solution is expressed\nas a telescoping product of ratios, each having bounded variance. We develop\nseveral novel techniques when implementing our framework, including a theory of\ncontinuous-space quantum walks with rigorous bounds on discretization error.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 13:13:47 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 22:29:31 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Chakrabarti", "Shouvanik", ""], ["Childs", "Andrew M.", ""], ["Hung", "Shih-Han", ""], ["Li", "Tongyang", ""], ["Wang", "Chunhao", ""], ["Wu", "Xiaodi", ""]]}, {"id": "1908.03948", "submitter": "Gramoz Goranci", "authors": "Gramoz Goranci, Monika Henzinger, Dariusz Leniowski, Christian Schulz,\n  Alexander Svozil", "title": "Fully Dynamic k-Center Clustering in Doubling Metrics", "comments": "This version includes an experimental evaluation of our algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most fundamental problems in unsupervised learning\nwith a large number of applications. However, classical clustering algorithms\nassume that the data is static, thus failing to capture many real-world\napplications where data is constantly changing and evolving. Driven by this, we\nstudy the metric $k$-center clustering problem in the fully dynamic setting,\nwhere the goal is to efficiently maintain a clustering while supporting an\nintermixed sequence of insertions and deletions of points. This model also\nsupports queries of the form (1) report whether a given point is a center or\n(2) determine the cluster a point is assigned to.\n  We present a deterministic dynamic algorithm for the $k$-center clustering\nproblem that provably achieves a $(2+\\epsilon)$-approximation in\npoly-logarithmic update and query time, if the underlying metric has bounded\ndoubling dimension, its aspect ratio is bounded by a polynomial and $\\epsilon$\nis a constant. An important feature of our algorithm is that the update and\nquery times are independent of $k$. We confirm the practical relevance of this\nfeature via an extensive experimental study which shows that for values of $k$\nand $\\epsilon$ suggested by theory, our algorithmic construction outperforms\nthe state-of-the-art algorithm in terms of solution quality and running time.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 18:46:25 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 20:12:46 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 03:24:35 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Goranci", "Gramoz", ""], ["Henzinger", "Monika", ""], ["Leniowski", "Dariusz", ""], ["Schulz", "Christian", ""], ["Svozil", "Alexander", ""]]}, {"id": "1908.03996", "submitter": "Joshua Brakensiek", "authors": "Joshua Brakensiek, Ray Li, Bruce Spang", "title": "Coded trace reconstruction in a constant number of traces", "comments": "34 pages, 2 figures; FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DS math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coded trace reconstruction problem asks to construct a code $C\\subset\n\\{0,1\\}^n$ such that any $x\\in C$ is recoverable from independent outputs\n(\"traces\") of $x$ from a binary deletion channel (BDC). We present binary codes\nof rate $1-\\varepsilon$ that are efficiently recoverable from\n${\\exp(O_q(\\log^{1/3}(\\frac{1}{\\varepsilon})))}$ (a constant independent of\n$n$) traces of a $\\operatorname{BDC}_q$ for any constant deletion probability\n$q\\in(0,1)$. We also show that, for rate $1-\\varepsilon$ binary codes, $\\tilde\n\\Omega(\\log^{5/2}(1/\\varepsilon))$ traces are required. The results follow from\na pair of black-box reductions that show that average-case trace reconstruction\nis essentially equivalent to coded trace reconstruction. We also show that\nthere exist codes of rate $1-\\varepsilon$ over an $O_{\\varepsilon}(1)$-sized\nalphabet that are recoverable from $O(\\log(1/\\varepsilon))$ traces, and that\nthis is tight.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 04:27:12 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 18:14:30 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 22:28:29 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Li", "Ray", ""], ["Spang", "Bruce", ""]]}, {"id": "1908.04083", "submitter": "Dominique Li", "authors": "Rui Liu and Dominique Li", "title": "An Efficient Skyline Computation Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline computation aims at looking for the set of tuples that are not worse\nthan any other tuples in all dimensions from a multidimensional database. In\nthis paper, we present SDI (Skyline on Dimension Index), a dimension indexing\nconducted general framework to skyline computation. We prove that to determine\nwhether a tuple belongs to the skyline, it is enough to compare this tuple with\na bounded subset of skyline tuples in an arbitrary dimensional index, but not\nwith all existing skyline tuples. Base on SDI, we also show that any skyline\ntuple can be used to stop the whole skyline computation process with outputting\nthe complete set of all skyline tuples. We develop an efficient algorithm\nSDI-RS that significantly reduces the skyline computation time, of which the\nspace and time complexity can be guaranteed. Our experimental evaluation shows\nthat SDI-RS outperforms the baseline algorithms in general and is especially\nvery efficient on high-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 10:57:01 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Liu", "Rui", ""], ["Li", "Dominique", ""]]}, {"id": "1908.04104", "submitter": "Sven Mallach", "authors": "Sven Mallach", "title": "A Natural Quadratic Approach to the Generalized Graph Layering Problem", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new exact approach to the generalized graph layering problem\nthat is based on a particular quadratic assignment formulation. It expresses,\nin a natural way, the associated layout restrictions and several possible\nobjectives, such as a minimum total arc length, minimum number of reversed\narcs, and minimum width, or the adaptation to a specific drawing area. Our\ncomputational experiments show a competitive performance compared to prior\nexact models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 11:59:42 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Mallach", "Sven", ""]]}, {"id": "1908.04141", "submitter": "Alexander Noe", "authors": "Monika Henzinger, Alexander Noe and Christian Schulz", "title": "Shared-Memory Branch-and-Reduce for Multiterminal Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the fastest known exact algorithm~for~the multiterminal cut\nproblem with k terminals. In particular, we engineer existing as well as new\ndata reduction rules. We use the rules within a branch-and-reduce framework and\nto boost the performance of an ILP formulation. Our algorithms achieve\nimprovements in running time of up to multiple orders of magnitudes over the\nILP formulation without data reductions, which has been the de facto standard\nused by practitioners. This allows us to solve instances to optimality that are\nsignificantly larger than was previously possible.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 13:26:57 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 18:32:14 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Henzinger", "Monika", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""]]}, {"id": "1908.04196", "submitter": "Gopinath Mishra", "authors": "Anup Bhattacharya, Arijit Bishnu, Arijit Ghosh and Gopinath Mishra", "title": "Hyperedge Estimation using Polylogarithmic Subset Queries", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we estimate the number of hyperedges in a hypergraph ${\\cal\nH}(U({\\cal H}), {\\cal F}({\\cal H}))$, where $U({\\cal H})$ denotes the set of\nvertices and ${\\cal F}({\\cal H}))$ denotes the set of hyperedges. We assume a\nquery oracle access to the hypergraph ${\\cal H}$. Estimating the number of\nedges, triangles or small subgraphs in a graph is a well studied problem. Beame\n\\etal~and Bhattacharya \\etal~gave algorithms to estimate the number of edges\nand triangles in a graph using queries to the {\\sc Bipartite Independent Set}\n({\\sc BIS}) and the {\\sc Tripartite Independent Set} ({\\sc TIS}) oracles,\nrespectively. We generalize the earlier works by estimating the number of\nhyperedges using a query oracle, known as the {\\bf Generalized $d$-partite\nindependent set oracle ({\\sc GPIS})}, that takes $d$ (non-empty) pairwise\ndisjoint subsets of vertices $A_1,\\ldots,A_d \\subseteq U({\\cal H})$ as input,\nand answers whether there exists a hyperedge in ${\\cal H}$ having (exactly) one\nvertex in each $A_i, i \\in \\{1,2,\\ldots,d\\}$. We give a randomized algorithm\nfor the hyperedge estimation problem using the {\\sc GPIS} query oracle to\noutput $\\widehat{m}$ for $m({\\cal H})$ satisfying $(1-\\epsilon) \\cdot m({\\cal\nH}) \\leq \\widehat{m} \\leq (1+\\epsilon) \\cdot m({\\cal H})$. The number of\nqueries made by our algorithm, assuming $d$ to be a constant, is\npolylogarithmic in the number of vertices of the hypergraph.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 15:30:08 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 19:51:30 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 19:50:29 GMT"}, {"version": "v4", "created": "Sat, 5 Sep 2020 20:25:32 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Bhattacharya", "Anup", ""], ["Bishnu", "Arijit", ""], ["Ghosh", "Arijit", ""], ["Mishra", "Gopinath", ""]]}, {"id": "1908.04263", "submitter": "Md. Helal Ahmed", "authors": "Md. Helal Ahmed, Dr. Jagmohan Tanti and Sumant Pushp", "title": "Computation of Jacobi sums of order l^2 and 2l^2 with prime l", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CR cs.DS math.RA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present the fast computational algorithms for the Jacobi\nsums of orders $l^2$ and $2l^{2}$ with odd prime $l$ by formulating them in\nterms of the minimum number of cyclotomic numbers of the corresponding orders.\nWe also implement two additional algorithms to validate these formulae, which\nare also useful for the demonstration of the minimality of cyclotomic numbers\nrequired.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 11:14:30 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 17:29:23 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ahmed", "Md. Helal", ""], ["Tanti", "Dr. Jagmohan", ""], ["Pushp", "Sumant", ""]]}, {"id": "1908.04381", "submitter": "Jeffrey M. Dudek", "authors": "Jeffrey M. Dudek, Leonardo Due\\~nas-Osorio, Moshe Y. Vardi", "title": "Efficient Contraction of Large Tensor Networks for Weighted Model\n  Counting through Graph Decompositions", "comments": "Submitted to AIJ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained counting is a fundamental problem in artificial intelligence. A\npromising new algebraic approach to constrained counting makes use of tensor\nnetworks, following a reduction from constrained counting to the problem of\ntensor-network contraction. Contracting a tensor network efficiently requires\ndetermining an efficient order to contract the tensors inside the network,\nwhich is itself a difficult problem.\n  In this work, we apply graph decompositions to find contraction orders for\ntensor networks. We prove that finding an efficient contraction order for a\ntensor network is equivalent to the well-known problem of finding an optimal\ncarving decomposition. Thus memory-optimal contraction orders for planar tensor\nnetworks can be found in cubic time. We show that tree decompositions can be\nused both to find carving decompositions and to factor tensor networks with\nhigh-rank, structured tensors.\n  We implement these algorithms on top of state-of-the-art solvers for tree\ndecompositions and show empirically that the resulting weighted model counter\nis quite effective and useful as part of a portfolio of counters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 21:01:49 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 23:39:11 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Dudek", "Jeffrey M.", ""], ["Due\u00f1as-Osorio", "Leonardo", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1908.04452", "submitter": "Adel Kacem Dr", "authors": "Adel Kacem and Abdelaziz Dammak", "title": "Multi-objective scheduling on two dedicated processors", "comments": "submitted to springer 2019", "journal-ref": "2021", "doi": "10.1007/s11750-020-00588-5", "report-no": "588", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a multi-objective scheduling problem on two dedicated processors.\nThe aim is to minimize simultaneously the makespan, the total tardiness and the\ntotal completion time. This NP-hard problem requires the use of well-adapted\nmethods. For this, we adapted genetic algorithms to multi-objective case. Three\nmethods are presented to solve this problem. The first is aggregative, the\nsecond is Pareto and the third is non-dominated sorting genetic algorithm II\n(NSGA-II). We proposed some adapted lower bounds for each criterion to evaluate\nthe quality of the found results on a large set of instances. Indeed, these\nbounds also make it possible to determine the dominance of one algorithm over\nanother based on the different results found by each of them. We used two\nmetrics to measure the quality of the Pareto front: the hypervolume indicator\n(HV) and the number of solutions in the optimal front (ND). The obtained\nresults show the effectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 01:18:05 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 15:13:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kacem", "Adel", ""], ["Dammak", "Abdelaziz", ""]]}, {"id": "1908.04468", "submitter": "Fred Zhang", "authors": "Zhixian Lei, Kyle Luh, Prayaag Venkat, Fred Zhang", "title": "A Fast Spectral Algorithm for Mean Estimation with Sub-Gaussian Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the algorithmic problem of estimating the mean of heavy-tailed\nrandom vector in $\\mathbb{R}^d$, given $n$ i.i.d. samples. The goal is to\ndesign an efficient estimator that attains the optimal sub-gaussian error\nbound, only assuming that the random vector has bounded mean and covariance.\nPolynomial-time solutions to this problem are known but have high runtime due\nto their use of semi-definite programming (SDP). Conceptually, it remains open\nwhether convex relaxation is truly necessary for this problem.\n  In this work, we show that it is possible to go beyond SDP and achieve better\ncomputational efficiency. In particular, we provide a spectral algorithm that\nachieves the optimal statistical performance and runs in time $\\widetilde\nO\\left(n^2 d \\right)$, improving upon the previous fastest runtime $\\widetilde\nO\\left(n^{3.5}+ n^2d\\right)$ by Cherapanamjeri el al. (COLT '19). Our algorithm\nis spectral in that it only requires (approximate) eigenvector computations,\nwhich can be implemented very efficiently by, for example, power iteration or\nthe Lanczos method.\n  At the core of our algorithm is a novel connection between the furthest\nhyperplane problem introduced by Karnin et al. (COLT '12) and a structural\nlemma on heavy-tailed distributions by Lugosi and Mendelson (Ann. Stat. '19).\nThis allows us to iteratively reduce the estimation error at a geometric rate\nusing only the information derived from the top singular vector of the data\nmatrix, leading to a significantly faster running time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 02:56:01 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 22:56:41 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Lei", "Zhixian", ""], ["Luh", "Kyle", ""], ["Venkat", "Prayaag", ""], ["Zhang", "Fred", ""]]}, {"id": "1908.04486", "submitter": "Ziqi Yan", "authors": "Ziqi Yan, Gang Li, Jiqiang Liu", "title": "Private Rank Aggregation under Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a method for answer aggregation in crowdsourced data management, rank\naggregation aims to combine different agents' answers or preferences over the\ngiven alternatives into an aggregate ranking which agrees the most with the\npreferences. However, since the aggregation procedure relies on a data curator,\nthe privacy within the agents' preference data could be compromised when the\ncurator is untrusted. Existing works that guarantee differential privacy in\nrank aggregation all assume that the data curator is trusted. In this paper, we\nformulate and address the problem of locally differentially private rank\naggregation, in which the agents have no trust in the data curator. By\nleveraging the approximate rank aggregation algorithm KwikSort, the Randomized\nResponse mechanism, and the Laplace mechanism, we propose an effective and\nefficient protocol LDP-KwikSort. Theoretical and empirical results show that\nthe solution LDP-KwikSort:RR can achieve the acceptable trade-off between the\nutility of aggregate ranking and the privacy protection of agents' pairwise\npreferences.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 04:46:10 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 04:14:25 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 10:17:26 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Yan", "Ziqi", ""], ["Li", "Gang", ""], ["Liu", "Jiqiang", ""]]}, {"id": "1908.04517", "submitter": "Zhi-Hong Deng", "authors": "Zhi-Hong Deng", "title": "Beyond the Inverted Index", "comments": "9 pages, 7 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new data structure named group-list is proposed. The\ngroup-list is as simple as the inverted index. However, the group-list divides\ndocument identifiers in an inverted index into groups, which makes it more\nefficient when it is used to perform the intersection or union operation on\ndocument identifiers. The experimental results on a synthetic dataset show that\nthe group-list outperforms the inverted index.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 07:14:01 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Deng", "Zhi-Hong", ""]]}, {"id": "1908.04628", "submitter": "Xindi Wang", "authors": "Xindi Wang, Onur Varol, Tina Eliassi-Rad", "title": "L2P: An Algorithm for Estimating Heavy-tailed Outcomes", "comments": "9 pages, 6 figures, 2 tables Nature of changes from previous version:\n  1. Added complexity analysis in Section 2.2 2. Datasets change 3. Added\n  LambdaMART in the baseline methods, also a brief discussion on why LambdaMart\n  failed in our problem. 4. Figure updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world prediction tasks have outcome variables that have\ncharacteristic heavy-tail distributions. Examples include copies of books sold,\nauction prices of art pieces, demand for commodities in warehouses, etc. By\nlearning heavy-tailed distributions, \"big and rare\" instances (e.g., the\nbest-sellers) will have accurate predictions. Most existing approaches are not\ndedicated to learning heavy-tailed distribution; thus, they heavily\nunder-predict such instances. To tackle this problem, we introduce Learning to\nPlace (L2P), which exploits the pairwise relationships between instances for\nlearning. In its training phase, L2P learns a pairwise preference classifier:\nis instance A > instance B? In its placing phase, L2P obtains a prediction by\nplacing the new instance among the known instances. Based on its placement, the\nnew instance is then assigned a value for its outcome variable. Experiments on\nreal data show that L2P outperforms competing approaches in terms of accuracy\nand ability to reproduce heavy-tailed outcome distribution. In addition, L2P\nprovides an interpretable model by placing each predicted instance in relation\nto its comparable neighbors. Interpretable models are highly desirable when\nlives and treasure are at stake.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 13:20:50 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 13:15:46 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wang", "Xindi", ""], ["Varol", "Onur", ""], ["Eliassi-Rad", "Tina", ""]]}, {"id": "1908.04673", "submitter": "L\\'aszl\\'o Kozma", "authors": "Benjamin Aram Berendsohn, L\\'aszl\\'o Kozma, D\\'aniel Marx", "title": "Finding and counting permutations via CSPs", "comments": "supersedes arXiv:1902.08809, with possible text overlap", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation patterns and pattern avoidance have been intensively studied in\ncombinatorics and computer science, going back at least to the seminal work of\nKnuth on stack-sorting (1968). Perhaps the most natural algorithmic question in\nthis area is deciding whether a given permutation of length $n$ contains a\ngiven pattern of length $k$.\n  In this work we give two new algorithms for this well-studied problem, one\nwhose running time is $n^{k/4 + o(k)}$, and a polynomial-space algorithm whose\nrunning time is the better of $O(1.6181^n)$ and $O(n^{k/2 + 1})$. These results\nimprove the earlier best bounds of $n^{0.47k + o(k)}$ and $O(1.79^n)$ due to\nAhal and Rabinovich (2000) resp. Bruner and Lackner (2012) and are the fastest\nalgorithms for the problem when $k \\in \\Omega(\\log{n})$. We show that both our\nnew algorithms and the previous exponential-time algorithms in the literature\ncan be viewed through the unifying lens of constraint-satisfaction.\n  Our algorithms can also count, within the same running time, the number of\noccurrences of a pattern. We show that this result is close to optimal: solving\nthe counting problem in time $f(k) \\cdot n^{o(k/\\log{k})}$ would contradict the\nexponential-time hypothesis (ETH). For some special classes of patterns we\nobtain improved running times. We further prove that $3$-increasing and\n$3$-decreasing permutations can, in some sense, embed arbitrary permutations of\nalmost linear length, which indicates that an algorithm with sub-exponential\nrunning time is unlikely, even for patterns from these restricted classes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 14:46:25 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Berendsohn", "Benjamin Aram", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1908.04686", "submitter": "Nicola Prezza", "authors": "Nicola Prezza and Giovanna Rosone", "title": "Space-Efficient Construction of Compressed Suffix Trees", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.05226", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to build several data structures of central importance to string\nprocessing, taking as input the Burrows-Wheeler transform (BWT) and using small\nextra working space. Let $n$ be the text length and $\\sigma$ be the alphabet\nsize. We first provide two algorithms that enumerate all LCP values and suffix\ntree intervals in $O(n\\log\\sigma)$ time using just $o(n\\log\\sigma)$ bits of\nworking space on top of the input BWT. Using these algorithms as building\nblocks, for any parameter $0 < \\epsilon \\leq 1$ we show how to build the PLCP\nbitvector and the balanced parentheses representation of the suffix tree\ntopology in $O\\left(n(\\log\\sigma + \\epsilon^{-1}\\cdot \\log\\log n)\\right)$ time\nusing at most $n\\log\\sigma \\cdot(\\epsilon + o(1))$ bits of working space on top\nof the input BWT and the output. In particular, this implies that we can build\na compressed suffix tree from the BWT using just succinct working space (i.e.\n$o(n\\log\\sigma)$ bits) and any time in $\\Theta(n\\log\\sigma) + \\omega(n\\log\\log\nn)$. This improves the previous most space-efficient algorithms, which worked\nin $O(n)$ bits and $O(n\\log n)$ time. We also consider the problem of merging\nBWTs of string collections, and provide a solution running in $O(n\\log\\sigma)$\ntime and using just $o(n\\log\\sigma)$ bits of working space. An efficient\nimplementation of our LCP construction and BWT merge algorithms use (in RAM) as\nfew as $n$ bits on top of a packed representation of the input/output and\nprocess data as fast as $2.92$ megabases per second.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 10:58:49 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Prezza", "Nicola", ""], ["Rosone", "Giovanna", ""]]}, {"id": "1908.04810", "submitter": "Jonathan Burns", "authors": "Jonathan Burns", "title": "On Occupancy Moments and Bloom Filter Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two multivariate committee distributions are shown to belong to Berg's family\nof factorial series distributions and Kemp's family of generalized\nhypergeometric factorial moment distributions. Exact moment formulas, upper and\nlower bounds, and statistical parameter estimators are provided for the classic\noccupancy and committee distributions. The derived moment equations are used to\ndetermine exact formulas for the false-positive rate and efficiency of Bloom\nfilters -- probabilistic data structures used to solve the set membership\nproblem. This study reveals that the conventional Bloom filter analysis\noverestimates the number of hash functions required to minimize the\nfalse-positive rate, and shows that Bloom filter efficiency is monotonic in the\nnumber of hash functions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 18:21:04 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Burns", "Jonathan", ""]]}, {"id": "1908.04933", "submitter": "Dominik K\\\"oppl", "authors": "Dominik K\\\"oppl and Tomohiro I and Isamu Furuya and Yoshimasa\n  Takabatake and Kensuke Sakai and Keisuke Goto", "title": "Re-Pair In Small Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-Pair is a grammar compression scheme with favorably good compression\nrates. The computation of Re-Pair comes with the cost of maintaining large\nfrequency tables, which makes it hard to compute Re-Pair on large scale data\nsets. As a solution for this problem we present, given a text of length $n$\nwhose characters are drawn from an integer alphabet, an $O(n^2) \\cap O(n^2 \\lg\n\\log_\\tau n \\lg \\lg \\lg n / \\log_\\tau n)$ time algorithm computing Re-Pair in\n$n \\lg \\max(n,\\tau)$ bits of space including the text space, where $\\tau$ is\nthe number of terminals and non-terminals. The algorithm works in the restore\nmodel, supporting the recovery of the original input in the time for the\nRe-Pair computation with $O(\\lg n)$ additional bits of working space. We give\nvariants of our solution working in parallel or in the external memory model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 02:42:36 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 03:38:43 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 07:48:28 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["K\u00f6ppl", "Dominik", ""], ["I", "Tomohiro", ""], ["Furuya", "Isamu", ""], ["Takabatake", "Yoshimasa", ""], ["Sakai", "Kensuke", ""], ["Goto", "Keisuke", ""]]}, {"id": "1908.04937", "submitter": "Siwoo Song", "authors": "Siwoo Song, Cheol Ryu, Simone Faro, Thierry Lecroq, Kunsoo Park", "title": "Fast Cartesian Tree Matching", "comments": "14 pages, 3 figures, Submitted to SPIRE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartesian tree matching is the problem of finding all substrings of a given\ntext which have the same Cartesian trees as that of a given pattern. So far\nthere is one linear-time solution for Cartesian tree matching, which is based\non the KMP algorithm. We improve the running time of the previous solution by\nintroducing new representations. We present the framework of a binary\nfiltration method and an efficient verification technique for Cartesian tree\nmatching. Any exact string matching algorithm can be used as a filtration for\nCartesian tree matching on our framework. We also present a SIMD solution for\nCartesian tree matching suitable for short patterns. By experiments we show\nthat known string matching algorithms combined on our framework of binary\nfiltration and efficient verification produce algorithms of good performances\nfor Cartesian tree matching.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 03:14:21 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Song", "Siwoo", ""], ["Ryu", "Cheol", ""], ["Faro", "Simone", ""], ["Lecroq", "Thierry", ""], ["Park", "Kunsoo", ""]]}, {"id": "1908.04976", "submitter": "Sanjay Subramanian", "authors": "Barna Saha and Sanjay Subramanian", "title": "Correlation Clustering with Same-Cluster Queries Bounded by Optimal Cost", "comments": "ESA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several clustering frameworks with interactive (semi-supervised) queries have\nbeen studied in the past. Recently, clustering with same-cluster queries has\nbecome popular. An algorithm in this setting has access to an oracle with full\nknowledge of an optimal clustering, and the algorithm can ask the oracle\nqueries of the form, \"Does the optimal clustering put vertices $ u $ and $ v $\nin the same cluster?\" Due to its simplicity, this querying model can easily be\nimplemented in real crowd-sourcing platforms and has attracted a lot of recent\nwork. In this paper, we study the popular correlation clustering problem\n(Bansal et al., 2002) under this framework. Given a complete graph $G=(V,E)$\nwith positive and negative edge labels, correlation clustering objective aims\nto compute a graph clustering that minimizes the total number of disagreements,\nthat is the negative intra-cluster edges and positive inter-cluster edges. Let\n$ C_{OPT} $ be the number of disagreements made by the optimal clustering. We\npresent algorithms for correlation clustering whose error and query bounds are\nparameterized by $C_{OPT}$ rather than by the number of clusters. Indeed, a\ngood clustering must have small $C_{OPT}$. Specifically, we present an\nefficient algorithm that recovers an exact optimal clustering using at most\n$2C_{OPT} $ queries and an efficient algorithm that outputs a $2$-approximation\nusing at most $C_{OPT} $ queries. In addition, we show under a plausible\ncomplexity assumption, there does not exist any polynomial time algorithm that\nhas an approximation ratio better than $1+\\alpha$ for an absolute constant\n$\\alpha >0$ with $o(C_{OPT})$ queries. We extensively evaluate our methods on\nseveral synthetic and real-world datasets using real crowd-sourced oracles.\nMoreover, we compare our approach against several known correlation clustering\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 06:26:15 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Saha", "Barna", ""], ["Subramanian", "Sanjay", ""]]}, {"id": "1908.05015", "submitter": "Fabrizio Montecchiani", "authors": "Emilio Di Giacomo, Giuseppe Liotta, Fabrizio Montecchiani", "title": "Sketched Representations and Orthogonal Planarity of Bounded Treewidth\n  Graphs", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a planar graph $G$ and an integer $b$, OrthogonalPlanarity is the\nproblem of deciding whether $G$ admits an orthogonal drawing with at most $b$\nbends in total. We show that OrthogonalPlanarity can be solved in polynomial\ntime if $G$ has bounded treewidth. Our proof is based on an FPT algorithm whose\nparameters are the number of bends, the treewidth and the number of degree-2\nvertices of $G$. This result is based on the concept of sketched orthogonal\nrepresentation that synthetically describes a family of equivalent orthogonal\nrepresentations. Our approach can be extended to related problems such as\nHV-Planarity and FlexDraw. In particular, both OrthogonalPlanarity and\nHV-Planarity can be decided in $O(n^3 \\log n)$ time for series-parallel graphs,\nwhich improves over the previously known $O(n^4)$ bounds.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 08:33:55 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Di Giacomo", "Emilio", ""], ["Liotta", "Giuseppe", ""], ["Montecchiani", "Fabrizio", ""]]}, {"id": "1908.05082", "submitter": "Iago Carvalho M.Sc.", "authors": "Iago A. Carvalho, Thiago F. Noronha, Christophe Duhamel", "title": "Algorithms the min-max regret 0-1 Integer Linear Programming Problem\n  with Interval Data", "comments": "3 pages, 1 table. Published at Metaheuristics International\n  Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the Interval Data Min-Max Regret 0-1 Integer Linear Programming\nproblem (MMR-ILP), a variant of the 0-1 Integer Linear Programming problem\nwhere the objective function coefficients are uncertain. We solve MMR-ILP using\na Benders-like Decomposition Algorithm and two metaheuristics for min-max\nregret problems with interval data. Computational experiments developed on\nvariations of MIPLIB instances show that the heuristics obtain good results in\na reasonable computational time when compared to the Benders-like Decomposition\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 11:46:37 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Carvalho", "Iago A.", ""], ["Noronha", "Thiago F.", ""], ["Duhamel", "Christophe", ""]]}, {"id": "1908.05198", "submitter": "Anders Aamand", "authors": "Anders Aamand, Piotr Indyk, Ali Vakilian", "title": "(Learned) Frequency Estimation Algorithms under Zipfian Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\begin{abstract} The frequencies of the elements in a data stream are an\nimportant statistical measure and the task of estimating them arises in many\napplications within data analysis and machine learning. Two of the most popular\nalgorithms for this problem, Count-Min and Count-Sketch, are widely used in\npractice.\n  In a recent work [Hsu et al., ICLR'19], it was shown empirically that\naugmenting Count-Min and Count-Sketch with a machine learning algorithm leads\nto a significant reduction of the estimation error. The experiments were\ncomplemented with an analysis of the expected error incurred by Count-Min (both\nthe standard and the augmented version) when the input frequencies follow a\nZipfian distribution. Although the authors established that the learned version\nof Count-Min has lower estimation error than its standard counterpart, their\nanalysis of the standard Count-Min algorithm was not tight. Moreover, they\nprovided no similar analysis for Count-Sketch.\n  In this paper we resolve these problems. First, we provide a simple tight\nanalysis of the expected error incurred by Count-Min. Second, we provide the\nfirst error bounds for both the standard and the augmented version of\nCount-Sketch. These bounds are nearly tight and again demonstrate an improved\nperformance of the learned version of Count-Sketch.\n  In addition to demonstrating tight gaps between the aforementioned\nalgorithms, we believe that our bounds for the standard versions of Count-Min\nand Count-Sketch are of independent interest. In particular, it is a typical\npractice to set the number of hash functions in those algorithms to $\\Theta\n(\\log n)$. In contrast, our results show that to minimize the \\emph{expected}\nerror, the number of hash functions should be a constant, strictly greater than\n$1$.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 16:17:05 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 15:58:29 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Aamand", "Anders", ""], ["Indyk", "Piotr", ""], ["Vakilian", "Ali", ""]]}, {"id": "1908.05415", "submitter": "Matthew Gray", "authors": "Matthew Gray", "title": "\"LOADS of Space\": Local Order Agnosticism and Bit Flip Efficient Data\n  Structure Codes", "comments": "This was an Undergraduate Thesis written in the winter and spring of\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithms, data structures, coding techniques, and other methods that reduce\nbit-flips are being sought to best utilize hardware where flipping bits is the\ndominating cost. Write efficient memories were introduced by Ahlswede and Zhang\nas a model for storage systems with these kinds of arbitrary read, write, and\nupdate costs. The introduction of non-volatile Random Access Memories like\nphase-change RAM, which have asymmetric read-write costs has re-motivated the\nfield. Our work focuses on potential bit-flip efficiencies to be gained at the\ndata structure layer. We examine Local Order Agnostic Data Structures (LOADS),\ndata structures in which local order does not convey information and in which\ncells are modified individually. We found that because these data structures\nhave a limited set of valid values and transitions, that bit flipping wins\nshould be possible without the use of additional hardware.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 04:31:58 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Gray", "Matthew", ""]]}, {"id": "1908.05445", "submitter": "Pedro Matias", "authors": "David Eppstein, Michael T. Goodrich, James A. Liu, Pedro Matias", "title": "Tracking Paths in Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the NP-complete problem of tracking paths in a graph, first\nintroduced by Banik et. al. [3]. Given an undirected graph with a source $s$\nand a destination $t$, find the smallest subset of vertices whose intersection\nwith any $s-t$ path results in a unique sequence. In this paper, we show that\nthis problem remains NP-complete when the graph is planar and we give a\n4-approximation algorithm in this setting. We also show, via Courcelle's\ntheorem, that it can be solved in linear time for graphs of bounded-clique\nwidth, when its clique decomposition is given in advance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 07:13:20 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 02:39:23 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Liu", "James A.", ""], ["Matias", "Pedro", ""]]}, {"id": "1908.05666", "submitter": "Konstantinos Konstantinidis", "authors": "Konstantinos Konstantinidis and Aditya Ramamoorthy", "title": "Resolvable Designs for Speeding up Distributed Computing", "comments": "14 pages, 3 figures, full paper for IEEE TON submission. arXiv admin\n  note: substantial text overlap with arXiv:1802.03049, arXiv:1901.07418", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing frameworks such as MapReduce are often used to process\nlarge computational jobs. They operate by partitioning each job into smaller\ntasks executed on different servers. The servers also need to exchange\nintermediate values to complete the computation. Experimental evidence suggests\nthat this so-called Shuffle phase can be a significant part of the overall\nexecution time for several classes of jobs. Prior work has demonstrated a\nnatural tradeoff between computation and communication whereby running\nredundant copies of jobs can reduce the Shuffle traffic load, thereby leading\nto reduced overall execution times. For a single job, the main drawback of this\napproach is that it requires the original job to be split into a number of\nfiles that grows exponentially in the system parameters. When extended to\nmultiple jobs (with specific function types), these techniques suffer from a\nlimitation of a similar flavor, i.e., they require an exponentially large\nnumber of jobs to be executed. In practical scenarios, these requirements can\nsignificantly reduce the promised gains of the method. In this work, we show\nthat a class of combinatorial structures called resolvable designs can be used\nto develop efficient coded distributed computing schemes for both the single\nand multiple job scenarios considered in prior work. We present both\ntheoretical analysis and exhaustive experimental results (on Amazon EC2\nclusters) that demonstrate the performance advantages of our method. For the\nsingle and multiple job cases, we obtain speed-ups of 4.69x (and 2.6x over\nprior work) and 4.31x over the baseline approach, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:02:28 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 18:14:16 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 00:36:27 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Konstantinidis", "Konstantinos", ""], ["Ramamoorthy", "Aditya", ""]]}, {"id": "1908.05700", "submitter": "Gal Oren", "authors": "Leonid Barenboim, Gal Oren", "title": "Distributed Backup Placement in One Round and its Applications to\n  Maximum Matching Approximation and Self-Stabilization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the distributed backup-placement problem each node of a network has to\nselect one neighbor, such that the maximum number of nodes that make the same\nselection is minimized. This is a natural relaxation of the perfect matching\nproblem, in which each node is selected just by one neighbor. Previous\n(approximate) solutions for backup placement are non-trivial, even for simple\ngraph topologies, such as dense graphs. In this paper we devise an algorithm\nfor dense graph topologies, including unit disk graphs, unit ball graphs, line\ngraphs, graphs with bounded diversity, and many more. Our algorithm requires\njust one round, and is as simple as the following operation. Consider a\ncircular list of neighborhood IDs, sorted in an ascending order, and select the\nID that is next to the selecting vertex ID. Surprisingly, such a simple\none-round strategy turns out to be very efficient for backup placement\ncomputation in dense networks. Not only that it improves the number of rounds\nof the solution, but also the approximation ratio is improved by a\nmultiplicative factor of at least $2$.\n  Our new algorithm has several interesting implications. In particular, it\ngives rise to a $(2 + \\epsilon)$-approximation to maximum matching within\n$O(\\log^* n)$ rounds in dense networks. The resulting algorithm is very simple\nas well, in sharp contrast to previous algorithms that compute such a solution\nwithin this running time. Moreover, these algorithms are applicable to a\nnarrower graph family than our algorithm. For the same graph family, the best\npreviously-known result has $O(\\log {\\Delta} + \\log^* n)$ running time. Another\ninteresting implication is the possibility to execute our backup placement\nalgorithm as-is in the self-stabilizing setting. This makes it possible to\nsimplify and improve other algorithms for the self-stabilizing setting, by\nemploying helpful properties of backup placement.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 18:31:16 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Barenboim", "Leonid", ""], ["Oren", "Gal", ""]]}, {"id": "1908.05706", "submitter": "David Eppstein", "authors": "Therese Biedl, Erin Wolf Chambers, David Eppstein, Arnaud De Mesmay,\n  and Tim Ophelders", "title": "Homotopy height, grid-major height and graph-drawing height", "comments": "28 pages, 11 figures. Expanded version of a paper in the Proceedings\n  of the 27th International Symposium on Graph Drawing and Network\n  Visualization (GD 2019); for the proceedings version, see version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well-known that both the pathwidth and the outer-planarity of a graph\ncan be used to obtain lower bounds on the height of a planar straight-line\ndrawing of a graph. But both bounds fall short for some graphs. In this paper,\nwe consider two other parameters, the (simple) homotopy height and the (simple)\ngrid-major height. We discuss the relationship between them and to the other\nparameters, and argue that they give lower bounds on the straight-line drawing\nheight that are never worse than the ones obtained from pathwidth and\nouter-planarity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 18:48:31 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 23:32:36 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Biedl", "Therese", ""], ["Chambers", "Erin Wolf", ""], ["Eppstein", "David", ""], ["De Mesmay", "Arnaud", ""], ["Ophelders", "Tim", ""]]}, {"id": "1908.05855", "submitter": "Masatoshi Hanai", "authors": "Masatoshi Hanai, Toyotaro Suzumura, Wen Jun Tan, Elvis Liu, Georgios\n  Theodoropoulos and Wentong Cai", "title": "Distributed Edge Partitioning for Trillion-edge Graphs", "comments": "VLDB 2020, Code in http://www.masahanai.jp/DistributedNE/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Distributed Neighbor Expansion (Distributed NE), a parallel and\ndistributed graph partitioning method that can scale to trillion-edge graphs\nwhile providing high partitioning quality. Distributed NE is based on a new\nheuristic, called parallel expansion, where each partition is constructed in\nparallel by greedily expanding its edge set from a single vertex in such a way\nthat the increase of the vertex cuts becomes local minimal. We theoretically\nprove that the proposed method has the upper bound in the partitioning quality.\nThe empirical evaluation with various graphs shows that the proposed method\nproduces higher-quality partitions than the state-of-the-art distributed graph\npartitioning algorithms. The performance evaluation shows that the space\nefficiency of the proposed method is an order-of-magnitude better than the\nexisting algorithms, keeping its time efficiency comparable. As a result,\nDistributed NE can partition a trillion-edge graph using only 256 machines\nwithin 70 minutes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 05:52:19 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 14:18:04 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Hanai", "Masatoshi", ""], ["Suzumura", "Toyotaro", ""], ["Tan", "Wen Jun", ""], ["Liu", "Elvis", ""], ["Theodoropoulos", "Georgios", ""], ["Cai", "Wentong", ""]]}, {"id": "1908.05930", "submitter": "Simone Faro", "authors": "Simone Faro, Arianna Pavone, Francesco Pio Marino", "title": "Efficient Online String Matching Based on Characters Distance Text\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for all occurrences of a pattern in a text is a fundamental problem\nin computer science with applications in many other fields, like natural\nlanguage processing, information retrieval and computational biology. Sampled\nstring matching is an efficient approach recently introduced in order to\novercome the prohibitive space requirements of an index construction, on the\none hand, and drastically reduce searching time for the online solutions, on\nthe other hand. In this paper we present a new algorithm for the sampled string\nmatching problem, based on a characters distance sampling approach. The main\nidea is to sample the distances between consecutive occurrences of a given\npivot character and then to search online the sampled data for any occurrence\nof the sampled pattern, before verifying the original text. From a theoretical\npoint of view we prove that, under suitable conditions, our solution can\nachieve both linear worst-case time complexity and optimal average-time\ncomplexity. From a practical point of view it turns out that our solution shows\na sub-linear behaviour in practice and speeds up online searching by a factor\nof up to 9, using limited additional space whose amount goes from 11% to 2.8%\nof the text size, with a gain up to 50% if compared with previous solutions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 11:11:06 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Faro", "Simone", ""], ["Pavone", "Arianna", ""], ["Marino", "Francesco Pio", ""]]}, {"id": "1908.06059", "submitter": "Alice Cortinovis", "authors": "Alice Cortinovis and Daniel Kressner", "title": "Low-rank approximation in the Frobenius norm by column and row subset\n  selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A CUR approximation of a matrix $A$ is a particular type of low-rank\napproximation $A \\approx C U R$, where $C$ and $R$ consist of columns and rows\nof $A$, respectively. One way to obtain such an approximation is to apply\ncolumn subset selection to $A$ and $A^T$. In this work, we describe a\nnumerically robust and much faster variant of the column subset selection\nalgorithm proposed by Deshpande and Rademacher, which guarantees an error close\nto the best approximation error in the Frobenius norm. For cross approximation,\nin which $U$ is required to be the inverse of a submatrix of $A$ described by\nthe intersection of $C$ and $R$, we obtain a new algorithm with an error bound\nthat stays within a factor $k + 1$ of the best rank-$k$ approximation error in\nthe Frobenius norm. To the best of our knowledge, this is the first\ndeterministic polynomial-time algorithm for which this factor is bounded by a\npolynomial in $k$. Our derivation and analysis of the algorithm is based on\nderandomizing a recent existence result by Zamarashkin and Osinsky. To\nillustrate the versatility of our new column subset selection algorithm, an\nextension to low multilinear rank approximations of tensors is provided as\nwell.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 17:08:08 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Cortinovis", "Alice", ""], ["Kressner", "Daniel", ""]]}, {"id": "1908.06242", "submitter": "Shinsaku Sakaue", "authors": "Shinsaku Sakaue", "title": "Guarantees of Stochastic Greedy Algorithms for Non-monotone Submodular\n  Maximization with Cardinality Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular maximization with a cardinality constraint can model various\nproblems, and those problems are often very large in practice. For the case\nwhere objective functions are monotone, many fast approximation algorithms have\nbeen developed. The stochastic greedy algorithm (SG) is one such algorithm,\nwhich is widely used thanks to its simplicity, efficiency, and high empirical\nperformance. However, its approximation guarantee has been proved only for\nmonotone objective functions. When it comes to non-monotone objective\nfunctions, existing approximation algorithms are inefficient relative to the\nfast algorithms developed for the case of monotone objectives. In this paper,\nwe prove that SG (with slight modification) can achieve almost\n$1/4$-approximation guarantees in expectation in linear time even if objective\nfunctions are non-monotone. Our result provides a constant-factor approximation\nalgorithm with the fewest oracle queries for non-monotone submodular\nmaximization with a cardinality constraint. Experiments validate the\nperformance of (modified) SG.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 04:50:57 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 01:43:30 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 03:13:54 GMT"}, {"version": "v4", "created": "Tue, 17 Dec 2019 06:46:51 GMT"}, {"version": "v5", "created": "Tue, 24 Dec 2019 03:35:02 GMT"}, {"version": "v6", "created": "Fri, 10 Jan 2020 10:47:02 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Sakaue", "Shinsaku", ""]]}, {"id": "1908.06270", "submitter": "Sebastian Brandt", "authors": "Sebastian Brandt, Yannic Maus, Jara Uitto", "title": "A Sharp Threshold Phenomenon for the Distributed Complexity of the\n  Lov\\'asz Local Lemma", "comments": "appeared at PODC 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lov\\'{a}sz Local Lemma (LLL) says that, given a set of bad events that\ndepend on the values of some random variables and where each event happens with\nprobability at most $p$ and depends on at most $d$ other events, there is an\nassignment of the variables that avoids all bad events if the LLL criterion\n$ep(d+1)<1$ is satisfied.\n  In this paper, we study the dependency of the distributed complexity of the\nLLL problem on the chosen LLL criterion. We show that for the fundamental case\nof each random variable of the considered LLL instance being associated with an\nedge of the input graph, that is, each random variable influences at most two\nevents, a sharp threshold phenomenon occurs at $p = 2^{-d}$: we provide a\nsimple deterministic (!) algorithm that matches a known $\\Omega(\\log^* n)$\nlower bound in bounded degree graphs, if $p < 2^{-d}$, whereas for $p \\geq\n2^{-d}$, a known $\\Omega(\\log \\log n)$ randomized and a known $\\Omega(\\log n)$\ndeterministic lower bounds hold.\n  In many applications variables affect more than two events; our main\ncontribution is to extend our algorithm to the case where random variables\ninfluence at most three different bad events. We show that, surprisingly, the\nsharp threshold occurs at the exact same spot, providing evidence for our\nconjecture that this phenomenon always occurs at $p = 2^{-d}$, independent of\nthe number $r$ of events that are affected by a variable. Almost all steps of\nthe proof framework we provide for the case $r=3$ extend directly to the case\nof arbitrary $r$; consequently, our approach serves as a step towards\ncharacterizing the complexity of the LLL under different exponential criteria.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 09:24:35 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 10:47:56 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Brandt", "Sebastian", ""], ["Maus", "Yannic", ""], ["Uitto", "Jara", ""]]}, {"id": "1908.06300", "submitter": "Stefan Weltge", "authors": "Michele Conforti, Samuel Fiorin, Tony Huynh, Gwena\\\"el Joret, Stefan\n  Weltge", "title": "The stable set problem in graphs with bounded genus and bounded odd\n  cycle packing number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the family of graphs without $ k $ node-disjoint odd cycles, where $\nk $ is a constant. Determining the complexity of the stable set problem for\nsuch graphs $ G $ is a long-standing problem. We give a polynomial-time\nalgorithm for the case that $ G $ can be further embedded in a (possibly\nnon-orientable) surface of bounded genus. Moreover, we obtain polynomial-size\nextended formulations for the respective stable set polytopes.\n  To this end, we show that $2$-sided odd cycles satisfy the Erd\\H{o}s-P\\'osa\nproperty in graphs embedded in a fixed surface. This extends the fact that odd\ncycles satisfy the Erd\\H{o}s-P\\'osa property in graphs embedded in a fixed\norientable surface (Kawarabayashi & Nakamoto, 2007).\n  Eventually, our findings allow us to reduce the original problem to the\nproblem of finding a minimum-cost non-negative integer circulation of a certain\nhomology class, which turns out to be efficiently solvable in our case.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 13:10:18 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Conforti", "Michele", ""], ["Fiorin", "Samuel", ""], ["Huynh", "Tony", ""], ["Joret", "Gwena\u00ebl", ""], ["Weltge", "Stefan", ""]]}, {"id": "1908.06318", "submitter": "Magnus Lie Hetland", "authors": "Magnus Lie Hetland", "title": "Comparison-Based Indexing From First Principles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic assumptions about comparison-based indexing are laid down and a general\ndesign space is derived from these. An index structure spanning this design\nspace (the sprawl) is described, along with an associated family of\npartitioning predicates, or regions (the ambits), as well as algorithms for\nsearch and, to some extent, construction. The sprawl of ambits forms a\nunification and generalization of current indexing methods, and a jumping-off\npoint for future designs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 16:54:40 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Hetland", "Magnus Lie", ""]]}, {"id": "1908.06320", "submitter": "Giannis Nikolentzos", "authors": "Giannis Nikolentzos and Michalis Vazirgiannis", "title": "Revisiting the Graph Isomorphism Problem with Semidefinite Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the graph isomorphism problem can be posed as an\nequivalent problem of determining whether an auxiliary graph structure contains\na clique of specific order. However, the algorithms that have been developed so\nfar for this problem are either not efficient or not exact. In this paper, we\npresent a new algorithm which solves this equivalent formulation via\nsemidefinite programming. Specifically, we show that the problem of determining\nwhether the auxiliary graph contains a clique of specific order can be\nformulated as a semidefinite programming problem, and can thus be (almost\nexactly) solved in polynomial time. Furthermore, we show that we can determine\nif the graph contains such a clique by rounding the optimal solution to the\nnearest integer. Our algorithm provides a significant complexity result in\ngraph isomorphism testing, and also represents the first use of semidefinite\nprogramming for solving this problem.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 17:08:57 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 16:23:28 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Nikolentzos", "Giannis", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1908.06418", "submitter": "Stefano Quer", "authors": "Andrea Marcelli, Stefano Quer, and Giovanni Squillero", "title": "The Maximum Common Subgraph Problem: A Portfolio Approach", "comments": null, "journal-ref": null, "doi": "10.3390/computation8020048", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Common Subgraph is a computationally challenging problem with\ncountless practical applications. Even if it has been long proven NP-hard, its\nimportance still motivates searching for exact solutions. This work starts by\ndiscussing the possibility to extend an existing, very effective\nbranch-and-bound procedure on parallel multi-core and many-core architectures.\nWe analyze a parallel multi-core implementation that exploits a\ndivide-and-conquer approach based on a thread-pool, which does not deteriorate\nthe original algorithmic efficiency and it is not memory bound. We extend the\nalgorithm to parallel many-core GPU architectures adopting the CUDA programming\nframework, and we show how to handle the heavily workload-unbalance and the\nmassive data dependency. Then, we suggest new heuristics that reorder the\nadjacency matrix, deal with \"dead-ends\" and randomize the search with automatic\nrestarts, achieving significant improvements on specific cases. Finally, we\npropose a portfolio approach, which integrates all the different local search\nalgorithms as component tools. Such portfolio, rather than choosing the best\ntool for a given instance up-front, takes the decision on-line. The proposed\napproach drastically limits memory bandwidth constraints and avoids other\ntypical portfolio fragilities as CPU and GPU versions often show a\ncomplementary efficiency and run on separated platforms. Experimental results\nsupport the claims and motivate further research to better exploit GPUs in\nembedded task-intensive, and multi-engine parallel applications.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 10:15:59 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Marcelli", "Andrea", ""], ["Quer", "Stefano", ""], ["Squillero", "Giovanni", ""]]}, {"id": "1908.06428", "submitter": "Danny Hucke", "authors": "Hideo Bannai, Momoko Hirayama, Danny Hucke, Shunsuke Inenaga, Artur\n  Jez, Markus Lohrey, Carl Philipp Reh", "title": "The smallest grammar problem revisited", "comments": "A short version of this paper appeared in the Proceedings of SPIRE\n  2016. This work has been supported by the DFG research project LO 748/10-1\n  (QUANT-KOMP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper of Charikar et al. on the smallest grammar problem, the\nauthors derive upper and lower bounds on the approximation ratios for several\ngrammar-based compressors, but in all cases there is a gap between the lower\nand upper bound. Here the gaps for $\\mathsf{LZ78}$ and $\\mathsf{BISECTION}$ are\nclosed by showing that the approximation ratio of $\\mathsf{LZ78}$ is $\\Theta(\n(n/\\log n)^{2/3})$, whereas the approximation ratio of $\\mathsf{BISECTION}$ is\n$\\Theta(\\sqrt{n/\\log n})$. In addition, the lower bound for $\\mathsf{RePair}$\nis improved from $\\Omega(\\sqrt{\\log n})$ to $\\Omega(\\log n/\\log\\log n)$.\nFinally, results of Arpe and Reischuk relating grammar-based compression for\narbitrary alphabets and binary alphabets are improved.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 11:38:09 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Bannai", "Hideo", ""], ["Hirayama", "Momoko", ""], ["Hucke", "Danny", ""], ["Inenaga", "Shunsuke", ""], ["Jez", "Artur", ""], ["Lohrey", "Markus", ""], ["Reh", "Carl Philipp", ""]]}, {"id": "1908.06460", "submitter": "Yasuo Yamane", "authors": "Yasuo Yamane and Hironobu Kitajima", "title": "A New k-Shortest Path Search Approach based on Graph Reduction", "comments": "18 pages, 16 figures (including 9 tables as figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach called GR (Graph Reduction) algorithm for searching\nloop-less k-shortest paths (1st to k-th shortest paths) in a graph based on\ngraph reduction. Let a source vertex and a target vertex of k-shortest paths be\nv_s and v_t respectively. First our approach computes shortest paths to every\nvertex from v_s and v_t respectively, and reduce a graph to a subgraph that\ncontains all vertices and edges of loop-less k-shortest paths using the already\ncomputed shortest paths, and apply an existing k-shortest path search algorithm\nto the reduced graph. A graph can be reduced quickly after computing the\nshortest paths using them, therefore a very efficient search can be achieved.\nIn an experiment using a hypercube graph which has 16384 vertices where k=128,\nthe number of vertices is reduced to about 1/22, and a variant of Dijkstra\nalgorithm for k-shortest path search were speeded up by about 365 times. We\nimplemented a fast k-shortest path variant of bidirectional Dijkstra algorithm\n(k-biDij) which is the state-of-the-art algorithm and the fastest as long as we\nknow, GR outperforms k-biDij in dense scale-free graphs. However, k-biDij\noutperforms GR in hypercube-shaped and sparse scale-free graphs, but even then\nGR can also speed up it by 12.3 and 2.0 times respectively by precomputing\nall-pairs shortest paths. We also show the graph reduction can be done in time\ncomplexity O(m + n log n). We also introduce our improvements to k-biDij\nsimply.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 15:15:38 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yamane", "Yasuo", ""], ["Kitajima", "Hironobu", ""]]}, {"id": "1908.06527", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "The Runtime of the Compact Genetic Algorithm on Jump Functions", "comments": "Revised version of the journal version of my GECCO 2019\n  (arXiv:1903.10983) and FOGA 2019 (arXiv:1904.08415) papers", "journal-ref": null, "doi": "10.1007/s00453-020-00780-w", "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first and so far only mathematical runtime analysis of an\nestimation-of-distribution algorithm (EDA) on a multimodal problem, Hasen\\\"ohrl\nand Sutton (GECCO 2018) showed for any $k = o(n)$ that the compact genetic\nalgorithm (cGA) with any hypothetical population size $\\mu = \\Omega(ne^{4k} +\nn^{3.5+\\varepsilon})$ with high probability finds the optimum of the\n$n$-dimensional jump function with jump size $k$ in time $O(\\mu n^{1.5} \\log\nn)$.\n  We significantly improve this result for small jump sizes $k \\le \\frac 1 {20}\n\\ln n -1$. In this case, already for $\\mu = \\Omega(\\sqrt n \\log n) \\cap\n\\text{poly}(n)$ the runtime of the cGA with high probability is only $O(\\mu\n\\sqrt n)$. For the smallest admissible values of $\\mu$, our result gives a\nruntime of $O(n \\log n)$, whereas the previous one only shows\n$O(n^{5+\\varepsilon})$. Since it is known that the cGA with high probability\nneeds at least $\\Omega(\\mu \\sqrt n)$ iterations to optimize the unimodal OneMx\nfunction, our result shows that the cGA in contrast to most classic\nevolutionary algorithms here is able to cross moderate-sized valleys of low\nfitness at no extra cost.\n  For large $k$, we show that the exponential (in $k$) runtime guarantee of\nHasen\\\"ohrl and Sutton is tight and cannot be improved, also not by using a\nsmaller hypothetical population size. We prove that any choice of the\nhypothetical population size leads to a runtime that, with high probability, is\nat least exponential in the jump size $k$. This result might be the first\nnon-trivial exponential lower bound for EDAs that holds for arbitrary parameter\nsettings.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 22:45:18 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 13:36:56 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "1908.06541", "submitter": "Rupei Xu", "authors": "Rupei Xu and Andr\\'as Farag\\'o", "title": "The Landscape of Minimum Label Cut (Hedge Connectivity) Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum Label Cut (or Hedge Connectivity) problem is defined as follows:\ngiven an undirected graph $G=(V, E)$ with $n$ vertices and $m$ edges, in which,\neach edge is labeled (with one or multiple labels) from a label set\n$L=\\{\\ell_1,\\ell_2, ..., \\ell_{|L|}\\}$, the edges may be weighted with weight\nset $W =\\{w_1, w_2, ..., w_m\\}$, the label cut problem(hedge connectivity)\nproblem asks for the minimum number of edge sets(each edge set (or hedge) is\nthe edges with the same label) whose removal disconnects the source-sink pair\nof vertices or the whole graph with minimum total weights(minimum cardinality\nfor unweighted version). This problem is more general than edge connectivity\nand hypergraph edge connectivity problem and has a lot of applications in MPLS,\nIP networks, synchronous optical networks, image segmentation, and other areas.\nHowever, due to limited communications between different communities, this\nproblem was studied in different names, with some important existing literature\ncitations missing, or sometimes the results are misleading with some errors. In\nthis paper, we make a further investigation of this problem, give uniform\ndefinitions, fix existing errors, provide new insights and show some new\nresults. Specifically, we show the relationship between non-overlapping\nversion(each edge only has one label) and overlapping version(each edge has\nmultiple labels), by fixing the error in the existing literature; hardness and\napproximation performance between weighted version and unweighted version and\nsome useful properties for further research.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 00:12:06 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 09:36:43 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Xu", "Rupei", ""], ["Farag\u00f3", "Andr\u00e1s", ""]]}, {"id": "1908.06649", "submitter": "Francesco Silvestri", "authors": "Rezaul Chowdhury and Francesco Silvestri and Flavio Vella", "title": "A Computational Model for Tensor Core Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To respond to the need of efficient training and inference of deep neural\nnetworks, a plethora of domain-specific hardware architectures have been\nintroduced, such as Google Tensor Processing Units and NVIDIA Tensor Cores. A\ncommon feature of these architectures is a hardware circuit for efficiently\ncomputing a dense matrix multiplication of a given small size. In order to\nbroaden the class of algorithms that exploit these systems, we propose a\ncomputational model, named the TCU model, that captures the ability to natively\nmultiply small matrices. We then use the TCU model for designing fast\nalgorithms for several problems, including matrix operations (dense and sparse\nmultiplication, Gaussian Elimination), graph algorithms (transitive closure,\nall pairs shortest distances), Discrete Fourier Transform, stencil\ncomputations, integer multiplication, and polynomial evaluation. We finally\nhighlight a relation between the TCU model and the external memory model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 08:59:46 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 07:25:17 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Chowdhury", "Rezaul", ""], ["Silvestri", "Francesco", ""], ["Vella", "Flavio", ""]]}, {"id": "1908.06657", "submitter": "Alessandro Luongo", "authors": "Iordanis Kerenidis, Alessandro Luongo, Anupam Prakash", "title": "Quantum Expectation-Maximization for Gaussian Mixture Models", "comments": "As to appear in ICML2020 conference - with improved algorithms and\n  runtimes", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expectation-Maximization (EM) algorithm is a fundamental tool in\nunsupervised machine learning. It is often used as an efficient way to solve\nMaximum Likelihood (ML) estimation problems, especially for models with latent\nvariables. It is also the algorithm of choice to fit mixture models: generative\nmodels that represent unlabelled points originating from $k$ different\nprocesses, as samples from $k$ multivariate distributions. In this work we\ndefine and use a quantum version of EM to fit a Gaussian Mixture Model. Given\nquantum access to a dataset of $n$ vectors of dimension $d$, our algorithm has\nconvergence and precision guarantees similar to the classical algorithm, but\nthe runtime is only polylogarithmic in the number of elements in the training\nset, and is polynomial in other parameters - as the dimension of the feature\nspace, and the number of components in the mixture. We generalize further the\nalgorithm in two directions. First, we show how to fit any mixture model of\nprobability distributions in the exponential family. Then, we show how to use\nthis algorithm to compute the Maximum a Posteriori (MAP) estimate of a mixture\nmodel: the Bayesian approach to likelihood estimation problems. We discuss the\nperformance of the algorithm on a dataset that is expected to be classified\nsuccessfully by this algorithm, arguing that on those cases we can give strong\nguarantees on the runtime.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 09:21:45 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 14:49:27 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kerenidis", "Iordanis", ""], ["Luongo", "Alessandro", ""], ["Prakash", "Anupam", ""]]}, {"id": "1908.06688", "submitter": "Luca Castelli Aleardi", "authors": "Luca Castelli Aleardi", "title": "Balanced Schnyder woods for planar triangulations: an experimental study\n  with applications to graph drawing and graph separators", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider balanced Schnyder woods for planar graphs, which are\nSchnyder woods where the number of incoming edges of each color at each vertex\nis balanced as much as possible. We provide a simple linear-time heuristic\nleading to obtain well balanced Schnyder woods in practice. As test\napplications we consider two important algorithmic problems: the computation of\nSchnyder drawings and of small cycle separators. While not being able to\nprovide theoretical guarantees, our experimental results (on a wide collection\nof planar graphs) suggest that the use of balanced Schnyder woods leads to an\nimprovement of the quality of the layout of Schnyder drawings, and provides an\nefficient tool for computing short and balanced cycle separators.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 10:47:21 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Aleardi", "Luca Castelli", ""]]}, {"id": "1908.06720", "submitter": "D\\'aniel Szil\\'agyi", "authors": "Iordanis Kerenidis, Anupam Prakash, D\\'aniel Szil\\'agyi", "title": "Quantum algorithms for Second-Order Cone Programming and Support Vector\n  Machines", "comments": "final version for Quantum", "journal-ref": "Quantum 5, 427 (2021)", "doi": "10.22331/q-2021-04-08-427", "report-no": null, "categories": "quant-ph cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a quantum interior-point method (IPM) for second-order cone\nprogramming (SOCP) that runs in time $\\widetilde{O} \\left( n\\sqrt{r}\n\\frac{\\zeta \\kappa}{\\delta^2} \\log \\left(1/\\epsilon\\right) \\right)$ where $r$\nis the rank and $n$ the dimension of the SOCP, $\\delta$ bounds the distance of\nintermediate solutions from the cone boundary, $\\zeta$ is a parameter upper\nbounded by $\\sqrt{n}$, and $\\kappa$ is an upper bound on the condition number\nof matrices arising in the classical IPM for SOCP. The algorithm takes as its\ninput a suitable quantum description of an arbitrary SOCP and outputs a\nclassical description of a $\\delta$-approximate $\\epsilon$-optimal solution of\nthe given problem.\n  Furthermore, we perform numerical simulations to determine the values of the\naforementioned parameters when solving the SOCP up to a fixed precision\n$\\epsilon$. We present experimental evidence that in this case our quantum\nalgorithm exhibits a polynomial speedup over the best classical algorithms for\nsolving general SOCPs that run in time $O(n^{\\omega+0.5})$ (here, $\\omega$ is\nthe matrix multiplication exponent, with a value of roughly $2.37$ in theory,\nand up to $3$ in practice). For the case of random SVM (support vector machine)\ninstances of size $O(n)$, the quantum algorithm scales as $O(n^k)$, where the\nexponent $k$ is estimated to be $2.59$ using a least-squares power law. On the\nsame family random instances, the estimated scaling exponent for an external\nSOCP solver is $3.31$ while that for a state-of-the-art SVM solver is $3.11$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 12:02:13 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 12:49:00 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 10:57:26 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 17:33:22 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Kerenidis", "Iordanis", ""], ["Prakash", "Anupam", ""], ["Szil\u00e1gyi", "D\u00e1niel", ""]]}, {"id": "1908.06727", "submitter": "Leah Epstein", "authors": "Leah Epstein", "title": "On bin packing with clustering and bin packing with delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We continue the study of two recently introduced bin packing type problems,\ncalled bin packing with clustering, and online bin packing with delays. A bin\npacking input consists of items of sizes not larger than 1, and the goal is to\npartition or pack them into bins, where the total size of items of every valid\nbin cannot exceed 1.\n  In bin packing with clustering, items also have colors associated with them.\nA globally optimal solution can combine items of different colors in bins,\nwhile a clustered solution can only pack monochromatic bins. The goal is to\ncompare a globally optimal solution to an optimal clustered solution, under\ncertain constraints on the coloring provided with the input. We show close\nbounds on the worst-case ratio between these two costs, called \"the price of\nclustering\", improving and simplifying previous results. Specifically, we show\nthat the price of clustering does not exceed 1.93667, improving over the\nprevious upper bound of 1.951, and that it is at least 1.93558, improving over\nthe previous lower bound of 1.93344.\n  In online bin packing with delays, items are presented over time. Items may\nwait to be packed, and an algorithm can create a new bin at any time, packing a\nsubset of already existing unpacked items into it, under the condition that the\nbin is valid. A created bin cannot be used again in the future, and all items\nhave to be packed into bins eventually. The objective is to minimize the number\nof used bins plus the sum of waiting costs of all items, called delays. We\nbuild on previous work and modify a simple phase-based algorithm. We combine\nthe modification with a careful analysis to improve the previously known\ncompetitive ratio from 3.951 to below 3.1551.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 12:16:50 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Epstein", "Leah", ""]]}, {"id": "1908.06795", "submitter": "Christian Schulz", "authors": "Demian Hespe, Sebastian Lamm, Christian Schulz, Darren Strash", "title": "WeGotYouCovered: The Winning Solver from the PACE 2019 Implementation\n  Challenge, Vertex Cover Track", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the winning solver of the PACE 2019 Implementation Challenge,\nVertex Cover Track. The minimum vertex cover problem is one of a handful of\nproblems for which kernelization---the repeated reducing of the input size via\ndata reduction rules---is known to be highly effective in practice. Our\nalgorithm uses a portfolio of techniques, including an aggressive kernelization\nstrategy, local search, branch-and-reduce, and a state-of-the-art\nbranch-and-bound solver. Of particular interest is that several of our\ntechniques were not from the literature on the vertex over problem: they were\noriginally published to solve the (complementary) maximum independent set and\nmaximum clique problems.\n  Aside from illustrating our solver's performance in the PACE 2019\nImplementation Challenge, our experiments provide several key insights not yet\nseen before in the literature. First, kernelization can boost the performance\nof branch-and-bound clique solvers enough to outperform branch-and-reduce\nsolvers. Second, local search can significantly boost the performance of\nbranch-and-reduce solvers. And finally, somewhat surprisingly, kernelization\ncan sometimes make branch-and-bound algorithms perform worse than running\nbranch-and-bound alone.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 13:34:15 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 07:54:58 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Hespe", "Demian", ""], ["Lamm", "Sebastian", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""]]}, {"id": "1908.06798", "submitter": "Yasuo Yamane", "authors": "Yasuo Yamane and Kenichi Kobayashi", "title": "A New Fast Weighted All-pairs Shortest Path Search Algorithm Based on\n  Pruning by Shortest Path Trees", "comments": "18 pages, 15 figures (including 6 tables as figures). arXiv admin\n  note: substantial text overlap with arXiv:1908.06806", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we submitted a paper, whose title is A New Fast Unweighted All-pairs\nShortest Path Search Algorithm Based on Pruning by Shortest Path Trees, to\narXiv. This is related to unweighted graphs. This paper also presents a new\nfast all-pairs shortest path algorithm for weighted graph based on the same\nidea. In Dijkstra algorithm which is said to be fast in weighted graphs, the\naverage number of accesses to adjacent vertices (expressed by {\\alpha}) is\nabout equal to the average degree of the graph. On the other hand, our\nalgorithm utilizes the shortest path trees of adjacent vertices of each source\nvertex in the same manner as the algorithm for unweighted graphs, and reduce\n{\\alpha} drastically in comparison with Dijkstra algorithm. Roughly speaking\n{\\alpha} is reduced to the value close to 1, because the average degree of a\ntree is about 2, and one is used to come in and the other is used to go out,\nalthough that does not hold true when the depth of the short path trees is\nsmall. In case of weighted graphs, a problem which does not occur in unweighted\ngraphs occurs. It is waiting for the generation of the shortest path tree of an\nadjacent vertex. Therefore, it is possible that a deadlock occurs. We prove our\nalgorithm is deadlock-free. We compared our algorithm with Dijkstra and Peng\nalgorithms. On Dijkstra algorithm ours outperforms it on speed and {\\alpha}\nexcept that Dijkstra algorithm slightly outperforms ours or they are almost the\nsame on CPU time in sparse scale-free graphs. The result on Peng algorithm is\nas follows: In speed and {\\alpha}, ours outperforms Peng algorithm in\nhypercube-shaped and dense scale-free graphs, but conversely Peng algorithm\noutperforms ours in sparse scale-free graphs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 13:44:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yamane", "Yasuo", ""], ["Kobayashi", "Kenichi", ""]]}, {"id": "1908.06806", "submitter": "Yasuo Yamane", "authors": "Yasuo Yamane and Kenichi Kobayashi", "title": "A New Fast Unweighted All-pairs Shortest Path Search Algorithm Based on\n  Pruning by Shortest Path Trees", "comments": "13 pages, 15 figures (including 6 tables as figures). arXiv admin\n  note: substantial text overlap with arXiv:1908.06798", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new fast all-pairs shortest path algorithm for unweighted\ngraphs. In breadth-first search which is said to representative and fast in\nunweighted graphs, the average number of accesses to adjacent vertices\n(expressed by {\\alpha}) is about equal to the average degree of the graph. On\nthe other hand, our algorithm utilizes the shortest path trees of adjacent\nvertices of each source vertex, and reduce {\\alpha} drastically. Roughly\nspeaking {\\alpha} is reduced to the value close to 1, because the average\ndegree of a tree is about 2, and one is used to come in and the other is used\nto go out, although that does not hold true when the depth of the shortest path\ntrees is small. We compared our algorithm with breadth-first search algorithm,\nand our results showed that ours outperforms breadth-first search on speed and\n{\\alpha}.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 13:55:35 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yamane", "Yasuo", ""], ["Kobayashi", "Kenichi", ""]]}, {"id": "1908.06818", "submitter": "Michal Moshkovitz", "authors": "Michal Moshkovitz", "title": "Unexpected Effects of Online no-Substitution k-means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline k-means clustering was studied extensively, and algorithms with a\nconstant approximation are available. However, online clustering is still\nuncharted. New factors come into play: the ordering of the dataset and whether\nthe number of points, n, is known in advance or not. Their exact effects are\nunknown. In this paper we focus on the online setting where the decisions are\nirreversible: after a point arrives, the algorithm needs to decide whether to\ntake the point as a center or not, and this decision is final. How many centers\nare needed and sufficient to achieve constant approximation in this setting? We\nshow upper and lower bounds for all the different cases. These bounds are\nexactly the same up to a constant, thus achieving optimal bounds. For example,\nfor k-means cost with constant k>1 and random order, Theta(log n) centers are\nenough to achieve a constant approximation, while the mere a priori knowledge\nof n reduces the number of centers to a constant. These bounds hold for any\ndistance function that obeys a triangle-type inequality.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 18:21:00 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 03:13:45 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Moshkovitz", "Michal", ""]]}, {"id": "1908.06821", "submitter": "Kai Wang", "authors": "Kai Wang", "title": "An Efficient Algorithm to Test Potentially Bipartiteness of Graphical\n  Degree Sequences", "comments": "11 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a partial answer to a question of Rao, a deterministic and customizable\nefficient algorithm is presented to test whether an arbitrary graphical degree\nsequence has a bipartite realization. The algorithm can be configured to run in\npolynomial time, at the expense of possibly producing an erroneous output on\nsome \"yes\" instances but with very low error rate.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 14:13:31 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wang", "Kai", ""]]}, {"id": "1908.06828", "submitter": "Jakob Spooner", "authors": "Thomas Erlebach and Jakob T. Spooner", "title": "A Game of Cops and Robbers on Graphs with Periodic Edge-Connectivity", "comments": "16 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a game in which a single cop and a single robber take\nturns moving along the edges of a given graph $G$. If there exists a strategy\nfor the cop which enables it to be positioned at the same vertex as the robber\neventually, then $G$ is called cop-win, and robber-win otherwise. We study this\nclassical combinatorial game in a novel context, broadening the class of\npotential game arenas to include the edge-periodic graphs. These are graphs\nwith an infinite lifetime comprised of discrete time steps such that each edge\n$e$ is assigned a bit pattern of length $l_e$, with a 1 in the $i$-th position\nof the pattern indicating the presence of edge $e$ in the $i$-th step of each\nconsecutive block of $l_e$ steps. Utilising the already-developed framework of\nreachability games, we extend existing techniques to obtain, amongst other\nresults, an $O(\\textsf{LCM}(L)\\cdot n^3)$ upper bound on the time required to\ndecide if a given $n$-vertex edge-periodic graph $G^\\tau$ is cop or robber win\nas well as compute a strategy for the winning player (here, $L$ is the set of\nall edge pattern lengths $l_e$, and $\\textsf{LCM}(L)$ denotes the least common\nmultiple of the set $L$). Separately, turning our attention to edge-periodic\ncycle graphs, we give proof of a $2\\cdot l \\cdot \\textsf{LCM}(L)$ upper bound\non the length required by any edge-periodic cycle to ensure that it is robber\nwin, where $l = 1$ if $\\textsf{LCM}(L) \\geq 2\\cdot \\max L $, and $l=2$\notherwise. Furthermore, we provide lower bound constructions in the form of\ncop-win edge-periodic cycles: one with length $1.5 \\cdot \\textsf{LCM}(L)$ in\nthe $l=1$ case and one with length $3\\cdot \\textsf{LCM}(L)$ in the $l=2$ case.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 14:27:23 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Erlebach", "Thomas", ""], ["Spooner", "Jakob T.", ""]]}, {"id": "1908.06964", "submitter": "Dhananjay Phatak", "authors": "Dhananjay Phatak, Alan T. Sherman, Steven D. Houston, Andrew Henry", "title": "PPT: New Low Complexity Deterministic Primality Tests Leveraging\n  Explicit and Implicit Non-Residues. A Set of Three Companion Manuscripts", "comments": "a set of 3 companion articles.217 (two hundred and seventeen) pages\n  including everything = table of contents, list of figures, list of tables and\n  an acknowledgment at the end. There is no watermark or highlighted text. Only\n  color is in hyper-links and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC cs.DS cs.SC math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this set of three companion manuscripts/articles, we unveil our new\nresults on primality testing and reveal new primality testing algorithms\nenabled by those results. The results have been classified (and referred to) as\nlemmas/corollaries/claims whenever we have complete analytic proof(s);\notherwise the results are introduced as conjectures.\n  In Part/Article 1, we start with the Baseline Primality Conjecture~(PBPC)\nwhich enables deterministic primality detection with a low complexity = O((log\nN)^2) ; when an explicit value of a Quadratic Non Residue (QNR) modulo-N is\navailable (which happens to be the case for an overwhelming majority = 11/12 =\n91.67% of all odd integers). We then demonstrate Primality Lemma PL-1, which\nreveals close connections between the state-of-the-art Miller-Rabin method and\nthe renowned Euler-Criterion. This Lemma, together with the Baseline Primality\nConjecture enables a synergistic fusion of Miller-Rabin iterations and our\nmethod(s), resulting in hybrid algorithms that are substantially better than\ntheir components. Next, we illustrate how the requirement of an explicit value\nof a QNR can be circumvented by using relations of the form: Polynomial(x) mod\nN = 0 ; whose solutions implicitly specify Non Residues modulo-N. We then\ndevelop a method to derive low-degree canonical polynomials that together\nguarantee implicit Non Residues modulo-N ; which along with the Generalized\nPrimality Conjectures enable algorithms that achieve a worst case deterministic\npolynomial complexity = O( (log N)^3 polylog(log N)) ; unconditionally ; for\nany/all values of N.\n  In Part/Article 2 , we present substantial experimental data that corroborate\nall the conjectures. No counter example has been found.\n  Finally in Part/Article 3, we present analytic proof(s) of the Baseline\nPrimality Conjecture that we have been able to complete for some special cases.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 16:48:15 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Phatak", "Dhananjay", ""], ["Sherman", "Alan T.", ""], ["Houston", "Steven D.", ""], ["Henry", "Andrew", ""]]}, {"id": "1908.07076", "submitter": "John Hooker", "authors": "J. N. Hooker", "title": "Improved Job sequencing Bounds from Decision Diagrams", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general method for relaxing decision diagrams that allows one\nto bound job sequencing problems by solving a Lagrangian dual problem on a\nrelaxed diagram. We also provide guidelines for identifying problems for which\nthis approach can result in useful bounds. These same guidelines can be applied\nto bounding deterministic dynamic programming problems in general, since\ndecision diagrams rely on DP formulations. Computational tests show that\n\\mbox{Lagrangian} relaxation on a decision diagram can yield very tight bounds\nfor certain classes of hard job sequencing problems. For example, it proves for\nthe first time that the best known solutions for Biskup-Feldman instances are\nwithin a small fraction of 1% of the optimal value, and sometimes optimal.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 21:29:04 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Hooker", "J. N.", ""]]}, {"id": "1908.07154", "submitter": "Peter Zeman", "authors": "Peter Zeman", "title": "Discrete and Fast Fourier Transform Made Clear", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Fourier transform was included in the Top 10 Algorithms of 20th Century\nby Computing in Science & Engineering. In this paper, we provide a new simple\nderivation of both the discrete Fourier transform and fast Fourier transform by\nmeans of elementary linear algebra. We start the exposition by introducing the\nconvolution product of vectors, represented by a circulant matrix, and derive\nthe discrete Fourier transform as the change of basis matrix that diagonalizes\nthe circulant matrix. We also generalize our approach to derive the Fourier\ntransform on any finite abelian group, where the case of Fourier transform on\nthe Boolean cube is especially important for many applications in theoretical\ncomputer science.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 07:59:57 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Zeman", "Peter", ""]]}, {"id": "1908.07291", "submitter": "Max Sondag", "authors": "Soeren Nickel, Max Sondag, Wouter Meulemans, Markus Chimani, Stephen\n  Kobourov, Jaakko Peltonen, Martin N\\\"ollenburg", "title": "Computing Stable Demers Cartograms", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartograms are popular for visualizing numerical data for map regions.\nMaintaining correct adjacencies is a primary quality criterion for cartograms.\nWhen there are multiple data values per region (over time or different\ndatasets) shown as animated or juxtaposed cartograms, preserving the viewer's\nmental-map in terms of stability between cartograms is another important\ncriterion. We present a method to compute stable Demers cartograms, where each\nregion is shown as a square and similar data yield similar cartograms. We\nenforce orthogonal separation constraints with linear programming, and measure\nquality in terms of keeping adjacent regions close (cartogram quality) and\nusing similar positions for a region between the different data values\n(stability). Our method guarantees ability to connect most lost adjacencies\nwith minimal leaders. Experiments show our method yields good quality and\nstability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 12:04:26 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 11:25:36 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Nickel", "Soeren", ""], ["Sondag", "Max", ""], ["Meulemans", "Wouter", ""], ["Chimani", "Markus", ""], ["Kobourov", "Stephen", ""], ["Peltonen", "Jaakko", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "1908.07315", "submitter": "Jaroslav Opatrny", "authors": "Iman Bagheri, Lata Narayanan, Jaroslav Opatrny", "title": "Evacuation of equilateral triangles by mobile agents of limited\n  communication range", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of evacuating $k \\geq 2$ mobile agents from a\nunit-sided equilateral triangle through an exit located at an unknown location\non the perimeter of the triangle. The agents are initially located at the\ncentroid of the triangle and they can communicate with other agents at distance\nat most $r$ with $0\\leq r \\leq 1$. An agent can move at speed at most one, and\nfinds the exit only when it reaches the point where the exit is located. The\nagents can collaborate in the search for the exit. The goal of the {\\em\nevacuation problem} is to minimize the evacuation time, defined as the\nworst-case time for {\\em all} the agents to reach the exit. We propose and\nanalyze several algorithms for the problem of evacuation by $k \\geq 2$ agents;\nour results indicate that the best strategy to be used varies depending on the\nvalues of $r$ and $k$. For two agents, we give three algorithms, each of which\nachieves the best performance for different sub-ranges of $r$ in the range $0\n\\leq r \\leq 1$. Finally, we show that for any $r$, evacuation of $k=6\n+2\\lceil(\\frac{1}{r}-1)\\rceil$ agents can be done in time $1+\\sqrt{3}/3$, which\nis optimal in terms of time, and asymptotically optimal in terms of the number\nof agents.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 12:56:16 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Bagheri", "Iman", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""]]}, {"id": "1908.07318", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "An algorithm for destroying claws and diamonds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the {Claw,Diamond}-Free Edge Deletion problem the input is a graph $G$ and\nan integer $k$, and the goal is to decide whether there is a set of edges of\nsize at most $k$ such that removing the edges of the set from $G$ results a\ngraph that does not contain an induced claw or diamond. In this paper we give\nan algorithm for this problem whose running time is $O^*(3.562^k)$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 13:02:03 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1908.07584", "submitter": "John Hooker", "authors": "J. G. Benade and J. N. Hooker", "title": "Optimization Bounds from the Branching Dual", "comments": "18 figures", "journal-ref": "INFORMS Journal on Computing, published online 19 July 2019", "doi": "10.1287/ijoc.2018.0884", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general method for obtaining strong bounds for discrete\noptimization problems that is based on a concept of branching duality. It can\nbe applied when no useful integer programming model is available, and we\nillustrate this with the minimum bandwidth problem. The method strengthens a\nknown bound for a given problem by formulating a dual problem whose feasible\nsolutions are partial branching trees. It solves the dual problem with a\n\"worst-bound\" local search heuristic that explores neighboring partial trees.\nAfter proving some optimality properties of the heuristic, we show that it\nsubstantially improves known combinatorial bounds for the minimum bandwidth\nproblem with a modest amount of computation. It also obtains significantly\ntighter bounds than depth-first and breadth-first branching, demonstrating that\nthe dual perspective can lead to better branching strategies when the object is\nto find valid bounds.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 19:45:46 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Benade", "J. G.", ""], ["Hooker", "J. N.", ""]]}, {"id": "1908.07792", "submitter": "Amyra Meidiana", "authors": "Amyra Meidiana, Seok-Hee Hong, Peter Eades, and Daniel Keim", "title": "A Quality Metric for Visualization of Clusters in Graphs", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, graph quality metrics focus on readability, but recent studies\nshow the need for metrics which are more specific to the discovery of patterns\nin graphs. Cluster analysis is a popular task within graph analysis, yet there\nis no metric yet explicitly quantifying how well a drawing of a graph\nrepresents its cluster structure. We define a clustering quality metric\nmeasuring how well a node-link drawing of a graph represents the clusters\ncontained in the graph. Experiments with deforming graph drawings verify that\nour metric effectively captures variations in the visual cluster quality of\ngraph drawings. We then use our metric to examine how well different graph\ndrawing algorithms visualize cluster structures in various graphs; the results\ncon-firm that some algorithms which have been specifically designed to show\ncluster structures perform better than other algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 10:57:30 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Meidiana", "Amyra", ""], ["Hong", "Seok-Hee", ""], ["Eades", "Peter", ""], ["Keim", "Daniel", ""]]}, {"id": "1908.08111", "submitter": "Jasper Marianczuk", "authors": "Jasper Marianczuk", "title": "Engineering Faster Sorters for Small Sets of Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sorting a set of items is a task that can be useful by itself or as a\nbuilding block for more complex operations. The more sophisticated and fast\nsorting algorithms become asymptotically, the less efficient they are for small\nsets of items due to large constant factor. This thesis aims to determine if\nthere is a faster way to sort base case sizes than using insertion sort. For\nthat we looked at sorting networks and how to implement them efficiently.\nBecause sorting networks need to be implemented explicitly for each input size,\nproviding networks for larger sizes becomess less efficient. That is why we\nmodified Super Scalar Sample Sort to break down larger sets into sizes that can\nin turn be sorted by sorting networks. We show that the task of sorting only\nsmall sets can be greatly improved by at least 25% when using sorting networks\ncompared to insertion sort, but that when integrating them into other sorting\nalgorithms the speed-up is hindered by the limited L1 instruction cache size.\nOn a machine with 64KiB of L1 instruction cache we achieved over 6% of\nimprovement when using sorting networks as a base case sorter instead of\ninsertion sort.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 20:31:34 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Marianczuk", "Jasper", ""]]}, {"id": "1908.08151", "submitter": "Seok-Hee Hong", "authors": "Seok-Hee Hong, Peter Eades, Marnijati Torkel, Ziyang Wang, David Chae,\n  Sungpack Hong, Daniel Langerenken, Hassan Chafi", "title": "Multi-level Graph Drawing using Infomap Clustering", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infomap clustering finds the community structures that minimize the expected\ndescription length of a random walk trajectory; algorithms for infomap\nclustering run fast in practice for large graphs. In this paper we leverage the\neffectiveness of Infomap clustering combined with the multi-level graph drawing\nparadigm. Experiments show that our new Infomap based multi-level algorithm\nproduces good visualization of large and complex networks, with significant\nimprovement in quality metrics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 00:24:06 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Hong", "Seok-Hee", ""], ["Eades", "Peter", ""], ["Torkel", "Marnijati", ""], ["Wang", "Ziyang", ""], ["Chae", "David", ""], ["Hong", "Sungpack", ""], ["Langerenken", "Daniel", ""], ["Chafi", "Hassan", ""]]}, {"id": "1908.08266", "submitter": "Dmitrij Koznov Mr", "authors": "D.V. Luciv, D.V. Koznov, A.A. Shelikhovskii, K.Yu. Romanovsky, G.A.\n  Chernishev, A.N. Terekhov, D.A. Grigoriev, A.N. Smirnova, D.V. Borovkov, A.I.\n  Vasenina", "title": "Interactive Duplicate Search in Software Documentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various software features such as classes, methods, requirements, and tests\noften have similar functionality. This can lead to emergence of duplicates in\ntheir descriptive documentation. Uncontrolled duplicates created via copy/paste\nhinder the process of documentation maintenance. Therefore, the task of\nduplicate detection in software documentation is of importance. Solving it\nmakes planned reuse possible, as well as creating and using templates for\nunification and automatic generation of documentation. In this paper, we\npresent an interactive process for duplicate detection that involves the user\nin order to conduct meaningful search. It includes a new formal definition of a\nnear duplicate, a pattern-based, and the proof of its completeness. Moreover,\nwe demonstrate the results of experimenting on a collection of documents of\nseveral industrial projects.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 09:12:59 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Luciv", "D. V.", ""], ["Koznov", "D. V.", ""], ["Shelikhovskii", "A. A.", ""], ["Romanovsky", "K. Yu.", ""], ["Chernishev", "G. A.", ""], ["Terekhov", "A. N.", ""], ["Grigoriev", "D. A.", ""], ["Smirnova", "A. N.", ""], ["Borovkov", "D. V.", ""], ["Vasenina", "A. I.", ""]]}, {"id": "1908.08384", "submitter": "Moritz Venzin", "authors": "M\\'arton Nasz\\'odi, Moritz Venzin", "title": "Covering convex bodies and the Closest Vector Problem", "comments": "Added a section connecting the modulus of smoothness to lattice\n  sparsification and its consequences to approximate CVP under $l_p$ norms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms for the $(1+\\epsilon)$-approximate version of the\nclosest vector problem for certain norms. The currently fastest algorithm\n(Dadush and Kun 2016) for general norms has running time of $2^{O(n)}\n(1/\\epsilon)^n$. We improve this substantially in the following two cases. For\n$\\ell_p$-norms with $p>2$ (resp. $p \\in [1,2]$) fixed, we present an algorithm\nwith a running time of $2^{O(n)} (1/\\epsilon)^{n/2}$ (resp. $2^{O(n)}\n(1/\\epsilon)^{n/p}$). This result is based on a geometric covering problem,\nthat was introduced in the context of CVP by Eisenbrand et al.: How many convex\nbodies are needed to cover the ball of the norm such that, if scaled by two\naround their centroids, each one is contained in the $(1+\\epsilon)$-scaled\nhomothet of the norm ball? We provide upper bounds for this problem by\nexploiting the \\emph{modulus of smoothness} of the $\\ell_p$-balls. Applying a\ncovering scheme, we can boost any constant approximation algorithm for CVP to a\n$(1+\\epsilon)$-approximation algorithm with the improved run time, either using\na straightforward sampling routine or using the deterministic algorithm of\nDadush for the construction of an epsilon net. The space requirement only\ndepends on the constant approximation CVP solver used. Furthermore, we\ngeneralise the result of Eisenbrand et al. for the $\\ell_\\infty$-norm. For\ncentrally symmetric polytopes (resp. zonotopes) with $O(n)$ facets (resp.\ngenerated by $O(n)$ line segments), we provide a deterministic\n$O(\\log_2(1/\\epsilon))^{O(n)}$ time algorithm. Finally, we establish a\nconnection between the \\emph{modulus of smoothness} and \\emph{lattice\nsparsification}. Using the enumeration and sparsification tools developped by\nDadush, Kun, Peikert and Vempala, this leads to a simple alternative to the\nboosting procedure for CVP under $\\ell_p$-norms. This connection might be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 13:58:33 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 07:23:32 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Nasz\u00f3di", "M\u00e1rton", ""], ["Venzin", "Moritz", ""]]}, {"id": "1908.08411", "submitter": "Benjamin Raichel", "authors": "Chenglin Fan, Anna C. Gilbert, Benjamin Raichel, Rishi Sonthalia,\n  Gregory Van Buskirk", "title": "Generalized Metric Repair on Graphs", "comments": "arXiv admin note: text overlap with arXiv:1807.08078", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern data analysis algorithms either assume or are considerably more\nefficient if the distances between the data points satisfy a metric. These\nalgorithms include metric learning, clustering, and dimension reduction. As\nreal data sets are noisy, distances often fail to satisfy a metric. For this\nreason, Gilbert and Jain and Fan et al. introduced the closely related sparse\nmetric repair and metric violation distance problems. The goal of these\nproblems is to repair as few distances as possible to ensure they satisfy a\nmetric. Three variants were considered, one admitting a polynomial time\nalgorithm. The other variants were shown to be APX-hard, and an\n$O(OPT^{1/3})$-approximation was given, where $OPT$ is the optimal solution\nsize.\n  In this paper, we generalize these problems to no longer consider all\ndistances between the data points. That is, we consider a weighted graph $G$\nwith corrupted weights $w$, and our goal is to find the smallest number of\nweight modifications so that the resulting weighted graph distances satisfy a\nmetric. This is a natural generalization and is more flexible as it takes into\naccount different relationships among the data points. As in previous work, we\ndistinguish among the types of repairs permitted and focus on the increase only\nand general versions. We demonstrate the inherent combinatorial structure of\nthe problem, and give an approximation-preserving reduction from MULTICUT.\nConversely, we show that for any fixed constant $\\varsigma$, for the large\nclass of $\\varsigma$-chordal graphs, the problems are fixed parameter\ntractable. Call a cycle broken if it contains an edge whose weight is larger\nthan the sum of all its other edges, and call the amount of this difference its\ndeficit. We present approximation algorithms, one which depends on the maximum\nnumber of edges in a broken cycle, and one which depends on the number of\ndistinct deficit values.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 17:14:52 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Fan", "Chenglin", ""], ["Gilbert", "Anna C.", ""], ["Raichel", "Benjamin", ""], ["Sonthalia", "Rishi", ""], ["Van Buskirk", "Gregory", ""]]}, {"id": "1908.08551", "submitter": "Noah Daniels", "authors": "Najib Ishaq, George Student, Noah M. Daniels", "title": "Clustered Hierarchical Entropy-Scaling Search of Astronomical and\n  Biological Data", "comments": "Accepted to IEEE Big Data 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both astronomy and biology are experiencing explosive growth of data,\nresulting in a \"big data\" problem that stands in the way of a \"big data\"\nopportunity for discovery. One common question asked of such data is that of\napproximate search ($\\rho-$nearest neighbors search). We present a hierarchical\nsearch algorithm for such data sets that takes advantage of particular\ngeometric properties apparent in both astronomical and biological data sets,\nnamely the metric entropy and fractal dimensionality of the data. We present\nCHESS (Clustered Hierarchical Entropy-Scaling Search), a search tool with\nvirtually no loss in specificity or sensitivity, demonstrating a $13.6\\times$\nspeedup over linear search on the Sloan Digital Sky Survey's APOGEE data set\nand a $68\\times$ speedup on the GreenGenes 16S metagenomic data set, as well as\nasymptotically fewer distance comparisons on APOGEE when compared to the\nFALCONN locality-sensitive hashing library. CHESS demonstrates an asymptotic\ncomplexity not directly dependent on data set size, and is in practice at least\nan order of magnitude faster than linear search by performing fewer distance\ncomparisons. Unlike locality-sensitive hashing approaches, CHESS can work with\nany user-defined distance function. CHESS also allows for implicit data\ncompression, which we demonstrate on the APOGEE data set. We also discuss an\nextension allowing for efficient k-nearest neighbors search.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 18:05:51 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 15:07:06 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Ishaq", "Najib", ""], ["Student", "George", ""], ["Daniels", "Noah M.", ""]]}, {"id": "1908.08623", "submitter": "Surjyendu Ray", "authors": "Surjyendu Ray, Bei Jia, Sam Safavi, Tim van Opijnen, Ralph Isberg,\n  Jason Rosch and Jos\\'e Bento", "title": "Exact inference under the perfect phylogeny model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivation: Many inference tools use the Perfect Phylogeny Model (PPM) to\nlearn trees from noisy variant allele frequency (VAF) data. Learning in this\nsetting is hard, and existing tools use approximate or heuristic algorithms. An\nalgorithmic improvement is important to help disentangle the limitations of the\nPPM's assumptions from the limitations in our capacity to learn under it.\nResults: We make such improvement in the scenario, where the mutations that are\nrelevant for evolution can be clustered into a small number of groups, and the\ntrees to be reconstructed have a small number of nodes. We use a careful\ncombination of algorithms, software, and hardware, to develop EXACT: a tool\nthat can explore the space of all possible phylogenetic trees, and performs\nexact inference under the PPM with noisy data. EXACT allows users to obtain not\njust the most-likely tree for some input data, but exact statistics about the\ndistribution of trees that might explain the data. We show that EXACT\noutperforms several existing tools for this same task. Availability:\nhttps://github.com/surjray-repos/EXACT\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 23:50:31 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Ray", "Surjyendu", ""], ["Jia", "Bei", ""], ["Safavi", "Sam", ""], ["van Opijnen", "Tim", ""], ["Isberg", "Ralph", ""], ["Rosch", "Jason", ""], ["Bento", "Jos\u00e9", ""]]}, {"id": "1908.08691", "submitter": "Shao-Heng Ko", "authors": "Shao-Heng Ko, Ying-Chun Lin, Hsu-Chao Lai, Wang-Chien Lee, De-Nian\n  Yang", "title": "On VR Spatial Query for Dual Entangled Worlds", "comments": "20 pages, 39 figures. A shorter version of this paper has been\n  accepted for publication in the 28th ACM International Conference on\n  Information and Knowledge Management (CIKM 2019); this is an expanded version\n  containing supplementary details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advent of Virtual Reality (VR) technology and virtual tour\napplications, there is a research need on spatial queries tailored for\nsimultaneous movements in both the physical and virtual worlds. Traditional\nspatial queries, designed mainly for one world, do not consider the entangled\ndual worlds in VR. In this paper, we first investigate the fundamental\nshortest-path query in VR as the building block for spatial queries, aiming to\navoid hitting boundaries and obstacles in the physical environment by\nleveraging Redirected Walking (RW) in Computer Graphics. Specifically, we first\nformulate Dual-world Redirected-walking Obstacle-free Path (DROP) to find the\nminimum-distance path in the virtual world, which is constrained by the RW cost\nin the physical world to ensure immersive experience in VR. We prove DROP is\nNP-hard and design a fully polynomial-time approximation scheme, Dual Entangled\nWorld Navigation (DEWN), by finding Minimum Immersion Loss Range (MIL Range).\nAfterward, we show that the existing spatial query algorithms and index\nstructures can leverage DEWN as a building block to support kNN and range\nqueries in the dual worlds of VR. Experimental results and a user study with\nimplementation in HTC VIVE manifest that DEWN outperforms the baselines with\nsmoother RW operations in various VR scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 06:53:07 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Ko", "Shao-Heng", ""], ["Lin", "Ying-Chun", ""], ["Lai", "Hsu-Chao", ""], ["Lee", "Wang-Chien", ""], ["Yang", "De-Nian", ""]]}, {"id": "1908.08708", "submitter": "Patrizio Angelini", "authors": "Patrizio Angelini, Henry F\\\"orster, Michael Hoffmann, Michael\n  Kaufmann, Stephen Kobourov, Giuseppe Liotta, Maurizio Patrignani", "title": "The QuaSEFE Problem", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of Simultaneous Graph Embedding with Fixed Edges in the\nbeyond planarity framework. In the QuaSEFE problem, we allow edge crossings, as\nlong as each graph individually is drawn quasiplanar, that is, no three edges\npairwise cross. We show that a triple consisting of two planar graphs and a\ntree admit a QuaSEFE. This result also implies that a pair consisting of a\n1-planar graph and a planar graph admits a QuaSEFE. We show several other\npositive results for triples of planar graphs, in which certain structural\nproperties for their common subgraphs are fulfilled. For the case in which\nsimplicity is also required, we give a triple consisting of two quasiplanar\ngraphs and a star that does not admit a QuaSEFE. Moreover, in contrast to the\nplanar SEFE problem, we show that it is not always possible to obtain a QuaSEFE\nfor two matchings if the quasiplanar drawing of one matching is fixed.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 08:07:58 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Angelini", "Patrizio", ""], ["F\u00f6rster", "Henry", ""], ["Hoffmann", "Michael", ""], ["Kaufmann", "Michael", ""], ["Kobourov", "Stephen", ""], ["Liotta", "Giuseppe", ""], ["Patrignani", "Maurizio", ""]]}, {"id": "1908.08762", "submitter": "John Chen", "authors": "John Chen, Ben Coleman, Anshumali Shrivastava", "title": "Revisiting Consistent Hashing with Bounded Loads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic load balancing lies at the heart of distributed caching. Here, the\ngoal is to assign objects (load) to servers (computing nodes) in a way that\nprovides load balancing while at the same time dynamically adjusts to the\naddition or removal of servers. One essential requirement is that the addition\nor removal of small servers should not require us to recompute the complete\nassignment. A popular and widely adopted solution is the two-decade-old\nConsistent Hashing (CH). Recently, an elegant extension was provided to account\nfor server bounds. In this paper, we identify that existing methodologies for\nCH and its variants suffer from cascaded overflow, leading to poor load\nbalancing. This cascading effect leads to decreasing performance of the hashing\nprocedure with increasing load. To overcome the cascading effect, we propose a\nsimple solution to CH based on recent advances in fast minwise hashing. We\nshow, both theoretically and empirically, that our proposed solution is\nsignificantly superior for load balancing and is optimal in many senses. On the\nAOL search dataset and Indiana University Clicks dataset with real user\nactivity, our proposed solution reduces cache misses by several magnitudes.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 11:27:47 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 15:11:37 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chen", "John", ""], ["Coleman", "Ben", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1908.08882", "submitter": "Peter Stumpf", "authors": "Ignaz Rutter, Darren Strash, Peter Stumpf, Michael Vollmer", "title": "Simultaneous Representation of Proper and Unit Interval Graphs", "comments": "28 pages, 16 figures, presented on ESA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a confluence of combinatorics and geometry, simultaneous representations\nprovide a way to realize combinatorial objects that share common structure. A\nstandard case in the study of simultaneous representations is the sunflower\ncase where all objects share the same common structure. While the recognition\nproblem for general simultaneous interval graphs -- the simultaneous version of\narguably one of the most well-studied graph classes -- is NP-complete, the\ncomplexity of the sunflower case for three or more simultaneous interval graphs\nis currently open. In this work we settle this question for proper interval\ngraphs. We give an algorithm to recognize simultaneous proper interval graphs\nin linear time in the sunflower case where we allow any number of simultaneous\ngraphs. Simultaneous unit interval graphs are much more 'rigid' and therefore\nhave less freedom in their representation. We show they can be recognized in\ntime O(|V|*|E|) for any number of simultaneous graphs in the sunflower case\nwhere G = (V, E) is the union of the simultaneous graphs. We further show that\nboth recognition problems are in general NP-complete if the number of\nsimultaneous graphs is not fixed. The restriction to the sunflower case is in\nthis sense necessary.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 15:52:30 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Rutter", "Ignaz", ""], ["Strash", "Darren", ""], ["Stumpf", "Peter", ""], ["Vollmer", "Michael", ""]]}, {"id": "1908.08905", "submitter": "Martin N\\\"ollenburg", "authors": "Matthias Hummel, Fabian Klute, Soeren Nickel, Martin N\\\"ollenburg", "title": "Maximizing Ink in Partial Edge Drawings of k-plane Graphs", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial edge drawing (PED) is a drawing style for non-planar graphs, in which\nedges are drawn only partially as pairs of opposing stubs on the respective\nend-vertices. In a PED, by erasing the central parts of edges, all edge\ncrossings and the resulting visual clutter are hidden in the undrawn parts of\nthe edges. In symmetric partial edge drawings (SPEDs), the two stubs of each\nedge are required to have the same length. It is known that maximizing the ink\n(or the total stub length) when transforming a straight-line graph drawing with\ncrossings into a SPED is tractable for 2-plane input drawings, but NP-hard for\nunrestricted inputs. We show that the problem remains NP-hard even for 3-plane\ninput drawings and establish NP-hardness of ink maximization for PEDs of\n4-plane graphs. Yet, for k-plane input drawings whose edge intersection graph\nforms a collection of trees or, more generally, whose intersection graph has\nbounded treewidth, we present efficient algorithms for computing maximum-ink\nPEDs and SPEDs. We implemented the treewidth-based algorithms and show a brief\nexperimental evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 17:08:43 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 12:55:51 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hummel", "Matthias", ""], ["Klute", "Fabian", ""], ["Nickel", "Soeren", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "1908.08911", "submitter": "Martin N\\\"ollenburg", "authors": "Sujoy Bhore, Robert Ganian, Fabrizio Montecchiani, Martin N\\\"ollenburg", "title": "Parameterized Algorithms for Book Embedding Problems", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A k-page book embedding of a graph G draws the vertices of G on a line and\nthe edges on k half-planes (called pages) bounded by this line, such that no\ntwo edges on the same page cross. We study the problem of determining whether G\nadmits a k-page book embedding both when the linear order of the vertices is\nfixed, called Fixed-Order Book Thickness, or not fixed, called Book Thickness.\nBoth problems are known to be NP-complete in general. We show that Fixed-Order\nBook Thickness and Book Thickness are fixed-parameter tractable parameterized\nby the vertex cover number of the graph and that Fixed-Order Book Thickness is\nfixed-parameter tractable parameterized by the pathwidth of the vertex order.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 17:36:13 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Bhore", "Sujoy", ""], ["Ganian", "Robert", ""], ["Montecchiani", "Fabrizio", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "1908.08938", "submitter": "Martin N\\\"ollenburg", "authors": "Philipp de Col, Fabian Klute, Martin N\\\"ollenburg", "title": "Mixed Linear Layouts: Complexity, Heuristics, and Experiments", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-page linear graph layout of a graph $G = (V,E)$ draws all vertices\nalong a line $\\ell$ and each edge in one of $k$ disjoint halfplanes called\npages, which are bounded by $\\ell$. We consider two types of pages. In a stack\npage no two edges should cross and in a queue page no edge should be nested by\nanother edge. A crossing (nesting) in a stack (queue) page is called a\nconflict. The algorithmic problem is twofold and requires to compute (i) a\nvertex ordering and (ii) a page assignment of the edges such that the resulting\nlayout is either conflict-free or conflict-minimal. While linear layouts with\nonly stack or only queue pages are well-studied, mixed $s$-stack $q$-queue\nlayouts for $s,q \\ge 1$ have received less attention. We show NP-completeness\nresults on the recognition problem of certain mixed linear layouts and present\na new heuristic for minimizing conflicts. In a computational experiment for the\ncase $s, q = 1$ we show that the new heuristic is an improvement over previous\nheuristics for linear layouts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 17:59:39 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["de Col", "Philipp", ""], ["Klute", "Fabian", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "1908.09041", "submitter": "Neil Lutz", "authors": "Christopher Jung, Sampath Kannan, Neil Lutz", "title": "A Center in Your Neighborhood: Fairness in Facility Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When selecting locations for a set of facilities, standard clustering\nalgorithms may place unfair burden on some individuals and neighborhoods. We\nformulate a fairness concept that takes local population densities into\naccount. In particular, given $k$ facilities to locate and a population of size\n$n$, we define the \"neighborhood radius\" of an individual $i$ as the minimum\nradius of a ball centered at $i$ that contains at least $n/k$ individuals. Our\nobjective is to ensure that each individual has a facility within at most a\nsmall constant factor of her neighborhood radius. We present several\ntheoretical results:\n  We show that optimizing this factor is NP-hard; we give an approximation\nalgorithm that guarantees a factor of at most 2 in all metric spaces; and we\nprove matching lower bounds in some metric spaces. We apply a variant of this\nalgorithm to real-world address data, showing that it is quite different from\nstandard clustering algorithms and outperforms them on our objective function\nand balances the load between facilities more evenly.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 22:04:57 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 15:42:36 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Jung", "Christopher", ""], ["Kannan", "Sampath", ""], ["Lutz", "Neil", ""]]}, {"id": "1908.09125", "submitter": "Sara Giuliani", "authors": "Sara Giuliani, Zsuzsanna Lipt\\'ak, Francesco Masillo, Romeo Rizzi", "title": "When a Dollar Makes a BWT", "comments": "This is the journal version of paper at ICTCS 2019 (20th Italian\n  Conference on Theoretical Computer Science, 9-11 Sept. 2019, Como, Italy).\n  Journal version appeared in TCS 2021", "journal-ref": "Theoretical Computer Science 857: 123-146 (2021)", "doi": "10.1016/j.tcs.2021.01.008", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler-Transform (BWT) is a reversible string transformation\nwhich plays a central role in text compression and is fundamental in many\nmodern bioinformatics applications. The BWT is a permutation of the characters,\nwhich is in general better compressible and allows to answer several different\nquery types more efficiently than the original string.\n  It is easy to see that not every string is a BWT image, and exact\ncharacterizations of BWT images are known. We investigate a related\ncombinatorial question. In many applications, a sentinel character dollar is\nadded to mark the end of the string, and thus the BWT of a string ending with\ndollar contains exactly one dollar-character. Given a string w, we ask in which\npositions, if any, the dollar-character can be inserted to turn w into the BWT\nimage of a word ending with dollar. We show that this depends only on the\nstandard permutation of w and present a O(n log n)-time algorithm for\nidentifying all such positions, improving on the naive quadratic time\nalgorithm. We also give a combinatorial characterization of such positions and\ndevelop bounds on their number and value. This is an extended version of\n[Giuliani et al. ICTCS 2019].\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 11:26:37 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 10:00:03 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 17:46:21 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Giuliani", "Sara", ""], ["Lipt\u00e1k", "Zsuzsanna", ""], ["Masillo", "Francesco", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1908.09135", "submitter": "Kiyohito Nagano", "authors": "Kiyohito Nagano, Akihiro Kishimoto", "title": "Subadditive Load Balancing", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set function optimization is essential in AI and machine learning. We focus\non a subadditive set function that generalizes submodularity, and examine the\nsubadditivity of non-submodular functions. We also deal with a minimax\nsubadditive load balancing problem, and present a modularization-minimization\nalgorithm that theoretically guarantees a worst-case approximation factor. In\naddition, we give a lower bound computation technique for the problem. We apply\nthese methods to the multi-robot routing problem for an empirical performance\nevaluation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 13:27:38 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Nagano", "Kiyohito", ""], ["Kishimoto", "Akihiro", ""]]}, {"id": "1908.09151", "submitter": "Peter Zeman", "authors": "V\\'it Kalisz, Pavel Klav\\'ik, Peter Zeman", "title": "Circle Graph Isomorphism in Almost Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circle graphs are intersection graphs of chords of a circle. In this paper,\nwe present a new algorithm for the circle graph isomorphism problem running in\ntime $O((n+m)\\alpha(n+m))$ where $n$ is the number of vertices, $m$ is the\nnumber of edges and $\\alpha$ is the inverse Ackermann function. Our algorithm\nis based on the minimal split decomposition [Cunnigham, 1982] and uses the\nstate-of-art circle graph recognition algorithm [Gioan, Paul, Tedder, Corneil,\n2014] in the same running time. It improves the running time $O(nm)$ of the\nprevious algorithm [Hsu, 1995] based on a similar approach.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 15:17:27 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kalisz", "V\u00edt", ""], ["Klav\u00edk", "Pavel", ""], ["Zeman", "Peter", ""]]}, {"id": "1908.09278", "submitter": "Shmuel Onn", "authors": "Gabriel Deza, Shmuel Onn", "title": "Optimization over Degree Sequences of Graphs", "comments": null, "journal-ref": "Discrete Applied Mathematics, 296:2--8, 2021", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a subgraph of a given graph minimizing the\nsum of given functions at vertices evaluated at their subgraph degrees. While\nthe problem is NP-hard already for bipartite graphs when the functions are\nconvex on one side and concave on the other, we show that when all functions\nare convex, the problem can be solved in polynomial time for any graph. We also\nprovide polynomial time solutions for bipartite graphs with one side fixed for\narbitrary functions, and for arbitrary graphs when all but a fixed number of\nfunctions are either nondecreasing or nonincreasing. We note that the general\nfactor problem and the (l,u)-factor problem over a graph are special cases of\nour problem, as well as the intriguing exact matching problem. The complexity\nof the problem remains widely open, particularly for arbitrary functions over\ncomplete graphs.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 08:36:51 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Deza", "Gabriel", ""], ["Onn", "Shmuel", ""]]}, {"id": "1908.09318", "submitter": "Fabrizio Frati", "authors": "Manuel Borrazzo, Giordano Da Lozzo, Giuseppe Di Battista, Fabrizio\n  Frati, and Maurizio Patrignani", "title": "Graph Stories in Small Area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of drawing a dynamic graph, where each vertex appears in\nthe graph at a certain time and remains in the graph for a fixed amount of\ntime, called the window size. This defines a graph story, i.e., a sequence of\nsubgraphs, each induced by the vertices that are in the graph at the same time.\nThe drawing of a graph story is a sequence of drawings of such subgraphs. To\nsupport readability, we require that each drawing is straight-line and planar\nand that each vertex maintains its placement in all the drawings. Ideally, the\narea of the drawing of each subgraph should be a function of the window size,\nrather than a function of the size of the entire graph, which could be too\nlarge. We show that the graph stories of paths and trees can be drawn on a $2W\n\\times 2W$ and on an $(8W + 1) \\times (8W + 1)$ grid, respectively, where $W$\nis the window size. These results are constructive and yield linear-time\nalgorithms. Further, we show that there exist graph stories of planar graphs\nwhose subgraphs cannot be drawn within an area that is only a function of $W$.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 13:03:08 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 05:41:29 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Borrazzo", "Manuel", ""], ["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""]]}, {"id": "1908.09325", "submitter": "Yoichi Iwata", "authors": "\\'Edouard Bonnet, Yoichi Iwata, Bart M. P. Jansen, {\\L}ukasz Kowalik", "title": "Fine-Grained Complexity of k-OPT in Bounded-Degree Graphs for Solving\n  TSP", "comments": "A new running time bound for counting cycles and paths in graphs was\n  added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search is a widely-employed strategy for finding good solutions to\nTraveling Salesman Problem. We analyze the problem of determining whether the\nweight of a given cycle can be decreased by a popular $k$-opt move. Earlier\nwork has shown that (i) assuming the Exponential Time Hypothesis, there is no\nalgorithm to find an improving $k$-opt move in time $f(k)n^{o(k/\\log k)}$ for\nany function $f$, while (ii) it is possible to improve on the brute-force\nrunning time of $O(n^k)$ and save linear factors in the exponent. Modern TSP\nheuristics show that very good global solutions can already be reached using\nonly the top-$O(1)$ most promising edges incident to each vertex. Motivated by\nthis, we study the problem of finding an improving $k$-move in bounded degree\ngraphs, presenting new algorithms and conditional lower bounds. We show that\nthe aforementioned ETH lower bound also holds for graphs of maximum degree\nthree, but that in bounded-degree graphs the best improving $k$-move can be\nfound in time $O(n^{23k/135+o(k)})$. This improves upon the best-known bounds\nfor general graphs. Due to its practical importance, we devote special\nattention to the range of $k$ in which improving $k$-moves in bounded-degree\ngraphs can be found in quasi-linear time. For $k\\le 7$, we give quasi-linear\ntime algorithms for general weights. For $k=8$ we obtain a quasi-linear time\nalgorithm for polylogarithmic weights. On the other hand, based on established\nfine-grained complexity hypotheses, we prove that the $k=9$ case does not admit\nquasi-linear time algorithms. Hence we fully characterize the values of $k$ for\nwhich quasi-linear time algorithms exist for polylogarithmic weights on\nbounded-degree graphs. As a byproduct, we show a new bound on pathwidth of even\ngraphs which results in improved running time bounds for counting $k$-vertex\npaths and cycles.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 13:28:53 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 07:01:59 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Iwata", "Yoichi", ""], ["Jansen", "Bart M. P.", ""], ["Kowalik", "\u0141ukasz", ""]]}, {"id": "1908.09378", "submitter": "Ben Karsin", "authors": "John Iacono, Ben Karsin, Nodari Sitchinava", "title": "A parallel priority queue with fast updates for GPU architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high computational throughput of modern graphics processing units (GPUs)\nmake them the de-facto architecture for high-performance computing\napplications. However, to achieve peak performance, GPUs require highly\nparallel workloads, as well as memory access patterns that exhibit good\nlocality of reference. As a result, many state-of-the-art algorithms and data\nstructures designed for GPUs sacrifice work-optimality to achieve the necessary\nparallelism. Furthermore, some abstract data types are avoided completely due\nto there being no corresponding data structure that performs well on the GPU.\nOne such abstract data type is the priority queue. Many well-known algorithms\nrely on priority queue operations as a building block. While various priority\nqueue structures have been developed that are parallel, cache-aware, or\ncache-oblivious, none has been shown to be efficient on GPUs. In this paper, we\npresent the parBucketHeap, a parallel, cache-efficient data structure designed\nfor modern GPU architectures that supports standard priority queue operations,\nas well as bulk update. We analyze the structure in several well-known\ncomputational models and show that it provides both optimal parallelism and is\ncache-efficient. We implement the parBucketHeap and, using it, we solve the\nsingle-source shortest path (SSSP) problem. Experimental results indicate that,\nfor sufficiently large, dense graphs with high diameter, we out-perform current\nstate-of-the-art SSSP algorithms on the GPU by up to a factor of 5. Unlike\nexisting GPU SSSP algorithms, our approach is work-optimal and places\nsignificantly less load on the GPU, reducing power consumption.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 19:23:42 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Iacono", "John", ""], ["Karsin", "Ben", ""], ["Sitchinava", "Nodari", ""]]}, {"id": "1908.09586", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Diana-Elena F\\u{a}l\\u{a}ma\\c{s}, R\\'emi Watrigant", "title": "Constraint Generation Algorithm for the Minimum Connectivity Inference\n  Problem", "comments": "16 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a hypergraph $H$, the Minimum Connectivity Inference problem asks for a\ngraph on the same vertex set as $H$ with the minimum number of edges such that\nthe subgraph induced by every hyperedge of $H$ is connected. This problem has\nreceived a lot of attention these recent years, both from a theoretical and\npractical perspective, leading to several implemented approximation, greedy and\nheuristic algorithms. Concerning exact algorithms, only Mixed Integer Linear\nProgramming (MILP) formulations have been experimented, all representing\nconnectivity constraints by the means of graph flows. In this work, we\ninvestigate the efficiency of a constraint generation algorithm, where we\niteratively add cut constraints to a simple ILP until a feasible (and optimal)\nsolution is found. It turns out that our method is faster than the previous\nbest flow-based MILP algorithm on random generated instances, which suggests\nthat a constraint generation approach might be also useful for other\noptimization problems dealing with connectivity constraints. At last, we\npresent the results of an enumeration algorithm for the problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 10:44:07 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["F\u0103l\u0103ma\u015f", "Diana-Elena", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "1908.09618", "submitter": "Miguel Coviello Gonzalez", "authors": "Miguel Coviello Gonzalez, Marek Chrobak", "title": "A Waste-Efficient Algorithm for Single-Droplet Sample Preparation on\n  Microfluidic Chips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of designing micro-fluidic chips for sample\npreparation, which is a crucial step in many experimental processes in chemical\nand biological sciences. One of the objectives of sample preparation is to\ndilute the sample fluid, called reactant, using another fluid called buffer, to\nproduce desired volumes of fluid with prespecified reactant concentrations. In\nthe model we adopt, these fluids are manipulated in discrete volumes called\ndroplets. The dilution process is represented by a mixing graph whose nodes\nrepresent 1-1 micro-mixers and edges represent channels for transporting\nfluids. In this work we focus on designing such mixing graphs when the given\nsample (also referred to as the target) consists of a single-droplet, and the\nobjective is to minimize total fluid waste. Our main contribution is an\nefficient algorithm called RPRIS that guarantees a better provable worst-case\nbound on waste and significantly outperforms state-of-the-art algorithms in\nexperimental comparison.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 00:39:03 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 20:17:35 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 06:07:07 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Gonzalez", "Miguel Coviello", ""], ["Chrobak", "Marek", ""]]}, {"id": "1908.09648", "submitter": "Nicolas Dupin", "authors": "Nicolas Dupin, Frank Nielsen, El-Ghazali Talbi", "title": "Planar p-center problems are solvable in polynomial time when clustering\n  a Pareto Front", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is motivated by real-life applications of bi-objective\noptimization. Having many non dominated solutions, one wishes to cluster the\nPareto front using Euclidian distances. The p-center problems, both in the\ndiscrete and continuous versions, are proven solvable in polynomial time with a\ncommon dynamic programming algorithm. Having $N$ points to partition in\n$K\\geqslant 3$ clusters, the complexity is proven in $O(KN\\log N)$ (resp\n$O(KN\\log^2 N)$) time and $O(KN)$ memory space for the continuous (resp\ndiscrete) $K$-center problem. $2$-center problems have complexities in $O(N\\log\nN)$. To speed-up the algorithm, parallelization issues are discussed. A\nposteriori, these results allow an application inside multi-objective\nheuristics to archive partial Pareto Fronts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 22:24:07 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Dupin", "Nicolas", ""], ["Nielsen", "Frank", ""], ["Talbi", "El-Ghazali", ""]]}, {"id": "1908.09808", "submitter": "Elaheh Fata", "authors": "Elaheh Fata, Will Ma, David Simchi-Levi", "title": "Multi-stage and Multi-customer Assortment Optimization with Inventory\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an assortment optimization problem where a customer chooses a\nsingle item from a sequence of sets shown to her, while limited inventories\nconstrain the items offered to customers over time. In the special case where\nall of the assortments have size one, our problem captures the online\nstochastic matching with timeouts problem. For this problem, we derive a\npolynomial-time approximation algorithm which earns at least 1-ln(2-1/e), or\n0.51, of the optimum. This improves upon the previous-best approximation ratio\nof 0.46, and furthermore, we show that it is tight. For the general assortment\nproblem, we establish the first constant-factor approximation ratio of 0.09 for\nthe case that different types of customers value items differently, and an\napproximation ratio of 0.15 for the case that different customers value each\nitem the same. Our algorithms are based on rounding an LP relaxation for\nmulti-stage assortment optimization, and improve upon previous randomized\nrounding schemes to derive the tight ratio of 1-ln(2-1/e).\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 17:30:41 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 21:35:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Fata", "Elaheh", ""], ["Ma", "Will", ""], ["Simchi-Levi", "David", ""]]}, {"id": "1908.09970", "submitter": "Raef Bassily", "authors": "Raef Bassily, Vitaly Feldman, Kunal Talwar, Abhradeep Thakurta", "title": "Private Stochastic Convex Optimization with Optimal Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study differentially private (DP) algorithms for stochastic convex\noptimization (SCO). In this problem the goal is to approximately minimize the\npopulation loss given i.i.d. samples from a distribution over convex and\nLipschitz loss functions. A long line of existing work on private convex\noptimization focuses on the empirical loss and derives asymptotically tight\nbounds on the excess empirical loss. However a significant gap exists in the\nknown bounds for the population loss. We show that, up to logarithmic factors,\nthe optimal excess population loss for DP algorithms is equal to the larger of\nthe optimal non-private excess population loss, and the optimal excess\nempirical loss of DP algorithms. This implies that, contrary to intuition based\non private ERM, private SCO has asymptotically the same rate of $1/\\sqrt{n}$ as\nnon-private SCO in the parameter regime most common in practice. The best\nprevious result in this setting gives rate of $1/n^{1/4}$. Our approach builds\non existing differentially private algorithms and relies on the analysis of\nalgorithmic stability to ensure generalization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 00:50:27 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Bassily", "Raef", ""], ["Feldman", "Vitaly", ""], ["Talwar", "Kunal", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "1908.10132", "submitter": "Robert Ganian", "authors": "Eduard Eiben, Robert Ganian, Thekla Hamm, O-joung Kwon", "title": "Measuring what Matters: A Hybrid Approach to Dynamic Programming with\n  Treewidth", "comments": "Appeared at MFCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for applying treewidth-based dynamic programming on\ngraphs with \"hybrid structure\", i.e., with parts that may not have small\ntreewidth but instead possess other structural properties. Informally, this is\nachieved by defining a refinement of treewidth which only considers parts of\nthe graph that do not belong to a pre-specified tractable graph class. Our\napproach allows us to not only generalize existing fixed-parameter algorithms\nexploiting treewidth, but also fixed-parameter algorithms which use the size of\na modulator as their parameter. As the flagship application of our framework,\nwe obtain a parameter that combines treewidth and rank-width to obtain\nfixed-parameter algorithms for Chromatic Number, Hamiltonian Cycle, and\nMax-Cut.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 11:19:03 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Hamm", "Thekla", ""], ["Kwon", "O-joung", ""]]}, {"id": "1908.10159", "submitter": "Philip Bille", "authors": "Philip Bille and Inge Li G{\\o}rtz and Frederik Rye Skjoldjensen", "title": "Partial Sums on the Ultra-Wide Word RAM", "comments": "Extended abstract appeared at TAMC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic partial sums problem on the ultra-wide word RAM model\nof computation. This model extends the classic $w$-bit word RAM model with\nspecial ultrawords of length $w^2$ bits that support standard arithmetic and\nboolean operation and scattered memory access operations that can access $w$\n(non-contiguous) locations in memory. The ultra-wide word RAM model captures\n(and idealizes) modern vector processor architectures.\n  Our main result is a new in-place data structure for the partial sum problem\nthat only stores a constant number of ultraword in addition to the input and\nsupports operations in doubly logarithmic time. This matches the best known\ntime bounds for the problem (among polynomial space data structures) while\nimproving the space from superlinear to a constant number of ultrawords. Our\nresults are based on a simple and elegant in-place word RAM data structure,\nknown as the Fenwick tree. Our main technical contribution is a new efficient\nparallel ultra-wide word RAM implementation of the Fenwick tree, which is\nlikely of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 12:33:32 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 12:09:37 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Skjoldjensen", "Frederik Rye", ""]]}, {"id": "1908.10170", "submitter": "Gabor Elek", "authors": "G\\'abor Elek", "title": "Learning Very Large Graphs with Unknown Vertex Distributions", "comments": "Change in the title. The property testing process is explained in\n  details, some short sections are added, the exposition is made more reader\n  friendly. 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Goldreich introduced the notion of property testing of\nbounded-degree graphs with an unknown distribution. We propose a slight\nmodification of his idea: the Radon-Nikodym Oracles. Using these oracles any\nreasonable graph property can be tested in constant-time against any reasonable\nunknown distribution in the category of planar graphs. We also discuss\nRandomized Local Distributed Algorithms, which work on very large graphs with\nunknown distributions. Finally, we discuss how can we learn graph properties\nusing observations instead of samplings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 12:57:05 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 18:59:36 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Elek", "G\u00e1bor", ""]]}, {"id": "1908.10284", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Lorenzo Rosasco", "title": "Statistical and Computational Trade-Offs in Kernel K-Means", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, pp. 9357-9367.\n  2018", "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the efficiency of k-means in terms of both statistical and\ncomputational requirements. More precisely, we study a Nystr\\\"om approach to\nkernel k-means. We analyze the statistical properties of the proposed method\nand show that it achieves the same accuracy of exact kernel k-means with only a\nfraction of computations. Indeed, we prove under basic assumptions that\nsampling $\\sqrt{n}$ Nystr\\\"om landmarks allows to greatly reduce computational\ncosts without incurring in any loss of accuracy. To the best of our knowledge\nthis is the first result of this kind for unsupervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:43:49 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Calandriello", "Daniele", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1908.10388", "submitter": "Maxwell Young", "authors": "Qian M. Zhou, Aiden Calvert, Maxwell Young", "title": "Singletons for Simpletons: Revisiting Windowed Backoff using Chernoff\n  Bounds", "comments": "Corrections to first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backoff algorithms are used in many distributed systems where multiple\ndevices contend for a shared resource. For the classic balls-into-bins problem,\nthe number of singletons---those bins with a single ball---is important to the\nanalysis of several backoff algorithms; however, existing analyses employ\nadvanced probabilistic tools to obtain concentration bounds. Here, we show that\nstandard Chernoff bounds can be used instead, and the simplicity of this\napproach is illustrated by re-analyzing some well-known backoff algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 18:17:24 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 14:28:18 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Zhou", "Qian M.", ""], ["Calvert", "Aiden", ""], ["Young", "Maxwell", ""]]}, {"id": "1908.10410", "submitter": "Daniel Probst", "authors": "Daniel Probst and Jean-Louis Reymond", "title": "Visualization of Very Large High-Dimensional Data Sets as Minimum\n  Spanning Trees", "comments": "33 pages, 14 figures, 1 table, supplementary information included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chemical sciences are producing an unprecedented amount of large,\nhigh-dimensional data sets containing chemical structures and associated\nproperties. However, there are currently no algorithms to visualize such data\nwhile preserving both global and local features with a sufficient level of\ndetail to allow for human inspection and interpretation. Here, we propose a\nsolution to this problem with a new data visualization method, TMAP, capable of\nrepresenting data sets of up to millions of data points and arbitrary high\ndimensionality as a two-dimensional tree (http://tmap.gdb.tools).\nVisualizations based on TMAP are better suited than t-SNE or UMAP for the\nexploration and interpretation of large data sets due to their tree-like\nnature, increased local and global neighborhood and structure preservation, and\nthe transparency of the methods the algorithm is based on. We apply TMAP to the\nmost used chemistry data sets including databases of molecules such as ChEMBL,\nFDB17, the Natural Products Atlas, DSSTox, as well as to the MoleculeNet\nbenchmark collection of data sets. We also show its broad applicability with\nfurther examples from biology, particle physics, and literature.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 15:14:19 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 10:43:40 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 14:32:02 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Probst", "Daniel", ""], ["Reymond", "Jean-Louis", ""]]}, {"id": "1908.10550", "submitter": "Venkata Rohit Jakkula", "authors": "Venkata Rohit Jakkula, George Karypis", "title": "Streaming and Batch Algorithms for Truss Decomposition", "comments": "9 pages, Graph Computing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truss decomposition is a method used to analyze large sparse graphs in order\nto identify successively better connected subgraphs. Since in many domains the\nunderlying graph changes over time, its associated truss decomposition needs to\nbe updated as well. This work focuses on the problem of incrementally updating\nan existing truss decomposition and makes the following three significant\ncontributions. First, it presents a theory that identifies how the truss\ndecomposition can change as new edges get added. Second, it develops an\nefficient incremental algorithm that incorporates various optimizations to\nupdate the truss decomposition after every edge addition. These optimizations\nare designed to reduce the number of edges that are explored by the algorithm.\nThird, it extends this algorithm to batch updates (i.e., where the truss\ndecomposition needs to be updated after a set of edges are added), which\nreduces the overall computations that need to be performed. We evaluated the\nperformance of our algorithms on real-world datasets. Our incremental algorithm\nachieves over 250000x average speedup for inserting an edge in a graph with 10\nmillion edges relative to the non-incremental algorithm. Further, our\nexperiments on batch updates show that our batch algorithm consistently\nperforms better than the incremental algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 05:02:18 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Jakkula", "Venkata Rohit", ""], ["Karypis", "George", ""]]}, {"id": "1908.10561", "submitter": "Clemens Thielen", "authors": "Arne Herzel, Cristina Bazgan, Stefan Ruzika, Clemens Thielen, Daniel\n  Vanderpooten", "title": "One-Exact Approximate Pareto Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Papadimitriou and Yannakakis show that the polynomial-time solvability of a\ncertain singleobjective problem determines the class of multiobjective\noptimization problems that admit a polynomial-time computable $(1+\\varepsilon,\n\\dots , 1+\\varepsilon)$-approximate Pareto set (also called an\n$\\varepsilon$-Pareto set). Similarly, in this article, we characterize the\nclass of problems having a polynomial-time computable approximate\n$\\varepsilon$-Pareto set that is exact in one objective by the efficient\nsolvability of an appropriate singleobjective problem. This class includes\nimportant problems such as multiobjective shortest path and spanning tree, and\nthe approximation guarantee we provide is, in general, best possible.\nFurthermore, for biobjective problems from this class, we provide an algorithm\nthat computes a one-exact $\\varepsilon$-Pareto set of cardinality at most twice\nthe cardinality of a smallest such set and show that this factor of 2 is best\npossible. For three or more objective functions, however, we prove that no\nconstant-factor approximation on the size of the set can be obtained\nefficiently.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 06:05:11 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Herzel", "Arne", ""], ["Bazgan", "Cristina", ""], ["Ruzika", "Stefan", ""], ["Thielen", "Clemens", ""], ["Vanderpooten", "Daniel", ""]]}, {"id": "1908.10583", "submitter": "Wenqing Lin", "authors": "Sibo Wang, Renchi Yang, Runhui Wang, Xiaokui Xiao, Zhewei Wei, Wenqing\n  Lin, Yin Yang, Nan Tang", "title": "Efficient Algorithms for Approximate Single-Source Personalized PageRank\n  Queries", "comments": "Accepted in the ACM Transactions on Database Systems (TODS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Given a graph $G$, a source node $s$ and a target node $t$, the personalized\nPageRank (PPR) of $t$ with respect to $s$ is the probability that a random walk\nstarting from $s$ terminates at $t$. An important variant of the PPR query is\nsingle-source PPR (SSPPR), which enumerates all nodes in $G$, and returns the\ntop-$k$ nodes with the highest PPR values with respect to a given source $s$.\nPPR in general and SSPPR in particular have important applications in web\nsearch and social networks, e.g., in Twitter's Who-To-Follow recommendation\nservice. However, PPR computation is known to be expensive on large graphs, and\nresistant to indexing. Consequently, previous solutions either use heuristics,\nwhich do not guarantee result quality, or rely on the strong computing power of\nmodern data centers, which is costly.\n  Motivated by this, we propose effective index-free and index-based algorithms\nfor approximate PPR processing, with rigorous guarantees on result quality. We\nfirst present FORA, an approximate SSPPR solution that combines two existing\nmethods Forward Push (which is fast but does not guarantee quality) and Monte\nCarlo Random Walk (accurate but slow) in a simple and yet non-trivial way,\nleading to both high accuracy and efficiency. Further, FORA includes a simple\nand effective indexing scheme, as well as a module for top-$k$ selection with\nhigh pruning power. Extensive experiments demonstrate that the proposed\nsolutions are orders of magnitude more efficient than their respective\ncompetitors. Notably, on a billion-edge Twitter dataset, FORA answers a top-500\napproximate SSPPR query within 1 second, using a single commodity server.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 07:34:08 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Wang", "Sibo", ""], ["Yang", "Renchi", ""], ["Wang", "Runhui", ""], ["Xiao", "Xiaokui", ""], ["Wei", "Zhewei", ""], ["Lin", "Wenqing", ""], ["Yang", "Yin", ""], ["Tang", "Nan", ""]]}, {"id": "1908.10644", "submitter": "Luca Calderoni", "authors": "Luca Calderoni, Dario Maio, Paolo Palmieri", "title": "Bloom filter variants for multiple sets: a comparative assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we compare two probabilistic data structures for association\nqueries derived from the well-known Bloom filter: the shifting Bloom filter\n(ShBF), and the spatial Bloom filter (SBF). With respect to the original data\nstructure, both variants add the ability to store multiple subsets in the same\nfilter, using different strategies. We analyse the performance of the two data\nstructures with respect to false positive probability, and the inter-set error\nprobability (the probability for an element in the set of being recognised as\nbelonging to the wrong subset). As part of our analysis, we extended the\nfunctionality of the shifting Bloom filter, optimising the filter for any\nnon-trivial number of subsets. We propose a new generalised ShBF definition\nwith applications outside of our specific domain, and present new probability\nformulas. Results of the comparison show that the ShBF provides better space\nefficiency, but at a significantly higher computational cost than the SBF.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 11:13:03 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Calderoni", "Luca", ""], ["Maio", "Dario", ""], ["Palmieri", "Paolo", ""]]}, {"id": "1908.10693", "submitter": "Homin Lee", "authors": "Charles Masson and Jee E. Rim and Homin K. Lee", "title": "DDSketch: A fast and fully-mergeable quantile sketch with relative-error\n  guarantees", "comments": "11 pages, 11 figures, VLDB", "journal-ref": "PVLDB, 12(12): 2195-2205, 2019", "doi": "10.14778/3352063.3352135", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Summary statistics such as the mean and variance are easily maintained for\nlarge, distributed data streams, but order statistics (i.e., sample quantiles)\ncan only be approximately summarized. There is extensive literature on\nmaintaining quantile sketches where the emphasis has been on bounding the rank\nerror of the sketch while using little memory. Unfortunately, rank error\nguarantees do not preclude arbitrarily large relative errors, and this often\noccurs in practice when the data is heavily skewed. Given the distributed\nnature of contemporary large-scale systems, another crucial property for\nquantile sketches is mergeablility, i.e., several combined sketches must be as\naccurate as a single sketch of the same data. We present the first\nfully-mergeable, relative-error quantile sketching algorithm with formal\nguarantees. The sketch is extremely fast and accurate, and is currently being\nused by Datadog at a wide-scale.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 12:48:44 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Masson", "Charles", ""], ["Rim", "Jee E.", ""], ["Lee", "Homin K.", ""]]}, {"id": "1908.10846", "submitter": "Patrick Rall", "authors": "Scott Aaronson, Patrick Rall", "title": "Quantum Approximate Counting, Simplified", "comments": "12 pages, 1 figure", "journal-ref": "Symposium on Simplicity in Algorithms. 2020, 24-32", "doi": "10.1137/1.9781611976014.5", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1998, Brassard, Hoyer, Mosca, and Tapp (BHMT) gave a quantum algorithm for\napproximate counting. Given a list of $N$ items, $K$ of them marked, their\nalgorithm estimates $K$ to within relative error $\\varepsilon$ by making only\n$O\\left( \\frac{1}{\\varepsilon}\\sqrt{\\frac{N}{K}}\\right) $ queries. Although\nthis speedup is of \"Grover\" type, the BHMT algorithm has the curious feature of\nrelying on the Quantum Fourier Transform (QFT), more commonly associated with\nShor's algorithm. Is this necessary? This paper presents a simplified\nalgorithm, which we prove achieves the same query complexity using Grover\niterations only. We also generalize this to a QFT-free algorithm for amplitude\nestimation. Related approaches to approximate counting were sketched previously\nby Grover, Abrams and Williams, Suzuki et al., and Wie (the latter two as we\nwere writing this paper), but in all cases without rigorous analysis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 17:34:40 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 23:21:34 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 18:09:45 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2020 14:01:32 GMT"}, {"version": "v5", "created": "Wed, 9 Sep 2020 15:14:59 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Aaronson", "Scott", ""], ["Rall", "Patrick", ""]]}, {"id": "1908.10859", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Yi-An Ma, Martin J. Wainwright, Peter L. Bartlett,\n  Michael I. Jordan", "title": "High-Order Langevin Diffusion Yields an Accelerated MCMC Algorithm", "comments": "Changes from v1: improved algorithm with $O (d^{1/4} /\n  \\varepsilon^{1/2})$ mixing time", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Markov chain Monte Carlo (MCMC) algorithm based on third-order\nLangevin dynamics for sampling from distributions with log-concave and smooth\ndensities. The higher-order dynamics allow for more flexible discretization\nschemes, and we develop a specific method that combines splitting with more\naccurate integration. For a broad class of $d$-dimensional distributions\narising from generalized linear models, we prove that the resulting third-order\nalgorithm produces samples from a distribution that is at most $\\varepsilon >\n0$ in Wasserstein distance from the target distribution in\n$O\\left(\\frac{d^{1/4}}{ \\varepsilon^{1/2}} \\right)$ steps. This result requires\nonly Lipschitz conditions on the gradient. For general strongly convex\npotentials with $\\alpha$-th order smoothness, we prove that the mixing time\nscales as $O \\left(\\frac{d^{1/4}}{\\varepsilon^{1/2}} +\n\\frac{d^{1/2}}{\\varepsilon^{1/(\\alpha - 1)}} \\right)$.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 17:59:29 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 15:10:59 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Mou", "Wenlong", ""], ["Ma", "Yi-An", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1908.10888", "submitter": "James Smith", "authors": "James Smith", "title": "Eliminating Left Recursion without the Epsilon", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard algorithm to eliminate indirect left recursion takes a\npreventative approach, reorganising a grammar's rules so that that indirect\nleft recursion is no longer possible, rather than eliminating it only as and\nwhen it occurs. This approach results in many of the rules being lost, so that\nthe parse trees are often devoid of the detail that the grammar was supposed to\ncapture in the first place. To avoid this pitfall, we revise the standard\nalgorithm to eliminate direct left recursion and then extend it to an algorithm\nthat explicitly eliminates both direct and indirect left recursion. By taking\nthis approach, all of the grammar's original rules are retained, resulting in\nparse trees that are still useful in practice. Furthermore, the vacuous\ndefinitions with $\\epsilon$ parts that the standard algorithm generates are no\nlonger needed, which increases the practical utility of the parse trees that\nresult still more.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 18:04:33 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 11:25:23 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 09:01:06 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Smith", "James", ""]]}, {"id": "1908.10942", "submitter": "Yipu Wang", "authors": "Yipu Wang", "title": "The Single-Face Ideal Orientation Problem in Planar Graphs", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the ideal orientation problem in planar graphs. In this problem,\nwe are given an undirected graph $G$ with positive edge lengths and $k$ pairs\nof distinct vertices $(s_1, t_1), \\dots, (s_k, t_k)$ called terminals, and we\nwant to assign an orientation to each edge such that for all $i$ the distance\nfrom $s_i$ to $t_i$ is preserved or report that no such orientation exists. We\nshow that the problem is NP-hard in planar graphs. On the other hand, we show\nthat the problem is polynomial-time solvable in planar graphs when $k$ is\nfixed, the vertices $s_1, t_1, \\dots, s_k, t_k$ are all on the same face, and\nno two of terminal pairs cross (a pair $(s_i, t_i)$ crosses $(s_j, t_j)$ if the\ncyclic order of the vertices is $s_i,s_j,t_i,t_j$). For serial instances, we\ngive a simpler and faster algorithm running in $O(n \\log n)$ time, even if $k$\nis part of the input. (An instance is serial if the terminals appear in cyclic\norder $u_1, v_1, \\dots, u_k, v_k$, where for each $i$ we have either $(u_i,\nv_i) = (s_i, t_i)$ or $(u_i, v_i) = (t_i, s_i)$.) Finally, we consider a\ngeneralization of the problem in which the sum of the distances from $s_i$ to\n$t_i$ is to be minimized; in this case we give an algorithm for serial\ninstances running in $O(kn^5)$ time.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 20:54:50 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 01:23:13 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Wang", "Yipu", ""]]}, {"id": "1908.10950", "submitter": "Roman H\\\"ollwieser", "authors": "Michael G\\\"unther, Roman H\\\"ollwieser, Francesco Knechtli", "title": "Constrained Hybrid Monte Carlo algorithms for gauge-Higgs models", "comments": "added comparison to one-loop potential in section 3.3, improved text;\n  version accepted for publication in Computer Physics Communications", "journal-ref": null, "doi": "10.1016/j.cpc.2020.107192", "report-no": null, "categories": "hep-lat cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Hybrid Monte Carlo (HMC) algorithms for constrained Hamiltonian\nsystems of gauge- Higgs models and introduce a new observable for the\nconstraint effective Higgs potential. We use an extension of the so-called\nRattle algorithm to general Hamiltonians for constrained systems, which we\nadapt to the 4D Abelian-Higgs model and the 5D SU(2) gauge theory on the torus\nand on the orbifold. The derivative of the potential is measured via the\nexpectation value of the Lagrange multiplier for the constraint condition and\nallows a much more precise determination of the effective potential than\nconventional histogram methods. With the new method, we can access the\npotential over the full domain of the Higgs variable, while the histogram\nmethod is restricted to a short region around the expectation value of the\nHiggs field in unconstrained simulations, and the statistical precision does\nnot deteriorate when the volume is increased. We further verify our results by\ncomparing to the one-loop Higgs potential of the 4D Abelian-Higgs model in\nunitary gauge and find good agreement. To our knowledge, this is the first time\nthis problem has been addressed for theories with gauge fields. The algorithm\ncan also be used in four dimensions to study finite temperature and density\ntransitions via effective Polyakov loop actions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 17:22:56 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 13:33:34 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["G\u00fcnther", "Michael", ""], ["H\u00f6llwieser", "Roman", ""], ["Knechtli", "Francesco", ""]]}, {"id": "1908.11071", "submitter": "Lin Yang", "authors": "Aaron Sidford, Mengdi Wang, Lin F. Yang, Yinyu Ye", "title": "Solving Discounted Stochastic Two-Player Games with Near-Optimal Time\n  and Sample Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we settle the sampling complexity of solving discounted\ntwo-player turn-based zero-sum stochastic games up to polylogarithmic factors.\nGiven a stochastic game with discount factor $\\gamma\\in(0,1)$ we provide an\nalgorithm that computes an $\\epsilon$-optimal strategy with high-probability\ngiven $\\tilde{O}((1 - \\gamma)^{-3} \\epsilon^{-2})$ samples from the transition\nfunction for each state-action-pair. Our algorithm runs in time nearly linear\nin the number of samples and uses space nearly linear in the number of\nstate-action pairs. As stochastic games generalize Markov decision processes\n(MDPs) our runtime and sample complexities are optimal due to Azar et al\n(2013). We achieve our results by showing how to generalize a near-optimal\nQ-learning based algorithms for MDP, in particular Sidford et al (2018), to\ntwo-player strategy computation algorithms. This overcomes limitations of\nstandard Q-learning and strategy iteration or alternating minimization based\napproaches and we hope will pave the way for future reinforcement learning\nresults by facilitating the extension of MDP results to multi-agent settings\nwith little loss.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 07:04:25 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Sidford", "Aaron", ""], ["Wang", "Mengdi", ""], ["Yang", "Lin F.", ""], ["Ye", "Yinyu", ""]]}, {"id": "1908.11105", "submitter": "Juan Carlos Saenz-Carrasco", "authors": "Juan Carlos Saenz-Carrasco", "title": "FunSeqSet: Towards a Purely Functional Data Structure for the\n  Linearisation Case of Dynamic Trees Problem", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic trees, originally described by Sleator and Tarjan, have been studied\ndeeply for non persistent structures providing $\\mathcal{O}(\\log n)$ time for\nupdate and lookup operations as shown in theory and practice by Werneck.\nHowever, discussions on how the most common dynamic trees operations (i.e. link\nand cut) are computed over a purely functional data structure have not been\nstudied. Even more, asking whether vertices $u$ and $v$ are connected (i.e.\nwithin the same forest) assumes that corresponding indices or locations for $u$\nand $v$ are taken for granted in most of the literature, and not performed as\npart of the whole computation for such a question. We present FunSeqSet, based\non the primitive version of finger trees, i.e. the de facto sequence data\nstructure for the purely functional programming language Haskell, augmented\nwith variants of the collection (i.e. sets) data structures in order to manage\nefficiently $k$-ary trees for the linearisation case of the dynamic trees\nproblem. Different implementations are discussed, and the performance is\nmeasured.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 08:49:29 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Saenz-Carrasco", "Juan Carlos", ""]]}, {"id": "1908.11181", "submitter": "Michael Wallner", "authors": "Andrew Elvey Price, Wenjie Fang, Michael Wallner", "title": "Compacted binary trees admit a stretched exponential", "comments": "37 pages, 14 figures. Version accepted for publication", "journal-ref": "J. Combin. Theory Ser. A 177 (2021), 105306", "doi": "10.1016/j.jcta.2020.105306", "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compacted binary tree is a directed acyclic graph encoding a binary tree in\nwhich common subtrees are factored and shared, such that they are represented\nonly once. We show that the number of compacted binary trees of size $n$ grows\nasymptotically like $$\\Theta\\left( n! \\, 4^n e^{3a_1n^{1/3}} n^{3/4} \\right),$$\nwhere $a_1\\approx-2.338$ is the largest root of the Airy function. Our method\ninvolves a new two parameter recurrence which yields an algorithm of quadratic\narithmetic complexity. We use empirical methods to estimate the values of all\nterms defined by the recurrence, then we prove by induction that these\nestimates are sufficiently accurate for large $n$ to determine the asymptotic\nform. Our results also lead to new bounds on the number of minimal finite\nautomata recognizing a finite language on a binary alphabet. As a consequence,\nthese also exhibit a stretched exponential.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 12:36:15 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 17:48:20 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 13:36:05 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Price", "Andrew Elvey", ""], ["Fang", "Wenjie", ""], ["Wallner", "Michael", ""]]}, {"id": "1908.11213", "submitter": "Giuseppe Di Molfetta Prof.", "authors": "Mathieu Roget, St\\'ephane Guillet, Pablo Arrighi, Giuseppe Di Molfetta", "title": "The Grover search as a naturally occurring phenomenon", "comments": null, "journal-ref": "Phys. Rev. Lett. 124, 180501 (2020)", "doi": "10.1103/PhysRevLett.124.180501", "report-no": null, "categories": "quant-ph cond-mat.other cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide first evidence that under certain conditions, 1/2-spin fermions\nmay naturally behave like a Grover search, looking for topological defects in a\nmaterial. The theoretical framework is that of discrete-time quantum walks\n(QW), i.e. local unitary matrices that drive the evolution of a single particle\non the lattice. Some QW are well-known to recover the $(2+1)$--dimensional\nDirac equation in continuum limit, i.e. the free propagation of the 1/2-spin\nfermion. We study two such Dirac QW, one on the square grid and the other on a\ntriangular grid reminiscent of graphene-like materials. The numerical\nsimulations show that the walker localises around the defects in $O(\\sqrt{N})$\nsteps with probability $O(1/\\log{N})$, in line with previous QW search on the\ngrid. The main advantage brought by those of this paper is that they could be\nimplemented as `naturally occurring' freely propagating particles over a\nsurface featuring topological---without the need for a specific oracle step.\nFrom a quantum computing perspective, however, this hints at novel applications\nof QW search : instead of using them to look for `good' solutions within the\nconfiguration space of a problem, we could use them to look for topological\nproperties of the entire configuration space.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 13:22:30 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 12:28:15 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 11:45:09 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Roget", "Mathieu", ""], ["Guillet", "St\u00e9phane", ""], ["Arrighi", "Pablo", ""], ["Di Molfetta", "Giuseppe", ""]]}, {"id": "1908.11248", "submitter": "Josef Mal\\'ik", "authors": "Josef Mal\\'ik, Ond\\v{r}ej Such\\'y, Tom\\'a\\v{s} Valla", "title": "Efficient Implementation of Color Coding Algorithm for Subgraph\n  Isomorphism Problem", "comments": "Extended abstract of this paper will appear in the proceedings of the\n  Special Event on Analysis of Experimental Algorithms, SEA2 2019, Lecture\n  Notes in Computer Science, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the subgraph isomorphism problem where, given two graphs G\n(source graph) and F (pattern graph), one is to decide whether there is a (not\nnecessarily induced) subgraph of G isomorphic to F. While many practical\nheuristic algorithms have been developed for the problem, as pointed out by\nMcCreesh et al. [JAIR 2018], for each of them there are rather small instances\nwhich they cannot cope. Therefore, developing an alternative approach that\ncould possibly cope with these hard instances would be of interest.\n  A seminal paper by Alon, Yuster and Zwick [J. ACM 1995] introduced the color\ncoding approach to solve the problem, where the main part is a dynamic\nprogramming over color subsets and partial mappings. As with many\nexponential-time dynamic programming algorithms, the memory requirements\nconstitute the main limiting factor for its usage. Because these requirements\ngrow exponentially with the treewidth of the pattern graph, all existing\nimplementations based on the color coding principle restrict themselves to\nspecific pattern graphs, e.g., paths or trees. In contrast, we provide an\nefficient implementation of the algorithm significantly reducing its memory\nrequirements so that it can be used for pattern graphs of larger treewidth.\nMoreover, our implementation not only decides the existence of an isomorphic\nsubgraph, but it also enumerates all such subgraphs (or given number of them).\n  We provide an extensive experimental comparison of our implementation to\nother available solvers for the problem.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 14:15:11 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Mal\u00edk", "Josef", ""], ["Such\u00fd", "Ond\u0159ej", ""], ["Valla", "Tom\u00e1\u0161", ""]]}, {"id": "1908.11335", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Pasin Manurangsi", "title": "Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a\n  Margin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of {\\em properly} learning large margin halfspaces in\nthe agnostic PAC model. In more detail, we study the complexity of properly\nlearning $d$-dimensional halfspaces on the unit ball within misclassification\nerror $\\alpha \\cdot \\mathrm{OPT}_{\\gamma} + \\epsilon$, where\n$\\mathrm{OPT}_{\\gamma}$ is the optimal $\\gamma$-margin error rate and $\\alpha\n\\geq 1$ is the approximation ratio. We give learning algorithms and\ncomputational hardness results for this problem, for all values of the\napproximation ratio $\\alpha \\geq 1$, that are nearly-matching for a range of\nparameters. Specifically, for the natural setting that $\\alpha$ is any constant\nbigger than one, we provide an essentially tight complexity characterization.\nOn the positive side, we give an $\\alpha = 1.01$-approximate proper learner\nthat uses $O(1/(\\epsilon^2\\gamma^2))$ samples (which is optimal) and runs in\ntime $\\mathrm{poly}(d/\\epsilon) \\cdot 2^{\\tilde{O}(1/\\gamma^2)}$. On the\nnegative side, we show that {\\em any} constant factor approximate proper\nlearner has runtime $\\mathrm{poly}(d/\\epsilon) \\cdot 2^{(1/\\gamma)^{2-o(1)}}$,\nassuming the Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 16:34:25 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "1908.11358", "submitter": "Noah Golowich", "authors": "Badih Ghazi, Noah Golowich, Ravi Kumar, Rasmus Pagh, Ameya Velingker", "title": "On the Power of Multiple Anonymous Messages", "comments": "70 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exciting new development in differential privacy is the shuffled model, in\nwhich an anonymous channel enables non-interactive, differentially private\nprotocols with error much smaller than what is possible in the local model,\nwhile relying on weaker trust assumptions than in the central model. In this\npaper, we study basic counting problems in the shuffled model and establish\nseparations between the error that can be achieved in the single-message\nshuffled model and in the shuffled model with multiple messages per user.\n  For the problem of frequency estimation for $n$ users and a domain of size\n$B$, we obtain:\n  - A nearly tight lower bound of $\\tilde{\\Omega}( \\min(\\sqrt[4]{n},\n\\sqrt{B}))$ on the error in the single-message shuffled model. This implies\nthat the protocols obtained from the amplification via shuffling work of\nErlingsson et al. (SODA 2019) and Balle et al. (Crypto 2019) are essentially\noptimal for single-message protocols. A key ingredient in the proof is a lower\nbound on the error of locally-private frequency estimation in the low-privacy\n(aka high $\\epsilon$) regime.\n  - Protocols in the multi-message shuffled model with $poly(\\log{B}, \\log{n})$\nbits of communication per user and $poly\\log{B}$ error, which provide an\nexponential improvement on the error compared to what is possible with\nsingle-message algorithms.\n  For the related selection problem on a domain of size $B$, we prove:\n  - A nearly tight lower bound of $\\Omega(B)$ on the number of users in the\nsingle-message shuffled model. This significantly improves on the\n$\\Omega(B^{1/17})$ lower bound obtained by Cheu et al. (Eurocrypt 2019), and\nwhen combined with their $\\tilde{O}(\\sqrt{B})$-error multi-message protocol,\nimplies the first separation between single-message and multi-message protocols\nfor this problem.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 17:26:23 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 16:40:51 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2019 20:42:51 GMT"}, {"version": "v4", "created": "Tue, 19 May 2020 05:53:03 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ghazi", "Badih", ""], ["Golowich", "Noah", ""], ["Kumar", "Ravi", ""], ["Pagh", "Rasmus", ""], ["Velingker", "Ameya", ""]]}, {"id": "1908.11402", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Yoann Dieudonn\\'e, Andrzej Pelc", "title": "Want to Gather? No Need to Chatter!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A team of mobile agents, starting from different nodes of an unknown network,\npossibly at different times, have to meet at the same node and declare that\nthey have all met. Agents have different labels and move in synchronous rounds\nalong links of the network. The above task is known as gathering and was\ntraditionally considered under the assumption that when some agents are at the\nsame node then they can talk. In this paper we ask the question of whether this\nability of talking is needed for gathering. The answer turns out to be no.\n  Our main contribution are two deterministic algorithms that always accomplish\ngathering in a much weaker model. We only assume that at any time an agent\nknows how many agents are at the node that it currently occupies but agents do\nnot see the labels of other co-located agents and cannot exchange any\ninformation with them. They also do not see other nodes than the current one.\nOur first algorithm works under the assumption that agents know a priori some\nupper bound N on the network size, and it works in time polynomial in N and in\nthe length l of the smallest label. Our second algorithm does not assume any a\npriori knowledge about the network but its complexity is exponential in the\nnetwork size and in the labels of agents. Its purpose is to show feasibility of\ngathering under this harsher scenario.\n  As a by-product of our techniques we obtain, in the same weak model, the\nsolution of the fundamental problem of leader election among agents. As an\napplication of our result we also solve, in the same model, the well-known\ngossiping problem: if each agent has a message at the beginning, we show how to\nmake all messages known to all agents, even without any a priori knowledge\nabout the network. If agents know an upper bound N on the network size then our\ngossiping algorithm works in time polynomial in N, in l and in the length of\nthe largest message.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 18:08:53 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1908.11491", "submitter": "Peng Zhang", "authors": "Peng Zhang, Linqing Tang", "title": "Minimum Label s-t Cut has Large Integrality Gaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph G=(V,E) with a label set L = {l_1, l_2, ..., l_q}, in which\neach edge has a label from L, a source s in V, and a sink t in V, the Min Label\ns-t Cut problem asks to pick a set L' subseteq L of labels with minimized\ncardinality, such that the removal of all edges with labels in L' from G\ndisconnects s and t. This problem comes from many applications in real world,\nfor example, information security and computer networks. In this paper, we\nstudy two linear programs for Min Label s-t Cut, proving that both of them have\nlarge integrality gaps, namely, Omega(m) and Omega(m^{1/3-epsilon}) for the\nrespective linear programs, where m is the number of edges in the graph and\nepsilon > 0 is any arbitrarily small constant. As Min Label s-t Cut is NP-hard\nand the linear programming technique is a main approach to design approximation\nalgorithms, our results give negative answer to the hope that designs better\napproximation algorithms for Min Label s-t Cut that purely rely on linear\nprogramming.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 00:15:26 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Zhang", "Peng", ""], ["Tang", "Linqing", ""]]}, {"id": "1908.11515", "submitter": "Tianhao Wang", "authors": "Tianhao Wang, Bolin Ding, Min Xu, Zhicong Huang, Cheng Hong, Jingren\n  Zhou, Ninghui Li, Somesh Jha", "title": "Improving Utility and Security of the Shuffler-based Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When collecting information, local differential privacy (LDP) alleviates\nprivacy concerns of users because their private information is randomized\nbefore being sent it to the central aggregator. LDP imposes large amount of\nnoise as each user executes the randomization independently. To address this\nissue, recent work introduced an intermediate server with the assumption that\nthis intermediate server does not collude with the aggregator. Under this\nassumption, less noise can be added to achieve the same privacy guarantee as\nLDP, thus improving utility for the data collection task.\n  This paper investigates this multiple-party setting of LDP. We analyze the\nsystem model and identify potential adversaries. We then make two improvements:\na new algorithm that achieves a better privacy-utility tradeoff; and a novel\nprotocol that provides better protection against various attacks. Finally, we\nperform experiments to compare different methods and demonstrate the benefits\nof using our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 03:02:04 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 03:06:01 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 14:54:55 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Tianhao", ""], ["Ding", "Bolin", ""], ["Xu", "Min", ""], ["Huang", "Zhicong", ""], ["Hong", "Cheng", ""], ["Zhou", "Jingren", ""], ["Li", "Ninghui", ""], ["Jha", "Somesh", ""]]}, {"id": "1908.11529", "submitter": "Hiroki Furue", "authors": "Hiroki Furue and Hiroshi Hirai", "title": "On a weighted linear matroid intersection algorithm by deg-det\n  computation", "comments": "19 pages, 4 figures, to appear in Japan Journal of Industrial and\n  Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the weighted linear matroid intersection problem\nfrom the computation of the degree of the determinants of a symbolic matrix. We\nshow that a generic algorithm computing the degree of noncommutative\ndeterminants, proposed by the second author, becomes an $O(mn^3 \\log n)$ time\nalgorithm for the weighted linear matroid intersection problem, where two\nmatroids are given by column vectors $n \\times m$ matrices $A,B$. We reveal\nthat our algorithm is viewed as a \"nonstandard\" implementation of Frank's\nweight splitting algorithm for linear matroids. This gives a linear algebraic\nreasoning to Frank's algorithm. Although our algorithm is slower than existing\nalgorithms in the worst case estimate, it has a notable feature: Contrary to\nexisting algorithms, our algorithm works on different matroids represented by\nanother \"sparse\" matrices $A^0,B^0$, which skips unnecessary Gaussian\neliminations for constructing residual graphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 04:36:09 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 04:56:09 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Furue", "Hiroki", ""], ["Hirai", "Hiroshi", ""]]}, {"id": "1908.11631", "submitter": "Suprovat Ghoshal", "authors": "Suprovat Ghoshal, Anand Louis and Rahul Raychaudhury", "title": "Approximation Algorithms for Partially Colorable Graphs", "comments": "25 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph coloring problems are a central topic of study in the theory of\nalgorithms. We study the problem of partially coloring partially colorable\ngraphs. For $\\alpha \\leq 1$ and $k \\in \\mathbb{Z}^+$, we say that a graph\n$G=(V,E)$ is $\\alpha$-partially $k$-colorable, if there exists a subset\n$S\\subset V$ of cardinality $ |S | \\geq \\alpha | V |$ such that the graph\ninduced on $S$ is $k$-colorable. Partial $k$-colorability is a more robust\nstructural property of a graph than $k$-colorability. For graphs that arise in\npractice, partial $k$-colorability might be a better notion to use than\n$k$-colorability, since data arising in practice often contains various forms\nof noise.\n  We give a polynomial time algorithm that takes as input a $(1 -\n\\epsilon)$-partially $3$-colorable graph $G$ and a constant $\\gamma \\in\n[\\epsilon, 1/10]$, and colors a $(1 - \\epsilon/\\gamma)$ fraction of the\nvertices using $\\tilde{O}\\left(n^{0.25 + O(\\gamma^{1/2})} \\right)$ colors. We\nalso study natural semi-random families of instances of partially $3$-colorable\ngraphs and partially $2$-colorable graphs, and give stronger bi-criteria\napproximation guarantees for these family of instances.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 10:19:44 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Ghoshal", "Suprovat", ""], ["Louis", "Anand", ""], ["Raychaudhury", "Rahul", ""]]}, {"id": "1908.11632", "submitter": "Michal Parnas", "authors": "Michal Parnas and Dana Ron and Adi Shraibman", "title": "Property testing of the Boolean and binary rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms for testing if a $(0,1)$-matrix $M$ has Boolean/binary\nrank at most $d$, or is $\\epsilon$-far from Boolean/binary rank $d$ (i.e., at\nleast an $\\epsilon$-fraction of the entries in $M$ must be modified so that it\nhas rank at most $d$).\n  The query complexity of our testing algorithm for the Boolean rank is\n$\\tilde{O}\\left(d^4/ \\epsilon^6\\right)$. For the binary rank we present a\ntesting algorithm whose query complexity is $O(2^{2d}/\\epsilon)$.\n  Both algorithms are $1$-sided error algorithms that always accept $M$ if it\nhas Boolean/binary rank at most $d$, and reject with probability at least $2/3$\nif $M$ is $\\epsilon$-far from Boolean/binary rank $d$.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 10:20:06 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Parnas", "Michal", ""], ["Ron", "Dana", ""], ["Shraibman", "Adi", ""]]}, {"id": "1908.11819", "submitter": "Adam Polak", "authors": "Lech Duraj, Krzysztof Kleiner, Adam Polak, Virginia Vassilevska\n  Williams", "title": "Equivalences between triangle and range query problems", "comments": null, "journal-ref": null, "doi": "10.1137/1.9781611975994.3", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a natural class of range query problems, and prove that all\nproblems within this class have the same time complexity (up to polylogarithmic\nfactors). The equivalence is very general, and even applies to online\nalgorithms. This allows us to obtain new improved algorithms for all of the\nproblems in the class.\n  We then focus on the special case of the problems when the queries are\noffline and the number of queries is linear. We show that our range query\nproblems are runtime-equivalent (up to polylogarithmic factors) to counting for\neach edge $e$ in an $m$-edge graph the number of triangles through $e$. This\nnatural triangle problem can be solved using the best known triangle counting\nalgorithm, running in $O(m^{2\\omega/(\\omega+1)}) \\leq O(m^{1.41})$ time.\nMoreover, if $\\omega=2$, the $O(m^{2\\omega/(\\omega+1)})$ running time is known\nto be tight (within $m^{o(1)}$ factors) under the 3SUM Hypothesis. In this\ncase, our equivalence settles the complexity of the range query problems. Our\nproblems constitute the first equivalence class with this peculiar running time\nbound.\n  To better understand the complexity of these problems, we also provide a\ndeeper insight into the family of triangle problems, in particular showing\nblack-box reductions between triangle listing and per-edge triangle detection\nand counting. As a byproduct of our reductions, we obtain a simple triangle\nlisting algorithm matching the state-of-the-art for all regimes of the number\nof triangles. We also give some not necessarily tight, but still surprising\nreductions from variants of matrix products, such as the $(\\min,\\max)$-product.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 16:17:45 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 20:11:11 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Duraj", "Lech", ""], ["Kleiner", "Krzysztof", ""], ["Polak", "Adam", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1908.11829", "submitter": "Bryce Sandlund", "authors": "Nalin Bhardwaj and Antonio Molina Lovett and Bryce Sandlund", "title": "A Simple Algorithm for Minimum Cuts in Near-Linear Time", "comments": "To appear in SWAT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimum cut problem in undirected, weighted graphs. We give a\nsimple algorithm to find a minimum cut that $2$-respects (cuts two edges of) a\nspanning tree $T$ of a graph $G$. This procedure can be used in place of the\ncomplicated subroutine given in Karger's near-linear time minimum cut algorithm\n(J. ACM, 2000). We give a self-contained version of Karger's algorithm with the\nnew procedure, which is easy to state and relatively simple to implement. It\nproduces a minimum cut on an $m$-edge, $n$-vertex graph in $O(m \\log^3 n)$ time\nwith high probability, matching the complexity of Karger's approach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 16:43:14 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 18:13:57 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 04:40:26 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bhardwaj", "Nalin", ""], ["Lovett", "Antonio Molina", ""], ["Sandlund", "Bryce", ""]]}]