[{"id": "1703.00066", "submitter": "Badih Ghazi", "authors": "Vitaly Feldman, Badih Ghazi", "title": "On the Power of Learning from $k$-Wise Queries", "comments": "32 pages, Appeared in Innovations in Theoretical Computer Science\n  (ITCS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several well-studied models of access to data samples, including statistical\nqueries, local differential privacy and low-communication algorithms rely on\nqueries that provide information about a function of a single sample. (For\nexample, a statistical query (SQ) gives an estimate of $Ex_{x \\sim D}[q(x)]$\nfor any choice of the query function $q$ mapping $X$ to the reals, where $D$ is\nan unknown data distribution over $X$.) Yet some data analysis algorithms rely\non properties of functions that depend on multiple samples. Such algorithms\nwould be naturally implemented using $k$-wise queries each of which is\nspecified by a function $q$ mapping $X^k$ to the reals. Hence it is natural to\nask whether algorithms using $k$-wise queries can solve learning problems more\nefficiently and by how much.\n  Blum, Kalai and Wasserman (2003) showed that for any weak PAC learning\nproblem over a fixed distribution, the complexity of learning with $k$-wise SQs\nis smaller than the (unary) SQ complexity by a factor of at most $2^k$. We show\nthat for more general problems over distributions the picture is substantially\nricher. For every $k$, the complexity of distribution-independent PAC learning\nwith $k$-wise queries can be exponentially larger than learning with\n$(k+1)$-wise queries. We then give two approaches for simulating a $k$-wise\nquery using unary queries. The first approach exploits the structure of the\nproblem that needs to be solved. It generalizes and strengthens (exponentially)\nthe results of Blum et al.. It allows us to derive strong lower bounds for\nlearning DNF formulas and stochastic constraint satisfaction problems that hold\nagainst algorithms using $k$-wise queries. The second approach exploits the\n$k$-party communication complexity of the $k$-wise query function.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 21:41:09 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Feldman", "Vitaly", ""], ["Ghazi", "Badih", ""]]}, {"id": "1703.00236", "submitter": "Davis Issac", "authors": "L. Sunil Chandran and Anita Das and Davis Issac and Erik Jan van\n  Leeuwen", "title": "Algorithms and Bounds for Very Strong Rainbow Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-studied coloring problem is to assign colors to the edges of a graph\n$G$ so that, for every pair of vertices, all edges of at least one shortest\npath between them receive different colors. The minimum number of colors\nnecessary in such a coloring is the strong rainbow connection number\n($\\src(G)$) of the graph. When proving upper bounds on $\\src(G)$, it is natural\nto prove that a coloring exists where, for \\emph{every} shortest path between\nevery pair of vertices in the graph, all edges of the path receive different\ncolors. Therefore, we introduce and formally define this more restricted edge\ncoloring number, which we call \\emph{very strong rainbow connection number}\n($\\vsrc(G)$).\n  In this paper, we give upper bounds on $\\vsrc(G)$ for several graph classes,\nsome of which are tight. These immediately imply new upper bounds on $\\src(G)$\nfor these classes, showing that the study of $\\vsrc(G)$ enables meaningful\nprogress on bounding $\\src(G)$. Then we study the complexity of the problem to\ncompute $\\vsrc(G)$, particularly for graphs of bounded treewidth, and show this\nis an interesting problem in its own right. We prove that $\\vsrc(G)$ can be\ncomputed in polynomial time on cactus graphs; in contrast, this question is\nstill open for $\\src(G)$. We also observe that deciding whether $\\vsrc(G) = k$\nis fixed-parameter tractable in $k$ and the treewidth of $G$. Finally, on\ngeneral graphs, we prove that there is no polynomial-time algorithm to decide\nwhether $\\vsrc(G) \\leq 3$ nor to approximate $\\vsrc(G)$ within a factor\n$n^{1-\\varepsilon}$, unless P$=$NP.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 11:22:52 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 13:01:47 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 10:12:12 GMT"}, {"version": "v4", "created": "Tue, 16 Jan 2018 17:53:31 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Chandran", "L. Sunil", ""], ["Das", "Anita", ""], ["Issac", "Davis", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1703.00440", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Fast k-Nearest Neighbour Search via Prioritized DCI", "comments": "14 pages, 6 figures; International Conference on Machine Learning\n  (ICML), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most exact methods for k-nearest neighbour search suffer from the curse of\ndimensionality; that is, their query times exhibit exponential dependence on\neither the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing\n(DCI) offers a promising way of circumventing the curse and successfully\nreduces the dependence of query time on intrinsic dimensionality from\nexponential to sublinear. In this paper, we propose a variant of DCI, which we\ncall Prioritized DCI, and show a remarkable improvement in the dependence of\nquery time on intrinsic dimensionality. In particular, a linear increase in\nintrinsic dimensionality, or equivalently, an exponential increase in the\nnumber of points near a query, can be mostly counteracted with just a linear\nincrease in space. We also demonstrate empirically that Prioritized DCI\nsignificantly outperforms prior methods. In particular, relative to\nLocality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of\ndistance evaluations by a factor of 14 to 116 and the memory consumption by a\nfactor of 21.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 18:51:13 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 17:46:04 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1703.00484", "submitter": "Rad Niazadeh", "authors": "Shuchi Chawla, Nikhil Devanur, Janardhan Kulkarni, Rad Niazadeh", "title": "Truth and Regret in Online Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a scheduling problem where a cloud service provider has multiple\nunits of a resource available over time. Selfish clients submit jobs, each with\nan arrival time, deadline, length, and value. The service provider's goal is to\nimplement a truthful online mechanism for scheduling jobs so as to maximize the\nsocial welfare of the schedule. Recent work shows that under a stochastic\nassumption on job arrivals, there is a single-parameter family of mechanisms\nthat achieves near-optimal social welfare. We show that given any such family\nof near-optimal online mechanisms, there exists an online mechanism that in the\nworst case performs nearly as well as the best of the given mechanisms. Our\nmechanism is truthful whenever the mechanisms in the given family are truthful\nand prompt, and achieves optimal (within constant factors) regret.\n  We model the problem of competing against a family of online scheduling\nmechanisms as one of learning from expert advice. A primary challenge is that\nany scheduling decisions we make affect not only the payoff at the current\nstep, but also the resource availability and payoffs in future steps.\nFurthermore, switching from one algorithm (a.k.a. expert) to another in an\nonline fashion is challenging both because it requires synchronization with the\nstate of the latter algorithm as well as because it affects the incentive\nstructure of the algorithms. We further show how to adapt our algorithm to a\nnon-clairvoyant setting where job lengths are unknown until jobs are run to\ncompletion. Once again, in this setting, we obtain truthfulness along with\nasymptotically optimal regret (within poly-logarithmic factors).\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 20:09:43 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Chawla", "Shuchi", ""], ["Devanur", "Nikhil", ""], ["Kulkarni", "Janardhan", ""], ["Niazadeh", "Rad", ""]]}, {"id": "1703.00575", "submitter": "Yong Chen", "authors": "An Zhang, Yong Chen, Lin Chen, Guangting Chen", "title": "On the NP-hardness of scheduling with time restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, Braun, Chung and Graham [1] have addressed a\nsingle-processor scheduling problem with time restrictions. Given a fixed\ninteger $B \\geq 2$, there is a set of jobs to be processed by a single\nprocessor subject to the following B-constraint. For any real $x$, no unit time\ninterval $[x, x+1)$ is allowed to intersect more than $B$ jobs. The problem has\nbeen shown to be NP-hard when $B$ is part of the input and left as an open\nquestion whether it remains NP-hard or not if $B$ is fixed [1, 5, 7]. This\npaper contributes to answering this question that we prove the problem is\nNP-hard even when $B=2$. A PTAS is also presented for any constant $B \\geq 2$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 01:19:26 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Zhang", "An", ""], ["Chen", "Yong", ""], ["Chen", "Lin", ""], ["Chen", "Guangting", ""]]}, {"id": "1703.00640", "submitter": "David Harvey", "authors": "David Harvey", "title": "Faster truncated integer multiplication", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for computing the low n bits or the high n bits of\nthe product of two n-bit integers. We show that these problems may be solved in\nasymptotically 75% of the time required to compute the full 2n-bit product,\nassuming that the underlying integer multiplication algorithm relies on\ncomputing cyclic convolutions of real sequences.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 07:12:12 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Harvey", "David", ""]]}, {"id": "1703.00667", "submitter": "Pierre Peterlongo", "authors": "Camille Marchet, Lolita Lecompte, Antoine Limasset, Lucie Bittner and\n  Pierre Peterlongo", "title": "A resource-frugal probabilistic dictionary and applications in\n  bioinformatics", "comments": "Submitted to Journal of Discrete Algorithms. arXiv admin note:\n  substantial text overlap with arXiv:1605.08319", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing massive data sets is extremely expensive for large scale problems.\nIn many fields, huge amounts of data are currently generated, however\nextracting meaningful information from voluminous data sets, such as computing\nsimilarity between elements, is far from being trivial. It remains nonetheless\na fundamental need. This work proposes a probabilistic data structure based on\na minimal perfect hash function for indexing large sets of keys. Our structure\nout-compete the hash table for construction, query times and for memory usage,\nin the case of the indexation of a static set. To illustrate the impact of\nalgorithms performances, we provide two applications based on similarity\ncomputation between collections of sequences, and for which this calculation is\nan expensive but required operation. In particular, we show a practical case in\nwhich other bioinformatics tools fail to scale up the tested data set or\nprovide lower recall quality results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 08:37:37 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 08:45:35 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Marchet", "Camille", ""], ["Lecompte", "Lolita", ""], ["Limasset", "Antoine", ""], ["Bittner", "Lucie", ""], ["Peterlongo", "Pierre", ""]]}, {"id": "1703.00687", "submitter": "Sebastian Deorowicz", "authors": "Marek Kokot and Sebastian Deorowicz and Maciej Dlugosz", "title": "Even faster sorting of (not only) integers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce RADULS2, the fastest parallel sorter based on\nradix algorithm. It is optimized to process huge amounts of data making use of\nmodern multicore CPUs. The main novelties include: extremely optimized\nalgorithm for handling tiny arrays (up to about a hundred of records) that\ncould appear even billions times as subproblems to handle and improved\nprocessing of larger subarrays with better use of non-temporal memory stores.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 09:54:11 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Kokot", "Marek", ""], ["Deorowicz", "Sebastian", ""], ["Dlugosz", "Maciej", ""]]}, {"id": "1703.00699", "submitter": "Lucie Pansart", "authors": "Lucie Pansart, Nicolas Catusse, Hadrien Cambazard", "title": "Exact algorithms for the order picking problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Order picking is the problem of collecting a set of products in a warehouse\nin a minimum amount of time. It is currently a major bottleneck in supply-chain\nbecause of its cost in time and labor force. This article presents two exact\nand effective algorithms for this problem. Firstly, a sparse formulation in\nmixed-integer programming is strengthened by preprocessing and valid\ninequalities. Secondly, a dynamic programming approach generalizing known\nalgorithms for two or three cross-aisles is proposed and evaluated\nexperimentally. Performances of these algorithms are reported and compared with\nthe Traveling Salesman Problem (TSP) solver Concorde.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 10:26:39 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 14:36:27 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 10:04:53 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Pansart", "Lucie", ""], ["Catusse", "Nicolas", ""], ["Cambazard", "Hadrien", ""]]}, {"id": "1703.00830", "submitter": "Colin White", "authors": "Pranjal Awasthi, Ainesh Bakshi, Maria-Florina Balcan, Colin White, and\n  David Woodruff", "title": "Robust Communication-Optimal Distributed Clustering Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the $k$-median and $k$-means clustering problems when\nthe data is distributed across many servers and can contain outliers. While\nthere has been a lot of work on these problems for worst-case instances, we\nfocus on gaining a finer understanding through the lens of beyond worst-case\nanalysis. Our main motivation is the following: for many applications such as\nclustering proteins by function or clustering communities in a social network,\nthere is some unknown target clustering, and the hope is that running a\n$k$-median or $k$-means algorithm will produce clusterings which are close to\nmatching the target clustering. Worst-case results can guarantee constant\nfactor approximations to the optimal $k$-median or $k$-means objective value,\nbut not closeness to the target clustering.\n  Our first result is a distributed algorithm which returns a near-optimal\nclustering assuming a natural notion of stability, namely, approximation\nstability [Balcan et. al 2013], even when a constant fraction of the data are\noutliers. The communication complexity is $\\tilde O(sk+z)$ where $s$ is the\nnumber of machines, $k$ is the number of clusters, and $z$ is the number of\noutliers.\n  Next, we show this amount of communication cannot be improved even in the\nsetting when the input satisfies various non-worst-case assumptions. We give a\nmatching $\\Omega(sk+z)$ lower bound on the communication required both for\napproximating the optimal $k$-means or $k$-median cost up to any constant, and\nfor returning a clustering that is close to the target clustering in Hamming\ndistance. These lower bounds hold even when the data satisfies approximation\nstability or other common notions of stability, and the cluster sizes are\nbalanced. Therefore, $\\Omega(sk+z)$ is a communication bottleneck, even for\nreal-world instances.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 15:27:14 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 19:08:04 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 19:18:19 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Bakshi", "Ainesh", ""], ["Balcan", "Maria-Florina", ""], ["White", "Colin", ""], ["Woodruff", "David", ""]]}, {"id": "1703.00893", "submitter": "Gautam Kamath", "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur\n  Moitra, Alistair Stewart", "title": "Being Robust (in High Dimensions) Can Be Practical", "comments": "Appeared in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation is much more challenging in high dimensions than it is in\none dimension: Most techniques either lead to intractable optimization problems\nor estimators that can tolerate only a tiny fraction of errors. Recent work in\ntheoretical computer science has shown that, in appropriate distributional\nmodels, it is possible to robustly estimate the mean and covariance with\npolynomial time algorithms that can tolerate a constant fraction of\ncorruptions, independent of the dimension. However, the sample and time\ncomplexity of these algorithms is prohibitively large for high-dimensional\napplications. In this work, we address both of these issues by establishing\nsample complexity bounds that are optimal, up to logarithmic factors, as well\nas giving various refinements that allow the algorithms to tolerate a much\nlarger fraction of corruptions. Finally, we show on both synthetic and real\ndata that our algorithms have state-of-the-art performance and suddenly make\nhigh-dimensional robust estimation a realistic possibility.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 18:50:33 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 21:55:28 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 17:59:21 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 17:51:44 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kamath", "Gautam", ""], ["Kane", "Daniel M.", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""], ["Stewart", "Alistair", ""]]}, {"id": "1703.00900", "submitter": "Manuela Fischer", "authors": "Manuela Fischer", "title": "Improved Deterministic Distributed Matching via Rounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improved deterministic distributed algorithms for a number of\nwell-studied matching problems, which are simpler, faster, more accurate,\nand/or more general than their known counterparts. The common denominator of\nthese results is a deterministic distributed rounding method for certain linear\nprograms, which is the first such rounding method, to our knowledge. A sampling\nof our end results is as follows.\n  -- An $O(\\log^2 \\Delta\\cdot \\log n)$-round deterministic distributed\nalgorithm for computing a maximal matching, in $n$-node graphs with maximum\ndegree $\\Delta$. This is the first improvement in about 20 years over the\ncelebrated $O(\\log^4 n)$-round algorithm of Ha\\'n\\'ckowiak, Karo\\'nski, and\nPanconesi [SODA'98, PODC'99].\n  -- A deterministic distributed algorithm for computing a\n$(2+\\varepsilon)$-approximation of maximum matching in $O(\\log^2 \\Delta \\cdot\n\\log \\frac{1}{\\varepsilon} + \\log^ * n)$ rounds. This is exponentially faster\nthan the classic $O(\\Delta +\\log^* n)$-round $2$-approximation of Panconesi and\nRizzi [DIST'01]. With some modifications, the algorithm can also find an\n$\\varepsilon$-maximal matching which leaves only an $\\varepsilon$-fraction of\nthe edges on unmatched nodes.\n  -- An $O(\\log^2 \\Delta \\cdot \\log \\frac{1}{\\varepsilon} + \\log^ * n)$-round\ndeterministic distributed algorithm for computing a\n$(2+\\varepsilon)$-approximation of a maximum weighted matching, and also for\nthe more general problem of maximum weighted $b$-matching. These improve over\nthe $O(\\log^4 n \\cdot \\log_{1+\\varepsilon} W)$-round\n$(6+\\varepsilon)$-approximation algorithm of Panconesi and Sozio [DIST'10],\nwhere $W$ denotes the maximum normalized weight.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 18:55:52 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 17:37:55 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Fischer", "Manuela", ""]]}, {"id": "1703.00941", "submitter": "Stefan Schneider", "authors": "Marvin K\\\"unnemann, Ramamohan Paturi, Stefan Schneider", "title": "On the Fine-grained Complexity of One-Dimensional Dynamic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the complexity of one-dimensional dynamic\nprogramming, or more specifically, of the Least-Weight Subsequence (LWS)\nproblem: Given a sequence of $n$ data items together with weights for every\npair of the items, the task is to determine a subsequence $S$ minimizing the\ntotal weight of the pairs adjacent in $S$. A large number of natural problems\ncan be formulated as LWS problems, yielding obvious $O(n^2)$-time solutions.\n  In many interesting instances, the $O(n^2)$-many weights can be succinctly\nrepresented. Yet except for near-linear time algorithms for some specific\nspecial cases, little is known about when an LWS instantiation admits a\nsubquadratic-time algorithm and when it does not. In particular, no lower\nbounds for LWS instantiations have been known before. In an attempt to remedy\nthis situation, we provide a general approach to study the fine-grained\ncomplexity of succinct instantiations of the LWS problem. In particular, given\nan LWS instantiation we identify a highly parallel core problem that is\nsubquadratically equivalent. This provides either an explanation for the\napparent hardness of the problem or an avenue to find improved algorithms as\nthe case may be.\n  More specifically, we prove subquadratic equivalences between the following\npairs (an LWS instantiation and the corresponding core problem) of problems: a\nlow-rank version of LWS and minimum inner product, finding the longest chain of\nnested boxes and vector domination, and a coin change problem which is closely\nrelated to the knapsack problem and (min,+)-convolution. Using these\nequivalences and known SETH-hardness results for some of the core problems, we\ndeduce tight conditional lower bounds for the corresponding LWS instantiations.\nWe also establish the (min,+)-convolution-hardness of the knapsack problem.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 20:25:04 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["K\u00fcnnemann", "Marvin", ""], ["Paturi", "Ramamohan", ""], ["Schneider", "Stefan", ""]]}, {"id": "1703.01009", "submitter": "Keisuke Goto", "authors": "Keisuke Goto", "title": "Optimal Time and Space Construction of Suffix Arrays and LCP Arrays for\n  Integer Alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffix arrays and LCP arrays are one of the most fundamental data structures\nwidely used for various kinds of string processing. We consider two problems\nfor a read-only string of length $N$ over an integer alphabet $[1, \\dots,\n\\sigma]$ for $1 \\leq \\sigma \\leq N$, the string contains $\\sigma$ distinct\ncharacters, the construction of the suffix array, and a simultaneous\nconstruction of both the suffix array and LCP array. For the word RAM model, we\npropose algorithms to solve both of the problems in $O(N)$ time by using $O(1)$\nextra words, which are optimal in time and space. Extra words means the\nrequired space except for the space of the input string and output suffix array\nand LCP array. Our contribution improves the previous most efficient\nalgorithms, $O(N)$ time using $\\sigma+O(1)$ extra words by [Nong, TOIS 2013]\nand $O(N \\log N)$ time using $O(1)$ extra words by [Franceschini and\nMuthukrishnan, ICALP 2007], for constructing suffix arrays, and it improves the\nprevious most efficient solution that runs in $O(N)$ time using $\\sigma + O(1)$\nextra words for constructing both suffix arrays and LCP arrays through a\ncombination of [Nong, TOIS 2013] and [Manzini, SWAT 2004].\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 01:30:36 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 04:41:19 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 09:30:55 GMT"}, {"version": "v4", "created": "Mon, 28 Jan 2019 15:43:21 GMT"}, {"version": "v5", "created": "Sat, 13 Jul 2019 08:59:02 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Goto", "Keisuke", ""]]}, {"id": "1703.01054", "submitter": "Aneesh Sharma Aneesh Sharma", "authors": "Aneesh Sharma and C. Seshadhri and Ashish Goel", "title": "When Hashes Met Wedges: A Distributed Algorithm for Finding High\n  Similarity Vectors", "comments": null, "journal-ref": null, "doi": "10.1145/3038912.3052633", "report-no": null, "categories": "cs.SI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding similar user pairs is a fundamental task in social networks, with\nnumerous applications in ranking and personalization tasks such as link\nprediction and tie strength detection. A common manifestation of user\nsimilarity is based upon network structure: each user is represented by a\nvector that represents the user's network connections, where pairwise cosine\nsimilarity among these vectors defines user similarity. The predominant task\nfor user similarity applications is to discover all similar pairs that have a\npairwise cosine similarity value larger than a given threshold $\\tau$. In\ncontrast to previous work where $\\tau$ is assumed to be quite close to 1, we\nfocus on recommendation applications where $\\tau$ is small, but still\nmeaningful. The all pairs cosine similarity problem is computationally\nchallenging on networks with billions of edges, and especially so for settings\nwith small $\\tau$. To the best of our knowledge, there is no practical solution\nfor computing all user pairs with, say $\\tau = 0.2$ on large social networks,\neven using the power of distributed algorithms.\n  Our work directly addresses this challenge by introducing a new algorithm ---\nWHIMP --- that solves this problem efficiently in the MapReduce model. The key\ninsight in WHIMP is to combine the \"wedge-sampling\" approach of Cohen-Lewis for\napproximate matrix multiplication with the SimHash random projection techniques\nof Charikar. We provide a theoretical analysis of WHIMP, proving that it has\nnear optimal communication costs while maintaining computation cost comparable\nwith the state of the art. We also empirically demonstrate WHIMP's scalability\nby computing all highly similar pairs on four massive data sets, and show that\nit accurately finds high similarity pairs. In particular, we note that WHIMP\nsuccessfully processes the entire Twitter network, which has tens of billions\nof edges.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 06:41:02 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Sharma", "Aneesh", ""], ["Seshadhri", "C.", ""], ["Goel", "Ashish", ""]]}, {"id": "1703.01166", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Gil Einziger, Roy Friedman", "title": "Give Me Some Slack: Efficient Network Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many networking applications require timely access to recent network\nmeasurements, which can be captured using a sliding window model. Maintaining\nsuch measurements is a challenging task due to the fast line speed and scarcity\nof fast memory in routers. In this work, we study the impact of allowing\n\\emph{slack} in the window size on the asymptotic requirements of sliding\nwindow problems. That is, the algorithm can dynamically adjust the window size\nbetween $W$ and $W(1+\\tau)$ where $\\tau$ is a small positive parameter. We\ndemonstrate this model's attractiveness by showing that it enables efficient\nalgorithms to problems such as MAX and GENERAL-SUM that require $\\Omega(W)$\nbits even for constant factor approximations in the exact sliding window model.\nAdditionally, for problems that admit sub-linear approximation algorithms such\nas BASIC-SUMMING and COUNT-DISTINCT, the slack model enables a further\nasymptotic improvement.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 14:22:44 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 09:05:51 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 07:02:55 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Friedman", "Roy", ""]]}, {"id": "1703.01474", "submitter": "Anindya De", "authors": "Anindya De and Ryan O'Donnell and Rocco Servedio", "title": "Sharp bounds for population recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population recovery problem is a basic problem in noisy unsupervised\nlearning that has attracted significant research attention in recent years\n[WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this\nproblem have been studied, often under assumptions on the unknown distribution\n(such as that it has restricted support size). In this work we study the sample\ncomplexity and algorithmic complexity of the most general version of the\nproblem, under both bit-flip noise and erasure noise model. We give essentially\nmatching upper and lower sample complexity bounds for both noise models, and\nefficient algorithms matching these sample complexity bounds up to polynomial\nfactors.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 15:13:41 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["De", "Anindya", ""], ["O'Donnell", "Ryan", ""], ["Servedio", "Rocco", ""]]}, {"id": "1703.01475", "submitter": "Krzysztof  Lorys", "authors": "Grzegorz G{\\l}uch, Krzysztof Lory\\'s", "title": "4/3 Rectangle Tiling lower bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem that we consider is the following: given an $n \\times n$ array\n$A$ of positive numbers, find a tiling using at most $p$ rectangles (which\nmeans that each array element must be covered by some rectangle and no two\nrectangles must overlap) that minimizes the maximum weight of any rectangle\n(the weight of a rectangle is the sum of elements which are covered by it). We\nprove that it is NP-hard to approximate this problem to within a factor of\n\\textbf{1$\\frac{1}{3}$} (the previous best result was $1\\frac{1}{4}$).\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 15:14:06 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["G\u0142uch", "Grzegorz", ""], ["Lory\u015b", "Krzysztof", ""]]}, {"id": "1703.01507", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Machine Learning Friendly Set Version of Johnson-Lindenstrauss Lemma", "comments": "38 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The\nLemma has an existential form saying that there exists a JL transformation $f$\nof the data points into lower dimensional space such that all of them fall into\npredefined error range $\\delta$.\n  We formulate in this paper a theorem stating that we can choose the target\ndimensionality in a random projection type JL linear transformation in such a\nway that with probability $1-\\epsilon$ all of them fall into predefined error\nrange $\\delta$ for any user-predefined failure probability $\\epsilon$.\n  This result is important for applications such a data clustering where we\nwant to have a priori dimensionality reducing transformation instead of trying\nout a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma.\nIn particular, we take a closer look at the $k$-means algorithm and prove that\na good solution in the projected space is also a good solution in the original\nspace. Furthermore, under proper assumptions local optima in the original space\nare also ones in the projected space. We define also conditions for which\nclusterability property of the original space is transmitted to the projected\nspace, so that special case algorithms for the original space are also\napplicable in the projected space.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 19:08:22 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 17:56:29 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 08:06:18 GMT"}, {"version": "v4", "created": "Mon, 18 Sep 2017 08:35:17 GMT"}, {"version": "v5", "created": "Thu, 9 Nov 2017 17:33:48 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1703.01532", "submitter": "Stephen Gismondi", "authors": "S.J. Gismondi and E.R.Swart", "title": "Using Matching to Detect Infeasibility of Some Integer Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel matching based heuristic algorithm designed to detect specially\nformulated infeasible zero-one IPs is presented. The algorithm input is a set\nof nested doubly stochastic subsystems and a set E of instance defining\nvariables set at zero level. The algorithm deduces additional variables at zero\nlevel until either a constraint is violated (the IP is infeasible), or no more\nvariables can be deduced zero (the IP is undecided). All feasible IPs, and all\ninfeasible IPs not detected infeasible are undecided. We successfully apply the\nalgorithm to a small set of specially formulated infeasible zero-one IP\ninstances of the Hamilton cycle decision problem. We show how to model both the\ngraph and subgraph isomorphism decision problems for input to the algorithm.\nIncreased levels of nested doubly stochastic subsystems can be implemented\ndynamically. The algorithm is designed for parallel processing, and for\ninclusion of techniques in addition to matching.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 22:56:03 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Gismondi", "S. J.", ""], ["Swart", "E. R.", ""]]}, {"id": "1703.01539", "submitter": "Yi Li", "authors": "Sudipto Guha and Yi Li and Qin Zhang", "title": "Distributed Partial Clustering", "comments": "A preliminary version is to appear in the Proceedings of SPAA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed an increasing popularity of algorithm design for\ndistributed data, largely due to the fact that massive datasets are often\ncollected and stored in different locations. In the distributed setting\ncommunication typically dominates the query processing time. Thus it becomes\ncrucial to design communication efficient algorithms for queries on distributed\ndata. Simultaneously, it has been widely recognized that partial optimizations,\nwhere we are allowed to disregard a small part of the data, provide us\nsignificantly better solutions. The motivation for disregarded points often\narise from noise and other phenomena that are pervasive in large data\nscenarios.\n  In this paper we focus on partial clustering problems, $k$-center, $k$-median\nand $k$-means, in the distributed model, and provide algorithms with\ncommunication sublinear of the input size. As a consequence we develop the\nfirst algorithms for the partial $k$-median and means objectives that run in\nsubquadratic running time. We also initiate the study of distributed algorithms\nfor clustering uncertain data, where each data point can possibly fall into\nmultiple locations under certain probability distribution.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 01:08:29 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 21:35:11 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 07:54:09 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Guha", "Sudipto", ""], ["Li", "Yi", ""], ["Zhang", "Qin", ""]]}, {"id": "1703.01634", "submitter": "Marc Uetz", "authors": "Varun Gupta, Benjamin Moseley, Marc Uetz, Qiaomin Xie", "title": "Greed Works -- Online Algorithms For Unrelated Machine Stochastic\n  Scheduling", "comments": "Preliminary version appeared in IPCO 2017", "journal-ref": "Mathematics of Operations Research 44(2), 2020, 497-516", "doi": "10.1287/moor.2019.0999", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes performance guarantees for online algorithms that\nschedule stochastic, nonpreemptive jobs on unrelated machines to minimize the\nexpected total weighted completion time. Prior work on unrelated machine\nscheduling with stochastic jobs was restricted to the offline case, and\nrequired linear or convex programming relaxations for the assignment of jobs to\nmachines. The algorithms introduced in this paper are purely combinatorial. The\nperformance bounds are of the same order of magnitude as those of earlier work,\nand depend linearly on an upper bound on the squared coefficient of variation\nof the jobs' processing times. Specifically for deterministic processing times,\nwithout and with release times, the competitive ratios are 4 and 7.216,\nrespectively. As to the technical contribution, the paper shows how dual\nfitting techniques can be used for stochastic and nonpreemptive scheduling\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 17:45:59 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 10:33:59 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2018 13:49:37 GMT"}, {"version": "v4", "created": "Wed, 13 May 2020 08:22:34 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Gupta", "Varun", ""], ["Moseley", "Benjamin", ""], ["Uetz", "Marc", ""], ["Xie", "Qiaomin", ""]]}, {"id": "1703.01638", "submitter": "Stefan Neumann", "authors": "Monika Henzinger, Andrea Lincoln, Stefan Neumann, and Virginia\n  Vassilevska Williams", "title": "Conditional Hardness for Sensitivity Problems", "comments": "Appeared in ITCS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years it has become popular to study dynamic problems in a\nsensitivity setting: Instead of allowing for an arbitrary sequence of updates,\nthe sensitivity model only allows to apply batch updates of small size to the\noriginal input data. The sensitivity model is particularly appealing since\nrecent strong conditional lower bounds ruled out fast algorithms for many\ndynamic problems, such as shortest paths, reachability, or subgraph\nconnectivity.\n  In this paper we prove conditional lower bounds for sensitivity problems. For\nexample, we show that under the Boolean Matrix Multiplication (BMM) conjecture\ncombinatorial algorithms cannot compute the (4/3 - {\\epsilon})-approximate\ndiameter of an undirected unweighted dense graph with truly subcubic\npreprocessing time and truly subquadratic update/query time. This result is\nsurprising since in the static setting it is not clear whether a reduction from\nBMM to diameter is possible. We further show under the BMM conjecture that many\nproblems, such as reachability or approximate shortest paths, cannot be solved\nfaster than by recomputation from scratch even after only one or two edge\ninsertions. We give more lower bounds under the Strong Exponential Time\nHypothesis and the All Pairs Shortest Paths Conjecture. Many of our lower\nbounds also hold for static oracle data structures where no sensitivity is\nrequired. Finally, we give the first algorithm for the (1 +\n{\\epsilon})-approximate radius, diameter, and eccentricity problems in directed\nor undirected unweighted graphs in case of single edges failures. The algorithm\nhas a truly subcubic running time for graphs with a truly subquadratic number\nof edges; it is tight w.r.t. the conditional lower bounds we obtain.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 18:12:06 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Henzinger", "Monika", ""], ["Lincoln", "Andrea", ""], ["Neumann", "Stefan", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1703.01640", "submitter": "Joseph S. B. Mitchell", "authors": "Adrian Dumitrescu and Joseph S. B. Mitchell", "title": "Approximation algorithms for TSP with neighborhoods in the plane", "comments": "Manuscript that was published in J. Algorithms (2003), with brief\n  clarifying remarks added (2014)", "journal-ref": "Journal of Algorithms, Volume 48, Issue 1, August 2003, Pages\n  135-159. (special issue for SODA 2001)", "doi": "10.1016/s0196-6774(03)00047-6", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Euclidean TSP with neighborhoods (TSPN), we are given a collection of\nn regions (neighborhoods) and we seek a shortest tour that visits each region.\nAs a generalization of the classical Euclidean TSP, TSPN is also NP-hard. In\nthis paper, we present new approximation results for the TSPN, including (1) a\nconstant-factor approximation algorithm for the case of arbitrary connected\nneighborhoods having comparable diameters; and (2) a PTAS for the important\nspecial case of disjoint unit disk neighborhoods (or nearly disjoint,\nnearly-unit disks). Our methods also yield improved approximation ratios for\nvarious special classes of neighborhoods, which have previously been studied.\nFurther, we give a linear-time O(1)-approximation algorithm for the case of\nneighborhoods that are (infinite) straight lines.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 18:24:23 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Dumitrescu", "Adrian", ""], ["Mitchell", "Joseph S. B.", ""]]}, {"id": "1703.01686", "submitter": "Ignasi Sau", "authors": "Julien Baste, Didem G\\\"oz\\\"upek, Christophe Paul, Ignasi Sau,\n  Mordechai Shalom, Dimitrios M. Thilikos", "title": "Parameterized complexity of finding a spanning tree with minimum reload\n  cost diameter", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimum diameter spanning tree problem under the reload cost\nmodel (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this\nproblem, given an undirected edge-colored graph $G$, reload costs on a path\narise at a node where the path uses consecutive edges of different colors. The\nobjective is to find a spanning tree of $G$ of minimum diameter with respect to\nthe reload costs. We initiate a systematic study of the parameterized\ncomplexity of the DIAMETER-TREE problem by considering the following\nparameters: the cost of a solution, and the treewidth and the maximum degree\n$\\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for\nany combination of two of these three parameters, and that it is FPT\nparameterized by the three of them. We also prove that the problem can be\nsolved in polynomial time on cactus graphs. This result is somehow surprising\nsince we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is\nbest possible as the problem can be trivially solved on forests. When the\nreload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved\nthat the problem can be solved in polynomial time on graphs with $\\Delta = 3$,\nand Galbiati (2008) proved that it is NP-hard if $\\Delta = 4$. Our results\nshow, in particular, that without the requirement of the triangle inequality,\nthe problem is NP-hard if $\\Delta = 3$, which is also best possible. Finally,\nin the case where the reload costs are polynomially bounded by the size of the\ninput graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized\nby the treewidth plus $\\Delta$.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 23:06:03 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 15:53:01 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Baste", "Julien", ""], ["G\u00f6z\u00fcpek", "Didem", ""], ["Paul", "Christophe", ""], ["Sau", "Ignasi", ""], ["Shalom", "Mordechai", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1703.01727", "submitter": "Md Saiful Islam", "authors": "Charles H. Goonetilleke, J. Wenny Rahayu and Md. Saiful Islam", "title": "Frequent Query Matching in Dynamic Data Warehousing", "comments": "2 Tables, 29 Figures, submitted to the Elsevier Journal of Systems\n  and Software for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the need for flexible and on-demand decision support, Dynamic Data\nWarehouses (DDW) provide benefits over traditional data warehouses due to their\ndynamic characteristics in structuring and access mechanism. A DDW is a data\nframework that accommodates data source changes easily to allow seamless\nquerying to users. Materialized Views (MV) are proven to be an effective\nmethodology to enhance the process of retrieving data from a DDW as results are\npre-computed and stored in it. However, due to the static nature of\nmaterialized views, the level of dynamicity that can be provided at the MV\naccess layer is restricted. As a result, the collection of materialized views\nis not compatible with ever-changing reporting requirements. It is important\nthat the MV collection is consistent with current and upcoming queries. The\nsolution to the above problem must consider the following aspects: (a) MV must\nbe matched against an OLAP query in order to recognize whether the MV can\nanswer the query, (b) enable scalability in the MV collection, an intuitive\nmechanism to prune it and retrieve closely matching MVs must be incorporated,\n(c) MV collection must be able to evolve in correspondence to the regularly\nchanging user query patterns. Therefore, the primary objective of this paper is\nto explore these aspects and provide a well-rounded solution for the MV access\nlayer to remove the mismatch between the MV collection and reporting\nrequirements. Our contribution to solve the problem includes a Query Matching\nTechnique, a Domain Matching Technique and Maintenance of the MV collection. We\ndeveloped an experimental platform using real data-sets to evaluate the\neffectiveness in terms of performance and precision of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 05:14:07 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Goonetilleke", "Charles H.", ""], ["Rahayu", "J. Wenny", ""], ["Islam", "Md. Saiful", ""]]}, {"id": "1703.01830", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "Alina Ene and Huy L. Nguyen and L\\'aszl\\'o A. V\\'egh", "title": "Decomposable Submodular Function Minimization: Discrete and Continuous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates connections between discrete and continuous\napproaches for decomposable submodular function minimization. We provide\nimproved running time estimates for the state-of-the-art continuous algorithms\nfor the problem using combinatorial arguments. We also provide a systematic\nexperimental comparison of the two types of methods, based on a clear\ndistinction between level-0 and level-1 algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 12:06:58 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "1703.01847", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi", "title": "Tight Space-Approximation Tradeoff for the Multi-Pass Streaming Set\n  Cover Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic set cover problem in the streaming model: the sets that\ncomprise the instance are revealed one by one in a stream and the goal is to\nsolve the problem by making one or few passes over the stream while maintaining\na sublinear space $o(mn)$ in the input size; here $m$ denotes the number of the\nsets and $n$ is the universe size. Notice that in this model, we are mainly\nconcerned with the space requirement of the algorithms and hence do not\nrestrict their computation time.\n  Our main result is a resolution of the space-approximation tradeoff for the\nstreaming set cover problem: we show that any $\\alpha$-approximation algorithm\nfor the set cover problem requires $\\widetilde{\\Omega}(mn^{1/\\alpha})$ space,\neven if it is allowed polylog${(n)}$ passes over the stream, and even if the\nsets are arriving in a random order in the stream. This space-approximation\ntradeoff matches the best known bounds achieved by the recent algorithm of\nHar-Peled et.al. (PODS 2016) that requires only $O(\\alpha)$ passes over the\nstream in an adversarial order, hence settling the space complexity of\napproximating the set cover problem in data streams in a quite robust manner.\nAdditionally, our approach yields tight lower bounds for the space complexity\nof $(1- \\epsilon)$-approximating the streaming maximum coverage problem studied\nin several recent works.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 12:55:21 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Assadi", "Sepehr", ""]]}, {"id": "1703.01905", "submitter": "Cristian Dumitrescu BSc.", "authors": "Cristian Dumitrescu", "title": "A randomized, efficient algorithm for 3SAT", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper I present a 3SAT algorithm based on the randomized algorithm of\nPapadimitriou from 1991, and Schoning from 1991. We also present strong\narguments that this algorithm finds a solution (if it exists) for a 3SAT\nproblem with high probability in polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 14:52:58 GMT"}, {"version": "v10", "created": "Mon, 11 Sep 2017 13:44:41 GMT"}, {"version": "v11", "created": "Mon, 25 Sep 2017 14:11:47 GMT"}, {"version": "v12", "created": "Tue, 2 Jan 2018 15:34:35 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 15:42:05 GMT"}, {"version": "v3", "created": "Thu, 11 May 2017 19:08:12 GMT"}, {"version": "v4", "created": "Tue, 23 May 2017 16:12:15 GMT"}, {"version": "v5", "created": "Mon, 19 Jun 2017 14:58:23 GMT"}, {"version": "v6", "created": "Mon, 26 Jun 2017 16:24:25 GMT"}, {"version": "v7", "created": "Tue, 4 Jul 2017 14:21:45 GMT"}, {"version": "v8", "created": "Mon, 10 Jul 2017 13:21:59 GMT"}, {"version": "v9", "created": "Thu, 31 Aug 2017 15:17:06 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Dumitrescu", "Cristian", ""]]}, {"id": "1703.01913", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vladimir Nikishkin", "title": "Near-Optimal Closeness Testing of Discrete Histogram Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of testing the equivalence between two discrete\nhistograms. A {\\em $k$-histogram} over $[n]$ is a probability distribution that\nis piecewise constant over some set of $k$ intervals over $[n]$. Histograms\nhave been extensively studied in computer science and statistics. Given a set\nof samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to\ndistinguish (with high probability) between the cases that $p = q$ and\n$\\|p-q\\|_1 \\geq \\epsilon$. The main contribution of this paper is a new\nalgorithm for this testing problem and a nearly matching information-theoretic\nlower bound. Specifically, the sample complexity of our algorithm matches our\nlower bound up to a logarithmic factor, improving on previous work by\npolynomial factors in the relevant parameters. Our algorithmic approach applies\nin a more general setting and yields improved sample upper bounds for testing\ncloseness of other structured distributions as well.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 15:03:55 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Nikishkin", "Vladimir", ""]]}, {"id": "1703.01939", "submitter": "Michael Elkin", "authors": "Michael Elkin", "title": "Distributed Exact Shortest Paths in Sublinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed single-source shortest paths problem is one of the most\nfundamental and central problems in the message-passing distributed computing.\nClassical Bellman-Ford algorithm solves it in $O(n)$ time, where $n$ is the\nnumber of vertices in the input graph $G$. Peleg and Rubinovich (FOCS'99)\nshowed a lower bound of $\\tilde{\\Omega}(D + \\sqrt{n})$ for this problem, where\n$D$ is the hop-diameter of $G$.\n  Whether or not this problem can be solved in $o(n)$ time when $D$ is\nrelatively small is a major notorious open question. Despite intensive research\n\\cite{LP13,N14,HKN15,EN16,BKKL16} that yielded near-optimal algorithms for the\napproximate variant of this problem, no progress was reported for the original\nproblem.\n  In this paper we answer this question in the affirmative. We devise an\nalgorithm that requires $O((n \\log n)^{5/6})$ time, for $D = O(\\sqrt{n \\log\nn})$, and $O(D^{1/3} \\cdot (n \\log n)^{2/3})$ time, for larger $D$. This\nrunning time is sublinear in $n$ in almost the entire range of parameters,\nspecifically, for $D = o(n/\\log^2 n)$. For the all-pairs shortest paths\nproblem, our algorithm requires $O(n^{5/3} \\log^{2/3} n)$ time, regardless of\nthe value of $D$.\n  We also devise the first algorithm with non-trivial complexity guarantees for\ncomputing exact shortest paths in the multipass semi-streaming model of\ncomputation.\n  From the technical viewpoint, our algorithm computes a hopset $G\"$ of a\nskeleton graph $G'$ of $G$ without first computing $G'$ itself. We then conduct\na Bellman-Ford exploration in $G' \\cup G\"$, while computing the required edges\nof $G'$ on the fly. As a result, our algorithm computes exactly those edges of\n$G'$ that it really needs, rather than computing approximately the entire $G'$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 16:01:27 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 14:17:51 GMT"}, {"version": "v3", "created": "Sun, 30 Apr 2017 08:46:20 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Elkin", "Michael", ""]]}, {"id": "1703.02059", "submitter": "Manuel Gomez Rodriguez", "authors": "Ali Zarezade and Abir De and Hamid Rabiee and Manuel Gomez Rodriguez", "title": "Cheshire: An Online Algorithm for Activity Maximization in Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User engagement in social networks depends critically on the number of online\nactions their users take in the network. Can we design an algorithm that finds\nwhen to incentivize users to take actions to maximize the overall activity in a\nsocial network? In this paper, we model the number of online actions over time\nusing multidimensional Hawkes processes, derive an alternate representation of\nthese processes based on stochastic differential equations (SDEs) with jumps\nand, exploiting this alternate representation, address the above question from\nthe perspective of stochastic optimal control of SDEs with jumps. We find that\nthe optimal level of incentivized actions depends linearly on the current level\nof overall actions. Moreover, the coefficients of this linear relationship can\nbe found by solving a matrix Riccati differential equation, which can be solved\nefficiently, and a first order differential equation, which has a closed form\nsolution. As a result, we are able to design an efficient online algorithm,\nCheshire, to sample the optimal times of the users' incentivized actions.\nExperiments on both synthetic and real data gathered from Twitter show that our\nalgorithm is able to consistently maximize the number of online actions more\neffectively than the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 19:01:03 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Zarezade", "Ali", ""], ["De", "Abir", ""], ["Rabiee", "Hamid", ""], ["Rodriguez", "Manuel Gomez", ""]]}, {"id": "1703.02100", "submitter": "Yatao A. Bian", "authors": "Andrew An Bian, Joachim M. Buhmann, Andreas Krause, Sebastian\n  Tschiatschek", "title": "Guarantees for Greedy Maximization of Non-submodular Functions with\n  Applications", "comments": "published at ICML 2017. First author is now known as Yatao Bian\n  <ybian@inf.ethz.ch>. ORCID: https://orcid.org/0000-0002-2368-4084", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of the standard Greedy algorithm for\ncardinality constrained maximization of non-submodular nondecreasing set\nfunctions. While there are strong theoretical guarantees on the performance of\nGreedy for maximizing submodular functions, there are few guarantees for\nnon-submodular ones. However, Greedy enjoys strong empirical performance for\nmany important non-submodular functions, e.g., the Bayesian A-optimality\nobjective in experimental design. We prove theoretical guarantees supporting\nthe empirical performance. Our guarantees are characterized by a combination of\nthe (generalized) curvature $\\alpha$ and the submodularity ratio $\\gamma$. In\nparticular, we prove that Greedy enjoys a tight approximation guarantee of\n$\\frac{1}{\\alpha}(1- e^{-\\gamma\\alpha})$ for cardinality constrained\nmaximization. In addition, we bound the submodularity ratio and curvature for\nseveral important real-world objectives, including the Bayesian A-optimality\nobjective, the determinantal function of a square submatrix and certain linear\nprograms with combinatorial constraints. We experimentally validate our\ntheoretical findings for both synthetic and real-world applications.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 20:28:23 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 07:51:03 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 08:22:12 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 12:37:18 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Bian", "Andrew An", ""], ["Buhmann", "Joachim M.", ""], ["Krause", "Andreas", ""], ["Tschiatschek", "Sebastian", ""]]}, {"id": "1703.02224", "submitter": "Freeson Kaniwa", "authors": "Freeson Kaniwa, Venu Madhav Kuthadi, Otlhapile Dinakenyane and Heiko\n  Schroeder", "title": "Space-efficient K-MER algorithm for generalized suffix tree", "comments": "10 pages", "journal-ref": "International Journal of Information Technology Convergence and\n  Services (IJITCS) Vol.7, No.1, February 2017", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffix trees have emerged to be very fast for pattern searching yielding O\n(m) time, where m is the pattern size. Unfortunately their high memory\nrequirements make it impractical to work with huge amounts of data. We present\na memory efficient algorithm of a generalized suffix tree which reduces the\nspace size by a factor of 10 when the size of the pattern is known beforehand.\nExperiments on the chromosomes and Pizza&Chili corpus show significant\nadvantages of our algorithm over standard linear time suffix tree construction\nin terms of memory usage for pattern searching.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 05:43:56 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Kaniwa", "Freeson", ""], ["Kuthadi", "Venu Madhav", ""], ["Dinakenyane", "Otlhapile", ""], ["Schroeder", "Heiko", ""]]}, {"id": "1703.02375", "submitter": "Anne Morvan", "authors": "Anne Morvan, Krzysztof Choromanski, C\\'edric Gouy-Pailler, Jamal Atif", "title": "Graph sketching-based Space-efficient Data Clustering", "comments": "Proceedings of the 2018 SIAM International Conference on Data Mining", "journal-ref": null, "doi": "10.1137/1.9781611975321.2", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of recovering arbitrary-shaped data\nclusters from datasets while facing \\emph{high space constraints}, as this is\nfor instance the case in many real-world applications when analysis algorithms\nare directly deployed on resources-limited mobile devices collecting the data.\nWe present DBMSTClu a new space-efficient density-based \\emph{non-parametric}\nmethod working on a Minimum Spanning Tree (MST) recovered from a limited number\nof linear measurements i.e. a \\emph{sketched} version of the dissimilarity\ngraph $\\mathcal{G}$ between the $N$ objects to cluster. Unlike $k$-means,\n$k$-medians or $k$-medoids algorithms, it does not fail at distinguishing\nclusters with particular forms thanks to the property of the MST for expressing\nthe underlying structure of a graph. No input parameter is needed contrarily to\nDBSCAN or the Spectral Clustering method. An approximate MST is retrieved by\nfollowing the dynamic \\emph{semi-streaming} model in handling the dissimilarity\ngraph $\\mathcal{G}$ as a stream of edge weight updates which is sketched in one\npass over the data into a compact structure requiring $O(N\n\\operatorname{polylog}(N))$ space, far better than the theoretical memory cost\n$O(N^2)$ of $\\mathcal{G}$. The recovered approximate MST $\\mathcal{T}$ as\ninput, DBMSTClu then successfully detects the right number of nonconvex\nclusters by performing relevant cuts on $\\mathcal{T}$ in a time linear in $N$.\nWe provide theoretical guarantees on the quality of the clustering partition\nand also demonstrate its advantage over the existing state-of-the-art on\nseveral datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 13:43:45 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 13:11:52 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 07:58:04 GMT"}, {"version": "v4", "created": "Wed, 20 Dec 2017 20:37:29 GMT"}, {"version": "v5", "created": "Sun, 27 May 2018 17:17:46 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Morvan", "Anne", ""], ["Choromanski", "Krzysztof", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Atif", "Jamal", ""]]}, {"id": "1703.02411", "submitter": "Michael Elkin", "authors": "Michael Elkin", "title": "A Simple Deterministic Distributed MST Algorithm, with Near-Optimal Time\n  and Message Complexities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed minimum spanning tree (MST) problem is one of the most central\nand fundamental problems in distributed graph algorithms. Garay et al.\n\\cite{GKP98,KP98} devised an algorithm with running time $O(D + \\sqrt{n} \\cdot\n\\log^* n)$, where $D$ is the hop-diameter of the input $n$-vertex $m$-edge\ngraph, and with message complexity $O(m + n^{3/2})$. Peleg and Rubinovich\n\\cite{PR99} showed that the running time of the algorithm of \\cite{KP98} is\nessentially tight, and asked if one can achieve near-optimal running time\n**together with near-optimal message complexity**.\n  In a recent breakthrough, Pandurangan et al. \\cite{PRS16} answered this\nquestion in the affirmative, and devised a **randomized** algorithm with time\n$\\tilde{O}(D+ \\sqrt{n})$ and message complexity $\\tilde{O}(m)$. They asked if\nsuch a simultaneous time- and message-optimality can be achieved by a\n**deterministic** algorithm.\n  In this paper, building upon the work of \\cite{PRS16}, we answer this\nquestion in the affirmative, and devise a **deterministic** algorithm that\ncomputes MST in time $O((D + \\sqrt{n}) \\cdot \\log n)$, using $O(m \\cdot \\log n\n+ n \\log n \\cdot \\log^* n)$ messages. The polylogarithmic factors in the time\nand message complexities of our algorithm are significantly smaller than the\nrespective factors in the result of \\cite{PRS16}. Also, our algorithm and its\nanalysis are very **simple** and self-contained, as opposed to rather\ncomplicated previous sublinear-time algorithms \\cite{GKP98,KP98,E04b,PRS16}.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 14:48:15 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Elkin", "Michael", ""]]}, {"id": "1703.02485", "submitter": "Marcin Kami\\'nski", "authors": "Marcin Kami\\'nski and Anna Pstrucha", "title": "Certifying coloring algorithms for graphs without long induced paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P_k$ be a path, $C_k$ a cycle on $k$ vertices, and $K_{k,k}$ a complete\nbipartite graph with $k$ vertices on each side of the bipartition. We prove\nthat (1) for any integers $k, t>0$ and a graph $H$ there are finitely many\nsubgraph minimal graphs with no induced $P_k$ and $K_{t,t}$ that are not\n$H$-colorable and (2) for any integer $k>4$ there are finitely many subgraph\nminimal graphs with no induced $P_k$ that are not $C_{k-2}$-colorable.\n  The former generalizes the result of Hell and Huang [Complexity of coloring\ngraphs without paths and cycles, Discrete Appl. Math. 216: 211--232 (2017)] and\nthe latter extends a result of Bruce, Hoang, and Sawada [A certifying algorithm\nfor 3-colorability of $P_5$-Free Graphs, ISAAC 2009: 594--604]. Both our\nresults lead to polynomial-time certifying algorithms for the corresponding\ncoloring problems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 17:35:51 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Kami\u0144ski", "Marcin", ""], ["Pstrucha", "Anna", ""]]}, {"id": "1703.02625", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Nick Duffield, Theodore Willke, Ryan A. Rossi", "title": "On Sampling from Massive Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Graph Priority Sampling (GPS), a new paradigm for order-based\nreservoir sampling from massive streams of graph edges. GPS provides a general\nway to weight edge sampling according to auxiliary and/or size variables so as\nto accomplish various estimation goals of graph properties. In the context of\nsubgraph counting, we show how edge sampling weights can be chosen so as to\nminimize the estimation variance of counts of specified sets of subgraphs. In\ndistinction with many prior graph sampling schemes, GPS separates the functions\nof edge sampling and subgraph estimation. We propose two estimation frameworks:\n(1) Post-Stream estimation, to allow GPS to construct a reference sample of\nedges to support retrospective graph queries, and (2) In-Stream estimation, to\nallow GPS to obtain lower variance estimates by incrementally updating the\nsubgraph count estimates during stream processing. Unbiasedness of subgraph\nestimators is established through a new Martingale formulation of graph stream\norder sampling, which shows that subgraph estimators, written as a product of\nconstituent edge estimators are unbiased, even when computed at different\npoints in the stream. The separation of estimation and sampling enables\nsignificant resource savings relative to previous work. We illustrate our\nframework with applications to triangle and wedge counting. We perform a\nlarge-scale experimental study on real-world graphs from various domains and\ntypes. GPS achieves high accuracy with less than 1% error for triangle and\nwedge counting, while storing a small fraction of the graph with average update\ntimes of a few microseconds per edge. Notably, for a large Twitter graph with\nmore than 260M edges, GPS accurately estimates triangle counts with less than\n1% error, while storing only 40K edges.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 22:18:35 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Willke", "Theodore", ""], ["Rossi", "Ryan A.", ""]]}, {"id": "1703.02689", "submitter": "Erik Lindgren", "authors": "Erik M. Lindgren, Alexandros G. Dimakis, Adam Klivans", "title": "Exact MAP Inference by Avoiding Fractional Vertices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graphical model, one essential problem is MAP inference, that is,\nfinding the most likely configuration of states according to the model.\nAlthough this problem is NP-hard, large instances can be solved in practice. A\nmajor open question is to explain why this is true. We give a natural condition\nunder which we can provably perform MAP inference in polynomial time. We\nrequire that the number of fractional vertices in the LP relaxation exceeding\nthe optimal solution is bounded by a polynomial in the problem size. This\nresolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for\ngeneral LP relaxations of integer programs, known techniques can only handle a\nconstant number of fractional vertices whose value exceeds the optimal\nsolution. We experimentally verify this condition and demonstrate how efficient\nvarious integer programming methods are at removing fractional solutions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 03:55:27 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Lindgren", "Erik M.", ""], ["Dimakis", "Alexandros G.", ""], ["Klivans", "Adam", ""]]}, {"id": "1703.02690", "submitter": "Erik Lindgren", "authors": "Erik M. Lindgren, Shanshan Wu, Alexandros G. Dimakis", "title": "Leveraging Sparsity for Efficient Submodular Data Summarization", "comments": "In NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The facility location problem is widely used for summarizing large datasets\nand has additional applications in sensor placement, image retrieval, and\nclustering. One difficulty of this problem is that submodular optimization\nalgorithms require the calculation of pairwise benefits for all items in the\ndataset. This is infeasible for large problems, so recent work proposed to only\ncalculate nearest neighbor benefits. One limitation is that several strong\nassumptions were invoked to obtain provable approximation guarantees. In this\npaper we establish that these extra assumptions are not necessary---solving the\nsparsified problem will be almost optimal under the standard assumptions of the\nproblem. We then analyze a different method of sparsification that is a better\nmodel for methods such as Locality Sensitive Hashing to accelerate the nearest\nneighbor computations and extend the use of the problem to a broader family of\nsimilarities. We validate our approach by demonstrating that it rapidly\ngenerates interpretable summaries.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 03:56:27 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Lindgren", "Erik M.", ""], ["Wu", "Shanshan", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1703.02693", "submitter": "Nick Duffield", "authors": "Nick Duffield, Yunhong Xu, Liangzhen Xia, Nesreen Ahmed, Minlan Yu", "title": "Stream Aggregation Through Order Sampling", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is paper introduces a new single-pass reservoir weighted-sampling stream\naggregation algorithm, Priority-Based Aggregation (PBA). While order sampling\nis a powerful and e cient method for weighted sampling from a stream of\nuniquely keyed items, there is no current algorithm that realizes the benefits\nof order sampling in the context of stream aggregation over non-unique keys. A\nnaive approach to order sample regardless of key then aggregate the results is\nhopelessly inefficient. In distinction, our proposed algorithm uses a single\npersistent random variable across the lifetime of each key in the cache, and\nmaintains unbiased estimates of the key aggregates that can be queried at any\npoint in the stream. The basic approach can be supplemented with a Sample and\nHold pre-sampling stage with a sampling rate adaptation controlled by PBA. This\napproach represents a considerable reduction in computational complexity\ncompared with the state of the art in adapting Sample and Hold to operate with\na fixed cache size. Concerning statistical properties, we prove that PBA\nprovides unbiased estimates of the true aggregates. We analyze the\ncomputational complexity of PBA and its variants, and provide a detailed\nevaluation of its accuracy on synthetic and trace data. Weighted relative error\nis reduced by 40% to 65% at sampling rates of 5% to 17%, relative to Adaptive\nSample and Hold; there is also substantial improvement for rank queries\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 04:18:58 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 15:01:03 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Duffield", "Nick", ""], ["Xu", "Yunhong", ""], ["Xia", "Liangzhen", ""], ["Ahmed", "Nesreen", ""], ["Yu", "Minlan", ""]]}, {"id": "1703.02743", "submitter": "Tomasz Jurdzinski", "authors": "Tomasz Jurdzinski and Krzysztof Nowicki", "title": "MSF and Connectivity in Limited Variants of the Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The congested clique is a synchronous, message-passing model of distributed\ncomputing in which each computational unit (node) in each round can send\nmessage of O(log n) bits to each other node of the network, where n is the\nnumber of nodes. This model has been considered under two extreme scanarios:\nunicast or broadcast. In the unicast model, a node can send (possibly)\ndifferent message to each other node of the network. In contrast, in the\nbroadcast model each node sends a single (the same) message to all other nodes.\nWe study the congested clique model parametrized by the range r, the maximum\nnumber of different messages a node can send in one round. Following recent\nprogress in design of algorihms for graph connectivity and minimum span- ning\nforest (MSF) in the unicast congested clique, we study these problems in\nlimited variants of the congested clique. We present the first sub-logarithmic\nalgorithm for connected components in the broadcast congested clique. Then, we\nshow that efficient unicast deterministic algorithm for MSF and randomized\nalgorithm for connected components can be efficiently imple- mented in the\nrcast model with range r = 2, the weakest model of the congested clique above\nthe broadcast variant (r = 1) in the hierarchy with respect to range. More\nimportantly, our al- gorithms give the first solutions with optimal capacity of\ncommunication edges, while preserving small round complexity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 08:15:40 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Jurdzinski", "Tomasz", ""], ["Nowicki", "Krzysztof", ""]]}, {"id": "1703.02784", "submitter": "Denis Kurz", "authors": "David Eppstein, Denis Kurz", "title": "$K$-Best Solutions of MSO Problems on Tree-Decomposable Graphs", "comments": "14 pages, 0 figures, submitted to the 44th International Colloquium\n  on Automata, Languages, and Programming (ICALP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that, for any graph optimization problem in which the feasible\nsolutions can be expressed by a formula in monadic second-order logic\ndescribing sets of vertices or edges and in which the goal is to minimize the\nsum of the weights in the selected sets, we can find the $k$ best solutions for\n$n$-vertex graphs of bounded treewidth in time $\\mathcal O(n+k\\log n)$. In\nparticular, this applies to the problem of finding the $k$ shortest simple\npaths between given vertices in directed graphs of bounded treewidth, giving an\nexponential speedup in the per-path cost over previous algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 10:56:03 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Eppstein", "David", ""], ["Kurz", "Denis", ""]]}, {"id": "1703.02866", "submitter": "Ramanujan M. S.", "authors": "Daniel Lokshtanov, M. S. Ramanujan, Saket Saurabh", "title": "The Half-integral Erd\\\"os-P\\'osa Property for Non-null Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Group Labeled Graph is a pair $(G,\\Lambda)$ where $G$ is an oriented graph\nand $\\Lambda$ is a mapping from the arcs of $G$ to elements of a group. A (not\nnecessarily directed) cycle $C$ is called non-null if for any cyclic ordering\nof the arcs in $C$, the group element obtained by `adding' the labels on\nforward arcs and `subtracting' the labels on reverse arcs is not the identity\nelement of the group. Non-null cycles in group labeled graphs generalize\nseveral well-known graph structures, including odd cycles.\n  In this paper, we prove that non-null cycles on Group Labeled Graphs have the\nhalf-integral Erd\\\"os-P\\'osa property. That is, there is a function $f:{\\mathbb\nN}\\to {\\mathbb N}$ such that for any $k\\in {\\mathbb N}$, any group labeled\ngraph $(G,\\Lambda)$ has a set of $k$ non-null cycles such that each vertex of\n$G$ appears in at most two of these cycles or there is a set of at most $f(k)$\nvertices that intersects every non-null cycle. Since it is known that non-null\ncycles do not have the integeral Erd\\\"os-P\\'osa property in general, a\nhalf-integral Erd\\\"os-P\\'osa result is the best one could hope for.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 15:16:05 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1703.02867", "submitter": "Fabian Klemm", "authors": "Andreas Brieden, Peter Gritzmann, Fabian Klemm", "title": "Constrained clustering via diagrams: A unified theory and its\n  applications to electoral district design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper develops a general framework for constrained clustering which is\nbased on the close connection of geometric clustering and diagrams. Various new\nstructural and algorithmic results are proved (and known results generalized\nand unified) which show that the approach is computationally efficient and\nflexible enough to pursue various conflicting demands.\n  The strength of the model is also demonstrated practically on real-world\ninstances of the electoral district design problem where municipalities of a\nstate have to be grouped into districts of nearly equal population while\nobeying certain politically motivated requirements.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 15:16:11 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 10:54:32 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Brieden", "Andreas", ""], ["Gritzmann", "Peter", ""], ["Klemm", "Fabian", ""]]}, {"id": "1703.03048", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "Quickest Visibility Queries in Polygonal Domains", "comments": "A preliminary version to appear in SoCG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $s$ be a point in a polygonal domain $\\mathcal{P}$ of $h-1$ holes and $n$\nvertices. We consider a quickest visibility query problem. Given a query point\n$q$ in $\\mathcal{P}$, the goal is to find a shortest path in $\\mathcal{P}$ to\nmove from $s$ to see $q$ as quickly as possible. Previously, Arkin et al. (SoCG\n2015) built a data structure of size $O(n^22^{\\alpha(n)}\\log n)$ that can\nanswer each query in $O(K\\log^2 n)$ time, where $\\alpha(n)$ is the inverse\nAckermann function and $K$ is the size of the visibility polygon of $q$ in\n$\\mathcal{P}$ (and $K$ can be $\\Theta(n)$ in the worst case). In this paper, we\npresent a new data structure of size $O(n\\log h + h^2)$ that can answer each\nquery in $O(h\\log h\\log n)$ time. Our result improves the previous work when\n$h$ is relatively small. In particular, if $h$ is a constant, then our result\neven matches the best result for the simple polygon case (i.e., $h=1$), which\nis optimal. As a by-product, we also have a new algorithm for a\nshortest-path-to-segment query problem. Given a query line segment $\\tau$ in\n$\\mathcal{P}$, the query seeks a shortest path from $s$ to all points of\n$\\tau$. Previously, Arkin et al. gave a data structure of size\n$O(n^22^{\\alpha(n)}\\log n)$ that can answer each query in $O(\\log^2 n)$ time,\nand another data structure of size $O(n^3\\log n)$ with $O(\\log n)$ query time.\nWe present a data structure of size $O(n)$ with query time $O(h\\log\n\\frac{n}{h})$, which also favors small values of $h$ and is optimal when\n$h=O(1)$.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 21:50:06 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "1703.03147", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Atri Rudra", "title": "Juggling Functions Inside a Database", "comments": "arXiv admin note: text overlap with arXiv:1504.04044", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study the Functional Aggregate Query (FAQ) problem, which\ncaptures common computational tasks across a very wide range of domains\nincluding relational databases, logic, matrix and tensor computation,\nprobabilistic graphical models, constraint satisfaction, and signal processing.\nSimply put, an FAQ is a declarative way of defining a new function from a\ndatabase of input functions.\n  We present \"InsideOut\", a dynamic programming algorithm, to evaluate an FAQ.\nThe algorithm rewrites the input query into a set of easier-to-compute FAQ\nsub-queries. Each sub-query is then evaluated using a worst-case optimal\nrelational join algorithm. The topic of designing algorithms to optimally\nevaluate the classic multiway join problem has seen exciting developments in\nthe past few years. Our framework tightly connects these new ideas in database\ntheory with a vast number of application areas in a coherent manner, showing\npotentially that a good database engine can be a general-purpose constraint\nsolver, relational data store, graphical model inference engine, and\nmatrix/tensor computation processor all at once.\n  The InsideOut algorithm is very simple, as shall be described in this paper.\nYet, in spite of solving an extremely general problem, its runtime either is as\ngood as or improves upon the best known algorithm for the applications that FAQ\nspecializes to. These corollaries include computational tasks in graphical\nmodel inference, matrix/tensor operations, relational joins, and logic. Better\nyet, InsideOut can be used within any database engine, because it is basically\na principled way of rewriting queries. Indeed, it is already part of the\nLogicBlox database engine, helping efficiently answer traditional database\nqueries, graphical model inference queries, and train a large class of machine\nlearning models inside the database itself.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 06:08:01 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Rudra", "Atri", ""]]}, {"id": "1703.03304", "submitter": "Sebastian Siebertz", "authors": "O-joung Kwon and Micha{\\l} Pilipczuk and Sebastian Siebertz", "title": "On low rank-width colorings", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of low rank-width colorings, generalising the notion\nof low tree-depth colorings introduced by Ne\\v{s}et\\v{r}il and Ossona de Mendez\nin [Grad and classes with bounded expansion I. Decompositions. EJC, 2008]. We\nsay that a class $\\mathcal{C}$ of graphs admits low rank-width colourings if\nthere exist functions $N\\colon \\mathbb{N}\\rightarrow\\mathbb{N}$ and $Q\\colon\n\\mathbb{N}\\rightarrow\\mathbb{N}$ such that for all $p\\in \\mathbb{N}$, every\ngraph $G\\in \\mathcal{C}$ can be vertex colored with at most $N(p)$ colors such\nthat the union of any $i\\leq p$ color classes induces a subgraph of rank-width\nat most $Q(i)$.\n  Graph classes admitting low rank-width colorings strictly generalize graph\nclasses admitting low tree-depth colorings and graph classes of bounded\nrank-width. We prove that for every graph class $\\mathcal{C}$ of bounded\nexpansion and every positive integer $r$, the class $\\{G^r\\colon G\\in\n\\mathcal{C}\\}$ of $r$th powers of graphs from $\\mathcal{C}$, as well as the\nclasses of unit interval graphs and bipartite permutation graphs admit low\nrank-width colorings. All of these classes have unbounded rank-width and do not\nadmit low tree-depth colorings. We also show that the classes of interval\ngraphs and permutation graphs do not admit low rank-width colorings. As\ninteresting side properties, we prove that every graph class admitting low\nrank-width colorings has the Erd\\H{o}s-Hajnal property and is $\\chi$-bounded.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 15:46:25 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 09:12:29 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Kwon", "O-joung", ""], ["Pilipczuk", "Micha\u0142", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1703.03484", "submitter": "Bryan Wilder", "authors": "Shaddin Dughmi, Bryan Wilder", "title": "Combinatorial Auctions with Online XOS Bidders", "comments": "Withdrawn due to similarity to previous work by Kesselheim et al. in\n  ESA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In combinatorial auctions, a designer must decide how to allocate a set of\nindivisible items amongst a set of bidders. Each bidder has a valuation\nfunction which gives the utility they obtain from any subset of the items. Our\nfocus is specifically on welfare maximization, where the objective is to\nmaximize the sum of valuations that the bidders place on the items that they\nwere allocated (the valuation functions are assumed to be reported truthfully).\nWe analyze an online problem in which the algorithm is not given the set of\nbidders in advance. Instead, the bidders are revealed sequentially in a\nuniformly random order, similarly to secretary problems. The algorithm must\nmake an irrevocable decision about which items to allocate to the current\nbidder before the next one is revealed. When the valuation functions lie in the\nclass $XOS$ (which includes submodular functions), we provide a black box\nreduction from offline to online optimization. Specifically, given an\n$\\alpha$-approximation algorithm for offline welfare maximization, we show how\nto create a $(0.199 \\alpha)$-approximation algorithm for the online problem.\nOur algorithm draws on connections to secretary problems; in fact, we show that\nthe online welfare maximization problem itself can be viewed as a particular\nkind of secretary problem with nonuniform arrival order.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 22:47:10 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 23:32:49 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Dughmi", "Shaddin", ""], ["Wilder", "Bryan", ""]]}, {"id": "1703.03575", "submitter": "Kasper Green Larsen", "authors": "Kasper Green Larsen, Omri Weinstein, Huacheng Yu", "title": "Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure\n  Lower Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves the first super-logarithmic lower bounds on the cell probe\ncomplexity of dynamic boolean (a.k.a. decision) data structure problems, a\nlong-standing milestone in data structure lower bounds.\n  We introduce a new method for proving dynamic cell probe lower bounds and use\nit to prove a $\\tilde{\\Omega}(\\log^{1.5} n)$ lower bound on the operational\ntime of a wide range of boolean data structure problems, most notably, on the\nquery time of dynamic range counting over $\\mathbb{F}_2$ ([Pat07]). Proving an\n$\\omega(\\lg n)$ lower bound for this problem was explicitly posed as one of\nfive important open problems in the late Mihai P\\v{a}tra\\c{s}cu's obituary\n[Tho13]. This result also implies the first $\\omega(\\lg n)$ lower bound for the\nclassical 2D range counting problem, one of the most fundamental data structure\nproblems in computational geometry and spatial databases. We derive similar\nlower bounds for boolean versions of dynamic polynomial evaluation and 2D\nrectangle stabbing, and for the (non-boolean) problems of range selection and\nrange median.\n  Our technical centerpiece is a new way of \"weakly\" simulating dynamic data\nstructures using efficient one-way communication protocols with small advantage\nover random guessing. This simulation involves a surprising excursion to\nlow-degree (Chebychev) polynomials which may be of independent interest, and\noffers an entirely new algorithmic angle on the \"cell sampling\" method of\nPanigrahy et al. [PTW10].\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 08:35:24 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Larsen", "Kasper Green", ""], ["Weinstein", "Omri", ""], ["Yu", "Huacheng", ""]]}, {"id": "1703.03603", "submitter": "Atsushi Miyauchi", "authors": "Yasushi Kawase, Atsushi Miyauchi", "title": "The Densest Subgraph Problem with a Convex/Concave Size Function", "comments": "19 pages, 3 figures; A preliminary version of this paper appeared in\n  ISAAC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the densest subgraph problem, given an edge-weighted undirected graph\n$G=(V,E,w)$, we are asked to find $S\\subseteq V$ that maximizes the density,\ni.e., $w(S)/|S|$, where $w(S)$ is the sum of weights of the edges in the\nsubgraph induced by $S$. This problem has often been employed in a wide variety\nof graph mining applications. However, the problem has a drawback; it may\nhappen that the obtained subset is too large or too small in comparison with\nthe size desired in the application at hand. In this study, we address the size\nissue of the densest subgraph problem by generalizing the density of\n$S\\subseteq V$. Specifically, we introduce the $f$-density of $S\\subseteq V$,\nwhich is defined as $w(S)/f(|S|)$, where $f:\\mathbb{Z}_{\\geq 0}\\rightarrow\n\\mathbb{R}_{\\geq 0}$ is a monotonically non-decreasing function. In the\n$f$-densest subgraph problem ($f$-DS), we aim to find $S\\subseteq V$ that\nmaximizes the $f$-density $w(S)/f(|S|)$. Although $f$-DS does not explicitly\nspecify the size of the output subset of vertices, we can handle the above size\nissue using a convex/concave size function $f$ appropriately. For $f$-DS with\nconvex function $f$, we propose a nearly-linear-time algorithm with a provable\napproximation guarantee. On the other hand, for $f$-DS with concave function\n$f$, we propose an LP-based exact algorithm, a flow-based $O(|V|^3)$-time exact\nalgorithm for unweighted graphs, and a nearly-linear-time approximation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 09:58:40 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Kawase", "Yasushi", ""], ["Miyauchi", "Atsushi", ""]]}, {"id": "1703.03849", "submitter": "Chao Xu", "authors": "Chandra Chekuri, Chao Xu", "title": "A note on approximate strengths of edges in a hypergraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $H=(V,E)$ be an edge-weighted hypergraph of rank $r$. Kogan and\nKrauthgamer extended Bencz\\'{u}r and Karger's random sampling scheme for cut\nsparsification from graphs to hypergraphs. The sampling requires an algorithm\nfor computing the approximate strengths of edges. In this note we extend the\nalgorithm for graphs to hypergraphs and describe a near-linear time algorithm\nto compute approximate strengths of edges; we build on a sparsification result\nfor hypergraphs from our recent work. Combined with prior results we obtain\nfaster algorithms for finding $(1+\\epsilon)$-approximate mincuts when the rank\nof the hypergraph is small.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 21:51:50 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Chekuri", "Chandra", ""], ["Xu", "Chao", ""]]}, {"id": "1703.03859", "submitter": "Guilherme Fran\\c{c}a", "authors": "Guilherme Fran\\c{c}a, Jos\\'e Bento", "title": "Markov Chain Lifting and Distributed ADMM", "comments": "This work was also selected for a talk at NIPS 2016, Optimization for\n  Machine Learning Workshop (OPT 2016)", "journal-ref": "IEEE Signal Processing Letters (Volume: 24, Issue: 3, March 2017)", "doi": "10.1109/LSP.2017.2654860", "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time to converge to the steady state of a finite Markov chain can be\ngreatly reduced by a lifting operation, which creates a new Markov chain on an\nexpanded state space. For a class of quadratic objectives, we show an analogous\nbehavior where a distributed ADMM algorithm can be seen as a lifting of\nGradient Descent algorithm. This provides a deep insight for its faster\nconvergence rate under optimal parameter tuning. We conjecture that this gain\nis always present, as opposed to the lifting of a Markov chain which sometimes\nonly provides a marginal speedup.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 22:25:56 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Fran\u00e7a", "Guilherme", ""], ["Bento", "Jos\u00e9", ""]]}, {"id": "1703.03900", "submitter": "Na Wang", "authors": "Na Wang, Dongxiao Yu, Hai Jin, Qiang-Sheng Hua, Xuanhua Shi, Xia Xie", "title": "Core Maintenance in Dynamic Graphs: A Parallel Approach based on\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core number of a vertex is a basic index depicting cohesiveness of a\ngraph, and has been widely used in large-scale graph analytics. In this paper,\nwe study the update of core numbers of vertices in dynamic graphs with edge\ninsertions/deletions, which is known as the core maintenance problem. Different\nfrom previous approaches that just focus on the case of single-edge\ninsertion/deletion and sequentially handle the edges when multiple edges are\ninserted/deleted, we investigate the parallelism in the core maintenance\nprocedure. Specifically, we show that if the inserted/deleted edges constitute\na matching, the core number update with respect to each inserted/deleted edge\ncan be handled in parallel. Based on this key observation, we propose parallel\nalgorithms for core maintenance in both cases of edge insertions and deletions.\nWe conduct extensive experiments to evaluate the efficiency, stability,\nparallelism and scalability of our algorithms on different types of real-world\nand synthetic graphs. Comparing with sequential approaches, our algorithms can\nimprove the core maintenance efficiency significantly.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 03:28:30 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Wang", "Na", ""], ["Yu", "Dongxiao", ""], ["Jin", "Hai", ""], ["Hua", "Qiang-Sheng", ""], ["Shi", "Xuanhua", ""], ["Xie", "Xia", ""]]}, {"id": "1703.03963", "submitter": "Anton Eremeev", "authors": "Anton Eremeev, Yulia Kovalenko", "title": "On Solving Travelling Salesman Problem with Vertex Requisitions", "comments": "To appear in Yugoslav Journal of Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Travelling Salesman Problem with Vertex Requisitions, where\nfor each position of the tour at most two possible vertices are given. It is\nknown that the problem is strongly NP-hard. The proposed algorithm for this\nproblem has less time complexity compared to the previously known one. In\nparticular, almost all feasible instances of the problem are solvable in O(n)\ntime using the new algorithm, where n is the number of vertices. The developed\napproach also helps in fast enumeration of a neighborhood in the local search\nand yields an integer programming model with O(n) binary variables for the\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 12:24:19 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Eremeev", "Anton", ""], ["Kovalenko", "Yulia", ""]]}, {"id": "1703.03998", "submitter": "Harold Gabow", "authors": "Harold N. Gabow", "title": "The Weighted Matching Approach to Maximum Cardinality Matching", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several papers have achieved time $O(\\sqrt n m)$ for cardinality matching,\nstarting from first principles. This results in a long derivation. We simplify\nthe task by employing well-known concepts for maximum weight matching. We use\nEdmonds' algorithm to derive the structure of shortest augmenting paths. We\nextend this to a complete algorithm for maximum cardinality matching in time\n$O(\\sqrt n m)$.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 16:53:04 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Gabow", "Harold N.", ""]]}, {"id": "1703.04040", "submitter": "Francesco Silvestri", "authors": "Anne Driemel and Francesco Silvestri", "title": "Locality-sensitive hashing of curves", "comments": "Proc. of 33rd International Symposium on Computational Geometry\n  (SoCG), 2017", "journal-ref": null, "doi": "10.4230/LIPIcs.SoCG.2017.37", "report-no": null, "categories": "cs.CG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study data structures for storing a set of polygonal curves in ${\\rm R}^d$\nsuch that, given a query curve, we can efficiently retrieve similar curves from\nthe set, where similarity is measured using the discrete Fr\\'echet distance or\nthe dynamic time warping distance. To this end we devise the first\nlocality-sensitive hashing schemes for these distance measures. A major\nchallenge is posed by the fact that these distance measures internally optimize\nthe alignment between the curves. We give solutions for different types of\nalignments including constrained and unconstrained versions. For unconstrained\nalignments, we improve over a result by Indyk from 2002 for short curves. Let\n$n$ be the number of input curves and let $m$ be the maximum complexity of a\ncurve in the input. In the particular case where $m \\leq \\frac{\\alpha}{4d} \\log\nn$, for some fixed $\\alpha>0$, our solutions imply an approximate near-neighbor\ndata structure for the discrete Fr\\'echet distance that uses space in\n$O(n^{1+\\alpha}\\log n)$ and achieves query time in $O(n^{\\alpha}\\log^2 n)$ and\nconstant approximation factor. Furthermore, our solutions provide a trade-off\nbetween approximation quality and computational performance: for any parameter\n$k \\in [m]$, we can give a data structure that uses space in $O(2^{2k}m^{k-1} n\n\\log n + nm)$, answers queries in $O( 2^{2k} m^{k}\\log n)$ time and achieves\napproximation factor in $O(m/k)$.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 23:16:23 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Driemel", "Anne", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1703.04143", "submitter": "Rad Niazadeh", "authors": "Shaddin Dughmi, Jason Hartline, Robert Kleinberg, Rad Niazadeh", "title": "Bernoulli Factories and Black-Box Reductions in Mechanism Design", "comments": "Forthcoming in the Journal of the ACM (JACM) - Nov 2020; conference\n  version appeared in Proc. 49th ACM Symposium on Theory of Computing (STOC\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a polynomial time reduction from Bayesian incentive compatible\nmechanism design to Bayesian algorithm design for welfare maximization\nproblems. Unlike prior results, our reduction achieves exact incentive\ncompatibility for problems with multi-dimensional and continuous type spaces.\nThe key technical barrier preventing exact incentive compatibility in prior\nblack-box reductions is that repairing violations of incentive constraints\nrequires understanding the distribution of the mechanism's output, which is\ntypically #P-hard to compute. Reductions that instead estimate the output\ndistribution by sampling inevitably suffer from sampling error, which typically\nprecludes exact incentive compatibility.\n  We overcome this barrier by employing and generalizing the computational\nmodel in the literature on $\\textit{Bernoulli factories}$. In a Bernoulli\nfactory problem, one is given a function mapping the bias of an \"input coin\" to\nthat of an \"output coin\", and the challenge is to efficiently simulate the\noutput coin given only sample access to the input coin. This is the key\ningredient in designing an incentive compatible mechanism for bipartite\nmatching, which can be used to make the approximately incentive compatible\nreduction of Hartline et al. (2015) exactly incentive compatible.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 17:11:49 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 23:57:59 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Dughmi", "Shaddin", ""], ["Hartline", "Jason", ""], ["Kleinberg", "Robert", ""], ["Niazadeh", "Rad", ""]]}, {"id": "1703.04230", "submitter": "Zeev Nutov", "authors": "Zeev Nutov", "title": "Improved approximation algorithms for $k$-connected $m$-dominating set\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is $k$-connected if it has $k$ internally-disjoint paths between\nevery pair of nodes. A subset $S$ of nodes in a graph $G$ is a $k$-connected\nset if the subgraph $G[S]$ induced by $S$ is $k$-connected; $S$ is an\n$m$-dominating set if every $v \\in V \\setminus S$ has at least $m$ neighbors in\n$S$. If $S$ is both $k$-connected and $m$-dominating then $S$ is a\n$k$-connected $m$-dominating set, or $(k,m)$-cds for short. In the\n$k$-Connected $m$-Dominating Set ($(k,m)$-CDS) problem the goal is to find a\nminimum weight $(k,m)$-cds in a node-weighted graph. We consider the case $m\n\\geq k$ and obtain the following approximation ratios. For unit disc-graphs we\nobtain ratio $O(k\\ln k)$, improving the previous ratio $O(k^2 \\ln k)$. For\ngeneral graphs we obtain the first non-trivial approximation ratio $O(k^2 \\ln\nn)$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 02:57:50 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Nutov", "Zeev", ""]]}, {"id": "1703.04381", "submitter": "Othon Michail", "authors": "Othon Michail, George Skretas, Paul G. Spirakis", "title": "On the Transformation Capability of Feasible Mechanisms for Programmable\n  Matter", "comments": "48 pages, 32 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study theoretical models of \\emph{programmable matter}\nsystems. The systems under consideration consist of spherical modules, kept\ntogether by magnetic forces and able to perform two minimal mechanical\noperations (or movements): \\emph{rotate} around a neighbor and \\emph{slide}\nover a line. In terms of modeling, there are $n$ nodes arranged in a\n2-dimensional grid and forming some initial \\emph{shape}. The goal is for the\ninitial shape $A$ to \\emph{transform} to some target shape $B$ by a sequence of\nmovements. Most of the paper focuses on \\emph{transformability} questions,\nmeaning whether it is in principle feasible to transform a given shape to\nanother. We first consider the case in which only rotation is available to the\nnodes. Our main result is that deciding whether two given shapes $A$ and $B$\ncan be transformed to each other, is in $\\mathbf{P}$. We then insist on\nrotation only and impose the restriction that the nodes must maintain global\nconnectivity throughout the transformation. We prove that the corresponding\ntransformability question is in $\\mathbf{PSPACE}$ and study the problem of\ndetermining the minimum \\emph{seeds} that can make feasible, otherwise\ninfeasible transformations. Next we allow both rotations and slidings and prove\nuniversality: any two connected shapes $A,B$ of the same order, can be\ntransformed to each other without breaking connectivity. The worst-case number\nof movements of the generic strategy is $\\Omega(n^2)$. We improve this to\n$O(n)$ parallel time, by a pipelining strategy, and prove optimality of both by\nmatching lower bounds. In the last part of the paper, we turn our attention to\ndistributed transformations. The nodes are now distributed processes able to\nperform communicate-compute-move rounds. We provide distributed algorithms for\na general type of transformations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 13:31:17 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Michail", "Othon", ""], ["Skretas", "George", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "1703.04466", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "Bicriteria Rectilinear Shortest Paths among Rectilinear Obstacles in the\n  Plane", "comments": "A preliminary version to appear in SoCG 2017; corrected a few typos\n  from the last version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a rectilinear domain $\\mathcal{P}$ of $h$ pairwise-disjoint rectilinear\nobstacles with a total of $n$ vertices in the plane, we study the problem of\ncomputing bicriteria rectilinear shortest paths between two points $s$ and $t$\nin $\\mathcal{P}$. Three types of bicriteria rectilinear paths are considered:\nminimum-link shortest paths, shortest minimum-link paths, and minimum-cost\npaths where the cost of a path is a non-decreasing function of both the number\nof edges and the length of the path. The one-point and two-point path queries\nare also considered. Algorithms for these problems have been given previously.\nOur contributions are threefold. First, we find a critical error in all\nprevious algorithms. Second, we correct the error in a not-so-trivial way.\nThird, we further improve the algorithms so that they are even faster than the\nprevious (incorrect) algorithms when $h$ is relatively small. For example, for\nthe minimum-link shortest paths, we obtain the following results. Our algorithm\ncomputes a minimum-link shortest $s$-$t$ path in $O(n+h\\log^{3/2} h)$ time. For\nthe one-point queries, we build a data structure of size $O(n+ h\\log h)$ in\n$O(n+h\\log^{3/2} h)$ time for a source point $s$, such that given any query\npoint $t$, a minimum-link shortest $s$-$t$ path can be determined in $O(\\log\nn)$ time. For the two-point queries, with $O(n+h^2\\log^2 h)$ time and space\npreprocessing, a minimum-link shortest $s$-$t$ path can be determined in\n$O(\\log n+\\log^2 h)$ time for any two query points $s$ and $t$; alternatively,\nwith $O(n+h^2\\cdot \\log^{2} h \\cdot 4^{\\sqrt{\\log h}})$ time and $O(n+h^2\\cdot\n\\log h \\cdot 4^{\\sqrt{\\log h}})$ space preprocessing, we can answer each\ntwo-point query in $O(\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 16:18:01 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 23:08:33 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "1703.04664", "submitter": "Anshumali Shrivastava", "authors": "Anshumali Shrivastava", "title": "Optimal Densification for Fast and Accurate Minwise Hashing", "comments": "Fast Minwise Hashing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing is a fundamental and one of the most successful hashing\nalgorithm in the literature. Recent advances based on the idea of\ndensification~\\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown\nthat it is possible to compute $k$ minwise hashes, of a vector with $d$\nnonzeros, in mere $(d + k)$ computations, a significant improvement over the\nclassical $O(dk)$. These advances have led to an algorithmic improvement in the\nquery complexity of traditional indexing algorithms based on minwise hashing.\nUnfortunately, the variance of the current densification techniques is\nunnecessarily high, which leads to significantly poor accuracy compared to\nvanilla minwise hashing, especially when the data is sparse. In this paper, we\nprovide a novel densification scheme which relies on carefully tailored\n2-universal hashes. We show that the proposed scheme is variance-optimal, and\nwithout losing the runtime efficiency, it is significantly more accurate than\nexisting densification techniques. As a result, we obtain a significantly\nefficient hashing scheme which has the same variance and collision probability\nas minwise hashing. Experimental evaluations on real sparse and\nhigh-dimensional datasets validate our claims. We believe that given the\nsignificant advantages, our method will replace minwise hashing implementations\nin practice.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 18:49:57 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Shrivastava", "Anshumali", ""]]}, {"id": "1703.04769", "submitter": "Virgile Galle", "authors": "Virgile Galle, Setareh Borjian Boroujeni, Vahideh H. Manshadi, Cynthia\n  Barnhart and Patrick Jaillet", "title": "The Stochastic Container Relocation Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Container Relocation Problem (CRP) is concerned with finding a sequence\nof moves of containers that minimizes the number of relocations needed to\nretrieve all containers, while respecting a given order of retrieval. However,\nthe assumption of knowing the full retrieval order of containers is\nparticularly unrealistic in real operations. This paper studies the stochastic\nCRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model,\ncalled the batch model, is introduced, motivated, and compared with an existing\nmodel (the online model). The two main contributions are an optimal algorithm\ncalled Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm\ncalled PBFS-Approximate with a bounded average error. Both algorithms,\napplicable in the batch and online models, are based on a new family of lower\nbounds for which we show some theoretical properties. Moreover, we introduce\ntwo new heuristics outperforming the best existing heuristics. Algorithms,\nbounds and heuristics are tested in an extensive computational section.\nFinally, based on strong computational evidence, we conjecture the optimality\nof the \"Leveling\" heuristic in a special \"no information\" case, where at any\nretrieval stage, any of the remaining containers is equally likely to be\nretrieved next.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:16:53 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 17:52:33 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Galle", "Virgile", ""], ["Boroujeni", "Setareh Borjian", ""], ["Manshadi", "Vahideh H.", ""], ["Barnhart", "Cynthia", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1703.04814", "submitter": "Oren Weimann", "authors": "Amir Abboud, Pawel Gawrychowski, Shay Mozes, Oren Weimann", "title": "Near-Optimal Compression for the Planar Graph Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Planar Graph Metric Compression Problem is to compactly encode the\ndistances among $k$ nodes in a planar graph of size $n$. Two na\\\"ive solutions\nare to store the graph using $O(n)$ bits, or to explicitly store the distance\nmatrix with $O(k^2 \\log{n})$ bits. The only lower bounds are from the seminal\nwork of Gavoille, Peleg, Prennes, and Raz [SODA'01], who rule out compressions\ninto a polynomially smaller number of bits, for {\\em weighted} planar graphs,\nbut leave a large gap for unweighted planar graphs. For example, when\n$k=\\sqrt{n}$, the upper bound is $O(n)$ and their constructions imply an\n$\\Omega(n^{3/4})$ lower bound. This gap is directly related to other major open\nquestions in labelling schemes, dynamic algorithms, and compact routing.\n  Our main result is a new compression of the planar graph metric into\n$\\tilde{O}(\\min (k^2 , \\sqrt{k\\cdot n}))$ bits, which is optimal up to log\nfactors. Our data structure breaks an $\\Omega(k^2)$ lower bound of Krauthgamer,\nNguyen, and Zondiner [SICOMP'14] for compression using minors, and the lower\nbound of Gavoille et al. for compression of weighted planar graphs. This is an\nunexpected and decisive proof that weights can make planar graphs inherently\nmore complex. Moreover, we design a new {\\em Subset Distance Oracle} for planar\ngraphs with $\\tilde O(\\sqrt{k\\cdot n})$ space, and $\\tilde O(n^{3/4})$ query\ntime.\n  Our work carries strong messages to related fields. In particular, the famous\n$O(n^{1/2})$ vs. $\\Omega(n^{1/3})$ gap for distance labelling schemes in planar\ngraphs {\\em cannot} be resolved with the current lower bound techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 23:06:33 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Abboud", "Amir", ""], ["Gawrychowski", "Pawel", ""], ["Mozes", "Shay", ""], ["Weimann", "Oren", ""]]}, {"id": "1703.04954", "submitter": "Hideo Bannai", "authors": "Keita Kuboi, Yuta Fujishige, Shunsuke Inenaga, Hideo Bannai, Masayuki\n  Takeda", "title": "Faster STR-IC-LCS computation via RLE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constrained LCS problem asks one to find a longest common subsequence of\ntwo input strings $A$ and $B$ with some constraints. The STR-IC-LCS problem is\na variant of the constrained LCS problem, where the solution must include a\ngiven constraint string $C$ as a substring. Given two strings $A$ and $B$ of\nrespective lengths $M$ and $N$, and a constraint string $C$ of length at most\n$\\min\\{M, N\\}$, the best known algorithm for the STR-IC-LCS problem, proposed\nby Deorowicz~({\\em Inf. Process. Lett.}, 11:423--426, 2012), runs in $O(MN)$\ntime. In this work, we present an $O(mN + nM)$-time solution to the STR-IC-LCS\nproblem, where $m$ and $n$ denote the sizes of the run-length encodings of $A$\nand $B$, respectively. Since $m \\leq M$ and $n \\leq N$ always hold, our\nalgorithm is always as fast as Deorowicz's algorithm, and is faster when input\nstrings are compressible via RLE.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:21:59 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Kuboi", "Keita", ""], ["Fujishige", "Yuta", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1703.05097", "submitter": "Taoyang Wu", "authors": "Vincent Moulton and James Oldman and Taoyang Wu", "title": "A cubic-time algorithm for computing the trinet distance between level-1\n  networks", "comments": "11pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In evolutionary biology, phylogenetic networks are constructed to represent\nthe evolution of species in which reticulate events are thought to have\noccurred, such as recombination and hybridization. It is therefore useful to\nhave efficiently computable metrics with which to systematically compare such\nnetworks. Through developing an optimal algorithm to enumerate all trinets\ndisplayed by a level-1 network (a type of network that is slightly more general\nthan an evolutionary tree), here we propose a cubic-time algorithm to compute\nthe trinet distance between two level-1 networks. Employing simulations, we\nalso present a comparison between the trinet metric and the so-called\nRobinson-Foulds phylogenetic network metric restricted to level-1 networks. The\nalgorithms described in this paper have been implemented in JAVA and are freely\navailable at https://www.uea.ac.uk/computing/TriLoNet.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 11:57:53 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Moulton", "Vincent", ""], ["Oldman", "James", ""], ["Wu", "Taoyang", ""]]}, {"id": "1703.05102", "submitter": "Petr Golovach", "authors": "Petr A. Golovach, Pinar Heggernes, Dieter Kratsch, Paloma T. Lima, and\n  Daniel Paulusma", "title": "Algorithms for outerplanar graph roots and graph roots of pathwidth at\n  most 2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding whether a given graph has a square root is a classical problem that\nhas been studied extensively both from graph theoretic and from algorithmic\nperspectives. The problem is NP-complete in general, and consequently\nsubstantial effort has been dedicated to deciding whether a given graph has a\nsquare root that belongs to a particular graph class. There are both\npolynomial-time solvable and NP-complete cases, depending on the graph class.\nWe contribute with new results in this direction. Given an arbitrary input\ngraph G, we give polynomial-time algorithms to decide whether G has an\nouterplanar square root, and whether G has a square root that is of pathwidth\nat most 2.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 12:06:53 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 08:21:09 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Golovach", "Petr A.", ""], ["Heggernes", "Pinar", ""], ["Kratsch", "Dieter", ""], ["Lima", "Paloma T.", ""], ["Paulusma", "Daniel", ""]]}, {"id": "1703.05156", "submitter": "Dorian Mazauric", "authors": "Nathann Cohen (LRI), Fr\\'ed\\'eric Havet (COATI, UCA), Dorian Mazauric\n  (UCA, ABS), Ignasi Sau (ALGCO), R\\'emi Watrigant (UCA, ABS)", "title": "Complexity Dichotomies for the Minimum F-Overlay Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a (possibly infinite) fixed family of graphs F, we say that a graph G\noverlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G\ninduced by every hyperedge of H contains some member of F as a spanning\nsubgraph.While it is easy to see that the complete graph on |V(H)| overlays F\non a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay\nproblem asks for such a graph with the minimum number of edges.This problem\nallows to generalize some natural problems which may arise in practice. For\ninstance, if the family F contains all connected graphs, then Minimum F-Overlay\ncorresponds to the Minimum Connectivity Inference problem (also known as Subset\nInterconnection Design problem) introduced for the low-resolution\nreconstruction of macro-molecular assembly in structural biology, or for the\ndesign of networks.Our main contribution is a strong dichotomy result regarding\nthe polynomial vs. NP-hard status with respect to the considered family F.\nRoughly speaking, we show that the easy cases one can think of (e.g. when\nedgeless graphs of the right sizes are in F, or if F contains only cliques) are\nthe only families giving rise to a polynomial problem: all others are\nNP-complete.We then investigate the parameterized complexity of the problem and\ngive similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard\nor FPT problems when the parameter is the size of the solution.This yields an\nFPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must\ncontain some member of F as a (non necessarily spanning) subgraph.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 13:51:23 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Cohen", "Nathann", "", "LRI"], ["Havet", "Fr\u00e9d\u00e9ric", "", "COATI, UCA"], ["Mazauric", "Dorian", "", "UCA, ABS"], ["Sau", "Ignasi", "", "ALGCO"], ["Watrigant", "R\u00e9mi", "", "UCA, ABS"]]}, {"id": "1703.05160", "submitter": "Ryan Spring", "authors": "Ryan Spring, Anshumali Shrivastava", "title": "A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators\n  for Partition Function Computation in Log-Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-linear models are arguably the most successful class of graphical models\nfor large-scale applications because of their simplicity and tractability.\nLearning and inference with these models require calculating the partition\nfunction, which is a major bottleneck and intractable for large state spaces.\nImportance Sampling (IS) and MCMC-based approaches are lucrative. However, the\ncondition of having a \"good\" proposal distribution is often not satisfied in\npractice.\n  In this paper, we add a new dimension to efficient estimation via sampling.\nWe propose a new sampling scheme and an unbiased estimator that estimates the\npartition function accurately in sub-linear time. Our samples are generated in\nnear-constant time using locality sensitive hashing (LSH), and so are\ncorrelated and unnormalized. We demonstrate the effectiveness of our proposed\napproach by comparing the accuracy and speed of estimating the partition\nfunction against other state-of-the-art estimation techniques including IS and\nthe efficient variant of Gumbel-Max sampling. With our efficient sampling\nscheme, we accurately train real-world language models using only 1-2% of\ncomputations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:01:21 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Spring", "Ryan", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1703.05199", "submitter": "Ramesh Krishnan S. Pallavoor", "authors": "Roksana Baleshzar, Deeparnab Chakrabarty, Ramesh Krishnan S.\n  Pallavoor, Sofya Raskhodnikova, C. Seshadhri", "title": "Optimal Unateness Testers for Real-Valued Functions: Adaptivity Helps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing unateness of functions $f:\\{0,1\\}^d \\to\n\\mathbb{R}.$ We give a $O(\\frac{d}{\\epsilon} \\cdot\n\\log\\frac{d}{\\epsilon})$-query nonadaptive tester and a\n$O(\\frac{d}{\\epsilon})$-query adaptive tester and show that both testers are\noptimal for a fixed distance parameter $\\epsilon$. Previously known unateness\ntesters worked only for Boolean functions, and their query complexity had worse\ndependence on the dimension both for the adaptive and the nonadaptive case.\nMoreover, no lower bounds for testing unateness were known. We also generalize\nour results to obtain optimal unateness testers for functions $f:[n]^d \\to\n\\mathbb{R}$.\n  Our results establish that adaptivity helps with testing unateness of\nreal-valued functions on domains of the form $\\{0,1\\}^d$ and, more generally,\n$[n]^d$. This stands in contrast to the situation for monotonicity testing\nwhere there is no adaptivity gap for functions $f:[n]^d \\to \\mathbb{R}$.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 15:10:16 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Baleshzar", "Roksana", ""], ["Chakrabarty", "Deeparnab", ""], ["Pallavoor", "Ramesh Krishnan S.", ""], ["Raskhodnikova", "Sofya", ""], ["Seshadhri", "C.", ""]]}, {"id": "1703.05418", "submitter": "Reut Levi", "authors": "Christoph Lenzen and Reut Levi", "title": "A Local Algorithm for the Sparse Spanning Graph Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing a sparse spanning subgraph is a fundamental primitive in graph\ntheory. In this paper, we study this problem in the Centralized Local model,\nwhere the goal is to decide whether an edge is part of the spanning subgraph by\nexamining only a small part of the input; yet, answers must be globally\nconsistent and independent of prior queries.\n  Unfortunately, maximally sparse spanning subgraphs, i.e., spanning trees,\ncannot be constructed efficiently in this model. Therefore, we settle for a\nspanning subgraph containing at most $(1+\\varepsilon)n$ edges (where $n$ is the\nnumber of vertices and $\\varepsilon$ is a given approximation/sparsity\nparameter). We achieve query complexity of\n$\\tilde{O}(poly(\\Delta/\\varepsilon)n^{2/3})$, ($\\tilde{O}$-notation hides\npolylogarithmic factors in $n$). where $\\Delta$ is the maximum degree of the\ninput graph. Our algorithm is the first to do so on arbitrary bounded degree\ngraphs. Moreover, we achieve the additional property that our algorithm outputs\na spanner, i.e., distances are approximately preserved. With high probability,\nfor each deleted edge there is a path of $O(poly(\\Delta/\\varepsilon)\\log^2 n)$\nhops in the output that connects its endpoints.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 23:02:07 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 22:04:19 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Lenzen", "Christoph", ""], ["Levi", "Reut", ""]]}, {"id": "1703.05496", "submitter": "Gregory  Karagiorgos", "authors": "Aristotelis Giannakos and Mhand Hifi and Gregory Karagiorgos", "title": "Data Delivery by Mobile Agents with Energy Constraints over a fixed path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider $k$ mobile agents of limited energy that are initially located at\nvertices of an edge-weighted graph $G$ and have to collectively deliver data\nfrom a source vertex $s$ to a target vertex $t$. The data are to be collected\nby an agent reaching $s$, who can carry and then hand them over another agent\netc., until some agent with the data reaches $t$. The data can be carried only\nover a fixed $s-t$ path of $G$; each agent has an initial energy budget and\neach time it passes an edge, it consumes the edge's weights in energy units and\nstalls if its energy is not anymore sufficient to move. The main result of this\npaper is a 3-approximation polynomial time algorithm for the data delivery\nproblem over a fixed $s-t$ path in the graph, for identical initial energy\nbudgets and at most one allowed data hand-over per agent.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 08:10:30 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Giannakos", "Aristotelis", ""], ["Hifi", "Mhand", ""], ["Karagiorgos", "Gregory", ""]]}, {"id": "1703.05509", "submitter": "Christian Schulz", "authors": "Christian Schulz and Jesper Larsson Tr\\\"aff", "title": "VieM v1.00 -- Vienna Mapping and Sparse Quadratic Assignment User Guide", "comments": "arXiv admin note: text overlap with arXiv:1311.1714", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper severs as a user guide to the mapping framework VieM (Vienna\nMapping and Sparse Quadratic Assignment). We give a rough overview of the\ntechniques used within the framework and describe the user interface as well as\nthe file formats used.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 08:52:21 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Schulz", "Christian", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1703.05559", "submitter": "Lukasz Kowalik", "authors": "Marek Cygan, Lukasz Kowalik, Arkadiusz Socala", "title": "Improving TSP tours using dynamic programming over tree decomposition", "comments": "Accepted to ESA'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a traveling salesman problem (TSP) tour $H$ in graph $G$ a $k$-move is\nan operation which removes $k$ edges from $H$, and adds $k$ edges of $G$ so\nthat a new tour $H'$ is formed. The popular $k$-OPT heuristics for TSP finds a\nlocal optimum by starting from an arbitrary tour $H$ and then improving it by a\nsequence of $k$-moves.\n  Until 2016, the only known algorithm to find an improving $k$-move for a\ngiven tour was the naive solution in time $O(n^k)$. At ICALP'16 de Berg,\nBuchin, Jansen and Woeginger showed an $O(n^{\\lfloor 2/3k \\rfloor+1})$-time\nalgorithm.\n  We show an algorithm which runs in $O(n^{(1/4+\\epsilon_k)k})$ time, where\n$\\lim \\epsilon_k = 0$. We are able to show that it improves over the state of\nthe art for every $k=5,\\ldots,10$. For the most practically relevant case $k=5$\nwe provide a slightly refined algorithm running in $O(n^{3.4})$ time. We also\nshow that for the $k=4$ case, improving over the $O(n^3)$-time algorithm of de\nBerg et al. would be a major breakthrough: an $O(n^{3-\\epsilon})$-time\nalgorithm for any $\\epsilon>0$ would imply an $O(n^{3-\\delta})$-time algorithm\nfor the ALL PAIRS SHORTEST PATHS problem, for some $\\delta>0$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 11:09:25 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 07:28:18 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Cygan", "Marek", ""], ["Kowalik", "Lukasz", ""], ["Socala", "Arkadiusz", ""]]}, {"id": "1703.05568", "submitter": "Ammar Daskin", "authors": "Ammar Daskin", "title": "Quantum Spectral Clustering through a Biased Phase Estimation Algorithm", "comments": "5 pages, submitted to a conference, and any comments are welcomed", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this brief paper, we go through the theoretical steps of the spectral\nclustering on quantum computers by employing the phase estimation and the\namplitude amplification algorithms. We discuss circuit designs for each step\nand show how to obtain the clustering solution from the output state. In\naddition, we introduce a biased version of the phase estimation algorithm which\nsignificantly speeds up the amplitude amplification process. The complexity of\nthe whole process is analyzed: it is shown that when the circuit representation\nof a data matrix of order $N$ is produced through an ancilla based circuit in\nwhich the matrix is written as a sum of $L$ number of Householder matrices; the\ncomputational complexity is bounded by $O(2^mLN)$ number of quantum gates.\nHere, $m$ represents the number of qubits (e.g., 6) involved in the phase\nregister of the phase estimation algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 11:39:22 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 07:30:12 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Daskin", "Ammar", ""]]}, {"id": "1703.05598", "submitter": "George Mertzios", "authors": "George B. Mertzios, Andr\\'e Nichterlein, Rolf Niedermeier", "title": "A Linear-Time Algorithm for Maximum-Cardinality Matching on\n  Cocomparability Graphs", "comments": "16 pages, 4 figures. to appear at SIDMA arXiv admin note: text\n  overlap with arXiv:1609.08879", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding maximum-cardinality matchings in undirected graphs is arguably one of\nthe most central graph problems. For general m-edge and n-vertex graphs, it is\nwell-known to be solvable in $O(m \\sqrt{n})$ time. We develop a linear-time\nalgorithm to find maximum-cardinality matchings on cocomparability graphs, a\nprominent subclass of perfect graphs that contains interval graphs as well as\npermutation graphs. Our algorithm is based on the recently discovered\nLexicographic Depth First Search (LDFS).\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 13:10:29 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 17:29:03 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 21:09:41 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Mertzios", "George B.", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1703.05997", "submitter": "Ben Strasser", "authors": "Julian Dibbelt, Thomas Pajor, Ben Strasser, Dorothea Wagner", "title": "Connection Scan Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Connection Scan Algorithm (CSA) to efficiently answer\nqueries to timetable information systems. The input consists, in the simplest\nsetting, of a source position and a desired target position. The output consist\nis a sequence of vehicles such as trains or buses that a traveler should take\nto get from the source to the target. We study several problem variations such\nas the earliest arrival and profile problems. We present algorithm variants\nthat only optimize the arrival time or additionally optimize the number of\ntransfers in the Pareto sense. An advantage of CSA is that is can easily adjust\nto changes in the timetable, allowing the easy incorporation of known vehicle\ndelays. We additionally introduce the Minimum Expected Arrival Time (MEAT)\nproblem to handle possible, uncertain, future vehicle delays. We present a\nsolution to the MEAT problem that is based upon CSA. Finally, we extend CSA\nusing the multilevel overlay paradigm to answer complex queries on nation-wide\nintegrated timetables with trains and buses.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 12:57:18 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Dibbelt", "Julian", ""], ["Pajor", "Thomas", ""], ["Strasser", "Ben", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1703.06040", "submitter": "Benjamin Niedermann", "authors": "Lukas Barth, Benjamin Niedermann, Ignaz Rutter, Matthias Wolf", "title": "Towards a Topology-Shape-Metrics Framework for Ortho-Radial Drawings", "comments": "Extended version of a paper to appear at the 33rd International\n  Symposium on Computational Geometry (SoCG 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ortho-Radial drawings are a generalization of orthogonal drawings to grids\nthat are formed by concentric circles and straight-line spokes emanating from\nthe circles' center. Such drawings have applications in schematic graph\nlayouts, e.g., for metro maps and destination maps.\n  A plane graph is a planar graph with a fixed planar embedding. We give a\ncombinatorial characterization of the plane graphs that admit a planar\northo-radial drawing without bends. Previously, such a characterization was\nonly known for paths, cycles, and theta graphs, and in the special case of\nrectangular drawings for cubic graphs, where the contour of each face is\nrequired to be a rectangle.\n  The characterization is expressed in terms of an ortho-radial representation\nthat, similar to Tamassia's orthogonal representations for orthogonal drawings\ndescribes such a drawing combinatorially in terms of angles around vertices and\nbends on the edges. In this sense our characterization can be seen as a first\nstep towards generalizing the Topology-Shape-Metrics framework of Tamassia to\northo-radial drawings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 14:53:55 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Barth", "Lukas", ""], ["Niedermann", "Benjamin", ""], ["Rutter", "Ignaz", ""], ["Wolf", "Matthias", ""]]}, {"id": "1703.06048", "submitter": "Michael Holzhauser", "authors": "Michael Holzhauser, Sven O. Krumke", "title": "An FPTAS for the Knapsack Problem with Parametric Weights", "comments": "arXiv admin note: text overlap with arXiv:1701.07822", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the parametric weight knapsack problem, in\nwhich the item weights are affine functions of the form $w_i(\\lambda) = a_i +\n\\lambda \\cdot b_i$ for $i \\in \\{1,\\ldots,n\\}$ depending on a real-valued\nparameter $\\lambda$. The aim is to provide a solution for all values of the\nparameter. It is well-known that any exact algorithm for the problem may need\nto output an exponential number of knapsack solutions. We present the first\nfully polynomial-time approximation scheme (FPTAS) for the problem that, for\nany desired precision $\\varepsilon \\in (0,1)$, computes\n$(1-\\varepsilon)$-approximate solutions for all values of the parameter. Our\nFPTAS is based on two different approaches and achieves a running time of\n$\\mathcal{O}(n^3/\\varepsilon^2 \\cdot \\min\\{ \\log^2 P, n^2 \\} \\cdot \\min\\{\\log\nM, n \\log (n/\\varepsilon) / \\log(n \\log (n/\\varepsilon) )\\})$ where $P$ is an\nupper bound on the optimal profit and $M := \\max\\{W, n \\cdot \\max\\{a_i,b_i: i\n\\in \\{1,\\ldots,n\\}\\}\\}$ for a knapsack with capacity $W$.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 15:08:17 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Holzhauser", "Michael", ""], ["Krumke", "Sven O.", ""]]}, {"id": "1703.06053", "submitter": "Pau Segui-Gasco", "authors": "Pau Segui-Gasco and Hyo-Sang Shin", "title": "Fast Non-Monotone Submodular Maximisation Subject to a Matroid\n  Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present the first practical\n$\\left(\\frac{1}{e}-\\epsilon\\right)$-approximation algorithm to maximise a\ngeneral non-negative submodular function subject to a matroid constraint. Our\nalgorithm is based on combining the decreasing-threshold procedure of\nBadanidiyuru and Vondrak (SODA 2014) with a smoother version of the measured\ncontinuous greedy algorithm of Feldman et al. (FOCS 2011). This enables us to\nobtain an algorithm that requires $O(\\frac{nr^2}{\\epsilon^4}\n\\big(\\frac{a+b}{a}\\big)^2 \\log^2({\\frac{n}{\\epsilon}}))$ value oracle calls,\nwhere $n$ is the cardinality of the ground set, $r$ is the matroid rank, and $\nb, a \\in \\mathbb{R}^+$ are the absolute values of the minimum and maximum\nmarginal values that the function $f$ can take i.e.: $ -b \\leq f_S(i) \\leq a$,\nfor all $i\\in E$ and $S\\subseteq E$, (here, $E$ is the ground set). The\nadditional value oracle calls with respect to the work of Badanidiyuru and\nVondrak come from the greater spread in the sampling of the multilinear\nextension that the possibility of negative marginal values introduce.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 15:38:37 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 12:29:29 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Segui-Gasco", "Pau", ""], ["Shin", "Hyo-Sang", ""]]}, {"id": "1703.06061", "submitter": "Markus Lohrey", "authors": "Danny Hucke, Artur Jez, and Markus Lohrey", "title": "Approximation ratio of RePair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper of Charikar et al.~on the smallest grammar problem, the\nauthors derive upper and lower bounds on the approximation ratios for several\ngrammar-based compressors. Here we improve the lower bound for the famous {\\sf\nRePair} algorithm from $\\Omega(\\sqrt{\\log n})$ to $\\Omega(\\log n/\\log\\log n)$.\nThe family of words used in our proof is defined over a binary alphabet, while\nthe lower bound from Charikar et al. needs an alphabet of logarithmic size in\nthe length of the provided words.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 15:56:50 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Hucke", "Danny", ""], ["Jez", "Artur", ""], ["Lohrey", "Markus", ""]]}, {"id": "1703.06065", "submitter": "Urvashi Oswal", "authors": "Urvashi Oswal, Swayambhoo Jain, Kevin S. Xu, and Brian Eriksson", "title": "Block CUR: Decomposing Matrices using Groups of Columns", "comments": "shorter version to appear in ECML-PKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in large-scale data analysis is to approximate a matrix\nusing a combination of specifically sampled rows and columns, known as CUR\ndecomposition. Unfortunately, in many real-world environments, the ability to\nsample specific individual rows or columns of the matrix is limited by either\nsystem constraints or cost. In this paper, we consider matrix approximation by\nsampling predefined \\emph{blocks} of columns (or rows) from the matrix. We\npresent an algorithm for sampling useful column blocks and provide novel\nguarantees for the quality of the approximation. This algorithm has application\nin problems as diverse as biometric data analysis to distributed computing. We\ndemonstrate the effectiveness of the proposed algorithms for computing the\nBlock CUR decomposition of large matrices in a distributed setting with\nmultiple nodes in a compute cluster, where such blocks correspond to columns\n(or rows) of the matrix stored on the same node, which can be retrieved with\nmuch less overhead than retrieving individual columns stored across different\nnodes. In the biometric setting, the rows correspond to different users and\ncolumns correspond to users' biometric reaction to external stimuli, {\\em\ne.g.,}~watching video content, at a particular time instant. There is\nsignificant cost in acquiring each user's reaction to lengthy content so we\nsample a few important scenes to approximate the biometric response. An\nindividual time sample in this use case cannot be queried in isolation due to\nthe lack of context that caused that biometric reaction. Instead, collections\nof time segments ({\\em i.e.,} blocks) must be presented to the user. The\npractical application of these algorithms is shown via experimental results\nusing real-world user biometric data from a content testing environment.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 16:08:23 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 14:27:52 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Oswal", "Urvashi", ""], ["Jain", "Swayambhoo", ""], ["Xu", "Kevin S.", ""], ["Eriksson", "Brian", ""]]}, {"id": "1703.06074", "submitter": "David Adjiashvili", "authors": "David Adjiashvili and Viktor Bindewald and Dennis Michaels", "title": "Robust Assignments with Vulnerable Nodes", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various real-life planning problems require making upfront decisions before\nall parameters of the problem have been disclosed. An important special case of\nsuch problem especially arises in scheduling and staff rostering problems,\nwhere a set of tasks needs to be assigned to an available set of resources\n(personnel or machines), in a way that each task is assigned to one resource,\nwhile no task is allowed to share a resource with another task. In its nominal\nform, the resulting computational problem reduces to the well-known assignment\nproblem that can be modeled as matching problems on bipartite graphs.\n  In recent work \\cite{adjiashvili_bindewald_michaels_icalp2016}, a new robust\nmodel for the assignment problem was introduced that can deal with situations\nin which certain resources, i.e.\\ nodes or edges of the underlying bipartite\ngraph, are vulnerable and may become unavailable after a solution has been\nchosen. In the original version from\n\\cite{adjiashvili_bindewald_michaels_icalp2016} the resources subject to\nuncertainty are the edges of the underlying bipartite graph.\n  In this follow-up work, we complement our previous study by considering nodes\nas being vulnerable, instead of edges. The goal is now to choose a minimum-cost\ncollection of nodes such that, if any vulnerable node becomes unavailable, the\nremaining part of the solution still contains sufficient nodes to perform all\ntasks. From a practical point of view, such type of unavailability is\ninteresting as it is typically caused e.g.\\ by an employee's sickness, or\nmachine failure. We present algorithms and hardness of approximation results\nfor several variants of the problem.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 16:18:31 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Adjiashvili", "David", ""], ["Bindewald", "Viktor", ""], ["Michaels", "Dennis", ""]]}, {"id": "1703.06113", "submitter": "Pedro Recuero", "authors": "Pedro Recuero", "title": "Toward an enumeration of unlabeled trees", "comments": "10 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that, on input $n$, lists every unlabeled tree of\norder $n$.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 02:10:17 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Recuero", "Pedro", ""]]}, {"id": "1703.06227", "submitter": "Mostafa Haghir Chehreghani", "authors": "Mostafa Haghir Chehreghani, Albert Bifet and Talel Abdessalem", "title": "Discriminative Distance-Based Network Indices with Application to Link\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large networks, using the length of shortest paths as the distance measure\nhas shortcomings. A well-studied shortcoming is that extending it to\ndisconnected graphs and directed graphs is controversial. The second\nshortcoming is that a huge number of vertices may have exactly the same score.\nThe third shortcoming is that in many applications, the distance between two\nvertices not only depends on the length of shortest paths, but also on the\nnumber of shortest paths. In this paper, first we develop a new distance\nmeasure between vertices of a graph that yields discriminative distance-based\ncentrality indices. This measure is proportional to the length of shortest\npaths and inversely proportional to the number of shortest paths. We present\nalgorithms for exact computation of the proposed discriminative indices.\nSecond, we develop randomized algorithms that precisely estimate average\ndiscriminative path length and average discriminative eccentricity and show\nthat they give $(\\epsilon,\\delta)$-approximations of these indices. Third, we\nperform extensive experiments over several real-world networks from different\ndomains. In our experiments, we first show that compared to the traditional\nindices, discriminative indices have usually much more discriminability. Then,\nwe show that our randomized algorithms can very precisely estimate average\ndiscriminative path length and average discriminative eccentricity, using only\nfew samples. Then, we show that real-world networks have usually a tiny average\ndiscriminative path length, bounded by a constant (e.g., 2). Fourth, in order\nto better motivate the usefulness of our proposed distance measure, we present\na novel link prediction method, that uses discriminative distance to decide\nwhich vertices are more likely to form a link in future, and show its superior\nperformance compared to the well-known existing measures.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 00:44:38 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 16:28:18 GMT"}, {"version": "v3", "created": "Sat, 31 Mar 2018 11:19:47 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Chehreghani", "Mostafa Haghir", ""], ["Bifet", "Albert", ""], ["Abdessalem", "Talel", ""]]}, {"id": "1703.06320", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow, Galina Cariowa and Marina Chicheva", "title": "Hardware-Efficient Schemes of Quaternion Multiplying Units for 2D\n  Discrete Quaternion Fourier Transform Processors", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we offer and discuss three efficient structural solutions for\nthe hardware-oriented implementation of discrete quaternion Fourier transform\nbasic operations with reduced implementation complexities. The first solution:\na scheme for calculating sq product, the second solution: a scheme for\ncalculating qt product, and the third solution: a scheme for calculating sqt\nproduct, where s is a so-called i-quaternion, t is an j-quaternion, and q is an\nusual quaternion. The direct multiplication of two usual quaternions requires\n16 real multiplications (or two-operand multipliers in the case of fully\nparallel hardware implementation) and 12 real additions (or binary adders). At\nthe same time, our solutions allow to design the computation units, which\nconsume only 6 multipliers plus 6 two input adders for implementation of sq or\nqt basic operations and 9 binary multipliers plus 6 two-input adders and 4\nfour-input adders for implementation of sqt basic operation.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 17:21:14 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Cariow", "Aleksandr", ""], ["Cariowa", "Galina", ""], ["Chicheva", "Marina", ""]]}, {"id": "1703.06327", "submitter": "Sewoong Oh", "authors": "Ashish Khetan, Sewoong Oh", "title": "Spectrum Estimation from a Few Entries", "comments": "52 pages; 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular values of a data in a matrix form provide insights on the structure\nof the data, the effective dimensionality, and the choice of hyper-parameters\non higher-level data analysis tools. However, in many practical applications\nsuch as collaborative filtering and network analysis, we only get a partial\nobservation. Under such scenarios, we consider the fundamental problem of\nrecovering spectral properties of the underlying matrix from a sampling of its\nentries. We are particularly interested in directly recovering the spectrum,\nwhich is the set of singular values, and also in sample-efficient approaches\nfor recovering a spectral sum function, which is an aggregate sum of the same\nfunction applied to each of the singular values. We propose first estimating\nthe Schatten $k$-norms of a matrix, and then applying Chebyshev approximation\nto the spectral sum function or applying moment matching in Wasserstein\ndistance to recover the singular values. The main technical challenge is in\naccurately estimating the Schatten norms from a sampling of a matrix. We\nintroduce a novel unbiased estimator based on counting small structures in a\ngraph and provide guarantees that match its empirical performance. Our\ntheoretical analysis shows that Schatten norms can be recovered accurately from\nstrictly smaller number of samples compared to what is needed to recover the\nunderlying low-rank matrix. Numerical experiments suggest that we significantly\nimprove upon a competing approach of using matrix completion methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 18:12:17 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Khetan", "Ashish", ""], ["Oh", "Sewoong", ""]]}, {"id": "1703.06644", "submitter": "Jhoirene Clemente", "authors": "Jhoirene B. Clemente and Henry N. Adorna", "title": "Reoptimization of the Closest Substring Problem under Pattern Length\n  Modification", "comments": "In Proceedings of the 17th Philippine Computing Society Congress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study investigates whether reoptimization can help in solving the\nclosest substring problem. We are dealing with the following reoptimization\nscenario. Suppose, we have an optimal l-length closest substring of a given set\nof sequences S. How can this information be beneficial in obtaining an\n(l+k)-length closest substring for S? In this study, we show that the problem\nis still computationally hard even with k=1. We present greedy approximation\nalgorithms that make use of the given information and prove that it has an\nadditive error that grows as the parameter k increases. Furthermore, we present\nhard instances for each algorithm to show that the computed approximation ratio\nis tight. We also show that we can slightly improve the running-time of the\nexisting polynomial-time approximation scheme (PTAS) for the original problem\nthrough reoptimization.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 09:34:51 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Clemente", "Jhoirene B.", ""], ["Adorna", "Henry N.", ""]]}, {"id": "1703.06680", "submitter": "Gabriele D'Angelo", "authors": "Moreno Marzolla, Gabriele D'Angelo", "title": "Parallel Sort-Based Matching for Data Distribution Management on\n  Shared-Memory Multiprocessors", "comments": "Proceedings of the 21-th ACM/IEEE International Symposium on\n  Distributed Simulation and Real Time Applications (DS-RT 2017). Best Paper\n  Award @DS-RT 2017", "journal-ref": null, "doi": "10.1109/DISTRA.2017.8167660", "report-no": null, "categories": "cs.DC cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of identifying intersections between\ntwo sets of d-dimensional axis-parallel rectangles. This is a common problem\nthat arises in many agent-based simulation studies, and is of central\nimportance in the context of High Level Architecture (HLA), where it is at the\ncore of the Data Distribution Management (DDM) service. Several realizations of\nthe DDM service have been proposed; however, many of them are either\ninefficient or inherently sequential. These are serious limitations since\nmulticore processors are now ubiquitous, and DDM algorithms -- being\nCPU-intensive -- could benefit from additional computing power. We propose a\nparallel version of the Sort-Based Matching algorithm for shared-memory\nmultiprocessors. Sort-Based Matching is one of the most efficient serial\nalgorithms for the DDM problem, but is quite difficult to parallelize due to\ndata dependencies. We describe the algorithm and compute its asymptotic running\ntime; we complete the analysis by assessing its performance and scalability\nthrough extensive experiments on two commodity multicore systems based on a\ndual socket Intel Xeon processor, and a single socket Intel Core i7 processor.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 11:17:39 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 06:02:24 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 08:42:20 GMT"}, {"version": "v4", "created": "Tue, 7 Aug 2018 07:20:05 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Marzolla", "Moreno", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "1703.06733", "submitter": "Sebastiaan J. van Zelst", "authors": "S.J. van Zelst, B.F. van Dongen, W.M.P. van der Aalst, H.M.W. Verbeek", "title": "Discovering Relaxed Sound Workflow Nets using Integer Linear Programming", "comments": "technical report related to manuscript submitted to computing journal", "journal-ref": null, "doi": "10.1007/s00607-017-0582-5", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is concerned with the analysis, understanding and improvement\nof business processes. Process discovery, i.e. discovering a process model\nbased on an event log, is considered the most challenging process mining task.\nState-of-the-art process discovery algorithms only discover local control-flow\npatterns and are unable to discover complex, non-local patterns. Region theory\nbased techniques, i.e. an established class of process discovery techniques, do\nallow for discovering such patterns. However, applying region theory directly\nresults in complex, over-fitting models, which is less desirable. Moreover,\nregion theory does not cope with guarantees provided by state-of-the-art\nprocess discovery algorithms, both w.r.t. structural and behavioural properties\nof the discovered process models. In this paper we present an ILP-based process\ndiscovery approach, based on region theory, that guarantees to discover relaxed\nsound workflow nets. Moreover, we devise a filtering algorithm, based on the\ninternal working of the ILP-formulation, that is able to cope with the presence\nof infrequent behaviour. We have extensively evaluated the technique using\ndifferent event logs with different levels of exceptional behaviour. Our\nexperiments show that the presented approach allow us to leverage the inherent\nshortcomings of existing region-based approaches. The techniques presented are\nimplemented and readily available in the HybridILPMiner package in the\nopen-source process mining tool-kits ProM and RapidProM.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:31:01 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["van Zelst", "S. J.", ""], ["van Dongen", "B. F.", ""], ["van der Aalst", "W. M. P.", ""], ["Verbeek", "H. M. W.", ""]]}, {"id": "1703.07107", "submitter": "Marco Fiorucci", "authors": "Marco Fiorucci, Alessandro Torcinovich, Manuel Curado, Francisco\n  Escolano and Marcello Pelillo", "title": "On the Interplay between Strong Regularity and Graph Densification", "comments": "GbR2017 to appear in Lecture Notes in Computer Science (LNCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the practical implications of Szemer\\'edi's\nregularity lemma in the preservation of metric information contained in large\ngraphs. To this end, we present a heuristic algorithm to find regular\npartitions. Our experiments show that this method is quite robust to the\nnatural sparsification of proximity graphs. In addition, this robustness can be\nenforced by graph densification.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 09:37:16 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Fiorucci", "Marco", ""], ["Torcinovich", "Alessandro", ""], ["Curado", "Manuel", ""], ["Escolano", "Francisco", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1703.07244", "submitter": "Sergey Polyakovskiy", "authors": "S. Polyakovskiy and R. M'Hallah", "title": "A Hybrid Feasibility Constraints-Guided Search to the Two-Dimensional\n  Bin Packing Problem with Due Dates", "comments": null, "journal-ref": null, "doi": "10.1016/j.ejor.2017.10.046", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-dimensional non-oriented bin packing problem with due dates packs a\nset of rectangular items, which may be rotated by 90 degrees, into identical\nrectangular bins. The bins have equal processing times. An item's lateness is\nthe difference between its due date and the completion time of its bin. The\nproblem packs all items without overlap as to minimize maximum lateness Lmax.\n  The paper proposes a tight lower bound that enhances an existing bound on\nLmax for 24.07% of the benchmark instances and matches it in 30.87% cases. In\naddition, it models the problem using mixed integer programming (MIP), and\nsolves small-sized instances exactly using CPLEX. It approximately solves\nlarger-sized instances using a two-stage heuristic. The first stage constructs\nan initial solution via a first-fit heuristic that applies an iterative\nconstraint programming (CP)-based neighborhood search. The second stage, which\nis iterative too, approximately solves a series of assignment low-level MIPs\nthat are guided by feasibility constraints. It then enhances the solution via a\nhigh-level random local search. The approximate approach improves existing\nupper bounds by 27.45% on average, and obtains the optimum for 33.93% of the\ninstances. Overall, the exact and approximate approaches identify the optimum\nfor 39.07% cases.\n  The proposed approach is applicable to complex problems. It applies CP and\nMIP sequentially, while exploring their advantages, and hybridizes heuristic\nsearch with MIP. It embeds a new lookahead strategy that guards against\ninfeasible search directions and constrains the search to improving directions\nonly; thus, differs from traditional lookahead beam searches.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 14:39:35 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 02:12:09 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Polyakovskiy", "S.", ""], ["M'Hallah", "R.", ""]]}, {"id": "1703.07247", "submitter": "Zeev Nutov", "authors": "Zeev Nutov", "title": "On the Tree Augmentation Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Tree Augmentation problem we are given a tree $T=(V,F)$ and a set $E\n\\subseteq V \\times V$ of edges with positive integer costs $\\{c_e:e \\in E\\}$.\nThe goal is to augment $T$ by a minimum cost edge set $J \\subseteq E$ such that\n$T \\cup J$ is $2$-edge-connected. We obtain the following results.\n  Recently, Adjiashvili [SODA 17] introduced a novel LP for the problem and\nused it to break the $2$-approximation barrier for instances when the maximum\ncost $M$ of an edge in $E$ is bounded by a constant; his algorithm computes a\n$1.96418+\\epsilon$ approximate solution in time $n^{{(M/\\epsilon^2)}^{O(1)}}$.\nUsing a simpler LP, we achieve ratio $\\frac{12}{7}+\\epsilon$ in time\n$2^{O(M/\\epsilon^2)} poly(n)$.This gives ratio better than $2$ for logarithmic\ncosts, and not only for constant costs.\n  One of the oldest open questions for the problem is whether for unit costs\n(when $M=1$) the standard LP-relaxation, so called Cut-LP, has integrality gap\nless than $2$. We resolve this open question by proving that for unit costs the\nintegrality gap of the Cut-LP is at most $28/15=2-2/15$. In addition, we will\nprove that another natural LP-relaxation, that is much simpler than the ones in\nprevious work, has integrality gap at most $7/4$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 14:46:39 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 06:17:14 GMT"}, {"version": "v3", "created": "Sat, 22 Dec 2018 16:31:10 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Nutov", "Zeev", ""]]}, {"id": "1703.07290", "submitter": "Sergey Polyakovskiy", "authors": "S. Polyakovskiy and A. Makarowsky and R. M'Hallah", "title": "Just-in-Time Batch Scheduling Problem with Two-dimensional Bin Packing\n  Constraints", "comments": null, "journal-ref": "Proceedings of the Genetic and Evolutionary Computation\n  Conference, 2017, pages 321-328", "doi": "10.1145/3071178.3071223", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and approximately solves a multi-component problem\nwhere small rectangular items are produced from large rectangular bins via\nguillotine cuts. An item is characterized by its width, height, due date, and\nearliness and tardiness penalties per unit time. Each item induces a cost that\nis proportional to its earliness and tardiness. Items cut from the same bin\nform a batch, whose processing and completion times depend on its assigned\nitems. The items of a batch have the completion time of their bin. The\nobjective is to find a cutting plan that minimizes the weighted sum of\nearliness and tardiness penalties. We address this problem via a constraint\nprogramming based heuristic (CP) and an agent based modelling heuristic (AB).\nCP is an impact-based search strategy, implemented in the general-purpose\nsolver IBM CP Optimizer. AB is constructive. It builds a solution through\nrepeated negotiations between the set of agents representing the items and the\nset representing the bins. The agents cooperate to minimize the weighted\nearliness-tardiness penalties. The computational investigation shows that CP\noutperforms AB on small-sized instances while the opposite prevails for larger\ninstances.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:57:42 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 11:43:36 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Polyakovskiy", "S.", ""], ["Makarowsky", "A.", ""], ["M'Hallah", "R.", ""]]}, {"id": "1703.07340", "submitter": "Balint Tillman", "authors": "B\\'alint Tillman, Athina Markopoulou, Carter T. Butts, Minas Gjoka", "title": "Construction of Directed 2K Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of constructing synthetic graphs that resemble\nreal-world directed graphs in terms of their degree correlations. We define the\nproblem of directed 2K construction (D2K) that takes as input the directed\ndegree sequence (DDS) and a joint degree and attribute matrix (JDAM) so as to\ncapture degree correlation specifically in directed graphs. We provide\nnecessary and sufficient conditions to decide whether a target D2K is\nrealizable, and we design an efficient algorithm that creates realizations with\nthat target D2K. We evaluate our algorithm in creating synthetic graphs that\ntarget real-world directed graphs (such as Twitter) and we show that it brings\nsignificant benefits compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 17:53:50 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Tillman", "B\u00e1lint", ""], ["Markopoulou", "Athina", ""], ["Butts", "Carter T.", ""], ["Gjoka", "Minas", ""]]}, {"id": "1703.07417", "submitter": "Yasamin Nazari", "authors": "Michael Dinitz, Yasamin Nazari", "title": "Distributed Distance-Bounded Network Design Through Distributed Convex\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving linear programs is often a challenging task in distributed settings.\nWhile there are good algorithms for solving packing and covering linear\nprograms in a distributed manner (Kuhn et al.~2006), this is essentially the\nonly class of linear programs for which such an algorithm is known. In this\nwork we provide a distributed algorithm for solving a different class of convex\nprograms which we call \"distance-bounded network design convex programs\". These\ncan be thought of as relaxations of network design problems in which the\nconnectivity requirement includes a distance constraint (most notably, graph\nspanners). Our algorithm runs in $O( (D/\\epsilon) \\log n)$ rounds in the\n$\\mathcal{LOCAL}$ model and finds a $(1+\\epsilon)$-approximation to the optimal\nLP solution for any $0 < \\epsilon \\leq 1$, where $D$ is the largest distance\nconstraint. While solving linear programs in a distributed setting is\ninteresting in its own right, this class of convex programs is particularly\nimportant because solving them is often a crucial step when designing\napproximation algorithms. Hence we almost immediately obtain new and improved\ndistributed approximation algorithms for a variety of network design problems,\nincluding Basic $3$- and $4$-Spanner, Directed $k$-Spanner, Lowest Degree\n$k$-Spanner, and Shallow-Light Steiner Network Design with a spanning demand\ngraph. Our algorithms do not require any \"heavy\" computation and essentially\nmatch the best-known centralized approximation algorithms, while previous\napproaches which do not use heavy computation give approximations which are\nworse than the best-known centralized bounds.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 20:28:52 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 18:05:45 GMT"}, {"version": "v3", "created": "Fri, 8 Sep 2017 19:37:51 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Dinitz", "Michael", ""], ["Nazari", "Yasamin", ""]]}, {"id": "1703.07432", "submitter": "Nika Haghtalab", "authors": "Pranjal Awasthi, Avrim Blum, Nika Haghtalab, Yishay Mansour", "title": "Efficient PAC Learning from the Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years crowdsourcing has become the method of choice for gathering\nlabeled training data for learning algorithms. Standard approaches to\ncrowdsourcing view the process of acquiring labeled data separately from the\nprocess of learning a classifier from the gathered data. This can give rise to\ncomputational and statistical challenges. For example, in most cases there are\nno known computationally efficient learning algorithms that are robust to the\nhigh level of noise that exists in crowdsourced data, and efforts to eliminate\nnoise through voting often require a large number of queries per example.\n  In this paper, we show how by interleaving the process of labeling and\nlearning, we can attain computational efficiency with much less overhead in the\nlabeling cost. In particular, we consider the realizable setting where there\nexists a true target function in $\\mathcal{F}$ and consider a pool of labelers.\nWhen a noticeable fraction of the labelers are perfect, and the rest behave\narbitrarily, we show that any $\\mathcal{F}$ that can be efficiently learned in\nthe traditional realizable PAC model can be learned in a computationally\nefficient manner by querying the crowd, despite high amounts of noise in the\nresponses. Moreover, we show that this can be done while each labeler only\nlabels a constant number of examples and the number of labels requested per\nexample, on average, is a constant. When no perfect labelers exist, a related\ntask is to find a set of the labelers which are good but not perfect. We show\nthat we can identify all good labelers, when at least the majority of labelers\nare good.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 21:05:27 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 21:21:41 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Blum", "Avrim", ""], ["Haghtalab", "Nika", ""], ["Mansour", "Yishay", ""]]}, {"id": "1703.07625", "submitter": "Joris Gu\\'erin", "authors": "Joris Gu\\'erin, Olivier Gibaru, St\\'ephane Thiery and Eric Nyiri", "title": "Clustering for Different Scales of Measurement - the Gap-Ratio Weighted\n  K-means Algorithm", "comments": "13 pages, 6 figures, 2 tables. This paper is under the review process\n  for AIAP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for clustering data that are spread out over\nlarge regions and which dimensions are on different scales of measurement. Such\nan algorithm was developed to implement a robotics application consisting in\nsorting and storing objects in an unsupervised way. The toy dataset used to\nvalidate such application consists of Lego bricks of different shapes and\ncolors. The uncontrolled lighting conditions together with the use of RGB color\nfeatures, respectively involve data with a large spread and different levels of\nmeasurement between data dimensions. To overcome the combination of these two\ncharacteristics in the data, we have developed a new weighted K-means\nalgorithm, called gap-ratio K-means, which consists in weighting each dimension\nof the feature space before running the K-means algorithm. The weight\nassociated with a feature is proportional to the ratio of the biggest gap\nbetween two consecutive data points, and the average of all the other gaps.\nThis method is compared with two other variants of K-means on the Lego bricks\nclustering problem as well as two other common classification datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 12:50:15 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Gu\u00e9rin", "Joris", ""], ["Gibaru", "Olivier", ""], ["Thiery", "St\u00e9phane", ""], ["Nyiri", "Eric", ""]]}, {"id": "1703.07734", "submitter": "Shai Vardi", "authors": "Uriel Feige, Boaz Patt-Shamir, Shai Vardi", "title": "On the Probe Complexity of Local Computation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Local Computation Algorithms (LCA) model is a computational model aimed\nat problem instances with huge inputs and output. For graph problems, the input\ngraph is accessed using probes: strong probes (SP) specify a vertex $v$ and\nreceive as a reply a list of $v$'s neighbors, and weak probes (WP) specify a\nvertex $v$ and a port number $i$ and receive as a reply $v$'s $i^{th}$\nneighbor. Given a local query (e.g., \"is a certain vertex in the vertex cover\nof the input graph?\"), an LCA should compute the corresponding local output\n(e.g., \"yes\" or \"no\") while making only a small number of probes, with the\nrequirement that all local outputs form a single global solution (e.g., a legal\nvertex cover). We study the probe complexity of LCAs that are required to work\non graphs that may have arbitrarily large degrees. In particular, such LCAs are\nexpected to probe the graph a number of times that is significantly smaller\nthan the maximum, average, or even minimum degree.\n  For weak probes, we focus on the weak coloring problem. Among our results we\nshow a separation between weak 3-coloring and weak 2-coloring for deterministic\nLCAs: $\\log^* n + O(1)$ weak probes suffice for weak 3-coloring, but\n$\\Omega\\left(\\frac{\\log n}{\\log\\log n}\\right)$ weak probes are required for\nweak 2-coloring.\n  For strong probes, we consider randomized LCAs for vertex cover and\nmaximal/maximum matching. Our negative results include showing that there are\ngraphs for which finding a \\emph{maximal} matching requires $\\Omega(\\sqrt{n})$\nstrong probes. On the positive side, we design a randomized LCA that finds a\n$(1-\\epsilon)$ approximation to \\emph{maximum} matching in regular graphs, and\nuses $\\frac{1}{\\epsilon }^{O\\left( \\frac{1}{\\epsilon ^2}\\right)}$ probes,\nindependently of the number of vertices and of their degrees.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 16:28:17 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Feige", "Uriel", ""], ["Patt-Shamir", "Boaz", ""], ["Vardi", "Shai", ""]]}, {"id": "1703.07867", "submitter": "Martin Aum\\\"uller", "authors": "Martin Aum\\\"uller, Tobias Christiani, Rasmus Pagh, Francesco Silvestri", "title": "Distance-Sensitive hashing", "comments": "Accepted at PODS'18. Abstract shortened due to character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Locality-sensitive hashing (LSH) is an important tool for managing\nhigh-dimensional noisy or uncertain data, for example in connection with data\ncleaning (similarity join) and noise-robust search (similarity search).\nHowever, for a number of problems the LSH framework is not known to yield good\nsolutions, and instead ad hoc solutions have been designed for particular\nsimilarity and distance measures. For example, this is true for\noutput-sensitive similarity search/join, and for indexes supporting annulus\nqueries that aim to report a point close to a certain given distance from the\nquery point.\n  In this paper we initiate the study of distance-sensitive hashing (DSH), a\ngeneralization of LSH that seeks a family of hash functions such that the\nprobability of two points having the same hash value is a given function of the\ndistance between them. More precisely, given a distance space $(X,\n\\text{dist})$ and a \"collision probability function\" (CPF) $f\\colon\n\\mathbb{R}\\rightarrow [0,1]$ we seek a distribution over pairs of functions\n$(h,g)$ such that for every pair of points $x, y \\in X$ the collision\nprobability is $\\Pr[h(x)=g(y)] = f(\\text{dist}(x,y))$. Locality-sensitive\nhashing is the study of how fast a CPF can decrease as the distance grows. For\nmany spaces, $f$ can be made exponentially decreasing even if we restrict\nattention to the symmetric case where $g=h$. We show that the asymmetry\nachieved by having a pair of functions makes it possible to achieve CPFs that\nare, for example, increasing or unimodal, and show how this leads to principled\nsolutions to problems not addressed by the LSH framework. This includes a novel\napplication to privacy-preserving distance estimation. We believe that the DSH\nframework will find further applications in high-dimensional data management.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 21:52:05 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 15:00:08 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Christiani", "Tobias", ""], ["Pagh", "Rasmus", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1703.07964", "submitter": "Hsueh-I Lu", "authors": "Hung-Chun Liang and Hsueh-I Lu", "title": "Minimum Cuts and Shortest Cycles in Directed Planar Graphs via\n  Noncrossing Shortest Paths", "comments": "25 pages, 14 figures", "journal-ref": "SIAM Journal on Discrete Mathematics 31 (1): 454-476, 2017", "doi": "10.1137/16M1057152", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be an $n$-node simple directed planar graph with nonnegative edge\nweights. We study the fundamental problems of computing (1) a global cut of $G$\nwith minimum weight and (2) a~cycle of $G$ with minimum weight. The best\npreviously known algorithm for the former problem, running in $O(n\\log^3 n)$\ntime, can be obtained from the algorithm of \\Lacki, Nussbaum, Sankowski, and\nWulff-Nilsen for single-source all-sinks maximum flows. The best previously\nknown result for the latter problem is the $O(n\\log^3 n)$-time algorithm of\nWulff-Nilsen. By exploiting duality between the two problems in planar graphs,\nwe solve both problems in $O(n\\log n\\log\\log n)$ time via a divide-and-conquer\nalgorithm that finds a shortest non-degenerate cycle. The kernel of our result\nis an $O(n\\log\\log n)$-time algorithm for computing noncrossing shortest paths\namong nodes well ordered on a common face of a directed plane graph, which is\nextended from the algorithm of Italiano, Nussbaum, Sankowski, and Wulff-Nilsen\nfor an undirected plane graph.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 08:37:54 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Liang", "Hung-Chun", ""], ["Lu", "Hsueh-I", ""]]}, {"id": "1703.08041", "submitter": "Palash Dey", "authors": "Palash Dey", "title": "Resolving the Complexity of Some Fundamental Problems in Computational\n  Social Choice", "comments": "Ph.D. Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis is in the area called computational social choice which is an\nintersection area of algorithms and social choice theory.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 12:32:10 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Dey", "Palash", ""]]}, {"id": "1703.08139", "submitter": "Jelani Nelson", "authors": "Jelani Nelson, Jakub Pachocki, Zhengyu Wang", "title": "Optimal lower bounds for universal relation, samplers, and finding\n  duplicates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the communication problem $\\mathbf{UR}$ (universal relation) [KRW95],\nAlice and Bob respectively receive $x$ and $y$ in $\\{0,1\\}^n$ with the promise\nthat $x\\neq y$. The last player to receive a message must output an index $i$\nsuch that $x_i\\neq y_i$. We prove that the randomized one-way communication\ncomplexity of this problem in the public coin model is exactly $\\Theta(\\min\\{n,\n\\log(1/\\delta)\\log^2(\\frac{n}{\\log(1/\\delta)})\\})$ bits for failure probability\n$\\delta$. Our lower bound holds even if promised $\\mathop{support}(y)\\subset\n\\mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for\n$\\ell_p$-sampling in strict turnstile streams for $0\\le p < 2$, as well as for\nthe problem of finding duplicates in a stream. Our lower bounds do not need to\nuse large weights, and hold even if it is promised that $x\\in\\{0,1\\}^n$ at all\npoints in the stream.\n  Our lower bound demonstrates that any algorithm $\\mathcal{A}$ solving\nsampling problems in turnstile streams in low memory can be used to encode\nsubsets of $[n]$ of certain sizes into a number of bits below the information\ntheoretic minimum. Our encoder makes adaptive queries to $\\mathcal{A}$\nthroughout its execution, but done carefully so as to not violate correctness.\nThis is accomplished by injecting random noise into the encoder's interactions\nwith $\\mathcal{A}$, which is loosely motivated by techniques in differential\nprivacy. Our correctness analysis involves understanding the ability of\n$\\mathcal{A}$ to correctly answer adaptive queries which have positive but\nbounded mutual information with $\\mathcal{A}$'s internal randomness, and may be\nof independent interest in the newly emerging area of adaptive data analysis\nwith a theoretical computer science lens.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 16:50:03 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Nelson", "Jelani", ""], ["Pachocki", "Jakub", ""], ["Wang", "Zhengyu", ""]]}, {"id": "1703.08273", "submitter": "Shiyu Ji", "authors": "Shiyu Ji, Kun Wan", "title": "An Asymptotically Tighter Bound on Sampling for Frequent Itemsets Mining", "comments": "13 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new error bound on sampling algorithms for\nfrequent itemsets mining. We show that the new bound is asymptotically tighter\nthan the state-of-art bounds, i.e., given the chosen samples, for small enough\nerror probability, the new error bound is roughly half of the existing bounds.\nBased on the new bound, we give a new approximation algorithm, which is much\nsimpler compared to the existing approximation algorithms, but can also\nguarantee the worst approximation error with precomputed sample size. We also\ngive an algorithm which can approximate the top-$k$ frequent itemsets with high\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 02:59:51 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Ji", "Shiyu", ""], ["Wan", "Kun", ""]]}, {"id": "1703.08433", "submitter": "Ching-Lueh Chang", "authors": "Ching-Lueh Chang", "title": "Metric random matchings with applications", "comments": "arXiv admin note: substantial text overlap with arXiv:1702.03106", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(\\{1,2,\\ldots,n\\},d)$ be a metric space. We analyze the expected value\nand the variance of $\\sum_{i=1}^{\\lfloor\nn/2\\rfloor}\\,d({\\boldsymbol{\\pi}}(2i-1),{\\boldsymbol{\\pi}}(2i))$ for a\nuniformly random permutation ${\\boldsymbol{\\pi}}$ of $\\{1,2,\\ldots,n\\}$,\nleading to the following results: (I) Consider the problem of finding a point\nin $\\{1,2,\\ldots,n\\}$ with the minimum sum of distances to all points. We show\nthat this problem has a randomized algorithm that (1) always outputs a\n$(2+\\epsilon)$-approximate solution in expected $O(n/\\epsilon^2)$ time and that\n(2) inherits Indyk's~\\cite{Ind99, Ind00} algorithm to output a\n$(1+\\epsilon)$-approximate solution in $O(n/\\epsilon^2)$ time with probability\n$\\Omega(1)$, where $\\epsilon\\in(0,1)$. (II) The average distance in\n$(\\{1,2,\\ldots,n\\},d)$ can be approximated in $O(n/\\epsilon)$ time to within a\nmultiplicative factor in $[\\,1/2-\\epsilon,1\\,]$ with probability\n$1/2+\\Omega(1)$, where $\\epsilon>0$. (III) Assume $d$ to be a graph metric.\nThen the average distance in $(\\{1,2,\\ldots,n\\},d)$ can be approximated in\n$O(n)$ time to within a multiplicative factor in $[\\,1-\\epsilon,1+\\epsilon\\,]$\nwith probability $1/2+\\Omega(1)$, where $\\epsilon=\\omega(1/n^{1/4})$.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 14:44:04 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Chang", "Ching-Lueh", ""]]}, {"id": "1703.08511", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "Compression with wildcards: All k-models of a BDD", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a Binary Decision Diagram B of a Boolean function \\phi in variables,\nall N many k-ones models of \\phi can be enumerated in time polynomial in n and\nN and |B|. Using novel wildcards enables a compressed enumeration of these\nmodels. Simple examples show that the compression can be arbitrarily high.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 17:10:23 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 13:34:50 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 14:51:46 GMT"}, {"version": "v4", "created": "Thu, 27 Feb 2020 15:21:43 GMT"}, {"version": "v5", "created": "Sun, 31 May 2020 17:20:53 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wild", "Marcel", ""]]}, {"id": "1703.08589", "submitter": "Shankarachary Ragi Mr", "authors": "Shankarachary Ragi, Edwin K. P. Chong, and Hans D. Mittelmann", "title": "Polynomial-Time Methods to Solve Unimodular Quadratic Programs With\n  Performance Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop polynomial-time heuristic methods to solve unimodular quadratic\nprograms (UQPs) approximately, which are known to be NP-hard. In the UQP\nframework, we maximize a quadratic function of a vector of complex variables\nwith unit modulus. Several problems in active sensing and wireless\ncommunication applications boil down to UQP. With this motivation, we present\nthree new heuristic methods with polynomial-time complexity to solve the UQP\napproximately. The first method is called dominant-eigenvector-matching; here\nthe solution is picked that matches the complex arguments of the dominant\neigenvector of the Hermitian matrix in the UQP formulation. We also provide a\nperformance guarantee for this method. The second method, a greedy strategy, is\nshown to provide a performance guarantee of (1-1/e) with respect to the optimal\nobjective value given that the objective function possesses a property called\nstring submodularity. The third heuristic method is called row-swap greedy\nstrategy, which is an extension to the greedy strategy and utilizes certain\nproperties of the UQP to provide a better performance than the greedy strategy\nat the expense of an increase in computational complexity. We present numerical\nresults to demonstrate the performance of these heuristic methods, and also\ncompare the performance of these methods against a standard heuristic method\ncalled semidefinite relaxation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 20:21:52 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Ragi", "Shankarachary", ""], ["Chong", "Edwin K. P.", ""], ["Mittelmann", "Hans D.", ""]]}, {"id": "1703.08658", "submitter": "David B. A. Epstein", "authors": "David B.A. Epstein, Mike Paterson", "title": "Maximizing the area of intersection of rectangles", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attacks the following problem. We are given a large number $N$ of\nrectangles in the plane, each with horizontal and vertical sides, and also a\nnumber $r<N$. The given list of $N$ rectangles may contain duplicates. The\nproblem is to find $r$ of these rectangles, such that, if they are discarded,\nthen the intersection of the remaining $(N-r)$ rectangles has an intersection\nwith as large an area as possible. We will find an upper bound, depending only\non $N$ and $r$, and not on the particular data presented, for the number of\nsteps needed to run the algorithm on (a mathematical model of) a computer. In\nfact our algorithm is able to determine, for each $s\\le r$, $s$ rectangles from\nthe given list of $N$ rectangles, such that the remaining $(N-s)$ rectangles\nhave as large an area as possible, and this takes hardly any more time than\ntaking care only of the case $s=r$. Our algorithm extends to $d$-dimensional\nrectangles. Our method is to exhaustively examine all possible\nintersections---this is much faster than it sounds, because we do not need to\nexamine all $\\binom Ns$ subsets in order to find all possible intersection\nrectangles. For an extreme example, suppose the rectangles are nested, for\nexample concentric squares of distinct sizes, then the only intersections\nexamined are the smallest $s+1$ rectangles.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 07:27:32 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Epstein", "David B. A.", ""], ["Paterson", "Mike", ""]]}, {"id": "1703.08702", "submitter": "Leran Cai", "authors": "Leran Cai, Thomas Sauerwald", "title": "Randomized Load Balancing on Networks with Stochastic Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Iterative load balancing algorithms for indivisible tokens have been studied\nintensively in the past. Complementing previous worst-case analyses, we study\nan average-case scenario where the load inputs are drawn from a fixed\nprobability distribution. For cycles, tori, hypercubes and expanders, we obtain\nalmost matching upper and lower bounds on the discrepancy, the difference\nbetween the maximum and the minimum load. Our bounds hold for a variety of\nprobability distributions including the uniform and binomial distribution but\nalso distributions with unbounded range such as the Poisson and geometric\ndistribution. For graphs with slow convergence like cycles and tori, our\nresults demonstrate a substantial difference between the convergence in the\nworst- and average-case. An important ingredient in our analysis is new upper\nbound on the t-step transition probability of a general Markov chain, which is\nderived by invoking the evolving set process.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 15:03:49 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Cai", "Leran", ""], ["Sauerwald", "Thomas", ""]]}, {"id": "1703.08790", "submitter": "Yun Kuen Cheung", "authors": "Yun Kuen Cheung", "title": "Steiner Point Removal --- Distant Terminals Don't (Really) Bother", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a weighted graph $G=(V,E,w)$ with a set of $k$ terminals $T\\subset V$,\nthe Steiner Point Removal problem seeks for a minor of the graph with vertex\nset $T$, such that the distance between every pair of terminals is preserved\nwithin a small multiplicative distortion. Kamma, Krauthgamer and Nguyen (SODA\n2014, SICOMP 2015) used a ball-growing algorithm to show that the distortion is\nat most $\\mathcal{O}(\\log^5 k)$ for general graphs.\n  In this paper, we improve the distortion bound to $\\mathcal{O}(\\log^2 k)$.\nThe improvement is achieved based on a known algorithm that constructs\nterminal-distance exact-preservation minor with $\\mathcal{O}(k^4)$ (which is\nindependent of $|V|$) vertices, and also two tail bounds on the sum of\nindependent exponential random variables, which allow us to show that it is\nunlikely for a non-terminal being contracted to a distant terminal.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 09:03:07 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Cheung", "Yun Kuen", ""]]}, {"id": "1703.08931", "submitter": "Jakub Radoszewski", "authors": "Micha{\\l} Adamczyk, Mai Alzamel, Panagiotis Charalampopoulos, Costas\n  S. Iliopoulos, and Jakub Radoszewski", "title": "Palindromic Decompositions with Gaps and Errors", "comments": "accepted to CSR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying palindromes in sequences has been an interesting line of research\nin combinatorics on words and also in computational biology, after the\ndiscovery of the relation of palindromes in the DNA sequence with the HIV\nvirus. Efficient algorithms for the factorization of sequences into palindromes\nand maximal palindromes have been devised in recent years. We extend these\nstudies by allowing gaps in decompositions and errors in palindromes, and also\nimposing a lower bound to the length of acceptable palindromes.\n  We first present an algorithm for obtaining a palindromic decomposition of a\nstring of length n with the minimal total gap length in time O(n log n * g) and\nspace O(n g), where g is the number of allowed gaps in the decomposition. We\nthen consider a decomposition of the string in maximal \\delta-palindromes (i.e.\npalindromes with \\delta errors under the edit or Hamming distance) and g\nallowed gaps. We present an algorithm to obtain such a decomposition with the\nminimal total gap length in time O(n (g + \\delta)) and space O(n g).\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 05:09:06 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Adamczyk", "Micha\u0142", ""], ["Alzamel", "Mai", ""], ["Charalampopoulos", "Panagiotis", ""], ["Iliopoulos", "Costas S.", ""], ["Radoszewski", "Jakub", ""]]}, {"id": "1703.08940", "submitter": "Oren Weimann", "authors": "Karl Bringmann, Pawe{\\l} Gawrychowski, Shay Mozes, Oren Weimann", "title": "Tree Edit Distance Cannot be Computed in Strongly Subcubic Time (unless\n  APSP can)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance between two rooted ordered trees with $n$ nodes labeled\nfrom an alphabet~$\\Sigma$ is the minimum cost of transforming one tree into the\nother by a sequence of elementary operations consisting of deleting and\nrelabeling existing nodes, as well as inserting new nodes. Tree edit distance\nis a well known generalization of string edit distance. The fastest known\nalgorithm for tree edit distance runs in cubic $O(n^3)$ time and is based on a\nsimilar dynamic programming solution as string edit distance. In this paper we\nshow that a truly subcubic $O(n^{3-\\varepsilon})$ time algorithm for tree edit\ndistance is unlikely: For $|\\Sigma| = \\Omega(n)$, a truly subcubic algorithm\nfor tree edit distance implies a truly subcubic algorithm for the all pairs\nshortest paths problem. For $|\\Sigma| = O(1)$, a truly subcubic algorithm for\ntree edit distance implies an $O(n^{k-\\varepsilon})$ algorithm for finding a\nmaximum weight $k$-clique.\n  Thus, while in terms of upper bounds string edit distance and tree edit\ndistance are highly related, in terms of lower bounds string edit distance\nexhibits the hardness of the strong exponential time hypothesis [Backurs, Indyk\nSTOC'15] whereas tree edit distance exhibits the hardness of all pairs shortest\npaths. Our result provides a matching conditional lower bound for one of the\nlast remaining classic dynamic programming problems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 05:48:36 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Bringmann", "Karl", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Mozes", "Shay", ""], ["Weimann", "Oren", ""]]}, {"id": "1703.08950", "submitter": "Damir Hasic", "authors": "Damir Hasic, Eric Tannier", "title": "Gene tree species tree reconciliation with gene conversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene tree/species tree reconciliation is a recent decisive progress in\nphylo-genetic methods, accounting for the possible differences between gene\nhistories and species histories. Reconciliation consists in explaining these\ndifferences by gene-scale events such as duplication, loss, transfer, which\ntranslates mathematically into a mapping between gene tree nodes and species\ntree nodes or branches. Gene conversion is a very frequent biological event,\nwhich results in the replacement of a gene by a copy of another from the same\nspecies and in the same gene tree. Including this event in reconciliations has\nnever been attempted because this changes as well the solutions as the methods\nto construct reconciliations. Standard algorithms based on dynamic programming\nbecome ineffective. We propose here a novel mathematical framework including\ngene conversion as an evolutionary event in gene tree/species tree\nreconciliation. We describe a randomized algorithm giving in polynomial running\ntime a reconciliation minimizing the number of duplications, losses and\nconversions. We show that the space of reconciliations includes an analog of\nthe Last Common Ancestor reconciliation, but is not limited to it. Our\nalgorithm outputs any optimal reconciliation with non null probability. We\nargue that this study opens a wide research avenue on including gene conversion\nin reconciliation, which can be important for biology.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 06:50:30 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 14:05:13 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Hasic", "Damir", ""], ["Tannier", "Eric", ""]]}, {"id": "1703.09083", "submitter": "Linda Farczadi", "authors": "Linda Farczadi, Nat\\'alia Guri\\v{c}anov\\'a", "title": "The weighted stable matching problem", "comments": "This is an extended version of a paper to appear at the The Fourth\n  International Workshop on Matching Under Preferences (MATCH-UP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stable matching problem in non-bipartite graphs with incomplete\nbut strict preference lists, where the edges have weights and the goal is to\ncompute a stable matching of minimum or maximum weight. This problem is known\nto be NP-hard in general. Our contribution is two fold: a polyhedral\ncharacterization and an approximation algorithm. Previously Chen et al. have\nshown that the stable matching polytope is integral if and only if the subgraph\nobtained after running phase one of Irving's algorithm is bipartite. We improve\nupon this result by showing that there are instances where this subgraph might\nnot be bipartite but one can further eliminate some edges and arrive at a\nbipartite subgraph. Our elimination procedure ensures that the set of stable\nmatchings remains the same, and thus the stable matching polytope of the final\nsubgraph contains the incidence vectors of all stable matchings of our original\ngraph. This allows us to characterize a larger class of instances for which the\nweighted stable matching problem is polynomial-time solvable. We also show that\nour edge elimination procedure is best possible, meaning that if the subgraph\nwe arrive at is not bipartite, then there is no bipartite subgraph that has the\nsame set of stable matchings as the original graph. We complement these results\nwith a $2$-approximation algorithm for the minimum weight stable matching\nproblem for instances where each agent has at most two possible partners in any\nstable matching. This is the first approximation result for any class of\ninstances with general weights.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 13:57:31 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Farczadi", "Linda", ""], ["Guri\u010danov\u00e1", "Nat\u00e1lia", ""]]}, {"id": "1703.09258", "submitter": "Fabio Protti", "authors": "Augusto Bordini and F\\'abio Protti", "title": "New algorithms for the Minimum Coloring Cut Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Coloring Cut Problem is defined as follows: given a connected\ngraph G with colored edges, find an edge cut E' of G (a minimal set of edges\nwhose removal renders the graph disconnected) such that the number of colors\nused by the edges in E' is minimum. In this work, we present two approaches\nbased on Variable Neighborhood Search to solve this problem. Our algorithms are\nable to find all the optimum solutions described in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 18:36:17 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 18:25:42 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Bordini", "Augusto", ""], ["Protti", "F\u00e1bio", ""]]}, {"id": "1703.09307", "submitter": "Ferran Par\\'es", "authors": "Ferran Par\\'es, Dario Garcia-Gasulla, Armand Vilalta, Jonatan Moreno,\n  Eduard Ayguad\\'e, Jes\\'us Labarta, Ulises Cort\\'es, Toyotaro Suzumura", "title": "Fluid Communities: A Competitive, Scalable and Diverse Community\n  Detection Algorithm", "comments": "Accepted at the 6th International Conference on Complex Networks and\n  Their Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a community detection algorithm (Fluid Communities) based on the\nidea of fluids interacting in an environment, expanding and contracting as a\nresult of that interaction. Fluid Communities is based on the propagation\nmethodology, which represents the state-of-the-art in terms of computational\ncost and scalability. While being highly efficient, Fluid Communities is able\nto find communities in synthetic graphs with an accuracy close to the current\nbest alternatives. Additionally, Fluid Communities is the first\npropagation-based algorithm capable of identifying a variable number of\ncommunities in network. To illustrate the relevance of the algorithm, we\nevaluate the diversity of the communities found by Fluid Communities, and find\nthem to be significantly different from the ones found by alternative methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 20:52:29 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 12:22:45 GMT"}, {"version": "v3", "created": "Mon, 9 Oct 2017 12:59:08 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Par\u00e9s", "Ferran", ""], ["Garcia-Gasulla", "Dario", ""], ["Vilalta", "Armand", ""], ["Moreno", "Jonatan", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jes\u00fas", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1703.09324", "submitter": "Vijay Sridhar", "authors": "Anastasios Sidiropoulos and Vijay Sridhar", "title": "Algorithmic interpretations of fractal dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithmic problems on subsets of Euclidean space of low fractal\ndimension. These spaces are the subject of intensive study in various branches\nof mathematics, including geometry, topology, and measure theory. There are\nseveral well-studied notions of fractal dimension for sets and measures in\nEuclidean space. We consider a definition of fractal dimension for finite\nmetric spaces which agrees with standard notions used to empirically estimate\nthe fractal dimension of various sets. We define the fractal dimension of some\nmetric space to be the infimum $\\delta>0$, such that for any $\\epsilon > 0$,\nfor any ball $B$ of radius $r\\geq 2\\epsilon$, and for any $\\epsilon $-net $N$\n(that is, for any maximal $\\epsilon $-packing), we have $|B\\cap\nN|=O((r/\\epsilon)^\\delta)$.\n  Using this definition we obtain faster algorithms for a plethora of classical\nproblems on sets of low fractal dimension in Euclidean space. Our results apply\nto exact and fixed-parameter algorithms, approximation schemes, and spanner\nconstructions. Interestingly, the dependence of the performance of these\nalgorithms on the fractal dimension nearly matches the currently best-known\ndependence on the standard Euclidean dimension. Thus, when the fractal\ndimension is strictly smaller than the ambient dimension, our results yield\nimproved solutions in all of these settings.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 22:10:13 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Sidiropoulos", "Anastasios", ""], ["Sridhar", "Vijay", ""]]}, {"id": "1703.09421", "submitter": "Paul Fischer", "authors": "Martin Ebbesen, Paul Fischer, Carsten Witt", "title": "Edge-matching Problems with Rotations", "comments": null, "journal-ref": "Proceedings of FCT 2011,~\\cite{DBLP:conf/fct/EbbesenFW11},\n  preliminary version", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-matching problems, also called edge matching puzzles, are abstractions\nof placement problems with neighborhood conditions. Pieces with colored edges\nhave to be placed on a board such that adjacent edges have the same color. The\nproblem has gained interest recently with the (now terminated) Eternity~II\npuzzle, and new complexity results. In this paper we consider a number of\nsettings which differ in size of the puzzles and the manipulations allowed on\nthe pieces. We investigate the effect of allowing rotations of the pieces on\nthe complexity of the problem, an aspect that is only marginally treated so\nfar. We show that some problems have polynomial time algorithms while others\nare NP-complete. Especially we show that allowing rotations in one-row puzzles\nmakes the problem NP-hard. The proofs of the hardness result uses a large\nnumber of colors. This is essential because we also show that this problem (and\nanother related one) is fixed-parameter tractable, where the relevant parameter\nis the number of colors.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 07:20:30 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Ebbesen", "Martin", ""], ["Fischer", "Paul", ""], ["Witt", "Carsten", ""]]}, {"id": "1703.09533", "submitter": "Wolfgang Mulzer", "authors": "Bahareh Banyassady, Man-Kwun Chiu, Matias Korman, Wolfgang Mulzer,\n  Andr\\'e van Renssen, Marcel Roeloffzen, Paul Seiferth, Yannik Stein, Birgit\n  Vogtenhuber, Max Willert", "title": "Routing in Polygonal Domains", "comments": "13 pages, 7 figures; a preliminary version appeared at ISAAC 2017", "journal-ref": null, "doi": "10.1016/j.comgeo.2019.101593", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of routing a data packet through the visibility graph\nof a polygonal domain $P$ with $n$ vertices and $h$ holes. We may preprocess\n$P$ to obtain a label and a routing table for each vertex of $P$. Then, we must\nbe able to route a data packet between any two vertices $p$ and $q$ of $P$,\nwhere each step must use only the label of the target node $q$ and the routing\ntable of the current node.\n  For any fixed $\\varepsilon > 0$, we present a routing scheme that always\nachieves a routing path whose length exceeds the shortest path by a factor of\nat most $1 + \\varepsilon$. The labels have $O(\\log n)$ bits, and the routing\ntables are of size $O((\\varepsilon^{-1}+h)\\log n)$. The preprocessing time is\n$O(n^2\\log n)$. It can be improved to $O(n^2)$ for simple polygons.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 12:16:50 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 13:23:38 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Banyassady", "Bahareh", ""], ["Chiu", "Man-Kwun", ""], ["Korman", "Matias", ""], ["Mulzer", "Wolfgang", ""], ["van Renssen", "Andr\u00e9", ""], ["Roeloffzen", "Marcel", ""], ["Seiferth", "Paul", ""], ["Stein", "Yannik", ""], ["Vogtenhuber", "Birgit", ""], ["Willert", "Max", ""]]}, {"id": "1703.09726", "submitter": "Ignasi Sau", "authors": "J\\'ulio Ara\\'ujo, Julien Baste, Ignasi Sau", "title": "Ruling out FPT algorithms for Weighted Coloring on forests", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, a proper $k$-coloring of $G$ is a partition $c =\n(S_i)_{i\\in [1,k]}$ of $V(G)$ into $k$ stable sets $S_1,\\ldots, S_{k}$. Given a\nweight function $w: V(G) \\to \\mathbb{R}^+$, the weight of a color $S_i$ is\ndefined as $w(i) = \\max_{v \\in S_i} w(v)$ and the weight of a coloring $c$ as\n$w(c) = \\sum_{i=1}^{k}w(i)$. Guan and Zhu [Inf. Process. Lett., 1997] defined\nthe weighted chromatic number of a pair $(G,w)$, denoted by $\\sigma(G,w)$, as\nthe minimum weight of a proper coloring of $G$. For a positive integer $r$,\nthey also defined $\\sigma(G,w;r)$ as the minimum of $w(c)$ among all proper\n$r$-colorings $c$ of $G$.\n  The complexity of determining $\\sigma(G,w)$ when $G$ is a tree was open for\nalmost 20 years, until Ara\\'ujo et al. [SIAM J. Discrete Math., 2014] recently\nproved that the problem cannot be solved in time $n^{o(\\log n)}$ on $n$-vertex\ntrees unless the Exponential Time Hypothesis (ETH) fails.\n  The objective of this article is to provide hardness results for computing\n$\\sigma(G,w)$ and $\\sigma(G,w;r)$ when $G$ is a tree or a forest, relying on\ncomplexity assumptions weaker than the ETH. Namely, we study the problem from\nthe viewpoint of parameterized complexity, and we assume the weaker hypothesis\n$FPT \\neq W[1]$. Building on the techniques of Ara\\'ujo et al., we prove that\nwhen $G$ is a forest, computing $\\sigma(G,w)$ is $W[1]$-hard parameterized by\nthe size of a largest connected component of $G$, and that computing\n$\\sigma(G,w;r)$ is $W[2]$-hard parameterized by $r$. Our results rule out the\nexistence of $FPT$ algorithms for computing these invariants on trees or\nforests for many natural choices of the parameter.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 18:01:14 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Ara\u00fajo", "J\u00falio", ""], ["Baste", "Julien", ""], ["Sau", "Ignasi", ""]]}, {"id": "1703.09947", "submitter": "Kai Zheng", "authors": "Jiaqi Zhang, Kai Zheng, Wenlong Mou, Liwei Wang", "title": "Efficient Private ERM for Smooth Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider efficient differentially private empirical risk\nminimization from the viewpoint of optimization algorithms. For strongly convex\nand smooth objectives, we prove that gradient descent with output perturbation\nnot only achieves nearly optimal utility, but also significantly improves the\nrunning time of previous state-of-the-art private optimization algorithms, for\nboth $\\epsilon$-DP and $(\\epsilon, \\delta)$-DP. For non-convex but smooth\nobjectives, we propose an RRPSGD (Random Round Private Stochastic Gradient\nDescent) algorithm, which provably converges to a stationary point with privacy\nguarantee. Besides the expected utility bounds, we also provide guarantees in\nhigh probability form. Experiments demonstrate that our algorithm consistently\noutperforms existing method in both utility and running time.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 09:31:47 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 12:57:48 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Zhang", "Jiaqi", ""], ["Zheng", "Kai", ""], ["Mou", "Wenlong", ""], ["Wang", "Liwei", ""]]}, {"id": "1703.10023", "submitter": "Peter Sanders", "authors": "Kurt Mehlhorn and Stefan N\\\"aher and Peter Sanders", "title": "Engineering DFS-Based Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth-first search (DFS) is the basis for many efficient graph algorithms. We\nintroduce general techniques for the efficient implementation of DFS-based\ngraph algorithms and exemplify them on three algorithms for computing strongly\nconnected components. The techniques lead to speed-ups by a factor of two to\nthree compared to the implementations provided by LEDA and BOOST.\n  We have obtained similar speed-ups for biconnected components algorithms. We\nalso compare the graph data types of LEDA and BOOST.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 13:18:52 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Mehlhorn", "Kurt", ""], ["N\u00e4her", "Stefan", ""], ["Sanders", "Peter", ""]]}, {"id": "1703.10031", "submitter": "Michael Wallner", "authors": "Antoine Genitrini, Bernhard Gittenberger, Manuel Kauers, Michael\n  Wallner", "title": "Asymptotic Enumeration of Compacted Binary Trees", "comments": "43 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compacted tree is a graph created from a binary tree such that repeatedly\noccurring subtrees in the original tree are represented by pointers to existing\nones, and hence every subtree is unique. Such representations form a special\nclass of directed acyclic graphs. We are interested in the asymptotic number of\ncompacted trees of given size, where the size of a compacted tree is given by\nthe number of its internal nodes. Due to its superexponential growth this\nproblem poses many difficulties. Therefore we restrict our investigations to\ncompacted trees of bounded right height, which is the maximal number of edges\ngoing to the right on any path from the root to a leaf.\n  We solve the asymptotic counting problem for this class as well as a closely\nrelated, further simplified class.\n  For this purpose, we develop a calculus on exponential generating functions\nfor compacted trees of bounded right height and for relaxed trees of bounded\nright height, which differ from compacted trees by dropping the above described\nuniqueness condition. This enables us to derive a recursively defined sequence\nof differential equations for the exponential generating functions. The\ncoefficients can then be determined by performing a singularity analysis of the\nsolutions of these differential equations.\n  Our main results are the computation of the asymptotic numbers of relaxed as\nwell as compacted trees of bounded right height and given size, when the size\ntends to infinity.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 13:41:35 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Genitrini", "Antoine", ""], ["Gittenberger", "Bernhard", ""], ["Kauers", "Manuel", ""], ["Wallner", "Michael", ""]]}, {"id": "1703.10049", "submitter": "Areg Karapetyan", "authors": "Rashid Alyassi, Majid Khonji, Sid Chi-Kin Chau, Khaled Elbassioni,\n  Chien-Ming Tseng and Areg Karapetyan", "title": "Autonomous Recharging and Flight Mission Planning for Battery-operated\n  Autonomous Drones", "comments": "Under review in IEEE Systems journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAVs), commonly known as drones, are being\nincreasingly deployed throughout the globe as a means to streamline logistic\nand monitoring routines. When dispatched on autonomous missions, drones require\nan intelligent decision-making system for trajectory planning and tour\noptimization. Given the limited capacity of their on-board batteries, a key\ndesign challenge is to ensure the underlying algorithms can efficiently\noptimize the mission objectives along with recharging operations during\nlong-haul flights. This paper presents a comprehensive study on automated\nmanagement systems for battery-operated drones: (1) We conduct empirical\nstudies to model the battery performance of drones, considering various flight\nscenarios. (2) We study a joint problem of flight mission planning and\nrecharging optimization for drones with an objective to complete a tour mission\nfor a set of sites of interest in the shortest time considering the\npossibilities of recharging. (3) We present algorithms for solving the problem\nof flight mission planning and recharging optimization. (4) We implemented our\nalgorithms in a drone management system, which supports real-time flight path\ntracking and re-computation in dynamic environments. We also evaluated the\nresults of our algorithms in a case study using data from empirical studies,\nwhich shows significant improvement over a typical benchmark algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 14:12:42 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 09:45:53 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 12:49:04 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Alyassi", "Rashid", ""], ["Khonji", "Majid", ""], ["Chau", "Sid Chi-Kin", ""], ["Elbassioni", "Khaled", ""], ["Tseng", "Chien-Ming", ""], ["Karapetyan", "Areg", ""]]}, {"id": "1703.10127", "submitter": "Gautam Kamath", "authors": "Bryan Cai, Constantinos Daskalakis, Gautam Kamath", "title": "Priv'IT: Private and Sample Efficient Identity Testing", "comments": "To appear in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop differentially private hypothesis testing methods for the small\nsample regime. Given a sample $\\cal D$ from a categorical distribution $p$ over\nsome domain $\\Sigma$, an explicitly described distribution $q$ over $\\Sigma$,\nsome privacy parameter $\\varepsilon$, accuracy parameter $\\alpha$, and\nrequirements $\\beta_{\\rm I}$ and $\\beta_{\\rm II}$ for the type I and type II\nerrors of our test, the goal is to distinguish between $p=q$ and\n$d_{\\rm{TV}}(p,q) \\geq \\alpha$.\n  We provide theoretical bounds for the sample size $|{\\cal D}|$ so that our\nmethod both satisfies $(\\varepsilon,0)$-differential privacy, and guarantees\n$\\beta_{\\rm I}$ and $\\beta_{\\rm II}$ type I and type II errors. We show that\ndifferential privacy may come for free in some regimes of parameters, and we\nalways beat the sample complexity resulting from running the $\\chi^2$-test with\nnoisy counts, or standard approaches such as repetition for endowing\nnon-private $\\chi^2$-style statistics with differential privacy guarantees. We\nexperimentally compare the sample complexity of our method to that of recently\nproposed methods for private hypothesis testing.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 16:42:21 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 14:53:34 GMT"}, {"version": "v3", "created": "Wed, 7 Jun 2017 02:46:11 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Cai", "Bryan", ""], ["Daskalakis", "Constantinos", ""], ["Kamath", "Gautam", ""]]}, {"id": "1703.10232", "submitter": "Gennadi Malaschonok I", "authors": "Gennadi Malaschonok", "title": "Recursive Method for the Solution of Systems of Linear Equations", "comments": null, "journal-ref": "Proceedings of the 15th IMACS World Congress, Vol. I, (Berlin,\n  August 1997), Wissenschaft & Technik Verlag, 475--480", "doi": null, "report-no": null, "categories": "cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New solution method for the systems of linear equations in commutative\nintegral domains is proposed. Its complexity is the same that the complexity of\nthe matrix multiplication.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 20:20:18 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Malaschonok", "Gennadi", ""]]}, {"id": "1703.10293", "submitter": "Parter Merav", "authors": "Greg Bodwin, Fabrizio Grandoni, Merav Parter and Virginia Vassilevska\n  Williams", "title": "Preserving Distances in Very Faulty Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preservers and additive spanners are sparse (hence cheap to store) subgraphs\nthat preserve the distances between given pairs of nodes exactly or with some\nsmall additive error, respectively. Since real-world networks are prone to\nfailures, it makes sense to study fault-tolerant versions of the above\nstructures. This turns out to be a surprisingly difficult task. For every small\nbut arbitrary set of edge or vertex failures, the preservers and spanners need\nto contain {\\em replacement paths} around the faulted set. In this paper we\nmake substantial progress on fault tolerant preservers and additive spanners:\n  (1) We present the first truly sub-quadratic size single-pair preservers in\nunweighted (possibly directed) graphs for \\emph{any} fixed number $f$ of\nfaults. Our result indeed generalizes to the single-source case, and can be\nused to build new fault-tolerant additive spanners (for all pairs).\n  (2) The size of the above single-pair preservers is $O(n^{2-g(f)})$ for some\npositive function $g$, and grows to $O(n^2)$ for increasing $f$. We show that\nthis is necessary even in undirected unweighted graphs, and even if you allow\nfor a small additive error: If you aim at size $O(n^{2-\\epsilon})$ for\n$\\epsilon>0$, then the additive error has to be $\\Omega(\\eps f)$. This\nsurprisingly matches known upper bounds in the literature.\n  (3) For weighted graphs, we provide matching upper and lower bounds for the\nsingle pair case. Namely, the size of the preserver is $\\Theta(n^2)$ for $f\\geq\n2$ in both directed and undirected graphs, while for $f=1$ the size is\n$\\Theta(n)$ in undirected graphs. For directed graphs, we have a superlinear\nupper bound and a matching lower bound.\n  Most of our lower bounds extend to the distance oracle setting, where rather\nthan a subgraph we ask for any compact data structure.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 02:47:26 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Bodwin", "Greg", ""], ["Grandoni", "Fabrizio", ""], ["Parter", "Merav", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1703.10380", "submitter": "S{\\o}ren Dahlgaard", "authors": "S{\\o}ren Dahlgaard, Mathias B{\\ae}k Tejs Knudsen, Morten St\\\"ockel", "title": "Finding Even Cycles Faster via Capped k-Walks", "comments": "To appear at STOC'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of finding a cycle of length $2k$ (a\n$C_{2k}$) in an undirected graph $G$ with $n$ nodes and $m$ edges for constant\n$k\\ge2$. A classic result by Bondy and Simonovits [J.Comb.Th.'74] implies that\nif $m \\ge100k n^{1+1/k}$, then $G$ contains a $C_{2k}$, further implying that\none needs to consider only graphs with $m = O(n^{1+1/k})$.\n  Previously the best known algorithms were an $O(n^2)$ algorithm due to Yuster\nand Zwick [J.Disc.Math'97] as well as a $O(m^{2-(1+\\lceil\nk/2\\rceil^{-1})/(k+1)})$ algorithm by Alon et al. [Algorithmica'97].\n  We present an algorithm that uses $O(m^{2k/(k+1)})$ time and finds a $C_{2k}$\nif one exists. This bound is $O(n^2)$ exactly when $m=\\Theta(n^{1+1/k})$. For\n$4$-cycles our new bound coincides with Alon et al., while for every $k>2$ our\nbound yields a polynomial improvement in $m$.\n  Yuster and Zwick noted that it is \"plausible to conjecture that $O(n^2)$ is\nthe best possible bound in terms of $n$\". We show \"conditional optimality\": if\nthis hypothesis holds then our $O(m^{2k/(k+1)})$ algorithm is tight as well.\nFurthermore, a folklore reduction implies that no combinatorial algorithm can\ndetermine if a graph contains a $6$-cycle in time $O(m^{3/2-\\epsilon})$ for any\n$\\epsilon>0$ under the widely believed combinatorial BMM conjecture. Coupled\nwith our main result, this gives tight bounds for finding $6$-cycles\ncombinatorially and also separates the complexity of finding $4$- and\n$6$-cycles giving evidence that the exponent of $m$ in the running time should\nindeed increase with $k$.\n  The key ingredient in our algorithm is a new notion of capped $k$-walks,\nwhich are walks of length $k$ that visit only nodes according to a fixed\nordering. Our main technical contribution is an involved analysis proving\nseveral properties of such walks which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 09:39:22 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["St\u00f6ckel", "Morten", ""]]}, {"id": "1703.10415", "submitter": "Haris Aziz", "authors": "Haris Aziz and Shenwei Huang", "title": "A Polynomial-time Algorithm to Achieve Extended Justified Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a committee voting setting in which each voter approves of a\nsubset of candidates and based on the approvals, a target number of candidates\nare to be selected. In particular we focus on the axiomatic property called\nextended justified representation (EJR). Although a committee satisfying EJR is\nguaranteed to exist, the computational complexity of finding such a committee\nhas been an open problem and explicitly mentioned in multiple recent papers. We\nsettle the complexity of finding a committee satisfying EJR by presenting a\npolynomial-time algorithm for the problem. Our algorithmic approach may be\nuseful for constructing other voting rules in multi-winner voting.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 11:25:13 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Aziz", "Haris", ""], ["Huang", "Shenwei", ""]]}, {"id": "1703.10446", "submitter": "Luis Veiga", "authors": "Miguel E. Coimbra and Mennan Selimi and Alexandre P. Francisco and\n  Felix Freitag and Lu\\'is Veiga", "title": "Gelly-Scheduling: Distributed Graph Processing for Network Service\n  Placement in Community Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "INESC-ID Lisboa Technical Report 4/2017, Feb 2017", "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community networks (CNs) have seen an increase in the last fifteen years.\nTheir members contact nodes which operate Internet proxies, web servers, user\nfile storage and video streaming services, to name a few. Detecting communities\nof nodes with properties (such as co-location) and assessing node eligibility\nfor service placement is thus a key-factor in optimizing the experience of\nusers. We present a novel solution for the problem of service placement as a\ntwo-phase approach, based on: 1) community finding using a scalable graph label\npropagation technique and 2) a decentralized election procedure to address the\nmulti-objective challenge of optimizing service placement in CNs. Herein we: i)\nhighlight the applicability of leader election heuristics which are important\nfor service placement in community networks and scheduler-dependent scenarios;\nii) present a parallel and distributed solution designed as a scalable\nalternative for the problem of service placement, which has mostly seen\ncomputational approaches based on centralization and sequential execution.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 12:52:46 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 14:45:37 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Coimbra", "Miguel E.", ""], ["Selimi", "Mennan", ""], ["Francisco", "Alexandre P.", ""], ["Freitag", "Felix", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "1703.10565", "submitter": "Sahar Bsaybes", "authors": "Sahar Bsaybes, Alain Quilliot, Annegret K. Wagler", "title": "Fleet management for autonomous vehicles: Online PDP under special\n  constraints", "comments": "arXiv admin note: substantial text overlap with arXiv:1609.01634", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The VIPAFLEET project consists in developing models and algorithms for man-\naging a fleet of Individual Public Autonomous Vehicles (VIPA). Hereby, we\nconsider a fleet of cars distributed at specified stations in an industrial\narea to supply internal transportation, where the cars can be used in different\nmodes of circulation (tram mode, elevator mode, taxi mode). One goal is to\ndevelop and implement suitable algorithms for each mode in order to satisfy all\nthe requests under an economic point of view by minimizing the total tour\nlength. The innovative idea and challenge of the project is to develop and\ninstall a dynamic fleet management system that allows the operator to switch\nbetween the different modes within the different periods of the day according\nto the dynamic transportation demands of the users. We model the underlying\nonline transportation system and propose a correspond- ing fleet management\nframework, to handle modes, demands and commands. We consider two modes of\ncirculation, tram and elevator mode, propose for each mode appropriate on- line\nalgorithms and evaluate their performance, both in terms of competitive\nanalysis and practical behavior.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 16:58:11 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Bsaybes", "Sahar", ""], ["Quilliot", "Alain", ""], ["Wagler", "Annegret K.", ""]]}, {"id": "1703.10594", "submitter": "Pratik Ghosal", "authors": "Pratik Ghosal, Adam Kunysz, Katarzyna Paluch", "title": "The Dynamics of Rank-Maximal and Popular Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a bipartite graph, where the two sets of vertices are applicants and\nposts and ranks on the edges represent preferences of applicants over posts, a\n{\\em rank-maximal} matching is one in which the maximum number of applicants is\nmatched to their rank one posts and subject to this condition, the maximum\nnumber of applicants is matched to their rank two posts, and so on. We study\nthe dynamic version of the problem in which a new applicant or post may be\nadded to the graph and we would like to maintain a rank-maximal matching. We\nshow that after the arrival of one vertex, we are always able to update the\nexisting rank-maximal matching in $\\mathcal{O}(\\min(c'n ,n^2) + m)$ time, where\n$n$ denotes the number of applicants, $m$ the number of edges and $c'$ the\nmaximum rank of an edge in an optimal solution. Additionally, we update the\nmatching using a minimal number of changes (replacements). All cases of a\ndeletion of a vertex/edge and an addition of an edge can be reduced to the\nproblem of handling the addition of a vertex. As a by-product, we also get an\nanalogous $\\mathcal{O}(m)$ result for the dynamic version of the (one-sided)\npopular matching problem.\n  Our results are based on the novel use of the properties of the\nEdmonds-Gallai decomposition. The presented ideas may find applications in\nother (dynamic) matching problems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 17:44:45 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 14:33:54 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 07:14:01 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ghosal", "Pratik", ""], ["Kunysz", "Adam", ""], ["Paluch", "Katarzyna", ""]]}, {"id": "1703.10628", "submitter": "Luis Veiga", "authors": "Miguel E. Coimbra and Alexandre P. Francisco and Luis Veiga", "title": "Study on Resource Efficiency of Distributed Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": "INESC-ID Lisboa Technical Report 17/2016, Dec. 2016", "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs may be used to represent many different problem domains -- a concrete\nexample is that of detecting communities in social networks, which are\nrepresented as graphs. With big data and more sophisticated applications\nbecoming widespread in recent years, graph processing has seen an emergence of\nrequirements pertaining data volume and volatility. This multidisciplinary\nstudy presents a review of relevant distributed graph processing systems.\nHerein they are presented in groups defined by common traits (distributed\nprocessing paradigm, type of graph operations, among others), with an overview\nof each system's strengths and weaknesses. The set of systems is then narrowed\ndown to a set of two, upon which quantitative analysis was performed. For this\nquantitative comparison of systems, focus was cast on evaluating the\nperformance of algorithms for the problem of detecting communities. To help\nfurther understand the evaluations performed, a background is provided on graph\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 18:26:53 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Coimbra", "Miguel E.", ""], ["Francisco", "Alexandre P.", ""], ["Veiga", "Luis", ""]]}, {"id": "1703.10633", "submitter": "Hung Le", "authors": "Glencora Borradaile and Hung Le", "title": "Light spanners for bounded treewidth graphs imply light spanners for\n  $H$-minor-free graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grigni and Hung~\\cite{GH12} conjectured that H-minor-free graphs have\n$(1+\\epsilon)$-spanners that are light, that is, of weight $g(|H|,\\epsilon)$\ntimes the weight of the minimum spanning tree for some function $g$. This\nconjecture implies the {\\em efficient} polynomial-time approximation scheme\n(PTAS) of the traveling salesperson problem in $H$-minor free graphs; that is,\na PTAS whose running time is of the form $2^{f(\\epsilon)}n^{O(1)}$ for some\nfunction $f$. The state of the art PTAS for TSP in H-minor-free-graphs has\nrunning time $n^{1/\\epsilon^c}$. We take a further step toward proving this\nconjecture by showing that if the bounded treewidth graphs have light greedy\nspanners, then the conjecture is true. We also prove that the greedy spanner of\na bounded pathwidth graph is light and discuss the possibility of extending our\nproof to bounded treewidth graphs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 18:42:52 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Borradaile", "Glencora", ""], ["Le", "Hung", ""]]}, {"id": "1703.10731", "submitter": "David Avis", "authors": "David Avis and Luc Devroye", "title": "An analysis of budgeted parallel search on conditional Galton-Watson\n  trees", "comments": "15 pages, 3 figures, 2 tables Minor revisions including an extended\n  description of the Q-process with additional figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Avis and Jordan have demonstrated the efficiency of a simple\ntechnique called budgeting for the parallelization of a number of tree search\nalgorithms. The idea is to limit the amount of work that a processor performs\nbefore it terminates its search and returns any unexplored nodes to a master\nprocess. This limit is set by a critical budget parameter which determines the\noverhead of the process. In this paper we study the behaviour of the budget\nparameter on conditional Galton-Watson trees obtaining asymptotically tight\nbounds on this overhead. We present empirical results to show that this bound\nis surprisingly accurate in practice.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 02:03:36 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 15:18:13 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Avis", "David", ""], ["Devroye", "Luc", ""]]}, {"id": "1703.10840", "submitter": "Georgios Stamoulis", "authors": "Steven Kelk, Georgios Stamoulis, Taoyang Wu", "title": "Treewidth distance on phylogenetic trees", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study the treewidth of the \\emph{display graph}, an\nauxiliary graph structure obtained from the fusion of phylogenetic (i.e.,\nevolutionary) trees at their leaves. Earlier work has shown that the treewidth\nof the display graph is bounded if the trees are in some formal sense\ntopologically similar. Here we further expand upon this relationship. We\nanalyse a number of reduction rules which are commonly used in the\nphylogenetics literature to obtain fixed parameter tractable algorithms. In\nsome cases (the \\emph{subtree} reduction) the reduction rules behave similarly\nwith respect to treewidth, while others (the \\emph{cluster} reduction) behave\nvery differently, and the behaviour of the \\emph{chain reduction} is\nparticularly intriguing because of its link with graph separators and forbidden\nminors. We also show that the gap between treewidth and Tree Bisection and\nReconnect (TBR) distance can be infinitely large, and that unlike, for example,\nplanar graphs the treewidth of the display graph can be as much as linear in\nits number of vertices. On a slightly different note we show that if a display\ngraph is formed from the fusion of a phylogenetic network and a tree, rather\nthan from two trees, the treewidth of the display graph is bounded whenever the\ntree can be topologically embedded (\"displayed\") within the network. This opens\nthe door to the formulation of the display problem in Monadic Second Order\nLogic (MSOL). A number of other auxiliary results are given. We conclude with a\ndiscussion and list a number of open problems.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 10:33:24 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Kelk", "Steven", ""], ["Stamoulis", "Georgios", ""], ["Wu", "Taoyang", ""]]}]