[{"id": "1512.00327", "submitter": "Isabel Wagner", "authors": "Isabel Wagner, David Eckhoff", "title": "Technical Privacy Metrics: a Systematic Survey", "comments": null, "journal-ref": "Isabel Wagner and David Eckhoff. 2018. Technical Privacy Metrics:\n  a Systematic Survey. ACM Comput. Surv. 51, 3, Article 57 (June 2018)", "doi": "10.1145/3168389", "report-no": null, "categories": "cs.CR cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of privacy metrics is to measure the degree of privacy enjoyed by\nusers in a system and the amount of protection offered by privacy-enhancing\ntechnologies. In this way, privacy metrics contribute to improving user privacy\nin the digital world. The diversity and complexity of privacy metrics in the\nliterature makes an informed choice of metrics challenging. As a result,\ninstead of using existing metrics, new metrics are proposed frequently, and\nprivacy studies are often incomparable. In this survey we alleviate these\nproblems by structuring the landscape of privacy metrics. To this end, we\nexplain and discuss a selection of over eighty privacy metrics and introduce\ncategorizations based on the aspect of privacy they measure, their required\ninputs, and the type of data that needs protection. In addition, we present a\nmethod on how to choose privacy metrics based on nine questions that help\nidentify the right privacy metrics for a given scenario, and highlight topics\nwhere additional work on privacy metrics is needed. Our survey spans multiple\nprivacy domains and can be understood as a general framework for privacy\nmeasurement.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 16:39:52 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 12:22:25 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Wagner", "Isabel", ""], ["Eckhoff", "David", ""]]}, {"id": "1512.01609", "submitter": "Xiaohan Wei", "authors": "Xiaohan Wei and Michael J. Neely", "title": "Data Center Server Provision: Distributed Asynchronous Control for\n  Coupled Renewal Systems", "comments": "This is a revised version for IEEE Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a cost minimization problem for data centers with N\nservers and randomly arriving service requests. A central router decides which\nserver to use for each new request. Each server has three types of states\n(active, idle, setup) with different costs and time durations. The servers\noperate asynchronously over their own states and can choose one of multiple\nsleep modes when idle. We develop an online distributed control algorithm so\nthat each server makes its own decisions, the request queues are bounded and\nthe overall time average cost is near optimal with probability 1. The algorithm\ndoes not need probability information for the arrival rate or job sizes. Next,\nan improved algorithm that uses a single queue is developed via a\n\"virtualization\" technique which is shown to provide the same (near optimal)\ncosts. Simulation experiments on a real data center traffic trace demonstrate\nthe efficiency of our algorithm compared to other existing algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 01:20:24 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 17:47:21 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Wei", "Xiaohan", ""], ["Neely", "Michael J.", ""]]}, {"id": "1512.02425", "submitter": "Steven Weber", "authors": "Lex Fridman and Jeffrey Wildman and Steven Weber", "title": "On the joint impact of bias and power control on downlink spectral\n  efficiency in cellular networks", "comments": "14 pages, 9 figures, submitted on December 8, 2015 to IEEE/ACM\n  Transactions on Networking, extension of Crowncom 2013 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell biasing and downlink transmit power are two controls that may be used to\nimprove the spectral efficiency of cellular networks. With cell biasing, each\nmobile user associates with the base station offering, say, the highest biased\nsignal to interference plus noise ratio. Biasing affects the cell association\ndecisions of mobile users, but not the received instantaneous downlink\ntransmission rates. Adjusting the collection of downlink transmission powers\ncan likewise affect the cell associations, but in contrast with biasing, it\nalso directly affects the instantaneous rates. This paper investigates the\njoint use of both cell biasing and transmission power control and their\n(individual and joint) effects on the statistical properties of the collection\nof per-user spectral efficiencies. Our analytical results and numerical\ninvestigations demonstrate in some cases a significant performance improvement\nin the Pareto efficient frontiers of both a mean-variance and\nthroughput-fairness tradeoff from using both bias and power controls over using\neither control alone.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 12:27:53 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Fridman", "Lex", ""], ["Wildman", "Jeffrey", ""], ["Weber", "Steven", ""]]}, {"id": "1512.02673", "submitter": "Kangwook Lee", "authors": "Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris\n  Papailiopoulos, Kannan Ramchandran", "title": "Speeding Up Distributed Machine Learning Using Codes", "comments": "This work is published in IEEE Transactions on Information Theory and\n  presented in part at the NIPS 2015 Workshop on Machine Learning Systems and\n  the IEEE ISIT 2016", "journal-ref": null, "doi": "10.1109/TIT.2017.2736066", "report-no": null, "categories": "cs.DC cs.IT cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Codes are widely used in many engineering applications to offer robustness\nagainst noise. In large-scale systems there are several types of noise that can\naffect the performance of distributed machine learning algorithms -- straggler\nnodes, system failures, or communication bottlenecks -- but there has been\nlittle interaction cutting across codes, machine learning, and distributed\nsystems. In this work, we provide theoretical insights on how coded solutions\ncan achieve significant gains compared to uncoded ones. We focus on two of the\nmost basic building blocks of distributed learning algorithms: matrix\nmultiplication and data shuffling. For matrix multiplication, we use codes to\nalleviate the effect of stragglers, and show that if the number of homogeneous\nworkers is $n$, and the runtime of each subtask has an exponential tail, coded\ncomputation can speed up distributed matrix multiplication by a factor of $\\log\nn$. For data shuffling, we use codes to reduce communication bottlenecks,\nexploiting the excess in storage. We show that when a constant fraction\n$\\alpha$ of the data matrix can be cached at each worker, and $n$ is the number\nof workers, \\emph{coded shuffling} reduces the communication cost by a factor\nof $(\\alpha + \\frac{1}{n})\\gamma(n)$ compared to uncoded shuffling, where\n$\\gamma(n)$ is the ratio of the cost of unicasting $n$ messages to $n$ users to\nmulticasting a common message (of the same size) to $n$ users. For instance,\n$\\gamma(n) \\simeq n$ if multicasting a message to $n$ users is as cheap as\nunicasting a message to one user. We also provide experiment results,\ncorroborating our theoretical gains of the coded algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 21:54:04 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 19:34:37 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 03:04:14 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Lee", "Kangwook", ""], ["Lam", "Maximilian", ""], ["Pedarsani", "Ramtin", ""], ["Papailiopoulos", "Dimitris", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1512.02972", "submitter": "Jorge Ortiz", "authors": "Jorge Ortiz (IBM Reserch) and Chien-Chin Huang (NYU Computer Science)\n  and Supriyo Chakraborty (IBM Research)", "title": "Get More With Less: Near Real-Time Image Clustering on Mobile Phones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms, in conjunction with user data, hold the promise\nof revolutionizing the way we interact with our phones, and indeed their\nwidespread adoption in the design of apps bear testimony to this promise.\nHowever, currently, the computationally expensive segments of the learning\npipeline, such as feature extraction and model training, are offloaded to the\ncloud, resulting in an over-reliance on the network and under-utilization of\ncomputing resources available on mobile platforms. In this paper, we show that\nby combining the computing power distributed over a number of phones, judicious\noptimization choices, and contextual information it is possible to execute the\nend-to-end pipeline entirely on the phones at the edge of the network,\nefficiently. We also show that by harnessing the power of this combination, it\nis possible to execute a computationally expensive pipeline at near real-time.\n  To demonstrate our approach, we implement an end-to-end image-processing\npipeline -- that includes feature extraction, vocabulary learning,\nvectorization, and image clustering -- on a set of mobile phones. Our results\nshow a 75% improvement over the standard, full pipeline implementation running\non the phones without modification -- reducing the time to one minute under\ncertain conditions. We believe that this result is a promising indication that\nfully distributed, infrastructure-less computing is possible on networks of\nmobile phones; enabling a new class of mobile applications that are less\nreliant on the cloud.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 18:08:59 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Ortiz", "Jorge", "", "IBM Reserch"], ["Huang", "Chien-Chin", "", "NYU Computer Science"], ["Chakraborty", "Supriyo", "", "IBM Research"]]}, {"id": "1512.03851", "submitter": "Subrata Ashe", "authors": "Subrata Ashe", "title": "A Design of Endurance Queue for Co-Existing Systems in Multi-Programmed\n  Environments", "comments": "4 pages. 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DM cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These days enterprise applications try to integrate online processing and\nbatch jobs into a common software stack for seamless monitoring and driverless\noperations. Continuous integration of these systems results in choking of the\npoorly performing sub-systems, when the service demand and throughput are not\nsynchronized. A poorly performing sub-system may become a serious performance\nbottleneck for the entire system if its serviceability and the capacity is over\nutilized by increased service demand from upstream systems. From all the\nintegrated sub-systems, queuing systems are majorly categorized as choking\nelements due to their limited service length and lack of processing details.\nThe situation becomes more pronounced in multiprogramming environments where\nthe queue performance exponentially degrades with increased degree of\nmultiprogramming at upstream levels. This paper presents an approach to compute\nthe queue length and devise a distribution model such that the queue length is\ndynamically adjusted depending on the sudden growth or decline of transmission\npackets. The idea is to design a heat map of the memory and correlate it with\nthe queue length distribution. With each degree of multi-programmability, the\ndata processing logic is adjusted by the distribution model to arrive at an\nendurance level queue for long term service under variable load conditions. It\nwill take away the current implementation of using delayed processing logic\nand/or batch processing of data at downstream systems.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 00:18:09 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Ashe", "Subrata", ""]]}, {"id": "1512.04057", "submitter": "Hossein Shokri Ghadikolaei", "authors": "Hossein Shokri-Ghadikolaei and Carlo Fischione", "title": "The Transitional Behavior of Interference in Millimeter Wave Networks\n  and Its Impact on Medium Access Control", "comments": "18 pages, 8 figures, 1 table, to appear in IEEE Transactions on\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millimeter wave (mmWave) communication systems use large number of antenna\nelements that can potentially overcome severe channel attenuation by narrow\nbeamforming. Narrow-beam operation in mmWave networks also reduces multiuser\ninterference, introducing the concept of noise-limited wireless networks as\nopposed to interference-limited ones. The noise-limited or interference-limited\nregime heavily reflects on the medium access control (MAC) layer throughput and\non proper resource allocation and interference management strategies. Yet,\nthese regimes are ignored in current approaches to mmWave MAC layer design,\nwith the potential disastrous consequences on the communication performance. In\nthis paper, we investigate these regimes in terms of collision probability and\nthroughput. We derive tractable closed-form expressions for the collision\nprobability and MAC layer throughput of mmWave ad hoc networks, operating under\nslotted ALOHA. The new analysis reveals that mmWave networks may exhibit a\nnon-negligible transitional behavior from a noise-limited regime to an\ninterference-limited one, depending on the density of the transmitters, density\nand size of obstacles, transmission probability, operating beamwidth, and\ntransmission power. Such transitional behavior necessitates a new framework of\nadaptive hybrid resource allocation procedure, containing both contention-based\nand contention-free phases with on-demand realization of the contention-free\nphase. Moreover, the conventional collision avoidance procedure in the\ncontention-based phase should be revisited, due to the transitional behavior of\ninterference, to maximize throughput/delay performance of mmWave networks. We\nconclude that, unless proper hybrid schemes are investigated, the severity of\nthe transitional behavior may significantly reduce throughput/delay performance\nof mmWave networks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 14:27:35 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Shokri-Ghadikolaei", "Hossein", ""], ["Fischione", "Carlo", ""]]}, {"id": "1512.05172", "submitter": "Steven Weber", "authors": "Ni An and Steven Weber", "title": "On the performance overhead tradeoff of distributed principal component\n  analysis via data partitioning", "comments": "6 pages, 6 figures, submitted to CISS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is not only a fundamental dimension\nreduction method, but is also a widely used network anomaly detection\ntechnique. Traditionally, PCA is performed in a centralized manner, which has\npoor scalability for large distributed systems, on account of the large network\nbandwidth cost required to gather the distributed state at a fusion center.\nConsequently, several recent works have proposed various distributed PCA\nalgorithms aiming to reduce the communication overhead incurred by PCA without\nlosing its inferential power. This paper evaluates the tradeoff between\ncommunication cost and solution quality of two distributed PCA algorithms on a\nreal domain name system (DNS) query dataset from a large network. We also apply\nthe distributed PCA algorithm in the area of network anomaly detection and\ndemonstrate that the detection accuracy of both distributed PCA-based methods\nhas little degradation in quality, yet achieves significant savings in\ncommunication bandwidth.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 13:35:47 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 14:07:29 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2015 13:01:59 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["An", "Ni", ""], ["Weber", "Steven", ""]]}, {"id": "1512.05882", "submitter": "Daniel-Marian Merezeanu", "authors": "Daniel Marian Merezeanu, Daniela Andone", "title": "Tandem Queueing Systems Maximum Throughput Problem", "comments": null, "journal-ref": "SINTES 10, Proc. of SINTES 10, Craiova, Romania, pp. 120-123 May\n  25-26, 2000", "doi": "10.13140/RG.2.1.2601.7362", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of maximum throughput for tandem\nqueueing system. We modeled this system as a Quasi-Birth-Death process. In\norder to do this we named level the number of customers waiting in the first\nbuffer (including the customer in service) and we called phase the state of the\nremining servers. Using this model we studied the problem of maximum throughput\nof the system: the maximum arrival rate that a given system could support\nbefore becoming saturated, or unstable. We considered different particular\ncases of such systems, which were obtained by modifying the capacity of the\nintermediary buffers, the arrival rate and the service rates. The results of\nthe simulations are presented in our paper and can be summed up in the\nfollowing conclusions: 1. The analytic formula for the maximum throughput of\nthe system tends to become rather complicated when the number of servers\nincrease 2. The maximum throughput of the system converges as the number of\nservers increases 3. The homogeneous case reveals an interesting\ncharacteristic: if we reverse the order of the servers, maximum thruoughput of\nthe system remains unchanged The QBD process used for the case of Poisson\narrivals can be extended to model more general arrival processes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 09:28:16 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Merezeanu", "Daniel Marian", ""], ["Andone", "Daniela", ""]]}, {"id": "1512.07004", "submitter": "Ahmed Altaher", "authors": "Ahmed Altaher (GIPSA-lab), St\\'ephane Mocanu (GIPSA-lab), Jean-Marc\n  Thiriet (GIPSA-lab)", "title": "Evaluation of Time-Critical Communications for IEC 61850-Substation\n  Network Architecture", "comments": null, "journal-ref": "Surveillance 8 International Conference , Oct 2015, Roanne,\n  France. Proceeding of Surveillance 8 2015,\n  \\&lt;http://surveillance8.sciencesconf.org/\\&gt;", "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Present-day developments, in electrical power transmission and distribution,\nrequire considerations of the status quo. In other meaning, international\nregulations enforce increasing of reliability and reducing of environment\nimpact, correspondingly they motivate developing of dependable systems. Power\ngrids especially intelligent (smart grids) ones become industrial solutions\nthat follow standardized development. The International standardization, in the\nfield of power transmission and distribution, improve technology influences.\nThe rise of dedicated standards for SAS (Substation Automation Systems)\ncommunications, such as the leading International Electro-technical Commission\nstandard IEC 61850, enforces modern technological trends in this field. Within\nthis standard, a constraint of low ETE (End-to-End) latency should be\nrespected, and time-critical status transmission must be achieved. This\nexperimental study emphasis on IEC 61850 SAS communication standard, e.g. IEC\n61850 GOOSE (Generic Object Oriented Substation Events), to implement an\ninvestigational method to determine the protection communication delay. This\nmethod observes GOOSE behaviour by adopting monitoring and analysis\ncapabilities. It is observed by using network test equipment, i.e. SPAN (Switch\nPort Analyser) and TAP (Test Access Point) devices, with on-the-shelf available\nhardware and software solutions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 09:54:54 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Altaher", "Ahmed", "", "GIPSA-lab"], ["Mocanu", "St\u00e9phane", "", "GIPSA-lab"], ["Thiriet", "Jean-Marc", "", "GIPSA-lab"]]}, {"id": "1512.08354", "submitter": "Markus Fidler", "authors": "Markus Fidler and Yuming Jiang", "title": "Non-Asymptotic Delay Bounds for (k,l) Fork-Join Systems and Multi-Stage\n  Fork-Join Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel systems have received increasing attention with numerous recent\napplications such as fork-join systems, load-balancing, and l-out-of-k\nredundancy. Common to these systems is a join or resequencing stage, where\ntasks that have finished service may have to wait for the completion of other\ntasks so that they leave the system in a predefined order. These\nsynchronization constraints make the analysis of parallel systems challenging\nand few explicit results are known. In this work, we model parallel systems\nusing a max-plus approach that enables us to derive statistical bounds of\nwaiting and sojourn times. Taking advantage of max-plus system theory, we also\nshow end-to-end delay bounds for multi-stage fork-join networks. We contribute\nsolutions for basic G|G|1 fork-join systems, parallel systems with\nload-balancing, as well as general (k,l) fork-join systems with redundancy. Our\nresults provide insights into the respective advantages of l-out-of-k\nredundancy vs. load-balancing.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 09:27:32 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Fidler", "Markus", ""], ["Jiang", "Yuming", ""]]}, {"id": "1512.08609", "submitter": "Aresh Dadlani", "authors": "Muthukrishnan Senthil Kumar and Aresh Dadlani and Kiseon Kim", "title": "Performance Analysis of an Unreliable $M/G/1$ Retrial Queue with Two-way\n  Communication", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient use of call center operators through technological innovations more\noften come at the expense of added operation management issues. In this paper,\nthe stationary characteristics of an $M/G/1$ retrial queue is investigated\nwhere the single server, subject to active failures, primarily attends incoming\ncalls and directs outgoing calls only when idle. The incoming calls arriving at\nthe server follow a Poisson arrival process, while outgoing calls are made in\nan exponentially distributed time. On finding the server unavailable (either\nbusy or temporarily broken down), incoming calls intrinsically join the virtual\norbit from which they re-attempt for service at exponentially distributed time\nintervals. The system stability condition along with probability generating\nfunctions for the joint queue length distribution of the number of calls in the\norbit and the state of the server are derived and evaluated numerically in the\ncontext of mean system size, server availability, failure frequency and orbit\nwaiting time.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 07:02:05 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 06:13:12 GMT"}, {"version": "v3", "created": "Tue, 26 Dec 2017 06:12:36 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Kumar", "Muthukrishnan Senthil", ""], ["Dadlani", "Aresh", ""], ["Kim", "Kiseon", ""]]}]