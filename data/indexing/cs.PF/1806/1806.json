[{"id": "1806.01126", "submitter": "Tobias Hossfeld", "authors": "Tobias Hossfeld, Poul E. Heegaard, Martin Varela, Lea Skorin-Kapov", "title": "Confidence Interval Estimators for MOS Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.HC cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the quantification of QoE, subjects often provide individual rating\nscores on certain rating scales which are then aggregated into Mean Opinion\nScores (MOS). From the observed sample data, the expected value is to be\nestimated. While the sample average only provides a point estimator, confidence\nintervals (CI) are an interval estimate which contains the desired expected\nvalue with a given confidence level. In subjective studies, the number of\nsubjects performing the test is typically small, especially in lab\nenvironments. The used rating scales are bounded and often discrete like the\n5-point ACR rating scale. Therefore, we review statistical approaches in the\nliterature for their applicability in the QoE domain for MOS interval\nestimation (instead of having only a point estimator, which is the MOS). We\nprovide a conservative estimator based on the SOS hypothesis and binomial\ndistributions and compare its performance (CI width, outlier ratio of CI\nviolating the rating scale bounds) and coverage probability with well known CI\nestimators. We show that the provided CI estimator works very well in practice\nfor MOS interval estimators, while the commonly used studentized CIs suffer\nfrom a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale.\nAs an alternative, bootstrapping, i.e., random sampling of the subjective\nratings with replacement, is an efficient CI estimator leading to typically\nsmaller CIs, but lower coverage than the proposed estimator.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 13:59:55 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hossfeld", "Tobias", ""], ["Heegaard", "Poul E.", ""], ["Varela", "Martin", ""], ["Skorin-Kapov", "Lea", ""]]}, {"id": "1806.01352", "submitter": "Mostafa Kishani", "authors": "Mostafa Kishani and Hossein Asadi", "title": "Modeling Impact of Human Errors on the Data Unavailability and Data Loss\n  of Storage Systems", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data storage systems and their availability play a crucial role in\ncontemporary datacenters. Despite using mechanisms such as automatic fail-over\nin datacenters, the role of human agents and consequently their destructive\nerrors is inevitable. Due to very large number of disk drives used in exascale\ndatacenters and their high failure rates, the disk subsystem in storage systems\nhas become a major source of Data Unavailability (DU) and Data Loss (DL)\ninitiated by human errors. In this paper, we investigate the effect of\nIncorrect Disk Replacement Service (IDRS) on the availability and reliability\nof data storage systems. To this end, we analyze the consequences of IDRS in a\ndisk array, and conduct Monte Carlo simulations to evaluate DU and DL during\nmission time. The proposed modeling framework can cope with a) different\nstorage array configurations and b) Data Object Survivability (DOS),\nrepresenting the effect of system level redundancies such as remote backups and\nmirrors. In the proposed framework, the model parameters are obtained from\nindustrial and scientific reports alongside field data which have been\nextracted from a datacenter operating with 70 storage racks. The results show\nthat ignoring the impact of IDRS leads to unavailability underestimation by up\nto three orders of magnitude. Moreover, our study suggests that by considering\nthe effect of human errors, the conventional beliefs about the dependability of\ndifferent Redundant Array of Independent Disks (RAID) mechanisms should be\nrevised. The results show that RAID1 can result in lower availability compared\nto RAID5 in the presence of human errors. The results also show that employing\nautomatic fail-over policy (using hot spare disks) can reduce the drastic\nimpacts of human errors by two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 07:48:18 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 14:08:07 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Kishani", "Mostafa", ""], ["Asadi", "Hossein", ""]]}, {"id": "1806.01360", "submitter": "Mostafa Kishani", "authors": "Mostafa Kishani, Reza Eftekhari, and Hossein Asadi", "title": "Evaluating Impact of Human Errors on the Availability of Data Storage\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the effect of incorrect disk replacement\nservice on the availability of data storage systems. To this end, we first\nconduct Monte Carlo simulations to evaluate the availability of disk subsystem\nby considering disk failures and incorrect disk replacement service. We also\npropose a Markov model that corroborates the Monte Carlo simulation results. We\nfurther extend the proposed model to consider the effect of automatic disk\nfail-over policy. The results obtained by the proposed model show that\noverlooking the impact of incorrect disk replacement can result up to three\norders of magnitude unavailability underestimation. Moreover, this study\nsuggests that by considering the effect of human errors, the conventional\nbelieves about the dependability of different RAID mechanisms should be\nrevised. The results show that in the presence of human errors, RAID1 can\nresult in lower availability compared to RAID5.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 08:32:39 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Kishani", "Mostafa", ""], ["Eftekhari", "Reza", ""], ["Asadi", "Hossein", ""]]}, {"id": "1806.01374", "submitter": "Sathish Gopalakrishnan", "authors": "Sathish Gopalakrishnan", "title": "Improving rewards in overloaded real-time systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive analysis of online algorithms has commonly been applied to\nunderstand the behaviour of real-time systems during overload conditions. While\ncompetitive analysis provides insight into the behaviour of certain algorithms,\nit is hard to make inferences about the performance of those algorithms in\npractice. Other approaches to dealing with overload resort to heuristics that\nseem to perform well but are hard to prove as being good. Further, most work on\nhandling overload in real-time systems does not consider using information\nregarding the distribution of arrival rates of jobs and execution times to make\nscheduling decisions. We present an scheduling policy (obtained through\nstochastic approximation, and using information about the workload) to handle\noverload in real-time systems and improve the revenue earned when each\nsuccessful job completion results in revenue accrual. We prove that the policy\nwe outline does lead to increased revenue when compared to a class of\nscheduling policies that make static resource allocations to different service\nclasses. We also use empirical evidence to underscore the fact that this policy\nperforms better than a variety of other scheduling policies. The ideas\npresented can be applied to several soft real-time systems, specifically\nsystems with multiple service classes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 20:28:01 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Gopalakrishnan", "Sathish", ""]]}, {"id": "1806.02508", "submitter": "Chen Chen", "authors": "Chen Chen, Qizhen Weng, Wei Wang, Baochun Li, Bo Li", "title": "Semi-Dynamic Load Balancing: Efficient Distributed Learning in\n  Non-Dedicated Environments", "comments": null, "journal-ref": null, "doi": "10.1145/3419111.3421299", "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are increasingly trained in clusters with\nnon-dedicated workers possessing heterogeneous resources. In such scenarios,\nmodel training efficiency can be negatively affected by stragglers -- workers\nthat run much slower than others. Efficient model training requires eliminating\nsuch stragglers, yet for modern ML workloads, existing load balancing\nstrategies are inefficient and even infeasible. In this paper, we propose a\nnovel strategy called semi-dynamic load balancing to eliminate stragglers of\ndistributed ML workloads. The key insight is that ML workers shall be\nload-balanced at iteration boundaries, being non-intrusive to intra-iteration\nexecution. We develop LB-BSP based on such an insight, which is an integrated\nworker coordination mechanism that adapts workers' load to their instantaneous\nprocessing capabilities by right-sizing the sample batches at the\nsynchronization barriers. We have custom-designed the batch sizing algorithm\nrespectively for CPU and GPU clusters based on their own characteristics.\nLB-BSP has been implemented as a Python module for ML frameworks like\nTensorFlow and PyTorch. Our EC2 deployment confirms that LB-BSP is practical,\neffective and light-weight, and is able to accelerating distributed training by\nup to $54\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 04:15:58 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 14:37:47 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Chen", "Chen", ""], ["Weng", "Qizhen", ""], ["Wang", "Wei", ""], ["Li", "Baochun", ""], ["Li", "Bo", ""]]}, {"id": "1806.05444", "submitter": "Debankur Mukherjee", "authors": "Mark van der Boor, Sem C. Borst, Johan S.H. van Leeuwaarden, Debankur\n  Mukherjee", "title": "Scalable load balancing in networked systems: A survey of recent\n  advances", "comments": "Extends the short review presented at ICM 2018 (arXiv:1712.08555). 69\n  pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic load balancing scenario involves a single dispatcher where tasks\narrive that must immediately be forwarded to one of $N$ single-server queues.\nWe discuss recent advances on scalable load balancing schemes which provide\nfavorable delay performance when $N$ grows large, and yet only require minimal\nimplementation overhead.\n  Join-the-Shortest-Queue (JSQ) yields vanishing delays as $N$ grows large, as\nin a centralized queueing arrangement, but involves a prohibitive communication\nburden. In contrast, power-of-$d$ or JSQ($d$) schemes that assign an incoming\ntask to a server with the shortest queue among $d$ servers selected uniformly\nat random require little communication, but lead to constant delays. In order\nto examine this fundamental trade-off between delay performance and\nimplementation overhead, we consider JSQ($d(N)$) schemes where the diversity\nparameter $d(N)$ depends on $N$ and investigate what growth rate of $d(N)$ is\nrequired to asymptotically match the optimal JSQ performance on fluid and\ndiffusion scale.\n  Stochastic coupling techniques and stochastic-process limits play an\ninstrumental role in establishing the asymptotic optimality. We demonstrate how\nthis methodology carries over to infinite-server settings, finite buffers,\nmultiple dispatchers, servers arranged on graph topologies, and token-based\nload balancing including the popular Join-the-Idle-Queue (JIQ) scheme. In this\nway we provide a broad overview of the many recent advances in the field. This\nsurvey extends the short review presented at ICM 2018 (arXiv:1712.08555).\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 10:07:48 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["van der Boor", "Mark", ""], ["Borst", "Sem C.", ""], ["van Leeuwaarden", "Johan S. H.", ""], ["Mukherjee", "Debankur", ""]]}, {"id": "1806.07060", "submitter": "Flavio Vella", "authors": "Marco Cianfriglia, Flavio Vella, Cedric Nugteren, Anton Lokhmotov,\n  Grigori Fursin", "title": "A model-driven approach for a new generation of adaptive libraries", "comments": "New detailed analysis will be provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.MS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient high-performance libraries often expose multiple tunable parameters\nto provide highly optimized routines. These can range from simple loop unroll\nfactors or vector sizes all the way to algorithmic changes, given that some\nimplementations can be more suitable for certain devices by exploiting hardware\ncharacteristics such as local memories and vector units. Traditionally, such\nparameters and algorithmic choices are tuned and then hard-coded for a specific\narchitecture and for certain characteristics of the inputs. However, emerging\napplications are often data-driven, thus traditional approaches are not\neffective across the wide range of inputs and architectures used in practice.\nIn this paper, we present a new adaptive framework for data-driven applications\nwhich uses a predictive model to select the optimal algorithmic parameters by\ntraining with synthetic and real datasets. We demonstrate the effectiveness of\na BLAS library and specifically on its matrix multiplication routine. We\npresent experimental results for two GPU architectures and show significant\nperformance gains of up to 3x (on a high-end NVIDIA Pascal GPU) and 2.5x (on an\nembedded ARM Mali GPU) when compared to a traditionally optimized library.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 06:17:38 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Cianfriglia", "Marco", ""], ["Vella", "Flavio", ""], ["Nugteren", "Cedric", ""], ["Lokhmotov", "Anton", ""], ["Fursin", "Grigori", ""]]}, {"id": "1806.07300", "submitter": "James Browne", "authors": "James Browne, Tyler M. Tomita, Disa Mhembere, Randal Burns, Joshua T.\n  Vogelstein", "title": "Forest Packing: Fast, Parallel Decision Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine learning has an emerging critical role in high-performance computing\nto modulate simulations, extract knowledge from massive data, and replace\nnumerical models with efficient approximations. Decision forests are a critical\ntool because they provide insight into model operation that is critical to\ninterpreting learned results. While decision forests are trivially\nparallelizable, the traversals of tree data structures incur many random memory\naccesses and are very slow. We present memory packing techniques that\nreorganize learned forests to minimize cache misses during classification. The\nresulting layout is hierarchical. At low levels, we pack the nodes of multiple\ntrees into contiguous memory blocks so that each memory access fetches data for\nmultiple trees. At higher levels, we use leaf cardinality to identify the most\npopular paths through a tree and collocate those paths in cache lines. We\nextend this layout with out-of-order execution and cache-line prefetching to\nincrease memory throughput. Together, these optimizations increase the\nperformance of classification in ensembles by a factor of four over an\noptimized C++ implementation and a actor of 50 over a popular R language\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 15:11:17 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Browne", "James", ""], ["Tomita", "Tyler M.", ""], ["Mhembere", "Disa", ""], ["Burns", "Randal", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1806.08299", "submitter": "Nicholas Sim", "authors": "Nicholas Sim", "title": "Optimising finite-difference methods for PDEs through parameterised\n  time-tiling in Devito", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite-difference methods are widely used in solving partial differential\nequations. In a large problem set, approximations can take days or weeks to\nevaluate, yet the bulk of computation may occur within a single loop nest. The\nmodelling process for researchers is not straightforward either, requiring\nmodels with differential equations to be translated into stencil kernels, then\noptimised separately. One tool that seeks to speed up and eliminate mistakes\nfrom this tedious procedure is Devito, used to efficiently employ\nfinite-difference methods.\n  In this work, we implement time-tiling, a loop nest optimisation, in Devito\nyielding a decrease in runtime of up to 45%, and at least 20% across stencils\nfrom the acoustic wave equation family, widely used in Devito's target domain\nof seismic imaging. We present an estimator for arithmetic intensity under\ntime-tiling and a model to predict runtime improvements in stencil\ncomputations. We also consider generalisation of time-tiling to imperfect loop\nnests, a less widely studied problem.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 15:50:20 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Sim", "Nicholas", ""]]}, {"id": "1806.11128", "submitter": "Justin Deters", "authors": "Justin Deters, Jiaye Wu, Yifan Xu, I-Ting Angelina Lee", "title": "A NUMA-Aware Provably-Efficient Task-Parallel Platform Based on the\n  Work-First Principle", "comments": "14 pages, 9 figures", "journal-ref": "2018 IEEE International Symposium on Workload Characterization\n  (IISWC), Raleigh, NC, USA, 2018, pp. 59-70", "doi": "10.1109/IISWC.2018.8573486", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task parallelism is designed to simplify the task of parallel programming.\nWhen executing a task parallel program on modern NUMA architectures, it can\nfail to scale due to the phenomenon called work inflation, where the overall\nprocessing time that multiple cores spend on doing useful work is higher\ncompared to the time required to do the same amount of work on one core, due to\neffects experienced only during parallel executions such as additional cache\nmisses, remote memory accesses, and memory bandwidth issues. It's possible to\nmitigate work inflation by co-locating the computation with the data, but this\nis nontrivial to do with task parallel programs. First, by design, the\nscheduling for task parallel programs is automated, giving the user little\ncontrol over where the computation is performed. Second, the platforms tend to\nemploy work stealing, which provides strong theoretical guarantees, but its\nrandomized protocol for load balancing does not discern between work items that\nare far away versus ones that are closer. In this work, we propose NUMA-WS, a\nNUMA-aware task parallel platform engineering based on the work-first\nprinciple. By abiding by the work-first principle, we are able to obtain a\nplatform that is work efficient, provides the same theoretical guarantees as\nthe classic work stealing scheduler, and mitigates work inflation. Furthermore,\nwe implemented a prototype platform by modifying Intel's Cilk Plus runtime\nsystem and empirically demonstrate that the resulting system is work efficient\nand scalable.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 18:00:42 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 17:02:41 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 18:29:45 GMT"}, {"version": "v4", "created": "Mon, 7 Jan 2019 15:46:26 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Deters", "Justin", ""], ["Wu", "Jiaye", ""], ["Xu", "Yifan", ""], ["Lee", "I-Ting Angelina", ""]]}]