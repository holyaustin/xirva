[{"id": "2011.00243", "submitter": "Georg Hager", "authors": "Ayesha Afzal and Georg Hager and Gerhard Wellein", "title": "An analytic performance model for overlapping execution of memory-bound\n  loop kernels on multicore CPUs", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex applications running on multicore processors show a rich performance\nphenomenology. The growing number of cores per ccNUMA domain complicates\nperformance analysis of memory-bound code since system noise, load imbalance,\nor task-based programming models can lead to thread desynchronization. Hence,\nthe simplifying assumption that all cores execute the same loop can not be\nupheld. Motivated by observations on plain and modified versions of the HPCG\nbenchmark, we construct a performance model of execution of memory-bound loop\nkernels. It can predict the memory bandwidth share per kernel on a memory\ncontention domain depending on the number of active cores and which other\nworkload the kernel is paired with. The only code features required are the\nsingle-thread cache line access frequency per kernel, which is directly related\nto the single-thread memory bandwidth, and its saturated bandwidth. It can\neither be measured directly or predicted using the Execution-Cache-Memory (ECM)\nperformance model. The computational intensity of the kernels and the detailed\nstructure of the code is of no significance. We validate our model on Intel\nBroadwell, Intel Cascade Lake, and AMD Rome processors pairing various\nstreaming and stencil kernels. The error in predicting the bandwidth share per\nkernel is less than 8%.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 11:08:49 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Afzal", "Ayesha", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "2011.00656", "submitter": "Giulia Guidi", "authors": "Giulia Guidi, Marquita Ellis, Aydin Buluc, Katherine Yelick, David\n  Culler", "title": "10 Years Later: Cloud Computing is Closing the Performance Gap", "comments": null, "journal-ref": "Companion of the 2021 ACM/SPEC International Conference on\n  Performance Engineering (ICPE21 Companion)", "doi": "10.1145/3447545.3451183", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can cloud computing infrastructures provide HPC-competitive performance for\nscientific applications broadly? Despite prolific related literature, this\nquestion remains open. Answers are crucial for designing future systems and\ndemocratizing high-performance computing. We present a multi-level approach to\ninvestigate the performance gap between HPC and cloud computing, isolating\ndifferent variables that contribute to this gap. Our experiments are divided\ninto (i) hardware and system microbenchmarks and (ii) user application proxies.\nThe results show that today's high-end cloud computing can deliver\nHPC-competitive performance not only for computationally intensive applications\nbut also for memory- and communication-intensive applications - at least at\nmodest scales - thanks to the high-speed memory systems and interconnects and\ndedicated batch scheduling now available on some cloud platforms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 00:35:38 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 20:57:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Guidi", "Giulia", ""], ["Ellis", "Marquita", ""], ["Buluc", "Aydin", ""], ["Yelick", "Katherine", ""], ["Culler", "David", ""]]}, {"id": "2011.00866", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Praveenkumar Kanumala, Stephen Guo, Kannan\n  Achan", "title": "An End-to-End ML System for Personalized Conversational Voice Models in\n  Walmart E-Commerce", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for and making decisions about products is becoming increasingly\neasier in the e-commerce space, thanks to the evolution of recommender systems.\nPersonalization and recommender systems have gone hand-in-hand to help\ncustomers fulfill their shopping needs and improve their experiences in the\nprocess. With the growing adoption of conversational platforms for shopping, it\nhas become important to build personalized models at scale to handle the large\ninflux of data and perform inference in real-time. In this work, we present an\nend-to-end machine learning system for personalized conversational voice\ncommerce. We include components for implicit feedback to the model, model\ntraining, evaluation on update, and a real-time inference engine. Our system\npersonalizes voice shopping for Walmart Grocery customers and is currently\navailable via Google Assistant, Siri and Google Home devices.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:14:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Kanumala", "Praveenkumar", ""], ["Guo", "Stephen", ""], ["Achan", "Kannan", ""]]}, {"id": "2011.01485", "submitter": "Nitish K Panigrahy", "authors": "Nitish K. Panigrahy, Thirupathaiah Vasantam, Prithwish Basu and Don\n  Towsley", "title": "Proximity Based Load Balancing Policies on Graphs: A Simulation Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed load balancing is the act of allocating jobs among a set of\nservers as evenly as possible. There are mainly two versions of the load\nbalancing problem that have been studied in the literature: static and dynamic.\nThe static interpretation leads to formulating the load balancing problem as a\ncase with jobs (balls) never leaving the system and accumulating at the servers\n(bins) whereas the dynamic setting deals with the case when jobs arrive and\nleave the system after service completion. This paper designs and evaluates\nserver proximity aware job allocation policies for treating load balancing\nproblems with a goal to reduce the communication cost associated with the jobs.\nWe consider a class of proximity aware Power of Two (POT) choice based\nassignment policies for allocating jobs to servers, where servers are\ninterconnected as an n-vertex graph G(V, E). For the static version, we assume\neach job arrives at one of the servers, u. For the dynamic setting, we assume G\nto be a circular graph and job arrival process at each server is described by a\nPoisson point process with the job service time exponentially distributed. For\nboth settings, we then assign each job to the server with minimum load among\nservers u and v where v is chosen according to one of the following two\npolicies: (i) Unif-POT(k): Sample a server v uniformly at random from k-hop\nneighborhood of u (ii) InvSq-POT(k): Sample a server v from k-hop neighborhood\nof u with probability proportional to the inverse square of the distance\nbetween u and v. Our simulation results show that both the policies\nconsistently produce a load distribution which is much similar to that of a\nclassical proximity oblivious POT policy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 05:15:23 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Panigrahy", "Nitish K.", ""], ["Vasantam", "Thirupathaiah", ""], ["Basu", "Prithwish", ""], ["Towsley", "Don", ""]]}, {"id": "2011.01740", "submitter": "Ferenc Heged\\H{u}s Dr.", "authors": "D\\'aniel Nagy and Lambert Plavecz and Ferenc Heged\\H{u}s", "title": "Solving large number of non-stiff, low-dimensional ordinary differential\n  equation systems on GPUs and CPUs: performance comparisons of MPGOS, ODEINT\n  and DifferentialEquations.jl", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the performance characteristics of different solution\ntechniques and program packages to solve a large number of independent ordinary\ndifferential equation systems is examined. The employed hardware are an Intel\nCore i7-4820K CPU with 30.4 GFLOPS peak double-precision performance per cores\nand an Nvidia GeForce Titan Black GPU that has a total of 1707 GFLOPS peak\ndouble-precision performance. The tested systems (Lorenz equation,\nKeller--Miksis equation and a pressure relief valve model) are non-stiff and\nhave low dimension. Thus, the performance of the codes are not limited by\nmemory bandwidth, and Runge--Kutta type solvers are efficient and suitable\nchoices. The tested program packages are MPGOS written in C++ and specialised\nonly for GPUs; ODEINT implemented in C++, which supports execution on both CPUs\nand GPUs; finally, DifferentialEquations.jl written in Julia that also supports\nexecution on both CPUs and GPUs. Using GPUs, the program package MPGOS is\nsuperior. For CPU computations, the ODEINT program package has the best\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:40:24 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Nagy", "D\u00e1niel", ""], ["Plavecz", "Lambert", ""], ["Heged\u0171s", "Ferenc", ""]]}, {"id": "2011.02327", "submitter": "Huaizheng Zhang", "authors": "Huaizheng Zhang, Yizheng Huang, Yonggang Wen, Jianxiong Yin and Kyle\n  Guan", "title": "InferBench: Understanding Deep Learning Inference Serving with an\n  Automatic Benchmarking System", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) models have become core modules for many applications.\nHowever, deploying these models without careful performance benchmarking that\nconsiders both hardware and software's impact often leads to poor service and\ncostly operational expenditure. To facilitate DL models' deployment, we\nimplement an automatic and comprehensive benchmark system for DL developers. To\naccomplish benchmark-related tasks, the developers only need to prepare a\nconfiguration file consisting of a few lines of code. Our system, deployed to a\nleader server in DL clusters, will dispatch users' benchmark jobs to follower\nworkers. Next, the corresponding requests, workload, and even models can be\ngenerated automatically by the system to conduct DL serving benchmarks.\nFinally, developers can leverage many analysis tools and models in our system\nto gain insights into the trade-offs of different system configurations. In\naddition, a two-tier scheduler is incorporated to avoid unnecessary\ninterference and improve average job compilation time by up to 1.43x\n(equivalent of 30\\% reduction). Our system design follows the best practice in\nDL clusters operations to expedite day-to-day DL service evaluation efforts by\nthe developers. We conduct many benchmark experiments to provide in-depth and\ncomprehensive evaluations. We believe these results are of great values as\nguidelines for DL service configuration and resource allocation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 14:56:57 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 03:21:32 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 05:08:16 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhang", "Huaizheng", ""], ["Huang", "Yizheng", ""], ["Wen", "Yonggang", ""], ["Yin", "Jianxiong", ""], ["Guan", "Kyle", ""]]}, {"id": "2011.02617", "submitter": "Huda Ibeid", "authors": "Gen Xu, Huda Ibeid, Xin Jiang, Vjekoslav Svilan, and Zhaojuan Bian", "title": "Simulation-Based Performance Prediction of HPC Applications: A Case\n  Study of HPL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simulation-based approach for performance modeling of parallel\napplications on high-performance computing platforms. Our approach enables\nfull-system performance modeling: (1) the hardware platform is represented by\nan abstract yet high-fidelity model; (2) the computation and communication\ncomponents are simulated at a functional level, where the simulator allows the\nuse of the components native interface; this results in a (3) fast and accurate\nsimulation of full HPC applications with minimal modifications to the\napplication source code. This hardware/software hybrid modeling methodology\nallows for low overhead, fast, and accurate exascale simulation and can be\neasily carried out on a standard client platform (desktop or laptop). We\ndemonstrate the capability and scalability of our approach with High\nPerformance LINPACK (HPL), the benchmark used to rank supercomputers in the\nTOP500 list. Our results show that our modeling approach can accurately and\nefficiently predict the performance of HPL at the scale of the TOP500 list\nsupercomputers. For instance, the simulation of HPL on Frontera takes less than\nfive hours with an error rate of four percent.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 02:18:04 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Xu", "Gen", ""], ["Ibeid", "Huda", ""], ["Jiang", "Xin", ""], ["Svilan", "Vjekoslav", ""], ["Bian", "Zhaojuan", ""]]}, {"id": "2011.02653", "submitter": "Nitish K Panigrahy", "authors": "Nitish K. Panigrahy, Prithwish Basu, Don Towsley, Ananthram Swami and\n  Kin K. Leung", "title": "On the Analysis of Spatially Constrained Power of Two Choice Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of power of two choice based assignment policies for\nallocating users to servers, where both users and servers are located on a\ntwo-dimensional Euclidean plane. In this framework, we investigate the inherent\ntradeoff between the communication cost, and load balancing performance of\ndifferent allocation policies. To this end, we first design and evaluate a\nSpatial Power of two (sPOT) policy in which each user is allocated to the least\nloaded server among its two geographically nearest servers sequentially. When\nservers are placed on a two-dimensional square grid, sPOT maps to the classical\nPower of two (POT) policy on the Delaunay graph associated with the Voronoi\ntessellation of the set of servers. We show that the associated Delaunay graph\nis 4-regular and provide expressions for asymptotic maximum load using results\nfrom the literature. For uniform placement of servers, we map sPOT to a\nclassical balls and bins allocation policy with bins corresponding to the\nVoronoi regions associated with the second order Voronoi diagram of the set of\nservers. We provide expressions for the lower bound on the asymptotic expected\nmaximum load on the servers and prove that sPOT does not achieve POT load\nbalancing benefits. However, experimental results suggest the efficacy of sPOT\nwith respect to expected communication cost. Finally, we propose two\nnon-uniform server sampling based POT policies that achieve the best of both\nthe performance metrics. Experimental results validate the effctiveness of our\nproposed policies.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 04:19:17 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Panigrahy", "Nitish K.", ""], ["Basu", "Prithwish", ""], ["Towsley", "Don", ""], ["Swami", "Ananthram", ""], ["Leung", "Kin K.", ""]]}, {"id": "2011.03262", "submitter": "Akash Kumar", "authors": "Behnaz Ranjbar, Tuan D.A.Nguyen, Alireza Ejlali, and Akash Kumar", "title": "Power-Aware Run-Time Scheduler for Mixed-Criticality Systems on\n  Multi-Core Platform", "comments": null, "journal-ref": null, "doi": "10.1109/TCAD.2020.3033374", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In modern multi-core Mixed-Criticality (MC) systems, a rise in peak power\nconsumption due to parallel execution of tasks with maximum frequency,\nspecially in the overload situation, may lead to thermal issues, which may\naffect the reliability and timeliness of MC systems. Therefore, managing peak\npower consumption has become imperative in multi-core MC systems. In this\nregard, we propose an online peak power and thermal management heuristic for\nmulti-core MC systems. This heuristic reduces the peak power consumption of the\nsystem as much as possible during runtime by exploiting dynamic slack and\nper-cluster Dynamic Voltage and Frequency Scaling (DVFS). Specifically, our\napproach examines multiple tasks ahead to determine the most appropriate one\nfor slack assignment, that has the most impact on the system peak power and\ntemperature. However, changing the frequency and selecting a proper task for\nslack assignment and a proper core for task re-mapping at runtime can be\ntime-consuming and may cause deadline violation which is not admissible for\nhigh-criticality tasks. Therefore, we analyze and then optimize our run-time\nscheduler and evaluate it for various platforms. The proposed approach is\nexperimentally validated on the ODROID-XU3 (DVFS-enabled heterogeneous\nmulti-core platform) with various embedded real-time benchmarks. Results show\nthat our heuristic achieves up to 5.25% reduction in system peak power and\n20.33\\% reduction in maximum temperature compared to an existing method while\nmeeting deadline constraints in different criticality modes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 10:12:57 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Ranjbar", "Behnaz", ""], ["Nguyen", "Tuan D. A.", ""], ["Ejlali", "Alireza", ""], ["Kumar", "Akash", ""]]}, {"id": "2011.03685", "submitter": "Francesco Vatalaro", "authors": "Giovanni Santella and Francesco Vatalaro", "title": "An approach to define Very High Capacity Networks with improved quality\n  at an affordable cost", "comments": "6 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to propose one possible approach in the setting of VHCNs\n(Very High Capacity Networks) performance targets that should be capable of\npromoting efficient investments for operators and, at the same time, improving\nthe benefits for end-users. To this aim, we suggest relying on some specific\nKPIs (Key Performance Indicators), especially throughput - i.e., the bandwidth\nas perceived by the customer - valid at the application layer, instead of the\nphysical layer data-rate. In this regard, the paper underlines that the\nbandwidth perceived is strictly linked to the latency. The most important\nimplication is that some of the most demanding services envisaged for the\nfuture (e.g., mobile virtual and augmented reality, tactile internet) cannot be\nmet by merely increasing the low-level protocol data-rate. Therefore, for the\nVHCNs reducing latency through Edge Cloud Computing (ECC) is a mandatory\npre-requisite.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 04:23:07 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Santella", "Giovanni", ""], ["Vatalaro", "Francesco", ""]]}, {"id": "2011.03897", "submitter": "Fuxun Yu", "authors": "Fuxun Yu, Zirui Xu, Tong Shen, Dimitrios Stamoulis, Longfei Shangguan,\n  Di Wang, Rishi Madhok, Chunshui Zhao, Xin Li, Nikolaos Karianakis, Dimitrios\n  Lymberopoulos, Ang Li, ChenChen Liu, Yiran Chen, Xiang Chen", "title": "Towards Latency-aware DNN Optimization with GPU Runtime Analysis and\n  Tail Effect Elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the superb performance of State-Of-The-Art (SOTA) DNNs, the\nincreasing computational cost makes them very challenging to meet real-time\nlatency and accuracy requirements. Although DNN runtime latency is dictated by\nmodel property (e.g., architecture, operations), hardware property (e.g.,\nutilization, throughput), and more importantly, the effective mapping between\nthese two, many existing approaches focus only on optimizing model property\nsuch as FLOPS reduction and overlook the mismatch between DNN model and\nhardware properties. In this work, we show that the mismatch between the varied\nDNN computation workloads and GPU capacity can cause the idle GPU tail effect,\nleading to GPU under-utilization and low throughput. As a result, the FLOPs\nreduction cannot bring effective latency reduction, which causes sub-optimal\naccuracy versus latency trade-offs. Motivated by this, we propose a GPU\nruntime-aware DNN optimization methodology to eliminate such GPU tail effect\nadaptively on GPU platforms. Our methodology can be applied on top of existing\nSOTA DNN optimization approaches to achieve better latency and accuracy\ntrade-offs. Experiments show 11%-27% latency reduction and 2.5%-4.0% accuracy\nimprovement over several SOTA DNN pruning and NAS methods, respectively\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 03:19:46 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 21:57:32 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Yu", "Fuxun", ""], ["Xu", "Zirui", ""], ["Shen", "Tong", ""], ["Stamoulis", "Dimitrios", ""], ["Shangguan", "Longfei", ""], ["Wang", "Di", ""], ["Madhok", "Rishi", ""], ["Zhao", "Chunshui", ""], ["Li", "Xin", ""], ["Karianakis", "Nikolaos", ""], ["Lymberopoulos", "Dimitrios", ""], ["Li", "Ang", ""], ["Liu", "ChenChen", ""], ["Chen", "Yiran", ""], ["Chen", "Xiang", ""]]}, {"id": "2011.04275", "submitter": "Angelica Sofia Valeriani", "authors": "Angelica Sofia Valeriani", "title": "Runtime Performances Benchmark for Knowledge Graph Embedding Methods", "comments": "arXiv admin note: text overlap with arXiv:1903.11406,\n  arXiv:2002.00819 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper wants to focus on providing a characterization of the runtime\nperformances of state-of-the-art implementations of KGE alghoritms, in terms of\nmemory footprint and execution time. Despite the rapidly growing interest in\nKGE methods, so far little attention has been devoted to their comparison and\nevaluation; in particular, previous work mainly focused on performance in terms\nof accuracy in specific tasks, such as link prediction. To this extent, a\nframework is proposed for evaluating available KGE implementations against\ngraphs with different properties, with a particular focus on the effectiveness\nof the adopted optimization strategies. Graphs and models have been trained\nleveraging different architectures, in order to enlighten features and\nproperties of both models and the architectures they have been trained on. Some\nresults enlightened with experiments in this document are the fact that\nmultithreading is efficient, but benefit deacreases as the number of threads\ngrows in case of CPU. GPU proves to be the best architecture for the given\ntask, even if CPU with some vectorized instructions still behaves well.\nFinally, RAM utilization for the loading of the graph never changes between\ndifferent architectures and depends only on the type of graph, not on the\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:58:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Valeriani", "Angelica Sofia", ""]]}, {"id": "2011.04654", "submitter": "Yixue Zhao", "authors": "Yixue Zhao, Siwei Yin, Adriana Sejfia, Marcelo Schmitt Laser, Haoyu\n  Wang, Nenad Medvidovic", "title": "Assessing the Feasibility of Web-Request Prediction Models on Mobile\n  Platforms", "comments": "MOBILESoft 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prefetching web pages is a well-studied solution to reduce network latency by\npredicting users' future actions based on their past behaviors. However, such\ntechniques are largely unexplored on mobile platforms. Today's privacy\nregulations make it infeasible to explore prefetching with the usual strategy\nof amassing large amounts of data over long periods and constructing\nconventional, \"large\" prediction models. Our work is based on the observation\nthat this may not be necessary: Given previously reported mobile-device usage\ntrends (e.g., repetitive behaviors in brief bursts), we hypothesized that\nprefetching should work effectively with \"small\" models trained on mobile-user\nrequests collected during much shorter time periods. To test this hypothesis,\nwe constructed a framework for automatically assessing prediction models, and\nused it to conduct an extensive empirical study based on over 15 million HTTP\nrequests collected from nearly 11,500 mobile users during a 24-hour period,\nresulting in over 7 million models. Our results demonstrate the feasibility of\nprefetching with small models on mobile platforms, directly motivating future\nwork in this area. We further introduce several strategies for improving\nprediction models while reducing the model size. Finally, our framework\nprovides the foundation for future explorations of effective prediction models\nacross a range of usage scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 14:15:56 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 01:52:04 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhao", "Yixue", ""], ["Yin", "Siwei", ""], ["Sejfia", "Adriana", ""], ["Laser", "Marcelo Schmitt", ""], ["Wang", "Haoyu", ""], ["Medvidovic", "Nenad", ""]]}, {"id": "2011.04893", "submitter": "Nitish K Panigrahy", "authors": "Nitish K. Panigrahy, Prithwish Basu, Philippe Nain, Don Towsley,\n  Ananthram Swami, Kevin S. Chan and Kin K. Leung", "title": "Resource Allocation in One-dimensional Distributed Service Networks with\n  Applications", "comments": "arXiv admin note: text overlap with arXiv:1901.02414", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider assignment policies that allocate resources to users, where both\nresources and users are located on a one-dimensional line. First, we consider\nunidirectional assignment policies that allocate resources only to users\nlocated to their left. We propose the Move to Right (MTR) policy, which scans\nfrom left to right assigning nearest rightmost available resource to a user,\nand contrast it to the Unidirectional Gale-Shapley (UGS) matching policy. While\nboth policies among all unidirectional policies, minimize the expected distance\ntraveled by a request (request distance), MTR is fairer. Moreover, we show that\nwhen user and resource locations are modeled by statistical point processes,\nand resources are allowed to satisfy more than one user, the spatial system\nunder unidirectional policies can be mapped into bulk service queueing systems,\nthus allowing the application of many queueing theory results that yield closed\nform expressions. As we consider a case where different resources can satisfy\ndifferent numbers of users, we also generate new results for bulk service\nqueues. We also consider bidirectional policies where there are no directional\nrestrictions on resource allocation and develop an algorithm for computing the\noptimal assignment which is more efficient than known algorithms in the\nliterature when there are more resources than users. Numerical evaluation of\nperformance of unidirectional and bidirectional allocation schemes yields\ndesign guidelines beneficial for resource placement. \\np{Finally, we present a\nheuristic algorithm, which leverages the optimal dynamic programming scheme for\none-dimensional inputs to obtain approximate solutions to the optimal\nassignment problem for the two-dimensional scenario and empirically yields\nrequest distances within a constant factor of the optimal solution.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 03:35:00 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Panigrahy", "Nitish K.", ""], ["Basu", "Prithwish", ""], ["Nain", "Philippe", ""], ["Towsley", "Don", ""], ["Swami", "Ananthram", ""], ["Chan", "Kevin S.", ""], ["Leung", "Kin K.", ""]]}, {"id": "2011.05160", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi, Fabrizio Petrini, Hongbo Rong, Andrei Valentin,\n  Carl Ebeling", "title": "Mapping Stencils on Coarse-grained Reconfigurable Spatial Architecture", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencils represent a class of computational patterns where an output grid\npoint depends on a fixed shape of neighboring points in an input grid. Stencil\ncomputations are prevalent in scientific applications engaging a significant\nportion of supercomputing resources. Therefore, it has been always important to\noptimize stencil programs for the best performance. A rich body of research has\nfocused on optimizing stencil computations on almost all parallel\narchitectures. Stencil applications have regular dependency patterns, inherent\npipeline-parallelism, and plenty of data reuse. This makes these applications a\nperfect match for a coarse-grained reconfigurable spatial architecture (CGRA).\nA CGRA consists of many simple, small processing elements (PEs) connected with\nan on-chip network. Each PE can be configured to execute part of a stencil\ncomputation and all PEs run in parallel; the network can also be configured so\nthat data loaded can be passed from a PE to a neighbor PE directly and thus\nreused by many PEs without register spilling and memory traffic. How to\nefficiently map a stencil computation to a CGRA is the key to performance. In\nthis paper, we show a few unique and generalizable ways of mapping one- and\nmultidimensional stencil computations to a CGRA, fully exploiting the data\nreuse opportunities and parallelism. Our simulation experiments demonstrate\nthat these mappings are efficient and enable the CGRA to outperform\nstate-of-the-art GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:51:20 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 18:31:46 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Petrini", "Fabrizio", ""], ["Rong", "Hongbo", ""], ["Valentin", "Andrei", ""], ["Ebeling", "Carl", ""]]}, {"id": "2011.05563", "submitter": "Abhishek  Sinha", "authors": "Abhishek Sinha, Rajarshi Bhattacharjee", "title": "Optimizing the Age-of-Information for Mobile Users in Adversarial and\n  Stochastic Environments", "comments": "arXiv admin note: text overlap with arXiv:2001.05471", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a multi-user downlink scheduling problem for optimizing the\nfreshness of information available to users roaming across multiple cells. We\nconsider both adversarial and stochastic settings and design scheduling\npolicies that optimize two distinct information freshness metrics, namely the\naverage age-of-information and the peak age-of-information. We show that a\nnatural greedy scheduling policy is competitive with the optimal offline policy\nin the adversarial setting. We also derive fundamental lower bounds to the\ncompetitive ratio achievable by any online policy. In the stochastic\nenvironment, we show that a Max-Weight scheduling policy that takes into\naccount the channel statistics achieves an approximation factor of $2$ for\nminimizing the average age of information in two extreme mobility scenarios. We\nconclude the paper by establishing a large-deviation optimality result achieved\nby the greedy policy for minimizing the peak age of information for static\nusers situated at a single cell.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 10:36:48 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Sinha", "Abhishek", ""], ["Bhattacharjee", "Rajarshi", ""]]}, {"id": "2011.05630", "submitter": "Zihan Liu", "authors": "Zihan Liu and Jingwen Leng and Quan Chen and Chao Li and Wenli Zheng\n  and Li Li and Minyi Guo", "title": "DLFusion: An Auto-Tuning Compiler for Layer Fusion on Deep Neural\n  Network Accelerator", "comments": "To be appeared in ISPA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many hardware vendors have introduced specialized deep neural networks (DNN)\naccelerators owing to their superior performance and efficiency. As such, how\nto generate and optimize the code for the hardware accelerator becomes an\nimportant yet less explored problem. In this paper, we perform the\ncompiler-stage optimization study using a novel and representative Cambricon\nDNN accelerator and demonstrate that the code optimization knobs play an\nimportant role in unleashing the potential of hardware computational\nhorsepower. However, even only two studied code optimization knobs, namely the\nnumber of cores and layer fusion scheme, present an enormous search space that\nprevents the naive brute-force search. This work introduces a joint,\nauto-tuning optimization framework to address this challenge. We first use a\nset of synthesized DNN layers to study the interplay between the hardware\nperformance and layer characteristics. Based on the insights, we extract the\noperation count and feature map channel size as each layer's characteristics\nand derive a joint optimization strategy to decide the performance-optimal core\nnumber and fusion scheme. We evaluate the performance of the proposed approach\nusing a set of representative DNN models and show that it achieves the minimal\nof 3.6x and the maximal of 7.9x performance speedup compared to no optimization\nbaseline. We also show that the achieved speedup is close to the oracle case\nthat is based on a reduced brute-force search but with much less search time.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:39:50 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Liu", "Zihan", ""], ["Leng", "Jingwen", ""], ["Chen", "Quan", ""], ["Li", "Chao", ""], ["Zheng", "Wenli", ""], ["Li", "Li", ""], ["Guo", "Minyi", ""]]}, {"id": "2011.06654", "submitter": "Xingfu Wu", "authors": "Xingfu Wu and Valerie Taylor", "title": "Utilizing Ensemble Learning for Performance and Power Modeling and\n  Improvement of Parallel Cancer Deep Learning CANDLE Benchmarks", "comments": "to be published in Cray User Group Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning (ML) continues to grow in importance across nearly all\ndomains and is a natural tool in modeling to learn from data. Often a tradeoff\nexists between a model's ability to minimize bias and variance. In this paper,\nwe utilize ensemble learning to combine linear, nonlinear, and tree-/rule-based\nML methods to cope with the bias-variance tradeoff and result in more accurate\nmodels. Hardware performance counter values are correlated with properties of\napplications that impact performance and power on the underlying system. We use\nthe datasets collected for two parallel cancer deep learning CANDLE benchmarks,\nNT3 (weak scaling) and P1B2 (strong scaling), to build performance and power\nmodels based on hardware performance counters using single-object and\nmultiple-objects ensemble learning to identify the most important counters for\nimprovement. Based on the insights from these models, we improve the\nperformance and energy of P1B2 and NT3 by optimizing the deep learning\nenvironments TensorFlow, Keras, Horovod, and Python under the huge page size of\n8 MB on the Cray XC40 Theta at Argonne National Laboratory. Experimental\nresults show that ensemble learning not only produces more accurate models but\nalso provides more robust performance counter ranking. We achieve up to 61.15%\nperformance improvement and up to 62.58% energy saving for P1B2 and up to\n55.81% performance improvement and up to 52.60% energy saving for NT3 on up to\n24,576 cores.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 21:18:20 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Wu", "Xingfu", ""], ["Taylor", "Valerie", ""]]}, {"id": "2011.06655", "submitter": "Xingfu Wu", "authors": "Xingfu Wu, Valerie Taylor, and Zhiling Lan", "title": "Performance and Power Modeling and Prediction Using MuMMI and Ten\n  Machine Learning Methods", "comments": "to be published in Cray User Group Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we use modeling and prediction tool MuMMI (Multiple Metrics\nModeling Infrastructure) and ten machine learning methods to model and predict\nperformance and power and compare their prediction error rates. We use a\nfault-tolerant linear algebra code and a fault-tolerant heat distribution code\nto conduct our modeling and prediction study on the Cray XC40 Theta and IBM\nBG/Q Mira at Argonne National Laboratory and the Intel Haswell cluster Shepard\nat Sandia National Laboratories. Our experiment results show that the\nprediction error rates in performance and power using MuMMI are less than 10%\nfor most cases. Based on the models for runtime, node power, CPU power, and\nmemory power, we identify the most significant performance counters for\npotential optimization efforts associated with the application characteristics\nand the target architectures, and we predict theoretical outcomes of the\npotential optimizations. When we compare the prediction accuracy using MuMMI\nwith that using 10 machine learning methods, we observe that MuMMI not only\nresults in more accurate prediction in both performance and power but also\npresents how performance counters impact the performance and power models. This\nprovides some insights about how to fine-tune the applications and/or systems\nfor energy efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 21:24:11 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Wu", "Xingfu", ""], ["Taylor", "Valerie", ""], ["Lan", "Zhiling", ""]]}, {"id": "2011.07160", "submitter": "Nan Wu", "authors": "Nan Wu, Pengcheng Li", "title": "Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for\n  Emerging Storage Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AI cs.LG cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With data durability, high access speed, low power efficiency and byte\naddressability, NVMe and SSD, which are acknowledged representatives of\nemerging storage technologies, have been applied broadly in many areas.\nHowever, one key issue with high-performance adoption of these technologies is\nhow to properly define intelligent cache layers such that the performance gap\nbetween emerging technologies and main memory can be well bridged. To this end,\nwe propose Phoebe, a reuse-aware reinforcement learning framework for the\noptimal online caching that is applicable for a wide range of emerging storage\nmodels. By continuous interacting with the cache environment and the data\nstream, Phoebe is capable to extract critical temporal data dependency and\nrelative positional information from a single trace, becoming ever smarter over\ntime. To reduce training overhead during online learning, we utilize periodical\ntraining to amortize costs. Phoebe is evaluated on a set of Microsoft cloud\nstorage workloads. Experiment results show that Phoebe is able to close the gap\nof cache miss rate from LRU and a state-of-the-art online learning based cache\npolicy to the Belady's optimal policy by 70.3% and 52.6%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 22:55:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wu", "Nan", ""], ["Li", "Pengcheng", ""]]}, {"id": "2011.07401", "submitter": "Bai Liu", "authors": "Bai Liu, Qiaomin Xie, Eytan Modiano", "title": "RL-QN: A Reinforcement Learning Framework for Optimal Control of\n  Queueing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advance of information technology, network systems have become\nincreasingly complex and hence the underlying system dynamics are often unknown\nor difficult to characterize. Finding a good network control policy is of\nsignificant importance to achieve desirable network performance (e.g., high\nthroughput or low delay). In this work, we consider using model-based\nreinforcement learning (RL) to learn the optimal control policy for queueing\nnetworks so that the average job delay (or equivalently the average queue\nbacklog) is minimized. Traditional approaches in RL, however, cannot handle the\nunbounded state spaces of the network control problem. To overcome this\ndifficulty, we propose a new algorithm, called Reinforcement Learning for\nQueueing Networks (RL-QN), which applies model-based RL methods over a finite\nsubset of the state space, while applying a known stabilizing policy for the\nrest of the states. We establish that the average queue backlog under RL-QN\nwith an appropriately constructed subset can be arbitrarily close to the\noptimal result. We evaluate RL-QN in dynamic server allocation, routing and\nswitching problems. Simulation results show that RL-QN minimizes the average\nqueue backlog effectively.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 22:12:27 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Liu", "Bai", ""], ["Xie", "Qiaomin", ""], ["Modiano", "Eytan", ""]]}, {"id": "2011.07479", "submitter": "Yin Yang", "authors": "Liang Yang, Yin Yang, Daniel Benevides da Costa, and Imene Trigui", "title": "Performance Analysis of an Interference-Limited RIS-Aided Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, the performance of reconfigurable intelligent surface\n(RIS)-aided communication systems corrupted by the co-channel interference\n(CCI) at the destination is investigated. Assuming Rayleigh fading and\nequal-power CCI, we present the analysis for the outage probability (OP),\naverage bit error rate (BER), and ergodic capacity. In addition, an asymptotic\noutage analysis is carried in order to obtain further insights. Our analysis\nshows that the number of reflecting elements as well as the number of\ninterferers have a great impact on the overall system performance.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 08:32:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yang", "Liang", ""], ["Yang", "Yin", ""], ["da Costa", "Daniel Benevides", ""], ["Trigui", "Imene", ""]]}, {"id": "2011.07968", "submitter": "Ricardo Melo Czekster", "authors": "Ricardo M. Czekster", "title": "Tools for modelling and simulating the Smart Grid", "comments": "16 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Smart Grid (SG) is a Cyber-Physical System (CPS) considered a critical\ninfrastructure divided into cyber (software) and physical (hardware)\ncounterparts that complement each other. It is responsible for timely power\nprovision wrapped by Information and Communication Technologies (ICT) for\nhandling bi-directional energy flows in electric power grids. Enacting control\nand performance over the massive infrastructure of the SG requires convenient\nanalysis methods. Modelling and simulation (M&S) is a performance evaluation\ntechnique used to study virtually any system by testing designs and\nartificially creating 'what-if' scenarios for system reasoning and advanced\nanalysis. M&S avoids stressing the actual physical infrastructure and systems\nin production by addressing the problem in a purely computational perspective.\nPresent work compiles a non-exhaustive list of tools for M&S of interest when\ntackling SG capabilities. Our contribution is to delineate available options\nfor modellers when considering power systems in combination with ICT. We also\nshow the auxiliary tools and details of most relevant solutions pointing out\nmajor features and combinations over the years.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 14:00:12 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 06:17:04 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 06:35:46 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Czekster", "Ricardo M.", ""]]}, {"id": "2011.08141", "submitter": "Masaki Aida", "authors": "Naoki Hirakura, Masaki Aida, Konosuke Kawashima", "title": "A Model of Polarization on Social Media Caused by Empathy and Repulsion", "comments": "5 pages , 4 figures, IEICE ICETC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.PF physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the ease with which social media can be accessed has led to\nthe unexpected problem of a shrinkage in information sources. This phenomenon\nis caused by a system that facilitates the connection of people with similar\nideas and recommendation systems. Bias in the selection of information sources\npromotes polarization that divides people into multiple groups with opposing\nviews and creates conflicts between opposing groups. This paper elucidates the\nmechanism of polarization by proposing a model of opinion formation in social\nmedia that considers users' reactions of empathy and repulsion. Based on the\nidea that opinion neutrality is only relative, this model offers a novel\ntechnology for dealing with polarization.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:08:07 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Hirakura", "Naoki", ""], ["Aida", "Masaki", ""], ["Kawashima", "Konosuke", ""]]}, {"id": "2011.08451", "submitter": "Vignesh Balaji", "authors": "Vignesh Balaji, Brandon Lucia", "title": "Optimizing Graph Processing and Preprocessing with Hardware Assisted\n  Propagation Blocking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extensive prior research has focused on alleviating the characteristic poor\ncache locality of graph analytics workloads. However, graph pre-processing\ntasks remain relatively unexplored. In many important scenarios, graph\npre-processing tasks can be as expensive as the downstream graph analytics\nkernel. We observe that Propagation Blocking (PB), a software optimization\ndesigned for SpMV kernels, generalizes to many graph analytics kernels as well\nas common pre-processing tasks. In this work, we identify the lingering\ninefficiencies of a PB execution on conventional multicores and propose\narchitecture support to eliminate PB's bottlenecks, further improving the\nperformance gains from PB. Our proposed architecture -- COBRA -- optimizes the\nPB execution of both graph processing and pre-processing alike to provide\nend-to-end speedups of up to 4.6x (3.5x on average).\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:13:52 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Balaji", "Vignesh", ""], ["Lucia", "Brandon", ""]]}, {"id": "2011.08781", "submitter": "Erick Carvajal Barboza", "authors": "Erick Carvajal Barboza and Sara Jacob and Mahesh Ketkar and Michael\n  Kishinevsky and Paul Gratz and Jiang Hu", "title": "Automatic Microprocessor Performance Bug Detection", "comments": "14 pages, 13 figures, to appear in the 27th International Symposium\n  on High-Performance Computer Architecture (HPCA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processor design validation and debug is a difficult and complex task, which\nconsumes the lion's share of the design process. Design bugs that affect\nprocessor performance rather than its functionality are especially difficult to\ncatch, particularly in new microarchitectures. This is because, unlike\nfunctional bugs, the correct processor performance of new microarchitectures on\ncomplex, long-running benchmarks is typically not deterministically known.\nThus, when performance benchmarking new microarchitectures, performance teams\nmay assume that the design is correct when the performance of the new\nmicroarchitecture exceeds that of the previous generation, despite significant\nperformance regressions existing in the design. In this work, we present a\ntwo-stage, machine learning-based methodology that is able to detect the\nexistence of performance bugs in microprocessors. Our results show that our\nbest technique detects 91.5% of microprocessor core performance bugs whose\naverage IPC impact across the studied applications is greater than 1% versus a\nbug-free design with zero false positives. When evaluated on memory system\nbugs, our technique achieves 100% detection with zero false positives.\nMoreover, the detection is automatic, requiring very little performance\nengineer time.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:18:45 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 15:39:21 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Barboza", "Erick Carvajal", ""], ["Jacob", "Sara", ""], ["Ketkar", "Mahesh", ""], ["Kishinevsky", "Michael", ""], ["Gratz", "Paul", ""], ["Hu", "Jiang", ""]]}, {"id": "2011.08879", "submitter": "Terry Cojean", "authors": "Terry Cojean, Yu-Hsiang \"Mike\" Tsai, Hartwig Anzt", "title": "Ginkgo -- A Math Library designed for Platform Portability", "comments": "Submitted to Parallel Computing Journal (PARCO)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first associations to software sustainability might be the existence of a\ncontinuous integration (CI) framework; the existence of a testing framework\ncomposed of unit tests, integration tests, and end-to-end tests; and also the\nexistence of software documentation. However, when asking what is a common\ndeathblow for a scientific software product, it is often the lack of platform\nand performance portability. Against this background, we designed the Ginkgo\nlibrary with the primary focus on platform portability and the ability to not\nonly port to new hardware architectures, but also achieve good performance. In\nthis paper we present the Ginkgo library design, radically separating\nalgorithms from hardware-specific kernels forming the distinct hardware\nexecutors, and report our experience when adding execution backends for NVIDIA,\nAMD, and Intel GPUs. We also comment on the different levels of performance\nportability, and the performance we achieved on the distinct hardware backends.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:10:12 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Cojean", "Terry", ""], ["Tsai", "Yu-Hsiang \"Mike\"", ""], ["Anzt", "Hartwig", ""]]}, {"id": "2011.09051", "submitter": "Xiaoqin Yan", "authors": "Liang Yang, Xiaoqin Yan, Sai Li, Daniel Benevides da Costa, and\n  Mohamed-Slim Alouini", "title": "Performance Analysis of Dual-Hop Mixed PLC/RF Communication Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study a dual-hop mixed power line communication and\nradio-frequency communication (PLC/RF) system, where the connection between the\nPLC link and the RF link is made by a decode-and-forward (DF) or\namplify-and-forward (AF) relay. Assume that the PLC channel is affected by both\nadditive background noise and impulsive noise suffers from Log-normal fading,\nwhile the RF link undergoes Rician fading. Based on this model, analytical\nexpressions of the outage probability (OP), average bit error rate (BER), and\nthe average channel capacity (ACC) are derived. Furthermore, an asymptotic\nanalysis for the OP and average BER, as well as an upper bound expression for\nthe ACC are presented. At last, numerical results are developed to validate our\nanalytical results, and in-depth discussions are conducted.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 02:51:31 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Yang", "Liang", ""], ["Yan", "Xiaoqin", ""], ["Li", "Sai", ""], ["da Costa", "Daniel Benevides", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2011.09060", "submitter": "Sai Li", "authors": "Sai Li, Liang Yang, Daniel Benevides da Costa, Marco Di Renzo, and\n  Mohamed-Slim Alouini", "title": "On the Performance of RIS-Assisted Dual-Hop Mixed RF-UWOC Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the performance of a reconfigurable intelligent\nsurface (RIS)-assisted dual-hop mixed radio-frequency underwater wireless\noptical communication (RF-UWOC) system. An RIS is an emerging and low-cost\ntechnology that aims to enhance the strength of the received signal, thus\nimproving the system performance. In the considered system setup, a ground\nsource does not have a reliable direct link to a given marine buoy and\ncommunicates with it through an RIS installed on a building. In particular, the\nbuoy acts as a relay that sends the signal to an underwater destination. In\nthis context, analytical expressions for the outage probability (OP), average\nbit error rate (ABER), and average channel capacity (ACC) are derived assuming\nfixed-gain amplify-and-forward (AF) and decode-and-forward (DF) relaying\nprotocols at the marine buoy. Moreover, asymptotic analyses of the OP and ABER\nare carried out in order to gain further insights from the analytical\nframeworks. In particular, the system diversity order is derived and it is\nshown to depend on the RF link parameters and on the detection schemes of the\nUWOC link. Finally, it is demonstrated that RIS-assisted systems can\neffectively improve the performance of mixed dual-hop RF-UWOC systems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:04:34 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Li", "Sai", ""], ["Yang", "Liang", ""], ["da Costa", "Daniel Benevides", ""], ["Di Renzo", "Marco", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2011.09062", "submitter": "Sai Li", "authors": "Sai Li, Liang Yang, and Daniel Benevides da Costa", "title": "Performance Analysis of UAV-based Mixed RF-UWOC Transmission Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the performance of a mixed\nradio-frequency-underwater wireless optical communication (RF-UWOC) system\nwhere an unmanned aerial vehicle (UAV), as a low-altitude mobile aerial base\nstation, transmits information to an autonomous underwater vehicle (AUV)\nthrough a fixed-gain amplify-and-forward (AF) or decode-and-forward (DF) relay.\nOur analysis accounts for the main factors that affect the system performance,\nsuch as the UAV height, air bubbles, temperature gradient, water salinity\nvariations, and detection techniques. Employing fixed-gain AF relaying and DF\nrelaying, we derive closed-form expressions for some key performance metrics,\ne.g., outage probability (OP), average bit error rate (ABER), and average\nchannel capacity (ACC). In addition, in order to get further insights,\nasymptotic analyses for the OP and ABER are also carried out. Furthermore,\nassuming DF relaying, we derive analytical expressions for the optimal UAV\naltitude that minimizes the OP. Simulation results show that the UAV altitude\ninfluences the system performance and there is an optimal altitude which\nensures a minimum OP. Moreover, based on the asymptotic results, it is\ndemonstrated that the diversity order of fixed-gain AF relaying and DF relaying\nare respectively determined by the RF link and by the detection techniques of\nthe UWOC link.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:15:45 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Li", "Sai", ""], ["Yang", "Liang", ""], ["da Costa", "Daniel Benevides", ""]]}, {"id": "2011.09655", "submitter": "Di Chai", "authors": "Di Chai and Leye Wang and Kai Chen and Qiang Yang", "title": "FedEval: A Benchmark System with a Comprehensive Evaluation Model for\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an innovative solution for privacy-preserving machine learning (ML),\nfederated learning (FL) is attracting much attention from research and industry\nareas. While new technologies proposed in the past few years do evolve the FL\narea, unfortunately, the evaluation results presented in these works fall short\nin integrity and are hardly comparable because of the inconsistent evaluation\nmetrics and the lack of a common platform. In this paper, we propose a\ncomprehensive evaluation framework for FL systems. Specifically, we first\nintroduce the ACTPR model, which defines five metrics that cannot be excluded\nin FL evaluation, including Accuracy, Communication, Time efficiency, Privacy,\nand Robustness. Then we design and implement a benchmarking system called\nFedEval, which enables the systematic evaluation and comparison of existing\nworks under consistent experimental conditions. We then provide an in-depth\nbenchmarking study between the two most widely-used FL mechanisms, FedSGD and\nFedAvg. The benchmarking results show that FedSGD and FedAvg both have\nadvantages and disadvantages under the ACTPR model. For example, FedSGD is\nbarely influenced by the none independent and identically distributed (non-IID)\ndata problem, but FedAvg suffers from a decline in accuracy of up to 9% in our\nexperiments. On the other hand, FedAvg is more efficient than FedSGD regarding\ntime consumption and communication. Lastly, we excavate a set of take-away\nconclusions, which are very helpful for researchers in the FL area.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 04:59:51 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 16:08:13 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Chai", "Di", ""], ["Wang", "Leye", ""], ["Chen", "Kai", ""], ["Yang", "Qiang", ""]]}, {"id": "2011.09676", "submitter": "Benjamin Berg", "authors": "Benjamin Berg, Rein Vesilo, Mor Harchol-Balter", "title": "heSRPT: Parallel Scheduling to Minimize Mean Slowdown", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.09346", "journal-ref": "Performance Evaluation (2020) 102147", "doi": "10.1016/j.peva.2020.102147", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data centers serve workloads which are capable of exploiting\nparallelism. When a job parallelizes across multiple servers it will complete\nmore quickly, but jobs receive diminishing returns from being allocated\nadditional servers. Because allocating multiple servers to a single job is\ninefficient, it is unclear how best to allocate a fixed number of servers\nbetween many parallelizable jobs. This paper provides the first optimal\nallocation policy for minimizing the mean slowdown of parallelizable jobs of\nknown size when all jobs are present at time 0. Our policy provides a simple\nclosed form formula for the optimal allocations at every moment in time.\nMinimizing mean slowdown usually requires favoring short jobs over long ones\n(as in the SRPT policy). However, because parallelizable jobs have sublinear\nspeedup functions, system efficiency is also an issue. System efficiency is\nmaximized by giving equal allocations to all jobs and thus competes with the\ngoal of prioritizing small jobs. Our optimal policy, high-efficiency SRPT\n(heSRPT), balances these competing goals. heSRPT completes jobs according to\ntheir size order, but maintains overall system efficiency by allocating some\nservers to each job at every moment in time. Our results generalize to also\nprovide the optimal allocation policy with respect to mean flow time. Finally,\nwe consider the online case where jobs arrive to the system over time. While\noptimizing mean slowdown in the online setting is even more difficult, we find\nthat heSRPT provides an excellent heuristic policy for the online setting. In\nfact, our simulations show that heSRPT significantly outperforms\nstate-of-the-art allocation policies for parallelizable jobs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:56:10 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Berg", "Benjamin", ""], ["Vesilo", "Rein", ""], ["Harchol-Balter", "Mor", ""]]}, {"id": "2011.09951", "submitter": "Jie Chen", "authors": "Jie Chen", "title": "A Bounded Multi-Vacation Queue Model for Multi-stage Sleep Control 5G\n  Base station", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Modelling and control of energy consumption is an important problem in\ntelecommunication systems.To model such systems, this paper publishes a bounded\nmulti-vacation queue model. The energy consumption predicted by the model shows\nan average error rate of 0.0177 and the delay predicted by the model shows an\naverage error rate of 0.0655 over 99 test instances.Subsequently, an\noptimization algorithm is proposed to minimize the energy consumption while not\nviolate the delay bound. Furthermore, given current state of art 5G base\nstation system configuration, numerical results shows that with the increase of\ntraffic load, energy saving rate becomes less.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 16:51:39 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Chen", "Jie", ""]]}, {"id": "2011.10521", "submitter": "Weina Wang", "authors": "Weina Wang, Qiaomin Xie and Mor Harchol-Balter", "title": "Zero Queueing for Multi-Server Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud computing today is dominated by multi-server jobs. These are jobs that\nrequest multiple servers simultaneously and hold onto all of these servers for\nthe duration of the job. Multi-server jobs add a lot of complexity to the\ntraditional one-job-per-server model: an arrival might not \"fit\" into the\navailable servers and might have to queue, blocking later arrivals and leaving\nservers idle. From a queueing perspective, almost nothing is understood about\nmulti-server job queueing systems; even understanding the exact stability\nregion is a very hard problem.\n  In this paper, we investigate a multi-server job queueing model under scaling\nregimes where the number of servers in the system grows. Specifically, we\nconsider a system with multiple classes of jobs, where jobs from different\nclasses can request different numbers of servers and have different service\ntime distributions, and jobs are served in first-come-first-served order. The\nmulti-server job model opens up new scaling regimes where both the number of\nservers that a job needs and the system load scale with the total number of\nservers. Within these scaling regimes, we derive the first results on\nstability, queueing probability, and the transient analysis of the number of\njobs in the system for each class. In particular we derive sufficient\nconditions for zero queueing. Our analysis introduces a novel way of extracting\ninformation from the Lyapunov drift, which can be applicable to a broader scope\nof problems in queueing systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 17:28:38 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 05:26:25 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Wang", "Weina", ""], ["Xie", "Qiaomin", ""], ["Harchol-Balter", "Mor", ""]]}, {"id": "2011.10760", "submitter": "Sukrit Mittal", "authors": "Sukrit Mittal and Dhish Kumar Saxena and Kalyanmoy Deb and Erik\n  Goodman", "title": "Enhanced Innovized Repair Operator for Evolutionary Multi- and\n  Many-objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": "COIN Lab Report: 2020020", "categories": "cs.NE cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Innovization\" is a task of learning common relationships among some or all\nof the Pareto-optimal (PO) solutions in multi- and many-objective optimization\nproblems. Recent studies have shown that a chronological sequence of\nnon-dominated solutions obtained in consecutive iterations during an\noptimization run also possess salient patterns that can be used to learn\nproblem features to help create new and improved solutions. In this paper, we\npropose a machine-learning- (ML-) assisted modelling approach that learns the\nmodifications in design variables needed to advance population members towards\nthe Pareto-optimal set. We then propose to use the resulting ML model as an\nadditional innovized repair (IR2) operator to be applied on offspring solutions\ncreated by the usual genetic operators, as a novel mean of improving their\nconvergence properties. In this paper, the well-known random forest (RF) method\nis used as the ML model and is integrated with various evolutionary multi- and\nmany-objective optimization algorithms, including NSGA-II, NSGA-III, and\nMOEA/D. On several test problems ranging from two to five objectives, we\ndemonstrate improvement in convergence behaviour using the proposed IR2-RF\noperator. Since the operator does not demand any additional solution\nevaluations, instead using the history of gradual and progressive improvements\nin solutions over generations, the proposed ML-based optimization opens up a\nnew direction of optimization algorithm development with advances in AI and ML\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 10:29:15 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Mittal", "Sukrit", ""], ["Saxena", "Dhish Kumar", ""], ["Deb", "Kalyanmoy", ""], ["Goodman", "Erik", ""]]}, {"id": "2011.10886", "submitter": "Gholamreza Ramezan", "authors": "Gholamreza Ramezan, Cyril Leung, and Chunyan Miao", "title": "Optimal Transaction Queue Waiting in Blockchain Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain systems are being used in a wide range of application domains.\nThey can support trusted transactions in time critical applications. In this\npaper, we study how miners should pick up transactions from a transaction pool\nso as to minimize the average waiting time per transaction. We derive an\nexpression for the average transaction waiting time of the proposed mining\nscheme and determine the optimum decision rule. Numerical results show that the\naverage waiting time per transaction can be reduced by about 10% compared to\nthe traditional no-wait scheme in which miners immediately start the next\nmining round using all transactions waiting in the pool.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 22:39:42 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ramezan", "Gholamreza", ""], ["Leung", "Cyril", ""], ["Miao", "Chunyan", ""]]}, {"id": "2011.10896", "submitter": "Masudul Quraishi", "authors": "Michael Riera, Erfan Bank Tavakoli, Masudul Hassan Quraishi, Fengbo\n  Ren", "title": "HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for\n  Enabling Hardware-agnostic Programming with True Performance Portability for\n  Heterogeneous HPC", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents HALO 1.0, an open-ended extensible multi-agent software\nframework that implements a set of proposed hardware-agnostic accelerator\norchestration (HALO) principles. HALO implements a novel compute-centric\nmessage passing interface (C^2MPI) specification for enabling the\nperformance-portable execution of a hardware-agnostic host application across\nheterogeneous accelerators. The experiment results of evaluating eight widely\nused HPC subroutines based on Intel Xeon E5-2620 CPUs, Intel Arria 10 GX FPGAs,\nand NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified\ncontrol flow for host programs to run across all the computing devices with a\nconsistently top performance portability score, which is up to five orders of\nmagnitude higher than the OpenCL-based solution.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 00:25:55 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 04:27:01 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 00:56:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Riera", "Michael", ""], ["Tavakoli", "Erfan Bank", ""], ["Quraishi", "Masudul Hassan", ""], ["Ren", "Fengbo", ""]]}, {"id": "2011.11759", "submitter": "Baudouin Denis de Senneville PhD", "authors": "Luc Lafitte, R\\'emi Giraud, Cornel Zachiu, Mario Ries, Olivier Sutter,\n  Antoine Petit, Olivier Seror, Clair Poignard, Baudouin Denis de Senneville", "title": "Patch-based field-of-view matching in multi-modal images for\n  electroporation-based ablations", "comments": "22 pages, 9 figures", "journal-ref": "Computerized Medical Imaging and Graphics (2020)", "doi": "10.1016/j.compmedimag.2020.101750", "report-no": null, "categories": "eess.IV cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various multi-modal imaging sensors are currently involved at different steps\nof an interventional therapeutic work-flow. Cone beam computed tomography\n(CBCT), computed tomography (CT) or Magnetic Resonance (MR) images thereby\nprovides complementary functional and/or structural information of the targeted\nregion and organs at risk. Merging this information relies on a correct spatial\nalignment of the observed anatomy between the acquired images. This can be\nachieved by the means of multi-modal deformable image registration (DIR),\ndemonstrated to be capable of estimating dense and elastic deformations between\nimages acquired by multiple imaging devices. However, due to the typically\ndifferent field-of-view (FOV) sampled across the various imaging modalities,\nsuch algorithms may severely fail in finding a satisfactory solution.\n  In the current study we propose a new fast method to align the FOV in\nmulti-modal 3D medical images. To this end, a patch-based approach is\nintroduced and combined with a state-of-the-art multi-modal image similarity\nmetric in order to cope with multi-modal medical images. The occurrence of\nestimated patch shifts is computed for each spatial direction and the shift\nvalue with maximum occurrence is selected and used to adjust the image\nfield-of-view.\n  We show that a regional registration approach using voxel patches provides a\ngood structural compromise between the voxel-wise and \"global shifts\"\napproaches. The method was thereby beneficial for CT to CBCT and MRI to CBCT\nregistration tasks, especially when highly different image FOVs are involved.\nBesides, the benefit of the method for CT to CBCT and MRI to CBCT image\nregistration is analyzed, including the impact of artifacts generated by\npercutaneous needle insertions. Additionally, the computational needs are\ndemonstrated to be compatible with clinical constraints in the practical case\nof on-line procedures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 11:27:45 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Lafitte", "Luc", ""], ["Giraud", "R\u00e9mi", ""], ["Zachiu", "Cornel", ""], ["Ries", "Mario", ""], ["Sutter", "Olivier", ""], ["Petit", "Antoine", ""], ["Seror", "Olivier", ""], ["Poignard", "Clair", ""], ["de Senneville", "Baudouin Denis", ""]]}, {"id": "2011.11840", "submitter": "Omobayode Fagbohungbe", "authors": "Omobayode Fagbohungbe, Lijun Qian", "title": "Benchmarking Inference Performance of Deep Learning Models on Analog\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analog hardware implemented deep learning models are promising for\ncomputation and energy constrained systems such as edge computing devices.\nHowever, the analog nature of the device and the associated many noise sources\nwill cause changes to the value of the weights in the trained deep learning\nmodels deployed on such devices. In this study, systematic evaluation of the\ninference performance of trained popular deep learning models for image\nclassification deployed on analog devices has been carried out, where additive\nwhite Gaussian noise has been added to the weights of the trained models during\ninference. It is observed that deeper models and models with more redundancy in\ndesign such as VGG are more robust to the noise in general. However, the\nperformance is also affected by the design philosophy of the model, the\ndetailed structure of the model, the exact machine learning task, as well as\nthe datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 02:14:39 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 22:04:52 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Fagbohungbe", "Omobayode", ""], ["Qian", "Lijun", ""]]}, {"id": "2011.12092", "submitter": "Ashish Panwar", "authors": "Venkat Sri Sai Ram, Ashish Panwar, Arkaprava Basu", "title": "Leveraging Architectural Support of Three Page Sizes with Trident", "comments": "13 pages, 16 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large pages are commonly deployed to reduce address translation overheads for\nbig-memory workloads. Modern x86-64 processors from Intel and AMD support two\nlarge page sizes -- 1GB and 2MB. However, previous works on large pages have\nprimarily focused on 2MB pages, partly due to lack of substantial evidence on\nthe profitability of 1GB pages to real-world applications. We argue that in\nfact, inadequate system software support is responsible for a decade of\nunderutilized hardware support for 1GB pages.\n  Through extensive experimentation on a real system, we demonstrate that 1GB\npages can improve performance over 2MB pages, and when used in tandem with 2MB\npages for an important set of applications; the support for the latter is\ncrucial but missing in current systems. Our design and implementation of\n\\trident{} in Linux fully exploit hardware supported large pages by dynamically\nand transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable.\n\\trident{} speeds up eight memory-intensive applications by {$18\\%$}, on\naverage, over Linux's use of 2MB pages. We also propose \\tridentpv{}, an\nextension to \\trident{} that effectively virtualizes 1GB pages via copy-less\npromotion and compaction in the guest OS. Overall, this paper shows that even\nGB-sized pages have considerable practical significance with adequate software\nenablement, in turn motivating architects to continue investing/innovating in\nlarge pages.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:54:55 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Ram", "Venkat Sri Sai", ""], ["Panwar", "Ashish", ""], ["Basu", "Arkaprava", ""]]}, {"id": "2011.12644", "submitter": "Luis F. Abanto-Leon", "authors": "Luis F. Abanto-Leon and Andreas Baeuml and Gek Hong (Allyson) Sim and\n  Matthias Hollick and Arash Asadi", "title": "Stay Connected, Leave no Trace: Enhancing Security and Privacy in WiFi\n  via Obfuscating Radiometric Fingerprints", "comments": "ACM Sigmetrics 2021 / In Proc. ACM Meas. Anal. Comput. Syst., Vol. 4,\n  3, Article 44 (December 2020)", "journal-ref": null, "doi": "10.1145/3428329", "report-no": null, "categories": "cs.CR cs.CY cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The intrinsic hardware imperfection of WiFi chipsets manifests itself in the\ntransmitted signal, leading to a unique radiometric fingerprint. This\nfingerprint can be used as an additional means of authentication to enhance\nsecurity. In fact, recent works propose practical fingerprinting solutions that\ncan be readily implemented in commercial-off-the-shelf devices. In this paper,\nwe prove analytically and experimentally that these solutions are highly\nvulnerable to impersonation attacks. We also demonstrate that such a unique\ndevice-based signature can be abused to violate privacy by tracking the user\ndevice, and, as of today, users do not have any means to prevent such privacy\nattacks other than turning off the device.\n  We propose RF-Veil, a radiometric fingerprinting solution that not only is\nrobust against impersonation attacks but also protects user privacy by\nobfuscating the radiometric fingerprint of the transmitter for non-legitimate\nreceivers. Specifically, we introduce a randomized pattern of phase errors to\nthe transmitted signal such that only the intended receiver can extract the\noriginal fingerprint of the transmitter. In a series of experiments and\nanalyses, we expose the vulnerability of adopting naive randomization to\nstatistical attacks and introduce countermeasures. Finally, we show the\nefficacy of RF-Veil experimentally in protecting user privacy and enhancing\nsecurity. More importantly, our proposed solution allows communicating with\nother devices, which do not employ RF-Veil.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:10:59 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 12:25:18 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Abanto-Leon", "Luis F.", "", "Allyson"], ["Baeuml", "Andreas", "", "Allyson"], ["Hong", "Gek", "", "Allyson"], ["Sim", "", ""], ["Hollick", "Matthias", ""], ["Asadi", "Arash", ""]]}, {"id": "2011.12875", "submitter": "Aidan Thompson", "authors": "Rahulkumar Gayatri, Stan Moore, Evan Weinberg, Nicholas Lubbers, Sarah\n  Anderson, Jack Deslippe, Danny Perez, and Aidan P. Thompson", "title": "Rapid Exploration of Optimization Strategies on Advanced Architectures\n  using TestSNAP and LAMMPS", "comments": "Submitted to IPDPS 2021, October 19, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exascale race is at an end with the announcement of the Aurora and\nFrontier machines. This next generation of supercomputers utilize diverse\nhardware architectures to achieve their compute performance, providing an added\nonus on the performance portability of applications. An expanding fragmentation\nof programming models would provide a compounding optimization challenge were\nit not for the evolution of performance-portable frameworks, providing unified\nmodels for mapping abstract hierarchies of parallelism to diverse\narchitectures. A solution to this challenge is the evolution of\nperformance-portable frameworks, providing unified models for mapping abstract\nhierarchies of parallelism to diverse architectures. Kokkos is one such\nperformance portable programming model for C++ applications, providing back-end\nimplementations for each major HPC platform. Even with a performance portable\nframework, restructuring algorithms to expose higher degrees of parallelism is\nnon-trivial. The Spectral Neighbor Analysis Potential (SNAP) is a\nmachine-learned inter-atomic potential utilized in cutting-edge molecular\ndynamics simulations. Previous implementations of the SNAP calculation showed a\ndownward trend in their performance relative to peak on newer-generation CPUs\nand low performance on GPUs. In this paper we describe the restructuring and\noptimization of SNAP as implemented in the Kokkos CUDA backend of the LAMMPS\nmolecular dynamics package, benchmarked on NVIDIA GPUs. We identify novel\npatterns of hierarchical parallelism, facilitating a minimization of memory\naccess overheads and pushing the implementation into a compute-saturated\nregime. Our implementation via Kokkos enables recompile-and-run efficiency on\nupcoming architectures. We find a $\\sim$22x time-to-solution improvement\nrelative to an existing implementation as measured on an NVIDIA Tesla V100-16GB\nfor an important benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:52:08 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Gayatri", "Rahulkumar", ""], ["Moore", "Stan", ""], ["Weinberg", "Evan", ""], ["Lubbers", "Nicholas", ""], ["Anderson", "Sarah", ""], ["Deslippe", "Jack", ""], ["Perez", "Danny", ""], ["Thompson", "Aidan P.", ""]]}, {"id": "2011.13075", "submitter": "Haoxin Wang", "authors": "Haoxin Wang, BaekGyu Kim, Jiang Xie and Zhu Han", "title": "Energy Drain of the Object Detection Processing Pipeline for Mobile\n  Devices: Analysis and Implications", "comments": "This is a personal copy of the authors. Not for redistribution. The\n  final version of this paper was accepted by IEEE Transactions on Green\n  Communications and Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.MM cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Applying deep learning to object detection provides the capability to\naccurately detect and classify complex objects in the real world. However,\ncurrently, few mobile applications use deep learning because such technology is\ncomputation-intensive and energy-consuming. This paper, to the best of our\nknowledge, presents the first detailed experimental study of a mobile augmented\nreality (AR) client's energy consumption and the detection latency of executing\nConvolutional Neural Networks (CNN) based object detection, either locally on\nthe smartphone or remotely on an edge server. In order to accurately measure\nthe energy consumption on the smartphone and obtain the breakdown of energy\nconsumed by each phase of the object detection processing pipeline, we propose\na new measurement strategy. Our detailed measurements refine the energy\nanalysis of mobile AR clients and reveal several interesting perspectives\nregarding the energy consumption of executing CNN-based object detection.\nFurthermore, several insights and research opportunities are proposed based on\nour experimental results. These findings from our experimental study will guide\nthe design of energy-efficient processing pipeline of CNN-based object\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:32:07 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wang", "Haoxin", ""], ["Kim", "BaekGyu", ""], ["Xie", "Jiang", ""], ["Han", "Zhu", ""]]}]