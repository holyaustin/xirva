[{"id": "2006.00508", "submitter": "Prateek Sharma", "authors": "Alexander Fuerst, Ahmed Ali-Eldin, Prashant Shenoy, Prateek Sharma", "title": "Cloud-scale VM Deflation for Running Interactive Applications On\n  Transient Servers", "comments": "To appear at ACM HPDC 2020", "journal-ref": null, "doi": "10.1145/3369583.3392675", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient computing has become popular in public cloud environments for\nrunning delay-insensitive batch and data processing applications at low cost.\nSince transient cloud servers can be revoked at any time by the cloud provider,\nthey are considered unsuitable for running interactive application such as web\nservices. In this paper, we present VM deflation as an alternative mechanism to\nserver preemption for reclaiming resources from transient cloud servers under\nresource pressure. Using real traces from top-tier cloud providers, we show the\nfeasibility of using VM deflation as a resource reclamation mechanism for\ninteractive applications in public clouds. We show how current hypervisor\nmechanisms can be used to implement VM deflation and present cluster deflation\npolicies for resource management of transient and on-demand cloud VMs.\nExperimental evaluation of our deflation system on a Linux cluster shows that\nmicroservice-based applications can be deflated by up to 50\\% with negligible\nperformance overhead. Our cluster-level deflation policies allow overcommitment\nlevels as high as 50\\%, with less than a 1\\% decrease in application\nthroughput, and can enable cloud platforms to increase revenue by 30\\%.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 12:40:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Fuerst", "Alexander", ""], ["Ali-Eldin", "Ahmed", ""], ["Shenoy", "Prashant", ""], ["Sharma", "Prateek", ""]]}, {"id": "2006.00515", "submitter": "Mariska Heemskerk", "authors": "M. Heemskerk, M. Mandjes and B. Mathijsen", "title": "Staffing for many-server systems facing non-standard arrival processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arrival processes to service systems often display (i) larger than\nanticipated fluctuations, (ii) a time-varying rate, and (iii) temporal\ncorrelation. Motivated by this, we introduce a specific non-homogeneous Poisson\nprocess that incorporates these three features. The resulting arrival process\nis fed into an infinite-server system, which is then used as a proxy for its\nmany-server counterpart. This leads to a staffing rule based on the square-root\nstaffing principle that acknowledges the three features. After a slight\nrearrangement of servers over the time slots, we succeed to stabilize system\nperformance even under highly varying and strongly correlated conditions. We\nfit the arrival stream model to real data from an emergency department and\ndemonstrate (by simulation) the performance of the novel staffing rule.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 13:09:31 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Heemskerk", "M.", ""], ["Mandjes", "M.", ""], ["Mathijsen", "B.", ""]]}, {"id": "2006.01991", "submitter": "Saeid Tizpaz-Niari", "authors": "Saeid Tizpaz-Niari and Pavol Cern\\'y and Ashutosh Trivedi", "title": "Detecting and Understanding Real-World Differential Performance Bugs in\n  Machine Learning Libraries", "comments": "To appear in ISSTA'20, 11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming errors that degrade the performance of systems are widespread,\nyet there is little tool support for analyzing these bugs. We present a method\nbased on differential performance analysis---we find inputs for which the\nperformance varies widely, despite having the same size. To ensure that the\ndifferences in the performance are robust (i.e. hold also for large inputs), we\ncompare the performance of not only single inputs, but of classes of inputs,\nwhere each class has similar inputs parameterized by their size. Thus, each\nclass is represented by a performance function from the input size to\nperformance. Importantly, we also provide an explanation for why the\nperformance differs in a form that can be readily used to fix a performance\nbug.\n  The two main phases in our method are discovery with fuzzing and explanation\nwith decision tree classifiers, each of which is supported by clustering.\nFirst, we propose an evolutionary fuzzing algorithm to generate inputs. For\nthis fuzzing task, the unique challenge is that we not only need the input\nclass with the worst performance, but rather a set of classes exhibiting\ndifferential performance. We use clustering to merge similar input classes\nwhich significantly improves the efficiency of our fuzzer. Second, we explain\nthe differential performance in terms of program inputs and internals. We adapt\ndiscriminant learning approaches with clustering and decision trees to localize\nsuspicious code regions.\n  We applied our techniques to a set of applications. On a set of\nmicro-benchmarks, we show that our approach outperforms state-of-the-art\nfuzzers in finding inputs to characterize the differential performance. On a\nset of case-studies, we discover and explain multiple performance bugs in\npopular machine learning frameworks. Four of these bugs, reported first in this\npaper, have since been fixed by the developers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 00:23:06 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Tizpaz-Niari", "Saeid", ""], ["Cern\u00fd", "Pavol", ""], ["Trivedi", "Ashutosh", ""]]}, {"id": "2006.02055", "submitter": "Mohsen Amini Salehi", "authors": "Davood Ghatreh Samani, Chavit Denninnart, Josef Bacik, Mohsen Amini\n  Salehi", "title": "The Art of CPU-Pinning: Evaluating and Improving the Performance of\n  Virtualization and Containerization Platforms", "comments": null, "journal-ref": "The 49th International Conference on Parallel Processing (ICPP\n  2020)", "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud providers offer a variety of execution platforms in form of bare-metal,\nVM, and containers. However, due to the pros and cons of each execution\nplatform, choosing the appropriate platform for a specific cloud-based\napplication has become a challenge for solution architects. The possibility to\ncombine these platforms (e.g. deploying containers within VMs) offers new\ncapacities that makes the challenge even further complicated. However, there is\na little study in the literature on the pros and cons of deploying different\napplication types on various execution platforms. In particular, evaluation of\ndiverse hardware configurations and different CPU provisioning methods, such as\nCPU pinning, have not been sufficiently studied in the literature. In this\nwork, the performance overhead of container, VM, and bare-metal execution\nplatforms are measured and analyzed for four categories of real-world\napplications, namely video processing, parallel processing (MPI), web\nprocessing, and No-SQL, respectively representing CPU intensive, parallel\nprocessing, and two IO intensive processes. Our analyses reveal a set of\ninteresting and sometimes counterintuitive findings that can be used as best\npractices by the solution architects to efficiently deploy cloud-based\napplications. Here are some notable mentions: (A) Under specific circumstances,\ncontainers can impose a higher overhead than VMs; (B) Containers on top of VMs\ncan mitigate the overhead of VMs for certain applications; (C) Containers with\na large number of cores impose a lower overhead than those with a few cores.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 05:47:14 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Samani", "Davood Ghatreh", ""], ["Denninnart", "Chavit", ""], ["Bacik", "Josef", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2006.02155", "submitter": "Brian Kroth", "authors": "Carlo Curino, Neha Godwal, Brian Kroth, Sergiy Kuryata, Greg Lapinski,\n  Siqi Liu, Slava Oks, Olga Poppe, Adam Smiechowski, Ed Thayer, Markus Weimer,\n  Yiwen Zhu", "title": "MLOS: An Infrastructure for Automated Software Performance Engineering", "comments": "4 pages, DEEM 2020", "journal-ref": null, "doi": "10.1145/3399579.3399927", "report-no": null, "categories": "cs.DC cs.DB cs.LG cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing modern systems software is a complex task that combines business\nlogic programming and Software Performance Engineering (SPE). The later is an\nexperimental and labor-intensive activity focused on optimizing the system for\na given hardware, software, and workload (hw/sw/wl) context.\n  Today's SPE is performed during build/release phases by specialized teams,\nand cursed by: 1) lack of standardized and automated tools, 2) significant\nrepeated work as hw/sw/wl context changes, 3) fragility induced by a\n\"one-size-fit-all\" tuning (where improvements on one workload or component may\nimpact others). The net result: despite costly investments, system software is\noften outside its optimal operating point - anecdotally leaving 30% to 40% of\nperformance on the table.\n  The recent developments in Data Science (DS) hints at an opportunity:\ncombining DS tooling and methodologies with a new developer experience to\ntransform the practice of SPE. In this paper we present: MLOS, an ML-powered\ninfrastructure and methodology to democratize and automate Software Performance\nEngineering. MLOS enables continuous, instance-level, robust, and trackable\nsystems optimization. MLOS is being developed and employed within Microsoft to\noptimize SQL Server performance. Early results indicated that component-level\noptimizations can lead to 20%-90% improvements when custom-tuning for a\nspecific hw/sw/wl, hinting at a significant opportunity. However, several\nresearch challenges remain that will require community involvement. To this\nend, we are in the process of open-sourcing the MLOS core infrastructure, and\nwe are engaging with academic institutions to create an educational program\naround Software 2.0 and MLOS ideas.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 22:38:30 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 11:10:53 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Curino", "Carlo", ""], ["Godwal", "Neha", ""], ["Kroth", "Brian", ""], ["Kuryata", "Sergiy", ""], ["Lapinski", "Greg", ""], ["Liu", "Siqi", ""], ["Oks", "Slava", ""], ["Poppe", "Olga", ""], ["Smiechowski", "Adam", ""], ["Thayer", "Ed", ""], ["Weimer", "Markus", ""], ["Zhu", "Yiwen", ""]]}, {"id": "2006.02318", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far and Emina Soljanin", "title": "Efficient Replication for Straggler Mitigation in Distributed Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Master-worker distributed computing systems use task replication in order to\nmitigate the effect of slow workers, known as stragglers. Tasks are grouped\ninto batches and assigned to one or more workers for execution. We first\nconsider the case when the batches do not overlap and, using the results from\nmajorization theory, show that, for a general class of workers' service time\ndistributions, a balanced assignment of batches to workers minimizes the\naverage job compute time. We next show that this balanced assignment of\nnon-overlapping batches achieves lower average job compute time compared to the\noverlapping schemes proposed in the literature. Furthermore, we derive the\noptimum redundancy level as a function of the service time distribution at\nworkers. We show that the redundancy level that minimizes average job compute\ntime is not necessarily the same as the redundancy level that maximizes the\npredictability of job compute time, and thus there exists a trade-off between\noptimizing the two metrics. Finally, by running experiments on Google cluster\ntraces, we observe that redundancy can reduce the compute time of the jobs in\nGoogle clusters by an order of magnitude, and that the optimum level of\nredundancy depends on the distribution of tasks' service time.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 15:05:04 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 15:42:30 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""]]}, {"id": "2006.02602", "submitter": "Weicheng Xue", "authors": "Weicheng Xue and Christopher J. Roy", "title": "Multi-GPU Performance Optimization of a CFD Code using OpenACC on\n  Different Platforms", "comments": null, "journal-ref": null, "doi": "10.1002/cpe.6036", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the multi-GPU performance of a 3D buoyancy driven\ncavity solver using MPI and OpenACC directives on different platforms. The\npaper shows that decomposing the total problem in different dimensions affects\nthe strong scaling performance significantly for the GPU. Without proper\nperformance optimizations, it is shown that 1D domain decomposition scales\npoorly on multiple GPUs due to the noncontiguous memory access. The performance\nusing whatever decompositions can be benefited from a series of performance\noptimizations in the paper. Since the buoyancy driven cavity code is\nlatency-bounded on the clusters examined, a series of optimizations both\nagnostic and tailored to the platforms are designed to reduce the latency cost\nand improve memory throughput between hosts and devices efficiently. First, the\nparallel message packing/unpacking strategy developed for noncontiguous data\nmovement between hosts and devices improves the overall performance by about a\nfactor of 2. Second, transferring different data based on the stencil sizes for\ndifferent variables further reduces the communication overhead. These two\noptimizations are general enough to be beneficial to stencil computations\nhaving ghost changes on all of the clusters tested. Third, GPUDirect is used to\nimprove the communication on clusters which have the hardware and software\nsupport for direct communication between GPUs without staging CPU's memory.\nFinally, overlapping the communication and computations is shown to be not\nefficient on multi-GPUs if only using MPI or MPI+OpenACC. Although we believe\nour implementation has revealed enough overlap, the actual running does not\nutilize the overlap well due to a lack of asynchronous progression.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 00:57:48 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Xue", "Weicheng", ""], ["Roy", "Christopher J.", ""]]}, {"id": "2006.03044", "submitter": "Sam Werner", "authors": "Dragos I. Ilie, Sam M. Werner, Iain Stewart, William J. Knottenbelt", "title": "Unstable Throughput: When the Difficulty Algorithm Breaks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Proof-of-Work blockchains, difficulty algorithms serve the crucial purpose\nof maintaining a stable transaction throughput by dynamically adjusting the\nblock difficulty in response to the miners' constantly changing computational\npower. Blockchains that may experience severe hash rate fluctuations need\ndifficulty algorithms that quickly adapt the mining difficulty. However,\nwithout careful design, the system could be gamed by miners using coin-hopping\nstrategies to manipulate the block difficulty for profit. Such miner behavior\nresults in an unreliable system due to the unstable processing of transactions.\n  We provide an empirical analysis of how Bitcoin Cash's difficulty algorithm\ndesign leads to cyclicality in block solve times as a consequence of a positive\nfeedback loop. In response, we mathematically derive a difficulty algorithm\nusing a negative exponential filter which prohibits the formation of positive\nfeedback and exhibits additional desirable properties, such as history\nagnosticism. We compare the described algorithm to that of Bitcoin Cash in a\nsimulated mining environment and verify that the former would eliminate the\nsevere oscillations in transaction throughput.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:53:14 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 14:43:39 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 10:21:09 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Ilie", "Dragos I.", ""], ["Werner", "Sam M.", ""], ["Stewart", "Iain", ""], ["Knottenbelt", "William J.", ""]]}, {"id": "2006.03318", "submitter": "Hongyu Zhu", "authors": "Hongyu Zhu, Amar Phanishayee, Gennady Pekhimenko", "title": "Daydream: Accurately Estimating the Efficacy of Optimizations for DNN\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural network (DNN) training jobs use complex and heterogeneous\nsoftware/hardware stacks. The efficacy of software-level optimizations can vary\nsignificantly when used in different deployment configurations. It is onerous\nand error-prone for ML practitioners and system developers to implement each\noptimization separately, and determine which ones will improve performance in\ntheir own configurations. Unfortunately, existing profiling tools do not aim to\nanswer predictive questions such as \"How will optimization X affect the\nperformance of my model?\". We address this critical limitation, and proposes a\nnew profiling tool, Daydream, to help programmers efficiently explore the\nefficacy of DNN optimizations. Daydream models DNN execution with a\nfine-grained dependency graph based on low-level traces collected by CUPTI, and\npredicts runtime by simulating execution based on the dependency graph.\nDaydream maps the low-level traces using DNN domain-specific knowledge, and\nintroduces a set of graph-transformation primitives that can easily model a\nwide variety of optimizations. We show that Daydream is able to model most\nmainstream DNN optimization techniques, and accurately predict the efficacy of\noptimizations that will result in significant performance improvements.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 09:08:16 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Zhu", "Hongyu", ""], ["Phanishayee", "Amar", ""], ["Pekhimenko", "Gennady", ""]]}, {"id": "2006.03404", "submitter": "Naoto Miyoshi", "authors": "Yukang Jiang and Naoto Miyoshi", "title": "Joint performance analysis of ages of information in a multi-source\n  pushout server", "comments": "22pages, 2 figures (2 subfigures in each)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age of information (AoI) has been widely accepted as a measure quantifying\nfreshness of status information in real-time status update systems. In many of\nsuch systems, multiple sources share a limited network resource and therefore\nthe AoIs defined for the individual sources should be correlated with each\nother. However, there are not found any results studying the correlation of two\nor more AoIs in a status update system with multiple sources. In this work, we\nconsider a multi-source system sharing a common service facility and provide a\nframework to investigate joint performance of the multiple AoIs. We then apply\nour framework to a simple pushout server with multiple sources and derive a\nclosed-form formula of the joint Laplace transform of the AoIs in the case with\nindependent M/G inputs. We further show some properties of the correlation\ncoefficient of AoIs in the two-source system.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 12:33:05 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Jiang", "Yukang", ""], ["Miyoshi", "Naoto", ""]]}, {"id": "2006.03616", "submitter": "Khaza Anuarul Hoque", "authors": "Shamik Kundu, Ahmet Soyyi\\u{g}it, Khaza Anuarul Hoque and Kanad Basu", "title": "High-level Modeling of Manufacturing Faults in Deep Neural Network\n  Accelerators", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": "10.1109/IOLTS50870.2020.9159704", "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of data-driven real-time applications requires the implementation\nof Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's\nTensor Processing Unit (TPU) is one such neural network accelerator that uses\nsystolic array-based matrix multiplication hardware for computation in its\ncrux. Manufacturing faults at any state element of the matrix multiplication\nunit can cause unexpected errors in these inference networks. In this paper, we\npropose a formal model of permanent faults and their propagation in a TPU using\nthe Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed\nusing the probabilistic model checking technique to reason about the likelihood\nof faulty outputs. The obtained quantitative results show that the\nclassification accuracy is sensitive to the type of permanent faults as well as\ntheir location, bit position and the number of layers in the neural network.\nThe conclusions from our theoretical model have been validated using\nexperiments on a digit recognition-based DNN.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 18:11:14 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 15:31:56 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kundu", "Shamik", ""], ["Soyyi\u011fit", "Ahmet", ""], ["Hoque", "Khaza Anuarul", ""], ["Basu", "Kanad", ""]]}, {"id": "2006.04658", "submitter": "Diego Didona Dr", "authors": "Diego Didona, Nikolas Ioannou, Radu Stoica, Kornilios Kourtis", "title": "Toward a Better Understanding and Evaluation of Tree Structures on Flash\n  SSDs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solid-state drives (SSDs) are extensively used to deploy persistent data\nstores, as they provide low latency random access, high write throughput, high\ndata density, and low cost. Tree-based data structures are widely used to build\npersistent data stores, and indeed they lie at the backbone of many of the data\nmanagement systems used in production and research today. In this paper, we\nshow that benchmarking a persistent tree-based data structure on an SSD is a\ncomplex process, which may easily incur subtle pitfalls that can lead to an\ninaccurate performance assessment. At a high-level, these pitfalls stem from\nthe interaction of complex software running on complex hardware. On one hand,\ntree structures implement internal operations that have nontrivial effects on\nperformance. On the other hand, SSDs employ firmware logic to deal with the\nidiosyncrasies of the underlying flash memory, which are well known to lead to\ncomplex performance dynamics. We identify seven benchmarking pitfalls using\nRocksDB and WiredTiger, two widespread implementations of an LSM-Tree and a\nB+Tree, respectively. We show that such pitfalls can lead to incorrect\nmeasurements of key performance indicators, hinder the reproducibility and the\nrepresentativeness of the results, and lead to suboptimal deployments in\nproduction environments. We also provide guidelines on how to avoid these\npitfalls to obtain more reliable performance measurements, and to perform more\nthorough and fair comparison among different design points.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:03:09 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Didona", "Diego", ""], ["Ioannou", "Nikolas", ""], ["Stoica", "Radu", ""], ["Kourtis", "Kornilios", ""]]}, {"id": "2006.04969", "submitter": "Heiko Hamann", "authors": "Heiko Hamann and Andreagiovanni Reina", "title": "Scalability in Computing and Robotics", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TC.2021.3089044", "report-no": null, "categories": "cs.DC cs.MA cs.PF cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient engineered systems require scalability. A scalable system has\nincreasing performance with increasing system size. In an ideal case, the\nincrease in performance (e.g., speedup) corresponds to the number of units that\nare added to the system. However, if multiple units work on the same task, then\ncoordination among these units is required. This coordination can introduce\noverheads with an impact on system performance. The coordination costs can lead\nto sublinear improvement or even diminishing performance with increasing system\nsize. However, there are also systems that implement efficient coordination and\nexploit collaboration of units to attain superlinear improvement. Modeling the\nscalability dynamics is key to understanding efficient systems. Known laws of\nscalability, such as Amdahl's law, Gustafson's law, and Gunther's Universal\nScalability Law, are minimalistic phenomenological models that explain a rich\nvariety of system behaviors through concise equations. While useful to gain\ngeneral insights, the phenomenological nature of these models may limit the\nunderstanding of the underlying dynamics, as they are detached from first\nprinciples that could explain coordination overheads among units. Through a\ndecentralized system approach, we propose a general model based on generic\ninteractions between units that is able to describe, as specific cases, any\ngeneral pattern of scalability included by previously reported laws. The\nproposed general model of scalability is built on first principles, or at least\non a microscopic description of interaction between units, and therefore has\nthe potential to contribute to a better understanding of system behavior and\nscalability. We show that this model can be applied to a diverse set of\nsystems, such as parallel supercomputers, robot swarms, or wireless sensor\nnetworks, creating a unified view on interdisciplinary design for scalability.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 22:28:59 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:25:50 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hamann", "Heiko", ""], ["Reina", "Andreagiovanni", ""]]}, {"id": "2006.05503", "submitter": "Vineet Sahula", "authors": "Ulhas Deshmukh and Vineet Sahula", "title": "Stochastic Automata Network for Performance Evaluation of Heterogeneous\n  SoC Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet ever increasing demand for performance of emerging System-on-Chip\n(SoC) applications, designer employ techniques for concurrent communication\nbetween components. Hence communication architecture becomes complex and major\nperformance bottleneck. An early performance evaluation of communication\narchitecture is the key to reduce design time, time-to-market and consequently\ncost of the system. Moreover, it helps to optimize system performance by\nselecting appropriate communication architecture. However, performance model of\nconcurrent communication is complex to describe and hard to solve. In this\npaper, we propose methodology for performance evaluation of bus based\ncommunication architectures, modeling for which is based on modular Stochastic\nAutomata Network (SAN). We employ Generalized Semi Markov Process (GSMP) model\nfor each module of the SAN that emulates dynamic behavior of a Processing\nElement (PE) of an SoC architecture. The proposed modeling approach provides an\nearly estimation of performance parameters viz. memory bandwidth, average queue\nlength at memory and average waiting time seen by a processing element; while\nwe provide parameters viz. number of processing elements, the mean computation\ntime of processing elements and the first and second moments of connection time\nbetween processing elements and memories, as input to the model.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 07:50:59 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Deshmukh", "Ulhas", ""], ["Sahula", "Vineet", ""]]}, {"id": "2006.05979", "submitter": "Kristen Gardner", "authors": "Kristen Gardner and Rhonda Righter", "title": "Product Forms for FCFS Queueing Models with Arbitrary Server-Job\n  Compatibilities: An Overview", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years a number of models involving different compatibilities\nbetween jobs and servers in queueing systems, or between agents and resources\nin matching systems, have been studied, and, under Markov assumptions and\nappropriate stability conditions, the stationary distributions have been shown\nto have product forms. We survey these results and show how, under an\nappropriate detailed description of the state, many are corollaries of similar\nresults for the Order Independent Queue. We also discuss how to use the product\nform results to determine distributions for steady-state response times.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 17:49:16 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gardner", "Kristen", ""], ["Righter", "Rhonda", ""]]}, {"id": "2006.06632", "submitter": "Wenxin Li", "authors": "Wenxin Li", "title": "Performance Analysis of Modified SRPT in Multiple-Processor Multitask\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the multiple-processor multitask scheduling problem in\nboth deterministic and stochastic models. We consider and analyze Modified\nShortest Remaining Processing Time (M-SRPT) scheduling algorithm, a simple\nmodification of SRPT, which always schedules jobs according to SRPT whenever\npossible, while processes tasks in an arbitrary order. The M-SRPT algorithm is\nproved to achieve a competitive ratio of $\\Theta(\\log \\alpha +\\beta)$ for\nminimizing response time, where $\\alpha$ denotes the ratio between maximum job\nworkload and minimum job workload, $\\beta$ represents the ratio between maximum\nnon-preemptive task workload and minimum job workload. In addition, the\ncompetitive ratio achieved is shown to be optimal (up to a constant factor),\nwhen there are constant number of machines. We further consider the problem\nunder Poisson arrival and general workload distribution (\\ie, $M/GI/N$ system),\nand show that M-SRPT achieves asymptotic optimal mean response time when the\ntraffic intensity $\\rho$ approaches $1$, if job size distribution has finite\nsupport. Beyond finite job workload, the asymptotic optimality of M-SRPT also\nholds for infinite job size distributions with certain probabilistic\nassumptions, for example, $M/M/N$ system with finite task workload.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:27:00 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 12:17:28 GMT"}, {"version": "v3", "created": "Sat, 3 Oct 2020 07:40:29 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2020 11:08:50 GMT"}, {"version": "v5", "created": "Sat, 24 Oct 2020 07:03:28 GMT"}, {"version": "v6", "created": "Fri, 15 Jan 2021 08:36:09 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Li", "Wenxin", ""]]}, {"id": "2006.06762", "submitter": "Lianmin Zheng", "authors": "Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer\n  Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E. Gonzalez,\n  Ion Stoica", "title": "Ansor : Generating High-Performance Tensor Programs for Deep Learning", "comments": "Published in OSDI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PF cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance tensor programs are crucial to guarantee efficient execution\nof deep neural networks. However, obtaining performant tensor programs for\ndifferent operators on various hardware platforms is notoriously challenging.\nCurrently, deep learning systems rely on vendor-provided kernel libraries or\nvarious search strategies to get performant tensor programs. These approaches\neither require significant engineering effort to develop platform-specific\noptimization code or fall short of finding high-performance programs due to\nrestricted search space and ineffective exploration strategy.\n  We present Ansor, a tensor program generation framework for deep learning\napplications. Compared with existing search strategies, Ansor explores many\nmore optimization combinations by sampling programs from a hierarchical\nrepresentation of the search space. Ansor then fine-tunes the sampled programs\nwith evolutionary search and a learned cost model to identify the best\nprograms. Ansor can find high-performance programs that are outside the search\nspace of existing state-of-the-art approaches. In addition, Ansor utilizes a\ntask scheduler to simultaneously optimize multiple subgraphs in deep neural\nnetworks. We show that Ansor improves the execution performance of deep neural\nnetworks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA\nGPU by up to $3.8\\times$, $2.6\\times$, and $1.7\\times$, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:40:09 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:21:22 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 18:32:44 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 07:29:29 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Zheng", "Lianmin", ""], ["Jia", "Chengfan", ""], ["Sun", "Minmin", ""], ["Wu", "Zhao", ""], ["Yu", "Cody Hao", ""], ["Haj-Ali", "Ameer", ""], ["Wang", "Yida", ""], ["Yang", "Jun", ""], ["Zhuo", "Danyang", ""], ["Sen", "Koushik", ""], ["Gonzalez", "Joseph E.", ""], ["Stoica", "Ion", ""]]}, {"id": "2006.09167", "submitter": "Szil\\'ard P\\'all", "authors": "Szil\\'ard P\\'all, Artem Zhmurov, Paul Bauer, Mark Abraham, Magnus\n  Lundborg, Alan Gray, Berk Hess, Erik Lindahl", "title": "Heterogeneous Parallelization and Acceleration of Molecular Dynamics\n  Simulations in GROMACS", "comments": "The following article has been submitted to the Journal of Chemical\n  Physics", "journal-ref": null, "doi": "10.1063/5.0018516", "report-no": null, "categories": "physics.comp-ph cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The introduction of accelerator devices such as graphics processing units\n(GPUs) has had profound impact on molecular dynamics simulations and has\nenabled order-of-magnitude performance advances using commodity hardware. To\nfully reap these benefits, it has been necessary to reformulate some of the\nmost fundamental algorithms, including the Verlet list, pair searching and\ncut-offs. Here, we present the heterogeneous parallelization and acceleration\ndesign of molecular dynamics implemented in the GROMACS codebase over the last\ndecade. The setup involves a general cluster-based approach to pair lists and\nnon-bonded pair interactions that utilizes both GPUs and CPU SIMD acceleration\nefficiently, including the ability to load-balance tasks between CPUs and GPUs.\nThe algorithm work efficiency is tuned for each type of hardware, and to use\naccelerators more efficiently we introduce dual pair lists with rolling pruning\nupdates. Combined with new direct GPU-GPU communication as well as GPU\nintegration, this enables excellent performance from single GPU simulations\nthrough strong scaling across multiple GPUs and efficient multi-node\nparallelization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:19:26 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 22:54:40 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["P\u00e1ll", "Szil\u00e1rd", ""], ["Zhmurov", "Artem", ""], ["Bauer", "Paul", ""], ["Abraham", "Mark", ""], ["Lundborg", "Magnus", ""], ["Gray", "Alan", ""], ["Hess", "Berk", ""], ["Lindahl", "Erik", ""]]}, {"id": "2006.09473", "submitter": "Kewen Meng", "authors": "Kewen Meng and Boyana Norris", "title": "Guiding Optimizations with Meliora: A Deep Walk down Memory Lane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance models can be very useful for understanding the behavior of\napplications and hence can help guide design and optimization decisions.\nUnfortunately, performance modeling of nontrivial computations typically\nrequires significant expertise and human effort. Moreover, even when performed\nby experts, it is necessarily limited in scope, accuracy, or both. However,\nsince models are not typically available, programmers, compilers or autotuners\ncannot use them easily to guide optimizations and are limited to\nheuristic-based methods that potentially take a lot of time to perform\nunnecessary transformations. We believe that streamlining model generation and\nmaking it scalable (both in terms of human effort and code size) would enable\ndramatic improvements in compilation techniques, as well as manual optimization\nand autotuning. To that end, we are building the Meliora code analysis\ninfrastructure for machine learning-based performance model generation of\narbitrary codes based on static analysis of intermediate language\nrepresentations. We demonstrate good accuracy in matching known codes and show\nhow Meliora can be used to optimize new codes though reusing optimization\nknowledge, either manually or in conjunction with an autotuner. When\nautotuning, Meliora eliminates or dramatically reduces the empirical search\nspace, while generally achieving competitive performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 03:59:31 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Meng", "Kewen", ""], ["Norris", "Boyana", ""]]}, {"id": "2006.13987", "submitter": "Kristen Gardner", "authors": "Kristen Gardner, Jazeem Abdul Jaleel, Alexander Wickeham, Sherwin\n  Doroudi", "title": "Scalable Load Balancing in the Presence of Heterogeneous Servers", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity is becoming increasingly ubiquitous in modern large-scale\ncomputer systems. Developing good load balancing policies for systems whose\nresources have varying speeds is crucial in achieving low response times.\nIndeed, how best to dispatch jobs to servers is a classical and well-studied\nproblem in the queueing literature. Yet the bulk of existing work on\nlarge-scale systems assumes homogeneous servers; unfortunately, policies that\nperform well in the homogeneous setting can cause unacceptably poor\nperformance---or even instability---in heterogeneous systems.\n  We adapt the \"power-of-d\" versions of both the Join-the-Idle-Queue and\nJoin-the-Shortest-Queue policies to design two corresponding families of\nheterogeneity-aware dispatching policies, each of which is parameterized by a\npair of routing probabilities. Unlike their heterogeneity-unaware counterparts,\nour policies use server speed information both when choosing which servers to\nquery and when probabilistically deciding where (among the queried servers) to\ndispatch jobs. Both of our policy families are analytically tractable: our mean\nresponse time and queue length distribution analyses are exact as the number of\nservers approaches infinity, under standard assumptions. Furthermore, our\npolicy families achieve maximal stability and outperform well-known dispatching\nrules---including heterogeneity-aware policies such as\nShortest-Expected-Delay---with respect to mean response time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 18:44:37 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Gardner", "Kristen", ""], ["Jaleel", "Jazeem Abdul", ""], ["Wickeham", "Alexander", ""], ["Doroudi", "Sherwin", ""]]}, {"id": "2006.14074", "submitter": "Abdul Ahad Abro", "authors": "Abdul Ahad Abro, Ufaque Shaikh", "title": "Design And Develop Network Storage Virtualization By Using GNS3", "comments": "Journal of Information & Communication Technology - JICT Vol. 13\n  Issue. 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization is an emerging and optimistic prospect in the IT industry. Its\nimpact has a footprint widely in digital infrastructure. Many innovativeness\nsectors utilized the concept of virtualization to reduce the cost of\nframeworks. In this paper, we have designed and developed storage\nvirtualization for physical functional solutions. It is an auspicious type of\nvirtualization that is accessible, secure, scalable, and manageable. In the\npaper, we have proposed the pool storage method used the RAID-Z file system\nwith the ZFS model which provides the duplication of site approach, compression\nblueprint, adequate backup methods, expansion in error-correcting techniques,\nand tested procedure on the real-time network location. Therefore, this study\nprovides useful guidelines to design and develop optimized storage\nvirtualization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 22:15:11 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Abro", "Abdul Ahad", ""], ["Shaikh", "Ufaque", ""]]}, {"id": "2006.15463", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "Queues with Small Advice", "comments": "16 pages, draft version, to be submitted, subject to cahnge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent work on scheduling with predicted job sizes, we consider\nthe performance of scheduling algorithms with minimal advice, namely a single\nbit. Besides demonstrating the power of very limited advice, such schemes are\nquite natural. In the prediction setting, one bit of advice can be used to\nmodel a simple prediction as to whether a job is \"large\" or \"small\"; that is,\nwhether a job is above or below a given threshold. Further, one-bit advice\nschemes can correspond to mechanisms that tell whether to put a job at the\nfront or the back for the queue, a limitation which may be useful in many\nimplementation settings. Finally, queues with a single bit of advice have a\nsimple enough state that they can be analyzed in the limiting mean-field\nanalysis framework for the power of two choices. Our work follows in the path\nof recent work by showing that even small amounts of even possibly inaccurate\ninformation can greatly improve scheduling performance.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 22:44:52 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "2006.15698", "submitter": "Andrew Kirby", "authors": "Andrew C. Kirby, Dimitri J. Mavriplis", "title": "GPU-Accelerated Discontinuous Galerkin Methods: 30x Speedup on 345\n  Billion Unknowns", "comments": "7 pages, 4 figures, 40 references. Accepted to 2020 IEEE HPEC", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.NA cs.PF math.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A discontinuous Galerkin method for the discretization of the compressible\nEuler equations, the governing equations of inviscid fluid dynamics, on\nCartesian meshes is developed for use of Graphical Processing Units via OCCA, a\nunified approach to performance portability on multi-threaded hardware\narchitectures. A 30x time-to-solution speedup over CPU-only implementations\nusing non-CUDA-Aware MPI communications is demonstrated up to 1,536 NVIDIA V100\nGPUs and parallel strong scalability is shown up to 6,144 NVIDIA V100 GPUs for\na problem containing 345 billion unknowns. A comparison of CUDA-Aware MPI\ncommunication to non-GPUDirect communication is performed demonstrating an\nadditional 24% speedup on eight nodes composed of 32 NVIDIA V100 GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 20:14:38 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 15:13:49 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 00:27:29 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Kirby", "Andrew C.", ""], ["Mavriplis", "Dimitri J.", ""]]}, {"id": "2006.16368", "submitter": "Majid Raeis", "authors": "Majid Raeis, Ali Tizghadam, Alberto Leon-Garcia", "title": "Probabilistic Bounds on the End-to-End Delay of Service Function Chains\n  using Deep MDN", "comments": "7 pages, to be presented at IEEE PIMRC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring the conformance of a service system's end-to-end delay to service\nlevel agreement (SLA) constraints is a challenging task that requires\nstatistical measures beyond the average delay. In this paper, we study the\nreal-time prediction of the end-to-end delay distribution in systems with\ncomposite services such as service function chains. In order to have a general\nframework, we use queueing theory to model service systems, while also adopting\na statistical learning approach to avoid the limitations of queueing-theoretic\nmethods such as stationarity assumptions or other approximations that are often\nused to make the analysis mathematically tractable. Specifically, we use deep\nmixture density networks (MDN) to predict the end-to-end distribution of the\ndelay given the network's state. As a result, our method is sufficiently\ngeneral to be applied in different contexts and applications. Our evaluations\nshow a good match between the learned distributions and the simulations, which\nsuggest that the proposed method is a good candidate for providing\nprobabilistic bounds on the end-to-end delay of more complex systems where\nsimulations or theoretical methods are not applicable.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 20:45:52 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Raeis", "Majid", ""], ["Tizghadam", "Ali", ""], ["Leon-Garcia", "Alberto", ""]]}]