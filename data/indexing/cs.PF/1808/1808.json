[{"id": "1808.00443", "submitter": "George Kesidis", "authors": "George Kesidis, Takis Konstantopoulos, Michael Zazanis", "title": "Relative Age of Information: Maintaining Freshness while Considering the\n  Most Recently Generated Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A queueing system handling a sequence of message arrivals is considered where\neach message obsoletes all previous messages. The objective is to assess the\nfreshness of the latest message/information that has been successfully\ntransmitted, i.e., \"age of information\" (AoI). We study a variation of\ntraditional AoI, the \"Relative AoI\", here defined so as to account for the\npresence of newly arrived messages/information to the queue to be transmitted.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:46:04 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 23:10:37 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 01:15:39 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Kesidis", "George", ""], ["Konstantopoulos", "Takis", ""], ["Zazanis", "Michael", ""]]}, {"id": "1808.01151", "submitter": "Quan-Lin Li", "authors": "Quan-Lin Li, Fan-Qi Ma and Jing-Yu Ma", "title": "A Stochastic Model for File Lifetime and Security in Data Center\n  Networks", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data center networks are an important infrastructure in various applications\nof modern information technologies. Note that each data center always has a\nfinite lifetime, thus once a data center fails, then it will lose all its\nstorage files and useful information. For this, it is necessary to replicate\nand copy each important file into other data centers such that this file can\nincrease its lifetime of staying in a data center network. In this paper, we\ndescribe a large-scale data center network with a file d-threshold policy,\nwhich is to replicate each important file into at most d-1 other data centers\nsuch that this file can maintain in the data center network under a given level\nof data security in the long-term. To this end, we develop three relevant\nMarkov processes to propose two effective methods for assessing the file\nlifetime and data security. By using the RG-factorizations, we show that the\ntwo methods are used to be able to more effectively evaluate the file lifetime\nof large-scale data center networks. We hope the methodology and results given\nin this paper are applicable in the file lifetime study of more general data\ncenter networks with replication mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 10:54:09 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Li", "Quan-Lin", ""], ["Ma", "Fan-Qi", ""], ["Ma", "Jing-Yu", ""]]}, {"id": "1808.02120", "submitter": "Weina Wang", "authors": "Weina Wang, Siva Theja Maguluri, R. Srikant, Lei Ying", "title": "Heavy-Traffic Insensitive Bounds for Weighted Proportionally Fair\n  Bandwidth Sharing Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a connection-level model proposed by Massouli\\'{e} and Roberts\nfor bandwidth sharing among file transfer flows in a communication network. We\nstudy weighted proportionally fair sharing policies and establish explicit-form\nbounds on the weighted sum of the expected numbers of flows on different routes\nin heavy traffic. The bounds are linear in the number of critically loaded\nlinks in the network, and they hold for a class of phase-type file-size\ndistributions; i.e., the bounds are heavy-traffic insensitive to the\ndistributions in this class. Our approach is Lyapunov-drift based, which is\ndifferent from the widely used diffusion approximation approach. A key\ntechnique we develop is to construct a novel inner product in the state space,\nwhich then allows us to obtain a multiplicative type of state-space collapse in\nsteady state. Furthermore, this state-space collapse result implies the\ninterchange of limits as a by-product for the diffusion approximation of the\nequal-weight case under phase-type file-size distributions, demonstrating the\nheavy-traffic insensitivity of the stationary distribution.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 21:17:14 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 02:13:59 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wang", "Weina", ""], ["Maguluri", "Siva Theja", ""], ["Srikant", "R.", ""], ["Ying", "Lei", ""]]}, {"id": "1808.02231", "submitter": "Gabriele D'Angelo", "authors": "Antonio Magnani, Gabriele D'Angelo, Stefano Ferretti, Moreno Marzolla", "title": "Anonymity and Confidentiality in Secure Distributed Simulation", "comments": "Proceedings of the IEEE/ACM International Symposium on Distributed\n  Simulation and Real Time Applications (DS-RT 2018)", "journal-ref": null, "doi": "10.1109/DISTRA.2018.8600922", "report-no": null, "categories": "cs.DC cs.CR cs.MA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on data confidentiality, integrity and availability is gaining\nmomentum in the ICT community, due to the intrinsically insecure nature of the\nInternet. While many distributed systems and services are now based on secure\ncommunication protocols to avoid eavesdropping and protect confidentiality, the\ntechniques usually employed in distributed simulations do not consider these\nissues at all. This is probably due to the fact that many real-world simulators\nrely on monolithic, offline approaches and therefore the issues above do not\napply. However, the complexity of the systems to be simulated, and the rise of\ndistributed and cloud based simulation, now impose the adoption of secure\nsimulation architectures. This paper presents a solution to ensure both\nanonymity and confidentiality in distributed simulations. A performance\nevaluation based on an anonymized distributed simulator is used for quantifying\nthe performance penalty for being anonymous. The obtained results show that\nthis is a viable solution.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 07:04:54 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 12:31:12 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Magnani", "Antonio", ""], ["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Marzolla", "Moreno", ""]]}, {"id": "1808.02515", "submitter": "Davis Blalock", "authors": "Davis Blalock, Samuel Madden, John Guttag", "title": "Sprintz: Time Series Compression for the Internet of Things", "comments": null, "journal-ref": null, "doi": "10.1145/3264903", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the rapid proliferation of connected devices, sensor-generated time\nseries constitute a large and growing portion of the world's data. Often, this\ndata is collected from distributed, resource-constrained devices and\ncentralized at one or more servers. A key challenge in this setup is reducing\nthe size of the transmitted data without sacrificing its quality. Lower quality\nreduces the data's utility, but smaller size enables both reduced network and\nstorage costs at the servers and reduced power consumption in sensing devices.\nA natural solution is to compress the data at the sensing devices.\nUnfortunately, existing compression algorithms either violate the memory and\nlatency constraints common for these devices or, as we show experimentally,\nperform poorly on sensor-generated time series.\n  We introduce a time series compression algorithm that achieves\nstate-of-the-art compression ratios while requiring less than 1KB of memory and\nadding virtually no latency. This method is suitable not only for low-power\ndevices collecting data, but also for servers storing and querying data; in the\nlatter context, it can decompress at over 3GB/s in a single thread, even faster\nthan many algorithms with much lower compression ratios. A key component of our\nmethod is a high-speed forecasting algorithm that can be trained online and\nsignificantly outperforms alternatives such as delta coding.\n  Extensive experiments on datasets from many domains show that these results\nhold not only for sensor data but also across a wide array of other time\nseries.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 18:48:46 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Blalock", "Davis", ""], ["Madden", "Samuel", ""], ["Guttag", "John", ""]]}, {"id": "1808.03031", "submitter": "Dmitrii Chemodanov", "authors": "Dmitrii Chemodanov, Flavio Esposito, Prasad Calyam, Andrei Sukhov", "title": "A Constrained Shortest Path Scheme for Virtual Network Service\n  Management", "comments": "Extended Technical Report for the IEEE Transactions on Network and\n  Service Management submission", "journal-ref": "D. Chemodanov, F. Esposito, P. Calyam, A. Sukhov, \"A Constrained\n  Shortest Path Scheme for Virtual Network Service Management\", IEEE\n  Transactions on Network and Service Management, 2018", "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual network services that span multiple data centers are important to\nsupport emerging data-intensive applications in fields such as bioinformatics\nand retail analytics. Successful virtual network service composition and\nmaintenance requires flexible and scalable 'constrained shortest path\nmanagement' both in the management plane for virtual network embedding (VNE) or\nnetwork function virtualization service chaining (NFV-SC), as well as in the\ndata plane for traffic engineering (TE). In this paper, we show analytically\nand empirically that leveraging constrained shortest paths within recent VNE,\nNFV-SC and TE algorithms can lead to network utilization gains (of up to 50%)\nand higher energy efficiency. The management of complex VNE, NFV-SC and TE\nalgorithms can be, however, intractable for large scale substrate networks due\nto the NP-hardness of the constrained shortest path problem. To address such\nscalability challenges, we propose a novel, exact constrained shortest path\nalgorithm viz., 'Neighborhoods Method' (NM). Our NM uses novel search space\nreduction techniques and has a theoretical quadratic speed-up making it\npractically faster (by an order of magnitude) than recent branch-and-bound\nexhaustive search solutions. Finally, we detail our NM-based SDN controller\nimplementation in a real-world testbed to further validate practical NM\nbenefits for virtual network services.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 06:19:48 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Chemodanov", "Dmitrii", ""], ["Esposito", "Flavio", ""], ["Calyam", "Prasad", ""], ["Sukhov", "Andrei", ""]]}, {"id": "1808.03518", "submitter": "Ishwar Singh Bhati", "authors": "Ishwar Bhati, Udit Dhawan, Jayesh Gaur, Sreenivas Subramoney, and Hong\n  Wang", "title": "MARS: Memory Aware Reordered Source", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory bandwidth is critical in today's high performance computing systems.\nThe bandwidth is particularly paramount for GPU workloads such as 3D Gaming,\nImaging and Perceptual Computing, GPGPU due to their data-intensive nature. As\nthe number of threads and data streams in the GPUs increases with each\ngeneration, along with a high available memory bandwidth, memory efficiency is\nalso crucial in order to achieve desired performance. In presence of multiple\nconcurrent data streams, the inherent locality in a single data stream is often\nlost as these streams are interleaved while moving through multiple levels of\nmemory system. In DRAM based main memory, the poor request locality reduces\nrow-buffer reuse resulting in underutilized and inefficient memory bandwidth.\n  In this paper we propose Memory-Aware Reordered Source (\\textit{MARS})\narchitecture to address memory inefficiency arising from highly interleaved\ndata streams. The key idea of \\textit{MARS} is that with a sufficiently large\nlookahead before the main memory, data streams can be reordered based on their\nrow-buffer address to regain the lost locality and improve memory efficiency.\nWe show that \\textit{MARS} improves achieved memory bandwidth by 11\\% for a set\nof synthetic microbenchmarks. Moreover, MARS does so without any specific\nknowledge of the memory configuration.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:11:47 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Bhati", "Ishwar", ""], ["Dhawan", "Udit", ""], ["Gaur", "Jayesh", ""], ["Subramoney", "Sreenivas", ""], ["Wang", "Hong", ""]]}, {"id": "1808.04201", "submitter": "Michael Schaffner", "authors": "Michael Schaffner, Luca Benini", "title": "On the Feasibility of FPGA Acceleration of Molecular Dynamics\n  Simulations", "comments": "Technical Report, 16 Pages, 4 Tables, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical molecular dynamics (MD) simulations are important tools in life and\nmaterial sciences since they allow studying chemical and biological processes\nin detail. However, the inherent scalability problem of particle-particle\ninteractions and the sequential dependency of subsequent time steps render MD\ncomputationally intensive and difficult to scale. To this end, specialized\nFPGA-based accelerators have been repeatedly proposed to ameliorate this\nproblem. However, to date none of the leading MD simulation packages fully\nsupport FPGA acceleration and a direct comparison of GPU versus FPGA\naccelerated codes has remained elusive so far. With this report, we aim at\nclarifying this issue by comparing measured application performance on\nGPU-dense compute nodes with performance and cost estimates of a FPGA-based\nsingle- node system. Our results show that an FPGA-based system can indeed\noutperform a similarly configured GPU-based system, but the overall\napplication-level speedup remains in the order of 2x due to software overheads\non the host. Considering the price for GPU and FPGA solutions, we observe that\nGPU-based solutions provide the better cost/performance tradeoff, and hence\npure FPGA-based solutions are likely not going to be commercially viable.\nHowever, we also note that scaled multi-node systems could potentially benefit\nfrom a hybrid composition, where GPUs are used for compute intensive parts and\nFPGAs for latency and communication sensitive tasks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 10:03:37 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Schaffner", "Michael", ""], ["Benini", "Luca", ""]]}, {"id": "1808.06074", "submitter": "Jyothi Krishna V S", "authors": "Jyothi Krishna V S and Shankar Balachandran", "title": "Compiler Enhanced Scheduling for OpenMP for Heterogeneous\n  Multiprocessors", "comments": "6 Pages, 4 figures, Presented in 2nd EEHCO (Energy Efficiency with\n  Heterogenous Computing) Workshop in Prague 2016 (Part of HiPEAC event 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling in Asymmetric Multicore Processors (AMP), a special case of\nHeterogeneous Multiprocessors, is a widely studied topic. The scheduling\ntechniques which are mostly runtime do not usually consider parallel\nprogramming pattern used in parallel programming frameworks like OpenMP. On the\nother hand, current compilers for these parallel programming platforms are\nhardware oblivious which prevent any compile-time optimization for platforms\nlike big.LITTLE and has to completely rely on runtime optimization. In this\npaper, we propose a hardware-aware Compiler Enhanced Scheduling (CES) where the\ncommon compiler transformations are coupled with compiler added scheduling\ncommands to take advantage of the hardware asymmetry and improve the runtime\nefficiency. We implement a compiler for OpenMP and demonstrate its efficiency\nin Samsung Exynos with big.LITTLE architecture. On an average, we see 18%\nreduction in runtime and 14% reduction in energy consumption in standard NPB\nand FSU benchmarks with CES across multiple frequencies and core configurations\nin big.LITTLE.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 11:39:54 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["S", "Jyothi Krishna V", ""], ["Balachandran", "Shankar", ""]]}, {"id": "1808.06911", "submitter": "Xingyu Zhou", "authors": "Xingyu Zhou, Jian Tan and Ness Shroff", "title": "Heavy-traffic Delay Optimality in Pull-based Load Balancing Systems:\n  Necessary and Sufficient Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider a load balancing system under a general pull-based\npolicy. In particular, each arrival is randomly dispatched to one of the\nservers whose queue lengths are below a threshold, if there are any; otherwise,\nthis arrival is randomly dispatched to one of the entire set of servers. We are\ninterested in the fundamental relationship between the threshold and the delay\nperformance of the system in heavy traffic. To this end, we first establish the\nfollowing necessary condition to guarantee heavy-traffic delay optimality: the\nthreshold will grow to infinity as the exogenous arrival rate approaches the\nboundary of the capacity region (i.e., the load intensity approaches one) but\nthe growth rate should be slower than a polynomial function of the mean number\nof tasks in the system. As a special case of this result, we directly show that\nthe delay performance of the popular pull-based policy Join-Idle-Queue (JIQ)\nlies strictly between that of any heavy-traffic delay optimal policy and that\nof random routing. We further show that a sufficient condition for\nheavy-traffic delay optimality is that the threshold grows logarithmically with\nthe mean number of tasks in the system. This result directly resolves a\ngeneralized version of the conjecture by Kelly and Laws.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 14:10:10 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Zhou", "Xingyu", ""], ["Tan", "Jian", ""], ["Shroff", "Ness", ""]]}, {"id": "1808.07168", "submitter": "Stanimire Tomov", "authors": "Nathalie-Sofia Tomov and Stanimire Tomov", "title": "On Deep Neural Networks for Detecting Heart Disease", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart disease is the leading cause of death, and experts estimate that\napproximately half of all heart attacks and strokes occur in people who have\nnot been flagged as \"at risk.\" Thus, there is an urgent need to improve the\naccuracy of heart disease diagnosis. To this end, we investigate the potential\nof using data analysis, and in particular the design and use of deep neural\nnetworks (DNNs) for detecting heart disease based on routine clinical data. Our\nmain contribution is the design, evaluation, and optimization of DNN\narchitectures of increasing depth for heart disease diagnosis. This work led to\nthe discovery of a novel five layer DNN architecture - named Heart Evaluation\nfor Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields\nbest prediction accuracy. HEARO-5's design employs regularization optimization\nand automatically deals with missing data and/or data outliers. To evaluate and\ntune the architectures we use k-way cross-validation as well as Matthews\ncorrelation coefficient (MCC) to measure the quality of our classifications.\nThe study is performed on the publicly available Cleveland dataset of medical\ninformation, and we are making our developments open source, to further\nfacilitate openness and research on the use of DNNs in medicine. The HEARO-5\narchitecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms\ncurrently published research in the area.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 00:51:57 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Tomov", "Nathalie-Sofia", ""], ["Tomov", "Stanimire", ""]]}, {"id": "1808.08512", "submitter": "Wanling Gao", "authors": "Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Fei\n  Tang, Biwei Xie, Chen Zheng, Xu Wen, Xiwen He, Hainan Ye, Rui Ren", "title": "Data Motifs: A Lens Towards Fully Understanding Big Data and AI\n  Workloads", "comments": "The paper will be published on The 27th International Conference on\n  Parallel Architectures and Compilation Techniques (PACT18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity and diversity of big data and AI workloads make understanding\nthem difficult and challenging. This paper proposes a new approach to modelling\nand characterizing big data and AI workloads. We consider each big data and AI\nworkload as a pipeline of one or more classes of units of computation performed\non different initial or intermediate data inputs. Each class of unit of\ncomputation captures the common requirements while being reasonably divorced\nfrom individual implementations, and hence we call it a data motif. For the\nfirst time, among a wide variety of big data and AI workloads, we identify\neight data motifs that take up most of the run time of those workloads,\nincluding Matrix, Sampling, Logic, Transform, Set, Graph, Sort and Statistic.\nWe implement the eight data motifs on different software stacks as the micro\nbenchmarks of an open-source big data and AI benchmark suite ---BigDataBench\n4.0 (publicly available from http://prof.ict.ac.cn/BigDataBench), and perform\ncomprehensive characterization of those data motifs from perspective of data\nsizes, types, sources, and patterns as a lens towards fully understanding big\ndata and AI workloads. We believe the eight data motifs are promising\nabstractions and tools for not only big data and AI benchmarking, but also\ndomain-specific hardware and software co-design.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 06:52:25 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Gao", "Wanling", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Luo", "Chunjie", ""], ["Zheng", "Daoyi", ""], ["Tang", "Fei", ""], ["Xie", "Biwei", ""], ["Zheng", "Chen", ""], ["Wen", "Xu", ""], ["He", "Xiwen", ""], ["Ye", "Hainan", ""], ["Ren", "Rui", ""]]}, {"id": "1808.08650", "submitter": "EPTCS", "authors": "Jane Hillston (University of Edinburgh, UK), Carla Piazza\n  (Universit\\`a di Udine, Italy), Sabina Rossi (Universit\\`a Ca` Foscari\n  Venezia, Italy)", "title": "Persistent Stochastic Non-Interference", "comments": "In Proceedings EXPRESS/SOS 2018, arXiv:1808.08071", "journal-ref": "EPTCS 276, 2018, pp. 53-68", "doi": "10.4204/EPTCS.276.6", "report-no": null, "categories": "cs.PF cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an information flow security property for\nstochastic, cooperating, processes expressed as terms of the Performance\nEvaluation Process Algebra (PEPA). We introduce the notion of Persistent\nStochastic Non-Interference (PSNI) based on the idea that every state reachable\nby a process satisfies a basic Stochastic Non-Interference (SNI) property. The\nstructural operational semantics of PEPA allows us to give two\ncharacterizations of PSNI: the first involves a single bisimulation-like\nequivalence check, while the second is formulated in terms of unwinding\nconditions. The observation equivalence at the base of our definition relies on\nthe notion of lumpability and ensures that, for a secure process P, the steady\nstate probability of observing the system being in a specific state P' is\nindependent from its possible high level interactions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 01:19:44 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Hillston", "Jane", "", "University of Edinburgh, UK"], ["Piazza", "Carla", "", "Universit\u00e0 di Udine, Italy"], ["Rossi", "Sabina", "", "Universit\u00e0 Ca` Foscari\n  Venezia, Italy"]]}, {"id": "1808.09999", "submitter": "Fabio Lorenzo Traversa Ph.D.", "authors": "Fabio L. Traversa, Massimiliano Di Ventra", "title": "MemComputing Integer Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.PF math.OC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer linear programming (ILP) encompasses a very important class of\noptimization problems that are of great interest to both academia and industry.\nSeveral algorithms are available that attempt to explore the solution space of\nthis class efficiently, while requiring a reasonable compute time. However,\nalthough these algorithms have reached various degrees of success over the\nyears, they still face considerable challenges when confronted with\nparticularly hard problem instances, such as those of the MIPLIB 2010 library.\nIn this work we propose a radically different non-algorithmic approach to ILP\nbased on a novel physics-inspired computing paradigm: Memcomputing. This\nparadigm is based on digital (hence scalable) machines represented by\nappropriate electrical circuits with memory. These machines can be either built\nin hardware or, as we do here, their equations of motion can be efficiently\nsimulated on our traditional computers. We first describe a new circuit\narchitecture of memcomputing machines specifically designed to solve for the\nlinear inequalities representing a general ILP problem. We call these\nself-organizing algebraic circuits, since they self-organize dynamically to\nsatisfy the correct (algebraic) linear inequalities. We then show simulations\nof these machines using MATLAB running on a single core of a Xeon processor for\nseveral ILP benchmark problems taken from the MIPLIB 2010 library, and compare\nour results against a renowned commercial solver. We show that our approach is\nvery efficient when dealing with these hard problems. In particular, we find\nwithin minutes feasible solutions for one of these hard problems (f2000 from\nMIPLIB 2010) whose feasibility, to the best of our knowledge, has remained\nunknown for the past eight years.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 18:19:48 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Traversa", "Fabio L.", ""], ["Di Ventra", "Massimiliano", ""]]}, {"id": "1808.10097", "submitter": "Behnam Dezfouli", "authors": "Immanuel Amirtharaj, Tai Groot, Behnam Dezfouli", "title": "Profiling and Improving the Duty-Cycling Performance of Linux-based IoT\n  Devices", "comments": "SCU's IoT Research Lab", "journal-ref": null, "doi": null, "report-no": "TR-SCU-SIOTLAB-AUG2018-PALLEX", "categories": "cs.OS cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the energy consumption of Linux-based devices is an essential step\ntowards their wide deployment in various IoT scenarios. Energy saving methods\nsuch as duty-cycling aim to address this constraint by limiting the amount of\ntime the device is powered on. In this work we study and improve the amount of\ntime a Linux-based IoT device is powered on to accomplish its tasks. We analyze\nthe processes of system boot up and shutdown on two platforms, the Raspberry Pi\n3 and Raspberry Pi Zero Wireless, and enhance duty-cycling performance by\nidentifying and disabling time-consuming or unnecessary units initialized in\nthe userspace. We also study whether SD card speed and SD card capacity\nutilization affect boot up duration and energy consumption. In addition, we\npropose 'Pallex', a parallel execution framework built on top of the 'systemd\ninit' system to run a user application concurrently with userspace\ninitialization. We validate the performance impact of Pallex when applied to\nvarious IoT application scenarios: (i) capturing an image, (ii) capturing and\nencrypting an image, (iii) capturing and classifying an image using the the\nk-nearest neighbor algorithm, and (iv) capturing images and sending them to a\ncloud server. Our results show that system lifetime is increased by 18.3%,\n16.8%, 13.9% and 30.2%, for these application scenarios, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 03:09:40 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 20:36:43 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Amirtharaj", "Immanuel", ""], ["Groot", "Tai", ""], ["Dezfouli", "Behnam", ""]]}]