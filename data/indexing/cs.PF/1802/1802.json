[{"id": "1802.00056", "submitter": "Andre van Hoorn", "authors": "Christoph Heger, Andr\\'e van Hoorn, Du\\v{s}an Okanovic, Stefan Siegl,\n  Christian V\\\"ogele, Alexander Wert", "title": "diagnoseIT: Expertengest\\\"utzte automatische Diagnose von\n  Performance-Probleme in Enterprise-Anwendungen (Abschlussbericht)", "comments": "The language of the report is German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.PF q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the final report of the collaborative research project diagnoseIT on\nexpert-guided automatic diagnosis of performance problems in enterprise\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 03:34:10 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Heger", "Christoph", ""], ["van Hoorn", "Andr\u00e9", ""], ["Okanovic", "Du\u0161an", ""], ["Siegl", "Stefan", ""], ["V\u00f6gele", "Christian", ""], ["Wert", "Alexander", ""]]}, {"id": "1802.00699", "submitter": "Wanling Gao", "authors": "Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Fei\n  Tang, Biwei Xie, Chen Zheng and Qiang Yang", "title": "Data Dwarfs: A Lens Towards Fully Understanding Big Data and AI\n  Workloads", "comments": "19 pages, 16 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity and diversity of big data and AI workloads make understanding\nthem difficult and challenging. This paper proposes a new approach to\ncharacterizing big data and AI workloads. We consider each big data and AI\nworkload as a pipeline of one or more classes of unit of computations performed\non different initial or intermediate data inputs. Each class of unit of\ncomputation captures the common requirements while being reasonably divorced\nfrom individual implementations, and hence we call it a data dwarf. For the\nfirst time, among a wide variety of big data and AI workloads, we identify\neight data dwarfs that takes up most of run time, including Matrix, Sampling,\nLogic, Transform, Set, Graph, Sort and Statistic. We implement the eight data\ndwarfs on different software stacks as the micro benchmarks of an open-source\nbig data and AI benchmark suite, and perform comprehensive characterization of\nthose data dwarfs from perspective of data sizes, types, sources, and patterns\nas a lens towards fully understanding big data and AI workloads.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 04:26:21 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 07:38:18 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Gao", "Wanling", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Luo", "Chunjie", ""], ["Zheng", "Daoyi", ""], ["Tang", "Fei", ""], ["Xie", "Biwei", ""], ["Zheng", "Chen", ""], ["Yang", "Qiang", ""]]}, {"id": "1802.01254", "submitter": "Liang Yuan", "authors": "Liang Yuan, Chen Ding, Peter Denning, Yunquan Zhang", "title": "A Measurement Theory of Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality is a fundamental principle used extensively in program and system\noptimization. It can be measured in many ways. This paper formalizes the\nmetrics of locality into a measurement theory. The new theory includes the\nprecise definition of locality metrics based on access frequency, reuse time,\nreuse distance, working set, footprint, and the cache miss ratio. It gives the\nformal relation between these definitions and the proofs of equivalence or\nnon-equivalence. It provides the theoretical justification for four successful\nlocality models in operating systems, programming languages, and computer\narchitectures which were developed empirically.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 03:35:27 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 03:30:40 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 14:16:59 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Yuan", "Liang", ""], ["Ding", "Chen", ""], ["Denning", "Peter", ""], ["Zhang", "Yunquan", ""]]}, {"id": "1802.01847", "submitter": "Emilio Leonardi", "authors": "Giovanni Luca Torrisi and Michele Garetto, Emilio Leonardi", "title": "A large deviation approach to super-critical bootstrap percolation on\n  the random graph $G_{n,p}$", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Erd\\\"{o}s--R\\'{e}nyi random graph $G_{n,p}$ and we analyze\nthe simple irreversible epidemic process on the graph, known in the literature\nas bootstrap percolation. We give a quantitative version of some results by\nJanson et al. (2012), providing a fine asymptotic analysis of the final size\n$A_n^*$ of active nodes, under a suitable super-critical regime. More\nspecifically, we establish large deviation principles for the sequence of\nrandom variables $\\{\\frac{n- A_n^*}{f(n)}\\}_{n\\geq 1}$ with explicit rate\nfunctions and allowing the scaling function $f$ to vary in the widest possible\nrange.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 08:59:24 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 08:32:45 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 14:51:26 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Torrisi", "Giovanni Luca", ""], ["Garetto", "Michele", ""], ["Leonardi", "Emilio", ""]]}, {"id": "1802.01870", "submitter": "Xiao-Liang Wang", "authors": "Zhi Wang, Xiaoliang Wang, Zhuzhong Qian, Baoliu Ye, Sanglu Lu", "title": "RDMAvisor: Toward Deploying Scalable and Simple RDMA as a Service in\n  Datacenters", "comments": "submitted to USENIX ATC 2017, No.9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDMA is increasingly adopted by cloud computing platforms to provide low CPU\noverhead, low latency, high throughput network services. On the other hand,\nhowever, it is still challenging for developers to realize fast deployment of\nRDMA-aware applications in the datacenter, since the performance is highly\nrelated to many lowlevel details of RDMA operations. To address this problem,\nwe present a simple and scalable RDMA as Service (RaaS) to mitigate the impact\nof RDMA operational details. RaaS provides careful message buffer management to\nimprove CPU/memory utilization and improve the scalability of RDMA operations.\nThese optimized designs lead to simple and flexible programming model for\ncommon and knowledgeable users. We have implemented a prototype of RaaS, named\nRDMAvisor, and evaluated its performance on a cluster with a large number of\nconnections. Our experiment results demonstrate that RDMAvisor achieves high\nthroughput for thousand of connections and maintains low CPU and memory\noverhead through adaptive RDMA transport selection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 10:09:12 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Wang", "Zhi", ""], ["Wang", "Xiaoliang", ""], ["Qian", "Zhuzhong", ""], ["Ye", "Baoliu", ""], ["Lu", "Sanglu", ""]]}, {"id": "1802.01957", "submitter": "Nirmal Prajapati", "authors": "Nirmal Prajapati, Sanjay Rajopadhye and Hristo Djidjev", "title": "Analytical Cost Metrics : Days of Future Past", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we move towards the exascale era, the new architectures must be capable of\nrunning the massive computational problems efficiently. Scientists and\nresearchers are continuously investing in tuning the performance of\nextreme-scale computational problems. These problems arise in almost all areas\nof computing, ranging from big data analytics, artificial intelligence, search,\nmachine learning, virtual/augmented reality, computer vision, image/signal\nprocessing to computational science and bioinformatics. With Moore's law\ndriving the evolution of hardware platforms towards exascale, the dominant\nperformance metric (time efficiency) has now expanded to also incorporate\npower/energy efficiency. Therefore, the major challenge that we face in\ncomputing systems research is: \"how to solve massive-scale computational\nproblems in the most time/power/energy efficient manner?\"\n  The architectures are constantly evolving making the current performance\noptimizing strategies less applicable and new strategies to be invented. The\nsolution is for the new architectures, new programming models, and applications\nto go forward together. Doing this is, however, extremely hard. There are too\nmany design choices in too many dimensions. We propose the following strategy\nto solve the problem: (i) Models - Develop accurate analytical models (e.g.\nexecution time, energy, silicon area) to predict the cost of executing a given\nprogram, and (ii) Complete System Design - Simultaneously optimize all the cost\nmodels for the programs (computational problems) to obtain the most\ntime/area/power/energy efficient solution. Such an optimization problem evokes\nthe notion of codesign.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 06:51:02 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Prajapati", "Nirmal", ""], ["Rajopadhye", "Sanjay", ""], ["Djidjev", "Hristo", ""]]}, {"id": "1802.02760", "submitter": "Zheng Wang", "authors": "Peng Zhang, Jianbin Fang, Tao Tang, Canqun Yang, Zheng Wang", "title": "Tuning Streamed Applications on Intel Xeon Phi: A Machine Learning Based\n  Approach", "comments": "Accepted to be published at 32nd IEEE International Parallel &\n  Distributed Processing Symposium (IPDPS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many-core accelerators, as represented by the XeonPhi coprocessors and\nGPGPUs, allow software to exploit spatial and temporal sharing of computing\nresources to improve the overall system performance. To unlock this performance\npotential requires software to effectively partition the hardware resource to\nmaximize the overlap between hostdevice communication and accelerator\ncomputation, and to match the granularity of task parallelism to the resource\npartition. However, determining the right resource partition and task\nparallelism on a per program, per dataset basis is challenging. This is because\nthe number of possible solutions is huge, and the benefit of choosing the right\nsolution may be large, but mistakes can seriously hurt the performance. In this\npaper, we present an automatic approach to determine the hardware resource\npartition and the task granularity for any given application, targeting the\nIntel XeonPhi architecture. Instead of hand-crafting the heuristic for which\nthe process will have to repeat for each hardware generation, we employ machine\nlearning techniques to automatically learn it. We achieve this by first\nlearning a predictive model offline using training programs; we then use the\nlearned model to predict the resource partition and task granularity for any\nunseen programs at runtime. We apply our approach to 23 representative parallel\napplications and evaluate it on a CPU-XeonPhi mixed heterogenous many-core\nplatform. Our approach achieves, on average, a 1.6x (upto 5.6x) speedup, which\ntranslates to 94.5% of the performance delivered by a theoretically perfect\npredictor.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 09:12:53 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Zhang", "Peng", ""], ["Fang", "Jianbin", ""], ["Tang", "Tao", ""], ["Yang", "Canqun", ""], ["Wang", "Zheng", ""]]}, {"id": "1802.02765", "submitter": "Sebastian Eibl", "authors": "Sebastian Eibl and Ulrich R\\\"ude", "title": "A local parallel communication algorithm for polydisperse rigid body\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of large ensembles of particles is usually parallelized by\npartitioning the domain spatially and using message passing to communicate\nbetween the processes handling neighboring subdomains. The particles are\nrepresented as individual geometric objects and are associated to the\nsubdomains. Handling collisions and migrating particles between subdomains, as\nrequired for proper parallel execution, requires a complex communication\nprotocol. Typically, the parallelization is restricted to handling only\nparticles that are smaller than a subdomain. In many applications, however,\nparticle sizes may vary drastically with some of them being larger than a\nsubdomain. In this article we propose a new communication and synchronization\nalgorithm that can handle the parallelization without size restrictions on the\nparticles. Despite the additional complexity and extended functionality, the\nnew algorithm introduces only minimal overhead. We demonstrate the scalability\nof the previous and the new communication algorithms up to almost two million\nparallel processes and for handling ten billion (1e10) geometrically resolved\nparticles on a state-of-the-art petascale supercomputer. Different scenarios\nare presented to analyze the performance of the new algorithm and to\ndemonstrate its capability to simulate polydisperse scenarios, where large\nindividual particles can extend across several subdomains.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 09:25:24 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 14:29:58 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Eibl", "Sebastian", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1802.04076", "submitter": "Fatima Ezzahra Airod", "authors": "Fatima Ezzahra Airod, Houda Chafnaji, and Halim Yanikomeroglu", "title": "Performance Analysis of Low Latency Multiple Full-Duplex Selective\n  Decode and Forward Relays", "comments": "Accepted to the Emerging Technologies, Architectures and Services of\n  the IEEE WCNC 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to follow up with mission-critical applications, new features need\nto be carried to satisfy a reliable communication with reduced latency. With\nthis regard, this paper proposes a low latency cooperative transmission scheme,\nwhere multiple full-duplex relays, simultaneously, assist the communication\nbetween a source node and a destination node. First, we present the\ncommunication model of the proposed transmission scheme. Then, we derive the\noutage probability closed-form for two cases: asynchronous transmission (where\nall relays have different processing delay) and synchronous transmissions\n(where all relays have the same processing delay). Finally, using simulations,\nwe confirm the theoretical results and compare the proposed multi-relays\ntransmission scheme with relay selection schemes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 17:38:32 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Airod", "Fatima Ezzahra", ""], ["Chafnaji", "Houda", ""], ["Yanikomeroglu", "Halim", ""]]}, {"id": "1802.05420", "submitter": "Benny Van Houdt", "authors": "Tim Hellemans and Benny Van Houdt", "title": "On the Power-of-d-choices with Least Loaded Server Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by distributed schedulers that combine the power-of-d-choices with\nlate binding and systems that use replication with cancellation-on-start, we\nstudy the performance of the LL(d) policy which assigns a job to a server that\ncurrently has the least workload among d randomly selected servers in\nlarge-scale homogeneous clusters. We consider general service time\ndistributions and propose a partial integro-differential equation to describe\nthe evolution of the system. This equation relies on the earlier proven ansatz\nfor LL(d) which asserts that the workload distribution of any finite set of\nqueues becomes independent of one another as the number of servers tends to\ninfinity. Based on this equation we propose a fixed point iteration for the\nlimiting workload distribution and study its convergence. For exponential job\nsizes we present a simple closed form expression for the limiting workload\ndistribution that is valid for any work-conserving service discipline as well\nas for the limiting response time distribution in case of\nfirst-come-first-served scheduling. We further show that for phase-type\ndistributed job sizes the limiting workload and response time distribution can\nbe expressed via the unique solution of a simple set of ordinary differential\nequations. Numerical and analytical results that compare response time of the\nclassic power-of-d-choices algorithm and the LL(d) policy are also presented\nand the accuracy of the limiting response time distribution for finite systems\nis illustrated using simulation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 07:13:37 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Hellemans", "Tim", ""], ["Van Houdt", "Benny", ""]]}, {"id": "1802.06566", "submitter": "Jonatha Anselmi", "authors": "Jonatha Anselmi and Francois Dufour", "title": "Power-of-$d$-Choices with Memory: Fluid Limit and Optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-server distributed queueing systems, the access of stochastically\narriving jobs to resources is often regulated by a dispatcher, also known as\nload balancer. A fundamental problem consists in designing a load balancing\nalgorithm that minimizes the delays experienced by jobs. During the last two\ndecades, the power-of-$d$-choice algorithm, based on the idea of dispatching\neach job to the least loaded server out of $d$ servers randomly sampled at the\narrival of the job itself, has emerged as a breakthrough in the foundations of\nthis area due to its versatility and appealing asymptotic properties. In this\npaper, we consider the power-of-$d$-choice algorithm with the addition of a\nlocal memory that keeps track of the latest observations collected over time on\nthe sampled servers. Then, each job is sent to a server with the lowest\nobservation. We show that this algorithm is asymptotically optimal in the sense\nthat the load balancer can always assign each job to an idle server in the\nlarge-system limit. This holds true if and only if the system load $\\lambda$ is\nless than $1-\\frac{1}{d}$. If this condition is not satisfied, we show that\nqueue lengths are tightly bounded by $\\left\\lceil - \\frac{ \\log\n(1-\\lambda)}{\\log (\\lambda d +1)} \\right\\rceil$. This is in contrast with the\nclassic version of the power-of-$d$-choice algorithm, where at the fluid scale\na strictly positive proportion of servers containing $i$ jobs exists for all\n$i\\ge 0$, in equilibrium. Our results quantify and highlight the importance of\nusing memory as a means to enhance performance in randomized load balancing.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 09:41:07 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 09:49:30 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Anselmi", "Jonatha", ""], ["Dufour", "Francois", ""]]}, {"id": "1802.06723", "submitter": "Subhashini Krishnasamy", "authors": "Subhashini Krishnasamy, Ari Arapostathis, Ramesh Johari and Sanjay\n  Shakkottai", "title": "On Learning the $c\\mu$ Rule in Single and Parallel Server Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning-based variants of the $c \\mu$ rule for scheduling in\nsingle and parallel server settings of multi-class queueing systems.\n  In the single server setting, the $c \\mu$ rule is known to minimize the\nexpected holding-cost (weighted queue-lengths summed over classes and a fixed\ntime horizon). We focus on the problem where the service rates $\\mu$ are\nunknown with the holding-cost regret (regret against the $c \\mu$ rule with\nknown $\\mu$) as our objective. We show that the greedy algorithm that uses\nempirically learned service rates results in a constant holding-cost regret\n(the regret is independent of the time horizon). This free exploration can be\nexplained in the single server setting by the fact that any work-conserving\npolicy obtains the same number of samples in a busy cycle.\n  In the parallel server setting, we show that the $c \\mu$ rule may result in\nunstable queues, even for arrival rates within the capacity region. We then\npresent sufficient conditions for geometric ergodicity under the $c \\mu$ rule.\nUsing these results, we propose an almost greedy algorithm that explores only\nwhen the number of samples falls below a threshold. We show that this algorithm\ndelivers constant holding-cost regret because a free exploration condition is\neventually satisfied.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 05:57:26 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 13:34:28 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Krishnasamy", "Subhashini", ""], ["Arapostathis", "Ari", ""], ["Johari", "Ramesh", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1802.07455", "submitter": "Antonio Sodre", "authors": "Antonio Sodre", "title": "Asymptotic efficiency of restart and checkpointing", "comments": "Theorem 2 is slightly restated and its proof in the appendix is fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks are subject to failure before completion. Two of the most common\nfailure recovery strategies are restart and checkpointing. Under restart, once\na failure occurs, it is restarted from the beginning. Under checkpointing, the\ntask is resumed from the preceding checkpoint after the failure. We study\nasymptotic efficiency of restart for an infinite sequence of tasks, whose sizes\nform a stationary sequence. We define asymptotic efficiency as the limit of the\nratio of the total time to completion in the absence of failures over the total\ntime to completion when failures take place. Whether the asymptotic efficiency\nis positive or not depends on the comparison of the tail of the distributions\nof the task size and the random variables governing failures. Our framework\nallows for variations in the failure rates and dependencies between task sizes.\nWe also study a similar notion of asymptotic efficiency for checkpointing when\nthe task is infinite a.s. and the inter-checkpoint times are i.i.d.. Moreover,\nin checkpointing, when the failures are exponentially distributed, we prove the\nexistence of an infinite sequence of universal checkpoints, which are always\nused whenever the system starts from any checkpoint that precedes them.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 07:48:27 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 17:35:58 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Sodre", "Antonio", ""]]}, {"id": "1802.07832", "submitter": "Justin Chang", "authors": "Justin Chang, Maurice S. Fabien, Matthew G. Knepley, Richard T. Mills", "title": "Comparative study of finite element methods using the Time-Accuracy-Size\n  (TAS) spectrum analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a performance analysis appropriate for comparing algorithms using\ndifferent numerical discretizations. By taking into account the total\ntime-to-solution, numerical accuracy with respect to an error norm, and the\ncomputation rate, a cost-benefit analysis can be performed to determine which\nalgorithm and discretization are particularly suited for an application. This\nwork extends the performance spectrum model in Chang et. al. 2017 for\ninterpretation of hardware and algorithmic tradeoffs in numerical PDE\nsimulation. As a proof-of-concept, popular finite element software packages are\nused to illustrate this analysis for Poisson's equation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 22:11:58 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Chang", "Justin", ""], ["Fabien", "Maurice S.", ""], ["Knepley", "Matthew G.", ""], ["Mills", "Richard T.", ""]]}, {"id": "1802.07855", "submitter": "Tao Gong", "authors": "Song Han and Tao Gong and Mark Nixon and Eric Rotvold and Kam-yiu Lam\n  and Krithi Ramamritham", "title": "RT-DAP: A Real-Time Data Analytics Platform for Large-scale Industrial\n  Process Monitoring and Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most process control systems nowadays, process measurements are\nperiodically collected and archived in historians. Analytics applications\nprocess the data, and provide results offline or in a time period that is\nconsiderably slow in comparison to the performance of the manufacturing\nprocess. Along with the proliferation of Internet-of-Things (IoT) and the\nintroduction of \"pervasive sensors\" technology in process industries,\nincreasing number of sensors and actuators are installed in process plants for\npervasive sensing and control, and the volume of produced process data is\ngrowing exponentially. To digest these data and meet the ever-growing\nrequirements to increase production efficiency and improve product quality,\nthere needs to be a way to both improve the performance of the analytics system\nand scale the system to closely monitor a much larger set of plant resources.\nIn this paper, we present a real-time data analytics platform, called RT-DAP,\nto support large-scale continuous data analytics in process industries. RT-DAP\nis designed to be able to stream, store, process and visualize a large volume\nof realtime data flows collected from heterogeneous plant resources, and\nfeedback to the control system and operators in a realtime manner. A prototype\nof the platform is implemented on Microsoft Azure. Our extensive experiments\nvalidate the design methodologies of RT-DAP and demonstrate its efficiency in\nboth component and system levels.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 00:25:25 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Han", "Song", ""], ["Gong", "Tao", ""], ["Nixon", "Mark", ""], ["Rotvold", "Eric", ""], ["Lam", "Kam-yiu", ""], ["Ramamritham", "Krithi", ""]]}, {"id": "1802.08254", "submitter": "Wanling Gao", "authors": "Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Xu\n  Wen, Rui Ren, Chen Zheng, Xiwen He, Hainan Ye, Haoning Tang, Zheng Cao,\n  Shujie Zhang and Jiahui Dai", "title": "BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several fundamental changes in technology indicate domain-specific hardware\nand software co-design is the only path left. In this context, architecture,\nsystem, data management, and machine learning communities pay greater attention\nto innovative big data and AI algorithms, architecture, and systems.\nUnfortunately, complexity, diversity, frequently-changed workloads, and rapid\nevolution of big data and AI systems raise great challenges. First, the\ntraditional benchmarking methodology that creates a new benchmark or proxy for\nevery possible workload is not scalable, or even impossible for Big Data and AI\nbenchmarking. Second, it is prohibitively expensive to tailor the architecture\nto characteristics of one or more application or even a domain of applications.\nWe consider each big data and AI workload as a pipeline of one or more classes\nof units of computation performed on different initial or intermediate data\ninputs, each class of which we call a data motif. On the basis of our previous\nwork that identifies eight data motifs taking up most of the run time of a wide\nvariety of big data and AI workloads, we propose a scalable benchmarking\nmethodology that uses the combination of one or more data motifs---to represent\ndiversity of big data and AI workloads. Following this methodology, we present\na unified big data and AI benchmark suite---BigDataBench 4.0, publicly\navailable from~\\url{http://prof.ict.ac.cn/BigDataBench}. This unified benchmark\nsuite sheds new light on domain-specific hardware and software co-design:\ntailoring the system and architecture to characteristics of the unified eight\ndata motifs other than one or more application case by case. Also, for the\nfirst time, we comprehensively characterize the CPU pipeline efficiency using\nthe benchmarks of seven workload types in BigDataBench 4.0.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 01:28:44 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 05:52:50 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Gao", "Wanling", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Luo", "Chunjie", ""], ["Zheng", "Daoyi", ""], ["Wen", "Xu", ""], ["Ren", "Rui", ""], ["Zheng", "Chen", ""], ["He", "Xiwen", ""], ["Ye", "Hainan", ""], ["Tang", "Haoning", ""], ["Cao", "Zheng", ""], ["Zhang", "Shujie", ""], ["Dai", "Jiahui", ""]]}, {"id": "1802.08800", "submitter": "Florin Rusu", "authors": "Yujing Ma, Florin Rusu, Martin Torres", "title": "Stochastic Gradient Descent on Highly-Parallel Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increased interest in building data analytics frameworks with\nadvanced algebraic capabilities both in industry and academia. Many of these\nframeworks, e.g., TensorFlow and BIDMach, implement their compute-intensive\nprimitives in two flavors---as multi-thread routines for multi-core CPUs and as\nhighly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is\nthe most popular optimization method for model training implemented extensively\non modern data analytics platforms. While the data-intensive properties of SGD\nare well-known, there is an intense debate on which of the many SGD variants is\nbetter in practice. In this paper, we perform a comprehensive study of parallel\nSGD for training generalized linear models. We consider the impact of three\nfactors -- computing architecture (multi-core CPU or GPU), synchronous or\nasynchronous model updates, and data sparsity -- on three measures---hardware\nefficiency, statistical efficiency, and time to convergence. In the process, we\ndesign an optimized asynchronous SGD algorithm for GPU that leverages warp\nshuffling and cache coalescing for data and model access. We draw several\ninteresting findings from our extensive experiments with logistic regression\n(LR) and support vector machines (SVM) on five real datasets. For synchronous\nSGD, GPU always outperforms parallel CPU---they both outperform a sequential\nCPU solution by more than 400X. For asynchronous SGD, parallel CPU is the\nsafest choice while GPU with data replication is better in certain situations.\nThe choice between synchronous GPU and asynchronous CPU depends on the task and\nthe characteristics of the data. As a reference, our best implementation\noutperforms TensorFlow and BIDMach consistently. We hope that our insights\nprovide a useful guide for applying parallel SGD to generalized linear models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 05:27:04 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Ma", "Yujing", ""], ["Rusu", "Florin", ""], ["Torres", "Martin", ""]]}, {"id": "1802.09080", "submitter": "Mohammad Noormohammadpour", "authors": "Mohammad Noormohammadpour and Cauligi S. Raghavendra", "title": "Minimizing Flow Completion Times using Adaptive Routing over\n  Inter-Datacenter Wide Area Networks", "comments": "Accepted into IEEE INFOCOM 2018 Poster Sessions, Honolulu, HI, USA", "journal-ref": "IEEE INFOCOM 2018 - IEEE Conference on Computer Communications\n  Workshops (2018) 1046-1047", "doi": "10.1109/INFCOMW.2018.8406853", "report-no": null, "categories": "cs.NI cs.DC cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-datacenter networks connect dozens of geographically dispersed\ndatacenters and carry traffic flows with highly variable sizes and different\nclasses. Adaptive flow routing can improve efficiency and performance by\nassigning paths to new flows according to network status and flow properties. A\npopular approach widely used for traffic engineering is based on current\nbandwidth utilization of links. We propose an alternative that reduces\nbandwidth usage by up to at least 50% and flow completion times by up to at\nleast 40% across various scheduling policies and flow size distributions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 21:14:30 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Noormohammadpour", "Mohammad", ""], ["Raghavendra", "Cauligi S.", ""]]}, {"id": "1802.09845", "submitter": "Kyriakos Georgiou", "authors": "Kyriakos Georgiou, Craig Blackmore, Samuel Xavier-de-Souza, Kerstin\n  Eder", "title": "Less is More: Exploiting the Standard Compiler Optimization Levels for\n  Better Performance and Energy Consumption", "comments": "15 pages, 3 figures, 71 benchmarks used for evaluation", "journal-ref": null, "doi": "10.1145/3207719.3207727", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the interesting observation that by performing fewer of\nthe optimizations available in a standard compiler optimization level such as\n-O2, while preserving their original ordering, significant savings can be\nachieved in both execution time and energy consumption. This observation has\nbeen validated on two embedded processors, namely the ARM Cortex-M0 and the ARM\nCortex-M3, using two different versions of the LLVM compilation framework; v3.8\nand v5.0. Experimental evaluation with 71 embedded benchmarks demonstrated\nperformance gains for at least half of the benchmarks for both processors. An\naverage execution time reduction of 2.4% and 5.3% was achieved across all the\nbenchmarks for the Cortex-M0 and Cortex-M3 processors, respectively, with\nexecution time improvements ranging from 1% up to 90% over the -O2. The savings\nthat can be achieved are in the same range as what can be achieved by the\nstate-of-the-art compilation approaches that use iterative compilation or\nmachine learning to select flags or to determine phase orderings that result in\nmore efficient code. In contrast to these time consuming and expensive to apply\ntechniques, our approach only needs to test a limited number of optimization\nconfigurations, less than 64, to obtain similar or even better savings.\nFurthermore, our approach can support multi-criteria optimization as it targets\nexecution time, energy consumption and code size at the same time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 12:12:37 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Georgiou", "Kyriakos", ""], ["Blackmore", "Craig", ""], ["Xavier-de-Souza", "Samuel", ""], ["Eder", "Kerstin", ""]]}, {"id": "1802.10461", "submitter": "Jianwei Zhao", "authors": "Jianwei Zhao, Hongxiang Xie, Feifei Gao, Weimin Jia, Shi Jin, and Hai\n  Lin", "title": "Time Varying Channel Tracking with Spatial and Temporal BEM for Massive\n  MIMO Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a channel tracking method for massive multi-input\nand multi-output systems under both time-varying and spatial-varying\ncircumstance. Exploiting the characteristics of massive antenna array, a\nspatial-temporal basis expansion model is designed to reduce the effective\ndimensions of up-link and down-link channel, which decomposes channel state\ninformation into the time-varying spatial information and gain information. We\nfirstly model the users movements as a one-order unknown Markov process, which\nis blindly learned by the expectation and maximization (EM) approach. Then, the\nup-link time varying spatial information can be blindly tracked by Taylor\nseries expansion of the steering vector, while the rest up-link channel gain\ninformation can be trained by only a few pilot symbols. Due to angle\nreciprocity (spatial reciprocity), the spatial information of the down-link\nchannel can be immediately obtained from the up-link counterpart, which greatly\nreduces the complexity of down-link channel tracking. Various numerical results\nare provided to demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 08:59:18 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Zhao", "Jianwei", ""], ["Xie", "Hongxiang", ""], ["Gao", "Feifei", ""], ["Jia", "Weimin", ""], ["Jin", "Shi", ""], ["Lin", "Hai", ""]]}]