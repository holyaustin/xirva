[{"id": "2001.00060", "submitter": "Issam Hammad", "authors": "Issam Hammad, Kamal El-Sankary, and Jason Gu", "title": "Deep Learning Training with Simulated Approximate Multipliers", "comments": "Presented at: IEEE International Conference on Robotics and\n  Biomimetics (ROBIO) 2019, Dali, China, December 2019. WINNER OF THE MOZI BEST\n  PAPER IN AI AWARD", "journal-ref": "2019 IEEE International Conference on Robotics and Biomimetics\n  (ROBIO)", "doi": "10.1109/ROBIO49542.2019.8961780", "report-no": null, "categories": "cs.LG cs.CV cs.PF eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents by simulation how approximate multipliers can be utilized\nto enhance the training performance of convolutional neural networks (CNNs).\nApproximate multipliers have significantly better performance in terms of\nspeed, power, and area compared to exact multipliers. However, approximate\nmultipliers have an inaccuracy which is defined in terms of the Mean Relative\nError (MRE). To assess the applicability of approximate multipliers in\nenhancing CNN training performance, a simulation for the impact of approximate\nmultipliers error on CNN training is presented. The paper demonstrates that\nusing approximate multipliers for CNN training can significantly enhance the\nperformance in terms of speed, power, and area at the cost of a small negative\nimpact on the achieved accuracy. Additionally, the paper proposes a hybrid\ntraining method which mitigates this negative impact on the accuracy. Using the\nproposed hybrid method, the training can start using approximate multipliers\nthen switches to exact multipliers for the last few epochs. Using this method,\nthe performance benefits of approximate multipliers in terms of speed, power,\nand area can be attained for a large portion of the training stage. On the\nother hand, the negative impact on the accuracy is diminished by using the\nexact multipliers for the last epochs of training.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 12:50:06 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 13:22:32 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hammad", "Issam", ""], ["El-Sankary", "Kamal", ""], ["Gu", "Jason", ""]]}, {"id": "2001.00660", "submitter": "Jiajia Li", "authors": "Jiajia Li and Mahesh Lakshminarasimhan and Xiaolong Wu and Ang Li and\n  Catherine Olschanowsky and Kevin Barker", "title": "A Parallel Sparse Tensor Benchmark Suite on CPUs and GPUs", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tensor computations present significant performance challenges that impact a\nwide spectrum of applications ranging from machine learning, healthcare\nanalytics, social network analysis, data mining to quantum chemistry and signal\nprocessing. Efforts to improve the performance of tensor computations include\nexploring data layout, execution scheduling, and parallelism in common tensor\nkernels. This work presents a benchmark suite for arbitrary-order sparse tensor\nkernels using state-of-the-art tensor formats: coordinate (COO) and\nhierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of\nreference tensor kernel implementations that are compatible with real-world\ntensors and power law tensors extended from synthetic graph generation\ntechniques. We also propose Roofline performance models for these kernels to\nprovide insights of computer platforms from sparse tensor view.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 22:56:15 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Li", "Jiajia", ""], ["Lakshminarasimhan", "Mahesh", ""], ["Wu", "Xiaolong", ""], ["Li", "Ang", ""], ["Olschanowsky", "Catherine", ""], ["Barker", "Kevin", ""]]}, {"id": "2001.00946", "submitter": "Quan-Lin Li", "authors": "Heng-Li Liu, Quan-Lin Li, Yan-Xia Chang, Chi Zhang", "title": "Block-Structured Double-Ended Queues and Bilateral QBD Processes", "comments": "43 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a block-structured double-ended queue, whose block\nstructure comes from two independent Markovian arrival processes (MAPs), and\nits stability is guaranteed by customers' impatient behaviors. We show that\nsuch a queue can be expressed as a new bilateral quasi birth-and-death (QBD)\nprocess. For this purpose, we provide a detailed analysis for the bilateral QBD\nprocess, including the system stability, the stationary probability vector, the\nsojourn time, and so forth. Furthermore, we develop three effective algorithms\nfor computing the performance measures (i.e., the probabilities of stationary\nqueue lengths, the average stationary queue lengths, and the average sojourn\ntimes) of the block-structured double-ended queue. Finally, numerical examples\nare employed to verify the correctness of our theoretical results, and\nillustrate how the performance measures of this queue are influenced by key\nsystem parameters. We believe that the methodology and results described in\nthis paper can be applied to deal with general matching queues (e.g., bilateral\nMarkov processes of GI/M/1 type and those of M/G/1 type) via developing their\ncorresponding bilateral block-structured Markov processes, which are very\nuseful in analyzing many practical issues, such as those encountered in sharing\neconomy, organ transplantation, intelligent manufacturing, intelligent\ntransportation, and so on.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 01:55:44 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 12:48:32 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 00:15:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Liu", "Heng-Li", ""], ["Li", "Quan-Lin", ""], ["Chang", "Yan-Xia", ""], ["Zhang", "Chi", ""]]}, {"id": "2001.01266", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Finally, how many efficiencies supercomputers have? And, what do they\n  measure?", "comments": "35 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using an extremely large number of processing elements in computing systems\nleads to unexpected phenomena, such as different efficiencies of the same\nsystem for different tasks, that cannot be explained in the frame of classical\ncomputing paradigm. The simple non-technical (but considering the temporal\nbehavior of the components) model, introduced here, enables us to set up a\nframe and formalism, needed to explain those unexpected experiences around\nsupercomputing. Introducing temporal behavior into computer science also\nexplains why only the extreme scale computing enabled us to reveal the\nexperienced limitations. The paper shows, that degradation of efficiency of\nparallelized sequential systems is a natural consequence of the classical\ncomputing paradigm, instead of being an engineering imperfectness. The\nworkload, that supercomputers run, is much responsible for wasting energy, as\nwell as limiting the size and type of tasks. Case studies provide insight, how\ndifferent contributions compete for dominating the resulting payload\nperformance of a computing system, and how enhancing the interconnection\ntechnology made computing+communication to dominate in defining the efficiency\nof supercomputers. Our model also enables to derive predictions about\nsupercomputer performance limitations for the near future, as well as it\nprovides hints for enhancing supercomputer components. Phenomena experienced in\nlarge-scale computing show interesting parallels with phenomena experienced in\nscience, more than a century ago, and through their studying a modern science\nwas developed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 16:11:13 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 11:48:20 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 13:07:29 GMT"}, {"version": "v4", "created": "Sun, 9 Aug 2020 16:52:25 GMT"}, {"version": "v5", "created": "Sat, 26 Sep 2020 19:12:50 GMT"}, {"version": "v6", "created": "Fri, 4 Dec 2020 19:21:54 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "2001.01653", "submitter": "Tobias Gysi", "authors": "Tobias Gysi, Tobias Grosser, Laurin Brandner, and Torsten Hoefler", "title": "A Fast Analytical Model of Fully Associative Caches", "comments": "14 pages, 16 figures, PLDI19", "journal-ref": null, "doi": "10.1145/3314221.3314606", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the cost of computation is an easy to understand local property, the\ncost of data movement on cached architectures depends on global state, does not\ncompose, and is hard to predict. As a result, programmers often fail to\nconsider the cost of data movement. Existing cache models and simulators\nprovide the missing information but are computationally expensive. We present a\nlightweight cache model for fully associative caches with least recently used\n(LRU) replacement policy that gives fast and accurate results. We count the\ncache misses without explicit enumeration of all memory accesses by using\nsymbolic counting techniques twice: 1) to derive the stack distance for each\nmemory access and 2) to count the memory accesses with stack distance larger\nthan the cache size. While this technique seems infeasible in theory, due to\nnon-linearities after the first round of counting, we show that the counting\nproblems are sufficiently linear in practice. Our cache model often computes\nthe results within seconds and contrary to simulation the execution time is\nmostly problem size independent. Our evaluation measures modeling errors below\n0.6% on real hardware. By providing accurate data placement information we\nenable memory hierarchy aware software development.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 16:35:46 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Gysi", "Tobias", ""], ["Grosser", "Tobias", ""], ["Brandner", "Laurin", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2001.02299", "submitter": "G\\'abor Sz\\'arnyas", "authors": "Renzo Angles, J\\'anos Benjamin Antal, Alex Averbuch, Peter Boncz, Orri\n  Erling, Andrey Gubichev, Vlad Haprian, Moritz Kaufmann, Josep Llu\\'is Larriba\n  Pey, Norbert Mart\\'inez, J\\'ozsef Marton, Marcus Paradies, Minh-Duc Pham,\n  Arnau Prat-P\\'erez, Mirko Spasi\\'c, Benjamin A. Steer, G\\'abor Sz\\'arnyas and\n  Jack Waudby", "title": "The LDBC Social Network Benchmark", "comments": "For the repository containing the source code of this technical\n  report, see https://github.com/ldbc/ldbc_snb_docs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Linked Data Benchmark Council's Social Network Benchmark (LDBC SNB) is an\neffort intended to test various functionalities of systems used for graph-like\ndata management. For this, LDBC SNB uses the recognizable scenario of operating\na social network, characterized by its graph-shaped data. LDBC SNB consists of\ntwo workloads that focus on different functionalities: the Interactive workload\n(interactive transactional queries) and the Business Intelligence workload\n(analytical queries). This document contains the definition of the Interactive\nWorkload and the first draft of the Business Intelligence Workload. This\nincludes a detailed explanation of the data used in the LDBC SNB benchmark, a\ndetailed description for all queries, and instructions on how to generate the\ndata and run the benchmark with the provided software.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 22:12:35 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 15:45:29 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Angles", "Renzo", ""], ["Antal", "J\u00e1nos Benjamin", ""], ["Averbuch", "Alex", ""], ["Boncz", "Peter", ""], ["Erling", "Orri", ""], ["Gubichev", "Andrey", ""], ["Haprian", "Vlad", ""], ["Kaufmann", "Moritz", ""], ["Pey", "Josep Llu\u00eds Larriba", ""], ["Mart\u00ednez", "Norbert", ""], ["Marton", "J\u00f3zsef", ""], ["Paradies", "Marcus", ""], ["Pham", "Minh-Duc", ""], ["Prat-P\u00e9rez", "Arnau", ""], ["Spasi\u0107", "Mirko", ""], ["Steer", "Benjamin A.", ""], ["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Waudby", "Jack", ""]]}, {"id": "2001.02396", "submitter": "Petros Spachos", "authors": "Andrew Mackey, Petros Spachos, Liang Song and Konstantinos Plataniotis", "title": "Improving BLE Beacon Proximity Estimation Accuracy through Bayesian\n  Filtering", "comments": null, "journal-ref": "JIOT.2020.2965583", "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interconnectedness of all things is continuously expanding which has\nallowed every individual to increase their level of interaction with their\nsurroundings. Internet of Things (IoT) devices are used in a plethora of\ncontext-aware application such as Proximity-Based Services (PBS), and\nLocation-Based Services (LBS). For these systems to perform, it is essential to\nhave reliable hardware and predict a user's position in the area with high\naccuracy in order to differentiate between individuals in a small area. A\nvariety of wireless solutions that utilize Received Signal Strength Indicators\n(RSSI) have been proposed to provide PBS and LBS for indoor environments,\nthough each solution presents its own drawbacks. In this work, Bluetooth Low\nEnergy (BLE) beacons are examined in terms of their accuracy in proximity\nestimation. Specifically, a mobile application is developed along with three\nBayesian filtering techniques to improve the BLE beacon proximity estimation\naccuracy. This includes a Kalman filter, a particle filter, and a\nNon-parametric Information (NI) filter. Since the RSSI is heavily influenced by\nthe environment, experiments were conducted to examine the performance of\nbeacons from three popular vendors in two different environments. The error is\ncompared in terms of Mean Absolute Error (MAE) and Root Mean Squared Error\n(RMSE). According to the experimental results, Bayesian filters can improve\nproximity estimation accuracy up to 30 % in comparison with traditional\nfiltering, when the beacon and the receiver are within 3 m.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 07:00:52 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mackey", "Andrew", ""], ["Spachos", "Petros", ""], ["Song", "Liang", ""], ["Plataniotis", "Konstantinos", ""]]}, {"id": "2001.02504", "submitter": "Pengfei Zhang", "authors": "Pengfei Zhang, Eric Lo, Baotong Lu", "title": "High Performance Depthwise and Pointwise Convolutions on Mobile Devices", "comments": "8 pages, Thirty-Four AAAI conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lightweight convolutional neural networks (e.g., MobileNets) are specifically\ndesigned to carry out inference directly on mobile devices. Among the various\nlightweight models, depthwise convolution (DWConv) and pointwise convolution\n(PWConv) are their key operations. In this paper, we observe that the existing\nimplementations of DWConv and PWConv are not well utilizing the ARM processors\nin the mobile devices, and exhibit lots of cache misses under multi-core and\npoor data reuse at register level. We propose techniques to re-optimize the\nimplementations of DWConv and PWConv based on ARM architecture. Experimental\nresults show that our implementation can respectively achieve a speedup of up\nto 5.5x and 2.1x against TVM (Chen et al. 2018) on DWConv and PWConv.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 09:39:18 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhang", "Pengfei", ""], ["Lo", "Eric", ""], ["Lu", "Baotong", ""]]}, {"id": "2001.02505", "submitter": "Romain Rouvoy", "authors": "Guillaume Fieni, Romain Rouvoy, Lionel Seinturier", "title": "SmartWatts: Self-Calibrating Software-Defined Power Meter for Containers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained power monitoring of software activities becomes unavoidable to\nmaximize the power usage efficiency of data centers. In particular, achieving\nan optimal scheduling of containers requires the deployment of software-defined\npower~meters to go beyond the granularity of hardware power monitoring sensors,\nsuch as Power Distribution Units (PDU) or Intel's Running Average Power Limit\n(RAPL), to deliver power estimations of activities at the granularity of\nsoftware~containers. However, the definition of the underlying power models\nthat estimate the power consumption remains a long and fragile process that is\ntightly coupled to the host machine.\n  To overcome these limitations, this paper introduces SmartWatts: a\nlightweight power monitoring system that adopts online calibration to\nautomatically adjust the CPU and DRAM power models in order to maximize the\naccuracy of runtime power estimations of containers. Unlike state-of-the-art\ntechniques, SmartWatts does not require any a priori training phase or hardware\nequipment to configure the power models and can therefore be deployed on a wide\nrange of machines including the latest power optimizations, at no cost.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 16:18:08 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Fieni", "Guillaume", ""], ["Rouvoy", "Romain", ""], ["Seinturier", "Lionel", ""]]}, {"id": "2001.02530", "submitter": "Jin Xu", "authors": "Jin Xu and Natarajan Gautam", "title": "On Competitive Analysis for Polling Systems", "comments": null, "journal-ref": "Naval Research Logistics. 2020; 67: 404-419", "doi": "10.1002/nav.21926", "report-no": null, "categories": "cs.PF cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polling systems have been widely studied, however most of these studies focus\non polling systems with renewal processes for arrivals and random variables for\nservice times. There is a need driven by practical applications to study\npolling systems with arbitrary arrivals (not restricted to time-varying or in\nbatches) and revealed service time upon a job's arrival. To address that need,\nour work considers a polling system with generic setting and for the first time\nprovides the worst-case analysis for online scheduling policies in this system.\nWe provide conditions for the existence of constant competitive ratios, and\ncompetitive lower bounds for general scheduling policies in polling systems.\nOur work also bridges the queueing and scheduling communities by proving the\ncompetitive ratios for several well-studied policies in the queueing\nliterature, such as cyclic policies with exhaustive, gated or l-limited service\ndisciplines for polling systems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 18:58:16 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 20:28:07 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Xu", "Jin", ""], ["Gautam", "Natarajan", ""]]}, {"id": "2001.04218", "submitter": "Devarpita Sinha", "authors": "Devarpita Sinha and Rajarshi Roy", "title": "Optimal Scheduling for Maximizing Information Freshness & System\n  Performance in Industrial Cyber-Physical Systems", "comments": "21 Pages, 6 figures, 2 tables", "journal-ref": "Computer Communications: available online 21 January, 2021", "doi": "10.1016/j.comcom.2021.01.015", "report-no": null, "categories": "cs.PF cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age of Information is a newly introduced metric, getting vivid attention for\nmeasuring the freshness of information in real-time networks. This parameter\nhas evolved to guarantee the reception of timely information from the latest\nstatus update, received by a user from any real-time application. In this\npaper, we study a centralized, closed-loop, networked controlled industrial\nwireless sensor-actuator network for cyber-physical production systems. Here,\nwe jointly address the problem of transmission scheduling of sensor updates and\nthe restoration of an information flow-line after any real-time update having\nhard-deadline drops from it, resulting a break in the loop. Unlike existing\nreal-time scheduling policies that only ensure timely updates, this work aims\nto accomplish both the time-sensitivity and data freshness in new and\nregenerative real-time updates in terms of the age of information. Here, the\ncoexistence of both cyber and physical units and their individual requirements\nfor providing the quality of service to the system, as a whole, seems to be one\nof the major challenges to handle. In this work, minimization of staleness of\nthe time-critical updates to extract maximum utilization out of its information\ncontent and its effects on other network performances are thoroughly\ninvestigated. A greedy scheduling policy called Deadline-aware highest latency\nfirst has been used to solve this problem; its performance optimality is proved\nanalytically. Finally, our claim is validated by comparing the results obtained\nby our algorithm with those of other popular scheduling policies through\nextensive simulations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:16:02 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 05:37:51 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Sinha", "Devarpita", ""], ["Roy", "Rajarshi", ""]]}, {"id": "2001.04438", "submitter": "Marat Dukhan", "authors": "Marat Dukhan and Artsiom Ablavatski", "title": "The Two-Pass Softmax Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The softmax (also called softargmax) function is widely used in machine\nlearning models to normalize real-valued scores into a probability\ndistribution. To avoid floating-point overflow, the softmax function is\nconventionally implemented in three passes: the first pass to compute the\nnormalization constant, and two other passes to compute outputs from normalized\ninputs. We analyze two variants of the Three-Pass algorithm and demonstrate\nthat in a well-optimized implementation on HPC-class processors performance of\nall three passes is limited by memory bandwidth. We then present a novel\nalgorithm for softmax computation in just two passes. The proposed Two-Pass\nalgorithm avoids both numerical overflow and the extra normalization pass by\nemploying an exotic representation for intermediate values, where each value is\nrepresented as a pair of floating-point numbers: one representing the\n\"mantissa\" and another representing the \"exponent\". Performance evaluation\ndemonstrates that on out-of-cache inputs on an Intel Skylake-X processor the\nnew Two-Pass algorithm outperforms the traditional Three-Pass algorithm by up\nto 28% in AVX512 implementation, and by up to 18% in AVX2 implementation. The\nproposed Two-Pass algorithm also outperforms the traditional Three-Pass\nalgorithm on Intel Broadwell and AMD Zen 2 processors. To foster\nreproducibility, we released an open-source implementation of the new Two-Pass\nSoftmax algorithm and other experiments in this paper as a part of XNNPACK\nlibrary at GitHub.com/google/XNNPACK.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:17:57 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Dukhan", "Marat", ""], ["Ablavatski", "Artsiom", ""]]}, {"id": "2001.05471", "submitter": "Abhishek  Sinha", "authors": "Subhankar Banerjee, Rajarshi Bhattacharjee, Abhishek Sinha", "title": "Fundamental Limits of Age-of-Information in Stationary and\n  Non-stationary Environments", "comments": "Submitted to ISIT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-user scheduling problem for minimizing the Age of\nInformation (AoI) in cellular wireless networks under stationary and\nnon-stationary regimes. We derive fundamental lower bounds for the scheduling\nproblem and design efficient online policies with provable performance\nguarantees. In the stationary setting, we consider the AoI optimization problem\nfor a set of mobile users travelling around multiple cells. In this setting, we\npropose a scheduling policy and show that it is $2$-optimal. Next, we propose a\nnew adversarial channel model for studying the scheduling problem in\nnon-stationary environments. For $N$ users, we show that the competitive ratio\nof any online scheduling policy in this setting is at least $\\Omega(N)$. We\nthen propose an online policy and show that it achieves a competitive ratio of\n$O(N^2)$. Finally, we introduce a relaxed adversarial model with channel state\nestimations for the immediate future. We propose a heuristic model predictive\ncontrol policy that exploits this feature and compare its performance through\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 18:35:47 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Banerjee", "Subhankar", ""], ["Bhattacharjee", "Rajarshi", ""], ["Sinha", "Abhishek", ""]]}, {"id": "2001.05572", "submitter": "Oliver Urbann", "authors": "Oliver Urbann, Simon Camphausen, Arne Moos, Ingmar Schwarz, S\\\"oren\n  Kerner, Maximilian Otten", "title": "A C Code Generator for Fast Inference and Simple Deployment of\n  Convolutional Neural Networks on Resource Constrained Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of Convolutional Neural Networks in time critical applications\nusually requires a GPU. In robotics or embedded devices these are often not\navailable due to energy, space and cost constraints. Furthermore, installation\nof a deep learning framework or even a native compiler on the target platform\nis not possible. This paper presents a neural network code generator (NNCG)\nthat generates from a trained CNN a plain ANSI C code file that encapsulates\nthe inference in single a function. It can easily be included in existing\nprojects and due to lack of dependencies, cross compilation is usually\npossible. Additionally, the code generation is optimized based on the known\ntrained CNN and target platform following four design principles. The system is\nevaluated utilizing small CNN designed for this application. Compared to\nTensorFlow XLA and Glow speed-ups of up to 11.81 can be shown and even GPUs are\noutperformed regarding latency.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 09:46:14 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Urbann", "Oliver", ""], ["Camphausen", "Simon", ""], ["Moos", "Arne", ""], ["Schwarz", "Ingmar", ""], ["Kerner", "S\u00f6ren", ""], ["Otten", "Maximilian", ""]]}, {"id": "2001.05811", "submitter": "Petr T\\r{u}ma", "authors": "Lubom\\'ir Bulej, Vojt\\v{e}ch Hork\\'y, Petr T\\r{u}ma, Fran\\c{c}ois\n  Farquet, Aleksandar Prokopec", "title": "Duet Benchmarking: Improving Measurement Accuracy in the Cloud", "comments": null, "journal-ref": null, "doi": "10.1145/3358960.3379132", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the duet measurement procedure, which helps improve the\naccuracy of performance comparison experiments conducted on shared machines by\nexecuting the measured artifacts in parallel and evaluating their relative\nperformance together, rather than individually. Specifically, we analyze the\nbehavior of the procedure in multiple cloud environments and use experimental\nevidence to answer multiple research questions concerning the assumption\nunderlying the procedure. We demonstrate improvements in accuracy ranging from\n2.3x to 12.5x (5.03x on average) for the tested ScalaBench (and DaCapo)\nworkloads, and from 23.8x to 82.4x (37.4x on average) for the SPEC CPU 2017\nworkloads.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 14:17:28 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 08:37:19 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Bulej", "Lubom\u00edr", ""], ["Hork\u00fd", "Vojt\u011bch", ""], ["T\u016fma", "Petr", ""], ["Farquet", "Fran\u00e7ois", ""], ["Prokopec", "Aleksandar", ""]]}, {"id": "2001.05952", "submitter": "Patrick Rodler", "authors": "Patrick Rodler", "title": "On Expert Behaviors and Question Types for Efficient Query-Based\n  Ontology Fault Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We challenge existing query-based ontology fault localization methods wrt.\nassumptions they make, criteria they optimize, and interaction means they use.\nWe find that their efficiency depends largely on the behavior of the\ninteracting expert, that performed calculations can be inefficient or\nimprecise, and that used optimization criteria are often not fully realistic.\nAs a remedy, we suggest a novel (and simpler) interaction approach which\novercomes all identified problems and, in comprehensive experiments on faulty\nreal-world ontologies, enables a successful fault localization while requiring\nfewer expert interactions in 66 % of the cases, and always at least 80 % less\nexpert waiting time, compared to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 17:23:07 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Rodler", "Patrick", ""]]}, {"id": "2001.06108", "submitter": "Hussain Al-Aqrabi", "authors": "Hussain Al-Aqrabi, Phil Lane, Richard Hill", "title": "Performance Evaluation of Multiparty Authentication in 5G IIoT\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of various emerging technologies such as the\nIndustrial Internet of Things (IIoT), there is a need to secure communications\nbetween such devices. Communication system delays are one of the factors that\nadversely affect the performance of an authentication system. 5G networks\nenable greater data throughput and lower latency, which presents new\nopportunities for the secure authentication of business transactions between\nIIoT devices. We evaluate an approach to developing a flexible and secure model\nfor authenticating IIoT components in dynamic 5G environments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 23:07:11 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Al-Aqrabi", "Hussain", ""], ["Lane", "Phil", ""], ["Hill", "Richard", ""]]}, {"id": "2001.06370", "submitter": "Nicholas Gerard Timmons", "authors": "Nicholas Gerard Timmons, Andrew Rice", "title": "Approximating Activation Functions", "comments": "10 Pages, 5 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ReLU is widely seen as the default choice for activation functions in neural\nnetworks. However, there are cases where more complicated functions are\nrequired. In particular, recurrent neural networks (such as LSTMs) make\nextensive use of both hyperbolic tangent and sigmoid functions. These functions\nare expensive to compute. We used function approximation techniques to develop\nreplacements for these functions and evaluated them empirically on three\npopular network configurations. We find safe approximations that yield a 10% to\n37% improvement in training times on the CPU. These approximations were\nsuitable for all cases we considered and we believe are appropriate\nreplacements for all networks using these activation functions. We also develop\nranged approximations which only apply in some cases due to restrictions on\ntheir input domain. Our ranged approximations yield a performance improvement\nof 20% to 53% in network training time. Our functions also match or\nconsiderably out perform the ad-hoc approximations used in Theano and the\nimplementation of Word2Vec.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 15:25:44 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Timmons", "Nicholas Gerard", ""], ["Rice", "Andrew", ""]]}, {"id": "2001.06841", "submitter": "Sungjin Im", "authors": "Sungjin Im, Benjamin Moseley, Kamesh Munagala, Kirk Pruhs", "title": "Dynamic Weighted Fairness with Minimal Disruptions", "comments": "To appear in Proceedings of the ACM on Measurement and Analysis of\n  Computing Systems (POMACS) 2020 (SIGMETRICS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the following dynamic fair allocation problem:\nGiven a sequence of job arrivals and departures, the goal is to maintain an\napproximately fair allocation of the resource against a target fair allocation\npolicy, while minimizing the total number of disruptions, which is the number\nof times the allocation of any job is changed. We consider a rich class of fair\nallocation policies that significantly generalize those considered in previous\nwork.\n  We first consider the models where jobs only arrive, or jobs only depart. We\npresent tight upper and lower bounds for the number of disruptions required to\nmaintain a constant approximate fair allocation every time step. In particular,\nfor the canonical case where jobs have weights and the resource allocation is\nproportional to the job's weight, we show that maintaining a constant\napproximate fair allocation requires $\\Theta(\\log^* n)$ disruptions per job,\nalmost matching the bounds in prior work for the unit weight case. For the more\ngeneral setting where the allocation policy only decreases the allocation to a\njob when new jobs arrive, we show that maintaining a constant approximate fair\nallocation requires $\\Theta(\\log n)$ disruptions per job. We then consider the\nmodel where jobs can both arrive and depart. We first show strong lower bounds\non the number of disruptions required to maintain constant approximate fairness\nfor arbitrary instances. In contrast we then show that there there is an\nalgorithm that can maintain constant approximate fairness with $O(1)$ expected\ndisruptions per job if the weights of the jobs are independent of the jobs\narrival and departure order. We finally show how our results can be extended to\nthe setting with multiple resources.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 14:55:46 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Im", "Sungjin", ""], ["Moseley", "Benjamin", ""], ["Munagala", "Kamesh", ""], ["Pruhs", "Kirk", ""]]}, {"id": "2001.06935", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Tim Davis, Chansup Byun, William Arcand, David Bestor,\n  William Bergeron, Vijay Gadepally, Matthew Hubbell, Michael Houle, Michael\n  Jones, Anna Klein, Peter Michaleas, Lauren Milechin, Julie Mullen, Andrew\n  Prout, Antonio Rosa, Siddharth Samsi, Charles Yee, Albert Reuther", "title": "75,000,000,000 Streaming Inserts/Second Using Hierarchical Hypersparse\n  GraphBLAS Matrices", "comments": "4 pages, 4 figures, 28 references, accepted to IPDPS GrAPL 2020.\n  arXiv admin note: substantial text overlap with arXiv:1907.04217", "journal-ref": null, "doi": "10.1109/IPDPSW50202.2020.00046", "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SuiteSparse GraphBLAS C-library implements high performance hypersparse\nmatrices with bindings to a variety of languages (Python, Julia, and\nMatlab/Octave). GraphBLAS provides a lightweight in-memory database\nimplementation of hypersparse matrices that are ideal for analyzing many types\nof network data, while providing rigorous mathematical guarantees, such as\nlinearity. Streaming updates of hypersparse matrices put enormous pressure on\nthe memory hierarchy. This work benchmarks an implementation of hierarchical\nhypersparse matrices that reduces memory pressure and dramatically increases\nthe update rate into a hypersparse matrices. The parameters of hierarchical\nhypersparse matrices rely on controlling the number of entries in each level in\nthe hierarchy before an update is cascaded. The parameters are easily tunable\nto achieve optimal performance for a variety of applications. Hierarchical\nhypersparse matrices achieve over 1,000,000 updates per second in a single\ninstance. Scaling to 31,000 instances of hierarchical hypersparse matrices\narrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update\nrate of 75,000,000,000 updates per second. This capability allows the MIT\nSuperCloud to analyze extremely large streaming network data sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 01:41:17 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 20:50:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kepner", "Jeremy", ""], ["Davis", "Tim", ""], ["Byun", "Chansup", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "2001.07104", "submitter": "Holger Fr\\\"oning", "authors": "Lorenz Braun, Sotirios Nikas, Chen Song, Vincent Heuveline, Holger\n  Fr\\\"oning", "title": "A Simple Model for Portable and Fast Prediction of Execution Time and\n  Power Consumption of GPU Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing compute kernel execution behavior on GPUs for efficient task\nscheduling is a non-trivial task. We address this with a simple model enabling\nportable and fast predictions among different GPUs using only\nhardware-independent features. This model is built based on random forests\nusing 189 individual compute kernels from benchmarks such as Parboil, Rodinia,\nPolybench-GPU and SHOC. Evaluation of the model performance using\ncross-validation yields a median Mean Average Percentage Error (MAPE) of\n8.86-52.00% and 1.84-2.94%, for time respectively power prediction across five\ndifferent GPUs, while latency for a single prediction varies between 15 and 108\nmilliseconds.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:40:54 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 12:15:51 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 12:47:57 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Braun", "Lorenz", ""], ["Nikas", "Sotirios", ""], ["Song", "Chen", ""], ["Heuveline", "Vincent", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2001.07747", "submitter": "Maciej Besta", "authors": "Robert Gerstenberger, Maciej Besta, and Torsten Hoefler", "title": "Enabling Highly-Scalable Remote Memory Access Programming with MPI-3 One\n  Sided", "comments": "Best Paper Award at ACM/IEEE Supercomputing'13 (1/92), also Best\n  Student Paper finalist (8/92); source code of foMPI can be downloaded from\n  http://spcl.inf.ethz.ch/Research/Parallel_Programming/foMPI", "journal-ref": "Proceedings of the ACM/IEEE International Conference on High\n  Performance Computing, Networking, Storage and Analysis, pages 53:1--53:12,\n  November 2013", "doi": "10.1145/2503210.2503286", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern interconnects offer remote direct memory access (RDMA) features. Yet,\nmost applications rely on explicit message passing for communications albeit\ntheir unwanted overheads. The MPI-3.0 standard defines a programming interface\nfor exploiting RDMA networks directly, however, it's scalability and\npracticability has to be demonstrated in practice. In this work, we develop\nscalable bufferless protocols that implement the MPI-3.0 specification. Our\nprotocols support scaling to millions of cores with negligible memory\nconsumption while providing highest performance and minimal overheads. To arm\nprogrammers, we provide a spectrum of performance models for all critical\nfunctions and demonstrate the usability of our library and models with several\napplication studies with up to half a million processes. We show that our\ndesign is comparable to, or better than UPC and Fortran Coarrays in terms of\nlatency, bandwidth, and message rate. We also demonstrate application\nperformance improvements with comparable programming complexity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 19:20:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 13:20:08 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gerstenberger", "Robert", ""], ["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2001.07817", "submitter": "Maria Apostolaki", "authors": "Maria Apostolaki, Ankit Singla, Laurent Vanbever", "title": "Performance-Driven Internet Path Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet routing can often be sub-optimal, with the chosen routes providing\nworse performance than other available policy-compliant routes. This stems from\nthe lack of visibility into route performance at the network layer. While this\nis an old problem, we argue that recent advances in programmable hardware\nfinally open up the possibility of performance-aware routing in a deployable,\nBGP-compatible manner. We introduce ROUTESCOUT, a hybrid hardware/software\nsystem supporting performance-based routing at ISP scale. In the data plane,\nROUTESCOUT leverages P4-enabled hardware to monitor performance across\npolicy-compliant route choices for each destination, at line-rate and with a\nsmall memory footprint. ROUTESCOUT's control plane then asynchronously pulls\naggregated performance metrics to synthesize a performance-aware forwarding\npolicy. We show that ROUTESCOUT can monitor performance across most of an ISP's\ntraffic, using only 4 MB of memory. Further, its control can flexibly satisfy a\nvariety of operator objectives, with sub-second operating times.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 23:51:17 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 21:34:35 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Apostolaki", "Maria", ""], ["Singla", "Ankit", ""], ["Vanbever", "Laurent", ""]]}, {"id": "2001.07938", "submitter": "Bruce Collie", "authors": "Philip Ginsbach, Bruce Collie, Michael F.P. O'Boyle", "title": "Automatically Harnessing Sparse Acceleration", "comments": "Accepted to CC 2020", "journal-ref": null, "doi": "10.1145/3377555.3377893", "report-no": null, "categories": "cs.PF cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear algebra is central to many scientific programs, yet compilers\nfail to optimize it well. High-performance libraries are available, but\nadoption costs are significant. Moreover, libraries tie programs into\nvendor-specific software and hardware ecosystems, creating non-portable code.\n  In this paper, we develop a new approach based on our specification Language\nfor implementers of Linear Algebra Computations (LiLAC). Rather than requiring\nthe application developer to (re)write every program for a given library, the\nburden is shifted to a one-off description by the library implementer. The\nLiLAC-enabled compiler uses this to insert appropriate library routines without\nsource code changes.\n  LiLAC provides automatic data marshaling, maintaining state between calls and\nminimizing data transfers. Appropriate places for library insertion are\ndetected in compiler intermediate representation, independent of source\nlanguages.\n  We evaluated on large-scale scientific applications written in FORTRAN;\nstandard C/C++ and FORTRAN benchmarks; and C++ graph analytics kernels. Across\nheterogeneous platforms, applications and data sets we show speedups of\n1.1$\\times$ to over 10$\\times$ without user intervention.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 10:04:36 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Ginsbach", "Philip", ""], ["Collie", "Bruce", ""], ["O'Boyle", "Michael F. P.", ""]]}, {"id": "2001.08448", "submitter": "Priyank Faldu", "authors": "Priyank Faldu and Jeff Diamond and Boris Grot", "title": "A Closer Look at Lightweight Graph Reordering", "comments": "Fixed ill-formatted page 6 from the earlier version. No content\n  changes", "journal-ref": "In Proceedings of the IEEE International Symposium on Workload\n  Characterization (IISWC), Orlando, Florida, USA, 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph analytics power a range of applications in areas as diverse as finance,\nnetworking and business logistics. A common property of graphs used in the\ndomain of graph analytics is a power-law distribution of vertex connectivity,\nwherein a small number of vertices are responsible for a high fraction of all\nconnections in the graph. These richly-connected (hot) vertices inherently\nexhibit high reuse. However, their sparse distribution in memory leads to a\nsevere underutilization of on-chip cache capacity. Prior works have proposed\nlightweight skew-aware vertex reordering that places hot vertices adjacent to\neach other in memory, reducing the cache footprint of hot vertices. However, in\ndoing so, they may inadvertently destroy the inherent community structure\nwithin the graph, which may negate the performance gains achieved from the\nreduced footprint of hot vertices.\n  In this work, we study existing reordering techniques and demonstrate the\ninherent tension between reducing the cache footprint of hot vertices and\npreserving original graph structure. We quantify the potential performance loss\ndue to disruption in graph structure for different graph datasets. We further\nshow that reordering techniques that employ fine-grain reordering significantly\nincrease misses in the higher level caches, even when they reduce misses in the\nlast-level cache.\n  To overcome the limitations of existing reordering techniques, we propose\nDegree-Based Grouping (DBG), a novel lightweight reordering technique that\nemploys a coarse-grain reordering to largely preserve graph structure while\nreducing the cache footprint of hot vertices. Our evaluation on 40 combinations\nof various graph applications and datasets shows that, compared to a baseline\nwith no reordering, DBG yields an average application speed-up of 16.8% vs\n11.6% for the best-performing existing lightweight technique.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 11:11:01 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 11:20:50 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Faldu", "Priyank", ""], ["Diamond", "Jeff", ""], ["Grot", "Boris", ""]]}, {"id": "2001.09018", "submitter": "Gabriele D'Angelo", "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D'Angelo", "title": "Are Distributed Ledger Technologies Ready for Smart Transportation\n  Systems?", "comments": "Proceedings of the 3rd Workshop on Cryptocurrencies and Blockchains\n  for Distributed Systems (CryBlock 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to understand whether Distributed Ledger\nTechnologies (DLTs) are ready to support complex services, such as those\nrelated to Intelligent Transportation Systems (ITS). In smart transportation\nservices, a huge amount of sensed data is generated by a multitude of vehicles.\nWhile DLTs provide very interesting features, such as immutability,\ntraceability and verifiability of data, some doubts on the scalability and\nresponsiveness of these technologies appear to be well-founded. We propose an\narchitecture for ITS that resorts to DLT features. Moreover, we provide\nexperimental results of a real test-bed over IOTA, a promising DLT for IoT.\nResults clearly show that, while the viability of the proposal cannot be\nrejected, further work is needed on the responsiveness of DLT infrastructures.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 10:32:07 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 12:14:48 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Zichichi", "Mirko", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "2001.09249", "submitter": "Ahsan Ali Mr", "authors": "Zheng Chai, Ahsan Ali, Syed Zawad, Stacey Truex, Ali Anwar, Nathalie\n  Baracaldo, Yi Zhou, Heiko Ludwig, Feng Yan, Yue Cheng", "title": "TiFL: A Tier-based Federated Learning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) enables learning a shared model across many clients\nwithout violating the privacy requirements. One of the key attributes in FL is\nthe heterogeneity that exists in both resource and data due to the differences\nin computation and communication capacity, as well as the quantity and content\nof data among different clients. We conduct a case study to show that\nheterogeneity in resource and data has a significant impact on training time\nand model accuracy in conventional FL systems. To this end, we propose TiFL, a\nTier-based Federated Learning System, which divides clients into tiers based on\ntheir training performance and selects clients from the same tier in each\ntraining round to mitigate the straggler problem caused by heterogeneity in\nresource and data quantity. To further tame the heterogeneity caused by non-IID\n(Independent and Identical Distribution) data and resources, TiFL employs an\nadaptive tier selection approach to update the tiering on-the-fly based on the\nobserved training performance and accuracy overtime. We prototype TiFL in a FL\ntestbed following Google's FL architecture and evaluate it using popular\nbenchmarks and the state-of-the-art FL benchmark LEAF. Experimental evaluation\nshows that TiFL outperforms the conventional FL in various heterogeneous\nconditions. With the proposed adaptive tier selection policy, we demonstrate\nthat TiFL achieves much faster training performance while keeping the same (and\nin some cases - better) test accuracy across the board.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 01:40:42 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Chai", "Zheng", ""], ["Ali", "Ahsan", ""], ["Zawad", "Syed", ""], ["Truex", "Stacey", ""], ["Anwar", "Ali", ""], ["Baracaldo", "Nathalie", ""], ["Zhou", "Yi", ""], ["Ludwig", "Heiko", ""], ["Yan", "Feng", ""], ["Cheng", "Yue", ""]]}, {"id": "2001.09399", "submitter": "Suraj Padmanaban Kesavan", "authors": "Suraj P. Kesavan, Takanori Fujiwara, Jianping Kelvin Li, Caitlin Ross,\n  Misbah Mubarak, Christopher D. Carothers, Robert B. Ross, Kwan-Liu Ma", "title": "A Visual Analytics Framework for Reviewing Streaming Performance Data", "comments": "This is the author's preprint version that will be published in\n  Proceedings of IEEE Pacific Visualization Symposium, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and tuning the performance of extreme-scale parallel computing\nsystems demands a streaming approach due to the computational cost of applying\noffline algorithms to vast amounts of performance log data. Analyzing large\nstreaming data is challenging because the rate of receiving data and limited\ntime to comprehend data make it difficult for the analysts to sufficiently\nexamine the data without missing important changes or patterns. To support\nstreaming data analysis, we introduce a visual analytic framework comprising of\nthree modules: data management, analysis, and interactive visualization. The\ndata management module collects various computing and communication performance\nmetrics from the monitored system using streaming data processing techniques\nand feeds the data to the other two modules. The analysis module automatically\nidentifies important changes and patterns at the required latency. In\nparticular, we introduce a set of online and progressive analysis methods for\nnot only controlling the computational costs but also helping analysts better\nfollow the critical aspects of the analysis results. Finally, the interactive\nvisualization module provides the analysts with a coherent view of the changes\nand patterns in the continuously captured performance data. Through a\nmulti-faceted case study on performance analysis of parallel discrete-event\nsimulation, we demonstrate the effectiveness of our framework for identifying\nbottlenecks and locating outliers.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 04:34:22 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Kesavan", "Suraj P.", ""], ["Fujiwara", "Takanori", ""], ["Li", "Jianping Kelvin", ""], ["Ross", "Caitlin", ""], ["Mubarak", "Misbah", ""], ["Carothers", "Christopher D.", ""], ["Ross", "Robert B.", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2001.09670", "submitter": "Rafael Pereira Pires", "authors": "Rafael Pereira Pires", "title": "Distributed systems and trusted execution environments: Trade-offs and\n  challenges", "comments": "133 pages, PhD thesis, University of Neuch\\^atel, Defended on\n  December 3rd, 2019", "journal-ref": null, "doi": "10.35662/unine-thesis-2812", "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Security and privacy concerns in computer systems have grown in importance\nwith the ubiquity of connected devices. TEEs provide security guarantees based\non cryptographic constructs built in hardware. Intel software guard extensions\n(SGX), in particular, implements powerful mechanisms that can shield sensitive\ndata even from privileged users with full control of system software. In this\nwork, we essentially explore some of the challenges of designing secure\ndistributed systems by using Intel SGX as cornerstone. We do so by designing\nand experimentally evaluating several elementary systems ranging from\ncommunication and processing middleware to a peer-to-peer privacy-preserving\nsolution. We start with support systems that naturally fit cloud deployment\nscenarios, namely content-based routing, batching and stream processing\nframeworks. We implement prototypes and use them to analyse the manifested\nmemory usage issues intrinsic to SGX. Next, we aim at protecting very sensitive\ndata: cryptographic keys. By leveraging TEEs, we design protocols for group\ndata sharing that have lower computational complexity than legacy methods. As a\nbonus, our proposals allow large savings on metadata volume and processing time\nof cryptographic operations, all with equivalent security guarantees. Finally,\nwe propose privacy-preserving systems against established services like\nweb-search engines. Our evaluation shows that we propose the most robust system\nin comparison to existing solutions with regard to user re-identification rates\nand results accuracy in a scalable way. Overall, this thesis proposes new\nmechanisms that take advantage of TEEs for distributed system architectures. We\nshow through an empirical approach on top of Intel SGX what are the trade-offs\nof distinct designs applied to distributed communication and processing,\ncryptographic protocols and private web search.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 10:27:55 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 12:05:33 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 10:52:12 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Pires", "Rafael Pereira", ""]]}, {"id": "2001.09783", "submitter": "Priyank Faldu", "authors": "Priyank Faldu and Jeff Diamond and Boris Grot", "title": "Domain-Specialized Cache Management for Graph Analytics", "comments": "No content changes from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph analytics power a range of applications in areas as diverse as finance,\nnetworking and business logistics. A common property of graphs used in the\ndomain of graph analytics is a power-law distribution of vertex connectivity,\nwherein a small number of vertices are responsible for a high fraction of all\nconnections in the graph. These richly-connected, hot, vertices inherently\nexhibit high reuse. However, this work finds that state-of-the-art hardware\ncache management schemes struggle in capitalizing on their reuse due to highly\nirregular access patterns of graph analytics.\n  In response, we propose GRASP, domain-specialized cache management at the\nlast-level cache for graph analytics. GRASP augments existing cache policies to\nmaximize reuse of hot vertices by protecting them against cache thrashing,\nwhile maintaining sufficient flexibility to capture the reuse of other vertices\nas needed. GRASP keeps hardware cost negligible by leveraging lightweight\nsoftware support to pinpoint hot vertices, thus eliding the need for\nstorage-intensive prediction mechanisms employed by state-of-the-art cache\nmanagement schemes. On a set of diverse graph-analytic applications with large\nhigh-skew graph datasets, GRASP outperforms prior domain-agnostic schemes on\nall datapoints, yielding an average speed-up of 4.2% (max 9.4%) over the\nbest-performing prior scheme. GRASP remains robust on low-/no-skew datasets,\nwhereas prior schemes consistently cause a slowdown.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 18:46:26 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 11:40:31 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Faldu", "Priyank", ""], ["Diamond", "Jeff", ""], ["Grot", "Boris", ""]]}, {"id": "2001.10381", "submitter": "Jaya Prakash Champati Dr", "authors": "Jaya Prakash Champati, Mikael Skoglund, and James Gross", "title": "Detecting State Transitions of a Markov Source: Sampling Frequency and\n  Age Trade-off", "comments": "6 pages, published in IEEE INFOCOM AoI Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a finite-state Discrete-Time Markov Chain (DTMC) source that can\nbe sampled for detecting the events when the DTMC transits to a new state. Our\ngoal is to study the trade-off between sampling frequency and staleness in\ndetecting the events. We argue that, for the problem at hand, using Age of\nInformation (AoI) for quantifying the staleness of a sample is conservative and\ntherefore, introduce \\textit{age penalty} for this purpose. We study two\noptimization problems: minimize average age penalty subject to an average\nsampling frequency constraint, and minimize average sampling frequency subject\nto an average age penalty constraint; both are Constrained Markov Decision\nProblems. We solve them using linear programming approach and compute Markov\npolicies that are optimal among all causal policies. Our numerical results\ndemonstrate that the computed Markov policies not only outperform optimal\nperiodic sampling policies, but also achieve sampling frequencies close to or\nlower than that of an optimal clairvoyant (non-causal) sampling policy, if a\nsmall age penalty is allowed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 07:35:09 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 16:36:25 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Champati", "Jaya Prakash", ""], ["Skoglund", "Mikael", ""], ["Gross", "James", ""]]}, {"id": "2001.10621", "submitter": "Xiaozhu Meng", "authors": "Xiaozhu Meng, Jonathon M. Anderson, John Mellor-Crummey, Mark W.\n  Krentel, Barton P. Miller, Sr{\\dj}an Milakovi\\'c", "title": "Parallel Binary Code Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary code analysis is widely used to assess a program's correctness,\nperformance, and provenance. Binary analysis applications often construct\ncontrol flow graphs, analyze data flow, and use debugging information to\nunderstand how machine code relates to source lines, inlined functions, and\ndata types. To date, binary analysis has been single-threaded, which is too\nslow for applications such as performance analysis and software forensics,\nwhere it is becoming common to analyze binaries that are gigabytes in size and\nin large batches that contain thousands of binaries.\n  This paper describes our design and implementation for accelerating the task\nof constructing control flow graphs (CFGs) from binaries with multithreading.\nExisting research focuses on addressing challenging code constructs encountered\nduring constructing CFGs, including functions sharing code, jump table\nanalysis, non-returning functions, and tail calls. However, existing analyses\ndo not consider the complex interactions between concurrent analysis of shared\ncode, making it difficult to extend existing serial algorithms to be parallel.\nA systematic methodology to guide the design of parallel algorithms is\nessential. We abstract the task of constructing CFGs as repeated applications\nof several core CFG operations regarding to creating functions, basic blocks,\nand edges. We then derive properties among CFG operations, including operation\ndependency, commutativity, monotonicity. These operation properties guide our\ndesign of a new parallel analysis for constructing CFGs. We achieved as much as\n25$\\times$ speedup for constructing CFGs on 64 hardware threads. Binary\nanalysis applications are significantly accelerated with the new parallel\nanalysis: we achieve 8$\\times$ for a performance analysis tool and 7$\\times$\nfor a software forensic tool with 16 hardware threads.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 22:23:51 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 19:44:29 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 20:22:35 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Meng", "Xiaozhu", ""], ["Anderson", "Jonathon M.", ""], ["Mellor-Crummey", "John", ""], ["Krentel", "Mark W.", ""], ["Miller", "Barton P.", ""], ["Milakovi\u0107", "Sr\u0111an", ""]]}, {"id": "2001.10710", "submitter": "Souvik Kundu", "authors": "Souvik Kundu, Mahdi Nazemi, Massoud Pedram, Keith M. Chugg, Peter A.\n  Beerel", "title": "Pre-defined Sparsity for Low-Complexity Convolutional Neural Networks", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high energy cost of processing deep convolutional neural networks impedes\ntheir ubiquitous deployment in energy-constrained platforms such as embedded\nsystems and IoT devices. This work introduces convolutional layers with\npre-defined sparse 2D kernels that have support sets that repeat periodically\nwithin and across filters. Due to the efficient storage of our periodic sparse\nkernels, the parameter savings can translate into considerable improvements in\nenergy efficiency due to reduced DRAM accesses, thus promising significant\nimprovements in the trade-off between energy consumption and accuracy for both\ntraining and inference. To evaluate this approach, we performed experiments\nwith two widely accepted datasets, CIFAR-10 and Tiny ImageNet in sparse\nvariants of the ResNet18 and VGG16 architectures. Compared to baseline models,\nour proposed sparse variants require up to 82% fewer model parameters with\n5.6times fewer FLOPs with negligible loss in accuracy for ResNet18 on CIFAR-10.\nFor VGG16 trained on Tiny ImageNet, our approach requires 5.8times fewer FLOPs\nand up to 83.3% fewer model parameters with a drop in top-5 (top-1) accuracy of\nonly 1.2% (2.1%). We also compared the performance of our proposed\narchitectures with that of ShuffleNet andMobileNetV2. Using similar\nhyperparameters and FLOPs, our ResNet18 variants yield an average accuracy\nimprovement of 2.8%.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 07:10:56 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 19:26:39 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Kundu", "Souvik", ""], ["Nazemi", "Mahdi", ""], ["Pedram", "Massoud", ""], ["Chugg", "Keith M.", ""], ["Beerel", "Peter A.", ""]]}, {"id": "2001.10845", "submitter": "Jun Zhao", "authors": "Yulan Gao, Chao Yong, Zehui Xiong, Dusit Niyato, Yue Xiao, Jun Zhao", "title": "Reconfigurable Intelligent Surface for MISO Systems with Proportional\n  Rate Constraints", "comments": "This paper appears in the Proceedings of IEEE International\n  Conference on Communications (ICC) 2020. Please feel free to contact us for\n  questions or remarks", "journal-ref": "Proceedings of IEEE International Conference on Communications\n  (ICC) 2020", "doi": null, "report-no": null, "categories": "cs.IT cs.NI cs.PF eess.SP math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the system spectral efficiency (SE) in reconfigurable\nintelligent surface (RIS)-aided multiuser multiple-input single-output (MISO)\nsystems, where RIS can reconfigure the propagation environment via a large\nnumber of controllable and intelligent phase shifters. In order to explore the\nsystem SE performance behavior with user proportional fairness for such a\nsystem, an optimization problem is formulated to maximize the SE by jointly\nconsidering the power allocation at the base station (BS) and phase shift at\nthe RIS, under nonlinear proportional rate fairness constraints. To solve the\nnonconvex optimization problem, an effective solution is developed, which\ncapitalizes on an iterative algorithm with closed-form expressions, i.e.,\nalternatively optimizing the transmit power at the BS and the reflecting phase\nshift at the RIS. Numerical simulations are provided to validate the\ntheoretical analysis and assess the performance of the proposed alternative\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 14:01:54 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Gao", "Yulan", ""], ["Yong", "Chao", ""], ["Xiong", "Zehui", ""], ["Niyato", "Dusit", ""], ["Xiao", "Yue", ""], ["Zhao", "Jun", ""]]}, {"id": "2001.11100", "submitter": "Gezim Sejdiu", "authors": "Gezim Sejdiu, Anisa Rula, Jens Lehmann, and Hajira Jabeen", "title": "A Scalable Framework for Quality Assessment of RDF Datasets", "comments": "International Semantic Web Conference (ISWC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last years, Linked Data has grown continuously. Today, we count more\nthan 10,000 datasets being available online following Linked Data standards.\nThese standards allow data to be machine readable and inter-operable.\nNevertheless, many applications, such as data integration, search, and\ninterlinking, cannot take full advantage of Linked Data if it is of low\nquality. There exist a few approaches for the quality assessment of Linked\nData, but their performance degrades with the increase in data size and quickly\ngrows beyond the capabilities of a single machine. In this paper, we present\nDistQualityAssessment -- an open source implementation of quality assessment of\nlarge RDF datasets that can scale out to a cluster of machines. This is the\nfirst distributed, in-memory approach for computing different quality metrics\nfor large RDF datasets using Apache Spark. We also provide a quality assessment\npattern that can be used to generate new scalable metrics that can be applied\nto big data. The work presented here is integrated with the SANSA framework and\nhas been applied to at least three use cases beyond the SANSA community. The\nresults show that our approach is more generic, efficient, and scalable as\ncompared to previously proposed approaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 21:30:14 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Sejdiu", "Gezim", ""], ["Rula", "Anisa", ""], ["Lehmann", "Jens", ""], ["Jabeen", "Hajira", ""]]}, {"id": "2001.11423", "submitter": "Mouktar Bello", "authors": "Mouktar Bello", "title": "Asymptotic regime analysis of NOMA uplink networks under QoS delay\n  Constraints", "comments": "32 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fifth generation and beyond (B5G) technologies, delay constrains\nemerge as a topic of particular interest for ultra reliable low latency\ncommunications (e.g., enhanced reality, haptic communications). In this report,\nwe study the performance of a two user uplink non orthogonal multiple access\n(NOMA) network under quality of service (QoS) delay constraints, captured\nthrough each user delay exponents in their effective capacity (EC). We propose\nnovel closed form expressions for the EC of the NOMA users and validate them\nthrough Monte Carlo simulations. Interestingly, our study shows that in the\nhigh signal to noise ratio (SNR) region, the strong NOMA user has a limited EC\nno matter how large the transmit SNR is, under the same delay constraint as the\nweak user. We show that for the weak user OMA achieves higher EC than NOMA at\nsmall values of the transmit SNR and that NOMA become more beneficial at high\nvalues of the transmit SNR. For the strong user, we show that NOMA achieves a\nhigher EC than OMA at small values of the transmit SNR and that at high values\nof the transmit SNR OMA becomes more beneficial. By introducing user pairing\nwhen more than two NOMA users are present, we show that NOMA with user pairing\noutperforms OMA in term of the total link layer EC. Finally, we find the set of\npairs which gives the highest total link-layer in the uplink for NOMA with\nmultiple user-pairs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 16:04:40 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Bello", "Mouktar", ""]]}]