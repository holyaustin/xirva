[{"id": "1202.0414", "submitter": "Ihab Sbeity", "authors": "Ihab Sbeity, Leonardo Brenner and Mohamed Dbouk", "title": "Generating a Performance Stochastic Model from UML Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its initiation by Connie Smith, the process of Software Performance\nEngineering (SPE) is becoming a growing concern. The idea is to bring\nperformance evaluation into the software design process. This suitable\nmethodology allows software designers to determine the performance of software\nduring design. Several approaches have been proposed to provide such\ntechniques. Some of them propose to derive from a UML (Unified Modeling\nLanguage) model a performance model such as Stochastic Petri Net (SPN) or\nStochastic process Algebra (SPA) models. Our work belongs to the same category.\nWe propose to derive from a UML model a Stochastic Automata Network (SAN) in\norder to obtain performance predictions. Our approach is more flexible due to\nthe SAN modularity and its high resemblance to UML' state-chart diagram.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 11:42:16 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Sbeity", "Ihab", ""], ["Brenner", "Leonardo", ""], ["Dbouk", "Mohamed", ""]]}, {"id": "1202.1877", "submitter": "Md. Tariq Aziz Tariq", "authors": "Md. Tariq Aziz (1), Mohammad Saiful Islam (1), Md. Nazmul Islam khan\n  (2) and Adrian Popescu (1) ((1) Blekinge Institute of Technology, Karlskrona,\n  Sweden (2) Presidency University, Dhaka, Bangladesh)", "title": "Effect of Packet Delay Variation on Video-Voice over DiffServ-MPLS in\n  IPv4-IPv6 Networks", "comments": "21 Pages, 8 Figures; January 2012, Volume 3, Number 1 (IJDPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, we have witnessed a rapid deployment of real-time\napplications on the Internet as well as many research works about Quality of\nService (QoS), in particular IPv4 (Internet Protocol version 4). The inevitable\nexhaustion of the remaining IPv4 address pool has become progressively evident.\nAs the evolution of Internet Protocol (IP) continues, the deployment of IPv6\nQoS is underway. Today, there is limited experience in the deployment of QoS\nfor IPv6 traffic in MPLS backbone networks in conjunction with DiffServ\n(Differentiated Services) support. DiffServ itself does not have the ability to\ncontrol the traffic which has been taken for end-to-end path while a number of\nlinks of the path are congested. In contrast, MPLS Traffic Engineering (TE) is\naccomplished to control the traffic and can set up end-to-end routing path\nbefore data has been forwarded. From the evolution of IPv4 QoS solutions, we\nknow that the integration of DiffServ and MPLS TE satisfies the guaranteed QoS\nrequirement for real-time applications. This paper presents a QoS performance\nstudy of real-time applications such as voice and video conferencing in terms\nof Packet Delay Variation (PDV) over DiffServ with or without MPLS TE in\nIPv4/IPv6 networks using Optimized Network Engineering Tool (OPNET). We also\nstudy the interaction of Expedited Forwarding (EF), Assured Forwarding (AF)\ntraffic aggregation, link congestion, as well as the effect of performance\nmetric such as PDV. The effectiveness of DiffServ and MPLS TE integration in\nIPv4/IPv6 network is illustrated and analyzed. This paper shows that IPv6\nexperiences more PDV than their IPv4 counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 02:47:14 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2012 15:00:14 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Aziz", "Md. Tariq", ""], ["Islam", "Mohammad Saiful", ""], ["khan", "Md. Nazmul Islam", ""], ["Popescu", "Adrian", ""]]}, {"id": "1202.3856", "submitter": "Kadir Akbudak Mr", "authors": "Kadir Akbudak, Enver Kayaaslan and Cevdet Aykanat", "title": "Technical Report on Hypergraph-Partitioning-Based Models and Methods for\n  Exploiting Cache Locality in Sparse-Matrix Vector Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": "BU-CE-1201", "categories": "cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse matrix-vector multiplication (SpMxV) is a kernel operation widely\nused in iterative linear solvers. The same sparse matrix is multiplied by a\ndense vector repeatedly in these solvers. Matrices with irregular sparsity\npatterns make it difficult to utilize cache locality effectively in SpMxV\ncomputations. In this work, we investigate single- and multiple-SpMxV\nframeworks for exploiting cache locality in SpMxV computations. For the\nsingle-SpMxV framework, we propose two cache-size-aware top-down\nrow/column-reordering methods based on 1D and 2D sparse matrix partitioning by\nutilizing the column-net and enhancing the row-column-net hypergraph models of\nsparse matrices. The multiple-SpMxV framework depends on splitting a given\nmatrix into a sum of multiple nonzero-disjoint matrices so that the SpMxV\noperation is performed as a sequence of multiple input- and output- dependent\nSpMxV operations. For an effective matrix splitting required in this framework,\nwe propose a cache- size-aware top-down approach based on 2D sparse matrix\npartitioning by utilizing the row-column-net hypergraph model. For this\nframework, we also propose two methods for effective ordering of individual\nSpMxV operations. The primary objective in all of the three methods is to\nmaximize the exploitation of temporal locality. We evaluate the validity of our\nmodels and methods on a wide range of sparse matrices using both cache-miss\nsimulations and actual runs by using OSKI. Experimental results show that\nproposed methods and models outperform state-of-the-art schemes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2012 09:28:24 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2012 13:02:08 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2012 09:31:09 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Akbudak", "Kadir", ""], ["Kayaaslan", "Enver", ""], ["Aykanat", "Cevdet", ""]]}, {"id": "1202.4423", "submitter": "Marek Rychlik", "authors": "Sarah Edge Mann, Michael Anderson, and Marek Rychlik", "title": "On the Reliability of RAID Systems: An Argument for More Check Drives", "comments": "13 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address issues of reliability of RAID systems. We focus on\n\"big data\" systems with a large number of drives and advanced error correction\nschemes beyond \\RAID{6}. Our RAID paradigm is based on Reed-Solomon codes, and\nthus we assume that the RAID consists of $N$ data drives and $M$ check drives.\nThe RAID fails only if the combined number of failed drives and sector errors\nexceeds $M$, a property of Reed-Solomon codes.\n  We review a number of models considered in the literature and build upon them\nto construct models usable for a large number of data and check drives. We\nattempt to account for a significant number of factors that affect RAID\nreliability, such as drive replacement or lack thereof, mistakes during service\nsuch as replacing the wrong drive, delayed repair, and the finite duration of\nRAID reconstruction. We evaluate the impact of sector failures that do not\nresult in drive replacement.\n  The reader who needs to consider large $M$ and $N$ will find applicable\nmathematical techniques concisely summarized here, and should be able to apply\nthem to similar problems. Most methods are based on the theory of continuous\ntime Markov chains, but we move beyond this framework when we consider the\nfixed time to rebuild broken hard drives, which we model using systems of delay\nand partial differential equations.\n  One universal statement is applicable across various models: increasing the\nnumber of check drives in all cases increases the reliability of the system,\nand is vastly superior to other approaches of ensuring reliability such as\nmirroring.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 03:54:13 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Mann", "Sarah Edge", ""], ["Anderson", "Michael", ""], ["Rychlik", "Marek", ""]]}, {"id": "1202.4661", "submitter": "Yang Yang", "authors": "Yang Yang and Jian Tan and Ness B. Shroff and Hesham El-Gamal", "title": "Delay Asymptotics with Retransmissions and Incremental Redundancy Codes\n  over Erasure Channels", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that retransmissions can cause heavy-tailed\ntransmission delays even when packet sizes are light-tailed. Moreover, the\nimpact of heavy-tailed delays persists even when packets size are upper\nbounded. The key question we study in this paper is how the use of coding\ntechniques to transmit information, together with different system\nconfigurations, would affect the distribution of delay. To investigate this\nproblem, we model the underlying channel as a Markov modulated binary erasure\nchannel, where transmitted bits are either received successfully or erased.\nErasure codes are used to encode information prior to transmission, which\nensures that a fixed fraction of the bits in the codeword can lead to\nsuccessful decoding. We use incremental redundancy codes, where the codeword is\ndivided into codeword trunks and these trunks are transmitted one at a time to\nprovide incremental redundancies to the receiver until the information is\nrecovered. We characterize the distribution of delay under two different\nscenarios: (I) Decoder uses memory to cache all previously successfully\nreceived bits. (II) Decoder does not use memory, where received bits are\ndiscarded if the corresponding information cannot be decoded. In both cases, we\nconsider codeword length with infinite and finite support. From a theoretical\nperspective, our results provide a benchmark to quantify the tradeoff between\nsystem complexity and the distribution of delay.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 19:05:12 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Yang", "Yang", ""], ["Tan", "Jian", ""], ["Shroff", "Ness B.", ""], ["El-Gamal", "Hesham", ""]]}, {"id": "1202.4880", "submitter": "Luca Muscariello", "authors": "Massimo Gallo, Bruno Kauffmann, Luca Muscariello, Alain Simonian,\n  Christian Tanguy", "title": "Performance Evaluation of the Random Replacement Policy for Networks of\n  Caches", "comments": "14 pages, 11 figures, accepted in a poster version at ACM SIGMETRICS\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overall performance of content distribution networks as well as recently\nproposed information-centric networks rely on both memory and bandwidth\ncapacities. In this framework, the hit ratio is the key performance indicator\nwhich captures the bandwidth / memory tradeoff for a given global\nperformance.This paper focuses on the estimation of the hit ratio in a network\nof caches that employ the Random replacement policy. Assuming that requests are\nindependent and identically distributed, general expressions of miss\nprobabilities for a single Random cache are provided as well as exact results\nfor specific popularity distributions. Moreover, for any Zipf popularity\ndistribution with exponent $\\alpha$ > 1, we obtain asymptotic equivalents for\nthe miss probability in the case of large cache size. We extend the analysis to\nnetworks of Random caches, when the topology is either a line or a homogeneous\ntree. In that case, approximations for miss probabilities across the network\nare derived by assuming that miss events at any node occur independently in\ntime; the obtained results are compared to the same network using the\nLeast-Recently-Used discipline, already addressed in the literature. We further\nanalyze the case of a mixed tandem cache network where the two nodes employ\neither Random or Least-Recently-Used policies. In all scenarios, asymptotic\nformulas and approximations are extensively compared to simulations and shown\nto perform very well. Finally, our results enable us to propose recommendations\nfor cache replacement disciplines in a network dedicated to content\ndistribution. These results also hold for a cache using the First-In-First-Out\npolicy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 11:08:33 GMT"}], "update_date": "2016-11-27", "authors_parsed": [["Gallo", "Massimo", ""], ["Kauffmann", "Bruno", ""], ["Muscariello", "Luca", ""], ["Simonian", "Alain", ""], ["Tanguy", "Christian", ""]]}, {"id": "1202.5202", "submitter": "Sheng Cai", "authors": "Sheng Cai, Jihang Ye, Minghua Chen, Jianxin Yan, Sidharth Jaggi", "title": "Secure Compressed Reading in Smart Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart Grids measure energy usage in real-time and tailor supply and delivery\naccordingly, in order to improve power transmission and distribution. For the\ngrids to operate effectively, it is critical to collect readings from\nmassively-installed smart meters to control centers in an efficient and secure\nmanner. In this paper, we propose a secure compressed reading scheme to address\nthis critical issue. We observe that our collected real-world meter data\nexpress strong temporal correlations, indicating they are sparse in certain\ndomains. We adopt Compressed Sensing technique to exploit this sparsity and\ndesign an efficient meter data transmission scheme. Our scheme achieves\nsubstantial efficiency offered by compressed sensing, without the need to know\nbeforehand in which domain the meter data are sparse. This is in contrast to\ntraditional compressed-sensing based scheme where such sparse-domain\ninformation is required a priori. We then design specific dependable scheme to\nwork with our compressed sensing based data transmission scheme to make our\nmeter reading reliable and secure. We provide performance guarantee for the\ncorrectness, efficiency, and security of our proposed scheme. Through analysis\nand simulations, we demonstrate the effectiveness of our schemes and compare\ntheir performance to prior arts.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 11:12:32 GMT"}], "update_date": "2012-02-24", "authors_parsed": [["Cai", "Sheng", ""], ["Ye", "Jihang", ""], ["Chen", "Minghua", ""], ["Yan", "Jianxin", ""], ["Jaggi", "Sidharth", ""]]}, {"id": "1202.5755", "submitter": "Kirill Kogan", "authors": "Kirill Kogan, Alejandro Lopez-Ortiz, Sergey I. Nikolenko, Gabriel\n  Scalosub, Michael Segal", "title": "Balancing Work and Size with Bounded Buffers", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of managing a bounded size queue buffer\nwhere traffic consists of packets of varying size, where each packet requires\nseveral rounds of processing before it can be transmitted from the queue\nbuffer. The goal in such an environment is to maximize the overall size of\npackets that are successfully transmitted. This model is motivated by the\never-growing ubiquity of network processors architectures, which must deal with\nheterogeneously-sized traffic, with heterogeneous processing requirements. Our\nwork addresses the tension between two conflicting algorithmic approaches in\nsuch settings: the tendency to favor packets with fewer processing\nrequirements, thus leading to fast contributions to the accumulated throughput,\nas opposed to preferring packets of larger size, which imply a large increase\nin throughput at each step. We present a model for studying such systems, and\npresent competitive algorithms whose performance depend on the maximum size a\npacket may have, and maximum amount of processing a packet may require. We\nfurther provide lower bounds on algorithms performance in such settings.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2012 12:53:15 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2013 18:56:30 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Kogan", "Kirill", ""], ["Lopez-Ortiz", "Alejandro", ""], ["Nikolenko", "Sergey I.", ""], ["Scalosub", "Gabriel", ""], ["Segal", "Michael", ""]]}, {"id": "1202.6522", "submitter": "Nathana\\\"el Schaeffer", "authors": "Nathana\\\"el Schaeffer", "title": "Efficient Spherical Harmonic Transforms aimed at pseudo-spectral\n  numerical simulations", "comments": "8 pages", "journal-ref": "Geochemistry, Geophysics, Geosystems, American Geophysical Union\n  (AGU), 2013, 14 (3), pp.751-758", "doi": "10.1002/ggge.20071", "report-no": null, "categories": "physics.comp-ph cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report on very efficient algorithms for the spherical\nharmonic transform (SHT). Explicitly vectorized variations of the algorithm\nbased on the Gauss-Legendre quadrature are discussed and implemented in the\nSHTns library which includes scalar and vector transforms. The main\nbreakthrough is to achieve very efficient on-the-fly computations of the\nLegendre associated functions, even for very high resolutions, by taking\nadvantage of the specific properties of the SHT and the advanced capabilities\nof current and future computers. This allows us to simultaneously and\nsignificantly reduce memory usage and computation time of the SHT. We measure\nthe performance and accuracy of our algorithms. Even though the complexity of\nthe algorithms implemented in SHTns are in $O(N^3)$ (where N is the maximum\nharmonic degree of the transform), they perform much better than any third\nparty implementation, including lower complexity algorithms, even for\ntruncations as high as N=1023. SHTns is available at\nhttps://bitbucket.org/nschaeff/shtns as open source software.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 12:05:12 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2012 13:37:52 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2013 13:20:31 GMT"}, {"version": "v4", "created": "Thu, 20 Nov 2014 15:14:32 GMT"}, {"version": "v5", "created": "Wed, 7 Jan 2015 14:40:46 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Schaeffer", "Nathana\u00ebl", ""]]}]