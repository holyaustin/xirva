[{"id": "1410.0804", "submitter": "Maciej Burak Mr.", "authors": "Maciej Burak", "title": "Multi-step Uniformization with Steady-State Detection in Nonstationary\n  M/M/s Queuing Systems", "comments": "added DOI information in References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to the steady state detection in the uniformization method of\nsolving continuous time Markov chains is introduced. The method is particularly\nuseful in solving inhomogenous CTMC's in multiple steps, where the desired\nerror bound of the whole solution can be distributed not proportionally to the\nlengths of the respective intervals, but rather in a way, that maximizes the\nchances of detecting a steady state. Additionally, the convergence properties\nof the underlying DTMC are used to further enhance the computational savings\ndue to the steady state detection. The method is applied to the problem of\nmodeling a Call Center using inhomogenous CTMC model of a M(t)/M(t)/s(t)\nqueuing system.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 10:13:03 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 08:01:08 GMT"}, {"version": "v3", "created": "Sat, 11 Oct 2014 09:11:41 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Burak", "Maciej", ""]]}, {"id": "1410.2167", "submitter": "Luigi Nardi", "authors": "Luigi Nardi, Bruno Bodin, M. Zeeshan Zia, John Mawer, Andy Nisbet,\n  Paul H. J. Kelly, Andrew J. Davison, Mikel Luj\\'an, Michael F. P. O'Boyle,\n  Graham Riley, Nigel Topham, Steve Furber", "title": "Introducing SLAMBench, a performance and accuracy benchmarking\n  methodology for SLAM", "comments": "8 pages, ICRA 2015 conference paper", "journal-ref": "http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7140009\n  IEEE Xplore 2015", "doi": "10.1109/ICRA.2015.7140009", "report-no": null, "categories": "cs.RO cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time dense computer vision and SLAM offer great potential for a new\nlevel of scene modelling, tracking and real environmental interaction for many\ntypes of robot, but their high computational requirements mean that use on mass\nmarket embedded platforms is challenging. Meanwhile, trends in low-cost,\nlow-power processing are towards massive parallelism and heterogeneity, making\nit difficult for robotics and vision researchers to implement their algorithms\nin a performance-portable way. In this paper we introduce SLAMBench, a\npublicly-available software framework which represents a starting point for\nquantitative, comparable and validatable experimental research to investigate\ntrade-offs in performance, accuracy and energy consumption of a dense RGB-D\nSLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP,\nOpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-D\nsequences with trajectory and scene ground truth for reliable accuracy\ncomparison of different implementation and algorithms. We present an analysis\nand breakdown of the constituent algorithmic elements of KinectFusion, and\nexperimentally investigate their execution time on a variety of multicore and\nGPUaccelerated platforms. For a popular embedded platform, we also present an\nanalysis of energy efficiency for different configuration alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 15:34:43 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 16:28:27 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Nardi", "Luigi", ""], ["Bodin", "Bruno", ""], ["Zia", "M. Zeeshan", ""], ["Mawer", "John", ""], ["Nisbet", "Andy", ""], ["Kelly", "Paul H. J.", ""], ["Davison", "Andrew J.", ""], ["Luj\u00e1n", "Mikel", ""], ["O'Boyle", "Michael F. P.", ""], ["Riley", "Graham", ""], ["Topham", "Nigel", ""], ["Furber", "Steve", ""]]}, {"id": "1410.2803", "submitter": "Ali Shoker", "authors": "Paulo S\\'ergio Almeida, Ali Shoker, and Carlos Baquero", "title": "Efficient State-based CRDTs by Delta-Mutation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CRDTs are distributed data types that make eventual consistency of a\ndistributed object possible and non ad-hoc. Specifically, state-based CRDTs\nensure convergence through disseminating the en- tire state, that may be large,\nand merging it to other replicas; whereas operation-based CRDTs disseminate\noperations (i.e., small states) assuming an exactly-once reliable dissemination\nlayer. We introduce Delta State Conflict-Free Replicated Datatypes\n({\\delta}-CRDT) that can achieve the best of both worlds: small messages with\nan incremental nature, as in operation-based CRDTs, disseminated over\nunreliable communication channels, as in traditional state-based CRDTs. This is\nachieved by defining {\\delta}-mutators to return a delta-state, typically with\na much smaller size than the full state, that is joined to both: local and\nremote states. We introduce the {\\delta}-CRDT framework, and we explain it\nthrough establishing a correspondence to current state-based CRDTs. In\naddition, we present an anti-entropy algorithm that ensures causal consistency,\nand we introduce two {\\delta}-CRDT specifications of well-known replicated\ndatatypes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 15:16:23 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 12:32:20 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Almeida", "Paulo S\u00e9rgio", ""], ["Shoker", "Ali", ""], ["Baquero", "Carlos", ""]]}, {"id": "1410.3677", "submitter": "Gevorg Poghosyan", "authors": "Gevorg Poghosyan, Sanchit Matta, Achim Streit, Micha{\\l} Bejger,\n  Andrzej Kr\\'olak", "title": "Architecture, implementation and parallelization of the software to\n  search for periodic gravitational wave signals", "comments": "11 pages, 9 figures. Submitted to Computer Physics Communications", "journal-ref": "Computer Physics Communications volume 188 pages 168 - 176 (2015)", "doi": "10.1016/j.cpc.2014.10.025", "report-no": null, "categories": "gr-qc astro-ph.HE cs.DC cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parallelization, design and scalability of the \\sky code to search for\nperiodic gravitational waves from rotating neutron stars is discussed. The code\nis based on an efficient implementation of the F-statistic using the Fast\nFourier Transform algorithm. To perform an analysis of data from the advanced\nLIGO and Virgo gravitational wave detectors' network, which will start\noperating in 2015, hundreds of millions of CPU hours will be required - the\ncode utilizing the potential of massively parallel supercomputers is therefore\nmandatory. We have parallelized the code using the Message Passing Interface\nstandard, implemented a mechanism for combining the searches at different\nsky-positions and frequency bands into one extremely scalable program. The\nparallel I/O interface is used to escape bottlenecks, when writing the\ngenerated data into file system. This allowed to develop a highly scalable\ncomputation code, which would enable the data analysis at large scales on\nacceptable time scales. Benchmarking of the code on a Cray XE6 system was\nperformed to show efficiency of our parallelization concept and to demonstrate\nscaling up to 50 thousand cores in parallel.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 13:15:38 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Poghosyan", "Gevorg", ""], ["Matta", "Sanchit", ""], ["Streit", "Achim", ""], ["Bejger", "Micha\u0142", ""], ["Kr\u00f3lak", "Andrzej", ""]]}, {"id": "1410.4054", "submitter": "Karl Rupp", "authors": "Karl Rupp, Josef Weinbub, Ansgar J\\\"ungel, Tibor Grasser", "title": "Pipelined Iterative Solvers with Kernel Fusion for Graphics Processing\n  Units", "comments": "27 pages, 9 figures, 3 tables", "journal-ref": "ACM Transactions on Mathematical Software (TOMS), Volume 43, Issue\n  2, Article No. 11 (2016)", "doi": "10.1145/2907944", "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the implementation of iterative solvers on discrete graphics\nprocessing units and demonstrate the benefit of implementations using extensive\nkernel fusion for pipelined formulations over conventional implementations of\nclassical formulations. The proposed implementations with both CUDA and OpenCL\nare freely available in ViennaCL and are shown to be competitive with or even\nsuperior to other solver packages for graphics processing units. Highest\nperformance gains are obtained for small to medium-sized systems, while our\nimplementations are on par with vendor-tuned implementations for very large\nsystems. Our results are especially beneficial for transient problems, where\nmany small to medium-sized systems instead of a single big system need to be\nsolved.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 13:23:31 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 15:56:36 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 11:18:16 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Rupp", "Karl", ""], ["Weinbub", "Josef", ""], ["J\u00fcngel", "Ansgar", ""], ["Grasser", "Tibor", ""]]}, {"id": "1410.4168", "submitter": "Adrien Devresse", "authors": "Adrien Devresse, Fabrizio Furano", "title": "Efficient HTTP based I/O on very large datasets for high performance\n  computing with the libdavix library", "comments": "Presented at: Very large Data Bases (VLDB) 2014, Hangzhou", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Remote data access for data analysis in high performance computing is\ncommonly done with specialized data access protocols and storage systems. These\nprotocols are highly optimized for high throughput on very large datasets,\nmulti-streams, high availability, low latency and efficient parallel I/O. The\npurpose of this paper is to describe how we have adapted a generic protocol,\nthe Hyper Text Transport Protocol (HTTP) to make it a competitive alternative\nfor high performance I/O and data analysis applications in a global computing\ngrid: the Worldwide LHC Computing Grid. In this work, we first analyze the\ndesign differences between the HTTP protocol and the most common high\nperformance I/O protocols, pointing out the main performance weaknesses of\nHTTP. Then, we describe in detail how we solved these issues. Our solutions\nhave been implemented in a toolkit called davix, available through several\nrecent Linux distributions. Finally, we describe the results of our benchmarks\nwhere we compare the performance of davix against a HPC specific protocol for a\ndata analysis use case.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 18:57:12 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Devresse", "Adrien", ""], ["Furano", "Fabrizio", ""]]}, {"id": "1410.5010", "submitter": "Georg Hager", "authors": "Holger Stengel, Jan Treibig, Georg Hager, Gerhard Wellein", "title": "Quantifying performance bottlenecks of stencil computations using the\n  Execution-Cache-Memory model", "comments": "10 pages, 8 figures. Added Roofline comparison and other minor\n  improvements", "journal-ref": null, "doi": "10.1145/2751205.2751240", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil algorithms on regular lattices appear in many fields of computational\nscience, and much effort has been put into optimized implementations. Such\nactivities are usually not guided by performance models that provide estimates\nof expected speedup. Understanding the performance properties and bottlenecks\nby performance modeling enables a clear view on promising optimization\nopportunities. In this work we refine the recently developed\nExecution-Cache-Memory (ECM) model and use it to quantify the performance\nbottlenecks of stencil algorithms on a contemporary Intel processor. This\nincludes applying the model to arrive at single-core performance and\nscalability predictions for typical corner case stencil loop kernels. Guided by\nthe ECM model we accurately quantify the significance of \"layer conditions,\"\nwhich are required to estimate the data traffic through the memory hierarchy,\nand study the impact of typical optimization approaches such as spatial\nblocking, strength reduction, and temporal blocking for their expected\nbenefits. We also compare the ECM model to the widely known Roofline model.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 21:49:45 GMT"}, {"version": "v2", "created": "Sat, 17 Jan 2015 14:07:26 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Stengel", "Holger", ""], ["Treibig", "Jan", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1410.5102", "submitter": "Diego Didona Mr", "authors": "Diego Didona and Paolo Romano", "title": "On Bootstrapping Machine Learning Performance Predictors via Analytical\n  Models", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance modeling typically relies on two antithetic methodologies: white\nbox models, which exploit knowledge on system's internals and capture its\ndynamics using analytical approaches, and black box techniques, which infer\nrelations among the input and output variables of a system based on the\nevidences gathered during an initial training phase. In this paper we\ninvestigate a technique, which we name Bootstrapping, which aims at reconciling\nthese two methodologies and at compensating the cons of the one with the pros\nof the other. We thoroughly analyze the design space of this gray box modeling\ntechnique, and identify a number of algorithmic and parametric trade-offs which\nwe evaluate via two realistic case studies, a Key-Value Store and a Total Order\nBroadcast service.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 18:32:37 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Didona", "Diego", ""], ["Romano", "Paolo", ""]]}, {"id": "1410.5242", "submitter": "Moritz Kreutzer", "authors": "Moritz Kreutzer, Georg Hager, Gerhard Wellein, Andreas Pieper, Andreas\n  Alvermann, Holger Fehske", "title": "Performance Engineering of the Kernel Polynomial Method on Large-Scale\n  CPU-GPU Systems", "comments": "10 pages, 12 figures", "journal-ref": "Proceedings of the 2015 IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS) 417-426", "doi": "10.1109/IPDPS.2015.76", "report-no": null, "categories": "cs.CE cond-mat.mes-hall cs.DC cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kernel Polynomial Method (KPM) is a well-established scheme in quantum\nphysics and quantum chemistry to determine the eigenvalue density and spectral\nproperties of large sparse matrices. In this work we demonstrate the high\noptimization potential and feasibility of peta-scale heterogeneous CPU-GPU\nimplementations of the KPM. At the node level we show that it is possible to\ndecouple the sparse matrix problem posed by KPM from main memory bandwidth both\non CPU and GPU. To alleviate the effects of scattered data access we combine\nloosely coupled outer iterations with tightly coupled block sparse matrix\nmultiple vector operations, which enables pure data streaming. All\noptimizations are guided by a performance analysis and modelling process that\nindicates how the computational bottlenecks change with each optimization step.\nFinally we use the optimized node-level KPM with a hybrid-parallel framework to\nperform large scale heterogeneous electronic structure calculations for novel\ntopological materials on a petascale-class Cray XC30 system.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 12:16:22 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 10:52:10 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Kreutzer", "Moritz", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""], ["Pieper", "Andreas", ""], ["Alvermann", "Andreas", ""], ["Fehske", "Holger", ""]]}, {"id": "1410.5561", "submitter": "Tareq Malas", "authors": "Tareq Malas, Georg Hager, Hatem Ltaief, David Keyes", "title": "Towards energy efficiency and maximum computational intensity for\n  stencil algorithms using wavefront diamond temporal blocking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the impact of tunable parameters on computational intensity (i.e.,\ninverse code balance) and energy consumption of multicore-optimized wavefront\ndiamond temporal blocking (MWD) applied to different stencil-based update\nschemes. MWD combines the concepts of diamond tiling and multicore-aware\nwavefront blocking in order to achieve lower cache size requirements than\nstandard single-core wavefront temporal blocking. We analyze the impact of the\ncache block size on the theoretical and observed code balance, introduce loop\ntiling in the leading dimension to widen the range of applicable diamond sizes,\nand show performance results on a contemporary Intel CPU. The impact of code\nbalance on power dissipation on the CPU and in the DRAM is investigated and\nshows that DRAM power is a decisive factor for energy consumption, which is\nstrongly influenced by the code balance. Furthermore we show that highest\nperformance does not necessarily lead to lowest energy even if the clock speed\nis fixed.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 07:35:32 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Malas", "Tareq", ""], ["Hager", "Georg", ""], ["Ltaief", "Hatem", ""], ["Keyes", "David", ""]]}, {"id": "1410.6952", "submitter": "Karan Mitra Dr", "authors": "Karan Mitra, Arkady Zaslavsky, Christer {\\AA}hlund", "title": "QoE Modelling, Measurement and Prediction: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mobile computing systems, users can access network services anywhere and\nanytime using mobile devices such as tablets and smart phones. These devices\nconnect to the Internet via network or telecommunications operators. Users\nusually have some expectations about the services provided to them by different\noperators. Users' expectations along with additional factors such as cognitive\nand behavioural states, cost, and network quality of service (QoS) may\ndetermine their quality of experience (QoE). If users are not satisfied with\ntheir QoE, they may switch to different providers or may stop using a\nparticular application or service. Thus, QoE measurement and prediction\ntechniques may benefit users in availing personalized services from service\nproviders. On the other hand, it can help service providers to achieve lower\nuser-operator switchover. This paper presents a review of the state-the-art\nresearch in the area of QoE modelling, measurement and prediction. In\nparticular, we investigate and discuss the strengths and shortcomings of\nexisting techniques. Finally, we present future research directions for\ndeveloping novel QoE measurement and prediction techniques\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 19:13:49 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Mitra", "Karan", ""], ["Zaslavsky", "Arkady", ""], ["\u00c5hlund", "Christer", ""]]}, {"id": "1410.7815", "submitter": "Quang-Hung Nguyen", "authors": "Nguyen Quang-Hung, Nam Thoai, Nguyen Thanh Son, Duy-Khanh Le", "title": "Energy-Aware Lease Scheduling in Virtualized Data Centers", "comments": "10 pages, 2 figures, Proceedings of the Fifth International\n  Conference on High Performance Scientific Computing, March 5-9, 2012, Hanoi,\n  Vietnam", "journal-ref": "Modeling, Simulation and Optimization of Complex Processes - HPSC\n  2012", "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Energy efficiency has become an important measurement of scheduling\nalgorithms in virtualized data centers. One of the challenges of\nenergy-efficient scheduling algorithms, however, is the trade-off between\nminimizing energy consumption and satisfying quality of service (e.g.\nperformance, resource availability on time for reservation requests). We\nconsider resource needs in the context of virtualized data centers of a private\ncloud system, which provides resource leases in terms of virtual machines (VMs)\nfor user applications. In this paper, we propose heuristics for scheduling VMs\nthat address the above challenge. On performance evaluation, simulated results\nhave shown a significant reduction on total energy consumption of our proposed\nalgorithms compared with an existing First-Come-First-Serve (FCFS) scheduling\nalgorithm with the same fulfillment of performance requirements. We also\ndiscuss the improvement of energy saving when additionally using migration\npolicies to the above mentioned algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 21:10:35 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Quang-Hung", "Nguyen", ""], ["Thoai", "Nam", ""], ["Son", "Nguyen Thanh", ""], ["Le", "Duy-Khanh", ""]]}]