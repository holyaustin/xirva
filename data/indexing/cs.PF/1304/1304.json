[{"id": "1304.1838", "submitter": "Jagan Sankaranarayanan", "authors": "Jeff LeFevre, Jagan Sankaranarayanan, Hakan Hacigumus, Junichi\n  Tatemura, Neoklis Polyzotis", "title": "Towards a Workload for Evolutionary Analytics", "comments": "10 pages", "journal-ref": "DanaC: Workshop on Data analytics in the Cloud, June 2013, New\n  York, NY", "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Emerging data analysis involves the ingestion and exploration of new data\nsets, application of complex functions, and frequent query revisions based on\nobserving prior query answers. We call this new type of analysis evolutionary\nanalytics and identify its properties. This type of analysis is not well\nrepresented by current benchmark workloads. In this paper, we present a\nworkload and identify several metrics to test system support for evolutionary\nanalytics. Along with our metrics, we present methodologies for running the\nworkload that capture this analytical scenario.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 00:26:41 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2013 07:00:10 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2013 19:19:22 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["LeFevre", "Jeff", ""], ["Sankaranarayanan", "Jagan", ""], ["Hacigumus", "Hakan", ""], ["Tatemura", "Junichi", ""], ["Polyzotis", "Neoklis", ""]]}, {"id": "1304.1863", "submitter": "Yongkun Li", "authors": "Yongkun Li, Patrick P.C. Lee, John C.S. Lui", "title": "Stochastic Analysis on RAID Reliability for Solid-State Drives", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solid-state drives (SSDs) have been widely deployed in desktops and data\ncenters. However, SSDs suffer from bit errors, and the bit error rate is time\ndependent since it increases as an SSD wears down. Traditional storage systems\nmainly use parity-based RAID to provide reliability guarantees by striping\nredundancy across multiple devices, but the effectiveness of RAID in SSDs\nremains debatable as parity updates aggravate the wearing and bit error rates\nof SSDs. In particular, an open problem is that how different parity\ndistributions over multiple devices, such as the even distribution suggested by\nconventional wisdom, or uneven distributions proposed in recent RAID schemes\nfor SSDs, may influence the reliability of an SSD RAID array. To address this\nfundamental problem, we propose the first analytical model to quantify the\nreliability dynamics of an SSD RAID array. Specifically, we develop a\n\"non-homogeneous\" continuous time Markov chain model, and derive the transient\nreliability solution. We validate our model via trace-driven simulations and\nconduct numerical analysis to provide insights into the reliability dynamics of\nSSD RAID arrays under different parity distributions and subject to different\nbit error rates and array configurations. Designers can use our model to decide\nthe appropriate parity distribution based on their reliability requirements.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 07:48:02 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Li", "Yongkun", ""], ["Lee", "Patrick P. C.", ""], ["Lui", "John C. S.", ""]]}, {"id": "1304.1966", "submitter": "Roberto Capuzzo-Dolcetta", "authors": "Roberto Capuzzo-Dolcetta and Mario Spera (Dep. of Physics, Sapienza,\n  Universit\\`a di Roma, Italy)", "title": "A Performance Comparison of Different Graphics Processing Units Running\n  Direct N-Body Simulations", "comments": "This paper has been submitted for publication to Computer Physics\n  Communications It consists of 26 pages, with 6 tables and 10 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2013.07.005", "report-no": null, "categories": "astro-ph.IM cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid computational architectures based on the joint power of Central\nProcessing Units and Graphic Processing Units (GPUs) are becoming popular and\npowerful hardware tools for a wide range of simulations in biology, chemistry,\nengineering, physics, etc..\n  In this paper we present a comparison of performance of various GPUs\navailable on market when applied to the numerical integration of the classic,\ngravitational, N-body problem. To do this, we developed an OpenCL version of\nthe parallel code (HiGPUs) to use for these tests, because this version is the\nonly apt to work on GPUs of different makes.\n  The main general result is that we confirm the reliability, speed and\ncheapness of GPUs when applied to the examined kind of problems (i.e. when the\nforces to evaluate are dependent on the mutual distances, as it happens in\ngravitational physics and molecular dynamics). More specifically, we find that\nalso the cheap GPUs built to be employed just for gaming applications are very\nperformant in terms of computing speed also in scientific applications and,\nalthough with some limitations in central memory and in bandwidth, can be a\ngood choice to implement a machine for scientific use at a very good\nperformance to cost ratio.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 08:08:51 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Capuzzo-Dolcetta", "Roberto", "", "Dep. of Physics, Sapienza,\n  Universit\u00e0 di Roma, Italy"], ["Spera", "Mario", "", "Dep. of Physics, Sapienza,\n  Universit\u00e0 di Roma, Italy"]]}, {"id": "1304.2554", "submitter": "Emilio Leonardi", "authors": "Emilio Leonardi", "title": "Throughput Optimal Scheduling Policies in Networks of Interacting Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report considers a fairly general model of constrained queuing networks\nthat allows us to represent both MMBP (Markov Modulated Bernoulli Processes)\narrivals and time-varying service constraints. We derive a set of sufficient\nconditions for throughput optimality of scheduling policies that encompass and\ngeneralize all the previously obtained results in the field. This leads to the\ndefinition of new classes of (non diagonal) throughput optimal scheduling\npolicies. We prove the stability of queues by extending the traditional\nLyapunov drift criteria methodology.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 12:37:41 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 14:57:06 GMT"}, {"version": "v3", "created": "Mon, 17 Mar 2014 15:50:53 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Leonardi", "Emilio", ""]]}, {"id": "1304.3767", "submitter": "Hemant Rotithor Dr", "authors": "Hemant Rotithor", "title": "A Taxonomy of Performance Assurance Methodologies and its Application in\n  High Performance Computer Architectures", "comments": "17 pages, 5 figures, 3 tables", "journal-ref": "International Journal of Software Engineering & Applications\n  (IJSEA), Vol.4, No.2, March 2013", "doi": "10.5121/ijsea.2013.4201", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a systematic approach to the complex problem of high\nconfidence performance assurance of high performance architectures based on\nmethods used over several generations of industrial microprocessors. A taxonomy\nis presented for performance assurance through three key stages of a product\nlife cycle-high level performance, RTL performance, and silicon performance.\nThe proposed taxonomy includes two components-independent performance assurance\nspace for each stage and a correlation performance assurance space between\nstages. It provides a detailed insight into the performance assurance space in\nterms of coverage provided taking into account capabilities and limitations of\ntools and methodologies used at each stage. An application of the taxonomy to\ncases described in the literature and to high performance Intel architectures\nis shown. The proposed work should be of interest to manufacturers of high\nperformance microprocessor/chipset architectures and has not been discussed in\nthe literature.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 03:48:53 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Rotithor", "Hemant", ""]]}, {"id": "1304.3804", "submitter": "Emilio Coppa", "authors": "Emilio Coppa, Camil Demetrescu, Irene Finocchi and Romolo Marotta", "title": "Multithreaded Input-Sensitive Profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Input-sensitive profiling is a recent performance analysis technique that\nmakes it possible to estimate the empirical cost function of individual\nroutines of a program, helping developers understand how performance scales to\nlarger inputs and pinpoint asymptotic bottlenecks in the code. A current\nlimitation of input-sensitive profilers is that they specifically target\nsequential computations, ignoring any communication between threads. In this\npaper we show how to overcome this limitation, extending the range of\napplicability of the original approach to multithreaded applications and to\napplications that operate on I/O streams. We develop new metrics for\nautomatically estimating the size of the input given to each routine\nactivation, addressing input produced by non-deterministic memory stores\nperformed by other threads as well as by the OS kernel (e.g., in response to\nI/O or network operations). We provide real case studies, showing that our\nextension allows it to characterize the behavior of complex applications more\nprecisely than previous approaches. An extensive experimental investigation on\na variety of benchmark suites (including the SPEC OMP2012 and the PARSEC\nbenchmarks) shows that our Valgrind-based input-sensitive profiler incurs an\noverhead comparable to other prominent heavyweight analysis tools, while\ncollecting significantly more performance points from each profiling session\nand correctly characterizing both thread-induced and external input.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 12:39:34 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Coppa", "Emilio", ""], ["Demetrescu", "Camil", ""], ["Finocchi", "Irene", ""], ["Marotta", "Romolo", ""]]}, {"id": "1304.4524", "submitter": "Gautam Bhanage", "authors": "Gautam Bhanage, Sanjit Kaul", "title": "Investigating Randomly Generated Adjacency Matrices For Their Use In\n  Modeling Wireless Topologies", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Generation of realistic topologies plays an important role in determining the\naccuracy and validity of simulation studies. This study presents a discussion\nto justify why, and how often randomly generated adjacency matrices may not not\nconform to wireless topologies in the physical world. Specifically, it shows\nthrough analysis and random trials that, more than 90% of times, a randomly\ngenerated adjacency matrix will not conform to a valid wireless topology, when\nit has more than 3 nodes. By showing that node triplets in the adjacency graph\nneed to adhere to rules of a geometric vector space, the study shows that the\nnumber of randomly chosen node triplets failing consistency checks grow at the\norder of O(base^3), where base is the granularity of the distance metric.\nFurther, the study models and presents a probability estimate with which any\nrandomly generated adjacency matrix would fail realization. This information\ncould be used to design simpler algorithms for generating k-connected wireless\ntopologies.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 17:23:20 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Bhanage", "Gautam", ""], ["Kaul", "Sanjit", ""]]}, {"id": "1304.5197", "submitter": "Daniele Cono D'Elia", "authors": "Daniele Cono D'Elia, Camil Demetrescu, Irene Finocchi", "title": "Ball-Larus Path Profiling Across Multiple Loop iterations", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the hottest paths in the control flow graph of a routine can\ndirect optimizations to portions of the code where most resources are consumed.\nThis powerful methodology, called path profiling, was introduced by Ball and\nLarus in the mid 90s and has received considerable attention in the last 15\nyears for its practical relevance. A shortcoming of Ball-Larus path profiling\nwas the inability to profile cyclic paths, making it difficult to mine\ninteresting execution patterns that span multiple loop iterations. Previous\nresults, based on rather complex algorithms, have attempted to circumvent this\nlimitation at the price of significant performance losses already for a small\nnumber of iterations. In this paper, we present a new approach to multiple\niterations path profiling, based on data structures built on top of the\noriginal Ball-Larus numbering technique. Our approach allows it to profile all\nexecuted paths obtained as a concatenation of up to k Ball-Larus acyclic paths,\nwhere k is a user-defined parameter. An extensive experimental investigation on\na large variety of Java benchmarks on the Jikes RVM shows that, surprisingly,\nour approach can be even faster than Ball-Larus due to fewer operations on\nsmaller hash tables, producing compact representations of cyclic paths even for\nlarge values of k.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 17:34:38 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["D'Elia", "Daniele Cono", ""], ["Demetrescu", "Camil", ""], ["Finocchi", "Irene", ""]]}, {"id": "1304.5302", "submitter": "Satoshi Eguchi", "authors": "Satoshi Eguchi", "title": "\"Superluminal\" FITS File Processing on Multiprocessors: Zero Time Endian\n  Conversion Technique", "comments": "25 pages, 9 figures, 12 tables, accepted for publication in PASP", "journal-ref": null, "doi": "10.1086/671105", "report-no": null, "categories": "astro-ph.IM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FITS is the standard file format in astronomy, and it has been extended\nto agree with astronomical needs of the day. However, astronomical datasets\nhave been inflating year by year. In case of ALMA telescope, a ~ TB scale\n4-dimensional data cube may be produced for one target. Considering that\ntypical Internet bandwidth is a few 10 MB/s at most, the original data cubes in\nFITS format are hosted on a VO server, and the region which a user is\ninterested in should be cut out and transferred to the user (Eguchi et al.,\n2012). The system will equip a very high-speed disk array to process a TB scale\ndata cube in a few 10 seconds, and disk I/O speed, endian conversion and data\nprocessing one will be comparable. Hence to reduce the endian conversion time\nis one of issues to realize our system. In this paper, I introduce a technique\nnamed \"just-in-time endian conversion\", which delays the endian conversion for\neach pixel just before it is really needed, to sweep out the endian conversion\ntime; by applying this method, the FITS processing speed increases 20% for\nsingle threading, and 40% for multi-threading compared to CFITSIO. The speed-up\nby the method tightly relates to modern CPU architecture to improve the\nefficiency of instruction pipelines due to break of \"causality\", a programmed\ninstruction code sequence.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 03:29:36 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Eguchi", "Satoshi", ""]]}, {"id": "1304.6007", "submitter": "Enoch Peserico", "authors": "Enoch Peserico", "title": "Paging with dynamic memory capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalization of the classic paging problem that allows the\namount of available memory to vary over time - capturing a fundamental property\nof many modern computing realities, from cloud computing to multi-core and\nenergy-optimized processors. It turns out that good performance in the\n\"classic\" case provides no performance guarantees when memory capacity\nfluctuates: roughly speaking, moving from static to dynamic capacity can mean\nthe difference between optimality within a factor 2 in space and time, and\nsuboptimality by an arbitrarily large factor. More precisely, adopting the\ncompetitive analysis framework, we show that some online paging algorithms,\ndespite having an optimal (h,k)-competitive ratio when capacity remains\nconstant, are not (3,k)-competitive for any arbitrarily large k in the presence\nof minimal capacity fluctuations. In this light it is surprising that several\nclassic paging algorithms perform remarkably well even if memory capacity\nchanges adversarially - even without taking those changes into explicit\naccount! In particular, we prove that LFD still achieves the minimum number of\nfaults, and that several classic online algorithms such as LRU have a \"dynamic\"\n(h,k)-competitive ratio that is the best one can achieve without knowledge of\nfuture page requests, even if one had perfect knowledge of future capacity\nfluctuations (an exact characterization of this ratio shows it is almost,\nalbeit not quite, equal to the \"classic\" ratio k/(k-h+1)). In other words, with\ncareful management, knowing/predicting future memory resources appears far less\ncrucial to performance than knowing/predicting future data accesses.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 16:23:24 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Peserico", "Enoch", ""]]}, {"id": "1304.7664", "submitter": "Georg Hager", "authors": "Markus Wittmann, Georg Hager, Thomas Zeiser, Jan Treibig, Gerhard\n  Wellein", "title": "Chip-level and multi-node analysis of energy-optimized lattice-Boltzmann\n  CFD simulations", "comments": "23 pages, 13 figures; post-peer-review version", "journal-ref": null, "doi": "10.1002/cpe.3489", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory-bound algorithms show complex performance and energy consumption\nbehavior on multicore processors. We choose the lattice-Boltzmann method (LBM)\non an Intel Sandy Bridge cluster as a prototype scenario to investigate if and\nhow single-chip performance and power characteristics can be generalized to the\nhighly parallel case. First we perform an analysis of a sparse-lattice LBM\nimplementation for complex geometries. Using a single-core performance model,\nwe predict the intra-chip saturation characteristics and the optimal operating\npoint in terms of energy to solution as a function of implementation details,\nclock frequency, vectorization, and number of active cores per chip. We show\nthat high single-core performance and a correct choice of the number of active\ncores per chip are the essential optimizations for lowest energy to solution at\nminimal performance degradation. Then we extrapolate to the MPI-parallel level\nand quantify the energy-saving potential of various optimizations and execution\nmodes, where we find these guidelines to be even more important, especially\nwhen communication overhead is non-negligible. In our setup we could achieve\nenergy savings of 35% in this case, compared to a naive approach. We also\ndemonstrate that a simple non-reflective reduction of the clock speed leaves\nmost of the energy saving potential unused.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 13:59:21 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2013 13:44:25 GMT"}, {"version": "v3", "created": "Fri, 22 May 2015 13:34:40 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Wittmann", "Markus", ""], ["Hager", "Georg", ""], ["Zeiser", "Thomas", ""], ["Treibig", "Jan", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1304.7966", "submitter": "Yi Fang", "authors": "Yi Fang, Lin Wang, and Guanrong Chen", "title": "Performance of a Multiple-Access DCSK-CC System over Nakagami-$m$ Fading\n  Channels", "comments": "4 pages, 5 figures, accepted, IEEE ISCAS, 2013", "journal-ref": null, "doi": "10.1109/ISCAS.2013.6571836", "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel cooperative scheme to enhance the\nperformance of multiple-access (MA) differential-chaos-shift-keying (DCSK)\nsystems. We provide the bit-error-rate (BER) performance and throughput\nanalyses for the new system with a decode-and-forward (DF) protocol over\nNakagami-$m$ fading channels. Our simulated results not only show that this\nsystem significantly improves the BER performance as compared to the existing\nDCSK non-cooperative (DCSK-NC) system and the multiple-input multiple-output\nDCSK (MIMO-DCSK) system, but also verify the theoretical analyses. Furthermore,\nwe show that the throughput of this system approximately equals that of the\nDCSK-NC system, both of which have prominent improvements over the MIMO-DCSK\nsystem. We thus believe that the proposed system can be a good framework for\nchaos-modulation-based wireless communications.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 11:57:20 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Fang", "Yi", ""], ["Wang", "Lin", ""], ["Chen", "Guanrong", ""]]}]