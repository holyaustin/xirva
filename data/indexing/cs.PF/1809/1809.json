[{"id": "1809.00223", "submitter": "Carlos Vega", "authors": "Carlos Vega Moreno, Eduardo Miravalls Sierra, Guillermo Juli\\'an\n  Moreno, Jorge E. L\\'opez de Vergara, Eduardo Maga\\~na, Javier Aracil", "title": "Evaluation of the performance challenges in automatic traffic report\n  generation with huge data volumes", "comments": "Preprint. Pre-peer reviewed version. 15 pages. 7 figures. 1 table", "journal-ref": null, "doi": "10.1002/nem.2044", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the performance issues involved in the generation of\nauto- mated traffic reports for large IT infrastructures. Such reports allows\nthe IT manager to proactively detect possible abnormal situations and roll out\nthe corresponding cor- rective actions. With the ever-increasing bandwidth of\ncurrent networks, the design of automated traffic report generation systems is\nvery challenging. In a first step, the huge volumes of collected traffic are\ntransformed into enriched flow records obtained from diverse collectors and\ndissectors. Then, such flow records, along with time series obtained from the\nraw traffic, are further processed to produce a usable report. As will be\nshown, the data volume in flow records is very large as well and requires\ncareful selection of the Key Performance Indicators (KPIs) to be included in\nthe report. In this regard, we discuss the use of high-level languages versus\nlow- level approaches, in terms of speed and versatility. Furthermore, our\ndesign approach is targeted for rapid development in commodity hardware, which\nis essential to cost-effectively tackle demanding traffic analysis scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 17:01:04 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Moreno", "Carlos Vega", ""], ["Sierra", "Eduardo Miravalls", ""], ["Moreno", "Guillermo Juli\u00e1n", ""], ["de Vergara", "Jorge E. L\u00f3pez", ""], ["Maga\u00f1a", "Eduardo", ""], ["Aracil", "Javier", ""]]}, {"id": "1809.00661", "submitter": "Gleb Gusev", "authors": "Roman Budylin, Alexey Drutsa, Gleb Gusev, Pavel Serdyukov, Igor\n  Yashkov", "title": "Online Evaluation for Effective Web Service Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of the majority of the leading web services and software products\ntoday is generally guided by data-driven decisions based on evaluation that\nensures a steady stream of updates, both in terms of quality and quantity.\nLarge internet companies use online evaluation on a day-to-day basis and at a\nlarge scale. The number of smaller companies using A/B testing in their\ndevelopment cycle is also growing. Web development across the board strongly\ndepends on quality of experimentation platforms. In this tutorial, we overview\nstate-of-the-art methods underlying everyday evaluation pipelines at some of\nthe leading Internet companies. Software engineers, designers, analysts,\nservice or product managers --- beginners, advanced specialists, and\nresearchers --- can learn how to make web service development data-driven and\ndo it effectively.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 17:14:47 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Budylin", "Roman", ""], ["Drutsa", "Alexey", ""], ["Gusev", "Gleb", ""], ["Serdyukov", "Pavel", ""], ["Yashkov", "Igor", ""]]}, {"id": "1809.00912", "submitter": "Julian Hammer", "authors": "Jan Laukemann, Julian Hammer, Johannes Hofmann, Georg Hager, Gerhard\n  Wellein", "title": "Automated Instruction Stream Throughput Prediction for Intel and AMD\n  Microarchitectures", "comments": "11 pages, 4 figures, 7 tables", "journal-ref": null, "doi": "10.1109/PMBS.2018.8641578", "report-no": null, "categories": "cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate prediction of scheduling and execution of instruction streams is\na necessary prerequisite for predicting the in-core performance behavior of\nthroughput-bound loop kernels on out-of-order processor architectures. Such\npredictions are an indispensable component of analytical performance models,\nsuch as the Roofline and the Execution-Cache-Memory (ECM) model, and allow a\ndeep understanding of the performance-relevant interactions between hardware\narchitecture and loop code. We present the Open Source Architecture Code\nAnalyzer (OSACA), a static analysis tool for predicting the execution time of\nsequential loops comprising x86 instructions under the assumption of an\ninfinite first-level cache and perfect out-of-order scheduling. We show the\nprocess of building a machine model from available documentation and\nsemi-automatic benchmarking, and carry it out for the latest Intel Skylake and\nAMD Zen micro-architectures. To validate the constructed models, we apply them\nto several assembly kernels and compare runtime predictions with actual\nmeasurements. Finally we give an outlook on how the method may be generalized\nto new architectures.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 12:05:29 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 11:28:46 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Laukemann", "Jan", ""], ["Hammer", "Julian", ""], ["Hofmann", "Johannes", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1809.01369", "submitter": "Vahid Mostofi", "authors": "Vahid Mostofi, Sadegh Aliakbary", "title": "Towards quantitative methods to assess network generative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing generative models is not an easy task. Generative models should\nsynthesize graphs which are not replicates of real networks but show\ntopological features similar to real graphs. We introduce an approach for\nassessing graph generative models using graph classifiers. The inability of an\nestablished graph classifier for distinguishing real and synthesized graphs\ncould be considered as a performance measurement for graph generators.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 07:56:11 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Mostofi", "Vahid", ""], ["Aliakbary", "Sadegh", ""]]}, {"id": "1809.01398", "submitter": "Chen Yuan", "authors": "Chen Yuan, Guangyi Liu, Renchang Dai, Zhiwei Wang", "title": "Power Flow Analysis Using Graph based Combination of Iterative Methods\n  and Vertex Contraction Approach", "comments": "8 pages, 8 figures, 2018 International Conference on Power System\n  Technology (POWERCON 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with relational database (RDB), graph database (GDB) is a more\nintuitive expression of the real world. Each node in the GDB is a both storage\nand logic unit. Since it is connected to its neighboring nodes through edges,\nand its neighboring information could be easily obtained in one-step graph\ntraversal. It is able to conduct local computation independently and all nodes\ncan do their local work in parallel. Then the whole system can be maximally\nanalyzed and assessed in parallel to largely improve the computation\nperformance without sacrificing the precision of final results. This paper\nfirstly introduces graph database, power system graph modeling and potential\ngraph computing applications in power systems. Two iterative methods based on\ngraph database and PageRank are presented and their convergence are discussed.\nVertex contraction is proposed to improve the performance by eliminating\nzero-impedance branch. A combination of the two iterative methods is proposed\nto make use of their advantages. Testing results based on a provincial 1425-bus\nsystem demonstrate that the proposed comprehensive approach is a good candidate\nfor power flow analysis.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:17:56 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Yuan", "Chen", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1809.02018", "submitter": "Debankur Mukherjee", "authors": "Debankur Mukherjee", "title": "Scalable Load Balancing Algorithms in Networked Systems", "comments": "Ph.D. thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in large-scale networked systems viz., data centers\nand cloud networks is to distribute tasks to a pool of servers, using minimal\ninstantaneous state information, while providing excellent delay performance.\nIn this thesis we design and analyze load balancing algorithms that aim to\nachieve a highly efficient distribution of tasks, optimize server utilization,\nand minimize communication overhead.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 14:40:10 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Mukherjee", "Debankur", ""]]}, {"id": "1809.02234", "submitter": "Tommaso Polonelli", "authors": "Tommaso Polonelli, Davide Brunelli and Luca Benini", "title": "Slotted ALOHA Overlay on LoRaWAN: a Distributed Synchronization Approach", "comments": "4 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LoRaWAN is one of the most promising standards for IoT applications.\nNevertheless, the high density of end-devices expected for each gateway, the\nabsence of an effective synchronization scheme between gateway and end-devices,\nchallenge the scalability of these networks. In this article, we propose to\nregulate the communication of LoRaWAN networks using a Slotted-ALOHA (S-ALOHA)\ninstead of the classic ALOHA approach used by LoRa. The implementation is an\noverlay on top of the standard LoRaWAN; thus no modification in pre-existing\nLoRaWAN firmware and libraries is necessary. Our method is based on a novel\ndistributed synchronization service that is suitable for low-cost IoT\nend-nodes. S-ALOHA supported by our synchronization service significantly\nimproves the performance of traditional LoRaWAN networks regarding packet loss\nrate and network throughput.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 22:00:35 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Polonelli", "Tommaso", ""], ["Brunelli", "Davide", ""], ["Benini", "Luca", ""]]}, {"id": "1809.03421", "submitter": "Pandurang Kamat", "authors": "Arati Baliga, I Subhod, Pandurang Kamat and Siddhartha Chatterjee", "title": "Performance Evaluation of the Quorum Blockchain Platform", "comments": "8 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Quorum is a permissioned blockchain platform built from the Ethereum codebase\nwith adaptations to make it a permissioned consortium platform. It is one of\nthe key contenders in the permissioned ledger space. Quorum supports\nconfidentiality and privacy of smart contracts and transactions, and crash and\nByzantine fault tolerant consensus algorithms. In this paper, we characterize\nthe performance features of Quorum. We study the throughput and latency\ncharacteristics of Quorum with different workloads and consensus algorithms\nthat it supports. Through a suite of micro-benchmarks, we explore how certain\ntransaction and smart contract parameters can affect transaction latencies.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 12:32:54 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Baliga", "Arati", ""], ["Subhod", "I", ""], ["Kamat", "Pandurang", ""], ["Chatterjee", "Siddhartha", ""]]}, {"id": "1809.04067", "submitter": "Minjia Zhang", "authors": "Minjia Zhang, Yuxiong He", "title": "Zoom: SSD-based Vector Search for Optimizing Accuracy, Latency and\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of machine learning and deep learning, vector search\nbecomes instrumental to many information retrieval systems, to search and find\nbest matches to user queries based on their semantic similarities.These online\nservices require the search architecture to be both effective with high\naccuracy and efficient with low latency and memory footprint, which existing\nwork fails to offer. We develop, Zoom, a new vector search solution that\ncollaboratively optimizes accuracy, latency and memory based on a multiview\napproach. (1) A \"preview\" step generates a small set of good candidates,\nleveraging compressed vectors in memory for reduced footprint and fast lookup.\n(2) A \"fullview\" step on SSDs reranks those candidates with their full-length\nvector, striking high accuracy. Our evaluation shows that, Zoom achieves an\norder of magnitude improvements on efficiency while attaining equal or higher\naccuracy, comparing with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 23:46:33 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Zhang", "Minjia", ""], ["He", "Yuxiong", ""]]}, {"id": "1809.04151", "submitter": "Leif Andersen", "authors": "Leif Andersen, Vincent St-Amour, Jan Vitek, Matthias Felleisen", "title": "Feature-Specific Profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While high-level languages come with significant readability and\nmaintainability benefits, their performance remains difficult to predict. For\nexample, programmers may unknowingly use language features inappropriately,\nwhich cause their programs to run slower than expected. To address this issue,\nwe introduce feature-specific profiling, a technique that reports performance\ncosts in terms of linguistic constructs. Feature-specific profilers help\nprogrammers find expensive uses of specific features of their language. We\ndescribe the architecture of a profiler that implements our approach, explain\nprototypes of the profiler for two languages with different characteristics and\nimplementation strategies, and provide empirical evidence for the approach's\ngeneral usefulness as a performance debugging tool.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:47:06 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Andersen", "Leif", ""], ["St-Amour", "Vincent", ""], ["Vitek", "Jan", ""], ["Felleisen", "Matthias", ""]]}, {"id": "1809.04324", "submitter": "Laksh Bhatia", "authors": "Laksh Bhatia, Ivana Tomic, Julie A. McCann", "title": "Poster Abstract: LPWA-MAC - a Low Power Wide Area network MAC protocol\n  for cyber-physical systems", "comments": null, "journal-ref": null, "doi": "10.1145/3274783.3275183", "report-no": null, "categories": "cs.SY cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-Power Wide-Area Networks (LPWANs) are being successfully used for the\nmonitoring of large-scale systems that are delay-tolerant and which have\nlow-bandwidth requirements. The next step would be instrumenting these for the\ncontrol of Cyber-Physical Systems (CPSs) distributed over large areas which\nrequire more bandwidth, bounded delays and higher reliability or at least more\nrigorous guarantees therein. This paper presents LPWA-MAC, a novel Low Power\nWide-Area network MAC protocol, that ensures bounded end-to-end delays, high\nchannel utility and supports many of the different traffic patterns and\ndata-rates typical of CPS.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 09:23:42 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Bhatia", "Laksh", ""], ["Tomic", "Ivana", ""], ["McCann", "Julie A.", ""]]}, {"id": "1809.05928", "submitter": "Zhengyu Yang", "authors": "Zhengyu Yang, Manu Awasthi, Mrinmoy Ghosh, Janki Bhimani, Ningfang Mi", "title": "I/O Workload Management for All-Flash Datacenter Storage Systems Based\n  on Total Cost of Ownership", "comments": null, "journal-ref": null, "doi": "10.1109/TBDATA.2018.2871114", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the capital expenditure of flash-based Solid State Driver (SSDs)\nkeeps declining and the storage capacity of SSDs keeps increasing. As a result,\nall-flash storage systems have started to become more economically viable for\nlarge shared storage installations in datacenters, where metrics like Total\nCost of Ownership (TCO) are of paramount importance. On the other hand, flash\ndevices suffer from write amplification, which, if unaccounted, can\nsubstantially increase the TCO of a storage system. In this paper, we first\ndevelop a TCO model for datacenter all-flash storage systems, and then plug a\nWrite Amplification model (WAF) of NVMe SSDs we build based on empirical data\ninto this TCO model. Our new WAF model accounts for workload characteristics\nlike write rate and percentage of sequential writes. Furthermore, using both\nthe TCO and WAF models as the optimization criterion, we design new flash\nresource management schemes (MINTCO) to guide datacenter managers to make\nworkload allocation decisions under the consideration of TCO for SSDs. Based on\nthat, we also develop MINTCO-RAID to support RAID SSDs and MINTCO-OFFLINE to\noptimize the offline workload-disk deployment problem during the initialization\nphase. Experimental results show that MINTCO can reduce the TCO and keep\nrelatively high throughput and space utilization of the entire datacenter\nstorage resources.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 18:25:43 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 03:48:28 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 22:33:12 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Yang", "Zhengyu", ""], ["Awasthi", "Manu", ""], ["Ghosh", "Mrinmoy", ""], ["Bhimani", "Janki", ""], ["Mi", "Ningfang", ""]]}, {"id": "1809.06691", "submitter": "Boyuan Feng", "authors": "Boyuan Feng, Kun Wan, Shu Yang, Yufei Ding", "title": "SECS: Efficient Deep Stream Processing via Class Skew Dichotomy", "comments": "arXiv admin note: text overlap with arXiv:1611.06453 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite that accelerating convolutional neural network (CNN) receives an\nincreasing research focus, the save on resource consumption always comes with a\ndecrease in accuracy. To both increase accuracy and decrease resource\nconsumption, we explore an environment information, called class skew, which is\neasily available and exists widely in daily life. Since the class skew may\nswitch as time goes, we bring up probability layer to utilize class skew\nwithout any overhead during the runtime. Further, we observe class skew\ndichotomy that some class skew may appear frequently in the future, called hot\nclass skew, and others will never appear again or appear seldom, called cold\nclass skew. Inspired by techniques from source code optimization, two modes,\ni.e., interpretation and compilation, are proposed. The interpretation mode\npursues efficient adaption during runtime for cold class skew and the\ncompilation mode aggressively optimize on hot ones for more efficient\ndeployment in the future. Aggressive optimization is processed by\nclass-specific pruning and provides extra benefit. Finally, we design a\nsystematic framework, SECS, to dynamically detect class skew, processing\ninterpretation and compilation, as well as select the most accurate\narchitectures under the runtime resource budget. Extensive evaluations show\nthat SECS can realize end-to-end classification speedups by a factor of 3x to\n11x relative to state-of-the-art convolutional neural networks, at a higher\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 18:03:47 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Feng", "Boyuan", ""], ["Wan", "Kun", ""], ["Yang", "Shu", ""], ["Ding", "Yufei", ""]]}, {"id": "1809.06970", "submitter": "Shuochao Yao", "authors": "Shuochao Yao, Yiran Zhao, Huajie Shao, Shengzhong Liu, Dongxin Liu, Lu\n  Su, Tarek Abdelzaher", "title": "FastDeepIoT: Towards Understanding and Optimizing Neural Network\n  Execution Time on Mobile and Embedded Devices", "comments": "Accepted by SenSys '18", "journal-ref": null, "doi": "10.1145/3274783.3274840", "report-no": null, "categories": "cs.LG cs.NI cs.PF cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks show great potential as solutions to many sensing\napplication problems, but their excessive resource demand slows down execution\ntime, pausing a serious impediment to deployment on low-end devices. To address\nthis challenge, recent literature focused on compressing neural network size to\nimprove performance. We show that changing neural network size does not\nproportionally affect performance attributes of interest, such as execution\ntime. Rather, extreme run-time nonlinearities exist over the network\nconfiguration space. Hence, we propose a novel framework, called FastDeepIoT,\nthat uncovers the non-linear relation between neural network structure and\nexecution time, then exploits that understanding to find network configurations\nthat significantly improve the trade-off between execution time and accuracy on\nmobile and embedded devices. FastDeepIoT makes two key contributions. First,\nFastDeepIoT automatically learns an accurate and highly interpretable execution\ntime model for deep neural networks on the target device. This is done without\nprior knowledge of either the hardware specifications or the detailed\nimplementation of the used deep learning library. Second, FastDeepIoT informs a\ncompression algorithm how to minimize execution time on the profiled device\nwithout impacting accuracy. We evaluate FastDeepIoT using three different\nsensing-related tasks on two mobile devices: Nexus 5 and Galaxy Nexus.\nFastDeepIoT further reduces the neural network execution time by $48\\%$ to\n$78\\%$ and energy consumption by $37\\%$ to $69\\%$ compared with the\nstate-of-the-art compression algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 00:43:26 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Yao", "Shuochao", ""], ["Zhao", "Yiran", ""], ["Shao", "Huajie", ""], ["Liu", "Shengzhong", ""], ["Liu", "Dongxin", ""], ["Su", "Lu", ""], ["Abdelzaher", "Tarek", ""]]}, {"id": "1809.07196", "submitter": "Elliot J. Crowley", "authors": "Jack Turner, Jos\\'e Cano, Valentin Radu, Elliot J. Crowley, Michael\n  O'Boyle, Amos Storkey", "title": "Characterising Across-Stack Optimisations for Deep Convolutional Neural\n  Networks", "comments": "IISWC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) are extremely computationally demanding,\npresenting a large barrier to their deployment on resource-constrained devices.\nSince such systems are where some of their most useful applications lie (e.g.\nobstacle detection for mobile robots, vision-based medical assistive\ntechnology), significant bodies of work from both machine learning and systems\ncommunities have attempted to provide optimisations that will make CNNs\navailable to edge devices. In this paper we unify the two viewpoints in a Deep\nLearning Inference Stack and take an across-stack approach by implementing and\nevaluating the most common neural network compression techniques (weight\npruning, channel pruning, and quantisation) and optimising their parallel\nexecution with a range of programming approaches (OpenMP, OpenCL) and hardware\narchitectures (CPU, GPU). We provide comprehensive Pareto curves to instruct\ntrade-offs under constraints of accuracy, execution time, and memory space.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 13:52:49 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Turner", "Jack", ""], ["Cano", "Jos\u00e9", ""], ["Radu", "Valentin", ""], ["Crowley", "Elliot J.", ""], ["O'Boyle", "Michael", ""], ["Storkey", "Amos", ""]]}, {"id": "1809.07687", "submitter": "Michal Zasadzinski", "authors": "Micha{\\l} Zasadzi\\'nski, Marc Sol\\'e, Alvaro Brandon, Victor\n  Munt\\'es-Mulero, David Carrera", "title": "Next Stop \"NoOps\": Enabling Cross-System Diagnostics Through Graph-based\n  Composition of Logs and Metrics", "comments": "Peer-reviewed, accepted as a regular paper to IEEE Cluster 2018. To\n  be published through proceedings in September 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing diagnostics in IT systems is an increasingly complicated task, and\nit is not doable in satisfactory time by even the most skillful operators.\nSystems and their architecture change very rapidly in response to business and\nuser demand. Many organizations see value in the maintenance and management\nmodel of NoOps that stands for No Operations. One of the implementations of\nthis model is a system that is maintained automatically without any human\nintervention. The path to NoOps involves not only precise and fast diagnostics\nbut also reusing as much knowledge as possible after the system is reconfigured\nor changed. The biggest challenge is to leverage knowledge on one IT system and\nreuse this knowledge for diagnostics of another, different system. We propose a\nframework of weighted graphs which can transfer knowledge, and perform\nhigh-quality diagnostics of IT systems. We encode all possible data in a graph\nrepresentation of a system state and automatically calculate weights of these\ngraphs. Then, thanks to the evaluation of similarity between graphs, we\ntransfer knowledge about failures from one system to another and use it for\ndiagnostics. We successfully evaluate the proposed approach on Spark, Hadoop,\nKafka and Cassandra systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 13:27:30 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Zasadzi\u0144ski", "Micha\u0142", ""], ["Sol\u00e9", "Marc", ""], ["Brandon", "Alvaro", ""], ["Munt\u00e9s-Mulero", "Victor", ""], ["Carrera", "David", ""]]}, {"id": "1809.07701", "submitter": "Pier Luigi Ventre", "authors": "Pier Luigi Ventre, Paolo Lungaroni, Giuseppe Siracusano, Claudio Pisa,\n  Florian Schmidt, Francesco Lombardo and Stefano Salsano", "title": "On the Fly Orchestration of Unikernels: Tuning and Performance\n  Evaluation of Virtual Infrastructure Managers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network operators are facing significant challenges meeting the demand for\nmore bandwidth, agile infrastructures, innovative services, while keeping costs\nlow. Network Functions Virtualization (NFV) and Cloud Computing are emerging as\nkey trends of 5G network architectures, providing flexibility, fast\ninstantiation times, support of Commercial Off The Shelf hardware and\nsignificant cost savings. NFV leverages Cloud Computing principles to move the\ndata-plane network functions from expensive, closed and proprietary hardware to\nthe so-called Virtual Network Functions (VNFs). In this paper we deal with the\nmanagement of virtual computing resources (Unikernels) for the execution of\nVNFs. This functionality is performed by the Virtual Infrastructure Manager\n(VIM) in the NFV MANagement and Orchestration (MANO) reference architecture. We\ndiscuss the instantiation process of virtual resources and propose a generic\nreference model, starting from the analysis of three open source VIMs, namely\nOpenStack, Nomad and OpenVIM. We improve the aforementioned VIMs introducing\nthe support for special-purpose Unikernels and aiming at reducing the duration\nof the instantiation process. We evaluate some performance aspects of the VIMs,\nconsidering both stock and tuned versions. The VIM extensions and performance\nevaluation tools are available under a liberal open source licence.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 12:17:01 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Ventre", "Pier Luigi", ""], ["Lungaroni", "Paolo", ""], ["Siracusano", "Giuseppe", ""], ["Pisa", "Claudio", ""], ["Schmidt", "Florian", ""], ["Lombardo", "Francesco", ""], ["Salsano", "Stefano", ""]]}, {"id": "1809.07851", "submitter": "Zhen Jia", "authors": "Aleksandar Zlateski, Zhen Jia, Kai Li, Fredo Durand", "title": "FFT Convolutions are Faster than Winograd on Modern CPUs, Here is Why", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Winograd-based convolution has quickly gained traction as a preferred\napproach to implement convolutional neural networks (ConvNet) on various\nhardware platforms because it requires fewer floating point operations than\nFFT-based or direct convolutions.\n  This paper compares three highly optimized implementations (regular FFT--,\nGauss--FFT--, and Winograd--based convolutions) on modern multi-- and\nmany--core CPUs. Although all three implementations employed the same\noptimizations for modern CPUs, our experimental results with two popular\nConvNets (VGG and AlexNet) show that the FFT--based implementations generally\noutperform the Winograd--based approach, contrary to the popular belief.\n  To understand the results, we use a Roofline performance model to analyze the\nthree implementations in detail, by looking at each of their computation phases\nand by considering not only the number of floating point operations, but also\nthe memory bandwidth and the cache sizes. The performance analysis explains\nwhy, and under what conditions, the FFT--based implementations outperform the\nWinograd--based one, on modern CPUs.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 20:55:15 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Zlateski", "Aleksandar", ""], ["Jia", "Zhen", ""], ["Li", "Kai", ""], ["Durand", "Fredo", ""]]}, {"id": "1809.08311", "submitter": "Carl Pearson", "authors": "Carl Pearson and Abdul Dakkak and Cheng Li and Sarah Hashash and\n  Jinjun Xiong and Wen-mei Hwu", "title": "SCOPE: C3SR Systems Characterization and Benchmarking Framework", "comments": "8 pages, draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents the design of the Scope infrastructure for extensible\nand portable benchmarking. Improvements in high- performance computing systems\nrely on coordination across different levels of system abstraction. Developing\nand defining accurate performance measurements is necessary at all levels of\nthe system hierarchy, and should be as accessible as possible to developers\nwith different backgrounds. The Scope project aims to lower the barrier to\nentry for developing performance benchmarks by providing a software\narchitecture that allows benchmarks to be developed independently, by providing\nuseful C/C++ abstractions and utilities, and by providing a Python package for\ngenerating publication-quality plots of resulting measurements.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 20:25:44 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Pearson", "Carl", ""], ["Dakkak", "Abdul", ""], ["Li", "Cheng", ""], ["Hashash", "Sarah", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1809.09003", "submitter": "Ting Yu Mu", "authors": "Ting-Yu Mu, Ala Al-Fuqaha, Khaled Shuaib, Farag M. Sallabi, Junaid\n  Qadir", "title": "SDN Flow Entry Management Using Reinforcement Learning", "comments": "19 pages, 11 figures, published on ACM Transactions on Autonomous and\n  Adaptive Systems (TAAS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern information technology services largely depend on cloud\ninfrastructures to provide their services. These cloud infrastructures are\nbuilt on top of datacenter networks (DCNs) constructed with high-speed links,\nfast switching gear, and redundancy to offer better flexibility and resiliency.\nIn this environment, network traffic includes long-lived (elephant) and\nshort-lived (mice) flows with partitioned and aggregated traffic patterns.\nAlthough SDN-based approaches can efficiently allocate networking resources for\nsuch flows, the overhead due to network reconfiguration can be significant.\nWith limited capacity of Ternary Content-Addressable Memory (TCAM) deployed in\nan OpenFlow enabled switch, it is crucial to determine which forwarding rules\nshould remain in the flow table, and which rules should be processed by the SDN\ncontroller in case of a table-miss on the SDN switch. This is needed in order\nto obtain the flow entries that satisfy the goal of reducing the long-term\ncontrol plane overhead introduced between the controller and the switches. To\nachieve this goal, we propose a machine learning technique that utilizes two\nvariations of reinforcement learning (RL) algorithms-the first of which is\ntraditional reinforcement learning algorithm based while the other is deep\nreinforcement learning based. Emulation results using the RL algorithm show\naround 60% improvement in reducing the long-term control plane overhead, and\naround 14% improvement in the table-hit ratio compared to the Multiple Bloom\nFilters (MBF) method given a fixed size flow table of 4KB.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 15:29:06 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Mu", "Ting-Yu", ""], ["Al-Fuqaha", "Ala", ""], ["Shuaib", "Khaled", ""], ["Sallabi", "Farag M.", ""], ["Qadir", "Junaid", ""]]}, {"id": "1809.09206", "submitter": "Millad Ghane", "authors": "Millad Ghane, Jeff Larkin, Larry Shi, Sunita Chandrasekaran, and\n  Margaret S. Cheung", "title": "Power and Energy-efficiency Roofline Model for GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption has been a great deal of concern in recent years and\ndevelopers need to take energy-efficiency into account when they design\nalgorithms. Their design needs to be energy-efficient and low-power while it\ntries to achieve attainable performance provided by underlying hardware.\nHowever, different optimization techniques have different effects on power and\nenergy-efficiency and a visual model would assist in the selection process.\n  In this paper, we extended the roofline model and provided a visual\nrepresentation of optimization strategies for power consumption. Our model is\ncomposed of various ceilings regarding each strategy we included in our models.\nOne roofline model for computational performance and one for memory performance\nis introduced. We assembled our models based on some optimization strategies\nfor two widespread GPUs from NVIDIA: Geforce GTX 970 and Tesla K80.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 20:19:14 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Ghane", "Millad", ""], ["Larkin", "Jeff", ""], ["Shi", "Larry", ""], ["Chandrasekaran", "Sunita", ""], ["Cheung", "Margaret S.", ""]]}, {"id": "1809.10572", "submitter": "Andrew Anderson", "authors": "Andrew Anderson, David Gregg", "title": "Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ARITH.2019.00018", "report-no": null, "categories": "cs.PF cs.CV cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization of weights and activations in Deep Neural Networks (DNNs) is a\npowerful technique for network compression, and has enjoyed significant\nattention and success. However, much of the inference-time benefit of\nquantization is accessible only through the use of customized hardware\naccelerators or by providing an FPGA implementation of quantized arithmetic.\n  Building on prior work, we show how to construct arbitrary bit-precise signed\nand unsigned integer operations using a software technique which logically\n\\emph{embeds} a vector architecture with custom bit-width lanes in universally\navailable fixed-width scalar arithmetic.\n  We evaluate our approach on a high-end Intel Haswell processor, and an\nembedded ARM processor. Our approach yields very fast implementations of\nbit-precise custom DNN operations, which often match or exceed the performance\nof operations quantized to the sizes supported in native arithmetic. At the\nstrongest level of quantization, our approach yields a maximum speedup of\n$\\thicksim6\\times$ on the Intel platform, and $\\thicksim10\\times$ on the ARM\nplatform versus quantization to native 8-bit integers.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 15:25:02 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 15:39:59 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Anderson", "Andrew", ""], ["Gregg", "David", ""]]}, {"id": "1809.10596", "submitter": "David Koops", "authors": "David Koops", "title": "Predicting the confirmation time of Bitcoin transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the probabilistic distribution of the confirmation time of Bitcoin\ntransactions, conditional on the current memory pool (i.e., the queue of\ntransactions awaiting confirmation). The results of this paper are particularly\ninteresting for users that want to make a Bitcoin transaction during\n`heavy-traffic situations', when the transaction demand exceeds the block\ncapacity. In such situations, Bitcoin users tend to bid up the transaction\nfees, in order to gain priority over other users that pay a lower fee. We argue\nthat the time until a Bitcoin transaction is confirmed can be modelled as a\nparticular stochastic fluid queueing process (to be precise: a\nCram\\'er-Lundberg process). We approximate the queueing process in two\ndifferent ways. The first approach leads to a lower bound on the confirmation\nprobability, which becomes increasingly tight as traffic decreases. The second\napproach relies on a diffusion approximation with a continuity correction,\nwhich becomes increasingly accurate as traffic intensifies. The accuracy of the\napproximations under different traffic loads are evaluated in a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 15:58:12 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Koops", "David", ""]]}, {"id": "1809.10663", "submitter": "James F. Brady", "authors": "James F Brady", "title": "Is Your Load Generator Launching Web Requests in Bunches?", "comments": "One link was updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One problem with load test quality, almost always overlooked, is the\npotential for the load generator's user thread pool to sync up and dispatch\nqueries in bunches rather than independently from each other like real users\ninitiate their requests. A spiky launch pattern misrepresents workload flow as\nwell as yields erroneous application response time statistics. This paper\ndescribes what a real user request timing pattern looks like, illustrates how\nto identify it in the load generation environment, and exercises a free\ndownloadable tool which measures how well the load generator is mimicking the\ntiming pattern of real web user requests.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 17:45:07 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 22:08:13 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 22:43:38 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2020 20:32:53 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Brady", "James F", ""]]}, {"id": "1809.10937", "submitter": "Tom\\'as F. Pena", "authors": "O. G. Lorenzo, M. L. Beco\\~na, T. F. Pena, J. C. Cabaleiro, J. A.\n  Lorenzo, and F. F. Rivera", "title": "New Thread Migration Strategies for NUMA Systems", "comments": "Unpublished work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multicore systems present on-board memory hierarchies and communication\nnetworks that influence performance when executing shared memory parallel\ncodes. Characterising this influence is complex, and understanding the effect\nof particular hardware configurations on different codes is of paramount\nimportance. In previous works, monitoring information extracted from hardware\ncounters at runtime has been used to characterise the behaviour of each thread\nin the parallel code in terms of the number of floating point operations per\nsecond, operational intensity, and latency of memory access. We propose to use\nthis information to guide thread migration strategies that improve execution\nefficiency by increasing locality and affinity. Different configurations of NAS\nParallel OpenMP benchmarks on multicores were used to validate the benefits of\nthe proposed thread migration strategies. Our proposed strategies produce up to\n70% improvement in scenarios where locality and affinity are low, there being a\nsmall degradation in performance for codes with high locality and affinity.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 09:56:05 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Lorenzo", "O. G.", ""], ["Beco\u00f1a", "M. L.", ""], ["Pena", "T. F.", ""], ["Cabaleiro", "J. C.", ""], ["Lorenzo", "J. A.", ""], ["Rivera", "F. F.", ""]]}]