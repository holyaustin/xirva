[{"id": "1412.0041", "submitter": "Liang Wang", "authors": "Liang Wang and Gareth Tyson and Jussi Kangasharju and Jon Crowcroft", "title": "FairCache: Introducing Fairness to ICN Caching - Technical Report", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.GT cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-centric networking extensively uses universal in-network caching.\nHowever, developing an efficient and fair collaborative caching algorithm for\nselfish caches is still an open question. In addition, the communication\noverhead induced by collaboration is especially poorly understood in a general\nnetwork setting such as realistic ISP and Autonomous System networks. In this\npaper, we address these two problems by modeling the in-network caching problem\nas a Nash bargaining game. We show that the game is a convex optimization\nproblem and further derive the corresponding distributed algorithm. We\nanalytically investigate the collaboration overhead on general graph\ntopologies, and theoretically show that collaboration has to be constrained\nwithin a small neighborhood due to its cost growing exponentially. Our proposed\nalgorithm achieves at least 16% performance gain over its competitors on\ndifferent network topologies in the evaluation, and guarantees provable\nconvergence, Pareto efficiency and proportional fairness.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 22:16:00 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2015 16:13:40 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 11:32:49 GMT"}, {"version": "v4", "created": "Tue, 2 May 2017 11:40:24 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Wang", "Liang", ""], ["Tyson", "Gareth", ""], ["Kangasharju", "Jussi", ""], ["Crowcroft", "Jon", ""]]}, {"id": "1412.0544", "submitter": "Mario Mulansky", "authors": "Mario Mulansky", "title": "Optimizing Large-Scale ODE Simulations", "comments": "18 pages, 9 figures submitted to SIAM Journal of Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.PF nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a strategy to speed up Runge-Kutta-based ODE simulations of large\nsystems with nearest-neighbor coupling. We identify the cache/memory bandwidth\nas the crucial performance bottleneck. To reduce the required bandwidth, we\nintroduce a granularity in the simulation and identify the optimal cluster size\nin a performance study. This leads to a considerable performance increase and\ntransforms the algorithm from bandwidth bound to CPU bound. By additionally\nemploying SIMD instructions we are able to boost the efficiency even further.\nIn the end, a total performance increase of up to a factor three is observed\nwhen using cache optimization and SIMD instructions compared to a standard\nimplementation. All simulation codes are written in C++ and made publicly\navailable. By using the modern C++ libraries Boost.odeint and Boost.SIMD, these\noptimizations can be implemented with minimal programming effort.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 17:08:51 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Mulansky", "Mario", ""]]}, {"id": "1412.2321", "submitter": "Harsha Honnappa", "authors": "Harsha Honnappa, Rahul Jain, Amy R. Ward", "title": "On Transitory Queueing", "comments": "Under review (and revision), Math. of Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework and develop a theory of transitory queueing models.\nThese are models that are not only non-stationary and time-varying but also\nhave other features such as the queueing system operates over finite time, or\nonly a finite population arrives. Such models are relevant in many real-world\nsettings, from queues at post-offces, DMV, concert halls and stadia to\nout-patient departments at hospitals. We develop fluid and diffusion limits for\na large class of transitory queueing models. We then introduce three specific\nmodels that fit within this framework, namely, the Delta(i)/GI/1 model, the\nconditioned G/GI/1 model, and an arrival model of scheduled traffic with epoch\nuncertainty. We show that asymptotically these models are distributionally\nequivalent, i.e., they have the same fluid and diffusion limits. We note that\nour framework provides the first ever way of analyzing the standard G/GI/1\nmodel when we condition on the number of arrivals. In obtaining these results,\nwe provide generalizations and extensions of the Glivenko-Cantelli and Donskers\nTheorem for empirical processes with triangular arrays. Our analysis uses the\npopulation acceleration technique that we introduce and develop. This may be\nuseful in analysis of other non-stationary and non-ergodic queuing models.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 06:04:53 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Honnappa", "Harsha", ""], ["Jain", "Rahul", ""], ["Ward", "Amy R.", ""]]}, {"id": "1412.2347", "submitter": "Christophe Dubach", "authors": "Christophe Dubach, Grigori Fursin", "title": "Proceedings of the 5th International Workshop on Adaptive Self-tuning\n  Computing Systems 2015 (ADAPT'15)", "comments": null, "journal-ref": null, "doi": null, "report-no": "ADAPT/2015/00", "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the proceedings of the 5th International Workshop on Adaptive\nSelf-tuning Computing Systems 2015 (ADAPT'15).\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 12:21:58 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Dubach", "Christophe", ""], ["Fursin", "Grigori", ""]]}, {"id": "1412.3022", "submitter": "Nicolas Le Scouarnec", "authors": "Nicolas Le Scouarnec", "title": "Fast Product-Matrix Regenerating Codes", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage systems support failures of individual devices by the use\nof replication or erasure correcting codes. While erasure correcting codes\noffer a better storage efficiency than replication for similar fault tolerance,\nthey incur higher CPU consumption, higher network consumption and higher disk\nI/Os. To address these issues, codes specific to storage systems have been\ndesigned. Their main feature is the ability to repair a single lost disk\nefficiently. In this paper, we focus on one such class of codes that minimize\nnetwork consumption during repair, namely regenerating codes. We implement the\noriginal Product-Matrix Regenerating codes as well as a new optimization we\npropose and show that the resulting optimized codes allow achieving 790 MB/s\nfor encoding in typical settings. Reported speeds are significantly higher than\nprevious studies, highlighting that regenerating codes can be used with little\nCPU penalty.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 17:07:52 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Scouarnec", "Nicolas Le", ""]]}, {"id": "1412.3624", "submitter": "Mostafa Zaman Chowdhury", "authors": "Mostafa Zaman Chowdhury, Muhammad Shahin Uddin, and Yeong Min Jang", "title": "Dynamic Channel Allocation for Class-Based QoS Provisioning and Call\n  Admission in Visible Light Communication", "comments": null, "journal-ref": "The Arabian Journal for Science and Engineering. vol. 39, no. 2,\n  pp. 1007-1016, Feb. 2014", "doi": "10.1007/s13369-013-0680-4", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provisioning of quality of service (QoS) is a key issue in visible light\ncommunication (VLC) system as well as in other wireless communication systems.\nDue to the fact that QoS requirements are not as strict for all traffic types,\nmore calls of higher priority traffic classes can be accommodated by blocking\nsome more calls of lower priority traffic classes. Diverse types of high data\nrate traffic are supported by existing wireless communication systems while the\nresource is limited. Hence, priority based resource allocation can ensure the\nservice quality for the calls of important traffic class. The fixed guard\nchannels to prioritize any class of calls always reduce the channel\nutilization. In this paper we propose a priority based dynamic channel\nreservation scheme for higher priority calls that does not reduce the channel\nutilization significantly. The number of reserved channels for each of the\nindividual traffic classes is calculated using real-time observation of the\ncall arrival rates of all the traffic classes. The features of the scheme allow\nreduction of the call blocking probability of higher priority calls along with\nthe increase of the channel utilization. The proposed Markov Chain model is\nexpected to be very much effective for the queuing analysis especially for the\npriority scheme of any number of traffic classes. The numerical results show\nthat the proposed scheme is able to attain reasonable call blocking probability\nof higher priority calls without sacrificing channel utilization.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 12:15:12 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Chowdhury", "Mostafa Zaman", ""], ["Uddin", "Muhammad Shahin", ""], ["Jang", "Yeong Min", ""]]}, {"id": "1412.3625", "submitter": "Mostafa Zaman Chowdhury", "authors": "Mostafa Zaman Chowdhury and Yeong Min Jang", "title": "Class-Based Service Connectivity using Multi-Level Bandwidth Adaptation\n  in Multimedia Wireless Networks", "comments": "Journal paper", "journal-ref": "Wireless Personal Communications, vol. 77, no 4, pp. 2735-2745,\n  August 2014", "doi": "10.1007/s11277-014-1665-7", "report-no": null, "categories": "cs.NI cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the fact that quality of service requirements are not very strict for\nall traffic types, more calls of higher priority can be accommodated by\nreducing some bandwidth allocation for the bandwidth adaptive calls. The\nbandwidth adaptation to accept a higher priority call is more than that of a\nlower priority call. Therefore, the multi-level bandwidth adaptation technique\nimproves the overall forced call termination probability as well as provides\npriority of the traffic classes in terms of call blocking probability without\nreducing the bandwidth utilization. We propose a novel bandwidth adaptation\nmodel that releases multi-level of bandwidth from the existing multimedia\ntraffic calls. The amount of released bandwidth is decided based on the\npriority of the requesting traffic calls and the number of existing bandwidth\nadaptive calls. This prioritization of traffic classes does not reduce the\nbandwidth utilization. Moreover, our scheme reduces the overall forced call\ntermination probability significantly. The proposed scheme is modeled using the\nMarkov Chain. The numerical results show that the proposed scheme is able to\nprovide negligible handover call dropping probability as well as significantly\nreduced new call blocking probability of higher priority calls without\nincreasing the overall forced call termination probability.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 12:15:51 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Chowdhury", "Mostafa Zaman", ""], ["Jang", "Yeong Min", ""]]}, {"id": "1412.3687", "submitter": "Nicolae Brinzei", "authors": "Gilles Deleuze, Nicolae Brinzei (CRAN), Nicolas Villaume", "title": "Modelling common cause failures of large digital I&C systems with\n  coloured Petri nets", "comments": "in French, 19\\`eme Congr\\`es de Ma\\^itrise des Risques et S\\^uret\\'e\n  de Fonctionnement, Lambda-Mu'2014, Oct 2014, Dijon, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is the representation of Common Cause Failures\n(CCF) in large digital systems. The system under study is representative of a\ncontrol system of a nuclear plant. The model for CCF is the generalized Atwood\nmodel. It can represent independent failures, CCF non-lethal for some system\nelements and CCF lethal to all. The Atwood model was modified to \"direct\"\nnon-lethal DCC on certain parts of the system and take into account the\ndifferent possible origins of DCC. Maintenance and repairs are taken into\naccount in the model that is thus dynamic. The main evaluation results are\nprobabilistic, the considered indicator is the probability of failure on demand\n(PFD). A comparison is made between the estimator of the PFD taking into\naccount all the failures and the estimator taking into account only the\ndetected failures.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 18:06:33 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Deleuze", "Gilles", "", "CRAN"], ["Brinzei", "Nicolae", "", "CRAN"], ["Villaume", "Nicolas", ""]]}, {"id": "1412.3906", "submitter": "Christian Plessl", "authors": "Marvin Damschen and Christian Plessl", "title": "Easy-to-Use On-the-Fly Binary Program Acceleration on Many-Cores", "comments": "Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)", "journal-ref": null, "doi": null, "report-no": "ADAPT/2015/05", "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Binary Acceleration At Runtime (BAAR), an easy-to-use\non-the-fly binary acceleration mechanism which aims to tackle the problem of\nenabling existent software to automatically utilize accelerators at runtime.\nBAAR is based on the LLVM Compiler Infrastructure and has a client-server\narchitecture. The client runs the program to be accelerated in an environment\nwhich allows program analysis and profiling. Program parts which are identified\nas suitable for the available accelerator are exported and sent to the server.\nThe server optimizes these program parts for the accelerator and provides RPC\nexecution for the client. The client transforms its program to utilize\naccelerated execution on the server for offloaded program parts.\n  We evaluate our work with a proof-of-concept implementation of BAAR that uses\nan Intel Xeon Phi 5110P as the acceleration target and performs automatic\noffloading, parallelization and vectorization of suitable program parts. The\npracticality of BAAR for real-world examples is shown based on a study of\nstencil codes. Our results show a speedup of up to 4x without any\ndeveloper-provided hints and 5.77x with hints over the same code compiled with\nthe Intel Compiler at optimization level O2 and running on an Intel Xeon\nE5-2670 machine. Based on our insights gained during implementation and\nevaluation we outline future directions of research, e.g., offloading more\nfine-granular program parts than functions, a more sophisticated communication\nmechanism or introducing on-stack-replacement.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 07:44:52 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Damschen", "Marvin", ""], ["Plessl", "Christian", ""]]}, {"id": "1412.6161", "submitter": "\\\"Ozkan Karabacak Mr.", "authors": "Ferruh \\.Ilhan and \\\"Ozkan Karabacak", "title": "Graph-Based Minimum Dwell Time and Average Dwell Time Computations for\n  Discrete-Time Switched Linear Systems", "comments": null, "journal-ref": "Asian Journal of Control, 18, 2018-2026, 2016", "doi": null, "report-no": null, "categories": "cs.SY cs.PF nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete-time switched linear systems where switchings are governed by a\ndigraph are considered. The minimum (or average) dwell time that guarantees the\nasymptotic stability can be computed by calculating the maximum cycle ratio (or\nmaximum cycle mean) of a doubly weighted digraph where weights depend on the\neigenvalues and eigenvectors of subsystem matrices. The graph-based method is\napplied to systems with defective subsystem matrices using Jordan\ndecomposition. In the case of bimodal switched systems scaling algorithms that\nminimizes the condition number can be used to give a better minimum (or\naverage) dwell time estimates.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 20:30:49 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 15:38:15 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["\u0130lhan", "Ferruh", ""], ["Karabacak", "\u00d6zkan", ""]]}, {"id": "1412.6382", "submitter": "Liang Wang", "authors": "Julien Mineraud, Liang Wang, Sasitharan Balasubramaniam, Jussi\n  Kangasharju", "title": "Renewable Energy-Aware Information-Centric Networking", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ICT industry today is placed as one of the major consumers of energy,\nwhere recent reports have also shown that the industry is a major contributor\nto global carbon emissions. While renewable energy-aware data centers have been\nproposed, these solutions have certain limitations. The primary limitation is\ndue to the design of data centers which focus on large-size facilities located\nin selected locations. This paper addresses this problem, by utilizing\nin-network caching with each router having storage and being powered by\nrenewable energy sources (wind and solar). Besides placing contents closer to\nend users, utilizing in-network caching could potentially increase probability\nof capturing renewable energy in diverse geographical locations. Our proposed\nsolution is dual- layered: on the first layer a distributed gradient-based\nrouting protocol is used to discover the paths along routers that are powered\nby the highest renewable energy, and on the second layer, a caching mechanism\nwill pull the contents from the data centre and place them on routers of the\npaths that are discovered by our routing protocol. Through our experiments on a\ntestbed utilizing real meteorological data, our proposed solution has\ndemonstrated increased quantity of renewable energy consumption, while reducing\nthe workload on the data centers.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:36:16 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Mineraud", "Julien", ""], ["Wang", "Liang", ""], ["Balasubramaniam", "Sasitharan", ""], ["Kangasharju", "Jussi", ""]]}, {"id": "1412.6765", "submitter": "Nassim Halli", "authors": "Nassim A. Halli, Henri-Pierre Charles and Jean-Fran\\c{c}ois Mehaut", "title": "Performance comparison between Java and JNI for optimal implementation\n  of computational micro-kernels", "comments": "Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)", "journal-ref": null, "doi": null, "report-no": "ADAPT/2015/06", "categories": "cs.PF cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General purpose CPUs used in high performance computing (HPC) support a\nvector instruction set and an out-of-order engine dedicated to increase the\ninstruction level parallelism. Hence, related optimizations are currently\ncritical to improve the performance of applications requiring numerical\ncomputation. Moreover, the use of a Java run-time environment such as the\nHotSpot Java Virtual Machine (JVM) in high performance computing is a promising\nalternative. It benefits from its programming flexibility, productivity and the\nperformance is ensured by the Just-In-Time (JIT) compiler. Though, the JIT\ncompiler suffers from two main drawbacks. First, the JIT is a black box for\ndevelopers. We have no control over the generated code nor any feedback from\nits optimization phases like vectorization. Secondly, the time constraint\nnarrows down the degree of optimization compared to static compilers like GCC\nor LLVM. So, it is compelling to use statically compiled code since it benefits\nfrom additional optimization reducing performance bottlenecks. Java enables to\ncall native code from dynamic libraries through the Java Native Interface\n(JNI). Nevertheless, JNI methods are not inlined and require an additional cost\nto be invoked compared to Java ones. Therefore, to benefit from better static\noptimization, this call overhead must be leveraged by the amount of computation\nperformed at each JNI invocation. In this paper we tackle this problem and we\npropose to do this analysis for a set of micro-kernels. Our goal is to select\nthe most efficient implementation considering the amount of computation defined\nby the calling context. We also investigate the impact on performance of\nseveral different optimization schemes which are vectorization, out-of-order\noptimization, data alignment, method inlining and the use of native memory for\nJNI methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 11:26:39 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Halli", "Nassim A.", ""], ["Charles", "Henri-Pierre", ""], ["Mehaut", "Jean-Fran\u00e7ois", ""]]}, {"id": "1412.7682", "submitter": "Roshan Ragel", "authors": "Hasindu Gamaarachchi, Roshan Ragel, and Darshana Jayasinghe", "title": "Accelerating Correlation Power Analysis Using Graphics Processing Units", "comments": "The 7th International Conference on Information and Automation for\n  Sustainability (ICIAfS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation Power Analysis (CPA) is a type of power analysis based side\nchannel attack that can be used to derive the secret key of encryption\nalgorithms including DES (Data Encryption Standard) and AES (Advanced\nEncryption Standard). A typical CPA attack on unprotected AES is performed by\nanalysing a few thousand power traces that requires about an hour of\ncomputational time on a general purpose CPU. Due to the severity of this\nsituation, a large number of researchers work on countermeasures to such\nattacks. Verifying that a proposed countermeasure works well requires\nperforming the CPA attack on about 1.5 million power traces. Such processing,\neven for a single attempt of verification on commodity hardware would run for\nseveral days making the verification process infeasible. Modern Graphics\nProcessing Units (GPUs) have support for thousands of light weight threads,\nmaking them ideal for parallelizable algorithms like CPA. While the cost of a\nGPU being lesser than a high performance multicore server, still the GPU\nperformance for this algorithm is many folds better than that of a multicore\nserver. We present an algorithm and its implementation on GPU for CPA on\n128-bit AES that is capable of executing 1300x faster than that on a single\nthreaded CPU and more than 60x faster than that on a 32 threaded multicore\nserver. We show that an attack that would take hours on the multicore server\nwould take even less than a minute on a much cost effective GPU.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 15:10:03 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Gamaarachchi", "Hasindu", ""], ["Ragel", "Roshan", ""], ["Jayasinghe", "Darshana", ""]]}, {"id": "1412.7789", "submitter": "Roshan Ragel", "authors": "Vajira Thambawita, Roshan Ragel and Dhammika Elkaduwe", "title": "To Use or Not to Use: Graphics Processing Units for Pattern Matching\n  Algorithms", "comments": "appears in The 7th International Conference on Information and\n  Automation for Sustainability (ICIAfS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String matching is an important part in today's computer applications and\nAho-Corasick algorithm is one of the main string matching algorithms used to\naccomplish this. This paper discusses that when can the GPUs be used for string\nmatching applications using the Aho-Corasick algorithm as a benchmark. We have\nto identify the best unit to run our string matching algorithm according to the\nperformance of our devices and the applications. Sometimes CPU gives better\nperformance than GPU and sometimes GPU gives better performance than CPU.\nTherefore, identifying this critical point is significant task for researchers\nwho are using GPUs to improve the performance of their string matching\napplications based on string matching algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 05:27:49 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Thambawita", "Vajira", ""], ["Ragel", "Roshan", ""], ["Elkaduwe", "Dhammika", ""]]}, {"id": "1412.8467", "submitter": "Roshan Ragel", "authors": "S.M.Vidanagamachchi, S.D. Dewasurendra, R.G. Ragel and M. Niranjan", "title": "A Structured Hardware Software Architecture for Peptide Based Diagnosis\n  - Sub-string Matching Problem with Limited Tolerance (ICIAfS14)", "comments": "appears in The 7th International Conference on Information and\n  Automation for Sustainability (ICIAfS) 2014. arXiv admin note: substantial\n  text overlap with arXiv:1412.7811", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inferring proteins from complex peptide samples in shotgun\nproteomic workflow sets extreme demands on computational resources. This is\nexacerbated by the fact that, in general, a given protein cannot be defined by\na fixed sequence of amino acids due to the existence of splice variants and\nisoforms of that protein. Therefore, the problem of protein inference could be\nconsidered as one of identifying sequences of amino acids with some limited\ntolerance. Two problems arise from this: a) due to these variations, the\napplicability of exact string matching methodologies could be questioned and b)\nthe difficulty of defining a reference sequence for a particular set of\nproteins that are functionally indistinguishable, but with some variation in\nfeatures. This paper presents a model-based inference approach that is\ndeveloped and validated to solve the inference problem. Our approach starts\nfrom an examination of the known set of splice variants and isoforms of a\ntarget protein to identify the Greatest Common Stable Substring (GCSS) of amino\nacids and the Substrings Subjects to Limited Variation (SSLV) and their\nrespective locations on the GCSS. Then we define and solve the Sub-string\nMatching Problem with Limited Tolerance (SMPLT). This approach is validated on\nidentified peptides in a labelled and clustered data set from UNIPROT.\nIdentification of Baylisascaris Procyonis infection was used as an application\ninstance that achieved up to 70 times speedup compared to a software only\nsystem. This workflow can be generalised to any inexact multiple pattern\nmatching application by replacing the patterns in a clustered and distributed\nenvironment which permits a distance between member strings to account for\npermitted deviations such as substitutions, insertions and deletions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 11:44:51 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Vidanagamachchi", "S. M.", ""], ["Dewasurendra", "S. D.", ""], ["Ragel", "R. G.", ""], ["Niranjan", "M.", ""]]}]