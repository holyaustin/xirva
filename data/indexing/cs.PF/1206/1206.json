[{"id": "1206.0919", "submitter": "Jochen Gerhard", "authors": "Jochen Gerhard, Volker Lindenstruth, and Marcus Bleicher", "title": "Relativistic Hydrodynamics on Graphic Cards", "comments": "Details and discussions added, replaced with accepted version. (15\n  pages, 8 figures, 2 listings)", "journal-ref": null, "doi": "10.1016/j.cpc.2012.09.013", "report-no": null, "categories": "hep-ph cs.DC cs.PF hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to accelerate relativistic hydrodynamics simulations using\ngraphic cards (graphic processing units, GPUs). These improvements are of\nhighest relevance e.g. to the field of high-energetic nucleus-nucleus\ncollisions at RHIC and LHC where (ideal and dissipative) relativistic\nhydrodynamics is used to calculate the evolution of hot and dense QCD matter.\nThe results reported here are based on the Sharp And Smooth Transport Algorithm\n(SHASTA), which is employed in many hydrodynamical models and hybrid simulation\npackages, e.g. the Ultrarelativistic Quantum Molecular Dynamics model (UrQMD).\nWe have redesigned the SHASTA using the OpenCL computing framework to work on\naccelerators like graphic processing units (GPUs) as well as on multi-core\nprocessors. With the redesign of the algorithm the hydrodynamic calculations\nhave been accelerated by a factor 160 allowing for event-by-event calculations\nand better statistics in hybrid calculations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 13:16:36 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2012 10:07:49 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Gerhard", "Jochen", ""], ["Lindenstruth", "Volker", ""], ["Bleicher", "Marcus", ""]]}, {"id": "1206.0951", "submitter": "Vincent Gauthier", "authors": "Teck Aguilar and Mohamed Chedly Ghedira and Syue-Ju Syue and Vincent\n  Gauthier and Hossam Afifi and Chin-Liang Wang", "title": "A Cross-Layer Design Based on Geographic Information for Cooperative\n  Wireless Networks", "comments": "in 2010 IEEE 71st Vehicular Technology Conference, 2010", "journal-ref": null, "doi": "10.1109/VETECS.2010.5494169", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of geographic routing approaches in wireless ad hoc and sensor networks\ndo not take into consideration the medium access control (MAC) and physical\nlayers when designing a routing protocol. In this paper, we focus on a\ncross-layer framework design that exploits the synergies between network, MAC,\nand physical layers. In the proposed CoopGeo, we use a beaconless forwarding\nscheme where the next hop is selected through a contention process based on the\ngeographic position of nodes. We optimize this Network-MAC layer interaction\nusing a cooperative relaying technique with a relay selection scheme also based\non geographic information in order to improve the system performance in terms\nof reliability.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 16:22:37 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Aguilar", "Teck", ""], ["Ghedira", "Mohamed Chedly", ""], ["Syue", "Syue-Ju", ""], ["Gauthier", "Vincent", ""], ["Afifi", "Hossam", ""], ["Wang", "Chin-Liang", ""]]}, {"id": "1206.1099", "submitter": "Andrey Bernstein", "authors": "Andrey Bernstein, Daniel Bienstock, David Hay, Meric Uzunoglu, and Gil\n  Zussman", "title": "Power Grid Vulnerability to Geographically Correlated Failures -\n  Analysis and Control Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": "Columbia University, Electrical Engineering, Technical Report\n  #2011-05-06, Nov. 2011", "categories": "cs.SY cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider power line outages in the transmission system of the power grid,\nand specifically those caused by a natural disaster or a large scale physical\nattack. In the transmission system, an outage of a line may lead to overload on\nother lines, thereby eventually leading to their outage. While such cascading\nfailures have been studied before, our focus is on cascading failures that\nfollow an outage of several lines in the same geographical area. We provide an\nanalytical model of such failures, investigate the model's properties, and show\nthat it differs from other models used to analyze cascades in the power grid\n(e.g., epidemic/percolation-based models). We then show how to identify the\nmost vulnerable locations in the grid and perform extensive numerical\nexperiments with real grid data to investigate the various effects of\ngeographically correlated outages and the resulting cascades. These results\nallow us to gain insights into the relationships between various parameters and\nperformance metrics, such as the size of the original event, the final number\nof connected components, and the fraction of demand (load) satisfied after the\ncascade. In particular, we focus on the timing and nature of optimal control\nactions used to reduce the impact of a cascade, in real time. We also compare\nresults obtained by our model to the results of a real cascade that occurred\nduring a major blackout in the San Diego area on Sept. 2011. The analysis and\nresults presented in this paper will have implications both on the design of\nnew power grids and on identifying the locations for shielding, strengthening,\nand monitoring efforts in grid upgrades.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 00:39:46 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Bernstein", "Andrey", ""], ["Bienstock", "Daniel", ""], ["Hay", "David", ""], ["Uzunoglu", "Meric", ""], ["Zussman", "Gil", ""]]}, {"id": "1206.1199", "submitter": "Naohito Nakasato", "authors": "Naohito Nakasato, Go Ogiya, Yohei Miki, Masao Mori and Ken'ichi Nomoto", "title": "Astrophysical Particle Simulations on Heterogeneous CPU-GPU Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A heterogeneous CPU-GPU node is getting popular in HPC clusters. We need to\nrethink algorithms and optimization techniques for such system depending on the\nrelative performance of CPU vs. GPU. In this paper, we report a performance\noptimized particle simulation code \"OTOO\", that is based on the octree method,\nfor heterogenous systems. Main applications of OTOO are astrophysical\nsimulations such as N-body models and the evolution of a violent merger of\nstars. We propose optimal task split between CPU and GPU where GPU is only used\nto compute the calculation of the particle force. Also, we describe\noptimization techniques such as control of the force accuracy, vectorized tree\nwalk, and work partitioning among multiple GPUs. We used OTOO for modeling a\nmerger of two white dwarf stars and found that OTOO is powerful and practical\nto simulate the fate of the process.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 12:23:54 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Nakasato", "Naohito", ""], ["Ogiya", "Go", ""], ["Miki", "Yohei", ""], ["Mori", "Masao", ""], ["Nomoto", "Ken'ichi", ""]]}, {"id": "1206.1204", "submitter": "Yanfu Li", "authors": "Yanfu Li (SSEC, LGI), Enrico Zio (SSEC)", "title": "Uncertainty Analysis of the Adequacy Assessment Model of a Distributed\n  Generation System", "comments": null, "journal-ref": "Renewable Energy 41 (2012) 235-244", "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the inherent aleatory uncertainties in renewable generators, the\nreliability/adequacy assessments of distributed generation (DG) systems have\nbeen particularly focused on the probabilistic modeling of random behaviors,\ngiven sufficient informative data. However, another type of uncertainty\n(epistemic uncertainty) must be accounted for in the modeling, due to\nincomplete knowledge of the phenomena and imprecise evaluation of the related\ncharacteristic parameters. In circumstances of few informative data, this type\nof uncertainty calls for alternative methods of representation, propagation,\nanalysis and interpretation. In this study, we make a first attempt to\nidentify, model, and jointly propagate aleatory and epistemic uncertainties in\nthe context of DG systems modeling for adequacy assessment. Probability and\npossibility distributions are used to model the aleatory and epistemic\nuncertainties, respectively. Evidence theory is used to incorporate the two\nuncertainties under a single framework. Based on the plausibility and belief\nfunctions of evidence theory, the hybrid propagation approach is introduced. A\ndemonstration is given on a DG system adapted from the IEEE 34 nodes\ndistribution test feeder. Compared to the pure probabilistic approach, it is\nshown that the hybrid propagation is capable of explicitly expressing the\nimprecision in the knowledge on the DG parameters into the final adequacy\nvalues assessed. It also effectively captures the growth of uncertainties with\nhigher DG penetration levels.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 12:48:25 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Li", "Yanfu", "", "SSEC, LGI"], ["Zio", "Enrico", "", "SSEC"]]}, {"id": "1206.1264", "submitter": "Siva Theja Maguluri", "authors": "Siva Theja Maguluri, R Srikant, Lei Ying", "title": "Heavy Traffic Optimal Resource Allocation Algorithms for Cloud Computing\n  Clusters", "comments": "Technical Report corresponding to the paper of same title to be\n  presented at International Teletraffic Conference 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is emerging as an important platform for business, personal\nand mobile computing applications. In this paper, we study a stochastic model\nof cloud computing, where jobs arrive according to a stochastic process and\nrequest resources like CPU, memory and storage space. We consider a model where\nthe resource allocation problem can be separated into a routing or load\nbalancing problem and a scheduling problem. We study the\njoin-the-shortest-queue routing and power-of-two-choices routing algorithms\nwith MaxWeight scheduling algorithm. It was known that these algorithms are\nthroughput optimal. In this paper, we show that these algorithms are queue\nlength optimal in the heavy traffic limit.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 16:07:58 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Maguluri", "Siva Theja", ""], ["Srikant", "R", ""], ["Ying", "Lei", ""]]}, {"id": "1206.1390", "submitter": "Mark Hoemmen", "authors": "Patrick G. Bridges, Kurt B. Ferreira, Michael A. Heroux, Mark Hoemmen", "title": "Fault-tolerant linear solvers via selective reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy increasingly constrains modern computer hardware, yet protecting\ncomputations and data against errors costs energy. This holds at all scales,\nbut especially for the largest parallel computers being built and planned\ntoday. As processor counts continue to grow, the cost of ensuring reliability\nconsistently throughout an application will become unbearable. However, many\nalgorithms only need reliability for certain data and phases of computation.\nThis suggests an algorithm and system codesign approach. We show that if the\nsystem lets applications apply reliability selectively, we can develop\nalgorithms that compute the right answer despite faults. These \"fault-tolerant\"\niterative methods either converge eventually, at a rate that degrades\ngracefully with increased fault rate, or return a clear failure indication in\nthe rare case that they cannot converge. Furthermore, they store most of their\ndata unreliably, and spend most of their time in unreliable mode.\n  We demonstrate this for the specific case of detected but uncorrectable\nmemory faults, which we argue are representative of all kinds of faults. We\ndeveloped a cross-layer application / operating system framework that\nintercepts and reports uncorrectable memory faults to the application, rather\nthan killing the application, as current operating systems do. The application\nin turn can mark memory allocations as subject to such faults. Using this\nframework, we wrote a fault-tolerant iterative linear solver using components\nfrom the Trilinos solvers library. Our solver exploits hybrid parallelism (MPI\nand threads). It performs just as well as other solvers if no faults occur, and\nconverges where other solvers do not in the presence of faults. We show\nconvergence results for representative test problems. Near-term future work\nwill include performance tests.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 03:18:42 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Bridges", "Patrick G.", ""], ["Ferreira", "Kurt B.", ""], ["Heroux", "Michael A.", ""], ["Hoemmen", "Mark", ""]]}, {"id": "1206.2016", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Javid Taheri, Reza Moraveji, Albert Y. Zomaya", "title": "Network Load Analysis and Provisioning of MapReduce Applications", "comments": "6 pages-submitted to The Thirteenth International Conference on\n  Parallel and Distributed Computing, Applications and Technologies(PDCAT-12),\n  Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the dependency between configuration parameters and\nnetwork load of fixed-size MapReduce applications in shuffle phase and then\npropose an analytical method to model this dependency. Our approach consists of\nthree key phases: profiling, modeling, and prediction. In the first stage, an\napplication is run several times with different sets of MapReduce configuration\nparameters (here number of mappers and number of reducers) to profile the\nnetwork load of the application in the shuffle phase on a given cluster. Then,\nthe relation between these parameters and the network load is modeled by\nmultivariate linear regression. For evaluation, three applications (WordCount,\nExim Mainlog parsing, and TeraSort) are utilized to evaluate our technique on a\n4-node MapReduce private cluster.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2012 10:39:04 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2012 02:07:41 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Taheri", "Javid", ""], ["Moraveji", "Reza", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1206.2129", "submitter": "Davide Zordan", "authors": "Davide Zordan, Borja Martinez, Ignasi Vilajosana, Michele Rossi", "title": "To Compress or Not To Compress: Processing vs Transmission Tradeoffs for\n  Energy Constrained Sensor Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, lossy compression has been widely applied in the field\nof wireless sensor networks (WSN), where energy efficiency is a crucial concern\ndue to the constrained nature of the transmission devices. Often, the common\nthinking among researchers and implementers is that compression is always a\ngood choice, because the major source of energy consumption in a sensor node\ncomes from the transmission of the data. Lossy compression is deemed a viable\nsolution as the imperfect reconstruction of the signal is often acceptable in\nWSN. In this paper, we thoroughly review a number of lossy compression methods\nfrom the literature, and analyze their performance in terms of compression\nefficiency, computational complexity and energy consumption. We consider two\ndifferent scenarios, namely, wireless and underwater communications, and show\nthat signal compression may or may not help in the reduction of the overall\nenergy consumption, depending on factors such as the compression algorithm, the\nsignal statistics and the hardware characteristics, i.e., micro-controller and\ntransmission technology. The lesson that we have learned, is that signal\ncompression may in fact provide some energy savings. However, its usage should\nbe carefully evaluated, as in quite a few cases processing and transmission\ncosts are of the same order of magnitude, whereas, in some other cases, the\nformer may even dominate the latter. In this paper, we show quantitative\ncomparisons to assess these tradeoffs in the above mentioned scenarios.\nFinally, we provide formulas, obtained through numerical fittings, to gauge\ncomputational complexity, overall energy consumption and signal representation\naccuracy for the best performing algorithms as a function of the most relevant\nsystem parameters.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2012 08:41:47 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Zordan", "Davide", ""], ["Martinez", "Borja", ""], ["Vilajosana", "Ignasi", ""], ["Rossi", "Michele", ""]]}, {"id": "1206.3738", "submitter": "Georg Hager", "authors": "Jan Treibig, Georg Hager, Gerhard Wellein", "title": "Best practices for HPM-assisted performance engineering on modern\n  multicore processors", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": "10.1007/978-3-642-36949-0_50", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tools and libraries employ hardware performance monitoring (HPM) on\nmodern processors, and using this data for performance assessment and as a\nstarting point for code optimizations is very popular. However, such data is\nonly useful if it is interpreted with care, and if the right metrics are chosen\nfor the right purpose. We demonstrate the sensible use of hardware performance\ncounters in the context of a structured performance engineering approach for\napplications in computational science. Typical performance patterns and their\nrespective metric signatures are defined, and some of them are illustrated\nusing case studies. Although these generic concepts do not depend on specific\ntools or environments, we restrict ourselves to modern x86-based multicore\nprocessors and use the likwid-perfctr tool under the Linux OS.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2012 09:53:53 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Treibig", "Jan", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1206.3768", "submitter": "Edoardo Di Napoli", "authors": "Edoardo Di Napoli (1) and Mario Berljafa (2) ((1) JSC,\n  Forschungszentrum Juelich) ((2) Dept. of Mathematics, Univ. of Zagreb)", "title": "Block Iterative Eigensolvers for Sequences of Correlated Eigenvalue\n  Problems", "comments": "12 Pages, 5 figures. Accepted for publication on Computer Physics\n  Communications", "journal-ref": null, "doi": "10.1016/j.cpc.2013.06.017", "report-no": "AICES-2012/12-1", "categories": "cs.DS cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Density Functional Theory simulations based on the LAPW method, each\nself-consistent field cycle comprises dozens of large dense generalized\neigenproblems. In contrast to real-space methods, eigenpairs solving for\nproblems at distinct cycles have either been believed to be independent or at\nmost very loosely connected. In a recent study [7], it was demonstrated that,\ncontrary to belief, successive eigenproblems in a sequence are strongly\ncorrelated with one another. In particular, by monitoring the subspace angles\nbetween eigenvectors of successive eigenproblems, it was shown that these\nangles decrease noticeably after the first few iterations and become close to\ncollinear. This last result suggests that we can manipulate the eigenvectors,\nsolving for a specific eigenproblem in a sequence, as an approximate solution\nfor the following eigenproblem. In this work we present results that are in\nline with this intuition. We provide numerical examples where opportunely\nselected block iterative eigensolvers benefit from the reuse of eigenvectors by\nachieving a substantial speed-up. The results presented will eventually open\nthe way to a widespread use of block iterative eigensolvers in ab initio\nelectronic structure codes based on the LAPW approach.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2012 17:03:24 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2012 16:49:51 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2013 12:24:16 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Di Napoli", "Edoardo", "", "Dept. of Mathematics, Univ. of Zagreb"], ["Berljafa", "Mario", "", "Dept. of Mathematics, Univ. of Zagreb"]]}, {"id": "1206.6808", "submitter": "Yanfu Li", "authors": "Yan-Fu Li (SSEC, LGI), Enrico Zio (SSEC, LGI)", "title": "A Multi-State Power Model for Adequacy Assessment of Distributed\n  Generation via Universal Generating Function", "comments": "Reliability Engineering & System Safety (2012) 1-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current and future developments of electric power systems are pushing the\nboundaries of reliability assessment to consider distribution networks with\nrenewable generators. Given the stochastic features of these elements, most\nmodeling approaches rely on Monte Carlo simulation. The computational costs\nassociated to the simulation approach force to treating mostly small-sized\nsystems, i.e. with a limited number of lumped components of a given renewable\ntechnology (e.g. wind or solar, etc.) whose behavior is described by a binary\nstate, working or failed. In this paper, we propose an analytical multi-state\nmodeling approach for the reliability assessment of distributed generation\n(DG). The approach allows looking to a number of diverse energy generation\ntechnologies distributed on the system. Multiple states are used to describe\nthe randomness in the generation units, due to the stochastic nature of the\ngeneration sources and of the mechanical degradation/failure behavior of the\ngeneration systems. The universal generating function (UGF) technique is used\nfor the individual component multi-state modeling. A multiplication-type\ncomposition operator is introduced to combine the UGFs for the mechanical\ndegradation and renewable generation source states into the UGF of the\nrenewable generator power output. The overall multi-state DG system UGF is then\nconstructed and classical reliability indices (e.g. loss of load expectation\n(LOLE), expected energy not supplied (EENS)) are computed from the DG system\ngeneration and load UGFs. An application of the model is shown on a DG system\nadapted from the IEEE 34 nodes distribution test feeder.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2012 19:50:53 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Li", "Yan-Fu", "", "SSEC, LGI"], ["Zio", "Enrico", "", "SSEC, LGI"]]}]