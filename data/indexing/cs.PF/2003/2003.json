[{"id": "2003.00532", "submitter": "Uday Bondhugula", "authors": "Uday Bondhugula", "title": "High Performance Code Generation in MLIR: An Early Case Study with GEMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article is primarily meant to present an early case study on using MLIR,\na new compiler intermediate representation infrastructure, for high-performance\ncode generation. Aspects of MLIR covered in particular include memrefs, the\naffine dialect, and polyhedral utilities and pass infrastructure surrounding\nthose. This article is also aimed at showing the role compiler infrastructure\ncould play in generating code that is competitive with highly tuned manually\ndeveloped libraries, albeit in a more modular, reusable, and automatable way.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 17:44:08 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Bondhugula", "Uday", ""]]}, {"id": "2003.00584", "submitter": "David Daly", "authors": "David Daly, William Brown, Henrik Ingo, Jim O'Leary, David Bradford", "title": "Change Point Detection in Software Performance Testing", "comments": null, "journal-ref": null, "doi": "10.1145/3358960.3375791", "report-no": null, "categories": "cs.SE cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe our process for automatic detection of performance changes for a\nsoftware product in the presence of noise. A large collection of tests run\nperiodically as changes to our software product are committed to our source\nrepository, and we would like to identify the commits responsible for\nperformance regressions. Previously, we relied on manual inspection of time\nseries graphs to identify significant changes. That was later replaced with a\nthreshold-based detection system, but neither system was sufficient for finding\nchanges in performance in a timely manner. This work describes our recent\nimplementation of a change point detection system built upon the E-Divisive\nmeans algorithm. The algorithm produces a list of change points representing\nsignificant changes from a given history of performance results. A human\nreviews the list of change points for actionable changes, which are then\ntriaged for further inspection. Using change point detection has had a dramatic\nimpact on our ability to detect performance changes. Quantitatively, it has\ndramatically dropped our false positive rate for performance changes, while\nqualitatively it has made the entire performance evaluation process easier,\nmore productive (ex. catching smaller regressions), and more timely.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 21:01:25 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Daly", "David", ""], ["Brown", "William", ""], ["Ingo", "Henrik", ""], ["O'Leary", "Jim", ""], ["Bradford", "David", ""]]}, {"id": "2003.00706", "submitter": "Puneet Kohli", "authors": "Puneet Kohli, Saravana Gunaseelan, Jason Orozco, Yiwen Hua, Edward Li,\n  and Nicolas Dahlquist", "title": "GPU-Accelerated Mobile Multi-view Style Transfer", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An estimated 60% of smartphones sold in 2018 were equipped with multiple rear\ncameras, enabling a wide variety of 3D-enabled applications such as 3D Photos.\nThe success of 3D Photo platforms (Facebook 3D Photo, Holopix, etc) depend on a\nsteady influx of user generated content. These platforms must provide simple\nimage manipulation tools to facilitate content creation, akin to traditional\nphoto platforms. Artistic neural style transfer, propelled by recent\nadvancements in GPU technology, is one such tool for enhancing traditional\nphotos. However, naively extrapolating single-view neural style transfer to the\nmulti-view scenario produces visually inconsistent results and is prohibitively\nslow on mobile devices. We present a GPU-accelerated multi-view style transfer\npipeline which enforces style consistency between views with on-demand\nperformance on mobile platforms. Our pipeline is modular and creates high\nquality depth and parallax effects from a stereoscopic image pair.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 08:20:47 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Kohli", "Puneet", ""], ["Gunaseelan", "Saravana", ""], ["Orozco", "Jason", ""], ["Hua", "Yiwen", ""], ["Li", "Edward", ""], ["Dahlquist", "Nicolas", ""]]}, {"id": "2003.00822", "submitter": "Maximilian Lam", "authors": "Maximilian Lam, Zachary Yedidia, Colby Banbury, Vijay Janapa Reddi", "title": "Quantized Neural Network Inference with Precision Batching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PrecisionBatching, a quantized inference algorithm for speeding up\nneural network execution on traditional hardware platforms at low bitwidths\nwithout the need for retraining or recalibration. PrecisionBatching decomposes\na neural network into individual bitlayers and accumulates them using fast\n1-bit operations while maintaining activations in full precision.\nPrecisionBatching not only facilitates quantized inference at low bitwidths (<\n8 bits) without the need for retraining/recalibration, but also 1) enables\ntraditional hardware platforms the ability to realize inference speedups at a\nfiner granularity of quantization (e.g: 1-16 bit execution) and 2) allows\naccuracy and speedup tradeoffs at runtime by exposing the number of bitlayers\nto accumulate as a tunable parameter. Across a variety of applications (MNIST,\nlanguage modeling, natural language inference) and neural network architectures\n(fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of\nover 8x on a GPU within a < 1% error margin of the full precision baseline,\noutperforming traditional 8-bit quantized inference by over 1.5x-2x at the same\nerror tolerance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 19:34:11 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lam", "Maximilian", ""], ["Yedidia", "Zachary", ""], ["Banbury", "Colby", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "2003.00869", "submitter": "Fateme Sarkohaki", "authors": "Fatemeh Sarkohaki, Reza Fotohi, Vahab Ashrafian", "title": "An Efficient Routing Protocol in Mobile Ad-hoc Networks by using\n  Artificial Immune System", "comments": "8 pages, 10 figures, journal", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA), 8(4), 554-561 (2017)", "doi": "10.14569/IJACSA.2017.080473", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characteristics of the mobile ad-hoc networks such as nodes high mobility and\nlimited energy are regarded as the routing challenges in these networks. OLSR\nprotocol is one of the routing protocols in mobile ad hoc network that selects\nthe shortest route between source and destination through Dijkstra's algorithm.\nHowever, OLSR suffers from a major problem. It does not consider parameters\nsuch as nodes energy level and links length in its route processing. This paper\nemploys the artificial immune system (AIS) to enhance efficiency of OLSR\nrouting protocol. The proposed algorithm, called AIS-OLSR, considers hop count,\nremaining energy in the intermediate nodes, and distance among node, which is\nrealized by negative selection and ClonalG algorithms of AIS. Widespread packet\n- level simulation in ns-2 environment, shows that AIS-OLSR outperforms OLSR\nand EA-OLSR in terms of packet delivery ratio, throughput, end-end delay and\nlifetime.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 15:56:08 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Sarkohaki", "Fatemeh", ""], ["Fotohi", "Reza", ""], ["Ashrafian", "Vahab", ""]]}, {"id": "2003.00870", "submitter": "Shahram Behzad", "authors": "Shahram Behzad, Reza Fotohi, Jaber Hosseini Balov, Mohammad Javad\n  Rabipour", "title": "An Artificial Immune Based Approach for Detection and Isolation\n  Misbehavior Attacks in Wireless Networks", "comments": "19 pages, 12 figures, Journal", "journal-ref": "JCP, 13(6), 705-720 (2018)", "doi": "10.17706/jcp.13.6.705-720", "report-no": null, "categories": "cs.NI cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MANETs (Mobile Ad-hoc Networks) is a temporal network, which is managed by\nautonomous nodes, which have the ability to communicate with each other without\nhaving fixed network infrastructure or any central base station. Due to some\nreasons such as dynamic changes of the network topology, trusting the nodes to\neach other, lack of fixed substructure for the analysis of nodes behaviors and\nloss of specific offensive lines, this type of networks is not supportive\nagainst malicious nodes attacks. One of these attacks is black hole attack. In\nthis attack, the malicious nodes absorb data packets and destroy them. Thus, it\nis essential to present an algorithm against the black hole attacks. This paper\nproposed a new approach, which improvement the security of DSR routing protocol\nto encounter the black hole attacks. This schema tries to identify malicious\nnodes according to nodes behaviors in a MANETs and isolate them from routing.\nThe proposed protocol, called AIS-DSR (Artificial Immune System DSR) employ AIS\n(Artificial Immune System) to defend against black hole attacks. AIS-DSR is\nevaluated through extensive simulations in the ns-2 environment. The results\nshow that AIS-DSR outperforms other existing solutions in terms of throughput,\nend-to-end delay, packets loss ratio and packets drop ratio.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 16:27:19 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Behzad", "Shahram", ""], ["Fotohi", "Reza", ""], ["Balov", "Jaber Hosseini", ""], ["Rabipour", "Mohammad Javad", ""]]}, {"id": "2003.01886", "submitter": "Dhanoop Karunakaran", "authors": "Dhanoop Karunakaran, Stewart Worrall, Eduardo Nebot", "title": "Efficient statistical validation with edge cases to evaluate Highly\n  Automated Vehicles", "comments": "8 pages and submitted to IEEE ITSC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widescale deployment of Autonomous Vehicles (AV) seems to be imminent\ndespite many safety challenges that are yet to be resolved. It is well known\nthat there are no universally agreed Verification and Validation (VV)\nmethodologies to guarantee absolute safety, which is crucial for the acceptance\nof this technology. Existing standards focus on deterministic processes where\nthe validation requires only a set of test cases that cover the requirements.\nModern autonomous vehicles will undoubtedly include machine learning and\nprobabilistic techniques that require a much more comprehensive testing regime\ndue to the non-deterministic nature of the operating design domain. A rigourous\nstatistical validation process is an essential component required to address\nthis challenge. Most research in this area focuses on evaluating system\nperformance in large scale real-world data gathering exercises (number of miles\ntravelled), or randomised test scenarios in simulation.\n  This paper presents a new approach to compute the statistical characteristics\nof a system's behaviour by biasing automatically generated test cases towards\nthe worst case scenarios, identifying potential unsafe edge cases.We use\nreinforcement learning (RL) to learn the behaviours of simulated actors that\ncause unsafe behaviour measured by the well established RSS safety metric. We\ndemonstrate that by using the method we can more efficiently validate a system\nusing a smaller number of test cases by focusing the simulation towards the\nworst case scenario, generating edge cases that correspond to unsafe\nsituations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 04:35:22 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Karunakaran", "Dhanoop", ""], ["Worrall", "Stewart", ""], ["Nebot", "Eduardo", ""]]}, {"id": "2003.02017", "submitter": "Onel Luis Alcaraz L\\'opez", "authors": "Onel L. Alcaraz L\\'opez, Nurul Huda Mahmood, Hirley Alves", "title": "Enabling URLLC for Low-Cost IoT Devices via Diversity Combining Schemes", "comments": "Accepted at ICC'20 Workshop - ULMC6GN, 6 pags, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supporting Ultra-Reliable Low-Latency Communication (URLLC) in the Internet\nof Things (IoT) era is challenging due to stringent constraints on latency and\nreliability combined with the simple circuitry of IoT nodes. Diversity is\nusually required for sustaining the reliability levels of URLLC, but there is\nan additional delay associated to auxiliary procedures to be considered,\nspecially when communication includes low-cost IoT devices. Herein, we analyze\nSelection Combining (SC) and Switch and Stay Combining (SSC) diversity schemes\nas plausible solutions for enabling ultra-reliable low-latency downlink\ncommunications to low-cost IoT devices. We demonstrate the necessity of\nconsidering the time spent in auxiliary procedures, which has not been\ntraditionally taken into account, while we show its impact on the reliability\nperformance. We show there is an optimum number of receive antennas, which\nsuggests that under certain conditions it might be required to turn off some of\nthem, specially under the SC operation. We highlight the superiority of SSC\nwith respect to SC as long the associated Signal-to-Noise Ratio threshold is\nproperly selected. We propose using a fixed threshold relying only on long-term\nchannel fading statistics, which leads to near-optimum results.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 11:51:58 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["L\u00f3pez", "Onel L. Alcaraz", ""], ["Mahmood", "Nurul Huda", ""], ["Alves", "Hirley", ""]]}, {"id": "2003.02874", "submitter": "Zhijing Li", "authors": "Zhijing Li, Christopher De Sa, Adrian Sampson", "title": "Optimizing JPEG Quantization for Classification Networks", "comments": "6 pages, 13 figures, Resource-Constrained Machine Learning (ReCoML)\n  Workshop of MLSys 2020 Conference, Austin, TX, USA, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.PF eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for computer vision depends on lossy image compression: it\nreduces the storage required for training and test data and lowers transfer\ncosts in deployment. Mainstream datasets and imaging pipelines all rely on\nstandard JPEG compression. In JPEG, the degree of quantization of frequency\ncoefficients controls the lossiness: an 8 by 8 quantization table (Q-table)\ndecides both the quality of the encoded image and the compression ratio. While\na long history of work has sought better Q-tables, existing work either seeks\nto minimize image distortion or to optimize for models of the human visual\nsystem. This work asks whether JPEG Q-tables exist that are \"better\" for\nspecific vision networks and can offer better quality--size trade-offs than\nones designed for human perception or minimal distortion. We reconstruct an\nImageNet test set with higher resolution to explore the effect of JPEG\ncompression under novel Q-tables. We attempt several approaches to tune a\nQ-table for a vision task. We find that a simple sorted random sampling method\ncan exceed the performance of the standard JPEG Q-table. We also use\nhyper-parameter tuning techniques including bounded random search, Bayesian\noptimization, and composite heuristic optimization methods. The new Q-tables we\nobtained can improve the compression rate by 10% to 200% when the accuracy is\nfixed, or improve accuracy up to $2\\%$ at the same compression rate.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 19:13:06 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Li", "Zhijing", ""], ["De Sa", "Christopher", ""], ["Sampson", "Adrian", ""]]}, {"id": "2003.03221", "submitter": "Dominik Scholz", "authors": "Dominik Scholz, Sebastian Gallenm\\\"uller, Henning Stubbe, Bassam\n  Jaber, Minoo Rouhi, Georg Carle", "title": "Me Love (SYN-)Cookies: SYN Flood Mitigation in Programmable Data Planes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SYN flood attack is a common attack strategy on the Internet, which tries\nto overload services with requests leading to a Denial-of-Service (DoS). Highly\nasymmetric costs for connection setup - putting the main burden on the attackee\n- make SYN flooding an efficient and popular DoS attack strategy. Abusing the\nwidely used TCP as an attack vector complicates the detection of malicious\ntraffic and its prevention utilizing naive connection blocking strategies.\nModern programmable data plane devices are capable of handling traffic in the\n10 Gbit/s range without overloading. We discuss how we can harness their\nperformance to defend entire networks against SYN flood attacks. Therefore, we\nanalyze different defense strategies, SYN authentication and SYN cookie, and\ndiscuss implementation difficulties when ported to different target data\nplanes: software, network processors, and FPGAs. We provide prototype\nimplementations and performance figures for all three platforms. Further, we\nfully disclose the artifacts leading to the experiments described in this work.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 14:02:34 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Scholz", "Dominik", ""], ["Gallenm\u00fcller", "Sebastian", ""], ["Stubbe", "Henning", ""], ["Jaber", "Bassam", ""], ["Rouhi", "Minoo", ""], ["Carle", "Georg", ""]]}, {"id": "2003.03283", "submitter": "David Goz Dr.", "authors": "David Goz, Georgios Ieronymakis, Vassilis Papaefstathiou, Nikolaos\n  Dimou, Sara Bertocco, Francesco Simula, Antonio Ragagnin, Luca Tornatore,\n  Igor Coretti, and Giuliano Taffoni", "title": "Performance and energy footprint assessment of FPGAs and GPUs on HPC\n  systems using Astrophysics application", "comments": "15 pages, 4 figures, 3 tables; Preprint (V2) submitted to MDPI\n  (Special Issue: Energy-Efficient Computing on Parallel Architectures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New challenges in Astronomy and Astrophysics (AA) are urging the need for a\nlarge number of exceptionally computationally intensive simulations. \"Exascale\"\n(and beyond) computational facilities are mandatory to address the size of\ntheoretical problems and data coming from the new generation of observational\nfacilities in AA. Currently, the High Performance Computing (HPC) sector is\nundergoing a profound phase of innovation, in which the primary challenge to\nthe achievement of the \"Exascale\" is the power-consumption. The goal of this\nwork is to give some insights about performance and energy footprint of\ncontemporary architectures for a real astrophysical application in an HPC\ncontext. We use a state-of-the-art N-body application that we re-engineered and\noptimized to exploit the heterogeneous underlying hardware fully. We\nquantitatively evaluate the impact of computation on energy consumption when\nrunning on four different platforms. Two of them represent the current HPC\nsystems (Intel-based and equipped with NVIDIA GPUs), one is a micro-cluster\nbased on ARM-MPSoC, and one is a \"prototype towards Exascale\" equipped with\nARM-MPSoCs tightly coupled with FPGAs. We investigate the behavior of the\ndifferent devices where the high-end GPUs excel in terms of time-to-solution\nwhile MPSoC-FPGA systems outperform GPUs in power consumption. Our experience\nreveals that considering FPGAs for computationally intensive application seems\nvery promising, as their performance is improving to meet the requirements of\nscientific applications. This work can be a reference for future platforms\ndevelopment for astrophysics applications where computationally intensive\ncalculations are required.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 15:37:41 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 09:56:52 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Goz", "David", ""], ["Ieronymakis", "Georgios", ""], ["Papaefstathiou", "Vassilis", ""], ["Dimou", "Nikolaos", ""], ["Bertocco", "Sara", ""], ["Simula", "Francesco", ""], ["Ragagnin", "Antonio", ""], ["Tornatore", "Luca", ""], ["Coretti", "Igor", ""], ["Taffoni", "Giuliano", ""]]}, {"id": "2003.03794", "submitter": "Philip Heinisch", "authors": "Philip Heinisch, Katharina Ostaszewski, Hendrik Ranocha", "title": "Towards Green Computing: A Survey of Performance and Energy Efficiency\n  of Different Platforms using OpenCL", "comments": null, "journal-ref": null, "doi": "10.1145/3388333.3403035", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering different hardware platforms, not just the time-to-solution\ncan be of importance but also the energy necessary to reach it. This is not\nonly the case with battery powered and mobile devices but also with\nhigh-performance parallel cluster systems due to financial and practical limits\non power consumption and cooling. Recent developments in hard- and software\nhave given programmers the ability to run the same code on a range of different\ndevices giving rise to the concept of heterogeneous computing. Many of these\ndevices are optimized for certain types of applications. To showcase the\ndifferences and give a basic outlook on the applicability of different\narchitectures for specific problems, the cross-platform OpenCL framework was\nused to compare both time- and energy-to-solution. A large set of devices\nranging from ARM processors to server CPUs and consumer and enterprise level\nGPUs has been used with different benchmarking testcases taken from applied\nresearch applications. While the results show the overall advantages of GPUs in\nterms of both runtime and energy efficiency compared to CPUs, ARM devices show\npotential for certain applications in massively parallel systems. This study\nalso highlights how OpenCL enables the use of the same codebase on many\ndifferent systems and hardware platforms without specific code adaptations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 15:06:27 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Heinisch", "Philip", ""], ["Ostaszewski", "Katharina", ""], ["Ranocha", "Hendrik", ""]]}, {"id": "2003.04228", "submitter": "Piotr Padlewski", "authors": "Piotr Padlewski, Krzysztof Pszeniczny and Richard Smith", "title": "Modeling the Invariance of Virtual Pointers in LLVM", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Devirtualization is a compiler optimization that replaces indirect (virtual)\nfunction calls with direct calls. It is particularly effective in\nobject-oriented languages, such as Java or C++, in which virtual methods are\ntypically abundant.\n  We present a novel abstract model to express the lifetimes of C++ dynamic\nobjects and invariance of virtual table pointers in the LLVM intermediate\nrepresentation. The model and the corresponding implementation in Clang and\nLLVM enable full devirtualization of virtual calls whenever the dynamic type is\nstatically known and elimination of redundant virtual table loads in other\ncases.\n  Due to the complexity of C++, this has not been achieved by any other C++\ncompiler so far. Although our model was designed for C++, it is also applicable\nto other languages that use virtual dispatch. Our benchmarks show an average of\n0.8% performance improvement on real-world C++ programs, with more than 30%\nspeedup in some cases. The implementation is already a part of the upstream\nLLVM/Clang and can be enabled with the -fstrict-vtable-pointers flag.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 12:38:14 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Padlewski", "Piotr", ""], ["Pszeniczny", "Krzysztof", ""], ["Smith", "Richard", ""]]}, {"id": "2003.04294", "submitter": "Zheng Wang", "authors": "Peng Zhang, Jianbin Fang, Canqun Yang, Chun Huang, Tao Tang, Zheng\n  Wang", "title": "Optimizing Streaming Parallelism on Heterogeneous Many-Core\n  Architectures: A Machine Learning Based Approach", "comments": "Accepted to be published at IEEE TPDS. arXiv admin note: substantial\n  text overlap with arXiv:1802.02760", "journal-ref": null, "doi": "10.1109/TPDS.2020.2978045", "report-no": null, "categories": "cs.DC cs.LG cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article presents an automatic approach to quickly derive a good solution\nfor hardware resource partition and task granularity for task-based parallel\napplications on heterogeneous many-core architectures. Our approach employs a\nperformance model to estimate the resulting performance of the target\napplication under a given resource partition and task granularity\nconfiguration. The model is used as a utility to quickly search for a good\nconfiguration at runtime. Instead of hand-crafting an analytical model that\nrequires expert insights into low-level hardware details, we employ machine\nlearning techniques to automatically learn it. We achieve this by first\nlearning a predictive model offline using training programs. The learnt model\ncan then be used to predict the performance of any unseen program at runtime.\nWe apply our approach to 39 representative parallel applications and evaluate\nit on two representative heterogeneous many-core platforms: a CPU-XeonPhi\nplatform and a CPU-GPU platform. Compared to the single-stream version, our\napproach achieves, on average, a 1.6x and 1.1x speedup on the XeonPhi and the\nGPU platform, respectively. These results translate to over 93% of the\nperformance delivered by a theoretically perfect predictor.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:18:21 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhang", "Peng", ""], ["Fang", "Jianbin", ""], ["Yang", "Canqun", ""], ["Huang", "Chun", ""], ["Tang", "Tao", ""], ["Wang", "Zheng", ""]]}, {"id": "2003.04470", "submitter": "Vuong M. Ngo", "authors": "V.M. Ngo, N.A. Le-Khac, and M.T. Kechadi", "title": "Data Warehouse and Decision Support on Integrated Crop Big Data", "comments": "13 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:1905.12411", "journal-ref": "International Journal of Business Process Integration and\n  Management 2020 Vol.10 No.1", "doi": "10.1504/IJBPIM.2020.113115", "report-no": null, "categories": "cs.DB cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, precision agriculture is becoming very popular. The\nintroduction of modern information and communication technologies for\ncollecting and processing Agricultural data revolutionise the agriculture\npractises. This has started a while ago (early 20th century) and it is driven\nby the low cost of collecting data about everything; from information on fields\nsuch as seed, soil, fertiliser, pest, to weather data, drones and satellites\nimages. Specially, the agricultural data mining today is considered as Big Data\napplication in terms of volume, variety, velocity and veracity. Hence it leads\nto challenges in processing vast amounts of complex and diverse information to\nextract useful knowledge for the farmer, agronomist, and other businesses. It\nis a key foundation to establishing a crop intelligence platform, which will\nenable efficient resource management and high quality agronomy decision making\nand recommendations. In this paper, we designed and implemented a continental\nlevel agricultural data warehouse (ADW). ADW is characterised by its (1)\nflexible schema; (2) data integration from real agricultural multi datasets;\n(3) data science and business intelligent support; (4) high performance; (5)\nhigh storage; (6) security; (7) governance and monitoring; (8) consistency,\navailability and partition tolerant; (9) cloud compatibility. We also evaluate\nthe performance of ADW and present some complex queries to extract and return\nnecessary knowledge about crop management.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 00:10:22 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:45:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ngo", "V. M.", ""], ["Le-Khac", "N. A.", ""], ["Kechadi", "M. T.", ""]]}, {"id": "2003.04570", "submitter": "Ois\\'in Creaner", "authors": "Ois\\'in Creaner, John Walsh, Kevin Nolan and Eugene Hickey", "title": "The Locus Algorithm IV: Performance metrics of a grid computing system\n  used to create catalogues of optimised pointings", "comments": "6 Pages, 1 Figure, arxiv references updated with correct links", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the requirements for and performance metrics of the the\nGrid Computing system used to implement the Locus Algorithm to identify optimum\npointings for differential photometry of 61,662,376 stars and 23,779 quasars.\nInitial operational tests indicated a need for a software system to analyse the\ndata and a High Performance Computing system to run that software in a scalable\nmanner. Practical assessments of the performance of the software in a serial\ncomputing environment were used to provide a benchmark against which the\nperformance metrics of the HPC solution could be compared, as well as to\nindicate any bottlenecks in performance. These performance metrics indicated a\ndistinct split in the performance dictated more by differences in the input\ndata than by differences in the design of the systems used. This indicates a\nneed for experimental analysis of system performance, and suggests that\nalgorithmic complexity analyses may lead to incorrect or naive conclusions,\nespecially in systems with high data I/O overhead such as grid computing.\nFurther, it implies that systems which reduce or eliminate this bottleneck such\nas in-memory processing could lead to a substantial increase in performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 08:26:54 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 08:58:33 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Creaner", "Ois\u00edn", ""], ["Walsh", "John", ""], ["Nolan", "Kevin", ""], ["Hickey", "Eugene", ""]]}, {"id": "2003.04821", "submitter": "Colby Banbury", "authors": "Colby R. Banbury, Vijay Janapa Reddi, Max Lam, William Fu, Amin Fazel,\n  Jeremy Holleman, Xinyuan Huang, Robert Hurtado, David Kanter, Anton\n  Lokhmotov, David Patterson, Danilo Pau, Jae-sun Seo, Jeff Sieracki, Urmish\n  Thakker, Marian Verhelst, Poonam Yadav", "title": "Benchmarking TinyML Systems: Challenges and Direction", "comments": "6 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in ultra-low-power machine learning (TinyML) hardware\npromises to unlock an entirely new class of smart applications. However,\ncontinued progress is limited by the lack of a widely accepted benchmark for\nthese systems. Benchmarking allows us to measure and thereby systematically\ncompare, evaluate, and improve the performance of systems and is therefore\nfundamental to a field reaching maturity. In this position paper, we present\nthe current landscape of TinyML and discuss the challenges and direction\ntowards developing a fair and useful hardware benchmark for TinyML workloads.\nFurthermore, we present our four benchmarks and discuss our selection\nmethodology. Our viewpoints reflect the collective thoughts of the TinyMLPerf\nworking group that is comprised of over 30 organizations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 15:58:12 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 15:42:53 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 21:57:32 GMT"}, {"version": "v4", "created": "Fri, 29 Jan 2021 21:21:37 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Banbury", "Colby R.", ""], ["Reddi", "Vijay Janapa", ""], ["Lam", "Max", ""], ["Fu", "William", ""], ["Fazel", "Amin", ""], ["Holleman", "Jeremy", ""], ["Huang", "Xinyuan", ""], ["Hurtado", "Robert", ""], ["Kanter", "David", ""], ["Lokhmotov", "Anton", ""], ["Patterson", "David", ""], ["Pau", "Danilo", ""], ["Seo", "Jae-sun", ""], ["Sieracki", "Jeff", ""], ["Thakker", "Urmish", ""], ["Verhelst", "Marian", ""], ["Yadav", "Poonam", ""]]}, {"id": "2003.04824", "submitter": "Dmitry Duplyakin", "authors": "Dmitry Duplyakin and Alexandru Uta and Aleksander Maricq and Robert\n  Ricci", "title": "In Datacenter Performance, The Only Constant Is Change", "comments": "To be presented at the 20th IEEE/ACM International Symposium on\n  Cluster, Cloud and Internet Computing (CCGrid,\n  http://cloudbus.org/ccgrid2020/) on May 11-14, 2020 in Melbourne, Victoria,\n  Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All computing infrastructure suffers from performance variability, be it\nbare-metal or virtualized. This phenomenon originates from many sources: some\ntransient, such as noisy neighbors, and others more permanent but sudden, such\nas changes or wear in hardware, changes in the underlying hypervisor stack, or\neven undocumented interactions between the policies of the computing resource\nprovider and the active workloads. Thus, performance measurements obtained on\nclouds, HPC facilities, and, more generally, datacenter environments are almost\nguaranteed to exhibit performance regimes that evolve over time, which leads to\nundesirable nonstationarities in application performance. In this paper, we\npresent our analysis of performance of the bare-metal hardware available on the\nCloudLab testbed where we focus on quantifying the evolving performance regimes\nusing changepoint detection. We describe our findings, backed by a dataset with\nnearly 6.9M benchmark results collected from over 1600 machines over a period\nof 2 years and 9 months. These findings yield a comprehensive characterization\nof real-world performance variability patterns in one computing facility, a\nmethodology for studying such patterns on other infrastructures, and contribute\nto a better understanding of performance variability in general.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 16:04:33 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Duplyakin", "Dmitry", ""], ["Uta", "Alexandru", ""], ["Maricq", "Aleksander", ""], ["Ricci", "Robert", ""]]}, {"id": "2003.04984", "submitter": "Reza Fotohi", "authors": "Reza Fotohi", "title": "Securing of Unmanned Aerial Systems (UAS) against security threats using\n  human immune system", "comments": "29 pages, 12 figures, 10 tables, 8 equations, Journal", "journal-ref": "Reliability Engineering & System Safety, 193, 106675 (2020)", "doi": "10.1016/j.ress.2019.106675", "report-no": null, "categories": "cs.CR cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UASs form a large part of the fighting ability of the advanced military\nforces. In particular, these systems that carry confidential information are\nsubject to security attacks. Accordingly, an Intrusion Detection System (IDS)\nhas been proposed in the proposed design to protect against the security\nproblems using the human immune system (HIS). The IDSs are used to detect and\nrespond to attempts to compromise the target system. Since the UASs operate in\nthe real world, the testing and validation of these systems with a variety of\nsensors is confronted with problems. This design is inspired by HIS. In the\nmapping, insecure signals are equivalent to an antigen that are detected by\nantibody-based training patterns and removed from the operation cycle. Among\nthe main uses of the proposed design are the quick detection of intrusive\nsignals and quarantining their activity. Moreover, SUAS-HIS method is evaluated\nhere via extensive simulations carried out in NS-3 environment. The simulation\nresults indicate that the UAS network performance metrics are improved in terms\nof false positive rate, false negative rate, detection rate, and packet\ndelivery rate.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 19:05:16 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Fotohi", "Reza", ""]]}, {"id": "2003.05113", "submitter": "Senthil Nathan", "authors": "Parth Thakkar, Senthilnathan Natarajan", "title": "Scaling Hyperledger Fabric Using Pipelined Execution and Sparse Peers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned blockchains are becoming popular as data management systems in\nthe enterprise setting. Compared to traditional distributed databases,\nblockchain platforms provide increased security guarantees but significantly\nlower performance. Further, these platforms are quite expensive to run for the\nlow throughput they provide. The following are two ways to improve performance\nand reduce cost: (1) make the system utilize allocated resources efficiently;\n(2) allow rapid and dynamic scaling of allocated resources based on load. We\nexplore both of these in this work.\n  We first investigate the reasons for the poor performance and scalability of\nthe dominant permissioned blockchain flavor called Execute-Order-Validate\n(EOV). We do this by studying the scaling characteristics of Hyperledger\nFabric, a popular EOV platform, using vertical scaling and horizontal scaling.\nWe find that the transaction throughput scales very poorly with these\ntechniques. At least in the permissioned setting, the real bottleneck is\ntransaction processing, not the consensus protocol. With vertical scaling, the\nallocated vCPUs go under-utilized. In contrast, with horizontal scaling, the\nallocated resources get wasted due to redundant work across nodes within an\norganization.\n  To mitigate the above concerns, we first improve resource efficiency by (a)\nimproving CPU utilization with a pipelined execution of validation & commit\nphases; (b) avoiding redundant work across nodes by introducing a new type of\npeer node called sparse peer that selectively commits transactions. We further\npropose a technique that enables the rapid scaling of resources. Our\nimplementation - SmartFabric, built on top of Hyperledger Fabric demonstrates\n3x higher throughput, 12-26x faster scale-up time, and provides Fabric's\nthroughput at 50% to 87% lower cost.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 05:02:12 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 06:36:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Thakkar", "Parth", ""], ["Natarajan", "Senthilnathan", ""]]}, {"id": "2003.05135", "submitter": "Bo Jiang", "authors": "Bo Jiang, Philippe Nain, Don Towsley", "title": "Covert Cycle Stealing in a Single FIFO Server", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a setting where Willie generates a Poisson stream of jobs and routes\nthem to a single server that follows the first-in first-out discipline. Suppose\nthere is an adversary Alice, who desires to receive service without being\ndetected. We ask the question: what is the number of jobs that she can receive\ncovertly, i.e. without being detected by Willie? In the case where both Willie\nand Alice jobs have exponential service times with respective rates $\\mu_1$ and\n$\\mu_2$, we demonstrate a phase-transition when Alice adopts the strategy of\ninserting a single job probabilistically when the server idles : over $n$ busy\nperiods, she can achieve a covert throughput, measured by the expected number\nof jobs covertly inserted, of $\\mathcal{O}(\\sqrt{n})$ when $\\mu_1 < 2\\mu_2$,\n$\\mathcal{O}(\\sqrt{n/\\log n})$ when $\\mu_1 = 2\\mu_2$, and\n$\\mathcal{O}(n^{\\mu_2/\\mu_1})$ when $\\mu_1 > 2\\mu_2$. When both Willie and\nAlice jobs have general service times we establish an upper bound for the\nnumber of jobs Alice can execute covertly. This bound is related to the Fisher\ninformation. More general insertion policies are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 06:34:12 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 02:18:56 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 12:54:10 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Jiang", "Bo", ""], ["Nain", "Philippe", ""], ["Towsley", "Don", ""]]}, {"id": "2003.05208", "submitter": "Mohammad Ashraful Hoque", "authors": "Mohammad A. Hoque, Ashwin Rao, Sasu Tarkoma", "title": "In Situ Network and Application Performance Measurement on Android\n  Devices and the Imperfections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding network and application performance are essential for\ndebugging, improving user experience, and performance comparison. Meanwhile,\nmodern mobile systems are optimized for energy-efficient computation and\ncommunications that may limit the performance of network and applications. In\nrecent years, several tools have emerged that analyze network performance of\nmobile applications in~situ with the help of the VPN service. There is a\nlimited understanding of how these measurement tools and system optimizations\naffect the network and application performance. In this study, we first\ndemonstrate that mobile systems employ energy-aware system hardware tuning,\nwhich affects application performance and network throughput. We next show that\nthe VPN-based application performance measurement tools, such as Lumen,\nPrivacyGuard, and Video Optimizer, aid in ambiguous network performance\nmeasurements and degrade the application performance. Our findings suggest that\nsound application and network performance measurement on Android devices\nrequires a good understanding of the device, networks, measurement tools, and\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 10:42:40 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Hoque", "Mohammad A.", ""], ["Rao", "Ashwin", ""], ["Tarkoma", "Sasu", ""]]}, {"id": "2003.06452", "submitter": "Guenter Hesse", "authors": "Guenter Hesse, Christoph Matthies, Matthias Uflacker", "title": "How Fast Can We Insert? An Empirical Performance Evaluation of Apache\n  Kafka", "comments": "IEEE International Conference on Parallel and Distributed Systems\n  (ICPADS) 2020", "journal-ref": null, "doi": "10.1109/ICPADS51040.2020.00089", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Message brokers see widespread adoption in modern IT landscapes, with Apache\nKafka being one of the most employed platforms. These systems feature\nwell-defined APIs for use and configuration and present flexible solutions for\nvarious data storage scenarios. Their ability to scale horizontally enables\nusers to adapt to growing data volumes and changing environments. However, one\nof the main challenges concerning message brokers is the danger of them\nbecoming a bottleneck within an IT architecture. To prevent this, knowledge\nabout the amount of data a message broker using a specific configuration can\nhandle needs to be available. In this paper, we propose a monitoring\narchitecture for message brokers and similar Java Virtual Machine-based\nsystems. We present a comprehensive performance analysis of the popular Apache\nKafka platform using our approach. As part of the benchmark, we study selected\ndata ingestion scenarios with respect to their maximum data ingestion rates.\nThe results show that we can achieve an ingestion rate of about 420,000\nmessages/second on the used commodity hardware and with the developed data\nsender tool.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 19:19:45 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 14:15:36 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 14:06:36 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hesse", "Guenter", ""], ["Matthies", "Christoph", ""], ["Uflacker", "Matthias", ""]]}, {"id": "2003.06477", "submitter": "Moritz Steiner", "authors": "Utkarsh Goel, Stephen Ludin, Moritz Steiner", "title": "Web Performance with Android's Battery-Saver Mode", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A Web browser utilizes a device's CPU to parse HTML, build a Document Object\nModel, a Cascading Style Sheets Object Model, and render trees, and parse,\ncompile, and execute computationally-heavy JavaScript. A powerful CPU is\nrequired to perform these tasks as quickly as possible and provide the user\nwith a great experience. However, increased CPU performance comes with\nincreased power consumption and reduced battery life on mobile devices. As an\noption to extend battery life, Android offers a battery-saver mode that when\nactivated, turns off the power-hungry and faster processor cores and turns on\nthe battery-conserving and slower processor cores on the device. The transition\nfrom using faster processor cores to using slower processor cores throttles the\nCPU clock speed on the device, and therefore impacts the webpage load process.\nWe utilize a large-scale data-set collected by a real user monitoring system of\na major content delivery network to investigate the impact of Android's\nbattery-saver mode on various mobile Web performance metrics. Our analysis\nsuggests that users of select smartphones of Huawei and Sony experience a\nsudden or gradual degradation in Web performance when battery-saver mode is\nactive. Battery-saver mode on newer flagship smartphones, however, does not\nimpact the mobile Web performance. Finally, we encourage for new website design\ngoals that treat slow (and throttled-CPU) devices kindly in favor of improving\nend-user experience and suggest that Web performance measurements should be\naware of user device battery charge levels to correctly associate Web\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 20:43:38 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Goel", "Utkarsh", ""], ["Ludin", "Stephen", ""], ["Steiner", "Moritz", ""]]}, {"id": "2003.06700", "submitter": "Xipeng Shen", "authors": "Shaoshan Liu, Bin Ren, Xipeng Shen, Yanzhi Wang", "title": "CoCoPIE: Making Mobile AI Sweet As PIE --Compression-Compilation\n  Co-Design Goes a Long Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming hardware is the major constraint for enabling real-time mobile\nintelligence, the industry has mainly dedicated their efforts to developing\nspecialized hardware accelerators for machine learning and inference. This\narticle challenges the assumption. By drawing on a recent real-time AI\noptimization framework CoCoPIE, it maintains that with effective\ncompression-compiler co-design, it is possible to enable real-time artificial\nintelligence on mainstream end devices without special hardware. CoCoPIE is a\nsoftware framework that holds numerous records on mobile AI: the first\nframework that supports all main kinds of DNNs, from CNNs to RNNs, transformer,\nlanguage models, and so on; the fastest DNN pruning and acceleration framework,\nup to 180X faster compared with current DNN pruning on other frameworks such as\nTensorFlow-Lite; making many representative AI applications able to run in\nreal-time on off-the-shelf mobile devices that have been previously regarded\npossible only with special hardware support; making off-the-shelf mobile\ndevices outperform a number of representative ASIC and FPGA solutions in terms\nof energy efficiency and/or performance.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 20:53:05 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 10:45:20 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 21:05:24 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Liu", "Shaoshan", ""], ["Ren", "Bin", ""], ["Shen", "Xipeng", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2003.06795", "submitter": "John Lawson", "authors": "John Lawson", "title": "Towards automated kernel selection in machine learning systems: A SYCL\n  case study", "comments": "4 pages, 4 figures, 1 table. Accepted to AsHES workshop at IPDPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated tuning of compute kernels is a popular area of research, mainly\nfocused on finding optimal kernel parameters for a problem with fixed input\nsizes. This approach is good for deploying machine learning models, where the\nnetwork topology is constant, but machine learning research often involves\nchanging network topologies and hyperparameters. Traditional kernel auto-tuning\nhas limited impact in this case; a more general selection of kernels is\nrequired for libraries to accelerate machine learning research.\n  In this paper we present initial results using machine learning to select\nkernels in a case study deploying high performance SYCL kernels in libraries\nthat target a range of heterogeneous devices from desktop GPUs to embedded\naccelerators. The techniques investigated apply more generally and could\nsimilarly be integrated with other heterogeneous programming systems. By\ncombining auto-tuning and machine learning these kernel selection processes can\nbe deployed with little developer effort to achieve high performance on new\nhardware.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 11:23:36 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lawson", "John", ""]]}, {"id": "2003.07182", "submitter": "Vincent Huang", "authors": "Bradley Butcher, Vincent S. Huang, Jeremy Reffin, Sema K. Sgaier,\n  Grace Charles, Novi Quadrianto", "title": "Causal datasheet: An approximate guide to practically assess Bayesian\n  networks in the real world", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In solving real-world problems like changing healthcare-seeking behaviors,\ndesigning interventions to improve downstream outcomes requires an\nunderstanding of the causal links within the system. Causal Bayesian Networks\n(BN) have been proposed as one such powerful method. In real-world\napplications, however, confidence in the results of BNs are often moderate at\nbest. This is due in part to the inability to validate against some ground\ntruth, as the DAG is not available. This is especially problematic if the\nlearned DAG conflicts with pre-existing domain doctrine. At the policy level,\none must justify insights generated by such analysis, preferably accompanying\nthem with uncertainty estimation. Here we propose a causal extension to the\ndatasheet concept proposed by Gebru et al (2018) to include approximate BN\nperformance expectations for any given dataset. To generate the results for a\nprototype Causal Datasheet, we constructed over 30,000 synthetic datasets with\nproperties mirroring characteristics of real data. We then recorded the results\ngiven by state-of-the-art structure learning algorithms. These results were\nused to populate the Causal Datasheet, and recommendations were automatically\ngenerated dependent on expected performance. As a proof of concept, we used our\nCausal Datasheet Generation Tool (CDG-T) to assign expected performance\nexpectations to a maternal health survey we conducted in Uttar Pradesh, India.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 23:31:11 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Butcher", "Bradley", ""], ["Huang", "Vincent S.", ""], ["Reffin", "Jeremy", ""], ["Sgaier", "Sema K.", ""], ["Charles", "Grace", ""], ["Quadrianto", "Novi", ""]]}, {"id": "2003.07336", "submitter": "Carole-Jean Wu", "authors": "Carole-Jean Wu and Robin Burke and Ed H. Chi and Joseph Konstan and\n  Julian McAuley and Yves Raimond and Hao Zhang", "title": "Developing a Recommendation Benchmark for MLPerf Training and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based recommendation models are used pervasively and broadly,\nfor example, to recommend movies, products, or other information most relevant\nto users, in order to enhance the user experience. Among various application\ndomains which have received significant industry and academia research\nattention, such as image classification, object detection, language and speech\ntranslation, the performance of deep learning-based recommendation models is\nless well explored, even though recommendation tasks unarguably represent\nsignificant AI inference cycles at large-scale datacenter fleets. To advance\nthe state of understanding and enable machine learning system development and\noptimization for the commerce domain, we aim to define an industry-relevant\nrecommendation benchmark for the MLPerf Training andInference Suites. The paper\nsynthesizes the desirable modeling strategies for personalized recommendation\nsystems. We lay out desirable characteristics of recommendation model\narchitectures and data sets. We then summarize the discussions and advice from\nthe MLPerf Recommendation Advisory Board.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:13:00 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 12:52:16 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Wu", "Carole-Jean", ""], ["Burke", "Robin", ""], ["Chi", "Ed H.", ""], ["Konstan", "Joseph", ""], ["McAuley", "Julian", ""], ["Raimond", "Yves", ""], ["Zhang", "Hao", ""]]}, {"id": "2003.07497", "submitter": "Naifeng Zhang", "authors": "Ajitesh Srivastava (1), Naifeng Zhang (1), Rajgopal Kannan (2), Viktor\n  K. Prasanna (1) ((1) University of Southern California, (2) US Army Research\n  Lab-West)", "title": "Towards High Performance, Portability, and Productivity: Lightweight\n  Augmented Neural Networks for Performance Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing high-performance code requires significant expertise in the\nprogramming language, compiler optimizations, and hardware knowledge. This\noften leads to poor productivity and portability and is inconvenient for a\nnon-programmer domain-specialist such as a Physicist. More desirable is a\nhigh-level language where the domain-specialist simply specifies the workload\nin terms of high-level operations (e.g., matrix-multiply(A, B)), and the\ncompiler identifies the best implementation fully utilizing the heterogeneous\nplatform. For creating a compiler that supports productivity, portability, and\nperformance simultaneously, it is crucial to predict the performance of various\navailable implementations (variants) of the dominant operations (kernels)\ncontained in the workload on various hardware to decide (a) which variant\nshould be chosen for each kernel in the workload, and (b) on which hardware\nresource the variant should run. To enable the performance prediction, we\npropose lightweight augmented neural networks for arbitrary combinations of\nkernel-variant-hardware. A key innovation is utilizing the mathematical\ncomplexity of the kernels as a feature to achieve higher accuracy. These models\nare compact to reduce training time and fast inference during compile-time and\nrun-time. Using models with less than 75 parameters, and only 250 training data\ninstances, we are able to obtain a low MAPE of 3%, significantly outperforming\ntraditional feed-forward neural networks on 48 kernel-variant-hardware\ncombinations. We further demonstrate that our variant-selection approach can be\nused in Halide implementations to obtain up to 1.7x speedup over Halide's\nauto-scheduler.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:19:54 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 08:30:24 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Srivastava", "Ajitesh", ""], ["Zhang", "Naifeng", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor K.", ""]]}, {"id": "2003.08011", "submitter": "Guang Chao Wang", "authors": "Guang Chao Wang, Kenny Gross, and Akshay Subramaniam", "title": "ContainerStress: Autonomous Cloud-Node Scoping Framework for Big-Data ML\n  Use Cases", "comments": "To be published in 6th Annual Conf. on Computational Science &\n  Computational Intelligence (CSCI'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying big-data Machine Learning (ML) services in a cloud environment\npresents a challenge to the cloud vendor with respect to the cloud container\nconfiguration sizing for any given customer use case. OracleLabs has developed\nan automated framework that uses nested-loop Monte Carlo simulation to\nautonomously scale any size customer ML use cases across the range of cloud\nCPU-GPU \"Shapes\" (configurations of CPUs and/or GPUs in Cloud containers\navailable to end customers). Moreover, the OracleLabs and NVIDIA authors have\ncollaborated on a ML benchmark study which analyzes the compute cost and GPU\nacceleration of any ML prognostic algorithm and assesses the reduction of\ncompute cost in a cloud container comprising conventional CPUs and NVIDIA GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 01:51:42 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Wang", "Guang Chao", ""], ["Gross", "Kenny", ""], ["Subramaniam", "Akshay", ""]]}, {"id": "2003.08477", "submitter": "Bernd Meyer", "authors": "Tobias Kl\\\"offel, Gerald Mathias, Bernd Meyer", "title": "Integrating State of the Art Compute, Communication, and Autotuning\n  Strategies to Multiply the Performance of the Application Programm CPMD for\n  Ab Initio Molecular Dynamics Simulations", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.PF physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our recent code modernizations of the of the ab initio molecular\ndynamics program CPMD (www.cpmd.org) with a special focus on the ultra-soft\npseudopotential (USPP) code path. Following the internal instrumentation of\nCPMD, all time critical routines have been revised to maximize the\ncomputational throughput and to minimize the communication overhead for optimal\nperformance. Throughout the program missing hybrid MPI+OpenMP parallelization\nhas been added to optimize scaling. For communication intensive routines, as\nthe multiple distributed 3d FFTs of the electronic states and distributed\nmatrix-matrix multiplications related to the $\\beta$-projectors of the\npseudopotentials, this MPI+OpenMP parallelization now overlaps computation and\ncommunication. The necessary partitioning of the workload is optimized by an\nauto-tuning algorithm. In addition, the largest global MPI_Allreduce operation\nhas been replaced by highly tuned node-local parallelized operations using MPI\nshared-memory windows to avoid inter-node communication. A batched algorithm\nfor the multiple 3d FFTs improves the throughput of the MPI_Alltoall\ncommunication and, thus, the scalability of the implementation, both for USPP\nand for the frequently used norm-conserving pseudopotential code path. The\nenhanced performance and scalability is demonstrated on a mid-sized benchmark\nsystem of 256 water molecules and further water systems of from 32 up to 2048\nmolecules.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 21:21:38 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Kl\u00f6ffel", "Tobias", ""], ["Mathias", "Gerald", ""], ["Meyer", "Bernd", ""]]}, {"id": "2003.09269", "submitter": "Jeremy Kepner", "authors": "Siddharth Samsi, Jeremy Kepner, Vijay Gadepally, Michael Hurley,\n  Michael Jones, Edward Kao, Sanjeev Mohindra, Albert Reuther, Steven Smith,\n  William Song, Diane Staheli, Paul Monticciolo", "title": "GraphChallenge.org Triangle Counting Performance", "comments": "10 pages, 8 figures, 121 references, to be submitted to IEEE HPEC\n  2020. This work reports new updated results on prior work reported in\n  arXiv:1805.09675 & arXiv:1708.06866", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286166", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of graph analytic systems has created a need for new ways to measure\nand compare the capabilities of graph processing systems. The MIT/Amazon/IEEE\nGraph Challenge has been developed to provide a well-defined community venue\nfor stimulating research and highlighting innovations in graph analysis\nsoftware, hardware, algorithms, and systems. GraphChallenge.org provides a wide\nrange of pre-parsed graph data sets, graph generators, mathematically defined\ngraph algorithms, example serial implementations in a variety of languages, and\nspecific metrics for measuring performance. The triangle counting component of\nGraphChallenge.org tests the performance of graph processing systems to count\nall the triangles in a graph and exercises key graph operations found in many\ngraph algorithms. In 2017, 2018, and 2019 many triangle counting submissions\nwere received from a wide range of authors and organizations. This paper\npresents a performance analysis of the best performers of these submissions.\nThese submissions show that their state-of-the-art triangle counting execution\ntime, $T_{\\rm tri}$, is a strong function of the number of edges in the graph,\n$N_e$, which improved significantly from 2017 ($T_{\\rm tri} \\approx\n(N_e/10^8)^{4/3}$) to 2018 ($T_{\\rm tri} \\approx N_e/10^9$) and remained\ncomparable from 2018 to 2019. Graph Challenge provides a clear picture of\ncurrent graph analysis systems and underscores the need for new innovations to\nachieve high performance on very large graphs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 20:36:29 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Samsi", "Siddharth", ""], ["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Hurley", "Michael", ""], ["Jones", "Michael", ""], ["Kao", "Edward", ""], ["Mohindra", "Sanjeev", ""], ["Reuther", "Albert", ""], ["Smith", "Steven", ""], ["Song", "William", ""], ["Staheli", "Diane", ""], ["Monticciolo", "Paul", ""]]}, {"id": "2003.10064", "submitter": "Pingcheng Ruan", "authors": "Pingcheng Ruan, Dumitrel Loghin, Quang-Trung Ta, Meihui Zhang, Gang\n  Chen, Beng Chin Ooi", "title": "A Transactional Perspective on Execute-order-validate Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts have enabled blockchain systems to evolve from simple\ncryptocurrency platforms, such as Bitcoin, to general transactional systems,\nsuch as Ethereum. Catering for emerging business requirements, a new\narchitecture called execute-order-validate has been proposed in Hyperledger\nFabric to support parallel transactions and improve the blockchain's\nthroughput. However, this new architecture might render many invalid\ntransactions when serializing them. This problem is further exaggerated as the\nblock formation rate is inherently limited due to other factors beside data\nprocessing, such as cryptography and consensus.\n  In this work, we propose a novel method to enhance the execute-order-validate\narchitecture, by reducing invalid transactions to improve the throughput of\nblockchains. Our method is inspired by state-of-the-art optimistic concurrency\ncontrol techniques in modern database systems. In contrast to existing\nblockchains that adopt database's preventive approaches which might abort\nserializable transactions, our method is theoretically more fine-grained.\nSpecifically, unserializable transactions are aborted before ordering and the\nremaining transactions are guaranteed to be serializable. For evaluation, we\nimplement our method in two blockchains respectively, FabricSharp on top of\nHyperledger Fabric, and FastFabricSharp on top of FastFabric. We compare the\nperformance of FabricSharp with vanilla Fabric and three related systems, two\nof which are respectively implemented with one standard and one\nstate-of-the-art concurrency control techniques from databases. The results\ndemonstrate that FabricSharp achieves 25% higher throughput compared to the\nother systems in nearly all experimental scenarios. Moreover, the\nFastFabricSharp's improvement over FastFabric is up to 66%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 03:30:45 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ruan", "Pingcheng", ""], ["Loghin", "Dumitrel", ""], ["Ta", "Quang-Trung", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2003.10482", "submitter": "Feliks Hibraj", "authors": "Feliks Hibraj, Marcello Pelillo, Saverio Salzo, Massimiliano Pontil", "title": "Efficient Tensor Kernel methods for sparse regression", "comments": "M.Sc. Thesis introducing a novel layout to efficiently store\n  symmetric tensor data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, classical kernel methods have been extended by the introduction of\nsuitable tensor kernels so to promote sparsity in the solution of the\nunderlying regression problem. Indeed, they solve an lp-norm regularization\nproblem, with p=m/(m-1) and m even integer, which happens to be close to a\nlasso problem. However, a major drawback of the method is that storing tensors\nrequires a considerable amount of memory, ultimately limiting its\napplicability. In this work we address this problem by proposing two advances.\nFirst, we directly reduce the memory requirement, by intriducing a new and more\nefficient layout for storing the data. Second, we use a Nystrom-type\nsubsampling approach, which allows for a training phase with a smaller number\nof data points, so to reduce the computational cost. Experiments, both on\nsynthetic and read datasets, show the effectiveness of the proposed\nimprovements. Finally, we take case of implementing the cose in C++ so to\nfurther speed-up the computation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 18:26:56 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Hibraj", "Feliks", ""], ["Pelillo", "Marcello", ""], ["Salzo", "Saverio", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "2003.10536", "submitter": "Chris Cummins", "authors": "Chris Cummins, Zacharias V. Fisches, Tal Ben-Nun, Torsten Hoefler,\n  Hugh Leather", "title": "ProGraML: Graph-based Deep Learning for Program Optimization and\n  Analysis", "comments": "20 pages, author preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of computing systems places a tremendous burden on\noptimizing compilers, requiring ever more accurate and aggressive\noptimizations. Machine learning offers significant benefits for constructing\noptimization heuristics but there remains a gap between what state-of-the-art\nmethods achieve and the performance of an optimal heuristic. Closing this gap\nrequires improvements in two key areas: a representation that accurately\ncaptures the semantics of programs, and a model architecture with sufficient\nexpressiveness to reason about this representation.\n  We introduce ProGraML - Program Graphs for Machine Learning - a novel\ngraph-based program representation using a low level, language agnostic, and\nportable format; and machine learning models capable of performing complex\ndownstream tasks over these graphs. The ProGraML representation is a directed\nattributed multigraph that captures control, data, and call relations, and\nsummarizes instruction and operand types and ordering. Message Passing Neural\nNetworks propagate information through this structured representation, enabling\nwhole-program or per-vertex classification tasks.\n  ProGraML provides a general-purpose program representation that equips\nlearnable models to perform the types of program analysis that are fundamental\nto optimization. To this end, we evaluate the performance of our approach first\non a suite of traditional compiler analysis tasks: control flow reachability,\ndominator trees, data dependencies, variable liveness, and common subexpression\ndetection. On a benchmark dataset of 250k LLVM-IR files covering six source\nprogramming languages, ProGraML achieves an average 94.0 F1 score,\nsignificantly outperforming the state-of-the-art approaches. We then apply our\napproach to two high-level tasks - heterogeneous device mapping and program\nclassification - setting new state-of-the-art performance in both.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 20:27:00 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Cummins", "Chris", ""], ["Fisches", "Zacharias V.", ""], ["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""], ["Leather", "Hugh", ""]]}, {"id": "2003.10579", "submitter": "Sanghamitra Dutta", "authors": "Sanghamitra Dutta, Jianyu Wang, Gauri Joshi", "title": "Slow and Stale Gradients Can Win the Race", "comments": "Some of the results have appeared in AISTATS 2018. This is an\n  extended version with additional results, in particular, an adaptive\n  synchronicity strategy called AdaSync. arXiv admin note: substantial text\n  overlap with arXiv:1803.01113", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Stochastic Gradient Descent (SGD) when run in a synchronous\nmanner, suffers from delays in runtime as it waits for the slowest workers\n(stragglers). Asynchronous methods can alleviate stragglers, but cause gradient\nstaleness that can adversely affect the convergence error. In this work, we\npresent a novel theoretical characterization of the speedup offered by\nasynchronous methods by analyzing the trade-off between the error in the\ntrained model and the actual training runtime(wallclock time). The main novelty\nin our work is that our runtime analysis considers random straggling delays,\nwhich helps us design and compare distributed SGD algorithms that strike a\nbalance between straggling and staleness. We also provide a new error\nconvergence analysis of asynchronous SGD variants without bounded or\nexponential delay assumptions. Finally, based on our theoretical\ncharacterization of the error-runtime trade-off, we propose a method of\ngradually varying synchronicity in distributed SGD and demonstrate its\nperformance on CIFAR10 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 23:27:50 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Dutta", "Sanghamitra", ""], ["Wang", "Jianyu", ""], ["Joshi", "Gauri", ""]]}, {"id": "2003.10745", "submitter": "Kashyap Thimmaraju", "authors": "Kashyap Thimmaraju and Stefan Schmid", "title": "Towards Fine-Grained Billing For Cloud Networking", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit multi-tenant network virtualization in data centers, and make the\ncase for tenant-specific virtual switches. In particular, tenant-specific\nvirtual switches allow cloud providers to extend fine-grained billing (known,\ne.g., from serverless architectures) to the network, accounting not only for\nIO, but also CPU or energy. We sketch an architecture and present economical\nmotivation and recent technological enablers. We also find that virtual\nswitches today do not offer sufficient multi-tenancy and can introduce\nartificial performance bottlenecks, e.g., in load balancers. We conclude by\ndiscussing additional use cases for tentant-specific switches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 10:06:06 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Thimmaraju", "Kashyap", ""], ["Schmid", "Stefan", ""]]}, {"id": "2003.10850", "submitter": "Antonio Ragagnin", "authors": "Antonio Ragagnin, Klaus Dolag, Mathias Wagner, Claudio Gheller,\n  Conradin Roffler, David Goz, David Hubber, Alexander Arth", "title": "Gadget3 on GPUs with OpenACC", "comments": "10 pages, 4 figures, printed by ParCo 2019 (at IOS Advances in\n  Parallel Computing, Volume 36, pages 209 - 218, ISBN 978-1-64368-070-5)", "journal-ref": null, "doi": "10.3233/APC200043", "report-no": null, "categories": "astro-ph.IM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present preliminary results of a GPU porting of all main Gadget3 modules\n(gravity computation, SPH density computation, SPH hydrodynamic force, and\nthermal conduction) using OpenACC directives. Here we assign one GPU to each\nMPI rank and exploit both the host and accellerator capabilities by overlapping\ncomputations on the CPUs and GPUs: while GPUs asynchronously compute\ninteractions between particles within their MPI ranks, CPUs perform tree-walks\nand MPI communications of neighbouring particles. We profile various portions\nof the code to understand the origin of our speedup, where we find that a peak\nspeedup is not achieved because of time-steps with few active particles. We run\na hydrodynamic cosmological simulation from the Magneticum project, with\n$2\\cdot10^{7}$ particles, where we find a final total speedup of $\\approx 2.$\nWe also present the results of an encouraging scaling test of a preliminary\ngravity-only OpenACC porting, run in the context of the EuroHack17 event, where\nthe prototype of the porting proved to keep a constant speedup up to $1024$\nGPUs.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 13:50:58 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Ragagnin", "Antonio", ""], ["Dolag", "Klaus", ""], ["Wagner", "Mathias", ""], ["Gheller", "Claudio", ""], ["Roffler", "Conradin", ""], ["Goz", "David", ""], ["Hubber", "David", ""], ["Arth", "Alexander", ""]]}, {"id": "2003.11174", "submitter": "Wei You", "authors": "Ward Whitt and Wei You", "title": "A Robust Queueing Network Analyzer Based on Indices of Dispersion", "comments": "Appendix available at\n  https://cnyouwei.github.io/papers/Whitt_You_RQNA_app.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a robust queueing network analyzer algorithm to approximate the\nsteady-state performance of a single-class open queueing network of\nsingle-server queues with Markovian routing. The algorithm allows non-renewal\nexternal arrival processes, general service-time distributions and customer\nfeedback. We focus on the customer flows, defined as the continuous-time\nprocesses counting customers flowing into or out of the network, or flowing\nfrom one queue to another. Each flow is partially characterized by its rate and\na continuous function that measures the stochastic variability over time. This\nfunction is a scaled version of the variance-time curve, called the index of\ndispersion for counts (IDC). The required IDC functions for the flows can be\ncalculated from the model primitives, estimated from data or approximated by\nsolving a set of linear equations. A robust queueing technique is used to\ngenerate approximations of the mean steady-state performance at each queue from\nthe IDC of the total arrival flow and the service specification at that queue.\nThe algorithm effectiveness is supported by extensive simulation studies and\nheavy-traffic limits.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 01:36:16 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Whitt", "Ward", ""], ["You", "Wei", ""]]}, {"id": "2003.11332", "submitter": "Dieter Weber", "authors": "Dieter Weber, Alexander Clausen, Rafal E. Dunin-Borkowski", "title": "Next-Generation Information Technology Systems for Fast Detectors in\n  Electron Microscop", "comments": null, "journal-ref": "Handbook on Big Data and Machine Learning in the Physical\n  Sciences, World Scientific, 2020, 83-120", "doi": "10.1142/9789811204579_0005", "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci cs.PF physics.ins-det", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Gatan K2 IS direct electron detector (Gatan Inc., 2018), which was\nintroduced in 2014, marked a watershed moment in the development of cameras for\ntransmission electron microscopy (TEM) (Pan & Czarnik, 2016). Its pixel\nfrequency, i.e. the number of data points (pixels) recorded per second, was two\norders of magnitude higher than the fastest cameras available only five years\nbefore. Starting from 2009, the data rate of TEM cameras has outpaced the\ndevelopment of network, mass storage and memory bandwidth by almost two orders\nof magnitude. Consequently, solutions based on personal computers (PCs) that\nwere adequate until then are no longer able to handle the resulting data rates.\nInstead, tailored high-performance setups are necessary. Similar developments\nhave occurred for advanced X-ray sources such as the European XFEL, requiring\nspecial information technology (IT) systems for data handling (Sauter, Hattne,\nGrosse-Kunstleve, & Echols, 2013) (Fangohr, et al., 2018). Information and\ndetector technology are currently under rapid development and involve\ndisruptive technological innovations. This chapter briefly reviews the\ntechnological developments of the past 20 years, presents a snapshot of the\ncurrent situation at the beginning of 2019 with many practical considerations,\nand looks forward to future developments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 11:24:42 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Weber", "Dieter", ""], ["Clausen", "Alexander", ""], ["Dunin-Borkowski", "Rafal E.", ""]]}, {"id": "2003.13232", "submitter": "Ziv Scully", "authors": "Ziv Scully, Isaac Grosof, Mor Harchol-Balter", "title": "Optimal Multiserver Scheduling with Unknown Job Sizes in Heavy Traffic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider scheduling to minimize mean response time of the M/G/k queue with\nunknown job sizes. In the single-server case, the optimal policy is the Gittins\npolicy, but it is not known whether Gittins or any other policy is optimal in\nthe multiserver case. Exactly analyzing the M/G/k under any scheduling policy\nis intractable, and Gittins is a particularly complicated policy that is hard\nto analyze even in the single-server case.\n  In this work we introduce monotonic Gittins (M-Gittins), a new variation of\nthe Gittins policy, and show that it minimizes mean response time in the\nheavy-traffic M/G/k for a wide class of finite-variance job size distributions.\nWe also show that the monotonic shortest expected remaining processing time\n(M-SERPT) policy, which is simpler than M-Gittins, is a 2-approximation for\nmean response time in the heavy traffic M/G/k under similar conditions. These\nresults constitute the most general optimality results to date for the M/G/k\nwith unknown job sizes. Our techniques build upon work by Grosof et al., who\nstudy simple policies, such as SRPT, in the M/G/k; Bansal et al., Kamphorst and\nZwart, and Lin et al., who analyze mean response time scaling of simple\npolicies in the heavy-traffic M/G/1; and Aalto et al. and Scully et al., who\ncharacterize and analyze the Gittins policy in the M/G/1.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 05:50:23 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 18:49:23 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 18:05:35 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Scully", "Ziv", ""], ["Grosof", "Isaac", ""], ["Harchol-Balter", "Mor", ""]]}, {"id": "2003.14085", "submitter": "Abhishek  Sinha", "authors": "Rajarshi Bhattacharjee, Subhankar Banerjee, Abhishek Sinha", "title": "Fundamental Limits of Online Network-Caching", "comments": "To appear in Sigmetrics 2020, Boston, MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal caching of files in a content distribution network (CDN) is a problem\nof fundamental and growing commercial interest. Although many different caching\nalgorithms are in use today, the fundamental performance limits of network\ncaching algorithms from an online learning point-of-view remain poorly\nunderstood to date. In this paper, we resolve this question in the following\ntwo settings: (1) a single user connected to a single cache, and (2) a set of\nusers and a set of caches interconnected through a bipartite network. Recently,\nan online gradient-based coded caching policy was shown to enjoy sub-linear\nregret. However, due to the lack of known regret lower bounds, the question of\nthe optimality of the proposed policy was left open. In this paper, we settle\nthis question by deriving tight non-asymptotic regret lower bounds in both of\nthe above settings. In addition to that, we propose a new\nFollow-the-Perturbed-Leader-based uncoded caching policy with near-optimal\nregret. Technically, the lower-bounds are obtained by relating the online\ncaching problem to the classic probabilistic paradigm of balls-into-bins. Our\nproofs make extensive use of a new result on the expected load in the most\npopulated half of the bins, which might also be of independent interest. We\nevaluate the performance of the caching policies by experimenting with the\npopular MovieLens dataset and conclude the paper with design recommendations\nand a list of open problems.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 10:46:37 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Bhattacharjee", "Rajarshi", ""], ["Banerjee", "Subhankar", ""], ["Sinha", "Abhishek", ""]]}, {"id": "2003.14087", "submitter": "Seva Shneer", "authors": "Binyamin Oz, Seva Shneer, Ilze Ziedins", "title": "Static vs accumulating priorities in healthcare queues under heavy loads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amid unprecedented times caused by COVID-19, healthcare systems all over the\nworld are strained to the limits of, or even beyond, capacity. A similar event\nis experienced by some healthcare systems regularly, due to for instance\nseasonal spikes in the number of patients. We model this as a queueing system\nin heavy traffic (where the arrival rate is approaching the service rate from\nbelow) or in overload (where the arrival rate exceeds the service rate). In\nboth cases we assume that customers (patients) may have different priorities\nand we consider two popular service disciplines: static priorities and\naccumulating priorities. It has been shown that the latter allows for patients\nof all classes to be seen in a timely manner as long as the system is stable.\nWe demonstrate however that if accumulating priorities are used in the heavy\ntraffic or overload regime, then all patients, including those with the highest\npriority, will experience very long waiting times. If on the other hand static\npriorities are applied, then one can ensure that the highest-priority patients\nwill be seen in a timely manner even in overloaded systems.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 10:49:41 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 08:48:02 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Oz", "Binyamin", ""], ["Shneer", "Seva", ""], ["Ziedins", "Ilze", ""]]}]