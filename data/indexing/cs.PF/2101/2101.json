[{"id": "2101.00257", "submitter": "Bin Li", "authors": "Bin Li", "title": "Efficient Learning-based Scheduling for Information Freshness in\n  Wireless Networks", "comments": "This paper has been accepted by IEEE INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the recent trend of integrating artificial intelligence into the\nInternet-of-Things (IoT), we consider the problem of scheduling packets from\nmultiple sensing sources to a central controller over a wireless network. Here,\npackets from different sensing sources have different values or degrees of\nimportance to the central controller for intelligent decision making. In such a\nsetup, it is critical to provide timely and valuable information for the\ncentral controller. In this paper, we develop a parameterized maximum-weight\ntype scheduling policy that combines both the AoI metrics and Upper Confidence\nBound (UCB) estimates in its weight measure with parameter $\\eta$. Here, UCB\nestimates balance the tradeoff between exploration and exploitation in learning\nand are critical for yielding a small cumulative regret. We show that our\nproposed algorithm yields the running average total age at most by\n$O(N^2\\eta)$. We also prove that our proposed algorithm achieves the cumulative\nregret over time horizon $T$ at most by $O(NT/\\eta+\\sqrt{NT\\log T})$. This\nreveals a tradeoff between the cumulative regret and the running average total\nage: when increasing $\\eta$, the cumulative regret becomes smaller, but is at\nthe cost of increasing running average total age. Simulation results are\nprovided to evaluate the efficiency of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 15:59:59 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Bin", ""]]}, {"id": "2101.00267", "submitter": "Christina Delimitrou", "authors": "Yu Gan, Mingyu Liang, Sundar Dev, David Lo, Christina Delimitrou", "title": "Sage: Using Unsupervised Learning for Scalable Performance Debugging in\n  Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud applications are increasingly shifting from large monolithic services\nto complex graphs of loosely-coupled microservices. Despite the advantages of\nmodularity and elasticity microservices offer, they also complicate cluster\nmanagement and performance debugging, as dependencies between tiers introduce\nbackpressure and cascading QoS violations.\n  We present Sage, a machine learning-driven root cause analysis system for\ninteractive cloud microservices. Sage leverages unsupervised ML models to\ncircumvent the overhead of trace labeling, captures the impact of dependencies\nbetween microservices to determine the root cause of unpredictable performance\nonline, and applies corrective actions to recover a cloud service's QoS. In\nexperiments on both dedicated local clusters and large clusters on Google\nCompute Engine we show that Sage consistently achieves over 93% accuracy in\ncorrectly identifying the root cause of QoS violations, and improves\nperformance predictability.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 16:44:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Gan", "Yu", ""], ["Liang", "Mingyu", ""], ["Dev", "Sundar", ""], ["Lo", "David", ""], ["Delimitrou", "Christina", ""]]}, {"id": "2101.00361", "submitter": "Olga Poppe", "authors": "Olga Poppe, Chuan Lei, Lei Ma, Allison Rozet, Elke A. Rundensteiner", "title": "To Share, or not to Share Online Event Trend Aggregation Over Bursty\n  Event Streams", "comments": "Technical report for the paper in SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex event processing (CEP) systems continuously evaluate large workloads\nof pattern queries under tight time constraints. Event trend aggregation\nqueries with Kleene patterns are commonly used to retrieve summarized insights\nabout the recent trends in event streams. State-of-art methods are limited\neither due to repetitive computations or unnecessary trend construction.\nExisting shared approaches are guided by statically selected and hence rigid\nsharing plans that are often sub-optimal under stream fluctuations. In this\nwork, we propose a novel framework Hamlet that is the first to overcome these\nlimitations. Hamlet introduces two key innovations. First, Hamlet adaptively\ndecides whether to share or not to share computations depending on the current\nstream properties at run time to harvest the maximum sharing benefit. Second,\nHamlet is equipped with a highly efficient shared trend aggregation strategy\nthat avoids trend construction. Our experimental study on both real and\nsynthetic data sets demonstrates that Hamlet consistently reduces query latency\nby up to five orders of magnitude compared to the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 03:21:32 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:18:38 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Poppe", "Olga", ""], ["Lei", "Chuan", ""], ["Ma", "Lei", ""], ["Rozet", "Allison", ""], ["Rundensteiner", "Elke A.", ""]]}, {"id": "2101.00740", "submitter": "Hina Tabassum Prof.", "authors": "Hazem Ibrahim, Hina Tabassum, and Uyen T. Nguyen", "title": "Exact Coverage Analysis of Intelligent Reflecting Surfaces with\n  Nakagami-{m} Channels", "comments": null, "journal-ref": "IEEE Transactions on Vehicular Technology ( Volume: 70, Issue: 1,\n  Jan. 2021)", "doi": "10.1109/TVT.2021.3050046", "report-no": null, "categories": "cs.SI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent Reflecting Surfaces (IRS) are a promising solution to enhance the\ncoverage of future wireless networks by tuning low-cost passive reflecting\nelements (referred to as {metasurfaces}), thereby constructing a favorable\nwireless propagation environment. Different from prior works, which assume\nRayleigh fading channels and do not consider the direct link between a base\nstation and a user, this article develops a framework based on moment\ngeneration functions (MGF) to characterize the coverage probability of a user\nin an IRS-aided wireless systems with generic Nakagami-m fading channels in the\npresence of direct links. In addition, we demonstrate that the proposed\nframework is tractable for both finite and asymptotically large values of the\nmetasurfaces. Furthermore, we derive the channel hardening factor as a function\nof the shape factor of Nakagami-m fading channel and the number of IRS\nelements. Finally, we derive a closed-form expression to calculate the maximum\ncoverage range of the IRS for given network parameters. Numerical results\nobtained from Monte-Carlo simulations validate the derived analytical results.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 02:20:49 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Ibrahim", "Hazem", ""], ["Tabassum", "Hina", ""], ["Nguyen", "Uyen T.", ""]]}, {"id": "2101.01177", "submitter": "Gihan  Ravideva Mudalige", "authors": "Kamalavasan Kamalakkannan, Gihan R. Mudalige, Istvan Z. Reguly, Suhaib\n  A. Fahmy", "title": "High-Level FPGA Accelerator Design for Structured-Mesh-Based Explicit\n  Numerical Solvers", "comments": "Preprint - Accepted to the 35th IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS 2021), May 2021, Portland, Oregon USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a workflow for synthesizing near-optimal FPGA\nimplementations for structured-mesh based stencil applications for explicit\nsolvers. It leverages key characteristics of the application class, its\ncomputation-communication pattern, and the architectural capabilities of the\nFPGA to accelerate solvers from the high-performance computing domain. Key new\nfeatures of the workflow are (1) the unification of standard state-of-the-art\ntechniques with a number of high-gain optimizations such as batching and\nspatial blocking/tiling, motivated by increasing throughput for real-world work\nloads and (2) the development and use of a predictive analytic model for\nexploring the design space, resource estimates and performance. Three\nrepresentative applications are implemented using the design workflow on a\nXilinx Alveo U280 FPGA, demonstrating near-optimal performance and over 85%\npredictive model accuracy. These are compared with equivalent highly-optimized\nimplementations of the same applications on modern HPC-grade GPUs (Nvidia V100)\nanalyzing time to solution, bandwidth and energy consumption. Performance\nresults indicate equivalent runtime performance of the FPGA implementations to\nthe V100 GPU, with over 2x energy savings, for the largest non-trivial\napplication synthesized on the FPGA compared to the best performing GPU-based\nsolution. Our investigation shows the considerable challenges in gaining high\nperformance on current generation FPGAs compared to traditional architectures.\nWe discuss determinants for a given stencil code to be amenable to FPGA\nimplementation, providing insights into the feasibility and profitability of a\ndesign and its resulting performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:27:15 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 09:57:16 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Kamalakkannan", "Kamalavasan", ""], ["Mudalige", "Gihan R.", ""], ["Reguly", "Istvan Z.", ""], ["Fahmy", "Suhaib A.", ""]]}, {"id": "2101.01335", "submitter": "Hoang-Dung Do", "authors": "Hoang-Dung Do, Valerie Hayot-Sasson, Rafael Ferreira da Silva,\n  Christopher Steele, Henri Casanova, Tristan Glatard", "title": "Modeling the Linux page cache for accurate simulation of data-intensive\n  applications", "comments": "10 pages, 8 figures, CCGrid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Big Data in recent years has resulted in a growing need for\nefficient data processing solutions. While infrastructures with sufficient\ncompute power are available, the I/O bottleneck remains. The Linux page cache\nis an efficient approach to reduce I/O overheads, but few experimental studies\nof its interactions with Big Data applications exist, partly due to limitations\nof real-world experiments. Simulation is a popular approach to address these\nissues, however, existing simulation frameworks do not simulate page caching\nfully, or even at all. As a result, simulation-based performance studies of\ndata-intensive applications lead to inaccurate results.\n  In this paper, we propose an I/O simulation model that includes the key\nfeatures of the Linux page cache. We have implemented this model as part of the\nWRENCH workflow simulation framework, which itself builds on the popular\nSimGrid distributed systems simulation framework. Our model and its\nimplementation enable the simulation of both single-threaded and multithreaded\napplications, and of both writeback and writethrough caches for local or\nnetwork-based filesystems. We evaluate the accuracy of our model in different\nconditions, including sequential and concurrent applications, as well as local\nand remote I/Os. We find that our page cache model reduces the simulation error\nby up to an order of magnitude when compared to state-of-the-art, cacheless\nsimulations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 03:36:36 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Do", "Hoang-Dung", ""], ["Hayot-Sasson", "Valerie", ""], ["da Silva", "Rafael Ferreira", ""], ["Steele", "Christopher", ""], ["Casanova", "Henri", ""], ["Glatard", "Tristan", ""]]}, {"id": "2101.02074", "submitter": "Tobias Kronauer", "authors": "Tobias Kronauer, Joshwa Pohlmann, Maximilian Matthe, Till Smejkal,\n  Gerhard Fettweis", "title": "Latency Analysis of ROS2 Multi-Node Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.MA cs.PF cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Robot Operating System 2 (ROS2) targets distributed real-time systems and\nis widely used in the robotics community. Especially in these systems, latency\nin data processing and communication can lead to instabilities. Though being\nhighly configurable with respect to latency, ROS2 is often used with its\ndefault settings.\n  In this paper, we investigate the end-to-end latency of ROS2 for distributed\nsystems with default settings and different Data Distribution Service (DDS)\nmiddlewares. In addition, we profile the ROS2 stack and point out latency\nbottlenecks. Our findings indicate that end-to-end latency strongly depends on\nthe used DDS middleware. Moreover, we show that ROS2 can lead to 50% latency\noverhead compared to using low-level DDS communications. Our results imply\nguidelines for designing distributed ROS2 architectures and indicate\npossibilities for reducing the ROS2 overhead.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 14:50:09 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 09:15:21 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 06:10:04 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Kronauer", "Tobias", ""], ["Pohlmann", "Joshwa", ""], ["Matthe", "Maximilian", ""], ["Smejkal", "Till", ""], ["Fettweis", "Gerhard", ""]]}, {"id": "2101.03342", "submitter": "Andrea Vandin", "authors": "Luca Cardelli, Isabel Cristina Perez-Verona, Mirco Tribastone, Max\n  Tschaikowski, Andrea Vandin, Tabea Waizmann", "title": "Exact maximal reduction of stochastic reaction networks by species\n  lumping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Stochastic reaction networks are a widespread model to describe\nbiological systems where the presence of noise is relevant, such as in cell\nregulatory processes. Unfortu-nately, in all but simplest models the resulting\ndiscrete state-space representation hinders analytical tractability and makes\nnumerical simulations expensive. Reduction methods can lower complexity by\ncomputing model projections that preserve dynamics of interest to the user.\nResults: We present an exact lumping method for stochastic reaction networks\nwith mass-action kinetics. It hinges on an equivalence relation between the\nspecies, resulting in a reduced network where the dynamics of each\nmacro-species is stochastically equivalent to the sum of the original species\nin each equivalence class, for any choice of the initial state of the system.\nFurthermore, by an appropriate encoding of kinetic parameters as additional\nspecies, the method can establish equivalences that do not depend on specific\nvalues of the parameters. The method is supported by an efficient algorithm to\ncompute the largest species equivalence, thus the maximal lumping. The\neffectiveness and scalability of our lumping technique, as well as the physical\ninterpretability of resulting reductions, is demonstrated in several models of\nsignaling pathways and epidemic processes on complex networks. Availability:\nThe algorithms for species equivalence have been implemented in the software\ntool ERODE, freely available for download from https://www.erode.eu.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 12:01:45 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Cardelli", "Luca", ""], ["Perez-Verona", "Isabel Cristina", ""], ["Tribastone", "Mirco", ""], ["Tschaikowski", "Max", ""], ["Vandin", "Andrea", ""], ["Waizmann", "Tabea", ""]]}, {"id": "2101.04485", "submitter": "Yohan Eguillon", "authors": "Yohan Eguillon, Bruno Lacabanne, Damien Tromeur-Dervout", "title": "IFOSMONDI Co-simulation Algorithm with Jacobian-Free Methods in PETSc", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  IFOSMONDI iterative algorithm for implicit co-simulation of coupled physical\nsystems (introduced by the authors in july 2019 during the Simultech\nconference, p.176-186) enables us to solve the nonlinear coupling function\nwhile keeping the smoothness of interfaces without introducing a delay.\nMoreover, it automatically adapts the size of the steps between data exchanges\namong the systems according to the difficulty of the solving of the coupling\nconstraint. The latter was solved by a fixed-point algorithm in the original\nimplementation whereas this paper introduces the JFM version (standing for\nJacobian-Free Methods). Most implementations of Newton-like methods require a\njacobian matrix which can be difficult to compute in the co-simulation context,\nexcept in the case where the interfaces are represented by a Zero-Order-Hold\n(ZOH). As far as IFOSMONDI coupling algorithm uses Hermite interpolation for\nsmoothness enhancement (up to Third-Order-Hold), we propose hereafter a new\nformulation of the non-linear coupling function including both the values and\nthe time-derivatives of the coupling variables. This formulation is well\ndesigned for solving the coupling through jacobian-free Newton type methods.\nConsequently, successive function evaluations consist in multiple simulations\nof the systems on a co-simulation time step using rollback. The\norchestrator-workers structure of the algorithm enables us to combine the PETSc\nframework on the orchestrator side for the non-linear Newton-type solvers with\nthe parallel integrations of the systems on the workers side thanks to MPI\nprocesses. Different nonlinear methods will be compared to one another and to\nthe original fixed-point implementation on a newly proposed 2-systems academic\ntest-case (mass-spring-damper type) with direct feedthrough on both sides.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 09:42:06 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Eguillon", "Yohan", ""], ["Lacabanne", "Bruno", ""], ["Tromeur-Dervout", "Damien", ""]]}, {"id": "2101.04489", "submitter": "Thomas Loruenser", "authors": "Thomas Loruenser, Benjamin Rainer and Florian Wohner", "title": "Towards a Performance Model for Byzantine Fault Tolerant (Storage)\n  Services", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine fault-tolerant systems have been researched for more than four\ndecades, and although shown possible early, the solutions were impractical for\na long time. With PBFT the first practical solution was proposed in 1999 and\nspawned new research which culminated in novel applications using it today.\nAlthough the safety and liveness properties of PBFT-type protocols have been\nrigorously analyzed, when it comes to practical performance only empirical\nresults - often in artificial settings - are known and imperfections on the\ncommunication channels are not specifically considered. In this work we present\nthe first performance model for PBFT specifically considering the impact of\nunreliable channels and the use of different transport protocols over them. We\nalso did extensive simulations to verify the model and to gain more insight on\nthe impact of deployment parameters on the overall transaction time. We show\nthat the usage of UDP can lead to significant speedup for PBFT protocols\ncompared to TCP when tuned accordingly even over lossy channels. Finally, we\ncompared the simulation to a real implementation and measure the benefits of a\ndeveloped improvement directly. We found that the impact on the design of the\nnetwork layer has been overlooked in the past but offers some additional room\nfor improvement when it comes to practical performance. In this work we are\nfocusing on the optimistic case with no node failures, as this is hopefully the\nmost relevant situation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:10:25 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Loruenser", "Thomas", ""], ["Rainer", "Benjamin", ""], ["Wohner", "Florian", ""]]}, {"id": "2101.04575", "submitter": "Georgios Karopoulos", "authors": "Jos\\'e Luis Hern\\'andez-Ramos, Georgios Karopoulos, Dimitris\n  Geneiatakis, Tania Martin, Georgios Kambourakis, and Igor Nai Fovino", "title": "Sharing pandemic vaccination certificates through blockchain: Case study\n  and performance evaluation", "comments": "10 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a scalable, blockchain-based platform for the secure\nsharing of COVID-19 or other disease vaccination certificates. As an indicative\nuse case, we simulate a large-scale deployment by considering the countries of\nthe European Union. The proposed platform is evaluated through extensive\nsimulations in terms of computing resource usage, network response time and\nbandwidth. Based on the results, the proposed scheme shows satisfactory\nperformance across all major evaluation criteria, suggesting that it can set\nthe pace for real implementations. Vis-\\`a-vis the related work, the proposed\nplatform is novel, especially through the prism of a large-scale, full-fledged\nimplementation and its assessment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 16:10:10 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Hern\u00e1ndez-Ramos", "Jos\u00e9 Luis", ""], ["Karopoulos", "Georgios", ""], ["Geneiatakis", "Dimitris", ""], ["Martin", "Tania", ""], ["Kambourakis", "Georgios", ""], ["Fovino", "Igor Nai", ""]]}, {"id": "2101.04627", "submitter": "Majid Raeis", "authors": "Majid Raeis, Ali Tizghadam, Alberto Leon-Garcia", "title": "Queue-Learning: A Reinforcement Learning Approach for Providing Quality\n  of Service", "comments": "8 pages, Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end delay is a critical attribute of quality of service (QoS) in\napplication domains such as cloud computing and computer networks. This metric\nis particularly important in tandem service systems, where the end-to-end\nservice is provided through a chain of services. Service-rate control is a\ncommon mechanism for providing QoS guarantees in service systems. In this\npaper, we introduce a reinforcement learning-based (RL-based) service-rate\ncontroller that provides probabilistic upper-bounds on the end-to-end delay of\nthe system, while preventing the overuse of service resources. In order to have\na general framework, we use queueing theory to model the service systems.\nHowever, we adopt an RL-based approach to avoid the limitations of\nqueueing-theoretic methods. In particular, we use Deep Deterministic Policy\nGradient (DDPG) to learn the service rates (action) as a function of the queue\nlengths (state) in tandem service systems. In contrast to existing RL-based\nmethods that quantify their performance by the achieved overall reward, which\ncould be hard to interpret or even misleading, our proposed controller provides\nexplicit probabilistic guarantees on the end-to-end delay of the system. The\nevaluations are presented for a tandem queueing system with non-exponential\ninter-arrival and service times, the results of which validate our controller's\ncapability in meeting QoS constraints.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:28:57 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Raeis", "Majid", ""], ["Tizghadam", "Ali", ""], ["Leon-Garcia", "Alberto", ""]]}, {"id": "2101.05323", "submitter": "S\\'ebastien Vaucher", "authors": "S\\'ebastien Vaucher, Niloofar Yazdani, Pascal Felber, Daniel E.\n  Lucani, Valerio Schiavoni", "title": "ZipLine: In-Network Compression at Line Speed", "comments": null, "journal-ref": "2020. Proceedings of the 16th International Conference on emerging\n  Networking EXperiments and Technologies. Association for Computing Machinery,\n  New York, NY, USA, 399-405", "doi": "10.1145/3386367.3431302", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network appliances continue to offer novel opportunities to offload\nprocessing from computing nodes directly into the data plane. One popular\nconcern of network operators and their customers is to move data increasingly\nfaster. A common technique to increase data throughput is to compress it before\nits transmission. However, this requires compression of the data -- a time and\nenergy demanding pre-processing phase -- and decompression upon reception -- a\nsimilarly resource consuming operation. Moreover, if multiple nodes transfer\nsimilar data chunks across the network hop (e.g., a given pair of switches),\neach node effectively wastes resources by executing similar steps. This paper\nproposes ZipLine, an approach to design and implement (de)compression at line\nspeed leveraging the Tofino hardware platform which is programmable using the\nP4_16 language. We report on lessons learned while building the system and show\nthroughput, latency and compression measurements on synthetic and real-world\ntraces, showcasing the benefits and trade-offs of our design.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:59:25 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Vaucher", "S\u00e9bastien", ""], ["Yazdani", "Niloofar", ""], ["Felber", "Pascal", ""], ["Lucani", "Daniel E.", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2101.05363", "submitter": "Mehrshad Zandigohar", "authors": "Mehrshad Zandigohar, Deniz Erdogmus, Gunar Schirner", "title": "NetCut: Real-Time DNN Inference Using Layer Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning plays a significant role in assisting humans in many aspects of\ntheir lives. As these networks tend to get deeper over time, they extract more\nfeatures to increase accuracy at the cost of additional inference latency. This\naccuracy-performance trade-off makes it more challenging for Embedded Systems,\nas resource-constrained processors with strict deadlines, to deploy them\nefficiently. This can lead to selection of networks that can prematurely meet a\nspecified deadline with excess slack time that could have potentially\ncontributed to increased accuracy.\n  In this work, we propose: (i) the concept of layer removal as a means of\nconstructing TRimmed Networks (TRNs) that are based on removing\nproblem-specific features of a pretrained network used in transfer learning,\nand (ii) NetCut, a methodology based on an empirical or an analytical latency\nestimator, which only proposes and retrains TRNs that can meet the\napplication's deadline, hence reducing the exploration time significantly.\n  We demonstrate that TRNs can expand the Pareto frontier that trades off\nlatency and accuracy to provide networks that can meet arbitrary deadlines with\npotential accuracy improvement over off-the-shelf networks. Our experimental\nresults show that such utilization of TRNs, while transferring to a simpler\ndataset, in combination with NetCut, can lead to the proposal of networks that\ncan achieve relative accuracy improvement of up to 10.43% among existing\noff-the-shelf neural architectures while meeting a specific deadline, and 27x\nspeedup in exploration time.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 22:02:43 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zandigohar", "Mehrshad", ""], ["Erdogmus", "Deniz", ""], ["Schirner", "Gunar", ""]]}, {"id": "2101.05615", "submitter": "Daya Khudia", "authors": "Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu,\n  Jongsoo Park, Mikhail Smelyanskiy", "title": "FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models typically use single-precision (FP32) floating point\ndata types for representing activations and weights, but a slew of recent\nresearch work has shown that computations with reduced-precision data types\n(FP16, 16-bit integers, 8-bit integers or even 4- or 2-bit integers) are enough\nto achieve same accuracy as FP32 and are much more efficient. Therefore, we\ndesigned fbgemm, a high-performance kernel library, from ground up to perform\nhigh-performance quantized inference on current generation CPUs. fbgemm\nachieves efficiency by fusing common quantization operations with a\nhigh-performance gemm implementation and by shape- and size-specific kernel\ncode generation at runtime. The library has been deployed at Facebook, where it\ndelivers greater than 2x performance gains with respect to our current\nproduction baseline.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:34:04 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Khudia", "Daya", ""], ["Huang", "Jianyu", ""], ["Basu", "Protonu", ""], ["Deng", "Summer", ""], ["Liu", "Haixin", ""], ["Park", "Jongsoo", ""], ["Smelyanskiy", "Mikhail", ""]]}, {"id": "2101.06529", "submitter": "Thirupathaiah Vasantam", "authors": "Thirupathaiah Vasantam, Ravi R. Mazumdar", "title": "Sensitivity of Mean-Field Fluctuations in Erlang loss models with\n  randomized routing", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a large system of $N$ servers each with capacity to\nprocess at most $C$ simultaneous jobs and an incoming job is routed to a server\nif it has the lowest occupancy amongst $d$ (out of N) randomly selected\nservers. A job that is routed to a server with no vacancy is assumed to be\nblocked and lost. Such randomized policies are referred to JSQ(d) (Join the\nShortest Queue out of $d$) policies. Under the assumption that jobs arrive\naccording to a Poisson process with rate $N\\lambda^{(N)}$ where\n$\\lambda^{(N)}=\\sigma-\\frac{\\beta}{\\sqrt{N}}$, $\\sigma\\in\\mb{R}_+$ and\n$\\beta\\in\\mb{R}$, we establish functional central limit theorems (FCLTs) for\nthe fluctuation process in both the transient and stationary regimes when\nservice time distributions are exponential. In particular, we show that the\nlimit is an Ornstein-Uhlenbeck process whose mean and variance depend on the\nmean-field of the considered model. Using this, we obtain approximations to the\nblocking probabilities for large $N$, where we can precisely estimate the\naccuracy of first-order approximations.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 21:45:31 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Vasantam", "Thirupathaiah", ""], ["Mazumdar", "Ravi R.", ""]]}, {"id": "2101.06584", "submitter": "Tomonori Kouya", "authors": "Tomonori Kouya", "title": "Acceleration of multiple precision matrix multiplication based on\n  multi-component floating-point arithmetic using AVX2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report the results obtained from the acceleration of\nmulti-binary64-type multiple precision matrix multiplication with AVX2. We\ntarget double-double (DD), triple-double (TD), and quad-double (QD) precision\narithmetic designed by certain types of error-free transformation (EFT)\narithmetic. Furthermore, we implement SIMDized EFT functions, which\nsimultaneously compute with four binary64 numbers on x86_64 computing\nenvironment, and by using help of them, we also develop SIMDized DD, TD, and QD\nadditions and multiplications. In addition, AVX2 load/store functions were\nadopted to efficiently speed up reading and storing matrix elements from/to\nmemory. Owing to these combined techniques, our implemented multiple precision\nmatrix multiplications have been accelerated more than three times compared\nwith non-accelerated ones. Our accelerated matrix multiplication modifies the\nperformance of parallelization with OpenMP.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 04:05:13 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kouya", "Tomonori", ""]]}, {"id": "2101.07043", "submitter": "Samrat Mukhopadhyay", "authors": "Samrat Mukhopadhyay, Abhishek Sinha", "title": "Online Caching with Optimal Switching Regret", "comments": "11 pages, 3 figures, to be submitted to ISIT, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical uncoded caching problem from an online learning\npoint-of-view. A cache of limited storage capacity can hold $C$ files at a time\nfrom a large catalog. A user requests an arbitrary file from the catalog at\neach time slot. Before the file request from the user arrives, a caching policy\npopulates the cache with any $C$ files of its choice. In the case of a\ncache-hit, the policy receives a unit reward and zero rewards otherwise. In\naddition to that, there is a cost associated with fetching files to the cache,\nwhich we refer to as the switching cost. The objective is to design a caching\npolicy that incurs minimal regret while considering both the rewards due to\ncache-hits and the switching cost due to the file fetches. The main\ncontribution of this paper is the switching regret analysis of a Follow the\nPerturbed Leader-based anytime caching policy, which is shown to have an order\noptimal switching regret. In this pursuit, we improve the best-known switching\nregret bound for this problem by a factor of $\\Theta(\\sqrt{C}).$ We conclude\nthe paper by comparing the performance of different popular caching policies\nusing a publicly available trace from a commercial CDN server.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:47:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mukhopadhyay", "Samrat", ""], ["Sinha", "Abhishek", ""]]}, {"id": "2101.07129", "submitter": "Yudi Huang", "authors": "Yudi Huang, Ting He, Nilanjan Ray Chaudhuri, Thomas La Porta", "title": "Verifiable Failure Localization in Smart Grid under Cyber-Physical\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical attacks impose a significant threat to the smart grid, as the\ncyber attack makes it difficult to identify the actual damage caused by the\nphysical attack. To defend against such attacks, various inference-based\nsolutions have been proposed to estimate the states of grid elements (e.g.,\ntransmission lines) from measurements outside the attacked area, out of which a\nfew have provided theoretical conditions for guaranteed accuracy. However,\nthese conditions are usually based on the ground truth states and thus not\nverifiable in practice. To solve this problem, we develop (i) verifiable\nconditions that can be tested based on only observable information, and (ii)\nefficient algorithms for verifying the states of links (i.e., transmission\nlines) within the attacked area based on these conditions. Our numerical\nevaluations based on the Polish power grid and IEEE 300-bus system demonstrate\nthat the proposed algorithms are highly successful in verifying the states of\ntruly failed links, and can thus greatly help in prioritizing repairs during\nthe recovery process.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:44:05 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Huang", "Yudi", ""], ["He", "Ting", ""], ["Chaudhuri", "Nilanjan Ray", ""], ["La Porta", "Thomas", ""]]}, {"id": "2101.07344", "submitter": "Arjun Balasubramanian", "authors": "Arjun Balasubramanian, Adarsh Kumar, Yuhan Liu, Han Cao, Shivaram\n  Venkataraman, Aditya Akella", "title": "Accelerating Deep Learning Inference via Learned Caches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are witnessing increased adoption in multiple\ndomains owing to their high accuracy in solving real-world problems. However,\nthis high accuracy has been achieved by building deeper networks, posing a\nfundamental challenge to the low latency inference desired by user-facing\napplications. Current low latency solutions trade-off on accuracy or fail to\nexploit the inherent temporal locality in prediction serving workloads.\n  We observe that caching hidden layer outputs of the DNN can introduce a form\nof late-binding where inference requests only consume the amount of computation\nneeded. This enables a mechanism for achieving low latencies, coupled with an\nability to exploit temporal locality. However, traditional caching approaches\nincur high memory overheads and lookup latencies, leading us to design learned\ncaches - caches that consist of simple ML models that are continuously updated.\nWe present the design of GATI, an end-to-end prediction serving system that\nincorporates learned caches for low-latency DNN inference. Results show that\nGATI can reduce inference latency by up to 7.69X on realistic workloads.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:13:08 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Balasubramanian", "Arjun", ""], ["Kumar", "Adarsh", ""], ["Liu", "Yuhan", ""], ["Cao", "Han", ""], ["Venkataraman", "Shivaram", ""], ["Akella", "Aditya", ""]]}, {"id": "2101.07690", "submitter": "Peng Jiang", "authors": "Peng Jiang, Rujia Wang, Bo Wu", "title": "Efficient Mining of Frequent Subgraphs with Two-Vertex Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Frequent Subgraph Mining (FSM) is the key task in many graph mining and\nmachine learning applications. Numerous systems have been proposed for FSM in\nthe past decade. Although these systems show good performance for small\npatterns (with no more than four vertices), we found that they have difficulty\nin mining larger patterns. In this work, we propose a novel two-vertex\nexploration strategy to accelerate the mining process. Compared with the\nsingle-vertex exploration adopted by previous systems, our two-vertex\nexploration avoids the large memory consumption issue and significantly reduces\nthe memory access overhead. We further enhance the performance through an\nindex-based quick pattern technique that reduces the overhead of isomorphism\nchecks, and a subgraph sampling technique that mitigates the issue of subgraph\nexplosion. The experimental results show that our system achieves significant\nspeedups against the state-of-the-art graph pattern mining systems and supports\nlarger pattern mining tasks that none of the existing systems can handle.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:35:24 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 21:08:42 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jiang", "Peng", ""], ["Wang", "Rujia", ""], ["Wu", "Bo", ""]]}, {"id": "2101.07956", "submitter": "Seung Won Min", "authors": "Seung Won Min, Kun Wu, Sitao Huang, Mert Hidayeto\\u{g}lu, Jinjun\n  Xiong, Eiman Ebrahimi, Deming Chen, Wen-mei Hwu", "title": "PyTorch-Direct: Enabling GPU Centric Data Access for Very Large Graph\n  Neural Network Training with Irregular Accesses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing adoption of graph neural networks (GNNs) in the machine\nlearning community, GPUs have become an essential tool to accelerate GNN\ntraining. However, training GNNs on very large graphs that do not fit in GPU\nmemory is still a challenging task. Unlike conventional neural networks,\nmini-batching input samples in GNNs requires complicated tasks such as\ntraversing neighboring nodes and gathering their feature values. While this\nprocess accounts for a significant portion of the training time, we find\nexisting GNN implementations using popular deep neural network (DNN) libraries\nsuch as PyTorch are limited to a CPU-centric approach for the entire data\npreparation step. This \"all-in-CPU\" approach has negative impact on the overall\nGNN training performance as it over-utilizes CPU resources and hinders GPU\nacceleration of GNN training. To overcome such limitations, we introduce\nPyTorch-Direct, which enables a GPU-centric data accessing paradigm for GNN\ntraining. In PyTorch-Direct, GPUs are capable of efficiently accessing\ncomplicated data structures in host memory directly without CPU intervention.\nOur microbenchmark and end-to-end GNN training results show that PyTorch-Direct\nreduces data transfer time by 47.1% on average and speeds up GNN training by up\nto 1.6x. Furthermore, by reducing CPU utilization, PyTorch-Direct also saves\nsystem power by 12.4% to 17.5% during training. To minimize programmer effort,\nwe introduce a new \"unified tensor\" type along with necessary changes to the\nPyTorch memory allocator, dispatch logic, and placement rules. As a result,\nusers need to change at most two lines of their PyTorch GNN training code for\neach tensor object to take advantage of PyTorch-Direct.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 04:24:39 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Min", "Seung Won", ""], ["Wu", "Kun", ""], ["Huang", "Sitao", ""], ["Hidayeto\u011flu", "Mert", ""], ["Xiong", "Jinjun", ""], ["Ebrahimi", "Eiman", ""], ["Chen", "Deming", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2101.08062", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Donghyun Kang, and Young Ik Eom", "title": "Thread Evolution Kit for Optimizing Thread Operations on CE/IoT Devices", "comments": null, "journal-ref": null, "doi": "10.1109/TCE.2020.3033328", "report-no": null, "categories": "cs.OS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern operating systems have adopted the one-to-one thread model to\nsupport fast execution of threads in both multi-core and single-core systems.\nThis thread model, which maps the kernel-space and user-space threads in a\none-to-one manner, supports quick thread creation and termination in\nhigh-performance server environments. However, the performance of time-critical\nthreads is degraded when multiple threads are being run in low-end CE devices\nwith limited system resources. When a CE device runs many threads to support\ndiverse application functionalities, low-level hardware specifications often\nlead to significant resource contention among the threads trying to obtain\nsystem resources. As a result, the operating system encounters challenges, such\nas excessive thread context switching overhead, execution delay of\ntime-critical threads, and a lack of virtual memory for thread stacks. This\npaper proposes a state-of-the-art Thread Evolution Kit (TEK) that consists of\nthree primary components: a CPU Mediator, Stack Tuner, and Enhanced Thread\nIdentifier. From the experiment, we can see that the proposed scheme\nsignificantly improves user responsiveness (7x faster) under high CPU\ncontention compared to the traditional thread model. Also, TEK solves the\nsegmentation fault problem that frequently occurs when a CE application\nincreases the number of threads during its execution.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:54:59 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Lim", "Geunsik", ""], ["Kang", "Donghyun", ""], ["Eom", "Young Ik", ""]]}, {"id": "2101.08458", "submitter": "Jian Weng", "authors": "Jian Weng, Animesh Jain, Jie Wang, Leyuan Wang, Yida Wang, and Tony\n  Nowatzki", "title": "UNIT: Unifying Tensorized Instruction Compilation", "comments": "13 pages, 13 figures, and 1 table", "journal-ref": "2021 IEEE/ACM International Symposium on Code Generation and\n  Optimization (CGO), Seoul, Korea (South), 2021, pp. 77-89", "doi": "10.1109/CGO51591.2021.9370330", "report-no": null, "categories": "cs.PL cs.AR cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because of the increasing demand for computation in DNN, researchers develope\nboth hardware and software mechanisms to reduce the compute and memory burden.\nA widely adopted approach is to use mixed precision data types. However, it is\nhard to leverage mixed precision without hardware support because of the\noverhead of data casting. Hardware vendors offer tensorized instructions for\nmixed-precision tensor operations, like Intel VNNI, Tensor Core, and ARM-DOT.\nThese instructions involve a computing idiom that reduces multiple low\nprecision elements into one high precision element. The lack of compilation\ntechniques for this makes it hard to utilize these instructions: Using\nvendor-provided libraries for computationally-intensive kernels is inflexible\nand prevents further optimizations, and manually writing hardware intrinsics is\nerror-prone and difficult for programmers. Some prior works address this\nproblem by creating compilers for each instruction. This requires excessive\neffort when it comes to many tensorized instructions. In this work, we develop\na compiler framework to unify the compilation for these instructions -- a\nunified semantics abstraction eases the integration of new instructions, and\nreuses the analysis and transformations. Tensorized instructions from different\nplatforms can be compiled via UNIT with moderate effort for favorable\nperformance. Given a tensorized instruction and a tensor operation, UNIT\nautomatically detects the applicability, transforms the loop organization of\nthe operation,and rewrites the loop body to leverage the tensorized\ninstruction. According to our evaluation, UNIT can target various mainstream\nhardware platforms. The generated end-to-end inference model achieves 1.3x\nspeedup over Intel oneDNN on an x86 CPU, 1.75x speedup over Nvidia cuDNN on an\nNvidiaGPU, and 1.13x speedup over a carefully tuned TVM solution for ARM DOT on\nan ARM CPU.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 06:22:58 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 03:36:45 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 04:11:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Weng", "Jian", ""], ["Jain", "Animesh", ""], ["Wang", "Jie", ""], ["Wang", "Leyuan", ""], ["Wang", "Yida", ""], ["Nowatzki", "Tony", ""]]}, {"id": "2101.08479", "submitter": "Xiao Hong", "authors": "Yue Hong Gao, Xiao Hong, Hao Tian Yang, Lu Chen, Xiao Nan Zhang", "title": "Comparison and Improvement for Delay Analysis Approaches: Theoretical\n  Models and Experimental Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer network tends to be subjected to the proliferation of mobile demands\nand increasingly multifarious, therefore it poses a great challenge to\nguarantee the quality of network service. By designing the model according to\ndifferent requirements, we may get some related indicators such as delay and\npacket loss rate in order to evaluate the quality of network service and verify\nthe user data surface and capacity of the network environment. In this paper,\nwe describe an analytical model based on the measurement for the delay of each\npacket passing through the single existing routers in the network environment.\nIn previous studies, the emulation of real network service behaviors was\ngenerally under ideal condition. In our work, the test environment is built to\nget the relevant test results of the actual network, and the corresponding\ntheoretical results are obtained by our model. The test results are compared\nwith the theoretical results, analyzed and corrected, in order to verify the\nfeasibility of our analysis model for the performance analysis of the actual\nnetwork. With this concern, calculation results are modified with different\nschemes to realize more precise calculation of delay boundary with the\ncomparison with the experimental test results. The results show the analysis\nmethods after the amendment can realistically estimate the performance of\nnetwork element.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:36:31 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gao", "Yue Hong", ""], ["Hong", "Xiao", ""], ["Yang", "Hao Tian", ""], ["Chen", "Lu", ""], ["Zhang", "Xiao Nan", ""]]}, {"id": "2101.08877", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Changwoo Min, and Young Ik Eom", "title": "Virtual Memory Partitioning for Enhancing Application Performance in\n  Mobile Platforms", "comments": null, "journal-ref": null, "doi": "10.1109/TCE.2013.6689690", "report-no": null, "categories": "cs.AR cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the amount of running software on smart mobile devices is gradually\nincreasing due to the introduction of application stores. The application store\nis a type of digital distribution platform for application software, which is\nprovided as a component of an operating system on a smartphone or tablet.\nMobile devices have limited memory capacity and, unlike server and desktop\nsystems, due to their mobility they do not have a memory slot that can expand\nthe memory capacity. Low memory killer (LMK) and out-of-memory killer (OOMK)\nare widely used memory management solutions in mobile systems. They forcibly\nterminate applications when the available physical memory becomes insufficient.\nIn addition, before the forced termination, the memory shortage incurs\nthrashing and fragmentation, thus slowing down application performance.\nAlthough the existing page reclamation mechanism is designed to secure\navailable memory, it could seriously degrade user responsiveness due to the\nthrashing. Memory management is therefore still important especially in mobile\ndevices with small memory capacity. This paper presents a new memory\npartitioning technique that resolves the deterioration of the existing\napplication life cycle induced by LMK and OOMK. It provides a completely\nisolated virtual memory node at the operating system level. Evaluation results\ndemonstrate that the proposed method improves application execution time under\nmemory shortage, compared with methods in previous studies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:54:21 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lim", "Geunsik", ""], ["Min", "Changwoo", ""], ["Eom", "Young Ik", ""]]}, {"id": "2101.08878", "submitter": "Aamir Shafi", "authors": "Aamir Shafi, Jahanzeb Maqbool Hashmi, Hari Subramoni and Dhabaleswar\n  K. Panda", "title": "Efficient MPI-based Communication for GPU-Accelerated Dask Applications", "comments": "10 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dask is a popular parallel and distributed computing framework, which rivals\nApache Spark to enable task-based scalable processing of big data. The Dask\nDistributed library forms the basis of this computing engine and provides\nsupport for adding new communication devices. It currently has two\ncommunication devices: one for TCP and the other for high-speed networks using\nUCX-Py -- a Cython wrapper to UCX. This paper presents the design and\nimplementation of a new communication backend for Dask -- called MPI4Dask --\nthat is targeted for modern HPC clusters built with GPUs. MPI4Dask exploits\nmpi4py over MVAPICH2-GDR, which is a GPU-aware implementation of the Message\nPassing Interface (MPI) standard. MPI4Dask provides point-to-point asynchronous\nI/O communication coroutines, which are non-blocking concurrent operations\ndefined using the async/await keywords from the Python's asyncio framework. Our\nlatency and throughput comparisons suggest that MPI4Dask outperforms UCX by 6x\nfor 1 Byte message and 4x for large messages (2 MBytes and beyond)\nrespectively. We also conduct comparative performance evaluation of MPI4Dask\nwith UCX using two benchmark applications: 1) sum of cuPy array with its\ntranspose, and 2) cuDF merge. MPI4Dask speeds up the overall execution time of\nthe two applications by an average of 3.47x and 3.11x respectively on an\nin-house cluster built with NVIDIA Tesla V100 GPUs for 1-6 Dask workers. We\nalso perform scalability analysis of MPI4Dask against UCX for these\napplications on TACC's Frontera (GPU) system with upto 32 Dask workers on 32\nNVIDIA Quadro RTX 5000 GPUs and 256 CPU cores. MPI4Dask speeds up the execution\ntime for cuPy and cuDF applications by an average of 1.71x and 2.91x\nrespectively for 1-32 Dask workers on the Frontera (GPU) system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:59:08 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Shafi", "Aamir", ""], ["Hashmi", "Jahanzeb Maqbool", ""], ["Subramoni", "Hari", ""], ["Panda", "Dhabaleswar K.", ""]]}, {"id": "2101.08889", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, MyungJoo Ham, Jijoong Moon, Wook Song, Sangjung Woo, and\n  Sewon Oh", "title": "TAOS-CI: Lightweight & Modular Continuous Integration System for Edge\n  Computing", "comments": "arXiv admin note: text overlap with arXiv:2101.07961", "journal-ref": null, "doi": "10.1109/ICCE.2019.8662017", "report-no": null, "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of IoT and edge devices, we are observing a lot of\nconsumer electronics becoming yet another IoT and edge devices. Unlike\ntraditional smart devices, such as smart phones, consumer electronics, in\ngeneral, have significant diversities with fewer number of devices per product\nmodel. With such high diversities, the proliferation of edge devices requires\nfrequent and seamless updates of consumer electronics, which makes the\nmanufacturers prone to regressions because the manufacturers have less resource\nper an instance of software release; i.e., they need to repeat releases by the\nnumber of product models times the number of updates. Continuous Integration\n(CI) systems can help prevent regression bugs from actively developing software\npackages including the frequently updated device software platforms. The\nproposed CI system provides a portable and modular software platform\nautomatically inspecting potential issues of incoming changes with the enabled\nmodules: code format and style, performance regressions, static checks on the\nsource code, build and packaging tests, and dynamic checks with the built\nbinary before deploying a platform image on the IoT and edge devices. Besides,\nour proposed approach is lightweight enough to be hosted in normal desktop\ncomputers even for dozens of developers. As a result, it can be easily applied\nto a lot of various source code repositories. Evaluation results demonstrate\nthat the proposed method drastically improves plug-ins execution time and\nmemory consumption, compared with methods in previous studies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:36:35 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lim", "Geunsik", ""], ["Ham", "MyungJoo", ""], ["Moon", "Jijoong", ""], ["Song", "Wook", ""], ["Woo", "Sangjung", ""], ["Oh", "Sewon", ""]]}, {"id": "2101.09309", "submitter": "Yohan Eguillon", "authors": "Yohan Eguillon, Bruno Lacabanne, Damien Tromeur-Dervout", "title": "F3ORNITS: A Flexible Variable Step Size Non-Iterative Co-simulation\n  Method handling Subsystems with Hybrid Advanced Capabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper introduces the F3ORNITS non-iterative co-simulation algorithm in\nwhich F3 stands for the 3 flexible aspects of the method: flexible polynomial\norder representation of coupling variables, flexible time-stepper applying\nvariable co-simulation step size rules on subsystems allowing it and flexible\nscheduler orchestrating the meeting times among the subsystems and capable of\nasynchronousness when subsystems constraints requires it. The motivation of the\nF3ORNITS method is to accept any kind of co-simulation model, including any\nkind of subsystem, regardless on their available capabilities. Indeed, one the\nmajor problems in industry is that the subsystems usually have constraints or\nlack of advanced capabilities making it impossible to implement most of the\nadvanced co-simulation algorithms on them. The method makes it possible to\npreserve the dynamics of the coupling constraints when necessary as well as to\navoid breaking C1 smoothness at communication times, and also to adapt the\nco-simulation step size in a way that is robust both to zero-crossing variables\n(contrary to classical relative error-based criteria) and to jumps. Two test\ncases are presented to illustrate the robustness of the F3ORNITS method as well\nas its higher accuracy than the non-iterative Jacobi coupling algorithm (the\nmost commonly used method in industry) for a smaller number of co-simulation\nsteps.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:58:24 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Eguillon", "Yohan", ""], ["Lacabanne", "Bruno", ""], ["Tromeur-Dervout", "Damien", ""]]}, {"id": "2101.09515", "submitter": "Dheryta Jaisinghani", "authors": "Dheryta Jaisinghani, Vinayak Naik, Rajesh Balan, Archan Misra, and\n  Youngki Lee", "title": "Experiences & Challenges with Server-Side WiFi Indoor Localization Using\n  Existing Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world deployments of WiFi-based indoor localization in large public\nvenues are few and far between as most state-of-the-art solutions require\neither client or infrastructure-side changes. Hence, even though high location\naccuracy is possible with these solutions, they are not practical due to cost\nand/or client adoption reasons. Majority of the public venues use commercial\ncontroller-managed WLAN solutions, %provided by Aruba, Cisco, etc., that\nneither allow client changes nor infrastructure changes. In fact, for such\nvenues we have observed highly heterogeneous devices with very low adoption\nrates for client-side apps.\n  In this paper, we present our experiences in deploying a scalable location\nsystem for such venues. We show that server-side localization is not trivial\nand present two unique challenges associated with this approach, namely\nCardinality Mismatch and High Client Scan Latency. The \"Mismatch\" challenge\nresults in a significant mismatch between the set of access points (APs)\nreporting a client in the offline and online phases, while the \"Latency\"\nchallenge results in a low number of APs reporting data for any particular\nclient. We collect three weeks of detailed ground truth data (~200 landmarks),\nfrom a WiFi setup that has been deployed for more than four years, to provide\nevidences for the extent and understanding the impact of these problems. Our\nanalysis of real-world client devices reveal that the current trend for the\nclients is to reduce scans, thereby adversely impacting their localization\naccuracy. We analyze how localization is impacted when scans are minimal. We\npropose heuristics to alleviate reduction in the accuracy despite lesser scans.\nBesides the number of scans, we summarize the other challenges and pitfalls of\nreal deployments which hamper the localization accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 14:48:17 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 04:58:28 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Jaisinghani", "Dheryta", ""], ["Naik", "Vinayak", ""], ["Balan", "Rajesh", ""], ["Misra", "Archan", ""], ["Lee", "Youngki", ""]]}, {"id": "2101.09795", "submitter": "Hassan Habibi Gharakheili", "authors": "Xiaohong Deng and Yun Feng and Thanchanok Sutjarittham and Hassan\n  Habibi Gharakheili and Blanca Gallego and Vijay Sivaraman", "title": "Comparing Broadband ISP Performance using Big Data from M-Lab", "comments": "14 pages, 16 figures. arXiv admin note: text overlap with\n  arXiv:1901.07059", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing ISPs on broadband speed is challenging, since measurements can vary\ndue to subscriber attributes such as operation system and test conditions such\nas access capacity, server distance, TCP window size, time-of-day, and network\nsegment size. In this paper, we draw inspiration from observational studies in\nmedicine, which face a similar challenge in comparing the effect of treatments\non patients with diverse characteristics, and have successfully tackled this\nusing \"causal inference\" techniques for {\\em post facto} analysis of medical\nrecords. Our first contribution is to develop a tool to pre-process and\nvisualize the millions of data points in M-Lab at various time- and\nspace-granularities to get preliminary insights on factors affecting broadband\nperformance. Next, we analyze 24 months of data pertaining to twelve ISPs\nacross three countries, and demonstrate that there is observational bias in the\ndata due to disparities amongst ISPs in their attribute distributions. For our\nthird contribution, we apply a multi-variate matching method to identify\nsuitable cohorts that can be compared without bias, which reveals that ISPs are\ncloser in performance than thought before. Our final contribution is to refine\nour model by developing a method for estimating speed-tier and re-apply\nmatching for comparison of ISP performance. Our results challenge conventional\nrankings of ISPs, and pave the way towards data-driven approaches for unbiased\ncomparisons of ISPs world-wide.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 20:37:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Deng", "Xiaohong", ""], ["Feng", "Yun", ""], ["Sutjarittham", "Thanchanok", ""], ["Gharakheili", "Hassan Habibi", ""], ["Gallego", "Blanca", ""], ["Sivaraman", "Vijay", ""]]}, {"id": "2101.10231", "submitter": "David Daly", "authors": "David Daly", "title": "Creating a Virtuous Cycle in Performance Testing at MongoDB", "comments": "Author's copy and preprint. Accepted for publication at ICPE2021. 9\n  pages, 5 figures", "journal-ref": null, "doi": "10.1145/3427921.3450234", "report-no": null, "categories": "cs.SE cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is important to detect changes in software performance during development\nin order to avoid performance decreasing release to release or dealing with\ncostly delays at release time. Performance testing is part of the development\nprocess at MongoDB, and integrated into our continuous integration system. We\ndescribe a set of changes to that performance testing environment designed to\nimprove testing effectiveness. These changes help improve coverage, provide\nfaster and more accurate signaling for performance changes, and help us better\nunderstand the state of performance. In addition to each component performing\nbetter, we believe that we have created and exploited a virtuous cycle:\nperformance test improvements drive impact, which drives more use, which drives\nfurther impact and investment in improvements. Overall, MongoDB is getting\nfaster and we avoid shipping major performance regressions to our customers\nbecause of this infrastructure.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:47:05 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 02:14:37 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Daly", "David", ""]]}, {"id": "2101.10464", "submitter": "Mirko Zichichi", "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D'Angelo, V\\'ictor\n  Rodr\\'iguez-Doncel", "title": "Personal Data Access Control Through Distributed Authorization", "comments": null, "journal-ref": "2020 IEEE 19th International Symposium on Network Computing and\n  Applications (NCA)", "doi": "10.1109/NCA51143.2020.9306721", "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an architecture of a Personal Information Management\nSystem, in which individuals can define the access to their personal data by\nmeans of smart contracts. These smart contracts, running on the Ethereum\nblockchain, implement access control lists and grant immutability, traceability\nand verifiability of the references to personal data, which is stored itself in\na (possibly distributed) file system. A distributed authorization mechanism is\ndevised, where trust from multiple network nodes is necessary to grant the\naccess to the data. To this aim, two possible alternatives are described: a\nSecret Sharing scheme and Threshold Proxy Re-Encryption scheme. The performance\nof these alternatives is experimentally compared in terms of execution time.\nThreshold Proxy Re-Encryption appears to be faster in different scenarios, in\nparticular when increasing message size, number of nodes and the threshold\nvalue, i.e. number of nodes needed to grant the data disclosure.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:34:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zichichi", "Mirko", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""], ["Rodr\u00edguez-Doncel", "V\u00edctor", ""]]}, {"id": "2101.10707", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Changwoo Min, and Young Ik Eom", "title": "Enhancing Application Performance by Memory Partitioning in Android\n  Platforms", "comments": null, "journal-ref": null, "doi": "10.1109/ICCE.2013.6487055", "report-no": null, "categories": "cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests a new memory partitioning scheme that can enhance process\nlifecycle, while avoiding Low Memory Killer and Out-of-Memory Killer operations\non mobile devices. Our proposed scheme offers the complete concept of virtual\nmemory nodes in operating systems of Android devices.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:03:02 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lim", "Geunsik", ""], ["Min", "Changwoo", ""], ["Eom", "Young Ik", ""]]}, {"id": "2101.12146", "submitter": "Navneet Garg", "authors": "Navneet Garg and Tharmalingam Ratnarajah", "title": "Random-Mode Frank-Wolfe Algorithm for Tensor Completion in Wireless Edge\n  Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wireless edge caching is a popular strategy to avoid backhaul congestion in\nthe next generation networks, where the content is cached in advance at the\nbase stations to fulfil the redundant requests during peak periods. In the edge\ncaching data, the missing observations are inevitable due to dynamic selective\npopularity. Among the completion methods, the tensor-based models have been\nshown to be the most advantageous for missing data imputation. Also, since the\nobservations are correlated across time, files, and base stations, in this\npaper, we formulate the caching, prediction and recommendation problem as a\nfourth-order tensor completion problem. Since the content library can be large\nleading to a large dimension tensor, we modify the latent norm-based\nFrank-Wolfe (FW) algorithm with tensor-ring decomposition towards a lower time\ncomplexity using random mode selection. Analyzing the time and space complexity\nof the algorithm shows $N$-times reduction in computational time where $N$ is\nthe order of tensor. Simulations with MovieLens dataset shows the approximately\nsimilar reconstruction errors for the presented FW algorithm as compared to\nthat of the recent FW algorithm, albeit with lower computation overhead. It is\nalso demonstrated that the completed tensor improves normalized cache hit rates\nfor linear prediction schemes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 17:56:01 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Garg", "Navneet", ""], ["Ratnarajah", "Tharmalingam", ""]]}, {"id": "2101.12160", "submitter": "Daan Rutten", "authors": "Daan Rutten, Debankur Mukherjee", "title": "A New Approach to Capacity Scaling Augmented With Unreliable Machine\n  Learning Predictions", "comments": "47 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NI cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data centers suffer from immense power consumption. The erratic\nbehavior of internet traffic forces data centers to maintain excess capacity in\nthe form of idle servers in case the workload suddenly increases. As an idle\nserver still consumes a significant fraction of the peak energy, data center\noperators have heavily invested in capacity scaling solutions. In simple terms,\nthese aim to deactivate servers if the demand is low and to activate them again\nwhen the workload increases. To do so, an algorithm needs to strike a delicate\nbalance between power consumption, flow-time, and switching costs. Over the\nlast decade, the research community has developed competitive online algorithms\nwith worst-case guarantees. In the presence of historic data patterns,\nprescription from Machine Learning (ML) predictions typically outperform such\ncompetitive algorithms. This, however, comes at the cost of sacrificing the\nrobustness of performance, since unpredictable surges in the workload are not\nuncommon. The current work builds on the emerging paradigm of augmenting\nunreliable ML predictions with online algorithms to develop novel robust\nalgorithms that enjoy the benefits of both worlds.\n  We analyze a continuous-time model for capacity scaling, where the goal is to\nminimize the weighted sum of flow-time, switching cost, and power consumption\nin an online fashion. We propose a novel algorithm, called Adaptive Balanced\nCapacity Scaling (ABCS), that has access to black-box ML predictions, but is\ncompletely oblivious to the accuracy of these predictions. In particular, if\nthe predictions turn out to be accurate in hindsight, we prove that ABCS is\n$(1+\\varepsilon)$-competitive. Moreover, even when the predictions are\ninaccurate, ABCS guarantees a bounded competitive ratio. The performance of the\nABCS algorithm on a real-world dataset positively support the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:14:18 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rutten", "Daan", ""], ["Mukherjee", "Debankur", ""]]}, {"id": "2101.12512", "submitter": "Bj{\\o}rn Ivar Teigen", "authors": "Bj{\\o}rn Ivar Teigen, Neil Davies, Kai Olav Ellefsen, Tor Skeie, Jim\n  Torresen", "title": "A Model of WiFi Performance With Bounded Latency", "comments": "12 pages, 19 figures, Submitted for review to SIGCOMM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In September 2020, the Broadband Forum published a new industry standard for\nmeasuring network quality. The standard centers on the notion of quality\nattenuation. Quality attenuation is a measure of the distribution of latency\nand packet loss between two points connected by a network path. A vital feature\nof the quality attenuation idea is that we can express detailed application\nrequirements and network performance measurements in the same mathematical\nframework. Performance requirements and measurements are both modeled as\nlatency distributions. To the best of our knowledge, existing models of the\n802.11 WiFi protocol do not permit the calculation of complete latency\ndistributions without assuming steady-state operation. We present a novel model\nof the WiFi protocol. Instead of computing throughput numbers from a\nsteady-state analysis of a Markov chain, we explicitly model latency and packet\nloss. Explicitly modeling latency and loss allows for both transient and\nsteady-state analysis of latency distributions, and we can derive throughput\nnumbers from the latency results. Our model is, therefore, more general than\nthe standard Markov chain methods. We reproduce several known results with this\nmethod. Using transient analysis, we derive bounds on WiFi throughput under the\nrequirement that latency and packet loss must be bounded.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:44:19 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Teigen", "Bj\u00f8rn Ivar", ""], ["Davies", "Neil", ""], ["Ellefsen", "Kai Olav", ""], ["Skeie", "Tor", ""], ["Torresen", "Jim", ""]]}, {"id": "2101.12588", "submitter": "Tareq Si Salem", "authors": "Tareq Si Salem, Giovanni Neglia and Stratis Ioannidis", "title": "No-Regret Caching via Online Mirror Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study an online caching problem in which requests can be served by a local\ncache to avoid retrieval costs from a remote server. The cache can update its\nstate after a batch of requests and store an arbitrarily small fraction of each\ncontent. We study no-regret algorithms based on Online Mirror Descent (OMD)\nstrategies. We show that the optimal OMD strategy depends on the request\ndiversity present in a batch. We also prove that, when the cache must store the\nentire content, rather than a fraction, OMD strategies can be coupled with a\nrandomized rounding scheme that preserves regret guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 13:56:51 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 08:14:49 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 10:52:42 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Salem", "Tareq Si", ""], ["Neglia", "Giovanni", ""], ["Ioannidis", "Stratis", ""]]}]