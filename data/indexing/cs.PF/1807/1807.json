[{"id": "1807.00107", "submitter": "Fabio Lorenzo Traversa Ph.D.", "authors": "Forrest Sheldon, Pietro Cicotti, Fabio L. Traversa, Massimiliano Di\n  Ventra", "title": "Stress-testing memcomputing on hard combinatorial optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.PF math.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memcomputing is a novel paradigm of computation that utilizes dynamical\nelements with memory to both store and process information on the same physical\nlocation. Its building blocks can be fabricated in hardware with standard\nelectronic circuits, thus offering a path to its practical realization. In\naddition, since memcomputing is based on non-quantum elements, the equations of\nmotion describing these machines can be simulated efficiently on standard\ncomputers. In fact, it was recently realized that memcomputing, and in\nparticular its digital (hence scalable) version, when simulated on a classical\nmachine provides a significant speed-up over state-of-the-art algorithms on a\nvariety of non-convex problems. Here, we stress-test the capabilities of this\napproach on finding approximate solutions to hard combinatorial optimization\nproblems. These fall into a class which is known to require exponentially\ngrowing resources in the worst cases, even to generate approximations. We\nrecently showed that in a region where state of the art algorithms demonstrate\nthis exponential growth, simulations of digital memcomputing machines performed\nusing the Falcon$^\\copyright$ simulator of MemComputing, Inc. only require time\nand memory resources that scale linearly. These results are extended in a\nstress-test up to $64\\times10^6$ variables (corresponding to about 1 billion\nliterals), namely the largest case that we could fit on a single node with 128\nGB of DRAM. Since memcomputing can be applied to a wide variety of optimization\nproblems, this stress test shows the considerable advantage of\nnon-combinatorial, physics-inspired approaches over standard combinatorial\nones.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 02:06:57 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Sheldon", "Forrest", ""], ["Cicotti", "Pietro", ""], ["Traversa", "Fabio L.", ""], ["Di Ventra", "Massimiliano", ""]]}, {"id": "1807.00146", "submitter": "Ralf-Peter Mundani", "authors": "J\\'er\\^ome Frisch (1), Ralf-Peter Mundani (2) ((1) RWTH Aachen\n  University, Aachen, Germany, (2) Technische Universit\\\"at M\\\"unchen, Munich,\n  Germany)", "title": "Measuring and comparing the scaling behaviour of a high-performance CFD\n  code on different supercomputing infrastructures", "comments": "8 pages, 8 figures", "journal-ref": "Proceedings of the 17th International Symposium on Symbolic and\n  Numeric Algorithms for Scientific Computing (2015) 371-378", "doi": "10.1109/SYNASC.2015.63", "report-no": null, "categories": "cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel code design is a challenging task especially when addressing\npetascale systems for massive parallel processing (MPP), i.e. parallel\ncomputations on several hundreds of thousands of cores. An in-house\ncomputational fluid dynamics code, developed by our group, was designed for\nsuch high-fidelity runs in order to exhibit excellent scalability values. Basis\nfor this code is an adaptive hierarchical data structure together with an\nefficient communication and (numerical) computation scheme that supports MPP.\nFor a detailled scalability analysis, we performed several experiments on two\nof Germany's national supercomputers up to 140,000 processes. In this paper, we\nwill show the results of those experiments and discuss any bottlenecks that\ncould be observed while solving engineering-based problems such as porous media\nflows or thermal comfort assessments for problem sizes up to several hundred\nbillion degrees of freedom.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 09:03:39 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Frisch", "J\u00e9r\u00f4me", ""], ["Mundani", "Ralf-Peter", ""]]}, {"id": "1807.00638", "submitter": "Ricardo Nobre", "authors": "Ricardo Nobre, Lu\\'is Reis, Jo\\~ao M. P. Cardoso", "title": "Compiler Phase Ordering as an Orthogonal Approach for Reducing Energy\n  Consumption", "comments": "10 pages, 11 figures, 19th Workshop on Compilers for Parallel\n  Computing (CPC 2016), July 6-8 2016, Valladolid, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compiler writers typically focus primarily on the performance of the\ngenerated program binaries when selecting the passes and the order in which\nthey are applied in the standard optimization levels, such as GCC -O3. In some\ndomains, such as embedded systems and High-Performance Computing (HPC), it\nmight be sometimes acceptable to slowdown computations if the energy consumed\ncan be significantly decreased. Embedded systems often rely on a battery and\nbesides energy also have power dissipation limitations, while HPC centers have\na growing concern with electricity and cooling costs. Relying on power policies\nto apply frequency/voltage scaling and/or change the CPU to idle states (e.g.,\nalternate between power levels in bursts) as the main method to reduce energy\nleaves potential for improvement using other orthogonal approaches. In this\nwork we evaluate the impact of compiler pass sequences specialization (also\nknown as compiler phase ordering) as a means to reduce the energy consumed by a\nset of programs/functions when comparing with the use of the standard compiler\nphase orders provided by, e.g., -OX flags. We use our phase selection and\nordering framework to explore the design space in the context of a Clang+LLVM\ncompiler targeting a multicore ARM processor in an ODROID board and a dual x86\ndesktop representative of a node in a Supercomputing center. Our experiments\nwith a set of representative kernels show that there we can reduce energy\nconsumption by up to 24% and that some of these improvements can only be\npartially explained by improvements to execution time. The experiments show\ncases where applications that run faster consume more energy. Additionally, we\nmake an effort to characterize the compiler sequence exploration space in terms\nof their impact on performance and energy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 12:53:01 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Nobre", "Ricardo", ""], ["Reis", "Lu\u00eds", ""], ["Cardoso", "Jo\u00e3o M. P.", ""]]}, {"id": "1807.01014", "submitter": "Jiajian Xiao", "authors": "Jiajian Xiao, Philipp Andelfinger, David Eckhoff, Wentong Cai, Alois\n  Knoll", "title": "A Survey on Agent-based Simulation using Hardware Accelerators", "comments": "Submitted for review to ACM Computing Surveys on 24/05/2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to decelerating gains in single-core CPU performance, computationally\nexpensive simulations are increasingly executed on highly parallel hardware\nplatforms. Agent-based simulations, where simulated entities act with a certain\ndegree of autonomy, frequently provide ample opportunities for parallelisation.\nThus, a vast variety of approaches proposed in the literature demonstrated\nconsiderable performance gains using hardware platforms such as many-core CPUs\nand GPUs, merged CPU-GPU chips as well as FPGAs. Typically, a combination of\ntechniques is required to achieve high performance for a given simulation\nmodel, putting substantial burden on modellers. To the best of our knowledge,\nno systematic overview of techniques for agent-based simulations on hardware\naccelerators has been given in the literature. To close this gap, we provide an\noverview and categorisation of the literature according to the applied\ntechniques. Since at the current state of research, challenges such as the\npartitioning of a model for execution on heterogeneous hardware are still a\nlargely manual process, we sketch directions for future research towards\nautomating the hardware mapping and execution. This survey targets modellers\nseeking an overview of suitable hardware platforms and execution techniques for\na specific simulation model, as well as methodology researchers interested in\npotential research gaps requiring further exploration.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 08:21:07 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Xiao", "Jiajian", ""], ["Andelfinger", "Philipp", ""], ["Eckhoff", "David", ""], ["Cai", "Wentong", ""], ["Knoll", "Alois", ""]]}, {"id": "1807.01340", "submitter": "Peng Wei", "authors": "Jason Cong, Zhenman Fang, Yuchen Hao, Peng Wei, Cody Hao Yu, Chen\n  Zhang, Peipei Zhou", "title": "Best-Effort FPGA Programming: A Few Steps Can Go a Long Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGA-based heterogeneous architectures provide programmers with the ability\nto customize their hardware accelerators for flexible acceleration of many\nworkloads. Nonetheless, such advantages come at the cost of sacrificing\nprogrammability. FPGA vendors and researchers attempt to improve the\nprogrammability through high-level synthesis (HLS) technologies that can\ndirectly generate hardware circuits from high-level language descriptions.\nHowever, reading through recent publications on FPGA designs using HLS, one\noften gets the impression that FPGA programming is still hard in that it leaves\nprogrammers to explore a very large design space with many possible\ncombinations of HLS optimization strategies.\n  In this paper we make two important observations and contributions. First, we\ndemonstrate a rather surprising result: FPGA programming can be made easy by\nfollowing a simple best-effort guideline of five refinement steps using HLS. We\nshow that for a broad class of accelerator benchmarks from MachSuite, the\nproposed best-effort guideline improves the FPGA accelerator performance by\n42-29,030x. Compared to the baseline CPU performance, the FPGA accelerator\nperformance is improved from an average 292.5x slowdown to an average 34.4x\nspeedup. Moreover, we show that the refinement steps in the best-effort\nguideline, consisting of explicit data caching, customized pipelining,\nprocessing element duplication, computation/communication overlapping and\nscratchpad reorganization, correspond well to the best practice guidelines for\nmulticore CPU programming. Although our best-effort guideline may not always\nlead to the optimal solution, it substantially simplifies the FPGA programming\neffort, and will greatly support the wide adoption of FPGA-based acceleration\nby the software programming community.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 18:34:47 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Cong", "Jason", ""], ["Fang", "Zhenman", ""], ["Hao", "Yuchen", ""], ["Wei", "Peng", ""], ["Yu", "Cody Hao", ""], ["Zhang", "Chen", ""], ["Zhou", "Peipei", ""]]}, {"id": "1807.01702", "submitter": "Jung Ho Ahn", "authors": "Wonkyung Jung and Daejin Jung and and Byeongho Kim and Sunjung Lee and\n  Wonjong Rhee and Jung Ho Ahn", "title": "Restructuring Batch Normalization to Accelerate CNN Training", "comments": "13 pages, 8 figures, to appear in SysML 2019, added ResNet-50 results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) has become a core design block of modern\nConvolutional Neural Networks (CNNs). A typical modern CNN has a large number\nof BN layers in its lean and deep architecture. BN requires mean and variance\ncalculations over each mini-batch during training. Therefore, the existing\nmemory access reduction techniques, such as fusing multiple CONV layers, are\nnot effective for accelerating BN due to their inability to optimize mini-batch\nrelated calculations during training. To address this increasingly important\nproblem, we propose to restructure BN layers by first splitting a BN layer into\ntwo sub-layers (fission) and then combining the first sub-layer with its\npreceding CONV layer and the second sub-layer with the following activation and\nCONV layers (fusion). The proposed solution can significantly reduce\nmain-memory accesses while training the latest CNN models, and the experiments\non a chip multiprocessor show that the proposed BN restructuring can improve\nthe performance of DenseNet-121 by 25.7%.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 02:00:19 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 08:27:20 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Jung", "Wonkyung", ""], ["Jung", "Daejin", ""], ["Kim", "and Byeongho", ""], ["Lee", "Sunjung", ""], ["Rhee", "Wonjong", ""], ["Ahn", "Jung Ho", ""]]}, {"id": "1807.01842", "submitter": "Qi Zhang", "authors": "Qi Zhang, Ling Liu, Calton Pu, Qiwei Dou, Liren Wu, Wei Zhou", "title": "A Comparative Study of Containers and Virtual Machines in Big Data\n  Environment", "comments": "Accepted by 2018 IEEE International Conference On Cloud Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Container technique is gaining increasing attention in recent years and has\nbecome an alternative to traditional virtual machines. Some of the primary\nmotivations for the enterprise to adopt the container technology include its\nconvenience to encapsulate and deploy applications, lightweight operations, as\nwell as efficiency and flexibility in resources sharing. However, there still\nlacks an in-depth and systematic comparison study on how big data applications,\nsuch as Spark jobs, perform between a container environment and a virtual\nmachine environment. In this paper, by running various Spark applications with\ndifferent configurations, we evaluate the two environments from many\ninteresting aspects, such as how convenient the execution environment can be\nset up, what are makespans of different workloads running in each setup, how\nefficient the hardware resources, such as CPU and memory, are utilized, and how\nwell each environment can scale. The results show that compared with virtual\nmachines, containers provide a more easy-to-deploy and scalable environment for\nbig data workloads. The research work in this paper can help practitioners and\nresearchers to make more informed decisions on tuning their cloud environment\nand configuring the big data applications, so as to achieve better performance\nand higher resources utilization.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 04:27:35 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Zhang", "Qi", ""], ["Liu", "Ling", ""], ["Pu", "Calton", ""], ["Dou", "Qiwei", ""], ["Wu", "Liren", ""], ["Zhou", "Wei", ""]]}, {"id": "1807.05308", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Ron Brightwell, Alan Edelman, Vijay Gadepally, Hayden\n  Jananthan, Michael Jones, Sam Madden, Peter Michaleas, Hamed Okhravi, Kevin\n  Pedretti, Albert Reuther, Thomas Sterling, Mike Stonebraker", "title": "TabulaROSA: Tabular Operating System Architecture for Massively Parallel\n  Heterogeneous Compute Engines", "comments": "8 pages, 6 figures, accepted at IEEE HPEC 2018", "journal-ref": null, "doi": "10.1109/HPEC.2018.8547577", "report-no": null, "categories": "cs.DC cs.DB cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise in computing hardware choices is driving a reevaluation of operating\nsystems. The traditional role of an operating system controlling the execution\nof its own hardware is evolving toward a model whereby the controlling\nprocessor is distinct from the compute engines that are performing most of the\ncomputations. In this context, an operating system can be viewed as software\nthat brokers and tracks the resources of the compute engines and is akin to a\ndatabase management system. To explore the idea of using a database in an\noperating system role, this work defines key operating system functions in\nterms of rigorous mathematical semantics (associative array algebra) that are\ndirectly translatable into database operations. These operations possess a\nnumber of mathematical properties that are ideal for parallel operating systems\nby guaranteeing correctness over a wide range of parallel operations. The\nresulting operating system equations provide a mathematical specification for a\nTabular Operating System Architecture (TabulaROSA) that can be implemented on\nany platform. Simulations of forking in TabularROSA are performed using an\nassociative array implementation and compared to Linux on a 32,000+ core\nsupercomputer. Using over 262,000 forkers managing over 68,000,000,000\nprocesses, the simulations show that TabulaROSA has the potential to perform\noperating system functions on a massively parallel scale. The TabulaROSA\nsimulations show 20x higher performance as compared to Linux while managing\n2000x more processes in fully searchable tables.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 00:02:55 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kepner", "Jeremy", ""], ["Brightwell", "Ron", ""], ["Edelman", "Alan", ""], ["Gadepally", "Vijay", ""], ["Jananthan", "Hayden", ""], ["Jones", "Michael", ""], ["Madden", "Sam", ""], ["Michaleas", "Peter", ""], ["Okhravi", "Hamed", ""], ["Pedretti", "Kevin", ""], ["Reuther", "Albert", ""], ["Sterling", "Thomas", ""], ["Stonebraker", "Mike", ""]]}, {"id": "1807.05523", "submitter": "Dheryta Jaisinghani", "authors": "Dheryta Jaisinghani, Vinayak Naik, Sanjit K. Kaul, Rajesh Balan, and\n  Sumit Roy", "title": "Improving the Performance of WLANs by Reducing Unnecessary Active Scans", "comments": "14 pages, TMC, NCC, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of excessive and unnecessary active scans in heavily\nutilized WLANs during which low rate probe requests and responses are\nbroadcast. These management frames severely impact the goodput. Our analysis of\ntwo production WLANs reveals that lesser number of non-overlapping channels in\n$2.4$ GHz makes it more prone to the effects of increased probe frames than $5$\nGHz. We find that not only up to $90$% of probe responses carry redundant\ninformation but the probe traffic can be as high as $60$\\% of the management\ntraffic. Furthermore, active scanning severely impacts real-time applications\nat a client as it increases the latency by $91$ times.\n  We present a detailed analysis of the impact of active scans on an individual\nclient and the whole network. We discuss three ways to control the probe\ntraffic in production WLANs -- access point configurations, network planning,\nand client modification. Our proposals for access point configuration are in\nline with current WLAN deployments, better network planning is device agnostic\nin nature, and client modification reduces the average number of probe requests\nper client by up to $50$% without hampering the ongoing WiFi connection.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 10:43:46 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Jaisinghani", "Dheryta", ""], ["Naik", "Vinayak", ""], ["Kaul", "Sanjit K.", ""], ["Balan", "Rajesh", ""], ["Roy", "Sumit", ""]]}, {"id": "1807.06002", "submitter": "Szymon Wasik", "authors": "Szymon Wasik (1 and 2), Maciej Antczak (1), Jan Badura (1), Artur\n  Laskowski (1) ((1) Institute of Computing Science, Poznan University of\n  Technology, (2) Institute of Bioorganic Chemistry, Polish Academy of\n  Sciences)", "title": "Evaluation as a Service architecture and crowdsourced problems solving\n  implemented in Optil.io platform", "comments": "Data Science meets Optimization Workshop, Federated Artificial\n  Intelligence Meeting 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable and trustworthy evaluation of algorithms is a challenging process.\nFirstly, each algorithm has its strengths and weaknesses, and the selection of\ntest instances can significantly influence the assessment process. Secondly,\nthe measured performance of the algorithm highly depends on the test\nenvironment architecture, i.e., CPU model, available memory, cache\nconfiguration, operating system's kernel, and even compilation flags. Finally,\nit is often difficult to compare algorithm with software prepared by other\nresearchers. Evaluation as a Service (EaaS) is a cloud computing architecture\nthat tries to make assessment process more reliable by providing online tools\nand test instances dedicated to the evaluation of algorithms. One of such\nplatforms is Optil.io which gives the possibility to define problems, store\nevaluation data and evaluate solutions submitted by researchers in almost real\ntime. In this paper, we briefly present this platform together with four\nchallenges that were organized with its support.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 12:42:19 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Wasik", "Szymon", "", "1 and 2"], ["Antczak", "Maciej", ""], ["Badura", "Jan", ""], ["Laskowski", "Artur", ""]]}, {"id": "1807.06534", "submitter": "Ralf-Peter Mundani", "authors": "Christoph Ertl (1), J\\'er\\^ome Frisch (2), Ralf-Peter Mundani (1) ((1)\n  Technische Universit\\\"at M\\\"unchen, Munich, Germany, (2) RWTH Aachen\n  University, Aachen, Germany)", "title": "Design and optimisation of an efficient HDF5 I/O kernel for massive\n  parallel fluid flow simulations", "comments": "24 pages, 8 figures", "journal-ref": "Concurrency and Computation: Practice and Experience 29 (2017)", "doi": "10.1002/cpe.4165", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more massive parallel codes running on several hundreds of thousands\nof cores enter the computational science and engineering domain, allowing\nhigh-fidelity computations on up to trillions of unknowns for very detailed\nanalyses of the underlying problems. During such runs, typically gigabytes of\ndata are being produced, hindering both efficient storage and (interactive)\ndata exploration. Here, advanced approaches based on inherently distributed\ndata formats such as HDF5 become necessary in order to avoid long latencies\nwhen storing the data and to support fast (random) access when retrieving the\ndata for visual processing. Avoiding file locking and using collective\nbuffering, write bandwidths to a single file close to the theoretical peak on a\nmodern supercomputing cluster were achieved. The structure of the output file\nsupports a very fast interactive visualisation and introduces additional\nsteering functionality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 17:40:39 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ertl", "Christoph", ""], ["Frisch", "J\u00e9r\u00f4me", ""], ["Mundani", "Ralf-Peter", ""]]}, {"id": "1807.08585", "submitter": "Nicolas Gast", "authors": "Nicolas Gast (POLARIS), Diego Latella (ISTI), Mieke Massink (ISTI)", "title": "A refined mean field approximation of synchronous discrete-time\n  population models", "comments": null, "journal-ref": "Performance Evaluation, Elsevier, 2018", "doi": "10.1016/j.peva.2018.05.002", "report-no": null, "categories": "cs.PF cs.SY math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field approximation is a popular method to study the behaviour of\nstochastic models composed of a large number of interacting objects. When the\nobjects are asynchronous, the mean field approximation of a population model\ncan be expressed as an ordinary differential equation. When the objects are\n(clock-) synchronous the mean field approximation is a discrete time dynamical\nsystem. We focus on the latter.We study the accuracy of mean field\napproximation when this approximation is a discrete-time dynamical system. We\nextend a result that was shown for the continuous time case and we prove that\nexpected performance indicators estimated by mean field approximation are\n$O(1/N)$-accurate. We provide simple expressions to effectively compute the\nasymptotic error of mean field approximation, for finite time-horizon and\nsteady-state, and we use this computed error to propose what we call a\n\\emph{refined} mean field approximation. We show, by using a few numerical\nexamples, that this technique improves the quality of approximation compared to\nthe classical mean field approximation, especially for relatively small\npopulation sizes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 11:39:12 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Gast", "Nicolas", "", "POLARIS"], ["Latella", "Diego", "", "ISTI"], ["Massink", "Mieke", "", "ISTI"]]}, {"id": "1807.08586", "submitter": "Shane Carroll", "authors": "Shane Carroll and Wei-Ming Ling", "title": "A Queuing Model for CPU Functional Unit and Issue Queue Configuration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a superscalar processor, instructions of various types flow through an\nexecution pipeline, traversing hardware resources which are mostly shared among\nmany different instruction types. A notable exception to shared pipeline\nresources is the collection of functional units, the hardware that performs\nspecific computations. In a trade-off of cost versus performance, a pipeline\ndesigner must decide how many of each type of functional unit to place in a\nprocessor's pipeline. In this paper, we model a superscalar processor's issue\nqueue and functional units as a novel queuing network. We treat the issue queue\nas a finite-sized waiting area and the functional units as servers. In addition\nto common queuing problems, customers of the network share the queue but wait\nfor specific servers to become ready (e.g., addition instructions wait for\nadders). Furthermore, the customers in this queue are not necessary ready for\nservice, since instructions may be waiting for operands. In this paper we model\na novel queuing network that provides a solution to the expected queue length\nof each type of instruction. This network and its solution can also be\ngeneralized to other problems, notably other resource-allocation issues that\narise in superscalar pipelines.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 21:59:19 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Carroll", "Shane", ""], ["Ling", "Wei-Ming", ""]]}, {"id": "1807.08673", "submitter": "Iker Perez", "authors": "Iker Perez, Giuliano Casale", "title": "Variational inequalities and mean-field approximations for partially\n  observed systems of queueing networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.PF stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Queueing networks are systems of theoretical interest that find widespread\nuse in the performance evaluation of interconnected resources. In comparison to\ncounterpart models in genetics or mathematical biology, the stochastic (jump)\nprocesses induced by queueing networks have distinctive coupling and\nsynchronization properties. This has prevented the derivation of variational\napproximations for conditional representations of transient dynamics, which\nrely on simplifying independence assumptions. Here, we present a model\naugmentation to a multivariate counting process for interactions across service\nstations, and we enable the variational evaluation of mean-field measures for\npartially-observed multi-class networks. We also show that our framework offers\nan efficient and improved alternative for inference tasks, where existing\nvariational or numerically intensive solutions do not work.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 15:22:21 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 17:08:22 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Perez", "Iker", ""], ["Casale", "Giuliano", ""]]}, {"id": "1807.08804", "submitter": "Erik Cambria", "authors": "Nguyen Ha Tran and Erik Cambria", "title": "GPU-based Commonsense Paradigms Reasoning for Real-Time Query Answering\n  and Multimodal Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize commonsense knowledge bases to address the problem of real- time\nmultimodal analysis. In particular, we focus on the problem of multimodal\nsentiment analysis, which consists in the simultaneous analysis of different\nmodali- ties, e.g., speech and video, for emotion and polarity detection. Our\napproach takes advantages of the massively parallel processing power of modern\nGPUs to enhance the performance of feature extraction from different\nmodalities. In addition, in order to ex- tract important textual features from\nmultimodal sources we generate domain-specific graphs based on commonsense\nknowledge and apply GPU-based graph traversal for fast feature detection. Then,\npowerful ELM classifiers are applied to build the senti- ment analysis model\nbased on the extracted features. We conduct our experiments on the YouTube\ndataset and achieve an accuracy of 78% which outperforms all previous systems.\nIn term of processing speed, our method shows improvements of several orders of\nmagnitude for feature extraction compared to CPU-based counterparts.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 14:46:03 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Tran", "Nguyen Ha", ""], ["Cambria", "Erik", ""]]}, {"id": "1807.09313", "submitter": "Kevin Kremer", "authors": "Lars Nagel, Tim S\\\"u{\\ss}, Kevin Kremer, M. Umar Hameed, Lingfang\n  Zeng, Andr\\'e Brinkmann", "title": "Time-efficient Garbage Collection in SSDs", "comments": "12 pages (excluding references), 13 pages (including references), 13\n  figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SSDs are currently replacing magnetic disks in many application areas. A\nchallenge of the underlying flash technology is that data cannot be updated\nin-place. A block consisting of many pages must be completely erased before a\nsingle page can be rewritten. This victim block can still contain valid pages\nwhich need to be copied to other blocks before erasure. The objective of\ngarbage collection strategies is to minimize write amplification induced by\ncopying valid pages from victim blocks while minimizing the performance\noverhead of the victim selection. Victim selection strategies minimizing write\namplification, like the cost-benefit approach, have linear runtime, while the\nwrite amplifications of time-efficient strategies, like the greedy strategy,\nsignificantly reduce the lifetime of SSDs. In this paper, we propose two\nstrategies which optimize the performance of cost-benefit, while (almost)\npreserving its write amplification. Trace-driven simulations for single- and\nmulti-channel SSDs show that the optimizations help to keep the write\namplification low while improving the runtime by up to 24-times compared to the\noriginal cost-benefit strategy, so that the new strategies can be used in\nmulti-TByte SSDs.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 19:22:25 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Nagel", "Lars", ""], ["S\u00fc\u00df", "Tim", ""], ["Kremer", "Kevin", ""], ["Hameed", "M. Umar", ""], ["Zeng", "Lingfang", ""], ["Brinkmann", "Andr\u00e9", ""]]}, {"id": "1807.10695", "submitter": "Ruolong Lian", "authors": "Jin Hee Kim, Brett Grady, Ruolong Lian, John Brothers, Jason H.\n  Anderson", "title": "FPGA-Based CNN Inference Accelerator Synthesized from Multi-Threaded C\n  Software", "comments": null, "journal-ref": "J. H. Kim, B. Grady, R. Lian, J. Brothers and J. H. Anderson,\n  \"FPGA-based CNN inference accelerator synthesized from multi-threaded C\n  software,\" 2017 30th IEEE International System-on-Chip Conference (SOCC),\n  Munich, 2017, pp. 268-273", "doi": "10.1109/SOCC.2017.8226056", "report-no": null, "categories": "cs.LG cs.AR cs.PF cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep-learning inference accelerator is synthesized from a C-language\nsoftware program parallelized with Pthreads. The software implementation uses\nthe well-known producer/consumer model with parallel threads interconnected by\nFIFO queues. The LegUp high-level synthesis (HLS) tool synthesizes threads into\nparallel FPGA hardware, translating software parallelism into spatial\nparallelism. A complete system is generated where convolution, pooling and\npadding are realized in the synthesized accelerator, with remaining tasks\nexecuting on an embedded ARM processor. The accelerator incorporates reduced\nprecision, and a novel approach for zero-weight-skipping in convolution. On a\nmid-sized Intel Arria 10 SoC FPGA, peak performance on VGG-16 is 138 effective\nGOPS.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 15:46:16 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Kim", "Jin Hee", ""], ["Grady", "Brett", ""], ["Lian", "Ruolong", ""], ["Brothers", "John", ""], ["Anderson", "Jason H.", ""]]}, {"id": "1807.11264", "submitter": "Hatem Hajri", "authors": "Hatem Hajri, Mohamed-Cherif Rahal", "title": "Real Time Lidar and Radar High-Level Fusion for Obstacle Detection and\n  Tracking with evaluation on a ground truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  - Both Lidars and Radars are sensors for obstacle detection. While Lidars are\nvery accurate on obstacles positions and less accurate on their velocities,\nRadars are more precise on obstacles velocities and less precise on their\npositions. Sensor fusion between Lidar and Radar aims at improving obstacle\ndetection using advantages of the two sensors. The present paper proposes a\nreal-time Lidar/Radar data fusion algorithm for obstacle detection and tracking\nbased on the global nearest neighbour standard filter (GNN). This algorithm is\nimplemented and embedded in an automative vehicle as a component generated by a\nreal-time multisensor software. The benefits of data fusion comparing with the\nuse of a single sensor are illustrated through several tracking scenarios (on a\nhighway and on a bend) and using real-time kinematic sensors mounted on the ego\nand tracked vehicles as a ground truth.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 09:58:48 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 09:59:53 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Hajri", "Hatem", ""], ["Rahal", "Mohamed-Cherif", ""]]}, {"id": "1807.11607", "submitter": "Wolfgang Fink", "authors": "Tzyy-Juin Kao and Wolfgang Fink", "title": "Pareto-Optimization Framework for Automated Network-on-Chip Design", "comments": "25 pages (1.5 line spacing), 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of multi-core processors, network-on-chip design has been key\nin addressing network performances, such as bandwidth, power consumption, and\ncommunication delays when dealing with on-chip communication between the\nincreasing number of processor cores. As the numbers of cores increase, network\ndesign becomes more complex. Therefore, there is a critical need in soliciting\ncomputer aid in determining network configurations that afford optimal\nperformance given resources and design constraints. We propose a\nPareto-optimization framework that explores the space of possible network\nconfigurations to determine optimal network latencies, power consumption, and\nthe corresponding link allocations. For a given number of routers, average\nnetwork latency and power consumption as example performance objectives can be\ndisplayed in form of Pareto-optimal fronts, thus not only offering a design\ntool, but also enabling trade-off studies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 23:22:46 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Kao", "Tzyy-Juin", ""], ["Fink", "Wolfgang", ""]]}, {"id": "1807.11824", "submitter": "David Chan", "authors": "David M. Chan, Roshan Rao, Forrest Huang, John F. Canny", "title": "t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data", "comments": "To appear in HPML 2018 High Performance Machine Learning Workshop\n  (Accepted, 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern datasets and models are notoriously difficult to explore and analyze\ndue to their inherent high dimensionality and massive numbers of samples.\nExisting visualization methods which employ dimensionality reduction to two or\nthree dimensions are often inefficient and/or ineffective for these datasets.\nThis paper introduces t-SNE-CUDA, a GPU-accelerated implementation of\nt-distributed Symmetric Neighbor Embedding (t-SNE) for visualizing datasets and\nmodels. t-SNE-CUDA significantly outperforms current implementations with\n50-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for\nthe first time, visualization of the neural network activations on the entire\nImageNet dataset - a feat that was previously computationally intractable. We\nalso demonstrate visualization performance in the NLP domain by visualizing the\nGloVe embedding vectors. From these visualizations, we can draw interesting\nconclusions about using the L2 metric in these embedding spaces. t-SNE-CUDA is\npublicly available athttps://github.com/CannyLab/tsne-cuda\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 14:04:33 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Chan", "David M.", ""], ["Rao", "Roshan", ""], ["Huang", "Forrest", ""], ["Canny", "John F.", ""]]}]