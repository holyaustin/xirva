[{"id": "1508.02055", "submitter": "Prasenjit Karmakar", "authors": "Prasenjit Karmakar and K. Gopinath", "title": "Scalable Reliability Modelling of RAID Storage Subsystems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability modelling of RAID storage systems with its various components\nsuch as RAID controllers, enclosures, expanders, interconnects and disks is\nimportant from a storage system designer's point of view. A model that can\nexpress all the failure characteristics of the whole RAID storage system can be\nused to evaluate design choices, perform cost reliability trade-offs and\nconduct sensitivity analyses. However, including such details makes the\ncomputational models of reliability quickly infeasible.\n  We present a CTMC reliability model for RAID storage systems that scales to\nmuch larger systems than heretofore reported and we try to model all the\ncomponents as accurately as possible. We use several state-space reduction\ntechniques at the user level, such as aggregating all in-series components and\nhierarchical decomposition, to reduce the size of our model. To automate\ncomputation of reliability, we use the PRISM model checker as a CTMC solver\nwhere appropriate. Our modelling techniques using PRISM are more practical (in\nboth time and effort) compared to previously reported Monte-Carlo simulation\ntechniques.\n  Our model for RAID storage systems (that includes, for example, disks,\nexpanders, enclosures) uses Weibull distributions for disks and, where\nappropriate, correlated failure modes for disks, while we use exponential\ndistributions with independent failure modes for all other components. To use\nthe CTMC solver, we approximate the Weibull distribution for a disk using sum\nof exponentials and we confirm that this model gives results that are in\nreasonably good agreement with those from the sequential Monte Carlo simulation\nmethods for RAID disk subsystems reported in literature earlier. Using a\ncombination of scalable techniques, we are able to model and compute\nreliability for fairly large configurations with upto 600 disks using this\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 10:02:39 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Karmakar", "Prasenjit", ""], ["Gopinath", "K.", ""]]}, {"id": "1508.03664", "submitter": "Ioannis Chatzigeorgiou", "authors": "Amjad Saeed Khan and Andrea Tassi and Ioannis Chatzigeorgiou", "title": "Rethinking the Intercept Probability of Random Linear Network Coding", "comments": "IEEE Communications Letters, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT cs.NI cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter considers a network comprising a transmitter, which employs\nrandom linear network coding to encode a message, a legitimate receiver, which\ncan recover the message if it gathers a sufficient number of linearly\nindependent coded packets, and an eavesdropper. Closed-form expressions for the\nprobability of the eavesdropper intercepting enough coded packets to recover\nthe message are derived. Transmission with and without feedback is studied.\nFurthermore, an optimization model that minimizes the intercept probability\nunder delay and reliability constraints is presented. Results validate the\nproposed analysis and quantify the secrecy gain offered by a feedback link from\nthe legitimate receiver.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 21:08:32 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Khan", "Amjad Saeed", ""], ["Tassi", "Andrea", ""], ["Chatzigeorgiou", "Ioannis", ""]]}, {"id": "1508.04465", "submitter": "Cristina Lopes", "authors": "Arthur Valadares, Eugenia Gabrielova, Cristina V. Lopes", "title": "On Designing and Testing Distributed Virtual Environments", "comments": "Wiley Journal on Concurrency and Computation: Practice and\n  Experience, to appear (preprint)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Real-Time (DRT) systems are among the most complex software\nsystems to design, test, maintain and evolve. The existence of components\ndistributed over a network often conflicts with real-time requirements, leading\nto design strategies that depend on domain- and even application-specific\nknowledge. Distributed Virtual Environment (DVE) systems are DRT systems that\nconnect multiple users instantly with each other and with a shared virtual\nspace over a network. DVE systems deviate from traditional DRT systems in the\nimportance of the quality of the end user experience. We present an analysis of\nimportant, but challenging, issues in the design, testing and evaluation of DVE\nsystems through the lens of experiments with a concrete DVE, OpenSimulator. We\nframe our observations within six dimensions of well-known design concerns:\ncorrectness, fault tolerance/prevention, scalability, time sensitivity,\nconsistency, and overhead of distribution. Furthermore, we place our\nexperimental work in a broader historical context, showing that these\nchallenges are intrinsic to DVEs and suggesting lines of future research.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 21:50:23 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Valadares", "Arthur", ""], ["Gabrielova", "Eugenia", ""], ["Lopes", "Cristina V.", ""]]}, {"id": "1508.04537", "submitter": "Jun He", "authors": "Hao Wu, Jun He, Bo Li, Yijian Pei", "title": "Personalized QoS Prediction of Cloud Services via Learning\n  Neighborhood-based Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion of cloud services on the Internet brings new challenges in\nservice discovery and selection. Particularly, the demand for efficient\nquality-of-service (QoS) evaluation is becoming urgently strong. To address\nthis issue, this paper proposes neighborhood-based approach for QoS prediction\nof cloud services by taking advantages of collaborative intelligence. Different\nfrom heuristic collaborative filtering and matrix factorization, we define a\nformal neighborhood-based prediction framework which allows an efficient global\noptimization scheme, and then exploit different baseline estimate component to\nimprove predictive performance. To validate the proposed methods, a large-scale\nQoS-specific dataset which consists of invocation records from 339 service\nusers on 5,825 web services on a world-scale distributed network is used.\nExperimental results demonstrate that the learned neighborhood-based models can\novercome existing difficulties of heuristic collaborative filtering methods and\nachieve superior performance than state-of-the-art prediction methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 06:32:54 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Wu", "Hao", ""], ["He", "Jun", ""], ["Li", "Bo", ""], ["Pei", "Yijian", ""]]}, {"id": "1508.04752", "submitter": "Andre Van Hoorn", "authors": "Andreas Brunnert, Andre van Hoorn, Felix Willnecker, Alexandru Danciu,\n  Wilhelm Hasselbring, Christoph Heger, Nikolas Herbst, Pooyan Jamshidi, Reiner\n  Jung, Joakim von Kistowski, Anne Koziolek, Johannes Kro{\\ss}, Simon Spinner,\n  Christian V\\\"ogele, J\\\"urgen Walter, Alexander Wert", "title": "Performance-oriented DevOps: A Research Agenda", "comments": null, "journal-ref": null, "doi": null, "report-no": "SPEC-RG-2015-01", "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DevOps is a trend towards a tighter integration between development (Dev) and\noperations (Ops) teams. The need for such an integration is driven by the\nrequirement to continuously adapt enterprise applications (EAs) to changes in\nthe business environment. As of today, DevOps concepts have been primarily\nintroduced to ensure a constant flow of features and bug fixes into new\nreleases from a functional perspective. In order to integrate a non-functional\nperspective into these DevOps concepts this report focuses on tools,\nactivities, and processes to ensure one of the most important quality\nattributes of a software system, namely performance.\n  Performance describes system properties concerning its timeliness and use of\nresources. Common metrics are response time, throughput, and resource\nutilization. Performance goals for EAs are typically defined by setting upper\nand/or lower bounds for these metrics and specific business transactions. In\norder to ensure that such performance goals can be met, several activities are\nrequired during development and operation of these systems as well as during\nthe transition from Dev to Ops. Activities during development are typically\nsummarized by the term Software Performance Engineering (SPE), whereas\nactivities during operations are called Application Performance Management\n(APM). SPE and APM were historically tackled independently from each other, but\nthe newly emerging DevOps concepts require and enable a tighter integration\nbetween both activity streams. This report presents existing solutions to\nsupport this integration as well as open research challenges in this area.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 12:39:05 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Brunnert", "Andreas", ""], ["van Hoorn", "Andre", ""], ["Willnecker", "Felix", ""], ["Danciu", "Alexandru", ""], ["Hasselbring", "Wilhelm", ""], ["Heger", "Christoph", ""], ["Herbst", "Nikolas", ""], ["Jamshidi", "Pooyan", ""], ["Jung", "Reiner", ""], ["von Kistowski", "Joakim", ""], ["Koziolek", "Anne", ""], ["Kro\u00df", "Johannes", ""], ["Spinner", "Simon", ""], ["V\u00f6gele", "Christian", ""], ["Walter", "J\u00fcrgen", ""], ["Wert", "Alexander", ""]]}, {"id": "1508.06705", "submitter": "Sekou Remy", "authors": "Jeff Kinnison and Sekou L. Remy", "title": "Using Genetic Algorithms to Benchmark the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": "Sp2015M03", "categories": "cs.DC cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel application of Genetic Algorithms(GAs) to\nquantify the performance of Platform as a Service (PaaS), a cloud service model\nthat plays a critical role in both industry and academia. While Cloud\nbenchmarks are not new, in this novel concept, the authors use a GA to take\nadvantage of the elasticity in Cloud services in a graceful manner that was not\npreviously possible. Using Google App Engine, Heroku, and Python Anywhere with\nthree distinct classes of client computers running our GA codebase, we\nquantified the completion time for application of the GA to search for the\nparameters of controllers for dynamical systems. Our results show statistically\nsignificant differences in PaaS performance by vendor, and also that the\nperformance of the PaaS performance is dependent upon the client that uses it.\nResults also show the effectiveness of our GA in determining the level of\nservice of PaaS providers, and for determining if the level of service of one\nPaaS vendor is repeatable with another. Such a concept could then increase the\nappeal of PaaS Cloud services by making them more financially appealing.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 03:09:08 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Kinnison", "Jeff", ""], ["Remy", "Sekou L.", ""]]}, {"id": "1508.06830", "submitter": "Antonio Filgueras", "authors": "Daniel Jim\\'enez-Gonz\\'alez, Carlos \\'Alvarez, Antonio Filgueras,\n  Xavier Martorell, Jan Langer, Juanjo Noguera, Kees Vissers", "title": "Coarse-Grain Performance Estimator for Heterogeneous Parallel Computing\n  Architectures like Zynq All-Programmable SoC", "comments": "Presented at Second International Workshop on FPGAs for Software\n  Programmers (FSP 2015) (arXiv:1508.06320)", "journal-ref": null, "doi": null, "report-no": "FSP/2015/07", "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous computing is emerging as a mandatory requirement for\npower-efficient system design. With this aim, modern heterogeneous platforms\nlike Zynq All-Programmable SoC, that integrates ARM-based SMP and programmable\nlogic, have been designed. However, those platforms introduce large design\ncycles consisting on hardware/software partitioning, decisions on granularity\nand number of hardware accelerators, hardware/software integration, bitstream\ngeneration, etc.\n  This paper presents a performance parallel heterogeneous estimation for\nsystems where hardware/software co-design and run-time heterogeneous task\nscheduling are key. The results show that the programmer can quickly decide,\nbased only on her/his OmpSs (OpenMP + extensions) application, which is the\nco-design that achieves nearly optimal heterogeneous parallel performance,\nbased on the methodology presented and considering only synthesis estimation\nresults. The methodology presented reduces the programmer co-design decision\nfrom hours to minutes and shows high potential on hardware/software\nheterogeneous parallel performance estimation on the Zynq All-Programmable SoC.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 12:35:00 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Jim\u00e9nez-Gonz\u00e1lez", "Daniel", ""], ["\u00c1lvarez", "Carlos", ""], ["Filgueras", "Antonio", ""], ["Martorell", "Xavier", ""], ["Langer", "Jan", ""], ["Noguera", "Juanjo", ""], ["Vissers", "Kees", ""]]}, {"id": "1508.07126", "submitter": "Nicholas Doyle", "authors": "Lesley Shannon, Eric Matthews, Nicholas Doyle, Alexandra Fedorova", "title": "Performance monitoring for multicore embedded computing systems on FPGAs", "comments": "Presented at Second International Workshop on FPGAs for Software\n  Programmers (FSP 2015) (arXiv:1508.06320)", "journal-ref": null, "doi": null, "report-no": "FSP/2015/13", "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing modern embedded computing systems, most software programmers\nchoose to use multicore processors, possibly in combination with\ngeneral-purpose graphics processing units (GPGPUs) and/or hardware\naccelerators. They also often use an embedded Linux O/S and run\nmulti-application workloads that may even be multi-threaded. Modern FPGAs are\nlarge enough to combine multicore hard/soft processors with multiple hardware\naccelerators as custom compute units, enabling entire embedded compute systems\nto be implemented on a single FPGA. Furthermore, the large FPGA vendors also\nsupport embedded Linux kernels for both their soft and embedded processors.\nWhen combined with high-level synthesis to generate hardware accelerators using\na C-to-gates flows, the necessary primitives for a framework that can enable\nsoftware designers to use FPGAs as their custom compute platform now exist.\nHowever, in order to ensure that computing resources are integrated and shared\neffectively, software developers need to be able to monitor and debug the\nruntime performance of the applications in their workload. This paper describes\nABACUS, a performance-monitoring framework that can be used to debug the\nexecution behaviours and interactions of multi-application workloads on\nmulticore systems. We also discuss how this framework is extensible for use\nwith hardware accelerators in heterogeneous systems.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 08:41:38 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Shannon", "Lesley", ""], ["Matthews", "Eric", ""], ["Doyle", "Nicholas", ""], ["Fedorova", "Alexandra", ""]]}, {"id": "1508.07740", "submitter": "Karel De Vogeleer", "authors": "Karel De Vogeleer, Gerard Memmi, Pierre Jouvelot", "title": "Parameter Sensitivity Analysis of the Energy/Frequency Convexity Rule\n  for Nanometer-scale Application Processors", "comments": "In submission to the Special Issue on Energy Efficient Multi-Core and\n  Many-Core Systems (The Elsevier Journal of Parallel and Distributed\n  Computing)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both theoretical and experimental evidence are presented in this work in\norder to validate the existence of an Energy/Frequency Convexity Rule, which\nrelates energy consumption and microprocessor frequency for nanometer-scale\nmicroprocessors. Data gathered during several month-long experimental\nacquisition campaigns, supported by several independent publications, suggest\nthat energy consumed is indeed depending on the microprocessor's clock\nfrequency, and, more interestingly, the curve exhibits a clear minimum over the\nprocessor's frequency range. An analytical model for this behavior is presented\nand motivated, which fits well with the experimental data. A parameter\nsensitivity analysis shows how parameters affect the energy minimum in the\nclock frequency space. The conditions are discussed under which this convexity\nrule can be exploited, and when other methods are more effective, with the aim\nof improving the computer system's energy management efficiency. We show that\nthe power requirements of the computer system, besides the microprocessor, and\nthe overhead affect the location of the energy minimum the most. The\nsensitivity analysis of the Energy/Frequency Convexity Rule puts forward a\nnumber of simple guidelines especially for by low-power systems, such as\nbattery-powered and embedded systems, and less likely by high-performance\ncomputer systems.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 09:41:46 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["De Vogeleer", "Karel", ""], ["Memmi", "Gerard", ""], ["Jouvelot", "Pierre", ""]]}]