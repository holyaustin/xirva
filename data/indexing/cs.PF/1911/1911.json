[{"id": "1911.00119", "submitter": "Chengcheng Wan", "authors": "Chengcheng Wan, Muhammad Santriaji, Eri Rogers, Henry Hoffmann,\n  Michael Maire, Shan Lu", "title": "ALERT: Accurate Learning for Energy and Timeliness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of software applications incorporate runtime Deep Neural\nNetworks (DNNs) to process sensor data and return inference results to humans.\nEffective deployment of DNNs in these interactive scenarios requires meeting\nlatency and accuracy constraints while minimizing energy, a problem exacerbated\nby common system dynamics. Prior approaches handle dynamics through either (1)\nsystem-oblivious DNN adaptation, which adjusts DNN latency/accuracy tradeoffs,\nor (2) application-oblivious system adaptation, which adjusts resources to\nchange latency/energy tradeoffs. In contrast, this paper improves on the\nstate-of-the-art by coordinating application- and system-level adaptation.\nALERT, our runtime scheduler, uses a probabilistic model to detect\nenvironmental volatility and then simultaneously select both a DNN and a system\nresource configuration to meet latency, accuracy, and energy constraints. We\nevaluate ALERT on CPU and GPU platforms for image and speech tasks in dynamic\nenvironments. ALERT's holistic approach achieves more than 13% energy\nreduction, and 27% error reduction over prior approaches that adapt solely at\nthe application or system level. Furthermore, ALERT incurs only 3% more energy\nconsumption and 2% higher DNN-inference error than an oracle scheme with\nperfect application and system knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 21:40:42 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 12:33:53 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Wan", "Chengcheng", ""], ["Santriaji", "Muhammad", ""], ["Rogers", "Eri", ""], ["Hoffmann", "Henry", ""], ["Maire", "Michael", ""], ["Lu", "Shan", ""]]}, {"id": "1911.00329", "submitter": "Suayb Arslan", "authors": "Suayb S. Arslan, James Peng and Turguy Goker", "title": "A Data-Assisted Reliability Model for Carrier-Assisted Cold Data Storage\n  Systems", "comments": "14 pages, 8 figures, accepted to Elsevier Reliability and Safety\n  Journal, 2019 (unedited)", "journal-ref": null, "doi": "10.1016/j.ress.2019.106708", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cold data storage systems are used to allow long term digital preservation\nfor institutions' archives. The common functionality among cold and warm/hot\ndata storage is that the data is stored on some physical medium for read-back\nat a later time. However in cold storage, write and read operations are not\nnecessarily done in the same exact geographical location. Hence, a third party\nassistance is typically utilized to bring together the medium and the drive. On\nthe other hand, the reliability modeling of such a decomposed system poses few\nchallenges that do not necessarily exist in other warm/hot storage alternatives\nsuch as fault detection and absence of the carrier, all totaling up to the data\nunavailability issues. In this paper, we propose a generalized non-homogenous\nMarkov model that encompasses the aging of the carriers in order to address the\nrequirements of today's cold data storage systems in which the data is encoded\nand spread across multiple nodes for the long-term data retention. We have\nderived useful lower/upper bounds on the overall system availability.\nFurthermore, the collected field data is used to estimate parameters of a\nWeibull distribution to accurately predict the lifetime of the carriers in an\nexample scale-out setting. In this study, we numerically demonstrate the\nsignificance of carriers' presence and the key role that their timely\nmaintenance plays on the long-term reliability and availability of the stored\ncontent.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 12:25:51 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Arslan", "Suayb S.", ""], ["Peng", "James", ""], ["Goker", "Turguy", ""]]}, {"id": "1911.00527", "submitter": "Konstantinos Drossos", "authors": "Niccol\\'o Nicodemo and Gaurav Naithani and Konstantinos Drossos and\n  Tuomas Virtanen and Roberto Saletti", "title": "Memory Requirement Reduction of Deep Neural Networks Using Low-bit\n  Quantization of Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.PF cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective employment of deep neural networks (DNNs) in mobile devices and\nembedded systems is hampered by requirements for memory and computational\npower. This paper presents a non-uniform quantization approach which allows for\ndynamic quantization of DNN parameters for different layers and within the same\nlayer. A virtual bit shift (VBS) scheme is also proposed to improve the\naccuracy of the proposed scheme. Our method reduces the memory requirements,\npreserving the performance of the network. The performance of our method is\nvalidated in a speech enhancement application, where a fully connected DNN is\nused to predict the clean speech spectrum from the input noisy speech spectrum.\nA DNN is optimized and its memory footprint and performance are evaluated using\nthe short-time objective intelligibility, STOI, metric. The application of the\nlow-bit quantization allows a 50% reduction of the DNN memory footprint while\nthe STOI performance drops only by 2.7%.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 18:03:12 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Nicodemo", "Niccol\u00f3", ""], ["Naithani", "Gaurav", ""], ["Drossos", "Konstantinos", ""], ["Virtanen", "Tuomas", ""], ["Saletti", "Roberto", ""]]}, {"id": "1911.01258", "submitter": "Reza Yazdani Aminabadi", "authors": "Reza Yazdani, Olatunji Ruwase, Minjia Zhang, Yuxiong He, Jose-Maria\n  Arnau, Antonio Gonzalez", "title": "LSTM-Sharp: An Adaptable, Energy-Efficient Hardware Accelerator for Long\n  Short-Term Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of LSTM neural networks for popular tasks such as Automatic\nSpeech Recognition has fostered an increasing interest in LSTM inference\nacceleration. Due to the recurrent nature and data dependencies of LSTM\ncomputations, designing a customized architecture specifically tailored to its\ncomputation pattern is crucial for efficiency. Since LSTMs are used for a\nvariety of tasks, generalizing this efficiency to diverse configurations, i.e.,\nadaptiveness, is another key feature of these accelerators. In this work, we\nfirst show the problem of low resource-utilization and adaptiveness for the\nstate-of-the-art LSTM implementations on GPU, FPGA and ASIC architectures. To\nsolve these issues, we propose an intelligent tiled-based dispatching mechanism\nthat efficiently handles the data dependencies and increases the adaptiveness\nof LSTM computation. To do so, we propose LSTM-Sharp as a hardware accelerator,\nwhich pipelines LSTM computation using an effective scheduling scheme to hide\nmost of the dependent serialization. Furthermore, LSTM-Sharp employs dynamic\nreconfigurable architecture to adapt to the model's characteristics. LSTM-Sharp\nachieves 1.5x, 2.86x, and 82x speedups on average over the state-of-the-art\nASIC, FPGA, and GPU implementations respectively, for different LSTM models and\nresource budgets. Furthermore, we provide significant energy-reduction with\nrespect to the previous solutions, due to the low power dissipation of\nLSTM-Sharp (383 GFLOPs/Watt).\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:51:27 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Yazdani", "Reza", ""], ["Ruwase", "Olatunji", ""], ["Zhang", "Minjia", ""], ["He", "Yuxiong", ""], ["Arnau", "Jose-Maria", ""], ["Gonzalez", "Antonio", ""]]}, {"id": "1911.02373", "submitter": "Davood Mohajerani", "authors": "Alexander Brandt, Davood Mohajerani, Marc Moreno Maza, Jeeva Paudel,\n  Linxiao Wang", "title": "KLARAPTOR: A Tool for Dynamically Finding Optimal Kernel Launch\n  Parameters Targeting CUDA Programs", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1906.00142", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present KLARAPTOR (Kernel LAunch parameters RAtional Program\nestimaTOR), a new tool built on top of the LLVM Pass Framework and NVIDIA CUPTI\nAPI to dynamically determine the optimal values of kernel launch parameters of\na CUDA program P. To be precise, we describe a novel technique to statically\nbuild (at the compile time of P) a so-called rational program R. Using a\nperformance prediction model, and knowing particular data and hardware\nparameters of P at runtime, the program R can automatically and dynamically\ndetermine the values of launch parameters of P that will yield optimal\nperformance. Our technique can be applied to parallel programs in general, as\nwell as to generic performance prediction models which account for program and\nhardware parameters. We are particularly interested in programs targeting\nmanycore accelerators. We have implemented and successfully tested our\ntechnique in the context of GPU kernels written in CUDA using the MWP-CWP\nperformance prediction model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 00:24:56 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Brandt", "Alexander", ""], ["Mohajerani", "Davood", ""], ["Maza", "Marc Moreno", ""], ["Paudel", "Jeeva", ""], ["Wang", "Linxiao", ""]]}, {"id": "1911.02430", "submitter": "Frederic Giroudot", "authors": "Frederic Giroudot, Ahlem Mifdaoui", "title": "Graph-based Approach for Buffer-aware Timing Analysis of Heterogeneous\n  Wormhole NoCs under Bursty Traffic", "comments": "21 pages, 22 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of worst-case timing analysis of\nheterogeneous wormhole NoCs, i.e., routers with different buffer sizes and\ntransmission speeds, when consecutive packet queuing (CPQ) occurs. The latter\nmeans that there are several consecutive packets of one flow queuing in the\nnetwork. This scenario happens in the case of bursty traffic but also for\nnon-schedulable traffic. Conducting such an analysis is known to be a\nchallenging issue due to the sophisticated congestion patterns when enabling\nbackpressure mechanisms. We tackle this problem through extending the\napplicability domain of our previous work for computing maximum delay bounds\nusing Network Calculus, called Buffer-aware worst-case Timing Analysis (BATA).\nWe propose a new Graph-based approach to improve the analysis of indirect\nblocking due to backpressure, while capturing the CPQ effect and keeping the\ninformation about dependencies between flows. Furthermore, the introduced\napproach improves the computation of indirect-blocking delay bounds in terms of\ncomplexity and ensures the safety of these bounds even for non-schedulable\ntraffic. We provide further insights into the tightness and complexity issues\nof worst-case delay bounds yielded by the extended BATA with the Graph-based\napproach, denoted G-BATA. Our assessments show that the complexity has\ndecreased by up to 100 times while offering an average tightness ratio of 71%,\nwith reference to the basic BATA. Finally, we evaluate the yielded improvements\nwith G-BATA for a realistic use case against a recent state-of-the-art\napproach. This evaluation shows the applicability of G-BATA under more general\nassumptions and the impact of such a feature on the tightness and computation\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 15:14:25 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Giroudot", "Frederic", ""], ["Mifdaoui", "Ahlem", ""]]}, {"id": "1911.02549", "submitter": "Cody Coleman", "authors": "Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson,\n  Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe,\n  Mark Charlebois, William Chou, Ramesh Chukka, Cody Coleman, Sam Davis, Pan\n  Deng, Greg Diamos, Jared Duke, Dave Fick, J. Scott Gardner, Itay Hubara,\n  Sachin Idgunji, Thomas B. Jablin, Jeff Jiao, Tom St. John, Pankaj Kanwar,\n  David Lee, Jeffery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius\n  Micikevicius, Colin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath\n  Rajan, Dilip Sequeira, Ashish Sirasao, Fei Sun, Hanlin Tang, Michael Thomson,\n  Frank Wei, Ephrem Wu, Lingjie Xu, Koichi Yamada, Bing Yu, George Yuan, Aaron\n  Zhong, Peizhao Zhang, Yuchen Zhou", "title": "MLPerf Inference Benchmark", "comments": "ISCA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning (ML) hardware and software system demand is burgeoning.\nDriven by ML applications, the number of different ML inference systems has\nexploded. Over 100 organizations are building ML inference chips, and the\nsystems that incorporate existing models span at least three orders of\nmagnitude in power consumption and five orders of magnitude in performance;\nthey range from embedded devices to data-center solutions. Fueling the hardware\nare a dozen or more software frameworks and libraries. The myriad combinations\nof ML hardware and ML software make assessing ML-system performance in an\narchitecture-neutral, representative, and reproducible manner challenging.\nThere is a clear need for industry-wide standard ML benchmarking and evaluation\ncriteria. MLPerf Inference answers that call. In this paper, we present our\nbenchmarking method for evaluating ML inference systems. Driven by more than 30\norganizations as well as more than 200 ML engineers and practitioners, MLPerf\nprescribes a set of rules and best practices to ensure comparability across\nsystems with wildly differing architectures. The first call for submissions\ngarnered more than 600 reproducible inference-performance measurements from 14\norganizations, representing over 30 systems that showcase a wide range of\ncapabilities. The submissions attest to the benchmark's flexibility and\nadaptability.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 18:43:10 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 23:40:20 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Reddi", "Vijay Janapa", ""], ["Cheng", "Christine", ""], ["Kanter", "David", ""], ["Mattson", "Peter", ""], ["Schmuelling", "Guenther", ""], ["Wu", "Carole-Jean", ""], ["Anderson", "Brian", ""], ["Breughe", "Maximilien", ""], ["Charlebois", "Mark", ""], ["Chou", "William", ""], ["Chukka", "Ramesh", ""], ["Coleman", "Cody", ""], ["Davis", "Sam", ""], ["Deng", "Pan", ""], ["Diamos", "Greg", ""], ["Duke", "Jared", ""], ["Fick", "Dave", ""], ["Gardner", "J. Scott", ""], ["Hubara", "Itay", ""], ["Idgunji", "Sachin", ""], ["Jablin", "Thomas B.", ""], ["Jiao", "Jeff", ""], ["John", "Tom St.", ""], ["Kanwar", "Pankaj", ""], ["Lee", "David", ""], ["Liao", "Jeffery", ""], ["Lokhmotov", "Anton", ""], ["Massa", "Francisco", ""], ["Meng", "Peng", ""], ["Micikevicius", "Paulius", ""], ["Osborne", "Colin", ""], ["Pekhimenko", "Gennady", ""], ["Rajan", "Arun Tejusve Raghunath", ""], ["Sequeira", "Dilip", ""], ["Sirasao", "Ashish", ""], ["Sun", "Fei", ""], ["Tang", "Hanlin", ""], ["Thomson", "Michael", ""], ["Wei", "Frank", ""], ["Wu", "Ephrem", ""], ["Xu", "Lingjie", ""], ["Yamada", "Koichi", ""], ["Yu", "Bing", ""], ["Yuan", "George", ""], ["Zhong", "Aaron", ""], ["Zhang", "Peizhao", ""], ["Zhou", "Yuchen", ""]]}, {"id": "1911.02987", "submitter": "Zihan Jiang", "authors": "Zihan Jiang, Jiansong Li, Jiangfeng Zhan", "title": "The Pitfall of Evaluating Performance on Emerging AI Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, domain-specific hardware has brought significant performance\nimprovements in deep learning (DL). Both industry and academia only focus on\nthroughput when evaluating these AI accelerators, which usually are custom\nASICs deployed in datacenter to speed up the inference phase of DL workloads.\nPursuing higher hardware throughput such as OPS (Operation Per Second) using\nvarious optimizations seems to be their main design target. However, they\nignore the importance of accuracy in the DL nature. Motivated by this, this\npaper argue that a single throughput metric can not comprehensively reflect the\nreal-world performance of AI accelerators. To reveal this pitfall, we evaluates\nseveral frequently-used optimizations on a typical AI accelerator and\nquantifies their impact on accuracy and throughout under representative DL\ninference workloads. Based on our experimental results, we find that some\noptimizations cause significant loss on accuracy in some workloads, although it\ncan improves the throughout. Furthermore, our results show the importance of\nend-to-end evaluation in DL.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 02:13:21 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Jiang", "Zihan", ""], ["Li", "Jiansong", ""], ["Zhan", "Jiangfeng", ""]]}, {"id": "1911.03011", "submitter": "Qinbin Li", "authors": "Qinbin Li, Zeyi Wen, Bingsheng He", "title": "Adaptive Kernel Value Caching for SVM Training", "comments": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2944562", "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVMs) can solve structured multi-output learning\nproblems such as multi-label classification, multiclass classification and\nvector regression. SVM training is expensive especially for large and high\ndimensional datasets. The bottleneck of the SVM training often lies in the\nkernel value computation. In many real-world problems, the same kernel values\nare used in many iterations during the training, which makes the caching of\nkernel values potentially useful. The majority of the existing studies simply\nadopt the LRU (least recently used) replacement strategy for caching kernel\nvalues. However, as we analyze in this paper, the LRU strategy generally\nachieves high hit ratio near the final stage of the training, but does not work\nwell in the whole training process. Therefore, we propose a new caching\nstrategy called EFU (less frequently used) which replaces the less frequently\nused kernel values that enhances LFU (least frequently used). Our experimental\nresults show that EFU often has 20\\% higher hit ratio than LRU in the training\nwith the Gaussian kernel. To further optimize the strategy, we propose a\ncaching strategy called HCST (hybrid caching for the SVM training), which has a\nnovel mechanism to automatically adapt the better caching strategy in the\ndifferent stages of the training. We have integrated the caching strategy into\nThunderSVM, a recent SVM library on many-core processors. Our experiments show\nthat HCST adaptively achieves high hit ratios with little runtime overhead\namong different problems including multi-label classification, multiclass\nclassification and regression problems. Compared with other existing caching\nstrategies, HCST achieves 20\\% more reduction in training time on average.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:06:42 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Li", "Qinbin", ""], ["Wen", "Zeyi", ""], ["He", "Bingsheng", ""]]}, {"id": "1911.03062", "submitter": "Christos Kotsalos", "authors": "Christos Kotsalos, Jonas Latt, Joel Beny and Bastien Chopard", "title": "Digital Blood in Massively Parallel CPU/GPU Systems for the Study of\n  Platelet Transport", "comments": null, "journal-ref": "Royal Society - Interface Focus (2020)", "doi": "10.1098/rsfs.2019.0116", "report-no": null, "categories": "physics.comp-ph cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a highly versatile computational framework for the simulation of\ncellular blood flow focusing on extreme performance without compromising\naccuracy or complexity. The tool couples the lattice Boltzmann solver Palabos\nfor the simulation of the blood plasma, a novel finite element method (FEM)\nsolver for the resolution of the deformable blood cells, and an immersed\nboundary method for the coupling of the two phases. The design of the tool\nsupports hybrid CPU-GPU executions (fluid, fluid-solid interaction on CPUs, the\nFEM solver on GPUs), and is non-intrusive, as each of the three components can\nbe replaced in a modular way. The FEM-based kernel for solid dynamics\noutperforms other FEM solvers and its performance is comparable to the\nstate-of-the-art mass-spring systems. We perform an exhaustive performance\nanalysis on Piz Daint at the Swiss National Supercomputing Centre and provide\ncase studies focused on platelet transport. The tests show that this versatile\nframework combines unprecedented accuracy with massive performance, rendering\nit suitable for the upcoming exascale architectures.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 05:52:06 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Kotsalos", "Christos", ""], ["Latt", "Jonas", ""], ["Beny", "Joel", ""], ["Chopard", "Bastien", ""]]}, {"id": "1911.03282", "submitter": "Andreas Abel", "authors": "Andreas Abel and Jan Reineke", "title": "nanoBench: A Low-Overhead Tool for Running Microbenchmarks on x86\n  Systems", "comments": null, "journal-ref": null, "doi": "10.1109/ISPASS48437.2020.00014", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present nanoBench, a tool for evaluating small microbenchmarks using\nhardware performance counters on Intel and AMD x86 systems. Most existing tools\nand libraries are intended to either benchmark entire programs, or program\nsegments in the context of their execution within a larger program. In\ncontrast, nanoBench is specifically designed to evaluate small, isolated pieces\nof code. Such code is common in microbenchmark-based hardware analysis\ntechniques.\n  Unlike previous tools, nanoBench can execute microbenchmarks directly in\nkernel space. This allows to benchmark privileged instructions, and it enables\nmore accurate measurements. The reading of the performance counters is\nimplemented with minimal overhead avoiding functions calls and branches. As a\nconsequence, nanoBench is precise enough to measure individual memory accesses.\n  We illustrate the utility of nanoBench at the hand of two case studies.\nFirst, we briefly discuss how nanoBench has been used to determine the latency,\nthroughput, and port usage of more than 13,000 instruction variants on recent\nx86 processors. Second, we show how to generate microbenchmarks to precisely\ncharacterize the cache architectures of eleven Intel Core microarchitectures.\nThis includes the most comprehensive analysis of the employed cache replacement\npolicies to date.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:31:18 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 23:24:28 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Abel", "Andreas", ""], ["Reineke", "Jan", ""]]}, {"id": "1911.03456", "submitter": "Gabriele D'Angelo", "authors": "Moreno Marzolla, Gabriele D'Angelo", "title": "Parallel Data Distribution Management on Shared-Memory Multiprocessors", "comments": "arXiv admin note: text overlap with arXiv:1703.06680", "journal-ref": "ACM Transactions on Modeling and Computer Simulation (TOMACS),\n  Vol. 30, No. 1, Article 5. ACM, February 2020. ISSN: 1049-3301", "doi": "10.1145/3369759", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying intersections between two sets of d-dimensional\naxis-parallel rectangles appears frequently in the context of agent-based\nsimulation studies. For this reason, the High Level Architecture (HLA)\nspecification -- a standard framework for interoperability among simulators --\nincludes a Data Distribution Management (DDM) service whose responsibility is\nto report all intersections between a set of subscription and update regions.\nThe algorithms at the core of the DDM service are CPU-intensive, and could\ngreatly benefit from the large computing power of modern multi-core processors.\nIn this paper we propose two parallel solutions to the DDM problem that can\noperate effectively on shared-memory multiprocessors. The first solution is\nbased on a data structure (the Interval Tree) that allows concurrent\ncomputation of intersections between subscription and update regions. The\nsecond solution is based on a novel parallel extension of the Sort Based\nMatching algorithm, whose sequential version is considered among the most\nefficient solutions to the DDM problem. Extensive experimental evaluation of\nthe proposed algorithms confirm their effectiveness on taking advantage of\nmultiple execution units in a shared-memory architecture.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 13:18:14 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 10:24:12 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Marzolla", "Moreno", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "1911.04200", "submitter": "Maciej Besta", "authors": "Maciej Besta, Raghavendra Kanakagiri, Harun Mustafa, Mikhail\n  Karasikov, Gunnar R\\\"atsch, Torsten Hoefler, Edgar Solomonik", "title": "Communication-Efficient Jaccard Similarity for High-Performance\n  Distributed Genome Comparisons", "comments": null, "journal-ref": "Proceedings of the 34st IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS'20), 2020", "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.PF q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jaccard similarity index is an important measure of the overlap of two\nsets, widely used in machine learning, computational genomics, information\nretrieval, and many other areas. We design and implement SimilarityAtScale, the\nfirst communication-efficient distributed algorithm for computing the Jaccard\nsimilarity among pairs of large datasets. Our algorithm provides an efficient\nencoding of this problem into a multiplication of sparse matrices. Both the\nencoding and sparse matrix product are performed in a way that minimizes data\nmovement in terms of communication and synchronization costs. We apply our\nalgorithm to obtain similarity among all pairs of a set of large samples of\ngenomes. This task is a key part of modern metagenomics analysis and an\nevergrowing need due to the increasing availability of high-throughput DNA\nsequencing data. The resulting scheme is the first to enable accurate Jaccard\ndistance derivations for massive datasets, using largescale distributed-memory\nsystems. We package our routines in a tool, called GenomeAtScale, that combines\nthe proposed algorithm with tools for processing input sequences. Our\nevaluation on real data illustrates that one can use GenomeAtScale to\neffectively employ tens of thousands of processors to reach new frontiers in\nlarge-scale genomic and metagenomic analysis. While GenomeAtScale can be used\nto foster DNA research, the more general underlying SimilarityAtScale algorithm\nmay be used for high-performance distributed similarity computations in other\ndata analytics application domains.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 11:57:54 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 13:59:00 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 15:57:32 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Besta", "Maciej", ""], ["Kanakagiri", "Raghavendra", ""], ["Mustafa", "Harun", ""], ["Karasikov", "Mikhail", ""], ["R\u00e4tsch", "Gunnar", ""], ["Hoefler", "Torsten", ""], ["Solomonik", "Edgar", ""]]}, {"id": "1911.04610", "submitter": "Wotao Yin", "authors": "Lei Guan, Wotao Yin, Dongsheng Li and Xicheng Lu", "title": "XPipe: Efficient Pipeline Model Parallelism for Multi-GPU DNN Training", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose XPipe, an efficient asynchronous pipeline model parallelism\napproach for multi-GPU DNN training. XPipe is designed to use multiple GPUs to\nconcurrently and continuously train different parts of a DNN model. To improve\nGPU utilization and achieve high throughput, it splits a mini-batch into a set\nof micro-batches. It allows the overlapping of the pipelines of multiple\nmicro-batches, including those belonging to different mini-batches. Most\nimportantly, the novel weight prediction strategy adopted by XPipe enables it\nto effectively address the weight inconsistency and staleness issues incurred\nby the asynchronous pipeline parallelism. As a result, XPipe incorporates the\nadvantages of both synchronous and asynchronous pipeline model parallelism\napproaches. Concretely, it can achieve very comparable (even slightly better)\nmodel accuracy as its synchronous counterpart while obtaining higher throughput\nthan it. Experimental results show that XPipe outperforms other\nstate-of-the-art synchronous and asynchronous model parallelism approaches.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:13:54 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 02:42:57 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 12:53:41 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Guan", "Lei", ""], ["Yin", "Wotao", ""], ["Li", "Dongsheng", ""], ["Lu", "Xicheng", ""]]}, {"id": "1911.04650", "submitter": "Wumo Yan", "authors": "Zhuojin Li, Wumo Yan, Marco Paolieri, Leana Golubchik", "title": "Throughput Prediction of Asynchronous SGD in TensorFlow", "comments": null, "journal-ref": null, "doi": "10.1145/3358960.3379141", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning frameworks can train neural networks using multiple\nnodes in parallel, each computing parameter updates with stochastic gradient\ndescent (SGD) and sharing them asynchronously through a central parameter\nserver. Due to communication overhead and bottlenecks, the total throughput of\nSGD updates in a cluster scales sublinearly, saturating as the number of nodes\nincreases. In this paper, we present a solution to predicting training\nthroughput from profiling traces collected from a single-node configuration.\nOur approach is able to model the interaction of multiple nodes and the\nscheduling of concurrent transmissions between the parameter server and each\nnode. By accounting for the dependencies between received parts and pending\ncomputations, we predict overlaps between computation and communication and\ngenerate synthetic execution traces for configurations with multiple nodes. We\nvalidate our approach on TensorFlow training jobs for popular image\nclassification neural networks, on AWS and on our in-house cluster, using nodes\nequipped with GPUs or only with CPUs. We also investigate the effects of data\ntransmission policies used in TensorFlow and the accuracy of our approach when\ncombined with optimizations of the transmission schedule.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 03:17:20 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 06:32:31 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Li", "Zhuojin", ""], ["Yan", "Wumo", ""], ["Paolieri", "Marco", ""], ["Golubchik", "Leana", ""]]}, {"id": "1911.04946", "submitter": "Zheng Wang", "authors": "Vicent Sanz Marco, Ben Taylor, Zheng Wang, Yehia Elkhatib", "title": "Optimizing Deep Learning Inference on Embedded Systems Through Adaptive\n  Model Selection", "comments": "Accepted to be published at ACM TECS. arXiv admin note: substantial\n  text overlap with arXiv:1805.04252", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks ( DNNs ) are becoming a key enabling technology for many\napplication domains. However, on-device inference on battery-powered,\nresource-constrained embedding systems is often infeasible due to prohibitively\nlong inferencing time and resource requirements of many DNNs. Offloading\ncomputation into the cloud is often unacceptable due to privacy concerns, high\nlatency, or the lack of connectivity. While compression algorithms often\nsucceed in reducing inferencing times, they come at the cost of reduced\naccuracy. This paper presents a new, alternative approach to enable efficient\nexecution of DNNs on embedded devices. Our approach dynamically determines\nwhich DNN to use for a given input, by considering the desired accuracy and\ninference time. It employs machine learning to develop a low-cost predictive\nmodel to quickly select a pre-trained DNN to use for a given input and the\noptimization constraint. We achieve this by first off-line training a\npredictive model, and then using the learned model to select a DNN model to use\nfor new, unseen inputs. We apply our approach to two representative DNN\ndomains: image classification and machine translation. We evaluate our approach\non a Jetson TX2 embedded deep learning platform and consider a range of\ninfluential DNN models including convolutional and recurrent neural networks.\nFor image classification, we achieve a 1.8x reduction in inference time with a\n7.52% improvement in accuracy, over the most-capable single DNN model. For\nmachine translation, we achieve a 1.34x reduction in inference time over the\nmost-capable single model, with little impact on the quality of translation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 23:56:19 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Marco", "Vicent Sanz", ""], ["Taylor", "Ben", ""], ["Wang", "Zheng", ""], ["Elkhatib", "Yehia", ""]]}, {"id": "1911.05146", "submitter": "Ammar Ahmad Awan", "authors": "Ammar Ahmad Awan, Arpan Jain, Quentin Anthony, Hari Subramoni, and\n  Dhabaleswar K. Panda", "title": "HyPar-Flow: Exploiting MPI and Keras for Scalable Hybrid-Parallel DNN\n  Training using TensorFlow", "comments": "18 pages, 10 figures, Accepted, to be presented at ISC '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce training time of large-scale DNNs, scientists have started to\nexplore parallelization strategies like data-parallelism, model-parallelism,\nand hybrid-parallelism. While data-parallelism has been extensively studied and\ndeveloped, several problems exist in realizing model-parallelism and\nhybrid-parallelism efficiently. Four major problems we focus on are: 1)\ndefining a notion of a distributed model across processes, 2) implementing\nforward/back-propagation across process boundaries that requires explicit\ncommunication, 3) obtaining parallel speedup on an inherently sequential task,\nand 4) achieving scalability without losing out on a model's accuracy. To\naddress these problems, we create HyPar-Flow --- a model-size/-type agnostic,\nscalable, practical, and user-transparent system for hybrid-parallel training\nby exploiting MPI, Keras, and TensorFlow. HyPar-Flow provides a single API that\ncan be used to perform data, model, and hybrid parallel training of any Keras\nmodel at scale. We create an internal distributed representation of the\nuser-provided Keras model, utilize TF's Eager execution features for\ndistributed forward/back-propagation across processes, exploit pipelining to\nimprove performance and leverage efficient MPI primitives for scalable\ncommunication. Between model partitions, we use send and recv to exchange\nlayer-data/partial-errors while allreduce is used to accumulate/average\ngradients across model replicas. Beyond the design and implementation of\nHyPar-Flow, we also provide comprehensive correctness and performance results\non three state-of-the-art HPC systems including TACC Frontera (#5 on\nTop500.org). For ResNet-1001, an ultra-deep model, HyPar-Flow provides: 1) Up\nto 1.6x speedup over Horovod-based data-parallel training, 2) 110x speedup over\nsingle-node on 128 Stampede2 nodes, and 3) 481x speedup over single-node on 512\nFrontera nodes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 21:07:42 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 15:16:53 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Awan", "Ammar Ahmad", ""], ["Jain", "Arpan", ""], ["Anthony", "Quentin", ""], ["Subramoni", "Hari", ""], ["Panda", "Dhabaleswar K.", ""]]}, {"id": "1911.05181", "submitter": "Jonathan Baxter", "authors": "Douglas Aberdeen, Jonathan Baxter and Robert Edwards", "title": "92c/MFlops/s, Ultra-Large-Scale Neural-Network Training on a PIII\n  Cluster", "comments": "SC '00: Proceedings of the 2000 ACM/IEEE Conference on Supercomputing", "journal-ref": "ACM/IEEE SC 2000 Conference (SC00)", "doi": "10.1109/SC.2000.10031", "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks with millions of adjustable parameters and a\nsimilar number of training examples are a potential solution for difficult,\nlarge-scale pattern recognition problems in areas such as speech and face\nrecognition, classification of large volumes of web data, and finance. The\nbottleneck is that neural network training involves iterative gradient descent\nand is extremely computationally intensive. In this paper we present a\ntechnique for distributed training of Ultra Large Scale Neural Networks (ULSNN)\non Bunyip, a Linux-based cluster of 196 Pentium III processors. To illustrate\nULSNN training we describe an experiment in which a neural network with 1.73\nmillion adjustable parameters was trained to recognize machine-printed Japanese\ncharacters from a database containing 9 million training patterns. The training\nruns with a average performance of 163.3 GFlops/s (single precision). With a\nmachine cost of \\$150,913, this yields a price/performance ratio of\n92.4c/MFlops/s (single precision). For comparison purposes, training using\ndouble precision and the ATLAS DGEMM produces a sustained performance of 70\nMFlops/s or \\$2.16 / MFlop/s (double precision).\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 22:57:09 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Aberdeen", "Douglas", ""], ["Baxter", "Jonathan", ""], ["Edwards", "Robert", ""]]}, {"id": "1911.06714", "submitter": "Ali Mohammed", "authors": "Ali Mohammed, Aurelien Cavelan, Florina M. Ciorba, Ruben M. Cabezon,\n  Ioana Banicesu", "title": "Two-level Dynamic Load Balancing for High Performance Scientific\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications are often complex, irregular, and\ncomputationally-intensive. To accommodate the ever-increasing computational\ndemands of scientific applications, high-performance computing (HPC) systems\nhave become larger and more complex, offering parallelism at multiple levels\n(e.g., nodes, cores per node, threads per core). Scientific applications need\nto exploit all the available multilevel hardware parallelism to harness the\navailable computational power. The performance of applications executing on\nsuch HPC systems may adversely be affected by load imbalance at multiple\nlevels, caused by problem, algorithmic, and systemic characteristics.\nNevertheless, most existing load balancing methods do not simultaneously\naddress load imbalance at multiple levels. This work investigates the impact of\nload imbalance on the performance of three scientific applications at the\nthread and process levels. We jointly apply and evaluate selected dynamic loop\nself-scheduling (DLS) techniques to both levels. Specifically, we employ the\nextended LaPeSD OpenMP runtime library at the thread level and extend the\nDLS4LB MPI-based dynamic load balancing library at the process level. This\napproach is generic and applicable to any multiprocess-multithreaded\ncomputationally-intensive application (programmed using MPI and OpenMP). We\nconduct an exhaustive set of experiments to assess and compare six DLS\ntechniques at the thread level and eleven at the process level. The results\nshow that improved application performance, by up to 21%, can only be achieved\nby jointly addressing load imbalance at the two levels. We offer insights into\nthe performance of the selected DLS techniques and discuss the interplay of\nload balancing at the thread level and process level.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 15:56:54 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Mohammed", "Ali", ""], ["Cavelan", "Aurelien", ""], ["Ciorba", "Florina M.", ""], ["Cabezon", "Ruben M.", ""], ["Banicesu", "Ioana", ""]]}, {"id": "1911.06922", "submitter": "Abdul Dakkak", "authors": "Cheng Li, Abdul Dakkak, Jinjun Xiong, Wen-mei Hwu", "title": "Benanza: Automatic $\\mu$Benchmark Generation to Compute \"Lower-bound\"\n  Latency and Inform Optimizations of Deep Learning Models on GPUs", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS47924.2020.00053", "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Deep Learning (DL) models have been increasingly used in latency-sensitive\napplications, there has been a growing interest in improving their response\ntime. An important venue for such improvement is to profile the execution of\nthese models and characterize their performance to identify possible\noptimization opportunities. However, the current profiling tools lack the\nhighly desired abilities to characterize ideal performance, identify sources of\ninefficiency, and quantify the benefits of potential optimizations. Such\ndeficiencies have led to slow characterization/optimization cycles that cannot\nkeep up with the fast pace at which new DL models are introduced.\n  We propose Benanza, a sustainable and extensible benchmarking and analysis\ndesign that speeds up the characterization/optimization cycle of DL models on\nGPUs. Benanza consists of four major components: a model processor that parses\nmodels into an internal representation, a configurable benchmark generator that\nautomatically generates micro-benchmarks given a set of models, a database of\nbenchmark results, and an analyzer that computes the \"lower-bound\" latency of\nDL models using the benchmark data and informs optimizations of model\nexecution. The \"lower-bound\" latency metric estimates the ideal model execution\non a GPU system and serves as the basis for identifying optimization\nopportunities in frameworks or system libraries. We used Benanza to evaluate 30\nONNX models in MXNet, ONNX Runtime, and PyTorch on 7 GPUs ranging from Kepler\nto the latest Turing, and identified optimizations in parallel layer execution,\ncuDNN convolution algorithm selection, framework inefficiency, layer fusion,\nand using Tensor Cores.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 00:24:05 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 01:15:16 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 16:46:32 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Li", "Cheng", ""], ["Dakkak", "Abdul", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1911.07434", "submitter": "Bin Li", "authors": "Bin Li, Shuseng Wang, Jun Zhang, Xainbin Cao, Chenglin Zhao", "title": "Fast-MUSIC for Automotive Massive-MIMO Radar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Massive multiple-input multiple-output (MIMO) radar, assisted by\nmillimeter-wave band virtual MIMO techniques, provides great promises to the\nhigh-resolution automotive sensing and target detection in unmanned\nground/aerial vehicles (UGA/UAV). As one long-standing challenging problem,\nhowever existing subspace methods may suffer from either the low\nresolution/accuracy or the high time complexity. In this study, we propose two\ncomputational efficient methods to accomplish the high-resolution estimation of\nangle of arrival (AoA) information. By leveraging randomized low-rank\napproximation, our fast-MUSIC approaches, relying on random sampling and\nprojection techniques, would speed up the subspace computation by orders of\nmagnitude. At the same time, we establish the theoretical bounds of our\nproposed approaches, which ensure the accuracy of approximated pseudo-spectrum.\nAs shown, in the case of high signal-to-noise ratio, the pseudo-spectrum\nacquired by our fast-MUSIC is highly precise, when compared to the exact MUSIC.\nComprehensive numerical study demonstrates that our new methods are\ntremendously faster than MUSIC, while the AoA estimation accuracy are almost as\ngood as MUSIC. As such, our fast-MUSIC enables the high-resolution yet\nreal-time sensing with massive MIMO radar, which has great potential in the\nemerging mobile computing and automotive applications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 05:27:46 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 08:31:41 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Bin", ""], ["Wang", "Shuseng", ""], ["Zhang", "Jun", ""], ["Cao", "Xainbin", ""], ["Zhao", "Chenglin", ""]]}, {"id": "1911.07449", "submitter": "Junfeng Li", "authors": "Junfeng Li, Sameer G. Kulkarni, K. K. Ramakrishnan, Dan Li", "title": "Understanding Open Source Serverless Platforms: Design Considerations\n  and Performance", "comments": null, "journal-ref": "Proceedings of the 5th International Workshop on Serverless\n  Computing, Pages 37-42, 2019", "doi": "10.1145/3366623.3368139", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing is increasingly popular because of the promise of lower\ncost and the convenience it provides to users who do not need to focus on\nserver management. This has resulted in the availability of a number of\nproprietary and open-source serverless solutions. We seek to understand how the\nperformance of serverless computing depends on a number of design issues using\nseveral popular open-source serverless platforms. We identify the\nidiosyncrasies affecting performance (throughput and latency) for different\nopen-source serverless platforms. Further, we observe that just having either\nresource-based (CPU and memory) or workload-based (request per second (RPS) or\nconcurrent requests) auto-scaling is inadequate to address the needs of the\nserverless platforms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 06:42:15 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 20:17:34 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 18:53:02 GMT"}, {"version": "v4", "created": "Fri, 13 Dec 2019 00:46:38 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Li", "Junfeng", ""], ["Kulkarni", "Sameer G.", ""], ["Ramakrishnan", "K. K.", ""], ["Li", "Dan", ""]]}, {"id": "1911.07617", "submitter": "Kai Li", "authors": "Kai Li, Wei Ni, Yousef Emami, Yiran Shen, Ricardo Severino, David\n  Pereira, Eduardo Tovar", "title": "Design and Implementation of Secret Key Agreement for Platoon-based\n  Vehicular Cyber-Physical Systems", "comments": "To be published in ACM Transactions on Cyber-Physical Systems (TCPS)", "journal-ref": null, "doi": "10.1145/3365996", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In platoon-based vehicular cyber-physical system (PVCPS), a lead vehicle that\nis responsible for managing the platoon's moving directions and velocity\nperiodically disseminates control messages to the vehicles that follow.\nSecuring wireless transmissions of the messages between the vehicles is\ncritical for privacy and confidentiality of platoon's driving pattern. However,\ndue to the broadcast nature of radio channels, the transmissions are vulnerable\nto eavesdropping. In this paper, we propose a cooperative secret key agreement\n(CoopKey) scheme for encrypting/decrypting the control messages, where the\nvehicles in PVCPS generate a unified secret key based on the quantized fading\nchannel randomness. Channel quantization intervals are optimized by dynamic\nprogramming to minimize the mismatch of keys. A platooning testbed is built\nwith autonomous robotic vehicles, where a TelosB wireless node is used for\nonboard data processing and multi-hop dissemination. Extensive real-world\nexperiments demonstrate that CoopKey achieves significantly low secret bit\nmismatch rate in a variety of settings. Moreover, the standard NIST test suite\nis employed to verify randomness of the generated keys, where the p-values of\nour CoopKey pass all the randomness tests. We also evaluate CoopKey with an\nextended platoon size via simulations to investigate the effect of system\nscalability on performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:49:58 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Li", "Kai", ""], ["Ni", "Wei", ""], ["Emami", "Yousef", ""], ["Shen", "Yiran", ""], ["Severino", "Ricardo", ""], ["Pereira", "David", ""], ["Tovar", "Eduardo", ""]]}, {"id": "1911.07738", "submitter": "Steven Van Rossem", "authors": "Steven Van Rossem, Wouter Tavernier, Didier Colle, Mario Pickavet,\n  Piet Demeester", "title": "Profile-based Resource Allocation for Virtualized Network Functions", "comments": "accepted in IEEE TNSM journal", "journal-ref": "IEEE Transactions on Network and Service Management, 2019, Early\n  Access", "doi": "10.1109/TNSM.2019.2943779", "report-no": null, "categories": "cs.NI cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The virtualization of compute and network resources enables an unseen\nflexibility for deploying network services. A wide spectrum of emerging\ntechnologies allows an ever-growing range of orchestration possibilities in\ncloud-based environments. But in this context it remains challenging to rhyme\ndynamic cloud configurations with deterministic performance. The service\noperator must somehow map the performance specification in the Service Level\nAgreement (SLA) to an adequate resource allocation in the virtualized\ninfrastructure. We propose the use of a VNF profile to alleviate this process.\nThis is illustrated by profiling the performance of four example network\nfunctions (a virtual router, switch, firewall and cache server) under varying\nworkloads and resource configurations. We then compare several methods to\nderive a model from the profiled datasets. We select the most accurate method\nto further train a model which predicts the services' performance, in function\nof incoming workload and allocated resources. Our presented method can offer\nthe service operator a recommended resource allocation for the targeted\nservice, in function of the targeted performance and maximum workload specified\nin the SLA. This helps to deploy the softwarized service with an optimal amount\nof resources to meet the SLA requirements, thereby avoiding unnecessary scaling\nsteps.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:21:41 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Van Rossem", "Steven", ""], ["Tavernier", "Wouter", ""], ["Colle", "Didier", ""], ["Pickavet", "Mario", ""], ["Demeester", "Piet", ""]]}, {"id": "1911.07967", "submitter": "Abdul Dakkak", "authors": "Cheng Li, Abdul Dakkak, Jinjun Xiong, Wen-mei Hwu", "title": "DLBricks: Composable Benchmark Generation to Reduce Deep Learning\n  Benchmarking Effort on CPUs (Extended)", "comments": null, "journal-ref": null, "doi": "10.1145/3358960.3379143", "report-no": null, "categories": "cs.LG cs.PF cs.SE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The past few years have seen a surge of applying Deep Learning (DL) models\nfor a wide array of tasks such as image classification, object detection,\nmachine translation, etc. While DL models provide an opportunity to solve\notherwise intractable tasks, their adoption relies on them being optimized to\nmeet latency and resource requirements. Benchmarking is a key step in this\nprocess but has been hampered in part due to the lack of representative and\nup-to-date benchmarking suites. This is exacerbated by the fast-evolving pace\nof DL models.\n  This paper proposes DLBricks, a composable benchmark generation design that\nreduces the effort of developing, maintaining, and running DL benchmarks on\nCPUs. DLBricks decomposes DL models into a set of unique runnable networks and\nconstructs the original model's performance using the performance of the\ngenerated benchmarks. DLBricks leverages two key observations: DL layers are\nthe performance building blocks of DL models and layers are extensively\nrepeated within and across DL models. Since benchmarks are generated\nautomatically and the benchmarking time is minimized, DLBricks can keep\nup-to-date with the latest proposed models, relieving the pressure of selecting\nrepresentative DL models. Moreover, DLBricks allows users to represent\nproprietary models within benchmark suites. We evaluate DLBricks using $50$\nMXNet models spanning $5$ DL tasks on $4$ representative CPU systems. We show\nthat DLBricks provides an accurate performance estimate for the DL models and\nreduces the benchmarking time across systems (e.g. within $95\\%$ accuracy and\nup to $4.4\\times$ benchmarking time speedup on Amazon EC2 c5.xlarge).\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 21:42:36 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 15:51:05 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 16:51:34 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Cheng", ""], ["Dakkak", "Abdul", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1911.08031", "submitter": "Abdul Dakkak", "authors": "Cheng Li, Abdul Dakkak, Jinjun Xiong, Wen-mei Hwu", "title": "The Design and Implementation of a Scalable DL Benchmarking Platform", "comments": null, "journal-ref": "2020 IEEE 13th International Conference on Cloud Computing\n  (CLOUD), 414-425", "doi": "10.1109/CLOUD49709.2020.00063", "report-no": null, "categories": "cs.DC cs.GL cs.LG cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current Deep Learning (DL) landscape is fast-paced and is rife with\nnon-uniform models, hardware/software (HW/SW) stacks, but lacks a DL\nbenchmarking platform to facilitate evaluation and comparison of DL\ninnovations, be it models, frameworks, libraries, or hardware. Due to the lack\nof a benchmarking platform, the current practice of evaluating the benefits of\nproposed DL innovations is both arduous and error-prone - stifling the adoption\nof the innovations.\n  In this work, we first identify $10$ design features which are desirable\nwithin a DL benchmarking platform. These features include: performing the\nevaluation in a consistent, reproducible, and scalable manner, being framework\nand hardware agnostic, supporting real-world benchmarking workloads, providing\nin-depth model execution inspection across the HW/SW stack levels, etc. We then\npropose MLModelScope, a DL benchmarking platform design that realizes the $10$\nobjectives. MLModelScope proposes a specification to define DL model\nevaluations and techniques to provision the evaluation workflow using the\nuser-specified HW/SW stack. MLModelScope defines abstractions for frameworks\nand supports board range of DL models and evaluation scenarios. We implement\nMLModelScope as an open-source project with support for all major frameworks\nand hardware architectures. Through MLModelScope's evaluation and automated\nanalysis workflows, we performed case-study analyses of $37$ models across $4$\nsystems and show how model, hardware, and framework selection affects model\naccuracy and performance under different benchmarking scenarios. We further\ndemonstrated how MLModelScope's tracing capability gives a holistic view of\nmodel execution and helps pinpoint bottlenecks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 01:16:08 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Cheng", ""], ["Dakkak", "Abdul", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1911.08779", "submitter": "Zheng Wang", "authors": "Donglin Chen, Jianbin Fang, Chuanfu Xu, Shizhao Chen, Zheng Wang", "title": "Characterizing Scalability of Sparse Matrix-Vector Multiplications on\n  Phytium FT-2000+ Many-cores", "comments": "Accepted to be published at IJPP", "journal-ref": null, "doi": "10.1007/s10766-019-00646-x", "report-no": null, "categories": "cs.DC cs.CL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the scalability of parallel programs is crucial for software\noptimization and hardware architecture design. As HPC hardware is moving\ntowards many-core design, it becomes increasingly difficult for a parallel\nprogram to make effective use of all available processor cores. This makes\nscalability analysis increasingly important. This paper presents a quantitative\nstudy for characterizing the scalability of sparse matrix-vector\nmultiplications (SpMV) on Phytium FT-2000+, an ARM-based many-core architecture\nfor HPC computing. We choose to study SpMV as it is a common operation in\nscientific and HPC applications. Due to the newness of ARM-based many-core\narchitectures, there is little work on understanding the SpMV scalability on\nsuch hardware design. To close the gap, we carry out a large-scale empirical\nevaluation involved over 1,000 representative SpMV datasets. We show that,\nwhile many computation-intensive SpMV applications contain extensive\nparallelism, achieving a linear speedup is non-trivial on Phytium FT-2000+. To\nbetter understand what software and hardware parameters are most important for\ndetermining the scalability of a given SpMV kernel, we develop a performance\nanalytical model based on the regression tree. We show that our model is highly\neffective in characterizing SpMV scalability, offering useful insights to help\napplication developers for better optimizing SpMV on an emerging HPC\narchitecture.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 09:12:58 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Chen", "Donglin", ""], ["Fang", "Jianbin", ""], ["Xu", "Chuanfu", ""], ["Chen", "Shizhao", ""], ["Wang", "Zheng", ""]]}, {"id": "1911.09512", "submitter": "Akbar Siami Namin", "authors": "Sima Siami-Namini and Neda Tavakoli and Akbar Siami Namin", "title": "A Comparative Analysis of Forecasting Financial Time Series Using ARIMA,\n  LSTM, and BiLSTM", "comments": "8 pages, 3 figures, 3 tables, 1 listing, IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine and deep learning-based algorithms are the emerging approaches in\naddressing prediction problems in time series. These techniques have been shown\nto produce more accurate results than conventional regression-based modeling.\nIt has been reported that artificial Recurrent Neural Networks (RNN) with\nmemory, such as Long Short-Term Memory (LSTM), are superior compared to\nAutoregressive Integrated Moving Average (ARIMA) with a large margin. The\nLSTM-based models incorporate additional \"gates\" for the purpose of memorizing\nlonger sequences of input data. The major question is that whether the gates\nincorporated in the LSTM architecture already offers a good prediction and\nwhether additional training of data would be necessary to further improve the\nprediction.\n  Bidirectional LSTMs (BiLSTMs) enable additional training by traversing the\ninput data twice (i.e., 1) left-to-right, and 2) right-to-left). The research\nquestion of interest is then whether BiLSTM, with additional training\ncapability, outperforms regular unidirectional LSTM. This paper reports a\nbehavioral analysis and comparison of BiLSTM and LSTM models. The objective is\nto explore to what extend additional layers of training of data would be\nbeneficial to tune the involved parameters. The results show that additional\ntraining of data and thus BiLSTM-based modeling offers better predictions than\nregular LSTM-based models. More specifically, it was observed that BiLSTM\nmodels provide better predictions compared to ARIMA and LSTM models. It was\nalso observed that BiLSTM models reach the equilibrium much slower than\nLSTM-based models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 14:58:52 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Siami-Namini", "Sima", ""], ["Tavakoli", "Neda", ""], ["Namin", "Akbar Siami", ""]]}, {"id": "1911.09925", "submitter": "Hasan Genc", "authors": "Hasan Genc, Seah Kim, Alon Amid, Ameer Haj-Ali, Vighnesh Iyer, Pranav\n  Prakash, Jerry Zhao, Daniel Grubb, Harrison Liew, Howard Mao, Albert Ou,\n  Colin Schmidt, Samuel Steffl, John Wright, Ion Stoica, Jonathan Ragan-Kelley,\n  Krste Asanovic, Borivoje Nikolic, Yakun Sophia Shao", "title": "Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via\n  Full-Stack Integration", "comments": "To appear at the 58th IEEE/ACM Design Automation Conference (DAC),\n  December 2021, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN accelerators are often developed and evaluated in isolation without\nconsidering the cross-stack, system-level effects in real-world environments.\nThis makes it difficult to appreciate the impact of System-on-Chip (SoC)\nresource contention, OS overheads, and programming-stack inefficiencies on\noverall performance/energy-efficiency. To address this challenge, we present\nGemmini, an open-source*, full-stack DNN accelerator generator. Gemmini\ngenerates a wide design-space of efficient ASIC accelerators from a flexible\narchitectural template, together with flexible programming stacks and full SoCs\nwith shared resources that capture system-level effects. Gemmini-generated\naccelerators have also been fabricated, delivering up to three\norders-of-magnitude speedups over high-performance CPUs on various DNN\nbenchmarks.\n  * https://github.com/ucb-bar/gemmini\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 08:51:28 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 10:33:50 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 06:53:12 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Genc", "Hasan", ""], ["Kim", "Seah", ""], ["Amid", "Alon", ""], ["Haj-Ali", "Ameer", ""], ["Iyer", "Vighnesh", ""], ["Prakash", "Pranav", ""], ["Zhao", "Jerry", ""], ["Grubb", "Daniel", ""], ["Liew", "Harrison", ""], ["Mao", "Howard", ""], ["Ou", "Albert", ""], ["Schmidt", "Colin", ""], ["Steffl", "Samuel", ""], ["Wright", "John", ""], ["Stoica", "Ion", ""], ["Ragan-Kelley", "Jonathan", ""], ["Asanovic", "Krste", ""], ["Nikolic", "Borivoje", ""], ["Shao", "Yakun Sophia", ""]]}, {"id": "1911.10735", "submitter": "Julien Girard-Satabin", "authors": "Julien Girard-Satabin (TAU, LIST), Guillaume Charpiat (LRI, TAU),\n  Zakaria Chihani (LIST), Marc Schoenauer (TAU)", "title": "CAMUS: A Framework to Build Formal Specifications for Deep Perception\n  Systems Using Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of provable deep neural network robustness has raised considerable\ninterest in recent years. Most research has focused on adversarial robustness,\nwhich studies the robustness of perceptive models in the neighbourhood of\nparticular samples. However, other works have proved global properties of\nsmaller neural networks. Yet, formally verifying perception remains uncharted.\nThis is due notably to the lack of relevant properties to verify, as the\ndistribution of possible inputs cannot be formally specified. We propose to\ntake advantage of the simulators often used either to train machine learning\nmodels or to check them with statistical tests, a growing trend in industry.\nOur formulation allows us to formally express and verify safety properties on\nperception units, covering all cases that could ever be generated by the\nsimulator, to the difference of statistical tests which cover only seen\nexamples. Along with this theoretical formulation , we provide a tool to\ntranslate deep learning models into standard logical formulae. As a proof of\nconcept, we train a toy example mimicking an autonomous car perceptive unit,\nand we formally verify that it will never fail to capture the relevant\ninformation in the provided inputs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:28:45 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Girard-Satabin", "Julien", "", "TAU, LIST"], ["Charpiat", "Guillaume", "", "LRI, TAU"], ["Chihani", "Zakaria", "", "LIST"], ["Schoenauer", "Marc", "", "TAU"]]}, {"id": "1911.11293", "submitter": "Terrell Mundhenk", "authors": "T. Nathan Mundhenk and Barry Y. Chen and Gerald Friedland", "title": "Efficient Saliency Maps for Explainable AI", "comments": "In submission to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an explainable AI saliency map method for use with deep\nconvolutional neural networks (CNN) that is much more efficient than popular\nfine-resolution gradient methods. It is also quantitatively similar or better\nin accuracy. Our technique works by measuring information at the end of each\nnetwork scale which is then combined into a single saliency map. We describe\nhow saliency measures can be made more efficient by exploiting Saliency Map\nOrder Equivalence. We visualize individual scale/layer contributions by using a\nLayer Ordered Visualization of Information. This provides an interesting\ncomparison of scale information contributions within the network not provided\nby other saliency map methods. Using our method instead of Guided Backprop,\ncoarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem\nto yield demonstrably superior results without sacrificing speed. This will\nmake fine-resolution saliency methods feasible on resource limited platforms\nsuch as robots, cell phones, low-cost industrial devices, astronomy and\nsatellite imagery.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 00:32:23 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 21:20:06 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Mundhenk", "T. Nathan", ""], ["Chen", "Barry Y.", ""], ["Friedland", "Gerald", ""]]}, {"id": "1911.11592", "submitter": "Harsh Singh", "authors": "Harsh Jot Singh and Abdelhakim Senhaji Hafid", "title": "Transaction Confirmation Time Prediction in Ethereum Blockchain Using\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain offers a decentralized, immutable, transparent system of records.\nIt offers a peer-to-peer network of nodes with no centralised governing entity\nmaking it unhackable and therefore, more secure than the traditional\npaper-based or centralised system of records like banks etc. While there are\ncertain advantages to the paper-based recording approach, it does not work well\nwith digital relationships where the data is in constant flux. Unlike\ntraditional channels, governed by centralized entities, blockchain offers its\nusers a certain level of anonymity by providing capabilities to interact\nwithout disclosing their personal identities and allows them to build trust\nwithout a third-party governing entity. Due to the aforementioned\ncharacteristics of blockchain, more and more users around the globe are\ninclined towards making a digital transaction via blockchain than via\nrudimentary channels. Therefore, there is a dire need for us to gain insight on\nhow these transactions are processed by the blockchain and how much time it may\ntake for a peer to confirm a transaction and add it to the blockchain network.\nThis paper presents a novel approach that would allow one to estimate the time,\nin block time or otherwise, it would take for a mining node to accept and\nconfirm a transaction to a block using machine learning. The paper also aims to\ncompare the predictive accuracy of two machine learning regression models-\nRandom Forest Regressor and Multilayer Perceptron against previously proposed\nstatistical regression model under a set evaluation criterion. The objective is\nto determine whether machine learning offers a more accurate predictive model\nthan conventional statistical models. The proposed model results in improved\naccuracy in prediction.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 17:20:27 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Singh", "Harsh Jot", ""], ["Hafid", "Abdelhakim Senhaji", ""]]}, {"id": "1911.11642", "submitter": "Ramya Akula", "authors": "Ramya Akula, Kartik Jain, and Deep Jigar Kotecha", "title": "System Performance with varying L1 Instruction and Data Cache Sizes: An\n  Empirical Analysis", "comments": "5 Figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we investigate the fluctuations in performance caused by\nchanging the Instruction (I-cache) size and the Data (D-cache) size in the L1\ncache. We employ the Gem5 framework to simulate a system with varying\nspecifications on a single host machine. We utilize the FreqMine benchmark\navailable under the PARSEC suite as the workload program to benchmark our\nsimulated system. The Out-order CPU (O3) with Ruby memory model was simulated\nin a Full-System X86 environment with Linux OS. The chosen metrics deal with\nHit Rate, Misses, Memory Latency, Instruction Rate, and Bus Traffic within the\nsystem. Performance observed by varying L1 size within a certain range of\nvalues was used to compute Confidence Interval based statistics for relevant\nmetrics. Our expectations, corresponding experimental observations, and\ndiscrepancies are also discussed in this report.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 15:33:56 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Akula", "Ramya", ""], ["Jain", "Kartik", ""], ["Kotecha", "Deep Jigar", ""]]}, {"id": "1911.11852", "submitter": "Chenye Wu", "authors": "Mingkuan Xu, Yang Yu and Chenye Wu", "title": "Rule Designs for Optimal Online Game Matchmaking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online games are the most popular form of entertainment among youngsters as\nwell as elders. Recognized as e-Sports, they may become an official part of the\nOlympic Games by 2020. However, a long waiting time for matchmaking will\nlargely affect players' experiences. We examine different matchmaking\nmechanisms for 2v2 games. By casting the mechanisms into a queueing theoretic\nframework, we decompose the rule design process into a sequence of decision\nmaking problems, and derive the optimal mechanism with minimum expected waiting\ntime. We further the result by exploring additional static as well as dynamic\nrule designs' impacts. In the static setting, we consider the game allows\nplayers to choose sides before the battle. In the dynamic setting, we consider\nthe game offers multiple zones for players of different skill levels. In both\nsettings, we examine the value of choice-free players. Closed form expressions\nfor the expected waiting time in different settings illuminate the guidelines\nfor online game rule designs.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 13:17:12 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Xu", "Mingkuan", ""], ["Yu", "Yang", ""], ["Wu", "Chenye", ""]]}, {"id": "1911.12162", "submitter": "Fran\\c{c}ois Tessier", "authors": "Fran\\c{c}ois Tessier, Maxime Martinasso, Matteo Chesi, Mark Klein,\n  Miguel Gila", "title": "Dynamically Provisioning Cray DataWarp Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Complex applications and workflows needs are often exclusively expressed in\nterms of computational resources on HPC systems. In many cases, other resources\nlike storage or network are not allocatable and are shared across the entire\nHPC system. By looking at the storage resource in particular, any workflow or\napplication should be able to select both its preferred data manager and its\nrequired storage capability or capacity. To achieve such a goal, new mechanisms\nshould be introduced. In this work, we introduce such a mechanism for\ndynamically provision a data management system on top of storage devices. We\nparticularly focus our effort on deploying a BeeGFS instance across multiple\nDataWarp nodes on a Cray XC50 system. However, we also demonstrate that the\nsame mechanism can be used to deploy BeeGFS on non-Cray system.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:08:36 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Tessier", "Fran\u00e7ois", ""], ["Martinasso", "Maxime", ""], ["Chesi", "Matteo", ""], ["Klein", "Mark", ""], ["Gila", "Miguel", ""]]}, {"id": "1911.12877", "submitter": "Bo Wu", "authors": "Daniel Mawhirter, Sam Reinehr, Connor Holmes, Tongping Liu, Bo Wu", "title": "GraphZero: Breaking Symmetry for Efficient Graph Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph mining for structural patterns is a fundamental task in many\napplications. Compilation-based graph mining systems, represented by AutoMine,\ngenerate specialized algorithms for the provided patterns and substantially\noutperform other systems. However, the generated code causes substantial\ncomputation redundancy and the compilation process incurs too much overhead to\nbe used online, both due to the inherent symmetry in the structural patterns.\n  In this paper, we propose an optimizing compiler, GraphZero, to completely\naddress these limitations through symmetry breaking based on group theory.\nGraphZero implements three novel techniques. First, its schedule explorer\nefficiently prunes the schedule space without missing any high-performance\nschedule. Second, it automatically generates and enforces a set of restrictions\nto eliminate computation redundancy. Third, it generalizes orientation, a\nsurprisingly effective optimization that was mainly used for clique patterns,\nto apply to arbitrary patterns. Evaluated on multiple graph mining applications\nand complex patterns with 7 real-world graph datasets, GraphZero demonstrates\nup to 40X performance improvement and up to 197X reduction on schedule\ngeneration overhead over AutoMine.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 22:07:22 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Mawhirter", "Daniel", ""], ["Reinehr", "Sam", ""], ["Holmes", "Connor", ""], ["Liu", "Tongping", ""], ["Wu", "Bo", ""]]}, {"id": "1911.12898", "submitter": "Faissal El Bouanani", "authors": "Mounia Bouabdellah, Faissal El Bouanani, and Mohamed-Slim Alouini", "title": "A PHY Layer Security Analysis of Uplink Cooperative Jamming-Based\n  Underlay CRNs with Multi-Eavesdroppers", "comments": "34 pages, 7 figiures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the physical layer security of a dual-hop underlay uplink\ncognitive radio network is investigated over Nakagami-m fading channels.\nSpecifically, multiple secondary sources are taking turns in accessing the\nlicensed spectrum of the primary users and communicating with a multiantenna\nsecondary base station D through the aid of a multiantenna relay R in the\npresence of M eavesdroppers that are also equipped with multiple antennas.\nAmong the remaining nodes, one jammer is randomly selected to transmit an\nartificial noise to disrupt all the eavesdroppers that are attempting to\nintercept the communication of the legitimate links i.e., S-R and R-D. The\nreceived signals at each node are combined using maximal-ratio combining.\nSecrecy analysis is provided by deriving closed-form and asymptotic expressions\nfor the secrecy outage probability. The impact of several key parameters on the\nsystem's secrecy e.g., transmit power of the sources, number of eavesdroppers,\nmaximum tolerated interference power, and the number of diversity branches is\ninvestigated. Importantly, by considering two scenarios, namely (i) absence and\n(ii) presence of a friendly jammer, new insights are obtained for the\nconsidered communication system. Especially, we tend to answer to the following\nquestion: Can better secrecy be achieved without jamming by considering a\nsingle antenna at eavesdroppers and multiple-ones at the legitimate users\n(i.e., relay and end-user) rather than sending permanently an artificial noise\nand considering that both the relay and the destination are equipped with a\nsingle antenna, while multiple antennas are used by the eavesdroppers? The\nobtained results are corroborated through Monte Carlo simulation and show that\nthe system's security can be enhanced by adjusting the aforementioned\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 23:16:42 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Bouabdellah", "Mounia", ""], ["Bouanani", "Faissal El", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1911.13027", "submitter": "Robert Speck", "authors": "Robert Speck and Michael Knobloch and Sebastian L\\\"uhrs and Andreas\n  Gocht", "title": "Using performance analysis tools for parallel-in-time integrators --\n  Does my time-parallel code do what I think it does?", "comments": "31 pages, 15 figures, CVS Proceedings of the 9th PinT Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many ideas and proofs of concept for parallel-in-time integration\nmethods exists, the number of large-scale, accessible time-parallel codes is\nrather small. This is often due to the apparent or subtle complexity of the\nalgorithms and the many pitfalls awaiting developers of parallel numerical\nsoftware. One example of such a time-parallel code is pySDC, which implements,\namong others, the parallel full approximation scheme in space and time\n(PFASST). Inspired by nonlinear multigrid ideas, PFASST allows to integrate\nmultiple time-steps simultaneously using a space-time hierarchy of spectral\ndeferred corrections. In this paper we demonstrate the application of\nperformance analysis tools to the PFASST implementation pySDC. Tracing the path\nwe took for this work, we highlight the obstacles encountered, describe\nremedies and explain the sometimes surprising findings made possible by the\ntools. Although focusing only on a single implementation of a particular\nparallel-in-time integrator, we hope that our results and in particular the way\nwe obtained them are a blueprint for other time-parallel codes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 09:58:23 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 12:32:34 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Speck", "Robert", ""], ["Knobloch", "Michael", ""], ["L\u00fchrs", "Sebastian", ""], ["Gocht", "Andreas", ""]]}, {"id": "1911.13074", "submitter": "Danijel Zlaus", "authors": "Danijel \\v{Z}laus and Domen Mongus", "title": "Efficient method for parallel computation of geodesic transformation on\n  CPU", "comments": "\\c{opyright} 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": "10.1109/TPDS.2019.2953057", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fast Central Processing Unit (CPU) implementation of\ngeodesic morphological operations using stream processing. In contrast to the\ncurrent state-of-the-art, that focuses on achieving insensitivity to the filter\nsizes with efficient data structures, the proposed approach achieves efficient\ncomputation of long chains of elementary $3 \\times 3$ filters using multicore\nand Single Instruction Multiple Data (SIMD) processing. In comparison to the\nrelated methods, up to $100$ times faster computation of common geodesic\noperators is achieved in this way, allowing for real-time processing (with over\n$30$ FPS) of up to $1500$ filters long chains, applied on $1024\\times 1024$\nimages. In addition, the proposed approach outperformed GPGPU, and proved to be\nmore efficient than the comparable streaming method for the computation of\nmorphological erosions and dilations with window sizes up to $183\\times 183$ in\nthe case of using char and $27\\times27$ when using double data types.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 12:09:39 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["\u017dlaus", "Danijel", ""], ["Mongus", "Domen", ""]]}]