[{"id": "1803.00419", "submitter": "R. Baghdadi", "authors": "Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo,\n  Patricia Suriana, Shoaib Kamil, Saman Amarasinghe", "title": "Technical Report about Tiramisu: a Three-Layered Abstraction for Hiding\n  Hardware Complexity from DSL Compilers", "comments": "This is a duplicate for 1804.10694. This version of the paper is\n  outdated and should be deleted and only 1804.10694 should be kept. Future\n  versions of the paper will replace 1804.10694 (as second, third version, ...)\n  but now we want to remove duplicates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance DSL developers work hard to take advantage of modern\nhardware. The DSL compilers have to build their own complex middle-ends before\nthey can target a common back-end such as LLVM, which only handles single\ninstruction streams with SIMD instructions. We introduce Tiramisu, a common\nmiddle-end that can generate efficient code for modern processors and\naccelerators such as multicores, GPUs, FPGAs and distributed clusters. Tiramisu\nintroduces a novel three-level IR that separates the algorithm, how that\nalgorithm is executed, and where intermediate data are stored. This separation\nsimplifies optimization and makes targeting multiple hardware architectures\nfrom the same algorithm easier. As a result, DSL compilers can be made\nconsiderably less complex with no loss of performance while immediately\ntargeting multiple hardware or hardware combinations such as distributed nodes\nwith both CPUs and GPUs. We evaluated Tiramisu by creating a new middle-end for\nthe Halide and Julia compilers. We show that Tiramisu extends Halide and Julia\nwith many new capabilities including the ability to: express new algorithms\n(such as recurrent filters and non-rectangular iteration spaces), perform new\ncomplex loop nest transformations (such as wavefront parallelization, loop\nshifting and loop fusion) and generate efficient code for more architectures\n(such as combinations of distributed clusters, multicores, GPUs and FPGAs).\nFinally, we demonstrate that Tiramisu can generate very efficient code that\nmatches the highly optimized Intel MKL gemm (generalized matrix multiplication)\nimplementation, we also show speedups reaching 4X in Halide and 16X in Julia\ndue to optimizations enabled by Tiramisu.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:05:22 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 13:48:28 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 19:49:55 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Baghdadi", "Riyadh", ""], ["Ray", "Jessica", ""], ["Romdhane", "Malek Ben", ""], ["Del Sozzo", "Emanuele", ""], ["Suriana", "Patricia", ""], ["Kamil", "Shoaib", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1803.00922", "submitter": "George Kesidis", "authors": "Yuquan Shan, Aman Jain, George Kesidis, Bhuvan Urgaonkar, Jalal\n  Khamse-Ashari and Ioannis Lambadaris", "title": "Online Scheduling of Spark Workloads with Mesos using Different Fair\n  Allocation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following, we present example illustrative and experimental results\ncomparing fair schedulers allocating resources from multiple servers to\ndistributed application frameworks. Resources are allocated so that at least\none resource is exhausted in every server. Schedulers considered include DRF\n(DRFH) and Best-Fit DRF (BF-DRF), TSF, and PS-DSF. We also consider server\nselection under Randomized Round Robin (RRR) and based on their residual\n(unreserved) resources. In the following, we consider cases with frameworks of\nequal priority and without server-preference constraints. We first give typical\nresults of a illustrative numerical study and then give typical results of a\nstudy involving Spark workloads on Mesos which we have modified and\nopen-sourced to prototype different schedulers.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 15:54:52 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 18:09:16 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 23:11:23 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Shan", "Yuquan", ""], ["Jain", "Aman", ""], ["Kesidis", "George", ""], ["Urgaonkar", "Bhuvan", ""], ["Khamse-Ashari", "Jalal", ""], ["Lambadaris", "Ioannis", ""]]}, {"id": "1803.01281", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Siddharth Samsi, William Arcand, David Bestor, Bill\n  Bergeron, Tim Davis, Vijay Gadepally, Michael Houle, Matthew Hubbell, Hayden\n  Jananthan, Michael Jones, Anna Klein, Peter Michaleas, Roger Pearce, Lauren\n  Milechin, Julie Mullen, Andrew Prout, Antonio Rosa, Geoff Sanders, Charles\n  Yee, Albert Reuther", "title": "Design, Generation, and Validation of Extreme Scale Power-Law Graphs", "comments": "8 pages, 6 figures, IEEE IPDPS 2018 Graph Algorithm Building Blocks\n  (GABB) workshop", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00055", "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.PF math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive power-law graphs drive many fields: metagenomics, brain mapping,\nInternet-of-things, cybersecurity, and sparse machine learning. The development\nof novel algorithms and systems to process these data requires the design,\ngeneration, and validation of enormous graphs with exactly known properties.\nSuch graphs accelerate the proper testing of new algorithms and systems and are\na prerequisite for success on real applications. Many random graph generators\ncurrently exist that require realizing a graph in order to know its exact\nproperties: number of vertices, number of edges, degree distribution, and\nnumber of triangles. Designing graphs using these random graph generators is a\ntime-consuming trial-and-error process. This paper presents a novel approach\nthat uses Kronecker products to allow the exact computation of graph properties\nprior to graph generation. In addition, when a real graph is desired, it can be\ngenerated quickly in memory on a parallel computer with no-interprocessor\ncommunication. To test this approach, graphs with $10^{12}$ edges are generated\non a 40,000+ core supercomputer in 1 second and exactly agree with those\npredicted by the theory. In addition, to demonstrate the extensibility of this\napproach, decetta-scale graphs with up to $10^{30}$ edges are simulated in a\nfew minutes on a laptop.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 01:47:49 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kepner", "Jeremy", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Davis", "Tim", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jananthan", "Hayden", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Pearce", "Roger", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Sanders", "Geoff", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1803.01618", "submitter": "Johannes Hofmann", "authors": "Johannes Hofmann and Georg Hager and Dietmar Fey", "title": "On the accuracy and usefulness of analytic energy models for\n  contemporary multicore processors", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-92040-5_2", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents refinements to the execution-cache-memory performance\nmodel and a previously published power model for multicore processors. The\ncombination of both enables a very accurate prediction of performance and\nenergy consumption of contemporary multicore processors as a function of\nrelevant parameters such as number of active cores as well as core and Uncore\nfrequencies. Model validation is performed on the Sandy Bridge-EP and\nBroadwell-EP microarchitectures. Production-related variations in chip quality\nare demonstrated through a statistical analysis of the fit parameters obtained\non one hundred Broadwell-EP CPUs of the same model. Insights from the models\nare used to explain the performance- and energy-related behavior of the\nprocessors for scalable as well as saturating (i.e., memory-bound) codes. In\nthe process we demonstrate the models' capability to identify optimal operating\npoints with respect to highest performance, lowest energy-to-solution, and\nlowest energy-delay product and identify a set of best practices for\nenergy-efficient execution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 11:53:24 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Hofmann", "Johannes", ""], ["Hager", "Georg", ""], ["Fey", "Dietmar", ""]]}, {"id": "1803.02156", "submitter": "Georg Hager", "authors": "Moritz Kreutzer, Georg Hager, Dominik Ernst, Holger Fehske, Alan R.\n  Bishop, Gerhard Wellein", "title": "Chebyshev Filter Diagonalization on Modern Manycore Processors and\n  GPGPUs", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": "10.1007/978-3-319-92040-5_17", "report-no": null, "categories": "cs.MS cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chebyshev filter diagonalization is well established in quantum chemistry and\nquantum physics to compute bulks of eigenvalues of large sparse matrices.\nChoosing a block vector implementation, we investigate optimization\nopportunities on the new class of high-performance compute devices featuring\nboth high-bandwidth and low-bandwidth memory. We focus on the transparent\naccess to the full address space supported by both architectures under\nconsideration: Intel Xeon Phi \"Knights Landing\" and Nvidia \"Pascal.\"\n  We propose two optimizations: (1) Subspace blocking is applied for improved\nperformance and data access efficiency. We also show that it allows\ntransparently handling problems much larger than the high-bandwidth memory\nwithout significant performance penalties. (2) Pipelining of communication and\ncomputation phases of successive subspaces is implemented to hide communication\ncosts without extra memory traffic.\n  As an application scenario we use filter diagonalization studies on\ntopological insulator materials. Performance numbers on up to 512 nodes of the\nOakForest-PACS and Piz Daint supercomputers are presented, achieving beyond 100\nTflop/s for computing 100 inner eigenvalues of sparse matrices of dimension one\nbillion.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 13:13:23 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Kreutzer", "Moritz", ""], ["Hager", "Georg", ""], ["Ernst", "Dominik", ""], ["Fehske", "Holger", ""], ["Bishop", "Alan R.", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1803.02481", "submitter": "Luke Olson", "authors": "Andrew Reisner, Luke N. Olson, J. David Moulton", "title": "Scaling Structured Multigrid to 500K+ Cores through Coarse-Grid\n  Redistribution", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": "Los Alamos Report LA-UR-17-22886", "categories": "cs.MS cs.NA cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient solution of sparse, linear systems resulting from the\ndiscretization of partial differential equations is crucial to the performance\nof many physics-based simulations. The algorithmic optimality of multilevel\napproaches for common discretizations makes them a good candidate for an\nefficient parallel solver. Yet, modern architectures for high-performance\ncomputing systems continue to challenge the parallel scalability of multilevel\nsolvers. While algebraic multigrid methods are robust for solving a variety of\nproblems, the increasing importance of data locality and cost of data movement\nin modern architectures motivates the need to carefully exploit structure in\nthe problem.\n  Robust logically structured variational multigrid methods, such as Black Box\nMultigrid (BoxMG), maintain structure throughout the multigrid hierarchy. This\navoids indirection and increased coarse-grid communication costs typical in\nparallel algebraic multigrid. Nevertheless, the parallel scalability of\nstructured multigrid is challenged by coarse-grid problems where the overhead\nin communication dominates computation. In this paper, an algorithm is\nintroduced for redistributing coarse-grid problems through incremental\nagglomeration. Guided by a predictive performance model, this algorithm\nprovides robust redistribution decisions for structured multilevel solvers.\n  A two-dimensional diffusion problem is used to demonstrate the significant\ngain in performance of this algorithm over the previous approach that used\nagglomeration to one processor. In addition, the parallel scalability of this\napproach is demonstrated on two large-scale computing systems, with solves on\nup to 500K+ cores.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 23:57:38 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Reisner", "Andrew", ""], ["Olson", "Luke N.", ""], ["Moulton", "J. David", ""]]}, {"id": "1803.03288", "submitter": "Sayed Hadi Hashemi", "authors": "Sayed Hadi Hashemi, Sangeetha Abdu Jyothi, Roy H. Campbell", "title": "TicTac: Accelerating Distributed Deep Learning with Communication\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning systems rely on iterative distributed training\nto tackle the increasing complexity of models and input data. The iteration\ntime in these communication-heavy systems depends on the computation time,\ncommunication time and the extent of overlap of computation and communication.\n  In this work, we identify a shortcoming in systems with graph representation\nfor computation, such as TensorFlow and PyTorch, that result in high variance\nin iteration time --- random order of received parameters across workers. We\ndevelop a system, TicTac, to improve the iteration time by fixing this issue in\ndistributed deep learning with Parameter Servers while guaranteeing\nnear-optimal overlap of communication and computation. TicTac identifies and\nenforces an order of network transfers which improves the iteration time using\nprioritization. Our system is implemented over TensorFlow and requires no\nchanges to the model or developer inputs. TicTac improves the throughput by up\nto $37.7\\%$ in inference and $19.2\\%$ in training, while also reducing\nstraggler effect by up to $2.3\\times$. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 20:03:51 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 00:38:36 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Hashemi", "Sayed Hadi", ""], ["Jyothi", "Sangeetha Abdu", ""], ["Campbell", "Roy H.", ""]]}, {"id": "1803.03914", "submitter": "Niklas Carlsson", "authors": "Niklas Carlsson and Derek Eager", "title": "Optimized Dynamic Cache Instantiation and Accurate LRU Approximations\n  under Time-varying Request Volume", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By caching content at geographically distributed servers, content delivery\napplications can achieve scalability and reduce wide-area network traffic.\nHowever, each deployed cache has an associated cost. When the request rate from\nthe local region is sufficiently high this cost will be justified, but as the\nrequest rate varies, for example according to a daily cycle, there may be long\nperiods when the benefit of the cache does not justify the cost. Cloud\ncomputing offers a solution to problems of this kind, by supporting the dynamic\nallocation and release of resources. In this paper, we analyze the potential\nbenefits from dynamically instantiating caches using resources from cloud\nservice providers. We develop novel analytic caching models that accommodate\ntime-varying request rates, transient behavior as a cache fills following\ninstantiation, and selective cache insertion policies. Using these models,\nwithin the context of a simple cost model, we then develop bounds and compare\npolicies with optimized parameter selections to obtain insights into key\ncost/performance tradeoffs. We find (among other things) that dynamic cache\ninstantiation can provide substantial cost reductions, that potential\nreductions strongly dependent on the object popularity skew, and that selective\ncache insertion can be even more beneficial in this context than with\nconventional edge caches. Finally, our contributions also include accurate and\neasy-to-compute approximations that are shown applicable to LRU caches under\ntime-varying workloads.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 06:49:10 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 00:05:06 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Carlsson", "Niklas", ""], ["Eager", "Derek", ""]]}, {"id": "1803.04014", "submitter": "Stefano Markidis Prof.", "authors": "Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng,\n  Jeffrey S. Vetter", "title": "NVIDIA Tensor Core Programmability, Performance & Precision", "comments": "This paper has been accepted by the Eighth International Workshop on\n  Accelerators and Hybrid Exascale Systems (AsHES) 2018", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00091", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called\n\"Tensor Core\" that performs one matrix-multiply-and-accumulate on 4x4 matrices\nper clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta\nmicroarchitecture, provides 640 Tensor Cores with a theoretical peak\nperformance of 125 Tflops/s in mixed precision. In this paper, we investigate\ncurrent approaches to program NVIDIA Tensor Cores, their performances and the\nprecision loss due to computation in mixed precision.\n  Currently, NVIDIA provides three different ways of programming\nmatrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply\nAccumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS\nGEMM. After experimenting with different approaches, we found that NVIDIA\nTensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100\nGPU, seven and three times the performance in single and half precision\nrespectively. A WMMA implementation of batched GEMM reaches a performance of 4\nTflops/s. While precision loss due to matrix multiplication with half precision\ninput might be critical in many HPC applications, it can be considerably\nreduced at the cost of increased computation. Our results indicate that HPC\napplications using matrix multiplications can strongly benefit from using of\nNVIDIA Tensor Cores.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 18:55:29 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Markidis", "Stefano", ""], ["Der Chien", "Steven Wei", ""], ["Laure", "Erwin", ""], ["Peng", "Ivy Bo", ""], ["Vetter", "Jeffrey S.", ""]]}, {"id": "1803.05788", "submitter": "Tao Liu", "authors": "Zihao Liu, Tao Liu, Wujie Wen, Lei Jiang, Jie Xu, Yanzhi Wang, Gang\n  Quan", "title": "DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression\n  Framework", "comments": "55th Design Automation Conference (DAC2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of most fascinating machine learning techniques, deep neural network\n(DNN) has demonstrated excellent performance in various intelligent tasks such\nas image classification. DNN achieves such performance, to a large extent, by\nperforming expensive training over huge volumes of training data. To reduce the\ndata storage and transfer overhead in smart resource-limited Internet-of-Thing\n(IoT) systems, effective data compression is a \"must-have\" feature before\ntransferring real-time produced dataset for training or classification. While\nthere have been many well-known image compression approaches (such as JPEG), we\nfor the first time find that a human-visual based image compression approach\nsuch as JPEG compression is not an optimized solution for DNN systems,\nespecially with high compression ratios. To this end, we develop an image\ncompression framework tailored for DNN applications, named \"DeepN-JPEG\", to\nembrace the nature of deep cascaded information process mechanism of DNN\narchitecture. Extensive experiments, based on \"ImageNet\" dataset with various\nstate-of-the-art DNNs, show that \"DeepN-JPEG\" can achieve ~3.5x higher\ncompression rate over the popular JPEG solution while maintaining the same\naccuracy level for image recognition, demonstrating its great potential of\nstorage and power efficiency in DNN-based smart IoT system design.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 02:18:55 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Liu", "Zihao", ""], ["Liu", "Tao", ""], ["Wen", "Wujie", ""], ["Jiang", "Lei", ""], ["Xu", "Jie", ""], ["Wang", "Yanzhi", ""], ["Quan", "Gang", ""]]}, {"id": "1803.06068", "submitter": "Bahar Asgari", "authors": "Bahar Asgari, Saibal Mukhopadhyay, Sudhakar Yalamanchili", "title": "Memory Slices: A Modular Building Block for Scalable, Intelligent Memory\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While reduction in feature size makes computation cheaper in terms of\nlatency, area, and power consumption, performance of emerging data-intensive\napplications is determined by data movement. These trends have introduced the\nconcept of scalability as reaching a desirable performance per unit cost by\nusing as few number of units as possible. Many proposals have moved compute\ncloser to the memory. However, these efforts ignored maintaining a balance\nbetween bandwidth and compute rate of an architecture, with those of\napplications, which is a key principle in designing scalable large systems.\nThis paper proposes the use of memory slices, a modular building block for\nscalable memory systems integrated with compute, in which performance scales\nwith memory size (and volume of data). The slice architecture utilizes a\nprogrammable memory interface feeding a systolic compute engine with high reuse\nrate. The modularity feature of slice-based systems is exploited with a\npartitioning and data mapping strategy across allocated memory slices where\ntraining performance scales with the data size. These features enable shifting\nthe most pressure to cheap compute units rather than expensive memory accesses\nor transfers via interconnection network. An application of the memory slices\nto a scale-out memory system is accelerating the training of recurrent,\nconvolutional, and hybrid neural networks (RNNs and RNNs+CNN) that are forming\ncloud workloads. The results of our cycle-level simulations show that memory\nslices exhibits a superlinear speedup when the number of slices increases.\nFurthermore, memory slices improve power efficiency to 747 GFLOPs/J for\ntraining LSTMs. While our current evaluation uses memory slices with 3D\npackaging, a major value is that slices can also be constructed with a variety\nof packaging options, for example with DDR-based memory units.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 03:39:45 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Asgari", "Bahar", ""], ["Mukhopadhyay", "Saibal", ""], ["Yalamanchili", "Sudhakar", ""]]}, {"id": "1803.06185", "submitter": "Stuart Biles", "authors": "Nigel Stephens, Stuart Biles, Matthias Boettcher, Jacob Eapen, Mbou\n  Eyole, Giacomo Gabrielli, Matt Horsnell, Grigorios Magklis, Alejandro\n  Martinez, Nathanael Premillieu, Alastair Reid, Alejandro Rico, Paul Walker", "title": "The ARM Scalable Vector Extension", "comments": "8 pages, 8 figures, IEEE Micro paper", "journal-ref": "IEEE Micro ( Volume: 37, Issue: 2, Mar.-Apr. 2017 )", "doi": "10.1109/MM.2017.35", "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the ARM Scalable Vector Extension (SVE). Several goals\nguided the design of the architecture. First was the need to extend the vector\nprocessing capability associated with the ARM AArch64 execution state to better\naddress the computational requirements in domains such as high-performance\ncomputing, data analytics, computer vision, and machine learning. Second was\nthe desire to introduce an extension that can scale across multiple\nimplementations, both now and into the future, allowing CPU designers to choose\nthe vector length most suitable for their power, performance, and area targets.\nFinally, the architecture should avoid imposing a software development cost as\nthe vector length changes and where possible reduce it by improving the reach\nof compiler auto-vectorization technologies. SVE achieves these goals. It\nallows implementations to choose a vector register length between 128 and 2,048\nbits. It supports a vector-length agnostic programming model that lets code run\nand scale automatically across all vector lengths without recompilation.\nFinally, it introduces several innovative features that begin to overcome some\nof the traditional barriers to autovectorization.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 12:16:36 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Stephens", "Nigel", ""], ["Biles", "Stuart", ""], ["Boettcher", "Matthias", ""], ["Eapen", "Jacob", ""], ["Eyole", "Mbou", ""], ["Gabrielli", "Giacomo", ""], ["Horsnell", "Matt", ""], ["Magklis", "Grigorios", ""], ["Martinez", "Alejandro", ""], ["Premillieu", "Nathanael", ""], ["Reid", "Alastair", ""], ["Rico", "Alejandro", ""], ["Walker", "Paul", ""]]}, {"id": "1803.07689", "submitter": "Debankur Mukherjee", "authors": "Debankur Mukherjee and Alexander Stolyar", "title": "Join-Idle-Queue with Service Elasticity: Large-Scale Asymptotics of a\n  Non-monotone System", "comments": "30 pages", "journal-ref": "Stoch. Syst. 9 4 (2019)", "doi": "10.1287/stsy.2019.0030", "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the model of a token-based joint auto-scaling and load balancing\nstrategy, proposed in a recent paper by Mukherjee, Dhara, Borst, and van\nLeeuwaarden (SIGMETRICS '17, arXiv:1703.08373), which offers an efficient\nscalable implementation and yet achieves asymptotically optimal steady-state\ndelay performance and energy consumption as the number of servers $N\\to\\infty$.\nIn the above work, the asymptotic results are obtained under the assumption\nthat the queues have fixed-size finite buffers, and therefore the fundamental\nquestion of stability of the proposed scheme with infinite buffers was left\nopen. In this paper, we address this fundamental stability question. The system\nstability under the usual subcritical load assumption is not automatic.\nMoreover, the stability may not even hold for all $N$. The key challenge stems\nfrom the fact that the process lacks monotonicity, which has been the powerful\nprimary tool for establishing stability in load balancing models. We develop a\nnovel method to prove that the subcritically loaded system is stable for large\nenough $N$, and establish convergence of steady-state distributions to the\noptimal one, as $N \\to \\infty$. The method goes beyond the state of the art\ntechniques -- it uses an induction-based idea and a \"weak monotonicity\"\nproperty of the model; this technique is of independent interest and may have\nbroader applicability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 23:24:57 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Mukherjee", "Debankur", ""], ["Stolyar", "Alexander", ""]]}, {"id": "1803.08121", "submitter": "Pierre-Cyrille Heam", "authors": "Louis-Claude Canon, Mohamad El Sayah, Pierre-Cyrille H\\'eam", "title": "A Markov Chain Monte Carlo Approach to Cost Matrix Generation for\n  Scheduling Performance Evaluation", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high performance computing, scheduling of tasks and allocation to machines\nis very critical especially when we are dealing with heterogeneous execution\ncosts. Simulations can be performed with a large variety of environments and\napplication models. However, this technique is sensitive to bias when it relies\non random instances with an uncontrolled distribution. We use methods from the\nliterature to provide formal guarantee on the distribution of the instance. In\nparticular, it is desirable to ensure a uniform distribution among the\ninstances with a given task and machine heterogeneity. In this article, we\npropose a method that generates instances (cost matrices) with a known\ndistribution where tasks are scheduled on machines with heterogeneous execution\ncosts.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 20:41:00 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Canon", "Louis-Claude", ""], ["Sayah", "Mohamad El", ""], ["H\u00e9am", "Pierre-Cyrille", ""]]}, {"id": "1803.08981", "submitter": "Elizabeth Bradley", "authors": "James Garnett and Elizabeth Bradley", "title": "Unix Memory Allocations are Not Poisson", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multitasking operating systems, requests for free memory are traditionally\nmodeled as a stochastic counting process with independent,\nexponentially-distributed interarrival times because of the analytic simplicity\nsuch Poisson models afford. We analyze the distribution of several million unix\npage commits to show that although this approach could be valid over relatively\nlong timespans, the behavior of the arrival process over shorter periods is\ndecidedly not Poisson. We find that this result holds regardless of the\noriginator of the request: unlike network packets, there is little difference\nbetween system- and user-level page-request distributions. We believe this to\nbe due to the bursty nature of page allocations, which tend to occur in either\nsmall or extremely large increments. Burstiness and persistent variance have\nrecently been found in self-similar processes in computer networks, but we show\nthat although page commits are both bursty and possess high variance over long\ntimescales, they are probably not self-similar. These results suggest that\naltogether different models are needed for fine-grained analysis of memory\nsystems, an important consideration not only for understanding behavior but\nalso for the design of online control systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 21:08:36 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Garnett", "James", ""], ["Bradley", "Elizabeth", ""]]}, {"id": "1803.09584", "submitter": "Radhika Jagtap", "authors": "Alexandra Ferreron, Radhika Jagtap, Sascha Bischoff, Roxana Rusitoru", "title": "Crossing the Architectural Barrier: Evaluating Representative Regions of\n  Parallel HPC Applications", "comments": "2017 IEEE International Symposium on Performance Analysis of Systems\n  and Software (ISPASS)", "journal-ref": null, "doi": "10.1109/ISPASS.2017.7975275", "report-no": null, "categories": "cs.PF cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exascale computing will get mankind closer to solving important social,\nscientific and engineering problems. Due to high prototyping costs, High\nPerformance Computing (HPC) system architects make use of simulation models for\ndesign space exploration and hardware-software co-design. However, as HPC\nsystems reach exascale proportions, the cost of simulation increases, since\nsimulators themselves are largely single-threaded. Tools for selecting\nrepresentative parts of parallel applications to reduce running costs are\nwidespread, e.g., BarrierPoint achieves this by analysing, in simulation,\nabstract characteristics such as basic blocks and reuse distances. However,\narchitectures new to HPC have a limited set of tools available.\n  In this work, we provide an independent cross-architectural evaluation on\nreal hardware - across Intel and ARM - of the BarrierPoint methodology, when\napplied to parallel HPC proxy applications. We present both cases: when the\nmethodology can be applied and when it cannot. In the former case, results show\nthat we can predict the performance of full application execution by running\nshorter representative sections. In the latter case, we dive into the\nunderlying issues and suggest improvements. We demonstrate a total simulation\ntime reduction of up to 178x, whilst keeping the error below 2.3% for both\ncycles and instructions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 19:40:03 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ferreron", "Alexandra", ""], ["Jagtap", "Radhika", ""], ["Bischoff", "Sascha", ""], ["Rusitoru", "Roxana", ""]]}, {"id": "1803.09866", "submitter": "Christoph Salge", "authors": "Christoph Salge, Christian Guckelsberger, Rodrigo Canaan and Tobias\n  Mahlmann", "title": "Accelerating Empowerment Computation with UCT Tree Search", "comments": "8ish pages, 6 figures, data for graphs included in sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of intrinsic motivation present an important means to produce sensible\nbehaviour in the absence of extrinsic rewards. Applications in video games are\nvaried, and range from intrinsically motivated general game-playing agents to\nnon-player characters such as companions and enemies. The information-theoretic\nquantity of Empowerment is a particularly promising candidate motivation to\nproduce believable, generic and robust behaviour. However, while it can be used\nin the absence of external reward functions that would need to be crafted and\nlearned, empowerment is computationally expensive. In this paper, we propose a\nmodified UCT tree search method to mitigate empowerment's computational\ncomplexity in discrete and deterministic scenarios. We demonstrate how to\nmodify a Monte-Carlo Search Tree with UCT to realise empowerment maximisation,\nand discuss three additional modifications that facilitate better sampling. We\nevaluate the approach both quantitatively, by analysing how close our approach\ngets to the baseline of exhaustive empowerment computation with varying amounts\nof computational resources, and qualitatively, by analysing the resulting\nbehaviour in a Minecraft-like scenario.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 03:05:35 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Salge", "Christoph", ""], ["Guckelsberger", "Christian", ""], ["Canaan", "Rodrigo", ""], ["Mahlmann", "Tobias", ""]]}, {"id": "1803.09948", "submitter": "Mustafa Abduljabbar", "authors": "Mustafa Abduljabbar, Mohammed Al Farhan, Noha Al-Harthi, Rui Chen, Rio\n  Yokota, Hakan Bagci, and David Keyes", "title": "Extreme Scale FMM-Accelerated Boundary Integral Equation Solver for Wave\n  Scattering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic and architecture-oriented optimizations are essential for\nachieving performance worthy of anticipated energy-austere exascale systems. In\nthis paper, we present an extreme scale FMM-accelerated boundary integral\nequation solver for wave scattering, which uses FMM as a matrix-vector\nmultiplication inside the GMRES iterative method. Our FMM Helmholtz kernels\ntreat nontrivial singular and near-field integration points. We implement\nhighly optimized kernels for both shared and distributed memory, targeting\nemerging Intel extreme performance HPC architectures. We extract the potential\nthread- and data-level parallelism of the key Helmholtz kernels of FMM. Our\napplication code is well optimized to exploit the AVX-512 SIMD units of Intel\nSkylake and Knights Landing architectures. We provide different performance\nmodels for tuning the task-based tree traversal implementation of FMM, and\ndevelop optimal architecture-specific and algorithm aware partitioning, load\nbalancing, and communication reducing mechanisms to scale up to 6,144 compute\nnodes of a Cray XC40 with 196,608 hardware cores. With shared memory\noptimizations, we achieve roughly 77% of peak single precision floating point\nperformance of a 56-core Skylake processor, and on average 60% of peak single\nprecision floating point performance of a 72-core KNL. These numbers represent\nnearly 5.4x and 10x speedup on Skylake and KNL, respectively, compared to the\nbaseline scalar code. With distributed memory optimizations, on the other hand,\nwe report near-optimal efficiency in the weak scalability study with respect to\nboth the logarithmic communication complexity as well as the theoretical\nscaling complexity of FMM. In addition, we exhibit up to 85% efficiency in\nstrong scaling. We compute in excess of 2 billion DoF on the full-scale of the\nCray XC40 supercomputer.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 08:12:13 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Abduljabbar", "Mustafa", ""], ["Farhan", "Mohammed Al", ""], ["Al-Harthi", "Noha", ""], ["Chen", "Rui", ""], ["Yokota", "Rio", ""], ["Bagci", "Hakan", ""], ["Keyes", "David", ""]]}, {"id": "1803.10553", "submitter": "Takashi Ikegawa", "authors": "Takashi Ikegawa", "title": "Effect of payload size on mean response time when message segmentations\n  occur using $\\rm{M}^{\\rm X}/\\rm{G}/1$ queueing model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the $\\rm{M}^{\\rm X}/\\rm{G}/1$ queueing model to represent\narrivals of segmented packets when message segmentations occur. This queueing\nmodel enables us to derive the closed form of mean response time, given payload\nsize, message size distribution and message arrival rate. From a numerical\nresult, we show that the mean response time is more convex in payload sizes if\nmessage arrival rate is larger in a scenario where Web objects are delivered\nover a physical link.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 12:10:26 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Ikegawa", "Takashi", ""]]}, {"id": "1803.10688", "submitter": "Olivier Bilenne", "authors": "Olivier Bilenne", "title": "Dispatching to Parallel Servers: Solutions of Poisson's Equation for\n  First-Policy Improvement", "comments": "34 pages, including 6 figures and 4 appendices; supplementary\n  material (11 pages) available under 'Ancillary files'. Submitted for\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy iteration techniques for multiple-server dispatching rely on the\ncomputation of value functions. In this context, we consider the\ncontinuous-space M/G/1-FCFS queue endowed with an arbitrarily-designed cost\nfunction for the waiting times of the incoming jobs. The associated value\nfunction is a solution of Poisson's equation for Markov chains, which in this\nwork we solve in the Laplace transform domain by considering an ancillary,\nunderlying stochastic process extended to (imaginary) negative backlog states.\nThis construction enables us to issue closed-form value functions for\npolynomial and exponential cost functions and for piecewise compositions of the\nlatter, in turn permitting the derivation of interval bounds for the value\nfunction in the form of power series or trigonometric sums. We review various\ncost approximation schemes and assess the convergence of the interval bounds\nthese induce on the value function. Namely: Taylor expansions (divergent,\nexcept for a narrow class of entire functions with low orders of growth), and\nuniform approximation schemes (polynomials, trigonometric), which achieve\noptimal convergence rates over finite intervals. This study addresses all the\nsteps to implementing dispatching policies for systems of parallel servers,\nfrom the specification of general cost functions towards the computation of\ninterval bounds for the value functions and the exact implementation of the\nfirst-policy improvement step.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 15:44:48 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 14:02:40 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 08:10:42 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Bilenne", "Olivier", ""]]}, {"id": "1803.11151", "submitter": "Crefeda Rodrigues", "authors": "Crefeda Faviola Rodrigues, Graham Riley and Mikel Lujan", "title": "Fine-Grained Energy and Performance Profiling framework for Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a huge demand for on-device execution of deep learning algorithms on\nmobile and embedded platforms. These devices present constraints on the\napplication due to limited resources and power. Hence, developing\nenergy-efficient solutions to address this issue will require innovation in\nalgorithmic design, software and hardware. Such innovation requires\nbenchmarking and characterization of Deep Neural Networks based on performance\nand energy-consumption alongside accuracy. However, current benchmarks studies\nin existing deep learning frameworks (for example, Caffe, Tensorflow, Torch and\nothers) are based on performance of these applications on high-end CPUs and\nGPUs. In this work, we introduce a benchmarking framework called \"SyNERGY\" to\nmeasure the energy and time of 11 representative Deep Convolutional Neural\nNetworks on embedded platforms such as NVidia Jetson TX1. We integrate ARM's\nStreamline Performance Analyser with standard deep learning frameworks such as\nCaffe and CuDNNv5, to study the execution behaviour of current deep learning\nmodels at a fine-grained level (or specific layers) on image processing tasks.\nIn addition, we build an initial multi-variable linear regression model to\npredict energy consumption of unseen neural network models based on the number\nof SIMD instructions executed and main memory accesses of the CPU cores of the\nTX1 with an average relative test error rate of 8.04 +/- 5.96 %. Surprisingly,\nwe find that it is possible to refine the model to predict the number of SIMD\ninstructions and main memory accesses solely from the application's\nMultiply-Accumulate (MAC) counts, hence, eliminating the need for actual\nmeasurements. Our predicted results demonstrate 7.08 +/- 6.0 % average relative\nerror over actual energy measurements of all 11 networks tested, except\nMobileNet. By including MobileNet the average relative test error increases to\n17.33 +/- 12.2 %.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 16:55:07 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 15:36:43 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Rodrigues", "Crefeda Faviola", ""], ["Riley", "Graham", ""], ["Lujan", "Mikel", ""]]}, {"id": "1803.11179", "submitter": "Benjamin Andreassen Bj{\\o}rnseth", "authors": "Benjamin Andreassen Bj{\\o}rnseth, Jan Christian Meyer, Lasse Natvig", "title": "Proof-of-Concept Examples of Performance-Transparent Programming Models", "comments": "Companion report to short-paper \"Make Software Harder\", to be\n  presented at Computing Frontiers 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-specific optimizations command the machine to behave in a specific\nway. As current programming models largely leave machine details unexposed,\nthey cannot accommodate direct encoding of such commands. In previous work we\nhave proposed the design of performance-transparent programming models to\nfacilitate this use-case; this report contains proof-of-concept examples of\nsuch programming models. We demonstrate how programming model abstractions may\nreveal the memory footprint, vector unit utilization and data reuse of an\napplication, with prediction accuracy ranging from 0 to 25 \\%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:51:18 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Bj\u00f8rnseth", "Benjamin Andreassen", ""], ["Meyer", "Jan Christian", ""], ["Natvig", "Lasse", ""]]}, {"id": "1803.11440", "submitter": "Avi Mendelson", "authors": "Ori Chalak, Cai Weiguang, Li Wei, Fang Lei, Zheng Libing, Wang\n  Jintang, Wu Zuguang, Gu Xiongli, Wang Haibin, Avi Mendelson", "title": "ScaleSimulator: A Fast and Cycle-Accurate Parallel Simulator for\n  Architectural Exploration", "comments": "Was published in SIMUTools 2017\n  https://drive.google.com/file/d/0B-bj84Yl7TM4R0NJRC16dnUxX0U/view", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of next generation computer systems should be supported by simulation\ninfrastructure that must achieve a few contradictory goals such as fast\nexecution time, high accuracy, and enough flexibility to allow comparison\nbetween large numbers of possible design points. Most existing architecture\nlevel simulators are designed to be flexible and to execute the code in\nparallel for greater efficiency, but at the cost of scarified accuracy. This\npaper presents the ScaleSimulator simulation environment, which is based on a\nnew design methodology whose goal is to achieve near cycle accuracy while still\nbeing flexible enough to simulate many different future system architectures\nand efficient enough to run meaningful workloads. We achieve these goals by\nmaking the parallelism a first-class citizen in our methodology. Thus, this\npaper focuses mainly on the ScaleSimulator design points that enable better\nparallel execution while maintaining the scalability and cycle accuracy of a\nsimulated architecture. The paper indicates that the new proposed\nScaleSimulator tool can (1) efficiently parallelize the execution of a\ncycle-accurate architecture simulator, (2) efficiently simulate complex\narchitectures (e.g., out-of-order CPU pipeline, cache coherency protocol, and\nnetwork) and massive parallel systems, and (3) use meaningful workloads, such\nas full simulation of OLTP benchmarks, to examine future architectural choices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 18:20:20 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Chalak", "Ori", ""], ["Weiguang", "Cai", ""], ["Wei", "Li", ""], ["Lei", "Fang", ""], ["Libing", "Zheng", ""], ["Jintang", "Wang", ""], ["Zuguang", "Wu", ""], ["Xiongli", "Gu", ""], ["Haibin", "Wang", ""], ["Mendelson", "Avi", ""]]}]