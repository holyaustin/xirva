[{"id": "2004.00518", "submitter": "Mehrnoosh Shafiee", "authors": "Mehrnoosh Shafiee and Javad Ghaderi", "title": "Scheduling Parallel-Task Jobs Subject to Packing and Placement\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by modern parallel computing applications, we consider the problem\nof scheduling parallel-task jobs with heterogeneous resource requirements in a\ncluster of machines. Each job consists of a set of tasks that can be processed\nin parallel, however, the job is considered completed only when all its tasks\nfinish their processing, which we refer to as \"synchronization\" constraint.\nFurther, assignment of tasks to machines is subject to \"placement\" constraints,\ni.e., each task can be processed only on a subset of machines, and processing\ntimes can also be machine dependent. Once a task is scheduled on a machine, it\nrequires a certain amount of resource from that machine for the duration of its\nprocessing. A machine can process (\"pack\") multiple tasks at the same time,\nhowever the cumulative resource requirement of the tasks should not exceed the\nmachine's capacity.\n  Our objective is to minimize the weighted average of the jobs' completion\ntimes. The problem, subject to synchronization, packing and placement\nconstraints, is NP-hard, and prior theoretical results only concern much\nsimpler models. For the case that migration of tasks among the\nplacement-feasible machines is allowed, we propose a preemptive algorithm with\nan approximation ratio of $(6+\\epsilon)$. In the special case that only one\nmachine can process each task, we design an algorithm with improved\napproximation ratio of $4$. Finally, in the case that migrations (and\npreemptions) are not allowed, we design an algorithm with an approximation\nratio of $24$. Our algorithms use a combination of linear program relaxation\nand greedy packing techniques. We present extensive simulation results, using a\nreal traffic trace, that demonstrate that our algorithms yield significant\ngains over the prior approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 15:42:30 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 01:34:46 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Shafiee", "Mehrnoosh", ""], ["Ghaderi", "Javad", ""]]}, {"id": "2004.00876", "submitter": "Tim Hellemans", "authors": "Tim Hellemans, Benny Van Houdt", "title": "Mean Waiting Time in Large-Scale and Critically Loaded Power of d Load\n  Balancing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field models are a popular tool used to analyse load balancing policies.\nIn some cases the waiting time distribution of the mean field limit has an\nexplicit form. In other cases it can be computed as the solution of a set of\ndifferential equations. Here we study the limit of the mean waiting time\n$E[W_\\lambda]$ as the arrival rate $\\lambda$ approaches $1$ for a number of\nload balancing policies when job sizes are exponential with mean $1$ (i.e. the\nsystem gets close to instability). As $E[W_\\lambda]$ diverges to infinity, we\nscale with $-\\log(1-\\lambda)$ and present a method to compute the limit\n$\\lim_{\\lambda\\rightarrow 1^-}-E[W_\\lambda]/\\log(1-\\lambda)$. This limit has a\nsurprisingly simple form for the load balancing algorithms considered. We\npresent a general result that holds for any policy for which the associated\ndifferential equation satisfies a list of assumptions. For the LL(d) policy\nwhich assigns an incoming job to a server with the least work left among d\nrandomly selected servers these assumptions are trivially verified. For this\npolicy we prove the limit is given by $\\frac{1}{d-1}$. We further show that the\nLL(d,K) policy, which assigns batches of $K$ jobs to the $K$ least loaded\nservers among d randomly selected servers, satisfies the assumptions and the\nlimit is equal to $\\frac{K}{d-K}$. For a policy which applies LL($d_i$) with\nprobability $p_i$, we show that the limit is given by\n$\\frac{1}{\\sum_ip_id_i-1}$. We further indicate that our main result can also\nbe used for load balancers with redundancy or memory. In addition, we propose\nan alternate scaling $-\\log(p_\\lambda)$ instead of $-\\log(1-\\lambda)$, for\nwhich the limit $\\lim_{\\lambda\\rightarrow 0^+}-E[W_\\lambda]/\\log(p_\\lambda)$ is\nwell defined and non-zero (contrary to $\\lim_{\\lambda\\rightarrow\n0^+}-E[W_\\lambda]/\\log(1-\\lambda)$), while $\\lim_{\\lambda\\rightarrow\n1^-}\\log(1-\\lambda) / \\log(p_\\lambda)=1$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 08:51:43 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 14:10:37 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Hellemans", "Tim", ""], ["Van Houdt", "Benny", ""]]}, {"id": "2004.00991", "submitter": "Weixing Ji", "authors": "Jie Liu, Xiaotian Wu, Kai Zhang, Bing Liu, Renyi Bao, Xiao Chen, Yiran\n  Cai, Yiming Shen, Xinjun He, Jun Yan, Weixing Ji", "title": "Computational Performance of a Germline Variant Calling Pipeline for\n  Next Generation Sequencing", "comments": "6 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the booming of next generation sequencing technology and its\nimplementation in clinical practice and life science research, the need for\nfaster and more efficient data analysis methods becomes pressing in the field\nof sequencing. Here we report on the evaluation of an optimized germline\nmutation calling pipeline, HummingBird, by assessing its performance against\nthe widely accepted BWA-GATK pipeline. We found that the HummingBird pipeline\ncan significantly reduce the running time of the primary data analysis for\nwhole genome sequencing and whole exome sequencing while without significantly\nsacrificing the variant calling accuracy. Thus, we conclude that expansion of\nsuch software usage will help to improve the primary data analysis efficiency\nfor next generation sequencing.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:55:11 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Liu", "Jie", ""], ["Wu", "Xiaotian", ""], ["Zhang", "Kai", ""], ["Liu", "Bing", ""], ["Bao", "Renyi", ""], ["Chen", "Xiao", ""], ["Cai", "Yiran", ""], ["Shen", "Yiming", ""], ["He", "Xinjun", ""], ["Yan", "Jun", ""], ["Ji", "Weixing", ""]]}, {"id": "2004.01609", "submitter": "Andrea Valassi", "authors": "Andrea Valassi, Manfred Alef, Jean-Michel Barbet, Olga Datskova,\n  Riccardo De Maria, Miguel Fontes Medeiros, Domenico Giordano, Costin\n  Grigoras, Christopher Hollowell, Martina Javurkova, Viktor Khristenko, David\n  Lange, Michele Michelotto, Lorenzo Rinaldi, Andrea Sciab\\`a and Cas Van Der\n  Laan", "title": "Using HEP experiment workflows for the benchmarking and accounting of\n  WLCG computing resources", "comments": "9 pages, submitted to CHEP2019 proceedings in EPJ Web of Conferences;\n  revised version addressing referee's comments", "journal-ref": "EPJ Web of Conferences 245, 07035 (2020)", "doi": "10.1051/epjconf/202024507035", "report-no": null, "categories": "cs.PF hep-ex physics.ins-det", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Benchmarking of CPU resources in WLCG has been based on the HEP-SPEC06 (HS06)\nsuite for over a decade. It has recently become clear that HS06, which is based\non real applications from non-HEP domains, no longer describes typical HEP\nworkloads. The aim of the HEP-Benchmarks project is to develop a new benchmark\nsuite for WLCG compute resources, based on real applications from the LHC\nexperiments. By construction, these new benchmarks are thus guaranteed to have\na score highly correlated to the throughputs of HEP applications, and a CPU\nusage pattern similar to theirs. Linux containers and the CernVM-FS filesystem\nare the two main technologies enabling this approach, which had been considered\nimpossible in the past. In this paper, we review the motivation, implementation\nand outlook of the new benchmark suite.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 14:57:46 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 08:26:23 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Valassi", "Andrea", ""], ["Alef", "Manfred", ""], ["Barbet", "Jean-Michel", ""], ["Datskova", "Olga", ""], ["De Maria", "Riccardo", ""], ["Medeiros", "Miguel Fontes", ""], ["Giordano", "Domenico", ""], ["Grigoras", "Costin", ""], ["Hollowell", "Christopher", ""], ["Javurkova", "Martina", ""], ["Khristenko", "Viktor", ""], ["Lange", "David", ""], ["Michelotto", "Michele", ""], ["Rinaldi", "Lorenzo", ""], ["Sciab\u00e0", "Andrea", ""], ["Van Der Laan", "Cas", ""]]}, {"id": "2004.02081", "submitter": "Wentao Weng", "authors": "Wentao Weng, Weina Wang", "title": "Achieving Zero Asymptotic Queueing Delay for Parallel Jobs", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero queueing delay is highly desirable in large-scale computing systems.\nExisting work has shown that it can be asymptotically achieved by using the\ncelebrated Power-of-$d$-choices (pod) policy with a probe overhead $d =\n\\omega\\left(\\frac{\\log N}{1-\\lambda}\\right)$, and it is impossible when $d =\nO\\left(\\frac{1}{1-\\lambda}\\right)$, where $N$ is the number of servers and\n$\\lambda$ is the load of the system. However, these results are based on the\nmodel where each job is an indivisible unit, which does not capture the\nparallel structure of jobs in today's predominant parallel computing paradigm.\n  This paper thus considers a model where each job consists of a batch of\nparallel tasks. Under this model, we propose a new notion of zero (asymptotic)\nqueueing delay that requires the job delay under a policy to approach the job\ndelay given by the max of its tasks' service times, i.e., the job delay\nassuming its tasks entered service right upon arrival. This notion quantifies\nthe effect of queueing on a job level for jobs consisting of multiple tasks,\nand thus deviates from the conventional zero queueing delay for single-task\njobs in the literature.\n  We show that zero queueing delay for parallel jobs can be achieved using the\nbatch-filling policy (a variant of the celebrated pod policy) with a probe\noverhead $d = \\omega\\left(\\frac{1}{(1-\\lambda)\\log k}\\right)$ in the\nsub-Halfin-Whitt heavy-traffic regime, where $k$ is the number of tasks in each\njob { and $k$ properly scales with $N$ (the number of servers)}. This result\ndemonstrates that for parallel jobs, zero queueing delay can be achieved with a\nsmaller probe overhead. We also establish an impossibility result: we show that\nzero queueing delay cannot be achieved if $d = \\exp\\left({o\\left(\\frac{\\log\nN}{\\log k}\\right)}\\right)$.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 03:11:35 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 13:41:39 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 11:31:52 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Weng", "Wentao", ""], ["Wang", "Weina", ""]]}, {"id": "2004.03072", "submitter": "Shijian Li", "authors": "Shijian Li and Robert J. Walls and Tian Guo", "title": "Characterizing and Modeling Distributed Training with Transient Cloud\n  GPU Servers", "comments": "11 pages, 12 figures, 5 tables, in proceedings of 40th IEEE\n  International Conference on Distributed Computing Systems (ICDCS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud GPU servers have become the de facto way for deep learning\npractitioners to train complex models on large-scale datasets. However, it is\nchallenging to determine the appropriate cluster configuration---e.g., server\ntype and number---for different training workloads while balancing the\ntrade-offs in training time, cost, and model accuracy. Adding to the complexity\nis the potential to reduce the monetary cost by using cheaper, but revocable,\ntransient GPU servers.\n  In this work, we analyze distributed training performance under diverse\ncluster configurations using CM-DARE, a cloud-based measurement and training\nframework. Our empirical datasets include measurements from three GPU types,\nsix geographic regions, twenty convolutional neural networks, and thousands of\nGoogle Cloud servers. We also demonstrate the feasibility of predicting\ntraining speed and overhead using regression-based models. Finally, we discuss\npotential use cases of our performance modeling such as detecting and\nmitigating performance bottlenecks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 01:49:58 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Li", "Shijian", ""], ["Walls", "Robert J.", ""], ["Guo", "Tian", ""]]}, {"id": "2004.03276", "submitter": "Joel Scheuner", "authors": "Joel Scheuner, Philipp Leitner", "title": "Function-as-a-Service Performance Evaluation: A Multivocal Literature\n  Review", "comments": "improvements including postprint updates", "journal-ref": null, "doi": "10.1016/j.jss.2020.110708", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Function-as-a-Service (FaaS) is one form of the serverless cloud computing\nparadigm and is defined through FaaS platforms (e.g., AWS Lambda) executing\nevent-triggered code snippets (i.e., functions). Many studies that empirically\nevaluate the performance of such FaaS platforms have started to appear but we\nare currently lacking a comprehensive understanding of the overall domain. To\naddress this gap, we conducted a multivocal literature review (MLR) covering\n112 studies from academic (51) and grey (61) literature. We find that existing\nwork mainly studies the AWS Lambda platform and focuses on micro-benchmarks\nusing simple functions to measure CPU speed and FaaS platform overhead (i.e.,\ncontainer cold starts). Further, we discover a mismatch between academic and\nindustrial sources on tested platform configurations, find that function\ntriggers remain insufficiently studied, and identify HTTP API gateways and\ncloud storages as the most used external service integrations. Following\nexisting guidelines on experimentation in cloud systems, we discover many flaws\nthreatening the reproducibility of experiments presented in the surveyed\nstudies. We conclude with a discussion of gaps in literature and highlight\nmethodological suggestions that may serve to improve future FaaS performance\nevaluation studies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:21:47 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 15:31:53 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 16:35:24 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Scheuner", "Joel", ""], ["Leitner", "Philipp", ""]]}, {"id": "2004.03695", "submitter": "Johannes Seiferth", "authors": "Johannes Seiferth, Matthias Korch, and Thomas Rauber", "title": "Offsite Autotuning Approach -- Performance Model Driven Autotuning\n  Applied to Parallel Explicit ODE Methods", "comments": "Accepted preprint version; 19 pages; 5 figures; 4 tables; 7 listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autotuning techniques are a promising approach to minimize the otherwise\ntedious manual effort of optimizing scientific applications for a specific\ntarget platform. Ideally, an autotuning approach is capable of reliably\nidentifying the most efficient implementation variant(s) for a new target\nsystem or new characteristics of the input by applying suitable program\ntransformations and analytic models. In this work, we introduce Offsite, an\noffline autotuning approach which automates this selection process at\ninstallation time by rating implementation variants based on an analytic\nperformance model without requiring time-consuming runtime experiments. From\nabstract multilevel YAML description languages, Offsite automatically derives\noptimized, platform-specific and problem-specific code of possible\nimplementation variants and applies the performance model to these\nimplementation variants.\n  We apply Offsite to parallel numerical methods for ordinary differential\nequations (ODEs). In particular, we investigate tuning a specific class of\nexplicit ODE solvers (PIRK methods) for various initial value problems (IVPs)\non shared-memory systems. Our experiments demonstrate that Offsite is able to\nreliably identify a set of the most efficient implementation variants for given\ntest configurations (ODE solver, IVP, platform) and is capable of effectively\nhandling important autotuning scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:39:28 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Seiferth", "Johannes", ""], ["Korch", "Matthias", ""], ["Rauber", "Thomas", ""]]}, {"id": "2004.04208", "submitter": "Hamed Jahromi", "authors": "Hamed Z. Jahromi, Declan T. Delaney, Andrew Hines", "title": "How Crisp is the Crease? A Subjective Study on Web Browsing Perception\n  of Above-The-Fold", "comments": null, "journal-ref": null, "doi": "10.1109/NetSoft48620.2020.9165497", "report-no": null, "categories": "cs.HC cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Quality of Experience (QoE) for various types of websites has gained\nsignificant attention in recent years. In order to design and evaluate\nwebsites, a metric that can estimate a user's experienced quality robustly for\ndiverse content is necessary. SpeedIndex (SI) has been widely adopted to\nestimate perceived web page loading progress. It measures the speed of\nrendering pixels for the webpage that is visible in the browser window. This is\ntermed Above-The-Fold (ATF). The influence of animated content on the\nperception of ATF has been less comprehensively explored. In this paper, we\npresent an experimental design and methodology to measure ATF perception for\nwebsites with and without animated elements for various page content\ncategories. We found that pages with animated elements caused people to have\nmore varied perceptions of ATF under different network conditions. Animated\ncontent also impacts the page load estimation accuracy of SI for websites. We\ndiscuss how the difference in the perception of ATF will impact the QoE\nmanagement of web applications. We explain the necessity of revisiting the\nvisual assessment of ATF to include the animated contents and improve the\nrobustness of metrics like SI.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 19:09:33 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Jahromi", "Hamed Z.", ""], ["Delaney", "Declan T.", ""], ["Hines", "Andrew", ""]]}, {"id": "2004.04359", "submitter": "Arnab Das", "authors": "Arnab Das, Sriram Krishnamoorthy, Ian Briggs, Ganesh Gopalakrishnan,\n  Ramakrishna Tipireddy", "title": "FPDetect: Efficient Reasoning About Stencil Programs Using Selective\n  Direct Evaluation", "comments": "Accepted in Journal of ACM Transactions on Architecture and Code\n  Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA cs.PF math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present FPDetect, a low overhead approach for detecting logical errors and\nsoft errors affecting stencil computations without generating false positives.\nWe develop an offline analysis that tightly estimates the number of\nfloating-point bits preserved across stencil applications. This estimate\nrigorously bounds the values expected in the data space of the computation.\nViolations of this bound can be attributed with certainty to errors. FPDetect\nhelps synthesize error detectors customized for user-specified levels of\naccuracy and coverage. FPDetect also enables overhead reduction techniques\nbased on deploying these detectors coarsely in space and time. Experimental\nevaluations demonstrate the practicality of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 04:09:16 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 20:18:45 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 02:36:13 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2020 21:51:00 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Das", "Arnab", ""], ["Krishnamoorthy", "Sriram", ""], ["Briggs", "Ian", ""], ["Gopalakrishnan", "Ganesh", ""], ["Tipireddy", "Ramakrishna", ""]]}, {"id": "2004.04500", "submitter": "Mahmoud Bokhari", "authors": "Mahmoud A. Bokhari, Brad Alexander and Markus Wagner", "title": "Towards Rigorous Validation of Energy Optimisation Experiments", "comments": "9 pages, 7 figures, to appear in GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimisation of software energy consumption is of growing importance\nacross all scales of modern computing, i.e., from embedded systems to\ndata-centres. Practitioners in the field of Search-Based Software Engineering\nand Genetic Improvement of Software acknowledge that optimising software energy\nconsumption is difficult due to noisy and expensive fitness evaluations.\nHowever, it is apparent from results to date that more progress needs to be\nmade in rigorously validating optimisation results. This problem is pressing\nbecause modern computing platforms have highly complex and variable behaviour\nwith respect to energy consumption. To compare solutions fairly we propose in\nthis paper a new validation approach called R3-validation which exercises\nsoftware variants in a rotated-round-robin order. Using a case study, we\npresent an in-depth analysis of the impacts of changing system states on\nsoftware energy usage, and we show how R3-validation mitigates these. We\ncompare it with current validation approaches across multiple devices and\noperating systems, and we show that it aligns better with actual platform\nbehaviour.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 11:59:23 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Bokhari", "Mahmoud A.", ""], ["Alexander", "Brad", ""], ["Wagner", "Markus", ""]]}, {"id": "2004.05137", "submitter": "Crefeda Rodrigues", "authors": "Crefeda Faviola Rodrigues, Graham Riley, Mikel Lujan", "title": "Energy Predictive Models for Convolutional Neural Networks on Mobile\n  Platforms", "comments": "9 pages, 4 Figures", "journal-ref": null, "doi": "10.13140/RG.2.2.15224.80644", "report-no": null, "categories": "cs.PF cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Energy use is a key concern when deploying deep learning models on mobile and\nembedded platforms. Current studies develop energy predictive models based on\napplication-level features to provide researchers a way to estimate the energy\nconsumption of their deep learning models. This information is useful for\nbuilding resource-aware models that can make efficient use of the hard-ware\nresources. However, previous works on predictive modelling provide little\ninsight into the trade-offs involved in the choice of features on the final\npredictive model accuracy and model complexity. To address this issue, we\nprovide a comprehensive analysis of building regression-based predictive models\nfor deep learning on mobile devices, based on empirical measurements gathered\nfrom the SyNERGY framework.Our predictive modelling strategy is based on two\ntypes of predictive models used in the literature:individual layers and\nlayer-type. Our analysis of predictive models show that simple layer-type\nfeatures achieve a model complexity of 4 to 32 times less for convolutional\nlayer predictions for a similar accuracy compared to predictive models using\nmore complex features adopted by previous approaches. To obtain an overall\nenergy estimate of the inference phase, we build layer-type predictive models\nfor the fully-connected and pooling layers using 12 representative\nConvolutional NeuralNetworks (ConvNets) on the Jetson TX1 and the Snapdragon\n820using software backends such as OpenBLAS, Eigen and CuDNN. We obtain an\naccuracy between 76% to 85% and a model complexity of 1 for the overall energy\nprediction of the test ConvNets across different hardware-software\ncombinations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 17:35:40 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Rodrigues", "Crefeda Faviola", ""], ["Riley", "Graham", ""], ["Lujan", "Mikel", ""]]}, {"id": "2004.05333", "submitter": "Soroush Ghodrati", "authors": "Soroush Ghodrati, Hardik Sharma, Cliff Young, Nam Sung Kim, Hadi\n  Esmaeilzadeh", "title": "Bit-Parallel Vector Composability for Neural Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional neural accelerators rely on isolated self-sufficient functional\nunits that perform an atomic operation while communicating the results through\nan operand delivery-aggregation logic. Each single unit processes all the bits\nof their operands atomically and produce all the bits of the results in\nisolation. This paper explores a different design style, where each unit is\nonly responsible for a slice of the bit-level operations to interleave and\ncombine the benefits of bit-level parallelism with the abundant data-level\nparallelism in deep neural networks. A dynamic collection of these units\ncooperate at runtime to generate bits of the results, collectively. Such\ncooperation requires extracting new grouping between the bits, which is only\npossible if the operands and operations are vectorizable. The abundance of Data\nLevel Parallelism and mostly repeated execution patterns, provides a unique\nopportunity to define and leverage this new dimension of Bit-Parallel Vector\nComposability. This design intersperses bit parallelism within data-level\nparallelism and dynamically interweaves the two together. As such, the building\nblock of our neural accelerator is a Composable Vector Unit that is a\ncollection of Narrower-Bitwidth Vector Engines, which are dynamically composed\nor decomposed at the bit granularity. Using six diverse CNN and LSTM deep\nnetworks, we evaluate this design style across four design points: with and\nwithout algorithmic bitwidth heterogeneity and with and without availability of\na high-bandwidth off-chip memory. Across these four design points, Bit-Parallel\nVector Composability brings (1.4x to 3.5x) speedup and (1.1x to 2.7x) energy\nreduction. We also comprehensively compare our design style to the Nvidia RTX\n2080 TI GPU, which also supports INT-4 execution. The benefits range between\n28.0x and 33.7x improvement in Performance-per-Watt.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 08:09:59 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ghodrati", "Soroush", ""], ["Sharma", "Hardik", ""], ["Young", "Cliff", ""], ["Kim", "Nam Sung", ""], ["Esmaeilzadeh", "Hadi", ""]]}, {"id": "2004.05628", "submitter": "Reena Nair", "authors": "Reena Nair and Tony Field", "title": "GAPP: A Fast Profiler for Detecting Serialization Bottlenecks in\n  Parallel Linux Applications", "comments": "8 pages", "journal-ref": null, "doi": "10.1145/3358960.3379136", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel profiling tool, GAPP, that identifies serialization\nbottlenecks in parallel Linux applications arising from load imbalance or\ncontention for shared resources . It works by tracing kernel context switch\nevents using kernel probes managed by the extended Berkeley Packet Filter\n(eBPF) framework. The overhead is thus extremely low (an average 4% run time\noverhead for the applications explored), the tool requires no program\ninstrumentation and works for a variety of serialization bottlenecks. We\nevaluate GAPP using the Parsec3.0 benchmark suite and two large open-source\nprojects: MySQL and Nektar++ (a spectral/hp element framework). We show that\nGAPP is able to reveal a wide range of bottleneck-related performance issues,\nfor example arising from synchronization primitives, busy-wait loops, memory\noperations, thread imbalance and resource contention.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 15:01:04 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Nair", "Reena", ""], ["Field", "Tony", ""]]}, {"id": "2004.06254", "submitter": "Amirali Daghighi", "authors": "Amirali Daghighi and Jim Q. Chen", "title": "Comparisons of Algorithms in Big Data Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel computing is the fundamental base for MapReduce framework in Hadoop.\nEach data chunk is replicated over 3 servers for increasing availability of\ndata and decreasing probability of data loss. Hence, the 3 servers that have\nMap task stored on their disk are fastest servers to process them, which are\ncalled local servers. All servers in the same rack as local servers are called\nrack-local servers that are slower than local servers since data chunk\nassociated with Map task should be fetched through top of the rack switch. All\nother servers are called remote servers that are slowest servers since they\nneed to fetch data from a local server in another rack, so data should be\ntransmitted through at least 2 top of rack switches and a core switch. Note\nthat number of switches in path of data transfer depends on internal network\nstructure of data centers. The First-In-First-Out (FIFO) and Hadoop Fair\nScheduler (HFS) algorithms do not take rack structure of data centers into\naccount, so they are known to not be heavy-traffic delay optimal or even\nthroughput optimal. The recent advances on scheduling for data centers\nconsidering rack structure of them and heterogeneity of servers resulted in\nstate-of-the-art Balanced-PANDAS algorithm that outperforms classic MaxWeight\nalgorithm. In both Balanced-PANDAS and MaxWeight algorithms, processing rate of\nlocal, rack-local, and remote servers are assumed to be known. However, with\nthe change of traffic over time in addition to estimation errors of processing\nrates, it is not realistic to consider processing rates to be known. In this\nwork, we study robustness of Balanced-PANDAS and MaxWeight algorithms in terms\nof inaccurate estimations of processing rates. We observe that Balanced-PANDAS\nis not as sensitive as MaxWeight on the accuracy of processing rates, making it\nmore appealing to use in data centers.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 01:09:25 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 12:02:41 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Daghighi", "Amirali", ""], ["Chen", "Jim Q.", ""]]}, {"id": "2004.06637", "submitter": "Clemens Dubslaff", "authors": "Clemens Dubslaff, Andrey Morozov, Christel Baier, Klaus Janschek", "title": "Reduction Methods on Probabilistic Control-flow Programs for Reliability\n  Analysis", "comments": "This paper is a preprint of the corresponding ESREL/PSAM 2020\n  conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern safety-critical systems are heterogeneous, complex, and highly\ndynamic. They require reliability evaluation methods that go beyond the\nclassical static methods such as fault trees, event trees, or reliability block\ndiagrams. Promising dynamic reliability analysis methods employ probabilistic\nmodel checking on various probabilistic state-based models. However, such\nmethods have to tackle the well-known state-space explosion problem. To compete\nwith this problem, reduction methods such as symmetry reduction and\npartial-order reduction have been successfully applied to probabilistic models\nby means of discrete Markov chains or Markov decision processes. Such models\nare usually specified using probabilistic programs provided in guarded command\nlanguage. In this paper, we propose two automated reduction methods for\nprobabilistic programs that operate on a purely syntactic level: reset value\noptimization and register allocation optimization. The presented techniques\nrely on concepts well known from compiler construction such as live range\nanalysis and register allocation through interference graph coloring. Applied\non a redundancy system model for an aircraft velocity control loop modeled in\nSIMULINK, we show effectiveness of our implementation of the reduction methods.\nWe demonstrate that model-size reductions in three orders of magnitude are\npossible and show that we can achieve significant speedups for a reliability\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:27:36 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Dubslaff", "Clemens", ""], ["Morozov", "Andrey", ""], ["Baier", "Christel", ""], ["Janschek", "Klaus", ""]]}, {"id": "2004.06722", "submitter": "Misun Min Dr", "authors": "Paul Fischer, Misun Min, Thilina Rathnayake, Som Dutta, Tzanio Kolev,\n  Veselin Dobrev, Jean-Sylvain Camier, Martin Kronbichler, Tim Warburton, Kasia\n  Swirydowicz, Jed Brown", "title": "Scalability of High-Performance PDE Solvers", "comments": "25 pages, 54 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance tests and analyses are critical to effective HPC software\ndevelopment and are central components in the design and implementation of\ncomputational algorithms for achieving faster simulations on existing and\nfuture computing architectures for large-scale application problems. In this\npaper, we explore performance and space-time trade-offs for important\ncompute-intensive kernels of large-scale numerical solvers for PDEs that govern\na wide range of physical applications. We consider a sequence of PDE- motivated\nbake-off problems designed to establish best practices for efficient high-order\nsimulations across a variety of codes and platforms. We measure peak\nperformance (degrees of freedom per second) on a fixed number of nodes and\nidentify effective code optimization strategies for each architecture. In\naddition to peak performance, we identify the minimum time to solution at 80%\nparallel efficiency. The performance analysis is based on spectral and p-type\nfinite elements but is equally applicable to a broad spectrum of numerical PDE\ndiscretizations, including finite difference, finite volume, and h-type finite\nelements.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:54:48 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Fischer", "Paul", ""], ["Min", "Misun", ""], ["Rathnayake", "Thilina", ""], ["Dutta", "Som", ""], ["Kolev", "Tzanio", ""], ["Dobrev", "Veselin", ""], ["Camier", "Jean-Sylvain", ""], ["Kronbichler", "Martin", ""], ["Warburton", "Tim", ""], ["Swirydowicz", "Kasia", ""], ["Brown", "Jed", ""]]}, {"id": "2004.07060", "submitter": "Etienne Rivi\\`ere", "authors": "Nicolae Berendea, Hugues Mercier, Emanuel Onica, and Etienne Rivi\\`ere", "title": "Fair and Efficient Gossip in Hyperledger Fabric", "comments": "To appear in IEEE ICDCS 2020, copyright is with IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned blockchains are supported by identified but individually\nuntrustworthy nodes, collectively maintaining a replicated ledger whose content\nis trusted. The Hyperledger Fabric permissioned blockchain system targets\nhigh-throughput transaction processing. Fabric uses a set of nodes tasked with\nthe ordering of transactions using consensus. Additional peers endorse and\nvalidate transactions, and maintain a copy of the ledger. The ability to\nquickly disseminate new transaction blocks from ordering nodes to all peers is\ncritical for both performance and consistency. Broadcast is handled by a gossip\nprotocol, using randomized exchanges of blocks between peers. We show that the\ncurrent implementation of gossip in Fabric leads to heavy tail distributions of\nblock propagation latencies, impacting performance, consistency, and fairness.\nWe contribute a novel design for gossip in Fabric that simultaneously optimizes\npropagation time, tail latency and bandwidth consumption. Using a 100-node\ncluster, we show that our enhanced gossip allows the dissemination of blocks to\nall peers more than 10 times faster than with the original implementation,\nwhile decreasing the overall network bandwidth consumption by more than 40%.\nWith a high throughput and concurrent application, this results in 17% to 36%\nfewer invalidated transactions for different block sizes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:45:48 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Berendea", "Nicolae", ""], ["Mercier", "Hugues", ""], ["Onica", "Emanuel", ""], ["Rivi\u00e8re", "Etienne", ""]]}, {"id": "2004.07519", "submitter": "Mieke Massink", "authors": "Nicolas Gast (1), Diego Latella (2), Mieke Massink (2) ((1) INRIA,\n  France (2) CNR-ISTI, Italy)", "title": "Refined Mean Field Analysis of the Gossip Shuffle Protocol -- extended\n  version --", "comments": "This paper is an extended version of a short paper accepted for the\n  LNCS proceedings of COORDINATION 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gossip protocols form the basis of many smart collective adaptive systems.\nThey are a class of fully decentralised, simple but robust protocols for the\ndistribution of information throughout large scale networks with hundreds or\nthousands of nodes. Mean field analysis methods have made it possible to\napproximate and analyse performance aspects of such large scale protocols in an\nefficient way. Taking the gossip shuffle protocol as a benchmark, we evaluate a\nrecently developed refined mean field approach. We illustrate the gain in\naccuracy this can provide for the analysis of medium size models analysing two\nkey performance measures. We also show that refined mean field analysis\nrequires special attention to correctly capture the coordination aspects of the\ngossip shuffle protocol.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 08:23:45 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Gast", "Nicolas", ""], ["Latella", "Diego", ""], ["Massink", "Mieke", ""]]}, {"id": "2004.08238", "submitter": "Wei-Chang Yeh", "authors": "Wei-Chang Yeh", "title": "Novel Binary-Addition Tree Algorithm (BAT) for Binary-State Network\n  Reliability Problem", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF eess.SP math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network structures and models have been widely adopted, e.g., for Internet of\nThings, wireless sensor networks, smart grids, transportation networks,\ncommunication networks, social networks, and computer grid systems. Network\nreliability is an effective and popular technique to estimate the probability\nthat the network is still functioning. Networks composed of binary-state (e.g.,\nworking or failed) components (arcs and/or nodes) are called binary-state\nnetworks. The binary-state network is the fundamental type of network; thus,\nthere is always a need for a more efficient algorithm to calculate the network\nreliability. Thus, a novel binary-addition tree (BAT) algorithm that employs\nbinary addition for finding all the possible state vectors and the path-based\nlayered-search algorithm for filtering out all the connected vectors is\nproposed for calculating the binary-state network reliability. According to the\ntime complexity and numerical examples, the efficiency of the proposed BAT is\nhigher than those of traditional algorithms for solving the binary-state\nnetwork reliability problem.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:51:48 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yeh", "Wei-Chang", ""]]}, {"id": "2004.08285", "submitter": "Shaocheng Huang", "authors": "Shaocheng Huang, Yu Ye, Ming Xiao", "title": "Learning Based Hybrid Beamforming Design for Full-Duplex Millimeter Wave\n  Systems", "comments": "13 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millimeter Wave (mmWave) communications with full-duplex (FD) have the\npotential of increasing the spectral efficiency, relative to those with\nhalf-duplex. However, the residual self-interference (SI) from FD and high\npathloss inherent to mmWave signals may degrade the system performance.\nMeanwhile, hybrid beamforming (HBF) is an efficient technology to enhance the\nchannel gain and mitigate interference with reasonable complexity. However,\nconventional HBF approaches for FD mmWave systems are based on optimization\nprocesses, which are either too complex or strongly rely on the quality of\nchannel state information (CSI). We propose two learning schemes to design HBF\nfor FD mmWave systems, i.e., extreme learning machine based HBF (ELM-HBF) and\nconvolutional neural networks based HBF (CNN-HBF). Specifically, we first\npropose an alternating direction method of multipliers (ADMM) based algorithm\nto achieve SI cancellation beamforming, and then use a\nmajorization-minimization (MM) based algorithm for joint transmitting and\nreceiving HBF optimization. To train the learning networks, we simulate noisy\nchannels as input, and select the hybrid beamformers calculated by proposed\nalgorithms as targets. Results show that both learning based schemes can\nprovide more robust HBF performance and achieve at least 22.1% higher spectral\nefficiency compared to orthogonal matching pursuit (OMP) algorithms. Besides,\nthe online prediction time of proposed learning based schemes is almost 20\ntimes faster than the OMP scheme. Furthermore, the training time of ELM-HBF is\nabout 600 times faster than that of CNN-HBF with 64 transmitting and receiving\nantennas.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 15:48:57 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Huang", "Shaocheng", ""], ["Ye", "Yu", ""], ["Xiao", "Ming", ""]]}, {"id": "2004.08425", "submitter": "David Daly", "authors": "Henrik Ingo and David Daly", "title": "Automated System Performance Testing at MongoDB", "comments": "Author Preprint. Appearing in DBTest.io 2020", "journal-ref": null, "doi": "10.1145/3395032.3395323", "report-no": null, "categories": "cs.DB cs.PF cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed Systems Infrastructure (DSI) is MongoDB's framework for running\nfully automated system performance tests in our Continuous Integration (CI)\nenvironment. To run in CI it needs to automate everything end-to-end:\nprovisioning and deploying multi-node clusters, executing tests, tuning the\nsystem for repeatable results, and collecting and analyzing the results. Today\nDSI is MongoDB's most used and most useful performance testing tool. It runs\nalmost 200 different benchmarks in daily CI, and we also use it for manual\nperformance investigations. As we can alert the responsible engineer in a\ntimely fashion, all but one of the major regressions were fixed before the\n4.2.0 release. We are also able to catch net new improvements, of which DSI\ncaught 17. We open sourced DSI in March 2020.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 19:14:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ingo", "Henrik", ""], ["Daly", "David", ""]]}, {"id": "2004.08838", "submitter": "Vijayshree Vijayshree", "authors": "Vijayshree Vijayshree, Markus Frank, Steffen Becker", "title": "Extended Abstract of Performance Analysis and Prediction of Model\n  Transformation", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": "10.1145/3358960.3383769", "report-no": null, "categories": "cs.SE cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the software development process, model transformation is increasingly\nassimilated. However, systems being developed with model transformation\nsometimes grow in size and become complex. Meanwhile, the performance of model\ntransformation tends to decrease. Hence, performance is an important quality of\nmodel transformation. According to current research model transformation\nperformance focuses on optimising the engines internally. However, there exists\nno research activities to support transformation engineer to identify\nperformance bottleneck in the transformation rules and hence, to predict the\noverall performance. In this paper we vision our aim at providing an approach\nof monitoring and profiling to identify the root cause of performance issues in\nthe transformation rules and to predict the performance of model\ntransformation. This will enable software engineers to systematically identify\nperformance issues as well as predict the performance of model transformation.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 12:36:10 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Vijayshree", "Vijayshree", ""], ["Frank", "Markus", ""], ["Becker", "Steffen", ""]]}, {"id": "2004.08991", "submitter": "James Flamino", "authors": "James Flamino, Christopher Abriola, Ben Zimmerman, Zhongheng Li, Joel\n  Douglas", "title": "Robust and Scalable Entity Alignment in Big Data", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Entity alignment has always had significant uses within a multitude of\ndiverse scientific fields. In particular, the concept of matching entities\nacross networks has grown in significance in the world of social science as\ncommunicative networks such as social media have expanded in scale and\npopularity. With the advent of big data, there is a growing need to provide\nanalysis on graphs of massive scale. However, with millions of nodes and\nbillions of edges, the idea of alignment between a myriad of graphs of similar\nscale using features extracted from potentially sparse or incomplete datasets\nbecomes daunting. In this paper we will propose a solution to the issue of\nlarge-scale alignments in the form of a multi-step pipeline. Within this\npipeline we introduce scalable feature extraction for robust temporal\nattributes, accompanied by novel and efficient clustering algorithms in order\nto find groupings of similar nodes across graphs. The features and their\nclusters are fed into a versatile alignment stage that accurately identifies\npartner nodes among millions of possible matches. Our results show that the\npipeline can process large data sets, achieving efficient runtimes within the\nmemory constraints.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 23:41:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Flamino", "James", ""], ["Abriola", "Christopher", ""], ["Zimmerman", "Ben", ""], ["Li", "Zhongheng", ""], ["Douglas", "Joel", ""]]}, {"id": "2004.09492", "submitter": "Igor Sfiligoi", "authors": "I. Sfiligoi, D. Schultz, B. Riedel, F. Wuerthwein, S. Barnet and V.\n  Brik", "title": "Demonstrating a Pre-Exascale, Cost-Effective Multi-Cloud Environment for\n  Scientific Computing", "comments": "5 pages, 7 figures, to be published in proceedings of PEARC'20. arXiv\n  admin note: text overlap with arXiv:2002.06667", "journal-ref": null, "doi": "10.1145/3311790.3396625", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scientific computing needs are growing dramatically with time and are\nexpanding in science domains that were previously not compute intensive. When\ncompute workflows spike well in excess of the capacity of their local compute\nresource, capacity should be temporarily provisioned from somewhere else to\nboth meet deadlines and to increase scientific output. Public Clouds have\nbecome an attractive option due to their ability to be provisioned with minimal\nadvance notice. The available capacity of cost-effective instances is not well\nunderstood. This paper presents expanding the IceCube's production HTCondor\npool using cost-effective GPU instances in preemptible mode gathered from the\nthree major Cloud providers, namely Amazon Web Services, Microsoft Azure and\nthe Google Cloud Platform. Using this setup, we sustained for a whole workday\nabout 15k GPUs, corresponding to around 170 PFLOP32s, integrating over one\nEFLOP32 hour worth of science output for a price tag of about $60k. In this\npaper, we provide the reasoning behind Cloud instance selection, a description\nof the setup and an analysis of the provisioned resources, as well as a short\ndescription of the actual science output of the exercise.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 05:06:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Sfiligoi", "I.", ""], ["Schultz", "D.", ""], ["Riedel", "B.", ""], ["Wuerthwein", "F.", ""], ["Barnet", "S.", ""], ["Brik", "V.", ""]]}, {"id": "2004.09645", "submitter": "Sergio Palomo", "authors": "Sergio Palomo and Jamol Pender and William Massey and Robert C.\n  Hampshire", "title": "Flattening the Curve: Insights From Queueing Theory", "comments": "27 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The worldwide outbreak of the coronavirus was first identified in 2019 in\nWuhan, China. Since then, the disease has spread worldwide. As it currently\nspreading in the United States, policy makers, public health officials and\ncitizens are racing to understand the impact of this virus on the United States\nhealthcare system. They fear that the rapid influx of patients will overwhelm\nthe healthcare system leading to unnecessary fatalities. Most countries and\nstates in America have introduced mitigation strategies, such as social\ndistancing, to decrease the rate of newly infected people, i.e. flattening the\ncurve.In this paper, we analyze the time evolution of the number of people\nhospitalized due to the coronavirus using the methods of queueing theory. Given\nthat the rate of new infections varies over time as the pandemic evolves, we\nmodel the number of coronavirus patients as a dynamical system based on the\ntheory of infinite server queues with non-stationary Poisson arrival rates.\nWith this model we are able to quantify how flattening the curve affects the\npeak demand for hospital resources. This allows us to characterize how\naggressively society must flatten the curve in order to avoid overwhelming the\ncapacity of healthcare system. We also demonstrate how flattening the curve\nimpacts the elapsed time between the peak rate of hospitalizations and the time\nof the peak demand for the hospital resources. Finally, we present empirical\nevidence from China, South Korea, Italy and the United States that supports the\ninsights from the model.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 21:28:30 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Palomo", "Sergio", ""], ["Pender", "Jamol", ""], ["Massey", "William", ""], ["Hampshire", "Robert C.", ""]]}, {"id": "2004.10044", "submitter": "Fabian Ritter", "authors": "Fabian Ritter and Sebastian Hack", "title": "PMEvo: Portable Inference of Port Mappings for Out-of-Order Processors\n  by Evolutionary Optimization", "comments": "15 pages, accepted at PLDI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving peak performance in a computer system requires optimizations in\nevery layer of the system, be it hardware or software. A detailed understanding\nof the underlying hardware, and especially the processor, is crucial to\noptimize software. One key criterion for the performance of a processor is its\nability to exploit instruction-level parallelism. This ability is determined by\nthe port mapping of the processor, which describes the execution units of the\nprocessor for each instruction.\n  Processor manufacturers usually do not share the port mappings of their\nmicroarchitectures. While approaches to automatically infer port mappings from\nexperiments exist, they are based on processor-specific hardware performance\ncounters that are not available on every platform.\n  We present PMEvo, a framework to automatically infer port mappings solely\nbased on the measurement of the execution time of short instruction sequences.\nPMEvo uses an evolutionary algorithm that evaluates the fitness of candidate\nmappings with an analytical throughput model formulated as a linear program.\nOur prototype implementation infers a port mapping for Intel's Skylake\narchitecture that predicts measured instruction throughput with an accuracy\nthat is competitive to existing work. Furthermore, it finds port mappings for\nAMD's Zen+ architecture and the ARM Cortex-A72 architecture, which are out of\nscope of existing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:34:09 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Ritter", "Fabian", ""], ["Hack", "Sebastian", ""]]}, {"id": "2004.10499", "submitter": "Galymzhan Nauryzbayev", "authors": "Sultangali Arzykulov, Galymzhan Nauryzbayev, Abdulkadir Celik, and\n  Ahmed M. Eltawil", "title": "Hardware and Interference Limited Cooperative CR-NOMA Networks under\n  Imperfect SIC and CSI", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conflation of cognitive radio (CR) and nonorthogonal multiple access\n(NOMA) concepts is a promising approach to fulfil the massive connectivity\ngoals of future networks given the spectrum scarcity. Accordingly, this letter\ninvestigates the outage performance of imperfect cooperative CR-NOMA networks\nunder hardware impairments and interference. Our analysis is involved with the\nderivation of the end-to-end outage probability (OP) for secondary NOMA users\nby accounting for imperfect channel state information (CSI), as well as the\nresidual interference caused by successive interference cancellation (SIC)\nerrors and coexisting primary/secondary users. The numerical results validated\nby Monte Carlo simulations show that CR-NOMA network provides a superior outage\nperformance over orthogonal multiple access. As imperfections become more\nsignificant, CR-NOMA is observed to deliver relatively poor outage performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 11:26:00 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Arzykulov", "Sultangali", ""], ["Nauryzbayev", "Galymzhan", ""], ["Celik", "Abdulkadir", ""], ["Eltawil", "Ahmed M.", ""]]}, {"id": "2004.10506", "submitter": "Galymzhan Nauryzbayev", "authors": "Leila Tlebaldiyeva, Galymzhan Nauryzbayev, Sultangali Arzykulov,\n  Yerassyl Akhmetkaziyev, Mohammad S. Hashmi, and Ahmed M. Eltawil", "title": "A Non-Ideal NOMA-based mmWave D2D Networks with Hardware and CSI\n  Imperfections", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter investigates a non-orthogonal multiple access (NOMA) assisted\nmillimeter-wave device-to-device (D2D) network practically limited by multiple\ninterference noises, transceiver hardware impairments, imperfect successive\ninterference cancellation, and channel state information mismatch. Generalized\noutage probability expressions for NOMA-D2D users are deduced and achieved\nresults, validated by Monte Carlo simulations, are compared with the orthogonal\nmultiple access to show the superior performance of the proposed network model\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 11:41:33 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Tlebaldiyeva", "Leila", ""], ["Nauryzbayev", "Galymzhan", ""], ["Arzykulov", "Sultangali", ""], ["Akhmetkaziyev", "Yerassyl", ""], ["Hashmi", "Mohammad S.", ""], ["Eltawil", "Ahmed M.", ""]]}, {"id": "2004.10519", "submitter": "Mathias Louboutin", "authors": "Mathias Louboutin, Fabio Luporini, Philipp Witte, Rhodri Nelson,\n  George Bisbas, Jan Thorbecke, Felix J. Herrmann, and Gerard Gorman", "title": "Scaling through abstractions -- high-performance vectorial wave\n  simulations for seismic inversion with Devito", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CL cs.PF physics.ao-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  [Devito] is an open-source Python project based on domain-specific language\nand compiler technology. Driven by the requirements of rapid HPC applications\ndevelopment in exploration seismology, the language and compiler have evolved\nsignificantly since inception. Sophisticated boundary conditions, tensor\ncontractions, sparse operations and features such as staggered grids and\nsub-domains are all supported; operators of essentially arbitrary complexity\ncan be generated. To accommodate this flexibility whilst ensuring performance,\ndata dependency analysis is utilized to schedule loops and detect\ncomputational-properties such as parallelism. In this article, the generation\nand simulation of MPI-parallel propagators (along with their adjoints) for the\npseudo-acoustic wave-equation in tilted transverse isotropic media and the\nelastic wave-equation are presented. Simulations are carried out on industry\nscale synthetic models in a HPC Cloud system and reach a performance of\n28TFLOP/s, hence demonstrating Devito's suitability for production-grade\nseismic inversion problems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 12:20:07 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Louboutin", "Mathias", ""], ["Luporini", "Fabio", ""], ["Witte", "Philipp", ""], ["Nelson", "Rhodri", ""], ["Bisbas", "George", ""], ["Thorbecke", "Jan", ""], ["Herrmann", "Felix J.", ""], ["Gorman", "Gerard", ""]]}, {"id": "2004.10926", "submitter": "Rujia Wang", "authors": "Zhou Ni, Rujia Wang", "title": "Performance Evaluation of Secure Multi-party Computation on\n  Heterogeneous Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure multi-party computation (MPC) is a broad cryptographic concept that\ncan be adopted for privacy-preserving computation. With MPC, a number of\nparties can collaboratively compute a function, without revealing the actual\ninput or output of the plaintext to others. The applications of MPC range from\nprivacy-preserving voting, arithmetic calculation, and large-scale data\nanalysis. From the system perspective, each party in MPC can run on one compute\nnode. The compute nodes of multiple parties could be either homogeneous or\nheterogeneous; however, the distributed workloads from the MPC protocols tend\nto be always homogeneous (symmetric). In this work, we study a representative\nMPC framework and a set of MPC applications from the system performance\nperspective. We show the detailed online computation workflow of a\nstate-of-the-art MPC protocol and analyze the root cause of its stall time and\nperformance bottleneck on homogeneous and heterogeneous compute nodes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 01:42:47 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ni", "Zhou", ""], ["Wang", "Rujia", ""]]}, {"id": "2004.11143", "submitter": "Galymzhan Nauryzbayev", "authors": "Galymzhan Nauryzbayev, Mohamed Abdallah, and Naofal Al-Dhahir", "title": "Outage Analysis of Cognitive Electric Vehicular Networks over Mixed\n  RF/VLC Channels", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern transportation infrastructures are considered as one of the main\nsources of the greenhouse gases emitted into the atmosphere. This situation\nrequires the decision-making players to enact the mass use of electric vehicles\n(EVs) which, in turn, highly demand novel secure communication technologies\nrobust to various cyber-attacks. Therefore, in this paper, we propose a novel\njamming-robust communication technique for different outdoor cognitive\nEV-enabled network cases over mixed radio-frequency (RF)/visible light\ncommunication (VLC) channels. One EV acts as a relaying node to allow an\naggregator to reach the jammed EV and, at the same time, operates in both RF\nand VLC spectrum bands while satisfying interference constraints imposed by the\nprimary network entities. We derive exact closed-form analytical expressions\nfor the outage probability and also provide their asymptotic analysis while\nconsidering various channel state information quality scenarios. Moreover, we\nquantify the outage reduction achievable by deploying such mixed VLC/RF\nchannels. Finally, analytical and simulation results validate the accuracy of\nour analysis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:38:48 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Nauryzbayev", "Galymzhan", ""], ["Abdallah", "Mohamed", ""], ["Al-Dhahir", "Naofal", ""]]}, {"id": "2004.11847", "submitter": "Jin Xu", "authors": "Jin Xu and I-Hong Hou and Natarajan Gautam", "title": "Age of Information for Single Buffer Systems with Vacation Server", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.IT math.IT math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we consider age-related metrics for queueing systems with\nvacation server. Assuming that there is a single buffer at the queue to receive\npackets, we consider three variations of this single buffer system, namely\nConventional Buffer System (CBS), Buffer Relaxation System (BRS), and\nConventional Buffer System with Preemption in Service (CBS-P). We introduce a\ndecomposition approach to derive the closed-form expressions for expected Age\nof Information (AoI), expected Peak Age of Information (PAoI) as well as the\nvariance of peak age for these systems. We then consider these three systems\nwith non-independent vacations, and use polling system as an example to show\nthat the decomposition approach can be applied to derive closed-form\nexpressions of PAoI for general situation. We explore the conditions under\nwhich one of these systems has advantage over the others, and we further\nperform numerical studies to validate our results and develop insights.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 16:42:42 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Xu", "Jin", ""], ["Hou", "I-Hong", ""], ["Gautam", "Natarajan", ""]]}, {"id": "2004.12768", "submitter": "Maher Alharby", "authors": "Maher Alharby, Roben Castagna Lunardi, Amjad Aldweesh and Aad van\n  Moorsel", "title": "Data-Driven Model-Based Analysis of the Ethereum Verifier's Dilemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In proof-of-work based blockchains such as Ethereum, verification of blocks\nis an integral part of establishing consensus across nodes. However, in\nEthereum, miners do not receive a reward for verifying. This implies that\nminers face the Verifier's Dilemma: use resources for verification, or use them\nfor the more lucrative mining of new blocks? We provide an extensive analysis\nof the Verifier's Dilemma, using a data-driven model-based approach that\ncombines closed-form expressions, machine learning techniques and\ndiscrete-event simulation. We collect data from over 300,000 smart contracts\nand experimentally obtain their CPU execution times. Gaussian Mixture Models\nand Random Forest Regression transform the data into distributions and inputs\nsuitable for the simulator. We show that, indeed, it is often economically\nrational not to verify. We consider two approaches to mitigate the implications\nof the Verifier's Dilemma, namely parallelization and active insertion of\ninvalid blocks, both will be shown to be effective.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:17:12 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Alharby", "Maher", ""], ["Lunardi", "Roben Castagna", ""], ["Aldweesh", "Amjad", ""], ["van Moorsel", "Aad", ""]]}, {"id": "2004.13287", "submitter": "EPTCS", "authors": "Clemens Dubslaff, Andrey Morozov, Christel Baier, Klaus Janschek", "title": "Iterative Variable Reordering: Taming Huge System Families", "comments": "In Proceedings MARS 2020, arXiv:2004.12403", "journal-ref": "EPTCS 316, 2020, pp. 121-133", "doi": "10.4204/EPTCS.316.5", "report-no": null, "categories": "cs.LO cs.PF cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the verification of systems using model-checking techniques, symbolic\nrepresentations based on binary decision diagrams (BDDs) often help to tackle\nthe well-known state-space explosion problem. Symbolic BDD-based\nrepresentations have been also shown to be successful for the analysis of\nfamilies of systems that arise, e.g., through configurable parameters or\nfollowing the feature-oriented modeling approach. The state space of such\nsystem families face an additional exponential blowup in the number of\nparameters or features. It is well known that the order of variables in ordered\nBDDs is crucial for the size of the model representation. Especially for\nautomatically generated models from real-world systems, family models might\neven be not constructible due to bad variable orders. In this paper we describe\na technique, called iterative variable reordering, that can enable the\nconstruction of large-scale family models. We exemplify feasibility of our\napproach by means of an aircraft velocity control system with redundancy\nmechanisms modeled in the input language of the probabilistic model checker\nPRISM. We show that standard reordering and dynamic reordering techniques fail\nto construct the family model due to memory and time constraints, respectively,\nwhile the new iterative approach succeeds to generate a symbolic family model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:23:14 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Dubslaff", "Clemens", ""], ["Morozov", "Andrey", ""], ["Baier", "Christel", ""], ["Janschek", "Klaus", ""]]}, {"id": "2004.13373", "submitter": "Maximilian H\\\"ob", "authors": "Maximilian H\\\"ob and Dieter Kranzlm\\\"uller", "title": "Enabling EASEY deployment of containerized applications for future HPC\n  systems", "comments": "International Conference on Computational Science ICCS2020, 13 pages", "journal-ref": "ICCS 2020: Computational Science 206-219", "doi": "10.1007/978-3-030-50371-0_15", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The upcoming exascale era will push the changes in computing architecture\nfrom classical CPU-based systems in hybrid GPU-heavy systems with much higher\nlevels of complexity. While such clusters are expected to improve the\nperformance of certain optimized HPC applications, it will also increase the\ndifficulties for those users who have yet to adapt their codes or are starting\nfrom scratch with new programming paradigms. Since there are still no\ncomprehensive automatic assistance mechanisms to enhance application\nperformance on such systems, we are proposing a support framework for future\nHPC architectures, called EASEY (Enable exASclae for EverYone). The solution\nbuilds on a layered software architecture, which offers different mechanisms on\neach layer for different tasks of tuning. This enables users to adjust the\nparameters on each of the layers, thereby enhancing specific characteristics of\ntheir codes. We introduce the framework with a Charliecloud-based solution,\nshowcasing the LULESH benchmark on the upper layers of our framework. Our\napproach can automatically deploy optimized container computations with\nnegligible overhead and at the same time reduce the time a scientist needs to\nspent on manual job submission configurations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 09:05:47 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 08:01:21 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["H\u00f6b", "Maximilian", ""], ["Kranzlm\u00fcller", "Dieter", ""]]}, {"id": "2004.13438", "submitter": "Maher Alharby", "authors": "Maher Alharby and Aad van Moorsel", "title": "BlockSim: An Extensible Simulation Tool for Blockchain Systems", "comments": "Frontiers in Blockchain:\n  https://www.frontiersin.org/article/10.3389/fbloc.2020.00028", "journal-ref": "Frontiers in Blockchain 3(2020)", "doi": "10.3389/fbloc.2020.00028", "report-no": null, "categories": "cs.CR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Both in the design and deployment of blockchain solutions many\nperformance-impacting configuration choices need to be made. We introduce\nBlockSim, a framework and software tool to build and simulate discrete-event\ndynamic systems models for blockchain systems. BlockSim is designed to support\nthe analysis of a large variety of blockchains and blockchain deployments as\nwell as a wide set of analysis questions. At the core of BlockSim is a Base\nModel, which contains the main model constructs common across various\nblockchain systems organized in three abstraction layers (network, consensus\nand incentives layer). The Base Model is usable for a wide variety of\nblockchain systems and can be extended easily to include system or deployment\nparticulars. The BlockSim software tool provides a simulator that implements\nthe Base Model in Python. This paper describes the Base Model, the simulator\nimplementation, and the application of BlockSim to Bitcoin, Ethereum and other\nconsensus algorithms. We validate BlockSim simulation results by comparison\nwith performance results from actual systems and from other studies in the\nliterature. We close the paper by a BlockSim simulation study of the impact of\nuncle blocks rewards on mining decentralization, for a variety of blockchain\nconfigurations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 11:44:07 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 21:48:10 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Alharby", "Maher", ""], ["van Moorsel", "Aad", ""]]}, {"id": "2004.14378", "submitter": "Johannes Klaus Fichte", "authors": "Johannes K. Fichte, Norbert Manthey, Julian Stecklina, Andr\\'e\n  Schidler", "title": "Towards Faster Reasoners By Using Transparent Huge Pages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various state-of-the-art automated reasoning (AR) tools are widely used as\nbackend tools in research of knowledge representation and reasoning as well as\nin industrial applications. In testing and verification, those tools often run\ncontinuously or nightly. In this work, we present an approach to reduce the\nruntime of AR tools by 10% on average and up to 20% for long running tasks. Our\nimprovement addresses the high memory usage that comes with the data structures\nused in AR tools, which are based on conflict driven no-good learning. We\nestablish a general way to enable faster memory access by using the memory\ncache line of modern hardware more effectively. Therefore, we extend the\nstandard C library (glibc) by dynamically allowing to use a memory management\nfeature called huge pages. Huge pages allow to reduce the overhead that is\nrequired to translate memory addresses between the virtual memory of the\noperating system and the physical memory of the hardware. In that way, we can\nreduce runtime, costs, and energy consumption of AR tools and applications with\nsimilar memory access patterns simply by linking the tool against this new\nglibc library when compiling it. In every day industrial applications this\neasily allows to be more eco-friendly in computation. To back up the claimed\nspeed-up, we present experimental results for tools that are commonly used in\nthe AR community, including the domains ASP, BMC, MaxSAT, SAT, and SMT.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:57:19 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Manthey", "Norbert", ""], ["Stecklina", "Julian", ""], ["Schidler", "Andr\u00e9", ""]]}, {"id": "2004.14639", "submitter": "Yu Su", "authors": "Yu Su, Xiaoqi Ren, Shai Vardi, Adam Wierman", "title": "Communication-Aware Scheduling of Precedence-Constrained Tasks on\n  Related Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling precedence-constrained tasks is a classical problem that has been\nstudied for more than fifty years. However, little progress has been made in\nthe setting where there are communication delays between tasks. Results for the\ncase of identical machines were derived nearly thirty years ago, and yet no\nresults for related machines have followed. In this work, we propose a new\nscheduler, Generalized Earliest Time First (GETF), and provide the first\nprovable, worst-case approximation guarantees for the goals of minimizing both\nthe makespan and total weighted completion time of tasks with precedence\nconstraints on related machines with machine-dependent communication times.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 08:56:48 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Su", "Yu", ""], ["Ren", "Xiaoqi", ""], ["Vardi", "Shai", ""], ["Wierman", "Adam", ""]]}, {"id": "2004.14793", "submitter": "Gal Mendelson", "authors": "Gal Mendelson", "title": "A Lower Bound on the stability region of Redundancy-d with FIFO service\n  discipline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy-d (R(d)) is a load balancing method used to route incoming jobs to\nK servers, each with its own queue. Every arriving job is replicated into\n2<=d<=K tasks, which are then routed to d servers chosen uniformly at random.\nWhen the first task finishes service, the remaining d-1 tasks are cancelled and\nthe job departs the system.\n  Despite the fact that R(d) is known, under certain conditions, to\nsubstantially improve job completion times compared to not using redundancy at\nall, little is known on a more fundamental performance criterion: what is the\nset of arrival rates under which the R(d) queueing system with FIFO service\ndiscipline is stable? In this context, due to the complex dynamics of systems\nwith redundancy and cancellations, existing results are scarce and are limited\nto very special cases with respect to the joint service time distribution of\ntasks.\n  In this paper we provide a non-trivial, closed form lower bound on the\nstability region of R(d) for a general joint service time distribution of tasks\nwith finite first and second moments. We consider a discrete time system with\nBernoulli arrivals and assume that jobs are processed by their order of\narrival. We use the workload processes and a quadratic Lyapunov function to\ncharacterize the set of arrival rates for which the system is stable. While\nsimulation results indicate our bound is not tight, it provides an\neasy-to-check performance guarantee.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:07:25 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 18:15:06 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Mendelson", "Gal", ""]]}]