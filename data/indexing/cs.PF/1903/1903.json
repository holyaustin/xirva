[{"id": "1903.00045", "submitter": "Tian Guo", "authors": "Shijian Li and Robert J. Walls and Lijie Xu and Tian Guo", "title": "Speeding up Deep Learning with Transient Servers", "comments": "Accepted to ICAC'19. 11 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training frameworks, like TensorFlow, have been proposed as a\nmeans to reduce the training time of deep learning models by using a cluster of\nGPU servers. While such speedups are often desirable---e.g., for rapidly\nevaluating new model designs---they often come with significantly higher\nmonetary costs due to sublinear scalability. In this paper, we investigate the\nfeasibility of using training clusters composed of cheaper transient GPU\nservers to get the benefits of distributed training without the high costs.\n  We conduct the first large-scale empirical analysis, launching more than a\nthousand GPU servers of various capacities, aimed at understanding the\ncharacteristics of transient GPU servers and their impact on distributed\ntraining performance. Our study demonstrates the potential of transient servers\nwith a speedup of 7.7X with more than 62.9% monetary savings for some cluster\nconfigurations. We also identify a number of important challenges and\nopportunities for redesigning distributed training frameworks to be\ntransient-aware. For example, the dynamic cost and availability characteristics\nof transient servers suggest the need for frameworks to dynamically change\ncluster configurations to best take advantage of current conditions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 19:47:59 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 07:20:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Li", "Shijian", ""], ["Walls", "Robert J.", ""], ["Xu", "Lijie", ""], ["Guo", "Tian", ""]]}, {"id": "1903.00771", "submitter": "Zong Peng", "authors": "Zong Peng and Beth Plale", "title": "Reliable Access to Massive Restricted Texts: Experience-based Evaluation", "comments": "a preprint version of submission to CCPE special issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Libraries are seeing growing numbers of digitized textual corpora that\nfrequently come with restrictions on their content. Computational analysis\ncorpora that are large, while of interest to scholars, can be cumbersome\nbecause of the combination of size, granularity of access, and access\nrestrictions. Efficient management of such a collection for general access\nespecially under failures depends on the primary storage system. In this paper,\nwe identify the requirements of managing for computational analysis a massive\ntext corpus and use it as basis to evaluate candidate storage solutions. The\nstudy based on the 5.9 billion page collection of the HathiTrust digital\nlibrary. Our findings led to the choice of Cassandra 3.x for the primary back\nend store, which is currently in deployment in the HathiTrust Research Center.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 21:36:35 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Peng", "Zong", ""], ["Plale", "Beth", ""]]}, {"id": "1903.01042", "submitter": "Sanghamitra Dutta", "authors": "Sanghamitra Dutta, Ziqian Bai, Tze Meng Low, Pulkit Grover", "title": "CodeNet: Training Large Scale Neural Networks in Presence of Soft-Errors", "comments": "Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes the first strategy to make distributed training of neural\nnetworks resilient to computing errors, a problem that has remained unsolved\ndespite being first posed in 1956 by von Neumann. He also speculated that the\nefficiency and reliability of the human brain is obtained by allowing for low\npower but error-prone components with redundancy for error-resilience. It is\nsurprising that this problem remains open, even as massive artificial neural\nnetworks are being trained on increasingly low-cost and unreliable processing\nunits. Our coding-theory-inspired strategy, \"CodeNet,\" solves this problem by\naddressing three challenges in the science of reliable computing: (i) Providing\nthe first strategy for error-resilient neural network training by encoding each\nlayer separately; (ii) Keeping the overheads of coding\n(encoding/error-detection/decoding) low by obviating the need to re-encode the\nupdated parameter matrices after each iteration from scratch. (iii) Providing a\ncompletely decentralized implementation with no central node (which is a single\npoint of failure), allowing all primary computational steps to be error-prone.\nWe theoretically demonstrate that CodeNet has higher error tolerance than\nreplication, which we leverage to speed up computation time. Simultaneously,\nCodeNet requires lower redundancy than replication, and equal computational and\ncommunication costs in scaling sense. We first demonstrate the benefits of\nCodeNet in reducing expected computation time over replication when accounting\nfor checkpointing. Our experiments show that CodeNet achieves the best\naccuracy-runtime tradeoff compared to both replication and uncoded strategies.\nCodeNet is a significant step towards biologically plausible neural network\ntraining, that could hold the key to orders of magnitude efficiency\nimprovements.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 01:45:14 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Dutta", "Sanghamitra", ""], ["Bai", "Ziqian", ""], ["Low", "Tze Meng", ""], ["Grover", "Pulkit", ""]]}, {"id": "1903.03266", "submitter": "Huang Yanpei", "authors": "Yanpei Huang, Etienne Burdet, Lin Cao, Phuoc Thien Phan, Anthony Meng\n  Huat Tiong, Pai Zheng and Soo Jay Phee", "title": "Performance evaluation of a foot-controlled human-robot interface", "comments": "7 pages, submit to 2019 IROS RA-Letter", "journal-ref": "IEEE Robotics and Automation Letters, 2019", "doi": "10.1109/LRA.2019.2926215", "report-no": null, "categories": "cs.RO cs.HC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic minimally invasive interventions typically require using more than\ntwo instruments. We thus developed a foot pedal interface which allows the user\nto control a robotic arm (simultaneously to working with the hands) with four\ndegrees of freedom in continuous directions and speeds. This paper evaluates\nand compares the performances of ten naive operators in using this new pedal\ninterface and a traditional button interface in completing tasks. These tasks\nare geometrically complex path-following tasks similar to those in laparoscopic\ntraining, and the traditional button interface allows axis-by-axis control with\nconstant speeds. Precision, time, and smoothness of the subjects' control\nmovements for these tasks are analysed. The results demonstrate that the pedal\ninterface can be used to control a robot for complex motion tasks. The subjects\nkept the average error rate at a low level of around 2.6% with both interfaces,\nbut the pedal interface resulted in about 30% faster operation speed and 60%\nsmoother movement, which indicates improved efficiency and user experience as\ncompared with the button interface. The results of a questionnaire show that\nthe operators found that controlling the robot with the pedal interface was\nmore intuitive, comfortable, and less tiring than using the button interface.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 03:45:45 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Huang", "Yanpei", ""], ["Burdet", "Etienne", ""], ["Cao", "Lin", ""], ["Phan", "Phuoc Thien", ""], ["Tiong", "Anthony Meng Huat", ""], ["Zheng", "Pai", ""], ["Phee", "Soo Jay", ""]]}, {"id": "1903.03684", "submitter": "Zakaria El Mrabet", "authors": "Zakaria El Mrabet, Youness Arjoune, Hassan El Ghazi, Badr Abou Al\n  Majd, Naima Kaabouch", "title": "Primary User Emulation Attacks: A Detection Technique Based on Kalman\n  Filter", "comments": "14 pages, 9 figures", "journal-ref": "J. Sens. Actuator Netw. 2018, 7(3), 26", "doi": "10.3390/jsan7030026", "report-no": null, "categories": "cs.CR cs.LG cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive radio technology addresses the problem of spectrum scarcity by\nallowing secondary users to use the vacant spectrum bands without causing\ninterference to the primary users. However, several attacks could disturb the\nnormal functioning of the cognitive radio network. Primary user emulation\nattacks are one of the most severe attacks in which a malicious user emulates\nthe primary user signal characteristics to either prevent other legitimate\nsecondary users from accessing the idle channels or causing harmful\ninterference to the primary users. There are several proposed approaches to\ndetect the primary user emulation attackers. However, most of these techniques\nassume that the primary user location is fixed, which does not make them valid\nwhen the primary user is mobile. In this paper, we propose a new approach based\non the Kalman filter framework for detecting the primary user emulation attacks\nwith a non-stationary primary user. Several experiments have been conducted and\nthe advantages of the proposed approach are demonstrated through the simulation\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 22:13:06 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Mrabet", "Zakaria El", ""], ["Arjoune", "Youness", ""], ["Ghazi", "Hassan El", ""], ["Majd", "Badr Abou Al", ""], ["Kaabouch", "Naima", ""]]}, {"id": "1903.04420", "submitter": "Gayane Vardoyan", "authors": "Gayane Vardoyan, Saikat Guha, Philippe Nain, Don Towsley", "title": "On the Stochastic Analysis of a Quantum Entanglement Switch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a quantum entanglement switch that serves $k$ users in a star\ntopology. We model variants of the system using Markov chains and standard\nqueueing theory and obtain expressions for switch capacity and the expected\nnumber of qubits stored in memory at the switch. While it is more accurate to\nuse a discrete-time Markov chain (DTMC) to model such systems, we quickly\nencounter practical constraints of using this technique and switch to using\ncontinuous-time Markov chains (CTMCs). Using CTMCs allows us to obtain a number\nof analytic results for systems in which the links are homogeneous or\nheterogeneous and for switches that have infinite or finite buffer sizes. In\naddition, we can model the effects of decoherence of quantum states fairly\neasily using CTMCs. We also compare the results we obtain from the DTMC against\nthe CTMC in the case of homogeneous links and infinite buffer, and learn that\nthe CTMC is a reasonable approximation of the DTMC. From numerical\nobservations, we discover that decoherence has little effect on capacity and\nexpected number of stored qubits for homogeneous systems. For heterogeneous\nsystems, especially those operating close to stability constraints, buffer size\nand decoherence can have significant effects on performance metrics. We also\nlearn that in general, increasing the buffer size from one to two qubits per\nlink is advantageous to most systems, while increasing the buffer size further\nyields diminishing returns.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 16:30:08 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 18:43:37 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Vardoyan", "Gayane", ""], ["Guha", "Saikat", ""], ["Nain", "Philippe", ""], ["Towsley", "Don", ""]]}, {"id": "1903.04611", "submitter": "Ang Li", "authors": "Ang Li and Shuaiwen Leon Song and Jieyang Chen and Jiajia Li and Xu\n  Liu and Nathan Tallent and Kevin Barker", "title": "Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and\n  GPUDirect", "comments": "15 pages. The paper is going to be submitted to TPDS", "journal-ref": null, "doi": "10.1109/TPDS.2019.2928289", "report-no": null, "categories": "cs.AR cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance multi-GPU computing becomes an inevitable trend due to the\never-increasing demand on computation capability in emerging domains such as\ndeep learning, big data and planet-scale simulations. However, the lack of deep\nunderstanding on how modern GPUs can be connected and the real impact of\nstate-of-the-art interconnect technology on multi-GPU application performance\nbecome a hurdle. In this paper, we fill the gap by conducting a thorough\nevaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1,\nNVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC\nplatforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit\nsupercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080\nGPUs. Based on the empirical evaluation, we have observed four new types of GPU\ncommunication network NUMA effects: three are triggered by NVLink's topology,\nconnectivity and routing, while one is caused by PCIe chipset design issue.\nThese observations indicate that, for an application running in a multi-GPU\nnode, choosing the right GPU combination can impose considerable impact on GPU\ncommunication efficiency, as well as the application's overall performance. Our\nevaluation can be leveraged in building practical multi-GPU performance models,\nwhich are vital for GPU task allocation, scheduling and migration in a shared\nenvironment (e.g., AI cloud and HPC centers), as well as communication-oriented\nperformance tuning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 21:21:21 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Li", "Ang", ""], ["Song", "Shuaiwen Leon", ""], ["Chen", "Jieyang", ""], ["Li", "Jiajia", ""], ["Liu", "Xu", ""], ["Tallent", "Nathan", ""], ["Barker", "Kevin", ""]]}, {"id": "1903.05714", "submitter": "Joseph Izraelevitz", "authors": "Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amirsaman\n  Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R. Dulloor, Jishen\n  Zhao and Steven Swanson", "title": "Basic Performance Measurements of the Intel Optane DC Persistent Memory\n  Module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scalable nonvolatile memory DIMMs will finally be commercially available with\nthe release of the Intel Optane DC Persistent Memory Module (or just \"Optane DC\nPMM\"). This new nonvolatile DIMM supports byte-granularity accesses with access\ntimes on the order of DRAM, while also providing data storage that survives\npower outages. This work comprises the first in-depth, scholarly, performance\nreview of Intel's Optane DC PMM, exploring its capabilities as a main memory\ndevice, and as persistent, byte-addressable memory exposed to user-space\napplications. This report details the technologies performance under a number\nof modes and scenarios, and across a wide variety of macro-scale benchmarks.\nOptane DC PMMs can be used as large memory devices with a DRAM cache to hide\ntheir lower bandwidth and higher latency. When used in this Memory (or cached)\nmode, Optane DC memory has little impact on applications with small memory\nfootprints. Applications with larger memory footprints may experience some\nslow-down relative to DRAM, but are now able to keep much more data in memory.\nWhen used under a file system, Optane DC PMMs can result in significant\nperformance gains, especially when the file system is optimized to use the\nload/store interface of the Optane DC PMM and the application uses many small,\npersistent writes. For instance, using the NOVA-relaxed NVMM file system, we\ncan improve the performance of Kyoto Cabinet by almost 2x. Optane DC PMMs can\nalso enable user-space persistence where the application explicitly controls\nits writes into persistent Optane DC media. In our experiments, modified\napplications that used user-space Optane DC persistence generally outperformed\ntheir file system counterparts. For instance, the persistent version of RocksDB\nperformed almost 2x faster than the equivalent program utilizing an NVMM-aware\nfile system.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 21:14:40 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 20:19:20 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 18:41:24 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Izraelevitz", "Joseph", ""], ["Yang", "Jian", ""], ["Zhang", "Lu", ""], ["Kim", "Juno", ""], ["Liu", "Xiao", ""], ["Memaripour", "Amirsaman", ""], ["Soh", "Yun Joon", ""], ["Wang", "Zixuan", ""], ["Xu", "Yi", ""], ["Dulloor", "Subramanya R.", ""], ["Zhao", "Jishen", ""], ["Swanson", "Steven", ""]]}, {"id": "1903.05898", "submitter": "Siqi Wang", "authors": "Siqi Wang, Gayathri Ananthanarayanan, Yifan Zeng, Neeraj Goel, Anuj\n  Pathania, Tulika Mitra", "title": "High-Throughput CNN Inference on Embedded ARM big.LITTLE Multi-Core\n  Processors", "comments": "Accepted to IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems", "journal-ref": "in IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems, vol. 39, no. 10, pp. 2254-2267, Oct. 2020", "doi": "10.1109/TCAD.2019.2944584", "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT Edge intelligence requires Convolutional Neural Network (CNN) inference\nto take place in the edge devices itself. ARM big.LITTLE architecture is at the\nheart of prevalent commercial edge devices. It comprises of single-ISA\nheterogeneous cores grouped into multiple homogeneous clusters that enable\npower and performance trade-offs. All cores are expected to be simultaneously\nemployed in inference to attain maximal throughput. However, high communication\noverhead involved in parallelization of computations from convolution kernels\nacross clusters is detrimental to throughput. We present an alternative\nframework called Pipe-it that employs pipelined design to split convolutional\nlayers across clusters while limiting parallelization of their respective\nkernels to the assigned cluster. We develop a performance-prediction model that\nutilizes only the convolutional layer descriptors to predict the execution time\nof each layer individually on all permitted core configurations (type and\ncount). Pipe-it then exploits the predictions to create a balanced pipeline\nusing an efficient design space exploration algorithm. Pipe-it on average\nresults in a 39% higher throughput than the highest antecedent throughput.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 10:24:57 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 07:43:41 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 15:46:27 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Wang", "Siqi", ""], ["Ananthanarayanan", "Gayathri", ""], ["Zeng", "Yifan", ""], ["Goel", "Neeraj", ""], ["Pathania", "Anuj", ""], ["Mitra", "Tulika", ""]]}, {"id": "1903.05918", "submitter": "Carsten Kutzner", "authors": "Carsten Kutzner, Szil\\'ard P\\'all, Martin Fechner, Ansgar Esztermann,\n  Bert L. de Groot, Helmut Grubm\\\"uller", "title": "More Bang for Your Buck: Improved use of GPU Nodes for GROMACS 2018", "comments": "41 pages, 13 figures, 4 tables. This updated version includes the\n  following improvements: - most notably, added benchmarks for two coarse grain\n  MARTINI systems VES and BIG, resulting in a new Figure 13 - fixed typos -\n  made text clearer in some places - added two more benchmarks for MEM and RIB\n  systems (E3-1240v6 + RTX 2080 / 2080Ti)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF physics.bio-ph physics.comp-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify hardware that is optimal to produce molecular dynamics\ntrajectories on Linux compute clusters with the GROMACS 2018 simulation\npackage. Therefore, we benchmark the GROMACS performance on a diverse set of\ncompute nodes and relate it to the costs of the nodes, which may include their\nlifetime costs for energy and cooling. In agreement with our earlier\ninvestigation using GROMACS 4.6 on hardware of 2014, the performance to price\nratio of consumer GPU nodes is considerably higher than that of CPU nodes.\nHowever, with GROMACS 2018, the optimal CPU to GPU processing power balance has\nshifted even more towards the GPU. Hence, nodes optimized for GROMACS 2018 and\nlater versions enable a significantly higher performance to price ratio than\nnodes optimized for older GROMACS versions. Moreover, the shift towards GPU\nprocessing allows to cheaply upgrade old nodes with recent GPUs, yielding\nessentially the same performance as comparable brand-new hardware.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 11:06:54 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 09:59:55 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Kutzner", "Carsten", ""], ["P\u00e1ll", "Szil\u00e1rd", ""], ["Fechner", "Martin", ""], ["Esztermann", "Ansgar", ""], ["de Groot", "Bert L.", ""], ["Grubm\u00fcller", "Helmut", ""]]}, {"id": "1903.06396", "submitter": "Dimo Brockhoff", "authors": "Ouassim Elhara (RANDOPT), Konstantinos Varelas (RANDOPT), Duc Nguyen\n  (HNUE), Tea Tusar (IJS), Dimo Brockhoff (RANDOPT), Nikolaus Hansen (RANDOPT),\n  Anne Auger (RANDOPT)", "title": "COCO: The Large Scale Black-Box Optimization Benchmarking\n  (bbob-largescale) Test Suite", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bbob-largescale test suite, containing 24 single-objective functions in\ncontinuous domain, extends the well-known single-objective noiseless bbob test\nsuite, which has been used since 2009 in the BBOB workshop series, to large\ndimension. The core idea is to make the rotational transformations R, Q in\nsearch space that appear in the bbob test suite computationally cheaper while\nretaining some desired properties. This documentation presents an approach that\nreplaces a full rotational transformation with a combination of a\nblock-diagonal matrix and two permutation matrices in order to construct test\nfunctions whose computational and memory costs scale linearly in the dimension\nof the problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 07:47:05 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 15:24:13 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Elhara", "Ouassim", "", "RANDOPT"], ["Varelas", "Konstantinos", "", "RANDOPT"], ["Nguyen", "Duc", "", "HNUE"], ["Tusar", "Tea", "", "IJS"], ["Brockhoff", "Dimo", "", "RANDOPT"], ["Hansen", "Nikolaus", "", "RANDOPT"], ["Auger", "Anne", "", "RANDOPT"]]}, {"id": "1903.06419", "submitter": "Mahdieh Ahmadi", "authors": "Mahdieh Ahmadi, James Roberts, Emilio Leonardi, Ali Movaghar", "title": "Impact of Traffic Characteristics on Request Aggregation in an NDN\n  Router", "comments": null, "journal-ref": null, "doi": "10.1016/j.peva.2020.102081", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper revisits the performance evaluation of caching in a Named Data\nNetworking (NDN) router where the content store (CS) is supplemented by a\npending interest table (PIT). The PIT aggregates requests for a given content\nthat arrive within the download delay and thus brings an additional reduction\nin upstream bandwidth usage beyond that due to CS hits. We extend prior work on\ncaching with non-zero download delay (non-ZDD) by proposing a novel\nmathematical framework that is more easily applicable to general traffic models\nand by considering alternative cache insertion policies. Specifically we\nevaluate the use of an LRU filter to improve CS hit rate performance in this\nnon-ZDD context. We also consider the impact of time locality in demand due to\nfinite content lifetimes. The models are used to quantify the impact of the PIT\non upstream bandwidth reduction, demonstrating notably that this is significant\nonly for relatively small content catalogues or high average request rate per\ncontent. We further explore how the effectiveness of the filter with finite\ncontent lifetimes depends on catalogue size and traffic intensity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 09:17:40 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Ahmadi", "Mahdieh", ""], ["Roberts", "James", ""], ["Leonardi", "Emilio", ""], ["Movaghar", "Ali", ""]]}, {"id": "1903.07525", "submitter": "Sergiy Popovych", "authors": "Sergiy Popovych, Davit Buniatyan, Aleksandar Zlateski, Kai Li, H.\n  Sebastian Seung", "title": "PZnet: Efficient 3D ConvNet Inference on Manycore CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional nets have been shown to achieve state-of-the-art accuracy in\nmany biomedical image analysis tasks. Many tasks within biomedical analysis\ndomain involve analyzing volumetric (3D) data acquired by CT, MRI and\nMicroscopy acquisition methods. To deploy convolutional nets in practical\nworking systems, it is important to solve the efficient inference problem.\nNamely, one should be able to apply an already-trained convolutional network to\nmany large images using limited computational resources. In this paper we\npresent PZnet, a CPU-only engine that can be used to perform inference for a\nvariety of 3D convolutional net architectures. PZNet outperforms MKL-based CPU\nimplementations of PyTorch and Tensorflow by more than 3.5x for the popular\nU-net architecture. Moreover, for 3D convolutions with low featuremap numbers,\ncloud CPU inference with PZnet outperfroms cloud GPU inference in terms of cost\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:07:12 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Popovych", "Sergiy", ""], ["Buniatyan", "Davit", ""], ["Zlateski", "Aleksandar", ""], ["Li", "Kai", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1903.08856", "submitter": "Thanh Son Lam Nguyen", "authors": "Thanh Son Lam Nguyen (LIP6, Sorbonne University, NPA), Guillaume\n  Jourjon (NICTA), Maria Potop-Butucaru (LIP6, Sorbonne University, LINCS,\n  NPA), Kim Thai (LIP6, Sorbonne University, NPA)", "title": "Impact of network delays on Hyperledger Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has become one of the most attractive technologies for\napplications, with a large range of deployments such as production, economy, or\nbanking. Under the hood, Blockchain technology is a type of distributed\ndatabase that supports untrusted parties. In this paper we focus Hyperledger\nFabric, the first blockchain in the market tailored for a private environment,\nallowing businesses to create a permissioned network. Hyperledger Fabric\nimplements a PBFT consensus in order to maintain a non forking blockchain at\nthe application level. We deployed this framework over an area network between\nFrance and Germany in order to evaluate its performance when potentially large\nnetwork delays are observed. Overall we found that when network delay increases\nsignificantly (i.e. up to 3.5 seconds at network layer between two clouds), we\nobserved that the blocks added to our blockchain had up to 134 seconds offset\nafter 100 th block from one cloud to another. Thus by delaying block\npropagation, we demonstrated that Hyperledger Fabric does not provide\nsufficient consistency guaranties to be deployed in critical environments. Our\nwork, is the fist to evidence the negative impact of network delays on a\nPBFT-based blockchain.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 07:21:33 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Nguyen", "Thanh Son Lam", "", "LIP6, Sorbonne University, NPA"], ["Jourjon", "Guillaume", "", "NICTA"], ["Potop-Butucaru", "Maria", "", "LIP6, Sorbonne University, LINCS,\n  NPA"], ["Thai", "Kim", "", "LIP6, Sorbonne University, NPA"]]}, {"id": "1903.09346", "submitter": "Benjamin Berg", "authors": "Benjamin Berg, Rein Vesilo, Mor Harchol-Balter", "title": "heSRPT: Optimal Parallel Scheduling of Jobs With Known Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When parallelizing a set of jobs across many servers, one must balance a\ntrade-off between granting priority to short jobs and maintaining the overall\nefficiency of the system. When the goal is to minimize the mean flow time of a\nset of jobs, it is usually the case that one wants to complete short jobs\nbefore long jobs. However, since jobs usually cannot be parallelized with\nperfect efficiency, granting strict priority to the short jobs can result in\nvery low system efficiency which in turn hurts the mean flow time across jobs.\nIn this paper, we derive the optimal policy for allocating servers to jobs at\nevery moment in time in order to minimize mean flow time across jobs. We assume\nthat jobs follow a sublinear, concave speedup function, and hence jobs\nexperience diminishing returns from being allocated additional servers. We show\nthat the optimal policy, heSRPT, will complete jobs according to their size\norder, but maintains overall system efficiency by allocating some servers to\neach job at every moment in time. We compare heSRPT with state-of-the-art\nallocation policies from the literature and show that heSRPT outperforms its\ncompetitors by at least 30%, and often by much more.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 03:42:48 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 20:27:47 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Berg", "Benjamin", ""], ["Vesilo", "Rein", ""], ["Harchol-Balter", "Mor", ""]]}, {"id": "1903.09347", "submitter": "Maria F. Borge", "authors": "Mar\\'ia F. Borge, Florin Dinu, Willy Zwaenepoel", "title": "Understanding and taming SSD read performance variability: HDFS case\n  study", "comments": "13 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the influence that lower layers (file system, OS,\nSSD) have on HDFS' ability to extract maximum performance from SSDs on the read\npath. We uncover and analyze three surprising performance slowdowns induced by\nlower layers that result in HDFS read throughput loss. First, intrinsic\nslowdown affects reads from every new file system extent for a variable amount\nof time. Second, temporal slowdown appears temporarily and periodically and is\nworkload-agnostic. Third, in permanent slowdown, some files can individually\nand permanently become slower after a period of time. We analyze the impact of\nthese slowdowns on HDFS and show significant throughput loss. Individually,\neach of the slowdowns can cause a read throughput loss of 10-15%. However,\ntheir effect is cumulative. When all slowdowns happen concurrently, read\nthroughput drops by as much as 30%. We further analyze mitigation techniques\nand show that two of the three slowdowns could be addressed via increased IO\nrequest parallelism in the lower layers. Unfortunately, HDFS cannot\nautomatically adapt to use such additional parallelism. Our results point to a\nneed for adaptability in storage stacks. The reason is that an access pattern\nthat maximizes performance in the common case is not necessarily the same one\nthat can mask performance fluctuations.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 03:46:22 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Borge", "Mar\u00eda F.", ""], ["Dinu", "Florin", ""], ["Zwaenepoel", "Willy", ""]]}, {"id": "1903.09548", "submitter": "Martin Geier", "authors": "Martin Geier (1), Dominik Faller (1), Marian Br\\\"andle (1) and\n  Samarjit Chakraborty (1) ((1) Technical University of Munich)", "title": "Cost-effective Energy Monitoring of a Zynq-based Real-time System\n  including dual Gigabit Ethernet", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing integration of fine-grained power management features already\nestablished in CPU-driven Systems-on-Chip (SoCs) enables both traditional Field\nProgrammable Gate Arrays (FPGAs) and, more recently, hybrid Programmable SoCs\n(pSoCs) to reach more energy-sensitive application domains (such as, e.g.,\nautomotive and robotics). By combining a fixed-function multi-core SoC with\nflexible, configurable FPGA fabric, the latter can be used to realize\nheterogeneous Real-time Systems (RTSs) commonly implementing complex\napplication-specific architectures with high computation and communication\n(I/O) densities. Their dynamic changes in workload, currently active power\nsaving features and thus power consumption require precise voltage and current\nsensing on all relevant supply rails to enable dependable evaluation of the\nvarious power management techniques. In this paper, we propose a low-cost\n18-channel 16-bit-resolution measurement (sub-)system capable of 200 kSPS\n(kilo-samples per second) for instrumentation of current pSoC development\nboards. To this end, we join simultaneously sampling analog-to-digital\nconverters (ADCs) and analog voltage/current sensing circuitry with a Cortex M7\nmicrocontroller using an SD card for storage. In addition, we propose to\ninclude crucial I/O components such as Ethernet PHYs into the power monitoring\nto gain a holistic view on the RTS's temporal behavior covering not only\ncomputation on FPGA and CPUs, but also communication in terms of, e.g.,\nreception of sensor values and transmission of actuation signals. We present an\nFMC-sized implementation of our measurement system combined with two Gigabit\nEthernet PHYs and one HDMI input. Paired with Xilinx' ZC702 development board,\nwe are able to synchronously acquire power traces of a Zynq pSoC and the two\nPHYs precise enough to identify individual Ethernet frames.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 15:15:42 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Geier", "Martin", "", "Technical University of Munich"], ["Faller", "Dominik", "", "Technical University of Munich"], ["Br\u00e4ndle", "Marian", "", "Technical University of Munich"], ["Chakraborty", "Samarjit", "", "Technical University of Munich"]]}, {"id": "1903.09799", "submitter": "Hao-Yun Chen", "authors": "Hao-Yun Chen, Jhao-Hong Liang, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting\n  Chen, Wei Wei, Da-Cheng Juan", "title": "Improving Adversarial Robustness via Guided Complement Entropy", "comments": "ICCV'19 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness has emerged as an important topic in deep learning as\ncarefully crafted attack samples can significantly disturb the performance of a\nmodel. Many recent methods have proposed to improve adversarial robustness by\nutilizing adversarial training or model distillation, which adds additional\nprocedures to model training. In this paper, we propose a new training paradigm\ncalled Guided Complement Entropy (GCE) that is capable of achieving\n\"adversarial defense for free,\" which involves no additional procedures in the\nprocess of improving adversarial robustness. In addition to maximizing model\nprobabilities on the ground-truth class like cross-entropy, we neutralize its\nprobabilities on the incorrect classes along with a \"guided\" term to balance\nbetween these two terms. We show in the experiments that our method achieves\nbetter model robustness with even better performance compared to the commonly\nused cross-entropy training objective. We also show that our method can be used\northogonal to adversarial training across well-known methods with noticeable\nrobustness gain. To the best of our knowledge, our approach is the first one\nthat improves model robustness without compromising performance.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 11:14:59 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 04:07:55 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 06:11:33 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Chen", "Hao-Yun", ""], ["Liang", "Jhao-Hong", ""], ["Chang", "Shih-Chieh", ""], ["Pan", "Jia-Yu", ""], ["Chen", "Yu-Ting", ""], ["Wei", "Wei", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "1903.10171", "submitter": "Takashi Ikegawa", "authors": "Takashi Ikegawa", "title": "Effect of payload size on goodput when message segmentations occur for\n  wireless networks: Case of packet corruptions recovered by stop-and-wait\n  protocol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the effect of payload size on goodput for wireless\nnetworks where packets created from a message through a segmentation function\nare lost due to bit errors and they are recovered by a stop-and-wait protocol.\nTo achieve this, we derive the exact analytical form of goodput using the\nanalytical form of a packet-size distribution, given a message-size\ndistribution and a payload size. In previous work, the packet sizes are assumed\nto be constant, which are payload size plus header size, although actual\nsegmented packets are not constant in size. Hence, this constant packet-size\nassumption may be not justified for goodput analysis. From numerical results,\nwe show that the constant packet-size assumption is not justified under low\nbit-error rates. Furthermore, we indicate that the curves of goodput are\nconcave in payload size under high bit-error rates. In addition, we show that\nthe larger mean bit-error burst length yields less concave curves of goodput.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 08:22:27 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Ikegawa", "Takashi", ""]]}, {"id": "1903.10982", "submitter": "Lev N. Shchur", "authors": "Alexander Russkov and Lev Shchur", "title": "Matrix multiplication and universal scalability of the time on the Intel\n  Scalable processors", "comments": null, "journal-ref": "Journal of Physics: Conference Series, volume 1163, 012079, 2019", "doi": "10.1088/1742-6596/1163/1/012079", "report-no": null, "categories": "cond-mat.stat-mech cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplication is one of the core operations in many areas of\nscientific computing. We present the results of the experiments with the matrix\nmultiplication of the big size comparable with the big size of the onboard\nmemory, which is 1.5 terabyte in our case. We run experiments on the computing\nboard with two sockets and with two Intel Xeon Platinum 8164 processors, each\nwith 26 cores and with multi-threading. The most interesting result of our\nstudy is the observation of the perfect scalability law of the matrix\nmultiplication, and of the universality of this law.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:11:47 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Russkov", "Alexander", ""], ["Shchur", "Lev", ""]]}, {"id": "1903.11588", "submitter": "Alina Costea", "authors": "Gh. Miscoi, A. Costea, R.I. \\c{T}icu, C. Pomazan", "title": "Algorithms of evaluation of the waiting time and the modelling of the\n  terminal activity", "comments": null, "journal-ref": "Ponte Journal, 2016", "doi": "10.21506/j.ponte.2016.8.17", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper approaches the application of the waiting model with Poisson\ninputs and priorities in the port activity. The arrival of ships in the\nmaritime terminal is numerically modelled, and specific parameters for the\ndistribution functions of service and of inputs are determined, in order to\nestablish the waiting time of ships in the seaport and a stationary process.\nThe modelling is based on waiting times and on the traffic coefficient.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 09:06:13 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Miscoi", "Gh.", ""], ["Costea", "A.", ""], ["\u0162icu", "R. I.", ""], ["Pomazan", "C.", ""]]}]