[{"id": "1506.00243", "submitter": "Hui Wang", "authors": "Hui Wang and Anthony TS Ho and Shujun Li", "title": "OR-Benchmark: An Open and Reconfigurable Digital Watermarking\n  Benchmarking Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmarking digital watermarking algorithms is not an easy task because\ndifferent applications of digital watermarking often have very different sets\nof requirements and trade-offs between conflicting requirements. While there\nhave been some general-purpose digital watermarking benchmarking systems\navailable, they normally do not support complicated benchmarking tasks and\ncannot be easily reconfigured to work with different watermarking algorithms\nand testing conditions. In this paper, we propose OR-Benchmark, an open and\nhighly reconfigurable general-purpose digital watermarking benchmarking\nframework, which has the following two key features: 1) all the interfaces are\npublic and general enough to support all watermarking applications and\nbenchmarking tasks we can think of; 2) end users can easily extend the\nfunctionalities and freely configure what watermarking algorithms are tested,\nwhat system components are used, how the benchmarking process runs, and what\nresults should be produced. We implemented a prototype of this framework as a\nMATLAB software package and used it to benchmark a number of digital\nwatermarking algorithms involving two types of watermarks for content\nauthentication and self-restoration purposes. The benchmarking results\ndemonstrated the advantages of the proposed benchmarking framework, and also\ngave us some useful insights about existing image authentication and\nself-restoration watermarking algorithms which are an important but less\nstudied topic in digital watermarking.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 14:50:26 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 14:49:01 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Wang", "Hui", ""], ["Ho", "Anthony TS", ""], ["Li", "Shujun", ""]]}, {"id": "1506.01494", "submitter": "Rui Han", "authors": "Rui Han, Zhen Jia, Wanling Gao, Xinhui Tian, and Lei Wang", "title": "Benchmarking Big Data Systems: State-of-the-Art and Future Directions", "comments": "9 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:1402.5194", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great prosperity of big data systems such as Hadoop in recent years makes\nthe benchmarking of these systems become crucial for both research and industry\ncommunities. The complexity, diversity, and rapid evolution of big data systems\ngives rise to various new challenges about how we design generators to produce\ndata with the 4V properties (i.e. volume, velocity, variety and veracity), as\nwell as implement application-specific but still comprehensive workloads.\nHowever, most of the existing big data benchmarks can be described as attempts\nto solve specific problems in benchmarking systems. This article investigates\nthe state-of-the-art in benchmarking big data systems along with the future\nchallenges to be addressed to realize a successful and efficient benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 07:48:37 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Han", "Rui", ""], ["Jia", "Zhen", ""], ["Gao", "Wanling", ""], ["Tian", "Xinhui", ""], ["Wang", "Lei", ""]]}, {"id": "1506.02833", "submitter": "Albert Saa-Garriga", "authors": "Albert Sa\\`a-Garriga and David Castells-Rufas and Jordi Carrabina", "title": "OMP2HMPP: Compiler Framework for Energy Performance Trade-off Analysis\n  of Automatically Generated Codes", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Volume 12,\n  Issue 2, March 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present OMP2HMPP, a tool that, in a first step, automatically translates\nOpenMP code into various possible transformations of HMPP. In a second step\nOMP2HMPP executes all variants to obtain the performance and power consumption\nof each transformation. The resulting trade-off can be used to choose the more\nconvenient version. After running the tool on a set of codes from the Polybench\nbenchmark we show that the best automatic transformation is equivalent to a\nmanual one done by an expert. Compared with original OpenMP code running in 2\nquad-core processors we obtain an average speed-up of 31x and 5.86x factor in\noperations per watt.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 09:24:38 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Sa\u00e0-Garriga", "Albert", ""], ["Castells-Rufas", "David", ""], ["Carrabina", "Jordi", ""]]}, {"id": "1506.03551", "submitter": "Abolfazl Diyanat", "authors": "Ahmad Khonsari, Seyed Pooya Shariatpanahi, Abolfazl Diyanat, Hossein\n  Shafiei", "title": "On the Feasibility of Wireless Interconnects for High-throughput Data\n  Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Centers (DCs) are required to be scalable to large data sets so as to\naccommodate ever increasing demands of resource-limited embedded and mobile\ndevices. Thanks to the availability of recent high data rate millimeter-wave\nfrequency spectrum such as 60GHz and due to the favorable attributes of this\ntechnology, wireless DC (WDC) exhibits the potentials of being a promising\nsolution especially for small to medium scale DCs. This paper investigates the\nproblem of throughput scalability of WDCs using the established theory of the\nasymptotic throughput of wireless multi-hop networks that are primarily\nproposed for homogeneous traffic conditions. The rate-heterogeneous traffic\ndistribution of a data center however, requires the asymptotic heterogeneous\nthroughput knowledge of a wireless network in order to study the performance\nand feasibility of WDCs for practical purposes. To answer these questions this\npaper presents a lower bound for the throughput scalability of a multi-hop\nrate-heterogeneous network when traffic generation rates of all nodes are\nsimilar, except one node. We demonstrate that the throughput scalability of\nconventional multi-hopping and the spatial reuse of the above bi-rate network\nis inefficient and henceforth develop a speculative 2-partitioning scheme that\nimproves the network throughput scaling potentials. A better lower bound of the\nthroughput is then obtained. Finally, we obtain the throughput scaling of an\ni.i.d. rate-heterogeneous network and obtain its lower bound. Again we propose\na speculative 2-partitioning scheme to achieve a network with higher throughput\nin terms of improved lower bound. All of the obtained results have been\nverified using simulation experiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 06:02:06 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Khonsari", "Ahmad", ""], ["Shariatpanahi", "Seyed Pooya", ""], ["Diyanat", "Abolfazl", ""], ["Shafiei", "Hossein", ""]]}, {"id": "1506.03997", "submitter": "Markus Wittmann", "authors": "Markus Wittmann and Thomas Zeiser and Georg Hager and Gerhard Wellein", "title": "Short Note on Costs of Floating Point Operations on current x86-64\n  Architectures: Denormals, Overflow, Underflow, and Division by Zero", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple floating point operations like addition or multiplication on\nnormalized floating point values can be computed by current AMD and Intel\nprocessors in three to five cycles. This is different for denormalized numbers,\nwhich appear when an underflow occurs and the value can no longer be\nrepresented as a normalized floating-point value. Here the costs are about two\nmagnitudes higher.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 12:09:03 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Wittmann", "Markus", ""], ["Zeiser", "Thomas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1506.04006", "submitter": "Sahar Vahdati", "authors": "Sahar Vahdati, Farah Karim, Jyun-Yao Huang, and Christoph Lange", "title": "Mapping Large Scale Research Metadata to Linked Data: A Performance\n  Comparison of HBase, CSV and XML", "comments": "Accepted in 0th Metadata and Semantics Research Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  OpenAIRE, the Open Access Infrastructure for Research in Europe, comprises a\ndatabase of all EC FP7 and H2020 funded research projects, including metadata\nof their results (publications and datasets). These data are stored in an HBase\nNoSQL database, post-processed, and exposed as HTML for human consumption, and\nas XML through a web service interface. As an intermediate format to facilitate\nstatistical computations, CSV is generated internally. To interlink the\nOpenAIRE data with related data on the Web, we aim at exporting them as Linked\nOpen Data (LOD). The LOD export is required to integrate into the overall data\nprocessing workflow, where derived data are regenerated from the base data\nevery day. We thus faced the challenge of identifying the best-performing\nconversion approach.We evaluated the performances of creating LOD by a\nMapReduce job on top of HBase, by mapping the intermediate CSV files, and by\nmapping the XML output.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 12:40:03 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 12:37:36 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Vahdati", "Sahar", ""], ["Karim", "Farah", ""], ["Huang", "Jyun-Yao", ""], ["Lange", "Christoph", ""]]}, {"id": "1506.04263", "submitter": "Kuang Xu", "authors": "Kuang Xu", "title": "Necessity of Future Information in Admission Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the necessity of predictive information in a class of queueing\nadmission control problems, where a system manager is allowed to divert\nincoming jobs up to a fixed rate, in order to minimize the queueing delay\nexperienced by the admitted jobs.\n  Spencer et al. (2014) show that the system's delay performance can be\nsignificantly improved by having access to future information in the form of a\nlookahead window, during which the times of future arrivals and services are\nrevealed. They prove that, while delay under an optimal online policy diverges\nto infinity in the heavy-traffic regime, it can stay bounded by making use of\nfuture information. However, the diversion polices of Spencer et al. (2014)\nrequire the length of the lookahead window to grow to infinity at a non-trivial\nrate in the heavy-traffic regime, and it remained open whether substantial\nperformance improvement could still be achieved with less future information.\n  We resolve this question to a large extent by establishing an asymptotically\ntight lower bound on how much future information is necessary to achieve\nsuperior performance, which matches the upper bound of Spencer et al. (2014) up\nto a constant multiplicative factor. Our result hence demonstrates that the\nsystem's heavy-traffic delay performance is highly sensitive to the amount of\nfuture information available. Our proof is based on analyzing certain excursion\nprobabilities of the input sample paths, and exploiting a connection between a\npolicy's diversion decisions and subsequent server idling, which may be of\nindependent interest for related dynamic resource allocation problems.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 12:42:33 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Xu", "Kuang", ""]]}, {"id": "1506.04657", "submitter": "Markus Fidler", "authors": "Nico Becker, Markus Fidler", "title": "A Non-stationary Service Curve Model for Performance Analysis of\n  Transient Phases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steady-state solutions for a variety of relevant queueing systems are known\ntoday, e.g., from queueing theory, effective bandwidths, and network calculus.\nThe behavior during transient phases, on the other hand, is understood to a\nmuch lesser extent as its analysis poses significant challenges. Considering\nthe majority of short-lived flows, transient effects that have diverse causes,\nsuch as TCP slow start, sleep scheduling in wireless networks, or signalling in\ncellular networks, are, however, predominant. This paper contributes a general\nmodel of regenerative service processes to characterize the transient behavior\nof systems. The model leads to a notion of non-stationary service curves that\ncan be conveniently integrated into the framework of the stochastic network\ncalculus. We derive respective models of sleep scheduling and show the\nsignificant impact of transient phases on backlogs and delays. We also consider\nmeasurement methods that estimate the service of an unknown system from\nobservations of selected probe traffic. We find that the prevailing rate\nscanning method does not recover the service during transient phases well. This\nlimitation is fundamental as it is explained by the non-convexity of\nnon-stationary service curves. A second key difficulty is proven to be due to\nthe super-additivity of network service processes. We devise a novel two-phase\nprobing technique that first determines a minimal pattern of probe traffic.\nThis probe is used to obtain an accurate estimate of the unknown transient\nservice.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 16:33:41 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Becker", "Nico", ""], ["Fidler", "Markus", ""]]}, {"id": "1506.05157", "submitter": "Martin Schreiber", "authors": "Martin Schreiber, Adam Peddle, Terry Haut, Beth Wingate", "title": "A Decentralized Parallelization-in-Time Approach with Parareal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With steadily increasing parallelism for high-performance architectures,\nsimulations requiring a good strong scalability are prone to be limited in\nscalability with standard spatial-decomposition strategies at a certain amount\nof parallel processors. This can be a show-stopper if the simulation results\nhave to be computed with wallclock time restrictions (e.g.\\,for weather\nforecasts) or as fast as possible (e.g. for urgent computing). Here, the\ntime-dimension is the only one left for parallelization and we focus on\nParareal as one particular parallelization-in-time method.\n  We discuss a software approach for making Parareal parallelization\ntransparent for application developers, hence allowing fast prototyping for\nParareal. Further, we introduce a decentralized Parareal which results in\nautonomous simulation instances which only require communicating with the\nprevious and next simulation instances, hence with strong locality for\ncommunication. This concept is evaluated by a prototypical solver for the\nrotational shallow-water equations which we use as a representative black-box\nsolver.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 21:46:19 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 14:51:22 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Schreiber", "Martin", ""], ["Peddle", "Adam", ""], ["Haut", "Terry", ""], ["Wingate", "Beth", ""]]}, {"id": "1506.06011", "submitter": "Guy Fayolle", "authors": "Guy Fayolle and Paul Muhlethaler", "title": "A Markovian Analysis of IEEE 802.11 Broadcast Transmission Networks with\n  Buffering", "comments": null, "journal-ref": "Prob. Eng. Inf. Sci. 30 (2016) 326-344", "doi": "10.1017/S0269964816000036", "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to analyze the so-called back-off technique of\nthe IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to\nexisting models, packets arriving when a station (or node) is in back-off state\nare not discarded, but are stored in a buffer of infinite capacity. As in\nprevious studies, the key point of our analysis hinges on the assumption that\nthe time on the channel is viewed as a random succession of transmission slots\n(whose duration corresponds to the length of a packet) and mini-slots during\nwhich the back-o? of the station is decremented. These events occur\nindependently, with given probabilities. The state of a node is represented by\na two-dimensional Markov chain in discrete-time, formed by the back-off counter\nand the number of packets at the station. Two models are proposed both of which\nare shown to cope reasonably well with the physical principles of the protocol.\nThe stabillity (ergodicity) conditions are obtained and interpreted in terms of\nmaximum throughput. Several approximations related to these models are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 14:12:00 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Fayolle", "Guy", ""], ["Muhlethaler", "Paul", ""]]}, {"id": "1506.06256", "submitter": "Grigori Fursin", "authors": "Grigori Fursin and Abdul Memon and Christophe Guillon and Anton\n  Lokhmotov", "title": "Collective Mind, Part II: Towards Performance- and Cost-Aware Software\n  Engineering as a Natural Science", "comments": "Presented at the 18th International Workshop on Compilers for\n  Parallel Computing (CPC'15), London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, engineers have to develop software often without even knowing which\nhardware it will eventually run on in numerous mobile phones, tablets,\ndesktops, laptops, data centers, supercomputers and cloud services.\nUnfortunately, optimizing compilers are not keeping pace with ever increasing\ncomplexity of computer systems anymore and may produce severely underperforming\nexecutable codes while wasting expensive resources and energy.\n  We present our practical and collaborative solution to this problem via\nlight-weight wrappers around any software piece when more than one\nimplementation or optimization choice available. These wrappers are connected\nwith a public Collective Mind autotuning infrastructure and repository of\nknowledge (c-mind.org/repo) to continuously monitor various important\ncharacteristics of these pieces (computational species) across numerous\nexisting hardware configurations together with randomly selected optimizations.\nSimilar to natural sciences, we can now continuously track winning solutions\n(optimizations for a given hardware) that minimize all costs of a computation\n(execution time, energy spent, code size, failures, memory and storage\nfootprint, optimization time, faults, contentions, inaccuracy and so on) of a\ngiven species on a Pareto frontier along with any unexpected behavior. The\ncommunity can then collaboratively classify solutions, prune redundant ones,\nand correlate them with various features of software, its inputs (data sets)\nand used hardware either manually or using powerful predictive analytics\ntechniques. Our approach can then help create a large, realistic, diverse,\nrepresentative, and continuously evolving benchmark with related optimization\nknowledge while gradually covering all possible software and hardware to be\nable to predict best optimizations and improve compilers and hardware depending\non usage scenarios and requirements.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 15:34:39 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Fursin", "Grigori", ""], ["Memon", "Abdul", ""], ["Guillon", "Christophe", ""], ["Lokhmotov", "Anton", ""]]}, {"id": "1506.07742", "submitter": "Ahsan Javed Awan", "authors": "Ahsan Javed Awan, Mats Brorsson, Vladimir Vlassov and Eduard Ayguade", "title": "Performance Characterization of In-Memory Data Analytics on a Modern\n  Cloud Server", "comments": "Accepted to The 5th IEEE International Conference on Big Data and\n  Cloud Computing (BDCloud 2015)", "journal-ref": null, "doi": "10.1109/BDCloud.2015.37", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In last decade, data analytics have rapidly progressed from traditional\ndisk-based processing to modern in-memory processing. However, little effort\nhas been devoted at enhancing performance at micro-architecture level. This\npaper characterizes the performance of in-memory data analytics using Apache\nSpark framework. We use a single node NUMA machine and identify the bottlenecks\nhampering the scalability of workloads. We also quantify the inefficiencies at\nmicro-architecture level for various data analysis workloads. Through empirical\nevaluation, we show that spark workloads do not scale linearly beyond twelve\nthreads, due to work time inflation and thread level load imbalance. Further,\nat the micro-architecture level, we observe memory bound latency to be the\nmajor cause of work time inflation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 13:23:54 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Awan", "Ahsan Javed", ""], ["Brorsson", "Mats", ""], ["Vlassov", "Vladimir", ""], ["Ayguade", "Eduard", ""]]}, {"id": "1506.07943", "submitter": "Lei Wang", "authors": "Lei Wang, Jianfeng Zhan, Zhen Jia, Rui Han", "title": "Characterization and Architectural Implications of Big Data Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data areas are expanding in a fast way in terms of increasing workloads\nand runtime systems, and this situation imposes a serious challenge to workload\ncharacterization, which is the foundation of innovative system and architecture\ndesign. The previous major efforts on big data benchmarking either propose a\ncomprehensive but a large amount of workloads, or only select a few workloads\naccording to so-called popularity, which may lead to partial or even biased\nobservations. In this paper, on the basis of a comprehensive big data benchmark\nsuite---BigDataBench, we reduced 77 workloads to 17 representative workloads\nfrom a micro-architectural perspective. On a typical state-of-practice\nplatform---Intel Xeon E5645, we compare the representative big data workloads\nwith SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive\nworkload characterization, we have the following observations. First, the big\ndata workloads are data movement dominated computing with more branch\noperations, taking up to 92% percentage in terms of instruction mix, which\nplaces them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC\n(HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark\nbased big data workloads have higher front-end stalls. Comparing with the\ntraditional workloads i. e. PARSEC, the big data workloads have larger\ninstructions footprint. But we also note that, in addition to varied\ninstruction-level parallelism, there are significant disparities of front-end\nefficiencies among different big data workloads. Third, we found complex\nsoftware stacks that fail to use state-of-practise processors efficiently are\none of the main factors leading to high front-end stalls. For the same\nworkloads, the L1I cache miss rates have one order of magnitude differences\namong diverse implementations with different software stacks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 02:29:22 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Wang", "Lei", ""], ["Zhan", "Jianfeng", ""], ["Jia", "Zhen", ""], ["Han", "Rui", ""]]}, {"id": "1506.08435", "submitter": "Kalyana Babu Nakshatrala", "authors": "J. Chang, S. Karra and K. B. Nakshatrala", "title": "Large-scale Optimization-based Non-negative Computational Framework for\n  Diffusion Equations: Parallel Implementation and Performance Studies", "comments": null, "journal-ref": null, "doi": "10.1007/s10915-016-0250-5", "report-no": null, "categories": "cs.NA cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the standard Galerkin formulation, which is often the\nformulation of choice under the finite element method for solving self-adjoint\ndiffusion equations, does not meet maximum principles and the non-negative\nconstraint for anisotropic diffusion equations. Recently, optimization-based\nmethodologies that satisfy maximum principles and the non-negative constraint\nfor steady-state and transient diffusion-type equations have been proposed. To\ndate, these methodologies have been tested only on small-scale academic\nproblems. The purpose of this paper is to systematically study the performance\nof the non-negative methodology in the context of high performance computing\n(HPC). PETSc and TAO libraries are, respectively, used for the parallel\nenvironment and optimization solvers. For large-scale problems, it is important\nfor computational scientists to understand the computational performance of\ncurrent algorithms available in these scientific libraries. The numerical\nexperiments are conducted on the state-of-the-art HPC systems, and a\nsingle-core performance model is used to better characterize the efficiency of\nthe solvers. Our studies indicate that the proposed non-negative computational\nframework for diffusion-type equations exhibits excellent strong scaling for\nreal-world large-scale problems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 18:57:25 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2015 21:45:18 GMT"}, {"version": "v3", "created": "Sat, 9 Apr 2016 18:44:40 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Chang", "J.", ""], ["Karra", "S.", ""], ["Nakshatrala", "K. B.", ""]]}, {"id": "1506.08988", "submitter": "Francisco Igual", "authors": "Sandra Catal\\'an, Francisco D. Igual, Rafael Mayo, Rafael\n  Rodr\\'iguez-S\\'anchez and Enrique S. Quintana-Ort\\'i", "title": "Architecture-Aware Configuration and Scheduling of Matrix Multiplication\n  on Asymmetric Multicore Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetric multicore processors (AMPs) have recently emerged as an appealing\ntechnology for severely energy-constrained environments, especially in mobile\nappliances where heterogeneity in applications is mainstream. In addition,\ngiven the growing interest for low-power high performance computing, this type\nof architectures is also being investigated as a means to improve the\nthroughput-per-Watt of complex scientific applications.\n  In this paper, we design and embed several architecture-aware optimizations\ninto a multi-threaded general matrix multiplication (gemm), a key operation of\nthe BLAS, in order to obtain a high performance implementation for ARM\nbig.LITTLE AMPs. Our solution is based on the reference implementation of gemm\nin the BLIS library, and integrates a cache-aware configuration as well as\nasymmetric--static and dynamic scheduling strategies that carefully tune and\ndistribute the operation's micro-kernels among the big and LITTLE cores of the\ntarget processor. The experimental results on a Samsung Exynos 5422, a\nsystem-on-chip with ARM Cortex-A15 and Cortex-A7 clusters that implements the\nbig.LITTLE model, expose that our cache-aware versions of gemm with asymmetric\nscheduling attain important gains in performance with respect to its\narchitecture-oblivious counterparts while exploiting all the resources of the\nAMP to deliver considerable energy efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 08:35:15 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Catal\u00e1n", "Sandra", ""], ["Igual", "Francisco D.", ""], ["Mayo", "Rafael", ""], ["Rodr\u00edguez-S\u00e1nchez", "Rafael", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""]]}]