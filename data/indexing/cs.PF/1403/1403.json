[{"id": "1403.2486", "submitter": "Hiroshi Saito", "authors": "Hiroshi Saito and Ryoichi Kawahara", "title": "Theoretical Evaluation of Offloading through Wireless LANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offloading of cellular traffic through a wireless local area network (WLAN)\nis theoretically evaluated. First, empirical data sets of the locations of WLAN\ninternet access points are analyzed and an inhomogeneous Poisson process\nconsisting of high, normal, and low density regions is proposed as a spatial\npoint process model for these configurations. Second, performance metrics, such\nas mean available bandwidth for a user and the number of vertical handovers,\nare evaluated for the proposed model through geometric analysis. Explicit\nformulas are derived for the metrics, although they depend on many parameters\nsuch as the number of WLAN access points, the shape of each WLAN coverage\nregion, the location of each WLAN access point, the available bandwidth (bps)\nof the WLAN, and the shape and available bandwidth (bps) of each subregion\nidentified by the channel quality indicator in a cell of the cellular network.\nExplicit formulas strongly suggest that the bandwidth a user experiences does\nnot depend on the user mobility. This is because the bandwidth available by a\nuser who does not move and that available by a user who moves are the same or\napproximately the same as a probabilistic distribution. Numerical examples show\nthat parameters, such as the size of regions where placement of WLAN access\npoints is not allowed and the mean density of WLANs in high density regions,\nhave a large impact on performance metrics. In particular, a homogeneous\nPoisson process model as the WLAN access point location model largely\noverestimates the mean available bandwidth for a user and the number of\nvertical handovers. The overestimated mean available bandwidth is, for example,\nabout 50% in a certain condition.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 06:55:04 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Saito", "Hiroshi", ""], ["Kawahara", "Ryoichi", ""]]}, {"id": "1403.2800", "submitter": "Peng Qin", "authors": "Peng Qin and Bin Dai and Benxiong Huang and Guan Xu", "title": "Bandwidth-Aware Scheduling with SDN in Hadoop: A New Trend for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Software Defined Networking (SDN) is a revolutionary network architecture\nthat separates out network control functions from the underlying equipment and\nis an increasingly trend to help enterprises build more manageable data centers\nwhere big data processing emerges as an important part of applications. To\nconcurrently process large-scale data, MapReduce with an open source\nimplementation named Hadoop is proposed. In practical Hadoop systems one kind\nof issue that vitally impacts the overall performance is know as the\nNP-complete minimum make span problem. One main solution is to assign tasks on\ndata local nodes to avoid link occupation since network bandwidth is a scarce\nresource. Many methodologies for enhancing data locality are proposed such as\nthe HDS and state-of-the-art scheduler BAR. However, all of them either ignore\nallocating tasks in a global view or disregard available bandwidth as the basis\nfor scheduling. In this paper we propose a heuristic bandwidth-aware task\nscheduler BASS to combine Hadoop with SDN. It is not only able to guarantee\ndata locality in a global view but also can efficiently assign tasks in an\noptimized way. Both examples and experiments demonstrate that BASS has the best\nperformance in terms of job completion time. To our knowledge, BASS is the\nfirst to exploit talent of SDN for big data processing and we believe it points\nout a new trend for large-scale data processing.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 03:31:52 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Qin", "Peng", ""], ["Dai", "Bin", ""], ["Huang", "Benxiong", ""], ["Xu", "Guan", ""]]}, {"id": "1403.3480", "submitter": "Fan Liang", "authors": "Fan Liang, Chen Feng, Xiaoyi Lu, Zhiwei Xu", "title": "Performance Benefits of DataMPI: A Case Study with BigDataBench", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Hadoop and Spark are gaining prominence in Big Data processing and\nanalytics. Both of them are widely deployed on Internet companies. On the other\nhand, high-performance data analysis requirements are causing academical and\nindustrial communities to adopt state-of-the-art technologies in HPC to solve\nBig Data problems. Recently, we have proposed a key-value pair based\ncommunication library, DataMPI, which is extending MPI to support\nHadoop/Spark-like Big Data Computing jobs. In this paper, we use BigDataBench,\na Big Data benchmark suite, to do comprehensive studies on performance and\nresource utilization characterizations of Hadoop, Spark and DataMPI. From our\nexperiments, we observe that the job execution time of DataMPI has up to 55%\nand 39% speedups compared with those of Hadoop and Spark, respectively. Most of\nthe benefits come from the high-efficiency communication mechanisms in DataMPI.\nWe also notice that the resource (CPU, memory, disk and network I/O)\nutilizations of DataMPI are also more efficient than those of the other two\nframeworks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 03:06:34 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Liang", "Fan", ""], ["Feng", "Chen", ""], ["Lu", "Xiaoyi", ""], ["Xu", "Zhiwei", ""]]}, {"id": "1403.5479", "submitter": "Felipe Olmos", "authors": "Felipe Olmos, Bruno Kauffmann, Alain Simonian, Yannick Carlinet", "title": "Catalog Dynamics: Impact of Content Publishing and Perishing on the\n  Performance of a LRU Cache", "comments": "13 Pages, 9 figures. Full version of the article submitted to the ITC\n  2014 conference. Small corrections in the appendix from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet heavily relies on Content Distribution Networks and transparent\ncaches to cope with the ever-increasing traffic demand of users. Content,\nhowever, is essentially versatile: once published at a given time, its\npopularity vanishes over time. All requests for a given document are then\nconcentrated between the publishing time and an effective perishing time.\n  In this paper, we propose a new model for the arrival of content requests,\nwhich takes into account the dynamical nature of the content catalog. Based on\ntwo large traffic traces collected on the Orange network, we use the\nsemi-experimental method and determine invariants of the content request\nprocess. This allows us to define a simple mathematical model for content\nrequests; by extending the so-called \"Che approximation\", we then compute the\nperformance of a LRU cache fed with such a request process, expressed by its\nhit ratio. We numerically validate the good accuracy of our model by comparison\nto trace-based simulation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 14:44:55 GMT"}, {"version": "v2", "created": "Mon, 24 Mar 2014 14:55:51 GMT"}, {"version": "v3", "created": "Thu, 11 Sep 2014 09:46:17 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Olmos", "Felipe", ""], ["Kauffmann", "Bruno", ""], ["Simonian", "Alain", ""], ["Carlinet", "Yannick", ""]]}, {"id": "1403.5828", "submitter": "Peng Qin", "authors": "Peng Qin and Bin Dai and Benxiong Huang and Guan Xu and Kui Wu", "title": "A Survey on Network Tomography with Network Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The overhead of internal network monitoring motivates techniques of network\ntomography. Network coding (NC) presents a new opportunity for network\ntomography as NC introduces topology-dependent correlation that can be further\nexploited in topology estimation. Compared with traditional methods, network\ntomography with NC has many advantages such as the improvement of tomography\naccuracy and the reduction of complexity in choosing monitoring paths. In this\npaper we first introduce the problem of tomography with NC and then propose the\ntaxonomy criteria to classify various methods. We also present existing\nsolutions and future trend. We expect that our comprehensive review on network\ntomography with NC can serve as a good reference for researchers and\npractitioners working in the area.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 01:34:35 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Qin", "Peng", ""], ["Dai", "Bin", ""], ["Huang", "Benxiong", ""], ["Xu", "Guan", ""], ["Wu", "Kui", ""]]}, {"id": "1403.7209", "submitter": "Istv\\'an Z Reguly", "authors": "Istv\\'an Z. Reguly and Gihan R. Mudalige and Carlo Bertolli and\n  Michael B. Giles and Adam Betts and Paul H. J. Kelly and David Radford", "title": "Acceleration of a Full-scale Industrial CFD Application with OP2", "comments": "Submitted to ACM Transactions on Parallel Computing", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 27,\n  no. 5, pp. 1265-1278, May 1 2016. doi: 10.1109/TPDS.2015.2453972", "doi": "10.1109/TPDS.2015.2453972", "report-no": null, "categories": "cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hydra is a full-scale industrial CFD application used for the design of\nturbomachinery at Rolls Royce plc. It consists of over 300 parallel loops with\na code base exceeding 50K lines and is capable of performing complex\nsimulations over highly detailed unstructured mesh geometries. Unlike simpler\nstructured-mesh applications, which feature high speed-ups when accelerated by\nmodern processor architectures, such as multi-core and many-core processor\nsystems, Hydra presents major challenges in data organization and movement that\nneed to be overcome for continued high performance on emerging platforms. We\npresent research in achieving this goal through the OP2 domain-specific\nhigh-level framework. OP2 targets the domain of unstructured mesh problems and\nfollows the design of an active library using source-to-source translation and\ncompilation to generate multiple parallel implementations from a single\nhigh-level application source for execution on a range of back-end hardware\nplatforms. We chart the conversion of Hydra from its original hand-tuned\nproduction version to one that utilizes OP2, and map out the key difficulties\nencountered in the process. To our knowledge this research presents the first\napplication of such a high-level framework to a full scale production code.\nSpecifically we show (1) how different parallel implementations can be achieved\nwith an active library framework, even for a highly complicated industrial\napplication such as Hydra, and (2) how different optimizations targeting\ncontrasting parallel architectures can be applied to the whole application,\nseamlessly, reducing developer effort and increasing code longevity.\nPerformance results demonstrate that not only the same runtime performance as\nthat of the hand-tuned original production code could be achieved, but it can\nbe significantly improved on conventional processor systems. Additionally, we\nachieve further...\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 20:14:24 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Reguly", "Istv\u00e1n Z.", ""], ["Mudalige", "Gihan R.", ""], ["Bertolli", "Carlo", ""], ["Giles", "Michael B.", ""], ["Betts", "Adam", ""], ["Kelly", "Paul H. J.", ""], ["Radford", "David", ""]]}, {"id": "1403.8006", "submitter": "Ashkan Tousimojarad Mr", "authors": "Ashkan Tousimojarad and Wim Vanderbauwhede", "title": "Cache-aware Parallel Programming for Manycore Processors", "comments": "This work was presented at the international symposium on Highly-\n  Efficient Accelerators and Reconfigurable Technologies (HEART2013),\n  Edinburgh, Scotland, June 13-14, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapidly evolving technology, multicore and manycore processors have\nemerged as promising architectures to benefit from increasing transistor\nnumbers. The transition towards these parallel architectures makes today an\nexciting time to investigate challenges in parallel computing. The TILEPro64 is\na manycore accelerator, composed of 64 tiles interconnected via multiple 8x8\nmesh networks. It contains per-tile caches and supports cache-coherent shared\nmemory by default. In this paper we present a programming technique to take\nadvantages of distributed caching facilities in manycore processors. However,\nunlike other work in this area, our approach does not use architecture-specific\nlibraries. Instead, we provide the programmer with a novel technique on how to\nprogram future Non-Uniform Cache Architecture (NUCA) manycore systems, bearing\nin mind their caching organisation. We show that our localised programming\napproach can result in a significant improvement of the parallelisation\nefficiency (speed-up).\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 14:13:13 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Tousimojarad", "Ashkan", ""], ["Vanderbauwhede", "Wim", ""]]}, {"id": "1403.8020", "submitter": "Ashkan Tousimojarad Mr", "authors": "Ashkan Tousimojarad and Wim Vanderbauwhede", "title": "An Efficient Thread Mapping Strategy for Multiprogramming on Manycore\n  Processors", "comments": "ParCo Conference, Munich, Germany, 2013", "journal-ref": "Parallel Computing: Accelerating Computational Science and\n  Engineering (CSE) 25 (2014) 63-71", "doi": "10.3233/978-1-61499-381-0-63", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of multicore and manycore processors is set to change the\nparallel computing world. Applications are shifting towards increased\nparallelism in order to utilise these architectures efficiently. This leads to\na situation where every application creates its desirable number of threads,\nbased on its parallel nature and the system resources allowance. Task\nscheduling in such a multithreaded multiprogramming environment is a\nsignificant challenge. In task scheduling, not only the order of the execution,\nbut also the mapping of threads to the execution resources is of a great\nimportance. In this paper we state and discuss some fundamental rules based on\nresults obtained from selected applications of the BOTS benchmarks on the\n64-core TILEPro64 processor. We demonstrate how previously efficient mapping\npolicies such as those of the SMP Linux scheduler become inefficient when the\nnumber of threads and cores grows. We propose a novel, low-overhead technique,\na heuristic based on the amount of time spent by each CPU doing some useful\nwork, to fairly distribute the workloads amongst the cores in a\nmultiprogramming environment. Our novel approach could be implemented as a\npragma similar to those in the new task-based OpenMP versions, or can be\nincorporated as a distributed thread mapping mechanism in future manycore\nprogramming frameworks. We show that our thread mapping scheme can outperform\nthe native GNU/Linux thread scheduler in both single-programming and\nmultiprogramming environments.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 14:40:02 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Tousimojarad", "Ashkan", ""], ["Vanderbauwhede", "Wim", ""]]}]