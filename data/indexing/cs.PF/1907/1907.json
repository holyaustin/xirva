[{"id": "1907.00048", "submitter": "Georg Hager", "authors": "Johannes Hofmann, Christie L. Alappat, Georg Hager, Dietmar Fey,\n  Gerhard Wellein", "title": "Bridging the Architecture Gap: Abstracting Performance-Relevant\n  Properties of Modern Server Processors", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.14529/jsfi200204", "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a universal modeling approach for predicting single- and\nmulticore runtime of steady-state loops on server processors. To this end we\nstrictly differentiate between application and machine models: An application\nmodel comprises the loop code, problem sizes, and other runtime parameters,\nwhile a machine model is an abstraction of all performance-relevant properties\nof a CPU. We introduce a generic method for determining machine models and\npresent results for relevant server-processor architectures by Intel, AMD, IBM,\nand Marvell/Cavium. Considering this wide range of architectures, the set of\nfeatures required for adequate performance modeling is surprisingly small. To\nvalidate our approach, we compare performance predictions to empirical data for\nan OpenMP-parallel preconditioned CG algorithm, which includes compute- and\nmemory-bound kernels. Both single- and multicore analysis shows that the model\nexhibits average and maximum relative errors of 5% and 10%. Deviations from the\nmodel and insights gained are discussed in detail.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:04:32 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Hofmann", "Johannes", ""], ["Alappat", "Christie L.", ""], ["Hager", "Georg", ""], ["Fey", "Dietmar", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1907.00050", "submitter": "Youry Khmelevsky", "authors": "Bernd Amann, Youry Khmelevsky and Gaetan Hains", "title": "State-of-the-Art on Query & Transaction Processing Acceleration", "comments": "7 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast amount of processing power and memory bandwidth provided by modern\nGraphics Processing Units (GPUs) make them a platform for data-intensive\napplications. The database community identified GPUs as effective co-processors\nfor data processing. In the past years, there were many approaches to make use\nof GPUs at different levels of a database system. In this Internal Technical\nReport, based on the [1] and some other research papers, we identify possible\nresearch areas at LIP6 for GPU-accelerated database management systems. We\ndescribe some key properties, typical challenges of GPU-aware database\narchitectures, and identify major open challenges.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 01:46:35 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Amann", "Bernd", ""], ["Khmelevsky", "Youry", ""], ["Hains", "Gaetan", ""]]}, {"id": "1907.00194", "submitter": "Adam Lev-Libfeld", "authors": "Adam Lev-Libfeld, Alex Margolin, Amnon Barak", "title": "Open-MPI over MOSIX: paralleled computing in a clustered world", "comments": "Engineering School graduation paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent increased interest in Cloud computing emphasizes the need to find an\nadequate solution to the load-balancing problem in parallel computing --\nefficiently running several jobs concurrently on a cluster of shared computers\n(nodes). One approach to solve this problem is by preemptive process migration\n-- the transfer of running processes between nodes. A possible drawback of this\napproach is the increased overhead between heavily communicating processes.\nThis project presents a solution to this last problem by incorporating the\nprocess migration capability of MOSIX into Open-MPI and by reducing the\nresulting communication overhead. Specifically, we developed a module for\ndirect communication (DiCOM) between migrated Open-MPI processes, to overcome\nthe increased communication latency of TCP/IP between such processes. The\noutcome is reduced run-time by improved resource allocation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 12:14:04 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lev-Libfeld", "Adam", ""], ["Margolin", "Alex", ""], ["Barak", "Amnon", ""]]}, {"id": "1907.00463", "submitter": "Erick Schmidt", "authors": "Erick Schmidt, David Akopian", "title": "Exploiting Acceleration Features of LabVIEW platform for Real-Time GNSS\n  Software Receiver Optimization", "comments": null, "journal-ref": "Proceedings of the 30th International Technical Meeting of the\n  Satellite Division of The Institute of Navigation (ION GNSS+ 2017)", "doi": "10.33012/2017.15179", "report-no": null, "categories": "eess.SP cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the new generation of LabVIEW-based GPS receiver testbed\nthat is based on National Instruments' (NI) LabVIEW (LV) platform in\nconjunction to C/C++ dynamic link libraries (DLL) used inside the platform for\nperformance execution. This GPS receiver has been optimized for real-time\noperation and has been developed for fast prototyping and easiness on future\nadditions and implementations to the system. The receiver DLLs are divided into\nthree baseband modules: acquisition, tracking, and navigation. The openness of\nreceived baseband modules allows for extensive research topics such as signal\nquality improvement on GPS-denied areas, signal spoofing, and signal\ninterferences. The hardware used in the system was chosen with an effort to\nachieve portability and mobility in the SDR receiver. Several acceleration\nfactors that accomplish real-time operation and that are inherent to LabVIEW\nmechanisms, such as multithreading, parallelization and dedicated\nloop-structures, are discussed. The proposed SDR also exploits C/C++\noptimization techniques for single-instruction multiple-data (SIMD) capable\nprocessors in software correlators for real-time operation of GNSS tracking\nloops. It is demonstrated that LabVIEW-based solutions provide competitive\nreal-time solutions for fast prototyping of receiver algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 21:10:10 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Schmidt", "Erick", ""], ["Akopian", "David", ""]]}, {"id": "1907.00465", "submitter": "Erick Schmidt", "authors": "Erick Schmidt, David Akopian", "title": "Fast prototyping of an SDR WLAN 802.11b receiver for an indoor\n  positioning system", "comments": null, "journal-ref": "Proceedings of the 31st International Technical Meeting of the\n  Satellite Division of The Institute of Navigation (ION GNSS+ 2018)", "doi": "10.33012/2018.16045", "report-no": null, "categories": "eess.SP cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor positioning systems (IPS) are emerging technologies due to an\nincreasing popularity and demand in location based service (LBS). Because\ntraditional positioning systems such as GPS are limited to outdoor\napplications, many IPS have been proposed in literature. WLAN-based IPS are the\nmost promising due to its proven accuracy and infrastructure deployment.\nSeveral WLAN-based IPS have been proposed in the past, from which the best\nresults have been shown by so-called fingerprint-based systems. This paper\nproposes an indoor positioning system which extends traditional WLAN\nfingerprinting by using received signal strength (RSS) measurements along with\nchannel estimates as an effort to improve classification accuracy for scenarios\nwith a low number of Access Points (APs). The channel estimates aim to\ncharacterize complex indoor environments making it a unique signature for\nfingerprinting-based IPS and therefore improving pattern recognition in\nradio-maps. Since commercial WLAN cards offer limited measurement information,\nsoftware-defined radio (SDR) as an emerging trend for fast prototyping and\nresearch integration is chosen as the best cost-effective option to extract\nchannel estimates. Therefore, this paper first proposes an 802.11b WLAN SDR\nbeacon receiver capable of measuring RSS and channel estimates. The SDR is\ndesigned using LabVIEW (LV) environment and leverages several inherent platform\nacceleration features that achieve real-time capturing. The receiver achieves a\nfast-rate measurement capture of 9 packets per second per AP. The\nclassification of the propose IPS uses a support vector machine (SVM) for\noffline training and online navigation. Several tests are conducted in a\ncluttered indoor environment with a single AP in 802.11b legacy mode. Finally,\nnavigation accuracy results are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 21:15:04 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Schmidt", "Erick", ""], ["Akopian", "David", ""]]}, {"id": "1907.00468", "submitter": "Erick Schmidt", "authors": "Erick Schmidt, David Akopian", "title": "A Fast-rate WLAN Measurement Tool for Improved Miss-rate in Indoor\n  Navigation", "comments": null, "journal-ref": "Proceedings of the 31st International Technical Meeting of the\n  Satellite Division of The Institute of Navigation (ION GNSS+ 2018)", "doi": "10.33012/2018.16042", "report-no": null, "categories": "cs.NI cs.PF cs.SE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, location-based services (LBS) have steered attention to indoor\npositioning systems (IPS). WLAN-based IPSs relying on received signal strength\n(RSS) measurements such as fingerprinting are gaining popularity due to proven\nhigh accuracy of their results. Typically, sets of RSS measurements at selected\nlocations from several WLAN access points (APs) are used to calibrate the\nsystem. Retrieval of such measurements from WLAN cards are commonly at one-Hz\nrate. Such measurement collection is needed for offline radio-map surveying\nstage which aligns fingerprints to locations, and for online navigation stage,\nwhen collected measurements are associated with the radio-map for user\nnavigation. As WLAN network is not originally designed for positioning, an RSS\nmeasurement miss could have a high impact on the fingerprinting system.\nAdditionally, measurement fluctuations require laborious signal processing, and\nsurveying process can be very time consuming. This paper proposes a fast-rate\nmeasurement collection method that addresses previously mentioned problems by\nachieving a higher probability of RSS measurement collection during a given\none-second window. This translates to more data for statistical processing and\nfaster surveying. The fast-rate collection approach is analyzed against the\nconventional measurement rate in a proposed testing methodology that mimics\nreal-life scenarios related to IPS surveying and online navigation.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 21:29:48 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Schmidt", "Erick", ""], ["Akopian", "David", ""]]}, {"id": "1907.02064", "submitter": "Mark Hill", "authors": "Mark D. Hill and Vijay Janapa Reddi", "title": "Accelerator-level Parallelism", "comments": "6 pages, 3 figures, & 7 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future applications demand more performance, but technology advances have\nbeen faltering. A promising approach to further improve computer system\nperformance under energy constraints is to employ hardware accelerators.\nAlready today, mobile systems concurrently employ multiple accelerators in what\nwe call accelerator-level parallelism (ALP). To spread the benefits of ALP more\nbroadly, we charge computer scientists to develop the science needed to best\nachieve the performance and cost goals of ALP hardware and software.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 21:04:47 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:53:53 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 20:11:05 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 14:58:25 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hill", "Mark D.", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1907.02162", "submitter": "Tian Guo", "authors": "Samuel S. Ogden and Tian Guo", "title": "CloudCoaster: Transient-aware Bursty Datacenter Workload Scheduling", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's clusters often have to divide resources among a diverse set of jobs.\nThese jobs are heterogeneous both in execution time and in their rate of\narrival. Execution time heterogeneity has lead to the development of hybrid\nschedulers that can schedule both short and long jobs to ensure good task\nplacement. However, arrival rate heterogeneity, or burstiness, remains a\nproblem in existing schedulers. These hybrid schedulers manage resources on\nstatically provisioned cluster, which can quickly be overwhelmed by bursts in\nthe number of arriving jobs.\n  In this paper we propose CloudCoaster, a hybrid scheduler that dynamically\nresizes the cluster by leveraging cheap transient servers. CloudCoaster\nschedules jobs in an intelligent way that increases job performance while\nreducing overall resource cost. We evaluate the effectiveness of CloudCoaster\nthrough simulations on real-world traces and compare it against a state-of-art\nhybrid scheduler. CloudCoaster improves the average queueing delay time of\nshort jobs by 4.8X while maintaining long job performance. In addition,\nCloudCoaster reduces the short partition budget by over 29.5%.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 23:50:26 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Ogden", "Samuel S.", ""], ["Guo", "Tian", ""]]}, {"id": "1907.02805", "submitter": "Alexey Lastovetsky", "authors": "Arsalan Shahid, Muhammad Fahad, Ravi Reddy Manumachu, Alexey\n  Lastovetsky", "title": "Energy of Computing on Multicore CPUs: Predictive Models and Energy\n  Conservation Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy is now a first-class design constraint along with performance in all\ncomputing settings. Energy predictive modelling based on performance monitoring\ncounts (PMCs) is the leading method used for prediction of energy consumption\nduring an application execution. We use a model-theoretic approach to formulate\nthe assumed properties of existing models in a mathematical form. We extend the\nformalism by adding properties, heretofore unconsidered, that account for a\nlimited form of energy conservation law. The extended formalism defines our\ntheory of energy of computing. By applying the basic practical implications of\nthe theory, we improve the prediction accuracy of state-of-the-art energy\nmodels from 31% to 18%. We also demonstrate that use of state-of-the-art\nmeasurement tools for energy optimisation may lead to significant losses of\nenergy (ranging from 56% to 65% for applications used in experiments) since\nthey do not take into account the energy conservation properties.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 12:56:57 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Shahid", "Arsalan", ""], ["Fahad", "Muhammad", ""], ["Manumachu", "Ravi Reddy", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "1907.02818", "submitter": "Jan H\\\"uckelheim", "authors": "Jan H\\\"uckelheim, Navjot Kukreja, Sri Hari Krishna Narayanan, Fabio\n  Luporini, Gerard Gorman, Paul Hovland", "title": "Automatic Differentiation for Adjoint Stencil Loops", "comments": "ICPP 2019", "journal-ref": null, "doi": "10.1145/3337821.3337906", "report-no": null, "categories": "cs.DC cs.LG cs.PF cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil loops are a common motif in computations including convolutional\nneural networks, structured-mesh solvers for partial differential equations,\nand image processing. Stencil loops are easy to parallelise, and their fast\nexecution is aided by compilers, libraries, and domain-specific languages.\nReverse-mode automatic differentiation, also known as algorithmic\ndifferentiation, autodiff, adjoint differentiation, or back-propagation, is\nsometimes used to obtain gradients of programs that contain stencil loops.\nUnfortunately, conventional automatic differentiation results in a memory\naccess pattern that is not stencil-like and not easily parallelisable.\n  In this paper we present a novel combination of automatic differentiation and\nloop transformations that preserves the structure and memory access pattern of\nstencil loops, while computing fully consistent derivatives. The generated\nloops can be parallelised and optimised for performance in the same way and\nusing the same tools as the original computation. We have implemented this new\ntechnique in the Python tool PerforAD, which we release with this paper along\nwith test cases derived from seismic imaging and computational fluid dynamics\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 13:33:34 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["H\u00fcckelheim", "Jan", ""], ["Kukreja", "Navjot", ""], ["Narayanan", "Sri Hari Krishna", ""], ["Luporini", "Fabio", ""], ["Gorman", "Gerard", ""], ["Hovland", "Paul", ""]]}, {"id": "1907.02894", "submitter": "Putt Sakdhnagool", "authors": "Putt Sakdhnagool, Amit Sabne, Rudolf Eigenmann", "title": "RegDem: Increasing GPU Performance via Shared Memory Register Spilling", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPU utilization, measured as occupancy, is limited by the parallel threads'\ncombined usage of on-chip resources, such as registers and the\nprogrammer-managed shared memory. Higher resource demand means lower effective\nparallel thread count, and therefore lower program performance. Our\ninvestigation found that registers are often the occupancy limiters.\n  The de-facto nvcc compiler-based approach spills excessive registers to the\noff-chip memory, ignoring the shared memory and leaving the on-chip resources\nunderutilized. To mitigate the register demand, this paper presents a binary\ntranslation technique, called RegDem, that spills excessive registers to the\nunderutilized shared memory by transforming the GPU assembly code (SASS). Most\nGPU programs do not fully use shared memory, thus allowing RegDem to use it for\nregister spilling. The higher occupancy achieved by RegDem outweighs the\nslightly higher cost of accessing shared memory instead of placing data in\nregisters. The paper also presents a compile-time performance predictor that\nmodels instructions stalls to choose the best version from a set of program\nvariants. Cumulatively, these techniques outperform the nvcc compiler with a 9%\ngeometric mean, the highest observed being 18%.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:30:04 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Sakdhnagool", "Putt", ""], ["Sabne", "Amit", ""], ["Eigenmann", "Rudolf", ""]]}, {"id": "1907.03195", "submitter": "Jeremy Kepner", "authors": "Chansup Byun, Jeremy Kepner, William Arcand, David Bestor, William\n  Bergeron, Matthew Hubbell, Vijay Gadepally, Michael Houle, Michael Jones,\n  Anne Klein, Lauren Milechin, Peter Michaleas, Julie Mullen, Andrew Prout,\n  Antonio Rosa, Siddharth Samsi, Charles Yee, Albert Reuther", "title": "Optimizing Xeon Phi for Interactive Data Analysis", "comments": "6 pages, 5 figures, accepted in IEEE High Performance Extreme\n  Computing (HPEC) conference 2019", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916300", "report-no": null, "categories": "cs.PF cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Intel Xeon Phi manycore processor is designed to provide high performance\nmatrix computations of the type often performed in data analysis. Common data\nanalysis environments include Matlab, GNU Octave, Julia, Python, and R.\nAchieving optimal performance of matrix operations within data analysis\nenvironments requires tuning the Xeon Phi OpenMP settings, process pinning, and\nmemory modes. This paper describes matrix multiplication performance results\nfor Matlab and GNU Octave over a variety of combinations of process counts and\nOpenMP threads and Xeon Phi memory modes. These results indicate that using\nKMP_AFFINITY=granlarity=fine, taskset pinning, and all2all cache memory mode\nallows both Matlab and GNU Octave to achieve 66% of the practical peak\nperformance for process counts ranging from 1 to 64 and OpenMP threads ranging\nfrom 1 to 64. These settings have resulted in generally improved performance\nacross a range of applications and has enabled our Xeon Phi system to deliver\nsignificant results in a number of real-world applications.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 22:04:25 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Byun", "Chansup", ""], ["Kepner", "Jeremy", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Hubbell", "Matthew", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anne", ""], ["Milechin", "Lauren", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1907.03382", "submitter": "Atilim Gunes Baydin", "authors": "At{\\i}l{\\i}m G\\\"une\\c{s} Baydin, Lei Shao, Wahid Bhimji, Lukas\n  Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi,\n  Bradley Gram-Hansen, Gilles Louppe, Mingfei Ma, Xiaohui Zhao, Philip Torr,\n  Victor Lee, Kyle Cranmer, Prabhat, Frank Wood", "title": "Etalumis: Bringing Probabilistic Programming to Scientific Simulators at\n  Scale", "comments": "14 pages, 8 figures", "journal-ref": "Proceedings of the International Conference for High Performance\n  Computing, Networking, Storage, and Analysis (SC19), November 17--22, 2019", "doi": "10.1145/3295500.3356180", "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming languages (PPLs) are receiving widespread attention\nfor performing Bayesian inference in complex generative models. However,\napplications to science remain limited because of the impracticability of\nrewriting complex scientific simulators in a PPL, the computational cost of\ninference, and the lack of scalable implementations. To address these, we\npresent a novel PPL framework that couples directly to existing scientific\nsimulators through a cross-platform probabilistic execution protocol and\nprovides Markov chain Monte Carlo (MCMC) and deep-learning-based inference\ncompilation (IC) engines for tractable inference. To guide IC inference, we\nperform distributed training of a dynamic 3DCNN--LSTM architecture with a\nPyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori\nsupercomputer with a global minibatch size of 128k: achieving a performance of\n450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron\nCollider (LHC) use-case with the C++ Sherpa simulator and achieve the\nlargest-scale posterior inference in a Turing-complete PPL.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 02:03:36 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 13:26:21 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Shao", "Lei", ""], ["Bhimji", "Wahid", ""], ["Heinrich", "Lukas", ""], ["Meadows", "Lawrence", ""], ["Liu", "Jialin", ""], ["Munk", "Andreas", ""], ["Naderiparizi", "Saeid", ""], ["Gram-Hansen", "Bradley", ""], ["Louppe", "Gilles", ""], ["Ma", "Mingfei", ""], ["Zhao", "Xiaohui", ""], ["Torr", "Philip", ""], ["Lee", "Victor", ""], ["Cranmer", "Kyle", ""], ["Prabhat", "", ""], ["Wood", "Frank", ""]]}, {"id": "1907.03427", "submitter": "Clemens Kreutz", "authors": "Clemens Kreutz", "title": "Guidelines for benchmarking of optimization approaches for fitting\n  mathematical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Insufficient performance of optimization approaches for fitting of\nmathematical models is still a major bottleneck in systems biology. In this\nmanuscript, the reasons and methodological challenges are summarized as well as\ntheir impact in benchmark studies. Important aspects for increasing evidence of\noutcomes of benchmark analyses are discussed. Based on general guidelines for\nbenchmarking in computational biology, a collection of tailored guidelines is\npresented for performing informative and unbiased benchmarking of\noptimization-based fitting approaches. Comprehensive benchmark studies based on\nthese recommendations are urgently required for establishing of a robust and\nreliable methodology for the systems biology community.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 07:16:36 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Kreutz", "Clemens", ""]]}, {"id": "1907.03626", "submitter": "Wei Dai", "authors": "Wei Dai and Daniel Berleant", "title": "Benchmarking Contemporary Deep Learning Hardware and Frameworks:A Survey\n  of Qualitative Metrics", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": null, "doi": "10.1109/CogMI48466.2019.00029", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys benchmarking principles, machine learning devices\nincluding GPUs, FPGAs, and ASICs, and deep learning software frameworks. It\nalso reviews these technologies with respect to benchmarking from the\nperspectives of a 6-metric approach to frameworks and an 11-metric approach to\nhardware platforms. Because MLPerf is a benchmark organization working with\nindustry and academia, and offering deep learning benchmarks that evaluate\ntraining and inference on deep learning hardware devices, the survey also\nmentions MLPerf benchmark results, benchmark metrics, datasets, deep learning\nframeworks and algorithms. We summarize seven benchmarking principles,\ndifferential characteristics of mainstream AI devices, and qualitative\ncomparison of deep learning hardware and frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:09:06 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 19:17:33 GMT"}, {"version": "v3", "created": "Sat, 19 Oct 2019 18:30:38 GMT"}, {"version": "v4", "created": "Sat, 23 Nov 2019 21:29:27 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Dai", "Wei", ""], ["Berleant", "Daniel", ""]]}, {"id": "1907.03628", "submitter": "Gewu Bu", "authors": "Gewu Bu (SU), Wassim Hana (SU), Maria Potop-Butucaru (SU)", "title": "Metamorphic IOTA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IOTA opened recently a new line of research in distributed ledgers area by\ntargeting algorithms that ensure a high throughput for the transactions\ngenerated in IoT systems. Transactions are continuously appended to an acyclic\nstructure called tangle and each new transaction selects as parents two\nexisting transactions (called tips) that it approves. G-IOTA, a very recent\nimprovement of IOTA, targets to protect tips left behind offering hence a good\nconfidence level. However, this improvement had a cost: the use of an\nadditional tip selection mechanism which may be critical in IoT systems since\nit needs additional energy consumption. In this paper we propose a new\nmetamorphic algorithm for tip selection that offers the best guaranties of both\nIOTA and G-IOTA. Our contribution is two fold. First, we propose a\nparameterized algorithm, E-IOTA, for tip selection which targets to reduce the\nnumber of random walks executed in previous versions (IOTA and G-IOTA) while\nmaintaining the same security guaranties as IOTA and the same confidence level\nand fairness with respect to tips selection as G-IOTA. Then we propose a formal\nanalysis of the security guaranties offered by E-IOTA against various attacks\nmentioned in the original IOTA proposal (e.g. large weight attack, parasite\nchain attack and splitting attack). Interestingly, to the best of our knowledge\nthis is the first formal analysis of the security guaranties of IOTA and its\nderivatives.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 14:05:27 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Bu", "Gewu", "", "SU"], ["Hana", "Wassim", "", "SU"], ["Potop-Butucaru", "Maria", "", "SU"]]}, {"id": "1907.03842", "submitter": "Anastasia Zvezdakova", "authors": "Anastasia Zvezdakova, Dmitriy Kulikov, Denis Kondranin, Dmitriy\n  Vatolin", "title": "Barriers towards no-reference metrics application to compressed video\n  quality analysis: on the example of no-reference metric NIQE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyses the application of no-reference metric NIQE to the task\nof video-codec comparison. A number of issues in the metric behaviour on videos\nwas detected and described. The metric has outlying scores on black and\nsolid-coloured frames. The proposed averaging technique for metric quality\nscores helped to improve the results in some cases. Also, NIQE has low-quality\nscores for videos with detailed textures and higher scores for videos of lower\nbitrates due to the blurring of these textures after compression. Although NIQE\nshowed natural results for many tested videos, it is not universal and\ncurrently can not be used for video-codec comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:07:16 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 14:30:22 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zvezdakova", "Anastasia", ""], ["Kulikov", "Dmitriy", ""], ["Kondranin", "Denis", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "1907.04080", "submitter": "Alexey Lastovetsky", "authors": "Hamidreza Khaleghzadeh, Muhammad Fahad, Arsalan Shahid, Ravi Reddy\n  Manumachu, Alexey Lastovetsky", "title": "Bi-objective Optimisation of Data-parallel Applications on Heterogeneous\n  Platforms for Performance and Energy via Workload Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance and energy are the two most important objectives for optimisation\non modern parallel platforms. Latest research demonstrated the importance of\nworkload distribution as a decision variable in the bi-objective optimisation\nfor performance and energy on homogeneous multicore clusters. We show in this\nwork that bi-objective optimisation for performance and energy on heterogeneous\nprocessors results in a large number of Pareto-optimal optimal solutions\n(workload distributions) even in the simple case of linear performance and\nenergy profiles. We then study performance and energy profiles of real-life\ndata-parallel applications and find that their shapes are non-linear, complex\nand non-smooth. We, therefore, propose an efficient and exact global\noptimisation algorithm, which takes as an input most general discrete\nperformance and dynamic energy profiles of the heterogeneous processors and\nsolves the bi-objective optimisation problem. The algorithm is also used as a\nbuilding block to solve the bi-objective optimisation problem for performance\nand total energy. We also propose a novel methodology to build discrete dynamic\nenergy profiles of individual computing devices, which are input to the\nalgorithm. The methodology is based purely on system-level measurements and\naddresses the fundamental challenge of accurate component-level energy\nmodelling of a hybrid data-parallel application running on a heterogeneous\nplatform integrating CPUs and accelerators. We experimentally validate the\nproposed method using two data-parallel applications, matrix multiplication and\n2D fast Fourier transform (2D-FFT).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:15:30 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Khaleghzadeh", "Hamidreza", ""], ["Fahad", "Muhammad", ""], ["Shahid", "Arsalan", ""], ["Manumachu", "Ravi Reddy", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "1907.04217", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Lauren Milechin, Siddharth Samsi,\n  William Arcand, David Bestor, William Bergeron, Chansup Byun, Matthew\n  Hubbell, Michael Houle, Michael Jones, Anne Klein, Peter Michaleas, Julie\n  Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M", "comments": "6 pages; 6 figures; accepted to IEEE High Performance Extreme\n  Computing (HPEC) Conference 2019. arXiv admin note: text overlap with\n  arXiv:1807.05308, arXiv:1902.00846", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916508", "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dynamic Distributed Dimensional Data Model (D4M) library implements\nassociative arrays in a variety of languages (Python, Julia, and Matlab/Octave)\nand provides a lightweight in-memory database implementation of hypersparse\narrays that are ideal for analyzing many types of network data. D4M relies on\nassociative arrays which combine properties of spreadsheets, databases,\nmatrices, graphs, and networks, while providing rigorous mathematical\nguarantees, such as linearity. Streaming updates of D4M associative arrays put\nenormous pressure on the memory hierarchy. This work describes the design and\nperformance optimization of an implementation of hierarchical associative\narrays that reduces memory pressure and dramatically increases the update rate\ninto an associative array. The parameters of hierarchical associative arrays\nrely on controlling the number of entries in each level in the hierarchy before\nan update is cascaded. The parameters are easily tunable to achieve optimal\nperformance for a variety of applications. Hierarchical arrays achieve over\n40,000 updates per second in a single instance. Scaling to 34,000 instances of\nhierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud\nachieved a sustained update rate of 1,900,000,000 updates per second. This\ncapability allows the MIT SuperCloud to analyze extremely large streaming\nnetwork data sets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:55:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Milechin", "Lauren", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anne", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1907.04241", "submitter": "Yurong Chen", "authors": "Yurong Chen, Hongfa Xue, Tian Lan, Guru Venkataramani", "title": "CHOP: Bypassing Runtime Bounds Checking Through Convex Hull OPtimization", "comments": "14 pages, 9 figures, 6 tables;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Unsafe memory accesses in programs written using popular programming\nlanguages like C/C++ have been among the leading causes for software\nvulnerability. Prior memory safety checkers such as SoftBound enforce memory\nspatial safety by checking if every access to array elements are within the\ncorresponding array bounds. However, it often results in high execution time\noverhead due to the cost of executing the instructions associated with bounds\nchecking. To mitigate this problem, redundant bounds check elimination\ntechniques are needed. In this paper, we propose CHOP, a Convex Hull\nOPtimization based framework, for bypassing redundant memory bounds checking\nvia profile-guided inferences. In contrast to existing check elimination\ntechniques that are limited by static code analysis, our solution leverages a\nmodel-based inference to identify redundant bounds checking based on runtime\ndata from past program executions. For a given function, it rapidly derives and\nupdates a knowledge base containing sufficient conditions for identifying\nredundant array bounds checking. We evaluate CHOP on real-world applications\nand benchmark (such as SPEC) and the experimental results show that on average\n80.12% of dynamic bounds check instructions can be avoided, resulting in\nimproved performance up to 95.80% over SoftBound.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 15:09:03 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Chen", "Yurong", ""], ["Xue", "Hongfa", ""], ["Lan", "Tian", ""], ["Venkataramani", "Guru", ""]]}, {"id": "1907.04693", "submitter": "Donglin Wang", "authors": "Wang Donglin, Raja R.Sattiraju, Qiu Anjie, Sanket Partani and Hans D.\n  Schotten", "title": "Methodologies of Link-Level Simulator and System-Level Simulator for\n  C-V2X Communication", "comments": "7 pages, 8 figures, VTC 2019 fall. arXiv admin note: substantial text\n  overlap with arXiv:1904.07962", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the time of the development, standardization, and further improvement are\nvital to the modern cellular systems such as the next generation wireless\ncommunication (5G). Simulations are essential to test and optimize algorithms\nand procedures prior to their implementation process of the equipment\nmanufactures. In order to evaluate system performance at different levels,\naccurate simulations of simple setups, as well as simulations of more complex\nsystems via abstracted models are necessary. In this work, two new simulators\nfor the sidelink Cooperative-Vehicle-to-Everything (C-V2X) communication have\nbeen implemented and carried out on both the physical layer (Link-Level (LL))\nand network layer (System-Level (SL)). Detailed methodologies of the LL and SL\nsimulators for C-V2X communication have been illustrated. In the LL simulator,\nwe get the mapping curves of BLER and Signal-to-Noise-Ratio (SNR), which are\nused as a baseline for measuring the performance of the LL simulation. In\naddition, these mapping curves are used as the important Link-to-System (L2S)\ninterfaces. The SL simulator is utilized for measuring the performance of cell\nnetworking and simulating large networks comprising of multiple eNBs and UEs.\nFinally, the simulation results of both simulators for CV2X communication are\npresented, which shows that different objectives can be met by using LL or SL\nsimulations types.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 08:38:39 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Donglin", "Wang", ""], ["Sattiraju", "Raja R.", ""], ["Anjie", "Qiu", ""], ["Partani", "Sanket", ""], ["Schotten", "Hans D.", ""]]}, {"id": "1907.04824", "submitter": "Matteo Dell'Amico Ph.D.", "authors": "Matteo Dell'Amico", "title": "Scheduling With Inexact Job Sizes: The Merits of Shortest Processing\n  Time First", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that size-based scheduling policies, which take into account\njob size (i.e., the time it takes to run them), can perform very desirably in\nterms of both response time and fairness. Unfortunately, the requirement of\nknowing a priori the exact job size is a major obstacle which is frequently\ninsurmountable in practice. Often, it is possible to get a coarse estimation of\njob size, but unfortunately analytical results with inexact job sizes are\nchallenging to obtain, and simulation-based studies show that several\nsize-based algorithm are severely impacted by job estimation errors. For\nexample, Shortest Remaining Processing Time (SRPT), which yields optimal mean\nsojourn time when job sizes are known exactly, can drastically underperform\nwhen it is fed inexact job sizes.\n  Some algorithms have been proposed to better handle size estimation errors,\nbut they are somewhat complex and this makes their analysis challenging. We\nconsider Shortest Processing Time (SPT), a simplification of SRPT that skips\nthe update of \"remaining\" job size and results in a preemptive algorithm that\nsimply schedules the job with the shortest estimated processing time. When job\nsize is inexact, SPT performs comparably to the best known algorithms in the\npresence of errors, while being definitely simpler. In this work, SPT is\nevaluated through simulation, showing near-optimal performance in many cases,\nwith the hope that its simplicity can open the way to analytical evaluation\neven when inexact inputs are considered.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 16:52:10 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Dell'Amico", "Matteo", ""]]}, {"id": "1907.05013", "submitter": "Toshio Endo", "authors": "Yuki Ito, Haruki Imai, Tung Le Duc, Yasushi Negishi, Kiyokuni\n  Kawachiya, Ryo Matsumiya, Toshio Endo", "title": "Profiling based Out-of-core Hybrid Method for Large Neural Networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are widely used to accelerate deep learning with NNs (NNs). On the other\nhand, since GPU memory capacity is limited, it is difficult to implement\nefficient programs that compute large NNs on GPU. To compute NNs exceeding GPU\nmemory capacity, data-swapping method and recomputing method have been proposed\nin existing work. However, in these methods, performance overhead occurs due to\ndata movement or increase of computation. In order to reduce the overhead, it\nis important to consider characteristics of each layer such as sizes and cost\nfor recomputation. Based on this direction, we proposed Profiling based\nout-of-core Hybrid method (PoocH). PoocH determines target layers of swapping\nor recomputing based on runtime profiling. We implemented PoocH by extending a\ndeep learning framework, Chainer, and we evaluated its performance. With PoocH,\nwe successfully computed an NN requiring 50 GB memory on a single GPU with 16\nGB memory. Compared with in-core cases, performance degradation was 38 \\% on\nx86 machine and 28 \\% on POWER9 machine.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 06:31:38 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Ito", "Yuki", ""], ["Imai", "Haruki", ""], ["Duc", "Tung Le", ""], ["Negishi", "Yasushi", ""], ["Kawachiya", "Kiyokuni", ""], ["Matsumiya", "Ryo", ""], ["Endo", "Toshio", ""]]}, {"id": "1907.05201", "submitter": "Vanlin Sathya", "authors": "Yuva Kumar, Vanlin Sathya, Sreenath Ramanath", "title": "Enhancing Spectral Utilization by Maximizing the Reuse in LTE Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Need for increased spectral efficiency is key to improve the quality of\nexperience for next-generation wireless applications like online gaming, HD\nVideo, etc.,. In our work, we consider an LTE Device-to-device (D2D) network\nwhere LTE UEs have primary access to the spectrum and D2D pairs have secondary\naccess. To enhance spectral efficiency, BS can offload the traffic by\nactivating multiple D2D pairs within the serving cell. This ensures that the\nsame radio resource will be reused across the primary LTE UEs and different D2D\npairs. In this context, we propose to enable more D2D secondary users in the\nserving cell, by utilizing neighboring BS spectrum to fairly co-exist with\nneighboring LTE primary users. We model the system and show via extensive\nsimulations, that the above configuration guarantees good throughput for the\nD2D pairs in the serving cell while ensuring that the primary LTE throughput\ndemand is not compromised.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 02:23:24 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Kumar", "Yuva", ""], ["Sathya", "Vanlin", ""], ["Ramanath", "Sreenath", ""]]}, {"id": "1907.05560", "submitter": "Vahid Noormofidi", "authors": "Vahid Noormofidi", "title": "Simulating Nonlinear Neutrino Oscillations on Next-Generation Many-Core\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work an astrophysical simulation code, XFLAT, is developed to study\nneutrino oscillations in supernovae. XFLAT is designed to utilize multiple\nlevels of parallelism through MPI, OpenMP, and SIMD instructions\n(vectorization). It can run on both the CPU and the Xeon Phi co-processor, the\nlatter of which is based on the Intel Many Integrated Core Architecture (MIC).\nThe performance of XFLAT on configurations and scenarios has been analyzed. In\naddition, the impact of I/O and the multi-node configuration on the Xeon\nPhi-equipped heterogeneous supercomputers such as Stampede at the Texas\nAdvanced Computing Center (TACC) was investigated.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 03:21:54 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Noormofidi", "Vahid", ""]]}, {"id": "1907.05853", "submitter": "Issam Damaj", "authors": "Issam Damaj (American University of Kuwait)", "title": "A Unified Analysis Approach for Hardware and Software Implementations", "comments": "5 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1904.01000", "journal-ref": "The 59th IEEE International Midwest Symposium on Circuits and\n  Systems. Abu Dhabi. UAE. 16-19 October, (2016) 577-580", "doi": "10.1109/MWSCAS.2016.7870083", "report-no": null, "categories": "cs.DC cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart gadgets are being embedded almost in every aspect of our lives. From\nsmart cities to smart watches, modern industries are increasingly supporting\nthe Internet-of-Things (IoT). SysMART aims at making supermarkets smart,\nproductive, and with a touch of modern lifestyle. While similar implementations\nto improve the shopping experience exists, they tend mainly to replace the\nshopping activity at the store with online shopping. Although online shopping\nreduces time and effort, it deprives customers from enjoying the experience.\nSysMART relies on cutting-edge devices and technology to simplify and reduce\nthe time required during grocery shopping inside the supermarket. In addition,\nthe system monitors and maintains perishable products in good condition\nsuitable for human consumption. SysMART is built using state-of-the-art\ntechnologies that support rapid prototyping and precision data acquisition. The\nselected development environment is LabVIEW with its world-class interfacing\nlibraries. The paper comprises a detailed system description, development\nstrategy, interface design, software engineering, and a thorough analysis and\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 19:31:32 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Damaj", "Issam", "", "American University of Kuwait"]]}, {"id": "1907.06487", "submitter": "Georg Hager", "authors": "Christie L. Alappat, Georg Hager, Olaf Schenk, Jonas Thies, Achim\n  Basermann, Alan R. Bishop, Holger Fehske, Gerhard Wellein", "title": "A Recursive Algebraic Coloring Technique for Hardware-Efficient\n  Symmetric Sparse Matrix-Vector Multiplication", "comments": "40 pages, 23 figures", "journal-ref": null, "doi": "10.1145/3399732", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The symmetric sparse matrix-vector multiplication (SymmSpMV) is an important\nbuilding block for many numerical linear algebra kernel operations or graph\ntraversal applications. Parallelizing SymmSpMV on today's multicore platforms\nwith up to 100 cores is difficult due to the need to manage conflicting updates\non the result vector. Coloring approaches can be used to solve this problem\nwithout data duplication, but existing coloring algorithms do not take load\nbalancing and deep memory hierarchies into account, hampering scalability and\nfull-chip performance. In this work, we propose the recursive algebraic\ncoloring engine (RACE), a novel coloring algorithm and open-source library\nimplementation, which eliminates the shortcomings of previous coloring methods\nin terms of hardware efficiency and parallelization overhead. We describe the\nlevel construction, distance-k coloring, and load balancing steps in RACE, use\nit to parallelize SymmSpMV, and compare its performance on 31 sparse matrices\nwith other state-of-the-art coloring techniques and Intel MKL on two modern\nmulticore processors. RACE outperforms all other approaches substantially and\nbehaves in accordance with the Roofline model. Outliers are discussed and\nanalyzed in detail. While we focus on SymmSpMV in this paper, our algorithm and\nsoftware is applicable to any sparse matrix operation with data dependencies\nthat can be resolved by distance-k coloring.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 13:15:35 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Alappat", "Christie L.", ""], ["Hager", "Georg", ""], ["Schenk", "Olaf", ""], ["Thies", "Jonas", ""], ["Basermann", "Achim", ""], ["Bishop", "Alan R.", ""], ["Fehske", "Holger", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1907.08044", "submitter": "Yoney Ever", "authors": "Yonal Kirsal and Yoney Kirsal Ever", "title": "Approximate Solution Approach and Performability Evaluation of Large\n  Scale Beowulf Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beowulf clusters are very popular and deployed worldwide in support of\nscientific computing, because of the high computational power and performance.\nHowever, they also pose several challenges, and yet they need to provide high\navailability. The practical large-scale Beowulf clusters result in\nunpredictable, fault-tolerant, often detrimental outcomes. Successful\ndevelopment of high performance in storing and processing huge amounts of data\nin large-scale clusters necessitates accurate quality of service (QoS)\nevaluation. This leads to develop as well as design, analytical models to\nunderstand and predict of complex system behaviour in order to ensure\navailability of large-scale systems. Exact modelling of such clusters is not\nfeasible due to the nature of the large scale nodes and the diversity of user\nrequests. An analytical model for QoS of large-scale server farms and solution\napproaches are necessary. In this paper, analytical modelling of large-scale\nBeowulf clusters is considered together with availability issues. A generic and\nflexible approximate solution approach is developed to handle large number of\nnodes for performability evaluation. The proposed analytical model and the\napproximate solution approach provide flexibility to evaluate the QoS\nmeasurements for such systems. In order to show the efficacy and the accuracy\nof the proposed approach, the results obtained from the analytical model are\nvalidated with the results obtained from the discrete event simulations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 13:40:22 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Kirsal", "Yonal", ""], ["Ever", "Yoney Kirsal", ""]]}, {"id": "1907.08302", "submitter": "Guenter Hesse", "authors": "Guenter Hesse, Christoph Matthies, Kelvin Glass, Johannes Huegle,\n  Matthias Uflacker", "title": "Quantitative Impact Evaluation of an Abstraction Layer for Data Stream\n  Processing Systems", "comments": null, "journal-ref": "2019 International Conference on Distributed Computing Systems\n  (ICDCS), pp. 1381-1392", "doi": "10.1109/ICDCS.2019.00137", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the demand to process ever-growing data volumes, a variety of new data\nstream processing frameworks have been developed. Moving an implementation from\none such system to another, e.g., for performance reasons, requires adapting\nexisting applications to new interfaces. Apache Beam addresses these high\nsubstitution costs by providing an abstraction layer that enables executing\nprograms on any of the supported streaming frameworks. In this paper, we\npresent a novel benchmark architecture for comparing the performance impact of\nusing Apache Beam on three streaming frameworks: Apache Spark Streaming, Apache\nFlink, and Apache Apex. We find significant performance penalties when using\nApache Beam for application development in the surveyed systems. Overall, usage\nof Apache Beam for the examined streaming applications caused a high variance\nof query execution times with a slowdown of up to a factor of 58 compared to\nqueries developed without the abstraction layer. All developed benchmark\nartifacts are publicly available to ensure reproducible results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 21:51:29 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Hesse", "Guenter", ""], ["Matthies", "Christoph", ""], ["Glass", "Kelvin", ""], ["Huegle", "Johannes", ""], ["Uflacker", "Matthias", ""]]}, {"id": "1907.08492", "submitter": "Martin Kronbichler", "authors": "Martin Kronbichler, Katharina Kormann, Niklas Fehn, Peter Munch and\n  Julius Witte", "title": "A Hermite-like basis for faster matrix-free evaluation of interior\n  penalty discontinuous Galerkin operators", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a basis for improved throughput of matrix-free evaluation\nof discontinuous Galerkin symmetric interior penalty discretizations on\nhexahedral elements. The basis relies on ideas of Hermite polynomials. It is\nused in a fully discontinuous setting not for higher order continuity but to\nminimize the effective stencil width, namely to limit the neighbor access of an\nelement to one data point for the function value and one for the derivative.\nThe basis is extended to higher orders with nodal contributions derived from\nroots of Jacobi polynomials and extended to multiple dimensions with tensor\nproducts, which enable the use of sum factorization. The beneficial effect of\nthe reduced data access on modern processors is shown. Furthermore, the\nviability of the basis in the context of multigrid solvers is analyzed. While a\nplain point-Jacobi approach is less efficient than with the best nodal\npolynomials, a basis change via sum-factorization techniques enables the\ncombination of the fast matrix-vector products with effective multigrid\nconstituents. The basis change is essentially for free on modern hardware\nbecause these computations can be hidden behind the cost of the data access.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 12:52:49 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Kronbichler", "Martin", ""], ["Kormann", "Katharina", ""], ["Fehn", "Niklas", ""], ["Munch", "Peter", ""], ["Witte", "Julius", ""]]}, {"id": "1907.09049", "submitter": "Jayakrishnan Nair", "authors": "Rahul Vaze and Jayakrishnan Nair", "title": "Multiple Server SRPT with speed scaling is competitive", "comments": "To appear in IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can the popular shortest remaining processing time (SRPT) algorithm achieve a\nconstant competitive ratio on multiple servers when server speeds are\nadjustable (speed scaling) with respect to the flow time plus energy\nconsumption metric? This question has remained open for a while, where a\nnegative result in the absence of speed scaling is well known. The main result\nof this paper is to show that multi-server SRPT can be constant competitive,\nwith a competitive ratio that only depends on the power-usage function of the\nservers, but not on the number of jobs/servers or the job sizes (unlike when\nspeed scaling is not allowed). When all job sizes are unity, we show that\nround-robin routing is optimal and can achieve the same competitive ratio as\nthe best known algorithm for the single server problem. Finally, we show that a\nclass of greedy dispatch policies, including policies that route to the least\nloaded or the shortest queue, do not admit a constant competitive ratio. When\njob arrivals are stochastic, with Poisson arrivals and i.i.d. job sizes, we\nshow that random routing and a simple gated-static speed scaling algorithm\nachieves a constant competitive ratio.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 23:03:42 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 13:21:43 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Vaze", "Rahul", ""], ["Nair", "Jayakrishnan", ""]]}, {"id": "1907.10701", "submitter": "Yu Emma Wang", "authors": "Yu Emma Wang, Gu-Yeon Wei, David Brooks", "title": "Benchmarking TPU, GPU, and CPU Platforms for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Training deep learning models is compute-intensive and there is an\nindustry-wide trend towards hardware specialization to improve performance. To\nsystematically benchmark deep learning platforms, we introduce ParaDnn, a\nparameterized benchmark suite for deep learning that generates end-to-end\nmodels for fully connected (FC), convolutional (CNN), and recurrent (RNN)\nneural networks. Along with six real-world models, we benchmark Google's Cloud\nTPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep\ndive into TPU architecture, reveal its bottlenecks, and highlight valuable\nlessons learned for future specialized system design. We also provide a\nthorough comparison of the platforms and find that each has unique strengths\nfor some types of models. Finally, we quantify the rapid performance\nimprovements that specialized software stacks provide for the TPU and GPU\nplatforms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:18:28 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 20:27:59 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 21:30:47 GMT"}, {"version": "v4", "created": "Tue, 22 Oct 2019 06:07:55 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Wang", "Yu Emma", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""]]}, {"id": "1907.10792", "submitter": "Ziv Scully", "authors": "Ziv Scully, Mor Harchol-Balter, Alan Scheller-Wolf", "title": "Simple Near-Optimal Scheduling for the M/G/1", "comments": "POMACS, 2020 (SIGMETRICS 2020 issue)", "journal-ref": null, "doi": "10.1145/3379477", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of preemptively scheduling jobs to minimize mean\nresponse time of an M/G/1 queue. When we know each job's size, the shortest\nremaining processing time (SRPT) policy is optimal. Unfortunately, in many\nsettings we do not have access to each job's size. Instead, we know only the\njob size distribution. In this setting the Gittins policy is known to minimize\nmean response time, but its complex priority structure can be computationally\nintractable. A much simpler alternative to Gittins is the shortest expected\nremaining processing time (SERPT) policy. While SERPT is a natural extension of\nSRPT to unknown job sizes, it is unknown whether or not SERPT is close to\noptimal for mean response time.\n  We present a new variant of SERPT called monotonic SERPT (M-SERPT) which is\nas simple as SERPT but has provably near-optimal mean response time at all\nloads for any job size distribution. Specifically, we prove the mean response\ntime ratio between M-SERPT and Gittins is at most 3 for load $\\rho \\leq 8/9$\nand at most 5 for any load. This makes M-SERPT the only non-Gittins scheduling\npolicy known to have a constant-factor approximation ratio for mean response\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 01:48:54 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 19:50:50 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 17:04:55 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Scully", "Ziv", ""], ["Harchol-Balter", "Mor", ""], ["Scheller-Wolf", "Alan", ""]]}, {"id": "1907.11052", "submitter": "Seva Shneer", "authors": "Ken Duffy and Seva Shneer", "title": "MDS coding is better than replication for job completion times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-server system, how can one get better performance than random\nassignment of jobs to servers if queue-states cannot be queried by the\ndispatcher? A replication strategy has recently been proposed where $d$ copies\nof each arriving job are sent to servers chosen at random. The job's completion\ntime is the first time that the service of any of its copies is complete. On\ncompletion, redundant copies of the job are removed from other queues so as not\nto overburden the system.\n  For digital jobs, where the objects to be served can be algebraically\nmanipulated, and for servers whose output is a linear function of their input,\nhere we consider an alternate strategy: Maximum Distance Separable (MDS) codes.\nFor every batch of $n$ digital jobs that arrive, $n+m$ linear combinations are\ncreated over the reals or a large finite field, and each coded job is sent to a\nrandom server. The batch completion time is the first time that any $n$ of the\n$n+m$ coded jobs are served, as the evaluation of $n$ original jobs can be\nrecovered by Gaussian elimination. If redundant jobs can be removed from queues\non batch completion, we establish that in order to get the improved\nresponse-time performance of sending $d$ copies of each of $n$ jobs via the\nreplication strategy, with the MDS methodology it suffices to send $n+d$ jobs.\nThat is, while replication is multiplicative, MDS is linear.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:49:02 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 18:57:13 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Duffy", "Ken", ""], ["Shneer", "Seva", ""]]}, {"id": "1907.11603", "submitter": "Mehmet Aktas", "authors": "Mehmet Fatih Aktas and Emina Soljanin", "title": "Anonymity Mixes as (Partial) Assembly Queues: Modeling and Analysis", "comments": "IEEE Information Theory Workshop, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymity platforms route the traffic over a network of special routers that\nare known as mixes and implement various traffic disruption techniques to hide\nthe communicating users' identities. Batch mixes in particular anonymize\ncommunicating peers by allowing message exchange to take place only after a\nsufficient number of messages (a batch) accumulate, thus introducing delay. We\nintroduce a queueing model for batch mix and study its delay properties. Our\nanalysis shows that delay of a batch mix grows quickly as the batch size gets\nclose to the number of senders connected to the mix. We then propose a\nrandomized batch mixing strategy and show that it achieves much better delay\nscaling in terms of the batch size. However, randomization is shown to reduce\nthe anonymity preserving capabilities of the mix. We also observe that queueing\nmodels are particularly useful to study anonymity metrics that are more\npractically relevant such as the time-to-deanonymize metric.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 14:47:10 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Soljanin", "Emina", ""]]}, {"id": "1907.12014", "submitter": "Takahiro Hirofuchi", "authors": "Takahiro Hirofuchi and Ryousei Takano", "title": "The Preliminary Evaluation of a Hypervisor-based Virtualization\n  Mechanism for Intel Optane DC Persistent Memory Module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) technologies, being accessible in the same manner\nas DRAM, are considered indispensable for expanding main memory capacities.\nIntel Optane DCPMM is a long-awaited product that drastically increases main\nmemory capacities. However, a substantial performance gap exists between DRAM\nand DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and\n407% higher than those of DRAM, respectively. The read/write bandwidths were\n37% and 8% of those of DRAM. This performance gap in main memory presents a new\nchallenge to researchers; we need a new system software technology supporting\nemerging hybrid memory architecture. In this paper, we present RAMinate, a\nhypervisor-based virtualization mechanism for hybrid memory systems, and a key\ntechnology to address the performance gap in main memory systems. It provides\ngreat flexibility in memory management and maximizes the performance of virtual\nmachines (VMs) by dynamically optimizing memory mappings. Through experiments,\nwe confirmed that even though a VM has only 1% of DRAM in its RAM, the\nperformance degradation of the VM was drastically alleviated by memory mapping\noptimization. The elapsed time to finish the build of Linux Kernel in the VM\nwas 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495\nseconds). When the optimization mechanism was disabled, the elapsed time\nincreased to 624 seconds (i.e. 26% increase from the 100% DRAM case).\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 05:06:24 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Hirofuchi", "Takahiro", ""], ["Takano", "Ryousei", ""]]}, {"id": "1907.12627", "submitter": "Anjul Tyagi", "authors": "Anjul Tyagi, Zhen Cao, Tyler Estro, Erez Zadok, Klaus Mueller", "title": "ICE: An Interactive Configuration Explorer for High Dimensional\n  Categorical Parameter Spaces", "comments": "10 pages, Published by IEEE at VIS 2019 (Vancouver, BC, Canada)", "journal-ref": "2019 IEEE Conference on Visual Analytics Science and Technology\n  (VAST), Vancouver, BC, Canada, 2019, pp. 23-34", "doi": "10.1109/VAST47406.2019.8986923", "report-no": null, "categories": "cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many applications where users seek to explore the impact of the\nsettings of several categorical variables with respect to one dependent\nnumerical variable. For example, a computer systems analyst might want to study\nhow the type of file system or storage device affects system performance. A\nusual choice is the method of Parallel Sets designed to visualize multivariate\ncategorical variables. However, we found that the magnitude of the parameter\nimpacts on the numerical variable cannot be easily observed here. We also\nattempted a dimension reduction approach based on Multiple Correspondence\nAnalysis but found that the SVD-generated 2D layout resulted in a loss of\ninformation. We hence propose a novel approach, the Interactive Configuration\nExplorer (ICE), which directly addresses the need of analysts to learn how the\ndependent numerical variable is affected by the parameter settings given\nmultiple optimization objectives. No information is lost as ICE shows the\ncomplete distribution and statistics of the dependent variable in context with\neach categorical variable. Analysts can interactively filter the variables to\noptimize for certain goals such as achieving a system with maximum performance,\nlow variance, etc. Our system was developed in tight collaboration with a group\nof systems performance researchers and its final effectiveness was evaluated\nwith expert interviews, a comparative user study, and two case studies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 20:23:27 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 23:49:11 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tyagi", "Anjul", ""], ["Cao", "Zhen", ""], ["Estro", "Tyler", ""], ["Zadok", "Erez", ""], ["Mueller", "Klaus", ""]]}, {"id": "1907.12650", "submitter": "Andrew Daw", "authors": "Andrew Daw, Robert C. Hampshire, Jamol Pender", "title": "Beyond Safety Drivers: Staffing a Teleoperations System for Autonomous\n  Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driverless vehicles promise a host of societal benefits including\ndramatically improved safety, increased accessibility, greater productivity,\nand higher quality of life. As this new technology approaches widespread\ndeployment, both industry and government are making provisions for\nteleoperations systems, in which remote human agents provide assistance to\ndriverless vehicles. This assistance can involve real-time remote operation and\neven ahead-of-time input via human-in-the-loop artificial intelligence systems.\nIn this paper, we address the problem of staffing such a remote support center.\nOur analysis focuses on the tradeoffs between the total number of remote\nagents, the reliability of the remote support system, and the resulting safety\nof the driverless vehicles. By establishing a novel connection between queues\nwith large batch arrivals and storage processes, we determine the probability\nof the system exceeding its service capacity. This connection drives our\nstaffing methodology. We also develop a numerical method to compute the exact\nstaffing level needed to achieve various performance measures. This moment\ngenerating function based technique may be of independent interest, and our\noverall staffing analysis may be of use in other applications that combine\nhuman expertise and automated systems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 15:41:01 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 19:33:53 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Daw", "Andrew", ""], ["Hampshire", "Robert C.", ""], ["Pender", "Jamol", ""]]}, {"id": "1907.12666", "submitter": "Gopinath Chennupati", "authors": "Atanu Barai and Gopinath Chennupati and Nandakishore Santhi and\n  Abdel-Hameed A. Badawy and Stephan Eidenbenz", "title": "Modeling Shared Cache Performance of OpenMP Programs using Reuse\n  Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-19-27398", "categories": "cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance modeling of parallel applications on multicore computers remains\na challenge in computational co-design due to the complex design of multicore\nprocessors including private and shared memory hierarchies. We present a\nScalable Analytical Shared Memory Model to predict the performance of parallel\napplications that runs on a multicore computer and shares the same level of\ncache in the hierarchy. This model uses a computationally efficient,\nprobabilistic method to predict the reuse distance profiles, where reuse\ndistance is a hardware architecture-independent measure of the patterns of\nvirtual memory accesses. It relies on a stochastic, static basic block-level\nanalysis of reuse profiles measured from the memory traces of applications ran\nsequentially on small instances rather than using a multi-threaded trace. The\nresults indicate that the hit-rate predictions on the shared cache are\naccurate.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 21:52:06 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Barai", "Atanu", ""], ["Chennupati", "Gopinath", ""], ["Santhi", "Nandakishore", ""], ["Badawy", "Abdel-Hameed A.", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "1907.12931", "submitter": "Md Vasimuddin", "authors": "Vasimuddin Md, Sanchit Misra, Heng Li, and Srinivas Aluru", "title": "Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Innovations in Next-Generation Sequencing are enabling generation of DNA\nsequence data at ever faster rates and at very low cost. Large sequencing\ncenters typically employ hundreds of such systems. Such high-throughput and\nlow-cost generation of data underscores the need for commensurate acceleration\nin downstream computational analysis of the sequencing data. A fundamental step\nin downstream analysis is mapping of the reads to a long reference DNA\nsequence, such as a reference human genome. Sequence mapping is a\ncompute-intensive step that accounts for more than 30% of the overall time of\nthe GATK workflow. BWA-MEM is one of the most widely used tools for sequence\nmapping and has tens of thousands of users.\n  In this work, we focus on accelerating BWA-MEM through an efficient\narchitecture aware implementation, while maintaining identical output. The\nvolume of data requires distributed computing environment, usually deploying\nmulticore processors. Since the application can be easily parallelized for\ndistributed memory systems, we focus on performance improvements on a single\nsocket multicore processor. BWA-MEM run time is dominated by three kernels,\ncollectively responsible for more than 85% of the overall compute time. We\nimproved the performance of these kernels by 1) improving cache reuse, 2)\nsimplifying the algorithms, 3) replacing small fragmented memory allocations\nwith a few large contiguous ones, 4) software prefetching, and 5) SIMD\nutilization wherever applicable - and massive reorganization of the source code\nenabling these improvements.\n  As a result, we achieved nearly 2x, 183x, and 8x speedups on the three\nkernels, respectively, resulting in up to 3.5x and 2.4x speedups on end-to-end\ncompute time over the original BWA-MEM on single thread and single socket of\nIntel Xeon Skylake processor. To the best of our knowledge, this is the highest\nreported speedup over BWA-MEM.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 13:18:53 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Md", "Vasimuddin", ""], ["Misra", "Sanchit", ""], ["Li", "Heng", ""], ["Aluru", "Srinivas", ""]]}, {"id": "1907.13030", "submitter": "Mathieu Dugr\\'e", "authors": "Mathieu Dugr\\'e, Val\\'erie Hayot-Sasson, Tristan Glatard", "title": "A performance comparison of Dask and Apache Spark for data-intensive\n  neuroimaging pipelines", "comments": "10 pages, 15 figures, 1 tables. To appear in the proceeding of the\n  14th WORKS Workshop on Topics in Workflows in Support of Large-Scale Science,\n  17 November 2019, Denver, CO, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few years, neuroimaging has entered the Big Data era due to the\njoint increase in image resolution, data sharing, and study sizes. However, no\nparticular Big Data engines have emerged in this field, and several\nalternatives remain available. We compare two popular Big Data engines with\nPython APIs, Apache Spark and Dask, for their runtime performance in processing\nneuroimaging pipelines. Our evaluation uses two synthetic pipelines processing\nthe 81GB BigBrain image, and a real pipeline processing anatomical data from\nmore than 1,000 subjects. We benchmark these pipelines using various\ncombinations of task durations, data sizes, and numbers of workers, deployed on\nan 8-node (8 cores ea.) compute cluster in Compute Canada's Arbutus cloud. We\nevaluate PySpark's RDD API against Dask's Bag, Delayed and Futures. Results\nshow that despite slight differences between Spark and Dask, both engines\nperform comparably. However, Dask pipelines risk being limited by Python's GIL\ndepending on task type and cluster configuration. In all cases, the major\nlimiting factor was data transfer. While either engine is suitable for\nneuroimaging pipelines, more effort needs to be placed in reducing data\ntransfer time.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 15:42:32 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 19:38:32 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 20:24:36 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Dugr\u00e9", "Mathieu", ""], ["Hayot-Sasson", "Val\u00e9rie", ""], ["Glatard", "Tristan", ""]]}, {"id": "1907.13442", "submitter": "Richard Nies", "authors": "Richard Nies and Matthias Hoelzl", "title": "Testing performance with and without Block Low Rank Compression in MUMPS\n  and the new PaStiX 6.0 for JOREK nonlinear MHD simulations", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interface to the MUMPS solver was updated in the JOREK MHD code to\nsupport Block Low Rank (BLR) compression and an interface to the new PaStiX\nsolver version 6 has been implemented supporting BLR as well. First tests were\ncarried out with JOREK, which solves a large sparse matrix system iteratively\nin each time step. For the preconditioning, a direct solver is applied in the\ncode to sub-matrices, and at this point BLR was applied with the results being\nsummarized in this report. For a simple case with a linearly growing mode,\nresults with both solvers look promising with a considerable reduction of the\nmemory consumption by several ten percent was obtained. A direct increase in\nperformance was seen in particular configurations already.\n  The choice of the BLR accuracy parameter $\\epsilon$ proves to be critical in\nthis simple test and also in more realistic simulations, which were carried out\nonly with MUMPS due to the limited time available. The more realistic test\nshowed an increase in run time when using BLR, which was mitigated when using\nlarger values of $\\epsilon$. However, the GMRes iterative solver does not reach\nconvergence anymore when $\\epsilon$ is too large, since the preconditioner\nbecomes too inaccurate in that case. It is thus critical to use an $\\epsilon$\nas large as possible, while still reaching convergence. More tests regarding\nthis optimum will be necessary in the future. BLR can also lead to an indirect\nspeed-up in particular cases, when the simulation can be run on a smaller\nnumber of compute nodes due to the reduced memory consumption.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 12:16:19 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Nies", "Richard", ""], ["Hoelzl", "Matthias", ""]]}]