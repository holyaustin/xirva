[{"id": "1902.00603", "submitter": "Aniket Shivam", "authors": "Aniket Shivam, Neftali Watkinson, Alexandru Nicolau, David Padua,\n  Alexander V. Veidenbaum", "title": "Towards an Achievable Performance for the Loop Nests", "comments": "Accepted at the 31st International Workshop on Languages and\n  Compilers for Parallel Computing (LCPC 2018)", "journal-ref": null, "doi": "10.1007/978-3-030-34627-0_6", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous code optimization techniques, including loop nest optimizations,\nhave been developed over the last four decades. Loop optimization techniques\ntransform loop nests to improve the performance of the code on a target\narchitecture, including exposing parallelism. Finding and evaluating an\noptimal, semantic-preserving sequence of transformations is a complex problem.\nThe sequence is guided using heuristics and/or analytical models and there is\nno way of knowing how close it gets to optimal performance or if there is any\nheadroom for improvement. This paper makes two contributions. First, it uses a\ncomparative analysis of loop optimizations/transformations across multiple\ncompilers to determine how much headroom may exist for each compiler. And\nsecond, it presents an approach to characterize the loop nests based on their\nhardware performance counter values and a Machine Learning approach that\npredicts which compiler will generate the fastest code for a loop nest. The\nprediction is made for both auto-vectorized, serial compilation and for\nauto-parallelization. The results show that the headroom for state-of-the-art\ncompilers ranges from 1.10x to 1.42x for the serial code and from 1.30x to\n1.71x for the auto-parallelized code. These results are based on the Machine\nLearning predictions.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 01:00:05 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Shivam", "Aniket", ""], ["Watkinson", "Neftali", ""], ["Nicolau", "Alexandru", ""], ["Padua", "David", ""], ["Veidenbaum", "Alexander V.", ""]]}, {"id": "1902.01321", "submitter": "Xiaohu Wu", "authors": "Xiaohu Wu, Francesco De Pellegrini, Guanyu Gao and Giuliano Casale", "title": "A Framework for Allocating Server Time to Spot and On-demand Services in\n  Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing delivers value to users by facilitating their access to\ncomputing capacity in periods when their need arises. An approach is to provide\nboth on-demand and spot services on shared servers. The former allows users to\naccess servers on demand at a fixed price and users occupy different periods of\nservers. The latter allows users to bid for the remaining unoccupied periods\nvia dynamic pricing; however, without appropriate design, such periods may be\narbitrarily small since on-demand users arrive randomly. This is also the\ncurrent service model adopted by Amazon Elastic Cloud Compute. In this paper,\nwe provide the first integral framework for sharing the time of servers between\non-demand and spot services while optimally pricing spot instances. It\nguarantees that on-demand users can get served quickly while spot users can\nstably utilize servers for a properly long period once accepted, which is a key\nfeature to make both on-demand and spot services accessible. Simulation results\nshow that, by complementing the on-demand market with a spot market, a cloud\nprovider can improve revenue by up to 464.7%. The framework is designed under\nassumptions which are met in real environments. It is a new tool that cloud\noperators can use to quantify the advantage of a hybrid spot and on-demand\nservice, eventually making the case for operating such service model in their\nown infrastructures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 17:18:22 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 15:38:27 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wu", "Xiaohu", ""], ["De Pellegrini", "Francesco", ""], ["Gao", "Guanyu", ""], ["Casale", "Giuliano", ""]]}, {"id": "1902.01437", "submitter": "Junhao Li", "authors": "Junhao Li, Hang Zhang", "title": "Blaze: Simplified High Performance Cluster Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  MapReduce and its variants have significantly simplified and accelerated the\nprocess of developing parallel programs. However, most MapReduce\nimplementations focus on data-intensive tasks while many real-world tasks are\ncompute intensive and their data can fit distributedly into the memory. For\nthese tasks, the speed of MapReduce programs can be much slower than those\nhand-optimized ones. We present Blaze, a C++ library that makes it easy to\ndevelop high performance parallel programs for such compute intensive tasks. At\nthe core of Blaze is a highly-optimized in-memory MapReduce function, which has\nthree main improvements over conventional MapReduce implementations: eager\nreduction, fast serialization, and special treatment for a small fixed key\nrange. We also offer additional conveniences that make developing parallel\nprograms similar to developing serial programs. These improvements make Blaze\nan easy-to-use cluster computing library that approaches the speed of\nhand-optimized parallel code. We apply Blaze to some common data mining tasks,\nincluding word frequency count, PageRank, k-means, expectation maximization\n(Gaussian mixture model), and k-nearest neighbors. Blaze outperforms Apache\nSpark by more than 10 times on average for these tasks, and the speed of Blaze\nscales almost linearly with the number of nodes. In addition, Blaze uses only\nthe MapReduce function and 3 utility functions in its implementation while\nSpark uses almost 30 different parallel primitives in its official\nimplementation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 19:28:15 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 02:59:19 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Li", "Junhao", ""], ["Zhang", "Hang", ""]]}, {"id": "1902.01701", "submitter": "Lan N. Nguyen", "authors": "Lan N. Nguyen, My T. Thai", "title": "Network Resilience Assessment via QoS Degradation Metrics: An\n  Algorithmic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on network resilience to perturbation of edge weight.\nOther than connectivity, many network applications nowadays rely upon some\nmeasure of network distance between a pair of connected nodes. In these\nsystems, a metric related to network functionality is associated to each edge.\nA pair of nodes only being functional if the weighted, shortest-path distance\nbetween the pair is below a given threshold \\texttt{T}. Consequently, a natural\nquestion is on which degree the change of edge weights can damage the network\nfunctionality? With this motivation, we study a new problem, \\textit{Quality of\nService Degradation}: given a set of pairs, find a minimum budget to increase\nthe edge weights which ensures the distance between each pair exceeds\n$\\mathtt{T}$. We introduce four algorithms with theoretical performance\nguarantees for this problem. Each of them has its own strength in trade-off\nbetween effectiveness and running time, which are illustrated both in theory\nand comprehensive experimental evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:32:35 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Nguyen", "Lan N.", ""], ["Thai", "My T.", ""]]}, {"id": "1902.01961", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Owen Kaser, Nathan Kurz", "title": "Faster Remainder by Direct Computation: Applications to Compilers and\n  Software Libraries", "comments": null, "journal-ref": "Software: Practice and Experience 49 (6), 2019", "doi": "10.1002/spe.2689", "report-no": null, "categories": "cs.MS cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  On common processors, integer multiplication is many times faster than\ninteger division. Dividing a numerator n by a divisor d is mathematically\nequivalent to multiplication by the inverse of the divisor (n / d = n x 1/d).\nIf the divisor is known in advance---or if repeated integer divisions will be\nperformed with the same divisor---it can be beneficial to substitute a less\ncostly multiplication for an expensive division.\n  Currently, the remainder of the division by a constant is computed from the\nquotient by a multiplication and a subtraction. But if just the remainder is\ndesired and the quotient is unneeded, this may be suboptimal. We present a\ngenerally applicable algorithm to compute the remainder more directly.\nSpecifically, we use the fractional portion of the product of the numerator and\nthe inverse of the divisor. On this basis, we also present a new, simpler\ndivisibility algorithm to detect nonzero remainders.\n  We also derive new tight bounds on the precision required when representing\nthe inverse of the divisor. Furthermore, we present simple C implementations\nthat beat the optimized code produced by state-of-art C compilers on recent x64\nprocessors (e.g., Intel Skylake and AMD Ryzen), sometimes by more than 25%. On\nall tested platforms including 64-bit ARM and POWER8, our divisibility-test\nfunctions are faster than state-of-the-art Granlund-Montgomery\ndivisibility-test functions, sometimes by more than 50%.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 22:33:20 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 18:49:00 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 16:52:47 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Lemire", "Daniel", ""], ["Kaser", "Owen", ""], ["Kurz", "Nathan", ""]]}, {"id": "1902.02137", "submitter": "Jan-Pieter Dorsman", "authors": "U. Ayesta, T. Bodas, J. L. Dorsman, I. M. Verloop", "title": "A token-based central queue with order-independent service rates", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a token-based central queue with multiple customer types. Customers\nof each type arrive according to a Poisson process and have an associated set\nof compatible tokens. Customers may only receive service when they have claimed\na compatible token. If upon arrival, more than one compatible token is\navailable, an assignment rule determines which token will be claimed. The\nservice rate obtained by a customer is state-dependent, i.e., it depends on the\nset of claimed tokens and on the number of customers in the system. Our first\nmain result shows that, provided the assignment rule and the service rates\nsatisfy certain conditions, the steady-state distribution has a product form.\nWe show that our model subsumes known families of models that have product-form\nsteady-state distributions including the order-independent queue of Krzesinski\n(2011) and the model of Visschers et al. (2012). Our second main contribution\ninvolves the derivation of expressions for relevant performance measures such\nas the sojourn time and the number of customers present in the system. We apply\nour framework to relevant models, including an M/M/K queue with heterogeneous\nservice rates, the MSCCC queue, multi-server models with redundancy and\nmatching models. For some of these models, we present expressions for\nperformance measures that have not been derived before.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 12:29:32 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Ayesta", "U.", ""], ["Bodas", "T.", ""], ["Dorsman", "J. L.", ""], ["Verloop", "I. M.", ""]]}, {"id": "1902.02816", "submitter": "Ajay Jain", "authors": "Charith Mendis, Ajay Jain, Paras Jain and Saman Amarasinghe", "title": "Revec: Program Rejuvenation through Revectorization", "comments": "The first two authors contributed equally to this work. Published at\n  CC 2019", "journal-ref": "Compiler Construction (CC) 2019", "doi": "10.1145/3302516.3307357", "report-no": null, "categories": "cs.PL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern microprocessors are equipped with Single Instruction Multiple Data\n(SIMD) or vector instructions which expose data level parallelism at a fine\ngranularity. Programmers exploit this parallelism by using low-level vector\nintrinsics in their code. However, once programs are written using vector\nintrinsics of a specific instruction set, the code becomes non-portable. Modern\ncompilers are unable to analyze and retarget the code to newer vector\ninstruction sets. Hence, programmers have to manually rewrite the same code\nusing vector intrinsics of a newer generation to exploit higher data widths and\ncapabilities of new instruction sets. This process is tedious, error-prone and\nrequires maintaining multiple code bases. We propose Revec, a compiler\noptimization pass which revectorizes already vectorized code, by retargeting it\nto use vector instructions of newer generations. The transformation is\ntransparent, happening at the compiler intermediate representation level, and\nenables performance portability of hand-vectorized code.\n  Revec can achieve performance improvements in real-world performance critical\nkernels. In particular, Revec achieves geometric mean speedups of 1.160$\\times$\nand 1.430$\\times$ on fast integer unpacking kernels, and speedups of\n1.145$\\times$ and 1.195$\\times$ on hand-vectorized x265 media codec kernels\nwhen retargeting their SSE-series implementations to use AVX2 and AVX-512\nvector instructions respectively. We also extensively test Revec's impact on\n216 intrinsic-rich implementations of image processing and stencil kernels\nrelative to hand-retargeting.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 19:37:45 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Mendis", "Charith", ""], ["Jain", "Ajay", ""], ["Jain", "Paras", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1902.02837", "submitter": "Pablo Abad", "authors": "Adrian Colaso, Pablo Prieto, Jose-Angel Herrero, Pablo Abad, Valentin\n  Puente, Jose-Angel Gregorio", "title": "Accuracy vs. Computational Cost Tradeoff in Distributed Computer System\n  Simulation", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation is a fundamental research tool in the computer architecture field.\nThese kinds of tools enable the exploration and evaluation of architectural\nproposals capturing the most relevant aspects of the highly complex systems\nunder study. Many state-of-the-art simulation tools focus on single-system\nscenarios, but the scalability required by trending applications has shifted\ntowards distributed computing systems integrated via complex software stacks.\nWeb services with client-server architectures or distributed storage and\nprocessing of scale-out data analytics (Big Data) are among the main exponents.\nThe complete simulation of a distributed computer system is the appropriate\nmethodology to conduct accurate evaluations. Unfortunately, this methodology\ncould have a significant impact on the already large computational effort\nderived from detailed simulation. In this work, we conduct a set of experiments\nto evaluate this accuracy/cost tradeoff. We measure the error made if\nclient-server applications are evaluated in a single-node environment, as well\nas the overhead induced by the methodology and simulation tool employed for\nmulti-node simulations. We quantify this error for different micro-architecture\ncomponents, such as last-level cache and instruction/data TLB. Our findings\nshow that accuracy loss can lead to completely wrong conclusions about the\neffects of proposed hardware optimizations. Fortunately, our results also\ndemonstrate that the computational overhead of a multi-node simulation\nframework is affordable, suggesting multi-node simulation as the most\nappropriate methodology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 08:14:32 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Colaso", "Adrian", ""], ["Prieto", "Pablo", ""], ["Herrero", "Jose-Angel", ""], ["Abad", "Pablo", ""], ["Puente", "Valentin", ""], ["Gregorio", "Jose-Angel", ""]]}, {"id": "1902.03018", "submitter": "Ma\\\"el Guiraud", "authors": "Dominique Barth, Ma\\\"el Guiraud, Yann Strozecki", "title": "Deterministic contention management for low latency Cloud RAN over an\n  optical ring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The N-GREEN project has for goal to design a low cost optical ring technology\nwith good performances (throughput, latency...) without using expensive\nend-to-end connections. We study the compatibility of such a technology with\nthe development of the Cloud RAN, a latency critical application which is a\nmajor aspect of 5G deployment. We show that deterministically managing Cloud\nRAN traffic minimizes its latency while also improving the latency of the other\ntraffics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 11:08:31 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 22:14:58 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Barth", "Dominique", ""], ["Guiraud", "Ma\u00ebl", ""], ["Strozecki", "Yann", ""]]}, {"id": "1902.03387", "submitter": "Hamzeh Khazaei", "authors": "Hamzeh Khazaei, Nima Mahmoudi, Cornel Barna and Marin Litoiu", "title": "Performance Modeling of Microservice Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservice architecture has transformed the way developers are building and\ndeploying applications in the nowadays cloud computing centers. This new\napproach provides increased scalability, flexibility, manageability, and\nperformance while reducing the complexity of the whole software development\nlife cycle. The increase in cloud resource utilization also benefits\nmicroservice providers. Various microservice platforms have emerged to\nfacilitate the DevOps of containerized services by enabling continuous\nintegration and delivery. Microservice platforms deploy application containers\non virtual or physical machines provided by public/private cloud\ninfrastructures in a seamless manner. In this paper, we study and evaluate the\nprovisioning performance of microservice platforms by incorporating the details\nof all layers (i.e., both micro and macro layers) in the modelling process. To\nthis end, we first build a microservice platform on top of Amazon EC2 cloud and\nthen leverage it to develop a comprehensive performance model to perform\nwhat-if analysis and capacity planning for microservice platforms at scale. In\nother words, the proposed performance model provides a systematic approach to\nmeasure the elasticity of the microservice platform by analyzing the\nprovisioning performance at both the microservice platform and the back-end\nmacroservice infrastructures.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 07:48:34 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 19:36:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Khazaei", "Hamzeh", ""], ["Mahmoudi", "Nima", ""], ["Barna", "Cornel", ""], ["Litoiu", "Marin", ""]]}, {"id": "1902.04067", "submitter": "Vaneet Aggarwal", "authors": "Abubakr O. Al-Abbasi and Vaneet Aggarwal and Moo-Ryong Ra", "title": "Multi-tier Caching Analysis in CDN-based Over-the-top Video Streaming\n  Systems", "comments": "Accepted to IEEE/ACM TON, 2019. arXiv admin note: substantial text\n  overlap with arXiv:1807.01147", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet video traffic has been been rapidly increasing and is further\nexpected to increase with the emerging 5G applications such as higher\ndefinition videos, IoT and augmented/virtual reality applications. As end-users\nconsume video in massive amounts and in an increasing number of ways, the\ncontent distribution network (CDN) should be efficiently managed to improve the\nsystem efficiency. The streaming service can include multiple caching tiers, at\nthe distributed servers and the edge routers, and efficient content management\nat these locations affect the quality of experience (QoE) of the end users. In\nthis paper, we propose a model for video streaming systems, typically composed\nof a centralized origin server, several CDN sites, and edge-caches located\ncloser to the end user. We comprehensively consider different systems design\nfactors including the limited caching space at the CDN sites, allocation of CDN\nfor a video request, choice of different ports (or paths) from the CDN and the\ncentral storage, bandwidth allocation, the edge-cache capacity, and the caching\npolicy. We focus on minimizing a performance metric, stall duration tail\nprobability (SDTP), and present a novel and efficient algorithm accounting for\nthe multiple design flexibilities. The theoretical bounds with respect to the\nSDTP metric are also analyzed and presented. The implementation on a\nvirtualized cloud system managed by Openstack demonstrate that the proposed\nalgorithms can significantly improve the SDTP metric, compared to the baseline\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 01:23:00 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Al-Abbasi", "Abubakr O.", ""], ["Aggarwal", "Vaneet", ""], ["Ra", "Moo-Ryong", ""]]}, {"id": "1902.04118", "submitter": "Sean Sedwards", "authors": "Jaeyoung Lee, Aravind Balakrishnan, Ashish Gaurav, Krzysztof\n  Czarnecki, Sean Sedwards", "title": "WiseMove: A Framework for Safe Deep Reinforcement Learning for\n  Autonomous Driving", "comments": null, "journal-ref": "International Conference on Quantitative Evaluation of Systems\n  (QEST 2019)", "doi": "10.1007/978-3-030-30281-8_20", "report-no": null, "categories": "cs.LG cs.NE cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning can provide efficient solutions to the complex problems\nencountered in autonomous driving, but ensuring their safety remains a\nchallenge. A number of authors have attempted to address this issue, but there\nare few publicly-available tools to adequately explore the trade-offs between\nfunctionality, scalability, and safety.\n  We thus present WiseMove, a software framework to investigate safe deep\nreinforcement learning in the context of motion planning for autonomous\ndriving. WiseMove adopts a modular learning architecture that suits our current\nresearch questions and can be adapted to new technologies and new questions. We\npresent the details of WiseMove, demonstrate its use on a common traffic\nscenario, and describe how we use it in our ongoing safe learning research.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 19:59:23 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lee", "Jaeyoung", ""], ["Balakrishnan", "Aravind", ""], ["Gaurav", "Ashish", ""], ["Czarnecki", "Krzysztof", ""], ["Sedwards", "Sean", ""]]}, {"id": "1902.04566", "submitter": "Mehdi Salehi Heydar Abad", "authors": "Mehdi Salehi Heydar Abad, Ozgur Ercetin", "title": "Finite Horizon Throughput Maximization for a Wirelessly Powered Device\n  over a Time Varying Channel", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.01834", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider an energy harvesting device (EHD) served by an\naccess point with a single antenna that is used for both wireless power\ntransfer (WPT) and data transfer. The objective is to maximize the expected\nthroughput of the EHD over a finite horizon when the channel state information\nis only available causally. The EHD is energized by WPT for a certain duration,\nwhich is subject to optimization, and then, EHD transmits its information bits\nto the AP until the end of the time horizon by employing optimal dynamic power\nallocation. The joint optimization problem is modeled as a dynamic programming\nproblem. Based on the characteristic of the problem, we prove that a time\ndependent threshold type structure exists for the optimal WPT duration, and we\nobtain closed form solution to the dynamic power allocation in the uplink\nperiod.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 06:55:35 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Abad", "Mehdi Salehi Heydar", ""], ["Ercetin", "Ozgur", ""]]}, {"id": "1902.04567", "submitter": "Mehdi Salehi Heydar Abad", "authors": "Mehdi Salehi Heydar Abad, Deniz Gunduz, Ozgur Ercetin", "title": "Communication over a time correlated channel with an energy harvesting\n  transmitter", "comments": "arXiv admin note: substantial text overlap with arXiv:1703.10519", "journal-ref": "2017 International Symposium on Wireless Communication Systems\n  (ISWCS)", "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, communication over a time-correlated point-to-point wireless\nchannel is studied for an energy harvesting (EH) transmitter. In this model, we\ntake into account the time and energy cost of acquiring channel state\ninformation. At the beginning of the time slot, the EH transmitter, has to\nchoose among three possible actions: i) deferring the transmission to save its\nenergy for future use, ii) transmitting without sensing, and iii) sensing the\nchannel before transmission. At each time slot, the transmitter chooses one of\nthe three possible actions to maximize the total expected discounted number of\nbits transmitted over an infinite time horizon. This problem can be formulated\nas a partially observable Markov decision process (POMDP) which is then\nconverted to an ordinary MDP by introducing a belief on the channel state, and\nthe optimal policy is shown to exhibit a threshold behavior on the belief\nstate, with battery-dependent threshold values. Optimal threshold values and\ncorresponding optimal performance are characterized through numerical\nsimulations, and it is shown that having the sensing action and intelligently\nusing it to track the channel state improves the achievable long-term\nthroughput significantly.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 06:55:47 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Abad", "Mehdi Salehi Heydar", ""], ["Gunduz", "Deniz", ""], ["Ercetin", "Ozgur", ""]]}, {"id": "1902.04568", "submitter": "Mehdi Salehi Heydar Abad", "authors": "Mehdi Salehi Heydar Abad, Ozgur Ercetin, Tamer Elbatt, Mohammed Nafie", "title": "Wireless energy and information transfer in networks with hybrid ARQ", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.02878", "journal-ref": "2018 IEEE Wireless Communications and Networking Conference (WCNC)", "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a class of wireless powered communication devices\nusing hybrid automatic repeat request (HARQ) protocol to ensure reliable\ncommunications. In particular, we analyze the trade-off between accumulating\nmutual information and harvesting RF energy at the receiver of a point-to-point\nlink over a time-varying independent and identically distributed (i.i.d.)\nchannel. The transmitter is assumed to have a constant energy source while the\nreceiver relies, solely, on the RF energy harvested from the received signal.\nAt each time slot, the incoming RF signal is split between information\naccumulation and energy accumulation with the objective of minimizing the\nexpected number of re-transmissions. A major finding of this work is that the\noptimal policy minimizing the expected number of re-transmissions utilizes the\nincoming RF signal to either exclusively harvest energy or to accumulate mutual\ninformation. This finding enables achieving an optimal solution in feasible\ntime by converting a two dimensional uncountable state Markov decision process\n(MDP) with continuous action space into a countable state MDP with binary\ndecision space.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 06:55:53 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Abad", "Mehdi Salehi Heydar", ""], ["Ercetin", "Ozgur", ""], ["Elbatt", "Tamer", ""], ["Nafie", "Mohammed", ""]]}, {"id": "1902.04738", "submitter": "Emery Berger", "authors": "Bobby Powers, David Tench, Emery D. Berger, Andrew McGregor", "title": "Mesh: Compacting Memory Management for C/C++ Applications", "comments": "Draft version, accepted at PLDI 2019", "journal-ref": null, "doi": "10.1145/3314221.3314582", "report-no": null, "categories": "cs.PL cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs written in C/C++ can suffer from serious memory fragmentation,\nleading to low utilization of memory, degraded performance, and application\nfailure due to memory exhaustion. This paper introduces Mesh, a plug-in\nreplacement for malloc that, for the first time, eliminates fragmentation in\nunmodified C/C++ applications. Mesh combines novel randomized algorithms with\nwidely-supported virtual memory operations to provably reduce fragmentation,\nbreaking the classical Robson bounds with high probability. Mesh generally\nmatches the runtime performance of state-of-the-art memory allocators while\nreducing memory consumption; in particular, it reduces the memory of\nconsumption of Firefox by 16% and Redis by 39%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 04:40:05 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 19:03:03 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Powers", "Bobby", ""], ["Tench", "David", ""], ["Berger", "Emery D.", ""], ["McGregor", "Andrew", ""]]}, {"id": "1902.04890", "submitter": "Mehdi Salehi Heydar Abad", "authors": "Mehdi Salehi Heydar Abad, Deniz Gunduz, Ozgur Ercetin", "title": "Energy harvesting wireless networks with correlated energy sources", "comments": null, "journal-ref": "2016 IEEE Wireless Communications and Networking Conference", "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers a system with two energy harvesting (EH) nodes\ntransmitting to a common destination over a random access channel. The amount\nof harvested energy is assumed to be random and independent over time, but\ncorrelated among the nodes possibly with respect to their relative position. A\nthreshold-based transmission policy is developed for the maximization of the\nexpected aggregate network throughput. Assuming that there is no a priori\nchannel state or EH information available to the nodes, the aggregate network\nthroughput is obtained. The optimal thresholds are determined for two\npractically important special cases: i) at any time only one of the sensors\nharvests energy due to, for example, physical separation of the nodes; ii) the\nnodes are spatially close, and at any time, either both nodes or none of them\nharvests energy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 06:55:41 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Abad", "Mehdi Salehi Heydar", ""], ["Gunduz", "Deniz", ""], ["Ercetin", "Ozgur", ""]]}, {"id": "1902.05462", "submitter": "Pengfei Su", "authors": "Pengfei Su, Shasha Wen, Hailong Yang, Milind Chabbi and Xu Liu", "title": "Redundant Loads: A Software Inefficiency Indicator", "comments": "This paper is a full-version of our ICSE paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software packages have become increasingly complex with millions of\nlines of code and references to many external libraries. Redundant operations\nare a common performance limiter in these code bases. Missed compiler\noptimization opportunities, inappropriate data structure and algorithm choices,\nand developers' inattention to performance are some common reasons for the\nexistence of redundant operations. Developers mainly depend on compilers to\neliminate redundant operations. However, compilers' static analysis often\nmisses optimization opportunities due to ambiguities and limited analysis\nscope; automatic optimizations to algorithmic and data structural problems are\nout of scope.\n  We develop LoadSpy, a whole-program profiler to pinpoint redundant memory\nload operations, which are often a symptom of many redundant operations. The\nstrength of LoadSpy exists in identifying and quantifying redundant load\noperations in programs and associating the redundancies with program execution\ncontexts and scopes to focus developers' attention on problematic code. LoadSpy\nworks on fully optimized binaries, adopts various optimization techniques to\nreduce its overhead, and provides a rich graphic user interface, which make it\na complete developer tool. Applying LoadSpy showed that a large fraction of\nredundant loads is common in modern software packages despite highest levels of\nautomatic compiler optimizations. Guided by LoadSpy, we optimize several\nwell-known benchmarks and real-world applications, yielding significant\nspeedups.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 15:54:17 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Su", "Pengfei", ""], ["Wen", "Shasha", ""], ["Yang", "Hailong", ""], ["Chabbi", "Milind", ""], ["Liu", "Xu", ""]]}, {"id": "1902.05808", "submitter": "Louis-Claude Canon", "authors": "Louis-Claude Canon and Mohamad El Sayah and Pierre-Cyrille H\\'eam", "title": "A Comparison of Random Task Graph Generation Methods for Scheduling\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to generate instances with relevant properties and without bias remains\nan open problem of critical importance for a fair comparison of heuristics. In\nthe context of scheduling with precedence constraints, the instance consists of\na task graph that determines a partial order on task executions. To avoid\nselecting instances among a set populated mainly with trivial ones, we rely on\nproperties that quantify the characteristics specific to difficult instances.\nAmong numerous identified such properties, the mass measures how much a task\ngraph can be decomposed into smaller ones. This property, together with an\nin-depth analysis of existing random task graph generation methods, establishes\nthe sub-exponential generic time complexity of the studied problem. Empirical\nobservations on the impact of existing generation methods on scheduling\nheuristics concludes our study.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 13:43:31 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Canon", "Louis-Claude", ""], ["Sayah", "Mohamad El", ""], ["H\u00e9am", "Pierre-Cyrille", ""]]}, {"id": "1902.06893", "submitter": "Chen Yuan", "authors": "Chen Yuan, Yi Lu, Wei Feng, Guangyi Liu, Renchang Dai, Yachen Tang,\n  Zhiwei Wang", "title": "Graph Computing based Distributed Fast Decoupled Power Flow Analysis", "comments": "5 figures, 3 tables, 2019 IEEE Power and Energy Society General\n  Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power flow analysis plays a fundamental and critical role in the energy\nmanagement system (EMS). It is required to well accommodate large and complex\npower system. To achieve a high performance and accurate power flow analysis, a\ngraph computing based distributed power flow analysis approach is proposed in\nthis paper. Firstly, a power system network is divided into multiple areas.\nSlack buses are selected for each area and, at each SCADA sampling period, the\ninter-area transmission line power flows are equivalently allocated as extra\nload injections to corresponding buses. Then, the system network is converted\ninto multiple independent areas. In this way, the power flow analysis could be\nconducted in parallel for each area and the solved system states could be\nguaranteed without compromise of accuracy. Besides, for each area, graph\ncomputing based fast decoupled power flow (FDPF) is employed to quickly analyze\nsystem states. IEEE 118-bus system and MP 10790-bus system are employed to\nverify the results accuracy and present the promising computation performance\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 04:56:00 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Yuan", "Chen", ""], ["Lu", "Yi", ""], ["Feng", "Wei", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Tang", "Yachen", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1902.07095", "submitter": "Erick Schmidt", "authors": "Erick Schmidt, David Akopian, Daniel J. Pack", "title": "Development of a Real-Time Software-Defined Radio GPS Receiver\n  Exploiting a LabVIEW-based Instrumentation Environment", "comments": null, "journal-ref": "IEEE Trans. Instrum. Meas., vol. 67, no. 9, pp. 2082-2096, Sep.\n  2018", "doi": "10.1109/TIM.2018.2811446", "report-no": null, "categories": "eess.SP cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquitousness of location based services (LBS) has proven effective for\nmany applications such as commercial, military, and emergency responders.\nSoftware-defined radio (SDR) has emerged as an adequate framework for\ndevelopment and testing of global navigational satellite systems (GNSS) such as\nthe Global Position System (GPS). SDR receivers are constantly developing in\nterms of acceleration factors and accurate algorithms for precise user\nnavigation. However, many SDR options for GPS receivers currently lack\nreal-time operation or could be costly. This paper presents a LabVIEW (LV) and\nC/C++ based GPS L1 receiver platform with real-time capabilities. The system\nrelies on LV acceleration factors as well as other C/C++ techniques such as\ndynamic link library (DLL) integration into LV and parallelizable loop\nstructures, and single input multiple data (SIMD) methods which leverage host\nPC multi-purpose processors. A hardware testbed is presented for compactness\nand mobility, as well as software functionality and data flow handling inherent\nin LV environment. Benchmarks and other real-time results are presented as well\nas compared against other state-of-the-art open-source GPS receivers.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 23:38:07 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Schmidt", "Erick", ""], ["Akopian", "David", ""], ["Pack", "Daniel J.", ""]]}, {"id": "1902.07187", "submitter": "Anastasios Giovanidis", "authors": "Anastasios Giovanidis, Bruno Baynat, Antoine Vendeville", "title": "Performance Analysis of Online Social Platforms", "comments": "Preliminary version of accepted paper at INFOCOM 2019 (Paris, France)", "journal-ref": "INFOCOM 2019, Paris, France", "doi": "10.1109/INFOCOM.2019.8737539", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an original mathematical model to analyze the diffusion of posts\nwithin a generic online social platform. Each user of such a platform has his\nown Wall and Newsfeed, as well as his own self-posting and re-posting activity.\nAs a main result, using our developed model, we derive in closed form the\nprobabilities that posts originating from a given user are found on the Wall\nand Newsfeed of any other. These probabilities are the solution of a linear\nsystem of equations. Conditions of existence of the solution are provided, and\ntwo ways of solving the system are proposed, one using matrix inversion and\nanother using fixed-point iteration. Comparisons with simulations show the\naccuracy of our model and its robustness with respect to the modeling\nassumptions. Hence, this article introduces a novel measure which allows to\nrank users by their influence on the social platform, by taking into account\nnot only the social graph structure, but also the platform design, user\nactivity (self- and re-posting), as well as competition among posts.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 18:38:29 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Giovanidis", "Anastasios", ""], ["Baynat", "Bruno", ""], ["Vendeville", "Antoine", ""]]}, {"id": "1902.07590", "submitter": "Zhang Yang", "authors": "Zhang Yang, Aiqing Zhang, Zeyao Mo", "title": "JArena: Partitioned Shared Memory for NUMA-awareness in Multi-threaded\n  Scientific Applications", "comments": "12 pages, 3 figures, submitted to Euro-Par 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed shared memory (DSM) architecture is widely used in today's\ncomputer design to mitigate the ever-widening processing-memory gap, and\ninevitably exhibits non-uniform memory access (NUMA) to shared-memory parallel\napplications. Failure to achieve full NUMA-awareness can significantly\ndowngrade application performance, especially on today's manycore platforms\nwith tens to hundreds of cores. Yet traditional approaches such as first-touch\nand memory policy fail short in either false page-sharing, fragmentation, or\nease-of-use. In this paper, we propose a partitioned shared memory approach\nwhich allows multi-threaded applications to achieve full NUMA-awareness with\nonly minor code changes and develop a companying NUMA-aware heap manager which\neliminates false page-sharing and minimizes fragmentation. Experiments on a\n256-core cc-NUMA computing node show that the proposed approach achieves true\nNUMA-awareness and improves the performance of typical multi-threaded\nscientific applications up to 4.3 folds with the increased use of cores.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 15:07:54 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Yang", "Zhang", ""], ["Zhang", "Aiqing", ""], ["Mo", "Zeyao", ""]]}, {"id": "1902.07609", "submitter": "Saugata Ghose", "authors": "Saugata Ghose, Tianshi Li, Nastaran Hajinazar, Damla Senol Cali, Onur\n  Mutlu", "title": "Understanding the Interactions of Workloads and DRAM Types: A\n  Comprehensive Experimental Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become increasingly difficult to understand the complex interaction\nbetween modern applications and main memory, composed of DRAM chips.\nManufacturers are now selling and proposing many different types of DRAM, with\neach DRAM type catering to different needs (e.g., high throughput, low power,\nhigh memory density). At the same time, the memory access patterns of prevalent\nand emerging workloads are rapidly diverging, as these applications manipulate\nlarger data sets in very different ways. As a result, the combined\nDRAM-workload behavior is often difficult to intuitively determine today, which\ncan hinder memory optimizations in both hardware and software.\n  In this work, we identify important families of workloads, as well as\nprevalent types of DRAM chips, and rigorously analyze the combined\nDRAM--workload behavior. To this end, we perform a comprehensive experimental\nstudy of the interaction between nine different DRAM types and 115 modern\napplications and multiprogrammed workloads. We draw 12 key observations from\nour characterization, enabled in part by our development of new metrics that\ntake into account contention between memory requests due to hardware design.\nNotably, we find that (1) newer DRAM types such as DDR4 and HMC often do not\noutperform older types such as DDR3, due to higher access latencies and, in the\ncase of HMC, poor exploitation of locality; (2) there is no single DRAM type\nthat can cater to all components of a heterogeneous system (e.g., GDDR5\nsignificantly outperforms other memories for multimedia acceleration, while HMC\nsignificantly outperforms other memories for network acceleration); and (3)\nthere is still a strong need to lower DRAM latency, but unfortunately the\ncurrent design trend of commodity DRAM is toward higher latencies to obtain\nother benefits. We hope that the trends we identify can drive optimizations in\nboth hardware and software design.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 15:52:01 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 18:09:09 GMT"}, {"version": "v3", "created": "Sat, 6 Jul 2019 18:21:14 GMT"}, {"version": "v4", "created": "Mon, 29 Jul 2019 15:51:38 GMT"}, {"version": "v5", "created": "Fri, 18 Oct 2019 13:34:25 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ghose", "Saugata", ""], ["Li", "Tianshi", ""], ["Hajinazar", "Nastaran", ""], ["Cali", "Damla Senol", ""], ["Mutlu", "Onur", ""]]}, {"id": "1902.08318", "submitter": "Daniel Lemire", "authors": "Geoff Langdale, Daniel Lemire", "title": "Parsing Gigabytes of JSON per Second", "comments": "software: https://github.com/lemire/simdjson", "journal-ref": "The VLDB Journal, 28(6), 2019", "doi": "10.1007/s00778-019-00578-5", "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  JavaScript Object Notation or JSON is a ubiquitous data exchange format on\nthe Web. Ingesting JSON documents can become a performance bottleneck due to\nthe sheer volume of data. We are thus motivated to make JSON parsing as fast as\npossible.\n  Despite the maturity of the problem of JSON parsing, we show that substantial\nspeedups are possible. We present the first standard-compliant JSON parser to\nprocess gigabytes of data per second on a single core, using commodity\nprocessors. We can use a quarter or fewer instructions than a state-of-the-art\nreference parser like RapidJSON. Unlike other validating parsers, our software\n(simdjson) makes extensive use of Single Instruction, Multiple Data (SIMD)\ninstructions. To ensure reproducibility, simdjson is freely available as\nopen-source software under a liberal license.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 00:24:01 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 19:45:23 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 21:51:55 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 00:34:45 GMT"}, {"version": "v5", "created": "Mon, 30 Dec 2019 23:10:47 GMT"}, {"version": "v6", "created": "Thu, 2 Jan 2020 14:56:46 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Langdale", "Geoff", ""], ["Lemire", "Daniel", ""]]}, {"id": "1902.08755", "submitter": "Stefan Eilemann", "authors": "Stefan Eilemann", "title": "Parallel Rendering and Large Data Visualization", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are living in the big data age: An ever increasing amount of data is being\nproduced through data acquisition and computer simulations. While large scale\nanalysis and simulations have received significant attention for cloud and\nhigh-performance computing, software to efficiently visualise large data sets\nis struggling to keep up.\n  Visualization has proven to be an efficient tool for understanding data, in\nparticular visual analysis is a powerful tool to gain intuitive insight into\nthe spatial structure and relations of 3D data sets. Large-scale visualization\nsetups are becoming ever more affordable, and high-resolution tiled display\nwalls are in reach even for small institutions. Virtual reality has arrived in\nthe consumer space, making it accessible to a large audience.\n  This thesis addresses these developments by advancing the field of parallel\nrendering. We formalise the design of system software for large data\nvisualization through parallel rendering, provide a reference implementation of\na parallel rendering framework, introduce novel algorithms to accelerate the\nrendering of large amounts of data, and validate this research and development\nwith new applications for large data visualization. Applications built using\nour framework enable domain scientists and large data engineers to better\nextract meaning from their data, making it feasible to explore more data and\nenabling the use of high-fidelity visualization installations to see more\ndetail of the data.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 08:44:59 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Eilemann", "Stefan", ""]]}, {"id": "1902.09046", "submitter": "David Warne", "authors": "David J. Warne (1), Scott A. Sisson (2), Christopher Drovandi (1) ((1)\n  Queensland University of Technology, (2) University of New South Wales)", "title": "Vector operations for accelerating expensive Bayesian computations -- a\n  tutorial guide", "comments": null, "journal-ref": null, "doi": "10.1214/21-BA1265", "report-no": null, "categories": "stat.CO cs.DC cs.MS cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in Bayesian statistics are extremely computationally\nintensive. However, they are often inherently parallel, making them prime\ntargets for modern massively parallel processors. Multi-core and distributed\ncomputing is widely applied in the Bayesian community, however, very little\nattention has been given to fine-grain parallelisation using single instruction\nmultiple data (SIMD) operations that are available on most modern commodity\nCPUs and is the basis of GPGPU computing. In this work, we practically\ndemonstrate, using standard programming libraries, the utility of the SIMD\napproach for several topical Bayesian applications. We show that SIMD can\nimprove the floating point arithmetic performance resulting in up to $6\\times$\nimprovement in serial algorithm performance. Importantly, these improvements\nare multiplicative to any gains achieved through multi-core processing. We\nillustrate the potential of SIMD for accelerating Bayesian computations and\nprovide the reader with techniques for exploiting modern massively parallel\nprocessing environments using standard tools.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 00:38:23 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 08:31:45 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 06:22:53 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Warne", "David J.", ""], ["Sisson", "Scott A.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1902.10345", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Johannes de Fine Licht, Alexandros Nikolaos Ziogas, Timo\n  Schneider, Torsten Hoefler", "title": "Stateful Dataflow Multigraphs: A Data-Centric Model for Performance\n  Portability on Heterogeneous Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of accelerators in high-performance computing has driven\nprogramming complexity beyond the skill-set of the average domain scientist. To\nmaintain performance portability in the future, it is imperative to decouple\narchitecture-specific programming paradigms from the underlying scientific\ncomputations. We present the Stateful DataFlow multiGraph (SDFG), a\ndata-centric intermediate representation that enables separating program\ndefinition from its optimization. By combining fine-grained data dependencies\nwith high-level control-flow, SDFGs are both expressive and amenable to program\ntransformations, such as tiling and double-buffering. These transformations are\napplied to the SDFG in an interactive process, using extensible pattern\nmatching, graph rewriting, and a graphical user interface. We demonstrate SDFGs\non CPUs, GPUs, and FPGAs over various motifs --- from fundamental computational\nkernels to graph analytics. We show that SDFGs deliver competitive performance,\nallowing domain scientists to develop applications naturally and port them to\napproach peak hardware performance without modifying the original scientific\ncode.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 06:12:16 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 17:55:12 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 21:43:32 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Licht", "Johannes de Fine", ""], ["Ziogas", "Alexandros Nikolaos", ""], ["Schneider", "Timo", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1902.11028", "submitter": "Martin Becker", "authors": "Martin Becker and Samarjit Chakraborty", "title": "A Valgrind Tool to Compute the Working Set of a Software Process", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new open-source tool for the dynamic analyzer\nValgrind. The tool measures the amount of memory that is actively being used by\na process at any given point in time. While there exist numerous tools to\nmeasure the memory requirements of a process, the vast majority only focuses on\nmetrics like resident or proportional set sizes, which include memory that was\nonce claimed, but is momentarily disused. Consequently, such tools do not\npermit drawing conclusions about how much cache or RAM a process actually\nrequires at each point in time, and thus cannot be used for performance\ndebugging. The few tools which do measure only actively used memory, however,\nhave limitations in temporal resolution and introspection. In contrast, our\ntool offers an easy way to compute the memory that has recently been accessed\nat any point in time, reflecting how cache and RAM requirements change over\ntime. In particular, this tool computes the set of memory references made\nwithin a fixed time interval before any point in time, known as the working\nset, and captures call stacks for interesting peaks in the working set size. We\nfirst introduce the tool, then we run some examples comparing the output from\nour tool with similar memory tools, and we close with a discussion of\nlimitations\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 11:48:30 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Becker", "Martin", ""], ["Chakraborty", "Samarjit", ""]]}, {"id": "1902.11119", "submitter": "Behnam Dezfouli", "authors": "Salma Abdel Magid, Francesco Petrini, and Behnam Dezfouli", "title": "Image Classification on IoT Edge Devices: Profiling and Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": "SIOTLAB-TECHREP-OCT2018", "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of powerful, low-cost IoT systems, processing data closer to\nwhere the data originates, known as edge computing, has become an increasingly\nviable option. In addition to lowering the cost of networking infrastructures,\nedge computing reduces edge-cloud delay, which is essential for\nmission-critical applications. In this paper, we show the feasibility and study\nthe performance of image classification using IoT devices. Specifically, we\nexplore the relationships between various factors of image classification\nalgorithms that may affect energy consumption such as dataset size, image\nresolution, algorithm type, algorithm phase, and device hardware. Our\nexperiments show a strong, positive linear relationship between three predictor\nvariables, namely model complexity, image resolution, and dataset size, with\nrespect to energy consumption. In addition, in order to provide a means of\npredicting the energy consumption of an edge device performing image\nclassification, we investigate the usage of three machine learning algorithms\nusing the data generated from our experiments. The performance as well as the\ntrade offs for using linear regression, Gaussian process, and random forests\nare discussed and validated. Our results indicate that the random forest model\noutperforms the two former algorithms, with an R-squared value of 0.95 and 0.79\nfor two different validation datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 06:41:29 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 05:28:16 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Magid", "Salma Abdel", ""], ["Petrini", "Francesco", ""], ["Dezfouli", "Behnam", ""]]}]