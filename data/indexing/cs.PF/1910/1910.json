[{"id": "1910.00178", "submitter": "Wenlei Bao", "authors": "Wenlei Bao, Li-Wen Chang, Yang Chen, Ke Deng, Amit Agarwal, Emad\n  Barsoum, Abe Taha", "title": "NGEMM: Optimizing GEMM for Deep Learning via Compiler-based Techniques", "comments": "Comments: performance figure updated, experiments added, formula\n  corrected, references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization has emerged to be an effective way to significantly boost the\nperformance of deep neural networks (DNNs) by utilizing low-bit computations.\nDespite having lower numerical precision, quantized DNNs are able to reduce\nboth memory bandwidth and computation cycles with little losses of accuracy.\nInteger GEMM (General Matrix Multiplication) is critical to running quantized\nDNN models efficiently, as GEMM operations often dominate the computations in\nthese models. Various approaches have been developed by leveraging techniques\nsuch as vectorization and memory layout to improve the performance of integer\nGEMM. However, these existing approaches are not fast enough in certain\nscenarios. We developed NGEMM, a compiler-based GEMM implementation for\naccelerating lower-precision training and inference. NGEMM has better use of\nthe vector units by avoiding unnecessary vector computation that is introduced\nduring tree reduction. We compared NGEMM's performance with the state-of-art\nBLAS libraries such as MKL. Our experimental results showed that NGEMM\noutperformed MKL non-pack and pack version by an average of 1.86x and 1.16x,\nrespectively. We have applied NGEMM to a number of production services in\nMicrosoft.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 02:27:17 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 23:12:33 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Bao", "Wenlei", ""], ["Chang", "Li-Wen", ""], ["Chen", "Yang", ""], ["Deng", "Ke", ""], ["Agarwal", "Amit", ""], ["Barsoum", "Emad", ""], ["Taha", "Abe", ""]]}, {"id": "1910.00214", "submitter": "Georg Hager", "authors": "Jan Laukemann, Julian Hammer, Georg Hager, Gerhard Wellein", "title": "Automatic Throughput and Critical Path Analysis of x86 and ARM Assembly\n  Kernels", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": "10.1109/PMBS49563.2019.00006", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Useful models of loop kernel runtimes on out-of-order architectures require\nan analysis of the in-core performance behavior of instructions and their\ndependencies. While an instruction throughput prediction sets a lower bound to\nthe kernel runtime, the critical path defines an upper bound. Such predictions\nare an essential part of analytic (i.e., white-box) performance models like the\nRoofline and Execution-Cache-Memory (ECM) models. They enable a better\nunderstanding of the performance-relevant interactions between hardware\narchitecture and loop code. The Open Source Architecture Code Analyzer (OSACA)\nis a static analysis tool for predicting the execution time of sequential\nloops. It previously supported only x86 (Intel and AMD) architectures and\nsimple, optimistic full-throughput execution. We have heavily extended OSACA to\nsupport ARM instructions and critical path prediction including the detection\nof loop-carried dependencies, which turns it into a versatile\ncross-architecture modeling tool. We show runtime predictions for code on Intel\nCascade Lake, AMD Zen, and Marvell ThunderX2 micro-architectures based on\nmachine models from available documentation and semi-automatic benchmarking.\nThe predictions are compared with actual measurements.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 06:18:27 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 13:39:14 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Laukemann", "Jan", ""], ["Hammer", "Julian", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1910.00651", "submitter": "Sarabjeet Singh", "authors": "Sarabjeet Singh and Manu Awasthi", "title": "Memory Centric Characterization and Analysis of SPEC CPU2017 Suite", "comments": "12 pages, 133 figures, A short version of this work has been\n  published at \"Proceedings of the 2019 ACM/SPEC International Conference on\n  Performance Engineering\"", "journal-ref": null, "doi": "10.1145/3297663.3310311", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a comprehensive, memory-centric characterization of\nthe SPEC CPU2017 benchmark suite, using a number of mechanisms including\ndynamic binary instrumentation, measurements on native hardware using hardware\nperformance counters and OS based tools.\n  We present a number of results including working set sizes, memory capacity\nconsumption and, memory bandwidth utilization of various workloads. Our\nexperiments reveal that the SPEC CPU2017 workloads are surprisingly memory\nintensive, with approximately 50% of all dynamic instructions being memory\nintensive ones. We also show that there is a large variation in the memory\nfootprint and bandwidth utilization profiles of the entire suite, with some\nbenchmarks using as much as 16 GB of main memory and up to 2.3 GB/s of memory\nbandwidth.\n  We also perform instruction execution and distribution analysis of the suite\nand find that the average instruction count for SPEC CPU2017 workloads is an\norder of magnitude higher than SPEC CPU2006 ones. In addition, we also find\nthat FP benchmarks of the SPEC 2017 suite have higher compute requirements: on\naverage, FP workloads execute three times the number of compute operations as\ncompared to INT workloads.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 07:28:47 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Singh", "Sarabjeet", ""], ["Awasthi", "Manu", ""]]}, {"id": "1910.01017", "submitter": "Nikolay A. Simakov", "authors": "Nikolay A. Simakov, Renette L. Jones-Ivey, Ali Akhavan-Safaei, Hossein\n  Aghakhani, Matthew D. Jones and Abani K. Patra", "title": "Modernizing Titan2D, a Parallel AMR Geophysical Flow Code to Support\n  Multiple Rheologies and Extendability", "comments": "First International Workshop on Legacy Software Refactoring for\n  Performance (REFAC'19) in conjunction with ISC'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.PF physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we report on strategies and results of our initial approach for\nmodernization of Titan2D code. Titan2D is a geophysical mass flow simulation\ncode designed for modeling of volcanic flows, debris avalanches and landslides\nover a realistic terrain model. It solves an underlying hyperbolic system of\npartial differential equations using parallel adaptive mesh Godunov scheme. The\nfollowing work was done during code refactoring and modernization. To\nfacilitate user input two level python interface was developed. Such design\npermits large changes in C++ and Python low-level while maintaining stable\nhigh-level interface exposed to the end user. Multiple diverged forks\nimplementing different material models were merged back together. Data storage\nlayout was changed from a linked list of structures to a structure of arrays\nrepresentation for better memory access and in preparation for further work on\nbetter utilization of vectorized instruction. Existing MPI parallelization was\naugmented with OpenMP parallelization. The performance of a hash table used to\nstore mesh elements and nodes references was improved by switching from a\nlinked list for overflow entries to dynamic arrays allowing the implementation\nof the binary search algorithm. The introduction of the new data layout made\npossible to reduce the number of hash table look-ups by replacing them with\ndirect use of indexes from the storage class. The modifications lead to 8-9\ntimes performance improvement for serial execution.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 15:20:11 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Simakov", "Nikolay A.", ""], ["Jones-Ivey", "Renette L.", ""], ["Akhavan-Safaei", "Ali", ""], ["Aghakhani", "Hossein", ""], ["Jones", "Matthew D.", ""], ["Patra", "Abani K.", ""]]}, {"id": "1910.01196", "submitter": "Chih-Chieh Yang", "authors": "Chih-Chieh Yang and Guojing Cong", "title": "Accelerating Data Loading in Deep Neural Network Training", "comments": "11 pages, 12 figures, accepted for publication in IEEE International\n  Conference on High Performance Computing, Data and Analytics (HiPC) 2019", "journal-ref": null, "doi": "10.1109/HiPC.2019.00037", "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data loading can dominate deep neural network training time on large-scale\nsystems. We present a comprehensive study on accelerating data loading\nperformance in large-scale distributed training. We first identify performance\nand scalability issues in current data loading implementations. We then propose\noptimizations that utilize CPU resources to the data loader design. We use an\nanalytical model to characterize the impact of data loading on the overall\ntraining time and establish the performance trend as we scale up distributed\ntraining. Our model suggests that I/O rate limits the scalability of\ndistributed training, which inspires us to design a locality-aware data loading\nmethod. By utilizing software caches, our method can drastically reduce the\ndata loading communication volume in comparison with the original data loading\nimplementation. Finally, we evaluate the proposed optimizations with various\nexperiments. We achieved more than 30x speedup in data loading using 256 nodes\nwith 1,024 learners.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:03:02 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Yang", "Chih-Chieh", ""], ["Cong", "Guojing", ""]]}, {"id": "1910.01304", "submitter": "Francois Demoullin", "authors": "Francois Demoullin, Ayub Gubran, Tor Aamodt", "title": "Hash-Based Ray Path Prediction: Skipping BVH Traversal Computation by\n  Exploiting Ray Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art ray tracing techniques operate on hierarchical acceleration\nstructures such as BVH trees which wrap objects in a scene into bounding\nvolumes of decreasing sizes. Acceleration structures reduce the amount of\nray-scene intersections that a ray has to perform to find the intersecting\nobject. However, we observe a large amount of redundancy when rays are\ntraversing these acceleration structures. While modern acceleration structures\nexplore the spatial organization of the scene, they neglect similarities\nbetween rays that traverse the structures and thereby cause redundant\ntraversals. This paper provides a limit study of a new promising technique,\nHash-Based Ray Path Prediction (HRPP), which exploits the similarity between\nrays to predict leaf nodes to avoid redundant acceleration structure\ntraversals. Our data shows that acceleration structure traversal consumes a\nsignificant proportion of the ray tracing rendering time regardless of the\nplatform or the target image quality. Our study quantifies unused ray locality\nand evaluates the theoretical potential for improved ray traversal performance\nfor both coherent and seemingly incoherent rays. We show that HRPP is able to\nskip, on average, 40% of all hit-all traversal computations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 05:09:58 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Demoullin", "Francois", ""], ["Gubran", "Ayub", ""], ["Aamodt", "Tor", ""]]}, {"id": "1910.01310", "submitter": "Pingcheng Ruan", "authors": "Pingcheng Ruan, Tien Tuan Anh Dinh, Dumitrel Loghin, Meihui Zhang,\n  Gang Chen, Qian Lin, Beng Chin Ooi", "title": "Blockchains vs. Distributed Databases: Dichotomy and Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has come a long way: a system that was initially proposed\nspecifically for cryptocurrencies is now being adapted and adopted as a\ngeneral-purpose transactional system. As blockchain evolves into another data\nmanagement system, the natural question is how it compares against distributed\ndatabase systems. Existing works on this comparison focus on high-level\nproperties, such as security and throughput. They stop short of showing how the\nunderlying design choices contribute to the overall differences. Our work fills\nthis important gap and provides a principled framework for analyzing the\nemerging trend of blockchain-database fusion.\n  We perform a twin study of blockchains and distributed database systems as\ntwo types of transactional systems. We propose a taxonomy that illustrates the\ndichotomy across four dimensions, namely replication, concurrency, storage, and\nsharding. Within each dimension, we discuss how the design choices are driven\nby two goals: security for blockchains, and performance for distributed\ndatabases. To expose the impact of different design choices on the overall\nperformance, we conduct an in-depth performance analysis of two blockchains,\nnamely Quorum and Hyperledger Fabric, and two distributed databases, namely\nTiDB, and etcd. Lastly, we propose a framework for back-of-the-envelope\nperformance forecast of blockchain-database hybrids.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 05:43:41 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 12:40:12 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ruan", "Pingcheng", ""], ["Dinh", "Tien Tuan Anh", ""], ["Loghin", "Dumitrel", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Lin", "Qian", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1910.01500", "submitter": "Cody Coleman", "authors": "Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius\n  Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor\n  Bittorf, David Brooks, Dehao Chen, Debojyoti Dutta, Udit Gupta, Kim\n  Hazelwood, Andrew Hock, Xinyuan Huang, Atsushi Ike, Bill Jia, Daniel Kang,\n  David Kanter, Naveen Kumar, Jeffery Liao, Guokai Ma, Deepak Narayanan, Tayo\n  Oguntebi, Gennady Pekhimenko, Lillian Pentecost, Vijay Janapa Reddi, Taylor\n  Robie, Tom St. John, Tsuguchika Tabaru, Carole-Jean Wu, Lingjie Xu, Masafumi\n  Yamazaki, Cliff Young, Matei Zaharia", "title": "MLPerf Training Benchmark", "comments": "MLSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) needs industry-standard performance benchmarks to\nsupport design and competitive evaluation of the many emerging software and\nhardware solutions for ML. But ML training presents three unique benchmarking\nchallenges absent from other domains: optimizations that improve training\nthroughput can increase the time to solution, training is stochastic and time\nto solution exhibits high variance, and software and hardware systems are so\ndiverse that fair benchmarking with the same binary, code, and even\nhyperparameters is difficult. We therefore present MLPerf, an ML benchmark that\novercomes these challenges. Our analysis quantitatively evaluates MLPerf's\nefficacy at driving performance and scalability improvements across two rounds\nof results from multiple vendors.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 17:55:34 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 19:31:47 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 17:22:28 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Mattson", "Peter", ""], ["Cheng", "Christine", ""], ["Coleman", "Cody", ""], ["Diamos", "Greg", ""], ["Micikevicius", "Paulius", ""], ["Patterson", "David", ""], ["Tang", "Hanlin", ""], ["Wei", "Gu-Yeon", ""], ["Bailis", "Peter", ""], ["Bittorf", "Victor", ""], ["Brooks", "David", ""], ["Chen", "Dehao", ""], ["Dutta", "Debojyoti", ""], ["Gupta", "Udit", ""], ["Hazelwood", "Kim", ""], ["Hock", "Andrew", ""], ["Huang", "Xinyuan", ""], ["Ike", "Atsushi", ""], ["Jia", "Bill", ""], ["Kang", "Daniel", ""], ["Kanter", "David", ""], ["Kumar", "Naveen", ""], ["Liao", "Jeffery", ""], ["Ma", "Guokai", ""], ["Narayanan", "Deepak", ""], ["Oguntebi", "Tayo", ""], ["Pekhimenko", "Gennady", ""], ["Pentecost", "Lillian", ""], ["Reddi", "Vijay Janapa", ""], ["Robie", "Taylor", ""], ["John", "Tom St.", ""], ["Tabaru", "Tsuguchika", ""], ["Wu", "Carole-Jean", ""], ["Xu", "Lingjie", ""], ["Yamazaki", "Masafumi", ""], ["Young", "Cliff", ""], ["Zaharia", "Matei", ""]]}, {"id": "1910.01716", "submitter": "Khaza Anuarul Hoque", "authors": "Gautam Raj Mode, Prasad Calyam, Khaza Anuarul Hoque", "title": "False Data Injection Attacks in Internet of Things and Deep Learning\n  enabled Predictive Analytics", "comments": "extended version of the manuscript entitled \"Impact of False Data\n  Injection Attacks on Deep Learning enabled Predictive Analytics\" accepted for\n  publication in the IEEE NOMS 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 is the latest industrial revolution primarily merging automation\nwith advanced manufacturing to reduce direct human effort and resources.\nPredictive maintenance (PdM) is an industry 4.0 solution, which facilitates\npredicting faults in a component or a system powered by state-of-the-art\nmachine learning (ML) algorithms and the Internet-of-Things (IoT) sensors.\nHowever, IoT sensors and deep learning (DL) algorithms, both are known for\ntheir vulnerabilities to cyber-attacks. In the context of PdM systems, such\nattacks can have catastrophic consequences as they are hard to detect due to\nthe nature of the attack. To date, the majority of the published literature\nfocuses on the accuracy of DL enabled PdM systems and often ignores the effect\nof such attacks. In this paper, we demonstrate the effect of IoT sensor attacks\non a PdM system. At first, we use three state-of-the-art DL algorithms,\nspecifically, Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and\nConvolutional Neural Network (CNN) for predicting the Remaining Useful Life\n(RUL) of a turbofan engine using NASA's C-MAPSS dataset. The obtained results\nshow that the GRU-based PdM model outperforms some of the recent literature on\nRUL prediction using the C-MAPSS dataset. Afterward, we model two different\ntypes of false data injection attacks (FDIA) on turbofan engine sensor data and\nevaluate their impact on CNN, LSTM, and GRU-based PdM systems. The obtained\nresults demonstrate that FDI attacks on even a few IoT sensors can strongly\ndefect the RUL prediction. However, the GRU-based PdM model performs better in\nterms of accuracy and resiliency. Lastly, we perform a study on the GRU-based\nPdM model using four different GRU networks with different sequence lengths.\nOur experiments reveal an interesting relationship between the accuracy,\nresiliency and sequence length for the GRU-based PdM models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 20:50:08 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 21:26:54 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 02:57:28 GMT"}, {"version": "v4", "created": "Fri, 13 Dec 2019 00:07:14 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Mode", "Gautam Raj", ""], ["Calyam", "Prasad", ""], ["Hoque", "Khaza Anuarul", ""]]}, {"id": "1910.01972", "submitter": "Karel Ad\\'amek", "authors": "Karel Ad\\'amek, Sofia Dimoudi, Mike Giles, Wesley Armour", "title": "GPU Fast Convolution via the Overlap-and-Save Method in Shared Memory", "comments": "accepted to ACM TACO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of the overlap-and-save method, a method for the\nconvolution of very long signals with short response functions, which is\ntailored to GPUs. We have implemented several FFT algorithms (using the CUDA\nprogramming language) which exploit GPU shared memory, allowing for GPU\naccelerated convolution. We compare our implementation with an implementation\nof the overlap-and-save algorithm utilizing the NVIDIA FFT library (cuFFT). We\ndemonstrate that by using a shared memory based FFT we can achieved significant\nspeed-ups for certain problem sizes and lower the memory requirements of the\noverlap-and-save method on GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 14:41:10 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 15:04:13 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Ad\u00e1mek", "Karel", ""], ["Dimoudi", "Sofia", ""], ["Giles", "Mike", ""], ["Armour", "Wesley", ""]]}, {"id": "1910.02276", "submitter": "Quan-Lin Li", "authors": "Rui-Na Fan, Quan-Lin Li, Xiaole Wu, Zhe George Zhang", "title": "Dockless Bike-Sharing Systems with Unusable Bikes: Removing, Repair and\n  Redistribution under Batch Policies", "comments": "51 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.SI math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a large-scale dockless bike-sharing system (DBSS) with\nunusable bikes, which can be removed, repaired, redistributed and reused under\ntwo batch policies: One for removing the unusable bikes from each parking\nregion to a maintenance shop, and the other for redistributing the repaired\nbikes from the maintenance shop to some suitable parking regions. For such a\nbike-sharing system, this paper proposes and develops a new computational\nmethod by applying the RG-factorizations of block-structured Markov processes\nin the closed queueing networks. Different from previous works in the\nliterature of queueing networks, a key contribution of our computational method\nis to set up a new nonlinear matrix equation to determine the relative arrival\nrates, and to show that the nonlinearity comes from two different groups of\nprocesses: The failure and removing processes; and the repair and\nredistributing processes. Once the relative arrival rate is introduced to each\nnode, these nodes are isolated from each other, so that the Markov processes of\nall the nodes are independent of each other, thus the Markov system of each\nnode is described as an elegant block-structured Markov process whose\nstationary probabilities can be easily computed by the RG-factorizations. Based\non this, this paper can establish a more general product-form solution of the\nclosed queueing network, and provides performance analysis of the DBSS through\na comprehensive discussion for the bikes' failure, removing, repair,\nredistributing and reuse processes under two batch policies. We hope that our\nmethod opens a new avenue to quantitative evaluation of more general DBSSs with\nunusable bikes.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 14:25:06 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 08:43:35 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Fan", "Rui-Na", ""], ["Li", "Quan-Lin", ""], ["Wu", "Xiaole", ""], ["Zhang", "Zhe George", ""]]}, {"id": "1910.02351", "submitter": "Aditya Kumar", "authors": "Evandro Menezes, Sebastian Pop, Aditya Kumar", "title": "Clustering case statements for indirect branch predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an O(nlogn) algorithm to compile a switch statement into jump\ntables. To generate jump tables that can be efficiently predicted by current\nhardware branch predictors, we added an upper bound on the number of entries\nfor each table. This modification of the previously best known algorithm\nreduces the complexity from O(n^2) to O(nlogn).\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 01:10:57 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 16:05:42 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 14:22:44 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Menezes", "Evandro", ""], ["Pop", "Sebastian", ""], ["Kumar", "Aditya", ""]]}, {"id": "1910.02516", "submitter": "Andrew McGough", "authors": "Alexander J. M. Kell and Matthew Forshaw and A. Stephen McGough", "title": "Optimising energy and overhead for large parameter space simulations", "comments": "Accepted for IGSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many systems require optimisation over multiple objectives, where objectives\nare characteristics of the system such as energy consumed or increase in time\nto perform the work. Optimisation is performed by selecting the `best' set of\ninput parameters to elicit the desired objectives. However, the parameter\nsearch space can often be far larger than can be searched in a reasonable time.\nAdditionally, the objectives are often mutually exclusive -- leading to a\ndecision being made as to which objective is more important or optimising over\na combination of the objectives. This work is an application of a Genetic\nAlgorithm to identify the Pareto frontier for finding the optimal parameter\nsets for all combinations of objectives. A Pareto frontier can be used to\nidentify the sets of optimal parameters for which each is the `best' for a\ngiven combination of objectives -- thus allowing decisions to be made with full\nknowledge. We demonstrate this approach for the HTC-Sim simulation system in\nthe case where a Reinforcement Learning scheduler is tuned for the two\nobjectives of energy consumption and task overhead. Demonstrating that this\napproach can reduce the energy consumed by ~36% over previously published work\nwithout significantly increasing the overhead.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 20:21:16 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Kell", "Alexander J. M.", ""], ["Forshaw", "Matthew", ""], ["McGough", "A. Stephen", ""]]}, {"id": "1910.03033", "submitter": "Erik Altman", "authors": "Erik R. Altman", "title": "Synthesizing Credit Card Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two elements have been essential to AI's recent boom: (1) deep neural nets\nand the theory and practice behind them; and (2) cloud computing with its\nabundant labeled data and large computing resources.\n  Abundant labeled data is available for key domains such as images, speech,\nnatural language processing, and recommendation engines. However, there are\nmany other domains where such data is not available, or access to it is highly\nrestricted for privacy reasons, as with health and financial data. Even when\nabundant data is available, it is often not labeled. Doing such labeling is\nlabor-intensive and non-scalable.\n  As a result, to the best of our knowledge, key domains still lack labeled\ndata or have at most toy data; or the synthetic data must have access to real\ndata from which it can mimic new data. This paper outlines work to generate\nrealistic synthetic data for an important domain: credit card transactions.\n  Some challenges: there are many patterns and correlations in real purchases.\nThere are millions of merchants and innumerable locations. Those merchants\noffer a wide variety of goods. Who shops where and when? How much do people\npay? What is a realistic fraudulent transaction?\n  We use a mixture of technical approaches and domain knowledge including\nmechanics of credit card processing, a broad set of consumer domains:\nelectronics, clothing, hair styling, etc. Connecting everything is a virtual\nworld. This paper outlines some of our key techniques and provides evidence\nthat the data generated is indeed realistic.\n  Beyond the scope of this paper: (1) use of our data to develop and train\nmodels to predict fraud; (2) coupling models and the synthetic dataset to\nassess performance in designing accelerators such as GPUs and TPUs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 01:18:57 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Altman", "Erik R.", ""]]}, {"id": "1910.03169", "submitter": "Koki Ho", "authors": "Koki Ho, Hai Wang, Paul A. DeTrempe, Tristan Sarton du Jonchay, and\n  Kento Tomita", "title": "Semi-Analytical Model for Design and Analysis of On-Orbit Servicing\n  Architecture", "comments": "21 pages, 8 figures, Accepted by Journal of Spacecraft and Rockets", "journal-ref": "Journal of Spacecraft and Rockets, 2020", "doi": "10.2514/1.A34663", "report-no": null, "categories": "cs.PF cs.SY eess.SY physics.space-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic on-orbit servicing (OOS) is expected to be a key technology and\nconcept for future sustainable space exploration. This paper develops a\nsemi-analytical model for OOS systems analysis, responding to the growing needs\nand ongoing trend of robotic OOS. An OOS infrastructure system is considered\nwhose goal is to provide responsive services to the random failures of a set of\ncustomer modular satellites distributed in space (e.g., at the geosynchronous\nequatorial orbit). The considered OOS architecture is comprised of a servicer\nthat travels and provides module-replacement services to the customer\nsatellites, an on-orbit depot to store the spares, and a series of launch\nvehicles to replenish the depot. The OOS system performance is analyzed by\nevaluating the mean waiting time before service completion for a given failure\nand its relationship with the depot capacity. Leveraging the queueing theory\nand inventory management methods, the developed semi-analytical model is\ncapable of analyzing the OOS system performance without relying on\ncomputationally costly simulations. The effectiveness of the proposed model is\ndemonstrated using a case study compared with simulation results. This paper is\nexpected to provide a critical step to push the research frontier of\nanalytical/semi-analytical models development for complex space systems design.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 02:00:26 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 12:30:40 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 20:13:57 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ho", "Koki", ""], ["Wang", "Hai", ""], ["DeTrempe", "Paul A.", ""], ["Jonchay", "Tristan Sarton du", ""], ["Tomita", "Kento", ""]]}, {"id": "1910.03679", "submitter": "Oded Green", "authors": "Oded Green, James Fox, Jeffrey Young, Jun Shirako, David Bader", "title": "Performance Impact of Memory Channels on Sparse and Irregular Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing is typically considered to be a memory-bound rather than\ncompute-bound problem. One common line of thought is that more available memory\nbandwidth corresponds to better graph processing performance. However, in this\nwork we demonstrate that the key factor in the utilization of the memory system\nfor graph algorithms is not necessarily the raw bandwidth or even the latency\nof memory requests. Instead, we show that performance is proportional to the\nnumber of memory channels available to handle small data transfers with limited\nspatial locality.\n  Using several widely used graph frameworks, including Gunrock (on the GPU)\nand GAPBS \\& Ligra (for CPUs), we evaluate key graph analytics kernels using\ntwo unique memory hierarchies, DDR-based and HBM/MCDRAM. Our results show that\nthe differences in the peak bandwidths of several Pascal-generation GPU memory\nsubsystems aren't reflected in the performance of various analytics.\nFurthermore, our experiments on CPU and Xeon Phi systems demonstrate that the\nnumber of memory channels utilized can be a decisive factor in performance\nacross several different applications. For CPU systems with smaller thread\ncounts, the memory channels can be underutilized while systems with high thread\ncounts can oversaturate the memory subsystem, which leads to limited\nperformance. Finally, we model the potential performance improvements of adding\nmore memory channels with narrower access widths than are found in current\nplatforms, and we analyze performance trade-offs for the two most prominent\ntypes of memory accesses found in graph algorithms, streaming and random\naccesses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 20:39:14 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Green", "Oded", ""], ["Fox", "James", ""], ["Young", "Jeffrey", ""], ["Shirako", "Jun", ""], ["Bader", "David", ""]]}, {"id": "1910.04877", "submitter": "Sek Chai", "authors": "Prateeth Nayak, David Zhang, Sek Chai", "title": "Bit Efficient Quantization for Deep Neural Networks", "comments": "EMC2 - NeurIPS workshop 2019, #latentai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization for deep neural networks have afforded models for edge devices\nthat use less on-board memory and enable efficient low-power inference. In this\npaper, we present a comparison of model-parameter driven quantization\napproaches that can achieve as low as 3-bit precision without affecting\naccuracy. The post-training quantization approaches are data-free, and the\nresulting weight values are closely tied to the dataset distribution on which\nthe model has converged to optimality. We show quantization results for a\nnumber of state-of-art deep neural networks (DNN) using large dataset like\nImageNet. To better analyze quantization results, we describe the overall range\nand local sparsity of values afforded through various quantization schemes. We\nshow the methods to lower bit-precision beyond quantization limits with object\nclass clustering.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:43:12 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Nayak", "Prateeth", ""], ["Zhang", "David", ""], ["Chai", "Sek", ""]]}, {"id": "1910.05398", "submitter": "Jayneel Gandhi", "authors": "Reto Achermann, Ashish Panwar, Abhishek Bhattacharjee, Timothy Roscoe,\n  Jayneel Gandhi", "title": "Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-socket machines with 1-100 TBs of physical memory are becoming\nprevalent. Applications running on multi-socket machines suffer non-uniform\nbandwidth and latency when accessing physical memory. Decades of research have\nfocused on data allocation and placement policies in NUMA settings, but there\nhave been no studies on the question of how to place page-tables amongst\nsockets. We make the case for explicit page-table allocation policies and show\nthat page-table placement is becoming crucial to overall performance. We\npropose Mitosis to mitigate NUMA effects on page-table walks by transparently\nreplicating and migrating page-tables across sockets without application\nchanges. This reduces the frequency of accesses to remote NUMA nodes when\nperforming page-table walks. Mitosis uses two components: (i) a mechanism to\nenable efficient page-table replication and migration; and (ii) policies for\nprocesses to efficiently manage and control page-table replication and\nmigration. We implement Mitosis in Linux and evaluate its benefits on real\nhardware. Mitosis improves performance for large-scale multi-socket workloads\nby up to 1.34x by replicating page-tables across sockets. Moreover, it improves\nperformance by up to 3.24x in cases when the OS migrates a process across\nsockets by enabling cross-socket page-table migration.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 20:26:14 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 06:36:11 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Achermann", "Reto", ""], ["Panwar", "Ashish", ""], ["Bhattacharjee", "Abhishek", ""], ["Roscoe", "Timothy", ""], ["Gandhi", "Jayneel", ""]]}, {"id": "1910.05482", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu and Jianxun Liu", "title": "ClassyTune: A Performance Auto-Tuner for Systems in the Cloud", "comments": "12 pages, Journal paper", "journal-ref": "ClassyTune: A Performance Auto-Tuner for Systems in the Cloud.\n  IEEE Transactions on Cloud Computing, 2019. doi: 10.1109/TCC.2019.2936567", "doi": "10.1109/TCC.2019.2936567", "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance tuning can improve the system performance and thus enable the\nreduction of cloud computing resources needed to support an application. Due to\nthe ever increasing number of parameters and complexity of systems, there is a\nnecessity to automate performance tuning for the complicated systems in the\ncloud. The state-of-the-art tuning methods are adopting either the\nexperience-driven tuning approach or the data-driven one. Data-driven tuning is\nattracting increasing attentions, as it has wider applicability. But existing\ndata-driven methods cannot fully address the challenges of sample scarcity and\nhigh dimensionality simultaneously. We present ClassyTune, a data-driven\nautomatic configuration tuning tool for cloud systems. ClassyTune exploits the\nmachine learning model of classification for auto-tuning. This exploitation\nenables the induction of more training samples without increasing the input\ndimension. Experiments on seven popular systems in the cloud show that\nClassyTune can effectively tune system performance to seven times higher for\nhigh-dimensional configuration space, outperforming expert tuning and the\nstate-of-the-art auto-tuning solutions. We also describe a use case in which\nperformance tuning enables the reduction of 33% computing resources needed to\nrun an online stateless service.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 04:05:24 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Zhu", "Yuqing", ""], ["Liu", "Jianxun", ""]]}, {"id": "1910.05713", "submitter": "Jin-Yuan Wang", "authors": "Jin-Yuan Wang, Yu Qiu, Sheng-Hong Lin, Jun-Bo Wang, Min Lin, Cheng Liu", "title": "On the Secrecy Performance of Random VLC Networks with Imperfect CSI and\n  Protected Zone", "comments": "Accepted by IEEE Systems Joutnal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the physical-layer security for a random indoor\nvisible light communication (VLC) network with imperfect channel state\ninformation (CSI) and a protected zone. The VLC network consists of three\nnodes, i.e., a transmitter (Alice), a legitimate receiver (Bob), and an\neavesdropper (Eve). Alice is fixed in the center of the ceiling, and the\nemitted signal at Alice satisfies the non-negativity and the dimmable average\noptical intensity constraint. Bob and Eve are randomly deployed on the receiver\nplane. By employing the protected zone and considering the imperfect CSI, the\nstochastic characteristics of the channel gains for both the main and the\neavesdropping channels is first analyzed. After that, the closed-form\nexpressions of the average secrecy capacity and the lower bound of secrecy\noutage probability are derived, respectively. Finally, Monte-Carlo simulations\nare provided to verify the accuracy of the derived theoretical expressions.\nMoreover, the impacts of the nominal optical intensity, the dimming target, the\nprotected zone and the imperfect CSI on secrecy performance are discussed,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 08:49:18 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Wang", "Jin-Yuan", ""], ["Qiu", "Yu", ""], ["Lin", "Sheng-Hong", ""], ["Wang", "Jun-Bo", ""], ["Lin", "Min", ""], ["Liu", "Cheng", ""]]}, {"id": "1910.05749", "submitter": "Mehran Sadeghi Lahijani", "authors": "Mehran Sadeghi Lahijani, Tasvirul Islam, Ashok Srinivasan, Sirish\n  Namilae", "title": "Constrained Linear Movement Model (CALM): Simulation of passenger\n  movement in airplanes", "comments": "12 pages, 6 figures, will be submitted to the PLOS One Journal", "journal-ref": null, "doi": "10.1371/journal.pone.0229690", "report-no": "PONE-D-19-28952", "categories": "physics.soc-ph cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian dynamics models the walking movement of individuals in a crowd. It\nhas recently been used in the analysis of procedures to reduce the risk of\ndisease spread in airplanes, relying on the SPED model. This is a social force\nmodel inspired by molecular dynamics; pedestrians are treated as point\nparticles, and their trajectories are determined in a simulation. A parameter\nsweep is performed to address uncertainties in human behavior, which requires a\nlarge number of simulations.\n  The SPED model's slow speed is a bottleneck to performing a large parameter\nsweep. This is a severe impediment to delivering real-time results, which are\noften required in the course of decision meetings, especially during\nemergencies. We propose a new model, called CALM, to remove this limitation. It\nis designed to simulate a crowd's movement in constrained linear passageways,\nsuch as inside an aircraft. We show that CALM yields realistic results while\nimproving performance by two orders of magnitude over the SPED model.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 13:42:08 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Lahijani", "Mehran Sadeghi", ""], ["Islam", "Tasvirul", ""], ["Srinivasan", "Ashok", ""], ["Namilae", "Sirish", ""]]}, {"id": "1910.05791", "submitter": "Mehmet Aktas", "authors": "Mehmet Fatih Aktas, Amir Behrouzi-Far, Emina Soljanin, Philip Whiting", "title": "Evaluating Load Balancing Performance in Distributed Storage with\n  Redundancy", "comments": "To appear in Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2021.3054385", "report-no": null, "categories": "cs.PF cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate load balancing, distributed systems store data redundantly. We\nevaluate the load balancing performance of storage schemes in which each object\nis stored at $d$ different nodes, and each node stores the same number of\nobjects. In our model, the load offered for the objects is sampled uniformly at\nrandom from all the load vectors with a fixed cumulative value. We find that\nthe load balance in a system of $n$ nodes improves multiplicatively with $d$ as\nlong as $d = o\\left(\\log(n)\\right)$, and improves exponentially once $d =\n\\Theta\\left(\\log(n)\\right)$. We show that the load balance improves in the same\nway with $d$ when the service choices are created with XOR's of $r$ objects\nrather than object replicas. In such redundancy schemes, storage overhead is\nreduced multiplicatively by $r$. However, recovery of an object requires\ndownloading content from $r$ nodes. At the same time, the load balance\nincreases additively by $r$. We express the system's load balance in terms of\nthe maximal spacing or maximum of $d$ consecutive spacings between the ordered\nstatistics of uniform random variables. Using this connection and the limit\nresults on the maximal $d$-spacings, we derive our main results.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 17:23:14 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 05:40:27 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2021 03:36:18 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""], ["Whiting", "Philip", ""]]}, {"id": "1910.05930", "submitter": "Guoping Long", "authors": "Mengdi Wang, Chen Meng, Guoping Long, Chuan Wu, Jun Yang, Wei Lin,\n  Yangqing Jia", "title": "Characterizing Deep Learning Training Workloads on Alibaba-PAI", "comments": "Accepted by IISWC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning models have been exploited in various domains, including\ncomputer vision (CV), natural language processing (NLP), search and\nrecommendation. In practical AI clusters, workloads training these models are\nrun using software frameworks such as TensorFlow, Caffe, PyTorch and CNTK. One\ncritical issue for efficiently operating practical AI clouds, is to\ncharacterize the computing and data transfer demands of these workloads, and\nmore importantly, the training performance given the underlying software\nframework and hardware configurations. In this paper, we characterize deep\nlearning training workloads from Platform of Artificial Intelligence (PAI) in\nAlibaba. We establish an analytical framework to investigate detailed execution\ntime breakdown of various workloads using different training architectures, to\nidentify performance bottleneck. Results show that weight/gradient\ncommunication during training takes almost 62% of the total execution time\namong all our workloads on average. The computation part, involving both GPU\ncomputing and memory access, are not the biggest bottleneck based on collective\nbehavior of the workloads. We further evaluate attainable performance of the\nworkloads on various potential software/hardware mappings, and explore\nimplications on software architecture selection and hardware configurations. We\nidentify that 60% of PS/Worker workloads can be potentially sped up when ported\nto the AllReduce architecture exploiting the high-speed NVLink for GPU\ninterconnect, and on average 1.7X speedup can be achieved when Ethernet\nbandwidth is upgraded from 25 Gbps to 100 Gbps.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 06:26:01 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Wang", "Mengdi", ""], ["Meng", "Chen", ""], ["Long", "Guoping", ""], ["Wu", "Chuan", ""], ["Yang", "Jun", ""], ["Lin", "Wei", ""], ["Jia", "Yangqing", ""]]}, {"id": "1910.06310", "submitter": "Yu-Hang Tang", "authors": "Yu-Hang Tang, Oguz Selvitopi, Doru Popovici, Ayd{\\i}n Bulu\\c{c}", "title": "A High-Throughput Solver for Marginalized Graph Kernels on GPU", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS47924.2020.00080", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and optimization of a linear solver on General Purpose\nGPUs for the efficient and high-throughput evaluation of the marginalized graph\nkernel between pairs of labeled graphs. The solver implements a preconditioned\nconjugate gradient (PCG) method to compute the solution to a generalized\nLaplacian equation associated with the tensor product of two graphs. To cope\nwith the gap between the instruction throughput and the memory bandwidth of\ncurrent generation GPUs, our solver forms the tensor product linear system\non-the-fly without storing it in memory when performing matrix-vector dot\nproduct operations in PCG. Such on-the-fly computation is accomplished by using\nthreads in a warp to cooperatively stream the adjacency and edge label matrices\nof individual graphs by small square matrix blocks called tiles, which are then\nstaged in registers and the shared memory for later reuse. Warps across a\nthread block can further share tiles via the shared memory to increase data\nreuse. We exploit the sparsity of the graphs hierarchically by storing only\nnon-empty tiles using a coordinate format and nonzero elements within each tile\nusing bitmaps. Besides, we propose a new partition-based reordering algorithm\nfor aggregating nonzero elements of the graphs into fewer but denser tiles to\nimprove the efficiency of the sparse format.\n  We carry out extensive theoretical analyses on the graph tensor product\nprimitives for tiles of various density and evaluate their performance on\nsynthetic and real-world datasets. Our solver delivers three to four orders of\nmagnitude speedup over existing CPU-based solvers such as GraKeL and\nGraphKernels. The capability of the solver enables kernel-based learning tasks\nat unprecedented scales.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 17:46:47 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 08:58:01 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 08:09:42 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 23:28:34 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Tang", "Yu-Hang", ""], ["Selvitopi", "Oguz", ""], ["Popovici", "Doru", ""], ["Bulu\u00e7", "Ayd\u0131n", ""]]}, {"id": "1910.06458", "submitter": "Ali Mirzaeian", "authors": "Ali Mirzaeian, Houman Homayoun, Avesta Sasan", "title": "TCD-NPE: A Re-configurable and Efficient Neural Processing Engine,\n  Powered by Novel Temporal-Carry-deferring MACs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we first propose the design of Temporal-Carry-deferring MAC\n(TCD-MAC) and illustrate how our proposed solution can gain significant energy\nand performance benefit when utilized to process a stream of input data. We\nthen propose using the TCD-MAC to build a reconfigurable, high speed, and low\npower Neural Processing Engine (TCD-NPE). We, further, propose a novel\nscheduler that lists the sequence of needed processing events to process an MLP\nmodel in the least number of computational rounds in our proposed TCD-NPE. We\nillustrate that our proposed TCD-NPE significantly outperform similar neural\nprocessing solutions that use conventional MACs in terms of both energy\nconsumption and execution time.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 23:05:34 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Mirzaeian", "Ali", ""], ["Homayoun", "Houman", ""], ["Sasan", "Avesta", ""]]}, {"id": "1910.06663", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang,\n  Felix Baum, Max Wu, Lirong Xu, Luc Van Gool", "title": "AI Benchmark: All About Deep Learning on Smartphones in 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of mobile AI accelerators has been evolving rapidly in the\npast two years, nearly doubling with each new generation of SoCs. The current\n4th generation of mobile NPUs is already approaching the results of\nCUDA-compatible Nvidia graphics cards presented not long ago, which together\nwith the increased capabilities of mobile deep learning frameworks makes it\npossible to run complex and deep AI models on mobile devices. In this paper, we\nevaluate the performance and compare the results of all chipsets from Qualcomm,\nHiSilicon, Samsung, MediaTek and Unisoc that are providing hardware\nacceleration for AI inference. We also discuss the recent changes in the\nAndroid ML pipeline and provide an overview of the deployment of deep learning\nmodels on mobile devices. All numerical results provided in this paper can be\nfound and are regularly updated on the official project website:\nhttp://ai-benchmark.com.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 11:31:36 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Ignatov", "Andrey", ""], ["Timofte", "Radu", ""], ["Kulik", "Andrei", ""], ["Yang", "Seungsoo", ""], ["Wang", "Ke", ""], ["Baum", "Felix", ""], ["Wu", "Max", ""], ["Xu", "Lirong", ""], ["Van Gool", "Luc", ""]]}, {"id": "1910.06674", "submitter": "Alexey Lastovetsky", "authors": "Semyon Khokhriakov, Ravi Reddy Manumachu, Alexey Lastovetsky", "title": "Modern Multicore CPUs are not Energy Proportional: Opportunity for\n  Bi-objective Optimization for Performance and Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy proportionality is the key design goal followed by architects of\nmodern multicore CPUs. One of its implications is that optimization of an\napplication for performance will also optimize it for energy. In this work, we\nshow that energy proportionality does not hold true for multicore CPUs. This\nfinding creates the opportunity for bi-objective optimization of applications\nfor performance and energy. We propose and study the first application-level\nmethod for bi-objective optimization of multithreaded data-parallel\napplications for performance and energy. The method uses two decision\nvariables, the number of identical multithreaded kernels (threadgroups)\nexecuting the application and the number of threads in each threadgroup, with\nthe workload always partitioned equally between the threadgroups. We\nexperimentally demonstrate the efficiency of the method using four highly\noptimized multithreaded data-parallel applications, 2D fast Fourier transform\nbased on FFTW and Intel MKL, and dense matrix-matrix multiplication using\nOpenBLAS and Intel MKL. Four modern multicore CPUs are used in the experiments.\nThe experiments show that optimization for performance alone results in the\nincrease in dynamic energy consumption by up to 89% and optimization for\ndynamic energy alone degrades the performance by up to 49%. By solving the\nbi-objective optimization problem, the method determines up to 11 globally\nPareto-optimal solutions. Finally, we propose a qualitative dynamic energy\nmodel employing performance monitoring counters as parameters, which we use to\nexplain the discovered energy nonproportionality and the Pareto-optimal\nsolutions determined by our method. The model shows that the energy\nnonproportionality in our case is due to the activity of the data translation\nlookaside buffer (dTLB), which is disproportionately energy expensive.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 12:17:21 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Khokhriakov", "Semyon", ""], ["Manumachu", "Ravi Reddy", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "1910.06844", "submitter": "Ali Mohammed", "authors": "Ali Mohammed, Ahmed Eleliemy, Florina M. Ciorba, Franziska Kasielke,\n  Ioana Banicescu", "title": "An Approach for Realistically Simulating the Performance of Scientific\n  Applications on High Performance Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications often contain large, computationally-intensive, and\nirregular parallel loops or tasks that exhibit stochastic characteristics.\nApplications may suffer from load imbalance during their execution on\nhigh-performance computing (HPC) systems due to such characteristics. Dynamic\nloop self-scheduling (DLS) techniques are instrumental in improving the\nperformance of scientific applications on HPC systems via load balancing.\nSelecting a DLS technique that results in the best performance for different\nproblems and system sizes requires a large number of exploratory experiments. A\ntheoretical model that can be used to predict the scheduling technique that\nyields the best performance for a given problem and system has not yet been\nidentified. Therefore, simulation is the most appropriate approach for\nconducting such exploratory experiments with reasonable costs. This work\ndevises an approach to realistically simulate computationally-intensive\nscientific applications that employ DLS and execute on HPC systems. Several\napproaches to represent the application tasks (or loop iterations) are compared\nto establish their influence on the simulative application performance. A novel\nsimulation strategy is introduced, which transforms a native application code\ninto a simulative code. The native and simulative performance of two\ncomputationally-intensive scientific applications are compared to evaluate the\nrealism of the proposed simulation approach. The comparison of the performance\ncharacteristics extracted from the native and simulative performance shows that\nthe proposed simulation approach fully captured most of the performance\ncharacteristics of interest. This work shows and establishes the importance of\nsimulations that realistically predict the performance of DLS techniques for\ndifferent applications and system configurations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:08:16 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Mohammed", "Ali", ""], ["Eleliemy", "Ahmed", ""], ["Ciorba", "Florina M.", ""], ["Kasielke", "Franziska", ""], ["Banicescu", "Ioana", ""]]}, {"id": "1910.07312", "submitter": "Huanhuan Huang", "authors": "Huanhuan Huang, Tong Ye and Tony T. Lee", "title": "Throughput and Delay Analysis of Slotted Aloha with Batch Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the throughput and delay performances of the slotted\nAloha with batch service, which has wide applications in random access\nnetworks. Different from the classical slotted Aloha, each node in the slotted\nAloha with batch service can transmit up to M packets once it succeeds in\nchannel competition. The throughput is substantially improved because up to M\npackets jointly undertake the overhead due to contention. In an innovative\nvacation model developed in this paper, we consider each batch of data\ntransmission as a busy period of each node, and the process between two\nsuccessive busy periods as a vacation period. We then formulate the number of\narrivals during a vacation period in a renewal-type equation, which\ncharacterizes the dependency between busy periods and vacation periods. Based\non this formulation, we derive the mean waiting time of a packet and the\nbounded delay region for the slotted Aloha with batch service. Our results\nindicate the throughput and delay performances are substantially improved with\nthe increase of batch sizeM, and the bounded delay region is enlarged\naccordingly. As M goes to infinity, we find the saturated throughput can\napproach 100% of channel capacity, and the system remains stable irrespective\nof the population size and transmission probability.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 12:41:13 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Huang", "Huanhuan", ""], ["Ye", "Tong", ""], ["Lee", "Tony T.", ""]]}, {"id": "1910.07377", "submitter": "Francesco Zola", "authors": "Francesco Zola, Cristina P\\'erez-Sol\\'a, Jon Ega\\~na Zubia, Maria\n  Eguimendia and Jordi Herrera-Joancomart\\'i", "title": "Kriptosare.gen, a dockerized Bitcoin testbed: analysis of server\n  performance", "comments": "9 pages, 4 figures, 1 table, presented during the 10th IFIP\n  International Conference on New Technologies, Mobility and Security (NTMS)", "journal-ref": "2019 10th IFIP International Conference on New Technologies,\n  Mobility and Security (NTMS) (pp. 1-5). IEEE", "doi": "10.1109/NTMS.2019.8763809", "report-no": null, "categories": "cs.PF cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin is a peer-to-peer distributed cryptocurrency system, that keeps all\ntransaction history in a public ledger known as blockchain. The Bitcoin network\nis implicitly pseudoanonymous and its nodes are controlled by independent\nentities making network analysis difficult. This calls for the development of a\nfully controlled testing environment.\n  This paper presents Kriptosare.gen, a dockerized automatized Bitcoin testbed,\nfor deploying full-scale custom Bitcoin networks. The testbed is deployed in a\nsingle machine executing four different experiments, each one with different\nnetwork configuration. We perform a cost analysis to investigate how the\nresources are related with network parameters and provide experimental data\nquantifying the amount of computational resources needed to run the different\ntypes of simulations. Obtained results demonstrate that it is possible to run\nthe testbed with a configuration similar to a real Bitcoin system.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:36:30 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Zola", "Francesco", ""], ["P\u00e9rez-Sol\u00e1", "Cristina", ""], ["Zubia", "Jon Ega\u00f1a", ""], ["Eguimendia", "Maria", ""], ["Herrera-Joancomart\u00ed", "Jordi", ""]]}, {"id": "1910.07776", "submitter": "Saeed Taheri", "authors": "Saeed Taheri and Apan Qasem and Martin Burtscher", "title": "A Tool for Automatically Suggesting Source-Code Optimizations for\n  Complex GPU Kernels", "comments": null, "journal-ref": "proceedings of the 2015 International Conference on Parallel and\n  Distributed Processing Techniques and Applications, page 589-599 :\n  WORLDCOMP'15, July 27-30, 2015, Las Vegas, Nevada", "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future computing systems, from handhelds to supercomputers, will undoubtedly\nbe more parallel and heterogeneous than todays systems to provide more\nperformance and energy efficiency. Thus, GPUs are increasingly being used to\naccelerate general purpose applications, including applications with data\ndependent, irregular control flow and memory access patterns. However, the\ngrowing complexity, exposed memory hierarchy, incoherence, heterogeneity, and\nparallelism will make accelerator based systems progressively more difficult to\nprogram. In the foreseeable future, the vast majority of programmers will no\nlonger be able to extract additional performance or energy savings from next\ngeneration systems be-cause the programming will be too difficult. Automatic\nperformance analysis and optimization recommendation tools have the potential\nto avert this situation. They embody expert knowledge and make it available to\nsoftware developers when needed. In this paper, we describe and evaluate such a\ntool.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:57:01 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Taheri", "Saeed", ""], ["Qasem", "Apan", ""], ["Burtscher", "Martin", ""]]}, {"id": "1910.07850", "submitter": "Salvatore Cielo", "authors": "Salvatore Cielo, Luigi Iapichino, Johannes G\\\"unther, Christoph\n  Federrath, Elisabeth Mayer, Markus Wiedemann", "title": "Visualizing the world's largest turbulence simulation", "comments": "6 pages, 5 figures, accompanying paper of SC19 visualization showcase\n  finalist. The full video is publicly available under\n  https://www.youtube.com/watch?v=EPe1Ho5qRuM", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph astro-ph.GA astro-ph.IM astro-ph.SR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this exploratory submission we present the visualization of the largest\ninterstellar turbulence simulations ever performed, unravelling key\nastrophysical processes concerning the formation of stars and the relative role\nof magnetic fields. The simulations, including pure hydrodynamical (HD) and\nmagneto-hydrodynamical (MHD) runs, up to a size of $10048^3$ grid elements,\nwere produced on the supercomputers of the Leibniz Supercomputing Centre and\nvisualized using the hybrid parallel (MPI+TBB) ray-tracing engine OSPRay\nassociated with VisIt. Besides revealing features of turbulence with an\nunprecedented resolution, the visualizations brilliantly showcase the\nstretching-and-folding mechanisms through which astrophysical processes such as\nsupernova explosions drive turbulence and amplify the magnetic field in the\ninterstellar gas, and how the first structures, the seeds of newborn stars are\nshaped by this process.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:16:48 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Cielo", "Salvatore", ""], ["Iapichino", "Luigi", ""], ["G\u00fcnther", "Johannes", ""], ["Federrath", "Christoph", ""], ["Mayer", "Elisabeth", ""], ["Wiedemann", "Markus", ""]]}, {"id": "1910.07855", "submitter": "Salvatore Cielo", "authors": "Salvatore Cielo, Luigi Iapichino, Fabio Baruffa", "title": "Speeding simulation analysis up with yt and Intel Distribution for\n  Python", "comments": "3 pages, 1 figure, published on Intel Parallel Universe Magazine", "journal-ref": "Issue 38, 2019, p. 27-32", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As modern scientific simulations grow ever more in size and complexity, even\ntheir analysis and post-processing becomes increasingly demanding, calling for\nthe use of HPC resources and methods. yt is a parallel, open source\npost-processing python package for numerical simulations in astrophysics, made\npopular by its cross-format compatibility, its active community of developers\nand its integration with several other professional Python instruments. The\nIntel Distribution for Python enhances yt's performance and parallel\nscalability, through the optimization of lower-level libraries Numpy and Scipy,\nwhich make use of the optimized Intel Math Kernel Library (Intel-MKL) and the\nIntel MPI library for distributed computing. The library package yt is used for\nseveral analysis tasks, including integration of derived quantities, volumetric\nrendering, 2D phase plots, cosmological halo analysis and production of\nsynthetic X-ray observation. In this paper, we provide a brief tutorial for the\ninstallation of yt and the Intel Distribution for Python, and the execution of\neach analysis task. Compared to the Anaconda python distribution, using the\nprovided solution one can achieve net speedups up to 4.6x on Intel Xeon\nScalable processors (codename Skylake).\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:28:46 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Cielo", "Salvatore", ""], ["Iapichino", "Luigi", ""], ["Baruffa", "Fabio", ""]]}, {"id": "1910.08951", "submitter": "Kleomenis Katevas", "authors": "Matteo Varvello, Kleomenis Katevas, Mihai Plesa, Hamed Haddadi,\n  Benjamin Livshits", "title": "BatteryLab, A Distributed Power Monitoring Platform For Mobile Devices", "comments": "8 pages, 8 figures, HotNets 2019 paper", "journal-ref": "HotNets 2019", "doi": "10.1145/3365609.3365852", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in cloud computing have simplified the way that both software\ndevelopment and testing are performed. Unfortunately, this is not true for\nbattery testing for which state of the art test-beds simply consist of one\nphone attached to a power meter. These test-beds have limited resources,\naccess, and are overall hard to maintain; for these reasons, they often sit\nidle with no experiment to run. In this paper, we propose to share existing\nbattery testing setups and build BatteryLab, a distributed platform for battery\nmeasurements. Our vision is to transform independent battery testing setups\ninto vantage points of a planetary-scale measurement platform offering\nheterogeneous devices and testing conditions. In the paper, we design and\ndeploy a combination of hardware and software solutions to enable BatteryLab's\nvision. We then preliminarily evaluate BatteryLab's accuracy of battery\nreporting, along with some system benchmarking. We also demonstrate how\nBatteryLab can be used by researchers to investigate a simple research\nquestion.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 11:23:34 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Varvello", "Matteo", ""], ["Katevas", "Kleomenis", ""], ["Plesa", "Mihai", ""], ["Haddadi", "Hamed", ""], ["Livshits", "Benjamin", ""]]}, {"id": "1910.09277", "submitter": "Zhi Zhang", "authors": "Zhi Zhang, Yueqiang Cheng", "title": "PiBooster: A Light-Weight Approach to Performance Improvements in Page\n  Table Management for Paravirtual Virtual-Machines", "comments": null, "journal-ref": null, "doi": "10.1109/CLOUD.2016.0074", "report-no": null, "categories": "cs.OS cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In paravirtualization, the page table management components of the guest\noperating systems are properly patched for the security guarantees of the\nhypervisor. However, none of them pay enough attentions to the performance\nimprovements, which results in two noticeable performance issues. First, such\nsecurity patches exacerbate the problem that the execution paths of the guest\npage table (de)allocations become extremely long, which would consequently\nincrease the latencies of process creations and exits. Second, the patches\nintroduce many additional IOTLB flushes, leading to extra IOTLB misses, and the\nmisses would have negative impacts on I/O performance of all peripheral\ndevices. In this paper, we propose PiBooster, a novel lightweight approach for\nimproving the performance in page table management. First, PiBooster shortens\nthe execution paths of the page table (de)allocations by the PiBooster cache,\nwhich maintains dedicated buffers for serving page table (de)allocations.\nSecond, PiBooster eliminates the additional IOTLB misses with a fine-grained\nvalidation scheme, which performs page table and DMA validations separately,\ninstead of doing both together. We implement a prototype on Xen with Linux as\nthe guest kernel. We do small modifications on Xen (166 SLoC) and Linux kernel\n(350 SLoC). We evaluate the I/O performance in both micro and macro ways. The\nmicro experiment results indicate that PiBooster is able to completely\neliminate the additional IOTLB flushes in the workload-stable environments, and\neffectively reduces (de)allocation time of the page table by 47% on average.\nThe macro benchmarks show that the latencies of the process creations and exits\nare expectedly reduced by 16% on average. Moreover, the SPECINT,lmbench and\nnetperf results indicate that PiBooster has no negative performance impacts on\nCPU computation, network I/O, and disk I/O.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 11:59:01 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhang", "Zhi", ""], ["Cheng", "Yueqiang", ""]]}, {"id": "1910.09602", "submitter": "Martin Zubeldia", "authors": "Martin Zubeldia", "title": "Delay-optimal policies in partial fork-join systems with redundancy and\n  random slowdowns", "comments": "69 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a large distributed service system consisting of $n$ homogeneous\nservers with infinite capacity FIFO queues. Jobs arrive as a Poisson process of\nrate $\\lambda n/k_n$ (for some positive constant $\\lambda$ and integer $k_n$).\nEach incoming job consists of $k_n$ identical tasks that can be executed in\nparallel, and that can be encoded into at least $k_n$ \"replicas\" of the same\nsize (by introducing redundancy) so that the job is considered to be completed\nwhen any $k_n$ replicas associated with it finish their service. Moreover, we\nassume that servers can experience random slowdowns in their processing rate so\nthat the service time of a replica is the product of its size and a random\nslowdown.\n  First, we assume that the server slowdowns are shifted exponential and\nindependent of the replica sizes. In this setting we show that the delay of a\ntypical job is asymptotically minimized (as $n\\to\\infty$) when the number of\nreplicas per task is a constant that only depends on the arrival rate\n$\\lambda$, and on the expected slowdown of servers.\n  Second, we introduce a new model for the server slowdowns in which larger\ntasks experience less variable slowdowns than smaller tasks. In this setting we\nshow that, under the class of policies where all replicas start their service\nat the same time, the delay of a typical job is asymptotically minimized (as\n$n\\to\\infty$) when the number of replicas per task is made to depend on the\nactual size of the tasks being replicated, with smaller tasks being replicated\nmore than larger tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:52:53 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zubeldia", "Martin", ""]]}, {"id": "1910.10234", "submitter": "Kunal Kishore Korgaonkar", "authors": "Kunal Korgaonkar, Ronny Ronen, Anupam Chattopadhyay, Shahar Kvatinsky", "title": "The Bitlet Model: Defining a Litmus Test for the Bitwise\n  Processing-in-Memory Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an analytical modeling tool called Bitlet that can be\nused, in a parameterized fashion, to understand the affinity of workloads to\nprocessing-in-memory (PIM) as opposed to traditional computing. The tool\nuncovers interesting trade-offs between operation complexity (cycles required\nto perform an operation through PIM) and other key parameters, such as system\nmemory bandwidth, data transfer size, the extent of data alignment, and\neffective memory capacity involved in PIM computations. Despite its simplicity,\nthe model has already proven useful. In the future, we intend to extend and\nrefine Bitlet to further increase its utility.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 21:14:16 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Korgaonkar", "Kunal", ""], ["Ronen", "Ronny", ""], ["Chattopadhyay", "Anupam", ""], ["Kvatinsky", "Shahar", ""]]}, {"id": "1910.11184", "submitter": "Yajun Zhao", "authors": "Yajun Zhao, Juan Liu, Saijin Xie", "title": "5G, OFDM, Cubic Metric, NR-U, Occupied Channel Bandwidth, CAZAC,\n  Zadoff-Chu Sequence", "comments": "20 pages, 12 figures, CIC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In NR-based Access to Unlicensed Spectrum (NR-U) of 5G system, to satisfy the\nrules of Occupied Channel Bandwidth (OCB) of unlicensed spectrum, the channels\nof PRACH and PUCCH have to use some sequence repetition mechanisms in frequency\ndomain. These repetition mechanisms will cause serious cubic metric(CM)\nproblems for these channels, although these two types of channels are composed\nof Constant Amplitude Zero Auto-correlation(CAZAC) sequences.. Based on the\ncharacteristics of CAZAC sequences, which are used for PRACH and PUCCH (refer\nto PUCCH format 0 and format 1) in 5G NR, in this paper, we propose some new\nmechanisms of CM reduction for these two types of channels considering the\ndesign principles to ensure the sequence performance of the auto-correlation\nand cross-correlation. Then the proposed CM schemes are evaluated and the\noptimized parameters are further provided considering CM performance and the\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 23:55:52 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 06:28:57 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Zhao", "Yajun", ""], ["Liu", "Juan", ""], ["Xie", "Saijin", ""]]}, {"id": "1910.11927", "submitter": "Ajay Shrestha", "authors": "Ajay Kumar Shrestha, Julita Vassileva", "title": "User Data Sharing Frameworks: A Blockchain-Based Incentive Solution", "comments": "7 pages, 4 figure, 1 table, IEEE IEMCON 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DB cs.GT cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, there is no universal method to track who shared what, with whom,\nwhen and for what purposes in a verifiable way to create an individual\nincentive for data owners. A platform that allows data owners to control,\ndelete, and get rewards from sharing their data would be an important enabler\nof user data-sharing. We propose a usable blockchain- and smart contracts-based\nframework that allows users to store research data locally and share without\nlosing control and ownership of it. We have created smart contracts for\nbuilding automatic verification of the conditions for data access that also\nnaturally supports building up a verifiable record of the provenance,\nincentives for users to share their data and accountability of access. The\npaper presents a review of the existing work of research data sharing, the\nproposed blockchain-based framework and an evaluation of the framework by\nmeasuring the transaction cost for smart contracts deployment. The results show\nthat nodes responded quickly in all tested cases with a befitting transaction\ncost.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 20:39:43 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Shrestha", "Ajay Kumar", ""], ["Vassileva", "Julita", ""]]}, {"id": "1910.12562", "submitter": "Michael Backenk\\\"ohler", "authors": "Michael Backenk\\\"ohler, Luca Bortolussi, Verena Wolf", "title": "Bounding Mean First Passage Times in Population Continuous-Time Markov\n  Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.PF cs.SY q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of bounding mean first passage times for a class of\ncontinuous-time Markov chains that captures stochastic interactions between\ngroups of identical agents. The quantitative analysis of such probabilistic\npopulation models is notoriously difficult since typically neither state-based\nnumerical approaches nor methods based on stochastic sampling give efficient\nand accurate results. Here, we propose a technique that extends recently\ndeveloped methods using semi-definite programming to determine bounds on mean\nfirst passage times. We further apply the technique to hybrid models and\ndemonstrate its accuracy and efficiency for some examples from biology.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 11:21:58 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 11:43:43 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Backenk\u00f6hler", "Michael", ""], ["Bortolussi", "Luca", ""], ["Wolf", "Verena", ""]]}, {"id": "1910.12894", "submitter": "Anmol Kagrecha", "authors": "Anmol Kagrecha, Jayakrishnan Nair", "title": "Please come back later: Benefiting from deferrals in service systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance evaluation of loss service systems, where customers who\ncannot be served upon arrival get dropped, has a long history going back to the\nclassical Erlang B model. In this paper, we consider the performance benefits\narising from the possibility of deferring customers who cannot be served upon\narrival. Specifically, we consider an Erlang B type loss system where the\nsystem operator can, subject to certain constraints, ask a customer arriving\nwhen all servers are busy, to come back at a specified time in the future. If\nthe system is still fully loaded when the deferred customer returns, she gets\ndropped for good. For such a system, we ask: How should the system operator\ndetermine the rearrival times of the deferred customers based on the state of\nthe system (which includes those customers already deferred and yet to arrive)?\nHow does one quantify the performance benefit of such a deferral policy? Our\ncontributions are as follows. We propose a simple state-dependent policy for\ndetermining the rearrival times of deferred customers. For this policy, we\ncharacterize the long run fraction of customers dropped. We also analyse a\nrelaxation where the deferral times are bounded in expectation. Via extensive\nnumerical evaluations, we demonstrate the superiority of the proposed\nstate-dependent policies over naive state-independent deferral policies.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:02:32 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Kagrecha", "Anmol", ""], ["Nair", "Jayakrishnan", ""]]}, {"id": "1910.13346", "submitter": "Changxi Liu", "authors": "Changxi Liu and Hailong Yang and Xu Liu and Zhongzhi Luan and Depei\n  Qian", "title": "Intelligent-Unrolling: Exploiting Regular Patterns in Irregular\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern optimizing compilers are able to exploit memory access or computation\npatterns to generate vectorization codes. However, such patterns in irregular\napplications are unknown until runtime due to the input dependence. Thus,\neither compiler's static optimization or profile-guided optimization based on\nspecific inputs cannot predict the patterns for any common input, which leads\nto suboptimal code generation. To address this challenge, we develop\nIntelligent-Unroll, a framework to automatically optimize irregular\napplications with vectorization. Intelligent-Unroll allows the users to depict\nthe computation task using \\textit{code seed} with the memory access and\ncomputation patterns represented in \\textit{feature table} and\n\\textit{information-code tree}, and generates highly efficient codes.\nFurthermore, Intelligent-Unroll employs several novel optimization techniques\nto optimize reduction operations and gather/scatter instructions. We evaluate\nIntelligent-Unroll with sparse matrix-vector multiplication (SpMV) and graph\napplications. Experimental results show that Intelligent-Unroll is able to\ngenerate more efficient vectorization codes compared to the state-of-the-art\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 04:23:49 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Liu", "Changxi", ""], ["Yang", "Hailong", ""], ["Liu", "Xu", ""], ["Luan", "Zhongzhi", ""], ["Qian", "Depei", ""]]}, {"id": "1910.14496", "submitter": "David Goz Dr.", "authors": "D. Goz, G. Ieronymakis, V. Papaefstathiou, N. Dimou, S. Bertocco, A.\n  Ragagnin, L. Tornatore, G. Taffoni, I. Coretti", "title": "Direct N-body application on low-power and energy-efficient parallel\n  architectures", "comments": "10 pages, 5 figure, 2 tables; The final publication will be available\n  at IOS Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to quantitatively evaluate the impact of computation\non the energy consumption on ARM MPSoC platforms, exploiting CPUs, embedded\nGPUs and FPGAs. One of them possibly represents the future of High Performance\nComputing systems: a prototype of an Exascale supercomputer. Performance and\nenergy measurements are made using a state-of-the-art direct $N$-body code from\nthe astrophysical domain. We provide a comparison of the time-to-solution and\nenergy delay product metrics, for different software configurations. We have\nshown that FPGA technologies can be used for application kernel acceleration\nand are emerging as a promising alternative to \"traditional\" technologies for\nHPC, which purely focus on peak-performance than on power-efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 14:33:30 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Goz", "D.", ""], ["Ieronymakis", "G.", ""], ["Papaefstathiou", "V.", ""], ["Dimou", "N.", ""], ["Bertocco", "S.", ""], ["Ragagnin", "A.", ""], ["Tornatore", "L.", ""], ["Taffoni", "G.", ""], ["Coretti", "I.", ""]]}, {"id": "1910.14546", "submitter": "Ajay Rajan", "authors": "Bharath Honnesara Sreenivasa, Ajay Rajan", "title": "Debian Package usage profiler for Debian based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The embedded devices of today due to their CPU, RAM capabilities can run\nvarious Linux distributions but in most cases they are different from general\npurpose distributions as they are usually lighter and specific to the needs of\nthat particular system. In this project, we share the problems associated in\nadopting a fully heavy-weight Debian based system like Ubuntu in\nembedded/automotive platforms and provide solutions to optimize them to\nidentify unused/redundant content in the system. This helps developer to reduce\nthe hefty general purpose distribution to an application specific distribution.\nThe solution involves collecting usage data in the system in a non-invasive\nmanner (to avoid any drop in performance) to suggest users the redundant,\nunused parts of the system that can be safely removed without impacting the\nsystem functionality.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 15:47:35 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Sreenivasa", "Bharath Honnesara", ""], ["Rajan", "Ajay", ""]]}]