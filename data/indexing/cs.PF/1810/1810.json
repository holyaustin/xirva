[{"id": "1810.00169", "submitter": "Mohammad Noormohammadpour", "authors": "Mohammad Noormohammadpour, Ajitesh Srivastava, Cauligi S. Raghavendra", "title": "On Minimizing the Completion Times of Long Flows over Inter-Datacenter\n  WAN", "comments": "Accepted for publication in IEEE Communications Letters", "journal-ref": "IEEE Communications Letters 22 (2018) 2475 - 2478", "doi": "10.1109/LCOMM.2018.2872980", "report-no": null, "categories": "cs.NI cs.DS cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long flows contribute huge volumes of traffic over inter-datacenter WAN. The\nFlow Completion Time (FCT) is a vital network performance metric that affects\nthe running time of distributed applications and the users' quality of\nexperience. Flow routing techniques based on propagation or queuing latency or\ninstantaneous link utilization are insufficient for minimization of the long\nflows' FCT. We propose a routing approach that uses the remaining sizes and\npaths of all ongoing flows to minimize the worst-case completion time of\nincoming flows assuming no knowledge of future flow arrivals. Our approach can\nbe formulated as an NP-Hard graph optimization problem. We propose BWRH, a\nheuristic to quickly generate an approximate solution. We evaluate BWRH against\nseveral real WAN topologies and two different traffic patterns. We see that\nBWRH provides solutions with an average optimality gap of less than $0.25\\%$.\nFurthermore, we show that compared to other popular routing heuristics, BWRH\nreduces the mean and tail FCT by up to $1.46\\times$ and $1.53\\times$,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 07:49:10 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Noormohammadpour", "Mohammad", ""], ["Srivastava", "Ajitesh", ""], ["Raghavendra", "Cauligi S.", ""]]}, {"id": "1810.00596", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo, Stefano Ferretti, Moreno Marzolla", "title": "Fault Tolerant Adaptive Parallel and Distributed Simulation through\n  Functional Replication", "comments": "arXiv admin note: substantial text overlap with arXiv:1606.07310", "journal-ref": "Simulation Modelling Practice and Theory, Elsevier, vol. 93 (May\n  2019)", "doi": "10.1016/j.simpat.2018.09.012", "report-no": null, "categories": "cs.DC cs.MA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents FT-GAIA, a software-based fault-tolerant parallel and\ndistributed simulation middleware. FT-GAIA has being designed to reliably\nhandle Parallel And Distributed Simulation (PADS) models, which are needed to\nproperly simulate and analyze complex systems arising in any kind of scientific\nor engineering field. PADS takes advantage of multiple execution units run in\nmulticore processors, cluster of workstations or HPC systems. However, large\ncomputing systems, such as HPC systems that include hundreds of thousands of\ncomputing nodes, have to handle frequent failures of some components. To cope\nwith this issue, FT-GAIA transparently replicates simulation entities and\ndistributes them on multiple execution nodes. This allows the simulation to\ntolerate crash-failures of computing nodes. Moreover, FT-GAIA offers some\nprotection against Byzantine failures, since interaction messages among the\nsimulated entities are replicated as well, so that the receiving entity can\nidentify and discard corrupted messages. Results from an analytical model and\nfrom an experimental evaluation show that FT-GAIA provides a high degree of\nfault tolerance, at the cost of a moderate increase in the computational load\nof the execution units.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 09:51:46 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 08:56:49 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Marzolla", "Moreno", ""]]}, {"id": "1810.00988", "submitter": "George Kesidis", "authors": "Yuquan Shan, George Kesidis, Bhuvan Urgaonkar, Jorg Schad, Jalal\n  Khamse-Ashari, Ioannis Lambadaris", "title": "Heterogeneous MacroTasking (HeMT) for Parallel Processing in the Public\n  Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using tiny, equal-sized tasks (Homogeneous microTasking, HomT) has long been\nregarded an effective way of load balancing in parallel computing systems. When\ncombined with nodes pulling in work upon becoming idle, HomT has the desirable\nproperty of automatically adapting its load distribution to the processing\ncapacities of participating nodes - more powerful nodes finish their work\nsooner and, therefore, pull in additional work faster. As a result, HomT is\ndeemed especially desirable in settings with heterogeneous (and possibly\npossessing dynamically changing) processing capacities. However, HomT does have\nadditional scheduling and I/O overheads that might make this load balancing\nscheme costly in some scenarios. In this paper, we first analyze these\nadvantages and disadvantages of HomT. We then propose an alternative load\nbalancing scheme - Heterogeneous MacroTasking (HeMT) - wherein workload is\nintentionally partitioned according to nodes' processing capacity. Our goal is\nto study when HeMT is able to overcome the performance disadvantages of HomT.\nWe implement a prototype of HeMT within the Apache Spark application framework\nwith complementary enhancements to the Apache Mesos cluster manager. Spark's\nbuilt-in scheduler, when parameterized appropriately, implements HomT. Our\nexperimental results show that HeMT out-performs HomT when accurate\nworkload-specific estimates of nodes' processing capacities are learned. As\nrepresentative results, Spark with HeMT offers about 10% better average\ncompletion times for realistic data processing workloads over the default\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 21:36:51 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Shan", "Yuquan", ""], ["Kesidis", "George", ""], ["Urgaonkar", "Bhuvan", ""], ["Schad", "Jorg", ""], ["Khamse-Ashari", "Jalal", ""], ["Lambadaris", "Ioannis", ""]]}, {"id": "1810.01540", "submitter": "Julio A. Reyes-Munoz", "authors": "Julio A. Reyes-Munoz and Michael McGarry", "title": "The Effect of Data Marshalling on Computation Offloading Decisions", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conducted an extensive set of experiments with an offloading testbed to\nunderstand the impact that data marshalling techniques have on computation\noffloading decisions. We find that the popular JSON format to marshall data\nbetween client and server comes at a significant computational expense compared\nto a minimalistic raw data transfer. The computational time is significant in\nthat it affects computation offloading decisions in a variety of conditions. We\noutline some of these conditions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 23:32:32 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Reyes-Munoz", "Julio A.", ""], ["McGarry", "Michael", ""]]}, {"id": "1810.03488", "submitter": "Albin Severinson", "authors": "Albin Severinson, Alexandre Graell i Amat, Eirik Rosnes, Francisco\n  Lazaro, and Gianluigi Liva", "title": "A Droplet Approach Based on Raptor Codes for Distributed Computing With\n  Straggling Servers", "comments": "Accepted at the 2018 International Symposium on Turbo Codes &\n  Iterative Information Processing, Hong Kong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a coded distributed computing scheme based on Raptor codes to\naddress the straggler problem. In particular, we consider a scheme where each\nserver computes intermediate values, referred to as droplets, that are either\nstored locally or sent over the network. Once enough droplets are collected,\nthe computation can be completed. Compared to previous schemes in the\nliterature, our proposed scheme achieves lower computational delay when the\ndecoding time is taken into account.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 14:23:27 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Severinson", "Albin", ""], ["Amat", "Alexandre Graell i", ""], ["Rosnes", "Eirik", ""], ["Lazaro", "Francisco", ""], ["Liva", "Gianluigi", ""]]}, {"id": "1810.04063", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, Roshan G. Ragel, Dhammike Elkaduwe", "title": "To Use or Not to Use: CPUs' Cache Optimization Techniques on GPGPUs", "comments": "6 pages, 15 Figures", "journal-ref": "ICIAfS 2016- IEEE International Conference on Information and\n  Automation for Sustainability", "doi": "10.1109/ICIAFS.2016.7946534", "report-no": null, "categories": "cs.DC cs.CC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General Purpose Graphic Processing Unit(GPGPU) is used widely for achieving\nhigh performance or high throughput in parallel programming. This capability of\nGPGPUs is very famous in the new era and mostly used for scientific computing\nwhich requires more processing power than normal personal computers. Therefore,\nmost of the programmers, researchers and industry use this new concept for\ntheir work. However, achieving high-performance or high-throughput using GPGPUs\nare not an easy task compared with conventional programming concepts in the CPU\nside. In this research, the CPU's cache memory optimization techniques have\nbeen adopted to the GPGPU's cache memory to identify rare performance\nimprovement techniques compared to GPGPU's best practices. The cache\noptimization techniques of blocking, loop fusion, array merging and array\ntranspose were tested on GPGPUs for finding suitability of these techniques.\nFinally, we identified that some of the CPU cache optimization techniques go\nwell with the cache memory system of the GPGPU and shows performance\nimprovements while some others show the opposite effect on the GPGPUs compared\nwith the CPUs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 15:10:33 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Thambawita", "Vajira", ""], ["Ragel", "Roshan G.", ""], ["Elkaduwe", "Dhammike", ""]]}, {"id": "1810.04413", "submitter": "Tam\\'as B\\'ela Feh\\'er", "authors": "T. B. Feh\\'er, M. H\\\"olzl, G. Latu, G.T.A. Huijsmans", "title": "Performance analysis and optimization of the JOREK code for many-core\n  CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report investigates the performance of the JOREK code on the Intel\nKnights Landing and Skylake processor architectures. The OpenMP scaling of the\nmatrix construction part of the code was analyzed and improved synchronization\nmethods were implemented. A new switch was implemented to control the number of\nthreads used for the linear equation solver independently from other parts of\nthe code. The matrix construction subroutine was vectorized, and the data\nlocality was also improved. These steps led to a factor of two speedup for the\nmatrix construction.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 08:34:56 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Feh\u00e9r", "T. B.", ""], ["H\u00f6lzl", "M.", ""], ["Latu", "G.", ""], ["Huijsmans", "G. T. A.", ""]]}, {"id": "1810.04509", "submitter": "Zhi Lin Ke", "authors": "Zhi-Lin Ke, Hsiang-Yun Cheng, Chia-Lin Yang", "title": "LIRS: Enabling efficient machine learning on NVM-based storage via a\n  lightweight implementation of random shuffling", "comments": null, "journal-ref": null, "doi": "10.6342/NTU201803514", "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms, such as Support Vector Machine (SVM) and Deep\nNeural Network (DNN), have gained a lot of interests recently. When training a\nmachine learning algorithm, randomly shuffle all the training data can improve\nthe testing accuracy and boost the convergence rate. Nevertheless, realizing\ntraining data random shuffling in a real system is not a straightforward\nprocess due to the slow random accesses in hard disk drive (HDD). To avoid\nfrequent random disk access, the effect of random shuffling is often limited in\nexisting approaches. With the emerging non-volatile memory-based storage\ndevice, such as Intel Optane SSD, which provides fast random accesses, we\npropose a lightweight implementation of random shuffling (LIRS) to randomly\nshuffle the indexes of the entire training dataset, and the selected training\ninstances are directly accessed from the storage and packed into batches.\nExperimental results show that LIRS can reduce the total training time of SVM\nand DNN by 49.9% and 43.5% on average, and improve the final testing accuracy\non DNN by 1.01%.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 13:22:47 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Ke", "Zhi-Lin", ""], ["Cheng", "Hsiang-Yun", ""], ["Yang", "Chia-Lin", ""]]}, {"id": "1810.04610", "submitter": "Andreas Abel", "authors": "Andreas Abel and Jan Reineke", "title": "uops.info: Characterizing Latency, Throughput, and Port Usage of\n  Instructions on Intel Microarchitectures", "comments": null, "journal-ref": null, "doi": "10.1145/3297858.3304062", "report-no": null, "categories": "cs.PF cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern microarchitectures are some of the world's most complex man-made\nsystems. As a consequence, it is increasingly difficult to predict, explain,\nlet alone optimize the performance of software running on such\nmicroarchitectures. As a basis for performance predictions and optimizations,\nwe would need faithful models of their behavior, which are, unfortunately,\nseldom available.\n  In this paper, we present the design and implementation of a tool to\nconstruct faithful models of the latency, throughput, and port usage of x86\ninstructions. To this end, we first discuss common notions of instruction\nthroughput and port usage, and introduce a more precise definition of latency\nthat, in contrast to previous definitions, considers dependencies between\ndifferent pairs of input and output operands. We then develop novel algorithms\nto infer the latency, throughput, and port usage based on\nautomatically-generated microbenchmarks that are more accurate and precise than\nexisting work.\n  To facilitate the rapid construction of optimizing compilers and tools for\nperformance prediction, the output of our tool is provided in a\nmachine-readable format. We provide experimental results for processors of all\ngenerations of Intel's Core architecture, i.e., from Nehalem to Coffee Lake,\nand discuss various cases where the output of our tool differs considerably\nfrom prior work.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 16:13:31 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 01:42:17 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 18:10:14 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Abel", "Andreas", ""], ["Reineke", "Jan", ""]]}, {"id": "1810.04875", "submitter": "Celine Comte", "authors": "Anne Bouillard, C\\'eline Comte, \\'Elie De Panafieu, Fabien Mathieu", "title": "Of Kernels and Queues: when network calculus meets analytic\n  combinatorics", "comments": null, "journal-ref": "NetCal 2018, Sep 2018, Vienne, Austria", "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic network calculus is a tool for computing error bounds on the\nperformance of queueing systems. However, deriving accurate bounds for networks\nconsisting of several queues or subject to non-independent traffic inputs is\nchallenging. In this paper, we investigate the relevance of the tools from\nanalytic combinatorics, especially the kernel method, to tackle this problem.\nApplying the kernel method allows us to compute the generating functions of the\nqueue state distributions in the stationary regime of the network. As a\nconsequence, error bounds with an arbitrary precision can be computed. In this\npreliminary work, we focus on simple examples which are representative of the\ndifficulties that the kernel method allows us to overcome.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 07:41:32 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Bouillard", "Anne", ""], ["Comte", "C\u00e9line", ""], ["De Panafieu", "\u00c9lie", ""], ["Mathieu", "Fabien", ""]]}, {"id": "1810.06008", "submitter": "Pier Luigi Ventre", "authors": "Pier Luigi Ventre, Mohammad Mahdi Tajiki, Stefano Salsano and Clarence\n  Filsfils", "title": "SDN Architecture and Southbound APIs for IPv6 Segment Routing Enabled\n  Wide Area Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TNSM.2018.2876251", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SRv6 architecture (Segment Routing based on IPv6 data plane) is a\npromising solution to support services like Traffic Engineering, Service\nFunction Chaining and Virtual Private Networks in IPv6 backbones and\ndatacenters. The SRv6 architecture has interesting scalability properties as it\nreduces the amount of state information that needs to be configured in the\nnodes to support the network services. In this paper, we describe the\nadvantages of complementing the SRv6 technology with an SDN based approach in\nbackbone networks. We discuss the architecture of a SRv6 enabled network based\non Linux nodes. In addition, we present the design and implementation of the\nSouthbound API between the SDN controller and the SRv6 device. We have defined\na data-model and four different implementations of the API, respectively based\non gRPC, REST, NETCONF and remote Command Line Interface (CLI). Since it is\nimportant to support both the development and testing aspects we have realized\nan Intent based emulation system to build realistic and reproducible\nexperiments. This collection of tools automate most of the configuration\naspects relieving the experimenter from a significant effort. Finally, we have\nrealized an evaluation of some performance aspects of our architecture and of\nthe different variants of the Southbound APIs and we have analyzed the effects\nof the configuration updates in the SRv6 enabled nodes.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 10:02:23 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Ventre", "Pier Luigi", ""], ["Tajiki", "Mohammad Mahdi", ""], ["Salsano", "Stefano", ""], ["Filsfils", "Clarence", ""]]}, {"id": "1810.06659", "submitter": "Yifan Wang", "authors": "Yifan Wang, Shaoshan Liu, Xiaopei Wu, Weisong Shi", "title": "CAVBench: A Benchmark Suite for Connected and Autonomous Vehicles", "comments": "13 pages, The Third ACM/IEEE Symposium on Edge Computing 2018 SEC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected and autonomous vehicles (CAVs) have recently attracted a\nsignificant amount of attention both from researchers and industry. Numerous\nstudies targeting algorithms, software frameworks, and applications on the CAVs\nscenario have emerged. Meanwhile, several pioneer efforts have focused on the\nedge computing system and architecture design for the CAVs scenario and\nprovided various heterogeneous platform prototypes for CAVs. However, a\nstandard and comprehensive application benchmark for CAVs is missing, hindering\nthe study of these emerging computing systems. To address this challenging\nproblem, we present CAVBench, the first benchmark suite for the edge computing\nsystem in the CAVs scenario. CAVBench is comprised of six typical applications\ncovering four dominate CAVs scenarios and takes four datasets as standard\ninput. CAVBench provides quantitative evaluation results via application and\nsystem perspective output metrics. We perform a series of experiments and\nacquire three systemic characteristics of the applications in CAVBench. First,\nthe operation intensity of the applications is polarized, which explains why\nheterogeneous hardware is important for a CAVs computing system. Second, all\napplications in CAVBench consume high memory bandwidth, so the system should be\nequipped with high bandwidth memory or leverage good memory bandwidth\nmanagement to avoid the performance degradation caused by memory bandwidth\ncompetition. Third, some applications have worse data/instruction locality\nbased on the cache miss observation, so the computing system targeting these\napplications should optimize the cache architecture. Last, we use the CAVBench\nto evaluate a typical edge computing platform and present the quantitative and\nqualitative analysis of the benchmarking results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 20:07:33 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Wang", "Yifan", ""], ["Liu", "Shaoshan", ""], ["Wu", "Xiaopei", ""], ["Shi", "Weisong", ""]]}, {"id": "1810.07730", "submitter": "Behnam Dezfouli", "authors": "Puneet Kumar and Behnam Dezfouli", "title": "Implementation and Analysis of QUIC for MQTT", "comments": "SCU Internet of Things Research Lab", "journal-ref": null, "doi": null, "report-no": "SCU-SIOTLAB-MQTTQUIC-Octobe2018", "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transport and security protocols are essential to ensure reliable and secure\ncommunication between two parties. For IoT applications, these protocols must\nbe lightweight, since IoT devices are usually resource constrained.\nUnfortunately, the existing transport and security protocols -- namely TCP/TLS\nand UDP/DTLS -- fall short in terms of connection overhead, latency, and\nconnection migration when used in IoT applications. In this paper, after\nstudying the root causes of these shortcomings, we show how utilizing QUIC in\nIoT scenarios results in a higher performance. Based on these observations, and\ngiven the popularity of MQTT as an IoT application layer protocol, we integrate\nMQTT with QUIC. By presenting the main APIs and functions developed, we explain\nhow connection establishment and message exchange functionalities work. We\nevaluate the performance of MQTTw/QUIC versus MQTTw/TCP using wired, wireless,\nand long-distance testbeds. Our results show that MQTTw/QUIC reduces connection\noverhead in terms of the number of packets exchanged with the broker by up to\n56%. In addition, by eliminating half-open connections, MQTTw/QUIC reduces\nprocessor and memory usage by up to 83% and 50%, respectively. Furthermore, by\nremoving the head-of-line blocking problem, delivery latency is reduced by up\nto 55%. We also show that the throughput drops experienced by MQTTw/QUIC when a\nconnection migration happens is considerably lower than that of MQTTw/TCP.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 18:28:12 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 20:44:07 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Kumar", "Puneet", ""], ["Dezfouli", "Behnam", ""]]}, {"id": "1810.07782", "submitter": "Manu Kumar Gupta", "authors": "Urtzi Ayesta, Manu K Gupta, Ina Maria Verloop", "title": "Load balancing with heterogeneous schedulers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load balancing is a common approach in web server farms or inventory routing\nproblems. An important issue in such systems is to determine the server to\nwhich an incoming request should be routed to optimize a given performance\ncriteria. In this paper, we assume the server's scheduling disciplines to be\nheterogeneous. More precisely, a server implements a scheduling discipline\nwhich belongs to the class of limited processor sharing (LPS-$d$) scheduling\ndisciplines. Under LPS-$d$, up to $d$ jobs can be served simultaneously, and\nhence, includes as special cases First Come First Served ($d=1$) and Processor\nSharing ($d=\\infty$).\n  In order to obtain efficient heuristics, we model the above load-balancing\nframework as a multi-armed restless bandit problem. Using the relaxation\ntechnique, as first developed in the seminal work of Whittle, we derive\nWhittle's index policy for general cost functions and obtain a closed-form\nexpression for Whittle's index in terms of the steady-state distribution.\nThrough numerical computations, we investigate the performance of Whittle's\nindex with two different performance criteria: linear cost criterion and a cost\ncriterion that depends on the first and second moment of the throughput. Our\nresults show that \\emph{(i)} the structure of Whittle's index policy can\nstrongly depend on the scheduling discipline implemented in the server, i.e.,\non $d$, and that \\emph{(ii)} Whittle's index policy significantly outperforms\nstandard dispatching rules such as Join the Shortest Queue (JSQ), Join the\nShortest Expected Workload (JSEW), and Random Server allocation (RSA).\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 20:33:08 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Ayesta", "Urtzi", ""], ["Gupta", "Manu K", ""], ["Verloop", "Ina Maria", ""]]}, {"id": "1810.08899", "submitter": "Jie Ren", "authors": "Qing Qin, Jie Ren, Jialong Yu, Ling Gao, Hai Wang, Jie Zheng, Yansong\n  Feng, Jianbin Fang, Zheng Wang", "title": "To Compress, or Not to Compress: Characterizing Deep Learning Model\n  Compression for Embedded Inference", "comments": "8 pages, To appear in ISPA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in deep neural networks (DNNs) make them attractive for\nembedded systems. However, it can take a long time for DNNs to make an\ninference on resource-constrained computing devices. Model compression\ntechniques can address the computation issue of deep inference on embedded\ndevices. This technique is highly attractive, as it does not rely on\nspecialized hardware, or computation-offloading that is often infeasible due to\nprivacy concerns or high latency. However, it remains unclear how model\ncompression techniques perform across a wide range of DNNs. To design efficient\nembedded deep learning solutions, we need to understand their behaviors. This\nwork develops a quantitative approach to characterize model compression\ntechniques on a representative embedded deep learning architecture, the NVIDIA\nJetson Tx2. We perform extensive experiments by considering 11 influential\nneural network architectures from the image classification and the natural\nlanguage processing domains. We experimentally show that how two mainstream\ncompression techniques, data quantization and pruning, perform on these network\narchitectures and the implications of compression techniques to the model\nstorage size, inference time, energy consumption and performance metrics. We\ndemonstrate that there are opportunities to achieve fast deep inference on\nembedded systems, but one must carefully choose the compression settings. Our\nresults provide insights on when and how to apply model compression techniques\nand guidelines for designing efficient embedded deep learning systems.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 05:09:45 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Qin", "Qing", ""], ["Ren", "Jie", ""], ["Yu", "Jialong", ""], ["Gao", "Ling", ""], ["Wang", "Hai", ""], ["Zheng", "Jie", ""], ["Feng", "Yansong", ""], ["Fang", "Jianbin", ""], ["Wang", "Zheng", ""]]}, {"id": "1810.09300", "submitter": "Srinivasan Keshav", "authors": "S. Keshav, W. Golab, B. Wong, S. Rizvi, and S. Gorbunov", "title": "RCanopus: Making Canopus Resilient to Failures and Byzantine Faults", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed consensus is a key enabler for many distributed systems including\ndistributed databases and blockchains. Canopus is a scalable distributed\nconsensus protocol that ensures that live nodes in a system agree on an ordered\nsequence of operations (called transactions). Unlike most prior consensus\nprotocols, Canopus does not rely on a single leader. Instead, it uses a virtual\ntree overlay for message dissemination to limit network traffic across\noversubscribed links. It leverages hardware redundancies, both within a rack\nand inside the network fabric, to reduce both protocol complexity and\ncommunication overhead. These design decisions enable Canopus to support large\ndeployments without significant performance degradation.\n  The existing Canopus protocol is resilient in the face of node and\ncommunication failures, but its focus is primarily on performance, so does not\nrespond well to other types of failures. For example, the failure of a single\nrack of servers causes all live nodes to stall. The protocol is also open to\nattack by Byzantine nodes, which can cause different live nodes to conclude the\nprotocol with different transaction orders. In this paper, we describe RCanopus\n(`resilent Canopus') which extends Canopus to add liveness, that is, allowing\nlive nodes to make progress, when possible, despite many types of failures.\nThis requires RCanopus to accurately detect and recover from failure despite\nusing unreliable failure detectors, and tolerance of Byzantine attacks. Second,\nRCanopus guarantees safety, that is, agreement amongst live nodes of\ntransaction order, in the presence of Byzantine attacks and network\npartitioning.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 14:08:16 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 18:07:36 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 22:30:07 GMT"}, {"version": "v4", "created": "Sun, 16 Jun 2019 12:51:20 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Keshav", "S.", ""], ["Golab", "W.", ""], ["Wong", "B.", ""], ["Rizvi", "S.", ""], ["Gorbunov", "S.", ""]]}, {"id": "1810.09376", "submitter": "Wanling Gao", "authors": "Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Zhen Jia, Daoyi\n  Zheng, Chen Zheng, Xiwen He, Hainan Ye, Haibin Wang, and Rui Ren", "title": "Data Motif-based Proxy Benchmarks for Big Data and AI Workloads", "comments": "The paper will be published on 2018 IEEE International Symposium on\n  Workload Characterization (IISWC 2018). arXiv admin note: text overlap with\n  arXiv:1802.00699, arXiv:1711.03229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the architecture community, reasonable simulation time is a strong\nrequirement in addition to performance data accuracy. However, emerging big\ndata and AI workloads are too huge at binary size level and prohibitively\nexpensive to run on cycle-accurate simulators. The concept of data motif, which\nis identified as a class of units of computation performed on initial or\nintermediate data, is the first step towards building proxy benchmark to mimic\nthe real-world big data and AI workloads. However, there is no practical way to\nconstruct a proxy benchmark based on the data motifs to help simulation-based\nresearch. In this paper, we embark on a study to bridge the gap between data\nmotif and a practical proxy benchmark. We propose a data motif-based proxy\nbenchmark generating methodology by means of machine learning method, which\ncombine data motifs with different weights to mimic the big data and AI\nworkloads. Furthermore, we implement various data motifs using light-weight\nstacks and apply the methodology to five real-world workloads to construct a\nsuite of proxy benchmarks, considering the data types, patterns, and\ndistributions. The evaluation results show that our proxy benchmarks shorten\nthe execution time by 100s times on real systems while maintaining the average\nsystem and micro-architecture performance data accuracy above 90%, even\nchanging the input data sets or cluster configurations. Moreover, the generated\nproxy benchmarks reflect consistent performance trends across different\narchitectures. To facilitate the community, we will release the proxy\nbenchmarks on the project homepage http://prof.ict.ac.cn/BigDataBench.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:58:00 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Gao", "Wanling", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Luo", "Chunjie", ""], ["Jia", "Zhen", ""], ["Zheng", "Daoyi", ""], ["Zheng", "Chen", ""], ["He", "Xiwen", ""], ["Ye", "Hainan", ""], ["Wang", "Haibin", ""], ["Ren", "Rui", ""]]}, {"id": "1810.09898", "submitter": "Antonio Ragagnin", "authors": "Antonio Ragagnin, Nikola Tchipev, Michael Bader, Klaus Dolag, Nicolay\n  J. Hammer", "title": "Exploiting the Space Filling Curve Ordering of Particles in the\n  Neighbour Search of Gadget3", "comments": "17 pages, 6 figures, published at Parallel Computing (ParCo)", "journal-ref": null, "doi": "10.3233/978-1-61499-621-7-411", "report-no": null, "categories": "astro-ph.IM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gadget3 is nowadays one of the most frequently used high performing parallel\ncodes for cosmological hydrodynamical simulations. Recent analyses have shown\nt\\ hat the Neighbour Search process of Gadget3 is one of the most\ntime-consuming parts. Thus, a considerable speedup can be expected from\nimprovements of the u\\ nderlying algorithms. In this work we propose a novel\napproach for speeding up the Neighbour Search which takes advantage of the\nspace-filling-curve particle ordering. Instead of performing Neighbour Search\nfor all particles individually, nearby active particles can be grouped and one\nsingle Neighbour Search can be performed to obta\\ in a common superset of\nneighbours. Thus, with this approach we reduce the number of searches. On the\nother hand, tree walks are performed within a larger searching radius. There is\nan optimal size of grouping that maximize the speedup, which we found by\nnumerical experiments. We tested the algorithm within the boxes of the\nMagneticum project. As a result we obtained a speedup of $1.65$ in the Density\nand of $1.30$ in the Hydrodynamics computation, respectively, and a total\nspeedup of $1.34.$\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 15:00:46 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Ragagnin", "Antonio", ""], ["Tchipev", "Nikola", ""], ["Bader", "Michael", ""], ["Dolag", "Klaus", ""], ["Hammer", "Nicolay J.", ""]]}, {"id": "1810.10460", "submitter": "Elliot J. Crowley", "authors": "Jack Turner, Elliot J. Crowley, Valentin Radu, Jos\\'e Cano, Amos\n  Storkey, Michael O'Boyle", "title": "Distilling with Performance Enhanced Students", "comments": "Preprint. Paper title has changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The task of accelerating large neural networks on general purpose hardware\nhas, in recent years, prompted the use of channel pruning to reduce network\nsize. However, the efficacy of pruning based approaches has since been called\ninto question. In this paper, we turn to distillation for model\ncompression---specifically, attention transfer---and develop a simple method\nfor discovering performance enhanced student networks. We combine channel\nsaliency metrics with empirical observations of runtime performance to design\nmore accurate networks for a given latency budget. We apply our methodology to\nresidual and densely-connected networks, and show that we are able to find\nresource-efficient student networks on different hardware platforms while\nmaintaining very high accuracy. These performance-enhanced student networks\nachieve up to 10% boosts in top-1 ImageNet accuracy over their channel-pruned\ncounterparts for the same inference time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 15:45:36 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 16:19:50 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Turner", "Jack", ""], ["Crowley", "Elliot J.", ""], ["Radu", "Valentin", ""], ["Cano", "Jos\u00e9", ""], ["Storkey", "Amos", ""], ["O'Boyle", "Michael", ""]]}, {"id": "1810.10496", "submitter": "Ricardo Nobre", "authors": "Ricardo Nobre, Lu\\'is Reis and Jo\\~ao M. P. Cardoso", "title": "Improving OpenCL Performance by Specializing Compiler Phase Selection\n  and Ordering", "comments": "26 pages, 7 figures, Extended version of paper presented at 15th\n  International Workshop on Algorithms, Models and Tools for Parallel Computing\n  on Heterogeneous Platforms (HeteroPar'2017), August 28th, 2017, Santiago de\n  Compostela, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic compiler phase selection/ordering has traditionally been focused on\nCPUs and, to a lesser extent, FPGAs. We present experiments regarding compiler\nphase ordering specialization of OpenCL kernels targeting a GPU. We use\niterative exploration to specialize LLVM phase orders on 15 OpenCL benchmarks\nto an NVIDIA GPU. We analyze the generated NVIDIA PTX code for the various\nversions to identify the main causes of the most significant improvements and\npresent results of a set of experiments that demonstrate the importance of\nusing specific phase orders. Using specialized compiler phase orders, we were\nable to achieve geometric mean improvements of 1.54x (up to 5.48x) and 1.65x\n(up to 5.7x) over PTX generated by the NVIDIA CUDA compiler from CUDA versions\nof the same kernels, and over execution of the OpenCL kernels compiled from\nsource with the NVIDIA OpenCL driver, respectively. We also evaluate the use of\ncode-features in the OpenCL kernels. More specifically, we evaluate an approach\nthat achieves geometric mean improvements of 1.49x and 1.56x over the same\nOpenCL baseline, by using the compiler sequences of the 1 or 3 most similar\nbenchmarks, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 16:59:25 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Nobre", "Ricardo", ""], ["Reis", "Lu\u00eds", ""], ["Cardoso", "Jo\u00e3o M. P.", ""]]}, {"id": "1810.11287", "submitter": "Csaba Kiraly", "authors": "Rom\\'an Sosa, Csaba Kiraly, Juan D. Parra Rodriguez", "title": "Offloading Execution from Edge to Cloud: a Dynamic Node-RED Based\n  Approach", "comments": "The 10th IEEE International Conference on Cloud Computing Technology\n  and Science (CloudCom 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing enables use cases where data produced in end devices are\nstored, processed, and acted on directly at the edges of the network, yet\ncomputation can be offloaded to more powerful instances through the edge to\ncloud continuum. Such offloading mechanism is especially needed in case of\nmodern multi-purpose IoT gateways, where both demand and operation conditions\ncan vary largely between deployments. To facilitate the development and\noperations of gateways, we implement offloading directly as part of the IoT\nrapid prototyping process embedded in the software stack, based on Node-RED. We\nevaluate the implemented method using an image processing example, and compare\nvarious offloading strategies based on resource consumption and other system\nmetrics, highlighting the differences in handling demand and service levels\nreached.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 12:26:18 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Sosa", "Rom\u00e1n", ""], ["Kiraly", "Csaba", ""], ["Rodriguez", "Juan D. Parra", ""]]}, {"id": "1810.11772", "submitter": "Huda Ibeid", "authors": "Huda Ibeid, Siping Meng, Oliver Dobon, Luke Olson, William Gropp", "title": "Learning with Analytical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand and predict the performance of scientific applications, several\nanalytical and machine learning approaches have been proposed, each having its\nadvantages and disadvantages. In this paper, we propose and validate a hybrid\napproach for performance modeling and prediction, which combines analytical and\nmachine learning models. The proposed hybrid model aims to minimize prediction\ncost while providing reasonable prediction accuracy. Our validation results\nshow that the hybrid model is able to learn and correct the analytical models\nto better match the actual performance. Furthermore, the proposed hybrid model\nimproves the prediction accuracy in comparison to pure machine learning\ntechniques while using small training datasets, thus making it suitable for\nhardware and workload changes.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 07:19:23 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 00:20:20 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Ibeid", "Huda", ""], ["Meng", "Siping", ""], ["Dobon", "Oliver", ""], ["Olson", "Luke", ""], ["Gropp", "William", ""]]}, {"id": "1810.11883", "submitter": "Huda Ibeid", "authors": "Huda Ibeid, Luke Olson, William Gropp", "title": "FFT, FMM, and Multigrid on the Road to Exascale: performance challenges\n  and opportunities", "comments": null, "journal-ref": null, "doi": "10.1016/j.jpdc.2019.09.014", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FFT, FMM, and multigrid methods are widely used fast and highly scalable\nsolvers for elliptic PDEs. However, emerging large-scale computing systems are\nintroducing challenges in comparison to current petascale computers. Recent\nefforts (Dongarra et al. 2011) have identified several constraints in the\ndesign of exascale software that includes massive concurrency, resilience\nmanagement, exploiting the high performance of heterogeneous systems, energy\nefficiency, and utilizing the deeper and more complex memory hierarchy expected\nat exascale. In this paper, we perform a model-based comparison of the FFT,\nFMM, and multigrid methods in the context of these projected constraints. In\naddition, we use performance models to offer predictions about the expected\nperformance on upcoming exascale system configurations based on current\ntechnology trends.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 21:05:20 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 19:12:01 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ibeid", "Huda", ""], ["Olson", "Luke", ""], ["Gropp", "William", ""]]}, {"id": "1810.11899", "submitter": "Hongkuan Zhou", "authors": "Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan,\n  Viktor Prasanna", "title": "Accurate, Efficient and Scalable Graph Embedding", "comments": "10 pages. 2019 IEEE International Parallel and Distributed Processing\n  Symposium (IPDPS)", "journal-ref": null, "doi": "10.1109/IPDPS.2019.00056", "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graph Convolutional Network (GCN) model and its variants are powerful\ngraph embedding tools for facilitating classification and clustering on graphs.\nHowever, a major challenge is to reduce the complexity of layered GCNs and make\nthem parallelizable and scalable on very large graphs -- state-of the art\ntechniques are unable to achieve scalability without losing accuracy and\nefficiency. In this paper, we propose novel parallelization techniques for\ngraph sampling-based GCNs that achieve superior scalable performance on very\nlarge graphs without compromising accuracy. Specifically, our GCN guarantees\nwork-efficient training and produces order of magnitude savings in computation\nand communication. To scale GCN training on tightly-coupled shared memory\nsystems, we develop parallelization strategies for the key steps in training:\nFor the graph sampling step, we exploit parallelism within and across multiple\nsampling instances, and devise an efficient data structure for concurrent\naccesses that provides theoretical guarantee of near-linear speedup with number\nof processing units. For the feature propagation step within the sampled graph,\nwe improve cache utilization and reduce DRAM communication by data\npartitioning. We prove that our partitioning strategy is a 2-approximation for\nminimizing the communication time compared to the optimal strategy. We\ndemonstrate that our parallel graph embedding outperforms state-of-the-art\nmethods in scalability (with respect to number of processors, graph size and\nGCN model size), efficiency and accuracy on several large datasets. On a\n40-core Xeon platform, our parallel training achieves $64\\times$ speedup (with\nAVX) in the sampling step and $25\\times$ speedup in the feature propagation\nstep, compared to the serial implementation, resulting in a net speedup of\n$21\\times$.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 22:44:51 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 04:49:54 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 17:51:08 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Zeng", "Hanqing", ""], ["Zhou", "Hongkuan", ""], ["Srivastava", "Ajitesh", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor", ""]]}, {"id": "1810.12092", "submitter": "Anna Engelmann", "authors": "Anna Engelmann, Admela Jukan, Rastin Pries", "title": "On Coding for Reliable VNF Chaining in DCNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how erasure coding can improve service reliability in Data Center\nNetworks (DCN). To this end, we find that coding can be best deployed in\nsystems, where i) traffic is split into multiple parallel sub-flows, ii) each\nsub-flow is encoded; iii) SFC along with their corresponding Virtual Network\nFunctions (VNF) concatenated are replicated into at least as many VNF instances\nas there are sub-flows, resulting in parallel sub- SFCs; and iv) all coded\nsub-flows are distributed over parallel paths and processed in parallel. We\nstudy service reliability as function of the level of parallelization within\nDCN and the resulting amount of redundancy. Based on the probability theory and\nby considering failures of path segments, VNF and server failures, we\nanalytically derive the probability that parallel subflows are successfully\nprocessed by the parallelized SFC and that the original serial traffic can be\nsuccessfully recovered without service interruptions.We compare the proposed\nfailure protection with coding and the standard backup protection and evaluate\nthe related overhead of both methods, including decoding, traffic redirection\nand VNF migration. The results not only show the benefit of our scheme for\nreliability, but also a reduced overhead required in comparison to backup\nprotection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:16:39 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Engelmann", "Anna", ""], ["Jukan", "Admela", ""], ["Pries", "Rastin", ""]]}, {"id": "1810.12210", "submitter": "Yanzhao Wu", "authors": "Yanzhao Wu, Ling Liu, Calton Pu, Wenqi Cao, Semih Sahin, Wenqi Wei, Qi\n  Zhang", "title": "A Comparative Measurement Study of Deep Learning as a Service Framework", "comments": "To appear on IEEE Transactions on Services Computing. The benchmark\n  tool used in this study is GTDLBench (https://git-disl.github.io/GTDLBench/)", "journal-ref": null, "doi": "10.1109/TSC.2019.2928551", "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data powered Deep Learning (DL) and its applications have blossomed in\nrecent years, fueled by three technological trends: a large amount of digitized\ndata openly accessible, a growing number of DL software frameworks in open\nsource and commercial markets, and a selection of affordable parallel computing\nhardware devices. However, no single DL framework, to date, dominates in terms\nof performance and accuracy even for baseline classification tasks on standard\ndatasets, making the selection of a DL framework an overwhelming task. This\npaper takes a holistic approach to conduct empirical comparison and analysis of\nfour representative DL frameworks with three unique contributions. First, given\na selection of CPU-GPU configurations, we show that for a specific DL\nframework, different configurations of its hyper-parameters may have a\nsignificant impact on both performance and accuracy of DL applications. Second,\nto the best of our knowledge, this study is the first to identify the\nopportunities for improving the training time performance and the accuracy of\nDL frameworks by configuring parallel computing libraries and tuning individual\nand multiple hyper-parameters. Third, we also conduct a comparative measurement\nstudy on the resource consumption patterns of four DL frameworks and their\nperformance and accuracy implications, including CPU and memory usage, and\ntheir correlations to varying settings of hyper-parameters under different\nconfiguration combinations of hardware, parallel computing libraries. We argue\nthat this measurement study provides in-depth empirical comparison and analysis\nof four representative DL frameworks, and offers practical guidance for service\nproviders to deploying and delivering DL as a Service (DLaaS) and for\napplication developers and DLaaS consumers to select the right DL frameworks\nfor the right DL workloads.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 16:03:41 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 18:24:19 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wu", "Yanzhao", ""], ["Liu", "Ling", ""], ["Pu", "Calton", ""], ["Cao", "Wenqi", ""], ["Sahin", "Semih", ""], ["Wei", "Wenqi", ""], ["Zhang", "Qi", ""]]}, {"id": "1810.12297", "submitter": "Shoumik Palkar", "authors": "Shoumik Palkar and Matei Zaharia", "title": "Optimizing Data-Intensive Computations in Existing Libraries with Split\n  Annotations", "comments": "Appearing in SOSP 2019, Huntsville, ON, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data movement between main memory and the CPU is a major bottleneck in\nparallel data-intensive applications. In response, researchers have proposed\nusing compilers and intermediate representations (IRs) that apply optimizations\nsuch as loop fusion under existing high-level APIs such as NumPy and\nTensorFlow. Even though these techniques generally do not require changes to\nuser applications, they require intrusive changes to the library itself: often,\nlibrary developers must rewrite each function using a new IR. In this paper, we\npropose a new technique called split annotations (SAs) that enables key data\nmovement optimizations over unmodified library functions. SAs only require\ndevelopers to annotate functions and implement an API that specifies how to\npartition data in the library. The annotation and API describe how to enable\ncross-function data pipelining and parallelization, while respecting each\nfunction's correctness constraints. We implement a parallel runtime for SAs in\na system called Mozart. We show that Mozart can accelerate workloads in\nlibraries such as Intel MKL and Pandas by up to 15x, with no library\nmodifications. Mozart also provides performance gains competitive with\nsolutions that require rewriting libraries, and can sometimes outperform these\nsystems by up to 2x by leveraging existing hand-optimized code.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 17:30:35 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 23:13:24 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Palkar", "Shoumik", ""], ["Zaharia", "Matei", ""]]}, {"id": "1810.13186", "submitter": "Benny Van Houdt", "authors": "Benny Van Houdt", "title": "Randomized Work Stealing versus Sharing in Large-scale Systems with\n  Non-exponential Job Sizes", "comments": "This paper was accepted in IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work sharing and work stealing are two scheduling paradigms to redistribute\nwork when performing distributed computations. In work sharing, processors\nattempt to migrate pending jobs to other processors in the hope of reducing\nresponse times. In work stealing, on the other hand, underutilized processors\nattempt to steal jobs from other processors. Both paradigms generate a certain\ncommunication overhead and the question addressed in this paper is which of the\ntwo reduces the response time the most given that they use the same amount of\ncommunication overhead.\n  Prior work presented explicit bounds, for large scale systems, on when\nrandomized work sharing outperforms randomized work stealing in case of Poisson\narrivals and exponential job durations and indicated that work sharing is best\nwhen the load is below $\\phi -1 \\approx 0.6180$, with $\\phi$ being the golden\nratio.\n  In this paper we revisit this problem and study the impact of the job size\ndistribution using a mean field model. We present an efficient method to\ndetermine the boundary between the regions where sharing or stealing is best\nfor a given job size distribution, as well as bounds that apply to any\n(phase-type) job size distribution. The main insight is that work stealing\nbenefits significantly from having more variable job sizes and work sharing may\nbecome inferior to work stealing for loads as small as $1/2 + \\epsilon$ for any\n$\\epsilon > 0$.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 09:49:52 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 12:44:03 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Van Houdt", "Benny", ""]]}]