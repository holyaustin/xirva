[{"id": "1912.00154", "submitter": "Muhammet Abdullah Soyturk", "authors": "Muhammet Abdullah Soyturk, Konstantinos Parasyris, Behzad Salami,\n  Osman Unsal, Gulay Yalcin, Leonardo Bautista Gomez", "title": "Hardware Versus Software Fault Injection of Modern Undervolted SRAMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve power efficiency, researchers are experimenting with dynamically\nadjusting the supply voltage of systems below the nominal operating points.\nHowever, production systems are typically not allowed to function on voltage\nsettings that is below the reliable limit. Consequently, existing software\nfault tolerance studies are based on fault models, which inject faults on\nrandom fault locations using fault injection techniques. In this work we study\nwhether random fault injection is accurate to simulate the behavior of\nundervolted SRAMs.\n  Our study extends the Gem5 simulator to support fault injection on the caches\nof the simulated system. The fault injection framework uses fault maps, which\ndescribe the faulty bits of SRAMs, as inputs. To compare random fault injection\nand hardware guided fault injection, we use two types of fault maps. The first\ntype of maps are created through undervolting real SRAMs and observing the\nlocation of the erroneous bits, whereas the second type of maps are created by\ncorrupting random bits of the SRAMs. During our study we corrupt the L1-Dcache\nof the simulated system and we monitor the behavior of the two types of fault\nmaps on the resiliency of six benchmarks. The difference among the resiliency\nof a benchmark when tested with the different fault maps can be up to 24%.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 07:57:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Soyturk", "Muhammet Abdullah", ""], ["Parasyris", "Konstantinos", ""], ["Salami", "Behzad", ""], ["Unsal", "Osman", ""], ["Yalcin", "Gulay", ""], ["Gomez", "Leonardo Bautista", ""]]}, {"id": "1912.00572", "submitter": "Wanling Gao", "authors": "Jianfeng Zhan, Lei Wang, Wanling Gao, and Rui Ren", "title": "BenchCouncil's View on Benchmarking AI and Other Emerging Workloads", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines BenchCouncil's view on the challenges, rules, and vision\nof benchmarking modern workloads like Big Data, AI or machine learning, and\nInternet Services. We conclude the challenges of benchmarking modern workloads\nas FIDSS (Fragmented, Isolated, Dynamic, Service-based, and Stochastic), and\npropose the PRDAERS benchmarking rules that the benchmarks should be specified\nin a paper-and-pencil manner, relevant, diverse, containing different levels of\nabstractions, specifying the evaluation metrics and methodology, repeatable,\nand scaleable. We believe proposing simple but elegant abstractions that help\nachieve both efficiency and general-purpose is the final target of benchmarking\nin future, which may be not pressing. In the light of this vision, we shortly\ndiscuss BenchCouncil's related projects.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 03:40:27 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 06:37:43 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Gao", "Wanling", ""], ["Ren", "Rui", ""]]}, {"id": "1912.00916", "submitter": "Markku-Juhani Saarinen", "authors": "Markku-Juhani O. Saarinen", "title": "Mobile Energy Requirements of the Upcoming NIST Post-Quantum\n  Cryptography Standards", "comments": "Related to a presentation given at the 7th ETSI/IQC Quantum Safe\n  Cryptography Workshop, 7 November 2019, Seattle USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardization of Post-Quantum Cryptography (PQC) was started by NIST in\n2016 and has proceeded to its second elimination round. The upcoming standards\nare intended to replace (or supplement) current RSA and Elliptic Curve\nCryptography (ECC) on all targets, including lightweight, embedded, and mobile\nsystems. We present an energy requirement analysis based on extensive\nmeasurements of PQC candidate algorithms on a Cortex M4 - based reference\nplatform. We relate computational (energy) costs of PQC algorithms to their\ndata transmission costs which are expected to increase with new types of public\nkeys and ciphertext messages. The energy, bandwidth, and latency needs of PQC\nalgorithms span several orders of magnitude, which is substantial enough to\nimpact battery life, user experience, and application protocol design. We\npropose metrics and guidelines for PQC algorithm usage in IoT and mobile\nsystems based on our findings. Our evidence supports the view that fast\nstructured-lattice PQC schemes are the preferred choice for cloud-connected\nmobile devices in most use cases, even when per-bit data transmission energy\ncost is relatively high.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:45:48 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 14:22:46 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 14:51:30 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2020 23:54:12 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Saarinen", "Markku-Juhani O.", ""]]}, {"id": "1912.02050", "submitter": "Ali Mohammed", "authors": "Ali Mohammed and Florina M. Ciorba", "title": "SimAS: A Simulation-assisted Approach for the Scheduling Algorithm\n  Selection under Perturbations", "comments": "arXiv admin note: text overlap with arXiv:1807.03577", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific applications consist of large and computationally-intensive\nloops. Dynamic loop self-scheduling (DLS) techniques are used to parallelize\nand to balance the load during the execution of such applications. Load\nimbalance arises from variations in the loop iteration (or tasks) execution\ntimes, caused by problem, algorithmic, or systemic characteristics. The\nvariations in systemic characteristics are referred to as perturbations, and\ncan be caused by other applications or processes that share the same resources,\nor a temporary system fault or malfunction. Therefore, the selection of the\nmost efficient DLS technique is critical to achieve the best application\nperformance. The following question motivates this work: Given an application,\nan HPC system, and their characteristics and interplay, which DLS technique\nwill achieve improved performance under unpredictable perturbations? Existing\nstudies focus on variations in the delivered computational speed only as the\nsource of perturbations in the system. However, perturbations in available\nnetwork bandwidth or latency are inevitable on production HPC systems. A\nSimulator-assisted scheduling (SimAS) is introduced as a new\ncontrol-theoretic-inspired approach to dynamically select DLS techniques that\nimprove the performance of applications executing on heterogeneous HPC systems\nunder perturbations. The present work examines the performance of seven\napplications on a heterogeneous system under all the above system\nperturbations. SimAS is evaluated as a proof of concept using native and\nsimulative experiments. The performance results confirm the original hypothesis\nthat no single DLS technique can deliver the absolute best performance in all\nscenarios, whereas the SimAS-based DLS selection resulted in improved\napplication performance in most experiments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:17:58 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Mohammed", "Ali", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1912.02322", "submitter": "Shijian Li", "authors": "Matthew LeMay and Shijian Li and Tian Guo", "title": "Perseus: Characterizing Performance and Cost of Multi-Tenant Serving for\n  CNN Models", "comments": "8 pages, 5 figures, and 6 tables. In proceedings of International\n  Conference on Cloud Engineering (IC2E) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are increasingly used for end-user applications,\nsupporting both novel features such as facial recognition, and traditional\nfeatures, e.g. web search. To accommodate high inference throughput, it is\ncommon to host a single pre-trained Convolutional Neural Network (CNN) in\ndedicated cloud-based servers with hardware accelerators such as Graphics\nProcessing Units (GPUs). However, GPUs can be orders of magnitude more\nexpensive than traditional Central Processing Unit (CPU) servers. These\nresources could also be under-utilized facing dynamic workloads, which may\nresult in inflated serving costs. One potential way to alleviate this problem\nis by allowing hosted models to share the underlying resources, which we refer\nto as multi-tenant inference serving. One of the key challenges is maximizing\nthe resource efficiency for multi-tenant serving given hardware with diverse\ncharacteristics, models with unique response time Service Level Agreement\n(SLA), and dynamic inference workloads. In this paper, we present Perseus, a\nmeasurement framework that provides the basis for understanding the performance\nand cost trade-offs of multi-tenant model serving. We implemented Perseus in\nPython atop a popular cloud inference server called Nvidia TensorRT Inference\nServer. Leveraging Perseus, we evaluated the inference throughput and cost for\nserving various models and demonstrated that multi-tenant model serving led to\nup to 12% cost reduction.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 00:35:43 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 22:02:51 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["LeMay", "Matthew", ""], ["Li", "Shijian", ""], ["Guo", "Tian", ""]]}, {"id": "1912.02708", "submitter": "Ehsan Taheri", "authors": "Ehsan Taheri, Ilya Kolmanovsky, Oleg Gusikhin", "title": "Survey of prognostics methods for condition-based maintenance in\n  engineering systems", "comments": "74 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is not surprising that the idea of efficient maintenance algorithms\n(originally motivated by strict emission regulations, and now driven by safety\nissues, logistics and customer satisfaction) has culminated in the so-called\ncondition-based maintenance program. Condition-based program/monitoring\nconsists of two major tasks, i.e., \\textit{diagnostics} and\n\\textit{prognostics} each of which has provided the impetus and technical\nchallenges to the scientists and engineers in various fields of engineering.\nPrognostics deals with the prediction of the remaining useful life, future\ncondition, or probability of reliable operation of an equipment based on the\nacquired condition monitoring data. This approach to modern maintenance\npractice promises to reduce the downtime, spares inventory, maintenance costs,\nand safety hazards. Given the significance of prognostics capabilities and the\nmaturity of condition monitoring technology, there have been an increasing\nnumber of publications on machinery prognostics in the past few years. These\npublications cover a wide range of issues important to prognostics.\nFortunately, improvement in computational resources technology has come to the\naid of engineers by presenting more powerful onboard computational resources to\nmake some aspects of these new problems tractable. In addition, it is possible\nto even leverage connected vehicle information through cloud-computing. Our\ngoal is to review the state of the art and to summarize some of the recent\nadvances in prognostics with the emphasis on models, algorithms and\ntechnologies used for data processing and decision making.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:32:21 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Taheri", "Ehsan", ""], ["Kolmanovsky", "Ilya", ""], ["Gusikhin", "Oleg", ""]]}, {"id": "1912.03213", "submitter": "Maede Zolanvari", "authors": "Maede Zolanvari, Marcio A. Teixeira, Raj Jain", "title": "Analysis of AeroMACS Data Link for Unmanned Aircraft Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aeronautical Mobile Airport Communications System (AeroMACS) is based on the\nIEEE 802.16e mobile wireless standard commonly known as WiMAX. It is expected\nto be the main part of the next-generation aviation communication system to\nsupport fixed and mobile services for manned and unmanned applications.\nAeroMACS will be an essential technology helping pave the way toward full\nintegration of Unmanned Aircraft Vehicle (UAV) into the national airspace. A\nnumber of practical tests and analyses have been done so far for AeroMACS. The\nmain contribution of this paper is to consider the theoretical concepts behind\nits features and discuss their suitability for UAV applications. Mathematical\nanalyses of the AeroMACS physical layer framework are provided to show the\ntheoretical trade-offs. We mainly focus on the analysis of the AeroMACS OFDMA\nstructure, which affects the speed limits, coverage cell, channel estimation\nrequirements, and inter-carrier interference.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:33:21 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Zolanvari", "Maede", ""], ["Teixeira", "Marcio A.", ""], ["Jain", "Raj", ""]]}, {"id": "1912.03348", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far and Emina Soljanin", "title": "Scheduling in the Presence of Data Intensive Compute Jobs", "comments": "Accepted in IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of non-adaptive scheduling policies in computing\nsystems with multiple servers. Compute jobs are mostly regular, with modest\nservice requirements. However, there are sporadic data intensive jobs, whose\nexpected service time is much higher than that of the regular jobs. Forthis\nmodel, we are interested in the effect of scheduling policieson the average\ntime a job spends in the system. To this end, we introduce two performance\nindicators in a simplified, only-arrival system. We believe that these\nperformance indicators are good predictors of the relative performance of the\npolicies in the queuing system, which is supported by simulations results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:38:55 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 16:40:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""]]}, {"id": "1912.03413", "submitter": "Daniele Scarpazza", "authors": "Zhe Jia and Blake Tillman and Marco Maggioni and Daniele Paolo\n  Scarpazza", "title": "Dissecting the Graphcore IPU Architecture via Microbenchmarking", "comments": "91 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report focuses on the architecture and performance of the Intelligence\nProcessing Unit (IPU), a novel, massively parallel platform recently introduced\nby Graphcore and aimed at Artificial Intelligence/Machine Learning (AI/ML)\nworkloads. We dissect the IPU's performance behavior using microbenchmarks that\nwe crafted for the purpose. We study the IPU's memory organization and\nperformance. We study the latency and bandwidth that the on-chip and off-chip\ninterconnects offer, both in point-to-point transfers and in a spectrum of\ncollective operations, under diverse loads. We evaluate the IPU's compute power\nover matrix multiplication, convolution, and AI/ML primitives. We discuss\nactual performance in comparison with its theoretical limits. Our findings\nreveal how the IPU's architectural design affects its performance. Moreover,\nthey offer simple mental models to predict an application's performance on the\nIPU, on the basis of the computation and communication steps it involves. This\nreport is the natural extension to a novel architecture of a continuing effort\nof ours that focuses on the microbenchmark-based discovery of massively\nparallel architectures.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 02:10:19 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jia", "Zhe", ""], ["Tillman", "Blake", ""], ["Maggioni", "Marco", ""], ["Scarpazza", "Daniele Paolo", ""]]}, {"id": "1912.04379", "submitter": "Jonathan Baxter", "authors": "Douglas Aberdeen and Jonathan Baxter", "title": "General Matrix-Matrix Multiplication Using SIMD features of the PIII", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.05181", "journal-ref": "Euro-Par '00 Proceedings from the 6th International Euro-Par\n  Conference on Parallel Processing (2000) Pages 980-983", "doi": "10.1007/3-540-44520-X_138", "report-no": null, "categories": "cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised matrix-matrix multiplication forms the kernel of many\nmathematical algorithms. A faster matrix-matrix multiply immediately benefits\nthese algorithms. In this paper we implement efficient matrix multiplication\nfor large matrices using the floating point Intel Pentium SIMD (Single\nInstruction Multiple Data) architecture. A description of the issues and our\nsolution is presented, paying attention to all levels of the memory hierarchy.\nOur results demonstrate an average performance of 2.09 times faster than the\nleading public domain matrix-matrix multiply routines.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:45:56 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Aberdeen", "Douglas", ""], ["Baxter", "Jonathan", ""]]}, {"id": "1912.05322", "submitter": "Onel Luis Alcaraz Lopez", "authors": "Onel L. Alcaraz L\\'opez, Hirley Alves, Richard Demo Souza, Samuel\n  Montejo-S\\'anchez, Evelio M. Garc\\'ia Fern\\'andez, Matti Latva-aho", "title": "Massive Wireless Energy Transfer: Enabling Sustainable IoT Towards 6G\n  Era", "comments": "19 pags, 15 figures, 5 tables, submitted to IEEE Internet of Things\n  Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances on wireless energy transfer (WET) make it a promising\nsolution for powering future Internet of Things (IoT) devices enabled by the\nupcoming sixth generation (6G) era. The main architectures, challenges and\ntechniques for efficient and scalable wireless powering are overviewed in this\npaper. Candidates enablers such as energy beamforming (EB), distributed antenna\nsystems (DAS), advances on devices' hardware and programmable medium, new\nspectrum opportunities, resource scheduling and distributed ledger technology\nare outlined. Special emphasis is placed on discussing the suitability of\nchannel state information (CSI)-limited/free strategies when powering\nsimultaneously a massive number of devices. The benefits from combining DAS and\nEB, and from using average CSI whenever available, are numerically illustrated.\nThe pros and cons of the state-of-the-art CSI-free WET techniques in ultra-low\npower setups are thoroughly revised, and some possible future enhancements are\noutlined. Finally, key research directions towards realizing WET-enabled\nmassive IoT networks in the 6G era are identified and discussed in detail.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:04:14 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 11:59:39 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["L\u00f3pez", "Onel L. Alcaraz", ""], ["Alves", "Hirley", ""], ["Souza", "Richard Demo", ""], ["Montejo-S\u00e1nchez", "Samuel", ""], ["Fern\u00e1ndez", "Evelio M. Garc\u00eda", ""], ["Latva-aho", "Matti", ""]]}, {"id": "1912.05364", "submitter": "Clemens Dubslaff", "authors": "Clemens Dubslaff, Kai Ding, Andrey Morozov, Christel Baier, Klaus\n  Janschek", "title": "Breaking the Limits of Redundancy Systems Analysis", "comments": "This paper is a preprint of the corresponding ESREL'19 conference\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LO cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy mechanisms such as triple modular redundancy protect\nsafety-critical components by replication and thus improve systems fault\ntolerance. However, the gained fault tolerance comes along with costs to be\ninvested, e.g., increasing execution time, energy consumption, or packaging\nsize, for which constraints have to be obeyed during system design. This turns\nthe question of finding suitable combinations of components to be protected\ninto a challenging task as the number of possible protection combinations grows\nexponentially in the number of components. We propose family-based approaches\nto tackle the combinatorial blowup in redundancy systems modeling and analysis\nphases. Based on systems designed in SIMULINK we show how to obtain models that\ninclude all possible protection combinations and present a tool chain that,\ngiven a probabilistic error model, generates discrete Markov chain families.\nUsing symbolic techniques that enable concise family representation and\nanalysis, we show how SIMULINK models of realistic size can be protected and\nanalyzed with a single family-based analysis run while a one-by-one analysis of\neach protection combination would clearly exceed any realistic time\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:46:39 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Dubslaff", "Clemens", ""], ["Ding", "Kai", ""], ["Morozov", "Andrey", ""], ["Baier", "Christel", ""], ["Janschek", "Klaus", ""]]}, {"id": "1912.05529", "submitter": "Ahmad Shawahna", "authors": "Ahmad Shawahna, Syed Abdul Salam, and Mayez Al-Mouhamed", "title": "Seismic Imaging: An Overview and Parallel Implementation of Poststack\n  Depth Migration", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.DC cs.PF eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seismic migration is the core step of seismic data processing which is\nimportant for oil exploration. Poststack depth migration in frequency-space\n(f-x) domain is one of commonly used algorithms. The wave-equation solution can\nbe approximated as FIR filtering process to extrapolate the raw data and\nextract the subsurface image. Because of its computational complexity, its\nparallel implementation is encouraged. For calculating the next depth level,\nprevious depth level is required. So, this part cannot be parallelized because\nof data dependence. But at each depth level there is plenty of roam for\nparallelism and can be parallelized. In case of CUDA programming, each thread\ncalculate a single pixel on the next depth plan. After calculating the next\ndepth plan, we can calculate the depth row by summing over all the frequencies\nand calculating all the depth rows results in the final migrated image. The\npoststack depth migration is implemented in CUDA and its performance is\nevaluated with the sequential code with different problem sizes.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 01:33:03 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Shawahna", "Ahmad", ""], ["Salam", "Syed Abdul", ""], ["Al-Mouhamed", "Mayez", ""]]}, {"id": "1912.06322", "submitter": "Yoshiaki Inoue", "authors": "Yoshiaki Inoue", "title": "Queueing Analysis of GPU-Based Inference Servers with Dynamic Batching:\n  A Closed-Form Characterization", "comments": null, "journal-ref": null, "doi": "10.1016/j.peva.2020.102183", "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPU-accelerated computing is a key technology to realize high-speed inference\nservers using deep neural networks (DNNs). An important characteristic of\nGPU-based inference is that the computational efficiency, in terms of the\nprocessing speed and energy consumption, drastically increases by processing\nmultiple jobs together in a batch. In this paper, we formulate GPU-based\ninference servers as a batch service queueing model with batch-size dependent\nprocessing times. We first show that the energy efficiency of the server\nmonotonically increases with the arrival rate of inference jobs, which suggests\nthat it is energy-efficient to operate the inference server under a utilization\nlevel as high as possible within a latency requirement of inference jobs. We\nthen derive a closed-form upper bound for the mean latency, which provides a\nsimple characterization of the latency performance. Through simulation and\nnumerical experiments, we show that the exact value of the mean latency is well\napproximated by this upper bound. We further compare this upper bound with the\nlatency curve measured in real implementation of GPU-based inference servers\nand we show that the real performance curve is well explained by the derived\nsimple formula.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 04:39:16 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 03:30:02 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 02:03:16 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Inoue", "Yoshiaki", ""]]}, {"id": "1912.08026", "submitter": "Michael R\\\"oder", "authors": "Michael R\\\"oder and Geraldo de Souza and Denis Kuchelev and\n  Abdelmoneim Amer Desouki and Axel-Cyrille Ngonga Ngomo", "title": "ORCA: a Benchmark for Data Web Crawlers", "comments": "8 pages, submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of RDF knowledge graphs available on the Web grows constantly.\nGathering these graphs at large scale for downstream applications hence\nrequires the use of crawlers. Although Data Web crawlers exist, and general Web\ncrawlers could be adapted to focus on the Data Web, there is currently no\nbenchmark to fairly evaluate their performance. Our work closes this gap by\npresenting the Orca benchmark. Orca generates a synthetic Data Web, which is\ndecoupled from the original Web and enables a fair and repeatable comparison of\nData Web crawlers. Our evaluations show that Orca can be used to reveal the\ndifferent advantages and disadvantages of existing crawlers. The benchmark is\nopen-source and available at https://github.com/dice-group/orca.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 14:08:01 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 17:18:08 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["R\u00f6der", "Michael", ""], ["de Souza", "Geraldo", ""], ["Kuchelev", "Denis", ""], ["Desouki", "Abdelmoneim Amer", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1912.08368", "submitter": "Majid Raeis", "authors": "Majid Raeis, Ali Tizghadam and Alberto Leon-Garcia", "title": "Real-Time Prediction of Delay Distribution in Service Systems using\n  Mixture Density Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by interest in providing more efficient services in customer\nservice systems, we use statistical learning methods and delay history\ninformation to predict the conditional distribution of the customers' waiting\ntimes in queueing systems. From the predicted distributions, descriptive\nstatistics of the system such as the mean, variance and percentiles of the\nwaiting times can be obtained, which can be used for delay announcements, SLA\nconformance and better system management. We model the conditional\ndistributions by mixtures of Gaussians, parameters of which can be estimated\nusing Mixture Density Networks. The evaluations show that exploiting more delay\nhistory information can result in much more accurate predictions under\nrealistic time-varying arrival assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 03:50:13 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Raeis", "Majid", ""], ["Tizghadam", "Ali", ""], ["Leon-Garcia", "Alberto", ""]]}, {"id": "1912.08950", "submitter": "Maciej Besta", "authors": "Maciej Besta, Simon Weber, Lukas Gianinazzi, Robert Gerstenberger,\n  Andrey Ivanov, Yishai Oltchik, Torsten Hoefler", "title": "Slim Graph: Practical Lossy Graph Compression for Approximate Graph\n  Processing, Storage, and Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Slim Graph: the first programming model and framework for\npractical lossy graph compression that facilitates high-performance approximate\ngraph processing, storage, and analytics. Slim Graph enables the developer to\nexpress numerous compression schemes using small and programmable compression\nkernels that can access and modify local parts of input graphs. Such kernels\nare executed in parallel by the underlying engine, isolating developers from\ncomplexities of parallel programming. Our kernels implement novel graph\ncompression schemes that preserve numerous graph properties, for example\nconnected components, minimum spanning trees, or graph spectra. Finally, Slim\nGraph uses statistical divergences and other metrics to analyze the accuracy of\nlossy graph compression. We illustrate both theoretically and empirically that\nSlim Graph accelerates numerous graph algorithms, reduces storage used by graph\ndatasets, and ensures high accuracy of results. Slim Graph may become the\ncommon ground for developing, executing, and analyzing emerging lossy graph\ncompression schemes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:48:29 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Besta", "Maciej", ""], ["Weber", "Simon", ""], ["Gianinazzi", "Lukas", ""], ["Gerstenberger", "Robert", ""], ["Ivanov", "Andrey", ""], ["Oltchik", "Yishai", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.09256", "submitter": "Alexandru Uta", "authors": "Alexandru Uta, Alexandru Custura, Dmitry Duplyakin, Ivo Jimenez, Jan\n  Rellermeyer, Carlos Maltzahn, Robert Ricci, Alexandru Iosup", "title": "Is Big Data Performance Reproducible in Modern Cloud Networks?", "comments": "12 pages paper, 3 pages references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance variability has been acknowledged as a problem for over a decade\nby cloud practitioners and performance engineers. Yet, our survey of top\nsystems conferences reveals that the research community regularly disregards\nvariability when running experiments in the cloud. Focusing on networks, we\nassess the impact of variability on cloud-based big-data workloads by gathering\ntraces from mainstream commercial clouds and private research clouds. Our data\ncollection consists of millions of datapoints gathered while transferring over\n9 petabytes of data. We characterize the network variability present in our\ndata and show that, even though commercial cloud providers implement mechanisms\nfor quality-of-service enforcement, variability still occurs, and is even\nexacerbated by such mechanisms and service provider policies. We show how\nbig-data workloads suffer from significant slowdowns and lack predictability\nand replicability, even when state-of-the-art experimentation techniques are\nused. We provide guidelines for practitioners to reduce the volatility of big\ndata performance, making experiments more repeatable.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:05:30 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Uta", "Alexandru", ""], ["Custura", "Alexandru", ""], ["Duplyakin", "Dmitry", ""], ["Jimenez", "Ivo", ""], ["Rellermeyer", "Jan", ""], ["Maltzahn", "Carlos", ""], ["Ricci", "Robert", ""], ["Iosup", "Alexandru", ""]]}, {"id": "1912.09596", "submitter": "Stefan Zellmann", "authors": "Stefan Zellmann", "title": "Comparing Hierarchical Data Structures for Sparse Volume Rendering with\n  Empty Space Skipping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empty space skipping can be efficiently implemented with hierarchical data\nstructures such as k-d trees and bounding volume hierarchies. This paper\ncompares several recently published hierarchical data structures with regard to\nconstruction and rendering performance. The papers that form our prior work\nhave primarily focused on interactively building the data structures and only\nshowed that rendering performance is superior to using simple acceleration data\nstructures such as uniform grids with macro cells. In the area of surface ray\ntracing, there exists a trade-off between construction and rendering\nperformance of hierarchical data structures. In this paper we present\nperformance comparisons for several empty space skipping data structures in\norder to determine if such a trade-off also exists for volume rendering with\nuniform data topologies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:33:03 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Zellmann", "Stefan", ""]]}, {"id": "1912.09609", "submitter": "EPTCS", "authors": "Mark Minas", "title": "Speeding up Generalized PSR Parsers by Memoization Techniques", "comments": "In Proceedings GCM 2019, arXiv:1912.08966", "journal-ref": "EPTCS 309, 2019, pp. 71-86", "doi": "10.4204/EPTCS.309.4", "report-no": null, "categories": "cs.FL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive shift-reduce (PSR) parsing for hyperedge replacement (HR) grammars\nis very efficient, but restricted to a subclass of unambiguous HR grammars. To\novercome this restriction, we have recently extended PSR parsing to generalized\nPSR (GPSR) parsing along the lines of Tomita-style generalized LR parsing.\nUnfortunately, GPSR parsers turned out to be too inefficient without manual\ntuning. This paper proposes to use memoization techniques to speed up GPSR\nparsers without any need of manual tuning, and which has been realized within\nthe graph parser distiller Grappa. We present running time measurements for\nsome example languages; they show a significant speed up by some orders of\nmagnitude when parsing valid graphs. But memoization techniques do not help\nwhen parsing invalid graphs or if all parses of an ambiguous input graph shall\nbe determined.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 01:40:51 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Minas", "Mark", ""]]}, {"id": "1912.09650", "submitter": "Tatsuaki Kimura", "authors": "Tatsuaki Kimura, Hiroshi Saito", "title": "Spatio-Temporal Correlation of Interference in MANET Under Spatially\n  Correlated Shadowing Environment", "comments": "to appear in IEEE Transactions on Mobile Computing", "journal-ref": null, "doi": "10.1109/TMC.2019.2959990", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation of interference affects spatio-temporal aspects of various\nwireless mobile systems, such as retransmission, multiple antennas and\ncooperative relaying. In this paper, we study the spatial and temporal\ncorrelation of interference in mobile ad-hoc networks under a correlated\nshadowing environment. By modeling the node locations as a Poisson point\nprocess with an i.i.d. mobility model and considering Gudmundson (1991)' s\nspatially correlated shadowing model, we theoretically analyze the relationship\nbetween the correlation distance of log-normal shadowing and the spatial and\ntemporal correlation coefficients of interference. Since the exact expressions\nof the correlation coefficients are intractable, we obtain their simple\nasymptotic expressions as the variance of log-normal shadowing increases. We\nfound in our numerical examples that the asymptotic expansions can be used as\ntight approximate formulas and useful for modeling general wireless systems\nunder spatially correlated shadowing.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 05:39:10 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Kimura", "Tatsuaki", ""], ["Saito", "Hiroshi", ""]]}, {"id": "1912.09765", "submitter": "Swanand Kadhe", "authors": "Mehmet Fatih Aktas, Swanand Kadhe, Emina Soljanin, Alex Sprintson", "title": "Download Time Analysis for Distributed Storage Codes with Locality and\n  Availability", "comments": "Accepted for a publication in IEEE Transactions on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The paper presents techniques for analyzing the expected download time in\ndistributed storage systems that employ systematic availability codes. These\ncodes provide access to hot data through the systematic server containing the\nobject and multiple recovery groups. When a request for an object is received,\nit can be replicated (forked) to the systematic server and all recovery groups.\nWe first consider the low-traffic regime and present the close-form expression\nfor the download time. By comparison across systems with availability, maximum\ndistance separable (MDS), and replication codes, we demonstrate that\navailability codes can reduce download time in some settings but are not always\noptimal. In the high-traffic regime, the system consists of multiple\ninter-dependent Fork-Join queues, making exact analysis intractable.\nAccordingly, we present upper and lower bounds on the download time, and an\nM/G/1 queue approximation for several cases of interest. Via extensive\nnumerical simulations, we evaluate our bounds and demonstrate that the M/G/1\nqueue approximation has a high degree of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 11:24:30 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 04:39:41 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 17:31:01 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 18:29:44 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Kadhe", "Swanand", ""], ["Soljanin", "Emina", ""], ["Sprintson", "Alex", ""]]}, {"id": "1912.09773", "submitter": "Roben Castagna Lunardi", "authors": "Roben Castagna Lunardi and Henry Cabral Nunes and Vinicius da Silva\n  Branco and Bruno Hugentobler Lipper and Charles Varlei Neu and Avelino\n  Francisco Zorzo", "title": "Performance and Cost Evaluation of Smart Contracts in Collaborative\n  Health Care Environments", "comments": "Presented at ICITST 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain emerged as a solution for data integrity, non-repudiation, and\navailability in different applications. Data sensitive scenarios, such as\nHealth Care, can also benefit from these blockchain properties. Consequently,\ndifferent research proposed the adoption of blockchain in Health Care\napplications. However, few are discussed about incentive methods to attract new\nusers, as well as to motivate the system or application usage by existing\nend-users. Also, little is discussed about performance during code execution in\nblockchains. In order to tackle these issues, this work presents the\npreliminary evaluation of TokenHealth, an application for collaborative health\npractice monitoring with gamification and token-based incentives. The proposed\nsolution is implemented through smart contracts using Solidity in the Ethereum\nblockchain. We evaluated the performance of both in Ropsten test network and in\na Private instance. The preliminary results show that the execution of smart\ncontracts takes less than a minute for a full cycle of different smart\ncontracts. Also, we present a discussion about costs for using a Private\ninstance and the public Ethereum main network.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 11:34:47 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Lunardi", "Roben Castagna", ""], ["Nunes", "Henry Cabral", ""], ["Branco", "Vinicius da Silva", ""], ["Lipper", "Bruno Hugentobler", ""], ["Neu", "Charles Varlei", ""], ["Zorzo", "Avelino Francisco", ""]]}, {"id": "1912.09816", "submitter": "Petr Chunaev", "authors": "Petr Chunaev", "title": "Community detection in node-attributed social networks: a survey", "comments": "This is an essentially revised version of the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental problem in social network analysis\nconsisting in unsupervised dividing social actors (nodes in a social graph)\nwith certain social connections (edges in a social graph) into densely knitted\nand highly related groups with each group well separated from the others.\nClassical approaches for community detection usually deal only with network\nstructure and ignore features of its nodes (called node attributes), although\nmany real-world social networks provide additional actors' information such as\ninterests. It is believed that the attributes may clarify and enrich the\nknowledge about the actors and give sense to the communities. This belief has\nmotivated the progress in developing community detection methods that use both\nthe structure and the attributes of network (i.e. deal with a node-attributed\ngraph) to yield more informative and qualitative results.\n  During the last decade many such methods based on different ideas have\nappeared. Although there exist partial overviews of them, a recent survey is a\nnecessity as the growing number of the methods may cause repetitions in\nmethodology and uncertainty in practice.\n  In this paper we aim at describing and clarifying the overall situation in\nthe field of community detection in node-attributed social networks. Namely, we\nperform an exhaustive search of known methods and propose a classification of\nthem based on when and how structure and attributes are fused. We not only give\na description of each class but also provide general technical ideas behind\neach method in the class. Furthermore, we pay attention to available\ninformation which methods outperform others and which datasets and quality\nmeasures are used for their evaluation. Basing on the information collected, we\nmake conclusions on the current state of the field and disclose several\nproblems that seem important to be resolved in future.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 13:35:32 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 11:01:39 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Chunaev", "Petr", ""]]}, {"id": "1912.09870", "submitter": "Young Myoung Ko", "authors": "Seung Min Baik and Young Myoung Ko", "title": "A QoS-aware workload routing and server speed scaling policy for\n  energy-efficient data centers: a robust queueing theoretic approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining energy efficiency in large data centers depends on the ability to\nmanage workload routing and control server speeds according to fluctuating\ndemand. The use of dynamic algorithms often means that management has to\ninstall the complicated software or expensive hardware needed to communicate\nwith routers and servers. This paper proposes a static routing and server speed\nscaling policy that may achieve energy efficiency similar to dynamic algorithms\nand eliminate the necessity of frequent communications among resources without\ncompromising quality of service (QoS). We use a robust queueing approach to\nconsider the response time constraints, e.g., service level agreements (SLAs).\nWe model each server as a $G/G/1$ processor sharing (PS) queue and use\nuncertainty sets to define the domain of random variables. A comparison with a\ndynamic algorithm shows that the proposed static policy provides competitive\nsolutions in terms of energy efficiency and satisfactory QoS.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:11:30 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Baik", "Seung Min", ""], ["Ko", "Young Myoung", ""]]}, {"id": "1912.11310", "submitter": "Devarpita Sinha", "authors": "Devarpita Sinha and Rajarshi Roy", "title": "Deadline-aware Scheduling for Maximizing Information Freshness in\n  Industrial Cyber-Physical System", "comments": "Accepted in IEEE Sensors Journal", "journal-ref": null, "doi": "10.1109/JSEN.2020.3014368", "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age of Information is an interesting metric that captures the freshness of\ninformation in the underlying applications. It is a combination of both packets\ninter-arrival time and packet transmission delay. In recent times, advanced\nreal-time systems rely on this metric for delivering status updates as timely\nas possible. This paper aims to accomplish optimal transmission scheduling\npolicy to maintain the information freshness of real-time updates in the\nindustrial cyber-physical systems. Here the coexistence of both cyber and\nphysical units and their individual requirements to provide the quality of\nservice is one of the critical challenges to handle. A greedy scheduling policy\ncalled deadline-aware highest latency first has been proposed for this purpose.\nThis paper also gives the analytical proof of its optimality, and finally, the\nclaim is validated by comparing the performance of our algorithm with other\nscheduling policies by extensive simulations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 12:15:59 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 04:58:26 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 11:32:09 GMT"}, {"version": "v4", "created": "Mon, 28 Sep 2020 07:22:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Sinha", "Devarpita", ""], ["Roy", "Rajarshi", ""]]}, {"id": "1912.11456", "submitter": "Andrew Sutton", "authors": "Grant Chung, Luc Desrosiers, Manav Gupta, Andrew Sutton, Kaushik\n  Venkatadri, Ontak Wong, and Goran Zugic", "title": "Performance Tuning and Scaling Enterprise Blockchain Applications", "comments": "49 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain scalability can be complicated and costly. As enterprises begin to\nadopt blockchain technology to solve business problems, there are valid\nconcerns if blockchain applications can support the transactional demands of\nproduction systems. In fact, the multiple distributed components and protocols\nthat underlie blockchain applications makes performance optimization a\nnon-trivial task. Blockchain performance optimization and scalability require a\nmethodology to reduce complexity and cost. Furthermore, existing performance\nresults often lack the requirements, load, and infrastructure of a production\napplication. In this paper, we first develop a methodical approach to\nperformance tuning enterprise blockchain applications to increase performance\nand transaction capacity. The methodology is applied to an enterprise\nblockchain-based application (leveraging Hyperledger Fabric) for performance\ntuning and optimization with the goal of bridging the gap between laboratory\nand production deployed system performance. We then present extensive results\nand analysis of our performance testing for on-premise and cloud deployments,\nin which we were able to scale the application from 30 to 3000 TPS without\nforking the Hyperledger Fabric source code and maintaining a reasonable\ninfrastructure footprint. We also provide blockchain application and platform\nrecommendations for performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:22:21 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Chung", "Grant", ""], ["Desrosiers", "Luc", ""], ["Gupta", "Manav", ""], ["Sutton", "Andrew", ""], ["Venkatadri", "Kaushik", ""], ["Wong", "Ontak", ""], ["Zugic", "Goran", ""]]}, {"id": "1912.11661", "submitter": "Dennis Schol", "authors": "Dennis Schol, Maria Vlasiou and Bert Zwart", "title": "Large fork-join queues with nearly deterministic arrival and service\n  times", "comments": "36 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an $N$ server fork-join queue with nearly\ndeterministic arrival and service times. Specifically, we present a fluid limit\nfor the maximum queue length as $N\\to\\infty$. This fluid limit depends on the\ninitial number of tasks. In order to prove these results, we develop extreme\nvalue theory and diffusion approximations for the queue lengths.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 12:53:40 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 15:27:14 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Schol", "Dennis", ""], ["Vlasiou", "Maria", ""], ["Zwart", "Bert", ""]]}, {"id": "1912.11794", "submitter": "Val\\'erie Hayot-Sasson", "authors": "Valerie Hayot-Sasson, Shawn T Brown and Tristan Glatard", "title": "Performance benefits of Intel(R) OptaneTM DC persistent memory for the\n  parallel processing of large neuroimaging data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Open-access neuroimaging datasets have reached petabyte scale, and continue\nto grow. The ability to leverage the entirety of these datasets is limited to a\nrestricted number of labs with both the capacity and infrastructure to process\nthe data. Whereas Big Data engines have significantly reduced application\nperformance penalties with respect to data movement, their applied strategies\n(e.g. data locality, in-memory computing and lazy evaluation) are not\nnecessarily practical within neuroimaging workflows where intermediary results\nmay need to be materialized to shared storage for post-processing analysis. In\nthis paper we evaluate the performance advantage brought by Intel(R) OptaneTM\nDC persistent memory for the processing of large neuroimaging datasets using\nthe two available configurations modes: Memory mode and App Direct mode. We\nemploy a synthetic algorithm on the 76 GiB and 603 GiB BigBrain, as well as\napply a standard neuroimaging application on the Consortium for Reliability and\nReproducibility (CoRR) dataset using 25 and 96 parallel processes in both\ncases. Our results show that the performance of applications leveraging\npersistent memory is superior to that of other storage devices,with the\nexception of DRAM. This is the case in both Memory and App Direct mode and\nirrespective of the amount of data and parallelism. Furthermore, persistent\nmemory in App Direct mode is believed to benefit from the use of DRAM as a\ncache for writing when output data is significantly smaller than available\nmemory. We believe the use of persistent memory will be beneficial to both\nneuroimaging applications running on HPC or visualization of large,\nhigh-resolution images.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 07:38:52 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Hayot-Sasson", "Valerie", ""], ["Brown", "Shawn T", ""], ["Glatard", "Tristan", ""]]}, {"id": "1912.11944", "submitter": "Antonio Fari\\~na", "authors": "Antonio Fari\\~na and Miguel A. Mart\\'inez-Prieto and Francisco Claude\n  and Gonzalo Navarro and Juan J. Lastra-D\\'iaz and Nicola Prezza and Diego\n  Seco", "title": "On the Reproducibility of Experiments of Indexing Repetitive Document\n  Collections", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Replication framework\n  available at: https://github.com/migumar2/uiHRDC/", "journal-ref": "Information Systems; Volume 83, July 2019; pages 181-194", "doi": "10.1016/j.is.2019.03.007", "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a companion reproducible paper with the aim of allowing\nthe exact replication of the methods, experiments, and results discussed in a\nprevious work [5]. In that parent paper, we proposed many and varied techniques\nfor compressing indexes which exploit that highly repetitive collections are\nformed mostly of documents that are near-copies of others. More concretely, we\ndescribe a replication framework, called uiHRDC (universal indexes for Highly\nRepetitive Document Collections), that allows our original experimental setup\nto be easily replicated using various document collections. The corresponding\nexperimentation is carefully explained, providing precise details about the\nparameters that can be tuned for each indexing solution. Finally, note that we\nalso provide uiHRDC as reproducibility package.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 22:51:13 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Fari\u00f1a", "Antonio", ""], ["Mart\u00ednez-Prieto", "Miguel A.", ""], ["Claude", "Francisco", ""], ["Navarro", "Gonzalo", ""], ["Lastra-D\u00edaz", "Juan J.", ""], ["Prezza", "Nicola", ""], ["Seco", "Diego", ""]]}, {"id": "1912.12339", "submitter": "Georgios Paschos", "authors": "Georgios Paschos and George Iosifidis and Giuseppe Caire", "title": "Cache Optimization Models and Algorithms", "comments": "127 pages, 46 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage resources and caching techniques permeate almost every area of\ncommunication networks today. In the near future, caching is set to play an\nimportant role in storage-assisted Internet architectures, information-centric\nnetworks, and wireless systems, reducing operating and capital expenditures and\nimproving the services offered to users. In light of the remarkable data\ntraffic growth and the increasing number of rich-media applications, the impact\nof caching is expected to become even more profound than it is today.\nTherefore, it is crucial to design these systems in an optimal fashion,\nensuring the maximum possible performance and economic benefits from their\ndeployment. To this end, this article presents a collection of detailed models\nand algorithms, which are synthesized to build a powerful analytical framework\nfor caching optimization.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 21:15:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Paschos", "Georgios", ""], ["Iosifidis", "George", ""], ["Caire", "Giuseppe", ""]]}, {"id": "1912.12383", "submitter": "Dawei Cheng", "authors": "Dawei Cheng, Chen Chen, Xiaoyang Wang, Sheng Xiang", "title": "Efficient Top-k Vulnerable Nodes Detection in Uncertain Graphs", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertain graphs have been widely used to model complex linked data in many\nreal-world applications, such as guaranteed-loan networks and power grids,\nwhere a node or edge may be associated with a probability. In these networks, a\nnode usually has a certain chance of default or breakdown due to self-factors\nor the influence from upstream nodes. For regulatory authorities and companies,\nit is critical to efficiently identify the vulnerable nodes, i.e., nodes with\nhigh default risks, such that they could pay more attention to these nodes for\nthe purpose of risk management. In this paper, we propose and investigate the\nproblem of top-$k$ vulnerable nodes detection in uncertain graphs. We formally\ndefine the problem and prove its hardness. To identify the $k$ most vulnerable\nnodes, a sampling-based approach is proposed. Rigorous theoretical analysis is\nconducted to bound the quality of returned results. Novel optimization\ntechniques and a bottom-$k$ sketch based approach are further developed in\norder to scale for large networks. In the experiments, we demonstrate the\nperformance of proposed techniques on 3 real financial networks and 5 benchmark\nnetworks. The evaluation results show that the proposed methods can achieve up\nto 2 orders of magnitudes speedup compared with the baseline approach.\nMoreover, to further verify the advantages of our model in real-life scenarios,\nwe integrate the proposed techniques with our current loan risk control system,\nwhich is deployed in the collaborated bank, for more evaluation. Particularly,\nwe show that our proposed new model has superior performance on real-life\nguaranteed-loan network data, which can better predict the default risks of\nenterprises compared to the state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 02:15:53 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 01:58:36 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 06:47:59 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 06:55:03 GMT"}, {"version": "v5", "created": "Tue, 29 Jun 2021 02:42:31 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Cheng", "Dawei", ""], ["Chen", "Chen", ""], ["Wang", "Xiaoyang", ""], ["Xiang", "Sheng", ""]]}, {"id": "1912.12535", "submitter": "Wenxin Li", "authors": "Wenxin Li", "title": "On the Asymptotic Optimality of Work-Conserving Disciplines in\n  Completion Time Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that under mild stochastic assumptions,\nwork-conserving disciplines are asymptotic optimal for minimizing total\ncompletion time. As a byproduct of our analysis, we obtain tight upper bound on\nthe competitive ratios of work-conserving disciplines on minimizing the metric\nof flow time.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 22:20:07 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 17:18:32 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 01:51:46 GMT"}, {"version": "v4", "created": "Sat, 12 Dec 2020 12:40:32 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Wenxin", ""]]}, {"id": "1912.12700", "submitter": "Jeremie Lagraviere", "authors": "J\\'er\\'emie Lagravi\\`ere, Johannes Langguth, Mohammed Sourouri, Phuong\n  H. Ha, Xing Cai", "title": "On the Performance and Energy Efficiency of the PGAS Programming Model\n  on Multicore Architectures", "comments": null, "journal-ref": "Published in: 2016 International Conference on High Performance\n  Computing & Simulation (HPCS) Date of Conference: 18-22 July 2016 Conference\n  Location: Innsbruck, Austria", "doi": "10.1109/HPCSim.2016.7568416", "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Using large-scale multicore systems to get the maximum performance and energy\nefficiency with manageable programmability is a major challenge. The\npartitioned global address space (PGAS) programming model enhances\nprogrammability by providing a global address space over large-scale computing\nsystems. However, so far the performance and energy efficiency of the PGAS\nmodel on multicore-based parallel architectures have not been investigated\nthoroughly. In this paper we use a set of selected kernels from the well-known\nNAS Parallel Benchmarks to evaluate the performance and energy efficiency of\nthe UPC programming language, which is a widely used implementation of the PGAS\nmodel. In addition, the MPI and OpenMP versions of the same parallel kernels\nare used for comparison with their UPC counterparts. The investigated hardware\nplatforms are based on multicore CPUs, both within a single 16-core node and\nacross multiple nodes involving up to 1024 physical cores. On the multi-node\nplatform we used the hardware measurement solution called High definition\nEnergy Efficiency Monitoring tool in order to measure energy. On the\nsingle-node system we used the hybrid measurement solution to make an effort\ninto understanding the observed performance differences, we use the Intel\nPerformance Counter Monitor to quantify in detail the communication time, cache\nhit/miss ratio and memory usage. Our experiments show that UPC is competitive\nwith OpenMP and MPI on single and multiple nodes, with respect to both the\nperformance and energy efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 17:51:10 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lagravi\u00e8re", "J\u00e9r\u00e9mie", ""], ["Langguth", "Johannes", ""], ["Sourouri", "Mohammed", ""], ["Ha", "Phuong H.", ""], ["Cai", "Xing", ""]]}, {"id": "1912.12701", "submitter": "Jeremie Lagraviere", "authors": "J\\'er\\'emie Lagravi\\`ere, Johannes Langguth, Martina Prugger, Lukas\n  Einkemmer, Phuong H. Ha, Xing Cai", "title": "Performance optimization and modeling of fine-grained irregular\n  communication in UPC", "comments": null, "journal-ref": "Scientific Programming Volume 2019, Article ID 6825728, 20 pages.\n  Hindawi", "doi": "10.1155/2019/6825728", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The UPC programming language offers parallelism via logically partitioned\nshared memory, which typically spans physically disjoint memory sub-systems.\nOne convenient feature of UPC is its ability to automatically execute\nbetween-thread data movement, such that the entire content of a shared data\narray appears to be freely accessible by all the threads. The programmer\nfriendliness, however, can come at the cost of substantial performance\npenalties. This is especially true when indirectly indexing the elements of a\nshared array, for which the induced between-thread data communication can be\nirregular and have a fine-grained pattern. In this paper we study performance\nenhancement strategies specifically targeting such fine-grained irregular\ncommunication in UPC. Starting from explicit thread privatization, continuing\nwith block-wise communication, and arriving at message condensing and\nconsolidation, we obtained considerable performance improvement of UPC programs\nthat originally require fine-grained irregular communication. Besides the\nperformance enhancement strategies, the main contribution of the present paper\nis to propose performance models for the different scenarios, in form of\nquantifiable formulas that hinge on the actual volumes of various data\nmovements plus a small number of easily obtainable hardware characteristic\nparameters. These performance models help to verify the enhancements obtained,\nwhile also providing insightful predictions of similar parallel\nimplementations, not limited to UPC, that also involve between-thread or\nbetween-process irregular communication. As a further validation, we also apply\nour performance modeling methodology and hardware characteristic parameters to\nan existing UPC code for solving a 2D heat equation on a uniform mesh.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 17:51:59 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lagravi\u00e8re", "J\u00e9r\u00e9mie", ""], ["Langguth", "Johannes", ""], ["Prugger", "Martina", ""], ["Einkemmer", "Lukas", ""], ["Ha", "Phuong H.", ""], ["Cai", "Xing", ""]]}, {"id": "1912.12740", "submitter": "Maciej Besta", "authors": "Maciej Besta, Marc Fischer, Vasiliki Kalavri, Michael Kapralov,\n  Torsten Hoefler", "title": "Practice of Streaming Processing of Dynamic Graphs: Concepts, Models,\n  and Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing has become an important part of various areas of computing,\nincluding machine learning, medical applications, social network analysis,\ncomputational sciences, and others. A growing amount of the associated graph\nprocessing workloads are dynamic, with millions of edges added or removed per\nsecond. Graph streaming frameworks are specifically crafted to enable the\nprocessing of such highly dynamic workloads. Recent years have seen the\ndevelopment of many such frameworks. However, they differ in their general\narchitectures (with key details such as the support for the concurrent\nexecution of graph updates and queries, or the incorporated graph data\norganization), the types of updates and workloads allowed, and many others. To\nfacilitate the understanding of this growing field, we provide the first\nanalysis and taxonomy of dynamic and streaming graph processing. We focus on\nidentifying the fundamental system designs and on understanding their support\nfor concurrency, and for different graph updates as well as analytics\nworkloads. We also crystallize the meaning of different concepts associated\nwith streaming graph processing, such as dynamic, temporal, online, and\ntime-evolving graphs, edge-centric processing, models for the maintenance of\nupdates, and graph databases. Moreover, we provide a bridge with the very rich\nlandscape of graph streaming theory by giving a broad overview of recent\ntheoretical related advances, and by discussing which graph streaming models\nand settings could be helpful in developing more powerful streaming frameworks\nand designs. We also outline graph streaming workloads and research challenges.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 21:41:14 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:24:01 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 13:00:50 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Besta", "Maciej", ""], ["Fischer", "Marc", ""], ["Kalavri", "Vasiliki", ""], ["Kapralov", "Michael", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.12795", "submitter": "Qinghe Jing", "authors": "Qinghe Jing, Weiyan Wang, Junxue Zhang, Han Tian, Kai Chen", "title": "Quantifying the Performance of Federated Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of data and isolated data islands encourage different\norganizations to share data with each other to train machine learning models.\nHowever, there are increasing concerns on the problems of data privacy and\nsecurity, which urges people to seek a solution like Federated Transfer\nLearning (FTL) to share training data without violating data privacy. FTL\nleverages transfer learning techniques to utilize data from different sources\nfor training, while achieving data privacy protection without significant\naccuracy loss. However, the benefits come with a cost of extra computation and\ncommunication consumption, resulting in efficiency problems. In order to\nefficiently deploy and scale up FTL solutions in practice, we need a deep\nunderstanding on how the infrastructure affects the efficiency of FTL. Our\npaper tries to answer this question by quantitatively measuring a real-world\nFTL implementation FATE on Google Cloud. According to the results of carefully\ndesigned experiments, we verified that the following bottlenecks can be further\noptimized: 1) Inter-process communication is the major bottleneck; 2) Data\nencryption adds considerable computation overhead; 3) The Internet networking\ncondition affects the performance a lot when the model is large.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 03:10:00 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Jing", "Qinghe", ""], ["Wang", "Weiyan", ""], ["Zhang", "Junxue", ""], ["Tian", "Han", ""], ["Chen", "Kai", ""]]}, {"id": "1912.13285", "submitter": "Nail Akar", "authors": "Omer Gursoy and Kamal Adli Mehr and Nail Akar", "title": "The MAP/M/s+G Call Center Model with General Patience Times: Stationary\n  Solutions and First Passage Times", "comments": "21 pages, 14 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the MAP/M/s+G queuing model with MAP (Markovian Arrival Process)\narrivals, exponentially distributed service times, infinite waiting room, and\ngenerally distributed patience times. Using sample-path arguments, we propose\nto obtain the steady-state distribution of the virtual waiting time and\nsubsequently the other relevant performance metrics of interest for the\nMAP/M/s+G queue by means of finding the steady-state solution of a properly\nconstructed Continuous Feedback Fluid Queue (CFFQ). The proposed method is\nexact when the patience time is a discrete random variable and is\nasymptotically exact when it is continuous/hybrid for which case discretization\nof the patience time distribution and subsequently the steady-state solution of\na Multi-Regime Markov Fluid Queue (MRMFQ) is required. Besides the steady-state\ndistribution, we also propose a new method to approximately obtain the first\npassage time distribution for the virtual and actual waiting times in the\n$MAP/M/s+G$ queue. Again, using sample-path arguments, finding the desired\ndistribution is also shown to reduce to obtaining the steady-state solution of\na larger dimensionality CFFQ where the deterministic time horizon is to be\napproximated by Erlang or Concentrated Matrix Exponential (CME) distributions.\nNumerical results are presented to validate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 12:15:30 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gursoy", "Omer", ""], ["Mehr", "Kamal Adli", ""], ["Akar", "Nail", ""]]}, {"id": "1912.13452", "submitter": "Jiri Borovec", "authors": "Jiri Borovec", "title": "BIRL: Benchmark on Image Registration methods with Landmark validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This report presents a generic image registration benchmark with automatic\nevaluation using landmark annotations. The key features of the BIRL framework\nare: easily extendable, performance evaluation, parallel experimentation,\nsimple visualisations, experiment's time-out limit, resuming unfinished\nexperiments. From the research practice, we identified and focused on these two\nmain use-cases: (a) comparison of user's (newly developed) method with some\nState-of-the-Art (SOTA) methods on a common dataset and (b) experimenting SOTA\nmethods on user's custom dataset (which should contain landmark annotation).\nMoreover, we present an integration of several standard image registration\nmethods aiming at biomedical imaging into the BIRL framework. This report also\ncontains experimental results of these SOTA methods on the CIMA dataset, which\nis a dataset of Whole Slice Imaging (WSI) from histology/pathology containing\nseveral multi-stain tissue samples from three tissue kinds. Source and results:\nhttps://borda.github.io/BIRL\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:50:12 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 09:56:17 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Borovec", "Jiri", ""]]}]