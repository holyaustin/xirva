[{"id": "2008.00176", "submitter": "Furkan Eris", "authors": "Furkan Eris, Sadullah Canakci, Cansu Demirkiran, Ajay Joshi", "title": "Custom Tailored Suite of Random Forests for Prefetcher Adaptation", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To close the gap between memory and processors, and in turn improve\nperformance, there has been an abundance of work in the area of\ndata/instruction prefetcher designs. Prefetchers are deployed in each level of\nthe memory hierarchy, but typically, each prefetcher gets designed without\ncomprehensively accounting for other prefetchers in the system. As a result,\nthese individual prefetcher designs do not always complement each other, and\nthat leads to low average performance gains and/or many negative outliers. In\nthis work, we propose SuitAP (Suite of random forests for Adaptation of\nPrefetcher system configuration), which is a hardware prefetcher adapter that\nuses a suite of random forests to determine at runtime which prefetcher should\nbe ON at each memory level, such that they complement each other. Compared to a\ndesign with no prefetchers, using SuitAP we improve IPC by 46% on average\nacross traces generated from SPEC2017 suite with 12KB overhead. Moreover, we\nalso reduce negative outliers using SuitAP.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 05:43:38 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Eris", "Furkan", ""], ["Canakci", "Sadullah", ""], ["Demirkiran", "Cansu", ""], ["Joshi", "Ajay", ""]]}, {"id": "2008.00842", "submitter": "Paris Carbone", "authors": "Marios Fragkoulis, Paris Carbone, Vasiliki Kalavri, Asterios\n  Katsifodimos", "title": "A Survey on the Evolution of Stream Processing Systems", "comments": "34 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream processing has been an active research field for more than 20 years,\nbut it is now witnessing its prime time due to recent successful efforts by the\nresearch community and numerous worldwide open-source communities. This survey\nprovides a comprehensive overview of fundamental aspects of stream processing\nsystems and their evolution in the functional areas of out-of-order data\nmanagement, state management, fault tolerance, high availability, load\nmanagement, elasticity, and reconfiguration. We review noteworthy past research\nfindings, outline the similarities and differences between early ('00-'10) and\nmodern ('11-'18) streaming systems, and discuss recent trends and open\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:43:46 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Fragkoulis", "Marios", ""], ["Carbone", "Paris", ""], ["Kalavri", "Vasiliki", ""], ["Katsifodimos", "Asterios", ""]]}, {"id": "2008.01040", "submitter": "Samuel Kaufman", "authors": "Samuel J. Kaufman, Phitchaya Mangpo Phothilimthana, Yanqi Zhou,\n  Charith Mendis, Sudip Roy, Amit Sabne, and Mike Burrows", "title": "A Learned Performance Model for Tensor Processing Units", "comments": "A version will appear in the Proceedings of the 4th MLSys Conference,\n  San Jose, CA, USA, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate hardware performance models are critical to efficient code\ngeneration. They can be used by compilers to make heuristic decisions, by\nsuperoptimizers as a minimization objective, or by autotuners to find an\noptimal configuration for a specific program. However, they are difficult to\ndevelop because contemporary processors are complex, and the recent\nproliferation of deep learning accelerators has increased the development\nburden. We demonstrate a method of learning performance models from a corpus of\ntensor computation graph programs for Tensor Processing Unit (TPU) instances.\nWe show that our learned model outperforms a heavily-optimized analytical\nperformance model on two tasks -- tile-size selection and operator fusion --\nand that it helps an autotuner discover faster programs in a setting where\naccess to TPUs is limited or expensive.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:24:52 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 04:49:15 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Kaufman", "Samuel J.", ""], ["Phothilimthana", "Phitchaya Mangpo", ""], ["Zhou", "Yanqi", ""], ["Mendis", "Charith", ""], ["Roy", "Sudip", ""], ["Sabne", "Amit", ""], ["Burrows", "Mike", ""]]}, {"id": "2008.01827", "submitter": "Somalee Datta", "authors": "Joseph Mesterhazy, Garrick Olson, Somalee Datta", "title": "High performance on-demand de-identification of a petabyte-scale medical\n  imaging data lake", "comments": "11 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in Artificial Intelligence driven approaches, researchers\nare requesting unprecedented volumes of medical imaging data which far exceed\nthe capacity of traditional on-premise client-server approaches for making the\ndata research analysis-ready. We are making available a flexible solution for\non-demand de-identification that combines the use of mature software\ntechnologies with modern cloud-based distributed computing techniques to enable\nfaster turnaround in medical imaging research. The solution is part of a\nbroader platform that supports a secure high performance clinical data science\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 21:01:30 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Mesterhazy", "Joseph", ""], ["Olson", "Garrick", ""], ["Datta", "Somalee", ""]]}, {"id": "2008.03478", "submitter": "Youri Raaijmakers", "authors": "Youri Raaijmakers, Sem Borst", "title": "Achievable Stability in Redundancy Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a system with $N$ parallel servers where incoming jobs are\nimmediately replicated to, say, $d$ servers. Each of the $N$ servers has its\nown queue and follows a FCFS discipline. As soon as the first job replica is\ncompleted, the remaining replicas are abandoned. We investigate the achievable\nstability region for a quite general workload model with different job types\nand heterogeneous servers, reflecting job-server affinity relations which may\narise from data locality issues and soft compatibility constraints. Under the\nassumption that job types are known beforehand we show for New-Better-than-Used\n(NBU) distributed speed variations that no replication $(d=1)$ gives a strictly\nlarger stability region than replication $(d>1)$. Strikingly, this does not\ndepend on the underlying distribution of the intrinsic job sizes, but observing\nthe job types is essential for this statement to hold. In case of\nnon-observable job types we show that for New-Worse-than-Used (NWU) distributed\nspeed variations full replication ($d=N$) gives a larger stability region than\nno replication $(d=1)$.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 08:54:10 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Raaijmakers", "Youri", ""], ["Borst", "Sem", ""]]}, {"id": "2008.03485", "submitter": "Leonid Sokolinsky", "authors": "Leonid B. Sokolinsky", "title": "BSF: a parallel computation model for scalability estimation of\n  iterative numerical algorithms on cluster computing systems", "comments": null, "journal-ref": "Journal of Parallel and Distributed Computing, 2021, Volume 149,\n  Pages 193-206", "doi": "10.1016/j.jpdc.2020.12.009", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines a new parallel computation model called bulk synchronous\nfarm (BSF) that focuses on estimating the scalability of compute-intensive\niterative algorithms aimed at cluster computing systems. In the BSF model, a\ncomputer is a set of processor nodes connected by a network and organized\naccording to the master/slave paradigm. A cost metric of the BSF model is\npresented. This cost metric requires the algorithm to be represented in the\nform of operations on lists. This allows us to derive an equation that predicts\nthe scalability boundary of a parallel program: the maximum number of processor\nnodes after which the speedup begins to decrease. The paper includes several\nexamples of applying the BSF model to designing and analyzing parallel\nnu-merical algorithms. The large-scale computational experiments conducted on a\ncluster computing system confirm the adequacy of the analytical estimations\nobtained using the BSF model.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 09:47:23 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 15:09:09 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 07:20:54 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2020 03:02:42 GMT"}, {"version": "v5", "created": "Sat, 2 Jan 2021 14:50:05 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sokolinsky", "Leonid B.", ""]]}, {"id": "2008.03904", "submitter": "Sumit Mandal", "authors": "Sumit K. Mandal, Anish Krishnakumar, Raid Ayoub, Michael Kishinevsky,\n  Umit Y. Ogras", "title": "Performance Analysis of Priority-Aware NoCs with Deflection Routing\n  under Traffic Congestion", "comments": "This article is in the Proceedings of ICCAD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priority-aware networks-on-chip (NoCs) are used in industry to achieve\npredictable latency under different workload conditions. These NoCs incorporate\ndeflection routing to minimize queuing resources within routers and achieve low\nlatency during low traffic load. However, deflected packets can exacerbate\ncongestion during high traffic load since they consume the NoC bandwidth.\nState-of-the-art analytical models for priority-aware NoCs ignore deflected\ntraffic despite its significant latency impact during congestion. This paper\nproposes a novel analytical approach to estimate end-to-end latency of\npriority-aware NoCs with deflection routing under bursty and heavy traffic\nscenarios. Experimental evaluations show that the proposed technique\noutperforms alternative approaches and estimates the average latency for real\napplications with less than 8% error compared to cycle-accurate simulations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 05:17:54 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 01:16:55 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 00:21:09 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Mandal", "Sumit K.", ""], ["Krishnakumar", "Anish", ""], ["Ayoub", "Raid", ""], ["Kishinevsky", "Michael", ""], ["Ogras", "Umit Y.", ""]]}, {"id": "2008.04693", "submitter": "Eunhyeok Park", "authors": "Eunhyeok Park and Sungjoo Yoo", "title": "PROFIT: A Novel Training Method for sub-4-bit MobileNet Models", "comments": "Published at ECCV2020, spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  4-bit and lower precision mobile models are required due to the\never-increasing demand for better energy efficiency in mobile devices. In this\nwork, we report that the activation instability induced by weight quantization\n(AIWQ) is the key obstacle to sub-4-bit quantization of mobile networks. To\nalleviate the AIWQ problem, we propose a novel training method called\nPROgressive-Freezing Iterative Training (PROFIT), which attempts to freeze\nlayers whose weights are affected by the instability problem stronger than the\nother layers. We also propose a differentiable and unified quantization method\n(DuQ) and a negative padding idea to support asymmetric activation functions\nsuch as h-swish. We evaluate the proposed methods by quantizing MobileNet-v1,\nv2, and v3 on ImageNet and report that 4-bit quantization offers comparable\n(within 1.48 % top-1 accuracy) accuracy to full precision baseline. In the\nablation study of the 3-bit quantization of MobileNet-v3, our proposed method\noutperforms the state-of-the-art method by a large margin, 12.86 % of top-1\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:29:50 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Park", "Eunhyeok", ""], ["Yoo", "Sungjoo", ""]]}, {"id": "2008.04751", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Yimeng Zhang, Xiongchang Liu, Song Bai, Site Li, Jane\n  You", "title": "Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation\n  in Autonomous Driving", "comments": "Accepted to IEEE Transactions on Intelligent Transportation Systems\n  (T-ITS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is important for many real-world systems, e.g.,\nautonomous vehicles, which predict the class of each pixel. Recently, deep\nnetworks achieved significant progress w.r.t. the mean Intersection-over Union\n(mIoU) with the cross-entropy loss. However, the cross-entropy loss can\nessentially ignore the difference of severity for an autonomous car with\ndifferent wrong prediction mistakes. For example, predicting the car to the\nroad is much more servery than recognize it as the bus. Targeting for this\ndifficulty, we develop a Wasserstein training framework to explore the\ninter-class correlation by defining its ground metric as misclassification\nseverity. The ground metric of Wasserstein distance can be pre-defined\nfollowing the experience on a specific task. From the optimization perspective,\nwe further propose to set the ground metric as an increasing function of the\npre-defined ground metric. Furthermore, an adaptively learning scheme of the\nground matrix is proposed to utilize the high-fidelity CARLA simulator.\nSpecifically, we follow a reinforcement alternative learning scheme. The\nexperiments on both CamVid and Cityscapes datasets evidenced the effectiveness\nof our Wasserstein loss. The SegNet, ENet, FCN and Deeplab networks can be\nadapted following a plug-in manner. We achieve significant improvements on the\npredefined important classes, and much longer continuous playtime in our\nsimulator.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 15:00:41 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Zhang", "Yimeng", ""], ["Liu", "Xiongchang", ""], ["Bai", "Song", ""], ["Li", "Site", ""], ["You", "Jane", ""]]}, {"id": "2008.04853", "submitter": "Praveen Damacharla", "authors": "Praveen Damacharla, Dhwani Mehta, Ahmad Y Javaid, Vijay K.\n  Devabhaktuni", "title": "Study on State-of-the-art Cloud Services Integration Capabilities with\n  Autonomous Ground Vehicles", "comments": null, "journal-ref": "2018 IEEE 88th Vehicular Technology Conference (VTC-Fall),\n  Chicago, IL, USA, 2018, pp. 1-5", "doi": "10.1109/VTCFall.2018.8690650", "report-no": null, "categories": "cs.CY cs.CC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing and intelligence are substantial requirements for the accurate\nperformance of autonomous ground vehicles (AGVs). In this context, the use of\ncloud services in addition to onboard computers enhances computing and\nintelligence capabilities of AGVs. In addition, the vast amount of data\nprocessed in a cloud system contributes to overall performance and capabilities\nof the onboard system. This research study entails a qualitative analysis to\ngather insights on the applicability of the leading cloud service providers in\nAGV operations. These services include Google Cloud, Microsoft Azure, Amazon\nAWS, and IBM Cloud. The study begins with a brief review of AGV technical\nrequirements that are necessary to determine the rationale for identifying the\nmost suitable cloud service. The qualitative analysis studies and addresses the\napplicability of the cloud service over the proposed generalized AGV's\narchitecture integration, performance, and manageability. Our findings conclude\nthat a generalized AGV architecture can be supported by state-of-the-art cloud\nservice, but there should be a clear line of separation between the primary and\nsecondary computing needs. Moreover, our results show significant lags while\nusing cloud services and preventing their use in real-time AGV operation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:56:14 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Damacharla", "Praveen", ""], ["Mehta", "Dhwani", ""], ["Javaid", "Ahmad Y", ""], ["Devabhaktuni", "Vijay K.", ""]]}, {"id": "2008.05631", "submitter": "Mingyue Ji", "authors": "Nicholas Woolsey, Xingyue Wang, Rong-Rong Chen, Mingyue Ji", "title": "FLCD: A Flexible Low Complexity Design of Coded Distributed Computing", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible low complexity design (FLCD) of coded distributed\ncomputing (CDC) with empirical evaluation on Amazon Elastic Compute Cloud\n(Amazon EC2). CDC can expedite MapReduce like computation by trading increased\nmap computations to reduce communication load and shuffle time. A main novelty\nof FLCD is to utilize the design freedom in defining map and reduce functions\nto develop asymptotic homogeneous systems to support varying intermediate\nvalues (IV) sizes under a general MapReduce framework. Compared to existing\ndesigns with constant IV sizes, FLCD offers greater flexibility in adapting to\nnetwork parameters and significantly reduces the implementation complexity by\nrequiring fewer input files and shuffle groups. The FLCD scheme is the first\nproposed low-complexity CDC design that can operate on a network with an\narbitrary number of nodes and computation load. We perform empirical\nevaluations of the FLCD by executing the TeraSort algorithm on an Amazon EC2\ncluster. This is the first time that theoretical predictions of the CDC shuffle\ntime are validated by empirical evaluations. The evaluations demonstrate a 2.0\nto 4.24x speedup compared to conventional uncoded MapReduce, a 12% to 52%\nreduction in total time, and a wider range of operating network parameters\ncompared to existing CDC schemes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 01:07:58 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Woolsey", "Nicholas", ""], ["Wang", "Xingyue", ""], ["Chen", "Rong-Rong", ""], ["Ji", "Mingyue", ""]]}, {"id": "2008.06135", "submitter": "Hongmei He Ph.D", "authors": "Hongmei He, Mengyuan Chen, Gang Xu, Zhilong Zhu, Zhenhuan Zhu", "title": "Learnability and Robustness of Shallow Neural Networks Learned With a\n  Performance-Driven BP and a Variant PSO For Edge Decision-Making", "comments": "36 pages, 21 figues for corresponding eps files. Neural Comput &\n  Applic (2021)", "journal-ref": null, "doi": "10.1007/s00521-021-06019-1", "report-no": null, "categories": "cs.NE cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases, the computing resources are limited without the benefit from\nGPU, especially in the edge devices of IoT enabled systems. It may not be easy\nto implement complex AI models in edge devices. The Universal Approximation\nTheorem states that a shallow neural network (SNN) can represent any nonlinear\nfunction. However, how fat is an SNN enough to solve a nonlinear\ndecision-making problem in edge devices? In this paper, we focus on the\nlearnability and robustness of SNNs, obtained by a greedy tight force heuristic\nalgorithm (performance driven BP) and a loose force meta-heuristic algorithm (a\nvariant of PSO). Two groups of experiments are conducted to examine the\nlearnability and the robustness of SNNs with Sigmoid activation,\nlearned/optimised by KPI-PDBPs and KPI-VPSOs, where, KPIs (key performance\nindicators: error (ERR), accuracy (ACC) and $F_1$ score) are the objectives,\ndriving the searching process. An incremental approach is applied to examine\nthe impact of hidden neuron numbers on the performance of SNNs,\nlearned/optimised by KPI-PDBPs and KPI-VPSOs. From the engineering prospective,\nall sensors are well justified for a specific task. Hence, all sensor readings\nshould be strongly correlated to the target. Therefore, the structure of an SNN\nshould depend on the dimensions of a problem space. The experimental results\nshow that the number of hidden neurons up to the dimension number of a problem\nspace is enough; the learnability of SNNs, produced by KPI-PDBP, is better than\nthat of SNNs, optimized by KPI-VPSO, regarding the performance and learning\ntime on the training data sets; the robustness of SNNs learned by KPI-PDBPs and\nKPI-VPSOs depends on the data sets; and comparing with other classic machine\nlearning models, ACC-PDBPs win for almost all tested data sets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 23:33:00 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["He", "Hongmei", ""], ["Chen", "Mengyuan", ""], ["Xu", "Gang", ""], ["Zhu", "Zhilong", ""], ["Zhu", "Zhenhuan", ""]]}, {"id": "2008.06152", "submitter": "Kazuichi Oe", "authors": "Kazuichi Oe", "title": "Consideration for effectively handling parallel workloads on public\n  cloud system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We retrieved and analyzed parallel storage workloads of the FUJITSU K5 cloud\nservice to clarify how to build cost-effective hybrid storage systems. A hybrid\nstorage system consists of fast but low-capacity tier (first tier) and slow but\nhigh-capacity tier (second tier). And, it typically consists of either SSDs and\nHDDs or NVMs and SSDs. As a result, we found that 1) regions for first tier\nshould be assigned only if a workload includes large number of IO accesses for\na whole day, 2) the regions that include a large number of IO accesses should\nbe dynamically chosen and moved from second tier to first tier for a short\ninterval, and 3) if a cache hit ratio is regularly low, use of the cache for\nthe workload should be cancelled, and the whole workload region should be\nassigned to the region for first tier. These workloads already have been\nreleased from the SNIA web site.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 01:18:04 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Oe", "Kazuichi", ""]]}, {"id": "2008.06571", "submitter": "Xingfu Wu", "authors": "Xingfu Wu and Aniruddha Marathe and Siddhartha Jana and Ondrej Vysocky\n  and Jophin John and Andrea Bartolini and Lubomir Riha and Michael Gerndt and\n  Valerie Taylor and Sridutt Bhalachandra", "title": "Toward an End-to-End Auto-tuning Framework in HPC PowerStack", "comments": "to be published in Energy Efficient HPC State of Practice 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently utilizing procured power and optimizing performance of scientific\napplications under power and energy constraints are challenging. The HPC\nPowerStack defines a software stack to manage power and energy of\nhigh-performance computing systems and standardizes the interfaces between\ndifferent components of the stack. This survey paper presents the findings of a\nworking group focused on the end-to-end tuning of the PowerStack. First, we\nprovide a background on the PowerStack layer-specific tuning efforts in terms\nof their high-level objectives, the constraints and optimization goals,\nlayer-specific telemetry, and control parameters, and we list the existing\nsoftware solutions that address those challenges. Second, we propose the\nPowerStack end-to-end auto-tuning framework, identify the opportunities in\nco-tuning different layers in the PowerStack, and present specific use cases\nand solutions. Third, we discuss the research opportunities and challenges for\ncollective auto-tuning of two or more management layers (or domains) in the\nPowerStack. This paper takes the first steps in identifying and aggregating the\nimportant R&D challenges in streamlining the optimization efforts across the\nlayers of the PowerStack.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 20:57:47 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wu", "Xingfu", ""], ["Marathe", "Aniruddha", ""], ["Jana", "Siddhartha", ""], ["Vysocky", "Ondrej", ""], ["John", "Jophin", ""], ["Bartolini", "Andrea", ""], ["Riha", "Lubomir", ""], ["Gerndt", "Michael", ""], ["Taylor", "Valerie", ""], ["Bhalachandra", "Sridutt", ""]]}, {"id": "2008.06823", "submitter": "Neil J. Gunther", "authors": "Neil J. Gunther", "title": "Erlang Redux: An Ansatz Method for Solving the M/M/m Queue", "comments": "13 pages, 7 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This exposition presents a novel approach to solving an M/M/m queue for the\nwaiting time and the residence time. The motivation comes from an algebraic\nsolution for the residence time of the M/M/1 queue. The key idea is the\nintroduction of an ansatz transformation, defined in terms of the Erlang B\nfunction, that avoids the more opaque derivation based on applied probability\ntheory. The only prerequisite is an elementary knowledge of the Poisson\ndistribution, which is already necessary for understanding the M/M/1 queue. The\napproach described here supersedes our earlier approximate morphing\ntransformation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 02:50:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gunther", "Neil J.", ""]]}, {"id": "2008.06991", "submitter": "Ian T Foster", "authors": "Tong Shu, Yanfei Guo, Justin Wozniak, Xiaoning Ding, Ian Foster,\n  Tahsin Kurc", "title": "In-situ Workflow Auto-tuning via Combining Performance Models of\n  Component Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In-situ parallel workflows couple multiple component applications, such as\nsimulation and analysis, via streaming data transfer. in order to avoid data\nexchange via shared file systems. Such workflows are challenging to configure\nfor optimal performance due to the large space of possible configurations.\nExpert experience is rarely sufficient to identify optimal configurations, and\nexisting empirical auto-tuning approaches are inefficient due to the high cost\nof obtaining training data for machine learning models. It is also infeasible\nto optimize individual components independently, due to component interactions.\nWe propose here a new auto-tuning method, Component-based Ensemble Active\nLearning (CEAL), that combines machine learning techniques with knowledge of\nin-situ workflow structure to enable automated workflow configuration with a\nlimited number of performance measurements.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 20:37:02 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Shu", "Tong", ""], ["Guo", "Yanfei", ""], ["Wozniak", "Justin", ""], ["Ding", "Xiaoning", ""], ["Foster", "Ian", ""], ["Kurc", "Tahsin", ""]]}, {"id": "2008.07562", "submitter": "Daan Rutten", "authors": "Daan Rutten and Debankur Mukherjee", "title": "Load Balancing Under Strict Compatibility Constraints", "comments": "51 pages, 5 figures. 08/23/2020: Minor errors fixed in the proof of\n  Theorem 4.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale systems operating under the JSQ$(d)$ policy in the\npresence of stringent task-server compatibility constraints. Consider a system\nwith $N$ identical single-server queues and $M(N)$ task types, where each\nserver is able to process only a small subset of possible task types. Each\narriving task selects $d\\geq 2$ random servers compatible to its type, and\njoins the shortest queue among them. The compatibility constraint is naturally\ncaptured by a fixed bipartite graph $G_N$ between the servers and the task\ntypes. When $G_N$ is complete bipartite, the meanfield approximation is proven\nto be accurate. However, such dense compatibility graphs are infeasible due to\ntheir overwhelming implementation cost and prohibitive storage capacity\nrequirement at the servers. Our goal in this paper is to characterize the class\nof sparse compatibility graphs for which the meanfield approximation remains\nvalid.\n  To achieve this, first, we introduce a novel graph expansion-based notion,\ncalled proportional sparsity, and establish that systems with proportionally\nsparse compatibility graphs match the performance of a fully flexible system,\nasymptotically in the large-system limit. Furthermore, for any $c(N)$\nsatisfying $$\\frac{Nc(N)}{M(N)\\ln(N)}\\to \\infty\\quad \\text{and}\\quad c(N)\\to\n\\infty,$$ as $N\\to\\infty$, we show that proportionally sparse random\ncompatibility graphs can be designed, so that the degree of each server is at\nmost $c(N)$. This reduces the server-degree almost by a factor $N/\\ln(N)$,\ncompared to the complete bipartite compatibility graph, while maintaining the\nsame asymptotic performance. Extensive simulation experiments are conducted to\ncorroborate the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 18:24:57 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 17:23:21 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Rutten", "Daan", ""], ["Mukherjee", "Debankur", ""]]}, {"id": "2008.07585", "submitter": "Fernando Freire Scattone", "authors": "Fernando Freire Scattone and Kelly Rosa Braghetto", "title": "A Microservices Architecture for Distributed Complex Event Processing in\n  Smart Cities", "comments": "Published in: 2018 IEEE 37th International Symposium on Reliable\n  Distributed Systems Workshops (SRDSW)", "journal-ref": "2018 IEEE 37th International Symposium on Reliable Distributed\n  Systems Workshops (SRDSW), Salvador, Brazil, 2018, pp. 6-9", "doi": "10.1109/SRDSW.2018.00012", "report-no": null, "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A considerable volume of data is collected from sensors today and needs to be\nprocessed in real time. Complex Event Processing (CEP) is one of the most\nimportant techniques developed for this purpose. In CEP, each new sensor\nmeasurement is considered an event and new event types can be defined based on\nother events occurrence. There exists several open-source CEP implementations\ncurrently available, but all of them use orchestration to distribute event\nprocessing. This kind of architectural organization may harm system resilience,\nsince it relies on a central core (i.e. the orchestrator). Any failures in the\ncore might impact the whole system. Moreover, the core can become a bottleneck\non system performance. In this work, a choreography-based microservices\narchitecture is proposed for distributed CEP, in order to benefit from the low\ncoupling and greater horizontal scalability this kind of architecture provides.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 19:30:43 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Scattone", "Fernando Freire", ""], ["Braghetto", "Kelly Rosa", ""]]}, {"id": "2008.08057", "submitter": "Siddharth Samsi", "authors": "Siddharth Samsi, Andrew Prout, Michael Jones, Andrew Kirby, Bill\n  Arcand, Bill Bergeron, David Bestor, Chansup Byun, Vijay Gadepally, Michael\n  Houle, Matthew Hubbell, Anna Klein, Peter Michaleas, Lauren Milechin, Julie\n  Mullen, Antonio Rosa, Charles Yee, Albert Reuther, Jeremy Kepner", "title": "Benchmarking network fabrics for data distributed training of deep\n  neural networks", "comments": "Accepted for publication at IEEE HPEC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence/Machine Learning applications require the training of\ncomplex models on large amounts of labelled data. The large computational\nrequirements for training deep models have necessitated the development of new\nmethods for faster training. One such approach is the data parallel approach,\nwhere the training data is distributed across multiple compute nodes. This\napproach is simple to implement and supported by most of the commonly used\nmachine learning frameworks. The data parallel approach leverages MPI for\ncommunicating gradients across all nodes. In this paper, we examine the effects\nof using different physical hardware interconnects and network-related software\nprimitives for enabling data distributed deep learning. We compare the effect\nof using GPUDirect and NCCL on Ethernet and OmniPath fabrics. Our results show\nthat using Ethernet-based networking in shared HPC systems does not have a\nsignificant effect on the training times for commonly used deep neural network\narchitectures or traditional HPC applications such as Computational Fluid\nDynamics.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:38:30 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Samsi", "Siddharth", ""], ["Prout", "Andrew", ""], ["Jones", "Michael", ""], ["Kirby", "Andrew", ""], ["Arcand", "Bill", ""], ["Bergeron", "Bill", ""], ["Bestor", "David", ""], ["Byun", "Chansup", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""], ["Kepner", "Jeremy", ""]]}, {"id": "2008.08478", "submitter": "Yuhsiang Tsai", "authors": "Yuhsiang Mike Tsai, Terry Cojean, Hartwig Anzt", "title": "Evaluating the Performance of NVIDIA's A100 Ampere GPU for Sparse Linear\n  Algebra Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPU accelerators have become an important backbone for scientific high\nperformance computing, and the performance advances obtained from adopting new\nGPU hardware are significant. In this paper we take a first look at NVIDIA's\nnewest server line GPU, the A100 architecture part of the Ampere generation.\nSpecifically, we assess its performance for sparse linear algebra operations\nthat form the backbone of many scientific applications and assess the\nperformance improvements over its predecessor.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 14:38:07 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Tsai", "Yuhsiang Mike", ""], ["Cojean", "Terry", ""], ["Anzt", "Hartwig", ""]]}, {"id": "2008.08509", "submitter": "Haoran Qiu", "authors": "Haoran Qiu, Subho S. Banerjee, Saurabh Jha, Zbigniew T. Kalbarczyk,\n  Ravishankar K. Iyer", "title": "FIRM: An Intelligent Fine-Grained Resource Management Framework for\n  SLO-Oriented Microservices", "comments": "This paper was accepted in OSDI '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern user-facing latency-sensitive web services include numerous\ndistributed, intercommunicating microservices that promise to simplify software\ndevelopment and operation. However, multiplexing of compute resources across\nmicroservices is still challenging in production because contention for shared\nresources can cause latency spikes that violate the service-level objectives\n(SLOs) of user requests. This paper presents FIRM, an intelligent fine-grained\nresource management framework for predictable sharing of resources across\nmicroservices to drive up overall utilization. FIRM leverages online telemetry\ndata and machine-learning methods to adaptively (a) detect/localize\nmicroservices that cause SLO violations, (b) identify low-level resources in\ncontention, and (c) take actions to mitigate SLO violations via dynamic\nreprovisioning. Experiments across four microservice benchmarks demonstrate\nthat FIRM reduces SLO violations by up to 16x while reducing the overall\nrequested CPU limit by up to 62%. Moreover, FIRM improves performance\npredictability by reducing tail latencies by up to 11x.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 15:37:16 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 23:54:07 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Qiu", "Haoran", ""], ["Banerjee", "Subho S.", ""], ["Jha", "Saurabh", ""], ["Kalbarczyk", "Zbigniew T.", ""], ["Iyer", "Ravishankar K.", ""]]}, {"id": "2008.08830", "submitter": "Wentao Weng", "authors": "Wentao Weng, Xingyu Zhou, R. Srikant", "title": "Optimal Load Balancing in Bipartite Graphs", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in cloud platforms motivate the study of efficient load\nbalancing under job-server constraints and server heterogeneity. In this paper,\nwe study load balancing on a bipartite graph where left nodes correspond to job\ntypes and right nodes correspond to servers, with each edge indicating that a\njob type can be served by a server. Thus edges represent locality constraints,\ni.e., each job can only be served at servers which contained certain data\nand/or machine learning (ML) models. Servers in this system can have\nheterogeneous service rates. In this setting, we investigate the performance of\ntwo policies named Join-the-Fastest-of-the-Shortest-Queue (JFSQ) and\nJoin-the-Fastest-of-the-Idle-Queue (JFIQ), which are simple variants of\nJoin-the-Shortest-Queue and Join-the-Idle-Queue, where ties are broken in favor\nof the fastest servers. Under a \"well-connected\" graph condition, we show that\nJFSQ and JFIQ are asymptotically optimal in the mean response time when the\nnumber of servers goes to infinity. In addition to asymptotic optimality, we\nalso obtain upper bounds on the mean response time for finite-size systems. We\nfurther show that the well-connectedness condition can be satisfied by a random\nbipartite graph construction with relatively sparse connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 08:11:16 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Weng", "Wentao", ""], ["Zhou", "Xingyu", ""], ["Srikant", "R.", ""]]}, {"id": "2008.08883", "submitter": "Kris Nikov", "authors": "Kris Nikov (1), Mohammad Hosseinabady (1), Rafael Asenjo (2), Andr\\'es\n  Rodr\\'iguezz (2), Angeles Navarro (2) and Jose Nunez-Yanez (1) ((1)\n  University of Bristol, UK, (2) Universidad de M\\'alaga, Spain)", "title": "High-Performance Simultaneous Multiprocessing for Heterogeneous\n  System-on-Chip", "comments": "7 pages, 5 figures, 1 table Presented at the 13th International\n  Workshop on Programmability and Architectures for Heterogeneous Multicores,\n  2020 (arXiv:2005.07619)", "journal-ref": null, "doi": null, "report-no": "MULTIPROG/2020/4", "categories": "cs.DC cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a methodology for simultaneous heterogeneous computing,\nnamed ENEAC, where a quad core ARM Cortex-A53 CPU works in tandem with a\npreprogrammed on-board FPGA accelerator. A heterogeneous scheduler distributes\nthe tasks optimally among all the resources and all compute units run\nasynchronously, which allows for improved performance for irregular workloads.\nENEAC achieves up to 17\\% performance improvement \\ignore{and 14\\% energy usage\nreduction,} when using all platform resources compared to just using the FPGA\naccelerators and up to 865\\% performance increase \\ignore{and up to 89\\% energy\nusage decrease} when using just the CPU. The workflow uses existing commercial\ntools and C/C++ as a single programming language for both accelerator design\nand CPU programming for improved productivity and ease of verification.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:53:32 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Nikov", "Kris", ""], ["Hosseinabady", "Mohammad", ""], ["Asenjo", "Rafael", ""], ["Rodr\u00edguezz", "Andr\u00e9s", ""], ["Navarro", "Angeles", ""], ["Nunez-Yanez", "Jose", ""]]}, {"id": "2008.08886", "submitter": "Daniele De Sensi PhD", "authors": "Daniele De Sensi, Salvatore Di Girolamo, Kim H. McMahon, Duncan\n  Roweth, Torsten Hoefler", "title": "An In-Depth Analysis of the Slingshot Interconnect", "comments": "To be published in Proceedings of The International Conference for\n  High Performance Computing Networking, Storage, and Analysis (SC '20) (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interconnect is one of the most critical components in large scale\ncomputing systems, and its impact on the performance of applications is going\nto increase with the system size. In this paper, we will describe Slingshot, an\ninterconnection network for large scale computing systems. Slingshot is based\non high-radix switches, which allow building exascale and hyperscale\ndatacenters networks with at most three switch-to-switch hops. Moreover,\nSlingshot provides efficient adaptive routing and congestion control\nalgorithms, and highly tunable traffic classes. Slingshot uses an optimized\nEthernet protocol, which allows it to be interoperable with standard Ethernet\ndevices while providing high performance to HPC applications. We analyze the\nextent to which Slingshot provides these features, evaluating it on\nmicrobenchmarks and on several applications from the datacenter and AI worlds,\nas well as on HPC applications. We find that applications running on Slingshot\nare less affected by congestion compared to previous generation networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:55:27 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["De Sensi", "Daniele", ""], ["Di Girolamo", "Salvatore", ""], ["McMahon", "Kim H.", ""], ["Roweth", "Duncan", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2008.09037", "submitter": "Matthew Hutchinson", "authors": "Matthew Hutchinson, Siddharth Samsi, William Arcand, David Bestor,\n  Bill Bergeron, Chansup Byun, Micheal Houle, Matthew Hubbell, Micheal Jones,\n  Jeremy Kepner, Andrew Kirby, Peter Michaleas, Lauren Milechin, Julie Mullen,\n  Andrew Prout, Antonio Rosa, Albert Reuther, Charles Yee, Vijay Gadepally", "title": "Accuracy and Performance Comparison of Video Action Recognition\n  Approaches", "comments": "Accepted for publication at IEEE HPEC 2020", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286249", "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, there has been significant interest in video action\nrecognition systems and models. However, direct comparison of accuracy and\ncomputational performance results remain clouded by differing training\nenvironments, hardware specifications, hyperparameters, pipelines, and\ninference methods. This article provides a direct comparison between fourteen\noff-the-shelf and state-of-the-art models by ensuring consistency in these\ntraining characteristics in order to provide readers with a meaningful\ncomparison across different types of video action recognition algorithms.\nAccuracy of the models is evaluated using standard Top-1 and Top-5 accuracy\nmetrics in addition to a proposed new accuracy metric. Additionally, we compare\ncomputational performance of distributed training from two to sixty-four GPUs\non a state-of-the-art HPC system.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 15:42:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Hutchinson", "Matthew", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Houle", "Micheal", ""], ["Hubbell", "Matthew", ""], ["Jones", "Micheal", ""], ["Kepner", "Jeremy", ""], ["Kirby", "Andrew", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Reuther", "Albert", ""], ["Yee", "Charles", ""], ["Gadepally", "Vijay", ""]]}, {"id": "2008.09590", "submitter": "Majid Raeis", "authors": "Majid Raeis, Ali Tizghadam and Alberto Leon-Garcia", "title": "Reinforcement Learning-based Admission Control in Delay-sensitive\n  Service Systems", "comments": "7 pages, to be presented at IEEE GLOBECOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AI cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring quality of service (QoS) guarantees in service systems is a\nchallenging task, particularly when the system is composed of more fine-grained\nservices, such as service function chains. An important QoS metric in service\nsystems is the end-to-end delay, which becomes even more important in\ndelay-sensitive applications, where the jobs must be completed within a time\ndeadline. Admission control is one way of providing end-to-end delay guarantee,\nwhere the controller accepts a job only if it has a high probability of meeting\nthe deadline. In this paper, we propose a reinforcement learning-based\nadmission controller that guarantees a probabilistic upper-bound on the\nend-to-end delay of the service system, while minimizes the probability of\nunnecessary rejections. Our controller only uses the queue length information\nof the network and requires no knowledge about the network topology or system\nparameters. Since long-term performance metrics are of great importance in\nservice systems, we take an average-reward reinforcement learning approach,\nwhich is well suited to infinite horizon problems. Our evaluations verify that\nthe proposed RL-based admission controller is capable of providing\nprobabilistic bounds on the end-to-end delay of the network, without using\nsystem model information.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 17:33:55 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Raeis", "Majid", ""], ["Tizghadam", "Ali", ""], ["Leon-Garcia", "Alberto", ""]]}, {"id": "2008.10169", "submitter": "Vikram Sharma Mailthody", "authors": "Zaid Qureshi, Vikram Sharma Mailthody, Seung Won Min, I-Hsin Chung,\n  Jinjun Xiong, Wen-mei Hwu", "title": "Tearing Down the Memory Wall", "comments": "SRC Techcon 2020 paper. Discusses vision of GPU-Centric architecture,\n  Erudite", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a vision for the Erudite architecture that redefines the compute\nand memory abstractions such that memory bandwidth and capacity become\nfirst-class citizens along with compute throughput. In this architecture, we\nenvision coupling a high-density, massively parallel memory technology like\nFlash with programmable near-data accelerators, like the streaming\nmultiprocessors in modern GPUs. Each accelerator has a local pool of\nstorage-class memory that it can access at high throughput by initiating very\nlarge numbers of overlapping requests that help to tolerate long access\nlatency. The accelerators can also communicate with each other and remote\nmemory through a high-throughput low-latency interconnect. As a result, systems\nbased on the Erudite architecture scale compute and memory bandwidth at the\nsame rate, tearing down the notorious memory wall that has plagued computer\narchitecture for generations. In this paper, we present the motivation,\nrationale, design, benefit, and research challenges for Erudite.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 03:07:23 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Qureshi", "Zaid", ""], ["Mailthody", "Vikram Sharma", ""], ["Min", "Seung Won", ""], ["Chung", "I-Hsin", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2008.10468", "submitter": "Iosif Meyerov", "authors": "V. Volokitin, S. Bastrakov, A. Bashinov, E. Efimenko, A. Muraviev, A.\n  Gonoskov, I. Meyerov", "title": "Optimized routines for event generators in QED-PIC codes", "comments": null, "journal-ref": "J. Phys.: Conf. Ser.1640 012015 (2020)", "doi": "10.1088/1742-6596/1640/1/012015", "report-no": null, "categories": "physics.comp-ph cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the prospects of performing fundamental and applied studies\nat the next-generation high-intensity laser facilities have greatly stimulated\nthe interest in performing large-scale simulations of laser interaction with\nmatter with the account for quantum electrodynamics (QED) processes such as\nemission of high energy photons and decay of such photons into\nelectron-positron pairs. These processes can be modeled via probabilistic\nroutines that include frequent computation of synchrotron functions and can\nconstitute significant computational demands within accordingly extended\nParticle-in-Cell (QED-PIC) algorithms. In this regard, the optimization of\nthese routines is of great interest. In this paper, we propose and describe two\nmodifications. First, we derive a more accurate upper-bound estimate for the\nrate of QED events and use it to arrange local sub-stepping of the global time\nstep in a significantly more efficient way than done previously. Second, we\npresent a new high-performance implementation of synchrotron functions. Our\noptimizations made it possible to speed up the computations by a factor of up\nto 13.7 depending on the problem. Our implementation is integrated into the\nPICADOR and Hi-Chi codes, the latter of which is distributed publicly\n(https://github.com/hi-chi/pyHiChi).\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 14:17:37 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Volokitin", "V.", ""], ["Bastrakov", "S.", ""], ["Bashinov", "A.", ""], ["Efimenko", "E.", ""], ["Muraviev", "A.", ""], ["Gonoskov", "A.", ""], ["Meyerov", "I.", ""]]}, {"id": "2008.11321", "submitter": "Maciej Besta", "authors": "Maciej Besta, Armon Carigiet, Zur Vonarburg-Shmaria, Kacper Janda,\n  Lukas Gianinazzi, Torsten Hoefler", "title": "High-Performance Parallel Graph Coloring with Strong Guarantees on Work,\n  Depth, and Quality", "comments": null, "journal-ref": "Proceedings of the ACM/IEEE International Conference on High\n  Performance Computing, Networking, Storage and Analysis (SC20), November 2020", "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the first parallel graph coloring heuristics with strong\ntheoretical guarantees on work and depth and coloring quality. The key idea is\nto design a relaxation of the vertex degeneracy order, a well-known graph\ntheory concept, and to color vertices in the order dictated by this relaxation.\nThis introduces a tunable amount of parallelism into the degeneracy ordering\nthat is otherwise hard to parallelize. This simple idea enables significant\nbenefits in several key aspects of graph coloring. For example, one of our\nalgorithms ensures polylogarithmic depth and a bound on the number of used\ncolors that is superior to all other parallelizable schemes, while maintaining\nwork-efficiency. In addition to provable guarantees, the developed algorithms\nhave competitive run-times for several real-world graphs, while almost always\nproviding superior coloring quality. Our degeneracy ordering relaxation is of\nseparate interest for algorithms outside the context of coloring.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 00:52:33 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 22:56:42 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 15:59:26 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Besta", "Maciej", ""], ["Carigiet", "Armon", ""], ["Vonarburg-Shmaria", "Zur", ""], ["Janda", "Kacper", ""], ["Gianinazzi", "Lukas", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2008.11326", "submitter": "Charlene Yang", "authors": "Charlene Yang", "title": "8 Steps to 3.7 TFLOP/s on NVIDIA V100 GPU: Roofline Analysis and Other\n  Tricks", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance optimization can be a daunting task especially as the hardware\narchitecture becomes more and more complex. This paper takes a kernel from the\nMaterials Science code BerkeleyGW, and demonstrates a few performance analysis\nand optimization techniques. Despite challenges such as high register usage,\nlow occupancy, complex data access patterns, and the existence of several\nlong-latency instructions, we have achieved 3.7 TFLOP/s of double-precision\nperformance on an NVIDIA V100 GPU, with 8 optimization steps. This is 55% of\nthe theoretical peak, 6.7 TFLOP/s, at nominal frequency 1312 MHz, and 70% of\nthe more customized peak based on our 58% FMA ratio, 5.3 TFLOP/s. An array of\ntechniques used to analyze this OpenACC kernel and optimize its performance are\nshown, including the use of hierarchical Roofline performance model and the\nperformance tool Nsight Compute. This kernel exhibits computational\ncharacteristics that are commonly seen in many high-performance computing (HPC)\napplications, and are expected to be very helpful to a general audience of HPC\ndevelopers and computational scientists, as they pursue more performance on\nNVIDIA GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 01:09:24 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 01:45:05 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 05:08:36 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 20:21:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Yang", "Charlene", ""]]}, {"id": "2008.11675", "submitter": "Karthee Sivalingam", "authors": "Nina Mujkanovic and Karthee Sivalingam and Alfio Lazzaro", "title": "Optimising AI Training Deployments using Graph Compilers and Containers", "comments": "HPEC IEEE, 6 pages, 5 figues, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) applications based on Deep Neural Networks (DNN)\nor Deep Learning (DL) have become popular due to their success in solving\nproblems likeimage analysis and speech recognition. Training a DNN is\ncomputationally intensive and High Performance Computing(HPC) has been a key\ndriver in AI growth. Virtualisation and container technology have led to the\nconvergence of cloud and HPC infrastructure. These infrastructures with diverse\nhardware increase the complexity of deploying and optimising AI training\nworkloads. AI training deployments in HPC or cloud can be optimised with\ntarget-specific libraries, graph compilers, andby improving data movement or\nIO. Graph compilers aim to optimise the execution of a DNN graph by generating\nan optimised code for a target hardware/backend. As part of SODALITE (a Horizon\n2020 project), MODAK tool is developed to optimise application deployment in\nsoftware defined infrastructures. Using input from the data scientist and\nperformance modelling, MODAK maps optimal application parameters to a target\ninfrastructure and builds an optimised container. In this paper, we introduce\nMODAK and review container technologies and graph compilers for AI. We\nillustrate optimisation of AI training deployments using graph compilers and\nSingularity containers. Evaluation using MNIST-CNN and ResNet50 training\nworkloads shows that custom built optimised containers outperform the official\nimages from DockerHub. We also found that the performance of graph compilers\ndepends on the target hardware and the complexity of the neural network.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:58:32 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 09:23:06 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Mujkanovic", "Nina", ""], ["Sivalingam", "Karthee", ""], ["Lazzaro", "Alfio", ""]]}, {"id": "2008.11827", "submitter": "Wenqian Dong", "authors": "Wenqian Dong, Zhen Xie, Gokcen Kestor and Dong Li", "title": "Smart-PGSim: Using Neural Network to Accelerate AC-OPF Power Grid\n  Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal power flow (OPF) problem is one of the most important\noptimization problems for the operation of the power grid. It calculates the\noptimum scheduling of the committed generation units. In this paper, we develop\na neural network approach to the problem of accelerating the current optimal\npower flow (AC-OPF) by generating an intelligent initial solution. The high\nquality of the initial solution and guidance of other outputs generated by the\nneural network enables faster convergence to the solution without losing\noptimality of final solution as computed by traditional methods. Smart-PGSim\ngenerates a novel multitask-learning neural network model to accelerate the\nAC-OPF simulation. Smart-PGSim also imposes the physical constraints of the\nsimulation on the neural network automatically. Smart-PGSim brings an average\nof 49.2% performance improvement (up to 91%), computed over 10,000 problem\nsimulations, with respect to the original AC-OPF implementation, without losing\nthe optimality of the final solution.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 21:31:08 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Dong", "Wenqian", ""], ["Xie", "Zhen", ""], ["Kestor", "Gokcen", ""], ["Li", "Dong", ""]]}, {"id": "2008.12017", "submitter": "Sajida Kairm Ms", "authors": "Sajida Karim, Hui He, Asif Ali Laghari and Hina Madiha", "title": "Quality of Service (QoS): Measurements of Video Streaming", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.3987056", "report-no": null, "categories": "cs.MM cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Nowadays video streaming is growing over the social clouds, where end-users\nalways want to share High Definition (HD) videos among friends. Mostly videos\nwere recorded via smartphones and other HD devices and short time videos have a\nbig file size. The big file size of videos required high bandwidth to upload\nand download on the Internet and also required more time to load in a web page\nfor play. So avoiding this problem social cloud compress videos during the\nupload for smooth play and fast loading in a web page. Compression decreases\nthe video quality which also decreases the quality of experience of end users.\nIn this paper we measure the QoS of different standard video file formats on\nsocial clouds; they varied from each other in resolution, audio/video bitrate,\nand storage size.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 09:38:46 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Karim", "Sajida", ""], ["He", "Hui", ""], ["Laghari", "Asif Ali", ""], ["Madiha", "Hina", ""]]}, {"id": "2008.12501", "submitter": "Kazuichi Oe", "authors": "Kazuichi Oe", "title": "Analysis of Interference between RDMA and Local Access on Hybrid Memory\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We can use a hybrid memory system consisting of DRAM and Intel Optane DC\nPersistent Memory (We call it DCPM in this paper) as DCPM is now commercially\navailable since April 2019. Even if the latency for DCPM is several times\nhigher than that for DRAM, the capacity for DCPM is several times higher than\nthat for DRAM and the cost of DCPM is also several times lower than that for\nDRAM. In addition, DCPM is non-volatile. A Server with this hybrid memory\nsystem could improve the performance for in-memory database systems and virtual\nmachine (VM) systems because these systems often consume a large amount of\nmemory. Moreover, a high-speed shared storage system can be implemented by\naccessing DCPM via remote direct memory access (RDMA). I assume that some of\nthe DCPM is often assigned as a shared area among other remote servers because\napplications executed on a server with a hybrid memory system often cannot use\nthe entire capacity of DCPM. This paper evaluates the interference between\nlocal memory access and RDMA from a remote server. As a result, I indicate that\nthe interference on this hybrid memory system is significantly different from\nthat on a conventional DRAM-only memory system. I also believe that some kind\nof throttling implementation is needed when this interference occures.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 06:52:41 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Oe", "Kazuichi", ""]]}, {"id": "2008.13145", "submitter": "John Lawson", "authors": "John Lawson", "title": "Performance portability through machine learning guided kernel selection\n  in SYCL libraries", "comments": "13 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically tuning parallel compute kernels allows libraries and frameworks\nto achieve performance on a wide range of hardware, however these techniques\nare typically focused on finding optimal kernel parameters for particular input\nsizes and parameters. General purpose compute libraries must be able to cater\nto all inputs and parameters provided by a user, and so these techniques are of\nlimited use. Additionally, parallel programming frameworks such as SYCL require\nthat the kernels be deployed in a binary format embedded within the library. As\nsuch it is impractical to deploy a large number of possible kernel\nconfigurations without inflating the library size.\n  Machine learning methods can be used to mitigate against both of these\nproblems and provide performance for general purpose routines with a limited\nnumber of kernel configurations. We show that unsupervised clustering methods\ncan be used to select a subset of the possible kernels that should be deployed\nand that simple classification methods can be trained to select from these\nkernels at runtime to give good performance. As these techniques are fully\nautomated, relying only on benchmark data, the tuning process for new hardware\nor problems does not require any developer effort or expertise.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 11:44:37 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lawson", "John", ""]]}, {"id": "2008.13402", "submitter": "Anastasios Papagiannis", "authors": "Stella Mikrou, Anastasios Papagiannis, Giorgos Saloustros, Manolis\n  Marazakis, Angelos Bilas", "title": "Power and Performance Analysis of Persistent Key-Value Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the current rate of data growth, processing needs are becoming difficult\nto fulfill due to CPU power and energy limitations. Data serving systems and\nespecially persistent key-value stores have become a substantial part of data\nprocessing stacks in the data center, providing access to massive amounts of\ndata for applications and services. Key-value stores exhibit high CPU and I/O\noverheads because of their constant need to reorganize data on the devices. In\nthis paper, we examine the efficiency of two key-value stores on four servers\nof different generations and with different CPU architectures. We use RocksDB,\na key-value that is deployed widely, e.g. in Facebook, and Kreon, a research\nkey-value store that has been designed to reduce CPU overhead. We evaluate\ntheir behavior and overheads on an ARM-based microserver and three different\ngenerations of x86 servers. Our findings show that microservers have better\npower efficiency in the range of 0.68-3.6x with a comparable tail latency.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 07:33:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Mikrou", "Stella", ""], ["Papagiannis", "Anastasios", ""], ["Saloustros", "Giorgos", ""], ["Marazakis", "Manolis", ""], ["Bilas", "Angelos", ""]]}, {"id": "2008.13453", "submitter": "Yuming Jiang", "authors": "Yordanos Tibebu Woldeyohannes, Besmir Tola, Yuming Jiang, K. K.\n  Ramakrishnan", "title": "CoShare: An Efficient Approach for Redundancy Allocation in NFV", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An appealing feature of Network Function Virtualization (NFV) is that in an\nNFV-based network, a network function (NF) instance may be placed at any node.\nThis, on the one hand, offers great flexibility in redundancy allocation to\nmeet the availability requirements of flows; on the other hand, it makes the\nchallenge unique and difficult. One particular highlight is that there is\ninherent correlation among nodes due to the structure of the network, implying\nthat special care is needed for redundancy allocation in NFV-based networks. To\nthis aim, a novel approach, called CoShare, is proposed. Originally, its design\ntakes into consideration the effect of network structural dependency. In\naddition, to efficiently make use of resources, CoShare proposes the idea of\nshared reservation, where multiple flows may be allowed to share the same\nreserved backup capacity at an NF instance. Furthermore, CoShare factors in the\nheterogeneity in nodes, NF instances and availability requirements of flows in\nthe design. The results from a number of experiments conducted using realistic\nnetwork topologies show that CoShare is able to meet diverse availability\nrequirements in a resource-efficient manner, requiring less resource overbuild\nthan using the idea of dedicated reservation commonly adopted for redundancy\nallocation in NFV.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 09:33:45 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Woldeyohannes", "Yordanos Tibebu", ""], ["Tola", "Besmir", ""], ["Jiang", "Yuming", ""], ["Ramakrishnan", "K. K.", ""]]}, {"id": "2008.13742", "submitter": "Shinjae Yoo", "authors": "Sungsoo Ha, Wonyong Jeong, Gyorgy Matyasfalvi, Cong Xie, Kevin Huck,\n  Jong Youl Choi, Abid Malik, Li Tang, Hubertus Van Dam, Line Pouchard, Wei Xu,\n  Shinjae Yoo, Nicholas D'Imperio, Kerstin Kleese Van Dam", "title": "Chimbuko: A Workflow-Level Scalable Performance Trace Analysis Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the limits input/output systems currently impose on\nhigh-performance computing systems, a new generation of workflows that include\nonline data reduction and analysis is emerging. Diagnosing their performance\nrequires sophisticated performance analysis capabilities due to the complexity\nof execution patterns and underlying hardware, and no tool could handle the\nvoluminous performance trace data needed to detect potential problems. This\nwork introduces Chimbuko, a performance analysis framework that provides\nreal-time, distributed, in situ anomaly detection. Data volumes are reduced for\nhuman-level processing without losing necessary details. Chimbuko supports\nonline performance monitoring via a visualization module that presents the\noverall workflow anomaly distribution, call stacks, and timelines. Chimbuko\nalso supports the capture and reduction of performance provenance. To the best\nof our knowledge, Chimbuko is the first online, distributed, and scalable\nworkflow-level performance trace analysis framework, and we demonstrate the\ntool's usefulness on Oak Ridge National Laboratory's Summit system.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:06:43 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ha", "Sungsoo", ""], ["Jeong", "Wonyong", ""], ["Matyasfalvi", "Gyorgy", ""], ["Xie", "Cong", ""], ["Huck", "Kevin", ""], ["Choi", "Jong Youl", ""], ["Malik", "Abid", ""], ["Tang", "Li", ""], ["Van Dam", "Hubertus", ""], ["Pouchard", "Line", ""], ["Xu", "Wei", ""], ["Yoo", "Shinjae", ""], ["D'Imperio", "Nicholas", ""], ["Van Dam", "Kerstin Kleese", ""]]}]