[{"id": "2103.00167", "submitter": "Dirk Fahland", "authors": "Dirk Fahland, Vadim Denisov, Wil. M.P. van der Aalst", "title": "Inferring Unobserved Events in Systems With Shared Resources and Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.FL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify the causes of performance problems or to predict process\nbehavior, it is essential to have correct and complete event data. This is\nparticularly important for distributed systems with shared resources, e.g., one\ncase can block another case competing for the same machine, leading to\ninter-case dependencies in performance. However, due to a variety of reasons,\nreal-life systems often record only a subset of all events taking place. For\nexample, to reduce costs, the number of sensors is minimized or parts of the\nsystem are not connected. To understand and analyze the behavior of processes\nwith shared resources, we aim to reconstruct bounds for timestamps of events\nthat must have happened but were not recorded. We present a novel approach that\ndecomposes system runs into entity traces of cases and resources that may need\nto synchronize in the presence of many-to-many relationships. Such\nrelationships occur, for example, in warehouses where packages for N incoming\norders are not handled in a single delivery but in M different deliveries. We\nuse linear programming over entity traces to derive the timestamps of\nunobserved events in an efficient manner. This helps to complete the event logs\nand facilitates analysis. We focus on material handling systems like baggage\nhandling systems in airports to illustrate our approach. However, the approach\ncan be applied to other settings where recording is incomplete. The ideas have\nbeen implemented in ProM and were evaluated using both synthetic and real-life\nevent logs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 09:34:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fahland", "Dirk", ""], ["Denisov", "Vadim", ""], ["van der Aalst", "Wil. M. P.", ""]]}, {"id": "2103.01216", "submitter": "Yan Gu", "authors": "Yan Gu, Omar Obeya, Julian Shun", "title": "Parallel In-Place Algorithms: Theory and Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many parallel algorithms use at least linear auxiliary space in the size of\nthe input to enable computations to be done independently without conflicts.\nUnfortunately, this extra space can be prohibitive for memory-limited machines,\npreventing large inputs from being processed. Therefore, it is desirable to\ndesign parallel in-place algorithms that use sublinear (or even\npolylogarithmic) auxiliary space.\n  In this paper, we bridge the gap between theory and practice for parallel\nin-place (PIP) algorithms. We first define two computational models based on\nfork-join parallelism, which reflect modern parallel programming environments.\nWe then introduce a variety of new parallel in-place algorithms that are simple\nand efficient, both in theory and in practice. Our algorithmic highlight is the\nDecomposable Property introduced in this paper, which enables existing\nnon-in-place but highly-optimized parallel algorithms to be converted into\nparallel in-place algorithms. Using this property, we obtain algorithms for\nrandom permutation, list contraction, tree contraction, and merging that take\nlinear work, $O(n^{1-\\epsilon})$ auxiliary space, and\n$O(n^\\epsilon\\cdot\\text{polylog}(n))$ span for $0<\\epsilon<1$. We also present\nnew parallel in-place algorithms for scan, filter, merge, connectivity,\nbiconnectivity, and minimum spanning forest using other techniques.\n  In addition to theoretical results, we present experimental results for\nimplementations of many of our parallel in-place algorithms. We show that on a\n72-core machine with two-way hyper-threading, the parallel in-place algorithms\nusually outperform existing parallel algorithms for the same problems that use\nlinear auxiliary space, indicating that the theory developed in this paper\nindeed leads to practical benefits in terms of both space usage and running\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:59:05 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gu", "Yan", ""], ["Obeya", "Omar", ""], ["Shun", "Julian", ""]]}, {"id": "2103.02916", "submitter": "Jakob Notland", "authors": "Henrik Knudsen, Jakob Svennevik Notland, Peter Halland Haro, Truls\n  Bakkejord R{\\ae}der, Jingyue Li", "title": "Consensus in Blockchain Systems with Low Network Throughput: A\n  Systematic Mapping Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain technologies originate from cryptocurrencies. Thus, most\nblockchain technologies assume an environment with a fast and stable network.\nHowever, in some blockchain-based systems, e.g., supply chain management (SCM)\nsystems, some Internet of Things (IOT) nodes can only rely on the low-quality\nnetwork sometimes to achieve consensus. Thus, it is critical to understand the\napplicability of existing consensus algorithms in such environments. We\nperformed a systematic mapping study to evaluate and compare existing consensus\nmechanisms' capability to provide integrity and security with varying network\nproperties. Our study identified 25 state-of-the-art consensus algorithms from\npublished and preprint literature. We categorized and compared the consensus\nalgorithms qualitatively based on established performance and integrity metrics\nand well-known blockchain security issues. Results show that consensus\nalgorithms rely on the synchronous network for correctness cannot provide the\nexpected integrity. Such consensus algorithms may also be vulnerable to\ndistributed-denial-of-service (DDOS) and routing attacks, given limited network\nthroughput. Conversely, asynchronous consensus algorithms, e.g.,\nHoney-BadgerBFT, are deemed more robust against many of these attacks and may\nprovide high integrity in asynchrony events.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 09:43:13 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Knudsen", "Henrik", ""], ["Notland", "Jakob Svennevik", ""], ["Haro", "Peter Halland", ""], ["R\u00e6der", "Truls Bakkejord", ""], ["Li", "Jingyue", ""]]}, {"id": "2103.03013", "submitter": "Georg Hager", "authors": "Christie Alappat and Nils Meyer and Jan Laukemann and Thomas Gruber\n  and Georg Hager and Gerhard Wellein and Tilo Wettig", "title": "ECM modeling and performance tuning of SpMV and Lattice QCD on A64FX", "comments": "31 pages, 24 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The A64FX CPU is arguably the most powerful Arm-based processor design to\ndate. Although it is a traditional cache-based multicore processor, its peak\nperformance and memory bandwidth rival accelerator devices. A good\nunderstanding of its performance features is of paramount importance for\ndevelopers who wish to leverage its full potential. We present an architectural\nanalysis of the A64FX used in the Fujitsu FX1000 supercomputer at a level of\ndetail that allows for the construction of Execution-Cache-Memory (ECM)\nperformance models for steady-state loops. In the process we identify\narchitectural peculiarities that point to viable generic optimization\nstrategies. After validating the model using simple streaming loops we apply\nthe insight gained to sparse matrix-vector multiplication (SpMV) and the domain\nwall (DW) kernel from quantum chromodynamics (QCD). For SpMV we show why the\nCRS matrix storage format is not a good practical choice on this architecture\nand how the SELL-$C$-$\\sigma$ format can achieve bandwidth saturation. For the\nDW kernel we provide a cache-reuse analysis and show how an appropriate choice\nof data layout for complex arrays can realize memory-bandwidth saturation in\nthis case as well. A comparison with state-of-the-art high-end Intel Cascade\nLake AP and Nvidia V100 systems puts the capabilities of the A64FX into\nperspective. We also explore the potential for power optimizations using the\ntuning knobs provided by the Fugaku system, achieving energy savings of about\n31% for SpMV and 18% for DW.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:21:38 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Alappat", "Christie", ""], ["Meyer", "Nils", ""], ["Laukemann", "Jan", ""], ["Gruber", "Thomas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""], ["Wettig", "Tilo", ""]]}, {"id": "2103.03102", "submitter": "Wei Dai", "authors": "Wei Dai, Daniel Berleant", "title": "Benchmarking Deep Learning Classifiers: Beyond Accuracy", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous research evaluating deep learning (DL) classifiers has often used\ntop-1/top-5 accuracy. However, the accuracy of DL classifiers is unstable in\nthat it often changes significantly when retested on imperfect or adversarial\nimages. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on imperfect images by proposing\na two-dimensional metric, consisting of mean accuracy and coefficient of\nvariation, to measure the robustness of DL classifiers. Spearman's rank\ncorrelation coefficient and Pearson's correlation coefficient are used and\ntheir independence evaluated. A statistical plot we call mCV is presented which\naims to help visualize the robustness of the performance of DL classifiers\nacross varying amounts of imperfection in tested images. Finally, we\ndemonstrate that defective images corrupted by two-factor corruption could be\nused to improve the robustness of DL classifiers. All source codes and related\nimage sets are shared on a website (http://www.animpala.com) to support future\nresearch projects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:10:54 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dai", "Wei", ""], ["Berleant", "Daniel", ""]]}, {"id": "2103.03175", "submitter": "Georg Hager", "authors": "Ayesha Afzal and Georg Hager and Gerhard Wellein", "title": "Analytic Modeling of Idle Waves in Parallel Programs: Communication,\n  Cluster Topology, and Noise Impact", "comments": "19 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most distributed-memory bulk-synchronous parallel programs in HPC assume that\ncompute resources are available continuously and homogeneously across the\nallocated set of compute nodes. However, long one-off delays on individual\nprocesses can cause global disturbances, so-called idle waves, by rippling\nthrough the system. This process is mainly governed by the communication\ntopology of the underlying parallel code. This paper makes significant\ncontributions to the understanding of idle wave dynamics. We study the\npropagation mechanisms of idle waves across the ranks of MPI-parallel programs.\nWe present a validated analytic model for their propagation velocity with\nrespect to communication parameters and topology, with a special emphasis on\nsparse communication patterns. We study the interaction of idle waves with MPI\ncollectives and show that, depending on the implementation, a collective may be\ntransparent to the wave. Finally we analyze two mechanisms of idle wave decay:\ntopological decay, which is rooted in differences in communication\ncharacteristics among parts of the system, and noise-induced decay, which is\ncaused by system or application noise. We show that noise-induced decay is\nlargely independent of noise characteristics but depends only on the overall\nnoise power. An analytic expression for idle wave decay rate with respect to\nnoise power is derived. For model validation we use microbenchmarks and stencil\nalgorithms on three different supercomputing platforms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:36:18 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Afzal", "Ayesha", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "2103.03222", "submitter": "Tuan  Phung-Duc", "authors": "E.V. Morozov, S. S. Rogozin, H.Q.Nguyen, T. Phung-Duc", "title": "Modified Erlang loss system for cognitive wireless networks", "comments": "Submitted to Journal of Mathematical Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers a modified Erlang loss system for cognitive wireless\nnetworks and related applications. A primary user has preemptive priority over\nsecondary users and the primary customer is lost if upon arrival all the\nchannels are used by other primary users. Secondary users cognitively use idle\nchannels and they can wait at an infinite buffer in cases idle channels are not\navailable upon arrival or they are interrupted by primary users. We obtain\nexplicit stability condition for the cases where arrival processes of primary\nusers and secondary users follow Poisson processes and their service times\nfollow two distinct arbitrary distributions. The stability condition is\ninsensitive to the service time distributions and implies the maximal\nthroughout of secondary users. For a special case of exponential service time\ndistributions, we analyze in depth to show the effect of parameters on the\ndelay performance and the mean number of interruptions of secondary users. Our\nsimulations for distributions rather than exponential reveal that the mean\nnumber of terminations for secondary users is less sensitive to the service\ntime distribution of primary users.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 09:52:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Morozov", "E. V.", ""], ["Rogozin", "S. S.", ""], ["Nguyen", "H. Q.", ""], ["Phung-Duc", "T.", ""]]}, {"id": "2103.03653", "submitter": "Maciej Besta", "authors": "Maciej Besta, Zur Vonarburg-Shmaria, Yannick Schaffner, Leonardo\n  Schwarz, Grzegorz Kwasniewski, Lukas Gianinazzi, Jakub Beranek, Kacper Janda,\n  Tobias Holenstein, Sebastian Leisinger, Peter Tatkowski, Esref Ozdemir,\n  Adrian Balla, Marcin Copik, Philipp Lindenberger, Pavel Kalvoda, Marek\n  Konieczny, Onur Mutlu, Torsten Hoefler", "title": "GraphMineSuite: Enabling High-Performance and Programmable Graph Mining\n  Algorithms with Set Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.DS cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose GraphMineSuite (GMS): the first benchmarking suite for graph\nmining that facilitates evaluating and constructing high-performance graph\nmining algorithms. First, GMS comes with a benchmark specification based on\nextensive literature review, prescribing representative problems, algorithms,\nand datasets. Second, GMS offers a carefully designed software platform for\nseamless testing of different fine-grained elements of graph mining algorithms,\nsuch as graph representations or algorithm subroutines. The platform includes\nparallel implementations of more than 40 considered baselines, and it\nfacilitates developing complex and fast mining algorithms. High modularity is\npossible by harnessing set algebra operations such as set intersection and\ndifference, which enables breaking complex graph mining algorithms into simple\nbuilding blocks that can be separately experimented with. GMS is supported with\na broad concurrency analysis for portability in performance insights, and a\nnovel performance metric to assess the throughput of graph mining algorithms,\nenabling more insightful evaluation. As use cases, we harness GMS to rapidly\nredesign and accelerate state-of-the-art baselines of core graph mining\nproblems: degeneracy reordering (by up to >2x), maximal clique listing (by up\nto >9x), k-clique listing (by 1.1x), and subgraph isomorphism (by up to 2.5x),\nalso obtaining better theoretical performance bounds.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:26:18 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Besta", "Maciej", ""], ["Vonarburg-Shmaria", "Zur", ""], ["Schaffner", "Yannick", ""], ["Schwarz", "Leonardo", ""], ["Kwasniewski", "Grzegorz", ""], ["Gianinazzi", "Lukas", ""], ["Beranek", "Jakub", ""], ["Janda", "Kacper", ""], ["Holenstein", "Tobias", ""], ["Leisinger", "Sebastian", ""], ["Tatkowski", "Peter", ""], ["Ozdemir", "Esref", ""], ["Balla", "Adrian", ""], ["Copik", "Marcin", ""], ["Lindenberger", "Philipp", ""], ["Kalvoda", "Pavel", ""], ["Konieczny", "Marek", ""], ["Mutlu", "Onur", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2103.06775", "submitter": "Guenter Hesse", "authors": "Guenter Hesse, Christoph Matthies, Michael Perscheid, Matthias\n  Uflacker, Hasso Plattner", "title": "ESPBench: The Enterprise Stream Processing Benchmark", "comments": "Accepted as full research paper at ACM/SPEC International Conference\n  on Performance Engineering 2021 (ICPE 21)", "journal-ref": null, "doi": "10.1145/3427921.3450242", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing data volumes and velocities in fields such as Industry 4.0 or the\nInternet of Things have led to the increased popularity of data stream\nprocessing systems. Enterprises can leverage these developments by enriching\ntheir core business data and analyses with up-to-date streaming data. Comparing\nstreaming architectures for these complex use cases is challenging, as existing\nbenchmarks do not cover them. ESPBench is a new enterprise stream processing\nbenchmark that fills this gap. We present its architecture, the benchmarking\nprocess, and the query workload. We employ ESPBench on three state-of-the-art\nstream processing systems, Apache Spark, Apache Flink, and Hazelcast Jet, using\nprovided query implementations developed with Apache Beam. Our results\nhighlight the need for the provided ESPBench toolkit that supports benchmark\nexecution, as it enables query result validation and objective latency\nmeasures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:35:39 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hesse", "Guenter", ""], ["Matthies", "Christoph", ""], ["Perscheid", "Michael", ""], ["Uflacker", "Matthias", ""], ["Plattner", "Hasso", ""]]}, {"id": "2103.08875", "submitter": "Asif Ahmed Sardar", "authors": "Asif Ahmed Sardar, Dibbendu Roy, Washim Uddin Mondal and Goutam Das", "title": "Queuing Analysis of Opportunistic Cognitive Radio IoT Network with\n  Imperfect Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze a Cognitive Radio-based Internet-of-Things (CR-IoT)\nnetwork comprising a Primary Network Provider (PNP) and an IoT operator. The\nPNP uses its licensed spectrum to serve its users. The IoT operator identifies\nthe white-space in the licensed band at regular intervals and opportunistically\nexploits them to serve the IoT nodes under its coverage. IoT nodes are\nbattery-operated devices that require periodical energy replenishment. We\nemploy the Microwave Power Transfer (MPT) technique for its superior energy\ntransfer efficiency over long-distance. The white-space detection process is\nnot always perfect and the IoT operator may jeopardize the PNP's transmissions\ndue to misdetection. To reduce the possibility of such interferences, some of\nthe spectrum holes must remain unutilized, even when the IoT nodes have data to\ntransmit. The IoT operator needs to decide what percentage of the white-space\nto keep unutilized and how to judiciously use the rest for data transmission\nand energy-replenishment to maintain an optimal balance between the average\ninterference inflicted on PNP's users and the Quality-of-Service (QoS)\nexperienced by IoT nodes. Due to the periodic nature of the spectrum-sensing\nprocess, Discrete Time Markov Chain (DTMC) method can realistically model this\nframework. In literature, activities of the PNP and IoT operator are assumed to\nbe mutually exclusive, for ease of analysis. Our model incorporates possible\noverlaps between these activities, making the analysis more realistic. Using\nour model, the sustainability region of the CR-IoT network can be obtained. The\naccuracy of our analysis is demonstrated via extensive simulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 06:50:29 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sardar", "Asif Ahmed", ""], ["Roy", "Dibbendu", ""], ["Mondal", "Washim Uddin", ""], ["Das", "Goutam", ""]]}, {"id": "2103.08983", "submitter": "Michel Gokan Khan", "authors": "Michel Gokan Khan, Javid Taheri, Auday Al-Dulaimy, Andreas Kassler", "title": "PerfSim: A Performance Simulator for Cloud Native Computing", "comments": "for the dataset used for evaluation, see\n  https://ieee-dataport.org/documents/experiments-data-used-evaluating-perfsim-simulation-accuracy-based-sfc-stress-workloads\n  and https://ui.neptune.ai/o/kau/org/PerfSim/experiments. Source code will be\n  published upon acceptance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud native computing paradigm allows microservice-based applications to\ntake advantage of cloud infrastructure in a scalable, reusable, and\ninteroperable way. However, in a cloud native system, the vast number of\nconfiguration parameters and highly granular resource allocation policies can\nsignificantly impact the performance and deployment cost of such applications.\nFor understanding and analyzing these implications in an easy, quick, and\ncost-effective way, we present PerfSim, a discrete-event simulator for\napproximating and predicting the performance of cloud native service chains in\nuser-defined scenarios. To this end, we proposed a systematic approach for\nmodeling the performance of microservices endpoint functions by collecting and\nanalyzing their performance and network traces. With a combination of the\nextracted models and user-defined scenarios, PerfSim can simulate the\nperformance behavior of service chains over a given period and provides an\napproximation for system KPIs, such as requests' average response time. Using\nthe processing power of a single laptop, we evaluated both simulation accuracy\nand speed of PerfSim in 104 prevalent scenarios and compared the simulation\nresults with the identical deployment in a real Kubernetes cluster. We achieved\n~81-99% simulation accuracy in approximating the average response time of\nincoming requests and ~16-1200 times speed-up factor for the simulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 11:21:04 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Khan", "Michel Gokan", ""], ["Taheri", "Javid", ""], ["Al-Dulaimy", "Auday", ""], ["Kassler", "Andreas", ""]]}, {"id": "2103.09019", "submitter": "Felippe Vieira Zacarias", "authors": "Felippe V. Zacarias (1, 2 and 3), Vinicius Petrucci (1 and 5), Rajiv\n  Nishtala (4), Paul Carpenter (3) and Daniel Moss\\'e (5) ((1) Universidade\n  Federal da Bahia, (2) Universitat Polit\\`ecnica de Catalunya, (3) Barcelona\n  Supercomputing Center, (4) Coop, Norway/Norwegian University of Science and\n  Technology, Norway, (5) University of Pittsburgh)", "title": "Intelligent colocation of HPC workloads", "comments": "Submitted to Journal of Parallel and Distributed Computing", "journal-ref": "Volume 151, May 2021, Pages 125-137", "doi": "10.1016/j.jpdc.2021.02.010", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many HPC applications suffer from a bottleneck in the shared caches,\ninstruction execution units, I/O or memory bandwidth, even though the remaining\nresources may be underutilized. It is hard for developers and runtime systems\nto ensure that all critical resources are fully exploited by a single\napplication, so an attractive technique for increasing HPC system utilization\nis to colocate multiple applications on the same server. When applications\nshare critical resources, however, contention on shared resources may lead to\nreduced application performance.\n  In this paper, we show that server efficiency can be improved by first\nmodeling the expected performance degradation of colocated applications based\non measured hardware performance counters, and then exploiting the model to\ndetermine an optimized mix of colocated applications. This paper presents a new\nintelligent resource manager and makes the following contributions: (1) a new\nmachine learning model to predict the performance degradation of colocated\napplications based on hardware counters and (2) an intelligent scheduling\nscheme deployed on an existing resource manager to enable application\nco-scheduling with minimum performance degradation. Our results show that our\napproach achieves performance improvements of 7% (avg) and 12% (max) compared\nto the standard policy commonly used by existing job managers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 12:35:35 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zacarias", "Felippe V.", "", "1, 2 and 3"], ["Petrucci", "Vinicius", "", "1 and 5"], ["Nishtala", "Rajiv", ""], ["Carpenter", "Paul", ""], ["Moss\u00e9", "Daniel", ""]]}, {"id": "2103.10116", "submitter": "Terry Cojean", "authors": "Yuhsiang M. Tsai and Terry Cojean and Hartwig Anzt", "title": "Porting a sparse linear algebra math library to Intel GPUs", "comments": "preprint, not submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the announcement that the Aurora Supercomputer will be composed of\ngeneral purpose Intel CPUs complemented by discrete high performance Intel\nGPUs, and the deployment of the oneAPI ecosystem, Intel has committed to enter\nthe arena of discrete high performance GPUs. A central requirement for the\nscientific computing community is the availability of production-ready software\nstacks and a glimpse of the performance they can expect to see on Intel high\nperformance GPUs. In this paper, we present the first platform-portable open\nsource math library supporting Intel GPUs via the DPC++ programming\nenvironment. We also benchmark some of the developed sparse linear algebra\nfunctionality on different Intel GPUs to assess the efficiency of the DPC++\nprogramming ecosystem to translate raw performance into application\nperformance. Aside from quantifying the efficiency within the hardware-specific\nroofline model, we also compare against routines providing the same\nfunctionality that ship with Intel's oneMKL vendor library.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 09:44:17 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Tsai", "Yuhsiang M.", ""], ["Cojean", "Terry", ""], ["Anzt", "Hartwig", ""]]}, {"id": "2103.10277", "submitter": "Sergio Martiradonna", "authors": "Sergio Martiradonna, Andrea Abrardo, Marco Moretti, Giuseppe Piro,\n  Gennaro Boggia", "title": "Deep Reinforcement Learning-Aided RAN Slicing Enforcement for B5G\n  Latency Sensitive Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of cloud computing capabilities at the network edge and\nartificial intelligence promise to turn future mobile networks into service-\nand radio-aware entities, able to address the requirements of upcoming\nlatency-sensitive applications. In this context, a challenging research goal is\nto exploit edge intelligence to dynamically and optimally manage the Radio\nAccess Network Slicing (that is a less mature and more complex technology than\nfifth-generation Network Slicing) and Radio Resource Management, which is a\nvery complex task due to the mostly unpredictably nature of the wireless\nchannel. This paper presents a novel architecture that leverages Deep\nReinforcement Learning at the edge of the network in order to address Radio\nAccess Network Slicing and Radio Resource Management optimization supporting\nlatency-sensitive applications. The effectiveness of our proposal against\nbaseline methodologies is investigated through computer simulation, by\nconsidering an autonomous-driving use-case.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:18:34 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Martiradonna", "Sergio", ""], ["Abrardo", "Andrea", ""], ["Moretti", "Marco", ""], ["Piro", "Giuseppe", ""], ["Boggia", "Gennaro", ""]]}, {"id": "2103.10635", "submitter": "Atanu Barai", "authors": "Atanu Barai, Gopinath Chennupati, Nandakishore Santhi, Abdel-Hameed\n  Badawy, Yehia Arafa, Stephan Eidenbenz", "title": "PPT-SASMM: Scalable Analytical Shared Memory Model: Predicting the\n  Performance of Multicore Caches from a Single-Threaded Execution Trace", "comments": "11 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1907.12666", "journal-ref": null, "doi": "10.1145/3422575.3422806", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Performance modeling of parallel applications on multicore processors remains\na challenge in computational co-design due to multicore processors' complex\ndesign. Multicores include complex private and shared memory hierarchies. We\npresent a Scalable Analytical Shared Memory Model (SASMM). SASMM can predict\nthe performance of parallel applications running on a multicore. SASMM uses a\nprobabilistic and computationally-efficient method to predict the reuse\ndistance profiles of caches in multicores. SASMM relies on a stochastic, static\nbasic block-level analysis of reuse profiles. The profiles are calculated from\nthe memory traces of applications that run sequentially rather than using\nmulti-threaded traces. The experiments show that our model can predict private\nL1 cache hit rates with 2.12% and shared L2 cache hit rates with about 1.50%\nerror rate.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 05:10:39 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Barai", "Atanu", ""], ["Chennupati", "Gopinath", ""], ["Santhi", "Nandakishore", ""], ["Badawy", "Abdel-Hameed", ""], ["Arafa", "Yehia", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "2103.10646", "submitter": "Samuli Aalto", "authors": "Samuli Aalto (Aalto University, Espoo, Finland)", "title": "Characterization of the Gittins index for sequential multistage jobs", "comments": "144 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.OC math.PR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The optimal scheduling problem in single-server queueing systems is a classic\nproblem in queueing theory. The Gittins index policy is known to be the optimal\npreemptive nonanticipating policy (both for the open version of the problem\nwith Poisson arrivals and the closed version without arrivals) minimizing the\nexpected holding costs. While the Gittins index is thoroughly characterized for\nordinary jobs whose state is described by the attained service, it is not at\nall the case with jobs that have more complex structure. Recently, a class of\nsuch jobs, the multistage jobs, were introduced, and it was shown that the\ncomputation of Gittins index of a multistage job reduces into separable\ncomputations for the individual stages. The characterization is, however,\nindirect in the sense that it relies on the recursion for an auxiliary function\n(so called SJP function) and not for the Gittins index itself. In this paper,\nwe answer the natural question: Is it possible to compute the Gittins index for\na multistage job more directly by recursively combining the Gittins indexes of\nits individual stages? According to our results, it seems to be possible, at\nleast, for sequential multistage jobs that have a fixed (deterministic)\nsequence of stages. We prove this for sequential two-stage jobs that have\nmonotonous hazard rates in both stages, but our numerical experiments give an\nindication that the result could possibly be generalized to any sequential\nmultistage jobs. Our approach, in this paper, is based on the Whittle index\noriginally developed in the context of restless bandits.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 06:02:43 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Aalto", "Samuli", "", "Aalto University, Espoo, Finland"]]}, {"id": "2103.10779", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar, Aravinda Prasad, Smruti R. Sarangi, Sreenivas\n  Subramoney", "title": "Page Table Management for Heterogeneous Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern enterprise servers are increasingly embracing tiered memory systems\nwith a combination of low latency DRAMs and large capacity but high latency\nnon-volatile main memories (NVMMs) such as Intel's Optane DC PMM. Prior works\nhave focused on efficient placement and migration of data on a tiered memory\nsystem, but have not studied the optimal placement of page tables.\n  Explicit and efficient placement of page tables is crucial for large memory\nfootprint applications with high TLB miss rates because they incur dramatically\nhigher page walk latency when page table pages are placed in NVMM. We show that\n(i) page table pages can end up on NVMM even when enough DRAM memory is\navailable and (ii) page table pages that spill over to NVMM due to DRAM memory\npressure are not migrated back later when memory is available in DRAM.\n  We study the performance impact of page table placement in a tiered memory\nsystem and propose an efficient and transparent page table management technique\nthat (i) applies different placement policies for data and page table pages,\n(ii) introduces a differentiating policy for page table pages by placing a\nsmall but critical part of the page table in DRAM, and (iii) dynamically and\njudiciously manages the rest of the page table by transparently migrating the\npage table pages between DRAM and NVMM. Our implementation on a real system\nequipped with Intel's Optane NVMM running Linux reduces the page table walk\ncycles by 12% and total cycles by 20% on an average. This improves the runtime\nby 20% on an average for a set of synthetic and real-world large memory\nfootprint applications when compared with various default Linux kernel\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 08:46:59 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Kumar", "Sandeep", ""], ["Prasad", "Aravinda", ""], ["Sarangi", "Smruti R.", ""], ["Subramoney", "Sreenivas", ""]]}, {"id": "2103.10891", "submitter": "Shabnam Daghaghi", "authors": "Shabnam Daghaghi, Nicholas Meisburger, Mengnan Zhao, Yong Wu, Sameh\n  Gobriel, Charlie Tai, Anshumali Shrivastava", "title": "Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization,\n  Quantizations, Memory Optimizations, and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning implementations on CPUs (Central Processing Units) are gaining\nmore traction. Enhanced AI capabilities on commodity x86 architectures are\ncommercially appealing due to the reuse of existing hardware and virtualization\nease. A notable work in this direction is the SLIDE system. SLIDE is a C++\nimplementation of a sparse hash table based back-propagation, which was shown\nto be significantly faster than GPUs in training hundreds of million parameter\nneural models. In this paper, we argue that SLIDE's current implementation is\nsub-optimal and does not exploit several opportunities available in modern\nCPUs. In particular, we show how SLIDE's computations allow for a unique\npossibility of vectorization via AVX (Advanced Vector Extensions)-512.\nFurthermore, we highlight opportunities for different kinds of memory\noptimization and quantizations. Combining all of them, we obtain up to 7x\nspeedup in the computations on the same hardware. Our experiments are focused\non large (hundreds of millions of parameters) recommendation and NLP models.\nOur work highlights several novel perspectives and opportunities for\nimplementing randomized algorithms for deep learning on modern CPUs. We provide\nthe code and benchmark scripts at https://github.com/RUSH-LAB/SLIDE\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 02:13:43 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Daghaghi", "Shabnam", ""], ["Meisburger", "Nicholas", ""], ["Zhao", "Mengnan", ""], ["Wu", "Yong", ""], ["Gobriel", "Sameh", ""], ["Tai", "Charlie", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2103.10942", "submitter": "Elene Anton", "authors": "Elene Anton (IRIT, Toulouse INP), Urtzi Ayesta (IRIT, Toulouse INP,\n  UPV/EHU), Matthieu Jonckheere, Ina Maria Verloop (IRIT, Toulouse INP)", "title": "A Survey of Stability Results for Redundancy Systems", "comments": null, "journal-ref": "A.B. Piunovskiy and Y. Zhang (eds.), Modern Trends in Controlled\n  Stochastic Processes : Theory and Applications, Volume III, Springer, 2021", "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy mechanisms consist in sending several copies of a same job to a\nsubset of servers. It constitutes one of the most promising ways to exploit\ndiversity in multiservers applications. However, its pros and cons are still\nnot sufficiently understood in the context of realistic models with generic\nstatistical properties of service-times distributions and correlation\nstructures of copies. We aim at giving a survey of recent results concerning\nthe stability-arguably the first benchmark of performance-of systems with\ncancel-oncompletion redundancy. We also point out open questions and\nconjectures.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:04:56 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Anton", "Elene", "", "IRIT, Toulouse INP"], ["Ayesta", "Urtzi", "", "IRIT, Toulouse INP,\n  UPV/EHU"], ["Jonckheere", "Matthieu", "", "IRIT, Toulouse INP"], ["Verloop", "Ina Maria", "", "IRIT, Toulouse INP"]]}, {"id": "2103.13042", "submitter": "Xiaoyan Liu", "authors": "Xiaoyan Liu, Yi Liu, Ming Dun, Bohong Yin, Hailong Yang, Zhongzhi\n  Luan, Depei Qian", "title": "Accelerating Sparse Approximate Matrix Multiplication on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although the matrix multiplication plays a vital role in computational linear\nalgebra, there are few efficient solutions for matrix multiplication of the\nnear-sparse matrices. The Sparse Approximate Matrix Multiply (SpAMM) is one of\nthe algorithms to fill the performance gap neglected by traditional\noptimizations for dense/sparse matrix multiplication. However, existing SpAMM\nalgorithms fail to exploit the performance potential of GPUs for acceleration.\nIn this paper, we present cuSpAMM, the first parallel SpAMM algorithm optimized\nfor multiple GPUs. Several performance optimizations have been proposed,\nincluding algorithm re-design to adapt to the thread parallelism, blocking\nstrategies for memory access optimization, and the acceleration with the tensor\ncore. In addition, we scale cuSpAMM to run on multiple GPUs with an effective\nload balance scheme. We evaluate cuSpAMM on both synthesized and real-world\ndatasets on multiple GPUs. The experiment results show that cuSpAMM achieves\nsignificant performance speedup compared to vendor optimized cuBLAS and\ncuSPARSE libraries.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 08:09:59 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Xiaoyan", ""], ["Liu", "Yi", ""], ["Dun", "Ming", ""], ["Yin", "Bohong", ""], ["Yang", "Hailong", ""], ["Luan", "Zhongzhi", ""], ["Qian", "Depei", ""]]}, {"id": "2103.13424", "submitter": "Luxi Zhao", "authors": "Luxi Zhao, Paul Pop, Sebastian Steinhorst", "title": "Quantitative Performance Comparison of Various Traffic Shapers in\n  Time-Sensitive Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owning to the sub-standards being developed by IEEE Time-Sensitive Networking\n(TSN) Task Group, the traditional IEEE 802.1 Ethernet is enhanced to support\nreal-time dependable communications for future time- and safety-critical\napplications. Several sub-standards have been recently proposed that introduce\nvarious traffic shapers (e.g., Time-Aware Shaper (TAS), Asynchronous Traffic\nShaper (ATS), Credit-Based Shaper (CBS), Strict Priority (SP)) for flow control\nmechanisms of queuing and scheduling, targeting different application\nrequirements. These shapers can be used in isolation or in combination and\nthere is limited work that analyzes, evaluates and compares their performance,\nwhich makes it challenging for end-users to choose the right combination for\ntheir applications. This paper aims at (i) quantitatively comparing various\ntraffic shapers and their combinations, (ii) summarizing, classifying and\nextending the architectures of individual and combined traffic shapers and\ntheir Network calculus (NC)-based performance analysis methods and (iii)\nfilling the gap in the timing analysis research on handling two novel hybrid\narchitectures of combined traffic shapers, i.e., TAS+ATS+SP and TAS+ATS+CBS. A\nlarge number of experiments, using both synthetic and realistic test cases, are\ncarried out for quantitative performance comparisons of various individual and\ncombined traffic shapers, from the perspective of upper bounds of delay,\nbacklog and jitter. To the best of our knowledge, we are the first to\nquantitatively compare the performance of the main traffic shapers in TSN. The\npaper aims at supporting the researchers and practitioners in the selection of\nsuitable TSN sub-protocols for their use cases.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:10:58 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhao", "Luxi", ""], ["Pop", "Paul", ""], ["Steinhorst", "Sebastian", ""]]}, {"id": "2103.13832", "submitter": "Stefan Hristozov", "authors": "Stefan Hristozov, Manuel Huber, Lei Xu, Jaro Fietz, Marco Liess, Georg\n  Sigl", "title": "The Cost of OSCORE and EDHOC for Constrained Devices", "comments": "A short version of this paper will appear on CODASPY 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern IoT applications rely on the Constrained Application Protocol\n(CoAP) because of its efficiency and seamless integrability in the existing\nInternet infrastructure. One of the strategies that CoAP leverages to achieve\nthese characteristics is the usage of proxies. Unfortunately, in order for a\nproxy to operate, it needs to terminate the (D)TLS channels between clients and\nservers. Therefore, end-to-end confidentiality, integrity and authenticity of\nthe exchanged data cannot be achieved. In order to overcome this problem, an\nalternative to (D)TLS was recently proposed by the Internet Engineering Task\nForce (IETF). This alternative consists of two novel protocols: 1) Object\nSecurity for Constrained RESTful Environments (OSCORE) providing authenticated\nencryption for the payload data and 2) Ephemeral Diffie-Hellman Over COSE\n(EDHOC) providing the symmetric session keys required for OSCORE. In this\npaper, we present the design of four firmware libraries for these protocols\nespecially targeted for constrained microcontrollers and their detailed\nevaluation. More precisely, we present the design of uOSCORE and uEDHOC\nlibraries for regular microcontrollers and uOSCORE-TEE and uEDHOC-TEE libraries\nfor microcontrollers with a Trusted Execution Environment (TEE), such as\nmicrocontrollers featuring ARM TrustZone-M. Our firmware design for the later\nclass of devices concerns the fact that attackers may exploit common software\nvulnerabilities, e.g., buffer overflows in the protocol logic, OS or\napplication to compromise the protocol security. uOSCORE-TEE and uEDHOC-TEE\nachieve separation of the cryptographic operations and keys from the remainder\nof the firmware, which could be vulnerable. We present an evaluation of our\nimplementations in terms of RAM/FLASH requirements, execution speed and energy\non a broad range of microcontrollers.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:21:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Hristozov", "Stefan", ""], ["Huber", "Manuel", ""], ["Xu", "Lei", ""], ["Fietz", "Jaro", ""], ["Liess", "Marco", ""], ["Sigl", "Georg", ""]]}, {"id": "2103.14404", "submitter": "Yang Su Mr.", "authors": "Yang Su and Damith C. Ranasinghe", "title": "ReaDmE: Read-Rate Based Dynamic Execution Scheduling for Intermittent\n  RF-Powered Devices", "comments": "Accepted by IEEE RFID 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for remotely and dynamically determining the\nexecution schedule of long-running tasks on intermittently powered devices such\nas computational RFID. Our objective is to prevent brown-out events caused by\nsudden power-loss due to the intermittent nature of the powering channel. We\nformulate, validate and demonstrate that the read-rate measured from an RFID\nreader (number of successful interrogations per second) can provide an adequate\nmeans of estimating the powering channel condition for passively powered CRFID\ndevices. This method is attractive because it can be implemented without\nimposing an added burden on the device or requiring additional hardware. We\nfurther propose ReaDmE, a dynamic execution scheduling scheme to mitigate\nbrownout events to support long-run execution of complex tasks, such as\ncryptographic algorithms, on CRFID. Experimental results demonstrate that the\nReaDmE method can improve CRFID's long-run execution success rate by 20% at the\ncritical operational range or reduce time overhead by up to 23% compared to\nprevious execution scheduling methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 11:22:56 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 05:33:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Su", "Yang", ""], ["Ranasinghe", "Damith C.", ""]]}, {"id": "2103.15860", "submitter": "J\\\"ames M\\'en\\'etrey", "authors": "J\\\"ames M\\'en\\'etrey, Marcelo Pasin, Pascal Felber, Valerio Schiavoni", "title": "Twine: An Embedded Trusted Runtime for WebAssembly", "comments": "12 pages. This is the author's version of the work. The definitive\n  version will be published in the proceedings of the 37th IEEE International\n  Conference on Data Engineering (ICDE'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WebAssembly is an increasingly popular lightweight binary instruction format,\nwhich can be efficiently embedded and sandboxed. Languages like C, C++, Rust,\nGo, and many others can be compiled into WebAssembly. This paper describes\nTwine, a WebAssembly trusted runtime designed to execute unmodified,\nlanguage-independent applications. We leverage Intel SGX to build the runtime\nenvironment without dealing with language-specific, complex APIs. While SGX\nhardware provides secure execution within the processor, Twine provides a\nsecure, sandboxed software runtime nested within an SGX enclave, featuring a\nWebAssembly system interface (WASI) for compatibility with unmodified\nWebAssembly applications. We evaluate Twine with a large set of general-purpose\nbenchmarks and real-world applications. In particular, we used Twine to\nimplement a secure, trusted version of SQLite, a well-known full-fledged\nembeddable database. We believe that such a trusted database would be a\nreasonable component to build many larger application services. Our evaluation\nshows that SQLite can be fully executed inside an SGX enclave via WebAssembly\nand existing system interface, with similar average performance overheads. We\nestimate that the performance penalties measured are largely compensated by the\nadditional security guarantees and its full compatibility with standard\nWebAssembly. An in-depth analysis of our results indicates that performance can\nbe greatly improved by modifying some of the underlying libraries. We describe\nand implement one such modification in the paper, showing up to $4.1\\times$\nspeedup. Twine is open-source, available at GitHub along with instructions to\nreproduce our experiments.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:10:51 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["M\u00e9n\u00e9trey", "J\u00e4mes", ""], ["Pasin", "Marcelo", ""], ["Felber", "Pascal", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2103.16139", "submitter": "Antonio J. Pe\\v{n}a", "authors": "Guillermo Lloret-Talavera, Marc Jorda, Harald Servat, Fabian Boemer,\n  Chetan Chauhan, Shigeki Tomishima, Nilesh N. Shah, Antonio J. Pe\\~na", "title": "Enabling Homomorphically Encrypted Inference for Large DNN Models", "comments": "Manuscript accepted for publication in IEEE Transactions on Computers", "journal-ref": null, "doi": "10.1109/TC.2021.3076123", "report-no": null, "categories": "cs.CR cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The proliferation of machine learning services in the last few years has\nraised data privacy concerns. Homomorphic encryption (HE) enables inference\nusing encrypted data but it incurs 100x-10,000x memory and runtime overheads.\nSecure deep neural network (DNN) inference using HE is currently limited by\ncomputing and memory resources, with frameworks requiring hundreds of gigabytes\nof DRAM to evaluate small models. To overcome these limitations, in this paper\nwe explore the feasibility of leveraging hybrid memory systems comprised of\nDRAM and persistent memory. In particular, we explore the recently-released\nIntel Optane PMem technology and the Intel HE-Transformer nGraph to run large\nneural networks such as MobileNetV2 (in its largest variant) and ResNet-50 for\nthe first time in the literature. We present an in-depth analysis of the\nefficiency of the executions with different hardware and software\nconfigurations. Our results conclude that DNN inference using HE incurs on\nfriendly access patterns for this memory configuration, yielding efficient\nexecutions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:53:34 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 08:36:39 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Lloret-Talavera", "Guillermo", ""], ["Jorda", "Marc", ""], ["Servat", "Harald", ""], ["Boemer", "Fabian", ""], ["Chauhan", "Chetan", ""], ["Tomishima", "Shigeki", ""], ["Shah", "Nilesh N.", ""], ["Pe\u00f1a", "Antonio J.", ""]]}]