[{"id": "2104.00258", "submitter": "Jiansong Li", "authors": "Jiansong Li, Xiao Dong, Guangli Li, Peng Zhao, Xueying Wang, Xiaobing\n  Chen, Xianzhi Yu, Yongxin Yang, Zihan Jiang, Wei Cao, Lei Liu, Xiaobing Feng", "title": "Pinpointing the Memory Behaviors of DNN Training", "comments": "Submitted to ISPASS'21 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The training of deep neural networks (DNNs) is usually memory-hungry due to\nthe limited device memory capacity of DNN accelerators. Characterizing the\nmemory behaviors of DNN training is critical to optimize the device memory\npressures. In this work, we pinpoint the memory behaviors of each device memory\nblock of GPU during training by instrumenting the memory allocators of the\nruntime system. Our results show that the memory access patterns of device\nmemory blocks are stable and follow an iterative fashion. These observations\nare useful for the future optimization of memory-efficient training from the\nperspective of raw memory access patterns.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 05:30:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Li", "Jiansong", ""], ["Dong", "Xiao", ""], ["Li", "Guangli", ""], ["Zhao", "Peng", ""], ["Wang", "Xueying", ""], ["Chen", "Xiaobing", ""], ["Yu", "Xianzhi", ""], ["Yang", "Yongxin", ""], ["Jiang", "Zihan", ""], ["Cao", "Wei", ""], ["Liu", "Lei", ""], ["Feng", "Xiaobing", ""]]}, {"id": "2104.00897", "submitter": "Yujia Zhai", "authors": "Yujia Zhai, Elisabeth Giem, Quan Fan, Kai Zhao, Jinyang Liu, Zizhong\n  Chen", "title": "FT-BLAS: A High Performance BLAS Implementation With Online Fault\n  Tolerance", "comments": "camera-ready version at ICS'21: International Conference on\n  Supercomputing 2021 with ISBN updated", "journal-ref": null, "doi": "10.1145/3447818.3460364", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Basic Linear Algebra Subprograms (BLAS) is a core library in scientific\ncomputing and machine learning. This paper presents FT-BLAS, a new\nimplementation of BLAS routines that not only tolerates soft errors on the fly,\nbut also provides comparable performance to modern state-of-the-art BLAS\nlibraries on widely-used processors such as Intel Skylake and Cascade Lake. To\naccommodate the features of BLAS, which contains both memory-bound and\ncomputing-bound routines, we propose a hybrid strategy to incorporate fault\ntolerance into our brand-new BLAS implementation: duplicating computing\ninstructions for memory-bound Level-1 and Level-2 BLAS routines and\nincorporating an Algorithm-Based Fault Tolerance mechanism for computing-bound\nLevel-3 BLAS routines. Our high performance and low overhead are obtained from\ndelicate assembly-level optimization and a kernel-fusion approach to the\ncomputing kernels. Experimental results demonstrate that FT-BLAS offers high\nreliability and high performance -- faster than Intel MKL, OpenBLAS, and BLIS\nby up to 3.50%, 22.14% and 21.70%, respectively, for routines spanning all\nthree levels of BLAS we benchmarked, even under hundreds of errors injected per\nminute.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:02:58 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 08:10:07 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 21:29:55 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zhai", "Yujia", ""], ["Giem", "Elisabeth", ""], ["Fan", "Quan", ""], ["Zhao", "Kai", ""], ["Liu", "Jinyang", ""], ["Chen", "Zizhong", ""]]}, {"id": "2104.00984", "submitter": "Umair Qudus", "authors": "Umair Qudus, Muhammad Saleem, Axel-Cyrille Ngonga Ngomo, Young-koo Lee", "title": "An Empirical Evaluation of Cost-based Federated SPARQL Query Processing\n  Engines", "comments": "24 pages, Semantic Web, 2020, #article", "journal-ref": "Semantic Web 2020", "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding a good query plan is key to the optimization of query runtime. This\nholds in particular for cost-based federation engines, which make use of\ncardinality estimations to achieve this goal. A number of studies compare\nSPARQL federation engines across different performance metrics, including query\nruntime, result set completeness and correctness, number of sources selected\nand number of requests sent. Albeit informative, these metrics are generic and\nunable to quantify and evaluate the accuracy of the cardinality estimators of\ncost-based federation engines. To thoroughly evaluate cost-based federation\nengines, the effect of estimated cardinality errors on the overall query\nruntime performance must be measured. In this paper, we address this challenge\nby presenting novel evaluation metrics targeted at a fine-grained benchmarking\nof cost-based federated SPARQL query engines. We evaluate five cost-based\nfederated SPARQL query engines using existing as well as novel evaluation\nmetrics by using LargeRDFBench queries. Our results provide a detailed analysis\nof the experimental outcomes that reveal novel insights, useful for the\ndevelopment of future cost-based federated SPARQL query processing engines.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:01:25 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Qudus", "Umair", ""], ["Saleem", "Muhammad", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Lee", "Young-koo", ""]]}, {"id": "2104.01281", "submitter": "Stefano Souza", "authors": "Stefano M P C Souza and Daniel G Silva", "title": "Monte Carlo execution time estimation for Privacy-preserving Distributed\n  Function Evaluation protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent developments in Machine Learning and Deep Learning depend heavily on\ncloud computing and specialized hardware, such as GPUs and TPUs. This forces\nthose using those models to trust private data to cloud servers. Such scenario\nhas prompted a large interest on Homomorphic Cryptography and Secure\nMulti-Party Computation protocols that allow the use of cloud computing power\nin a privacy-preserving manner.\n  When comparing the efficiency of such protocols, most works in literature\nresort to complexity analysis that gives asymptotic higher-bounding limits of\ncomputational cost when input size tends to infinite. These limits may be very\ndifferent from the actual cost or execution time, when performing such\ncomputations over small, or average-sized datasets.\n  We argue that Monte Carlo methods can render better computational cost and\ntime estimates, fostering better design and implementation decisions for\ncomplex systems, such as Privacy-Preserving Machine Learning Frameworks.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 00:04:07 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Souza", "Stefano M P C", ""], ["Silva", "Daniel G", ""]]}, {"id": "2104.02222", "submitter": "Roch Gu\\'erin", "authors": "Jiayi Song, Roch Gu\\'erin, and Henry Sariowan", "title": "Minimizing network bandwidth under latency constraints: The single node\n  case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Datacenters have become a significant source of traffic, much of which is\ncarried over private networks. The operators of those networks commonly have\naccess to detailed traffic profiles and performance goals, which they seek to\nmeet as efficiently as possible. Of interest are solutions for offering latency\nguarantees while minimizing the required network bandwidth. Of particular\ninterest is the extent to which traffic (re)shaping can be of benefit. The\npaper focuses on the most basic network configuration, namely, a single node,\nsingle link network, with extensions to more general, multi-node networks\ndiscussed in a companion paper. The main results are in the form of optimal\nsolutions for different types of schedulers of varying complexity, and\ntherefore cost. The results demonstrate how judicious traffic shaping can help\nlower complexity schedulers reduce the bandwidth they require, often performing\nas well as more complex ones.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:22:59 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 13:52:26 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Song", "Jiayi", ""], ["Gu\u00e9rin", "Roch", ""], ["Sariowan", "Henry", ""]]}, {"id": "2104.02856", "submitter": "Yingcheng Bu", "authors": "Yi Fang, Yingcheng Bu, Pingping Chen, Shahid Mumtaz, Francis C. M. Lau\n  and Sattam Al Otaibi", "title": "Irregular-Mapped Protograph LDPC-Coded Modulation: A Bandwidth-Efficient\n  Solution for $5$G Networks with Massive Data-Storage Requirement", "comments": "More research effort should be made to improve the quality of this\n  paper with the help of other collegues. The paper must be withdrawed at this\n  stage as some content should be revised and changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The huge amount of data produced in the fifth-generation (5G) networks not\nonly brings new challenges to the reliability and efficiency of mobile devices\nbut also drives rapid development of new storage techniques. With the benefits\nof fast access speed and high reliability, NAND flash memory has become a\npromising storage solution for the 5G networks. In this paper, we investigate a\nprotograph-coded bit-interleaved coded modulation with iterative detection and\ndecoding (BICM-ID) utilizing irregular mapping (IM) in the multi-level-cell\n(MLC) NAND flash-memory systems. First, we propose an enhanced protograph-based\nextrinsic information transfer (EPEXIT) algorithm to facilitate the analysis of\nprotograph codes in the IM-BICM-ID systems. With the use of EPEXIT algorithm, a\nsimple design method is conceived for the construction of a family of high-rate\nprotograph codes, called irregular-mapped accumulate-repeat-accumulate (IMARA)\ncodes, which possess both excellent decoding thresholds and\nlinear-minimum-distance-growth property. Furthermore, motivated by the\nvoltage-region iterative gain characteristics of IM-BICM-ID systems, a novel\nread-voltage optimization scheme is developed to acquire accurate read-voltage\nlevels, thus minimizing the decoding thresholds of protograph codes.\nTheoretical analyses and error-rate simulations indicate that the proposed\nIMARA-aided IM-BICM-ID scheme and the proposed read-voltage optimization scheme\nremarkably improve the convergence and decoding performance of flash-memory\nsystems. Thus, the proposed protograph-coded IM-BICM-ID flash-memory systems\ncan be viewed as a reliable and efficient storage solution for the\nnew-generation mobile networks with massive data-storage requirement.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 01:51:57 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 02:19:39 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Fang", "Yi", ""], ["Bu", "Yingcheng", ""], ["Chen", "Pingping", ""], ["Mumtaz", "Shahid", ""], ["Lau", "Francis C. M.", ""], ["Otaibi", "Sattam Al", ""]]}, {"id": "2104.03142", "submitter": "Jos\\'e Moreira", "authors": "Jos\\'e E. Moreira, Kit Barton, Steven Battle, Peter Bergner, Ramon\n  Bertran, Puneeth Bhat, Pedro Caldeira, David Edelsohn, Gordon Fossum, Brad\n  Frey, Nemanja Ivanovic, Chip Kerchner, Vincent Lim, Shakti Kapoor, Tulio\n  Machado Filho, Silvia Melitta Mueller, Brett Olsson, Satish Sadasivam,\n  Baptiste Saleil, Bill Schmidt, Rajalakshmi Srinivasaraghavan, Shricharan\n  Srivatsan, Brian Thompto, Andreas Wagner, Nelson Wu", "title": "A matrix math facility for Power ISA(TM) processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power ISA(TM) Version 3.1 has introduced a new family of matrix math\ninstructions, collectively known as the Matrix-Multiply Assist (MMA) facility.\nThe instructions in this facility implement numerical linear algebra operations\non small matrices and are meant to accelerate computation-intensive kernels,\nsuch as matrix multiplication, convolution and discrete Fourier transform.\nThese instructions have led to a power- and area-efficient implementation of a\nhigh throughput math engine in the future POWER10 processor. Performance per\ncore is 4 times better, at constant frequency, than the previous generation\nPOWER9 processor. We also advocate the use of compiler built-ins as the\npreferred way of leveraging these instructions, which we illustrate through\ncase studies covering matrix multiplication and convolution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:17:32 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Moreira", "Jos\u00e9 E.", ""], ["Barton", "Kit", ""], ["Battle", "Steven", ""], ["Bergner", "Peter", ""], ["Bertran", "Ramon", ""], ["Bhat", "Puneeth", ""], ["Caldeira", "Pedro", ""], ["Edelsohn", "David", ""], ["Fossum", "Gordon", ""], ["Frey", "Brad", ""], ["Ivanovic", "Nemanja", ""], ["Kerchner", "Chip", ""], ["Lim", "Vincent", ""], ["Kapoor", "Shakti", ""], ["Filho", "Tulio Machado", ""], ["Mueller", "Silvia Melitta", ""], ["Olsson", "Brett", ""], ["Sadasivam", "Satish", ""], ["Saleil", "Baptiste", ""], ["Schmidt", "Bill", ""], ["Srinivasaraghavan", "Rajalakshmi", ""], ["Srivatsan", "Shricharan", ""], ["Thompto", "Brian", ""], ["Wagner", "Andreas", ""], ["Wu", "Nelson", ""]]}, {"id": "2104.03187", "submitter": "Pierangelo Di Sanzo", "authors": "Pierangelo Di Sanzo", "title": "A Preliminary Proposal for an Analytical Model for Evaluating the Impact\n  on Performance of Data Access Patterns in Transaction Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a preliminary proposal for an analytical model for evaluating the\nimpact on performance of data access patterns in concurrent transaction\nexecution. We consider the case of concurrency control protocols that use\nlocking to ensure isolation in the execution of transactions. We analyse\nscenarios where transactions access one or more sets of data items in the same\norder or in different order.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:29:12 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 17:42:19 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 10:49:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Di Sanzo", "Pierangelo", ""]]}, {"id": "2104.03388", "submitter": "Pengfei Su", "authors": "Bolun Li, Pengfei Su, Milind Chabbi, Shuyin Jiao, Xu Liu", "title": "DJXPerf: Identifying Memory Inefficiencies via Object-centric Profiling\n  for Java", "comments": "13 pages (including 2-page reference), 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Java is the \"go-to\" programming language choice for developing scalable\nenterprise cloud applications. In such systems, even a few percent CPU time\nsavings can offer a significant competitive advantage and cost saving. Although\nperformance tools abound in Java, those that focus on the data locality in the\nmemory hierarchy are rare.\n  In this paper, we present DJXPerf, a lightweight, object-centric memory\nprofiler for Java, which associates memory-hierarchy performance metrics (e.g.,\ncache/TLB misses) with Java objects. DJXPerf uses statistical sampling of\nhardware performance monitoring counters to attribute metrics to not only\nsource code locations but also Java objects. DJXPerf presents Java object\nallocation contexts combined with their usage contexts and presents them\nordered by the poor locality behaviors. DJXPerf's performance measurement,\nobject attribution, and presentation techniques guide optimizing object\nallocation, layout, and access patterns. DJXPerf incurs only ~8% runtime\noverhead and ~5% memory overhead on average, requiring no modifications to\nhardware, OS, Java virtual machine, or application source code, which makes it\nattractive to use in production. Guided by DJXPerf, we study and optimize a\nnumber of Java and Scala programs, including well-known benchmarks and\nreal-world applications, and demonstrate significant speedups.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 20:49:36 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Bolun", ""], ["Su", "Pengfei", ""], ["Chabbi", "Milind", ""], ["Jiao", "Shuyin", ""], ["Liu", "Xu", ""]]}, {"id": "2104.04254", "submitter": "Aymeric Vie", "authors": "Aymeric Vie", "title": "Population network structure impacts genetic algorithm optimisation\n  performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.PF cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A genetic algorithm (GA) is a search method that optimises a population of\nsolutions by simulating natural evolution. Good solutions reproduce together to\ncreate better candidates. The standard GA assumes that any two solutions can\nmate. However, in nature and social contexts, social networks can condition the\nlikelihood that two individuals mate. This impact of population network\nstructure over GAs performance is unknown. Here we introduce the Networked\nGenetic Algorithm (NGA) to evaluate how various random and scale-free\npopulation networks influence the optimisation performance of GAs on benchmark\nfunctions. We show evidence of significant variations in performance of the NGA\nas the network varies. In addition, we find that the best-performing population\nnetworks, characterised by intermediate density and low average shortest path\nlength, significantly outperform the standard complete network GA. These\nresults may constitute a starting point for network tuning and network control:\nseeing the network structure of the population as a parameter that can be tuned\nto improve the performance of evolutionary algorithms, and offer more realistic\nmodelling of social learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 09:06:04 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Vie", "Aymeric", ""]]}, {"id": "2104.04579", "submitter": "Iosif Meyerov", "authors": "Valentin Volokitin, Alexey Bashinov, Evgeny Efimenko, Arkady Gonoskov,\n  Iosif Meyerov", "title": "High Performance Implementation of Boris Particle Pusher on DPC++. A\n  First Look at oneAPI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New hardware architectures open up immense opportunities for supercomputer\nsimulations. However, programming techniques for different architectures vary\nsignificantly, which leads to the necessity of developing and supporting\nmultiple code versions, each being optimized for specific hardware features.\nThe oneAPI framework, recently introduced by Intel, contains a set of\nprogramming tools for the development of portable codes that can be compiled\nand fine-tuned for CPUs, GPUs, FPGAs, and accelerators. In this paper, we\nreport on the experience of porting the implementation of Boris particle pusher\nto oneAPI. Boris particle pusher is one of the most demanding computational\nstages of the Particle-in-Cell method, which, in particular, is used for\nsupercomputer simulations of laser-plasma interactions. We show how to adapt\nthe C++ implementation of the particle push algorithm from the Hi-Chi project\nto the DPC++ programming language and report the performance of the code on\nhigh-end Intel CPUs (Xeon Platinum 8260L) and Intel GPUs (P630 and Iris Xe\nMax). It turned out that our C++ code can be easily ported to DPC++. We found\nthat on CPUs the resulting DPC++ code is only ~10% on average inferior to the\noptimized C++ code. Moreover, the code is compiled and run on new Intel GPUs\nwithout any specific optimizations and shows the expected performance, taking\ninto account the parameters of the hardware.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 19:27:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Volokitin", "Valentin", ""], ["Bashinov", "Alexey", ""], ["Efimenko", "Evgeny", ""], ["Gonoskov", "Arkady", ""], ["Meyerov", "Iosif", ""]]}, {"id": "2104.04731", "submitter": "Dennis Rieber", "authors": "Dennis Rieber, Axel Acosta, Holger Fr\\\"oning", "title": "Joint Program and Layout Transformations to enable DNN Operators on\n  Specialized Hardware based on Constraint Programming", "comments": "25 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of Deep Artificial Neural Networks (DNNs) in many domains created\na rich body of research concerned with hardwareaccelerators for\ncompute-intensive DNN operators. However, implementing such operators\nefficiently with complex hardwareintrinsics such as matrix multiply is a task\nnot yet automated gracefully. Solving this task often requires joint program\nand data layouttransformations. First solutions to this problem have been\nproposed, such as TVM, UNIT or ISAMIR, which work on a loop-levelrepresentation\nof operators and specify data layout and possible program transformations\nbefore the embedding into the operator isperformed. This top-down approach\ncreates a tension between exploration range and search space complexity,\nespecially when alsoexploring data layout transformations such as im2col,\nchannel packing or padding.In this work, we propose a new approach to this\nproblem. We created a bottom-up method that allows the joint transformation\nofboth compuation and data layout based on the found embedding. By formulating\nthe embedding as a constraint satisfaction problemover the scalar dataflow,\nevery possible embedding solution is contained in the search space. Adding\nadditional constraints andoptmization targets to the solver generates the\nsubset of preferable solutions.An evaluation using the VTA hardware accelerator\nwith the Baidu DeepBench inference benchmark shows that our approach\ncanautomatically generate code competitive to reference implementations.\nFurther, we show that dynamically determining the data layoutbased on intrinsic\nand workload is beneficial for hardware utilization and performance. In cases\nwhere the reference implementationhas low hardware utilization due to its fixed\ndeployment strategy, we achieve a geomean speedup of up to x2.813, while\nindividualoperators can improve as much as x170.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 10:39:47 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 06:16:45 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 06:42:29 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Rieber", "Dennis", ""], ["Acosta", "Axel", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2104.05026", "submitter": "Anna Melman", "authors": "Yaroslav Meshcheryakov, Anna Melman, Oleg Evsutin, Vladimir Morozov,\n  Yevgeni Koucheryavy", "title": "On performance of PBFT for IoT-applications with constrained devices", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems and the Internet of things (IoT) is becoming an\nintegral part of the digital society. The use of IoT services improves human\nlife in many ways. Protection against cyber threats is an important aspect of\nthe functioning of IoT devices. Malicious activities lead to confidential data\nleakages and incorrect performance of devices are becoming critical. Therefore,\ndevelopment of effective solutions that can protect both IoT devices data and\ndata exchange networks turns in to a real challenge. This study provides a\ncritical analysis of the feasibility of using blockchain technology to protect\nconstrained IoT devices data, justifies the choice of Practical Byzantine Fault\nTolerance (PBFT) consensus algorithm for implementation on such devices, and\nsimulates the main distributed ledger scenarios using PBFT. The simulation\nresults demonstrate the efficiency of the blockchain technology for constrained\ndevices and make it possible to evaluate the applicability limits of the chosen\nconsensus algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 15:20:12 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 09:39:23 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Meshcheryakov", "Yaroslav", ""], ["Melman", "Anna", ""], ["Evsutin", "Oleg", ""], ["Morozov", "Vladimir", ""], ["Koucheryavy", "Yevgeni", ""]]}, {"id": "2104.05102", "submitter": "Atanu Barai", "authors": "Atanu Barai and Yehia Arafa and Abdel-Hameed Badawy and Gopinath\n  Chennupati and Nandakishore Santhi and Stephan Eidenbenz", "title": "PPT-Multicore: Performance Prediction of OpenMP applications using Reuse\n  Profiles and Analytical Modeling", "comments": "arXiv admin note: text overlap with arXiv:2103.10635", "journal-ref": null, "doi": null, "report-no": "LA-UR-21-22749", "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PPT-Multicore, an analytical model embedded in the Performance\nPrediction Toolkit (PPT) to predict parallel application performance running on\na multicore processor. PPT-Multicore builds upon our previous work towards a\nmulticore cache model. We extract LLVM basic block labeled memory trace using\nan architecture-independent LLVM-based instrumentation tool only once in an\napplication's lifetime. The model uses the memory trace and other parameters\nfrom an instrumented sequentially executed binary. We use a probabilistic and\ncomputationally efficient reuse profile to predict the cache hit rates and\nruntimes of OpenMP programs' parallel sections. We model Intel's Broadwell,\nHaswell, and AMD's Zen2 architectures and validate our framework using\ndifferent applications from PolyBench and PARSEC benchmark suites. The results\nshow that PPT-Multicore can predict cache hit rates with an overall average\nerror rate of 1.23% while predicting the runtime with an error rate of 9.08%.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 20:49:55 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Barai", "Atanu", ""], ["Arafa", "Yehia", ""], ["Badawy", "Abdel-Hameed", ""], ["Chennupati", "Gopinath", ""], ["Santhi", "Nandakishore", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "2104.05158", "submitter": "Dheevatsa Mudigere", "authors": "Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Andrew Tulloch, Srinivas\n  Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie\n  Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan\n  K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat\n  Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie\n  Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna\n  Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi\n  Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar\n  Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr\n  Lapukhov, Maxim Naumov, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao", "title": "High-performance, Distributed Training of Large-scale Deep Learning\n  Recommendation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning recommendation models (DLRMs) are used across many\nbusiness-critical services at Facebook and are the single largest AI\napplication in terms of infrastructure demand in its data-centers. In this\npaper we discuss the SW/HW co-designed solution for high-performance\ndistributed training of large-scale DLRMs. We introduce a high-performance\nscalable software stack based on PyTorch and pair it with the new evolution of\nZion platform, namely ZionEX. We demonstrate the capability to train very large\nDLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup\nin terms of time to solution over previous systems. We achieve this by (i)\ndesigning the ZionEX platform with dedicated scale-out network, provisioned\nwith high bandwidth, optimal topology and efficient transport (ii) implementing\nan optimized PyTorch-based training stack supporting both model and data\nparallelism (iii) developing sharding algorithms capable of hierarchical\npartitioning of the embedding tables along row, column dimensions and load\nbalancing them across multiple workers; (iv) adding high-performance core\noperators while retaining flexibility to support optimizers with fully\ndeterministic updates (v) leveraging reduced precision communications,\nmulti-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we\ndevelop and briefly comment on distributed data ingestion and other supporting\nservices that are required for the robust and efficient end-to-end training in\nproduction environments.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 02:15:55 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 01:30:23 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 22:58:58 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Mudigere", "Dheevatsa", ""], ["Hao", "Yuchen", ""], ["Huang", "Jianyu", ""], ["Tulloch", "Andrew", ""], ["Sridharan", "Srinivas", ""], ["Liu", "Xing", ""], ["Ozdal", "Mustafa", ""], ["Nie", "Jade", ""], ["Park", "Jongsoo", ""], ["Luo", "Liang", ""], ["Yang", "Jie Amy", ""], ["Gao", "Leon", ""], ["Ivchenko", "Dmytro", ""], ["Basant", "Aarti", ""], ["Hu", "Yuxi", ""], ["Yang", "Jiyan", ""], ["Ardestani", "Ehsan K.", ""], ["Wang", "Xiaodong", ""], ["Komuravelli", "Rakesh", ""], ["Chu", "Ching-Hsiang", ""], ["Yilmaz", "Serhat", ""], ["Li", "Huayu", ""], ["Qian", "Jiyuan", ""], ["Feng", "Zhuobo", ""], ["Ma", "Yinbin", ""], ["Yang", "Junjie", ""], ["Wen", "Ellie", ""], ["Li", "Hong", ""], ["Yang", "Lin", ""], ["Sun", "Chonglin", ""], ["Zhao", "Whitney", ""], ["Melts", "Dimitry", ""], ["Dhulipala", "Krishna", ""], ["Kishore", "KR", ""], ["Graf", "Tyler", ""], ["Eisenman", "Assaf", ""], ["Matam", "Kiran Kumar", ""], ["Gangidi", "Adi", ""], ["Chen", "Guoqiang Jerry", ""], ["Krishnan", "Manoj", ""], ["Nayak", "Avinash", ""], ["Nair", "Krishnakumar", ""], ["Muthiah", "Bharath", ""], ["khorashadi", "Mahmoud", ""], ["Bhattacharya", "Pallab", ""], ["Lapukhov", "Petr", ""], ["Naumov", "Maxim", ""], ["Qiao", "Lin", ""], ["Smelyanskiy", "Mikhail", ""], ["Jia", "Bill", ""], ["Rao", "Vijay", ""]]}, {"id": "2104.05200", "submitter": "Valeriu Motroi", "authors": "Valeriu Motroi, Stefan Ciobaca", "title": "A Note on the Performance of Algorithms for Solving Linear Diophantine\n  Equations in the Naturals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We implement four algorithms for solving linear Diophantine equations in the\nnaturals: a lexicographic enumeration algorithm, a completion procedure, a\ngraph-based algorithm, and the Slopes algorithm. As already known, the\nlexicographic enumeration algorithm and the completion procedure are slower\nthan the other two algorithms. We compare in more detail the graph-based\nalgorithm and the Slopes algorithm. In contrast to previous comparisons, our\nwork suggests that they are equally fast on small inputs, but the graph-based\nalgorithm gets much faster as the input grows. We conclude that implementations\nof AC-unification algorithms should use the graph-based algorithm for maximum\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:45:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Motroi", "Valeriu", ""], ["Ciobaca", "Stefan", ""]]}, {"id": "2104.05829", "submitter": "Misun Min Dr", "authors": "Paul Fischer, Stefan Kerkemeier, Misun Min, Yu-Hsiang Lan, Malachi\n  Phillips, Thilina Rathnayake, Elia Merzari, Ananias Tomboulides, Ali Karakus,\n  Noel Chalmers, Tim Warburton", "title": "NekRS, a GPU-Accelerated Spectral Element Navier-Stokes Solver", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of NekRS, a GPU-oriented thermal-fluids simulation code based\non the spectral element method (SEM) is described. For performance portability,\nthe code is based on the open concurrent compute abstraction and leverages\nscalable developments in the SEM code Nek5000 and in libParanumal, which is a\nlibrary of high-performance kernels for high-order discretizations and\nPDE-based miniapps. Critical performance sections of the Navier-Stokes time\nadvancement are addressed. Performance results on several platforms are\npresented, including scaling to 27,648 V100s on OLCF Summit, for calculations\nof up to 60B gridpoints.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:32:01 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Fischer", "Paul", ""], ["Kerkemeier", "Stefan", ""], ["Min", "Misun", ""], ["Lan", "Yu-Hsiang", ""], ["Phillips", "Malachi", ""], ["Rathnayake", "Thilina", ""], ["Merzari", "Elia", ""], ["Tomboulides", "Ananias", ""], ["Karakus", "Ali", ""], ["Chalmers", "Noel", ""], ["Warburton", "Tim", ""]]}, {"id": "2104.06210", "submitter": "R Ravi", "authors": "Joseph Cheriyan, R. Ravi, Martin Skutella", "title": "A simple proof of the Moore-Hodgson Algorithm for minimizing the number\n  of late jobs", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF math.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Moore-Hodgson Algorithm minimizes the number of late jobs on a single\nmachine. That is, it finds an optimal schedule for the classical problem\n$1~|\\;|~\\sum{U_j}$. Several proofs of the correctness of this algorithm have\nbeen published. We present a new short proof.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:01:58 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cheriyan", "Joseph", ""], ["Ravi", "R.", ""], ["Skutella", "Martin", ""]]}, {"id": "2104.06225", "submitter": "Daniel Waddington", "authors": "Daniel Waddington, Clem Dickey, Luna Xu, Moshik Hershcovitch,\n  Sangeetha Seshadri", "title": "A High-Performance Persistent Memory Key-Value Store with Near-Memory\n  Compute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MCAS (Memory Centric Active Storage) is a persistent memory tier for\nhigh-performance durable data storage. It is designed from the ground-up to\nprovide a key-value capability with low-latency guarantees and data durability\nthrough memory persistence and replication. To reduce data movement and make\nfurther gains in performance, we provide support for user-defined \"push-down\"\noperations (known as Active Data Objects) that can execute directly and safely\non the value-memory associated with one or more keys. The ADO mechanism allows\ncomplex pointer-based dynamic data structures (e.g., trees) to be stored and\noperated on in persistent memory. To this end, we examine a real-world use case\nfor MCAS-ADO in the handling of enterprise storage system metadata for\nContinuous Data Protection (CDP). This requires continuously updating complex\nmetadata that must be kept consistent and durable. In this paper, we i.)\npresent the MCAS-ADO system architecture, ii.) show how the CDP use case is\nimplemented, and finally iii.) give an evaluation of system performance in the\ncontext of this use case.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:06:43 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Waddington", "Daniel", ""], ["Dickey", "Clem", ""], ["Xu", "Luna", ""], ["Hershcovitch", "Moshik", ""], ["Seshadri", "Sangeetha", ""]]}, {"id": "2104.06262", "submitter": "Gregory Chance", "authors": "Greg Chance, Abanoub Ghobrial, Kevin McAreavey, Severin Lemaignan,\n  Tony Pipe, Kerstin Eder", "title": "On Determinism of Game Engines used for Simulation-based Autonomous\n  Vehicle Verification", "comments": "17 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game engines are increasingly used as simulation platforms by the autonomous\nvehicle (AV) community to develop vehicle control systems and test\nenvironments. A key requirement for simulation-based development and\nverification is determinism, since a deterministic process will always produce\nthe same output given the same initial conditions and event history. Thus, in a\ndeterministic simulation environment, tests are rendered repeatable and yield\nsimulation results that are trustworthy and straightforward to debug. However,\ngame engines are seldom deterministic. This paper reviews and identifies the\npotential causes of non-deterministic behaviours in game engines. A case study\nusing CARLA, an open-source autonomous driving simulation environment powered\nby Unreal Engine, is presented to highlight its inherent shortcomings in\nproviding sufficient precision in experimental results. Different\nconfigurations and utilisations of the software and hardware are explored to\ndetermine an operational domain where the simulation precision is sufficiently\nlow i.e.\\ variance between repeated executions becomes negligible for\ndevelopment and testing work. Finally, a method of a general nature is\nproposed, that can be used to find the domains of permissible variance in game\nengine simulations for any given system configuration.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:05:35 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 11:31:24 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chance", "Greg", ""], ["Ghobrial", "Abanoub", ""], ["McAreavey", "Kevin", ""], ["Lemaignan", "Severin", ""], ["Pipe", "Tony", ""], ["Eder", "Kerstin", ""]]}, {"id": "2104.07097", "submitter": "Mario Vazquez Corte MSc", "authors": "Mario Vazquez Corte, Luis V. Montiel", "title": "Novel Matrix Hit and Run for Sampling Polytopes and Its GPU\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a new Markov Chain Monte Carlo algorithm that\ngenerates a uniform sample over full and non-full dimensional polytopes. This\nalgorithm, termed \"Matrix Hit and Run\" (MHAR), is a modification of the Hit and\nRun framework. For the regime $n^{1+\\frac{1}{3}} \\ll m$, MHAR has a lower\nasymptotic cost per sample in terms of soft-O notation ($\\SO$) than do existing\nsampling algorithms after a \\textit{warm start}. MHAR is designed to take\nadvantage of matrix multiplication routines that require less computational and\nmemory resources. Our tests show this implementation to be substantially faster\nthan the \\textit{hitandrun} R package, especially for higher dimensions.\nFinally, we provide a python library based on Pytorch and a Colab notebook with\nthe implementation ready for deployment in architectures with GPU or just CPU.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:55:04 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Corte", "Mario Vazquez", ""], ["Montiel", "Luis V.", ""]]}, {"id": "2104.07458", "submitter": "Youri Raaijmakers", "authors": "Youri Raaijmakers", "title": "Comparison of the FCFS and PS discipline in Redundancy Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the c.o.c. redundancy system with $N$ parallel servers where\nincoming jobs are immediately replicated to $d$ servers chosen uniformly at\nrandom (without replacement). A job finishes service as soon as the first\nreplica is completed, after which all the remaining replicas are abandoned. We\ncompare the performance of the first-come first-served (FCFS) and\nprocessor-sharing (PS) discipline based on the stability condition, the tail\nbehavior of the latency and the expected latency.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:06:26 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Raaijmakers", "Youri", ""]]}, {"id": "2104.07582", "submitter": "Maciej Besta", "authors": "Maciej Besta, Raghavendra Kanakagiri, Grzegorz Kwasniewski, Rachata\n  Ausavarungnirun, Jakub Ber\\'anek, Konstantinos Kanellopoulos, Kacper Janda,\n  Zur Vonarburg-Shmaria, Lukas Gianinazzi, Ioana Stefan, Juan G\\'omez Luna,\n  Marcin Copik, Lukas Kapp-Schwoerer, Salvatore Di Girolamo, Marek Konieczny,\n  Onur Mutlu, Torsten Hoefler", "title": "SISA: Set-Centric Instruction Set Architecture for Graph Mining on\n  Processing-in-Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple graph algorithms such as PageRank have recently been the target of\nnumerous hardware accelerators. Yet, there also exist much more complex graph\nmining algorithms for problems such as clustering or maximal clique listing.\nThese algorithms are memory-bound and thus could be accelerated by hardware\ntechniques such as Processing-in-Memory (PIM). However, they also come with\nnon-straightforward parallelism and complicated memory access patterns. In this\nwork, we address this with a simple yet surprisingly powerful observation:\noperations on sets of vertices, such as intersection or union, form a large\npart of many complex graph mining algorithms, and can offer rich and simple\nparallelism at multiple levels. This observation drives our cross-layer design,\nin which we (1) expose set operations using a novel programming paradigm, (2)\nexpress and execute these operations efficiently with carefully designed\nset-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA\ninstructions. The key design idea is to alleviate the bandwidth needs of SISA\ninstructions by mapping set operations to two types of PIM: in-DRAM bulk\nbitwise computing for bitvectors representing high-degree vertices, and\nnear-memory logic layers for integer arrays representing low-degree vertices.\nSet-centric SISA-enhanced algorithms are efficient and outperform hand-tuned\nbaselines, offering more than 10x speedup over the established Bron-Kerbosch\nalgorithm for listing maximal cliques. We deliver more than 10 SISA set-centric\nalgorithm formulations, illustrating SISA's wide applicability.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:37:10 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Besta", "Maciej", ""], ["Kanakagiri", "Raghavendra", ""], ["Kwasniewski", "Grzegorz", ""], ["Ausavarungnirun", "Rachata", ""], ["Ber\u00e1nek", "Jakub", ""], ["Kanellopoulos", "Konstantinos", ""], ["Janda", "Kacper", ""], ["Vonarburg-Shmaria", "Zur", ""], ["Gianinazzi", "Lukas", ""], ["Stefan", "Ioana", ""], ["Luna", "Juan G\u00f3mez", ""], ["Copik", "Marcin", ""], ["Kapp-Schwoerer", "Lukas", ""], ["Di Girolamo", "Salvatore", ""], ["Konieczny", "Marek", ""], ["Mutlu", "Onur", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2104.07857", "submitter": "Samyam Rajbhandari", "authors": "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith,\n  Yuxiong He", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last three years, the largest dense deep learning models have grown\nover 1000x to reach hundreds of billions of parameters, while the GPU memory\nhas only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has\nbeen supported primarily though system innovations that allow large models to\nfit in the aggregate GPU memory of multiple GPUs. However, we are getting close\nto the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion\nparameter model for training, and such clusters are simply out of reach for\nmost data scientists. In addition, training models at that scale requires\ncomplex combinations of parallelism techniques that puts a big burden on the\ndata scientists to refactor their model.\n  In this paper we present ZeRO-Infinity, a novel heterogeneous system\ntechnology that leverages GPU, CPU, and NVMe memory to allow for unprecedented\nmodel scale on limited resources without requiring model code refactoring. At\nthe same time it achieves excellent training throughput and scalability,\nunencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models\nwith tens and even hundreds of trillions of parameters for training on current\ngeneration GPU clusters. It can be used to fine-tune trillion parameter models\non a single NVIDIA DGX-2 node, making large models more accessible. In terms of\ntraining throughput and scalability, it sustains over 25 petaflops on 512\nNVIDIA V100 GPUs(40% of peak), while also demonstrating super linear\nscalability. An open source implementation of ZeRO-Infinity is available\nthrough DeepSpeed, a deep learning optimization library that makes distributed\ntraining easy, efficient, and effective.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 02:22:12 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Rajbhandari", "Samyam", ""], ["Ruwase", "Olatunji", ""], ["Rasley", "Jeff", ""], ["Smith", "Shaden", ""], ["He", "Yuxiong", ""]]}, {"id": "2104.08037", "submitter": "Ioannis Dimitriou", "authors": "Ioannis Dimitriou", "title": "The generalized join the shortest orbit queue system: Stability, exact\n  tail asymptotics and stationary approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the generalized join the shortest queue model with retrials and\ntwo infinite capacity orbit queues. Three independent Poisson streams of jobs,\nnamely a \\textit{smart}, and two \\textit{dedicated} streams, flow into a single\nserver system, which can hold at most one job. Arriving jobs that find the\nserver occupied are routed to the orbits as follows: Blocked jobs from the\n\\textit{smart} stream are routed to the shortest orbit queue, and in case of a\ntie, they choose an orbit randomly. Blocked jobs from the \\textit{dedicated}\nstreams are routed directly to their orbits. Orbiting jobs retry to connect\nwith the server at different retrial rates, i.e., heterogeneous orbit queues.\nApplications of such a system are found in the modelling of wireless\ncooperative networks. We are interested in the asymptotic behaviour of the\nstationary distribution of this model, provided that the system is stable. More\nprecisely, we investigate the conditions under which the tail asymptotic of the\nminimum orbit queue length is exactly geometric. Moreover, we apply a heuristic\nasymptotic approach to obtain approximations of the steady-state joint orbit\nqueue-length distribution. Useful numerical examples are presented and shown\nthat the results obtained through the asymptotic analysis and the heuristic\napproach agreed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:20:09 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Dimitriou", "Ioannis", ""]]}, {"id": "2104.08050", "submitter": "George Kesidis", "authors": "George Kesidis, Takis Konstantopoulos, Michael A. Zazanis", "title": "Age of information without service preemption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing a message transmission system, from the point of view of\nmaking sure that the information transmitted is as fresh as possible, two rules\nof thumb seem reasonable: use small buffers and adopt a last-in-first-out\npolicy. In this paper, we measure freshness of information using the recently\nadopted \"age of information\" performance measure. Considering it as a\nstochastic process operating in a stationary regime, we compute not just the\nfirst moment but the whole marginal distribution of the age of information\n(something important in applications) for two well-performing systems. In\nneither case do we allow for preemption of the message being processed because\nthis may be difficult to implement in practice. We assume that the arrival\nprocess is Poisson and that the messages have independent sizes (service times)\nwith common distribution. We use Palm and Markov-renewal theory to derive\nexplicit results for Laplace transforms which, in many cases can be inverted\nanalytically. We discuss how well the systems we analyze performs and examine\nhow close to optimality they are. In particular, we answer an open question\nthat was raised in [9] regarding the optimality of the system denoted as P2.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:53:10 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 14:55:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Kesidis", "George", ""], ["Konstantopoulos", "Takis", ""], ["Zazanis", "Michael A.", ""]]}, {"id": "2104.08364", "submitter": "Shijian Li", "authors": "Shijian Li, Oren Mangoubi, Lijie Xu, Tian Guo", "title": "Sync-Switch: Hybrid Parameter Synchronization for Distributed Deep\n  Learning", "comments": "15 pages, 16 figures, 6 tables, ICDCS'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) has become the de facto way to train deep\nneural networks in distributed clusters. A critical factor in determining the\ntraining throughput and model accuracy is the choice of the parameter\nsynchronization protocol. For example, while Bulk Synchronous Parallel (BSP)\noften achieves better converged accuracy, the corresponding training throughput\ncan be negatively impacted by stragglers. In contrast, Asynchronous Parallel\n(ASP) can have higher throughput, but its convergence and accuracy can be\nimpacted by stale gradients. To improve the performance of synchronization\nprotocol, recent work often focuses on designing new protocols with a heavy\nreliance on hard-to-tune hyper-parameters. In this paper, we design a hybrid\nsynchronization approach that exploits the benefits of both BSP and ASP, i.e.,\nreducing training time while simultaneously maintaining the converged accuracy.\nBased on extensive empirical profiling, we devise a collection of adaptive\npolicies that determine how and when to switch between synchronization\nprotocols. Our policies include both offline ones that target recurring jobs\nand online ones for handling transient stragglers. We implement the proposed\npolicies in a prototype system, called Sync-Switch, on top of TensorFlow, and\nevaluate the training performance with popular deep learning models and\ndatasets. Our experiments show that Sync-Switch achieves up to 5.13X throughput\nspeedup and similar converged accuracy when comparing to BSP. Further, we\nobserve that Sync-Switch achieves 3.8% higher converged accuracy with just\n1.23X the training time compared to training with ASP. Moreover, Sync-Switch\ncan be used in settings when training with ASP leads to divergence errors.\nSync-Switch achieves all of these benefits with very low overhead, e.g., the\nframework overhead can be as low as 1.7% of the total training time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 20:49:28 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 00:25:37 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Shijian", ""], ["Mangoubi", "Oren", ""], ["Xu", "Lijie", ""], ["Guo", "Tian", ""]]}, {"id": "2104.08396", "submitter": "Jongwook Woo", "authors": "Mohsen Alam, Benjamin Cevallos, Oscar Flores, Randall Lunetto, Kotaro\n  Yayoshi, Jongwook Woo", "title": "Yelp Dataset Analysis using Scalable Big Data", "comments": "4 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Yelp has served and will continue to serve as a data-driven application. Yelp\nhas published a dataset containing business information, reviews, user\ninformation, and check-in information. This paper will examine this dataset to\nprovide descriptive analytics to understand business performance, geo-spatial\ndistribution of businesses, reviewers' rating and other characteristics, and\ntemporal distribution of check-ins in business premises. With these analysis we\nare able to establish that yelp reviews, tips, elite users and check ins have\nstarted to plummet over the years. Coincidentally, the paper also establishes\nthat Canadians have a more stable star ratings as well as sentiment ratings\nwhen compared to Americans.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 22:53:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Alam", "Mohsen", ""], ["Cevallos", "Benjamin", ""], ["Flores", "Oscar", ""], ["Lunetto", "Randall", ""], ["Yayoshi", "Kotaro", ""], ["Woo", "Jongwook", ""]]}, {"id": "2104.08416", "submitter": "Roger Ara\\'ujo", "authors": "Roger R. F. Ara\\'ujo, Lutz Gross, Samuel Xavier-de-Souza", "title": "Boosting Memory Access Locality of the Spectral Element Method with\n  Hilbert Space-Filling Curves", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm based on Hilbert space-filling curves to reorder mesh\nelements in memory for use with the Spectral Element Method, aiming to attain\nfewer cache misses, better locality of data reference and faster execution. We\npresent a technique to numerically simulate acoustic wave propagation in 2D\ndomains using the Spectral Element Method, and discuss computational\nperformance aspects of this procedure. We reorder mesh-related data via Hilbert\ncurves to achieve sizable reductions in execution time under several mesh\nconfigurations in shared-memory systems. Our experiments show that the Hilbert\ncurve approach works well with meshes of several granularities and also with\nsmall and large variations in element sizes, achieving reductions between 9%\nand 25% in execution time when compared to three other ordering schemes.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 01:27:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ara\u00fajo", "Roger R. F.", ""], ["Gross", "Lutz", ""], ["Xavier-de-Souza", "Samuel", ""]]}, {"id": "2104.08571", "submitter": "Robert Clucas", "authors": "Robert Clucas, Philip Blakely, Nikolaos Nikiforakis", "title": "Ripple : Simplified Large-Scale Computation on Heterogeneous\n  Architectures with Polymorphic Data Layout", "comments": "Preprint submitted to the Journal of Parallel and Distributed\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are now used for a wide range of problems within HPC. However, making\nefficient use of the computational power available with multiple GPUs is\nchallenging. The main challenges in achieving good performance are memory\nlayout, affecting memory bandwidth, effective use of the memory spaces with a\nGPU, inter-GPU communication, and synchronization. We address these problems\nwith the Ripple library, which provides a unified view of the computational\nspace across multiple dimensions and multiple GPUs, allows polymorphic data\nlayout, and provides a simple graph interface to describe an algorithm from\nwhich inter-GPU data transfers can be optimally scheduled. We describe the\nabstractions provided by Ripple to allow complex computations to be described\nsimply, and to execute efficiently across many GPUs with minimal overhead. We\nshow performance results for a number of examples, from particle motion to\nfinite-volume methods and the eikonal equation, as well as showing good strong\nand weak scaling results across multiple GPUs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:25:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Clucas", "Robert", ""], ["Blakely", "Philip", ""], ["Nikiforakis", "Nikolaos", ""]]}, {"id": "2104.09273", "submitter": "Fabrice Guillemin", "authors": "Fabrice Guillemin, Alain Simonian, Ridha Nasri, and Veronica Quintuna\n  Rodriguez", "title": "Asymptotic analysis of the sojourn time of a batch in an $M^{[X]}/M/1$\n  Processor Sharing Queue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exploit results obtained in an earlier study for the\nLaplace transform of the sojourn time $\\Omega$ of an entire batch in the\n$M^{[X]}/M/1$ Processor Sharing (PS) queue in order to derive the asymptotic\nbehavior of the complementary probability distribution function of this random\nvariable, namely the behavior of $P(\\Omega>x)$ when $x$ tends to infinity. We\nprecisely show that up to a multiplying factor, the behavior of $P(\\Omega>x)$\nfor large $x$ is of the same order of magnitude as $P(\\omega>x)$, where\n$\\omega$ is the sojourn time of an arbitrary job is the system. From a\npractical point of view, this means that if a system has to be dimensioned to\nguarantee processing time for jobs then the system can also guarantee\nprocessing times for entire batches by introducing a marginal amount of\nprocessing capacity.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:16:46 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Guillemin", "Fabrice", ""], ["Simonian", "Alain", ""], ["Nasri", "Ridha", ""], ["Rodriguez", "Veronica Quintuna", ""]]}, {"id": "2104.09433", "submitter": "Duncan Paul Attard", "authors": "Luca Aceto and Duncan Paul Attard and Adrian Francalanza and Anna\n  Ing\\'olfsd\\'ottir", "title": "A Choreographed Outline Instrumentation Algorithm for Asynchronous\n  Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The runtime analysis of decentralised software requires instrumentation\nmethods that are scalable, but also minimally invasive. This paper presents a\nnew algorithm that instruments choreographed outline monitors. Our\ninstrumentation algorithm scales and reorganises monitors dynamically as the\nsystem executes. We demonstrate the implementability of choreographed outline\ninstrumentation and compare it to inline instrumentation, subject to rigorous\nand comprehensive benchmarking. Our results debunk the general notion that\noutline monitoring is necessarily infeasible, and show that our implementation\ninduces runtime overhead comparable to that of its inline counterpart for many\npractical cases.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:27:04 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Aceto", "Luca", ""], ["Attard", "Duncan Paul", ""], ["Francalanza", "Adrian", ""], ["Ing\u00f3lfsd\u00f3ttir", "Anna", ""]]}, {"id": "2104.10030", "submitter": "Alessandro Pellegrini", "authors": "Alessandro Pellegrini", "title": "Reproducibility Report for the Paper: QN-based Modeling and Analysis of\n  Software Performance Antipatterns for Cyber-Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The authors have uploaded their artifact to Zenodo, which ensures a long-term\nretention of the artifact. The artifact allows to re-run the experiments very\nsmoothly, and the dependencies are well documented. The process to regenerate\ndata for the figures and tables in the paper completes, and all results are\nreproducible.\n  This paper can thus receive the Artifacts Available badge. The software in\nthe artifact runs correctly with no trouble, and is relevant to the paper, thus\ndeserving the Artifacts Evaluated -- Functional badge. Given the successful\nreproduction of all figures and tables, the Results Reproduced badge can be\nassigned.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:09:53 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Pellegrini", "Alessandro", ""]]}, {"id": "2104.10426", "submitter": "Jonatha Anselmi", "authors": "Jonatha Anselmi and Neil Walton", "title": "Stability and Optimization of Speculative Queueing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We provide a queueing-theoretic framework for job replication schemes based\non the principle \"\\emph{replicate a job as soon as the system detects it as a\n\\emph{straggler}}\". This is called job \\emph{speculation}. Recent works have\nanalyzed {replication} on arrival, which we refer to as \\emph{replication}.\nReplication is motivated by its implementation in Google's BigTable. However,\nsystems such as Apache Spark and Hadoop MapReduce implement speculative job\nexecution. The performance and optimization of speculative job execution is not\nwell understood. To this end, we propose a queueing network model for load\nbalancing where each server can speculate on the execution time of a job.\nSpecifically, each job is initially assigned to a single server by a frontend\ndispatcher. Then, when its execution begins, the server sets a timeout. If the\njob completes before the timeout, it leaves the network, otherwise the job is\nterminated and relaunched or resumed at another server where it will complete.\nWe provide a necessary and sufficient condition for the stability of\nspeculative queueing networks with heterogeneous servers, general job sizes and\nscheduling disciplines. We find that speculation can increase the stability\nregion of the network when compared with standard load balancing models and\nreplication schemes. We provide general conditions under which timeouts\nincrease the size of the stability region and derive a formula for the optimal\nspeculation time, i.e., the timeout that minimizes the load induced through\nspeculation. We compare speculation with redundant-$d$ and\nredundant-to-idle-queue-$d$ rules under an $S\\& X$ model. For light loaded\nsystems, redundancy schemes provide better response times. However, for\nmoderate to heavy loadings, redundancy schemes can lose capacity and have\nmarkedly worse response times when compared with a speculative scheme.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 09:17:07 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Anselmi", "Jonatha", ""], ["Walton", "Neil", ""]]}, {"id": "2104.10698", "submitter": "Arjan Cornelissen", "authors": "Arjan Cornelissen, Johannes Bausch, and Andr\\'as Gily\\'en", "title": "Scalable Benchmarks for Gate-Based Quantum Computers", "comments": "54 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the near-term \"NISQ\"-era of noisy, intermediate-scale, quantum hardware\nand beyond, reliably determining the quality of quantum devices becomes\nincreasingly important: users need to be able to compare them with one another,\nand make an estimate whether they are capable of performing a given task ahead\nof time. In this work, we develop and release an advanced quantum benchmarking\nframework in order to help assess the state of the art of current quantum\ndevices. Our testing framework measures the performance of universal quantum\ndevices in a hardware-agnostic way, with metrics that are aimed to facilitate\nan intuitive understanding of which device is likely to outperform others on a\ngiven task. This is achieved through six structured tests that allow for an\nimmediate, visual assessment of how devices compare. Each test is designed with\nscalability in mind, making this framework not only suitable for testing the\nperformance of present-day quantum devices, but also of those released in the\nforeseeable future. The series of tests are motivated by real-life scenarios,\nand therefore emphasise the interplay between various relevant characteristics\nof quantum devices, such as qubit count, connectivity, and gate and measurement\nfidelity. We present the benchmark results of twenty-one different quantum\ndevices from IBM, Rigetti and IonQ.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:00:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Cornelissen", "Arjan", ""], ["Bausch", "Johannes", ""], ["Gily\u00e9n", "Andr\u00e1s", ""]]}, {"id": "2104.10873", "submitter": "Hengjie Wang", "authors": "Hengjie Wang, Robert Planas, Aparna Chandramowlishwaran, Ramin\n  Bostanabad", "title": "Train Once and Use Forever: Solving Boundary Value Problems in Unseen\n  Domains with Pre-trained Deep Learning Models", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physics-informed neural networks (PINNs) are increasingly employed to\nreplace/augment traditional numerical methods in solving partial differential\nequations (PDEs). While having many attractive features, state-of-the-art PINNs\nsurrogate a specific realization of a PDE system and hence are\nproblem-specific. That is, each time the boundary conditions and domain shape\nchange, the model needs to be re-trained. This limitation prohibits the\napplication of PINNs in realistic or large-scale engineering problems\nespecially since the costs and efforts associated with their training are\nconsiderable.\n  This paper introduces a transferable framework for solving boundary value\nproblems (BVPs) via deep neural networks which can be trained once and used\nforever for various domains of unseen sizes, shapes, and boundary conditions.\nFirst, we introduce \\emph{genomic flow network} (GFNet), a neural network that\ncan infer the solution of a BVP across arbitrary boundary conditions on a small\nsquare domain called \\emph{genome}. Then, we propose \\emph{mosaic flow} (MF)\npredictor, a novel iterative algorithm that assembles or stitches the GFNet's\ninferences to obtain the solution of BVPs on unseen, large domains while\npreserving the spatial regularity of the solution. We demonstrate that our\nframework can estimate the solution of Laplace and Navier-Stokes equations in\ndomains of unseen shapes and boundary conditions that are, respectively, $1200$\nand $12$ times larger than the domains where training is performed. Since our\nframework eliminates the need to re-train, it demonstrates up to 3 orders of\nmagnitude speedups compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 05:20:27 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wang", "Hengjie", ""], ["Planas", "Robert", ""], ["Chandramowlishwaran", "Aparna", ""], ["Bostanabad", "Ramin", ""]]}, {"id": "2104.11069", "submitter": "S\\'ebastien Lafond", "authors": "Ivan Porres and Hergys Rexha and S\\'ebastien Lafond", "title": "Online GANs for Automatic Performance Testing", "comments": "5th International Workshop on Testing Extra-Functional Properties and\n  Quality Characteristics of Software Systems -\n  https://icst2021.icmc.usp.br/details/iteqs-2021-papers/4/Online-GANs-for-Automatic-Performance-Testing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a novel algorithm for automatic performance testing\nthat uses an online variant of the Generative Adversarial Network (GAN) to\noptimize the test generation process. The objective of the proposed approach is\nto generate, for a given test budget, a test suite containing a high number of\ntests revealing performance defects. This is achieved using a GAN to generate\nthe tests and predict their outcome. This GAN is trained online while\ngenerating and executing the tests. The proposed approach does not require a\nprior training set or model of the system under test. We provide an initial\nevaluation the algorithm using an example test system, and compare the obtained\nresults with other possible approaches.\n  We consider that the presented algorithm serves as a proof of concept and we\nhope that it can spark a research discussion on the application of GANs to test\ngeneration.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:03:27 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Porres", "Ivan", ""], ["Rexha", "Hergys", ""], ["Lafond", "S\u00e9bastien", ""]]}, {"id": "2104.11393", "submitter": "George Kesidis", "authors": "George Kesidis, Takis Konstantopoulos and Michael A. Zazanis", "title": "Age of information distribution under dynamic service preemption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age of Information (AoI) has emerged as an important quality-of-service\nmeasure in queueing systems. We derive the Laplace transform of the stationary\nAoI for the M/GI/1/2 system with a dynamic service preemption and pushout\npolicy depending on the existing service time of the in-service message. Thus,\nour system generalizes both the static M/GI/1/2 queue-pushout system without\nservice preemption and the M/GI/1/1 bufferless system with service preemption.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 03:04:41 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 16:50:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kesidis", "George", ""], ["Konstantopoulos", "Takis", ""], ["Zazanis", "Michael A.", ""]]}, {"id": "2104.12893", "submitter": "Mahshid Helali Moghadam", "authors": "Mahshid Helali Moghadam, Golrokh Hamidi, Markus Borg, Mehrdad\n  Saadatmand, Markus Bohlin, Bj\\\"orn Lisper, Pasqualina Potena", "title": "Performance Testing Using a Smart Reinforcement Learning-Driven Test\n  Agent", "comments": "10 pages, IEEE Congress on Evolutionary Computation 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Performance testing with the aim of generating an efficient and effective\nworkload to identify performance issues is challenging. Many of the automated\napproaches mainly rely on analyzing system models, source code, or extracting\nthe usage pattern of the system during the execution. However, such information\nand artifacts are not always available. Moreover, all the transactions within a\ngenerated workload do not impact the performance of the system the same way, a\nfinely tuned workload could accomplish the test objective in an efficient way.\nModel-free reinforcement learning is widely used for finding the optimal\nbehavior to accomplish an objective in many decision-making problems without\nrelying on a model of the system. This paper proposes that if the optimal\npolicy (way) for generating test workload to meet a test objective can be\nlearned by a test agent, then efficient test automation would be possible\nwithout relying on system models or source code. We present a self-adaptive\nreinforcement learning-driven load testing agent, RELOAD, that learns the\noptimal policy for test workload generation and generates an effective workload\nefficiently to meet the test objective. Once the agent learns the optimal\npolicy, it can reuse the learned policy in subsequent testing activities. Our\nexperiments show that the proposed intelligent load test agent can accomplish\nthe test objective with lower test cost compared to common load testing\nprocedures, and results in higher test efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 22:04:41 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Moghadam", "Mahshid Helali", ""], ["Hamidi", "Golrokh", ""], ["Borg", "Markus", ""], ["Saadatmand", "Mehrdad", ""], ["Bohlin", "Markus", ""], ["Lisper", "Bj\u00f6rn", ""], ["Potena", "Pasqualina", ""]]}, {"id": "2104.13242", "submitter": "Xingfu Wu", "authors": "Xingfu Wu, Michael Kruse, Prasanna Balaprakash, Hal Finkel, Paul\n  Hovland, Valerie Taylor, and Mary Hall", "title": "Autotuning PolyBench Benchmarks with LLVM Clang/Polly Loop Optimization\n  Pragmas Using Bayesian Optimization (extended version)", "comments": "Submitted to CCPE journal. arXiv admin note: substantial text overlap\n  with arXiv:2010.08040", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we develop a ytopt autotuning framework that leverages\nBayesian optimization to explore the parameter space search and compare four\ndifferent supervised learning methods within Bayesian optimization and evaluate\ntheir effectiveness. We select six of the most complex PolyBench benchmarks and\napply the newly developed LLVM Clang/Polly loop optimization pragmas to the\nbenchmarks to optimize them. We then use the autotuning framework to optimize\nthe pragma parameters to improve their performance. The experimental results\nshow that our autotuning approach outperforms the other compiling methods to\nprovide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu,\nand covariance with two large datasets in 200 code evaluations for effectively\nsearching the parameter spaces with up to 170,368 different configurations. We\nfind that the Floyd-Warshall benchmark did not benefit from autotuning because\nPolly uses heuristics to optimize the benchmark to make it run much slower. To\ncope with this issue, we provide some compiler option solutions to improve the\nperformance. Then we present loop autotuning without a user's knowledge using a\nsimple mctree autotuning framework to further improve the performance of the\nFloyd-Warshall benchmark. We also extend the ytopt autotuning framework to tune\na deep learning application.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:46:57 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wu", "Xingfu", ""], ["Kruse", "Michael", ""], ["Balaprakash", "Prasanna", ""], ["Finkel", "Hal", ""], ["Hovland", "Paul", ""], ["Taylor", "Valerie", ""], ["Hall", "Mary", ""]]}, {"id": "2104.13350", "submitter": "Philip Doldo", "authors": "Philip Doldo and Jamol Pender", "title": "Queues with Updating Information: Finding the Amplitude of Oscillations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.PF math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many service systems provide customers with information about the system so\nthat customers can make an informed decision about whether to join or not. Many\nof these systems provide information in the form of an update. Thus, the\ninformation about the system is updated periodically in increments of size\n$\\Delta$. It is known that these updates can cause oscillations in the\nresulting dynamics. However, it is an open problem to explicitly characterize\nthe size of these oscillations when they occur. In this paper, we solve this\nopen problem and show how to exactly calculate the amplitude of these\noscillations via a fixed point equation. We also calculate closed form\napproximations via Taylor expansions of the fixed point equation and show that\nthese approximations are very accurate, especially when $\\Delta$ is large. Our\nanalysis provides new insight for systems that use updates as a way of\ndisseminating information to customers.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:33:06 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Doldo", "Philip", ""], ["Pender", "Jamol", ""]]}, {"id": "2104.13732", "submitter": "Alexander Brauckmann", "authors": "Alexander Brauckmann, Andr\\'es Goens, Jeronimo Castrillon", "title": "A Reinforcement Learning Environment for Polyhedral Optimizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polyhedral model allows a structured way of defining semantics-preserving\ntransformations to improve the performance of a large class of loops. Finding\nprofitable points in this space is a hard problem which is usually approached\nby heuristics that generalize from domain-expert knowledge. Existing problem\nformulations in state-of-the-art heuristics depend on the shape of particular\nloops, making it hard to leverage generic and more powerful optimization\ntechniques from the machine learning domain. In this paper, we propose PolyGym,\na shape-agnostic formulation for the space of legal transformations in the\npolyhedral model as a Markov Decision Process (MDP). Instead of using\ntransformations, the formulation is based on an abstract space of possible\nschedules. In this formulation, states model partial schedules, which are\nconstructed by actions that are reusable across different loops. With a simple\nheuristic to traverse the space, we demonstrate that our formulation is\npowerful enough to match and outperform state-of-the-art heuristics. On the\nPolybench benchmark suite, we found transformations that led to a speedup of\n3.39x over LLVM O3, which is 1.83x better than the speedup achieved by ISL. Our\ngeneric MDP formulation enables using reinforcement learning to learn\noptimization policies over a wide range of loops. This also contributes to the\nemerging field of machine learning in compilers, as it exposes a novel problem\nformulation that can push the limits of existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 12:41:52 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 08:04:04 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Brauckmann", "Alexander", ""], ["Goens", "Andr\u00e9s", ""], ["Castrillon", "Jeronimo", ""]]}, {"id": "2104.13774", "submitter": "Ahmed Saeed", "authors": "Yimeng Zhao and Ahmed Saeed and Mostafa Ammar and Ellen Zegura", "title": "Scouting the Path to a Million-Client Server", "comments": null, "journal-ref": "In: Hohlfeld O., Lutu A., Levin D. (eds) Passive and Active\n  Measurement. PAM 2021. Lecture Notes in Computer Science, vol 12671.\n  Springer, Cham", "doi": "10.1007/978-3-030-72582-2_20", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To keep up with demand, servers will scale up to handle hundreds of thousands\nof clients simultaneously. Much of the focus of the community has been on\nscaling servers in terms of aggregate traffic intensity (packets transmitted\nper second). However, bottlenecks caused by the increasing number of concurrent\nclients, resulting in a large number of concurrent flows, have received little\nattention. In this work, we focus on identifying such bottlenecks. In\nparticular, we define two broad categories of problems; namely, admitting more\npackets into the network stack than can be handled efficiently, and increasing\nper-packet overhead within the stack. We show that these problems contribute to\nhigh CPU usage and network performance degradation in terms of aggregate\nthroughput and RTT. Our measurement and analysis are performed in the context\nof the Linux networking stack, the the most widely used publicly available\nnetworking stack. Further, we discuss the relevance of our findings to other\nnetwork stacks. The goal of our work is to highlight considerations required in\nthe design of future networking stacks to enable efficient handling of large\nnumbers of clients and flows.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:03:46 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 15:14:36 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhao", "Yimeng", ""], ["Saeed", "Ahmed", ""], ["Ammar", "Mostafa", ""], ["Zegura", "Ellen", ""]]}, {"id": "2104.14050", "submitter": "Ahmed Ali-Eldin", "authors": "Ahmed Ali-Eldin, Bin Wang and Prashant Shenoy", "title": "The Hidden cost of the Edge: A Performance Comparison of Edge and Cloud\n  Latencies", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing has emerged as a popular paradigm for running\nlatency-sensitive applications due to its ability to offer lower network\nlatencies to end-users. In this paper, we argue that despite its lower network\nlatency, the resource-constrained nature of the edge can result in higher\nend-to-end latency, especially at higher utilizations, when compared to cloud\ndata centers. We study this edge performance inversion problem through an\nanalytic comparison of edge and cloud latencies and analyze conditions under\nwhich the edge can yield worse performance than the cloud. To verify our\nanalytic results, we conduct a detailed experimental comparison of the edge and\nthe cloud latencies using a realistic application and real cloud workloads.\nBoth our analytical and experimental results show that even at moderate\nutilizations, the edge queuing delays can offset the benefits of lower network\nlatencies, and even result in performance inversion where running in the cloud\nwould provide superior latencies. We finally discuss practical implications of\nour results and provide insights into how application designers and service\nproviders should design edge applications and systems to avoid these pitfalls.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:15:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ali-Eldin", "Ahmed", ""], ["Wang", "Bin", ""], ["Shenoy", "Prashant", ""]]}, {"id": "2104.14215", "submitter": "Paul Nikolaus", "authors": "Anne Bouillard, Paul Nikolaus, Jens Schmitt", "title": "Fully Unleashing the Power of Paying Multiplexing Only Once in\n  Stochastic Network Calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic network calculus (SNC) holds promise as a framework to\ncalculate probabilistic performance bounds in networks of queues. A great\nchallenge to accurate bounds and efficient calculations are stochastic\ndependencies between flows due to resource sharing inside the network. However,\nby carefully utilizing the basic SNC concepts in the network analysis the\nnecessity of taking these dependencies into account can be minimized. To that\nend, we fully unleash the power of the pay multiplexing only once principle\n(PMOO, known from the deterministic network calculus) in the SNC analysis. We\nchoose an analytic combinatorics presentation of the results in order to ease\ncomplex calculations. In tree-reducible networks, a subclass of a general\nfeedforward networks, we obtain a perfect analysis in terms of avoiding the\nneed to take internal flow dependencies into account. In a comprehensive\nnumerical evaluation, we demonstrate how this unleashed PMOO analysis can\nreduce the known gap between simulations and SNC calculations significantly,\nand how it favourably compares to state-of-the art SNC calculations in terms of\naccuracy and computational effort. Driven by these promising results, we also\nconsider general feedforward networks, when some flow dependencies have to be\ntaken into account. To that end, the unleashed PMOO analysis is extended to the\npartially dependent case and a case study of a canonical example topology,\nknown as the diamond network, is provided, again displaying favourable results\nover the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 09:11:02 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bouillard", "Anne", ""], ["Nikolaus", "Paul", ""], ["Schmitt", "Jens", ""]]}, {"id": "2104.14246", "submitter": "Roberto Rocco", "authors": "Roberto Rocco, Davide Gadioli, Gianluca Palermo", "title": "Legio: Fault Resiliency for Embarrassingly Parallel MPI Applications", "comments": null, "journal-ref": null, "doi": "10.1007/s11227-021-03951-w", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the increasing size of HPC machines, the fault presence is becoming an\neventuality that applications must face. Natively, MPI provides no support for\nthe execution past the detection of a fault, and this is becoming more and more\nconstraining. With the introduction of ULFM (User Level Fault Mitigation\nlibrary), it has been provided with a possible way to overtake a fault during\nthe application execution at the cost of code modifications. ULFM is intrusive\nin the application and requires also a deep understanding of its recovery\nprocedures.\n  In this paper we propose Legio, a framework that lowers the complexity of\nintroducing resiliency in an embarrassingly parallel MPI application. By hiding\nULFM behind the MPI calls, the library is capable to expose resiliency features\nto the application in a transparent manner thus removing any integration\neffort. Upon fault, the failed nodes are discarded and the execution continues\nonly with the non-failed ones. A hierarchical implementation of the solution\nhas been also proposed to reduce the overhead of the repair process when\nscaling towards a large number of nodes.\n  We evaluated our solutions on the Marconi100 cluster at CINECA, showing that\nthe overhead introduced by the library is negligible and it does not limit the\nscalability properties of MPI. Moreover, we also integrated the solution in\nreal-world applications to further prove its robustness by injecting faults.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 10:34:21 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rocco", "Roberto", ""], ["Gadioli", "Davide", ""], ["Palermo", "Gianluca", ""]]}, {"id": "2104.14392", "submitter": "Shreshth Tuli", "authors": "Shreshth Tuli, Shivananda Poojara, Satish N. Srirama, Giuliano Casale,\n  Nicholas R. Jennings", "title": "COSCO: Container Orchestration using Co-Simulation and Gradient Based\n  Optimization for Fog Computing Environments", "comments": "Accepted in IEEE Transactions on Parallel and Distributed Systems,\n  2021", "journal-ref": null, "doi": "10.1109/TPDS.2021.3087349", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent task placement and management of tasks in large-scale fog\nplatforms is challenging due to the highly volatile nature of modern workload\napplications and sensitive user requirements of low energy consumption and\nresponse time. Container orchestration platforms have emerged to alleviate this\nproblem with prior art either using heuristics to quickly reach scheduling\ndecisions or AI driven methods like reinforcement learning and evolutionary\napproaches to adapt to dynamic scenarios. The former often fail to quickly\nadapt in highly dynamic environments, whereas the latter have run-times that\nare slow enough to negatively impact response time. Therefore, there is a need\nfor scheduling policies that are both reactive to work efficiently in volatile\nenvironments and have low scheduling overheads. To achieve this, we propose a\nGradient Based Optimization Strategy using Back-propagation of gradients with\nrespect to Input (GOBI). Further, we leverage the accuracy of predictive\ndigital-twin models and simulation capabilities by developing a Coupled\nSimulation and Container Orchestration Framework (COSCO). Using this, we create\na hybrid simulation driven decision approach, GOBI*, to optimize Quality of\nService (QoS) parameters. Co-simulation and the back-propagation approaches\nallow these methods to adapt quickly in volatile environments. Experiments\nconducted using real-world data on fog applications using the GOBI and GOBI*\nmethods, show a significant improvement in terms of energy consumption,\nresponse time, Service Level Objective and scheduling time by up to 15, 40, 4,\nand 82 percent respectively when compared to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:09:44 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 17:32:14 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 13:08:48 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Tuli", "Shreshth", ""], ["Poojara", "Shivananda", ""], ["Srirama", "Satish N.", ""], ["Casale", "Giuliano", ""], ["Jennings", "Nicholas R.", ""]]}, {"id": "2104.14447", "submitter": "Quang-Thinh Ha", "authors": "Quang-Thinh Ha, Paul A. Kuberry, Nathaniel A. Trask, Emily M. Ryan", "title": "Parallel implementation of a compatible high-order meshless method for\n  the Stokes' equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA cs.PF math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel implementation of a compatible discretization scheme for\nsteady-state Stokes problems is presented in this work. The scheme uses\ngeneralized moving least squares to generate differential operators and apply\nboundary conditions. This meshless scheme allows a high-order convergence for\nboth the velocity and pressure, while also incorporates finite-difference-like\nsparse discretization. Additionally, the method is inherently scalable: the\nstencil generation process requires local inversion of matrices amenable to GPU\nacceleration, and the divergence-free treatment of velocity replaces the\ntraditional saddle point structure of the global system with elliptic diagonal\nblocks amenable to algebraic multigrid. The implementation in this work uses a\nvariety of Trilinos packages to exploit this local and global parallelism, and\nbenchmarks demonstrating high-order convergence and weak scalability are\nprovided.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:06:13 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ha", "Quang-Thinh", ""], ["Kuberry", "Paul A.", ""], ["Trask", "Nathaniel A.", ""], ["Ryan", "Emily M.", ""]]}, {"id": "2104.14677", "submitter": "Leila Zahedi", "authors": "Leila Zahedi, Farid Ghareh Mohammadi, Shabnam Rezapour, Matthew W.\n  Ohland, M. Hadi Amini", "title": "Search Algorithms for Automated Hyper-Parameter Tuning", "comments": "10 pages, 3 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a powerful method for modeling in different fields such\nas education. Its capability to accurately predict students' success makes it\nan ideal tool for decision-making tasks related to higher education. The\naccuracy of machine learning models depends on selecting the proper\nhyper-parameters. However, it is not an easy task because it requires time and\nexpertise to tune the hyper-parameters to fit the machine learning model. In\nthis paper, we examine the effectiveness of automated hyper-parameter tuning\ntechniques to the realm of students' success. Therefore, we develop two\nautomated Hyper-Parameter Optimization methods, namely grid search and random\nsearch, to assess and improve a previous study's performance. The experiment\nresults show that applying random search and grid search on machine learning\nalgorithms improves accuracy. We empirically show automated methods'\nsuperiority on real-world educational data (MIDFIELD) for tuning HPs of\nconventional machine learning classifiers. This work emphasizes the\neffectiveness of automated hyper-parameter optimization while applying machine\nlearning in the education field to aid faculties, directors', or non-expert\nusers' decisions to improve students' success.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:11:52 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zahedi", "Leila", ""], ["Mohammadi", "Farid Ghareh", ""], ["Rezapour", "Shabnam", ""], ["Ohland", "Matthew W.", ""], ["Amini", "M. Hadi", ""]]}, {"id": "2104.14879", "submitter": "Emmanuel Hyon", "authors": "Thomas Tournaire and Hind Castel-Taleb and Emmanuel Hyon", "title": "Optimal control policies for resource allocation in the Cloud:\n  comparison between Markov decision process and heuristic approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider an auto-scaling technique in a cloud system where virtual\nmachines hosted on a physical node are turned on and off depending on the\nqueue's occupation (or thresholds), in order to minimise a global cost\nintegrating both energy consumption and performance. We propose several\nefficient optimisation methods to find threshold values minimising this global\ncost: local search heuristics coupled with aggregation of Markov chain and with\nqueues approximation techniques to reduce the execution time and improve the\naccuracy. The second approach tackles the problem with a Markov Decision\nProcess (MDP) for which we proceed to a theoretical study and provide\ntheoretical comparison with the first approach. We also develop structured MDP\nalgorithms integrating hysteresis properties. We show that MDP algorithms\n(value iteration, policy iteration) and especially structured MDP algorithms\noutperform the devised heuristics, in terms of time execution and accuracy.\nFinally, we propose a cost model for a real scenario of a cloud system to apply\nour optimisation algorithms and show their relevance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 10:07:44 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 20:41:20 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Tournaire", "Thomas", ""], ["Castel-Taleb", "Hind", ""], ["Hyon", "Emmanuel", ""]]}, {"id": "2104.15109", "submitter": "Jean-Baptiste Truong", "authors": "Jean-Baptiste Truong, William Gallagher, Tian Guo, Robert J. Walls", "title": "Memory-Efficient Deep Learning Inference in Trusted Execution\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study identifies and proposes techniques to alleviate two key\nbottlenecks to executing deep neural networks in trusted execution environments\n(TEEs): page thrashing during the execution of convolutional layers and the\ndecryption of large weight matrices in fully-connected layers. For the former,\nwe propose a novel partitioning scheme, y-plane partitioning, designed to (ii)\nprovide consistent execution time when the layer output is large compared to\nthe TEE secure memory; and (ii) significantly reduce the memory footprint of\nconvolutional layers. For the latter, we leverage quantization and compression.\nIn our evaluation, the proposed optimizations incurred latency overheads\nranging from 1.09X to 2X baseline for a wide range of TEE sizes; in contrast,\nan unmodified implementation incurred latencies of up to 26X when running\ninside of the TEE.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:48:14 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Truong", "Jean-Baptiste", ""], ["Gallagher", "William", ""], ["Guo", "Tian", ""], ["Walls", "Robert J.", ""]]}]