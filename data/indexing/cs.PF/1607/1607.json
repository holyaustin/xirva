[{"id": "1607.00145", "submitter": "Paul Springer", "authors": "Paul Springer and Paolo Bientinesi", "title": "Design of a high-performance GEMM-like Tensor-Tensor Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \"GEMM-like Tensor-Tensor multiplication\" (GETT), a novel approach\nto tensor contractions that mirrors the design of a high-performance general\nmatrix-matrix multiplication (GEMM). The critical insight behind GETT is the\nidentification of three index sets, involved in the tensor contraction, which\nenable us to systematically reduce an arbitrary tensor contraction to loops\naround a highly tuned \"macro-kernel\". This macro-kernel operates on suitably\nprepared (\"packed\") sub-tensors that reside in a specified level of the cache\nhierarchy. In contrast to previous approaches to tensor contractions, GETT\nexhibits desirable features such as unit-stride memory accesses,\ncache-awareness, as well as full vectorization, without requiring auxiliary\nmemory. To compare our technique with other modern tensor contractions, we\nintegrate GETT alongside the so called Transpose-Transpose-GEMM-Transpose and\nLoops-over-GEMM approaches into an open source \"Tensor Contraction Code\nGenerator\" (TCCG). The performance results for a wide range of tensor\ncontractions suggest that GETT has the potential of becoming the method of\nchoice: While GETT exhibits excellent performance across the board, its\neffectiveness for bandwidth-bound tensor contractions is especially impressive,\noutperforming existing approaches by up to $12.4\\times$. More precisely, GETT\nachieves speedups of up to $1.41\\times$ over an equivalent-sized GEMM for\nbandwidth-bound tensor contractions while attaining up to $91.3\\%$ of peak\nfloating-point performance for compute-bound tensor contractions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 08:13:50 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 07:28:12 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 08:21:02 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Springer", "Paul", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1607.00291", "submitter": "Devin Matthews", "authors": "Devin A. Matthews", "title": "High-Performance Tensor Contraction without Transposition", "comments": "24 pages, 8 figures, uses pgfplots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor computations--in particular tensor contraction (TC)--are important\nkernels in many scientific computing applications. Due to the fundamental\nsimilarity of TC to matrix multiplication (MM) and to the availability of\noptimized implementations such as the BLAS, tensor operations have\ntraditionally been implemented in terms of BLAS operations, incurring both a\nperformance and a storage overhead. Instead, we implement TC using the flexible\nBLIS framework, which allows for transposition (reshaping) of the tensor to be\nfused with internal partitioning and packing operations, requiring no explicit\ntransposition operations or additional workspace. This implementation, TBLIS,\nachieves performance approaching that of MM, and in some cases considerably\nhigher than that of traditional TC. Our implementation supports multithreading\nusing an approach identical to that used for MM in BLIS, with similar\nperformance characteristics. The complexity of managing tensor-to-matrix\ntransformations is also handled automatically in our approach, greatly\nsimplifying its use in scientific applications.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:37:59 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 21:16:54 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 20:49:01 GMT"}, {"version": "v4", "created": "Tue, 11 Jul 2017 15:03:52 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Matthews", "Devin A.", ""]]}, {"id": "1607.00372", "submitter": "\\v{L}ubo\\v{s} Koren\\v{c}iak", "authors": "\\v{L}ubo\\v{s} Koren\\v{c}iak and Anton\\'in Ku\\v{c}era and Vojt\\v{e}ch\n  \\v{R}eh\\'ak", "title": "Efficient Timeout Synthesis in Fixed-Delay CTMC Using Policy Iteration", "comments": "This article is a full version of a paper published at Modeling,\n  Analysis, and Simulation On Computer and Telecommunication Systems (MASCOTS)\n  2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fixed-delay synthesis problem for continuous-time Markov\nchains extended with fixed-delay transitions (fdCTMC). The goal is to\nsynthesize concrete values of the fixed-delays (timeouts) that minimize the\nexpected total cost incurred before reaching a given set of target states. The\nsame problem has been considered and solved in previous works by computing an\noptimal policy in a certain discrete-time Markov decision process (MDP) with a\nhuge number of actions that correspond to suitably discretized values of the\ntimeouts.\n  In this paper, we design a symbolic fixed-delay synthesis algorithm which\navoids the explicit construction of large action spaces. Instead, the algorithm\ncomputes a small sets of \"promising\" candidate actions on demand. The candidate\nactions are selected by minimizing a certain objective function by computing\nits symbolic derivative and extracting a univariate polynomial whose roots are\nprecisely the points where the derivative takes zero value. Since roots of high\ndegree univariate polynomials can be isolated very efficiently using modern\nmathematical software, we achieve not only drastic memory savings but also\nspeedup by three orders of magnitude compared to the previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 19:56:19 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Koren\u010diak", "\u013dubo\u0161", ""], ["Ku\u010dera", "Anton\u00edn", ""], ["\u0158eh\u00e1k", "Vojt\u011bch", ""]]}, {"id": "1607.00714", "submitter": "Yongkun Li", "authors": "Gaoying Ju, Yongkun Li, Yinlong Xu, Jiqiang Chen, John C. S. Lui", "title": "Stochastic Modeling of Hybrid Cache Systems", "comments": "14 pages; mascots 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there is an increasing demand of big memory systems so to\nperform large scale data analytics. Since DRAM memories are expensive, some\nresearchers are suggesting to use other memory systems such as non-volatile\nmemory (NVM) technology to build large-memory computing systems. However,\nwhether the NVM technology can be a viable alternative (either economically and\ntechnically) to DRAM remains an open question. To answer this question, it is\nimportant to consider how to design a memory system from a \"system\nperspective\", that is, incorporating different performance characteristics and\nprice ratios from hybrid memory devices.\n  This paper presents an analytical model of a \"hybrid page cache system\" so to\nunderstand the diverse design space and performance impact of a hybrid cache\nsystem. We consider (1) various architectural choices, (2) design strategies,\nand (3) configuration of different memory devices. Using this model, we provide\nguidelines on how to design hybrid page cache to reach a good trade-off between\nhigh system throughput (in I/O per sec or IOPS) and fast cache reactivity which\nis defined by the time to fill the cache. We also show how one can configure\nthe DRAM capacity and NVM capacity under a fixed budget. We pick PCM as an\nexample for NVM and conduct numerical analysis. Our analysis indicates that\nincorporating PCM in a page cache system significantly improves the system\nperformance, and it also shows larger benefit to allocate more PCM in page\ncache in some cases. Besides, for the common setting of performance-price ratio\nof PCM, \"flat architecture\" offers as a better choice, but \"layered\narchitecture\" outperforms if PCM write performance can be significantly\nimproved in the future.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 01:10:11 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 07:28:02 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Ju", "Gaoying", ""], ["Li", "Yongkun", ""], ["Xu", "Yinlong", ""], ["Chen", "Jiqiang", ""], ["Lui", "John C. S.", ""]]}, {"id": "1607.01249", "submitter": "Paul Springer", "authors": "Paul Springer, Aravind Sankaran, Paolo Bientinesi", "title": "TTC: A Tensor Transposition Compiler for Multiple Architectures", "comments": null, "journal-ref": null, "doi": "10.1145/2935323.2935328", "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of transposing tensors of arbitrary dimension and\ndescribe TTC, an open source domain-specific parallel compiler. TTC generates\noptimized parallel C++/CUDA C code that achieves a significant fraction of the\nsystem's peak memory bandwidth. TTC exhibits high performance across multiple\narchitectures, including modern AVX-based systems (e.g.,~Intel Haswell, AMD\nSteamroller), Intel's Knights Corner as well as different CUDA-based GPUs such\nas NVIDIA's Kepler and Maxwell architectures. We report speedups of TTC over a\nmeaningful baseline implementation generated by external C++ compilers; the\nresults suggest that a domain-specific compiler can outperform its general\npurpose counterpart significantly: For instance, comparing with Intel's latest\nC++ compiler on the Haswell and Knights Corner architecture, TTC yields\nspeedups of up to $8\\times$ and $32\\times$, respectively. We also showcase\nTTC's support for multiple leading dimensions, making it a suitable candidate\nfor the generation of performance-critical packing functions that are at the\ncore of the ubiquitous BLAS 3 routines.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 13:53:57 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Springer", "Paul", ""], ["Sankaran", "Aravind", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1607.01948", "submitter": "Evgeny Tsimbalo", "authors": "Evgeny Tsimbalo, Andrea Tassi, Robert J. Piechocki", "title": "Novel Performance Analysis of Network Coded Communications in\n  Single-Relay Networks", "comments": "Proceedings of IEEE GLOBECOM 2016 Communication Theory Symposium, to\n  appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI cs.PF math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the performance of a single-relay network in which\nthe reliability is provided by means of Random Linear Network Coding (RLNC). We\nconsider a scenario when both source and relay nodes can encode packets. Unlike\nthe traditional approach to relay networks, we introduce a passive relay mode,\nin which the relay node simply retransmits collected packets in case it cannot\ndecode them. In contrast with the previous studies, we derive a novel\ntheoretical framework for the performance characterization of the considered\nrelay network. We extend our analysis to a more general scenario, in which\ncoding coefficients are generated from non-binary fields. The theoretical\nresults are verified using simulation, for both binary and non-binary fields.\nIt is also shown that the passive relay mode significantly improves the\nperformance compared with the active-only case, offering an up to two-fold gain\nin terms of the decoding probability. The proposed framework can be used as a\nbuilding block for the analysis of more complex network topologies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 10:21:05 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 15:42:42 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Tsimbalo", "Evgeny", ""], ["Tassi", "Andrea", ""], ["Piechocki", "Robert J.", ""]]}, {"id": "1607.02001", "submitter": "EPTCS", "authors": "Maurice H. ter Beek (ISTI-CNR, Pisa, Italy), Michele Loreti\n  (University of Florence, Italy)", "title": "Proceedings of the Workshop on FORmal methods for the quantitative\n  Evaluation of Collective Adaptive SysTems", "comments": null, "journal-ref": "EPTCS 217, 2016", "doi": "10.4204/EPTCS.217", "report-no": null, "categories": "cs.LO cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective Adaptive Systems (CAS) consist of a large number of spatially\ndistributed heterogeneous entities with decentralised control and varying\ndegrees of complex autonomous behaviour that may be competing for shared\nresources even when collaborating to reach common goals. It is important to\ncarry out thorough quantitative modelling and analysis and verification of\ntheir design to investigate all aspects of their behaviour before they are put\ninto operation. This requires combinations of formal methods and applied\nmathematics which moreover scale to large-scale CAS. The primary goal of\nFORECAST is to raise awareness in the software engineering and formal methods\ncommunities of the particularities of CAS and the design and control problems\nwhich they bring.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 13:23:39 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["ter Beek", "Maurice H.", "", "ISTI-CNR, Pisa, Italy"], ["Loreti", "Michele", "", "University of Florence, Italy"]]}, {"id": "1607.02233", "submitter": "EPTCS", "authors": "Diego Latella (CNR-ISTI)", "title": "On Formal Methods for Collective Adaptive System Engineering. {Scalable\n  Approximated, Spatial} Analysis Techniques. Extended Abstract", "comments": "In Proceedings FORECAST 2016, arXiv:1607.02001", "journal-ref": "EPTCS 217, 2016, pp. 53-61", "doi": "10.4204/EPTCS.217.7", "report-no": null, "categories": "cs.LO cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this extended abstract a view on the role of Formal Methods in System\nEngineering is briefly presented. Then two examples of useful analysis\ntechniques based on solid mathematical theories are discussed as well as the\nsoftware tools which have been built for supporting such techniques. The first\ntechnique is Scalable Approximated Population DTMC Model-checking. The second\none is Spatial Model-checking for Closure Spaces. Both techniques have been\ndeveloped in the context of the EU funded project QUANTICOL.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:36:36 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Latella", "Diego", "", "CNR-ISTI"]]}, {"id": "1607.02561", "submitter": "Cong Yan", "authors": "Cong Yan, Alvin Cheung, Shan Lu", "title": "Database-Backed Web Applications in the Wild: How Well Do They Work?", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern database-backed web applications are built upon Object Relational\nMapping (ORM) frameworks. While ORM frameworks ease application development by\nabstracting persistent data as objects, such convenience often comes with a\nperformance cost. In this paper, we present CADO, a tool that analyzes the\napplication logic and its interaction with databases using the Ruby on Rails\nORM framework. CADO includes a static program analyzer, a profiler and a\nsynthetic data generator to extract and understand application's performance\ncharacteristics. We used CADO to analyze the performance problems of 27\nreal-world open-source Rails applications, covering domains such as online\nforums, e-commerce, project management, blogs, etc. Based on the results, we\nuncovered a number of issues that lead to sub-optimal application performance,\nranging from issuing queries, how result sets are used, and physical design. We\nsuggest possible remedies for each issue, and highlight new research\nopportunities that arise from them.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 02:40:37 GMT"}, {"version": "v2", "created": "Sat, 16 Jul 2016 01:01:43 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 21:15:15 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Yan", "Cong", ""], ["Cheung", "Alvin", ""], ["Lu", "Shan", ""]]}, {"id": "1607.02904", "submitter": "Markus H\\\"ohnerbach", "authors": "Markus H\\\"ohnerbach, Ahmed E. Ismail, Paolo Bientinesi", "title": "The Vectorization of the Tersoff Multi-Body Potential: An Exercise in\n  Performance Portability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular dynamics simulations, an indispensable research tool in\ncomputational chemistry and materials science, consume a significant portion of\nthe supercomputing cycles around the world. We focus on multi-body potentials\nand aim at achieving performance portability. Compared with well-studied pair\npotentials, multibody potentials deliver increased simulation accuracy but are\ntoo complex for effective compiler optimization. Because of this, achieving\ncross-platform performance remains an open question. By abstracting from target\narchitecture and computing precision, we develop a vectorization scheme\napplicable to both CPUs and accelerators. We present results for the Tersoff\npotential within the molecular dynamics code LAMMPS on several architectures,\ndemonstrating efficiency gains not only for computational kernels, but also for\nlarge-scale simulations. On a cluster of Intel Xeon Phi's, our optimized solver\nis between 3 and 5 times faster than the pure MPI reference.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 11:23:04 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["H\u00f6hnerbach", "Markus", ""], ["Ismail", "Ahmed E.", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1607.02966", "submitter": "EPTCS", "authors": "Mirco Tribastone (IMT School for Advanced Studies Lucca, Italy)", "title": "Challenges in Quantitative Abstractions for Collective Adaptive Systems", "comments": "In Proceedings FORECAST 2016, arXiv:1607.02001", "journal-ref": "EPTCS 217, 2016, pp. 62-68", "doi": "10.4204/EPTCS.217.8", "report-no": null, "categories": "cs.SY cs.LO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like with most large-scale systems, the evaluation of quantitative properties\nof collective adaptive systems is an important issue that crosscuts all its\ndevelopment stages, from design (in the case of engineered systems) to runtime\nmonitoring and control. Unfortunately it is a difficult problem to tackle in\ngeneral, due to the typically high computational cost involved in the analysis.\nThis calls for the development of appropriate quantitative abstraction\ntechniques that preserve most of the system's dynamical behaviour using a more\ncompact representation. This paper focuses on models based on ordinary\ndifferential equations and reviews recent results where abstraction is achieved\nby aggregation of variables, reflecting on the shortcomings in the state of the\nart and setting out challenges for future research.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:36:45 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Tribastone", "Mirco", "", "IMT School for Advanced Studies Lucca, Italy"]]}, {"id": "1607.04077", "submitter": "Kristina Kapanova G", "authors": "K.G.Kapanova, J.M.Sellier", "title": "Designing a High Performance Parallel Personal Cluster", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, many scientific and engineering areas require high performance\ncomputing to perform computationally intensive experiments. For example, many\nadvances in transport phenomena, thermodynamics, material properties,\ncomputational chemistry and physics are possible only because of the\navailability of such large scale computing infrastructures. Yet many challenges\nare still open. The cost of energy consumption, cooling, competition for\nresources have been some of the reasons why the scientific and engineering\ncommunities are turning their interests to the possibility of implementing\nenergy-efficient servers utilizing low-power CPUs for computing-intensive\ntasks. In this paper we introduce a novel approach, which was recently\npresented at Linux Conference Europe 2015, based on the Beowulf concept and\nutilizing single board computers (SBC). We present a low-energy consumption\narchitecture capable to tackle heavily demanding scientific computational\nproblems. Additionally, our goal is to provide a low cost personal solution for\nscientists and engineers. In order to evaluate the performance of the proposed\narchitecture we ran several standard benchmarking tests. Furthermore, we assess\nthe reliability of the machine in real life situations by performing two\nbenchmark tools involving practical TCAD for physicist and engineers in the\nsemiconductor industry.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 10:41:52 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Kapanova", "K. G.", ""], ["Sellier", "J. M.", ""]]}, {"id": "1607.04298", "submitter": "Farshid Farhat", "authors": "Diman Zad Tootaghaj and Farshid Farhat", "title": "Optimal Placement of Cores, Caches and Memory Controllers in Network\n  On-Chip", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel programming is emerging fast and intensive applications need more\nresources, so there is a huge demand for on-chip multiprocessors. Accessing L1\ncaches beside the cores are the fastest after registers but the size of private\ncaches cannot increase because of design, cost and technology limits. Then\nsplit I-cache and D-cache are used with shared LLC (last level cache). For a\nunified shared LLC, bus interface is not scalable, and it seems that\ndistributed shared LLC (DSLLC) is a better choice. Most of papers assume a\ndistributed shared LLC beside each core in on-chip network. Many works assume\nthat DSLLCs are placed in all cores; however, we will show that this design\nignores the effect of traffic congestion in on-chip network. In fact, our work\nfocuses on optimal placement of cores, DSLLCs and even memory controllers to\nminimize the expected latency based on traffic load in a mesh on-chip network\nwith fixed number of cores and total cache capacity. We try to do some\nanalytical modeling deriving intended cost function and then optimize the mean\ndelay of the on-chip network communication. This work is supposed to be\nverified using some traffic patterns that are run on CSIM simulator.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 20:09:47 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 23:52:56 GMT"}, {"version": "v3", "created": "Thu, 25 Aug 2016 19:05:46 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 16:48:31 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Tootaghaj", "Diman Zad", ""], ["Farhat", "Farshid", ""]]}, {"id": "1607.05356", "submitter": "Neil J. Gunther", "authors": "James F. Brady and Neil J. Gunther", "title": "How to Emulate Web Traffic Using Standard Load Testing Tools", "comments": "29 pages, 12 figures. To appear in the proceedings of CMG imPACt, La\n  Jolla, CA, Nov. 7-10, 2016. v2 has new Figs. 1 and 5, as well as major text\n  reformatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional load-testing tools are based on a fifty-year old time-share\ncomputer paradigm where a finite number of users submit requests and respond in\na synchronized fashion. Conversely, modern web traffic is essentially\nasynchronous and driven by an unknown number of users. This difference presents\na conundrum for testing the performance of modern web applications. Even when\nthe difference is recognized, performance engineers often introduce\nmodifications to their test scripts based on folklore or hearsay published in\nvarious Internet fora, much of which can lead to wrong results. We present a\ncoherent methodology, based on two fundamental principles, for emulating web\ntraffic using a standard load-test environment.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 23:57:15 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 03:06:52 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Brady", "James F.", ""], ["Gunther", "Neil J.", ""]]}, {"id": "1607.05537", "submitter": "Gaurav Khanna", "authors": "Glenn Volkema, Gaurav Khanna", "title": "Scientific Computing Using Consumer Video-Gaming Hardware Devices", "comments": "8 pages, 6 figures; includes benchmark results on common scientific\n  computing kernels", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.ET cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commodity video-gaming hardware (consoles, graphics cards, tablets, etc.)\nperformance has been advancing at a rapid pace owing to strong consumer demand\nand stiff market competition. Gaming hardware devices are currently amongst the\nmost powerful and cost-effective computational technologies available in\nquantity. In this article, we evaluate a sample of current generation\nvideo-gaming hardware devices for scientific computing and compare their\nperformance with specialized supercomputing general purpose graphics processing\nunits (GPGPUs). We use the OpenCL SHOC benchmark suite, which is a measure of\nthe performance of compute hardware on various different scientific application\nkernels, and also a popular public distributed computing application,\nEinstein@Home in the field of gravitational physics for the purposes of this\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 12:14:36 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Volkema", "Glenn", ""], ["Khanna", "Gaurav", ""]]}, {"id": "1607.05840", "submitter": "Isabel Wagner", "authors": "Isabel Wagner", "title": "Evaluating the Strength of Genomic Privacy Metrics", "comments": "27 pages, submitted to ACM Transaction on Privacy and Security", "journal-ref": "ACM Trans. Priv. Secur. 20, 1, Article 2 (January 2017), 34 pages", "doi": "10.1145/3020003", "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The genome is a unique identifier for human individuals. The genome also\ncontains highly sensitive information, creating a high potential for misuse of\ngenomic data (for example, genetic discrimination). In this paper, I\ninvestigated how genomic privacy can be measured in scenarios where an\nadversary aims to infer a person's genomic markers by constructing probability\ndistributions on the values of genetic variations. I measured the strength of\nprivacy metrics by requiring that metrics are monotonic with increasing\nadversary strength and uncovered serious problems with several existing metrics\ncurrently used to measure genomic privacy. I provide suggestions on metric\nselection, interpretation, and visualization, and illustrate the work flow\nusing a case study on Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 07:34:17 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Wagner", "Isabel", ""]]}, {"id": "1607.08102", "submitter": "Neda Petreska", "authors": "Neda Petreska and Hussein Al-Zubaidy and Barbara Staehle and Rudi\n  Knorr and James Gross", "title": "Statistical Delay Bound for WirelessHART Networks", "comments": "Accepted at PE-WASUN 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a performance analysis framework for wireless\nindustrial networks by deriving a service curve and a bound on the delay\nviolation probability. For this purpose we use the (min,x) stochastic network\ncalculus as well as a recently presented recursive formula for an end-to-end\ndelay bound of wireless heterogeneous networks. The derived results are mapped\nto WirelessHART networks used in process automation and were validated via\nsimulations. In addition to WirelessHART, our results can be applied to any\nwireless network whose physical layer conforms the IEEE 802.15.4 standard,\nwhile its MAC protocol incorporates TDMA and channel hopping, like e.g.\nISA100.11a or TSCH-based networks. The provided delay analysis is especially\nuseful during the network design phase, offering further research potential\ntowards optimal routing and power management in QoS-constrained wireless\nindustrial networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:15:26 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Petreska", "Neda", ""], ["Al-Zubaidy", "Hussein", ""], ["Staehle", "Barbara", ""], ["Knorr", "Rudi", ""], ["Gross", "James", ""]]}, {"id": "1607.08344", "submitter": "Nicolas Gilberto Gutierrez Ortiz", "authors": "Nicolas Gutierrez and Manuela Wiesinger-Widi", "title": "AUGURY: A time-series based application for the analysis and forecasting\n  of system and network performance metrics", "comments": "8 pages, 9 figures, submitted to SYNASC2016", "journal-ref": null, "doi": "10.1109/SYNASC.2016.062", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents AUGURY, an application for the analysis of monitoring\ndata from computers, servers or cloud infrastructures. The analysis is based on\nthe extraction of patterns and trends from historical data, using elements of\ntime-series analysis. The purpose of AUGURY is to aid a server administrator by\nforecasting the behaviour and resource usage of specific applications and in\npresenting a status report in a concise manner. AUGURY provides tools for\nidentifying network traffic congestion and peak usage times, and for making\nmemory usage projections. The application data processing specialises in two\ntasks: the parametrisation of the memory usage of individual applications and\nthe extraction of the seasonal component from network traffic data. AUGURY uses\na different underlying assumption for each of these two tasks. With respect to\nthe memory usage, a limited number of single-valued parameters are assumed to\nbe sufficient to parameterize any application being hosted on the server.\nRegarding the network traffic data, long-term patterns, such as hourly or daily\nexist and are being induced by work-time schedules and automatised\nadministrative jobs. In this paper, the implementation of each of the two tasks\nis presented, tested using locally-generated data, and applied to data from\nweather forecasting applications hosted on a web server. This data is used to\ndemonstrate the insight that AUGURY can add to the monitoring of server and\ncloud infrastructures.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 08:02:29 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Gutierrez", "Nicolas", ""], ["Wiesinger-Widi", "Manuela", ""]]}, {"id": "1607.08450", "submitter": "Aresh Dadlani", "authors": "Muthukrishnan Senthil Kumar, Aresh Dadlani, Kiseon Kim, Richard O.\n  Afolabi", "title": "Overlay Secondary Spectrum Sharing with Independent Re-attempts in\n  Cognitive Radios", "comments": "IEEE Sarnoff Symposium 2016", "journal-ref": null, "doi": "10.1109/SARNOF.2016.7846750", "report-no": null, "categories": "cs.PF cs.NI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opportunistic spectrum access (OSA) is a promising reform paradigm envisioned\nto address the issue of spectrum scarcity in cognitive radio networks (CRNs).\nWhile current models consider various aspects of the OSA scheme, the impact of\nretrial phenomenon in multi-channel CRNs has not yet been analyzed. In this\nwork, we present a continuous-time Markov chain (CTMC) model in which the\nblocked/preempted secondary users (SUs) enter a finite retrial group (or orbit)\nand re-attempt independently for service in an exponentially distributed random\nmanner. Taking into account the inherent retrial tendency of SUs, we\nnumerically assess the performance of the proposed scheme in terms of dropping\nprobability and throughput of SUs.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:34:40 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Kumar", "Muthukrishnan Senthil", ""], ["Dadlani", "Aresh", ""], ["Kim", "Kiseon", ""], ["Afolabi", "Richard O.", ""]]}]