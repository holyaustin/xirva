[{"id": "1811.00156", "submitter": "Beau Johnston", "authors": "Beau Johnston, Greg Falzon and Josh Milthorpe", "title": "OpenCL Performance Prediction using Architecture-Independent Features", "comments": "9 pages, 6 figures, International Workshop on High Performance and\n  Dynamic Reconfigurable Systems and Networks (DRSN-2018) published in\n  conjunction with The 2018 International Conference on High Performance\n  Computing & Simulation (HPCS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenCL is an attractive model for heterogeneous high-performance computing\nsystems, with wide support from hardware vendors and significant performance\nportability. To support efficient scheduling on HPC systems it is necessary to\nperform accurate performance predictions for OpenCL workloads on varied compute\ndevices, which is challenging due to diverse computation, communication and\nmemory access characteristics which result in varying performance between\ndevices. The Architecture Independent Workload Characterization (AIWC) tool can\nbe used to characterize OpenCL kernels according to a set of\narchitecture-independent features. This work presents a methodology where AIWC\nfeatures are used to form a model capable of predicting accelerator execution\ntimes. We used this methodology to predict execution times for a set of 37\ncomputational kernels running on 15 different devices representing a broad\nrange of CPU, GPU and MIC architectures. The predictions are highly accurate,\ndiffering from the measured experimental run-times by an average of only 1.2%,\nand correspond to actual execution time mispredictions of 9 {\\mu}s to 1 sec\naccording to problem size. A previously unencountered code can be instrumented\nonce and the AIWC metrics embedded in the kernel, to allow performance\nprediction across the full range of modelled devices. The results suggest that\nthis methodology supports correct selection of the most appropriate device for\na previously unencountered code, which is highly relevant to the HPC scheduling\nsetting.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:33:15 GMT"}], "update_date": "2018-11-04", "authors_parsed": [["Johnston", "Beau", ""], ["Falzon", "Greg", ""], ["Milthorpe", "Josh", ""]]}, {"id": "1811.00824", "submitter": "Marc Goerigk", "authors": "Marc Goerigk and Stephen J. Maher", "title": "Generating Hard Instances for Robust Combinatorial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While research in robust optimization has attracted considerable interest\nover the last decades, its algorithmic development has been hindered by several\nfactors. One of them is a missing set of benchmark instances that make\nalgorithm performance better comparable, and makes reproducing instances\nunnecessary. Such a benchmark set should contain hard instances in particular,\nbut so far, the standard approach to produce instances has been to sample\nvalues randomly from a uniform distribution.\n  In this paper we introduce a new method to produce hard instances for min-max\ncombinatorial optimization problems, which is based on an optimization model\nitself. Our approach does not make any assumptions on the problem structure and\ncan thus be applied to any combinatorial problem. Using the Selection and\nTraveling Salesman problems as examples, we show that it is possible to produce\ninstances which are up to 500 times harder to solve for a mixed-integer\nprogramming solver than the current state-of-the-art instances.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 11:20:04 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 16:16:07 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Goerigk", "Marc", ""], ["Maher", "Stephen J.", ""]]}, {"id": "1811.01412", "submitter": "Martin Becker", "authors": "Martin Becker and Samarjit Chakraborty", "title": "Measuring Software Performance on Linux", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring and analyzing the performance of software has reached a high\ncomplexity, caused by more advanced processor designs and the intricate\ninteraction between user programs, the operating system, and the processor's\nmicroarchitecture. In this report, we summarize our experience about how\nperformance characteristics of software should be measured when running on a\nLinux operating system and a modern processor. In particular, (1) We provide a\ngeneral overview about hardware and operating system features that may have a\nsignificant impact on timing and how they interact, (2) we identify sources of\nerrors that need to be controlled in order to obtain unbiased measurement\nresults, and (3) we propose a measurement setup for Linux to minimize errors.\nAlthough not the focus of this report, we describe the measurement process\nusing hardware performance counters, which can faithfully reflect the real\nbottlenecks on a given processor. Our experiments confirm that our measurement\nsetup has a large impact on the results. More surprisingly, however, they also\nsuggest that the setup can be negligible for certain analysis methods.\nFurthermore, we found that our setup maintains significantly better performance\nunder background load conditions, which means it can be used to improve\nsoftware in high-performance applications.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 18:18:27 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 16:53:49 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Becker", "Martin", ""], ["Chakraborty", "Samarjit", ""]]}, {"id": "1811.01611", "submitter": "Young Myoung Ko", "authors": "Yongkyu Cho and Young Myoung Ko", "title": "Stabilizing the virtual response time in single-server processor sharing\n  queues with slowly time-varying arrival rates", "comments": null, "journal-ref": "Annals of Operations Research, 293 (2020), 27-55", "doi": "10.1007/s10479-019-03511-9", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the work of Whitt, who studied stabilization of the mean virtual\nwaiting time (excluding service time) in a $GI_t/GI_t/1/FCFS$ queue, this paper\ninvestigates the stabilization of the mean virtual response time in a\nsingle-server processor sharing (PS) queueing system with a time-varying\narrival rate and a service rate control (a $GI_t/GI_t/1/PS$ queue). We propose\nand compare a modified square-root (SR) control and a difference-matching (DM)\ncontrol to stabilize the mean virtual response time of a $GI_t/GI_t/1/PS$\nqueue. Extensive simulation studies with various settings of arrival processes\nand service times show that the DM control outperforms the SR control for\nheavy-traffic conditions, and that the SR control performs better for\nlight-traffic conditions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 11:04:08 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Cho", "Yongkyu", ""], ["Ko", "Young Myoung", ""]]}, {"id": "1811.02287", "submitter": "Drew Schmidt", "authors": "Drew Schmidt, Junqi Yin, Michael Matheson, Bronson Messer, Mallikarjun\n  Shankar", "title": "Defining Big Data Analytics Benchmarks for Next Generation\n  Supercomputers", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and construction of high performance computing (HPC) systems\nrelies on exhaustive performance analysis and benchmarking. Traditionally this\nactivity has been geared exclusively towards simulation scientists, who,\nunsurprisingly, have been the primary customers of HPC for decades. However,\nthere is a large and growing volume of data science work that requires these\nlarge scale resources, and as such the calls for inclusion and investments in\ndata for HPC have been increasing. So when designing a next generation HPC\nplatform, it is necessary to have HPC-amenable big data analytics benchmarks.\nIn this paper, we propose a set of big data analytics benchmarks and sample\ncodes designed for testing the capabilities of current and next generation\nsupercomputers.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 11:10:38 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Schmidt", "Drew", ""], ["Yin", "Junqi", ""], ["Matheson", "Michael", ""], ["Messer", "Bronson", ""], ["Shankar", "Mallikarjun", ""]]}, {"id": "1811.02761", "submitter": "Yohei Miki", "authors": "Yohei Miki", "title": "Gravitational octree code performance evaluation on Volta GPU", "comments": "10 pages, 10 figures, 2 tables, submitted to Computer Physics\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS astro-ph.IM cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the gravitational octree code originally optimized for the\nFermi, Kepler, and Maxwell GPU architectures is adapted to the Volta\narchitecture. The Volta architecture introduces independent thread scheduling\nrequiring either the insertion of the explicit synchronizations at appropriate\nlocations or the enforcement of the same implicit synchronizations as do the\nPascal or earlier architectures by specifying \\texttt{-gencode\narch=compute\\_60,code=sm\\_70}. The performance measurements on Tesla V100, the\ncurrent flagship GPU by NVIDIA, revealed that the $N$-body simulations of the\nAndromeda galaxy model with $2^{23} = 8388608$ particles took $3.8 \\times\n10^{-2}$~s or $3.3 \\times 10^{-2}$~s per step for each case. Tesla V100\nachieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the\nflagship GPU in the previous generation. The observed speed-up of 2.2 is\ngreater than 1.5, which is the ratio of the theoretical peak performance of the\ntwo GPUs. The independence of the units for integer operations from those for\nfloating-point number operations enables the overlapped execution of integer\nand floating-point number operations. It hides the execution time of the\ninteger operations leading to the speed-up rate above the theoretical peak\nperformance ratio. Tesla V100 can execute $N$-body simulation with up to $25\n\\times 2^{20} = 26214400$ particles, and it took $2.0 \\times 10^{-1}$~s per\nstep. It corresponds to $3.5$~TFlop/s, which is 22\\% of the single-precision\ntheoretical peak performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 05:00:23 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Miki", "Yohei", ""]]}, {"id": "1811.03743", "submitter": "Jeffrey Young", "authors": "Patrick Lavin, Jeffrey Young, Jason Riedy, Richard Vuduc, Aaron Vose,\n  Dan Ernst", "title": "Spatter: A Tool for Evaluating Gather / Scatter Performance", "comments": "Updated paper results and text to reflect longer conference\n  submission limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new benchmark tool, Spatter, for assessing memory\nsystem architectures in the context of a specific category of indexed accesses\nknown as gather and scatter. These types of operations are increasingly used to\nexpress sparse and irregular data access patterns, and they have widespread\nutility in many modern HPC applications including scientific simulations, data\nmining and analysis computations, and graph processing. However, many\ntraditional benchmarking tools like STREAM, STRIDE, and GUPS focus on\ncharacterizing only uniform stride or fully random accesses despite evidence\nthat modern applications use varied sets of more complex access patterns.\n  Spatter is an open-source benchmark that provides a tunable and configurable\nframework to benchmark a variety of indexed access patterns, including\nvariations of gather/scatter that are seen in HPC mini-apps evaluated in this\nwork. The design of Spatter includes tunable backends for OpenMP and CUDA, and\nexperiments show how it can be used to evaluate 1) uniform access patterns for\nCPU and GPU, 2) prefetching regimes for gather/scatter, 3) compiler\nimplementations of vectorization for gather/scatter, and 4) trace-driven \"proxy\npatterns\" that reflect the patterns found in multiple applications. The results\nfrom Spatter experiments show that GPUs typically outperform CPUs for these\noperations, and that Spatter can better represent the performance of some\ncache-dependent mini-apps than traditional STREAM bandwidth measurements.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:26:34 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 19:04:24 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 21:20:05 GMT"}, {"version": "v4", "created": "Fri, 1 Nov 2019 15:41:24 GMT"}, {"version": "v5", "created": "Wed, 8 Jul 2020 01:07:12 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Lavin", "Patrick", ""], ["Young", "Jeffrey", ""], ["Riedy", "Jason", ""], ["Vuduc", "Richard", ""], ["Vose", "Aaron", ""], ["Ernst", "Dan", ""]]}, {"id": "1811.04556", "submitter": "Junhao Li", "authors": "Junhao Li", "title": "HPS: A C++11 High Performance Serialization Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data serialization is a common and crucial component in high performance\ncomputing. In this paper, I present a C++11 based serialization library for\nperformance critical systems. It provides an interface similar to Boost but up\nto 150% faster and beats several popular serialization libraries.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 05:09:04 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 20:52:22 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Li", "Junhao", ""]]}, {"id": "1811.04875", "submitter": "Junhao Li", "authors": "Junhao Li", "title": "Comparing Spark vs MPI/OpenMP On Word Count MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spark provides an in-memory implementation of MapReduce that is widely used\nin the big data industry. MPI/OpenMP is a popular framework for high\nperformance parallel computing. This paper presents a high performance\nMapReduce design in MPI/OpenMP and uses that to compare with Spark on the\nclassic word count MapReduce task. My result shows that the MPI/OpenMP\nMapReduce outperforms Apache Spark by about 300%.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 17:43:53 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 20:52:11 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Li", "Junhao", ""]]}, {"id": "1811.05239", "submitter": "Benny Van Houdt", "authors": "Benny Van Houdt", "title": "Global attraction of ODE-based mean field models with hyperexponential\n  job sizes", "comments": "This paper was accepted at ACM Sigmetrics 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field modeling is a popular approach to assess the performance of large\nscale computer systems. The evolution of many mean field models is\ncharacterized by a set of ordinary differential equations that have a unique\nfixed point. In order to prove that this unique fixed point corresponds to the\nlimit of the stationary measures of the finite systems, the unique fixed point\nmust be a global attractor. While global attraction was established for various\nsystems in case of exponential job sizes, it is often unclear whether these\nproof techniques can be generalized to non-exponential job sizes. In this paper\nwe show how simple monotonicity arguments can be used to prove global\nattraction for a broad class of ordinary differential equations that capture\nthe evolution of mean field models with hyperexponential job sizes. This class\nincludes both existing as well as previously unstudied load balancing schemes\nand can be used for systems with either finite or infinite buffers. The main\nnovelty of the approach exists in using a Coxian representation for the\nhyperexponential job sizes and a partial order that is stronger than the\ncomponentwise partial order used in the exponential case.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:07:04 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 09:12:23 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Van Houdt", "Benny", ""]]}, {"id": "1811.05618", "submitter": "Ian Briggs", "authors": "Michael Bentley, Ian Briggs, Ganesh Gopalakrishnan, Dong H. Ahn,\n  Ignacio Laguna, Gregory L. Lee, Holger E. Jones", "title": "Multi-level analysis of compiler induced variability and performance\n  tradeoffs", "comments": "12 pages, 11 figures, accepted in HPDC 2019", "journal-ref": null, "doi": "10.1145/3307681.3325960", "report-no": "LLNL-CONF-759867", "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful HPC software applications are long-lived. When ported across\nmachines and their compilers, these applications often produce different\nnumerical results, many of which are unacceptable. Such variability is also a\nconcern while optimizing the code more aggressively to gain performance.\nEfficient tools that help locate the program units (files and functions) within\nwhich most of the variability occurs are badly needed, both to plan for code\nports and to root-cause errors due to variability when they happen in the\nfield. In this work, we offer an enhanced version of the open-source testing\nframework FLiT to serve these roles. Key new features of FLiT include a suite\nof bisection algorithms that help locate the root causes of variability.\nAnother added feature allows an analysis of the tradeoffs between performance\nand the degree of variability. Our new contributions also include a collection\nof case studies. Results on the MFEM finite-element library include\nvariability/performance tradeoffs, and the identification of a (hitherto\nunknown) abnormal level of result-variability even under mild compiler\noptimizations. Results from studying the Laghos proxy application include\nidentifying a significantly divergent floating-point result-variability and\nsuccessful root-causing down to the problematic function over as little as 14\nprogram executions. Finally, in an evaluation of 4,376 controlled injections of\nfloating-point perturbations on the LULESH proxy application, we showed that\nthe FLiT framework has 100 precision and recall in discovering the file and\nfunction locations of the injections all within an average of only 15 program\nexecutions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 03:15:37 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 19:58:49 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Bentley", "Michael", ""], ["Briggs", "Ian", ""], ["Gopalakrishnan", "Ganesh", ""], ["Ahn", "Dong H.", ""], ["Laguna", "Ignacio", ""], ["Lee", "Gregory L.", ""], ["Jones", "Holger E.", ""]]}, {"id": "1811.08047", "submitter": "Chunhui Guo", "authors": "Chunhui Guo, Hao Wu, Xiayu Hua, Shangping Ren, Jerzy Nogiec", "title": "Optimizing System Quality of Service through Rejuvenation for\n  Long-Running Applications with Real-Time Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability, longevity, availability, and deadline guarantees are the four\nmost important metrics to measure the QoS of long-running safety-critical\nreal-time applications. Software aging is one of the major factors that impact\nthe safety of long-running real-time applications as the degraded performance\nand increased failure rate caused by software aging can lead to deadline\nmissing and catastrophic consequences. Software rejuvenation is one of the most\ncommonly used approaches to handle issues caused by software aging. In this\npaper, we study the optimal time when software rejuvenation shall take place so\nthat the system's reliability, longevity, and availability are maximized, and\napplication delays caused by software rejuvenation is minimized. In particular,\nwe formally analyze the relationships between software rejuvenation frequency\nand system reliability, longevity, and availability. Based on the theoretic\nanalysis, we develop approaches to maximizing system reliability, longevity,\nand availability, and use simulation to evaluate the developed approaches. In\naddition, we design the MIN-DELAY semi-priority-driven scheduling algorithm to\nminimize application delays caused by rejuvenation processes. The simulation\nexperiments show that the developed semi-priority-driven scheduling algorithm\nreduces application delays by 9.01% and 14.24% over the earliest deadline first\n(EDF) and least release time (LRT) scheduling algorithms, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 02:51:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Guo", "Chunhui", ""], ["Wu", "Hao", ""], ["Hua", "Xiayu", ""], ["Ren", "Shangping", ""], ["Nogiec", "Jerzy", ""]]}, {"id": "1811.08985", "submitter": "Alexandru Paler", "authors": "Alexandru Paler", "title": "On the Influence of Initial Qubit Placement During NISQ Circuit\n  Compilation", "comments": "accepted at QTOP 2019, to appear in Volume 11413 of the Lecture Notes\n  in Computer Science series", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.ET cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy Intermediate-Scale Quantum (NISQ) machines are not fault-tolerant,\noperate few qubits (currently, less than hundred), but are capable of executing\ninteresting computations. Above the quantum supremacy threshold (approx. 60\nqubits), NISQ machines are expected to be more powerful than existing classical\ncomputers. One of the most stringent problems is that computations (expressed\nas quantum circuits) have to be adapted (compiled) to the NISQ hardware,\nbecause the hardware does not support arbitrary interactions between the\nqubits. This procedure introduces additional gates (e.g. SWAP gates) into the\ncircuits while leaving the implemented computations unchanged. Each additional\ngate increases the failure rate of the adapted (compiled) circuits, because the\nhardware and the circuits are not fault-tolerant. It is reasonable to expect\nthat the placement influences the number of additionally introduced gates.\nTherefore, a combinatorial problem arises: how are circuit qubits allocated\n(placed) initially to the hardware qubits? The novelty of this work relies on\nthe methodology used to investigate the influence of the initial placement. To\nthis end, we introduce a novel heuristic and cost model to estimate the number\nof gates necessary to adapt a circuit to a given NISQ architecture. We\nimplement the heuristic (source code available on github) and benchmark it\nusing a standard compiler (e.g. from IBM Qiskit) treated as a black box.\nPreliminary results indicate that cost reductions of up to 10\\% can be achieved\nfor practical circuit instances on realistic NISQ architectures only by placing\nqubits differently than default (trivial placement).\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 01:31:56 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 12:59:48 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Paler", "Alexandru", ""]]}, {"id": "1811.09736", "submitter": "Abdul Dakkak", "authors": "Abdul Dakkak, Cheng Li, Isaac Gelado, Jinjun Xiong, Wen-mei Hwu", "title": "Accelerating Reduction and Scan Using Tensor Core Units", "comments": "In Proceedings of the ACM International Conference on Supercomputing\n  (ICS '19)", "journal-ref": null, "doi": "10.1145/3330345.3331057", "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driven by deep learning, there has been a surge of specialized processors for\nmatrix multiplication, referred to as TensorCore Units (TCUs). These TCUs are\ncapable of performing matrix multiplications on small matrices (usually 4x4 or\n16x16) to accelerate the convolutional and recurrent neural networks in deep\nlearning workloads. In this paper we leverage NVIDIA's TCU to express both\nreduction and scan with matrix multiplication and show the benefits -- in terms\nof program simplicity, efficiency, and performance. Our algorithm exercises the\nNVIDIA TCUs which would otherwise be idle, achieves 89%-98% of peak memory copy\nbandwidth, and is orders of magnitude faster (up to 100x for reduction and 3x\nfor scan) than state-of-the-art methods for small segment sizes -- common in\nmachine learning and scientific applications. Our algorithm achieves this while\ndecreasing the power consumption by up to 22% for reduction and16%for scan.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 01:17:51 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 22:34:29 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Dakkak", "Abdul", ""], ["Li", "Cheng", ""], ["Gelado", "Isaac", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1811.09833", "submitter": "Amandeep Bhatia", "authors": "Amandeep Singh Bhatia and Mandeep Kaur Saggi", "title": "Implementing Entangled States on a Quantum Computer", "comments": "13", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.ET cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of tensor network theory is an important field and promises a wide\nrange of experimental and quantum information theoretical applications. Matrix\nproduct state is the most well-known example of tensor network states, which\nprovides an effective and efficient representation of one-dimensional quantum\nsystems. Indeed, it lies at the heart of density matrix renormalization group\n(DMRG), a most common method for simulation of one-dimensional strongly\ncorrelated quantum systems. It has got attention from several areas varying\nfrom solid-state systems to quantum computing and quantum simulators. We have\nconsidered maximally entangled matrix product states (GHZ and W). Here, we\ndesigned the quantum circuits for implementing the matrix product states. In\nthis paper, we simulated the matrix product states in customized IBM (2-qubit,\n3-qubit and 4-qubit) quantum systems and determined the probability\ndistribution among the quantum states.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 13:37:11 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 06:53:53 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bhatia", "Amandeep Singh", ""], ["Saggi", "Mandeep Kaur", ""]]}, {"id": "1811.10220", "submitter": "Vladimir Mironov", "authors": "Vladimir Mironov, Andrey Kudryavtsev, Yuri Alexeev, Alexander\n  Moskovsky, Igor Kulikov, Igor Chernykh", "title": "Evaluation of Intel Memory Drive Technology Performance for Scientific\n  Applications", "comments": null, "journal-ref": "In Proceedings of the Workshop on Memory Centric High Performance\n  Computing (MCHPC'18). ACM, New York, NY, USA, 14-21, 2018", "doi": "10.1145/3286475.3286479", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present benchmark data for Intel Memory Drive Technology\n(IMDT), which is a new generation of Software-defined Memory (SDM) based on\nIntel ScaleMP collaboration and using 3D XPointTM based Intel Solid-State\nDrives (SSDs) called Optane. We studied IMDT performance for synthetic\nbenchmarks, scientific kernels, and applications. We chose these benchmarks to\nrepresent different patterns for computation and accessing data on disks and\nmemory. To put performance of IMDT in comparison, we used two memory\nconfigurations: hybrid IMDT DDR4/Optane and DDR4 only systems. The performance\nwas measured as a percentage of used memory and analyzed in detail. We found\nthat for some applications DDR4/Optane hybrid configuration outperforms DDR4\nsetup by up to 20%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 07:49:13 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mironov", "Vladimir", ""], ["Kudryavtsev", "Andrey", ""], ["Alexeev", "Yuri", ""], ["Moskovsky", "Alexander", ""], ["Kulikov", "Igor", ""], ["Chernykh", "Igor", ""]]}, {"id": "1811.10498", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita and Roshan G. Ragel and Dhammike Elkaduwe", "title": "An optimized Parallel Failure-less Aho-Corasick algorithm for DNA\n  sequence matching", "comments": "6 pages, 3 figures, 4 tables, 5 graphs, 2016 IEEE International\n  Conference on Information and Automation for Sustainability (ICIAfS)", "journal-ref": null, "doi": "10.1109/ICIAFS.2016.7946533", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Aho-Corasick algorithm is multiple patterns searching algorithm running\nsequentially in various applications like network intrusion detection and\nbioinformatics for finding several input strings within a given large input\nstring. The parallel version of the Aho-Corasick algorithm is called as\nParallel Failure-less Aho-Corasick algorithm because it doesn't need failure\nlinks like in the original Aho-Corasick algorithm. In this research, we\nimplemented an application specific parallel failureless Aho-Corasick algorithm\nto the general purpose graphics processing unit by applying several cache\noptimization techniques for matching DNA sequences. Our parallel Aho-Corasick\nalgorithm shows better performance than the available parallel Aho-Corasick\nalgorithm library due to its simplicity and optimized cache memory usage of\ngraphics processing units for matching DNA sequences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:45:30 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Thambawita", "Vajira", ""], ["Ragel", "Roshan G.", ""], ["Elkaduwe", "Dhammike", ""]]}, {"id": "1811.11475", "submitter": "David Rohr", "authors": "David Rohr, Gvozden Neskovic, Volker Lindenstruth", "title": "The L-CSC cluster: Optimizing power efficiency to become the greenest\n  supercomputer in the world in the Green500 list of November 2014", "comments": "7 pages, 2 figures", "journal-ref": "Supercomputing frontiers and innovations, vol. 2 , no. 3, 2015", "doi": "10.14529/jsfi150304", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The L-CSC (Lattice Computer for Scientific Computing) is a general purpose\ncompute cluster built with commodity hardware installed at GSI. Its main\noperational purpose is Lattice QCD (LQCD) calculations for physics simulations.\nQuantum Chromo Dynamics (QCD) is the physical theory describing the strong\nforce, one of the four known fundamental interactions in the universe. L-CSC\nleverages a multi-GPU design accommodating the huge demand of LQCD for memory\nbandwidth. In recent years, heterogeneous clusters with accelerators such as\nGPUs have become more and more powerful while supercomputers in general have\nshown enormous increases in power consumption making electricity costs and\ncooling a significant factor in the total cost of ownership. Using mainly GPUs\nfor processing, L-CSC is very power-efficient, and its architecture was\noptimized to provide the greatest possible power efficiency. This paper\npresents the cluster design as well as optimizations to improve the power\nefficiency. It examines the power measurements performed for the Green500 list\nof the most power-efficient supercomputers in the world which led to the number\n1 position as the greenest supercomputer in November 2014.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 10:07:34 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Rohr", "David", ""], ["Neskovic", "Gvozden", ""], ["Lindenstruth", "Volker", ""]]}, {"id": "1811.12341", "submitter": "Neil J. Gunther", "authors": "Neil J. Gunther and Mohit Chawla", "title": "Linux-Tomcat Application Performance on Amazon AWS", "comments": "10 pages, 25 figures. To appear in Linux Magazin (in German),\n  February 2, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for Linux system administrators to do performance management has\nreturned with a vengeance. Why? The cloud. Resource consumption in the cloud is\nall about pay-as-you-go. This article shows you how performance models can find\nthe most cost-effective deployment of an application on Amazon's cloud.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:41:05 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Gunther", "Neil J.", ""], ["Chawla", "Mohit", ""]]}]