[{"id": "2107.00064", "submitter": "Yu Chen", "authors": "Jialiang Tan, Yu Chen, Zhenming Liu, Bin Ren, Shuaiwen Leon Song,\n  Xipeng Shen and Xu Liu", "title": "Toward Efficient Interactions between Python and Native Libraries", "comments": "In Proceedings of the 29th ACM Joint European Software Engineering\n  Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE\n  2021), August 23-27, 2021, Athens, Greece. ACM, New York,NY, USA, 12 pages", "journal-ref": null, "doi": "10.1145/3468264.3468541", "report-no": null, "categories": "cs.PL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Python has become a popular programming language because of its excellent\nprogrammability. Many modern software packages utilize Python for high-level\nalgorithm design and depend on native libraries written in C/C++/Fortran for\nefficient computation kernels. Interaction between Python code and native\nlibraries introduces performance losses because of the abstraction lying on the\nboundary of Python and native libraries. On the one side, Python code,\ntypically run with interpretation, is disjoint from its execution behavior. On\nthe other side, native libraries do not include program semantics to understand\nalgorithm defects.\n  To understand the interaction inefficiencies, we extensively study a large\ncollection of Python software packages and categorize them according to the\nroot causes of inefficiencies. We extract two inefficiency patterns that are\ncommon in interaction inefficiencies. Based on these patterns, we develop\nPieProf, a lightweight profiler, to pinpoint interaction inefficiencies in\nPython applications. The principle of PieProf is to measure the inefficiencies\nin the native execution and associate inefficiencies with high-level Python\ncode to provide a holistic view. Guided by PieProf, we optimize 17 real-world\napplications, yielding speedups up to 6.3$\\times$ on application level.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 00:48:02 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tan", "Jialiang", ""], ["Chen", "Yu", ""], ["Liu", "Zhenming", ""], ["Ren", "Bin", ""], ["Song", "Shuaiwen Leon", ""], ["Shen", "Xipeng", ""], ["Liu", "Xu", ""]]}, {"id": "2107.00183", "submitter": "Quan-Lin Li", "authors": "Fan-Qi Ma, Quan-Lin Li, Yi-Han Liu, Yan-Xia Chang", "title": "Stochastic Performance Modeling for Practical Byzantine Fault Tolerance\n  Consensus in Blockchain", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.PF math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The practical Byzantine fault tolerant (PBFT) consensus mechanism is one of\nthe most basic consensus algorithms (or protocols) in blockchain technologies,\nthus its performance evaluation is an interesting and challenging topic due to\na higher complexity of its consensus work in the peer-to-peer network. This\npaper describes a simple stochastic performance model of the PBFT consensus\nmechanism, which is refined as not only a queueing system with complicated\nservice times but also a level-independent quasi-birth-and-death (QBD) process.\nFrom the level-independent QBD process, we apply the matrix-geometric solution\nto obtain a necessary and sufficient condition under which the PBFT consensus\nsystem is stable, and to be able to numerically compute the stationary\nprobability vector of the QBD process. Thus we provide four useful performance\nmeasures of the PBFT consensus mechanism, and can numerically calculate the\nfour performance measures. Finally, we use some numerical examples to verify\nthe validity of our theoretical results, and show how the four performance\nmeasures are influenced by some key parameters of the PBFT consensus. By means\nof the theory of multi-dimensional Markov processes, we are optimistic that the\nmethodology and results given in this paper are applicable in a wide range\nresearch of PBFT consensus mechanism and even other types of consensus\nmechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:24:45 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ma", "Fan-Qi", ""], ["Li", "Quan-Lin", ""], ["Liu", "Yi-Han", ""], ["Chang", "Yan-Xia", ""]]}, {"id": "2107.00555", "submitter": "Alexandros Nikolaos Ziogas", "authors": "Alexandros Nikolaos Ziogas, Timo Schneider, Tal Ben-Nun, Alexandru\n  Calotoiu, Tiziano De Matteis, Johannes de Fine Licht, Luca Lavarini, and\n  Torsten Hoefler", "title": "Productivity, Portability, Performance: Data-Centric Python", "comments": null, "journal-ref": null, "doi": "10.1145/1122445.1122456", "report-no": null, "categories": "cs.PL cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Python has become the de facto language for scientific computing. Programming\nin Python is highly productive, mainly due to its rich science-oriented\nsoftware ecosystem built around the NumPy module. As a result, the demand for\nPython support in High Performance Computing (HPC) has skyrocketed. However,\nthe Python language itself does not necessarily offer high performance. In this\nwork, we present a workflow that retains Python's high productivity while\nachieving portable performance across different architectures. The workflow's\nkey features are HPC-oriented language extensions and a set of automatic\noptimizations powered by a data-centric intermediate representation. We show\nperformance results and scaling across CPU, GPU, FPGA, and the Piz Daint\nsupercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over\nprevious-best solutions, first-ever Xilinx and Intel FPGA results of annotated\nPython, and up to 93.16% scaling efficiency on 512 nodes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:51:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ziogas", "Alexandros Nikolaos", ""], ["Schneider", "Timo", ""], ["Ben-Nun", "Tal", ""], ["Calotoiu", "Alexandru", ""], ["De Matteis", "Tiziano", ""], ["Licht", "Johannes de Fine", ""], ["Lavarini", "Luca", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2107.00957", "submitter": "Tareq Si Salem", "authors": "Tareq Si Salem, Giovanni Neglia, Damiano Carra", "title": "A\\c{C}AI: Ascent Similarity Caching with Approximate Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Similarity search is a key operation in multimedia retrieval systems and\nrecommender systems, and it will play an important role also for future machine\nlearning and augmented reality applications. When these systems need to serve\nlarge objects with tight delay constraints, edge servers close to the end-user\ncan operate as similarity caches to speed up the retrieval. In this paper we\npresent A\\c{C}AI, a new similarity caching policy which improves on the state\nof the art by using (i) an (approximate) index for the whole catalog to decide\nwhich objects to serve locally and which to retrieve from the remote server,\nand (ii) a mirror ascent algorithm to update the set of local objects with\nstrong guarantees even when the request process does not exhibit any\nstatistical regularity.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:40:47 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Salem", "Tareq Si", ""], ["Neglia", "Giovanni", ""], ["Carra", "Damiano", ""]]}, {"id": "2107.00998", "submitter": "Youry Khmelevsky", "authors": "Albert Wong, Chun Yin Chiu, Ga\\'etan Hains, Jack Humphrey, Hans\n  Fuhrmann, Youry Khmelevsky, Chris Mazur", "title": "Gamers Private Network Performance Forecasting. From Raw Data to the\n  Data Warehouse with Machine Learning and Neural Nets", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gamers Private Network (GPN) is a client/server technology that guarantees a\nconnection for online video games that is more reliable and lower latency than\na standard internet connection. Users of the GPN technology benefit from a\nstable and high-quality gaming experience for online games, which are hosted\nand played across the world. After transforming a massive volume of raw\nnetworking data collected by WTFast, we have structured the cleaned data into a\nspecial-purpose data warehouse and completed the extensive analysis using\nmachine learning and neural nets technologies, and business intelligence tools.\nThese analyses demonstrate the ability to predict and quantify changes in the\nnetwork and demonstrate the benefits gained from the use of a GPN for users\nwhen connected to an online game session.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 00:45:01 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Wong", "Albert", ""], ["Chiu", "Chun Yin", ""], ["Hains", "Ga\u00e9tan", ""], ["Humphrey", "Jack", ""], ["Fuhrmann", "Hans", ""], ["Khmelevsky", "Youry", ""], ["Mazur", "Chris", ""]]}, {"id": "2107.01143", "submitter": "Dominik Ernst", "authors": "Dominik Ernst (1), Georg Hager (1), Markus Holzer (2), Matthias Knorr,\n  Gerhard Wellein (1) ((1) Friedrich-Alexander-Universit\\\"at\n  Erlangen-N\\\"urnberg, (2) Chair for System Simulation,\n  Friedrich-Alexander-Universtit\\\"at Erlangen-N\\\"urnberg)", "title": "Opening the Black Box: Performance Estimation during Code Generation for\n  GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic code generation is frequently used to create implementations of\nalgorithms specifically tuned to particular hardware and application\nparameters. The code generation process involves the selection of adequate code\ntransformations, tuning parameters, and parallelization strategies. To cover\nthe huge search space, code generation frameworks may apply time-intensive\nautotuning, exploit scenario-specific performance models, or treat performance\nas an intangible black box that must be described via machine learning.\n  This paper addresses the selection problem by identifying the relevant\nperformance-defining mechanisms through a performance model coupled with an\nanalytic hardware metric estimator. This enables a quick exploration of large\nconfiguration spaces to identify highly efficient candidates with high\naccuracy.\n  Our current approach targets memory-intensive GPGPU applications and focuses\non the correct modeling of data transfer volumes to all levels of the memory\nhierarchy. We show how our method can be coupled to the pystencils stencil code\ngenerator, which is used to generate kernels for a range four 3D25pt stencil\nand a complex two phase fluid solver based on the Lattice Boltzmann Method. For\nboth, it delivers a ranking that can be used to select the best performing\ncandidate.\n  The method is not limited to stencil kernels, but can be integrated into any\ncode generator that can generate the required address expressions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:38:47 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Ernst", "Dominik", ""], ["Hager", "Georg", ""], ["Holzer", "Markus", ""], ["Knorr", "Matthias", ""], ["Wellein", "Gerhard", ""]]}, {"id": "2107.01914", "submitter": "Anastasios Giovanidis", "authors": "Anastasios Giovanidis, Bruno Baynat, Cl\\'emence Magnien, Antoine\n  Vendeville", "title": "Ranking Online Social Users by their Influence", "comments": "18 pages, 7 figures, journal publications. arXiv admin note: text\n  overlap with arXiv:1902.07187", "journal-ref": "IEEE/ACM Transactions on Networking 2021", "doi": "10.1109/TNET.2021.3085201", "report-no": null, "categories": "cs.SI cs.NI cs.PF math.PR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce an original mathematical model to analyse the diffusion of posts\nwithin a generic online social platform. The main novelty is that each user is\nnot simply considered as a node on the social graph, but is further equipped\nwith his/her own Wall and Newsfeed, and has his/her own individual self-posting\nand re-posting activity. As a main result using our developed model, we derive\nin closed form the probabilities that posts originating from a given user are\nfound on the Wall and Newsfeed of any other. These are the solution of a linear\nsystem of equations, which can be resolved iteratively. In fact, our model is\nvery flexible with respect to the modelling assumptions. Using the\nprobabilities derived from the solution, we define a new measure of per-user\ninfluence over the entire network, the $\\Psi$-score, which combines the user\nposition on the graph with user (re-)posting activity. In the homogeneous case\nwhere all users have the same activity rates, it is shown that a variant of the\n$\\Psi$-score is equal to PageRank. Furthermore, we compare the new model and\nits $\\Psi$-score against the empirical influence measured from very large data\ntraces (Twitter, Weibo). The results illustrate that these new tools can\naccurately rank influencers with asymmetric (re-)posting activity for such real\nworld applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 10:14:55 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Giovanidis", "Anastasios", ""], ["Baynat", "Bruno", ""], ["Magnien", "Cl\u00e9mence", ""], ["Vendeville", "Antoine", ""]]}, {"id": "2107.03357", "submitter": "Ben Burnett", "authors": "Ben Burnett, Sigal Gottlieb, Zachary J. Grant, Alfa Heryudono", "title": "Performance Evaluation of Mixed-Precision Runge-Kutta Methods", "comments": "IEEE HPEC 2021 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Additive Runge-Kutta methods designed for preserving highly accurate\nsolutions in mixed-precision computation were proposed and analyzed in [8].\nThese specially designed methods use reduced precision or the implicit\ncomputations and full precision for the explicit computations. We develop a\nFORTRAN code to solve a nonlinear system of ordinary differential equations\nusing the mixed precision additive Runge-Kutta (MP-ARK) methods on IBM POWER9\nand Intel x86\\_64 chips. The convergence, accuracy, runtime, and energy\nconsumption of these methods is explored. We show that these MP-ARK methods\nefficiently produce accurate solutions with significant reductions in runtime\n(and by extension energy consumption).\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 17:03:51 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Burnett", "Ben", ""], ["Gottlieb", "Sigal", ""], ["Grant", "Zachary J.", ""], ["Heryudono", "Alfa", ""]]}, {"id": "2107.03467", "submitter": "In Kee Kim", "authors": "Jianwei Hao, Ting Jiang, Wei Wang, In Kee Kim", "title": "An Empirical Analysis of VM Startup Times in Public IaaS Clouds: An\n  Extended Report", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VM startup time is an essential factor in designing elastic cloud\napplications. For example, a cloud application with autoscaling can reduce\nunder- and over-provisioning of VM instances with a precise estimation of VM\nstartup time, and in turn, it is likely to guarantee the application's\nperformance and improve the cost efficiency. However, VM startup time has been\nlittle studied, and available measurement results performed previously did not\nconsider various configurations of VMs for modern cloud applications. In this\nwork, we perform comprehensive measurements and analysis of VM startup time\nfrom two major cloud providers, namely Amazon Web Services (AWS) and Google\nCloud Platform (GCP). With three months of measurements, we collected more than\n300,000 data points from each provider by applying a set of configurations,\nincluding 11+ VM types, four different data center locations, four VM image\nsizes, two OS types, and two purchase models (e.g., spot/preemptible VMs vs.\non-demand VMs). With extensive analysis, we found that VM startup time can vary\nsignificantly because of several important factors, such as VM image sizes,\ndata center locations, VM types, and OS types. Moreover, by comparing with\nprevious measurement results, we confirm that cloud providers (specifically\nAWS) made significant improvements for the VM startup times and currently have\nmuch quicker VM startup times than in the past.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 20:20:49 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Hao", "Jianwei", ""], ["Jiang", "Ting", ""], ["Wang", "Wei", ""], ["Kim", "In Kee", ""]]}, {"id": "2107.03484", "submitter": "Xin Fan", "authors": "Xin Fan and Yan Huo", "title": "An Overview of Low latency for Wireless Communications: an Evolutionary\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra-low latency supported by the fifth generation (5G) give impetus to the\nprosperity of many wireless network applications, such as autonomous driving,\nrobotics, telepresence, virtual reality and so on. Ultra-low latency is not\nachieved in a moment, but requires long-term evolution of network structure and\nkey enabling communication technologies. In this paper, we provide an\nevolutionary overview of low latency in mobile communication systems, including\ntwo different evolutionary perspectives: 1) network architecture; 2) physical\nlayer air interface technologies. We firstly describe in detail the evolution\nof communication network architecture from the second generation (2G) to 5G,\nhighlighting the key points reducing latency. Moreover, we review the evolution\nof key enabling technologies in the physical layer from 2G to 5G, which is also\naimed at reducing latency. We also discussed the challenges and future research\ndirections for low latency in network architecture and physical layer.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 21:15:47 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Fan", "Xin", ""], ["Huo", "Yan", ""]]}, {"id": "2107.04092", "submitter": "Dennis Bautembach", "authors": "Dennis Bautembach, Iason Oikonomidis, Antonis Argyros", "title": "Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared\n  Atomics", "comments": "Submitted to IEEE-HPEC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two novel optimizations that accelerate clock-based spiking neural\nnetwork (SNN) simulators. The first one targets spike timing dependent\nplasticity (STDP). It combines lazy- with event-driven plasticity and\nefficiently facilitates the computation of pre- and post-synaptic spikes using\nbitfields and integer intrinsics. It offers higher bandwidth than event-driven\nplasticity alone and achieves a 1.5x-2x speedup over our closest competitor.\nThe second optimization targets spike delivery. We partition our graph\nrepresentation in a way that bounds the number of neurons that need be updated\nat any given time which allows us to perform said update in shared memory\ninstead of global memory. This is 2x-2.5x faster than our closest competitor.\nBoth optimizations represent the final evolutionary stages of years of\niteration on STDP and spike delivery inside \"Spice\" (/spaIk/), our state of the\nart SNN simulator. The proposed optimizations are not exclusive to our graph\nrepresentation or pipeline but are applicable to a multitude of simulator\ndesigns. We evaluate our performance on three well-established models and\ncompare ourselves against three other state of the art simulators.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 20:13:54 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Bautembach", "Dennis", ""], ["Oikonomidis", "Iason", ""], ["Argyros", "Antonis", ""]]}, {"id": "2107.04904", "submitter": "Nikolay Ivanov", "authors": "Nikolay Ivanov, Qiben Yan and Qingyang Wang", "title": "Blockumulus: A Scalable Framework for Smart Contracts on the Cloud", "comments": "41st IEEE International Conference on Distributed Computing Systems\n  (ICDCS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public blockchains have spurred the growing popularity of decentralized\ntransactions and smart contracts, but they exhibit limitations on the\ntransaction throughput, storage, and computation. To avoid transaction\ngridlock, public blockchains impose large fees and per-block resource limits,\nmaking it difficult to accommodate the ever-growing transaction demand.\nPrevious research endeavors to improve the scalability of blockchain through\nvarious technologies, such as side-chaining, sharding, secured off-chain\ncomputation, communication network optimizations, and efficient consensus\nprotocols. However, these approaches have not attained a widespread adoption\ndue to their inability in delivering a cloud-like performance, in terms of the\nscalability in transaction throughput, storage, and compute capacity. In this\nwork, we determine that the major obstacle to public blockchain scalability is\ntheir underlying unstructured P2P networks. We further show that a centralized\nnetwork can support the deployment of decentralized smart contracts. We propose\na novel approach for achieving scalable decentralization: instead of trying to\nmake blockchain scalable, we deliver decentralization to already scalable cloud\nby using an Ethereum smart contract. We introduce Blockumulus, a framework that\ncan deploy decentralized cloud smart contract environments using a novel\ntechnique called overlay consensus. Through experiments, we demonstrate that\nBlockumulus is scalable in all three dimensions: computation, data storage, and\ntransaction throughput. Besides eliminating the current code execution and\nstorage restrictions, Blockumulus delivers a transaction latency between 2 and\n5 seconds under normal load. Moreover, the stress test of our prototype reveals\nthe ability to execute 20,000 simultaneous transactions under 26 seconds, which\nis on par with the average throughput of worldwide credit card transactions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 20:18:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ivanov", "Nikolay", ""], ["Yan", "Qiben", ""], ["Wang", "Qingyang", ""]]}, {"id": "2107.05021", "submitter": "Yuming Jiang", "authors": "Yuming Jiang", "title": "Some Properties of Length Rate Quotient Shapers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Length Rate Quotient (LRQ) is the first algorithm of interleaved shaping -- a\nnovel concept proposed to provide per-flow shaping for a flow aggregate without\nper-flow queuing. This concept has been adopted by Time-Sensitive Networking\n(TSN) and Deterministic Networking (DetNet). An appealing property of\ninterleaved shaping is that, when an interleaved shaper is appended to a FIFO\nsystem, it does not increase the worst-case delay of the system. Based on this\n\"shaping-for-free\" property, an approach has been introduced to deliver bounded\nend-to-end latency. Specifically, at each output link of a node, class-based\naggregate scheduling is used together with one interleaved shaper per-input\nlink and per-class, and the interleaved shaper re-shapes every flow to its\ninitial traffic constraint. In this paper, we investigate other properties of\ninterleaved LRQ shapers, particularly as stand-alone elements. In addition,\nunder per-flow setting, we also investigate per-flow LRQ based flow aggregation\nand derive its properties. The analysis focuses directly on the timing of\noperations, such as shaping and scheduling, in the network. This timing based\nmethod can be found in the Guaranteed Rate (GR) server model and more generally\nthe max-plus branch of network calculus. With the derived properties, we not\nonly show that an improved end-to-end latency bound can be obtained for the\ncurrent approach, but also demonstrate with two examples that new approaches\nmay be devised. End-to-end delay bounds for the three approaches are derived\nand compared. As a highlight, the two new approaches do not require different\nnode architectures in allocating (shaping / scheduling) queues, which implies\nthat they can be readily adapted for use in TSN and DetNet. This together with\nthe derived properties of LRQ shed new insights on providing the TSN / DetNet\nqualities of service.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 10:58:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jiang", "Yuming", ""]]}, {"id": "2107.05397", "submitter": "Igor Sfiligoi", "authors": "Igor Sfiligoi, Daniel McDonald and Rob Knight", "title": "Enabling microbiome research on personal devices", "comments": "2 pages, 4 figures, to be published in proceedings of eScience 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.PF q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Microbiome studies have recently transitioned from experimental designs with\na few hundred samples to designs spanning tens of thousands of samples. Modern\nstudies such as the Earth Microbiome Project (EMP) afford the statistics\ncrucial for untangling the many factors that influence microbial community\ncomposition. Analyzing those data used to require access to a compute cluster,\nmaking it both expensive and inconvenient. We show that recent improvements in\nboth hardware and software now allow to compute key bioinformatics tasks on\nEMP-sized data in minutes using a gaming-class laptop, enabling much faster and\nbroader microbiome science insights.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 21:48:04 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sfiligoi", "Igor", ""], ["McDonald", "Daniel", ""], ["Knight", "Rob", ""]]}, {"id": "2107.05473", "submitter": "Hung-Wei Tseng", "authors": "Kuan-Chieh Hsu and Hung-Wei Tseng", "title": "GPTPU: Accelerating Applications using Edge Tensor Processing Units", "comments": "This paper is a pre-print of a paper in the 2021 SC, the\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network (NN) accelerators have been integrated into a wide-spectrum of\ncomputer systems to accommodate the rapidly growing demands for artificial\nintelligence (AI) and machine learning (ML) applications. NN accelerators share\nthe idea of providing native hardware support for operations on\nmultidimensional tensor data. Therefore, NN accelerators are theoretically\ntensor processors that can improve system performance for any problem that uses\ntensors as inputs/outputs. Unfortunately, commercially available NN\naccelerators only expose computation capabilities through AI/ML-specific\ninterfaces. Furthermore, NN accelerators reveal very few hardware design\ndetails, so applications cannot easily leverage the tensor operations NN\naccelerators provide.\n  This paper introduces General-Purpose Computing on Edge Tensor Processing\nUnits (GPTPU), an open-source, open-architecture framework that allows the\ndeveloper and research communities to discover opportunities that NN\naccelerators enable for applications. GPTPU includes a powerful programming\ninterface with efficient runtime system-level support -- similar to that of\nCUDA/OpenCL in GPGPU computing -- to bridge the gap between application demands\nand mismatched hardware/software interfaces.\n  We built GPTPU machine uses Edge Tensor Processing Units (Edge TPUs), which\nare widely available and representative of many commercial NN accelerators. We\nidentified several novel use cases and revisited the algorithms. By leveraging\nthe underlying Edge TPUs to perform tensor-algorithm-based compute kernels, our\nresults reveal that GPTPU can achieve a 2.46x speedup over high-end CPUs and\nreduce energy consumption by 40%.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:03:22 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 06:04:21 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Hsu", "Kuan-Chieh", ""], ["Tseng", "Hung-Wei", ""]]}, {"id": "2107.05681", "submitter": "Charitha Saumya Gusthinna Waduge", "authors": "Charitha Saumya, Kirshanthan Sundararajah and Milind Kulkarni", "title": "CFM: SIMT Thread Divergence Reduction by Melding Similar Control-Flow\n  Regions in GPGPU Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPGPUs use the Single-Instruction-Multiple-Thread (SIMT) execution model\nwhere a group of threads--wavefront or war--execute instructions in lockstep.\nWhen threads in a group encounter a branching instruction, not all threads in\nthe group take the same path, a phenomenon known as control-flow divergence.\nThe control-flow divergence causes performance degradation because both paths\nof the branch must be executed one after the other. Prior research has\nprimarily addressed this issue through architectural modifications. We observe\nthat certain GPGPU kernels with control-flow divergence have similar\ncontrol-flow structures with similar instructions on both sides of a branch.\nThis structure can be exploited to reduce control-flow divergence by melding\nthe two sides of the branch allowing threads to reconverge early, reducing\ndivergence. In this work, we present CFM, a compiler analysis and\ntransformation framework that can meld divergent control-flow structures with\nsimilar instruction sequences. We show that CFM can reduce the performance\ndegradation from control-flow divergence.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:34:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Saumya", "Charitha", ""], ["Sundararajah", "Kirshanthan", ""], ["Kulkarni", "Milind", ""]]}, {"id": "2107.06127", "submitter": "Daniele Di Pompeo", "authors": "Vittorio Cortellessa, Daniele Di Pompeo, Vincenzo Stoico, Michele\n  Tucci", "title": "On the impact of Performance Antipatterns in multi-objective software\n  model refactoring optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software quality estimation is a challenging and time-consuming activity, and\nmodels are crucial to face the complexity of such activity on modern software\napplications.\n  One main challenge is that the improvement of distinctive quality attributes\nmay require contrasting refactoring actions on an application, as for trade-off\nbetween performance and reliability. In such cases, multi-objective\noptimization can provide the designer with a wider view on these trade-offs\nand, consequently, can lead to identify suitable actions that take into account\nindependent or even competing objectives.\n  In this paper, we present an approach that exploits the NSGA-II\nmulti-objective evolutionary algorithm to search optimal Pareto solution\nfrontiers for software refactoring while considering as objectives: i)\nperformance variation, ii) reliability, iii) amount of performance\nantipatterns, and iv) architectural distance. The algorithm combines randomly\ngenerated refactoring actions into solutions (i.e., sequences of actions) and\ncompares them according to the objectives.\n  We have applied our approach on a train ticket booking service case study,\nand we have focused the analysis on the impact of performance antipatterns on\nthe quality of solutions. Indeed, we observe that the approach finds better\nsolutions when antipatterns enter the multi-objective optimization. In\nparticular, performance antipatterns objective leads to solutions improving the\nperformance by up to 15% with respect to the case where antipatterns are not\nconsidered, without affecting the solution quality on other objectives.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:28:06 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Cortellessa", "Vittorio", ""], ["Di Pompeo", "Daniele", ""], ["Stoico", "Vincenzo", ""], ["Tucci", "Michele", ""]]}, {"id": "2107.06433", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi and Fabrizio Petrini", "title": "A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its\n  Performance on PIUMA and Xeon CPU", "comments": "11 Pages. arXiv admin note: substantial text overlap with\n  arXiv:2005.06727", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Word Movers Distance (WMD) measures the semantic dissimilarity between\ntwo text documents by computing the cost of optimally moving all words of a\nsource/query document to the most similar words of a target document. Computing\nWMD between two documents is costly because it requires solving an optimization\nproblem that costs $O (V^3 \\log(V)) $ where $V$ is the number of unique words\nin the document. Fortunately, WMD can be framed as an Earth Mover's Distance\n(EMD) for which the algorithmic complexity can be reduced to $O(V^2)$ by adding\nan entropy penalty to the optimization problem and solving it using the\nSinkhorn-Knopp algorithm. Additionally, the computation can be made highly\nparallel by computing the WMD of a single query document against multiple\ntarget documents at once, for example by finding whether a given tweet is\nsimilar to any other tweets of a given day.\n  In this paper, we first present a shared-memory parallel Sinkhorn-Knopp\nalgorithm to compute the WMD of one document against many other documents by\nadopting the $ O(V^2)$ EMD algorithm. We then algorithmically transform the\noriginal $O(V^2)$ dense compute-heavy version into an equivalent sparse one\nwhich is mapped onto the new Intel Programmable Integrated Unified Memory\nArchitecture (PIUMA) system. The WMD parallel implementation achieves 67x\nspeedup on 96 cores across 4 NUMA sockets of an Intel Cascade Lake system. We\nalso show that PIUMA cores are around 1.2-2.6x faster than Xeon cores on\nSinkhorn-WMD and also provide better strong scaling.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 00:29:18 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Petrini", "Fabrizio", ""]]}, {"id": "2107.07157", "submitter": "Jens Domke", "authors": "Jens Domke", "title": "A64FX -- Your Compiler You Must Decide!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current number one of the TOP500 list, Supercomputer Fugaku, has\ndemonstrated that CPU-only HPC systems aren't dead and CPUs can be used for\nmore than just being the host controller for a discrete accelerators. While the\nspecifications of the chip and overall system architecture, and benchmarks\nsubmitted to various lists, like TOP500 and Green500, etc., are clearly\nhighlighting the potential, the proliferation of Arm into the HPC business is\nrather recent and hence the software stack might not be fully matured and\ntuned, yet. We test three state-of-the-art compiler suite against a broad set\nof benchmarks. Our measurements show that orders of magnitudes in performance\ncan be gained by deviating from the recommended usage model of the A64FX\ncompute nodes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 07:05:36 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Domke", "Jens", ""]]}, {"id": "2107.07784", "submitter": "Igor Ivkic", "authors": "Igor Ivkic, Patrizia Sailer, Antonios Gouglidis, Andreas Mauthe,\n  Markus Tauber", "title": "A Security Cost Modelling Framework for Cyber-Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-Physical Systems (CPS) are formed through interconnected components\ncapable of computation, communication, sensing and changing the physical world.\nThe development of these systems poses a significant challenge since they have\nto be designed in a way to ensure cyber-security without impacting their\nperformance. This article presents the Security Cost Modelling Framework (SCMF)\nand shows supported by an experimental study how it can be used to measure,\nnormalise and aggregate the overall performance of a CPS. Unlike previous\nstudies, our approach uses different metrics to measure the overall performance\nof a CPS and provides a methodology for normalising the measurement results of\ndifferent units to a common Cost Unit. Moreover, we show how the Security Costs\ncan be extracted from the overall performance measurements which allows to\nquantify the overhead imposed by performing security-related tasks.\nFurthermore, we describe the architecture of our experimental testbed and\ndemonstrate the applicability of SCMF in an experimental study. Our results\nshow that measuring the overall performance and extracting the security costs\nusing SCMF can serve as basis to redesign interactions to achieve the same\noverall goal at less costs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:19:47 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ivkic", "Igor", ""], ["Sailer", "Patrizia", ""], ["Gouglidis", "Antonios", ""], ["Mauthe", "Andreas", ""], ["Tauber", "Markus", ""]]}, {"id": "2107.09333", "submitter": "Mahyar Emami", "authors": "Endri Bezati, Mahyar Emami, J\\\"orn Janneck, James Larus", "title": "StreamBlocks: A compiler for heterogeneous dataflow computing (technical\n  report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To increase performance and efficiency, systems use FPGAs as reconfigurable\naccelerators. A key challenge in designing these systems is partitioning\ncomputation between processors and an FPGA. An appropriate division of labor\nmay be difficult to predict in advance and require experiments and\nmeasurements. When an investigation requires rewriting part of the system in a\nnew language or with a new programming model, its high cost can retard the\nstudy of different configurations. A single-language system with an appropriate\nprogramming model and compiler that targets both platforms simplifies this\nexploration to a simple recompile with new compiler directives.\n  This work introduces StreamBlocks, an open-source compiler and runtime that\nuses the CAL dataflow programming language to partition computations across\nheterogeneous (CPU/accelerator) platforms. Because of the dataflow model's\nsemantics and the CAL language, StreamBlocks can exploit both thread\nparallelism in multi-core CPUs and the inherent parallelism of FPGAs.\nStreamBlocks supports exploring the design space with a profile-guided tool\nthat helps identify the best hardware-software partitions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 08:46:47 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bezati", "Endri", ""], ["Emami", "Mahyar", ""], ["Janneck", "J\u00f6rn", ""], ["Larus", "James", ""]]}, {"id": "2107.09351", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu, Yanzhe An, Yuan Zi, Yu Feng, Jianmin Wang", "title": "IoTDataBench: Extending TPCx-IoT for Compression and Scalability", "comments": "16 pages, 7 figures, TPCTC accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a record-breaking result and lessons learned in practicing\nTPCx-IoT benchmarking for a real-world use case. We find that more system\ncharacteristics need to be benchmarked for its application to real-world use\ncases. We introduce an extension to the TPCx-IoT benchmark, covering\nfundamental requirements of time-series data management for IoT infrastructure.\nWe characterize them as data compression and system scalability. To evaluate\nthese two important features of IoT databases, we propose IoTDataBench and\nupdate four aspects of TPCx-IoT, i.e., data generation, workloads, metrics and\ntest procedures. Preliminary evaluation results show systems that fail to\neffectively compress data or flexibly scale can negatively affect the\nredesigned metrics, while systems with high compression ratios and linear\nscalability are rewarded in the final metrics. Such systems have the ability to\nscale up computing resources on demand and can thus save dollar costs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:13:08 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zhu", "Yuqing", ""], ["An", "Yanzhe", ""], ["Zi", "Yuan", ""], ["Feng", "Yu", ""], ["Wang", "Jianmin", ""]]}, {"id": "2107.09735", "submitter": "Itzik Mizrahi", "authors": "Itzik Mizrahi, Shai Avidan", "title": "kNet: A Deep kNN Network To Handle Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks require large amounts of labeled data for their\ntraining. Collecting this data at scale inevitably causes label noise.Hence,the\nneed to develop learning algorithms that are robust to label noise. In recent\nyears, k Nearest Neighbors (kNN) emerged as a viable solution to this problem.\nDespite its success, kNN is not without its problems. Mainly, it requires a\nhuge memory footprint to store all the training samples and it needs an\nadvanced data structure to allow for fast retrieval of the relevant examples,\ngiven a query sample. We propose a neural network, termed kNet, that learns to\nperform kNN. Once trained, we no longer need to store the training data, and\nprocessing a query sample is a simple matter of inference. To use kNet, we\nfirst train a preliminary network on the data set, and then train kNet on the\npenultimate layer of the preliminary network.We find that kNet gives a smooth\napproximation of kNN,and cannot handle the sharp label changes between samples\nthat kNN can exhibit. This indicates that currently kNet is best suited to\napproximate kNN with a fairly large k. Experiments on two data sets show that\nthis is the regime in which kNN works best,and can therefore be replaced by\nkNet.In practice, kNet consistently improve the results of all preliminary\nnetworks, in all label noise regimes, by up to 3%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 19:12:45 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Mizrahi", "Itzik", ""], ["Avidan", "Shai", ""]]}, {"id": "2107.10008", "submitter": "Mohak Chadha", "authors": "Mohak Chadha, Anshul Jindal, Michael Gerndt", "title": "Architecture-Specific Performance Optimization of Compute-Intensive FaaS\n  Functions", "comments": "Extended version IEEE CLOUD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FaaS allows an application to be decomposed into functions that are executed\non a FaaS platform. The FaaS platform is responsible for the resource\nprovisioning of the functions. Recently, there is a growing trend towards the\nexecution of compute-intensive FaaS functions that run for several seconds.\nHowever, due to the billing policies followed by commercial FaaS offerings, the\nexecution of these functions can incur significantly higher costs. Moreover,\ndue to the abstraction of underlying processor architectures on which the\nfunctions are executed, the performance optimization of these functions is\nchallenging. As a result, most FaaS functions use pre-compiled libraries\ngeneric to x86-64 leading to performance degradation. In this paper, we examine\nthe underlying processor architectures for Google Cloud Functions (GCF) and\ndetermine their prevalence across the 19 available GCF regions. We modify,\nadapt, and optimize three compute-intensive FaaS workloads written in Python\nusing Numba, a JIT compiler based on LLVM, and present results wrt performance,\nmemory consumption, and costs on GCF. Results from our experiments show that\nthe optimization of FaaS functions can improve performance by 12.8x (geometric\nmean) and save costs by 73.4% on average for the three functions. Our results\nshow that optimization of the FaaS functions for the specific architecture is\nvery important. We achieved a maximum speedup of 1.79x by tuning the function\nespecially for the instruction set of the underlying processor architecture.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 11:11:19 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Chadha", "Mohak", ""], ["Jindal", "Anshul", ""], ["Gerndt", "Michael", ""]]}, {"id": "2107.10047", "submitter": "Panagiotis Miliadis", "authors": "Panagiotis Miliadis, Christos-Savvas Bouganis, Dionisios Pnevmatikatos", "title": "Performance landscape of resource-constrained platforms targeting DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the recent years, a significant number of complex, deep neural networks\nhave been developed for a variety of applications including speech and face\nrecognition, computer vision in the areas of health-care, automatic\ntranslation, image classification, etc. Moreover, there is an increasing demand\nin deploying these networks in resource-constrained edge devices. As the\ncomputational demands of these models keep increasing, pushing to their limits\nthe targeted devices, the constant development of new hardware systems tailored\nto those workloads has been observed. Since programmability of these diverse\nand complex platforms -- compounded by the rapid development of new DNN models\n-- is a major challenge, platform vendors have developed Machine Learning\ntailored SDKs to maximize the platform's performance.\n  This work investigates the performance achieved on a number of modern\ncommodity embedded platforms coupled with the vendors' provided software\nsupport when state-of-the-art DNN models from image classification, object\ndetection and image segmentation are targeted. The work quantifies the relative\nlatency gains of the particular embedded platforms and provides insights on the\nrelationship between the required minimum batch size for achieving maximum\nthroughput, concluding that modern embedded systems reach their maximum\nperformance even for modest batch sizes when a modern state of the art DNN\nmodel is targeted. Overall, the presented results provide a guide for the\nexpected performance for a number of state-of-the-art DNNs on popular embedded\nplatforms across the image classification, detection and segmentation domains.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 12:33:21 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Miliadis", "Panagiotis", ""], ["Bouganis", "Christos-Savvas", ""], ["Pnevmatikatos", "Dionisios", ""]]}, {"id": "2107.10299", "submitter": "Onel Luis Alcaraz Lopez", "authors": "Onel Luis Alcaraz L\\'opez, Bruno Clerckx, Matti Latva-aho", "title": "Dynamic RF Combining for Multi-Antenna Ambient Energy Harvesting", "comments": "5 pags, 5 figs, submitted to IEEE Wireless Communications Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambient radio frequency (RF) energy harvesting (EH) technology is key to\nrealize self-sustainable, always-on, low-power, massive Internet of Things\nnetworks. Typically, rigid (non-adaptable to channel fluctuations)\nmulti-antenna receive architectures are proposed to support reliable EH\noperation. Herein, we introduce a dynamic RF combining architecture for ambient\nRF EH use cases, and exemplify the attainable performance gains via three\nsimple mechanisms, namely, brute force (BF), sequential testing (ST) and\ncodebook based (CB). Among the proposed mechanisms, BF demands the highest\npower consumption, while CB requires the highest-resolution phase shifters,\nthus tipping the scales in favor of ST. Finally, we show that the performance\ngains of ST over a rigid RF combining scheme increase with the number of\nreceive antennas and energy transmitters' deployment density.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 18:22:37 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["L\u00f3pez", "Onel Luis Alcaraz", ""], ["Clerckx", "Bruno", ""], ["Latva-aho", "Matti", ""]]}, {"id": "2107.12246", "submitter": "Gayane Vardoyan", "authors": "Gayane Vardoyan, Matthew Skrzypczyk, Stephanie Wehner", "title": "On the Quantum Performance Evaluation of Two Distributed Quantum\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed quantum applications impose requirements on the quality of the\nquantum states that they consume. When analyzing architecture implementations\nof quantum hardware, characterizing this quality forms an important factor in\nunderstanding their performance. Fundamental characteristics of quantum\nhardware lead to inherent tradeoffs between the quality of states and\ntraditional performance metrics such as throughput. Furthermore, any real-world\nimplementation of quantum hardware exhibits time-dependent noise that degrades\nthe quality of quantum states over time. Here, we study the performance of two\npossible architectures for interfacing a quantum processor with a quantum\nnetwork. The first corresponds to the current experimental state of the art in\nwhich the same device functions both as a processor and a network device. The\nsecond corresponds to a future architecture that separates these two functions\nover two distinct devices. We model these architectures as Markov chains and\ncompare their quality of executing quantum operations and producing entangled\nquantum states as functions of their memory lifetimes, as well as the time that\nit takes to perform various operations within each architecture. As an\nillustrative example, we apply our analysis to architectures based on\nNitrogen-Vacancy centers in diamond, where we find that for present-day device\nparameters one architecture is more suited to computation-heavy applications,\nand the other for network-heavy ones. Besides the detailed study of these\narchitectures, a novel contribution of our work are several formulas that\nconnect an understanding of waiting time distributions to the decay of quantum\nquality over time for the most common noise models employed in quantum\ntechnologies. This provides a valuable new tool for performance evaluation\nexperts, and its applications extend beyond the two architectures studied in\nthis work.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 14:49:47 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Vardoyan", "Gayane", ""], ["Skrzypczyk", "Matthew", ""], ["Wehner", "Stephanie", ""]]}, {"id": "2107.12550", "submitter": "Tomonori Kouya", "authors": "Tomonori Kouya", "title": "Accelerated Multiple Precision Direct Method and Mixed Precision\n  Iterative Refinement on Python Programming Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Python programming environment does not have any reliable and\nefficient multiple precision floating-point (MPF) arithmetic except ``mpmath\"\nand ``gmpy2\" packages based on GNU MP(GMP) and MPFR libraries. Although it is\nwell known that multi-component-type MPF library can be utilized for middle\nlength precision arithmetic under 200 bits, they are not widely used on Python\nenvironment. In this paper, we describe our accelerated MPF direct method with\nAVX2 techniques and its application to mixed precision iterative refinement\ncombined with mpmath, and demonstrate their efficiency on x86\\_64 computational\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 01:57:03 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Kouya", "Tomonori", ""]]}, {"id": "2107.13166", "submitter": "Sai Li", "authors": "Sai Li, Liang Yang", "title": "Performance Analysis of Dual-Hop THz Transmission Systems over\n  $\\alpha$-$\\mu$ Fading Channels with Pointing Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, the performance of a dual-hop relaying terahertz (THz)\nwireless communication system is investigated. In particular, the behaviors of\nthe two THz hops are determined by three factors, which are the deterministic\npath loss, the fading effects, and pointing errors. Assuming that both THz\nlinks are subject to the $\\alpha$-$\\mu$ fading with pointing errors, we derive\nexact expressions for the cumulative distribution function (CDF) and\nprobability density function (PDF) of the end-to-end signal-to-noise ratio\n(SNR). Relying on the CDF and PDF, important performance metrics are evaluated,\nsuch as the outage probability, average bit error rate, and average channel\ncapacity. Moreover, the asymptotic analyses are presented to obtain more\ninsights. Results show that the dual-hop relaying scheme has better performance\nthan the single THz link. The system's diversity order is\n$\\min\\left\\{\\frac{\\phi_1}{2},\\frac{\\alpha_1\\mu_1}{2},\\phi_2,\\alpha_2\\mu_2\\right\\}$,\nwhere $\\alpha_i$ and $\\mu_i$ represent the fading parameters of the $i$-th THz\nlink for $i\\in(1,2)$, and $\\phi_i$ denotes the pointing error parameter. In\naddition, we extend the analysis to a multi-relay cooperative system and derive\nthe asymptotic symbol error rate expressions. Results demonstrate that the\ndiversity order of the multi-relay system is\n$K\\min\\left\\{\\frac{\\phi_1}{2},\\frac{\\alpha_1\\mu_1}{2},\\phi_2,\\alpha_2\\mu_2\\right\\}$,\nwhere $K$ is the number of relays. Finally, the derived analytical expressions\nare verified by Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 04:48:26 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Li", "Sai", ""], ["Yang", "Liang", ""]]}, {"id": "2107.14210", "submitter": "Andreas Abel", "authors": "Andreas Abel and Jan Reineke", "title": "Accurate Throughput Prediction of Basic Blocks on Recent Intel\n  Microarchitectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tools to predict the throughput of basic blocks on a specific\nmicroarchitecture are useful to optimize software performance and to build\noptimizing compilers. In recent work, several such tools have been proposed.\nHowever, the accuracy of their predictions has been shown to be relatively low.\n  In this paper, we identify the most important factors for these inaccuracies.\nTo a significant degree these inaccuracies are due to elements and parameters\nof the pipelines of recent CPUs that are not taken into account by previous\ntools. A primary reason for this is that the necessary details are often\nundocumented. In this paper, we build more precise models of relevant\ncomponents by reverse engineering using microbenchmarks. Based on these models,\nwe develop a simulator for predicting the throughput of basic blocks. In\naddition to predicting the throughput, our simulator also provides insights\ninto how the code is executed.\n  Our tool supports all Intel Core microarchitecture generations released in\nthe last decade. We evaluate it on an improved version of the BHive benchmark\nsuite. On many recent microarchitectures, its predictions are more accurate\nthan the predictions of state-of-the-art tools by more than an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:48:04 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Abel", "Andreas", ""], ["Reineke", "Jan", ""]]}]