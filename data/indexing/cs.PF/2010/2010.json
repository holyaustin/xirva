[{"id": "2010.00631", "submitter": "Isaac Grosof", "authors": "Isaac Grosof, Mor Harchol-Balter, Alan Scheller-Wolf", "title": "Stability for Two-class Multiserver-job Systems", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiserver-job systems, where jobs require concurrent service at many\nservers, occur widely in practice. Much is known in the dropping setting, where\njobs are immediately discarded if they require more servers than are currently\navailable. However, very little is known in the more practical setting where\njobs queue instead.\n  In this paper, we derive a closed-form analytical expression for the\nstability region of a two-class (non-dropping) multiserver-job system where\neach class of jobs requires a distinct number of servers and requires a\ndistinct exponential distribution of service time, and jobs are served in\nfirst-come-first-served (FCFS) order. This is the first result of any kind for\nan FCFS multiserver-job system where the classes have distinct service\ndistributions. Our work is based on a technique that leverages the idea of a\n\"saturated\" system, in which an unlimited number of jobs are always available.\n  Our analytical formula provides insight into the behavior of FCFS\nmultiserver-job systems, highlighting the huge wastage (idle servers while jobs\nare in the queue) that can occur, as well as the nonmonotonic effects of the\nservice rates on wastage.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 11:59:00 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Grosof", "Isaac", ""], ["Harchol-Balter", "Mor", ""], ["Scheller-Wolf", "Alan", ""]]}, {"id": "2010.00754", "submitter": "Neil J. Gunther", "authors": "Neil J. Gunther", "title": "P = FS: Parallel is Just Fast Serial", "comments": "7 pages, 3 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that parallel processing with homogeneous processors is logically\nequivalent to fast serial processing. The reverse proposition can also be used\nto identify obscure opportunities for applying parallelism. To our knowledge,\nthis theorem has not been previously reported in the queueing theory\nliterature. A plausible explanation is offered for why this might be. The basic\nhomogeneous theorem is also extended to optimizing the latency of heterogenous\nparallel arrays.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 02:39:07 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gunther", "Neil J.", ""]]}, {"id": "2010.00871", "submitter": "Niloofar Okati", "authors": "Niloofar Okati and Taneli Riihonen", "title": "Stochastic Analysis of Satellite Broadband by Mega-Constellations with\n  Inclined LEOs", "comments": "Accepted in the 31st International Symposium on Personal, Indoor and\n  Mobile Radio Communications (PIMRC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As emerging massive constellations are intended to provide seamless\nconnectivity for remote areas using hundreds of small low Earth orbit (LEO)\nsatellites, new methodologies have great importance to study the performance of\nthese networks. In this paper, we derive both downlink and uplink analytical\nexpressions for coverage probability and data rate of an inclined LEO\nconstellation under general fading, regardless of exact satellites' positions.\nOur solution involves two phases as we, first, abstract the network into a\nuniformly distributed network. Secondly, we obtain a new parameter, effective\nnumber of satellites, for every user's latitude which compensates for the\nperformance mismatch between the actual and uniform constellations. In addition\nto exact derivation of the network performance metrics, this study provides\ninsight into selecting the constellation parameters, e.g., the total number of\nsatellites, altitude, and inclination angle.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:58:09 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Okati", "Niloofar", ""], ["Riihonen", "Taneli", ""]]}, {"id": "2010.01542", "submitter": "Nick Brown", "authors": "Ludovic Anthony Richard Capelli, Nick Brown, Jonathan Mark Bull", "title": "iPregel: Strategies to Deal with an Extreme Form of Irregularity in\n  Vertex-Centric Graph Processing", "comments": "Preprint of paper submitted to 9th Workshop on Irregular\n  Applications: Architectures and Algorithms (IA3)", "journal-ref": "In 2019 IEEE/ACM 9th Workshop on Irregular Applications:\n  Architectures and Algorithms (IA3) (pp. 45-50). IEEE", "doi": "10.1109/IA349570.2019.00013", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, the vertex-centric programming model has attracted\nsignificant attention in the world of graph processing, resulting in the\nemergence of a number of vertex-centric frameworks. Its simple programming\ninterface, where computation is expressed from a vertex point of view, offers\nboth ease of programming to the user and inherent parallelism for the\nunderlying framework to leverage. However, vertex-centric programs represent an\nextreme form of irregularity, both inter and intra core. This is because they\nexhibit a variety of challenges from a workload that may greatly vary across\nsupersteps, through fine-grain synchronisations, to memory accesses that are\nunpredictable both in terms of quantity and location. In this paper, we explore\nthree optimisations which address these irregular challenges; a hybrid combiner\ncarefully coupling lock-free and lock-based combinations, the partial\nexternalisation of vertex structures to improve locality and the shift to an\nedge-centric representation of the workload. The optimisations were integrated\ninto the iPregel vertex-centric framework, enabling the evaluation of each\noptimisation in the context of graph processing across three general purpose\nbenchmarks common in the vertex-centric community, each run on four publicly\navailable graphs covering all orders of magnitude from a million to a billion\nedges. The result of this work is a set of techniques which we believe not only\nprovide a significant performance improvement in vertex-centric graph\nprocessing, but are also applicable more generally to irregular applications.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:28:24 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Capelli", "Ludovic Anthony Richard", ""], ["Brown", "Nick", ""], ["Bull", "Jonathan Mark", ""]]}, {"id": "2010.01709", "submitter": "William S. Moses", "authors": "William S. Moses and Valentin Churavy", "title": "Instead of Rewriting Foreign Code for Machine Learning, Automatically\n  Synthesize Fast Gradients", "comments": "To be published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AI cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying differentiable programming techniques and machine learning\nalgorithms to foreign programs requires developers to either rewrite their code\nin a machine learning framework, or otherwise provide derivatives of the\nforeign code. This paper presents Enzyme, a high-performance automatic\ndifferentiation (AD) compiler plugin for the LLVM compiler framework capable of\nsynthesizing gradients of statically analyzable programs expressed in the LLVM\nintermediate representation (IR). Enzyme synthesizes gradients for programs\nwritten in any language whose compiler targets LLVM IR including C, C++,\nFortran, Julia, Rust, Swift, MLIR, etc., thereby providing native AD\ncapabilities in these languages. Unlike traditional source-to-source and\noperator-overloading tools, Enzyme performs AD on optimized IR. On a\nmachine-learning focused benchmark suite including Microsoft's ADBench, AD on\noptimized IR achieves a geometric mean speedup of 4.5x over AD on IR before\noptimization allowing Enzyme to achieve state-of-the-art performance. Packaging\nEnzyme for PyTorch and TensorFlow provides convenient access to gradients of\nforeign code with state-of-the art performance, enabling foreign code to be\ndirectly incorporated into existing machine learning workflows.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:32:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Moses", "William S.", ""], ["Churavy", "Valentin", ""]]}, {"id": "2010.02147", "submitter": "Pei Peng", "authors": "Pei Peng, Emina Soljanin and Philip Whiting", "title": "Diversity/Parallelism Trade-off in Distributed Systems with Redundancy", "comments": "Described several open problems and outlined future directions, added\n  references, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As numerous machine learning and other algorithms increase in complexity and\ndata requirements, distributed computing becomes necessary to satisfy the\ngrowing computational and storage demands, because it enables parallel\nexecution of smaller tasks that make up a large computing job. However, random\nfluctuations in task service times lead to straggling tasks with long execution\ntimes. Redundancy, in the form of task replication and erasure coding, provides\ndiversity that allows a job to be completed when only a subset of redundant\ntasks is executed, thus removing the dependency on the straggling tasks. In\nsituations of constrained resources (here a fixed number of parallel servers),\nincreasing redundancy reduces the available resources for parallelism. In this\npaper, we characterize the diversity vs. parallelism trade-off and identify the\noptimal strategy, among replication, coding and splitting, which minimizes the\nexpected job completion time. We consider three common service time\ndistributions and establish three models that describe scaling of these\ndistributions with the task size. We find that different distributions with\ndifferent scaling models operate optimally at different levels of redundancy,\nand thus may require very different code rates.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:52:46 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 22:04:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Peng", "Pei", ""], ["Soljanin", "Emina", ""], ["Whiting", "Philip", ""]]}, {"id": "2010.02164", "submitter": "Kevin Yang", "authors": "Kevin Yang, Violet Yao, John DeNero, Dan Klein", "title": "A Streaming Approach For Efficient Batched Beam Search", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient batching strategy for variable-length decoding on GPU\narchitectures. During decoding, when candidates terminate or are pruned\naccording to heuristics, our streaming approach periodically \"refills\" the\nbatch before proceeding with a selected subset of candidates. We apply our\nmethod to variable-width beam search on a state-of-the-art machine translation\nmodel. Our method decreases runtime by up to 71% compared to a fixed-width beam\nsearch baseline and 17% compared to a variable-width baseline, while matching\nbaselines' BLEU. Finally, experiments show that our method can speed up\ndecoding in other domains, such as semantic and syntactic parsing.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:13:34 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 22:22:27 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yang", "Kevin", ""], ["Yao", "Violet", ""], ["DeNero", "John", ""], ["Klein", "Dan", ""]]}, {"id": "2010.02781", "submitter": "Andra Lutu", "authors": "Andra Lutu and Diego Perino and Marcelo Bagnulo and Enrique\n  Frias-Martinez and Javad Khangosstar", "title": "A Characterization of the COVID-19 Pandemic Impact on a Mobile Network\n  Operator Traffic", "comments": "This paper will appear in ACM Internet Measurement Conference\n  (IMC'20). Andra Lutu, Diego Perino, Marcelo Bagnulo, Enrique Frias-Martinez,\n  and Javad Khangosstar. 2020. A Characterization of the COVID-19 Pandemic\n  Impact on a Mobile Network Operator Traffic. In ACM Internet Measurement\n  Conference (IMC'20), October 27-29, 2020, Virtual Event, USA. ACM, New York,\n  NY, USA, 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During early 2020, the SARS-CoV-2 virus rapidly spread worldwide, forcing\nmany governments to impose strict lockdown measures to tackle the pandemic.\nThis significantly changed people's mobility and habits, subsequently impacting\nhow they use telecommunication networks. In this paper, we investigate the\neffects of the COVID-19 emergency on a UK Mobile Network Operator (MNO). We\nquantify the changes in users' mobility and investigate how this impacted the\ncellular network usage and performance. Our analysis spans from the entire\ncountry to specific regions, and geodemographic area clusters. We also provide\na detailed analysis for London. Our findings bring insights at different\ngeo-temporal granularity on the status of the cellular network, from the\ndecrease in data traffic volume in the cellular network and lower load on the\nradio network, counterposed to a surge in the conversational voice traffic\nvolume.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 14:50:07 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Lutu", "Andra", ""], ["Perino", "Diego", ""], ["Bagnulo", "Marcelo", ""], ["Frias-Martinez", "Enrique", ""], ["Khangosstar", "Javad", ""]]}, {"id": "2010.02871", "submitter": "Evgeny Ponomarev", "authors": "Evgeny Ponomarev and Sergey Matveev and Ivan Oseledets", "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks\n  inference on Mobile GPU", "comments": "Please, don't use this draft version because there are some mistakes\n  in data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of deep learning applications are desired to be run on mobile devices.\nBoth accuracy and inference time are meaningful for a lot of them. While the\nnumber of FLOPs is usually used as a proxy for neural network latency, it may\nbe not the best choice. In order to obtain a better approximation of latency,\nresearch community uses look-up tables of all possible layers for latency\ncalculation for the final prediction of the inference on mobile CPU. It\nrequires only a small number of experiments. Unfortunately, on mobile GPU this\nmethod is not applicable in a straight-forward way and shows low precision. In\nthis work, we consider latency approximation on mobile GPU as a data and\nhardware-specific problem. Our main goal is to construct a convenient latency\nestimation tool for investigation(LETI) of neural network inference and\nbuilding robust and accurate latency prediction models for each specific task.\nTo achieve this goal, we build open-source tools which provide a convenient way\nto conduct massive experiments on different target devices focusing on mobile\nGPU. After evaluation of the dataset, we learn the regression model on\nexperimental data and use it for future latency prediction and analysis. We\nexperimentally demonstrate the applicability of such an approach on a subset of\npopular NAS-Benchmark 101 dataset and also evaluate the most popular neural\nnetwork architectures for two mobile GPUs. As a result, we construct latency\nprediction model with good precision on the target evaluation subset. We\nconsider LETI as a useful tool for neural architecture search or massive\nlatency evaluation. The project is available at https://github.com/leti-ai\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:51:35 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 17:27:45 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ponomarev", "Evgeny", ""], ["Matveev", "Sergey", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2010.02987", "submitter": "Olga Poppe", "authors": "Olga Poppe, Chuan Lei, Elke A. Rundensteiner, David Maier", "title": "Event Trend Aggregation Under Rich Event Matching Semantics", "comments": "Technical report for the paper in SIGMOD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming applications from health care analytics to algorithmic trading\ndeploy Kleene queries to detect and aggregate event trends. Rich event matching\nsemantics determine how to compose events into trends. The expressive power of\nstate-of-the-art systems remains limited in that they do not support the rich\nvariety of these semantics. Worse yet, they suffer from long delays and high\nmemory costs because they opt to maintain aggregates at a fine granularity. To\novercome these limitations, our Coarse-Grained Event Trend Aggregation (Cogra)\napproach supports this rich diversity of event matching semantics within one\nsystem. Better yet, Cogra incrementally maintains aggregates at the coarsest\ngranularity possible for each of these semantics. In this way, Cogra minimizes\nthe number of aggregates -- reducing both time and space complexity. Our\nexperiments demonstrate that Cogra achieves up to four orders of magnitude\nspeed-up and up to eight orders of magnitude memory reduction compared to\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:26:09 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Poppe", "Olga", ""], ["Lei", "Chuan", ""], ["Rundensteiner", "Elke A.", ""], ["Maier", "David", ""]]}, {"id": "2010.02988", "submitter": "Olga Poppe", "authors": "Olga Poppe, Chuan Lei, Elke A. Rundensteiner, David Maier", "title": "GRETA: Graph-based Real-time Event Trend Aggregation", "comments": "Technical report for the paper in VLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming applications from algorithmic trading to traffic management deploy\nKleene patterns to detect and aggregate arbitrarily-long event sequences,\ncalled event trends. State-of-the-art systems process such queries in two\nsteps. Namely, they first construct all trends and then aggregate them. Due to\nthe exponential costs of trend construction, this two-step approach suffers\nfrom both a long delays and high memory costs. To overcome these limitations,\nwe propose the Graph-based Real-time Event Trend Aggregation (Greta) approach\nthat dynamically computes event trend aggregation without first constructing\nthese trends. We define the Greta graph to compactly encode all trends. Our\nGreta runtime incrementally maintains the graph, while dynamically propagating\naggregates along its edges. Based on the graph, the final aggregate is\nincrementally updated and instantaneously returned at the end of each query\nwindow. Our Greta runtime represents a win-win solution, reducing both the time\ncomplexity from exponential to quadratic and the space complexity from\nexponential to linear in the number of events. Our experiments demonstrate that\nGreta achieves up to four orders of magnitude speed-up and up to 50--fold\nmemory reduction compared to the state-of-the-art two-step approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:26:42 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Poppe", "Olga", ""], ["Lei", "Chuan", ""], ["Rundensteiner", "Elke A.", ""], ["Maier", "David", ""]]}, {"id": "2010.02989", "submitter": "Olga Poppe", "authors": "Olga Poppe, Allison Rozet, Chuan Lei, Elke A. Rundensteiner, David\n  Maier", "title": "Sharon: Shared Online Event Sequence Aggregation", "comments": "Technical report for the paper in ICDE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming systems evaluate massive workloads of event sequence aggregation\nqueries. State-of-the-art approaches suffer from long delays caused by not\nsharing intermediate results of similar queries and by constructing event\nsequences prior to their aggregation. To overcome these limitations, our Shared\nOnline Event Sequence Aggregation (Sharon) approach shares intermediate\naggregates among multiple queries while avoiding the expensive construction of\nevent sequences. Our Sharon optimizer faces two challenges. One, a sharing\ndecision is not always beneficial. Two, a sharing decision may exclude other\nsharing opportunities. To guide our Sharon optimizer, we compactly encode\nsharing candidates, their benefits, and conflicts among candidates into the\nSharon graph. Based on the graph, we map our problem of finding an optimal\nsharing plan to the Maximum Weight Independent Set (MWIS) problem. We then use\nthe guaranteed weight of a greedy algorithm for the MWIS problem to prune the\nsearch of our sharing plan finder without sacrificing its optimality. The\nSharon optimizer is shown to produce sharing plans that achieve up to an\n18-fold speed-up compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:27:26 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Poppe", "Olga", ""], ["Rozet", "Allison", ""], ["Lei", "Chuan", ""], ["Rundensteiner", "Elke A.", ""], ["Maier", "David", ""]]}, {"id": "2010.03193", "submitter": "Urmish Thakker", "authors": "Urmish Thakker, Jesse Beu, Dibakar Gope, Ganesh Dasika, Matthew\n  Mattina", "title": "Rank and run-time aware compression of NLP Applications", "comments": "Published at SustaiNLP@EMNLP 2020. arXiv admin note: text overlap\n  with arXiv:1906.04886", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence model based NLP applications can be large. Yet, many applications\nthat benefit from them run on small devices with very limited compute and\nstorage capabilities, while still having run-time constraints. As a result,\nthere is a need for a compression technique that can achieve significant\ncompression without negatively impacting inference run-time and task accuracy.\nThis paper proposes a new compression technique called Hybrid Matrix\nFactorization that achieves this dual objective. HMF improves low-rank matrix\nfactorization (LMF) techniques by doubling the rank of the matrix using an\nintelligent hybrid-structure leading to better accuracy than LMF. Further, by\npreserving dense matrices, it leads to faster inference run-time than pruning\nor structure matrix based compression technique. We evaluate the impact of this\ntechnique on 5 NLP benchmarks across multiple tasks (Translation, Intent\nDetection, Language Modeling) and show that for similar accuracy values and\ncompression factors, HMF can achieve more than 2.32x faster inference run-time\nthan pruning and 16.77% better accuracy than LMF.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:03:15 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Thakker", "Urmish", ""], ["Beu", "Jesse", ""], ["Gope", "Dibakar", ""], ["Dasika", "Ganesh", ""], ["Mattina", "Matthew", ""]]}, {"id": "2010.04212", "submitter": "Gopinath Chennupati", "authors": "Gopinath Chennupati and Nandakishore Santhi and Phill Romero and\n  Stephan Eidenbenz", "title": "Machine Learning Enabled Scalable Performance Prediction of Scientific\n  Codes", "comments": "Under review at ACM TOMACS 2020 for a special issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Analytical Memory Model with Pipelines (AMMP) of the\nPerformance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and\nhardware architecture parameters as input, predicts runtime of that code on the\ntarget hardware platform, which is defined in the input parameters. PPT-AMMP\ntransforms the code to an (architecture-independent) intermediate\nrepresentation, then (i) analyzes the basic block structure of the code, (ii)\nprocesses architecture-independent virtual memory access patterns that it uses\nto build memory reuse distance distribution models for each basic block, (iii)\nruns detailed basic-block level simulations to determine hardware pipeline\nusage.\n  PPT-AMMP uses machine learning and regression techniques to build the\nprediction models based on small instances of the input code, then integrates\ninto a higher-order discrete-event simulation model of PPT running on Simian\nPDES engine. We validate PPT-AMMP on four standard computational physics\nbenchmarks, finally present a use case of hardware parameter sensitivity\nanalysis to identify bottleneck hardware resources on different code inputs. We\nfurther extend PPT-AMMP to predict the performance of scientific application\n(radiation transport), SNAP. We analyze the application of multi-variate\nregression models that accurately predict the reuse profiles and the basic\nblock counts. The predicted runtimes of SNAP when compared to that of actual\ntimes are accurate.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 18:50:29 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 21:13:26 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Chennupati", "Gopinath", ""], ["Santhi", "Nandakishore", ""], ["Romero", "Phill", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "2010.04868", "submitter": "Liang Yuan", "authors": "Liang Yuan and Hang Cao and Yunquan Zhang and Kun Li and Pengqi Lu and\n  Yue Yue", "title": "Temporal Vectorization for Stencils", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil computations represent a very common class of nested loops in\nscientific and engineering applications. Exploiting vector units in modern CPUs\nis crucial to achieving peak performance. Previous vectorization approaches\noften consider the data space, in particular the innermost unit-strided loop.\nIt leads to the well-known data alignment conflict problem that vector loads\nare overlapped due to the data sharing between continuous stencil computations.\nThis paper proposes a novel temporal vectorization scheme for stencils. It\nvectorizes the stencil computation in the iteration space and assembles points\nwith different time coordinates in one vector. The temporal vectorization leads\nto a small fixed number of vector reorganizations that is irrelevant to the\nvector length, stencil order, and dimension. Furthermore, it is also applicable\nto Gauss-Seidel stencils, whose vectorization is not well-studied. The\neffectiveness of the temporal vectorization is demonstrated by various Jacobi\nand Gauss-Seidel stencils.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 01:47:34 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yuan", "Liang", ""], ["Cao", "Hang", ""], ["Zhang", "Yunquan", ""], ["Li", "Kun", ""], ["Lu", "Pengqi", ""], ["Yue", "Yue", ""]]}, {"id": "2010.06312", "submitter": "Vibhatha Abeykoon", "authors": "Vibhatha Abeykoon, Niranda Perera, Chathura Widanage, Supun\n  Kamburugamuve, Thejaka Amila Kanewala, Hasara Maithree, Pulasthi\n  Wickramasinghe, Ahmet Uyar and Geoffrey Fox", "title": "Data Engineering for HPC with Python", "comments": "9 pages, 11 images, Accepted in 9th Workshop on Python for\n  High-Performance and Scientific Computing (In conjunction with Supercomputing\n  20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data engineering is becoming an increasingly important part of scientific\ndiscoveries with the adoption of deep learning and machine learning. Data\nengineering deals with a variety of data formats, storage, data extraction,\ntransformation, and data movements. One goal of data engineering is to\ntransform data from original data to vector/matrix/tensor formats accepted by\ndeep learning and machine learning applications. There are many structures such\nas tables, graphs, and trees to represent data in these data engineering\nphases. Among them, tables are a versatile and commonly used format to load and\nprocess data. In this paper, we present a distributed Python API based on table\nabstraction for representing and processing data. Unlike existing\nstate-of-the-art data engineering tools written purely in Python, our solution\nadopts high performance compute kernels in C++, with an in-memory table\nrepresentation with Cython-based Python bindings. In the core system, we use\nMPI for distributed memory computations with a data-parallel approach for\nprocessing large datasets in HPC clusters.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:53:11 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Abeykoon", "Vibhatha", ""], ["Perera", "Niranda", ""], ["Widanage", "Chathura", ""], ["Kamburugamuve", "Supun", ""], ["Kanewala", "Thejaka Amila", ""], ["Maithree", "Hasara", ""], ["Wickramasinghe", "Pulasthi", ""], ["Uyar", "Ahmet", ""], ["Fox", "Geoffrey", ""]]}, {"id": "2010.07226", "submitter": "Aravind Sankaran", "authors": "Aravind Sankaran, Paolo Bientinesi", "title": "Robust Ranking of Equivalent Algorithms via Relative Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scientific computing, it is common that one target computation can be\ntranslated into many different sequences of library calls, each identifying an\nalgorithm. Although mathematically equivalent, those algorithms might exhibit\nsignificant differences in terms of performance. In practice, we observe that\nsubsets of algorithms show comparable performance characteristics. For this\nreason, we aim to identify and separate not one algorithm, but the subset of\nalgorithms, that are reliably faster than the rest. One of the motivations for\nthis setup is that it makes it then possible to select an algorithm based on\nadditional performance metrics such as those based on energy or scalability. To\nthis end, instead of using the usual methods of quantifying the performance of\nan algorithm in absolute terms, we present a measurement-based approach that\nassigns a relative score to the algorithms in comparison to one another. The\nrelative performance is encoded by sorting the algorithms based on pair-wise\ncomparisons and by ranking them into equivalence (or performance) classes, so\nthat multiple algorithms can obtain the same rank. We show that this approach,\nbased on relative performance, leads to robust identification of the fastest\nalgorithms, that is, reliable identification even under noisy system\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:40:23 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 10:41:03 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Sankaran", "Aravind", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "2010.08040", "submitter": "Xingfu Wu", "authors": "Xingfu Wu, Michael Kruse, Prasanna Balaprakash, Hal Finkel, Paul\n  Hovland, Valerie Taylor, Mary Hall", "title": "Autotuning PolyBench Benchmarks with LLVM Clang/Polly Loop Optimization\n  Pragmas Using Bayesian Optimization", "comments": "to be published in the 11th International Workshop on Performance\n  Modeling, Benchmarking and Simulation of High Performance Computer Systems\n  (PMBS20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autotuning is an approach that explores a search space of possible\nimplementations/configurations of a kernel or an application by selecting and\nevaluating a subset of implementations/configurations on a target platform\nand/or use models to identify a high performance implementation/configuration.\nIn this paper, we develop an autotuning framework that leverages Bayesian\noptimization to explore the parameter space search. We select six of the most\ncomplex benchmarks from the application domains of the PolyBench benchmarks\n(syr2k, 3mm, heat-3d, lu, covariance, and Floyd-Warshall) and apply the newly\ndeveloped LLVM Clang/Polly loop optimization pragmas to the benchmarks to\noptimize them. We then use the autotuning framework to optimize the pragma\nparameters to improve their performance. The experimental results show that our\nautotuning approach outperforms the other compiling methods to provide the\nsmallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and\ncovariance with two large datasets in 200 code evaluations for effectively\nsearching the parameter spaces with up to 170,368 different configurations. We\ncompare four different supervised learning methods within Bayesian optimization\nand evaluate their effectiveness. We find that the Floyd-Warshall benchmark did\nnot benefit from autotuning because Polly uses heuristics to optimize the\nbenchmark to make it run much slower. To cope with this issue, we provide some\ncompiler option solutions to improve the performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:09:42 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wu", "Xingfu", ""], ["Kruse", "Michael", ""], ["Balaprakash", "Prasanna", ""], ["Finkel", "Hal", ""], ["Hovland", "Paul", ""], ["Taylor", "Valerie", ""], ["Hall", "Mary", ""]]}, {"id": "2010.08455", "submitter": "Fatima Ezzahra Airod", "authors": "Fatima Ezzahra Airod, Houda Chafnaji, and Halim Yanikomeroglu", "title": "HARQ in Full-Duplex Relay-Assisted Transmissions for URLLC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Release 16 completion unlocks the road to an exciting phase pertain to\nthe sixth generation (6G) era. Meanwhile, to sustain far-reaching applications\nwith unprecedented challenges in terms of latency and reliability, much\ninterest is already getting intensified toward physical layer specifications of\n6G. In support of this vision, this work exhibits the forward-looking\nperception of full-duplex (FD) cooperative relaying in support of upcoming\ngenerations and adopts as a mean concern the critical contribution of hybrid\nautomatic repeat request (HARQ) mechanism to ultra-reliable and low-latency\ncommunication (URLLC). Indeed, the HARQ roundtrip time (RTT) is known to\ninclude basic physical delays that may cause the HARQ abandonment for the 1 ms\nlatency use case of URLLC. Taking up these challenges, this article proposes a\nhybrid FD amplify-and-forward (AF)-selective decode-and-forward (SDF)\nrelay-based system for URLLC. Over this build system, two HARQ procedures\nwithin which the HARQ RTT is shortened, are suggested to face latency and\nreliability issues, namely, the conventional and the enhanced HARQ procedures.\nWe develop then an analytical framework of this relay based HARQ system within\nits different procedures. Finally, using Monte-Carlo simulations, we confirm\nthe theoretical results and compare the proposed relay-assisted HARQ procedures\nto the source-to-destination (S2D) HARQ-based system where no relay assists the\ncommunication between the source and the destination.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 15:36:20 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 15:13:01 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Airod", "Fatima Ezzahra", ""], ["Chafnaji", "Houda", ""], ["Yanikomeroglu", "Halim", ""]]}, {"id": "2010.09063", "submitter": "Gautam Kamath", "authors": "Pranav Subramani, Nicholas Vadivelu, Gautam Kamath", "title": "Enabling Fast Differentially Private SGD via Just-in-Time Compilation\n  and Vectorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common pain point in differentially private machine learning is the\nsignificant runtime overhead incurred when executing Differentially Private\nStochastic Gradient Descent (DPSGD), which may be as large as two orders of\nmagnitude. We thoroughly demonstrate that by exploiting powerful language\nprimitives, including vectorization, just-in-time compilation, and static graph\noptimization, one can dramatically reduce these overheads, in many cases nearly\nmatching the best non-private running times. These gains are realized in two\nframeworks: JAX and TensorFlow. JAX provides rich support for these primitives\nas core features of the language through the XLA compiler. We also rebuild core\nparts of TensorFlow Privacy, integrating features from TensorFlow 2 as well as\nXLA compilation, granting significant memory and runtime improvements over the\ncurrent release version. These approaches allow us to achieve up to 50x\nspeedups in comparison to the best alternatives. Our code is available at\nhttps://github.com/TheSalon/fast-dpsgd.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 18:45:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Subramani", "Pranav", ""], ["Vadivelu", "Nicholas", ""], ["Kamath", "Gautam", ""]]}, {"id": "2010.09263", "submitter": "Anne Bouillard", "authors": "Anne Bouillard", "title": "Trade-off between accuracy and tractability of network calculus in FIFO\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing accurate deterministic performance bounds is a strong need for\ncommunication technologies having strong requirements on latency and\nreliability. Beyond new scheduling protocols such as TSN, the FIFO policy\nremains at work within each class of communication.\n  In this paper, we focus on computing deterministic performance bounds in FIFO\nnetworks in the network calculus framework. We propose a new algorithm based on\nlinear programming that presents a trade-off between accuracy and tractability.\nThis algorithm is first presented for tree networks. In a second time, we\ngeneralize our approach and present a linear program for computing performance\nbounds for arbitrary topologies, including cyclic dependencies. Finally, we\nprovide numerical results, both of toy examples and real topologies, to assess\nthe interest of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:13:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bouillard", "Anne", ""]]}, {"id": "2010.09454", "submitter": "Joshua Hoke Davis", "authors": "Joshua Hoke Davis, Christopher Daley, Swaroop Pophale, Thomas Huber,\n  Sunita Chandrasekaran, Nicholas J. Wright", "title": "Performance Assessment of OpenMP Compilers Targeting NVIDIA V100 GPUs", "comments": "20 pages, 7 figures, accepted in WACCPD 2020 at SC20 (under\n  publication with Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous systems are becoming increasingly prevalent. In order to\nexploit the rich compute resources of such systems, robust programming models\nare needed for application developers to seamlessly migrate legacy code from\ntoday's systems to tomorrow's. Over the past decade and more, directives have\nbeen established as one of the promising paths to tackle programmatic\nchallenges on emerging systems. This work focuses on applying and demonstrating\nOpenMP offloading directives on five proxy applications. We observe that the\nperformance varies widely from one compiler to the other; a crucial aspect of\nour work is reporting best practices to application developers who use OpenMP\noffloading compilers. While some issues can be worked around by the developer,\nthere are other issues that must be reported to the compiler vendors. By\nrestructuring OpenMP offloading directives, we gain an 18x speedup for the su3\nproxy application on NERSC's Cori system when using the Clang compiler, and a\n15.7x speedup by switching max reductions to add reductions in the laplace\nmini-app when using the Cray-llvm compiler on Cori.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:05:44 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 13:45:20 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 20:26:15 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Davis", "Joshua Hoke", ""], ["Daley", "Christopher", ""], ["Pophale", "Swaroop", ""], ["Huber", "Thomas", ""], ["Chandrasekaran", "Sunita", ""], ["Wright", "Nicholas J.", ""]]}, {"id": "2010.09852", "submitter": "Maciej Besta", "authors": "Hermann Schweizer, Maciej Besta, Torsten Hoefler", "title": "Evaluating the Cost of Atomic Operations on Modern Architectures", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Parallel\n  Architectures and Compilation (PACT'15), 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomic operations (atomics) such as Compare-and-Swap (CAS) or Fetch-and-Add\n(FAA) are ubiquitous in parallel programming. Yet, performance tradeoffs\nbetween these operations and various characteristics of such systems, such as\nthe structure of caches, are unclear and have not been thoroughly analyzed. In\nthis paper we establish an evaluation methodology, develop a performance model,\nand present a set of detailed benchmarks for latency and bandwidth of different\natomics. We consider various state-of-the-art x86 architectures: Intel Haswell,\nXeon Phi, Ivy Bridge, and AMD Bulldozer. The results unveil surprising\nperformance relationships between the considered atomics and architectural\nproperties such as the coherence state of the accessed cache lines. One key\nfinding is that all the tested atomics have comparable latency and bandwidth\neven if they are characterized by different consensus numbers. Another insight\nis that the hardware implementation of atomics prevents any instruction-level\nparallelism even if there are no dependencies between the issued operations.\nFinally, we discuss solutions to the discovered performance issues in the\nanalyzed architectures. Our analysis enables simpler and more effective\nparallel programming and accelerates data processing on various architectures\ndeployed in both off-the-shelf machines and large compute systems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:43:06 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Schweizer", "Hermann", ""], ["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.10248", "submitter": "George Bisbas", "authors": "George Bisbas, Fabio Luporini, Mathias Louboutin, Rhodri Nelson,\n  Gerard Gorman, Paul H. J. Kelly", "title": "Temporal blocking of finite-difference stencil operators with sparse\n  \"off-the-grid\" sources", "comments": "Accepted for publication at 35th IEEE International Parallel &\n  Distributed Processing Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil kernels dominate a range of scientific applications, including\nseismic and medical imaging, image processing, and neural networks. Temporal\nblocking is a performance optimization that aims to reduce the required memory\nbandwidth of stencil computations by re-using data from the cache for multiple\ntime steps. It has already been shown to be beneficial for this class of\nalgorithms. However, applying temporal blocking to practical applications'\nstencils remains challenging. These computations often consist of sparsely\nlocated operators not aligned with the computational grid (\"off-the-grid\"). Our\nwork is motivated by modeling problems in which source injections result in\nwavefields that must then be measured at receivers by interpolation from the\ngrided wavefield. The resulting data dependencies make the adoption of temporal\nblocking much more challenging. We propose a methodology to inspect these data\ndependencies and reorder the computation, leading to performance gains in\nstencil codes where temporal blocking has not been applicable. We implement\nthis novel scheme in the Devito domain-specific compiler toolchain. Devito\nimplements a domain-specific language embedded in Python to generate optimized\npartial differential equation solvers using the finite-difference method from\nhigh-level symbolic problem definitions. We evaluate our scheme using isotropic\nacoustic, anisotropic acoustic, and isotropic elastic wave propagators of\nindustrial significance. After auto-tuning, performance evaluation shows that\nthis enables substantial performance improvement through temporal blocking over\nhighly-optimized vectorized spatially-blocked code of up to 1.6x.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:20:07 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 14:26:45 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Bisbas", "George", ""], ["Luporini", "Fabio", ""], ["Louboutin", "Mathias", ""], ["Nelson", "Rhodri", ""], ["Gorman", "Gerard", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "2010.10350", "submitter": "Ying Mao", "authors": "Ying Mao, Yuqi Fu, Suwen Gu, Sudip Vhaduri, Long Cheng and Qingzhi Liu", "title": "Resource Management Schemes for Cloud-Native Platforms with Computing\n  Containers of Docker and Kubernetes", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Businesses have made increasing adoption and incorporation of cloud\ntechnology into internal processes in the last decade. The cloud-based\ndeployment provides on-demand availability without active management. More\nrecently, the concept of cloud-native application has been proposed and\nrepresents an invaluable step toward helping organizations develop software\nfaster and update it more frequently to achieve dramatic business outcomes.\nCloud-native is an approach to build and run applications that exploit the\ncloud computing delivery model's advantages. It is more about how applications\nare created and deployed than where. The container-based virtualization\ntechnology, such as Docker and Kubernetes, serves as the foundation for\ncloud-native applications. This paper investigates the performance of two\npopular computational-intensive applications, big data, and deep learning, in a\ncloud-native environment. We analyze the system overhead and resource usage for\nthese applications. Through extensive experiments, we show that the completion\ntime reduces by up to 79.4% by changing the default setting and increases by up\nto 96.7% due to different resource management schemes on two platforms.\nAdditionally, the resource release is delayed by up to 116.7% across different\nsystems. Our work can guide developers, administrators, and researchers to\nbetter design and deploy their applications by selecting and configuring a\nhosting platform.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:13:25 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Mao", "Ying", ""], ["Fu", "Yuqi", ""], ["Gu", "Suwen", ""], ["Vhaduri", "Sudip", ""], ["Cheng", "Long", ""], ["Liu", "Qingzhi", ""]]}, {"id": "2010.10621", "submitter": "Rik Mulder", "authors": "Rik Mulder, Valentin Radu, Christophe Dubach", "title": "Optimising the Performance of Convolutional Neural Networks across\n  Computing Systems using Transfer Learning", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of convolutional routines (primitives) to implement neural\nnetworks has a tremendous impact on their inference performance (execution\nspeed) on a given hardware platform. To optimise a neural network by primitive\nselection, the optimal primitive is identified for each layer of the network.\nThis process requires a lengthy profiling stage, iterating over all the\navailable primitives for each layer configuration, to measure their execution\ntime on the target platform. Because each primitive exploits the hardware in\ndifferent ways, new profiling is needed to obtain the best performance when\nmoving to another platform. In this work, we propose to replace this\nprohibitively expensive profiling stage with a machine learning based approach\nof performance modeling. Our approach speeds up the optimisation time\ndrastically. After training, our performance model can estimate the performance\nof convolutional primitives in any layer configuration. The time to optimise\nthe execution of large neural networks via primitive selection is reduced from\nhours to just seconds. Our performance model is easily transferable to other\ntarget platforms. We demonstrate this by training a performance model on an\nIntel platform and performing transfer learning to AMD and ARM processor\ndevices with minimal profiled samples.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 20:58:27 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Mulder", "Rik", ""], ["Radu", "Valentin", ""], ["Dubach", "Christophe", ""]]}, {"id": "2010.11307", "submitter": "Ying Mao", "authors": "Ying Mao, Yuqi Fu, Wenjia Zheng, Long Cheng, Qingzhi Liu, and Dingwen\n  Tao", "title": "Speculative Container Scheduling for Deep Learning Applications in a\n  Kubernetes Cluster", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, we have witnessed a dramatically increasing volume of\ndata collected from varied sources. The explosion of data has transformed the\nworld as more information is available for collection and analysis than ever\nbefore. To maximize the utilization, various machine and deep learning models\nhave been developed, e.g. CNN [1] and RNN [2], to study data and extract\nvaluable information from different perspectives. While data-driven\napplications improve countless products, training models for hyperparameter\ntuning is still a time-consuming and resource-intensive process. Cloud\ncomputing provides infrastructure support for the training of deep learning\napplications. The cloud service providers, such as Amazon Web Services [3],\ncreate an isolated virtual environment (virtual machines and containers) for\nclients, who share physical resources, e.g., CPU and memory. On the cloud,\nresource management schemes are implemented to enable better sharing among\nusers and boost the system-wide performance. However, general scheduling\napproaches, such as spread priority and balanced resource schedulers, do not\nwork well with deep learning workloads. In this project, we propose SpeCon, a\nnovel container scheduler that is optimized for shortlived deep learning\napplications. Based on virtualized containers, such as Kubernetes [4] and\nDocker [5], SpeCon analyzes the common characteristics of training processes.\nWe design a suite of algorithms to monitor the progress of the training and\nspeculatively migrate the slow-growing models to release resources for\nfast-growing ones. Specifically, the extensive experiments demonstrate that\nSpeCon improves the completion time of an individual job by up to 41.5%, 14.8%\nsystem-wide and 24.7% in terms of makespan.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 20:56:50 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mao", "Ying", ""], ["Fu", "Yuqi", ""], ["Zheng", "Wenjia", ""], ["Cheng", "Long", ""], ["Liu", "Qingzhi", ""], ["Tao", "Dingwen", ""]]}, {"id": "2010.12420", "submitter": "Jinfu Zhu", "authors": "Jinfu Zhu, Tianhao Wang, Tao Xue, Liangjun Wei, Jingjun Wen, Lin\n  Jiang, Jianmin Li", "title": "Analysis and Verification of Relation between Digitizer's Sampling\n  Properties and Energy Resolution of HPGe Detectors", "comments": "2 pages, 4 figures, 2020 IEEE Real Time Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CDEX (China Dark matter Experiment) aims at detection of WIMPs (Weakly\nInteracting Massive Particles) and 0vbb (Neutrinoless double beta decay) of\n76Ge. It now uses ~10 kg HPGe (High Purity Germanium) detectors in CJPL (China\nJinping Underground Laboratory). The energy resolution of detectors is\ncalculated via height spectrum of waveforms with 6-us shaping time. It is\nnecessary to know how sampling properties of a digitizer effect the energy\nresolution. This paper will present preliminary energy resolution results of\nwaveforms at different sampling properties. The preliminary results show that\nthe ENOB (effective number of bits) with 8.25-bit or better can meet the energy\nresolution @122keV of CDEX HPGe detectors. Based on the ADC (Analog-to-Digital\nConverter) quantized error theory, this paper will also make a quantitative\nanalysis on energy resolution in CDEX HPGe detectors. It will provide guidance\nfor ADC design in full-chain cryogenic readout electronics for HPGe detectors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:10:21 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhu", "Jinfu", ""], ["Wang", "Tianhao", ""], ["Xue", "Tao", ""], ["Wei", "Liangjun", ""], ["Wen", "Jingjun", ""], ["Jiang", "Lin", ""], ["Li", "Jianmin", ""]]}, {"id": "2010.12607", "submitter": "Ra\\'ul Nozal", "authors": "Ra\\'ul Nozal, Jose Luis Bosque and Ramon Beivide", "title": "Towards Co-execution on Commodity Heterogeneous Systems: Optimizations\n  for Time-Constrained Scenarios", "comments": "8 pages, 6 figures, conference", "journal-ref": "2019 International Conference on High Performance Computing &\n  Simulation (HPCS), pp. 628-635", "doi": "10.1109/HPCS48598.2019.9188188", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous systems are present from powerful supercomputers, to mobile\ndevices, including desktop computers, thanks to their excellent performance and\nenergy consumption. The ubiquity of these architectures in both desktop systems\nand medium-sized service servers allow enough variability to exploit a wide\nrange of problems, such as multimedia workloads, video encoding, image\nfiltering and inference in machine learning. Due to the heterogeneity, some\nefforts have been done to reduce the programming effort and preserve\nperformance portability, but these systems include a set of challenges. The\ncontext in which applications offload the workload along with the management\noverheads introduced when doing co-execution, penalize the performance gains\nunder time-constrained scenarios. Therefore, this paper proposes optimizations\nfor the EngineCL runtime to reduce the penalization when co-executing in\ncommodity systems, as well as algorithmic improvements when load balancing. An\nexhaustive experimental evaluation is performed, showing optimization\nimprovements of 7.5\\% and 17.4\\% for binary and ROI-based offloading modes,\nrespectively. Thanks to all the optimizations, the new load balancing algorithm\nis always the most efficient scheduling configuration, achieving an average\nefficiency of 0.84 under a pessimistic scenario.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 18:32:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Nozal", "Ra\u00fal", ""], ["Bosque", "Jose Luis", ""], ["Beivide", "Ramon", ""]]}, {"id": "2010.12635", "submitter": "John Brennan", "authors": "John Brennan, Stephen Bonner, Amir Atapour-Abarghouei, Philip T\n  Jackson, Boguslaw Obara, Andrew Stephen McGough", "title": "Not Half Bad: Exploring Half-Precision in Graph Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing significance of graphs as an effective representation of\ndata in numerous applications, efficient graph analysis using modern machine\nlearning is receiving a growing level of attention. Deep learning approaches\noften operate over the entire adjacency matrix -- as the input and intermediate\nnetwork layers are all designed in proportion to the size of the adjacency\nmatrix -- leading to intensive computation and large memory requirements as the\ngraph size increases. It is therefore desirable to identify efficient measures\nto reduce both run-time and memory requirements allowing for the analysis of\nthe largest graphs possible. The use of reduced precision operations within the\nforward and backward passes of a deep neural network along with novel\nspecialised hardware in modern GPUs can offer promising avenues towards\nefficiency. In this paper, we provide an in-depth exploration of the use of\nreduced-precision operations, easily integrable into the highly popular PyTorch\nframework, and an analysis of the effects of Tensor Cores on graph\nconvolutional neural networks. We perform an extensive experimental evaluation\nof three GPU architectures and two widely-used graph analysis tasks (vertex\nclassification and link prediction) using well-known benchmark and\nsynthetically generated datasets. Thus allowing us to make important\nobservations on the effects of reduced-precision operations and Tensor Cores on\ncomputational and memory usage of graph convolutional neural networks -- often\nneglected in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:47:42 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Brennan", "John", ""], ["Bonner", "Stephen", ""], ["Atapour-Abarghouei", "Amir", ""], ["Jackson", "Philip T", ""], ["Obara", "Boguslaw", ""], ["McGough", "Andrew Stephen", ""]]}, {"id": "2010.12728", "submitter": "Ying Mao", "authors": "Ying Mao, Weifeng Yan, Yun Song, Yue Zeng, Ming Chen, Long Cheng, and\n  Qingzhi Liu", "title": "Differentiate Quality of Experience Scheduling for Deep Learning\n  Applications with Docker Containers in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of big-data-driven applications, such as face recognition\non smartphones and tailored recommendations from Google Ads, we are on the road\nto a lifestyle with significantly more intelligence than ever before. For\nexample, Aipoly Vision [1] is an object and color recognizer that helps the\nblind, visually impaired, and color blind understand their surroundings. At the\nback end side of their intelligence, various neural networks powered models are\nrunning to enable quick responses to users. Supporting those models requires\nlots of cloud-based computational resources, e.g. CPUs and GPUs. The cloud\nproviders charge their clients by the amount of resources that they occupied.\nFrom clients' perspective, they have to balance the budget and quality of\nexperiences (e.g. response time). The budget leans on individual business\nowners and the required Quality of Experience (QoE) depends on usage scenarios\nof different applications, for instance, an autonomous vehicle requires\nrealtime response, but, unlocking your smartphone can tolerate delays. However,\ncloud providers fail to offer a QoE based option to their clients. In this\npaper, we propose DQoES, a differentiate quality of experience scheduler for\ndeep learning applications. DQoES accepts client's specification on targeted\nQoEs, and dynamically adjust resources to approach their targets. Through\nextensive, cloud-based experiments, DQoES demonstrates that it can schedule\nmultiple concurrent jobs with respect to various QoEs and achieve up to 8x\ntimes more satisfied models compared to the existing system.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 00:57:31 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mao", "Ying", ""], ["Yan", "Weifeng", ""], ["Song", "Yun", ""], ["Zeng", "Yue", ""], ["Chen", "Ming", ""], ["Cheng", "Long", ""], ["Liu", "Qingzhi", ""]]}, {"id": "2010.12869", "submitter": "Farhad Merchant", "authors": "Suresh Nambi, Salim Ullah, Aditya Lohana, Siva Satyendra Sahoo, Farhad\n  Merchant, Akash Kumar", "title": "ExPAN(N)D: Exploring Posits for Efficient Artificial Neural Network\n  Design in FPGA-based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.ET cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in machine learning, in general, and Artificial Neural\nNetworks (ANN), in particular, has made smart embedded systems an attractive\noption for a larger number of application areas. However, the high\ncomputational complexity, memory footprints, and energy requirements of machine\nlearning models hinder their deployment on resource-constrained embedded\nsystems. Most state-of-the-art works have considered this problem by proposing\nvarious low bit-width data representation schemes, optimized arithmetic\noperators' implementations, and different complexity reduction techniques such\nas network pruning. To further elevate the implementation gains offered by\nthese individual techniques, there is a need to cross-examine and combine these\ntechniques' unique features. This paper presents ExPAN(N)D, a framework to\nanalyze and ingather the efficacy of the Posit number representation scheme and\nthe efficiency of fixed-point arithmetic implementations for ANNs. The Posit\nscheme offers a better dynamic range and higher precision for various\napplications than IEEE $754$ single-precision floating-point format. However,\ndue to the dynamic nature of the various fields of the Posit scheme, the\ncorresponding arithmetic circuits have higher critical path delay and resource\nrequirements than the single-precision-based arithmetic units. Towards this\nend, we propose a novel Posit to fixed-point converter for enabling\nhigh-performance and energy-efficient hardware implementations for ANNs with\nminimal drop in the output accuracy. We also propose a modified Posit-based\nrepresentation to store the trained parameters of a network. Compared to an\n$8$-bit fixed-point-based inference accelerator, our proposed implementation\noffers $\\approx46\\%$ and $\\approx18\\%$ reductions in the storage requirements\nof the parameters and energy consumption of the MAC units, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:02:25 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 05:28:28 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Nambi", "Suresh", ""], ["Ullah", "Salim", ""], ["Lohana", "Aditya", ""], ["Sahoo", "Siva Satyendra", ""], ["Merchant", "Farhad", ""], ["Kumar", "Akash", ""]]}, {"id": "2010.13296", "submitter": "Ying Mao", "authors": "Ying Mao and Peizhao Hu", "title": "Enhancing Cloud Storage with Shareable Instances for Social Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud storage plays an important role in social computing. This paper aims to\ndevelop a cloud storage management system for mobile devices to support an\nextended set of file operations. Because of the limit of storage, bandwidth,\npower consumption, and other resource restrictions, most existing cloud storage\napps for smartphones do not keep local copies of files. This efficient design,\nhowever, limits the application capacities. In this paper, we attempt to extend\nthe available file operations for cloud storage service to better serve\nsmartphone users. We develop an efficient and secure file management system,\nSkyfiles, to support more advanced file operations. The basic idea of our\ndesign is to utilize cloud instances to assist file operations. Particularly,\nSkyfiles supports downloading, compressing, encrypting, and converting\noperations, as well as file transfer between two smartphone users' cloud\nstorage spaces. In addition, we propose a protocol for users to share their\nidle instances. All file operations supported by Skyfiles can be efficiently\nand securely accomplished with either a self-created instance or shared\ninstance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 02:52:43 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mao", "Ying", ""], ["Hu", "Peizhao", ""]]}, {"id": "2010.13861", "submitter": "Luis Sequeira Dr", "authors": "Luis Sequeira, Juan Luis de la Cruz, Jose Ruiz-Mas, Jose Saldana,\n  Julian Fernandez-Navajas and Jose Almodovar", "title": "Building a SDN Enterprise WLAN Based on Virtual APs", "comments": null, "journal-ref": "IEEE Communications Letters. Vol. 21, no. 2, pp. 374-377, Feb.\n  2017", "doi": "10.1109/LCOMM.2016.2623602", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter the development and testing of an open enterprise Wi-Fi\nsolution based on virtual APs, managed by a central WLAN controller is\npresented. It allows seamless handovers between APs in different channels,\nmaintaining the QoS of real-time services. The potential scalability issues\nassociated to the beacon generation and channel assignment have been addressed.\nA battery of tests has been run in a real environment, and the results are\nreported in terms of packet loss and delay.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 19:30:48 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Sequeira", "Luis", ""], ["de la Cruz", "Juan Luis", ""], ["Ruiz-Mas", "Jose", ""], ["Saldana", "Jose", ""], ["Fernandez-Navajas", "Julian", ""], ["Almodovar", "Jose", ""]]}, {"id": "2010.14027", "submitter": "Runyu Jin", "authors": "Qirui Yang, Runyu Jin, Nabil Gandhi, Xiongzi Ge, Hoda Aghaei Khouzani,\n  and Ming Zhao", "title": "EdgeBench: A Workflow-based Benchmark for Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Edge computing has been developed to utilize multiple tiers of resources for\nprivacy, cost and Quality of Service (QoS) reasons. Edge workloads have the\ncharacteristics of data-driven and latency-sensitive. Because of this, edge\nsystems have developed to be both heterogeneous and distributed. The unique\ncharacteristics of edge workloads and edge systems have motivated EdgeBench, a\nworkflow-based benchmark aims to provide the ability to explore the full design\nspace of edge workloads and edge systems. EdgeBench is both customizable and\nrepresentative. It allows users to customize the workflow logic of edge\nworkloads, the data storage backends, and the distribution of the individual\nworkflow stages to different computing tiers. To illustrate the usability of\nEdgeBench, we also implements two representative edge workflows, a video\nanalytics workflow and an IoT hub workflow that represents two distinct but\ncommon edge workloads. Both workflows are evaluated using the workflow-level\nand function-level metrics reported by EdgeBench to illustrate both the\nperformance bottlenecks of the edge systems and the edge workloads.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 03:11:41 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yang", "Qirui", ""], ["Jin", "Runyu", ""], ["Gandhi", "Nabil", ""], ["Ge", "Xiongzi", ""], ["Khouzani", "Hoda Aghaei", ""], ["Zhao", "Ming", ""]]}, {"id": "2010.15012", "submitter": "Vanlin Sathya", "authors": "Vanlin Sathya, Muhammad Iqbal Rochman, and Monisha Ghosh", "title": "Measurement-based coexistence studies of LAA & Wi-Fi deployments in\n  Chicago", "comments": "IEEE Wireless Communication Magazine, October 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF eess.SP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  LTE-Licensed Assisted Access (LAA) networks are beginning to be deployed\nwidely in major metropolitan areas in the US in the unlicensed 5 GHz bands,\nwhich have existing dense deployments of Wi-Fi as well. Various aspects of the\ncoexistence scenarios such deployments give rise to have been considered ina\nvast body of academic and industry research. However, there is very little data\nand research on how these coexisting networks will behave in practice. The\nquestion of fair coexistence between Wi-Fi and LAA has moved from a theoretical\nquestion to reality. The recent roll-out of LAA deployments provides an\nopportunity to collect data on the operation of these networks as well as\nstudying coexistence issues on the ground. In this paper we describe the first\nresults of a measurement campaign conducted over many months, using custom apps\nas well as off-the-shelf tools, in several areas of Chicago where the major\ncarriers have been expanding LAA deployments. The measurements reveal that\ncoexistence between LAA and Wi-Fi in dense, urban environments where both\nsystems aggregate multiple channels, continues to be a challenging problem that\nrequires further research.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 14:37:40 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Sathya", "Vanlin", ""], ["Rochman", "Muhammad Iqbal", ""], ["Ghosh", "Monisha", ""]]}, {"id": "2010.15444", "submitter": "Andreas Gocht", "authors": "Andreas Gocht and Robert Sch\\\"one and Jan Frenzel", "title": "Advanced Python Performance Monitoring with Score-P", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the last years, Python became more prominent in the scientific\ncommunity and is now used for simulations, machine learning, and data analysis.\nAll these tasks profit from additional compute power offered by parallelism and\noffloading. In the domain of High Performance Computing (HPC), we can look back\nto decades of experience exploiting different levels of parallelism on the\ncore, node or inter-node level, as well as utilising accelerators. By using\nperformance analysis tools to investigate all these levels of parallelism, we\ncan tune applications for unprecedented performance. Unfortunately, standard\nPython performance analysis tools cannot cope with highly parallel programs.\nSince the development of such software is complex and error-prone, we\ndemonstrate an easy-to-use solution based on an existing tool infrastructure\nfor performance analysis. In this paper, we describe how to apply the\nestablished instrumentation framework \\scorep to trace Python applications. We\nfinish with a study of the overhead that users can expect for instrumenting\ntheir applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 09:35:50 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Gocht", "Andreas", ""], ["Sch\u00f6ne", "Robert", ""], ["Frenzel", "Jan", ""]]}, {"id": "2010.15525", "submitter": "Diego Goldsztajn", "authors": "Diego Goldsztajn, Sem C. Borst, Johan S. H. van Leeuwaarden, Debankur\n  Mukherjee, Philip A. Whiting", "title": "Self-Learning Threshold-Based Load Balancing", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a large-scale service system where incoming tasks have to be\ninstantaneously dispatched to one out of many parallel server pools. The\ndispatcher uses a threshold for balancing the load and keeping the maximum\nnumber of concurrent tasks across server pools low. We demonstrate that such a\npolicy is optimal on the fluid and diffusion scales for a suitable threshold\nvalue, while only involving a small communication overhead. In order to set the\nthreshold optimally, it is important, however, to learn the load of the system,\nwhich may be uncertain or even time-varying. For that purpose, we design a\ncontrol rule for tuning the threshold in an online manner. We provide\nconditions which guarantee that this adaptive threshold settles at the optimal\nvalue, along with estimates for the time until this happens.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:45:47 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 15:04:01 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Goldsztajn", "Diego", ""], ["Borst", "Sem C.", ""], ["van Leeuwaarden", "Johan S. H.", ""], ["Mukherjee", "Debankur", ""], ["Whiting", "Philip A.", ""]]}, {"id": "2010.15534", "submitter": "Sebastian Frischbier", "authors": "Manuel Coenen, Christoph Wagner, Alexander Echler, Sebastian\n  Frischbier", "title": "Poster: Benchmarking Financial Data Feed Systems", "comments": "Authors' version of the accepted submission; final version published\n  by ACM as part of the proceedings of DEBS '19: The 13th ACM International\n  Conference on Distributed and Event-based Systems (DEBS '19); 2 pages, 2\n  figures", "journal-ref": null, "doi": "10.1145/3328905.3332506", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven solutions for the investment industry require event-based backend\nsystems to process high-volume financial data feeds with low latency, high\nthroughput, and guaranteed delivery modes.\n  At vwd we process an average of 18 billion incoming event notifications from\n500+ data sources for 30 million symbols per day and peak rates of 1+ million\nnotifications per second using custom-built platforms that keep audit logs of\nevery event.\n  We currently assess modern open source event-processing platforms such as\nKafka, NATS, Redis, Flink or Storm for the use in our ticker plant to reduce\nthe maintenance effort for cross-cutting concerns and leverage hybrid\ndeployment models. For comparability and repeatability we benchmark candidates\nwith a standardized workload we derived from our real data feeds.\n  We have enhanced an existing light-weight open source benchmarking tool in\nits processing, logging, and reporting capabilities to cope with our workloads.\nThe resulting tool wrench can simulate workloads or replay snapshots in volume\nand dynamics like those we process in our ticker plant. We provide the tool as\nopen source.\n  As part of ongoing work we contribute details on (a) our workload and\nrequirements for benchmarking candidate platforms for financial feed\nprocessing; (b) the current state of the tool wrench.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:59:27 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Coenen", "Manuel", ""], ["Wagner", "Christoph", ""], ["Echler", "Alexander", ""], ["Frischbier", "Sebastian", ""]]}, {"id": "2010.15572", "submitter": "Taichi Miya", "authors": "Taichi Miya, Kohta Ohshima, Yoshiaki Kitaguchi and Katsunori Yamaoka", "title": "Experimental Analysis of Communication Relaying Delay in Low-Energy\n  Ad-hoc Networks", "comments": "6 pages, 19 figures, IEEE Consumer Communications & Networking\n  Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.OS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, more and more applications use ad-hoc networks for local M2M\ncommunications, but in some cases such as when using WSNs, the software\nprocessing delay induced by packets relaying may not be negligible. In this\npaper, we planned and carried out a delay measurement experiment using\nRaspberry Pi Zero W. The results demonstrated that, in low-energy ad-hoc\nnetworks, processing delay of the application is always too large to ignore; it\nis at least ten times greater than the kernel routing and corresponds to 30% of\nthe transmission delay. Furthermore, if the task is CPU-intensive, such as\npacket encryption, the processing delay can be greater than the transmission\ndelay and its behavior is represented by a simple linear model. Our findings\nindicate that the key factor for achieving QoS in ad-hoc networks is an\nappropriate node-to-node load balancing that takes into account the CPU\nperformance and the amount of traffic passing through each node.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 13:44:58 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 09:37:11 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Miya", "Taichi", ""], ["Ohshima", "Kohta", ""], ["Kitaguchi", "Yoshiaki", ""], ["Yamaoka", "Katsunori", ""]]}, {"id": "2010.16165", "submitter": "Guangli Li", "authors": "Guangli Li, Xiu Ma, Xueying Wang, Lei Liu, Jingling Xue and Xiaobing\n  Feng", "title": "Fusion-Catalyzed Pruning for Optimizing Deep Learning on Intelligent\n  Edge Devices", "comments": "Published in IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems (TCAD)", "journal-ref": null, "doi": "10.1109/TCAD.2020.3013050", "report-no": null, "categories": "cs.NE cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing computational cost of deep neural network models limits the\napplicability of intelligent applications on resource-constrained edge devices.\nWhile a number of neural network pruning methods have been proposed to compress\nthe models, prevailing approaches focus only on parametric operators (e.g.,\nconvolution), which may miss optimization opportunities. In this paper, we\npresent a novel fusion-catalyzed pruning approach, called FuPruner, which\nsimultaneously optimizes the parametric and non-parametric operators for\naccelerating neural networks. We introduce an aggressive fusion method to\nequivalently transform a model, which extends the optimization space of pruning\nand enables non-parametric operators to be pruned in a similar manner as\nparametric operators, and a dynamic filter pruning method is applied to\ndecrease the computational cost of models while retaining the accuracy\nrequirement. Moreover, FuPruner provides configurable optimization options for\ncontrolling fusion and pruning, allowing much more flexible\nperformance-accuracy trade-offs to be made. Evaluation with state-of-the-art\nresidual neural networks on five representative intelligent edge platforms,\nJetson TX2, Jetson Nano, Edge TPU, NCS, and NCS2, demonstrates the\neffectiveness of our approach, which can accelerate the inference of models on\nCIFAR-10 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:10:08 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 12:50:10 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Guangli", ""], ["Ma", "Xiu", ""], ["Wang", "Xueying", ""], ["Liu", "Lei", ""], ["Xue", "Jingling", ""], ["Feng", "Xiaobing", ""]]}, {"id": "2010.16225", "submitter": "Matteo Croci", "authors": "Matteo Croci, Michael Bryce Giles", "title": "Effects of round-to-nearest and stochastic rounding in the numerical\n  solution of the heat equation in low precision", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the advent of machine learning, the last few years saw the\nreturn of hardware-supported low-precision computing. Computations with fewer\ndigits are faster and more memory and energy efficient, but can be extremely\nsusceptible to rounding errors. An application that can largely benefit from\nthe advantages of low-precision computing is the numerical solution of partial\ndifferential equations (PDEs), but a careful implementation and rounding error\nanalysis are required to ensure that sensible results can still be obtained.\n  In this paper we study the accumulation of rounding errors in the solution of\nthe heat equation, a proxy for parabolic PDEs, via Runge-Kutta finite\ndifference methods using round-to-nearest (RtN) and stochastic rounding (SR).\nWe demonstrate how to implement the scheme to reduce rounding errors and we\nderive \\emph{a priori} estimates for local and global rounding errors. Let $u$\nbe the roundoff unit. While the worst-case local errors are $O(u)$ with respect\nto the discretization parameters, the RtN and SR error behavior is\nsubstantially different. We prove that the RtN solution is discretization,\ninitial condition and precision dependent, and always stagnates for small\nenough $\\Delta t$. Until stagnation, the global error grows like $O(u\\Delta\nt^{-1})$. In contrast, we show that the leading order errors introduced by SR\nare zero-mean, independent in space and mean-independent in time, making SR\nresilient to stagnation and rounding error accumulation. In fact, we prove that\nfor SR the global rounding errors are only $O(u\\Delta t^{-1/4})$ in 1D and are\nessentially bounded (up to logarithmic factors) in higher dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 12:39:18 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Croci", "Matteo", ""], ["Giles", "Michael Bryce", ""]]}]