[{"id": "1904.00447", "submitter": "Negin Abhar", "authors": "Amir Moaddeli, Iman Nabati Ahmadi, Negin Abhar", "title": "The Power of d Choices in Scheduling for Data Centers with Heterogeneous\n  Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce framework is the de facto in big data and its applications where a\nbig data-set is split into small data chunks that are replicated on different\nservers among thousands of servers. The heterogeneous server structure of the\nsystem makes the scheduling much harder than scheduling for systems with\nhomogeneous servers. Throughput optimality of the system on one hand and delay\noptimality on the other hand creates a dilemma for assigning tasks to servers.\nThe JSQ-MaxWeight and Balanced-Pandas algorithms are the states of the arts\nalgorithms with theoretical guarantees on throughput and delay optimality for\nsystems with two and three levels of data locality. However, the scheduling\ncomplexity of these two algorithms are way too much. Hence, we use the power of\n$d$ choices algorithm combined with the Balanced-Pandas algorithm and the\nJSQ-MaxWeight algorithm, and compare the complexity of the simple algorithms\nand the power of $d$ choices versions of them. We will further show that the\nBalanced-Pandas algorithm combined with the power of the $d$ choices,\nBalanced-Pandas-Pod, not only performs better than simple Balanced-Pandas, but\nalso is less sensitive to the parameter $d$ than the combination of the\nJSQ-MaxWeight algorithm and the power of the $d$ choices, JSQ-MaxWeight-Pod. In\nfact in our extensive simulation results, the Balanced-Pandas-Pod algorithm is\nperforming better than the simple Balanced-Pandas algorithm in low and medium\nloads, where data centers are usually performing at, and performs almost the\nsame as the Balanced-Pandas algorithm at high loads. Note that the load\nbalancing complexity of Balanced-Pandas and JSQ-MaxWeight algorithms are\n$O(M)$, where $M$ is the number of servers in the system which is in the order\nof thousands servers, whereas the complexity of Balanced-Pandas-Pod and\nJSQ-MaxWeight-Pod are $O(1)$, that makes the central scheduler faster and saves\nenergy.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 17:05:43 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Moaddeli", "Amir", ""], ["Ahmadi", "Iman Nabati", ""], ["Abhar", "Negin", ""]]}, {"id": "1904.00629", "submitter": "Issam Damaj", "authors": "Safaa Kasbah, Issam Damaj, Ramzi Haraty", "title": "Multigrid Solvers in Reconfigurable Hardware", "comments": "24 Pages, 11 Figures, 10 Tables", "journal-ref": "Journal of Computational and Applied Mathematics, Elsevier. 213\n  (2008) 79-94", "doi": "10.1016/j.cam.2006.12.031", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding the solution of Partial Differential Equations (PDEs)\nplays a central role in modeling real world problems. Over the past years,\nMultigrid solvers have showed their robustness over other techniques, due to\nits high convergence rate which is independent of the problem size. For this\nreason, many attempts for exploiting the inherent parallelism of Multigrid have\nbeen made to achieve the desired efficiency and scalability of the method. Yet,\nmost efforts fail in this respect due to many factors (time, resources)\ngoverned by software implementations. In this paper, we present a hardware\nimplementation of the V-cycle Multigrid method for finding the solution of a\n2D-Poisson equation. We use Handel-C to implement our hardware design, which we\nmap onto available Field Programmable Gate Arrays (FPGAs). We analyze the\nimplementation performance using the FPGA vendor's tools. We demonstrate the\nrobustness of Multigrid over other iterative solvers, such as Jacobi and\nSuccessive Over Relaxation (SOR), in both hardware and software. We compare our\nfindings with a C++ version of each algorithm. The obtained results show better\nperformance when compared to existing software versions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:17:47 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Kasbah", "Safaa", ""], ["Damaj", "Issam", ""], ["Haraty", "Ramzi", ""]]}, {"id": "1904.01000", "submitter": "Issam Damaj", "authors": "Issam Damaj (1), Safaa Kasbah (2) ((1), American University of Kuwait,\n  (2) Lebanese American University)", "title": "An Analysis Framework for Hardware and Software Implementations with\n  Applications from Cryptography", "comments": "20 Pages, 6 Figures, 5 Tables", "journal-ref": "Electrical & Computer Engineering, Elsevier. 69 (2018) 572-584", "doi": "10.1016/j.compeleceng.2017.06.008", "report-no": null, "categories": "cs.DC cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the richness of present-day hardware architectures, tightening the\nsynergy between hardware and software has attracted a great attention. The\ninterest in unified approaches paved the way for newborn frameworks that target\nhardware and software co-design. This paper confirms that a unified statistical\nframework can successfully classify algorithms based on a combination of the\nheterogeneous characteristics of their hardware and software implementations.\nThe proposed framework produces customizable indicators for any hybridization\nof processing systems and can be contextualized for any area of application.\nThe framework is used to develop the Lightness Indicator System (LIS) as a\ncase-study that targets a set of cryptographic algorithms that are known in the\nliterature to be tiny and light. The LIS targets state-of-the-art multi-core\nprocessors and high-end Field Programmable Gate Arrays (FPGAs). The presented\nwork includes a generic benchmark model that aids the clear presentation of the\nframework and extensive performance analysis and evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 18:38:55 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Damaj", "Issam", ""], ["Kasbah", "Safaa", ""]]}, {"id": "1904.01831", "submitter": "Cai Shaofeng", "authors": "Shaofeng Cai, Gang Chen, Beng Chin Ooi, Jinyang Gao", "title": "Model Slicing for Supporting Complex Analytics with Elastic Inference\n  Cost and Resource Constraints", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": "10.14778/3364324.3364325", "report-no": null, "categories": "cs.LG cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have been used to support analytics beyond simple\naggregation, where deeper and wider models have been shown to yield great\nresults. These models consume a huge amount of memory and computational\noperations. However, most of the large-scale industrial applications are often\ncomputational budget constrained. In practice, the peak workload of inference\nservice could be 10x higher than the average cases, with the presence of\nunpredictable extreme cases. Lots of computational resources could be wasted\nduring off-peak hours and the system may crash when the workload exceeds system\ncapacity. How to support deep learning services with a dynamic workload\ncost-efficiently remains a challenging problem. In this paper, we address the\nchallenge with a general and novel training scheme called model slicing, which\nenables deep learning models to provide predictions within the prescribed\ncomputational resource budget dynamically. Model slicing could be viewed as an\nelastic computation solution without requiring more computational resources.\nSuccinctly, each layer in the model is divided into groups of a contiguous\nblock of basic components (i.e. neurons in dense layers and channels in\nconvolutional layers), and then partially ordered relation is introduced to\nthese groups by enforcing that groups participated in each forward pass always\nstarts from the first group to the dynamically-determined rightmost group.\nTrained by dynamically indexing the rightmost group with a single parameter\nslice rate, the network is engendered to build up group-wise and residual\nrepresentation. Then during inference, a sub-model with fewer groups can be\nreadily deployed for efficiency whose computation is roughly quadratic to the\nwidth controlled by the slice rate. Extensive experiments show that models\ntrained with model slicing can effectively support on-demand workload with\nelastic inference cost.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 08:16:24 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 08:49:08 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 07:06:20 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Shaofeng", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""], ["Gao", "Jinyang", ""]]}, {"id": "1904.02108", "submitter": "Joel Beny", "authors": "Joel Beny and Jonas Latt", "title": "Efficient LBM on GPUs for dense moving objects using immersed boundary\n  condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists an increasing interest for using immersed boundary methods\n(IBMs) (Peskin 2000) to model moving objects in computational fluid dynamics.\nIndeed, this approach is particularly efficient, because the fluid mesh does\nnot require to be body-fitted or to adjust dynamically to the motion of the\nbody. Frequently, IBMs are implemented in combination with the lattice\nBoltzmann methods (LBM) (Kr\\\"uger 2016). They fit elegantly into the framework\nof this method, and yield impressive parallel performances. It has also become\nquite common to accelerate LBM simulations with the use of Graphics Processing\nUnits (GPUs) (T\\\"olke 2010), as the underlying algorithm adjusts naturally to\nthe architecture of such platforms. It is not uncommon that speedups of an\norder of magnitude, or more, at equal financial cost or energy consumption are\nobserved, as compared to classical CPUs. IBM algorithms are however more\ndifficult to adapt to GPUs, because their complex memory access pattern\nconflicts with a GPU's strategy of broadcasting data to a large number of GPU\ncores in single memory accesses. In the existing literature, GPU\nimplementations of LBM-IBM codes are therefore restricted to situations in\nwhich the immersed surfaces are very small compared to the total number of\nfluid cells (Valero-Lara 2014), as is often the case in exterior flow\nsimulations around an obstacle. This assumption is however not valid in many\nother cases of interest.\n  We propose a new method for the implementation of a LBM-IBM on GPUs in the\nCUDA language, which allows to handle a substantially larger immersed surfaces\nwith acceptable performance than previous implementations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 12:40:06 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Beny", "Joel", ""], ["Latt", "Jonas", ""]]}, {"id": "1904.02612", "submitter": "Lenore Mullin", "authors": "Lenore Mullin, Paul Sebexen", "title": "MoA Interpretation of the Iterative Conjugate Gradient Method with Psi\n  Reduction - A Tutorial to teach the Mathematically literate in Linear and\n  Tensor Algebra: Part I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is often difficult to learn new mathematics semantically and\nsyntactically, even when there are similarities in the words and meaning when\ndiscussed aloud. The goal of this document is to facilitate learning through\nexplanations and definitions relating our common mathematical knowledge and\nhighlighting what is new. It is meant to be a working document that will evolve\nbased on feedback from target audiences, those mathematically literate in\nlinear and tensor algebra, those that want to learn MoA, Psi Calculus, and its\nuses, those that want and need the ability to prove a design, either in\nhardware or software through the ONF, Operational Normal Form, and those\nwanting to exploit all resources optimally, especially when Tensor Algebra,\ni.e. algorithms foundational to their application,are needed: Knowledge\nRepresentation, Machine Learning, Signal Processing, AI, HPC, etc.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 17:09:00 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Mullin", "Lenore", ""], ["Sebexen", "Paul", ""]]}, {"id": "1904.04174", "submitter": "John Lawson", "authors": "Rod Burns, John Lawson, Duncan McBain and Daniel Soutar", "title": "Accelerated Neural Networks on OpenCL Devices Using SYCL-DNN", "comments": "4 pages, 3 figures. In International Workshop on OpenCL (IWOCL '19),\n  May 13-15, 2019, Boston", "journal-ref": null, "doi": "10.1145/3318170.3318183", "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years machine learning has seen a renewed explosion of\ninterest, following a number of studies showing the effectiveness of neural\nnetworks in a range of tasks which had previously been considered incredibly\nhard. Neural networks' effectiveness in the fields of image recognition and\nnatural language processing stems primarily from the vast amounts of data\navailable to companies and researchers, coupled with the huge amounts of\ncompute power available in modern accelerators such as GPUs, FPGAs and ASICs.\nThere are a number of approaches available to developers for utilizing GPGPU\ntechnologies such as SYCL, OpenCL and CUDA, however many applications require\nthe same low level mathematical routines. Libraries dedicated to accelerating\nthese common routines allow developers to easily make full use of the available\nhardware without requiring low level knowledge of the hardware themselves,\nhowever such libraries are often provided by hardware manufacturers for\nspecific hardware such as cuDNN for Nvidia hardware or MIOpen for AMD hardware.\n  SYCL-DNN is a new open-source library dedicated to providing accelerated\nroutines for neural network operations which are hardware and vendor agnostic.\nBuilt on top of the SYCL open standard and written entirely in standard C++,\nSYCL-DNN allows a user to easily accelerate neural network code for a wide\nrange of hardware using a modern C++ interface. The library is tested on AMD's\nOpenCL for GPU, Intel's OpenCL for CPU and GPU, ARM's OpenCL for Mali GPUs as\nwell as ComputeAorta's OpenCL for R-Car CV engine and host CPU. In this talk we\nwill present performance figures for SYCL-DNN on this range of hardware, and\ndiscuss how high performance was achieved on such a varied set of accelerators\nwith such different hardware features.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:29:40 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Burns", "Rod", ""], ["Lawson", "John", ""], ["McBain", "Duncan", ""], ["Soutar", "Daniel", ""]]}, {"id": "1904.04279", "submitter": "Chen Yuan", "authors": "Guangyi Liu, Chen Yuan, Xi Chen, Jingjin Wu, Renchang Dai, Zhiwei Wang", "title": "A High-Performance Energy Management System based on Evolving Graph", "comments": "5 pages, 6 figures, 4 tables, accepted by IEEE Transactions on\n  Circuits and Systems II: Express Briefs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the fast growth and large integration of distributed generation, renewable\nenergy resource, energy storage system and load response, the modern power\nsystem operation becomes much more complicated with increasing uncertainties\nand frequent changes. Increased operation risks are introduced to the existing\ncommercial Energy Management System (EMS), due to its limited computational\ncapability. In this paper, a high-performance EMS analysis framework based on\nthe evolving graph is developed. A power grid is first modeled as an evolving\ngraph and then the power system dynamic analysis applications, like network\ntopology processing (NTP), state estimation (SE), power flow (PF), and\ncontingency analysis (CA), are efficiently implemented on the system evolving\ngraph to build a high-performance EMS analysis framework. Its computation\nperformance is field tested using a 2749-bus power system in Sichuan, China.\nThe results illustrate that the proposed EMS remarkably speeds up the\ncomputation performance and reaches the goal of real-time power system\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:16:07 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Liu", "Guangyi", ""], ["Yuan", "Chen", ""], ["Chen", "Xi", ""], ["Wu", "Jingjin", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1904.04702", "submitter": "Paul Ezhilchelvan", "authors": "Jim Webber, Paul Ezhilchelvan and Isi Mitrani", "title": "Modeling Corruption in Eventually-Consistent Graph Databases", "comments": "6 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a model and analysis of an eventually consistent graph database\nwhere loosely cooperating servers accept concurrent updates to a partitioned,\ndistributed graph. The model is high-fidelity and preserves design choices from\ncontemporary graph database management systems. To explore the problem space,\nwe use two common graph topologies as data models for realistic\nexperimentation. The analysis reveals, even assuming completely fault-free\nhardware and bug-free software, that if it is possible for updates to interfere\nwith one-another, corruption will occur and spread significantly through the\ngraph within the production database lifetime. Using our model, database\ndesigners and operators can compute the rate of corruption for their systems\nand determine whether they are sufficiently dependable for their intended use.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:35:42 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Webber", "Jim", ""], ["Ezhilchelvan", "Paul", ""], ["Mitrani", "Isi", ""]]}, {"id": "1904.05040", "submitter": "Brendan Patch Dr", "authors": "Brendan Patch, Mark S. Squillante, Peter M. Van de Ven", "title": "Optimisation of stochastic networks with blocking: a functional-form\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a class of stochastic networks with blocking, motivated\nby applications arising in cellular network planning, mobile cloud computing,\nand spare parts supply chains. Blocking results in lost revenue due to\ncustomers or jobs being permanently removed from the system. We are interested\nin striking a balance between mitigating blocking by increasing service\ncapacity, and maintaining low costs for service capacity. This problem is\nfurther complicated by the stochastic nature of the system. Owing to the\ncomplexity of the system there are no analytical results available that\nformulate and solve the relevant optimization problem in closed form.\nTraditional simulation-based methods may work well for small instances, but the\nassociated computational costs are prohibitive for networks of realistic size.\n  We propose a hybrid functional-form based approach for finding the optimal\nresource allocation, combining the speed of an analytical approach with the\naccuracy of simulation-based optimisation. The key insight is to replace the\ncomputationally expensive gradient estimation in simulation optimisation with a\nclosed-form analytical approximation that is calibrated using a single\nsimulation run. We develop two implementations of this approach and conduct\nextensive computational experiments on complex examples to show that it is\ncapable of substantially improving system performance. We also provide evidence\nthat our approach has substantially lower computational costs compared to\nstochastic approximation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 07:55:30 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 21:42:33 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Patch", "Brendan", ""], ["Squillante", "Mark S.", ""], ["Van de Ven", "Peter M.", ""]]}, {"id": "1904.05347", "submitter": "Mehdi Goli", "authors": "John Lawson, Mehdi Goli, Duncan McBain, Daniel Soutar, Louis Sugy", "title": "Cross-Platform Performance Portability Using Highly Parametrized SYCL\n  Kernels", "comments": "11 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over recent years heterogeneous systems have become more prevalent across HPC\nsystems, with over 100 supercomputers in the TOP500 incorporating GPUs or other\naccelerators. These hardware platforms have different performance\ncharacteristics and optimization requirements. In order to make the most of\nmultiple accelerators a developer has to provide implementations of their\nalgorithms tuned for each device. Hardware vendors provide libraries targeting\ntheir devices specifically, which provide good performance but frequently have\ndifferent API designs, hampering portability.\n  The SYCL programming model allows users to write heterogeneous programs using\ncompletely standard C++, and so developers have access to the power of C++\ntemplates when developing compute kernels. In this paper we show that by\nwriting highly parameterized kernels for matrix multiplies and convolutions we\nachieve performance competitive with vendor implementations across different\narchitectures. Furthermore, tuning for new devices amounts to choosing the\ncombinations of kernel parameters that perform best on the hardware.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:58:23 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Lawson", "John", ""], ["Goli", "Mehdi", ""], ["McBain", "Duncan", ""], ["Soutar", "Daniel", ""], ["Sugy", "Louis", ""]]}, {"id": "1904.05456", "submitter": "Mohammad Hosseini", "authors": "Boyang Peng, Mohammad Hosseini, Zhihao Hong, Reza Farivar, Roy\n  Campbell", "title": "R-Storm: Resource-Aware Scheduling in Storm", "comments": "Proceedings of the 16th Annual ACM Middleware Conference, Pages\n  149-161, Vancouver, BC, Canada, December 07 - 11, 2015", "journal-ref": "In Proceedings of the 16th Annual Middleware Conference\n  (Middleware 2015). ACM, New York, NY, USA, 149-161", "doi": "10.1145/2814576.2814808", "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The era of big data has led to the emergence of new systems for real-time\ndistributed stream processing, e.g., Apache Storm is one of the most popular\nstream processing systems in industry today. However, Storm, like many other\nstream processing systems lacks an intelligent scheduling mechanism. The\ndefault round-robin scheduling currently deployed in Storm disregards resource\ndemands and availability, and can therefore be inefficient at times. We present\nR-Storm (Resource-Aware Storm), a system that implements resource-aware\nscheduling within Storm. R-Storm is designed to increase overall throughput by\nmaximizing resource utilization while minimizing network latency. When\nscheduling tasks, R-Storm can satisfy both soft and hard resource constraints\nas well as minimizing network distance between components that communicate with\neach other. We evaluate R-Storm on set of micro-benchmark Storm applications as\nwell as Storm applications used in production at Yahoo! Inc. From our\nexperimental results we conclude that R-Storm achieves 30-47% higher throughput\nand 69-350% better CPU utilization than default Storm for the micro-benchmarks.\nFor the Yahoo! Storm applications, R-Storm outperforms default Storm by around\n50% based on overall throughput. We also demonstrate that R-Storm performs much\nbetter when scheduling multiple Storm applications than default Storm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:45:39 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Peng", "Boyang", ""], ["Hosseini", "Mohammad", ""], ["Hong", "Zhihao", ""], ["Farivar", "Reza", ""], ["Campbell", "Roy", ""]]}, {"id": "1904.05615", "submitter": "Fabrice Guillemin", "authors": "Fabrice Guillemin, Veronica Quintuna Rodriguez, Alain Simonian", "title": "A Processor-Sharing model for the Performance of Virtualized Network\n  Functions", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parallel execution of requests in a Cloud Computing platform, as for\nVirtualized Network Functions, is modeled by an $M^{[X]}/M/1$ Processor-Sharing\n(PS) system, where each request is seen as a batch of unit jobs. The\nperformance of such paralleled system can then be measured by the quantiles of\nthe batch sojourn time distribution. In this paper, we address the evaluation\nof this distribution for the $M^{[X]}/M/1$-PS queue with batch arrivals and\ngeometrically distributed batch size. General results on the residual busy\nperiod (after a tagged batch arrival time) and the number of unit jobs served\nduring this residual busy period are first derived. This enables us to provide\nan approximation for the distribution tail of the batch sojourn time whose\naccuracy is confirmed by simulation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 10:32:02 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Guillemin", "Fabrice", ""], ["Rodriguez", "Veronica Quintuna", ""], ["Simonian", "Alain", ""]]}, {"id": "1904.05654", "submitter": "Fabrice Guillemin", "authors": "Fabrice Guillemin and Veronica Quintuna Rodriguez", "title": "On the sojourn of an arbitrary customer in an $M/M/1$ Processor Sharing\n  Queue", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the number of both arrivals and departures seen by\na tagged customer while in service in a classical $M/M/1$ processor sharing\nqueue. By exploiting the underlying orthogonal structure of this queuing system\nrevealed in an earlier study, we compute the distributions of these two\nquantities and prove that they are equal in distribution. We moreover derive\nthe asymptotic behavior of this common distribution. The knowledge of the\nnumber of departures seen by a tagged customer allows us to test the validity\nof an approximation, which consists of assuming that the tagged customer is\nrandomly served among those customers in the residual busy period of the queue\nfollowing the arrival of the tagged customer. A numerical evidence shows that\nthis approximation is reasonable for moderate values of the number of\ndepartures, given that the asymptotic behaviors of the distributions are very\ndifferent even if the exponential decay rates are equal.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 12:11:13 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Guillemin", "Fabrice", ""], ["Rodriguez", "Veronica Quintuna", ""]]}, {"id": "1904.05838", "submitter": "Amanda Bienz", "authors": "Amanda Bienz, Luke Olson, William Gropp", "title": "Reducing Communication in Algebraic Multigrid with Multi-step Node Aware\n  Communication", "comments": "11 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic multigrid (AMG) is often viewed as a scalable $\\mathcal{O}(n)$\nsolver for sparse linear systems. Yet, parallel AMG lacks scalability due to\nincreasingly large costs associated with communication, both in the initial\nconstruction of a multigrid hierarchy as well as the iterative solve phase.\nThis work introduces a parallel implementation of AMG to reduce the cost of\ncommunication, yielding an increase in scalability. Standard inter-process\ncommunication consists of sending data regardless of the send and receive\nprocess locations. Performance tests show notable differences in the cost of\nintra- and inter-node communication, motivating a restructuring of\ncommunication. In this case, the communication schedule takes advantage of the\nless costly intra-node communication, reducing both the number and size of\ninter-node messages. Node-centric communication extends to the range of\ncomponents in both the setup and solve phase of AMG, yielding an increase in\nthe weak and strong scalability of the entire method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:43:11 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 12:27:20 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Bienz", "Amanda", ""], ["Olson", "Luke", ""], ["Gropp", "William", ""]]}, {"id": "1904.05924", "submitter": "George Kesidis", "authors": "George Kesidis, Takis Konstantopoulos, Michael Zazanis", "title": "The distribution of age-of-information performance measures for message\n  processing systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea behind the recently introduced \"age of information\" performance\nmeasure of a networked message processing system is that it indicates our\nknowledge regarding the \"freshness\" of the most recent piece of information\nthat can be used as a criterion for real-time control. In this foundational\npaper, we examine two such measures, one that has been extensively studied in\nthe recent literature and a new one that could be more relevant from the point\nof view of the processor. Considering these measures as stochastic processes in\na stationary environment (defined by the arrival processes, message processing\ntimes and admission controls in bufferless systems), we characterize their\ndistributions using the Palm inversion formula. Under renewal assumptions we\nderive explicit solutions for their Laplace transforms and show some\ninteresting decomposition properties. Previous work has mostly focused on\ncomputation of expectations in very particular cases. We argue that using\nbufferless or very small buffer systems is best and support this by simulation.\nWe also pose some open problems including assessment of enqueueing policies\nthat may be better in cases where one wishes to minimize more general\nfunctionals of the age of information measures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 18:55:55 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 21:30:00 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Kesidis", "George", ""], ["Konstantopoulos", "Takis", ""], ["Zazanis", "Michael", ""]]}, {"id": "1904.05980", "submitter": "Issam Damaj", "authors": "Issam Damaj", "title": "Parallel algorithms development for programmable logic devices", "comments": "47 Pages, 25 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:1904.03756, arXiv:1904.05437", "journal-ref": "Advances in Engineering Software, Elsevier. 37 (2006) 561-582", "doi": "10.1016/j.advengsoft.2006.01.009", "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable Logic Devices (PLDs) continue to grow in size and currently\ncontain several millions of gates. At the same time, research effort is going\ninto higher-level hardware synthesis methodologies for reconfigurable computing\nthat can exploit PLD technology. In this paper, we explore the effectiveness\nand extend one such formal methodology in the design of massively parallel\nalgorithms. We take a step-wise refinement approach to the development of\ncorrect reconfigurable hardware circuits from formal specifications. A\nfunctional programming notation is used for specifying algorithms and for\nreasoning about them. The specifications are realised through the use of a\ncombination of function decomposition strategies, data refinement techniques,\nand off-the-shelf refinements based upon higher-order functions. The\noff-the-shelf refinements are inspired by the operators of Communicating\nSequential Processes (CSP) and map easily to programs in Handel-C (a hardware\ndescription language). The Handel-C descriptions are directly compiled into\nreconfigurable hardware. The practical realisation of this methodology is\nevidenced by a case studying the matrix multiplication algorithm as it is\nrelatively simple and well known. In this paper, we obtain several hardware\nimplementations with different performance characteristics by applying\ndifferent refinements to the algorithm. The developed designs are compiled and\ntested under Celoxica's RC-1000 reconfigurable computer with its 2 million\ngates Virtex-E FPGA. Performance analysis and evaluation of these\nimplementations are included.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 13:48:08 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Damaj", "Issam", ""]]}, {"id": "1904.06399", "submitter": "Leonel Merino", "authors": "Leonel Merino, Mario Hess, Alexandre Bergel, Oscar Nierstrasz, Daniel\n  Weiskopf", "title": "PerfVis: Pervasive Visualization in Immersive AugmentedReality for\n  Performance Awareness", "comments": "ICPE'19 vision, 4 pages, 2 figure, conference", "journal-ref": null, "doi": "10.1145/3302541.3313104", "report-no": null, "categories": "cs.HC cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Developers are usually unaware of the impact of code changes to the\nperformance of software systems. Although developers can analyze the\nperformance of a system by executing, for instance, a performance test to\ncompare the performance of two consecutive versions of the system, changing\nfrom a programming task to a testing task would disrupt the development flow.\nIn this paper, we propose the use of a city visualization that dynamically\nprovides developers with a pervasive view of the continuous performance of a\nsystem. We use an immersive augmented reality device (Microsoft HoloLens) to\ndisplay our visualization and extend the integrated development environment on\na computer screen to use the physical space. We report on technical details of\nthe design and implementation of our visualization tool, and discuss early\nfeedback that we collected of its usability. Our investigation explores a new\nvisual metaphor to support the exploration and analysis of possibly very large\nand multidimensional performance data. Our initial result indicates that the\ncity metaphor can be adequate to analyze dynamic performance data on a large\nand non-trivial software system.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:59:01 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Merino", "Leonel", ""], ["Hess", "Mario", ""], ["Bergel", "Alexandre", ""], ["Nierstrasz", "Oscar", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "1904.06480", "submitter": "Jayakrishnan Nair", "authors": "Kiran Chaudhary, Veeraruna Kavitha, Jayakrishnan Nair", "title": "Dynamic scheduling in a partially fluid, partially lossy queueing system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a single server queueing system with two classes of jobs: eager\njobs with small sizes that require service to begin almost immediately upon\narrival, and tolerant jobs with larger sizes that can wait for service. While\nblocking probability is the relevant performance metric for the eager class,\nthe tolerant class seeks to minimize its mean sojourn time. In this paper, we\ndiscuss the performance of each class under dynamic scheduling policies, where\nthe scheduling of both classes depends on the instantaneous state of the\nsystem. This analysis is carried out under a certain fluid limit, where the\narrival rate and service rate of the eager class are scaled to infinity,\nholding the offered load constant. Our performance characterizations reveal a\n(dynamic) pseudo-conservation law that ties the performance of both the classes\nto the standalone blocking probabilities of the eager class. Further, the\nperformance is robust to other specifics of the scheduling policies. We also\ncharacterize the Pareto frontier of the achievable region of performance\nvectors under the same fluid limit, and identify a (two-parameter) class of\nPareto-complete scheduling policies.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 04:40:12 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 07:53:29 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Chaudhary", "Kiran", ""], ["Kavitha", "Veeraruna", ""], ["Nair", "Jayakrishnan", ""]]}, {"id": "1904.06825", "submitter": "Suraj Kumar", "authors": "Suraj Kumar, Lionel Eyraud-Dubois, Sriram Krishnamoorthy", "title": "Performance Models for Data Transfers: A Case Study with Molecular\n  Chemistry Kernels", "comments": null, "journal-ref": "https://www.hpcs.cs.tsukuba.ac.jp/icpp2019/", "doi": "10.1145/3337821.3337921", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing complexity of hardwares, systems with different memory nodes\nare ubiquitous in High Performance Computing (HPC). It is paramount to develop\nstrategies to overlap the data transfers between memory nodes with computations\nin order to exploit the full potential of these systems. In this article, we\nconsider the problem of deciding the order of data transfers between two memory\nnodes for a set of independent tasks with the objective to minimize the\nmakespan. We prove that with limited memory capacity, obtaining the optimal\norder of data transfers is a NP-complete problem. We propose several heuristics\nfor this problem and provide details about their favorable situations. We\npresent an analysis of our heuristics on traces, obtained by running 2\nmolecular chemistry kernels, namely, Hartree-Fock (HF) and Coupled Cluster\nSingle Double (CCSD) on 10 nodes of an HPC system. Our results show that some\nof our heuristics achieve significant overlap for moderate memory capacities\nand are very close to the lower bound of makespan.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 03:13:13 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Kumar", "Suraj", ""], ["Eyraud-Dubois", "Lionel", ""], ["Krishnamoorthy", "Sriram", ""]]}, {"id": "1904.07061", "submitter": "Laith Sakka", "authors": "Laith Sakka, Kirshanthan Sundararajah, Ryan R. Newton, Milind Kulkarni", "title": "Sound, Fine-Grained Traversal Fusion for Heterogeneous Trees - Extended\n  Version", "comments": "Extended version of \"Sound Fine-Grained Traversal Fusion for\n  Heterogeneous Trees\" Sakka et al., PLDI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in many domains are based on a series of traversals of tree\nstructures, and fusing these traversals together to reduce the total number of\npasses over the tree is a common, important optimization technique. In\napplications such as compilers and render trees, these trees are heterogeneous:\ndifferent nodes of the tree have different types. Unfortunately, prior work for\nfusing traversals falls short in different ways: they do not handle\nheterogeneity; they require using domain-specific languages to express an\napplication; they rely on the programmer to aver that fusing traversals is\nsafe, without any soundness guarantee; or they can only perform coarse-grain\nfusion, leading to missed fusion opportunities. This paper addresses these\nshortcomings to build a framework for fusing traversals of heterogeneous trees\nthat is automatic, sound, and fine-grained. We show across several case studies\nthat our approach is able to allow programmers to write simple, intuitive\ntraversals, and then automatically fuse them to substantially improve\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 18:36:58 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Sakka", "Laith", ""], ["Sundararajah", "Kirshanthan", ""], ["Newton", "Ryan R.", ""], ["Kulkarni", "Milind", ""]]}, {"id": "1904.07141", "submitter": "Gleb Polevoy", "authors": "Gleb Polevoy", "title": "Defence Efficiency", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to automate actions, such as defences against network attacks, one\nneeds to quantify their efficiency. This can subsequently be used in\npost-evaluation, learning, etc. In order to quantify the defence efficiency as\na function of the impact of the defence and its total cost, we present several\nnatural requirements from such a definition of efficiency and provide a natural\ndefinition that complies with these requirements. Next, we precisely\ncharacterize our definition of efficiency by the axiomatic approach; namely, we\nstrengthen the original requirements from such a definition and prove that the\ngiven definition is the unique definition that satisfies those requirements.\nFinally, we generalize the definition to the case of any number of input\nvariables in two natural ways, and compare these generalizations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 22:59:06 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Polevoy", "Gleb", ""]]}, {"id": "1904.07714", "submitter": "Yung-Hsiang Lu", "authors": "Sergei Alyamkin, Matthew Ardi, Alexander C. Berg, Achille Brighton, Bo\n  Chen, Yiran Chen, Hsin-Pai Cheng, Zichen Fan, Chen Feng, Bo Fu, Kent Gauen,\n  Abhinav Goel, Alexander Goncharenko, Xuyang Guo, Soonhoi Ha, Andrew Howard,\n  Xiao Hu, Yuanjun Huang, Donghyun Kang, Jaeyoun Kim, Jong Gook Ko, Alexander\n  Kondratyev, Junhyeok Lee, Seungjae Lee, Suwoong Lee, Zichao Li, Zhiyu Liang,\n  Juzheng Liu, Xin Liu, Yang Lu, Yung-Hsiang Lu, Deeptanshu Malik, Hong Hanh\n  Nguyen, Eunbyung Park, Denis Repin, Liang Shen, Tao Sheng, Fei Sun, David\n  Svitov, George K. Thiruvathukal, Baiwu Zhang, Jingchi Zhang, Xiaopeng Zhang,\n  Shaojie Zhuo", "title": "Low-Power Computer Vision: Status, Challenges, Opportunities", "comments": "Preprint, Accepted by IEEE Journal on Emerging and Selected Topics in\n  Circuits and Systems. arXiv admin note: substantial text overlap with\n  arXiv:1810.01732", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has achieved impressive progress in recent years. Meanwhile,\nmobile phones have become the primary computing platforms for millions of\npeople. In addition to mobile phones, many autonomous systems rely on visual\ndata for making decisions and some of these systems have limited energy (such\nas unmanned aerial vehicles also called drones and mobile robots). These\nsystems rely on batteries and energy efficiency is critical. This article\nserves two main purposes: (1) Examine the state-of-the-art for low-power\nsolutions to detect objects in images. Since 2015, the IEEE Annual\nInternational Low-Power Image Recognition Challenge (LPIRC) has been held to\nidentify the most energy-efficient computer vision solutions. This article\nsummarizes 2018 winners' solutions. (2) Suggest directions for research as well\nas opportunities for low-power computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:48:48 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Alyamkin", "Sergei", ""], ["Ardi", "Matthew", ""], ["Berg", "Alexander C.", ""], ["Brighton", "Achille", ""], ["Chen", "Bo", ""], ["Chen", "Yiran", ""], ["Cheng", "Hsin-Pai", ""], ["Fan", "Zichen", ""], ["Feng", "Chen", ""], ["Fu", "Bo", ""], ["Gauen", "Kent", ""], ["Goel", "Abhinav", ""], ["Goncharenko", "Alexander", ""], ["Guo", "Xuyang", ""], ["Ha", "Soonhoi", ""], ["Howard", "Andrew", ""], ["Hu", "Xiao", ""], ["Huang", "Yuanjun", ""], ["Kang", "Donghyun", ""], ["Kim", "Jaeyoun", ""], ["Ko", "Jong Gook", ""], ["Kondratyev", "Alexander", ""], ["Lee", "Junhyeok", ""], ["Lee", "Seungjae", ""], ["Lee", "Suwoong", ""], ["Li", "Zichao", ""], ["Liang", "Zhiyu", ""], ["Liu", "Juzheng", ""], ["Liu", "Xin", ""], ["Lu", "Yang", ""], ["Lu", "Yung-Hsiang", ""], ["Malik", "Deeptanshu", ""], ["Nguyen", "Hong Hanh", ""], ["Park", "Eunbyung", ""], ["Repin", "Denis", ""], ["Shen", "Liang", ""], ["Sheng", "Tao", ""], ["Sun", "Fei", ""], ["Svitov", "David", ""], ["Thiruvathukal", "George K.", ""], ["Zhang", "Baiwu", ""], ["Zhang", "Jingchi", ""], ["Zhang", "Xiaopeng", ""], ["Zhuo", "Shaojie", ""]]}, {"id": "1904.07813", "submitter": "Milan Yadav", "authors": "Milan Yadav, Kanak Khanna", "title": "Energy Saving Strategy Based on Profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraints imposed by power consumption and the related costs are one of the\nkey roadblocks to the design and development of next generation exascale\nsystems. To mitigate these issues, strategies that reduce the power consumption\nof the processor are the need of the hour. Techniques such as Dynamic Voltage\nand Frequency Scaling (DVFS) exist which reduce the power consumption of a\nprocessor at runtime but they should be used in such a manner so that their\noverhead does not hamper application performance. In this paper, we propose an\nenergy saving strategy which operates on timeslice basis to apply DVFS under a\nuser defined performance constraint. Results show energy savings up to 7% when\nNAS benchmarks are tested on a laptop platform\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 07:58:51 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yadav", "Milan", ""], ["Khanna", "Kanak", ""]]}, {"id": "1904.08283", "submitter": "Alain Simonian", "authors": "Ridha Nasri, Alain Simonian, and Fabrice Guillemin", "title": "Inversion formula with hypergeometric polynomials and its application to\n  an integral equation", "comments": "22 pages, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.DM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any complex parameters $x$ and $\\nu$, we provide a new class of linear\ninversion formulas $T = A(x,\\nu) \\cdot S \\Leftrightarrow S = B(x,\\nu) \\cdot T$\nbetween sequences $S = (S_n)_{n \\in \\mathbb{N}^*}$ and $T = (T_n)_{n \\in\n\\mathbb{N}^*}$, where the infinite lower-triangular matrix $A(x,\\nu)$ and its\ninverse $B(x,\\nu)$ involve Hypergeometric polynomials $F(\\cdot)$, namely $$\n  \\left\\{\n  \\begin{array}{ll}\n  A_{n,k}(x,\\nu) = \\displaystyle (-1)^k\\binom{n}{k}F(k-n,-n\\nu;-n;x),\n  \\\\\n  B_{n,k}(x,\\nu) = \\displaystyle (-1)^k\\binom{n}{k}F(k-n,k\\nu;k;x)\n  \\end{array} \\right. $$ for $1 \\leqslant k \\leqslant n$. Functional relations\nbetween the ordinary (resp. exponential) generating functions of the related\nsequences $S$ and $T$ are also given.\n  These new inversion formulas have been initially motivated by the resolution\nof an integral equation recently appeared in the field of Queuing Theory; we\napply them to the full resolution of this integral equation. Finally, matrices\ninvolving generalized Laguerre polynomials polynomials are discussed as\nspecific cases of our general inversion scheme.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:23:32 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Nasri", "Ridha", ""], ["Simonian", "Alain", ""], ["Guillemin", "Fabrice", ""]]}, {"id": "1904.08762", "submitter": "Ahsan Javed Awan Dr", "authors": "Stefano Corda, Gagandeep Singh, Ahsan Javed Awan, Roel Jordans and\n  Henk Corporaal", "title": "Memory and Parallelism Analysis Using a Platform-Independent Approach", "comments": "22nd ACM International Workshop on Software and Compilers for\n  Embedded Systems (SCOPES '19), May 2019", "journal-ref": null, "doi": "10.1145/3323439.3323988", "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging computing architectures such as near-memory computing (NMC) promise\nimproved performance for applications by reducing the data movement between CPU\nand memory. However, detecting such applications is not a trivial task. In this\nongoing work, we extend the state-of-the-art platform-independent software\nanalysis tool with NMC related metrics such as memory entropy, spatial\nlocality, data-level, and basic-block-level parallelism. These metrics help to\nidentify the applications more suitable for NMC architectures.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:25:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Corda", "Stefano", ""], ["Singh", "Gagandeep", ""], ["Awan", "Ahsan Javed", ""], ["Jordans", "Roel", ""], ["Corporaal", "Henk", ""]]}, {"id": "1904.09538", "submitter": "James Stevens", "authors": "James D. Stevens, Andreas Kl\\\"ockner", "title": "A mechanism for balancing accuracy and scope in cross-machine black-box\n  GPU performance modeling", "comments": "25 pages, 9 figures", "journal-ref": "The International Journal of High Performance Computing\n  Applications, June 2020", "doi": "10.1177/1094342020921340", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to model, analyze, and predict execution time of computations is\nan important building block supporting numerous efforts, such as load\nbalancing, performance optimization, and automated performance tuning for high\nperformance, parallel applications. In today's increasingly heterogeneous\ncomputing environment, this task must be accomplished efficiently across\nmultiple architectures, including massively parallel coprocessors like GPUs. To\naddress this challenge, we present an approach for constructing customizable,\ncross-machine performance models for GPU kernels, including a mechanism to\nautomatically and symbolically gather performance-relevant kernel operation\ncounts, a tool for formulating mathematical models using these counts, and a\ncustomizable parameterized collection of benchmark kernels used to calibrate\nmodels to GPUs in a black-box fashion. Our approach empowers a user to manage\ntrade-offs between model accuracy, evaluation speed, and generalizability. A\nuser can define a model and customize the calibration process, making it as\nsimple or complex as desired, and as application-targeted or general as\ndesired. To evaluate our approach, we demonstrate both linear and nonlinear\nmodels; each example models execution times for multiple variants of a\nparticular computation: two matrix multiplication variants, four Discontinuous\nGalerkin (DG) differentiation operation variants, and two 2-D five-point finite\ndifference stencil variants. For each variant, we present accuracy results on\nGPUs from multiple vendors and hardware generations. We view this customizable\napproach as a response to a central question in GPU performance modeling: how\ncan we model GPU performance in a cost-explanatory fashion while maintaining\naccuracy, evaluation speed, portability, and ease of use, an attribute we\nbelieve precludes manual collection of kernel or hardware statistics.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 03:50:20 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 20:57:09 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 15:46:53 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Stevens", "James D.", ""], ["Kl\u00f6ckner", "Andreas", ""]]}, {"id": "1904.11423", "submitter": "Sebastian Gallenm\\\"uller", "authors": "Sebastian Gallenm\\\"uller, Dominik Sch\\\"offmann, Dominik Scholz, Fabien\n  Geyer, and Georg Carle", "title": "DTLS Performance - How Expensive is Security?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure communication is an integral feature of many Internet services. The\nwidely deployed TLS protects reliable transport protocols. DTLS extends TLS\nsecurity services to protocols relying on plain UDP packet transport, such as\nVoIP or IoT applications. In this paper, we construct a model to determine the\nperformance of generic DTLS-enabled applications. Our model considers basic\nnetwork characteristics, e.g., number of connections, and the chosen security\nparameters, e.g., the encryption algorithm in use. Measurements are presented\ndemonstrating the applicability of our model. These experiments are performed\nusing a high-performance DTLS-enabled VPN gateway built on top of the\nwell-established libraries DPDK and OpenSSL. This VPN solution represents the\nmost essential parts of DTLS, creating a DTLS performance baseline. Using this\nbaseline the model can be extended to predict even more complex DTLS protocols\nbesides the measured VPN. Code and measured data used in this paper are\npublicly available at https://git.io/MoonSec and https://git.io/Sdata.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 15:57:34 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Gallenm\u00fcller", "Sebastian", ""], ["Sch\u00f6ffmann", "Dominik", ""], ["Scholz", "Dominik", ""], ["Geyer", "Fabien", ""], ["Carle", "Georg", ""]]}, {"id": "1904.11965", "submitter": "Michael J(\\\"U)Nger", "authors": "Michael Juenger, Elisabeth Lobe, Petra Mutzel, Gerhard Reinelt, Franz\n  Rendl, Giovanni Rinaldi, Tobias Stollenwerk", "title": "Performance of a Quantum Annealer for Ising Ground State Computations on\n  Chimera Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum annealing is getting increasing attention in combinatorial\noptimization. The quantum processing unit by D-Wave is constructed to\napproximately solve Ising models on so-called Chimera graphs. Ising models are\nequivalent to quadratic unconstrained binary optimization (QUBO) problems and\nmaximum cut problems on the associated graphs. We have tailored branch-and-cut\nas well as semidefinite programming algorithms for solving Ising models for\nChimera graphs to provable optimality and use the strength of these approaches\nfor comparing our solution values to those obtained on the current quantum\nannealing machine D-Wave 2000Q. This allows for the assessment of the quality\nof solutions produced by the D-Wave hardware. It has been a matter of\ndiscussion in the literature how well the D-Wave hardware performs at its\nnative task, and our experiments shed some more light on this issue.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 12:39:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Juenger", "Michael", ""], ["Lobe", "Elisabeth", ""], ["Mutzel", "Petra", ""], ["Reinelt", "Gerhard", ""], ["Rendl", "Franz", ""], ["Rinaldi", "Giovanni", ""], ["Stollenwerk", "Tobias", ""]]}, {"id": "1904.12660", "submitter": "Chao-Yang Chen", "authors": "Chao-Yang Chen, Weihua Gui, Lianghong Wu, Zhaohua Liu, Huaicheng Yan", "title": "Tracking Performance Limitations of MIMO Networked Control Systems with\n  Multiple Communication Constraints", "comments": "This paper give a fundament performance limitation of networked\n  control systems with multiple communication constraints", "journal-ref": null, "doi": "10.1109/TCYB.2019.2912973", "report-no": null, "categories": "cs.SY cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the tracking performance limitation of networked control\nsystems (NCSs) is studied. The NCSs is considered as continuous-time linear\nmulti-input multi-output (MIMO) systems with random reference noises. The\ncontrolled plants include unstable poles and non-minimum phase (NMP) zeros. The\noutput feedback path is affected by multiple communication constraints. We\nfocus on some basic communication constraints, including additive white noise\n(AWN), quantization noise, bandwidth, as well as encoder-decoder. The system\nperformance is evaluated with the tracking error energy, and used a two-degree\nof freedom (2DOF) controller. The explicit representation of the tracking\nperformance is given in this paper. The results indicate the tracking\nperformance limitations rely to internal characteristics of the plant (unstable\npoles and NMP zeros), reference noises (the reference noise power distribution\n(RNPD) and its directions) and the characteristics of communication\nconstraints. Moreover, the tracking performance limitations are also affected\nby the angles between the each transform NMP zero direction and RNPD direction,\nand these angles between each transform unstable poles direction and the\ndirection of communication constraint distribution/allocation. In addition, for\nMIMO NCSs, bandwidth (there are not identical two channels) always can affects\nthe direction of unstable poles, and the channel allocation of bandwidth and\nencode-decode may be used for a feasible method for the performance allocation\nof each channels. Lastly, a instance is given for verifying the effectiveness\nof the theoretical outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 21:25:58 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Chen", "Chao-Yang", ""], ["Gui", "Weihua", ""], ["Wu", "Lianghong", ""], ["Liu", "Zhaohua", ""], ["Yan", "Huaicheng", ""]]}, {"id": "1904.12676", "submitter": "Mauricio Fadel Argerich", "authors": "M. Fadel Argerich, B. Cheng, J. F\\\"urst", "title": "Reinforcement Learning Based Orchestration for Elastic Services", "comments": "2019 IEEE 5th World Forum on Internet of Things (WF-IoT), 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the highly variable execution context in which edge services run,\nadapting their behavior to the execution context is crucial to comply with\ntheir requirements. However, adapting service behavior is a challenging task\nbecause it is hard to anticipate the execution contexts in which it will be\ndeployed, as well as assessing the impact that each behavior change will\nproduce. In order to provide this adaptation efficiently, we propose a\nReinforcement Learning (RL) based Orchestration for Elastic Services. We\nimplement and evaluate this approach by adapting an elastic service in\ndifferent simulated execution contexts and comparing its performance to a\nHeuristics based approach. We show that elastic services achieve high precision\nand requirement satisfaction rates while creating an overhead of less than 0.5%\nto the overall service. In particular, the RL approach proves to be more\nefficient than its rule-based counterpart; yielding a 10 to 25% higher\nprecision while being 25% less computationally expensive.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 10:20:34 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Argerich", "M. Fadel", ""], ["Cheng", "B.", ""], ["F\u00fcrst", "J.", ""]]}, {"id": "1904.12974", "submitter": "Fabrizio Romano Genovese PhD", "authors": "Fabrizio Genovese, Alex Gryzlov, Jelle Herold, Marco Perone, Erik\n  Post, Andr\\'e Videla", "title": "Computational Petri Nets: Adjunctions Considered Harmful", "comments": "29 Pages, 6 Figures, 1 Listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review some of the endeavors in trying to connect Petri nets with free\nsymmetric monoidal categories. We give a list of requirement such connections\nshould respect if they are meant to be useful for practical/implementation\npurposes. We show how previous approaches do not satisfy them, and give\ncompelling evidence that this depends on trying to make the correspondence\nfunctorial in the direction from nets to free symmetric monoidal categories, in\norder to produce an adjunction. We show that dropping this immediately honors\nour desiderata, and conclude by introducing an Idris library which implements\nthem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 22:17:09 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 20:06:14 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Genovese", "Fabrizio", ""], ["Gryzlov", "Alex", ""], ["Herold", "Jelle", ""], ["Perone", "Marco", ""], ["Post", "Erik", ""], ["Videla", "Andr\u00e9", ""]]}]