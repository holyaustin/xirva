[{"id": "2012.00211", "submitter": "Ying-Chiao Liao", "authors": "Chuan-Chi Wang, Ying-Chiao Liao, Ming-Chang Kao, Wen-Yew Liang,\n  Shih-Hao Hung", "title": "Toward Accurate Platform-Aware Performance Modeling for Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a fine-grain machine learning-based method,\nPerfNetV2, which improves the accuracy of our previous work for modeling the\nneural network performance on a variety of GPU accelerators. Given an\napplication, the proposed method can be used to predict the inference time and\ntraining time of the convolutional neural networks used in the application,\nwhich enables the system developer to optimize the performance by choosing the\nneural networks and/or incorporating the hardware accelerators to deliver\nsatisfactory results in time. Furthermore, the proposed method is capable of\npredicting the performance of an unseen or non-existing device, e.g. a new GPU\nwhich has a higher operating frequency with less processor cores, but more\nmemory capacity. This allows a system developer to quickly search the hardware\ndesign space and/or fine-tune the system configuration. Compared to the\nprevious works, PerfNetV2 delivers more accurate results by modeling detailed\nhost-accelerator interactions in executing the full neural networks and\nimproving the architecture of the machine learning model used in the predictor.\nOur case studies show that PerfNetV2 yields a mean absolute percentage error\nwithin 13.1% on LeNet, AlexNet, and VGG16 on NVIDIA GTX-1080Ti, while the error\nrate on a previous work published in ICBD 2018 could be as large as 200%.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 01:42:23 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Wang", "Chuan-Chi", ""], ["Liao", "Ying-Chiao", ""], ["Kao", "Ming-Chang", ""], ["Liang", "Wen-Yew", ""], ["Hung", "Shih-Hao", ""]]}, {"id": "2012.00217", "submitter": "Roel Van Beeumen", "authors": "Roel Van Beeumen and Khaled Z. Ibrahim and Gregory D. Kahanamoku-Meyer\n  and Norman Y. Yao and Chao Yang", "title": "Enhancing Scalability of a Matrix-Free Eigensolver for Studying\n  Many-Body Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [Van Beeumen, et. al, HPC Asia 2020,\nhttps://www.doi.org/10.1145/3368474.3368497] a scalable and matrix-free\neigensolver was proposed for studying the many-body localization (MBL)\ntransition of two-level quantum spin chain models with nearest-neighbor $XX+YY$\ninteractions plus $Z$ terms. This type of problem is computationally\nchallenging because the vector space dimension grows exponentially with the\nphysical system size, and averaging over different configurations of the random\ndisorder is needed to obtain relevant statistical behavior. For each eigenvalue\nproblem, eigenvalues from different regions of the spectrum and their\ncorresponding eigenvectors need to be computed. Traditionally, the interior\neigenstates for a single eigenvalue problem are computed via the\nshift-and-invert Lanczos algorithm. Due to the extremely high memory footprint\nof the LU factorizations, this technique is not well suited for large number of\nspins $L$, e.g., one needs thousands of compute nodes on modern high\nperformance computing infrastructures to go beyond $L = 24$. The matrix-free\napproach does not suffer from this memory bottleneck, however, its scalability\nis limited by a computation and communication imbalance. We present a few\nstrategies to reduce this imbalance and to significantly enhance the\nscalability of the matrix-free eigensolver. To optimize the communication\nperformance, we leverage the consistent space runtime, CSPACER, and show its\nefficiency in accelerating the MBL irregular communication patterns at scale\ncompared to optimized MPI non-blocking two-sided and one-sided RMA\nimplementation variants. The efficiency and effectiveness of the proposed\nalgorithm is demonstrated by computing eigenstates on a massively parallel\nmany-core high performance computer.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 02:10:14 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Van Beeumen", "Roel", ""], ["Ibrahim", "Khaled Z.", ""], ["Kahanamoku-Meyer", "Gregory D.", ""], ["Yao", "Norman Y.", ""], ["Yang", "Chao", ""]]}, {"id": "2012.00547", "submitter": "Wang Guo", "authors": "Liang Yang, Wang Guo, Daniel Benevides da Costa, and Mohamed-Slim\n  Alouini", "title": "Free-Space Optical Communication With Reconfigurable Intelligent\n  Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the promising gains, free-space optical (FSO) communication is\nseverely influenced by atmospheric turbulence and pointing error issues, which\nmake its practical design a bit challenging. In this paper, with the aim to\nincrease the communication coverage and improve the system performance,\nreconfigurable intelligent surfaces (RISs) are considered in an FSO\ncommunication setup, in which both atmospheric turbulence and pointing errors\nare considered. Closed-form expressions for the outage probability, average bit\nerror rate, and channel capacity are derived assuming large number of\nreflecting elements at the RIS. Specifically, according to central limit\ntheorem (CLT), while assuming multiple reflecting elements approximate\nexpressions are proposed. It is shown that the respective accuracies increase\nas the number of elements at the RIS increases. Illustrative numerical examples\nare shown along with insightful discussions. Finally, Monte Carlo simulations\nare presented to verify the correctness of the analytical results.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:21:51 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Yang", "Liang", ""], ["Guo", "Wang", ""], ["da Costa", "Daniel Benevides", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2012.00851", "submitter": "Celine Comte", "authors": "C\\'eline Comte", "title": "Stochastic Non-Bipartite Matching Models and Order-Independent Loss\n  Queues", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for matching items with one another while meeting assignment\nconstraints or preferences has given rise to several well-known problems like\nthe stable marriage and roommate problems. While most of the literature on\nmatching problems focuses on a static setting with a fixed number of items,\nseveral recent works incorporated time by considering stochastic models, in\nwhich items of different classes arrive according to independent Poisson\nprocesses and assignment constraints are described by an undirected\nnon-bipartite graph on the classes. In this paper, we prove that the\ncontinuous-time Markov chain associated with this model has the same transition\ndiagram as in a product-form queueing model called an order-independent loss\nqueue. This allows us to adapt existing results on order-independent (loss)\nqueues to stochastic matching models, and in particular to derive closed-form\nexpressions for several performance metrics, like the waiting probability or\nthe mean matching time, that can be implemented using dynamic programming. Both\nthese formulas and the numerical results that they allow us to derive are used\nto gain insight into the impact of parameters on performance. In particular, we\ncharacterize performance in a so-called heavy-traffic regime in which the\nnumber of items of a subset of the classes goes to infinity while items of\nother classes become scarce.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:43:49 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 18:43:40 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Comte", "C\u00e9line", ""]]}, {"id": "2012.01520", "submitter": "Daniel Dunlavy", "authors": "Jeremy M. Myers, Daniel M. Dunlavy, Keita Teranishi, D. S. Hollman", "title": "Parameter Sensitivity Analysis of the SparTen High Performance Sparse\n  Tensor Decomposition Software: Extended Analysis", "comments": "33 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": "SAND2020-11901R", "categories": "math.NA cs.MS cs.NA cs.PF stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decomposition models play an increasingly important role in modern\ndata science applications. One problem of particular interest is fitting a\nlow-rank Canonical Polyadic (CP) tensor decomposition model when the tensor has\nsparse structure and the tensor elements are nonnegative count data. SparTen is\na high-performance C++ library which computes a low-rank decomposition using\ndifferent solvers: a first-order quasi-Newton or a second-order damped Newton\nmethod, along with the appropriate choice of runtime parameters. Since default\nparameters in SparTen are tuned to experimental results in prior published work\non a single real-world dataset conducted using MATLAB implementations of these\nmethods, it remains unclear if the parameter defaults in SparTen are\nappropriate for general tensor data. Furthermore, it is unknown how sensitive\nalgorithm convergence is to changes in the input parameter values. This report\naddresses these unresolved issues with large-scale experimentation on three\nbenchmark tensor data sets. Experiments were conducted on several different CPU\narchitectures and replicated with many initial states to establish generalized\nprofiles of algorithm convergence behavior.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 20:47:29 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Myers", "Jeremy M.", ""], ["Dunlavy", "Daniel M.", ""], ["Teranishi", "Keita", ""], ["Hollman", "D. S.", ""]]}, {"id": "2012.01671", "submitter": "Ying-Chiao Liao", "authors": "Chuan-Chi Wang, Ying-Chiao Liao, Chia-Heng Tu, Ming-Chang Kao, Wen-Yew\n  Liang, Shih-Hao Hung", "title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling\n  of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advancements of computing technology facilitate the development of\ndiverse deep learning applications. Unfortunately, the efficiency of parallel\ncomputing infrastructures varies widely with neural network models, which\nhinders the exploration of the design space to find high-performance neural\nnetwork architectures on specific computing platforms for a given application.\nTo address such a challenge, we propose a deep learning-based method,\nResPerfNet, which trains a residual neural network with representative datasets\nobtained on the target platform to predict the performance for a deep neural\nnetwork. Our experimental results show that ResPerfNet can accurately predict\nthe execution time of individual neural network layers and full network models\non a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean\nabsolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX\n1080Ti, which is substantially lower than the previously published works.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 03:02:42 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wang", "Chuan-Chi", ""], ["Liao", "Ying-Chiao", ""], ["Tu", "Chia-Heng", ""], ["Kao", "Ming-Chang", ""], ["Liang", "Wen-Yew", ""], ["Hung", "Shih-Hao", ""]]}, {"id": "2012.03695", "submitter": "Josu Doncel", "authors": "E. Bachmat and J. Doncel", "title": "Non-Asymptotic Performance Analysis of Size-Based Routing Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.OC math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the performance of two size-based routing policies: the Size\nInterval Task Assignment (SITA) and Task Assignment based on Guessing Size\n(TAGS). We consider a system with two servers and Bounded Pareto distributed\njob sizes with tail parameter 1 where the difference between the size of the\nlargest and the smallest job is finite. We show that the ratio between the mean\nwaiting time of TAGS over the mean waiting time of SITA is unbounded when the\nlargest job size is large and the arrival rate times the largest job size is\nless than one. We provide numerical experiments that show that our theoretical\nfindings extend to Bounded Pareto distributed job sizes with tail parameter\ndifferent to 1.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:49:37 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Bachmat", "E.", ""], ["Doncel", "J.", ""]]}, {"id": "2012.03776", "submitter": "Anastasios Giovanidis", "authors": "Theodoros Giannakas, Anastasios Giovanidis, Thrasyvoulos Spyropoulos", "title": "SOBA: Session optimal MDP-based network friendly recommendations", "comments": "10 pages double column, accepted at IEEE INFOCOM 2021. This is the\n  reviewed version submitted by the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Caching content over CDNs or at the network edge has been solidified as a\nmeans to improve network cost and offer better streaming experience to users.\nFurthermore, nudging the users towards low-cost content has recently gained\nmomentum as a strategy to boost network performance. We focus on the problem of\noptimal policy design for Network Friendly Recommendations (NFR). We depart\nfrom recent modeling attempts, and propose a Markov Decision Process (MDP)\nformulation. MDPs offer a unified framework that can model a user with random\nsession length. As it turns out, many state-of-the-art approaches can be cast\nas subcases of our MDP formulation. Moreover, the approach offers flexibility\nto model users who are reactive to the quality of the received recommendations.\nIn terms of performance, for users consuming an arbitrary number of contents in\nsequence, we show theoretically and using extensive validation over real traces\nthat the MDP approach outperforms myopic algorithms both in session cost as\nwell as in offered recommendation quality. Finally, even compared to optimal\nstate-of-art algorithms targeting specific subcases, our MDP framework is\nsignificantly more efficient, speeding the execution time by a factor of 10,\nand enjoying better scaling with the content catalog and recommendation batch\nsizes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:12:21 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Giannakas", "Theodoros", ""], ["Giovanidis", "Anastasios", ""], ["Spyropoulos", "Thrasyvoulos", ""]]}, {"id": "2012.06128", "submitter": "Qin Wang", "authors": "Qin Wang, Jiangshan Yu, Shiping Chen, Yang Xiang", "title": "SoK: Diving into DAG-based Blockchain Systems", "comments": "Full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain plays an important role in cryptocurrency markets and technology\nservices. However, limitations on high latency and low scalability retard their\nadoptions and applications in classic designs. Reconstructed blockchain systems\nhave been proposed to avoid the consumption of competitive transactions caused\nby linear sequenced blocks. These systems, instead, structure\ntransactions/blocks in the form of Directed Acyclic Graph (DAG) and\nconsequently re-build upper layer components including consensus, incentives,\n\\textit{etc.} The promise of DAG-based blockchain systems is to enable fast\nconfirmation (complete transactions within million seconds) and high\nscalability (attach transactions in parallel) without significantly\ncompromising security. However, this field still lacks systematic work that\nsummarises the DAG technique. To bridge the gap, this Systematization of\nKnowledge (SoK) provides a comprehensive analysis of DAG-based blockchain\nsystems. Through deconstructing open-sourced systems and reviewing academic\nresearches, we conclude the main components and featured properties of systems,\nand provide the approach to establish a DAG. With this in hand, we analyze the\nsecurity and performance of several leading systems, followed by discussions\nand comparisons with concurrent (scaling blockchain) techniques. We further\nidentify open challenges to highlight the potentiality of DAG-based solutions\nand indicate their promising directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 05:13:18 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 02:11:31 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Qin", ""], ["Yu", "Jiangshan", ""], ["Chen", "Shiping", ""], ["Xiang", "Yang", ""]]}, {"id": "2012.06144", "submitter": "Markus Holzer", "authors": "Markus Holzer, Martin Bauer, Ulrich R\\\"ude", "title": "Highly Efficient Lattice-Boltzmann Multiphase Simulations of Immiscible\n  Fluids at High-Density Ratios on CPUs and GPUs through Code Generation", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A high-performance implementation of a multiphase lattice Boltzmann method\nbased on the conservative Allen-Cahn model supporting high-density ratios and\nhigh Reynolds numbers is presented. Metaprogramming techniques are used to\ngenerate optimized code for CPUs and GPUs automatically. The coupled model is\nspecified in a high-level symbolic description and optimized through automatic\ntransformations. The memory footprint of the resulting algorithm is reduced\nthrough the fusion of compute kernels. A roofline analysis demonstrates the\nexcellent efficiency of the generated code on a single GPU. The resulting\nsingle GPU code has been integrated into the multiphysics framework waLBerla to\nrun massively parallel simulations on large domains. Communication hiding and\nGPUDirect-enabled MPI yield near-perfect scaling behaviour. Scaling experiments\nare conducted on the Piz Daint supercomputer with up to 2048 GPUs, simulating\nseveral hundred fully resolved bubbles. Further, validation of the\nimplementation is shown in a physically relevant scenario-a three-dimensional\nrising air bubble in water.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 06:07:58 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Holzer", "Markus", ""], ["Bauer", "Martin", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "2012.06473", "submitter": "Mich\\`ele Weiland", "authors": "Michele Weiland and Bernhard Homoelle", "title": "Usage Scenarios for Byte-Addressable Persistent Memory\n  inHigh-Performance and Data Intensive Computing", "comments": null, "journal-ref": null, "doi": "10.1007/s11390-020-0776-8", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Byte-addressable persistent memory (B-APM) presents a new opportunity to\nbridge the performance gap between main memory and storage. In this paper, we\npresent the usage scenarios for this new technology, based on the capabilities\nof Intel's DCPMM. We outline some of the basic performance characteristics of\nDCPMM, and explain how it can be configured and used to address the needs of\nmemory and I/O intensive applications in the HPC and data intensive domains.\nTwo decision trees are presented to advise on the configuration options for\nB-APM; their use is illustrated with two examples. We show that the flexibility\nof the technology has the potential to be truly disruptive, not only because of\nthe performance improvements it can deliver, but also because it allows systems\nto cater for wider range of applications on homogeneous hardware.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 10:35:52 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Weiland", "Michele", ""], ["Homoelle", "Bernhard", ""]]}, {"id": "2012.06554", "submitter": "Do Le Quoc", "authors": "Robert Krahn and Donald Dragoti and Franz Gregor and Do Le Quoc and\n  Valerio Schiavoni and Pascal Felber and Clenimar Souza and Andrey Brito and\n  Christof Fetzer", "title": "TEEMon: A continuous performance monitoring framework for TEEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trusted Execution Environments (TEEs), such as Intel Software Guard\neXtensions (SGX), are considered as a promising approach to resolve security\nchallenges in clouds. TEEs protect the confidentiality and integrity of\napplication code and data even against privileged attackers with root and\nphysical access by providing an isolated secure memory area, i.e., enclaves.\nThe security guarantees are provided by the CPU, thus even if system software\nis compromised, the attacker can never access the enclave's content. While this\napproach ensures strong security guarantees for applications, it also\nintroduces a considerable runtime overhead in part by the limited availability\nof protected memory (enclave page cache). Currently, only a limited number of\nperformance measurement tools for TEE-based applications exist and none offer\nperformance monitoring and analysis during runtime.\n  This paper presents TEEMon, the first continuous performance monitoring and\nanalysis tool for TEE-based applications. TEEMon provides not only fine-grained\nperformance metrics during runtime, but also assists the analysis of\nidentifying causes of performance bottlenecks, e.g., excessive system calls.\nOur approach smoothly integrates with existing open-source tools (e.g.,\nPrometheus or Grafana) towards a holistic monitoring solution, particularly\noptimized for systems deployed through Docker containers or Kubernetes and\noffers several dedicated metrics and visualizations. Our evaluation shows that\nTEEMon's overhead ranges from 5% to 17%.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:33:23 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Krahn", "Robert", ""], ["Dragoti", "Donald", ""], ["Gregor", "Franz", ""], ["Quoc", "Do Le", ""], ["Schiavoni", "Valerio", ""], ["Felber", "Pascal", ""], ["Souza", "Clenimar", ""], ["Brito", "Andrey", ""], ["Fetzer", "Christof", ""]]}, {"id": "2012.06613", "submitter": "Fnu Hairi", "authors": "Hairi, Xin Liu and Lei Ying", "title": "Beyond Scaling: Calculable Error Bounds of the Power-of-Two-Choices\n  Mean-Field Model in Heavy-Traffic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a recipe for deriving calculable approximation errors of\nmean-field models in heavy-traffic with the focus on the well-known load\nbalancing algorithm -- power-of-two-choices (Po2). The recipe combines Stein's\nmethod for linearized mean-field models and State Space Concentration (SSC)\nbased on geometric tail bounds. In particular, we divide the state space into\ntwo regions, a neighborhood near the mean-field equilibrium and the complement\nof that. We first use a tail bound to show that the steady-state probability\nbeing outside the neighborhood is small. Then, we use a linearized mean-field\nmodel and Stein's method to characterize the generator difference, which\nprovides the dominant term of the approximation error. From the dominant term,\nwe are able to obtain an asymptotically-tight bound and a nonasymptotic upper\nbound, both are calculable bounds, not order-wise scaling results like most\nresults in the literature. Finally, we compared the theoretical bounds with\nnumerical evaluations to show the effectiveness of our results. We note that\nthe simulation results show that both bounds are valid even for small size\nsystems such as a system with only ten servers.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 19:40:19 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hairi", "", ""], ["Liu", "Xin", ""], ["Ying", "Lei", ""]]}, {"id": "2012.06828", "submitter": "Ioannis Dimitriou", "authors": "Ioannis Dimitriou", "title": "On partially homogeneous nearest-neighbour random walks in the quarter\n  plane and their application in the analysis of two-dimensional queues with\n  limited state-dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with the stationary analysis of two-dimensional partially\nhomogeneous nearest-neighbour random walks. Such type of random walks are\ncharacterized by the fact that the one-step transition probabilities are\nfunctions of the state-space. We show that its stationary behaviour is\ninvestigated by solving a finite system of linear equations, two matrix\nfunctional equations, and a functional equation with the aid of the theory of\nRiemann (-Hilbert) boundary value problems. This work is strongly motivated by\nemerging applications in flow level performance of wireless networks that give\nrise in queueing models with scalable service capacity, as well as in\nqueue-based random access protocols, where the network's parameters are\nfunctions of the queue lengths. A simple numerical illustration, along with\nsome details on the numerical implementation are also presented.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 14:25:52 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 17:01:18 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Dimitriou", "Ioannis", ""]]}, {"id": "2012.06989", "submitter": "Furqan Khan", "authors": "Anees Al-Najjar, Furqan Hameed Khan, Marius Portmann", "title": "Network Traffic Control for Multi-homed End-hosts via SDN", "comments": "13 pages, 26 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Defined Networking (SDN) is an emerging technology of efficiently\ncontrolling and managing computer networks, such as in data centres, Wide Area\nNetworks (WANs), as well as in ubiquitous communication. In this paper, we\nexplore the idea of embedding the SDN components, represented by SDN controller\nand virtual switch, in end-hosts to improve network performance. In particular,\nwe consider load balancing across multiple network interfaces on end-hosts with\ndifferent link capacity scenarios. We have explored and implemented different\nSDN-based load balancing approaches based on OpenFlow software switches, and\nhave demonstrated the feasibility and the potential of this approach. The\nproposed system has been evaluated with multipath transmission control protocol\n(MPTCP). Our results demonstrated the potential of applying the SDN concepts on\nmulti-homed devices resulting in an increase in achieved throughput of 55\\%\ncompared to the legacy single network approach and 10\\% compared to the MPTCP.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 07:25:37 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Al-Najjar", "Anees", ""], ["Khan", "Furqan Hameed", ""], ["Portmann", "Marius", ""]]}, {"id": "2012.07152", "submitter": "Andras Farago", "authors": "Andras Farago", "title": "On Non-Markovian Performance Models", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach that can be useful when the network or system\nperformance is described by a model that is not Markovian. Although most\nperformance models are based on Markov chains or Markov processes, in some\ncases the Markov property does not hold. This can occur, for example, when the\nsystem exhibits long range dependencies. For such situations, and other\nnon-Markovian cases, our method may provide useful help.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 21:10:11 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Farago", "Andras", ""]]}, {"id": "2012.07163", "submitter": "Maksim Levental", "authors": "Maksim Levental, Elena Orlova", "title": "Comparing the costs of abstraction for DL frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High level abstractions for implementing, training, and testing Deep Learning\n(DL) models abound. Such frameworks function primarily by abstracting away the\nimplementation details of arbitrary neural architectures, thereby enabling\nresearchers and engineers to focus on design. In principle, such frameworks\ncould be \"zero-cost abstractions\"; in practice, they incur translation and\nindirection overheads. We study at which points exactly in the engineering\nlife-cycle of a DL model the highest costs are paid and whether they can be\nmitigated. We train, test, and evaluate a representative DL model using\nPyTorch, LibTorch, TorchScript, and cuDNN on representative datasets, comparing\naccuracy, execution time and memory efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 22:00:23 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Levental", "Maksim", ""], ["Orlova", "Elena", ""]]}, {"id": "2012.07224", "submitter": "Brian Mark", "authors": "Hanoch Lev-Ari, Yariv Ephraim, Brian L. Mark", "title": "Traffic Rate Network Tomography with Higher-Order Cumulants", "comments": "10 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network tomography aims at estimating source-destination traffic rates from\nlink traffic measurements. This inverse problem was formulated by Vardi in 1996\nfor Poisson traffic over networks operating under deterministic as well as\nrandom routing regimes. In this paper we expand Vardi's second-order moment\nmatching rate estimation approach to higher-order cumulant matching with the\ngoal of increasing the column rank of the mapping and consequently improving\nthe rate estimation accuracy. We develop a systematic set of linear cumulant\nmatching equations and express them compactly in terms of the Khatri-Rao\nproduct. Both least squares estimation and iterative minimum I-divergence\nestimation are considered. We develop an upper bound on the mean squared error\n(MSE) in least squares rate estimation from empirical cumulants. We demonstrate\nfor the NSFnet that supplementing Vardi's approach with third-order empirical\ncumulant reduces its averaged normalized MSE relative to the theoretical\nminimum of the second-order moment matching approach by about 12%-18%. This\nminimum MSE is obtained when Vardi's second-order moment matching approach is\nbased on the theoretical rather than the empirical moments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 02:51:47 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lev-Ari", "Hanoch", ""], ["Ephraim", "Yariv", ""], ["Mark", "Brian L.", ""]]}, {"id": "2012.08297", "submitter": "Kondo Adjallah", "authors": "K. Adjallah (ISTIT), K. Adzakpa", "title": "Minimizing Maintenance Cost Involving Flow-time and Tardiness Penalty\n  with Unequal Release Dates", "comments": null, "journal-ref": "Proceedings of the Institution of Mechanical Engineers, Part O:\n  Journal of Risk and Reliability, SAGE Publications, 2007, 221 (1), pp.57-65", "doi": "10.1243/1748006XJRR24", "report-no": null, "categories": "cs.PF cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes important and useful results relating to the minimization\nof the sum of the flow-time and the tardiness of tasks or jobs with unequal\nrelease dates (occurrence date), with application to maintenance planning and\nscheduling. Firstly, the policy of real-time maintenance is defined for\nminimizing the cost of tardiness and critical states. The required local\noptimality rule (FTR) is proved, in order to minimize the sum or the linear\ncombination of the tasks' flow-time and tardiness costs. This rule has served\nto design a scheduling algorithm, with O(n 3) complexity when it is applied to\nschedule a set of n tasks on one processor. To evaluate its performance, the\nresults are compared to a lower bound that is provided, in a numerical case\nstudy. Using this algorithm in combination with the tasks urgency criterion, a\nreal-time algorithm is developed to schedule the tasks on q parallel\nprocessors. This latter algorithm is finally applied to schedule and assign\npreventive maintenance tasks to processors in the case of a distributed system.\nIts efficiency enables, as shown in the numerical example, to minimize the cost\nof preventive maintenance tasks, expressed as the sum of the tasks tardiness\nand flow-time. This corresponds to costs of critical states and of tardiness of\npreventive maintenance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:05:41 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Adjallah", "K.", "", "ISTIT"], ["Adzakpa", "K.", ""]]}, {"id": "2012.08357", "submitter": "Mark Van Der Boor", "authors": "Mark van der Boor, Sem Borst, Johan van Leeuwaarden", "title": "Optimal Hyper-Scalable Load Balancing with a Strict Queue Limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Load balancing plays a critical role in efficiently dispatching jobs in\nparallel-server systems such as cloud networks and data centers. A fundamental\nchallenge in the design of load balancing algorithms is to achieve an optimal\ntrade-off between delay performance and implementation overhead (e.g.\ncommunication or memory usage). This trade-off has primarily been studied so\nfar from the angle of the amount of overhead required to achieve asymptotically\noptimal performance, particularly vanishing delay in large-scale systems. In\ncontrast, in the present paper, we focus on an arbitrarily sparse communication\nbudget, possibly well below the minimum requirement for vanishing delay,\nreferred to as the hyper-scalable operating region. Furthermore, jobs may only\nbe admitted when a specific limit on the queue position of the job can be\nguaranteed.\n  The centerpiece of our analysis is a universal upper bound for the achievable\nthroughput of any dispatcher-driven algorithm for a given communication budget\nand queue limit. We also propose a specific hyper-scalable scheme which can\noperate at any given message rate and enforce any given queue limit, while\nallowing the server states to be captured via a closed product-form network, in\nwhich servers act as customers traversing various nodes. The product-form\ndistribution is leveraged to prove that the bound is tight and that the\nproposed hyper-scalable scheme is throughput-optimal in a many-server regime\ngiven the communication and queue limit constraints. Extensive simulation\nexperiments are conducted to illustrate the results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 12:43:08 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["van der Boor", "Mark", ""], ["Borst", "Sem", ""], ["van Leeuwaarden", "Johan", ""]]}, {"id": "2012.09064", "submitter": "Nicolas Gast", "authors": "Nicolas Gast (POLARIS), Bruno Gaujal (POLARIS), Chen Yan (POLARIS)", "title": "Exponential Convergence Rate for the Asymptotic Optimality of Whittle\n  Index Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the performance of Whittle index policy for restless Markovian\nbandits, when the number of bandits grows. It is proven in [30] that this\nperformance is asymptotically optimal if the bandits are indexable and the\nassociated deterministic system has a global attractor fixed point. In this\npaper we show that, under the same conditions, the convergence rate is\nexponential in the number of bandits, unless the fixed point is singular (to be\ndefined later). Our proof is based on the nature of the deterministic equation\ngoverning the stochastic system: We show that it is a piecewise affine\ncontinuous dynamical system inside the simplex of the empirical measure of the\nbandits. Using simulations and numerical solvers, we also investigate the cases\nwhere the conditions for the exponential rate theorem are violated, notably\nwhen attracting limit cycles appear, or when the fixed point is singular. We\nillustrate our theorem on a Markovian fading channel model, which has been well\nstudied in the literature. Finally, we extend our synchronous model results to\nthe asynchronous model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:34:39 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Gast", "Nicolas", "", "POLARIS"], ["Gaujal", "Bruno", "", "POLARIS"], ["Yan", "Chen", "", "POLARIS"]]}, {"id": "2012.09303", "submitter": "William D. Gropp", "authors": "William Gropp, Sujata Banerjee, and Ian Foster", "title": "Infrastructure for Artificial Intelligence, Quantum and High Performance\n  Computing", "comments": "A Computing Community Consortium (CCC) white paper, 3 pages", "journal-ref": null, "doi": null, "report-no": "ccc2020whitepaper_11", "categories": "cs.CY cs.AI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High Performance Computing (HPC), Artificial Intelligence (AI)/Machine\nLearning (ML), and Quantum Computing (QC) and communications offer immense\nopportunities for innovation and impact on society. Researchers in these areas\ndepend on access to computing infrastructure, but these resources are in short\nsupply and are typically siloed in support of their research communities,\nmaking it more difficult to pursue convergent and interdisciplinary research.\nSuch research increasingly depends on complex workflows that require different\nresources for each stage. This paper argues that a more-holistic approach to\ncomputing infrastructure, one that recognizes both the convergence of some\ncapabilities and the complementary capabilities from new computing approaches,\nbe it commercial cloud to Quantum Computing, is needed to support computer\nscience research.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 22:41:24 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Gropp", "William", ""], ["Banerjee", "Sujata", ""], ["Foster", "Ian", ""]]}, {"id": "2012.09666", "submitter": "H Jacinto", "authors": "Luka Daoud, Muhammad Kamran Latif, H S. Jacinto, Nader Rafla", "title": "A fully pipelined FPGA accelerator for scale invariant feature transform\n  keypoint descriptor matching,", "comments": null, "journal-ref": "Microprocessors and Microsystems 72 (2020): 102919", "doi": "10.1016/j.micpro.2019.102919", "report-no": null, "categories": "cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The scale invariant feature transform (SIFT) algorithm is considered a\nclassical feature extraction algorithm within the field of computer vision.\nSIFT keypoint descriptor matching is a computationally intensive process due to\nthe amount of data consumed. In this work, we designed a novel fully pipelined\nhardware accelerator architecture for SIFT keypoint descriptor matching. The\naccelerator core was implemented and tested on a field programmable gate array\n(FPGA). The proposed hardware architecture is able to properly handle the\nmemory bandwidth necessary for a fully-pipelined implementation and hits the\nroofline performance model, achieving the potential maximum throughput. The\nfully pipelined matching architecture was designed based on the consine angle\ndistance method. Our architecture was optimized for 16-bit fixed-point\noperations and implemented on hardware using a Xilinx Zynq-based FPGA\ndevelopment board. Our proposed architecture shows a noticeable reduction of\narea resources compared with its counterparts in literature, while maintaining\nhigh throughput by alleviating memory bandwidth restrictions. The results show\na reduction in consumed device resources of up to 91 percent in LUTs and 79\npercent of BRAMs. Our hardware implementation is 15.7 times faster than the\ncomparable software approach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:29:41 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Daoud", "Luka", ""], ["Latif", "Muhammad Kamran", ""], ["Jacinto", "H S.", ""], ["Rafla", "Nader", ""]]}, {"id": "2012.10142", "submitter": "Diego Goldsztajn", "authors": "Diego Goldsztajn, Sem C. Borst, Johan S.H. van Leeuwaarden", "title": "Learning and balancing time-varying loads in large-scale systems", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a system of $n$ parallel server pools where tasks arrive as a\ntime-varying Poisson process. The system aims at balancing the load by using an\ninner control loop with an admission threshold to assign incoming tasks to\nserver pools; as an outer control loop, a learning scheme adjusts this\nthreshold over time in steps of $\\Delta$ units, to keep it aligned with the\ntime-varying overall load. If the fluctuations in the normalized load are\nsmaller than $\\Delta$, then we prove that the threshold settles for all large\nenough $n$ and balances the load when $\\Delta = 1$. Our model captures a\ntradeoff between optimality and stability, since for higher $\\Delta$ the degree\nof balance decreases, but the threshold remains constant under larger load\nfluctuations. The analysis of this model is mathematically challenging,\nparticularly since the learning scheme relies on subtle variations in the\noccupancy state of the system which vanish on the fluid scale; the methodology\ndeveloped in this paper overcomes this hurdle by leveraging the tractability of\nthe specific system dynamics. Strong approximations are used to prove certain\ndynamical properties which are then used to characterize the behavior of the\nsystem, without relying on a traditional fluid-limit analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 10:08:05 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Goldsztajn", "Diego", ""], ["Borst", "Sem C.", ""], ["van Leeuwaarden", "Johan S. H.", ""]]}, {"id": "2012.11473", "submitter": "Nicolas Derumigny", "authors": "Nicolas Derumigny, Fabian Gruber, Th\\'eophile Bastian, Christophe\n  Guillon, Louis-Noel Pouchet, Fabrice Rastello", "title": "From micro-OPs to abstract resources: constructing a simpler CPU\n  performance model through microbenchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes PALMED, a tool that automatically builds a resource\nmapping, a performance model for pipelined, super-scalar, out-of-order CPU\narchitectures. Resource mappings describe the execution of a program by\nassigning instructions in the program to abstract resources. They can be used\nto predict the throughput of basic blocks or as a machine model for the backend\nof an optimizing compiler. PALMED does not require hardware performance\ncounters, and relies solely on runtime measurements to construct resource\nmappings. This allows it to model not only execution port usage, but also other\nlimiting resources, such as the frontend or the reorder buffer. Also, thanks to\na dual representation of resource mappings, our algorithm for constructing\nmappings scales to large instruction sets, like that of x86. We evaluate the\nalgorithmic contribution of the paper in two ways. First by showing that our\napproach can reverse engineering an accurate resource mapping from an\nidealistic performance model produced by an existing port-mapping. We also\nevaluate the pertinence of our dual representation, as opposed to the standard\nport-mapping, for throughput modeling by extracting a representative set of\nbasic-blocks from the compiled binaries of the Spec CPU 2017 benchmarks and\ncomparing the throughput predicted by existing machine models to that produced\nby PALMED.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 16:42:50 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 11:03:53 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Derumigny", "Nicolas", ""], ["Gruber", "Fabian", ""], ["Bastian", "Th\u00e9ophile", ""], ["Guillon", "Christophe", ""], ["Pouchet", "Louis-Noel", ""], ["Rastello", "Fabrice", ""]]}, {"id": "2012.13548", "submitter": "Shubhendra Singhal Mr.", "authors": "Shubhendra Pal Singhal", "title": "Graph500 from OCaml-Multicore Perspective", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  OCaml is an industrial-strength, multi-paradigm programming language, widely\nused in industry and academia. OCaml was developed for solving numerical and\nscientific problems involving large scale data-intensive operations and one\nsuch classic application set is Graph Algorithms, which are a core part of most\nanalytics workloads. In this paper, we aim to implement the graph benchmarks\nalong with the performance analysis. Graph500 is one such serious benchmark\nwhich aims at developing data intensive applications requiring extreme\ncomputational power. We try to implement Graph Construction, BFS, Shortest-Path\nproblems using the desired specifications and rules posed by graph500. This\npaper aims at providing a clear direction of choices of several data structures\nused, algorithms developed and pose a reason behind every step of program. The\nfirst few sections of the paper discusses a formal approach to the problem with\na small guide for starters in OCaml. The latter sections describe the\nalgorithms in detail with the possibilities of future exploration and several\nmistakes which we committed or encountered whilst approaching the solution. All\nperformance metrics were tested on Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz 24\ncore machine. Every section talks about the initial performance failures\nencountered, which will help analyse and prioritise our preferred\nimplementation from a performance perspective.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 09:11:18 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Singhal", "Shubhendra Pal", ""]]}, {"id": "2012.13796", "submitter": "Marina Sokolova", "authors": "YuanZheng Hu, Marina Sokolova", "title": "Explainable Multi-class Classification of Medical Data", "comments": "21 pages; 23 tables; 2 appendixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning applications have brought new insights into a secondary\nanalysis of medical data. Machine Learning helps to develop new drugs, define\npopulations susceptible to certain illnesses, identify predictors of many\ncommon diseases. At the same time, Machine Learning results depend on\nconvolution of many factors, including feature selection, class (im)balance,\nalgorithm preference, and performance metrics. In this paper, we present\nexplainable multi-class classification of a large medical data set. We in\ndetails discuss knowledge-based feature engineering, data set balancing, best\nmodel selection, and parameter tuning. Six algorithms are used in this study:\nSupport Vector Machine (SVM), Na\\\"ive Bayes, Gradient Boosting, Decision Trees,\nRandom Forest, and Logistic Regression. Our empirical evaluation is done on the\nUCI Diabetes 130-US hospitals for years 1999-2008 dataset, with the task to\nclassify patient hospital re-admission stay into three classes: 0 days, <30\ndays, or > 30 days. Our results show that using 23 medication features in\nlearning experiments improves Recall of five out of the six applied learning\nalgorithms. This is a new result that expands the previous studies conducted on\nthe same data. Gradient Boosting and Random Forest outperformed other\nalgorithms in terms of the three-class classification Accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 18:56:07 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hu", "YuanZheng", ""], ["Sokolova", "Marina", ""]]}, {"id": "2012.14635", "submitter": "Behnam Dezfouli", "authors": "Chia-Chi Li, Vikram K. Ramanna, Daniel Webber, Cole Hunter, Tyler\n  Hack, and Behnam Dezfouli", "title": "Sensifi: A Wireless Sensing System for Ultra-High-Rate Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": "SIOTLAB-2020-12-DEC", "categories": "cs.NI cs.OS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wireless Sensor Networks (WSNs) are being used in various applications such\nas structural health monitoring and industrial control. Since energy efficiency\nis one of the major design factors, the existing WSNs primarily rely on\nlow-power, low-rate wireless technologies such as 802.15.4 and Bluetooth. In\nthis paper, we strive to tackle the challenges of developing ultra-high-rate\nWSNs based on 802.11 (WiFi) standard by proposing Sensifi. As an illustrative\napplication of this system, we consider vibration test monitoring of spacecraft\nand identify system design requirements and challenges. Our main contributions\nare as follows. First, we propose packet encoding methods to reduce the\noverhead of assigning accurate timestamps to samples. Second, we propose energy\nefficiency methods to enhance the system's lifetime. Third, we reduce the\noverhead of processing outgoing packets through network stack to enhance\nsampling rate and mitigate sampling rate instability. Fourth, we study and\nreduce the delay of processing incoming packets through network stack to\nenhance the accuracy of time synchronization among nodes. Fifth, we propose a\nlow-power node design for ultra-high-rate applications. Sixth, we use our node\ndesign to empirically evaluate the system.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 07:17:38 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 00:16:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Chia-Chi", ""], ["Ramanna", "Vikram K.", ""], ["Webber", "Daniel", ""], ["Hunter", "Cole", ""], ["Hack", "Tyler", ""], ["Dezfouli", "Behnam", ""]]}, {"id": "2012.14968", "submitter": "Themis Melissaris", "authors": "Themis Melissaris, Kelly Shaw, Margaret Martonosi", "title": "Optimizing IoT and Web Traffic Using Selective Edge Compression", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet of Things (IoT) devices and applications are generating and\ncommunicating vast quantities of data, and the rate of data collection is\nincreasing rapidly. These high communication volumes are challenging for\nenergy-constrained, data-capped, wireless mobile devices and networked sensors.\nCompression is commonly used to reduce web traffic, to save energy, and to make\nnetwork transfers faster. If not used judiciously, however, compression can\nhurt performance. This work proposes and evaluates mechanisms that employ\nselective compression at the network's edge, based on data characteristics and\nnetwork conditions. This approach (i) improves the performance of network\ntransfers in IoT environments, while (ii) providing significant data savings.\nWe demonstrate that our library speeds up web transfers by an average of 2.18x\nand 2.03x under fixed and dynamically changing network conditions respectively.\nFurthermore, it also provides consistent data savings, compacting data down to\n19% of the original data size.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 22:59:56 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Melissaris", "Themis", ""], ["Shaw", "Kelly", ""], ["Martonosi", "Margaret", ""]]}, {"id": "2012.15295", "submitter": "Yuankun Fu", "authors": "Yuankun Fu, Fengguang Song", "title": "SDN helps Big Data to optimize access to data", "comments": null, "journal-ref": "Big Data and Software Defined Networks, March, 2018", "doi": "10.1049/PBPC015E_ch14", "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter introduces the state-of-the-art in the emerging area of\ncombining High Performance Computing (HPC) with Big Data Analysis. To\nunderstand the new area, the chapter first surveys the existing approaches to\nintegrating HPC with Big Data. Next, the chapter introduces several\noptimization solutions that focus on how to minimize the data transfer time\nfrom computation-intensive applications to analysis-intensive applications as\nwell as minimizing the end-to-end time-to-solution. The solutions utilize SDN\nto adaptively use both high speed interconnect network and high performance\nparallel file systems to optimize the application performance. A computational\nframework called DataBroker is designed and developed to enable a tight\nintegration of HPC with data analysis. Multiple types of experiments have been\nconducted to show different performance issues in both message passing and\nparallel file systems and to verify the effectiveness of the proposed research\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:33:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fu", "Yuankun", ""], ["Song", "Fengguang", ""]]}, {"id": "2012.15592", "submitter": "Marcin Copik", "authors": "Marcin Copik, Alexandru Calotoiu, Tobias Grosser, Nicolas Wicki, Felix\n  Wolf, Torsten Hoefler", "title": "Extracting Clean Performance Models from Tainted Programs", "comments": "Accepted at PPoPP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance models are well-known instruments to understand the scaling\nbehavior of parallel applications. They express how performance changes as key\nexecution parameters, such as the number of processes or the size of the input\nproblem, vary. Besides reasoning about program behavior, such models can also\nbe automatically derived from performance data. This is called empirical\nperformance modeling. While this sounds simple at the first glance, this\napproach faces several serious interrelated challenges, including expensive\nperformance measurements, inaccuracies inflicted by noisy benchmark data, and\noverall complex experiment design, starting with the selection of the right\nparameters. The more parameters one considers, the more experiments are needed\nand the stronger the impact of noise. In this paper, we show how taint\nanalysis, a technique borrowed from the domain of computer security, can\nsubstantially improve the modeling process, lowering its cost, improving model\nquality, and help validate performance models and experimental setups.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 12:59:19 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Copik", "Marcin", ""], ["Calotoiu", "Alexandru", ""], ["Grosser", "Tobias", ""], ["Wicki", "Nicolas", ""], ["Wolf", "Felix", ""], ["Hoefler", "Torsten", ""]]}]