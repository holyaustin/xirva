[{"id": "2102.00223", "submitter": "Patrick Diehl", "authors": "Patrick Diehl and Dominic Marcello and Parsa Amini and Hartmut Kaiser\n  and Sagiv Shiber and Geoffrey C. Clayton and Juhan Frank and Gregor Dai{\\ss}\n  and Dirk Pfl\\\"uger and David Eder and Alice Koniges and Kevin Huck", "title": "Performance Measurements within Asynchronous Task-based Runtime Systems:\n  A Double White Dwarf Merger as an Application", "comments": null, "journal-ref": null, "doi": "10.1109/MCSE.2021.3073626", "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Analyzing performance within asynchronous many-task-based runtime systems is\nchallenging because millions of tasks are launched concurrently. Especially for\nlong-term runs the amount of data collected becomes overwhelming. We study HPX\nand its performance-counter framework and APEX to collect performance data and\nenergy consumption. We added HPX application-specific performance counters to\nthe Octo-Tiger full 3D AMR astrophysics application. This enables the combined\nvisualization of physical and performance data to highlight bottlenecks with\nrespect to different solvers. We examine the overhead introduced by these\nmeasurements, which is around 1%, with respect to the overall application\nruntime. We perform a convergence study for four different levels of refinement\nand analyze the application's performance with respect to adaptive grid\nrefinement. The measurements' overheads are small, enabling the combined use of\nperformance data and physical properties with the goal of improving the code's\nperformance. All of these measurements were obtained on NERSC's Cori, Louisiana\nOptical Network Infrastructure's QueenBee2, and Indiana University's Big Red 3.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 13:08:30 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 01:16:11 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 02:09:31 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 14:57:35 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Diehl", "Patrick", ""], ["Marcello", "Dominic", ""], ["Amini", "Parsa", ""], ["Kaiser", "Hartmut", ""], ["Shiber", "Sagiv", ""], ["Clayton", "Geoffrey C.", ""], ["Frank", "Juhan", ""], ["Dai\u00df", "Gregor", ""], ["Pfl\u00fcger", "Dirk", ""], ["Eder", "David", ""], ["Koniges", "Alice", ""], ["Huck", "Kevin", ""]]}, {"id": "2102.00273", "submitter": "Joberto Martins Prof. Dr.", "authors": "Rafael F. Reale and Walter P. neto and Joberto S. B. Martins", "title": "BAMSim Simulator", "comments": "4 pages; 1 figure, International Workshop on ADVANCEs in ICT\n  Infrastructures and Services (ADVANCE 2021)", "journal-ref": null, "doi": "10.5281/zenodo.4473102", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Resource allocation is an essential design aspect for current systems and\nbandwidth allocation is an essential design aspect in multi-protocol label\nswitched and OpenFlow/SDN network infrastructures. The bandwidth allocation\nmodels (BAMs) are an alternative to allocate and share bandwidth among network\nusers. BAMs have an extensive number of parameters that need to be defined and\ntuned to achieve an expected network performance. This paper presents the\nBAMSim simulator to support the network manager decision process in choosing a\nset of BAM configuration parameters for network design or during network\noperation.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 16:59:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Reale", "Rafael F.", ""], ["neto", "Walter P.", ""], ["Martins", "Joberto S. B.", ""]]}, {"id": "2102.00527", "submitter": "Geoffrey Yu", "authors": "Geoffrey X. Yu, Yubo Gao, Pavel Golikov, Gennady Pekhimenko", "title": "A Runtime-Based Computational Performance Predictor for Deep Neural\n  Network Training", "comments": "19 pages, 7 figures. Appears in the Proceedings of the 2021 USENIX\n  Annual Technical Conference (USENIX ATC '21). Code available at\n  https://github.com/geoffxy/habitat", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning researchers and practitioners usually leverage GPUs to help\ntrain their deep neural networks (DNNs) faster. However, choosing which GPU to\nuse is challenging both because (i) there are many options, and (ii) users\ngrapple with competing concerns: maximizing compute performance while\nminimizing costs. In this work, we present a new practical technique to help\nusers make informed and cost-efficient GPU selections: make performance\npredictions with the help of a GPU that the user already has. Our technique\nexploits the observation that, because DNN training consists of repetitive\ncompute steps, predicting the execution time of a single iteration is usually\nenough to characterize the performance of an entire training process. We make\npredictions by scaling the execution time of each operation in a training\niteration from one GPU to another using either (i) wave scaling, a technique\nbased on a GPU's execution model, or (ii) pre-trained multilayer perceptrons.\nWe implement our technique into a Python library called Habitat and find that\nit makes accurate iteration execution time predictions (with an average error\nof 11.8%) on ResNet-50, Inception v3, the Transformer, GNMT, and DCGAN across\nsix different GPU architectures. Habitat supports PyTorch, is easy to use, and\nis open source.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 20:17:46 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 19:23:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yu", "Geoffrey X.", ""], ["Gao", "Yubo", ""], ["Golikov", "Pavel", ""], ["Pekhimenko", "Gennady", ""]]}, {"id": "2102.00932", "submitter": "Bernhard Klein", "authors": "Bernhard Klein and Christoph Gratl and Manfred M\\\"ucke and Holger\n  Fr\\\"oning", "title": "Understanding Cache Boundness of ML Operators on ARM Processors", "comments": "published at the HiPEAC 2021 Conference, at the 3rd Workshop on\n  Accelerated Machine Learning (AccML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning compilers like TVM allow a fast and flexible deployment on\nembedded CPUs. This enables the use of non-standard operators, which are common\nin ML compression techniques. However, it is necessary to understand the\nlimitations of typical compute-intense operators in ML workloads to design a\nproper solution. This is the first in-detail analysis of dense and convolution\noperators, generated with TVM, that compares to the fundamental hardware limits\nof embedded ARM processors. Thereby it explains the gap between computational\npeak performance, theoretical and measured, and real-world state-of-the-art\nresults, created with TVM and openBLAS. Instead, one can see that\nsingle-precision general matrix multiply (GEMM) and convolutions are bound by\nL1-cache-read bandwidth. Explorations of 8-bit and bit-serial quantized\noperators show that quantization can be used to achieve relevant speedups\ncompared to cache-bound floating-point operators. However, the performance of\nquantized operators highly depends on the interaction between data layout and\nbit packing.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 16:05:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Klein", "Bernhard", ""], ["Gratl", "Christoph", ""], ["M\u00fccke", "Manfred", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2102.02330", "submitter": "Anshul Jindal", "authors": "Anshul Jindal, Michael Gerndt, Mohak Chadha, Vladimir Podolskiy and\n  Pengfei Chen", "title": "Function Delivery Network: Extending Serverless Computing for\n  Heterogeneous Platforms", "comments": "Accepted at Journal of Software: Practice and Experience", "journal-ref": null, "doi": "10.1002/spe.2966", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Serverless computing has rapidly grown following the launch of Amazon's\nLambda platform. Function-as-a-Service (FaaS) a key enabler of serverless\ncomputing allows an application to be decomposed into simple, standalone\nfunctions that are executed on a FaaS platform. The FaaS platform is\nresponsible for deploying and facilitating resources to the functions. Several\nof today's cloud applications spread over heterogeneous connected computing\nresources and are highly dynamic in their structure and resource requirements.\nHowever, FaaS platforms are limited to homogeneous clusters and homogeneous\nfunctions and do not account for the data access behavior of functions before\nscheduling.\n  We introduce an extension of FaaS to heterogeneous clusters and to support\nheterogeneous functions through a network of distributed heterogeneous target\nplatforms called Function Delivery Network (FDN). A target platform is a\ncombination of a cluster of homogeneous nodes and a FaaS platform on top of it.\nFDN provides Function-Delivery-as-a-Service (FDaaS), delivering the function to\nthe right target platform. We showcase the opportunities such as varied target\nplatform's characteristics, possibility of collaborative execution between\nmultiple target platforms, and localization of data that the FDN offers in\nfulfilling two objectives: Service Level Objective (SLO) requirements and\nenergy efficiency when scheduling functions by evaluating over five distributed\ntarget platforms using the FDNInspector, a tool developed by us for\nbenchmarking distributed target platforms. Scheduling functions on an edge\ntarget platform in our evaluation reduced the overall energy consumption by 17x\nwithout violating the SLO requirements in comparison to scheduling on a\nhigh-end target platform.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 23:24:48 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Jindal", "Anshul", ""], ["Gerndt", "Michael", ""], ["Chadha", "Mohak", ""], ["Podolskiy", "Vladimir", ""], ["Chen", "Pengfei", ""]]}, {"id": "2102.02645", "submitter": "Christopher A. Metz", "authors": "Christopher A. Metz, Mehran Goli, Rolf Drechsler", "title": "Pick the Right Edge Device: Towards Power and Performance Estimation of\n  CUDA-based CNNs on GPGPUs", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/06", "categories": "cs.LG cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Machine Learning (ML) as a powerful technique has been\nhelping nearly all fields of business to increase operational efficiency or to\ndevelop new value propositions. Besides the challenges of deploying and\nmaintaining ML models, picking the right edge device (e.g., GPGPUs) to run\nthese models (e.g., CNN with the massive computational process) is one of the\nmost pressing challenges faced by organizations today. As the cost of renting\n(on Cloud) or purchasing an edge device is directly connected to the cost of\nfinal products or services, choosing the most efficient device is essential.\nHowever, this decision making requires deep knowledge about performance and\npower consumption of the ML models running on edge devices that must be\nidentified at the early stage of ML workflow.\n  In this paper, we present a novel ML-based approach that provides ML\nengineers with the early estimation of both power consumption and performance\nof CUDA-based CNNs on GPGPUs. The proposed approach empowers ML engineers to\npick the most efficient GPGPU for a given CNN model at the early stage of\ndevelopment.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:46:53 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Metz", "Christopher A.", ""], ["Goli", "Mehran", ""], ["Drechsler", "Rolf", ""]]}, {"id": "2102.02710", "submitter": "Angelos Aveklouris", "authors": "Angelos Aveklouris, Levi DeValve, Amy R. Ward, Xiaofan Wu", "title": "Matching Impatient and Heterogeneous Demand and Supply", "comments": "13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.PF cs.SY eess.SY math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service platforms must determine rules for matching heterogeneous demand\n(customers) and supply (workers) that arrive randomly over time and may be lost\nif forced to wait too long for a match. We show how to balance the trade-off\nbetween making a less good match quickly and waiting for a better match, at the\nrisk of losing impatient customers and/or workers. When the objective is to\nmaximize the cumulative value of matches over a finite-time horizon, we propose\ndiscrete-review matching policies, both for the case in which the platform has\naccess to arrival rate parameter information and the case in which the platform\ndoes not. We show that both the blind and nonblind policies are asymptotically\noptimal in a high-volume setting. However, the blind policy requires frequent\nre-solving of a linear program. For that reason, we also investigate a blind\npolicy that makes decisions in a greedy manner, and we are able to establish an\nasymptotic lower bound for the greedy, blind policy that depends on the\nmatching values and is always higher than half of the value of an optimal\npolicy. Next, we develop a fluid model that approximates the evolution of the\nstochastic model and captures explicitly the nonlinear dependence between the\namount of demand and supply waiting and the distribution of their patience\ntimes. We use the fluid model to propose a policy for a more general objective\nthat additionally penalizes queue build-up. We run numerous simulations to\ninvestigate the performance of the aforementioned proposed matching policies.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 16:10:05 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Aveklouris", "Angelos", ""], ["DeValve", "Levi", ""], ["Ward", "Amy R.", ""], ["Wu", "Xiaofan", ""]]}, {"id": "2102.03614", "submitter": "Jeremie Lagraviere", "authors": "J\\'er\\'emie Lagravi\\`ere, Johannes Langguth, Martina Prugger, Phuong\n  H. Ha, Xing Cai", "title": "A Newcomer In The PGAS World -- UPC++ vs UPC: A Comparative Study", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A newcomer in the Partitioned Global Address Space (PGAS) 'world' has arrived\nin its version 1.0: Unified Parallel C++ (UPC++). UPC++ targets distributed\ndata structures where communication is irregular or fine-grained. The key\nabstractions are global pointers, asynchronous programming via RPC, futures and\npromises. UPC++ API for moving non-contiguous data and handling memories with\ndifferent optimal access methods resemble those used in modern C++. In this\nstudy we provide two kernels implemented in UPC++: a sparse-matrix vector\nmultiplication (SpMV) as part of a Partial-Differential Equation solver, and an\nimplementation of the Heat Equation on a 2D-domain. Code listings of these two\nkernels are available in the article in order to show the differences in\nprogramming style between UPC and UPC++. We provide a performance comparison\nbetween UPC and UPC++ using single-node, multi-node hardware and many-core\nhardware (Intel Xeon Phi Knight's Landing).\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 17:06:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lagravi\u00e8re", "J\u00e9r\u00e9mie", ""], ["Langguth", "Johannes", ""], ["Prugger", "Martina", ""], ["Ha", "Phuong H.", ""], ["Cai", "Xing", ""]]}, {"id": "2102.03751", "submitter": "Hossein Ahmadvand", "authors": "Hossein Ahmadvand, Fouzhan Foroutan, Mahmood Fathy", "title": "DV-DVFS: Merging Data Variety and DVFS Technique to Manage the Energy\n  Consumption of Big Data Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data variety is one of the most important features of Big Data. Data variety\nis the result of aggregating data from multiple sources and uneven distribution\nof data. This feature of Big Data causes high variation in the consumption of\nprocessing resources such as CPU consumption. This issue has been overlooked in\nprevious works. To overcome the mentioned problem, in the present work, we used\nDynamic Voltage and Frequency Scaling (DVFS) to reduce the energy consumption\nof computation. To this goal, we consider two types of deadlines as our\nconstraint. Before applying the DVFS technique to computer nodes, we estimate\nthe processing time and the frequency needed to meet the deadline. In the\nevaluation phase, we have used a set of data sets and applications. The\nexperimental results show that our proposed approach surpasses the other\nscenarios in processing real datasets. Based on the experimental results in\nthis paper, DV-DVFS can achieve up to 15% improvement in energy consumption.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 09:22:55 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ahmadvand", "Hossein", ""], ["Foroutan", "Fouzhan", ""], ["Fathy", "Mahmood", ""]]}, {"id": "2102.03848", "submitter": "arXiv Admin", "authors": "Mohamed A. Hamada and Abdelrahman Abdallah", "title": "Estimate The Efficiency Of Multiprocessor's Cash Memory Work Algorithms", "comments": "This article has been withdrawn by arXiv administrators due to\n  disputed authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many computer systems for calculating the proper organization of memory are\namong the most critical issues. Using a tier cache memory (along with branching\nprediction) is an effective means of increasing modern multi-core processors'\nperformance. Designing high-performance processors is a complex task and\nrequires preliminary verification and analysis of the model level, usually used\nin analytical and simulation modeling. The refinement of extreme programming is\nan unfortunate challenge. Few experts disagree with the synthesis of access\npoints. This article demonstrates that Internet QoS and 16-bit architectures\nare always incompatible, but it's the same situation for write-back caches. The\nsolution to this problem can be implemented by analyzing simulation models of\ndifferent complexity in combination with the analytical evaluation of\nindividual algorithms. This work is devoted to designing a multi-parameter\nsimulation model of a multi-process for evaluating the performance of cache\nmemory algorithms and the optimality of the structure. Optimization of the\nstructures and algorithms of the cache memory allows you to accelerate the\ninteraction of the memory process and improve the performance of the entire\nsystem.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 17:02:36 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 20:08:21 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hamada", "Mohamed A.", ""], ["Abdallah", "Abdelrahman", ""]]}, {"id": "2102.04090", "submitter": "Alberto Sardi", "authors": "A. Sardi and E. Sorano", "title": "Dynamic Performance Management: An Approach for Managing the Common\n  Goods", "comments": null, "journal-ref": "https://www.mdpi.com/2071-1050/11/22/6435", "doi": "10.3390/su11226435", "report-no": null, "categories": "econ.GN cs.PF q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Public organizations need innovative approaches for managing common goods and\nto explain the dynamics linking the (re)generation of common goods and\norganizational performance. Although system dynamics is recognised as a useful\napproach for managing common goods, public organizations rarely adopt the\nsystem dynamics for this goal. The paper aims to review the literature on the\nsystem dynamics and its recent application, known as dynamic performance\nmanagement, to highlight the state of the art and future opportunities on the\nmanagement of common goods. The authors analyzed 144 documents using a\nsystematic literature review. The results obtained outline a fair number of\ndocuments, countries and journals involving the study of system dynamics, but\ndo not cover sufficient research on the linking between the (re)generation of\ncommon goods and organizational performance. This paper outlines academic and\npractical contributions. Firstly, it contributes to the theory of common goods.\nIt provides insight for linking the management of common goods and\norganizational performance through the use of dynamic performance management\napproach. Furthermore, it shows scholars the main research opportunities.\nSecondly, it indicates to practitioners the documents providing useful ideas on\nthe adoption of system dynamics for managing common goods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 09:53:19 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sardi", "A.", ""], ["Sorano", "E.", ""]]}, {"id": "2102.04288", "submitter": "Nikita Korzhitskii", "authors": "Nikita Korzhitskii and Niklas Carlsson", "title": "Revocation Statuses on the Internet", "comments": "Accepted to Passive and Active Measurement Conference 2021. Author's\n  edition, 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern Internet is highly dependent on the trust communicated via X.509\ncertificates. However, in some cases certificates become untrusted and it is\nnecessary to revoke them. In practice, the problem of secure certificate\nrevocation has not yet been solved, and today no revocation procedure (similar\nto Certificate Transparency w.r.t. certificate issuance) has been adopted to\nprovide transparent and immutable history of all revocations. Instead, the\nstatus of most certificates can only be checked with Online Certificate Status\nProtocol (OCSP) and/or Certificate Revocation Lists (CRLs). In this paper, we\npresent the first longitudinal characterization of the revocation statuses\ndelivered by CRLs and OCSP servers from the time of certificate expiration to\nstatus disappearance. The analysis captures the status history of over 1\nmillion revoked certificates, including 773K certificates mass-revoked by Let's\nEncrypt. Our characterization provides a new perspective on the Internet's\nrevocation rates, quantifies how short-lived the revocation statuses are,\nhighlights differences in revocation practices within and between different\nCAs, and captures biases and oddities in the handling of revoked certificates.\nCombined, the findings motivate the development and adoption of a revocation\ntransparency standard.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 15:46:23 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Korzhitskii", "Nikita", ""], ["Carlsson", "Niklas", ""]]}, {"id": "2102.04681", "submitter": "Dennis Bautembach", "authors": "Dennis Bautembach, Iason Oikonomidis, Antonis Argyros", "title": "Multi-GPU SNN Simulation with Static Load Balancing", "comments": "Camera-ready version, accepted to IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a SNN simulator which scales to millions of neurons, billions of\nsynapses, and 8 GPUs. This is made possible by 1) a novel, cache-aware spike\ntransmission algorithm 2) a model parallel multi-GPU distribution scheme and 3)\na static, yet very effective load balancing strategy. The simulator further\nfeatures an easy to use API and the ability to create custom models. We compare\nthe proposed simulator against two state of the art ones on a series of\nbenchmarks using three well-established models. We find that our simulator is\nfaster, consumes less memory, and scales linearly with the number of GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 07:07:34 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 18:50:23 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bautembach", "Dennis", ""], ["Oikonomidis", "Iason", ""], ["Argyros", "Antonis", ""]]}, {"id": "2102.05204", "submitter": "Tongping Liu", "authors": "Xin Zhao (University of Massachusetts Amherst), Jin Zhou (University\n  of Massachusetts Amherst), Hui Guan (University of Massachusetts Amherst),\n  Wei Wang (University of Texas at San Antonio), Xu Liu (North Carolina State\n  University), Tongping Liu (University of Massachusetts Amherst)", "title": "NumaPerf: Predictive and Full NUMA Profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Parallel applications are extremely challenging to achieve the optimal\nperformance on the NUMA architecture, which necessitates the assistance of\nprofiling tools. However, existing NUMA-profiling tools share some similar\nshortcomings, such as portability, effectiveness, and helpfulness issues. This\npaper proposes a novel profiling tool - NumaPerf - that overcomes these issues.\nNumaPerf aims to identify potential performance issues for any NUMA\narchitecture, instead of only on the current hardware. To achieve this,\nNumaPerf focuses on memory sharing patterns between threads, instead of real\nremote accesses. NumaPerf further detects potential thread migrations and load\nimbalance issues that could significantly affect the performance but are\nomitted by existing profilers. NumaPerf also separates cache coherence issues\nthat may require different fix strategies. Based on our extensive evaluation,\nNumaPerf is able to identify more performance issues than any existing tool,\nwhile fixing these bugs leads to up to 5.94x performance speedup.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:07:41 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Zhao", "Xin", "", "University of Massachusetts Amherst"], ["Zhou", "Jin", "", "University\n  of Massachusetts Amherst"], ["Guan", "Hui", "", "University of Massachusetts Amherst"], ["Wang", "Wei", "", "University of Texas at San Antonio"], ["Liu", "Xu", "", "North Carolina State\n  University"], ["Liu", "Tongping", "", "University of Massachusetts Amherst"]]}, {"id": "2102.05297", "submitter": "Ji\\v{r}\\'i Filipovi\\v{c}", "authors": "Ji\\v{r}\\'i Filipovi\\v{c} and Jana Hozzov\\'a and Amin Nezarat and\n  Jaroslav O\\v{l}ha and Filip Petrovi\\v{c}", "title": "Using hardware performance counters to speed up autotuning convergence\n  on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, GPU accelerators are commonly used to speed up general-purpose\ncomputing tasks on a variety of hardware. However, due to the diversity of GPU\narchitectures and processed data, optimization of codes for a particular type\nof hardware and specific data characteristics can be extremely challenging. The\nautotuning of performance-relevant source-code parameters allows for automatic\noptimization of applications and keeps their performance portable. Although the\nautotuning process typically results in code speed-up, searching the tuning\nspace can bring unacceptable overhead if (i) the tuning space is vast and full\nof poorly-performing implementations, or (ii) the autotuning process has to be\nrepeated frequently because of changes in processed data or migration to\ndifferent hardware.\n  In this paper, we introduce a novel method for searching tuning spaces. The\nmethod takes advantage of collecting hardware performance counters (also known\nas profiling counters) during empirical tuning. Those counters are used to\nnavigate the searching process towards faster implementations. The method\nrequires the tuning space to be sampled on any GPU. It builds a\nproblem-specific model, which can be used during autotuning on various, even\npreviously unseen inputs or GPUs. Using a set of five benchmarks, we\nexperimentally demonstrate that our method can speed up autotuning when an\napplication needs to be ported to different hardware or when it needs to\nprocess data with different characteristics. We also compared our method to\nstate of the art and show that our method is superior in terms of the number of\nsearching steps and typically outperforms other searches in terms of\nconvergence time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 07:42:39 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Filipovi\u010d", "Ji\u0159\u00ed", ""], ["Hozzov\u00e1", "Jana", ""], ["Nezarat", "Amin", ""], ["O\u013eha", "Jaroslav", ""], ["Petrovi\u010d", "Filip", ""]]}, {"id": "2102.05405", "submitter": "Andrea Vandin", "authors": "Andrea Vandin, Daniele Giachini, Francesco Lamperti, Francesca\n  Chiaromonte", "title": "Automated and Distributed Statistical Analysis of Economic Agent-Based\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.MA cs.PF q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to the statistical analysis of simulation models\nand, especially, agent-based models (ABMs). Our main goal is to provide a fully\nautomated and model-independent tool-kit to inspect simulations and perform\ncounterfactual analysis. Our approach: (i) is easy-to-use by the modeller, (ii)\nimproves reproducibility of results, (iii) optimizes running time given the\nmodeller's machine, (iv) automatically chooses the number of required\nsimulations and simulation steps to reach user-specified statistical\nconfidence, and (v) automatically performs a variety of statistical tests. In\nparticular, our framework is designed to distinguish the transient dynamics of\nthe model from its steady-state behaviour (if any), estimate properties of the\nmodel in both \"phases\", and provide indications on the ergodic (or non-ergodic)\nnature of the simulated processes -- which, in turns allows one to gauge the\nreliability of a steady-state analysis. Estimates are equipped with statistical\nguarantees, allowing for robust comparisons across computational experiments.\nTo demonstrate the effectiveness of our approach, we apply it to two models\nfrom the literature: a large scale macro-financial ABM and a small scale\nprediction market model. Compared to prior analyses of these models, we obtain\nnew insights and we are able to identify and fix some erroneous conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 12:39:34 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Vandin", "Andrea", ""], ["Giachini", "Daniele", ""], ["Lamperti", "Francesco", ""], ["Chiaromonte", "Francesca", ""]]}, {"id": "2102.07477", "submitter": "Ahmed M. Abdelmoniem", "authors": "Ahmed M. Abdelmoniem and Brahim Bensaou", "title": "T-RACKs: A Faster Recovery Mechanism for TCP in Data Center Networks", "comments": "Accepted for Publication in ACM/IEEE Transactions on Networking (ToN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud interactive data-driven applications generate swarms of small TCP flows\nthat compete for the small buffer space in data-center switches. Such\napplications require a short flow completion time (FCT) to perform their jobs\neffectively. However, TCP is oblivious to the composite nature of application\ndata and artificially inflates the FCT of such flows by several orders of\nmagnitude. This is due to TCP's Internet-centric design that fixes the\nretransmission timeout (RTO) to be at least hundreds of milliseconds. To better\nunderstand this problem, in this paper, we use empirical measurements in a\nsmall testbed to study, at a microscopic level, the effects of various types of\npacket losses on TCP's performance. In particular, we single out packet losses\nthat impact the tail end of small flows, as well as bursty losses, that span a\nsignificant fraction of the small congestion window of TCP flows in\ndata-centers, to show a non-negligible effect on the FCT. Based on this, we\npropose the so-called, timely-retransmitted ACKs (or T-RACKs), a simple loss\nrecovery mechanism to conceal the drawbacks of the long RTO even in the\npresence of heavy packet losses. Interestingly enough, T-RACKS achieves this\ntransparently to TCP itself as it does not require any change to TCP in the\ntenant's virtual machine (VM). T-RACKs can be implemented as a software shim\nlayer in the hypervisor between the VMs and server's NIC or in hardware as a\nnetworking function in a SmartNIC. Simulation and real testbed results show\nthat T-RACKs achieves remarkable performance improvements.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 11:36:33 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Abdelmoniem", "Ahmed M.", ""], ["Bensaou", "Brahim", ""]]}, {"id": "2102.07500", "submitter": "Ahmed Mohamed Abdelmoniem Sayed", "authors": "Ahmed M. Abdelmoniem and Chen-Yu Ho and Pantelis Papageorgiou and\n  Muhammad Bilal and Marco Canini", "title": "On the Impact of Device and Behavioral Heterogeneity in Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is becoming a popular paradigm for collaborative\nlearning over distributed, private datasets owned by non-trusting entities. FL\nhas seen successful deployment in production environments, and it has been\nadopted in services such as virtual keyboards, auto-completion, item\nrecommendation, and several IoT applications. However, FL comes with the\nchallenge of performing training over largely heterogeneous datasets, devices,\nand networks that are out of the control of the centralized FL server.\nMotivated by this inherent setting, we make a first step towards characterizing\nthe impact of device and behavioral heterogeneity on the trained model. We\nconduct an extensive empirical study spanning close to 1.5K unique\nconfigurations on five popular FL benchmarks. Our analysis shows that these\nsources of heterogeneity have a major impact on both model performance and\nfairness, thus sheds light on the importance of considering heterogeneity in FL\nsystem design.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:04:38 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Abdelmoniem", "Ahmed M.", ""], ["Ho", "Chen-Yu", ""], ["Papageorgiou", "Pantelis", ""], ["Bilal", "Muhammad", ""], ["Canini", "Marco", ""]]}, {"id": "2102.07660", "submitter": "Tarek Ramadan", "authors": "Nathan Pinnow, Tarek Ramadan, Tanzima Z. Islam, Chase Phelps,\n  Jayaraman J. Thiagarajan", "title": "Comparative Code Structure Analysis using Deep Learning for Performance\n  Prediction", "comments": "11 pages, To appear in proceedings of the International Symposium on\n  Performance Analysis of Systems and Software (ISPASS) 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance analysis has always been an afterthought during the application\ndevelopment process, focusing on application correctness first. The learning\ncurve of the existing static and dynamic analysis tools are steep, which\nrequires understanding low-level details to interpret the findings for\nactionable optimizations. Additionally, application performance is a function\nof an infinite number of unknowns stemming from the application-, runtime-, and\ninteractions between the OS and underlying hardware, making it difficult, if\nnot impossible, to model using any deep learning technique, especially without\na large labeled dataset. In this paper, we address both of these problems by\npresenting a large corpus of a labeled dataset for the community and take a\ncomparative analysis approach to mitigate all unknowns except their source code\ndifferences between different correct implementations of the same problem. We\nput the power of deep learning to the test for automatically extracting\ninformation from the hierarchical structure of abstract syntax trees to\nrepresent source code. This paper aims to assess the feasibility of using\npurely static information (e.g., abstract syntax tree or AST) of applications\nto predict performance change based on the change in code structure. This\nresearch will enable performance-aware application development since every\nversion of the application will continue to contribute to the corpora, which\nwill enhance the performance of the model. Our evaluations of several deep\nembedding learning methods demonstrate that tree-based Long Short-Term Memory\n(LSTM) models can leverage the hierarchical structure of source-code to\ndiscover latent representations and achieve up to 84% (individual problem) and\n73% (combined dataset with multiple of problems) accuracy in predicting the\nchange in performance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 16:59:12 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 01:12:51 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Pinnow", "Nathan", ""], ["Ramadan", "Tarek", ""], ["Islam", "Tanzima Z.", ""], ["Phelps", "Chase", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "2102.07731", "submitter": "Johannes Sedlmeir", "authors": "Tobias Guggenberger and Johannes Sedlmeir and Gilbert Fridgen and\n  Andr\\'e Luckow", "title": "An In-Depth Investigation of Performance Characteristics of Hyperledger\n  Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CR cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Private permissioned blockchains, such as Hyperledger Fabric, are widely\ndeployed across the industry to facilitate cross-organizational processes and\npromise improved performance compared to their public counterparts. However,\nthe lack of empirical and theoretical results prevent precise prediction of the\nreal-world performance. We address this gap by conducting an in-depth\nperformance analysis of Hyperledger Fabric. The paper presents a detailed\ncompilation of various performance characteristics using an enhanced version of\nthe Distributed Ledger Performance Scan. Researchers and practitioners alike\ncan use the results as guidelines to better configure and implement their\nblockchains and utilize the DLPS framework to conduct their measurements.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:30:43 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Guggenberger", "Tobias", ""], ["Sedlmeir", "Johannes", ""], ["Fridgen", "Gilbert", ""], ["Luckow", "Andr\u00e9", ""]]}, {"id": "2102.08323", "submitter": "Mahdi Nikdast", "authors": "Ebadollah Taheri and Ryan G. Kim and Mahdi Nikdast", "title": "AdEle: An Adaptive Congestion-and-Energy-Aware Elevator Selection for\n  Partially Connected 3D NoCs", "comments": "This paper will be published in Proc. IEEE/ACM Design Automation\n  Conference (DAC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By lowering the number of vertical connections in fully connected 3D\nnetworks-on-chip (NoCs), partially connected 3D NoCs (PC-3DNoCs) help alleviate\nreliability and fabrication issues. This paper proposes a novel, adaptive\ncongestion- and energy-aware elevator-selection scheme called AdEle to improve\nthe traffic distribution in PC-3DNoCs. AdEle employs an offline multi-objective\nsimulated-annealing-based algorithm to find good elevator subsets and an online\nelevator selection policy to enhance elevator selection during routing.\nCompared to the state-of-the-art techniques under different real-application\ntraffics and configuration scenarios, AdEle improves the network latency by\n14.9% on average (up to 21.4%) with less than 10.5% energy consumption\noverhead.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:10:26 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Taheri", "Ebadollah", ""], ["Kim", "Ryan G.", ""], ["Nikdast", "Mahdi", ""]]}, {"id": "2102.08505", "submitter": "Adetokunbo Adedoyin", "authors": "Adetokunbo A. Adedoyin, Christian F. A. Negre, Jamaludin Mohd-Yusof,\n  Nicolas Bock, Daniel Osei-Kuffuor, Jean-Luc Fattebert, Michael E. Wall,\n  Anders M. N. Niklasson, Susan M. Mniszewski", "title": "Performance Optimizations of Recursive Electronic Structure Solvers\n  targeting Multi-Core Architectures (LA-UR-20-26665)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we rapidly approach the frontiers of ultra large computing resources,\nsoftware optimization is becoming of paramount interest to scientific\napplication developers interested in efficiently leveraging all available\non-Node computing capabilities and thereby improving a requisite science per\nwatt metric. The scientific application of interest here is the Basic Math\nLibrary (BML) that provides a singular interface for linear algebra operation\nfrequently used in the Quantum Molecular Dynamics (QMD) community. The\nprovisioning of a singular interface indicates the presence of an abstraction\nlayer which in-turn suggests commonalities in the code-base and therefore any\noptimization or tuning introduced in the core of code-base has the ability to\npositively affect the performance of the aforementioned library as a whole.\nWith that in mind, we proceed with this investigation by performing a survey of\nthe entirety of the BML code-base, and extract, in form of micro-kernels,\ncommon snippets of code. We introduce several optimization strategies into\nthese micro-kernels including 1.) Strength Reduction 2.) Memory Alignment for\nlarge arrays 3.) Non Uniform Memory Access (NUMA) aware allocations to enforce\ndata locality and 4.) appropriate thread affinity and bindings to enhance the\noverall multi-threaded performance. After introducing these optimizations, we\nbenchmark the micro-kernels and compare the run-time before and after\noptimization for several target architectures. Finally we use the results as a\nguide to propagating the optimization strategies into the BML code-base. As a\ndemonstration, herein, we test the efficacy of these optimization strategies by\ncomparing the benchmark and optimized versions of the code.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:04:19 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Adedoyin", "Adetokunbo A.", ""], ["Negre", "Christian F. A.", ""], ["Mohd-Yusof", "Jamaludin", ""], ["Bock", "Nicolas", ""], ["Osei-Kuffuor", "Daniel", ""], ["Fattebert", "Jean-Luc", ""], ["Wall", "Michael E.", ""], ["Niklasson", "Anders M. N.", ""], ["Mniszewski", "Susan M.", ""]]}, {"id": "2102.08778", "submitter": "Giacomo Da Col", "authors": "Giacomo Da Col and Erich Teppan", "title": "Large-Scale Benchmarks for the Job Shop Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report contains the description of two novel job shop scheduling\nbenchmarks that resemble instances of real scheduling problem as they appear in\nindustry. In particular, the aim was to provide large-scale benchmarks (up to 1\nmillion operations) to test the state-of-the-art scheduling solutions on\nproblems that are closer to what occurs in a real industrial context. The first\nbenchmark is an extension of the well known Taillard benchmark (1992), while\nthe second is a collection of scheduling instances with a known-optimum\nsolution.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:18:48 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 15:07:02 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Da Col", "Giacomo", ""], ["Teppan", "Erich", ""]]}, {"id": "2102.08814", "submitter": "Majid Raeis", "authors": "Majid Raeis, S. Jamaloddin Golestani", "title": "Distributed Fair Scheduling for Information Exchange in Multi-Agent\n  Systems", "comments": "9 pages, Accepted at ICAPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information exchange is a crucial component of many real-world multi-agent\nsystems. However, the communication between the agents involves two major\nchallenges: the limited bandwidth, and the shared communication medium between\nthe agents, which restricts the number of agents that can simultaneously\nexchange information. While both of these issues need to be addressed in\npractice, the impact of the latter problem on the performance of the\nmulti-agent systems has often been neglected. This becomes even more important\nwhen the agents' information or observations have different importance, in\nwhich case the agents require different priorities for accessing the medium and\nsharing their information. Representing the agents' priorities by fairness\nweights and normalizing each agent's share by the assigned fairness weight, the\ngoal can be expressed as equalizing the agents' normalized shares of the\ncommunication medium. To achieve this goal, we adopt a queueing theoretic\napproach and propose a distributed fair scheduling algorithm for providing\nweighted fairness in single-hop networks. Our proposed algorithm guarantees an\nupper-bound on the normalized share disparity among any pair of agents. This\ncan particularly improve the short-term fairness, which is important in\nreal-time applications. Moreover, our scheduling algorithm adjusts itself\ndynamically to achieve a high throughput at the same time. The simulation\nresults validate our claims and comparisons with the existing methods show our\nalgorithm's superiority in providing short-term fairness, while achieving a\nhigh throughput.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 15:20:26 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Raeis", "Majid", ""], ["Golestani", "S. Jamaloddin", ""]]}, {"id": "2102.09073", "submitter": "Kian Paimani", "authors": "Kian Paimani", "title": "SonicChain: A Wait-free, Pseudo-Static Approach Toward Concurrency in\n  Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains have a two-sided reputation: they are praised for disrupting some\nof our institutions through innovative technology for good, yet notorious for\nbeing slow and expensive to use. In this work, we tackle this issue with\nconcurrency, yet we aim to take a radically different approach by valuing\nsimplicity. We embrace the simplicity through two steps: first, we formulate a\nsimple runtime mechanism to deal with conflicts called concurrency delegation.\nThis method is much simpler and has less overhead, particularly in scenarios\nwhere conflicting transactions are relatively rare. Moreover, to further reduce\nthe number of conflicting transactions, we propose using static annotations\nattached to each transaction, provided by the programmer. These annotations are\npseudo-static: they are static with respect to the lifetime of the transaction,\nand therefore are free to use information such as the origin and parameters of\nthe transaction. We propose a distributor component that can use the output of\nthis pseudo-static annotations and use them to effectively distribute\ntransactions between threads in the least-conflicting way. We name the outcome\nof a system combining concurrency delegation and pseudo-static annotations as\nSonicChain. We evaluate SonicChain for both validation and authoring tasks\nagainst a common workload in blockchains, namely, balance transfers, and\nobserve that it performs expectedly well while introducing very little overhead\nand additional complexity to the system.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 19:42:28 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Paimani", "Kian", ""]]}, {"id": "2102.09166", "submitter": "Sungho Lee", "authors": "Sungho Lee, Minsu Kim, Jemin Lee, Min-Soo Kim, Ruei-Hau Hsu, Tony Q.\n  S. Quek", "title": "Latency Modeling of Hyperledger Fabric for Blockchain-enabled IoT\n  Networks", "comments": "11 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the worldwide growth of the Internet of Things (IoT), new security\nrequirements have been raised, such as strong security, data integrity, or\nprivacy preservation. While blockchain technology is capable of addressing the\nissues, HLF, which is a private blockchain platform, has been leveraged for\nblockchain-enabled IoT (BC-IoT) networks. From the perspective of an IoT\napplication, however, the additional processing time spent in HLF may emerge as\nanother problem because many IoT applications handle real-time and\nlatency-critical jobs. In other words, it is necessary to estimate the HLF\nlatency before practical deployment of BC-IoT networks. However, this problem\nhas still remained unresolved because the distribution of the HLF latency has\nnot been revealed until the present. In this paper, therefore, we develop an\nHLF latency distribution model using probability distribution fitting and show\nthe latency can be modeled with the gamma distribution. Furthermore, we define\nthree influential HLF parameters on the HLF latency. Their impacts on the HLF\nlatency are analyzed to provide strategies for decreasing and minimizing the\nmean HLF latency.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 05:51:22 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 12:54:41 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Lee", "Sungho", ""], ["Kim", "Minsu", ""], ["Lee", "Jemin", ""], ["Kim", "Min-Soo", ""], ["Hsu", "Ruei-Hau", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "2102.09880", "submitter": "Alexander Felfernig", "authors": "Alexander Felfernig and Rouven Walter and Jose A. Galindo and David\n  Benavides and Seda Polat-Erdeniz and Muesluem Atas and Stefan Reiterer", "title": "Anytime Diagnosis for Reconfiguration", "comments": "Preprint, cite as: A. Felfernig. R. Walter, J. Galindo, D. Benavides,\n  M. Atas, S. Polat-Erdeniz, and S. Reiterer. Anytime Diagnosis for\n  Reconfiguration. Journal of Intelligent Information Systems, vol. 51, pp.\n  161-182, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many domains require scalable algorithms that help to determine diagnoses\nefficiently and often within predefined time limits. Anytime diagnosis is able\nto determine solutions in such a way and thus is especially useful in real-time\nscenarios such as production scheduling, robot control, and communication\nnetworks management where diagnosis and corresponding reconfiguration\ncapabilities play a major role. Anytime diagnosis in many cases comes along\nwith a trade-off between diagnosis quality and the efficiency of diagnostic\nreasoning. In this paper we introduce and analyze FlexDiag which is an anytime\ndirect diagnosis approach. We evaluate the algorithm with regard to performance\nand diagnosis quality using a configuration benchmark from the domain of\nfeature models and an industrial configuration knowledge base from the\nautomotive domain. Results show that FlexDiag helps to significantly increase\nthe performance of direct diagnosis search with corresponding quality tradeoffs\nin terms of minimality and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 11:45:52 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Felfernig", "Alexander", ""], ["Walter", "Rouven", ""], ["Galindo", "Jose A.", ""], ["Benavides", "David", ""], ["Polat-Erdeniz", "Seda", ""], ["Atas", "Muesluem", ""], ["Reiterer", "Stefan", ""]]}, {"id": "2102.10195", "submitter": "Satyabrata Sarangi", "authors": "Satyabrata Sarangi and Bevan Baas", "title": "DeepScaleTool : A Tool for the Accurate Estimation of Technology Scaling\n  in the Deep-Submicron Era", "comments": "This paper has been accepted for the 2021 IEEE International\n  Symposium on Circuits and Systems. \\copyright 2021 IEEE. Copyright statements\n  are posted on the first page. 5 Pages, 5 Figures", "journal-ref": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "doi": "10.1109/ISCAS51556.2021.9401196", "report-no": null, "categories": "eess.SY cs.AR cs.PF cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The estimation of classical CMOS \"constant-field\" or \"Dennard\" scaling\nmethods that define scaling factors for various dimensional and electrical\nparameters have become less accurate in the deep-submicron regime, which drives\nthe need for better estimation approaches especially in the educational and\nresearch domains. We present DeepScaleTool, a tool for the accurate estimation\nof deep-submicron technology scaling by modeling and curve fitting published\ndata by a leading commercial fabrication company for silicon fabrication\ntechnology generations from 130~nm to 7~nm for the key parameters of area,\ndelay, and energy. Compared to 10~nm--7~nm scaling data published by a leading\nfoundry, the DeepScaleTool achieves an error of 1.7% in area, 2.5% in delay,\nand 5% in power. This compares favorably with another leading academic\nestimation method that achieves an error of 24% in area, 9.1% in delay, and\n24.9% in power.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:53:44 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Sarangi", "Satyabrata", ""], ["Baas", "Bevan", ""]]}, {"id": "2102.10245", "submitter": "Ahmed E. Helal", "authors": "Ahmed E. Helal, Jan Laukemann, Fabio Checconi, Jesmin Jahan Tithi,\n  Teresa Ranadive, Fabrizio Petrini, Jeewhan Choi", "title": "ALTO: Adaptive Linearized Storage of Sparse Tensors", "comments": "Accepted to ICS 2021", "journal-ref": null, "doi": "10.1145/3447818.3461703", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of high-dimensional sparse data is becoming increasingly popular\nin many important domains. However, real-world sparse tensors are challenging\nto process due to their irregular shapes and data distributions. We propose the\nAdaptive Linearized Tensor Order (ALTO) format, a novel mode-agnostic (general)\nrepresentation that keeps neighboring nonzero elements in the multi-dimensional\nspace close to each other in memory. To generate the indexing metadata, ALTO\nuses an adaptive bit encoding scheme that trades off index computations for\nlower memory usage and more effective use of memory bandwidth. Moreover, by\ndecoupling its sparse representation from the irregular spatial distribution of\nnonzero elements, ALTO eliminates the workload imbalance and greatly reduces\nthe synchronization overhead of tensor computations. As a result, the parallel\nperformance of ALTO-based tensor operations becomes a function of their\ninherent data reuse. On a gamut of tensor datasets, ALTO outperforms an oracle\nthat selects the best state-of-the-art format for each dataset, when used in\nkey tensor decomposition operations. Specifically, ALTO achieves a geometric\nmean speedup of 8X over the best mode-agnostic (coordinate and hierarchical\ncoordinate) formats, while delivering a geometric mean compression ratio of\n4.3X relative to the best mode-specific (compressed sparse fiber) formats.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 03:32:08 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 07:36:07 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Helal", "Ahmed E.", ""], ["Laukemann", "Jan", ""], ["Checconi", "Fabio", ""], ["Tithi", "Jesmin Jahan", ""], ["Ranadive", "Teresa", ""], ["Petrini", "Fabrizio", ""], ["Choi", "Jeewhan", ""]]}, {"id": "2102.10353", "submitter": "Michel Rottleuthner", "authors": "Michel Rottleuthner, Thomas C. Schmidt, Matthias W\\\"ahlisch", "title": "FlexClock: Generic Clock Reconfiguration for Low-end IoT Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clock configuration within constrained general-purpose microcontrollers takes\na key role in tuning performance, power consumption, and timing accuracy of\napplications in the Internet of Things (IoT). Subsystems governing the\nunderlying clock tree must nonetheless cope with a huge parameter space,\ncomplex dependencies, and dynamic constraints. Manufacturers expose the\nunderlying functions in very diverse ways, which leads to specialized\nimplementations of low portability. In this paper, we propose FlexClock, an\napproach for generic online clock reconfiguration on constrained IoT devices.\nWe argue that (costly) generic clock configuration of general purpose computers\nand powerful mobile devices need to slim down to the lower end of the device\nspectrum. In search of a generalized solution, we identify recurring patterns\nand building blocks, which we use to decompose clock trees into independent,\nreusable components. With this segmentation we derive an abstract\nrepresentation of vendor-specific clock trees, which then can be dynamically\nreconfigured at runtime. We evaluate our implementation on common hardware. Our\nmeasurements demonstrate how FlexClock significantly improves peak power\nconsumption and energy efficiency by enabling dynamic voltage and frequency\nscaling (DVFS) in a platform-agnostic way.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 14:11:47 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rottleuthner", "Michel", ""], ["Schmidt", "Thomas C.", ""], ["W\u00e4hlisch", "Matthias", ""]]}, {"id": "2102.10786", "submitter": "Hao Jiang", "authors": "Hao Jiang, Shuangkaisheng Bi, and Linglong Dai", "title": "Residual-Aided End-to-End Learning of Communication System without Known\n  Channel", "comments": "20 pages, 7 figures. To address the gradient vanishing and\n  overfitting problems of the GAN-based training scheme in E2E learning of\n  communication system, this paper proposes a residual aided GAN (RA-GAN) based\n  training scheme. Simulation codes will be provided to reproduce the results\n  presented in this paper after publication:\n  http://oa.ee.tsinghua.edu.cn/dailinglong/publications/publications.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.PF eess.SP math.IT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Leveraging powerful deep learning techniques, the end-to-end (E2E) learning\nof communication system is able to outperform the classical communication\nsystem. Unfortunately, this communication system cannot be trained by deep\nlearning without known channel. To deal with this problem, a generative\nadversarial network (GAN) based training scheme has been recently proposed to\nimitate the real channel. However, the gradient vanishing and overfitting\nproblems of GAN will result in the serious performance degradation of E2E\nlearning of communication system. To mitigate these two problems, we propose a\nresidual aided GAN (RA-GAN) based training scheme in this paper. Particularly,\ninspired by the idea of residual learning, we propose a residual generator to\nmitigate the gradient vanishing problem by realizing a more robust gradient\nbackpropagation. Moreover, to cope with the overfitting problem, we reconstruct\nthe loss function for training by adding a regularizer, which limits the\nrepresentation ability of RA-GAN. Simulation results show that the trained\nresidual generator has better generation performance than the conventional\ngenerator, and the proposed RA-GAN based training scheme can achieve the\nnear-optimal block error rate (BLER) performance with a negligible\ncomputational complexity increase in both the theoretical channel model and the\nray-tracing based channel dataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 05:47:49 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jiang", "Hao", ""], ["Bi", "Shuangkaisheng", ""], ["Dai", "Linglong", ""]]}, {"id": "2102.10837", "submitter": "Subho Sankar Banerjee", "authors": "Subho S. Banerjee, Saurabh Jha, Zbigniew T. Kalbarczyk, Ravishankar K.\n  Iyer", "title": "BayesPerf: Minimizing Performance Monitoring Errors Using Bayesian\n  Statistics", "comments": null, "journal-ref": "Proceedings of the Twenty-Sixth International Conference on\n  Architectural Support for Programming Languages and Operating Systems (ASPLOS\n  21), 2021", "doi": "10.1145/3445814.3446739", "report-no": null, "categories": "cs.DC cs.AI cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware performance counters (HPCs) that measure low-level architectural and\nmicroarchitectural events provide dynamic contextual information about the\nstate of the system. However, HPC measurements are error-prone due to non\ndeterminism (e.g., undercounting due to event multiplexing, or OS\ninterrupt-handling behaviors). In this paper, we present BayesPerf, a system\nfor quantifying uncertainty in HPC measurements by using a domain-driven\nBayesian model that captures microarchitectural relationships between HPCs to\njointly infer their values as probability distributions. We provide the design\nand implementation of an accelerator that allows for low-latency and low-power\ninference of the BayesPerf model for x86 and ppc64 CPUs. BayesPerf reduces the\naverage error in HPC measurements from 40.1% to 7.6% when events are being\nmultiplexed. The value of BayesPerf in real-time decision-making is illustrated\nwith a simple example of scheduling of PCIe transfers.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 09:00:14 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Banerjee", "Subho S.", ""], ["Jha", "Saurabh", ""], ["Kalbarczyk", "Zbigniew T.", ""], ["Iyer", "Ravishankar K.", ""]]}, {"id": "2102.10997", "submitter": "Subhash Sagar Mr.", "authors": "Subhash Sagar, Adnan Mahmood, Quan Z. Sheng, and Wei Emma Zhang", "title": "Trust Computational Heuristic for Social Internet of Things: A Machine\n  Learning-based Approach", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Internet of Things (IoT) is an evolving network of billions of\ninterconnected physical objects, such as numerous sensors, smartphones,\nwearables, and embedded devices. These physical objects, generally referred to\nas the smart objects, when deployed in the real-world aggregates useful\ninformation from their surrounding environment. As-of-late, this notion of IoT\nhas been extended to incorporate the social networking facets which have led to\nthe promising paradigm of the `Social Internet of Things' (SIoT). In SIoT, the\ndevices operate as an autonomous agent and provide an exchange of information\nand service discovery in an intelligent manner by establishing social\nrelationships among them with respect to their owners. Trust plays an important\nrole in establishing trustworthy relationships among the physical objects and\nreduces probable risks in the decision-making process. In this paper, a trust\ncomputational model is proposed to extract individual trust features in a SIoT\nenvironment. Furthermore, a machine learning-based heuristic is used to\naggregate all the trust features in order to ascertain an aggregate trust\nscore. Simulation results illustrate that the proposed trust-based model\nisolates the trustworthy and untrustworthy nodes within the network in an\nefficient manner.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 10:52:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Sagar", "Subhash", ""], ["Mahmood", "Adnan", ""], ["Sheng", "Quan Z.", ""], ["Zhang", "Wei Emma", ""]]}, {"id": "2102.11198", "submitter": "Ruslan Savchenko", "authors": "Ruslan Savchenko", "title": "Reading from External Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern external memory is represented by several device classes. At present,\nHDD, SATA SSD and NVMe SSD are widely used. Recently ultra-low latency SSD such\nas Intel Optane became available on the market. Each of these types exhibits\nit's own pattern for throughput, latency and parallelism. To achieve the\nhighest performance one has to pick an appropriate I/O interface provided by\nthe operating system. In this work we present a detailed overview and\nevaluation of modern storage reading performance with regard to available Linux\nsynchronous and asynchronous interfaces. While throughout this work we aim for\nthe highest throughput we also measure latency and CPU usage. We provide this\nreport in hope the detailed results could be interesting to both researchers\nand practitioners.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 17:24:08 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Savchenko", "Ruslan", ""]]}, {"id": "2102.12740", "submitter": "Aravind Sankaran", "authors": "Aravind Sankaran and Paolo Bientinesi", "title": "Performance Comparison for Scientific Computations on the Edge via\n  Relative Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical Internet-of-Things setting that involves scientific\napplications, a target computation can be evaluated in many different ways\ndepending on the split of computations among various devices. On the one hand,\ndifferent implementations (or algorithms)--equivalent from a mathematical\nperspective--might exhibit significant difference in terms of performance. On\nthe other hand, some of the implementations are likely to show similar\nperformance characteristics. In this paper, we focus on analyzing the\nperformance of a given set of algorithms by clustering them into performance\nclasses. To this end, we use a measurement-based approach to evaluate and score\nalgorithms based on pair-wise comparisons; we refer to this approach\nas\"Relative performance analysis\". Each comparison yields one of three\noutcomes: one algorithm can be \"better\", \"worse\", or \"equivalent\" to another;\nthose algorithms evaluating to have equivalent performance are merged into the\nsame performance class. We show that our clustering methodology facilitates\nalgorithm selection with respect to more than one metric; for instance, from\nthe subset of equivalently fast algorithms, one could then select an algorithm\nthat consumes the least energy on a certain device.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 09:10:27 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 09:37:21 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Sankaran", "Aravind", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "2102.12848", "submitter": "Zihan Jiang", "authors": "Zihan Jiang, Wanling Gao, Fei Tang, Xingwang Xiong, Lei Wang, Chuanxin\n  Lan, Chunjie Luo, Hongxiao Li, Jianfeng Zhan", "title": "HPC AI500: Representative, Repeatable and Simple HPC AI Benchmarking", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.00279", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years witness a trend of applying large-scale distributed deep\nlearning algorithms (HPC AI) in both business and scientific computing areas,\nwhose goal is to speed up the training time to achieve a state-of-the-art\nquality. The HPC AI benchmarks accelerate the process. Unfortunately,\nbenchmarking HPC AI systems at scale raises serious challenges. This paper\npresents a representative, repeatable and simple HPC AI benchmarking\nmethodology. Among the seventeen AI workloads of AIBench Training -- by far the\nmost comprehensive AI Training benchmarks suite -- we choose two representative\nand repeatable AI workloads. The selected HPC AI benchmarks include both\nbusiness and scientific computing: Image Classification and Extreme Weather\nAnalytics. To rank HPC AI systems, we present a new metric named Valid FLOPS,\nemphasizing both throughput performance and a target quality. The\nspecification, source code, datasets, and HPC AI500 ranking numbers are\npublicly available from \\url{https://www.benchcouncil.org/HPCAI500/}.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 13:40:17 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Jiang", "Zihan", ""], ["Gao", "Wanling", ""], ["Tang", "Fei", ""], ["Xiong", "Xingwang", ""], ["Wang", "Lei", ""], ["Lan", "Chuanxin", ""], ["Luo", "Chunjie", ""], ["Li", "Hongxiao", ""], ["Zhan", "Jianfeng", ""]]}]