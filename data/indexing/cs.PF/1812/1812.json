[{"id": "1812.00669", "submitter": "Christian Pinto", "authors": "Christian Pinto, Yiannis Gkoufas, Andrea Reale, Seetharami Seelam,\n  Steven Eliuk", "title": "Hoard: A Distributed Data Caching System to Accelerate Deep Learning\n  Training on the Cloud", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning system architects strive to design a balanced system where the\ncomputational accelerator -- FPGA, GPU, etc, is not starved for data. Feeding\ntraining data fast enough to effectively keep the accelerator utilization high\nis difficult when utilizing dedicated hardware like GPUs. As accelerators are\ngetting faster, the storage media \\& data buses feeding the data have not kept\npace and the ever increasing size of training data further compounds the\nproblem. We describe the design and implementation of a distributed caching\nsystem called Hoard that stripes the data across fast local disks of multiple\nGPU nodes using a distributed file system that efficiently feeds the data to\nensure minimal degradation in GPU utilization due to I/O starvation. Hoard can\ncache the data from a central storage system before the start of the job or\nduring the initial execution of the job and feeds the cached data for\nsubsequent epochs of the same job and for different invocations of the jobs\nthat share the same data requirements, e.g. hyper-parameter tuning. Hoard\nexposes a POSIX file system interface so the existing deep learning frameworks\ncan take advantage of the cache without any modifications. We show that Hoard,\nusing two NVMe disks per node and a distributed file system for caching,\nachieves a 2.1x speed-up over a 10Gb/s NFS central storage system on a 16 GPU\n(4 nodes, 4 GPUs per node) cluster for a challenging AlexNet ImageNet image\nclassification benchmark with 150GB of input dataset. As a result of the\ncaching, Hoard eliminates the I/O bottlenecks introduced by the shared storage\nand increases the utilization of the system by 2x compared to using the shared\nstorage without the cache.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 11:16:01 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Pinto", "Christian", ""], ["Gkoufas", "Yiannis", ""], ["Reale", "Andrea", ""], ["Seelam", "Seetharami", ""], ["Eliuk", "Steven", ""]]}, {"id": "1812.03131", "submitter": "Miltiades Anagnostou", "authors": "Miltiades E. Anagnostou and Maria A. Lambrou", "title": "Playing with and against Hedge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.NI cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hedge has been proposed as an adaptive scheme, which guides an agent's\ndecision in resource selection and distribution problems that can be modeled as\na multi-armed bandit full information game. Such problems are encountered in\nthe areas of computer and communication networks, e.g. network path selection,\nload distribution, network interdiction, and also in problems in the area of\ntransportation. We study Hedge under the assumption that the total loss that\ncan be suffered by the player in each round is upper bounded. In this paper, we\nstudy the worst performance of Hedge.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 10:25:56 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Anagnostou", "Miltiades E.", ""], ["Lambrou", "Maria A.", ""]]}, {"id": "1812.03431", "submitter": "Martin Schreiber", "authors": "Martin Schreiber, Richard Loft", "title": "A Parallel Time-Integrator for Solving the Linearized Shallow Water\n  Equations on the Rotating Sphere", "comments": null, "journal-ref": "Journal of Numerical Linear Algebra with Applications, 2018", "doi": "10.1002/nla.2220", "report-no": null, "categories": "physics.comp-ph cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the stagnation of processor core performance, further reductions in the\ntime-to-solution for geophysical fluid problems are becoming increasingly\ndifficult with standard time integrators. Parallel-in-time exposes and exploits\nadditional parallelism in the time dimension which is inherently sequential in\ntraditional methods. The rational approximation of exponential integrators\n(REXI) method allows taking arbitrarily long time steps based on a sum over a\nnumber of decoupled complex PDEs that can be solved independently massively\nparallel. Hence REXI is assumed to be well suited for modern massively parallel\nsuper computers which are currently trending. To date the study and development\nof the REXI approach has been limited to linearized problems on the periodic 2D\nplane. This work extends the REXI time stepping method to the linear\nshallow-water equations (SWE) on the rotating sphere, thus moving the method\none step closer to solving fully nonlinear fluid problems of geophysical\ninterest on the sphere. The rotating sphere poses particular challenges for\nfinding an efficient solver due to the zonal dependence of the Coriolis term.\nHere we present an efficient REXI solver based on spherical harmonics, showing\nthe results of: a geostrophic balance test, a comparison with alternative time\nstepping methods, an analysis of dispersion relations, indicating superior\nproperties of REXI, and finally a performance comparison on Cheyenne\nsupercomputer. Our results indicate that REXI is not only able to take larger\ntime steps, but that REXI can also be used to gain higher accuracy and\nsignificantly reduced time-to-solution compared to currently existing time\nstepping methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 05:12:57 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 15:51:14 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Schreiber", "Martin", ""], ["Loft", "Richard", ""]]}, {"id": "1812.03650", "submitter": "Tram Truong-Huu", "authors": "Srinikethan Madapuzi Srinivasan, Tram Truong-Huu, Mohan Gurusamy", "title": "Machine Learning-based Link Fault Identification and Localization in\n  Complex Networks", "comments": "This paper has been accepted for publication in the IEEE Internet of\n  Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2019.2908019", "report-no": null, "categories": "cs.NI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of network devices and rapid development in\ninformation technology, networks such as Internet of Things are increasing in\nsize and becoming more complex with heterogeneous wired and wireless links. In\nsuch networks, link faults may result in a link disconnection without immediate\nreplacement or a link reconnection, e.g., a wireless node changes its access\npoint. Identifying whether a link disconnection or a link reconnection has\noccurred and localizing the failed link become a challenging problem. An active\nprobing approach requires a long time to probe the network by sending signaling\nmessages on different paths, thus incurring significant communication delay and\noverhead. In this paper, we adopt a passive approach and develop a three-stage\nmachine learning-based technique, namely ML-LFIL that identifies and localizes\nlink faults by analyzing the measurements captured from the normal traffic\nflows, including aggregate flow rate, end-to-end delay and packet loss. ML-LFIL\nlearns the traffic behavior in normal working conditions and different link\nfault scenarios. We train the learning model using support vector machine,\nmulti-layer perceptron and random forest. We implement ML-LFIL and carry out\nextensive experiments using Mininet platform. Performance studies show that\nML-LFIL achieves high accuracy while requiring much lower fault localization\ntime compared to the active probing approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:06:44 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 02:34:32 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Srinivasan", "Srinikethan Madapuzi", ""], ["Truong-Huu", "Tram", ""], ["Gurusamy", "Mohan", ""]]}, {"id": "1812.03862", "submitter": "Manu Kumar Gupta", "authors": "Veeraruna Kavitha, Manu K. Gupta, Veronique Capdevielle, Rahul Kishor\n  and Majed Haddad", "title": "Speed Based Optimal Power Control in Small Cell Networks", "comments": null, "journal-ref": "Computer Communication 2019", "doi": "10.1016/j.comcom.2019.04.009", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small cell networks promise good quality of service (QoS) even for cell edge\nusers, however pose challenges to cater to the high-speed users. The major\ndifficulty being that of frequent handovers and the corresponding handover\nlosses, which significantly depend upon the speed of the user. It was shown\npreviously that the optimal cell size increases with speed. Thus, in scenarios\nwith diverse users (speeds spanning over large ranges), it would be inefficient\nto serve all users using common cell radius and it is practically infeasible to\ndesign different cell sizes for different speeds. Alternatively, we propose to\nallocate power to a user based on its speed, e.g., higher power virtually\nincreases the cell size. We solve well known Hamiltonian Jacobi equations under\ncertain assumptions to obtain a power law, optimal for load factor and busy\nprobability, for any given average power constraint and cell size. The optimal\npower control turns out to be linear in speed. We build a system level\nsimulator for small cell network, using elaborate Monte-Carlo simulations, and\nshow that the performance of the system improves significantly with linear\npower law. The power law is tested even for the cases, for which the system\ndoes not satisfy the assumptions required by the theory. For example, the\nlinear power law has significant improvement in comparison with the 'equal\npower' system, even in presence of time varying and random interference. We\nobserve good improvement in almost all cases with improvements up to 89\\% for\ncertain configurations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 15:12:26 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Kavitha", "Veeraruna", ""], ["Gupta", "Manu K.", ""], ["Capdevielle", "Veronique", ""], ["Kishor", "Rahul", ""], ["Haddad", "Majed", ""]]}, {"id": "1812.04122", "submitter": "Reza Salkhordeh", "authors": "Reza Salkhordeh and Mostafa Hadizadeh and Hossein Asadi", "title": "An Efficient Hybrid I/O Caching Architecture Using Heterogeneous SSDs", "comments": null, "journal-ref": "R. Salkhordeh, M. Hadizadeh and H. Asadi, \"An Efficient Hybrid I/O\n  Caching Architecture Using Heterogeneous SSDs,\" in IEEE Transactions on\n  Parallel and Distributed Systems. doi: 10.1109/TPDS.2018.2883745", "doi": "10.1109/TPDS.2018.2883745", "report-no": null, "categories": "cs.PF cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SSDs are emerging storage devices which unlike HDDs, do not have mechanical\nparts and therefore, have superior performance compared to HDDs. Due to the\nhigh cost of SSDs, entirely replacing HDDs with SSDs is not economically\njustified. Additionally, SSDs can endure a limited number of writes before\nfailing. To mitigate the shortcomings of SSDs while taking advantage of their\nhigh performance, SSD caching is practiced in both academia and industry.\nPreviously proposed caching architectures have only focused on either\nperformance or endurance and neglected to address both parameters in suggested\narchitectures. Moreover, the cost, reliability, and power consumption of such\narchitectures is not evaluated. This paper proposes a hybrid I/O caching\narchitecture that while offers higher performance than previous studies, it\nalso improves power consumption with a similar budget. The proposed\narchitecture uses DRAM, Read-Optimized SSD, and Write-Optimized SSD in a\nthree-level cache hierarchy and tries to efficiently redirect read requests to\neither DRAM or RO-SSD while sending writes to WO-SSD. To provide high\nreliability, dirty pages are written to at least two devices which removes any\nsingle point of failure. The power consumption is also managed by reducing the\nnumber of accesses issued to SSDs. The proposed architecture reconfigures\nitself between performance- and endurance-optimized policies based on the\nworkload characteristics to maintain an effective tradeoff between performance\nand endurance. We have implemented the proposed architecture on a server\nequipped with industrial SSDs and HDDs. The experimental results show that as\ncompared to state-of-the-art studies, the proposed architecture improves\nperformance and power consumption by an average of 8% and 28%, respectively,\nand reduces the cost by 5% while increasing the endurance cost by 4.7% and\nnegligible reliability penalty.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 22:00:39 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Salkhordeh", "Reza", ""], ["Hadizadeh", "Mostafa", ""], ["Asadi", "Hossein", ""]]}, {"id": "1812.04514", "submitter": "Sushant Kondguli", "authors": "Sushant Kondguli and Michael Huang", "title": "R3-DLA (Reduce, Reuse, Recycle): A More Efficient Approach to Decoupled\n  Look-Ahead Architectures", "comments": "16 pages, 16 Figures, Scheduled to appear in 25th IEEE International\n  Symposium on High-Performance Computer Architecture 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern societies have developed insatiable demands for more computation\ncapabilities. Exploiting implicit parallelism to provide automatic performance\nimprovement remains a central goal in engineering future general-purpose\ncomputing systems. One approach is to use a separate thread context to perform\ncontinuous look-ahead to improve the data and instruction supply to the main\npipeline. Such a decoupled look-ahead (DLA) architecture can be quite effective\nin accelerating a broad range of applications in a relatively straightforward\nimplementation. It also has broad design flexibility as the look-ahead agent\nneed not be concerned with correctness constraints. In this paper, we explore a\nnumber of optimizations that make the look-ahead agent more efficient and yet\nextract more utility from it. With these optimizations, a DLA architecture can\nachieve an average speedup of 1.4 over a state-of-the-art microarchitecture for\na broad set of benchmark suites, making it a powerful tool to enhance\nsingle-thread performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 16:13:35 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 17:19:36 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 16:05:14 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Kondguli", "Sushant", ""], ["Huang", "Michael", ""]]}, {"id": "1812.05257", "submitter": "Timur Bazhirov", "authors": "Mohammad Mohammadi and Timur Bazhirov", "title": "Continuous evaluation of the performance of cloud infrastructure for\n  scientific applications", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing recently developed into a viable alternative to on-premises\nsystems for executing high-performance computing (HPC) applications. With the\nemergence of new vendors and hardware options, there is now a growing need to\ncontinuously evaluate the performance of the infrastructure with respect to the\nmost commonly-used simulation workflows. We present an online ecosystem and the\ncorresponding tools aimed at providing a collaborative and repeatable way to\nassess the performance of the underlying hardware for multiple real-world\napplication-specific benchmark cases. The ecosystem allows for the benchmark\nresults to be stored and shared online in a centrally accessible database in\norder to facilitate their comparison, traceability, and curation. We include\nthe current up-to-date example results for multiple cloud vendors and explain\nhow to contribute new results and benchmark cases.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 04:26:29 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Mohammadi", "Mohammad", ""], ["Bazhirov", "Timur", ""]]}, {"id": "1812.05955", "submitter": "Thomas Rolinger", "authors": "Thomas B. Rolinger, Christopher D. Krieger", "title": "Impact of Traditional Sparse Optimizations on a Migratory Thread\n  Architecture", "comments": "8th Workshop on Irregular Applications: Architectures and Algorithms\n  (IA^3) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving high performance for sparse applications is challenging due to\nirregular access patterns and weak locality. These properties preclude many\nstatic optimizations and degrade cache performance on traditional systems. To\naddress these challenges, novel systems such as the Emu architecture have been\nproposed. The Emu design uses light-weight migratory threads, narrow memory,\nand near-memory processing capabilities to address weak locality and reduce the\ntotal load on the memory system. Because the Emu architecture is fundamentally\ndifferent than cache based hierarchical memory systems, it is crucial to\nunderstand the cost-benefit tradeoffs of standard sparse algorithm\noptimizations on Emu hardware. In this work, we explore sparse matrix-vector\nmultiplication (SpMV) on the Emu architecture. We investigate the effects of\ndifferent sparse optimizations such as dense vector data layouts, work\ndistributions, and matrix reorderings. Our study finds that initially\ndistributing work evenly across the system is inadequate to maintain load\nbalancing over time due to the migratory nature of Emu threads. In severe\ncases, matrix sparsity patterns produce hot-spots as many migratory threads\nconverge on a single resource. We demonstrate that known matrix reordering\ntechniques can improve SpMV performance on the Emu architecture by as much as\n70% by encouraging more consistent load balancing. This can be compared with a\nperformance gain of no more than 16% on a cache-memory based system.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 14:27:38 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Rolinger", "Thomas B.", ""], ["Krieger", "Christopher D.", ""]]}, {"id": "1812.05961", "submitter": "Thomas Rolinger", "authors": "Thomas B. Rolinger, Tyler A. Simon, Christopher D. Krieger", "title": "Parallel Sparse Tensor Decomposition in Chapel", "comments": "2018 IEEE International Parallel and Distributed Processing Symposium\n  Workshops (IPDPSW), 5th Annual Chapel Implementers and Users Workshop (CHIUW\n  2018)", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00143", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big-data analytics, using tensor decomposition to extract patterns from\nlarge, sparse multivariate data is a popular technique. Many challenges exist\nfor designing parallel, high performance tensor decomposition algorithms due to\nirregular data accesses and the growing size of tensors that are processed.\nThere have been many efforts at implementing shared-memory algorithms for\ntensor decomposition, most of which have focused on the traditional C/C++ with\nOpenMP framework. However, Chapel is becoming an increasingly popular\nprograming language due to its expressiveness and simplicity for writing\nscalable parallel programs. In this work, we port a state of the art C/OpenMP\nparallel sparse tensor decomposition tool, SPLATT, to Chapel. We present a\nperformance study that investigates bottlenecks in our Chapel code and\ndiscusses approaches for improving its performance. Also, we discuss features\nin Chapel that would have been beneficial to our porting effort. We demonstrate\nthat our Chapel code is competitive with the C/OpenMP code for both runtime and\nscalability, achieving 83%-96% performance of the original code and near linear\nscalability up to 32 cores.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 14:39:26 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Rolinger", "Thomas B.", ""], ["Simon", "Tyler A.", ""], ["Krieger", "Christopher D.", ""]]}, {"id": "1812.05964", "submitter": "Thomas Rolinger", "authors": "Thomas B. Rolinger, Tyler A. Simon, Christopher D. Krieger", "title": "An Empirical Evaluation of Allgatherv on Multi-GPU Systems", "comments": "2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid\n  Computing (CCGRID)", "journal-ref": null, "doi": "10.1109/CCGRID.2018.00027", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications for deep learning and big data analytics have compute and memory\nrequirements that exceed the limits of a single GPU. However, effectively\nscaling out an application to multiple GPUs is challenging due to the\ncomplexities of communication between the GPUs, particularly for collective\ncommunication with irregular message sizes. In this work, we provide a\nperformance evaluation of the Allgatherv routine on multi-GPU systems, focusing\non GPU network topology and the communication library used. We present results\nfrom the OSU-micro benchmark as well as conduct a case study for sparse tensor\nfactorization, one application that uses Allgatherv with highly irregular\nmessage sizes. We extend our existing tensor factorization tool to run on\nsystems with different node counts and varying number of GPUs per node. We then\nevaluate the communication performance of our tool when using traditional MPI,\nCUDA-aware MVAPICH and NCCL across a suite of real-world data sets on three\ndifferent systems: a 16-node cluster with one GPU per node, NVIDIA's DGX-1 with\n8 GPUs and Cray's CS-Storm with 16 GPUs. Our results show that irregularity in\nthe tensor data sets produce trends that contradict those in the OSU\nmicro-benchmark, as well as trends that are absent from the benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 14:46:25 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Rolinger", "Thomas B.", ""], ["Simon", "Tyler A.", ""], ["Krieger", "Christopher D.", ""]]}, {"id": "1812.06553", "submitter": "Mohammad Noormohammadpour", "authors": "Mohammad Noormohammadpour, Cauligi S. Raghavendra, Srikanth Kandula,\n  Sriram Rao", "title": "Fast and Efficient Bulk Multicasting over Dedicated Inter-Datacenter\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several organizations have built multiple datacenters connected via dedicated\nwide area networks over which large inter-datacenter transfers take place. This\nincludes tremendous volumes of bulk multicast traffic generated as a result of\ndata and content replication. Although one can perform these transfers using a\nsingle multicast forwarding tree, that can lead to poor performance as the\nslowest receiver on each tree dictates the completion time for all receivers.\nUsing multiple trees per transfer each connected to a subset of receivers\nalleviates this concern. The choice of multicast trees also determines the\ntotal bandwidth usage. To further improve the performance, bandwidth over\ndedicated inter-datacenter networks can be carved for different multicast trees\nover specific time periods to avoid congestion and minimize the average\nreceiver completion times.\n  In this paper, we break this problem into the three sub-problems of\npartitioning, tree selection, and rate allocation. We present an algorithm\ncalled QuickCast which is computationally fast and allows us to significantly\nspeed up multiple receivers per bulk multicast transfer with control over extra\nbandwidth consumption. We evaluate QuickCast against a variety of synthetic and\nreal traffic patterns as well as real WAN topologies. Compared to performing\nbulk multicast transfers as separate unicast transfers, QuickCast achieves up\nto $3.64\\times$ reduction in mean completion times while at the same time using\n$0.71\\times$ the bandwidth. Also, QuickCast allows the top $50\\%$ of receivers\nto complete between $3\\times$ to $35\\times$ faster on average compared with\nwhen a single forwarding multicast tree is used for data delivery.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 23:04:02 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Noormohammadpour", "Mohammad", ""], ["Raghavendra", "Cauligi S.", ""], ["Kandula", "Srikanth", ""], ["Rao", "Sriram", ""]]}, {"id": "1812.07264", "submitter": "Niklas Carlsson", "authors": "Niklas Carlsson and Derek Eager", "title": "Worst-case Bounds and Optimized Cache on $M^{th}$ Request Cache\n  Insertion Policies under Elastic Conditions", "comments": "To appear in IFIP Performance, Dec. 2018, Toulouse, France. The final\n  version will appear in Performance Evaluation, volumes 127-128, Nov. 2018,\n  pp. 70-92", "journal-ref": null, "doi": "10.1016/j.peva.2018.09.006", "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud services and other shared third-party infrastructures allow individual\ncontent providers to easily scale their services based on current resource\ndemands. In this paper, we consider an individual content provider that wants\nto minimize its delivery costs under the assumptions that the storage and\nbandwidth resources it requires are elastic, the content provider only pays for\nthe resources that it consumes, and costs are proportional to the resource\nusage. Within this context, we (i) derive worst-case bounds for the optimal\ncost and competitive cost ratios of different classes of \"cache on $M^{th}$\nrequest\" cache insertion policies, (ii) derive explicit average cost\nexpressions and bounds under arbitrary inter-request distributions, (iii)\nderive explicit average cost expressions and bounds for short-tailed\n(deterministic, Erlang, and exponential) and heavy-tailed (Pareto)\ninter-request distributions, and (iv) present numeric and trace-based\nevaluations that reveal insights into the relative cost performance of the\npolicies. Our results show that a window-based \"cache on $2^{nd}$ request\"\npolicy using a single threshold optimized to minimize worst-case costs provides\ngood average performance across the different distributions and the full\nparameter ranges of each considered distribution, making it an attractive\nchoice for a wide range of practical conditions where request rates of\nindividual file objects typically are not known and can change quickly.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 09:48:47 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Carlsson", "Niklas", ""], ["Eager", "Derek", ""]]}, {"id": "1812.07513", "submitter": "Xiaoda Jiang", "authors": "Xiaoda Jiang, Hancheng Lu", "title": "Joint Rate and Resource Allocation in Hybrid Digital-Analog Transmission\n  over Fading Channels", "comments": "14 pages, 10 figures, This paper has already been published in IEEE\n  Transactions on Vehicular Technology", "journal-ref": "Published in IEEE Transactions on Vehicular Technology ( Volume:\n  67 , Issue: 10 , Oct. 2018 )", "doi": "10.1109/TVT.2018.2857515", "report-no": null, "categories": "eess.SP cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hybrid digital-analog (HDA) systems, resource allocation has been utilized\nto achieve desired distortion performance. However, existing studies on this\nissue assume error-free digital transmission, which is not valid for fading\nchannels. With time-varying channel fading, the exact channel state information\nis not available at the transmitter. Thus, random outage and resulting digital\ndistortion cannot be ignored. Moreover, rate allocation should be considered in\nresource allocation, since it not only determines the amount of information for\ndigital transmission and that for analog transmission, but also affects the\noutage probability. Based on above observations, in this paper, we attempt to\nperform joint rate and resource allocation strategies to optimize system\ndistortion in HDA systems over fading channels. Consider a bandwidth expansion\nscenario where a memoryless Gaussian source is transmitted in an HDA system\nwith the entropy-constrained scalar quantizer (ECSQ). Firstly, we formulate the\njoint allocation problem as an expected system distortion minimization problem\nwhere both analog and digital distortion are considered. Then, in the limit of\nlow outage probability, we decompose the problem into two coupled sub-problems\nbased on the block coordinate descent method, and propose an iterative gradient\nalgorithm to approach the optimal solution. Furthermore, we extend our work to\nthe multivariate Gaussian source scenario where a two-stage fast algorithm\nintegrating rounding and greedy strategies is proposed to optimize the joint\nrate and resource allocation problem. Finally, simulation results demonstrate\nthat the proposed algorithms can achieve up to 2.3dB gains in terms of\nsignal-to-distortion ratio over existing schemes under the single Gaussian\nsource scenario, and up to 3.5dB gains under the multivariate Gaussian source\nscenario.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 11:35:10 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Jiang", "Xiaoda", ""], ["Lu", "Hancheng", ""]]}, {"id": "1812.07561", "submitter": "Wenqian Dong", "authors": "Wenqian Dong, Anzheng Guolu and Dong Li", "title": "A Preliminary Study of Neural Network-based Approximation for HPC\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning, as a tool to learn and model complicated (non)linear\nrelationships between input and output data sets, has shown preliminary success\nin some HPC problems. Using machine learning, scientists are able to augment\nexisting simulations by improving accuracy and significantly reducing\nlatencies. Our ongoing research work is to create a general framework to apply\nneural network-based models to HPC applications. In particular, we want to use\nthe neural network to approximate and replace code regions within the HPC\napplication to improve performance (i.e., reducing the execution time) of the\nHPC application. In this paper, we present our preliminary study and results.\nUsing two applications (the Newton-Raphson method and the Lennard-Jones (LJ)\npotential in LAMMP) for our case study, we achieve up to 2.7x and 2.46x\nspeedup, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 18:48:26 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Dong", "Wenqian", ""], ["Guolu", "Anzheng", ""], ["Li", "Dong", ""]]}, {"id": "1812.07589", "submitter": "Gian Giacomo Guerreschi", "authors": "G. G. Guerreschi and A. Y. Matsuura", "title": "QAOA for Max-Cut requires hundreds of qubits for quantum speed-up", "comments": null, "journal-ref": "Sci Rep 9, 6903 (2019)", "doi": "10.1038/s41598-019-43176-9", "report-no": null, "categories": "quant-ph cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational quantum technologies are entering a new phase in which noisy\nintermediate-scale quantum computers are available, but are still too small to\nbenefit from active error correction. Even with a finite coherence budget to\ninvest in quantum information processing, noisy devices with about 50 qubits\nare expected to experimentally demonstrate quantum supremacy in the next few\nyears. Defined in terms of artificial tasks, current proposals for quantum\nsupremacy, even if successful, will not help to provide solutions to practical\nproblems. Instead, we believe that future users of quantum computers are\ninterested in actual applications and that noisy quantum devices may still\nprovide value by approximately solving hard combinatorial problems via hybrid\nclassical-quantum algorithms. To lower bound the size of quantum computers with\npractical utility, we perform realistic simulations of the Quantum Approximate\nOptimization Algorithm and conclude that quantum speedup will not be\nattainable, at least for a representative combinatorial problem, until several\nhundreds of qubits are available.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 19:00:13 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Guerreschi", "G. G.", ""], ["Matsuura", "A. Y.", ""]]}, {"id": "1812.07778", "submitter": "Mahesh Lakshminarasimhan", "authors": "Mahesh Lakshminarasimhan, Catherine Olschanowsky", "title": "AdaptMemBench: Application-Specific MemorySubsystem Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing scientific applications to take full advan-tage of modern memory\nsubsystems is a continual challenge forapplication and compiler developers.\nFactors beyond working setsize affect performance. A benchmark framework that\nexploresthe performance in an application-specific manner is essential\ntocharacterize memory performance and at the same time informmemory-efficient\ncoding practices. We present AdaptMemBench,a configurable benchmark framework\nthat measures achievedmemory performance by emulating application-specific\naccesspatterns with a set of kernel-independent driver templates. Thisframework\ncan explore the performance characteristics of a widerange of access patterns\nand can be used as a testbed for potentialoptimizations due to the flexibility\nof polyhedral code generation.We demonstrate the effectiveness of AdaptMemBench\nwith casestudies on commonly used computational kernels such as triadand\nmultidimensional stencil patterns.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 06:51:24 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Lakshminarasimhan", "Mahesh", ""], ["Olschanowsky", "Catherine", ""]]}, {"id": "1812.07816", "submitter": "Haruki Imai", "authors": "Haruki Imai, Samuel Matzek, Tung D. Le, Yasushi Negishi, Kiyokuni\n  Kawachiya", "title": "Fast and Accurate 3D Medical Image Segmentation with Data-swapping\n  Method", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models used for medical image segmentation are large\nbecause they are trained with high-resolution three-dimensional (3D) images.\nGraphics processing units (GPUs) are widely used to accelerate the trainings.\nHowever, the memory on a GPU is not large enough to train the models. A popular\napproach to tackling this problem is patch-based method, which divides a large\nimage into small patches and trains the models with these small patches.\nHowever, this method would degrade the segmentation quality if a target object\nspans multiple patches. In this paper, we propose a novel approach for 3D\nmedical image segmentation that utilizes the data-swapping, which swaps out\nintermediate data from GPU memory to CPU memory to enlarge the effective GPU\nmemory size, for training high-resolution 3D medical images without patching.\nWe carefully tuned parameters in the data-swapping method to obtain the best\ntraining performance for 3D U-Net, a widely used deep neural network model for\nmedical image segmentation. We applied our tuning to train 3D U-Net with\nfull-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result,\ncommunication overhead, which is the most important issue, was reduced by\n17.1%. Compared with the patch-based method for patches of 128 x 128 x 128\nvoxels, our training for full-size images achieved improvement on the mean Dice\nscore by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core\nsub-region, respectively. The total training time was reduced from 164 hours to\n47 hours, resulting in 3.53 times of acceleration.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:49:50 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Imai", "Haruki", ""], ["Matzek", "Samuel", ""], ["Le", "Tung D.", ""], ["Negishi", "Yasushi", ""], ["Kawachiya", "Kiyokuni", ""]]}, {"id": "1812.08720", "submitter": "Saba Ahmadian", "authors": "Saba Ahmadian, Reza Salkhordeh, and Hossein Asadi", "title": "LBICA: A Load Balancer for I/O Cache Architectures", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, enterprise Solid-State Drives (SSDs) are used in the caching\nlayer of high-performance servers to close the growing performance gap between\nprocessing units and storage subsystem. SSD-based I/O caching is typically not\neffective in workloads with burst accesses in which the caching layer itself\nbecomes the performance bottleneck because of the large number of accesses.\nExisting I/O cache architectures mainly focus on maximizing the cache hit ratio\nwhile they neglect the average queue time of accesses. Previous studies\nsuggested bypassing the cache when burst accesses are identified. These\nschemes, however, are not applicable to a general cache configuration and also\nresult in significant performance degradation on burst accesses. In this paper,\nwe propose a novel I/O cache load balancing scheme (LBICA) with adaptive write\npolicy management to prevent the I/O cache from becoming performance bottleneck\nin burst accesses. Our proposal, unlike previous schemes, which disable the I/O\ncache or bypass the requests into the disk subsystem in burst accesses,\nselectively reduces the number of waiting accesses in the SSD queue and\nbalances the load between the I/O cache and the disk subsystem while providing\nthe maximum performance. The proposed scheme characterizes the workload based\non the type of in-queue requests and assigns an effective cache write policy.\nWe aim to bypass the accesses which 1) are served faster by the disk subsystem\nor 2) cannot be merged with other accesses in the I/O cache queue. Doing so,\nthe selected requests are responded by the disk layer, preventing from\noverloading the I/O cache. Our evaluations on a physical system shows that\nLBICA reduces the load on the I/O cache by 48% and improves the performance of\nburst workloads by 30% compared to the latest state-of-the-art load balancing\nscheme.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 19:36:30 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Ahmadian", "Saba", ""], ["Salkhordeh", "Reza", ""], ["Asadi", "Hossein", ""]]}, {"id": "1812.09107", "submitter": "Emilio Leonardi", "authors": "Giovanni Luca Torrisi and Michele Garetto and Emilio Leonardi", "title": "Bootstrap percolation on the stochastic block model with k communities", "comments": "53 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the bootstrap percolation process on the stochastic block model\n(SBM), a natural extension of the Erd\\\"{o}s--R\\'{e}nyi random graph that allows\nrepresenting the \"community structure\" observed in many real systems. In the\nSBM, nodes are partitioned into subsets, which represent different communities,\nand pairs of nodes are independently connected with a probability that depends\non the communities they belong to. Under mild assumptions on system parameters,\nwe prove the existence of a sharp phase transition for the final number of\nactive nodes and characterize sub-critical and super-critical regimes in terms\nof the number of initially active nodes, which are selected uniformly at random\nin each community.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 13:32:00 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2019 10:10:09 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 12:57:09 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Torrisi", "Giovanni Luca", ""], ["Garetto", "Michele", ""], ["Leonardi", "Emilio", ""]]}, {"id": "1812.09162", "submitter": "Nicolas Le Scouarnec", "authors": "Fabien Andr\\'e, Anne-Marie Kermarrec and Nicolas Le Scouarnec", "title": "Quicker ADC : Unlocking the hidden potential of Product Quantization\n  with SIMD", "comments": "Open-source implementation at\n  http://github.com/nlescoua/faiss-quickeradc", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019 Early Access", "doi": "10.1109/TPAMI.2019.2952606", "report-no": null, "categories": "cs.CV cs.AI cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. A common approach is to rely\non Product Quantization, which allows the storage of large vector databases in\nmemory and efficient distance computations. Yet, implementations of nearest\nneighbor search with Product Quantization have their performance limited by the\nmany memory accesses they perform. Following this observation, Andr\\'e et al.\nproposed Quick ADC with up to $6\\times$ faster implementations of $m\\times{}4$\nproduct quantizers (PQ) leveraging specific SIMD instructions. Quicker ADC is a\ngeneralization of Quick ADC not limited to $m\\times{}4$ codes and supporting\nAVX-512, the latest revision of SIMD instruction set. In doing so, Quicker ADC\nfaces the challenge of using efficiently 5,6 and 7-bit shuffles that do not\nalign to computer bytes or words. To this end, we introduce (i) irregular\nproduct quantizers combining sub-quantizers of different granularity and (ii)\nsplit tables allowing lookup tables larger than registers. We evaluate Quicker\nADC with multiple indexes including Inverted Multi-Indexes and IVF HNSW and\nshow that it outperforms the reference optimized implementations (i.e., FAISS\nand polysemous codes) for numerous configurations. Finally, we release an\nopen-source fork of FAISS enhanced with Quicker ADC at\nhttp://github.com/nlescoua/faiss-quickeradc.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:51:27 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 21:39:48 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Andr\u00e9", "Fabien", ""], ["Kermarrec", "Anne-Marie", ""], ["Scouarnec", "Nicolas Le", ""]]}, {"id": "1812.11731", "submitter": "Yuri G. Gordienko", "authors": "Yuriy Kochura, Yuri Gordienko, Vlad Taran, Nikita Gordienko, Alexandr\n  Rokovyi, Oleg Alienin, Sergii Stirenko", "title": "Batch Size Influence on Performance of Graphic and Tensor Processing\n  Units during Training and Inference Phases", "comments": "10 pages, 7 figures, 2 tables", "journal-ref": "Hu Z., Petoukhov S., Dychka I., He M. (eds) Advances in Computer\n  Science for Engineering and Education II. ICCSEEA 2019. Advances in\n  Intelligent Systems and Computing, vol 938 (pp. 658-668). Springer, Cham", "doi": "10.1007/978-3-030-16621-2_61", "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of the maximally possible batch size (for the better runtime) on\nperformance of graphic processing units (GPU) and tensor processing units (TPU)\nduring training and inference phases is investigated. The numerous runs of the\nselected deep neural network (DNN) were performed on the standard MNIST and\nFashion-MNIST datasets. The significant speedup was obtained even for extremely\nlow-scale usage of Google TPUv2 units (8 cores only) in comparison to the quite\npowerful GPU NVIDIA Tesla K80 card with the speedup up to 10x for training\nstage (without taking into account the overheads) and speedup up to 2x for\nprediction stage (with and without taking into account overheads). The precise\nspeedup values depend on the utilization level of TPUv2 units and increase with\nthe increase of the data volume under processing, but for the datasets used in\nthis work (MNIST and Fashion-MNIST with images of sizes 28x28) the speedup was\nobserved for batch sizes >512 images for training phase and >40 000 images for\nprediction phase. It should be noted that these results were obtained without\ndetriment to the prediction accuracy and loss that were equal for both GPU and\nTPU runs up to the 3rd significant digit for MNIST dataset, and up to the 2nd\nsignificant digit for Fashion-MNIST dataset.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 09:24:36 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Kochura", "Yuriy", ""], ["Gordienko", "Yuri", ""], ["Taran", "Vlad", ""], ["Gordienko", "Nikita", ""], ["Rokovyi", "Alexandr", ""], ["Alienin", "Oleg", ""], ["Stirenko", "Sergii", ""]]}]