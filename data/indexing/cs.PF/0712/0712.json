[{"id": "0712.0811", "submitter": "Radim Ba\\v{c}a Ing.", "authors": "R.Baca, V.Snasel, J.Platos, M.Kratky, E.El-Qawasmeh", "title": "The Fast Fibonacci Decompression Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.OH", "license": null, "abstract": "  Data compression has been widely applied in many data processing areas.\nCompression methods use variable-size codes with the shorter codes assigned to\nsymbols or groups of symbols that appear in the data frequently. Fibonacci\ncoding, as a representative of these codes, is used for compressing small\nnumbers. Time consumption of a decompression algorithm is not usually as\nimportant as the time of a compression algorithm. However, efficiency of the\ndecompression may be a critical issue in some cases. For example, a real-time\ncompression of tree data structures follows this issue. Tree's pages are\ndecompressed during every reading from a secondary storage into the main\nmemory. In this case, the efficiency of a decompression algorithm is extremely\nimportant. We have developed a Fast Fibonacci decompression for this purpose.\nOur approach is up to $3.5\\times$ faster than the original implementation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2007 19:55:16 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2007 08:05:54 GMT"}], "update_date": "2007-12-19", "authors_parsed": [["Baca", "R.", ""], ["Snasel", "V.", ""], ["Platos", "J.", ""], ["Kratky", "M.", ""], ["El-Qawasmeh", "E.", ""]]}, {"id": "0712.1854", "submitter": "Soung Liew", "authors": "S.C. Liew, C. Kai, J. Leung, B. Wong", "title": "Back-of-the-Envelope Computation of Throughput Distributions in CSMA\n  Wireless Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": null, "abstract": "  This work started out with our accidental discovery of a pattern of\nthroughput distributions among links in IEEE 802.11 networks from experimental\nresults. This pattern gives rise to an easy computation method, which we term\nback-of-the-envelop (BoE) computation, because for many network configurations,\nvery accurate results can be obtained within minutes, if not seconds, by simple\nhand computation. BoE beats prior methods in terms of both speed and accuracy.\nWhile the computation procedure of BoE is simple, explaining why it works is by\nno means trivial. Indeed the majority of our investigative efforts have been\ndevoted to the construction of a theory to explain BoE. This paper models an\nideal CSMA network as a set of interacting on-off telegraph processes. In\ndeveloping the theory, we discovered a number of analytical techniques and\nobservations that have eluded prior research, such as that the carrier-sensing\ninteractions among links in an ideal CSMA network result in a system state\nevolution that is time-reversible; and that the probability distribution of the\nsystem state is insensitive to the distributions of the \"on\" and \"off\"\ndurations given their means, and is a Markov random field. We believe these\ntheoretical frameworks are useful not just for explaining BoE, but could also\nbe a foundation for a fundamental understanding of how links in CSMA networks\ninteract. Last but not least, because of their basic nature, we surmise that\nsome of the techniques and results developed in this paper may be applicable to\nnot just CSMA networks, but also to other physical and engineering systems\nconsisting of entities interacting with each other in time and space.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2007 01:41:54 GMT"}], "update_date": "2007-12-13", "authors_parsed": [["Liew", "S. C.", ""], ["Kai", "C.", ""], ["Leung", "J.", ""], ["Wong", "B.", ""]]}, {"id": "0712.2302", "submitter": "Georg Hager", "authors": "Georg Hager, Thomas Zeiser, Gerhard Wellein", "title": "Data access optimizations for highly threaded multi-core CPUs with\n  multiple memory controllers", "comments": "12 pages, 7 figures. Accepted for Workshop on Large-Scale Parallel\n  Processing 2008. Revised and extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": null, "abstract": "  Processor and system architectures that feature multiple memory controllers\nare prone to show bottlenecks and erratic performance numbers on codes with\nregular access patterns. Although such effects are well known in the form of\ncache thrashing and aliasing conflicts, they become more severe when memory\naccess is involved. Using the new Sun UltraSPARC T2 processor as a prototypical\nmulti-core design, we analyze performance patterns in low-level and application\nbenchmarks and show ways to circumvent bottlenecks by careful data layout and\npadding.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2007 08:14:20 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2008 16:18:50 GMT"}], "update_date": "2008-01-28", "authors_parsed": [["Hager", "Georg", ""], ["Zeiser", "Thomas", ""], ["Wellein", "Gerhard", ""]]}, {"id": "0712.2773", "submitter": "Emmanuel Cecchet", "authors": "Emmanuel Cecchet, George Candea, Anastasia Ailamaki", "title": "Middleware-based Database Replication: The Gaps between Theory and\n  Practice", "comments": "14 pages. Appears in Proc. ACM SIGMOD International Conference on\n  Management of Data, Vancouver, Canada, June 2008", "journal-ref": null, "doi": null, "report-no": "EPFL technical report DSLAB-REPORT-2007-001", "categories": "cs.DB cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for high availability and performance in data management systems has\nbeen fueling a long running interest in database replication from both academia\nand industry. However, academic groups often attack replication problems in\nisolation, overlooking the need for completeness in their solutions, while\ncommercial teams take a holistic approach that often misses opportunities for\nfundamental innovation. This has created over time a gap between academic\nresearch and industrial practice.\n  This paper aims to characterize the gap along three axes: performance,\navailability, and administration. We build on our own experience developing and\ndeploying replication systems in commercial and academic settings, as well as\non a large body of prior related work. We sift through representative examples\nfrom the last decade of open-source, academic, and commercial database\nreplication systems and combine this material with case studies from real\nsystems deployed at Fortune 500 customers. We propose two agendas, one for\nacademic research and one for industrial R&D, which we believe can bridge the\ngap within 5-10 years. This way, we hope to both motivate and help researchers\nin making the theory and practice of middleware-based database replication more\nrelevant to each other.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2007 18:42:15 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2008 20:53:51 GMT"}], "update_date": "2008-11-05", "authors_parsed": [["Cecchet", "Emmanuel", ""], ["Candea", "George", ""], ["Ailamaki", "Anastasia", ""]]}, {"id": "0712.3389", "submitter": "Georg Hager", "authors": "Georg Hager, Holger Stengel, Thomas Zeiser, Gerhard Wellein", "title": "RZBENCH: Performance evaluation of current HPC architectures using\n  low-level and application benchmarks", "comments": "Contribution to the HLRB/KONWIHR results and review workshop, Dec\n  3rd/4th 2007, LRZ Munich, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": null, "abstract": "  RZBENCH is a benchmark suite that was specifically developed to reflect the\nrequirements of scientific supercomputer users at the University of\nErlangen-Nuremberg (FAU). It comprises a number of application and low-level\ncodes under a common build infrastructure that fosters maintainability and\nexpandability. This paper reviews the structure of the suite and briefly\nintroduces the most relevant benchmarks. In addition, some widely known\nstandard benchmark codes are reviewed in order to emphasize the need for a\ncritical review of often-cited performance results. Benchmark data is presented\nfor the HLRB-II at LRZ Munich and a local InfiniBand Woodcrest cluster as well\nas two uncommon system architectures: A bandwidth-optimized InfiniBand cluster\nbased on single socket nodes (\"Port Townsend\") and an early version of Sun's\nhighly threaded T2 architecture (\"Niagara 2\").\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2007 12:15:26 GMT"}], "update_date": "2007-12-21", "authors_parsed": [["Hager", "Georg", ""], ["Stengel", "Holger", ""], ["Zeiser", "Thomas", ""], ["Wellein", "Gerhard", ""]]}, {"id": "0712.3870", "submitter": "Bruce Hajek", "authors": "Bruce Hajek", "title": "Substitute Valuations: Generation and Structure", "comments": "Revision includes more background and explanations", "journal-ref": null, "doi": "10.1016/j.peva.2008.07.001", "report-no": null, "categories": "cs.GT cs.PF", "license": null, "abstract": "  Substitute valuations (in some contexts called gross substitute valuations)\nare prominent in combinatorial auction theory. An algorithm is given in this\npaper for generating a substitute valuation through Monte Carlo simulation. In\naddition, the geometry of the set of all substitute valuations for a fixed\nnumber of goods K is investigated. The set consists of a union of polyhedrons,\nand the maximal polyhedrons are identified for K=4. It is shown that the\nmaximum dimension of the maximal polyhedrons increases with K nearly as fast as\ntwo to the power K. Consequently, under broad conditions, if a combinatorial\nalgorithm can present an arbitrary substitute valuation given a list of input\nnumbers, the list must grow nearly as fast as two to the power K.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2007 16:52:39 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2008 16:14:03 GMT"}, {"version": "v3", "created": "Mon, 19 May 2008 14:50:58 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Hajek", "Bruce", ""]]}]