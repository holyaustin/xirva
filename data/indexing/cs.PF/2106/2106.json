[{"id": "2106.00831", "submitter": "Thirupathaiah Vasantam", "authors": "Thirupathaiah Vasantam, Don Towsley", "title": "Stability Analysis of a Quantum Network with Max-Weight Scheduling", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study a quantum network that distributes entangled quantum states to\nmultiple sets of users that are connected to the network. Each user is\nconnected to a switch of the network via a link. All the links of the network\ngenerate bipartite Bell-state entangled states in each time-slot with certain\nprobabilities, and each end node stores one qubit of the entanglement generated\nby the link. To create shared entanglements for a set of users, measurement\noperations are performed on qubits of link-level entanglements on a set of\nrelated links, and these operations are probabilistic in nature and are\nsuccessful with certain probabilities. Requests arrive to the system seeking\nshared entanglements for different sets of users. Each request is for the\ncreation of shared entanglements for a fixed set of users using link-level\nentanglements on a fixed set of links. Requests are processed according to\nFirst-Come-First-Served service discipline and unserved requests are stored in\nbuffers. Once a request is selected for service, measurement operations are\nperformed on qubits of link-level entanglements on related links to create a\nshared entanglement. For given set of request arrival rates and link-level\nentanglement generation rates, we obtain necessary conditions for the stability\nof queues of requests. In each time-slot, the scheduler has to schedule\nentanglement swapping operations for different sets of users to stabilize the\nnetwork. Next, we propose a Max-Weight scheduling policy and show that this\npolicy stabilizes the network for all feasible arrival rates. We also provide\nnumerical results to support our analysis. The analysis of a single quantum\nswitch that creates multipartite entanglements for different sets of users is a\nspecial case of our work.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 22:19:33 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Vasantam", "Thirupathaiah", ""], ["Towsley", "Don", ""]]}, {"id": "2106.01034", "submitter": "Seyed Mohammadhossein Tabatabaee", "authors": "Seyed Mohammadhossein Tabatabaee, Jean-Yves Le Boudec", "title": "Deficit Round-Robin: A Second Network Calculus Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deficit Round-Robin (DRR) is a widespread scheduling algorithm that provides\nfair queueing with variable-length packets. Bounds on worst-case delays for DRR\nwere found by Boyer et al., who used a rigorous network calculus approach and\ncharacterized the service obtained by one flow of interest by means of a convex\nstrict service curve. These bounds do not make any assumptions on the\ninterfering traffic hence are pessimistic when the interfering traffic is\nconstrained by some arrival curves. For such cases, two improvements were\nproposed. The former, by Soni et al., uses a correction term derived from a\nsemi-rigorous heuristic; unfortunately, these bounds are incorrect, as we show\nby exhibiting a counter-example. The latter, by Bouillard, rigorously derive\nconvex strict service curves for DRR that account for the arrival curve\nconstraints of the interfering traffic. In this paper, we improve on these\nresults in two ways. First, we derive a non-convex strict service curve for DRR\nthat improves on Boyer et al. when there is no arrival constraint on the\ninterfering traffic. Second, we provide an iterative method to improve any\nstrict service curve (including Bouillard's) when there are arrival constraints\nfor the interfering traffic. As of today, our results provide the best-known\nworst-case delay bounds for DRR. They are obtained by using the method of the\npseudo-inverse.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:06:14 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tabatabaee", "Seyed Mohammadhossein", ""], ["Boudec", "Jean-Yves Le", ""]]}, {"id": "2106.01492", "submitter": "Isaac Grosof", "authors": "Isaac Grosof, Kunhe Yang, Ziv Scully, Mor Harchol-Balter", "title": "Nudge: Stochastically Improving upon FCFS", "comments": "29 pages, 4 figures. To appear in SIGMETRICS 2021", "journal-ref": null, "doi": "10.1145/3410220.3460102", "report-no": null, "categories": "cs.PF math.PR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The First-Come First-Served (FCFS) scheduling policy is the most popular\nscheduling algorithm used in practice. Furthermore, its usage is theoretically\nvalidated: for light-tailed job size distributions, FCFS has weakly optimal\nasymptotic tail of response time. But what if we don't just care about the\nasymptotic tail? What if we also care about the 99th percentile of response\ntime, or the fraction of jobs that complete in under one second? Is FCFS still\nbest? Outside of the asymptotic regime, only loose bounds on the tail of FCFS\nare known, and optimality is completely open.\n  In this paper, we introduce a new policy, Nudge, which is the first policy to\nprovably stochastically improve upon FCFS. We prove that Nudge simultaneously\nimproves upon FCFS at every point along the tail, for light-tailed job size\ndistributions. As a result, Nudge outperforms FCFS for every moment and every\npercentile of response time. Moreover, Nudge provides a multiplicative\nimprovement over FCFS in the asymptotic tail. This resolves a long-standing\nopen problem by showing that, counter to previous conjecture, FCFS is not\nstrongly asymptotically optimal.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 22:18:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Grosof", "Isaac", ""], ["Yang", "Kunhe", ""], ["Scully", "Ziv", ""], ["Harchol-Balter", "Mor", ""]]}, {"id": "2106.01710", "submitter": "Milind Chabbi", "authors": "Zhizhou Zhang, Milind Chabbi, Adam Welc, Timothy Sherwood", "title": "Optimistic Concurrency Control for Real-world Go Programs (Extended\n  Version with Appendix)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a source-to-source transformation framework, GOCC, that consumes\nlock-based pessimistic concurrency programs in the Go language and transforms\nthem into optimistic concurrency programs that use Hardware Transactional\nMemory (HTM). The choice of the Go language is motivated by the fact that\nconcurrency is a first-class citizen in Go, and it is widely used in Go\nprograms. GOCC performs rich inter-procedural program analysis to detect and\nfilter lock-protected regions and performs AST-level code transformation of the\nsurrounding locks when profitable. Profitability is driven by both static\nanalyses of critical sections and dynamic analysis via execution profiles. A\ncustom HTM library, using perceptron, learns concurrency behavior and\ndynamically decides whether to use HTM in the rewritten lock/unlock points.\nGiven the rich history of transactional memory research but its lack of\nadoption in any industrial setting, we believe this workflow, which ultimately\nproduces source-code patches, is more apt for industry-scale adoption. Results\non widely adopted Go libraries and applications demonstrate significant (up to\n10x) and scalable performance gains resulting from our automated transformation\nwhile avoiding major performance regressions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:27:37 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Zhizhou", ""], ["Chabbi", "Milind", ""], ["Welc", "Adam", ""], ["Sherwood", "Timothy", ""]]}, {"id": "2106.01847", "submitter": "Xiaohu Wu", "authors": "Xiaohu Wu, Han Yu, Giuliano Casale, and Guanyu Gao", "title": "Towards Cost-Optimal Policies for DAGs to Utilize IaaS Clouds with\n  Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Premier cloud service providers (CSPs) offer two types of purchase options,\nnamely on-demand and spot instances, with time-varying features in availability\nand price. Users like startups have to operate on a limited budget and\nsimilarly others hope to reduce their costs. While interacting with a CSP,\ncentral to their concerns is the process of cost-effectively utilizing\ndifferent purchase options possibly in addition to self-owned instances. A job\nin data-intensive applications is typically represented by a directed acyclic\ngraph which can further be transformed into a chain of tasks. The key to\nachieving cost efficiency is determining the allocation of a specific deadline\nto each task, as well as the allocation of different types of instances to the\ntask. In this paper, we propose a framework that determines the optimal\nallocation of deadlines to tasks. The framework also features an optimal policy\nto determine the allocation of spot and on-demand instances in a predefined\ntime window, and a near-optimal policy for allocating self-owned instances. The\npolicies are designed to be parametric to support the usage of online learning\nto infer the optimal values against the dynamics of cloud markets. Finally,\nseveral intuitive heuristics are used as baselines to validate the cost\nimprovement brought by the proposed solutions. We show that the cost\nimprovement over the state-of-the-art is up to 24.87% when spot and on-demand\ninstances are considered and up to 59.05% when self-owned instances are\nconsidered.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 13:51:54 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wu", "Xiaohu", ""], ["Yu", "Han", ""], ["Casale", "Giuliano", ""], ["Gao", "Guanyu", ""]]}, {"id": "2106.04034", "submitter": "Leonardo Trujillo Dr", "authors": "Leonardo Trujillo, Jose Manuel Mu\\~noz Contreras, Daniel E Hernandez,\n  Mauro Castelli and Juan J Tapia", "title": "GSGP-CUDA -- a CUDA framework for Geometric Semantic Genetic Programming", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geometric Semantic Genetic Programming (GSGP) is a state-of-the-art machine\nlearning method based on evolutionary computation. GSGP performs search\noperations directly at the level of program semantics, which can be done more\nefficiently then operating at the syntax level like most GP systems. Efficient\nimplementations of GSGP in C++ exploit this fact, but not to its full\npotential. This paper presents GSGP-CUDA, the first CUDA implementation of GSGP\nand the most efficient, which exploits the intrinsic parallelism of GSGP using\nGPUs. Results show speedups greater than 1,000X relative to the\nstate-of-the-art sequential implementation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 00:58:39 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Trujillo", "Leonardo", ""], ["Contreras", "Jose Manuel Mu\u00f1oz", ""], ["Hernandez", "Daniel E", ""], ["Castelli", "Mauro", ""], ["Tapia", "Juan J", ""]]}, {"id": "2106.04284", "submitter": "Bernhard Manfred Gruber", "authors": "Bernhard Manfred Gruber, Guilherme Amadio, Jakob Blomer, Alexander\n  Matthes, Ren\\'e Widera, Michael Bussmann", "title": "LLAMA: The Low Level Abstraction For Memory Access", "comments": "32 pages, 7 figures, 10 listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance gap between CPU and memory widens continuously. Choosing the\nbest memory layout for each hardware architecture is increasingly important as\nmore and more programs become memory bound. For portable codes that run across\nheterogeneous hardware architectures, the choice of the memory layout for data\nstructures is therefore ideally decoupled from the rest of a program. This can\nbe accomplished via a zero-runtime-overhead abstraction layer, underneath which\nmemory layouts can be freely exchanged.\n  We present the C++ library LLAMA, which provides such a data structure\nabstraction layer with example implementations for multidimensional arrays of\nnested, structured data. LLAMA provides fully C++ compliant methods for\ndefining and switching custom memory layouts for user-defined data types.\nProviding two close-to-life examples, we show that the LLAMA-generated AoS\n(Array of Struct) and SoA (Struct of Array) layouts produce identical code with\nthe same performance characteristics as manually written data structures.\nLLAMA's layout-aware copy routines can significantly speed up transfer and\nreshuffling of data between layouts compared with naive element-wise copying.\nThe library is fully extensible with third-party allocators and allows users to\nsupport their own memory layouts with custom mappings.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:23:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gruber", "Bernhard Manfred", ""], ["Amadio", "Guilherme", ""], ["Blomer", "Jakob", ""], ["Matthes", "Alexander", ""], ["Widera", "Ren\u00e9", ""], ["Bussmann", "Michael", ""]]}, {"id": "2106.06242", "submitter": "Dragi Kimovski", "authors": "Roland Math\\'a, Dragi Kimovski, Anatoliy Zabrovskiy, Christian\n  Timmerer and Radu Prodan", "title": "Where to Encode: A Performance Analysis of x86 and Arm-based Amazon EC2\n  Instances", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video streaming became an undivided part of the Internet. To efficiently\nutilize the limited network bandwidth it is essential to encode the video\ncontent. However, encoding is a computationally intensive task, involving\nhigh-performance resources provided by private infrastructures or public\nclouds. Public clouds, such as Amazon EC2, provide a large portfolio of\nservices and instances optimized for specific purposes and budgets. The\nmajority of Amazon instances use x86 processors, such as Intel Xeon or AMD\nEPYC. However, following the recent trends in computer architecture, Amazon\nintroduced Arm-based instances that promise up to 40% better cost-performance\nratio than comparable x86 instances for specific workloads. We evaluate in this\npaper the video encoding performance of x86 and Arm instances of four instance\nfamilies using the latest FFmpeg version and two video codecs. We examine the\nimpact of the encoding parameters, such as different presets and bitrates, on\nthe time and cost for encoding. Our experiments reveal that Arm instances show\nhigh time and cost-saving potential of up to 33.63% for specific bitrates and\npresets, especially for the x264 codec. However, the x86 instances are more\ngeneral and achieve low encoding times, regardless of the codec.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:50:28 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 13:21:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Math\u00e1", "Roland", ""], ["Kimovski", "Dragi", ""], ["Zabrovskiy", "Anatoliy", ""], ["Timmerer", "Christian", ""], ["Prodan", "Radu", ""]]}, {"id": "2106.06457", "submitter": "Nitish K Panigrahy", "authors": "Nitish K. Panigrahy, Philippe Nain, Giovanni Neglia and Don Towsley", "title": "A New Upper Bound on Cache Hit Probability for Non-anticipative Caching\n  Policies", "comments": "IFIP WG 7.3 Performance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Caching systems have long been crucial for improving the performance of a\nwide variety of network and web based online applications. In such systems,\nend-to-end application performance heavily depends on the fraction of objects\ntransferred from the cache, also known as the cache hit probability. Many\ncaching policies have been proposed and implemented to improve the hit\nprobability. In this work, we propose a new method to compute an upper bound on\nhit probability for all non-anticipative caching policies, i.e., for policies\nthat have no knowledge of future requests. Our key insight is to order the\nobjects according to the ratio of their Hazard Rate (HR) function values to\ntheir sizes and place in the cache the objects with the largest ratios till the\ncache capacity is exhausted. Under some statistical assumptions, we prove that\nour proposed HR to size ratio based ordering model computes the maximum\nachievable hit probability and serves as an upper bound for all\nnon-anticipative caching policies. We derive closed form expressions for the\nupper bound under some specific object request arrival processes. We also\nprovide simulation results to validate its correctness and to compare it to the\nstate-of-the-art upper bounds. We find it to be tighter than state-of-the-art\nupper bounds for a variety of object request arrival processes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 15:30:06 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Panigrahy", "Nitish K.", ""], ["Nain", "Philippe", ""], ["Neglia", "Giovanni", ""], ["Towsley", "Don", ""]]}, {"id": "2106.08026", "submitter": "Daniel Goodman Dr", "authors": "Daniel Goodman, Roni Haecki, Tim Harris", "title": "Modeling memory bandwidth patterns on NUMA machines with performance\n  counters", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computers used for data analytics are often NUMA systems with multiple\nsockets per machine, multiple cores per socket, and multiple thread contexts\nper core. To get the peak performance out of these machines requires the\ncorrect number of threads to be placed in the correct positions on the machine.\nOne particularly interesting element of the placement of memory and threads is\nthe way it effects the movement of data around the machine, and the increased\nlatency this can introduce to reads and writes. In this paper we describe work\non modeling the bandwidth requirements of an application on a NUMA compute node\nbased on the placement of threads. The model is parameterized by sampling\nperformance counters during 2 application runs with carefully chosen thread\nplacements. Evaluating the model with thousands of measurements shows a median\ndifference from predictions of 2.34% of the bandwidth. The results of this\nmodeling can be used in a number of ways varying from: Performance debugging\nduring development where the programmer can be alerted to potentially\nproblematic memory access patterns; To systems such as Pandia which take an\napplication and predict the performance and system load of a proposed thread\ncount and placement; To libraries of data structures such as Parallel\nCollections and Smart Arrays that can abstract from the user memory placement\nand thread placement issues when parallelizing code.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:20:00 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Goodman", "Daniel", ""], ["Haecki", "Roni", ""], ["Harris", "Tim", ""]]}, {"id": "2106.08473", "submitter": "George Kesidis", "authors": "George Kesidis, Takis Konstantopoulos, Michael Zazanis", "title": "Age of Information for Small Buffer Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a message processing system whose objective is to produce the most\ncurrent information as measured by the quantity known as \"age of information\".\nWe have argued in previous papers that if we are allowed to design the message\nprocessing policy ad libitum, we should keep a small buffer and operate\naccording to a LIFO policy. In this small note we provide an analysis for the\nAoI of the P_m system which uses a buffer of size m, a single server, operating\nwithout service preemption and in a LIFO manner for stored messages. Analytical\nexpressions for the mean (or even distribution) of the AoI in steady-state are\npossible but with the aid computer algebra. We explain the the analysis for\nm=3.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:43:55 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Kesidis", "George", ""], ["Konstantopoulos", "Takis", ""], ["Zazanis", "Michael", ""]]}, {"id": "2106.08729", "submitter": "Joberto Martins Prof. Dr.", "authors": "David S. Barreto and Rafael F. Reale and Joberto S. B. Martins", "title": "Modeling and Accomplishing the BEREC Network Neutrality Policy", "comments": "17 pages, 8 figures, IJNM preprint", "journal-ref": "International Journal of Network Management, vol na, p e2148, 2020", "doi": "10.5281/zenodo.4554025", "report-no": null, "categories": "cs.NI cs.CY cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Network neutrality (NN) is a principle of equal treatment of data in network\ninfrastructures with fairness and universality being the primary outcomes of\nthe NN management practice. For networks, the accomplishment of NN management\npractice is essential to deal with heterogeneous user requirements and the\never-increasing data traffic. Current tools and methods address the NN problem\nby detecting network neutrality violations and detecting traffic\ndifferentiation. This paper proposes the NN-PCM (Network Neutrality Policy\nConformance Module) that deploys the BEREC network neutrality policy using a\nbandwidth allocation model (BAM). The NN-PCM new approach allocates bandwidth\nto network users and accomplishes the BEREC NN policy concomitantly. Network\nneutrality is achieved by grouping users with similar traffic requirements in\nclasses and leveraging the bandwidth allocation model's characteristics. The\nconceptual analysis and simulation results indicate that NN-PCM allocates\nbandwidth to users and accomplishes BEREC network neutrality conformance by\ndesign with transparent, non-discriminatory, exceptional, and proportional\nmanagement practices.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 12:11:11 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Barreto", "David S.", ""], ["Reale", "Rafael F.", ""], ["Martins", "Joberto S. B.", ""]]}, {"id": "2106.09166", "submitter": "Geng Yuan", "authors": "Geng Yuan, Zhiheng Liao, Xiaolong Ma, Yuxuan Cai, Zhenglun Kong, Xuan\n  Shen, Jingyan Fu, Zhengang Li, Chengming Zhang, Hongwu Peng, Ning Liu, Ao\n  Ren, Jinhui Wang, Yanzhi Wang", "title": "Improving DNN Fault Tolerance using Weight Pruning and Differential\n  Crossbar Mapping for ReRAM-based Edge AI", "comments": "In Proceedings of the 22nd International Symposium on Quality\n  Electronic Design (ISQED), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research demonstrated the promise of using resistive random access\nmemory (ReRAM) as an emerging technology to perform inherently parallel analog\ndomain in-situ matrix-vector multiplication -- the intensive and key\ncomputation in deep neural networks (DNNs). However, hardware failure, such as\nstuck-at-fault defects, is one of the main concerns that impedes the ReRAM\ndevices to be a feasible solution for real implementations. The existing\nsolutions to address this issue usually require an optimization to be conducted\nfor each individual device, which is impractical for mass-produced products\n(e.g., IoT devices). In this paper, we rethink the value of weight pruning in\nReRAM-based DNN design from the perspective of model fault tolerance. And a\ndifferential mapping scheme is proposed to improve the fault tolerance under a\nhigh stuck-on fault rate. Our method can tolerate almost an order of magnitude\nhigher failure rate than the traditional two-column method in representative\nDNN tasks. More importantly, our method does not require extra hardware cost\ncompared to the traditional two-column mapping scheme. The improvement is\nuniversal and does not require the optimization process for each individual\ndevice.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 22:38:04 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 22:27:16 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yuan", "Geng", ""], ["Liao", "Zhiheng", ""], ["Ma", "Xiaolong", ""], ["Cai", "Yuxuan", ""], ["Kong", "Zhenglun", ""], ["Shen", "Xuan", ""], ["Fu", "Jingyan", ""], ["Li", "Zhengang", ""], ["Zhang", "Chengming", ""], ["Peng", "Hongwu", ""], ["Liu", "Ning", ""], ["Ren", "Ao", ""], ["Wang", "Jinhui", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2106.09206", "submitter": "Liuying Ma", "authors": "Liuying Ma, Zhenqing Liu, Jin Xiong and Dejun Jiang", "title": "QWin: Enforcing Tail Latency SLO at Shared Storage Backend", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consolidating latency-critical (LC) and best-effort (BE) tenants at storage\nbackend helps to increase resources utilization. Even if tenants use dedicated\nqueues and threads to achieve performance isolation, threads are still contend\nfor CPU cores. Therefore, we argue that it is necessary to partition cores\nbetween LC and BE tenants, and meanwhile each core is dedicated to run a\nthread. Expect for frequently changing bursty load, fluctuated service time at\nstorage backend also drastically changes the need of cores. In order to\nguarantee tail latency service level objectives (SLOs), the abrupt changing\nneed of cores must be satisfied immediately. Otherwise, tail latency SLO\nviolation happens. Unfortunately, partitioning-based approaches lack the\nability to react the changing need of cores, resulting in extreme spikes in\nlatency and SLO violation happens. In this paper, we present QWin, a tail\nlatency SLO aware core allocation to enforce tail latency SLO at shared storage\nbackend. QWin consists of an SLO-to-core calculation model that accurately\ncalculates the number of cores combining with definitive runtime load\ndetermined by a flexible request-based window, and an autonomous core\nallocation that adjusts cores at adaptive frequency by dynamically changing\ncore policies. When consolidating multiple LC and BE tenants, QWin outperforms\nthe-state-of-the-art approaches in guaranteeing tail latency SLO for LC tenants\nand meanwhile increasing bandwidth of BE tenants by up to 31x.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 02:10:14 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ma", "Liuying", ""], ["Liu", "Zhenqing", ""], ["Xiong", "Jin", ""], ["Jiang", "Dejun", ""]]}, {"id": "2106.09787", "submitter": "Benjamin Michalowicz", "authors": "Benjamin Michalowicz, Eric Raut, Yan Kang, Tony Curtis, Barbara\n  Chapman, Dossay Oryspayev", "title": "Comparing the behavior of OpenMP Implementations with various\n  Applications on two different Fujitsu A64FX platforms", "comments": null, "journal-ref": null, "doi": "10.1145/3437359.3465592", "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The development of the A64FX processor by Fujitsu has been a massive\ninnovation in vectorized processors and led to Fugaku: the current world's\nfastest supercomputer. We use a variety of tools to analyze the behavior and\nperformance of several OpenMP applications with different compilers, and how\nthese applications scale on the different A64FX processors on clusters at Stony\nBrook University and RIKEN.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:00:16 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Michalowicz", "Benjamin", ""], ["Raut", "Eric", ""], ["Kang", "Yan", ""], ["Curtis", "Tony", ""], ["Chapman", "Barbara", ""], ["Oryspayev", "Dossay", ""]]}, {"id": "2106.09966", "submitter": "Mriganka Chakravarty Mr.", "authors": "Mriganka Shekhar Chakravarty and Biswabandan Panda", "title": "Introducing Fast and Secure Deterministic Stash Free Write Only\n  Oblivious RAMs for Demand Paging in Keystone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keystone is a trusted execution environment, based on RISC-V architecture. It\ndivides the memory into a secure Keystone private memory and an unsecure\nnon-Keystone memory, and allows code that lies inside the Keystone private\nmemory to execute securely. Simple demand paging in Keystone ends up leaking\nsensitive access patterns of Keystone application to the Operating System(OS),\nthat is assumed to be malicious. This is because, to access the unsecure\nnon-Keystone memory, Keystone needs support of the OS. To mitigate this,\nKeystone needs to implement oblivious demand paging while obfuscating its page\naccess patterns by using Oblivious RAM(ORAM) techniques. This causes\nsubstantial slowdown in the application execution.\n  In this paper, we bridge the performance gap between application execution\ntime with unsecure and secure demand paging in Keystone by using Deterministic,\nstash free, Write only ORAM (DetWoORAM) for oblivious demand paging. We also\nshow why DetWoORAM, that is a write-only ORAM, is sufficient for oblivious\ndemand paging. DetWoORAM logically partitions the memory into a main area and a\nholding area. The actual pages are stored in main area. We propose two\nenhancements over DetWoORAM that improves the application execution slowdown.\nThe first enhancement, which we call the Eager DetWoORAM, involves page\npreloading that exploits the deterministic access pattern of DetWoORAM, and\ntries to hide the ORAM latency. The second enhancement, which we call the\nParallel DetWoORAM, involves spawning multiple threads and each thread performs\na part of the DetWoORAM memory access algorithm. Compared to DetWoORAM that\nshows slowdown of [1.4x, 2x, and 3.24x], Eager DetWoORAM and Parallel DetWoORAM\nprovide slowdown of [1.2x, 1.8x, and 3.2x] and [1.1x, 1.1x, and 1.4x], for k=\n3, 7, and 15, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 07:42:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chakravarty", "Mriganka Shekhar", ""], ["Panda", "Biswabandan", ""]]}, {"id": "2106.10334", "submitter": "Michael Chang", "authors": "Michael Alan Chang, Aurojit Panda, Hantao Wang, Yuancheng Tsai, Rahul\n  Balakrishnan, Scott Shenker", "title": "AutoTune: Improving End-to-end Performance and Resource Efficiency for\n  Microservice Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most large web-scale applications are now built by composing collections\n(from a few up to 100s or 1000s) of microservices. Operators need to decide how\nmany resources are allocated to each microservice, and these allocations can\nhave a large impact on application performance. Manually determining\nallocations that are both cost-efficient and meet performance requirements is\nchallenging, even for experienced operators. In this paper we present AutoTune,\nan end-to-end tool that automatically minimizes resource utilization while\nmaintaining good application performance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:49:46 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 00:40:28 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chang", "Michael Alan", ""], ["Panda", "Aurojit", ""], ["Wang", "Hantao", ""], ["Tsai", "Yuancheng", ""], ["Balakrishnan", "Rahul", ""], ["Shenker", "Scott", ""]]}, {"id": "2106.10860", "submitter": "Davis Blalock", "authors": "Davis Blalock, John Guttag", "title": "Multiplying Matrices Without Multiplying", "comments": "To appear at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplying matrices is among the most fundamental and compute-intensive\noperations in machine learning. Consequently, there has been significant work\non efficiently approximating matrix multiplies. We introduce a learning-based\nalgorithm for this task that greatly outperforms existing methods. Experiments\nusing hundreds of matrices from diverse domains show that it often runs\n$100\\times$ faster than exact matrix products and $10\\times$ faster than\ncurrent approximate methods. In the common case that one matrix is known ahead\nof time, our method also has the interesting property that it requires zero\nmultiply-adds. These results suggest that a mixture of hashing, averaging, and\nbyte shuffling$-$the core operations of our method$-$could be a more promising\nbuilding block for machine learning than the sparsified, factorized, and/or\nscalar quantized matrix products that have recently been the focus of\nsubstantial research and hardware investment.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 05:08:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Blalock", "Davis", ""], ["Guttag", "John", ""]]}, {"id": "2106.11710", "submitter": "Hannes Hattenbach", "authors": "Hannes Hattenbach", "title": "Quantum-resistant digital signatures schemes for low-power IoT", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum computers are on the horizon to get to a sufficient size. These will\nthen be able to break most of the encryption and signature schemes currently in\nuse. This is the case for human interface devices as well as for IoT nodes. In\nthis paper i am comparing some signature schemes currently in the process of\nstandardization by the NIST. After explaining the underlying basis on why some\nschemes are different in some aspects compared to others i will evaluate which\ncurrently available implementations are better suited for usage in IoT\nuse-cases. We will come to further focus on the most promising schemes FALCON\nand Dilithium, which differ in one signifiant aspect that makes FALCON worse\nfor signing but very good for verification purposes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 12:29:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Hattenbach", "Hannes", ""]]}, {"id": "2106.12089", "submitter": "Anup Sarma", "authors": "Anup Sarma, Sonali Singh, Huaipan Jiang, Rui Zhang, Mahmut T Kandemir\n  and Chita R Das", "title": "Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for\n  Efficient Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent Neural Networks (RNNs), more specifically their Long Short-Term\nMemory (LSTM) variants, have been widely used as a deep learning tool for\ntackling sequence-based learning tasks in text and speech. Training of such\nLSTM applications is computationally intensive due to the recurrent nature of\nhidden state computation that repeats for each time step. While sparsity in\nDeep Neural Nets has been widely seen as an opportunity for reducing\ncomputation time in both training and inference phases, the usage of non-ReLU\nactivation in LSTM RNNs renders the opportunities for such dynamic sparsity\nassociated with neuron activation and gradient values to be limited or\nnon-existent. In this work, we identify dropout induced sparsity for LSTMs as a\nsuitable mode of computation reduction. Dropout is a widely used regularization\nmechanism, which randomly drops computed neuron values during each iteration of\ntraining. We propose to structure dropout patterns, by dropping out the same\nset of physical neurons within a batch, resulting in column (row) level hidden\nstate sparsity, which are well amenable to computation reduction at run-time in\ngeneral-purpose SIMD hardware as well as systolic arrays. We conduct our\nexperiments for three representative NLP tasks: language modelling on the PTB\ndataset, OpenNMT based machine translation using the IWSLT De-En and En-Vi\ndatasets, and named entity recognition sequence labelling using the CoNLL-2003\nshared task. We demonstrate that our proposed approach can be used to translate\ndropout-based computation reduction into reduced training time, with\nimprovement ranging from 1.23x to 1.64x, without sacrificing the target metric.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 22:44:32 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Sarma", "Anup", ""], ["Singh", "Sonali", ""], ["Jiang", "Huaipan", ""], ["Zhang", "Rui", ""], ["Kandemir", "Mahmut T", ""], ["Das", "Chita R", ""]]}, {"id": "2106.13602", "submitter": "Kai Pfeiffer", "authors": "Kai Pfeiffer, Adrien Escande, Ludovic Righetti", "title": "$\\mathcal{N}$IPM-HLSP: An Efficient Interior-Point Method for\n  Hierarchical Least-Squares Programs", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.NA cs.PF cs.RO cs.SY eess.SY math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hierarchical least-squares programs with linear constraints (HLSP) are a type\nof optimization problem very common in robotics. Each priority level contains\nan objective in least-squares form which is subject to the linear constraints\nof the higher priority hierarchy levels. Active-set methods (ASM) are a popular\nchoice for solving them. However, they can perform poorly in terms of\ncomputational time if there are large changes of the active set. We therefore\npropose a computationally efficient primal-dual interior-point method (IPM) for\nHLSP's which is able to maintain constant numbers of solver iterations in these\nsituations. We base our IPM on the null-space method which requires only a\nsingle decomposition per Newton iteration instead of two as it is the case for\nother IPM solvers. After a priority level has converged we compose a set of\nactive constraints judging upon the dual and project lower priority levels into\ntheir null-space. We show that the IPM-HLSP can be expressed in least-squares\nform which avoids the formation of the quadratic Karush-Kuhn-Tucker (KKT)\nHessian. Due to our choice of the null-space basis the IPM-HLSP is as fast as\nthe state-of-the-art ASM-HLSP solver for equality only problems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 12:54:31 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Pfeiffer", "Kai", ""], ["Escande", "Adrien", ""], ["Righetti", "Ludovic", ""]]}, {"id": "2106.14402", "submitter": "Aydin Buluc", "authors": "Ariful Azad, Oguz Selvitopi, Md Taufique Hussain, John R. Gilbert,\n  Aydin Buluc", "title": "Combinatorial BLAS 2.0: Scaling combinatorial algorithms on\n  distributed-memory systems", "comments": "To appear in IEEE Transactions on Parallel and Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.PF math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial algorithms such as those that arise in graph analysis, modeling\nof discrete systems, bioinformatics, and chemistry, are often hard to\nparallelize. The Combinatorial BLAS library implements key computational\nprimitives for rapid development of combinatorial algorithms in\ndistributed-memory systems. During the decade since its first introduction, the\nCombinatorial BLAS library has evolved and expanded significantly.\n  This paper details many of the key technical features of Combinatorial BLAS\nversion 2.0, such as communication avoidance, hierarchical parallelism via\nin-node multithreading, accelerator support via GPU kernels, generalized\nsemiring support, implementations of key data structures and functions, and\nscalable distributed I/O operations for human-readable files. Our paper also\npresents several rules of thumb for choosing the right data structures and\nfunctions in Combinatorial BLAS 2.0, under various common application\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 05:32:37 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Azad", "Ariful", ""], ["Selvitopi", "Oguz", ""], ["Hussain", "Md Taufique", ""], ["Gilbert", "John R.", ""], ["Buluc", "Aydin", ""]]}, {"id": "2106.14776", "submitter": "Martin Trefzer", "authors": "Ziwei Wang, Martin A. Trefzer, Simon J. Bale, Andy M. Tyrrell", "title": "Multi-objective Evolutionary Approach for Efficient Kernel Size and\n  Shape for CNN", "comments": "13 pages paper, plus 17 papers supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While state-of-the-art development in CNN topology, such as VGGNet and\nResNet, have become increasingly accurate, these networks are computationally\nexpensive involving billions of arithmetic operations and parameters. To\nimprove the classification accuracy, state-of-the-art CNNs usually involve\nlarge and complex convolutional layers. However, for certain applications, e.g.\nInternet of Things (IoT), where such CNNs are to be implemented on\nresource-constrained platforms, the CNN architectures have to be small and\nefficient. To deal with this problem, reducing the resource consumption in\nconvolutional layers has become one of the most significant solutions. In this\nwork, a multi-objective optimisation approach is proposed to trade-off between\nthe amount of computation and network accuracy by using Multi-Objective\nEvolutionary Algorithms (MOEAs). The number of convolution kernels and the size\nof these kernels are proportional to computational resource consumption of\nCNNs. Therefore, this paper considers optimising the computational resource\nconsumption by reducing the size and number of kernels in convolutional layers.\nAdditionally, the use of unconventional kernel shapes has been investigated and\nresults show these clearly outperform the commonly used square convolution\nkernels. The main contributions of this paper are therefore a methodology to\nsignificantly reduce computational cost of CNNs, based on unconventional kernel\nshapes, and provide different trade-offs for specific use cases. The\nexperimental results further demonstrate that the proposed method achieves\nlarge improvements in resource consumption with no significant reduction in\nnetwork performance. Compared with the benchmark CNN, the best trade-off\narchitecture shows a reduction in multiplications of up to 6X and with slight\nincrease in classification accuracy on CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:47:29 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Ziwei", ""], ["Trefzer", "Martin A.", ""], ["Bale", "Simon J.", ""], ["Tyrrell", "Andy M.", ""]]}, {"id": "2106.15284", "submitter": "Ahsan Javed Awan Dr", "authors": "Stefano Corda, Madhurya Kumaraswamy, Ahsan Javed Awan, Roel Jordans,\n  Akash Kumar, Henk Corporaal", "title": "NMPO: Near-Memory Computing Profiling and Offloading", "comments": "Euromicro Conference on Digital System Design 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world applications are now processing big-data sets, often bottlenecked\nby the data movement between the compute units and the main memory. Near-memory\ncomputing (NMC), a modern data-centric computational paradigm, can alleviate\nthese bottlenecks, thereby improving the performance of applications. The lack\nof NMC system availability makes simulators the primary evaluation tool for\nperformance estimation. However, simulators are usually time-consuming, and\nmethods that can reduce this overhead would accelerate the early-stage design\nprocess of NMC systems. This work proposes Near-Memory computing Profiling and\nOffloading (NMPO), a high-level framework capable of predicting NMC offloading\nsuitability employing an ensemble machine learning model. NMPO predicts NMC\nsuitability with an accuracy of 85.6% and, compared to prior works, can reduce\nthe prediction time by using hardware-dependent applications features by up to\n3 order of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 11:55:05 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Corda", "Stefano", ""], ["Kumaraswamy", "Madhurya", ""], ["Awan", "Ahsan Javed", ""], ["Jordans", "Roel", ""], ["Kumar", "Akash", ""], ["Corporaal", "Henk", ""]]}]