[{"id": "1909.00260", "submitter": "J Joaquin Garcia-Luna-Aceves", "authors": "JJ Garcia-Luna-Aceves, A. Varma", "title": "SCALABLE INTERNETWORKING: Final Technical Report", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": "TR-CCRG-95-F19628-93-C-0175", "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the work completed at the University of California,\nSanta Cruz under the project Scalable Internetworking sponsored by ARPA under\nContract No. F19628-93-C-0175. This report covers work performed from 1 April\n1993 to 31 December 1995. Results on routing and multicasting for large-scale\ninternets are summarized. The technical material discussed assumes familiarity\nwith the content of our proposal and previous quarterly reports submitted in\nthis project.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 18:34:35 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Garcia-Luna-Aceves", "JJ", ""], ["Varma", "A.", ""]]}, {"id": "1909.00394", "submitter": "Stanislav Polyakov", "authors": "Julia Dubenskaya and Stanislav Polyakov", "title": "Improving the Effective Utilization of Supercomputer Resources by Adding\n  Low-Priority Containerized Jobs", "comments": "11 pages, 5 figures", "journal-ref": "CEUR Workshop Proceedings. - 2019. - Vol. 2406. - P. 43-53", "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to utilize idle computational resources of\nsupercomputers. The idea is to maintain an additional queue of low-priority\nnon-parallel jobs and execute them in containers, using container migration\ntools to break the execution down into separate intervals. We propose a\ncontainer management system that can maintain this queue and interact with the\nsupercomputer scheduler. We conducted a series of experiments simulating\nsupercomputer scheduler and the proposed system. The experiments demonstrate\nthat the proposed system increases the effective utilization of supercomputer\nresources under most of the conditions, in some cases significantly improving\nthe performance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 13:25:38 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Dubenskaya", "Julia", ""], ["Polyakov", "Stanislav", ""]]}, {"id": "1909.00553", "submitter": "Prashant Jayaprakash Nair", "authors": "Seokin Hong, Bulent Abali, Alper Buyuktosunoglu, Michael B. Healy, and\n  Prashant J. Nair", "title": "Touch\\'e: Towards Ideal and Efficient Cache Compression By Mitigating\n  Tag Area Overheads", "comments": "Keywords: Compression, Caches, Tag Array, Data Array, Hashing", "journal-ref": "Proceedings of the 52nd Annual IEEE/ACM International Symposium on\n  Microarchitecture, October 2019, Pages 453-465", "doi": "10.1145/3352460.3358281", "report-no": null, "categories": "cs.AR cs.DC cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression is seen as a simple technique to increase the effective cache\ncapacity. Unfortunately, compression techniques either incur tag area overheads\nor restrict data placement to only include neighboring compressed cache blocks\nto mitigate tag area overheads. Ideally, we should be able to place arbitrary\ncompressed cache blocks without any placement restrictions and tag area\noverheads.\n  This paper proposes Touch\\'e, a framework that enables storing multiple\narbitrary compressed cache blocks within a physical cacheline without any tag\narea overheads. The Touch\\'e framework consists of three components. The first\ncomponent, called the ``Signature'' (SIGN) engine, creates shortened signatures\nfrom the tag addresses of compressed blocks. Due to this, the SIGN engine can\nstore multiple signatures in each tag entry. On a cache access, the physical\ncacheline is accessed only if there is a signature match (which has a\nnegligible probability of false positive). The second component, called the\n``Tag Appended Data'' (TADA) mechanism, stores the full tag addresses with\ndata. TADA enables Touch\\'e to detect false positive signature matches by\nensuring that the actual tag address is available for comparison. The third\ncomponent, called the ``Superblock Marker'' (SMARK) mechanism, uses a unique\nmarker in the tag entry to indicate the occurrence of compressed cache blocks\nfrom neighboring physical addresses in the same cacheline. Touch\\'e is\ncompletely hardware-based and achieves an average speedup of 12\\% (ideal 13\\%)\nwhen compared to an uncompressed baseline.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 05:39:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hong", "Seokin", ""], ["Abali", "Bulent", ""], ["Buyuktosunoglu", "Alper", ""], ["Healy", "Michael B.", ""], ["Nair", "Prashant J.", ""]]}, {"id": "1909.00709", "submitter": "Aur\\'elien Cavelan", "authors": "Aur\\'elien Cavelan and Florina M. Ciorba", "title": "Algorithm-Based Fault Tolerance for Parallel Stencil Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase in HPC systems size and complexity, together with increasing\non-chip transistor density, power limitations, and number of components, render\nmodern HPC systems subject to soft errors. Silent data corruptions (SDCs) are\ntypically caused by such soft errors in the form of bit-flips in the memory\nsubsystem and hinder the correctness of scientific applications. This work\naddresses the problem of protecting a class of iterative computational kernels,\ncalled stencils, against SDCs when executing on parallel HPC systems. Existing\nSDC detection and correction methods are in general either inaccurate,\ninefficient, or targeting specific application classes that do not include\nstencils. This work proposes a novel algorithm-based fault tolerance (ABFT)\nmethod to protect scientific applications that contain arbitrary stencil\ncomputations against SDCs. The ABFT method can be applied both online and\noffline to accurately detect and correct SDCs in 2D and 3D parallel stencil\ncomputations. We present a formal model for the proposed method including\ntheorems and proofs for the computation of the associated checksums as well as\nerror detection and correction. We experimentally evaluate the use of the\nproposed ABFT method on a real 3D stencil-based application (HotSpot3D) via a\nfault-injection, detection, and correction campaign. Results show that the\nproposed ABFT method achieves less than 8% overhead compared to the performance\nof the unprotected stencil application. Moreover, it accurately detects and\ncorrects SDCs. While the offline ABFT version corrects errors more accurately,\nit may incur a small additional overhead than its online counterpart.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 13:26:18 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Cavelan", "Aur\u00e9lien", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1909.01392", "submitter": "Matheus D'E\\c{c}a Torquato De Melo", "authors": "Matheus Torquato, Marco Vieira", "title": "Towards Models for Availability and Security Evaluation of Cloud\n  Computing with Moving Target Defense", "comments": "Student Forum paper of the 15th European Dependable Computing\n  Conference (EDCC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security is one of the most relevant concerns in cloud computing. With the\nevolution of cyber-security threats, developing innovative techniques to thwart\nattacks is of utmost importance. One recent method to improve cloud computing\nsecurity is Moving Target Defense (MTD). MTD makes use of dynamic\nreconfiguration in virtualized environments to \"confuse\" attackers or to\nnullify their knowledge about the system state. However, there is still no\nconsolidated mechanism to evaluate the trade-offs between availability and\nsecurity when using MTD on cloud computing. The evaluation through measurements\nis complex as one needs to deal with unexpected events as failures and attacks.\nTo overcome this challenge, we intend to propose a set of models to evaluate\nthe availability and security of MTD in cloud computing environments. The\nexpected results include the quantification of availability and security levels\nunder different conditions (e.g., different software aging rates, varying\nworkloads, different attack intensities).\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:38:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Torquato", "Matheus", ""], ["Vieira", "Marco", ""]]}, {"id": "1909.02053", "submitter": "Samuel S. Ogden", "authors": "Samuel S. Ogden and Tian Guo", "title": "ModiPick: SLA-aware Accuracy Optimization For Mobile Deep Inference", "comments": "11 pages (13 with citations), 9 figures. Expansion of work done for\n  PhD research qualifier presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile applications are increasingly leveraging complex deep learning models\nto deliver features, e.g., image recognition, that require high prediction\naccuracy. Such models can be both computation and memory-intensive, even for\nnewer mobile devices, and are therefore commonly hosted in powerful remote\nservers. However, current cloud-based inference services employ static model\nselection approach that can be suboptimal for satisfying application SLAs\n(service level agreements), as they fail to account for inherent dynamic mobile\nenvironment.\n  We introduce a cloud-based technique called ModiPick that dynamically selects\nthe most appropriate model for each inference request, and adapts its selection\nto match different SLAs and execution time budgets that are caused by variable\nmobile environments. The key idea of ModiPick is to make inference speed and\naccuracy trade-offs at runtime with a pool of managed deep learning models. As\nsuch, ModiPick masks unpredictable inference time budgets and therefore meets\nSLA targets, while improving accuracy within mobile network constraints. We\nevaluate ModiPick through experiments based on prototype systems and through\nsimulations. We show that ModiPick achieves comparable inference accuracy to a\ngreedy approach while improving SLA adherence by up to 88.5%.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 18:50:16 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Ogden", "Samuel S.", ""], ["Guo", "Tian", ""]]}, {"id": "1909.02765", "submitter": "Zhuoran Ji", "authors": "Zhuoran Ji", "title": "ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution\n  Neural Network Inference on Mobile GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural networks are widely used for mobile applications. However,\nGPU convolution algorithms are designed for mini-batch neural network training,\nthe single-image convolution neural network inference algorithm on mobile GPUs\nis not well-studied. After discussing the usage difference and examining the\nexisting convolution algorithms, we proposed the HNTMP convolution algorithm.\nThe HNTMP convolution algorithm achieves $14.6 \\times$ speedup than the most\npopular \\textit{im2col} convolution algorithm, and $2.30 \\times$ speedup than\nthe fastest existing convolution algorithm (direct convolution) as far as we\nknow.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 08:36:05 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 06:04:24 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ji", "Zhuoran", ""]]}, {"id": "1909.02852", "submitter": "Nachshon Cohen", "authors": "Yoav Zuriel, Michal Friedman, Gali Sheffi, Nachshon Cohen, Erez\n  Petrank", "title": "Efficient Lock-Free Durable Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-volatile memory is expected to co-exist or replace DRAM in upcoming\narchitectures. Durable concurrent data structures for non-volatile memories are\nessential building blocks for constructing adequate software for use with these\narchitectures. In this paper, we propose a new approach for durable concurrent\nsets and use this approach to build the most efficient durable hash tables\navailable today. Evaluation shows a performance improvement factor of up to\n3.3x over existing technology.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:21:12 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 16:56:11 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zuriel", "Yoav", ""], ["Friedman", "Michal", ""], ["Sheffi", "Gali", ""], ["Cohen", "Nachshon", ""], ["Petrank", "Erez", ""]]}, {"id": "1909.04550", "submitter": "Jianshen Liu", "authors": "Jianshen Liu, Philip Kufeldt, Carlos Maltzahn", "title": "MBWU: Benefit Quantification for Data Access Function Offloading", "comments": "16 pages, 11 figures", "journal-ref": "HPC I/O in the Data Center Workshop, 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The storage industry is considering new kinds of storage devices that support\ndata access function offloading, i.e. the ability to perform data access\nfunctions on the storage device itself as opposed to performing it on a\nseparate compute system to which the storage device is connected. But what is\nthe benefit of offloading to a storage device that is controlled by an embedded\nplatform, very different from a host platform? To quantify the benefit, we need\na measurement methodology that enables apple-to-apple comparisons between\ndifferent platforms. We propose a Media-based Work Unit (MBWU, pronounced\n\"MibeeWu\"), and an MBWU-based measurement methodology to standardize the\nplatform efficiency evaluation so as to quantify the benefit of offloading. To\ndemonstrate the merit of this methodology, we implemented a prototype to\nautomate quantifying the benefit of offloading the key-value data access\nfunction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 04:15:33 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Liu", "Jianshen", ""], ["Kufeldt", "Philip", ""], ["Maltzahn", "Carlos", ""]]}, {"id": "1909.04783", "submitter": "Samuel S. Ogden", "authors": "Samuel S. Ogden and Tian Guo", "title": "Characterizing the Deep Neural Networks Inference Performance of Mobile\n  Applications", "comments": "11 pages (12 with references and bios), 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's mobile applications are increasingly leveraging deep neural networks\nto provide novel features, such as image and speech recognitions. To use a\npre-trained deep neural network, mobile developers can either host it in a\ncloud server, referred to as cloud-based inference, or ship it with their\nmobile application, referred to as on-device inference. In this work, we\ninvestigate the inference performance of these two common approaches on both\nmobile devices and public clouds, using popular convolutional neural networks.\nOur measurement study suggests the need for both on-device and cloud-based\ninferences for supporting mobile applications. In particular, newer mobile\ndevices is able to run mobile-optimized CNN models in reasonable time. However,\nfor older mobile devices or to use more complex CNN models, mobile applications\nshould opt in for cloud-based inference. We further demonstrate that variable\nnetwork conditions can lead to poor cloud-based inference end-to-end time. To\nsupport efficient cloud-based inference, we propose a CNN model selection\nalgorithm called CNNSelect that dynamically selects the most appropriate CNN\nmodel for each inference request, and adapts its selection to match different\nSLAs and execution time budgets that are caused by variable mobile\nenvironments. The key idea of CNNSelect is to make inference speed and accuracy\ntrade-offs at runtime using a set of CNN models. We demonstrated that CNNSelect\nsmoothly improves inference accuracy while maintaining SLA attainment in 88.5%\nmore cases than a greedy baseline.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 22:33:10 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ogden", "Samuel S.", ""], ["Guo", "Tian", ""]]}, {"id": "1909.04885", "submitter": "Michael Kaufmann", "authors": "Michael Kaufmann, Kornilios Kourtis, Celestine Mendler-D\\\"unner,\n  Adrian Sch\\\"upbach, Thomas Parnell", "title": "Addressing Algorithmic Bottlenecks in Elastic Machine Learning with\n  Chicle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning training is one of the most common and important\nworkloads running on data centers today, but it is rarely executed alone.\nInstead, to reduce costs, computing resources are consolidated and shared by\ndifferent applications. In this scenario, elasticity and proper load balancing\nare vital to maximize efficiency, fairness, and utilization. Currently, most\ndistributed training frameworks do not support the aforementioned properties. A\nfew exceptions that do support elasticity, imitate generic distributed\nframeworks and use micro-tasks. In this paper we illustrate that micro-tasks\nare problematic for machine learning applications, because they require a high\ndegree of parallelism which hinders the convergence of distributed training at\na pure algorithmic level (i.e., ignoring overheads and scalability\nlimitations). To address this, we propose Chicle, a new elastic distributed\ntraining framework which exploits the nature of machine learning algorithms to\nimplement elasticity and load balancing without micro-tasks. We use Chicle to\ntrain deep neural network as well as generalized linear models, and show that\nChicle achieves performance competitive with state of the art rigid frameworks,\nwhile efficiently enabling elastic execution and dynamic load balancing.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 07:37:05 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kaufmann", "Michael", ""], ["Kourtis", "Kornilios", ""], ["Mendler-D\u00fcnner", "Celestine", ""], ["Sch\u00fcpbach", "Adrian", ""], ["Parnell", "Thomas", ""]]}, {"id": "1909.05182", "submitter": "Jie Ren", "authors": "Jie Ren, Jiaolin Luo, Kai Wu, Minjia Zhang and Dong Li", "title": "Sentinel: Runtime Data Management on Heterogeneous Main MemorySystems\n  for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-managed heterogeneous memory (HM) provides a promising solution to\nincrease memory capacity and cost efficiency. However, to release the\nperformance potential of HM, we face a problem of data management. Given an\napplication with various execution phases and each with possibly distinct\nworking sets, we must move data between memory components of HM to optimize\nperformance. The deep neural network (DNN), as a common workload on data\ncenters, imposes great challenges on data management on HM. This workload often\nemploys a task dataflow execution model, and is featured with a large amount of\nsmall data objects and fine-grained operations (tasks). This execution model\nimposes challenges on memory profiling and efficient data migration.\n  We present Sentinel, a runtime system that automatically optimizes data\nmigration (i.e., data management) on HM to achieve performance similar to that\non the fast memory-only system with a much smaller capacity of fast memory. To\nachieve this,Sentinel exploits domain knowledge about deep learning to adopt a\ncustom approach for data management. Sentinel leverages workload repeatability\nto break the dilemma between profiling accuracy and overhead; It enables\nprofiling and data migration at the granularity of data objects (not pages), by\ncontrolling memory allocation. This method bridges the semantic gap between\noperating system and applications. By associating data objects with the DNN\ntopology, Sentinel avoids unnecessary data movement and proactively triggers\ndata movement. Using only 20% of peak memory consumption of DNN models as fast\nmemory size, Sentinel achieves the same or comparable performance (at most 8%\nperformance difference) to that of the fast memory-only system on common DNN\nmodels; Sentinel also consistently outperforms a state-of-the-art solution by\n18%.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:30:46 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ren", "Jie", ""], ["Luo", "Jiaolin", ""], ["Wu", "Kai", ""], ["Zhang", "Minjia", ""], ["Li", "Dong", ""]]}, {"id": "1909.06301", "submitter": "Alessandro Fanfarillo", "authors": "Alessandro Fanfarillo, Davide Del Vento", "title": "AITuning: Machine Learning-based Tuning Tool for Run-Time Communication\n  Libraries", "comments": "11 pages, 1 figure, ParCo 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of tuning communication libraries by\nusing a deep reinforcement learning approach. Reinforcement learning is a\nmachine learning technique incredibly effective in solving game-like\nsituations. In fact, tuning a set of parameters in a communication library in\norder to get better performance in a parallel application can be expressed as a\ngame: Find the right combination/path that provides the best reward. Even\nthough AITuning has been designed to be utilized with different run-time\nlibraries, we focused this work on applying it to the OpenCoarrays run-time\ncommunication library, built on top of MPI-3. This work not only shows the\npotential of using a reinforcement learning algorithm for tuning communication\nlibraries, but also demonstrates how the MPI Tool Information Interface,\nintroduced by the MPI-3 standard, can be used effectively by run-time libraries\nto improve the performance without human intervention.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 15:48:16 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Fanfarillo", "Alessandro", ""], ["Del Vento", "Davide", ""]]}, {"id": "1909.06618", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano and Ond\\v{r}ej Bojar", "title": "Efficiency Metrics for Data-Driven Models: A Text Summarization Case\n  Study", "comments": "11 pages, 4 tables, 2 figures, 6 equations. Published in proceedings\n  of INLG 2019, the 12th International Conference on Natural Language\n  Generation, Tokyo, Japan", "journal-ref": null, "doi": "10.18653/v1/W19-8630", "report-no": null, "categories": "cs.CL cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using data-driven models for solving text summarization or similar tasks has\nbecome very common in the last years. Yet most of the studies report basic\naccuracy scores only, and nothing is known about the ability of the proposed\nmodels to improve when trained on more data. In this paper, we define and\npropose three data efficiency metrics: data score efficiency, data time\ndeficiency and overall data efficiency. We also propose a simple scheme that\nuses those metrics and apply it for a more comprehensive evaluation of popular\nmethods on text summarization and title generation tasks. For the latter task,\nwe process and release a huge collection of 35 million abstract-title pairs\nfrom scientific articles. Our results reveal that among the tested models, the\nTransformer is the most efficient on both tasks.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 16:03:49 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "1909.07532", "submitter": "Arvind Narayanan", "authors": "Arvind Narayanan, Eman Ramadan, Jason Carpenter, Qingxu Liu, Yu Liu,\n  Feng Qian, Zhi-Li Zhang", "title": "A First Look at Commercial 5G Performance on Smartphones", "comments": "Published at The Web Conference 2020 (WWW 2020). Please include WWW\n  in any citations", "journal-ref": "Proceedings of The Web Conference 2020 (WWW'20)", "doi": "10.1145/3366423.3380169", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We conduct to our knowledge a first measurement study of commercial 5G\nperformance on smartphones by closely examining 5G networks of three carriers\n(two mmWave carriers, one mid-band carrier) in three U.S. cities. We conduct\nextensive field tests on 5G performance in diverse urban environments. We\nsystematically analyze the handoff mechanisms in 5G and their impact on network\nperformance. We explore the feasibility of using location and possibly other\nenvironmental information to predict the network performance. We also study the\napp performance (web browsing and HTTP download) over 5G. Our study consumes\nmore than 15 TB of cellular data. Conducted when 5G just made its debut, it\nprovides a \"baseline\" for studying how 5G performance evolves, and identifies\nkey research directions on improving 5G users' experience in a cross-layer\nmanner. We have released the data collected from our study (referred to as\n5Gophers) at https://fivegophers.umn.edu/www20.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 00:32:43 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 12:51:01 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Narayanan", "Arvind", ""], ["Ramadan", "Eman", ""], ["Carpenter", "Jason", ""], ["Liu", "Qingxu", ""], ["Liu", "Yu", ""], ["Qian", "Feng", ""], ["Zhang", "Zhi-Li", ""]]}, {"id": "1909.07865", "submitter": "Daniele De Sensi", "authors": "Daniele De Sensi, Salvatore Di Girolamo, Torsten Hoefler", "title": "Mitigating Network Noise on Dragonfly Networks through Application-Aware\n  Routing", "comments": "Accepted at The International Conference for High Performance\n  Computing Networking, Storage, and Analysis (SC '19)", "journal-ref": "Published in Proceedings of The International Conference for High\n  Performance Computing Networking, Storage, and Analysis (SC '19) (2019)", "doi": "10.1145/3295500.3356196", "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System noise can negatively impact the performance of HPC systems, and the\ninterconnection network is one of the main factors contributing to this\nproblem. To mitigate this effect, adaptive routing sends packets on non-minimal\npaths if they are less congested. However, while this may mitigate interference\ncaused by congestion, it also generates more traffic since packets traverse\nadditional hops, causing in turn congestion on other applications and on the\napplication itself. In this paper, we first describe how to estimate network\nnoise. By following these guidelines, we show how noise can be reduced by using\nrouting algorithms which select minimal paths with a higher probability. We\nexploit this knowledge to design an algorithm which changes the probability of\nselecting minimal paths according to the application characteristics. We\nvalidate our solution on microbenchmarks and real-world applications on two\nsystems relying on a Dragonfly interconnection network, showing noise reduction\nand performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:54:09 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["De Sensi", "Daniele", ""], ["Di Girolamo", "Salvatore", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1909.08247", "submitter": "EPTCS", "authors": "Giacomo Da Col, Erich Teppan", "title": "Google vs IBM: A Constraint Solving Challenge on the Job-Shop Scheduling\n  Problem", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 259-265", "doi": "10.4204/EPTCS.306.30", "report-no": null, "categories": "cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The job-shop scheduling is one of the most studied optimization problems from\nthe dawn of computer era to the present day. Its combinatorial nature makes it\neasily expressible as a constraint satisfaction problem. In this paper, we\ncompare the performance of two constraint solvers on the job-shop scheduling\nproblem. The solvers in question are: OR-Tools, an open-source solver developed\nby Google and winner of the last MiniZinc Challenge, and CP Optimizer, a\nproprietary IBM constraint solver targeted at industrial scheduling problems.\nThe comparison is based on the goodness of the solutions found and the time\nrequired to solve the problem instances. First, we target the classic\nbenchmarks from the literature, then we carry out the comparison on a benchmark\nthat was created with known optimal solution, with size comparable to\nreal-world industrial problems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:08:10 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Da Col", "Giacomo", ""], ["Teppan", "Erich", ""]]}, {"id": "1909.08999", "submitter": "Suryanarayana Murthy Durbhakula", "authors": "Murthy Durbhakula", "title": "Branch prediction related Optimizations for Multithreaded Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major chip manufacturers have all introduced Multithreaded processors. These\nprocessors are used for running a variety of workloads. Efficient resource\nutilization is an important design aspect in such processors. Depending on the\nworkload, mis-speculated execution can severely impact resource utilization and\npower utilization. In general, compared to a uniprocessor, a multithreaded\nprocessor may have better tolerance towards mis-speculation. However there can\nstill be phases where even a multi-threaded processor performance may get\nimpacted by branch induced mis-speculation. In this paper I propose monitoring\nthe branch predictor behavior of various hardware threads running on the\nmulti-threaded processor and use that information as a feedback to the thread\narbiter/picker which schedules the next thread to fetch instructions from. If I\nfind that a particular thread is going through a phase where it is consistently\nmis-predicting its branches and its average branch misprediction stall is above\na specific threshold then I temporarily reduce the priority for picking that\nthread. I do a qualitative comparison of various solutions to the problem of\nresource inefficiency caused due to mis-speculated branches in multithreaded\nprocessors. This work can be extended by doing a quantitative evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:53:55 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Durbhakula", "Murthy", ""]]}, {"id": "1909.09390", "submitter": "Gildas Morvan", "authors": "Yu-Lin Huang and Gildas Morvan and Fr\\'ed\\'eric Pichon and David\n  Mercier", "title": "SPSC: a new execution policy for exploring discrete-time stochastic\n  simulations", "comments": "Accepted in PRIMA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new method called SPSC (Simulation,\nPartitioning, Selection, Cloning) to estimate efficiently the probability of\npossible solutions in stochastic simulations. This method can be applied to any\ntype of simulation, however it is particularly suitable for multi-agent-based\nsimulations (MABS). Therefore, its performance is evaluated on a well-known\nMABS and compared to the classical approach, i.e., Monte Carlo.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 09:37:06 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Huang", "Yu-Lin", ""], ["Morvan", "Gildas", ""], ["Pichon", "Fr\u00e9d\u00e9ric", ""], ["Mercier", "David", ""]]}, {"id": "1909.09694", "submitter": "Alain Simonian", "authors": "R. Nasri, A. Simonian, and F. Guillemin", "title": "An inversion formula with hypergeometric polynomials and application to\n  singular integral operators", "comments": "29 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:1904.08283", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given parameters $x \\notin \\mathbb{R}^- \\cup \\{1\\}$ and $\\nu$,\n$\\mathrm{Re}(\\nu) < 0$, and the space $\\mathscr{H}_0$ of entire functions in\n$\\mathbb{C}$ vanishing at $0$, we consider the family of operators\n$\\mathfrak{L} = c_0 \\cdot \\delta \\circ \\mathfrak{M}$ with constant $c_0 =\n\\nu(1-\\nu)x/(1-x)$, $\\delta = z \\, \\mathrm{d}/\\mathrm{d}z$ and integral\noperator $\\mathfrak{M}$ defined by $$ \\mathfrak{M}f(z) = \\int_0^1 e^{-\n\\frac{z}{x}t^{-\\nu}(1-(1-x)t)} \\, f \\left ( \\frac{z}{x} \\, t^{-\\nu}(1-t) \\right\n) \\, \\frac{\\mathrm{d}t}{t}, \\qquad z \\in \\mathbb{C}, $$ for all $f \\in\n\\mathscr{H}_0$. Inverting $\\mathfrak{L}$ or $\\mathfrak{M}$ proves equivalent to\nsolve a singular Volterra equation of the first kind.\n  The inversion of operator $\\mathfrak{L}$ on $\\mathscr{H}_0$ leads us to\nderive a new class of linear inversion formulas $T = A(x,\\nu) \\cdot S\n\\Leftrightarrow S = B(x,\\nu) \\cdot T$ between sequences $S = (S_n)_{n \\in\n\\mathbb{N}^*}$ and $T = (T_n)_{n \\in \\mathbb{N}^*}$, where the infinite\nlower-triangular matrix $A(x,\\nu)$ and its inverse $B(x,\\nu)$ involve\nHypergeometric polynomials $F(\\cdot)$, namely $$\n  \\left\\{\n  \\begin{array}{ll}\n  A_{n,k}(x,\\nu) = \\displaystyle (-1)^k\\binom{n}{k}F(k-n,-n\\nu;-n;x),\n  B_{n,k}(x,\\nu) = \\displaystyle (-1)^k\\binom{n}{k}F(k-n,k\\nu;k;x)\n  \\end{array} \\right. $$ for $1 \\leqslant k \\leqslant n$. Functional relations\nbetween the ordinary (resp. exponential) generating functions of the related\nsequences $S$ and $T$ are also given. These relations finally enable us to\nderive the integral representation $$ \\mathfrak{L}^{-1}f(z) = \\frac{1-x}{2i\\pi\nx} \\, e^{z} \\int_{(0+)}^1 \\frac{e^{-xtz}}{t(1-t)} \\, f \\left ( xz \\,\n(-t)^{\\nu}(1-t)^{1-\\nu} \\right ) \\, \\mathrm{d}t, \\quad z \\in \\mathbb{C}, $$ for\nthe inverse $\\mathfrak{L}^{-1}$ of operator $\\mathfrak{L}$ on $\\mathscr{H}_0$,\nwhere the integration contour encircles the point 0.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:40:57 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Nasri", "R.", ""], ["Simonian", "A.", ""], ["Guillemin", "F.", ""]]}, {"id": "1909.09731", "submitter": "M Sadegh Riazi", "authors": "M. Sadegh Riazi and Kim Laine and Blake Pelton and Wei Dai", "title": "HEAX: An Architecture for Computing on Encrypted Data", "comments": "To appear in proceedings of ACM ASPLOS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in cloud computing, concerns surrounding data\nprivacy, security, and confidentiality also have been increased significantly.\nNot only cloud providers are susceptible to internal and external hacks, but\nalso in some scenarios, data owners cannot outsource the computation due to\nprivacy laws such as GDPR, HIPAA, or CCPA. Fully Homomorphic Encryption (FHE)\nis a groundbreaking invention in cryptography that, unlike traditional\ncryptosystems, enables computation on encrypted data without ever decrypting\nit. However, the most critical obstacle in deploying FHE at large-scale is the\nenormous computation overhead.\n  In this paper, we present HEAX, a novel hardware architecture for FHE that\nachieves unprecedented performance improvement. HEAX leverages multiple levels\nof parallelism, ranging from ciphertext-level to fine-grained modular\narithmetic level. Our first contribution is a new highly-parallelizable\narchitecture for number-theoretic transform (NTT) which can be of independent\ninterest as NTT is frequently used in many lattice-based cryptography systems.\nBuilding on top of NTT engine, we design a novel architecture for computation\non homomorphically encrypted data. We also introduce several techniques to\nenable an end-to-end, fully pipelined design as well as reducing on-chip memory\nconsumption. Our implementation on reconfigurable hardware demonstrates\n164-268x performance improvement for a wide range of FHE parameters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 22:27:06 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 21:17:05 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Riazi", "M. Sadegh", ""], ["Laine", "Kim", ""], ["Pelton", "Blake", ""], ["Dai", "Wei", ""]]}, {"id": "1909.09756", "submitter": "Sameer Kumar", "authors": "Sameer Kumar, Victor Bitorff, Dehao Chen, Chiachen Chou, Blake\n  Hechtman, HyoukJoong Lee, Naveen Kumar, Peter Mattson, Shibo Wang, Tao Wang,\n  Yuanzhong Xu, Zongwei Zhou", "title": "Scale MLPerf-0.6 models on Google TPU-v3 Pods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent submission of Google TPU-v3 Pods to the industry wide MLPerf v0.6\ntraining benchmark demonstrates the scalability of a suite of industry relevant\nML models. MLPerf defines a suite of models, datasets and rules to follow when\nbenchmarking to ensure results are comparable across hardware, frameworks and\ncompanies. Using this suite of models, we discuss the optimizations and\ntechniques including choice of optimizer, spatial partitioning and weight\nupdate sharding necessary to scale to 1024 TPU chips. Furthermore, we identify\nproperties of models that make scaling them challenging, such as limited data\nparallelism and unscaled weights. These optimizations contribute to record\nperformance in transformer, Resnet-50 and SSD in the Google MLPerf-0.6\nsubmission.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 01:12:38 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 17:03:37 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 18:37:01 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Kumar", "Sameer", ""], ["Bitorff", "Victor", ""], ["Chen", "Dehao", ""], ["Chou", "Chiachen", ""], ["Hechtman", "Blake", ""], ["Lee", "HyoukJoong", ""], ["Kumar", "Naveen", ""], ["Mattson", "Peter", ""], ["Wang", "Shibo", ""], ["Wang", "Tao", ""], ["Xu", "Yuanzhong", ""], ["Zhou", "Zongwei", ""]]}, {"id": "1909.10123", "submitter": "Rohan Kadekodi", "authors": "Rohan Kadekodi, Se Kwon Lee, Sanidhya Kashyap, Taesoo Kim, Aasheesh\n  Kolli, Vijay Chidambaram", "title": "SplitFS: Reducing Software Overhead in File Systems for Persistent\n  Memory", "comments": null, "journal-ref": null, "doi": "10.1145/3341301.3359631", "report-no": null, "categories": "cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SplitFS, a file system for persistent memory (PM) that reduces\nsoftware overhead significantly compared to state-of-the-art PM file systems.\nSplitFS presents a novel split of responsibilities between a user-space library\nfile system and an existing kernel PM file system. The user-space library file\nsystem handles data operations by intercepting POSIX calls, memory-mapping the\nunderlying file, and serving the read and overwrites using processor loads and\nstores. Metadata operations are handled by the kernel PM file system (ext4\nDAX). SplitFS introduces a new primitive termed relink to efficiently support\nfile appends and atomic data operations. SplitFS provides three consistency\nmodes, which different applications can choose from, without interfering with\neach other. SplitFS reduces software overhead by up-to 4x compared to the NOVA\nPM file system, and 17x compared to ext4-DAX. On a number of micro-benchmarks\nand applications such as the LevelDB key-value store running the YCSB\nbenchmark, SplitFS increases application performance by up to 2x compared to\next4 DAX and NOVA while providing similar consistency guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 02:03:13 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Kadekodi", "Rohan", ""], ["Lee", "Se Kwon", ""], ["Kashyap", "Sanidhya", ""], ["Kim", "Taesoo", ""], ["Kolli", "Aasheesh", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "1909.10562", "submitter": "Wei Wei", "authors": "Wei Zhang, Wei Wei, Lingjie Xu, Lingling Jin, Cheng Li", "title": "AI Matrix: A Deep Learning Benchmark for Alibaba Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alibaba has China's largest e-commerce platform. To support its diverse\nbusinesses, Alibaba has its own large-scale data centers providing the\ncomputing foundation for a wide variety of software applications. Among these\napplications, deep learning (DL) has been playing an important role in\ndelivering services like image recognition, objection detection, text\nrecognition, recommendation, and language processing. To build more efficient\ndata centers that deliver higher performance for these DL applications, it is\nimportant to understand their computational needs and use that information to\nguide the design of future computing infrastructure. An effective way to\nachieve this is through benchmarks that can fully represent Alibaba's DL\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 18:29:25 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhang", "Wei", ""], ["Wei", "Wei", ""], ["Xu", "Lingjie", ""], ["Jin", "Lingling", ""], ["Li", "Cheng", ""]]}, {"id": "1909.10609", "submitter": "Michel Rottleuthner", "authors": "Michel Rottleuthner, Thomas C. Schmidt, Matthias W\\\"ahlisch", "title": "Eco: A Hardware-Software Co-Design for In Situ Power Measurement on\n  Low-end IoT Systems", "comments": null, "journal-ref": "Proceedings of ENSsys 2019", "doi": null, "report-no": null, "categories": "eess.SP cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-constrained sensor nodes can adaptively optimize their energy\nconsumption if a continuous measurement exists. This is of particular\nimportance in scenarios of high dynamics such as energy harvesting or adaptive\ntask scheduling. However, self-measuring of power consumption at reasonable\ncost and complexity is unavailable as a generic system service. In this paper,\nwe present Eco, a hardware-software co-design enabling generic energy\nmanagement on IoT nodes. Eco is tailored to devices with limited resources and\nthus targets most of the upcoming IoT scenarios. The proposed measurement\nmodule combines commodity components with a common system interfaces to achieve\neasy, flexible integration with various hardware platforms and the RIOT IoT\noperating system. We thoroughly evaluate and compare accuracy and overhead. Our\nfindings indicate that our commodity design competes well with highly optimized\nsolutions, while being significantly more versatile. We employ Eco for energy\nmanagement on RIOT and validate its readiness for deployment in a five-week\nfield trial integrated with energy harvesting.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 20:39:33 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Rottleuthner", "Michel", ""], ["Schmidt", "Thomas C.", ""], ["W\u00e4hlisch", "Matthias", ""]]}, {"id": "1909.11469", "submitter": "Vinu Joseph", "authors": "Mark Van der Merwe, Vinu Joseph, Ganesh Gopalakrishnan", "title": "Message Scheduling for Performant, Many-Core Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Belief Propagation (BP) is a message-passing algorithm for approximate\ninference over Probabilistic Graphical Models (PGMs), finding many applications\nsuch as computer vision, error-correcting codes, and protein-folding. While\ngeneral, the convergence and speed of the algorithm has limited its practical\nuse on difficult inference problems. As an algorithm that is highly amenable to\nparallelization, many-core Graphical Processing Units (GPUs) could\nsignificantly improve BP performance. Improving BP through many-core systems is\nnon-trivial: the scheduling of messages in the algorithm strongly affects\nperformance. We present a study of message scheduling for BP on GPUs. We\ndemonstrate that BP exhibits a tradeoff between speed and convergence based on\nparallelism and show that existing message schedulings are not able to utilize\nthis tradeoff. To this end, we present a novel randomized message scheduling\napproach, Randomized BP (RnBP), which outperforms existing methods on the GPU.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 05:19:33 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Van der Merwe", "Mark", ""], ["Joseph", "Vinu", ""], ["Gopalakrishnan", "Ganesh", ""]]}, {"id": "1909.11822", "submitter": "Adam Rupe", "authors": "Adam Rupe, Nalini Kumar, Vladislav Epifanov, Karthik Kashinath,\n  Oleksandr Pavlyk, Frank Schlimbach, Mostofa Patwary, Sergey Maidanov, Victor\n  Lee, Prabhat, James P. Crutchfield", "title": "DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in\n  Spatiotemporal Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting actionable insight from complex unlabeled scientific data is an\nopen challenge and key to unlocking data-driven discovery in science.\nComplementary and alternative to supervised machine learning approaches,\nunsupervised physics-based methods based on behavior-driven theories hold great\npromise. Due to computational limitations, practical application on real-world\ndomain science problems has lagged far behind theoretical development. We\npresent our first step towards bridging this divide - DisCo - a\nhigh-performance distributed workflow for the behavior-driven local causal\nstate theory. DisCo provides a scalable unsupervised physics-based\nrepresentation learning method that decomposes spatiotemporal systems into\ntheir structurally relevant components, which are captured by the latent local\ncausal state variables. Complex spatiotemporal systems are generally highly\nstructured and organize around a lower-dimensional skeleton of coherent\nstructures, and in several firsts we demonstrate the efficacy of DisCo in\ncapturing such structures from observational and simulated scientific data. To\nthe best of our knowledge, DisCo is also the first application software\ndeveloped entirely in Python to scale to over 1000 machine nodes, providing\ngood performance along with ensuring domain scientists' productivity. We\ndeveloped scalable, performant methods optimized for Intel many-core processors\nthat will be upstreamed to open-source Python library packages. Our capstone\nexperiment, using newly developed DisCo workflow and libraries, performs\nunsupervised spacetime segmentation analysis of CAM5.1 climate simulation data,\nprocessing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel\nHaswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64%\nstrong-scaling efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 23:52:57 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Rupe", "Adam", ""], ["Kumar", "Nalini", ""], ["Epifanov", "Vladislav", ""], ["Kashinath", "Karthik", ""], ["Pavlyk", "Oleksandr", ""], ["Schlimbach", "Frank", ""], ["Patwary", "Mostofa", ""], ["Maidanov", "Sergey", ""], ["Lee", "Victor", ""], ["Prabhat", "", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1909.13639", "submitter": "Ameer Haj-Ali", "authors": "Ameer Haj-Ali, Nesreen K. Ahmed, Ted Willke, Sophia Shao, Krste\n  Asanovic, Ion Stoica", "title": "NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges arising when compilers vectorize loops for today's\nSIMD-compatible architectures is to decide if vectorization or interleaving is\nbeneficial. Then, the compiler has to determine how many instructions to pack\ntogether and how many loop iterations to interleave. Compilers are designed\ntoday to use fixed-cost models that are based on heuristics to make\nvectorization decisions on loops. However, these models are unable to capture\nthe data dependency, the computation graph, or the organization of\ninstructions. Alternatively, software engineers often hand-write the\nvectorization factors of every loop. This, however, places a huge burden on\nthem, since it requires prior experience and significantly increases the\ndevelopment time. In this work, we explore a novel approach for handling loop\nvectorization and propose an end-to-end solution using deep reinforcement\nlearning (RL). We conjecture that deep RL can capture different instructions,\ndependencies, and data structures to enable learning a sophisticated model that\ncan better predict the actual performance cost and determine the optimal\nvectorization factors. We develop an end-to-end framework, from code to\nvectorization, that integrates deep RL in the LLVM compiler. Our proposed\nframework takes benchmark codes as input and extracts the loop codes. These\nloop codes are then fed to a loop embedding generator that learns an embedding\nfor these loops. Finally, the learned embeddings are used as input to a Deep RL\nagent, which determines the vectorization factors for all the loops. We further\nextend our framework to support multiple supervised learning methods. We\nevaluate our approaches against the currently used LLVM vectorizer and loop\npolyhedral optimization techniques. Our experiments show 1.29X-4.73X\nperformance speedup compared to baseline and only 3% worse than the brute-force\nsearch on a wide range of benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 12:29:09 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 12:29:38 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 22:57:00 GMT"}, {"version": "v4", "created": "Sat, 4 Jan 2020 09:11:03 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Haj-Ali", "Ameer", ""], ["Ahmed", "Nesreen K.", ""], ["Willke", "Ted", ""], ["Shao", "Sophia", ""], ["Asanovic", "Krste", ""], ["Stoica", "Ion", ""]]}, {"id": "1909.13654", "submitter": "Tian Zhao", "authors": "Tian Zhao, Yaqi Zhang, Kunle Olukotun", "title": "Serving Recurrent Neural Networks Efficiently with a Spatial Accelerator", "comments": null, "journal-ref": "Proceedings of the 2 nd SysML Conference, Palo Alto, CA, USA,\n  2019. Copyright 2019 by the author(s)", "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) applications form a major class of AI-powered,\nlow-latency data center workloads. Most execution models for RNN acceleration\nbreak computation graphs into BLAS kernels, which lead to significant\ninter-kernel data movement and resource underutilization. We show that by\nsupporting more general loop constructs that capture design parameters in\naccelerators, it is possible to improve resource utilization using cross-kernel\noptimization without sacrificing programmability. Such abstraction level\nenables a design space search that can lead to efficient usage of on-chip\nresources on a spatial architecture across a range of problem sizes. We\nevaluate our optimization strategy on such abstraction with DeepBench using a\nconfigurable spatial accelerator. We demonstrate that this implementation\nprovides a geometric speedup of 30x in performance, 1.6x in area, and 2x in\npower efficiency compared to a Tesla V100 GPU, and a geometric speedup of 2x\ncompared to Microsoft Brainwave implementation on a Stratix 10 FPGA.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 00:55:17 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhao", "Tian", ""], ["Zhang", "Yaqi", ""], ["Olukotun", "Kunle", ""]]}]