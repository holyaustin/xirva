[{"id": "1309.0052", "submitter": "Kamran Karimi", "authors": "Kamran Karimi, Aleks G. Pamir, M. Haris Afzal", "title": "Accelerating a Cloud-Based Software GNSS Receiver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss ways to reduce the execution time of a software\nGlobal Navigation Satellite System (GNSS) receiver that is meant for offline\noperation in a cloud environment. Client devices record satellite signals they\nreceive, and send them to the cloud, to be processed by this software. The goal\nof this project is for each client request to be processed as fast as possible,\nbut also to increase total system throughput by making sure as many requests as\npossible are processed within a unit of time. The characteristics of our\napplication provided both opportunities and challenges for increasing\nperformance. We describe the speedups we obtained by enabling the software to\nexploit multi-core CPUs and GPGPUs. We mention which techniques worked for us\nand which did not. To increase throughput, we describe how we control the\nresources allocated to each invocation of the software to process a client\nrequest, such that multiple copies of the application can run at the same time.\nWe use the notion of effective running time to measure the system's throughput\nwhen running multiple instances at the same time, and show how we can determine\nwhen the system's computing resources have been saturated.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2013 01:18:14 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2013 05:51:08 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Karimi", "Kamran", ""], ["Pamir", "Aleks G.", ""], ["Afzal", "M. Haris", ""]]}, {"id": "1309.0534", "submitter": "Raphael kena Poss", "authors": "Raphael 'kena' Poss", "title": "Machines are benchmarked by code, not algorithms", "comments": "34 pages, 11 figures, 11 listings, 17 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article highlights how small modifications to either the source code of\na benchmark program or the compilation options may impact its behavior on a\nspecific machine. It argues that for evaluating machines, benchmark providers\nand users be careful to ensure reproducibility of results based on the machine\ncode actually running on the hardware and not just source code. The article\nuses color to grayscale conversion of digital images as a running example.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 20:34:09 GMT"}], "update_date": "2013-09-04", "authors_parsed": [["Poss", "Raphael 'kena'", ""]]}, {"id": "1309.0551", "submitter": "Shyam Srinivasan", "authors": "Shyam Srinivasan", "title": "Optimizing the performance of Lattice Gauge Theory simulations with\n  Streaming SIMD extensions", "comments": "Technical report of results of optimization of physics (lattice gauge\n  theory) simulations for Intel Architectures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two factors, which affect simulation quality are the amount of computing\npower and implementation. The Streaming SIMD (single instruction multiple data)\nextensions (SSE) present a technique for influencing both by exploiting the\nprocessor's parallel functionalism. In this paper, we show how SSE improves\nperformance of lattice gauge theory simulations. We identified two significant\ntrends through an analysis of data from various runs. The speed-ups were higher\nfor single precision than double precision floating point numbers. Notably,\nthough the use of SSE significantly improved simulation time, it did not\ndeliver the theoretical maximum. There are a number of reasons for this:\narchitectural constraints imposed by the FSB speed, the spatial and temporal\npatterns of data retrieval, ratio of computational to non-computational\ninstructions, and the need to interleave miscellaneous instructions with\ncomputational instructions. We present a model for analyzing the SSE\nperformance, which could help factor in the bottlenecks or weaknesses in the\nimplementation, the computing architecture, and the mapping of software to the\ncomputing substrate while evaluating the improvement in efficiency. The model\nor framework would be useful in evaluating the use of other computational\nframeworks, and in predicting the benefits that can be derived from future\nhardware or architectural improvements.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 21:52:49 GMT"}], "update_date": "2013-09-04", "authors_parsed": [["Srinivasan", "Shyam", ""]]}, {"id": "1309.0569", "submitter": "Wanyang Dai", "authors": "Wanyang Dai", "title": "Product-form solutions for integrated services packet networks and cloud\n  computing systems", "comments": "26 pages, 3 figures, short conference version is reported at MICAI\n  2006", "journal-ref": "Mathematical Problems in Engineering, Vol. 2014 (Regular Issue),\n  Article ID 767651, 16 pages, 2014", "doi": null, "report-no": null, "categories": "math.PR cs.IT cs.NI cs.PF math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We iteratively derive the product-form solutions of stationary distributions\nof priority multiclass queueing networks with multi-sever stations. The\nnetworks are Markovian with exponential interarrival and service time\ndistributions. These solutions can be used to conduct performance analysis or\nas comparison criteria for approximation and simulation studies of large scale\nnetworks with multi-processor shared-memory switches and cloud computing\nsystems with parallel-server stations. Numerical comparisons with existing\nBrownian approximating model are provided to indicate the effectiveness of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 01:07:05 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Dai", "Wanyang", ""]]}, {"id": "1309.0718", "submitter": "Nicola Blefari Melazzi", "authors": "G. Bianchi, N. Blefari Melazzi, A. Caponi, A. Detti", "title": "A General, Tractable and Accurate Model for a Cascade of Caches", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance evaluation of caching systems is an old and widely investigated\nresearch topic. The research community is once again actively working on this\ntopic because the Internet is evolving towards new transfer modes, which\nenvisage to cache both contents and instructions within the network. In\nparticular, there is interest in characterizing multi-cache systems, in which\nrequests not satisfied by a cache are forwarded to other caches.\n  In this field, this paper contributes as follows. First, we devise a simple\nbut accurate approximate analysis for caches fed by general \"renewal\" traffic\npatterns. Second, we characterize and model the traffic statistics for the\noutput (miss) stream. Third, we show in the simple example case of tandem\ncaches how the resulting output stream model can be conveniently exploited to\nanalyze the performance of subsequent cache stages. The main novelty of our\nwork stems in the ability to handle traffic patterns beyond the traditional\nindependent reference model, thus permitting simple assessment of cascade of\ncaches as well as improved understanding of the phenomena involved in cache\nhierarchies.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 15:14:44 GMT"}], "update_date": "2013-09-04", "authors_parsed": [["Bianchi", "G.", ""], ["Melazzi", "N. Blefari", ""], ["Caponi", "A.", ""], ["Detti", "A.", ""]]}, {"id": "1309.1110", "submitter": "Longbo Huang", "authors": "Longbo Huang, Shaoquan Zhang, Minghua Chen, Xin Liu", "title": "When Backpressure Meets Predictive Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing popularity of learning and predicting human user\nbehavior in communication and computing systems, in this paper, we investigate\nthe fundamental benefit of predictive scheduling, i.e., predicting and\npre-serving arrivals, in controlled queueing systems. Based on a lookahead\nwindow prediction model, we first establish a novel equivalence between the\npredictive queueing system with a \\emph{fully-efficient} scheduling scheme and\nan equivalent queueing system without prediction. This connection allows us to\nanalytically demonstrate that predictive scheduling necessarily improves system\ndelay performance and can drive it to zero with increasing prediction power. We\nthen propose the \\textsf{Predictive Backpressure (PBP)} algorithm for achieving\noptimal utility performance in such predictive systems. \\textsf{PBP}\nefficiently incorporates prediction into stochastic system control and avoids\nthe great complication due to the exponential state space growth in the\nprediction window size. We show that \\textsf{PBP} can achieve a utility\nperformance that is within $O(\\epsilon)$ of the optimal, for any $\\epsilon>0$,\nwhile guaranteeing that the system delay distribution is a\n\\emph{shifted-to-the-left} version of that under the original Backpressure\nalgorithm. Hence, the average packet delay under \\textsf{PBP} is strictly\nbetter than that under Backpressure, and vanishes with increasing prediction\nwindow size. This implies that the resulting utility-delay tradeoff with\npredictive scheduling beats the known optimal $[O(\\epsilon),\nO(\\log(1/\\epsilon))]$ tradeoff for systems without prediction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 17:17:49 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Huang", "Longbo", ""], ["Zhang", "Shaoquan", ""], ["Chen", "Minghua", ""], ["Liu", "Xin", ""]]}, {"id": "1309.1365", "submitter": "Richard Combes", "authors": "Veeraruna Kavitha and Richard Combes", "title": "Mixed Polling with Rerouting and Applications", "comments": "to appear in Performance Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Queueing systems with a single server in which customers wait to be served at\na finite number of distinct locations (buffers/queues) are called discrete\npolling systems. Polling systems in which arrivals of users occur anywhere in a\ncontinuum are called continuous polling systems. Often one encounters a\ncombination of the two systems: the users can either arrive in a continuum or\nwait in a finite set (i.e. wait at a finite number of queues). We call these\nsystems mixed polling systems. Also, in some applications, customers are\nrerouted to a new location (for another service) after their service is\ncompleted. In this work, we study mixed polling systems with rerouting. We\nobtain their steady state performance by discretization using the known pseudo\nconservation laws of discrete polling systems. Their stationary expected\nworkload is obtained as a limit of the stationary expected workload of a\ndiscrete system. The main tools for our analysis are: a) the fixed point\nanalysis of infinite dimensional operators and; b) the convergence of Riemann\nsums to an integral.\n  We analyze two applications using our results on mixed polling systems and\ndiscuss the optimal system design. We consider a local area network, in which a\nmoving ferry facilitates communication (data transfer) using a wireless link.\nWe also consider a distributed waste collection system and derive the optimal\ncollection point. In both examples, the service requests can arrive anywhere in\na subset of the two dimensional plane. Namely, some users arrive in a\ncontinuous set while others wait for their service in a finite set. The only\npolling systems that can model these applications are mixed systems with\nrerouting as introduced in this manuscript.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 14:50:33 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Kavitha", "Veeraruna", ""], ["Combes", "Richard", ""]]}, {"id": "1309.1613", "submitter": "Alireza Pourranjbar", "authors": "Alireza Pourranjbar, Jane Hillston", "title": "An Aggregation Technique For Large-Scale PEPA Models With Non-Uniform\n  Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Performance analysis based on modelling consists of two major steps: model\nconstruction and model analysis. Formal modelling techniques significantly aid\nmodel construction but can exacerbate model analysis. In particular, here we\nconsider the analysis of large-scale systems which consist of one or more\nentities replicated many times to form large populations. The replication of\nentities in such models can cause their state spaces to grow exponentially to\nthe extent that their exact stochastic analysis becomes computationally\nexpensive or even infeasible.\n  In this paper, we propose a new approximate aggregation algorithm for a class\nof large-scale PEPA models. For a given model, the method quickly checks if it\nsatisfies a syntactic condition, indicating that the model may be solved\napproximately with high accuracy. If so, an aggregated CTMC is generated\ndirectly from the model description. This CTMC can be used for efficient\nderivation of an approximate marginal probability distribution over some of the\nmodel's populations. In the context of a large-scale client-server system, we\ndemonstrate the usefulness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 11:48:02 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Pourranjbar", "Alireza", ""], ["Hillston", "Jane", ""]]}, {"id": "1309.1714", "submitter": "Jalil Boukhobza", "authors": "Pierre Olivier (Lab-STICC), Jalil Boukhobza (Lab-STICC), Eric Senn\n  (Lab-STICC)", "title": "Flashmon V2: Monitoring Raw NAND Flash Memory I/O Requests on Embedded\n  Linux", "comments": "EWiLi, the Embedded Operating Systems Workshop, Toulouse : France\n  (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Flashmon version 2, a tool for monitoring embedded Linux\nNAND flash memory I/O requests. It is designed for embedded boards based\ndevices containing raw flash chips. Flashmon is a kernel module and stands for\n\"flash monitor\". It traces flash I/O by placing kernel probes at the NAND\ndriver level. It allows tracing at runtime the 3 main flash operations: page\nreads / writes and block erasures. Flashmon is (1) generic as it was\nsuccessfully tested on the three most widely used flash file systems that are\nJFFS2, UBIFS and YAFFS, and several NAND chip models. Moreover, it is (2) non\nintrusive, (3) has a controllable memory footprint, and (4) exhibits a low\noverhead (<6%) on the traced system. Finally, it is (5) simple to integrate and\nused as a standalone module or as a built-in function / module in existing\nkernel sources. Monitoring flash memory operations allows a better\nunderstanding of existing flash management systems by studying and analyzing\ntheir behavior. Moreover it is useful in development phase for prototyping and\nvalidating new solutions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 18:14:04 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Olivier", "Pierre", "", "Lab-STICC"], ["Boukhobza", "Jalil", "", "Lab-STICC"], ["Senn", "Eric", "", "Lab-STICC"]]}, {"id": "1309.1894", "submitter": "Azamat Mametjanov", "authors": "Azamat Mametjanov and Boyana Norris", "title": "Software Autotuning for Sustainable Performance Portability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Scientific software applications are increasingly developed by large\ninterdiscplinary teams operating on functional modules organized around a\ncommon software framework, which is capable of integrating new functional\ncapabilities without modifying the core of the framework. In such environment,\nsoftware correctness and modularity take precedence at the expense of code\nperformance, which is an important concern during execution on supercomputing\nfacilities, where the allocation of core-hours is a valuable resource. To\nalleviate the performance problems, we propose automated performance tuning\n(autotuning) of software to extract the maximum performance on a given hardware\nplatform and to enable performance portability across heterogeneous hardware\nplatforms. The resulting code remains generic without committing to a\nparticular software stack and yet is compile-time specializable for maximal\nsustained performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 18:49:59 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Mametjanov", "Azamat", ""], ["Norris", "Boyana", ""]]}, {"id": "1309.1983", "submitter": "Mark Mawson", "authors": "Mark Mawson, Alistair Revell", "title": "Memory transfer optimization for a lattice Boltzmann solver on Kepler\n  architecture nVidia GPUs", "comments": "12 pages", "journal-ref": null, "doi": "10.1016/j.cpc.2014.06.003", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lattice Boltzmann method (LBM) for solving fluid flow is naturally well\nsuited to an efficient implementation for massively parallel computing, due to\nthe prevalence of local operations in the algorithm. This paper presents and\nanalyses the performance of a 3D lattice Boltzmann solver, optimized for third\ngeneration nVidia GPU hardware, also known as `Kepler'. We provide a review of\nprevious optimisation strategies and analyse data read/write times for\ndifferent memory types. In LBM, the time propagation step (known as streaming),\ninvolves shifting data to adjacent locations and is central to parallel\nperformance; here we examine three approaches which make use of different\nhardware options. Two of which make use of `performance enhancing' features of\nthe GPU; shared memory and the new shuffle instruction found in Kepler based\nGPUs. These are compared to a standard transfer of data which relies instead on\noptimised storage to increase coalesced access. It is shown that the more\nsimple approach is most efficient; since the need for large numbers of\nregisters per thread in LBM limits the block size and thus the efficiency of\nthese special features is reduced. Detailed results are obtained for a D3Q19\nLBM solver, which is benchmarked on nVidia K5000M and K20C GPUs. In the latter\ncase the use of a read-only data cache is explored, and peak performance of\nover 1036 Million Lattice Updates Per Second (MLUPS) is achieved. The\nappearance of a periodic bottleneck in the solver performance is also reported,\nbelieved to be hardware related; spikes in iteration-time occur with a\nfrequency of around 11Hz for both GPUs, independent of the size of the problem.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2013 17:37:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Mawson", "Mark", ""], ["Revell", "Alistair", ""]]}, {"id": "1309.5686", "submitter": "Vineeth Bala Sukumaran", "authors": "Vineeth B. S. and Utpal Mukherji", "title": "On the tradeoff of average delay and average power for fading\n  point-to-point links with monotone policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fading point-to-point link with packets arriving randomly at\nrate $\\lambda$ per slot to the transmitter queue. We assume that the\ntransmitter can control the number of packets served in a slot by varying the\ntransmit power for the slot. We restrict to transmitter scheduling policies\nthat are monotone and stationary, i.e., the number of packets served is a\nnon-decreasing function of the queue length at the beginning of the slot for\nevery slot fade state. For such policies, we obtain asymptotic lower bounds for\nthe minimum average delay of the packets, when average transmitter power is a\nsmall positive quantity $V$ more than the minimum average power required for\ntransmitter queue stability. We show that the minimum average delay grows\neither to a finite value or as $\\Omega\\brap{\\log(1/V)}$ or $\\Omega\\brap{1/V}$\nwhen $V \\downarrow 0$, for certain sets of values of $\\lambda$. These sets are\ndetermined by the distribution of fading gain, the maximum number of packets\nwhich can be transmitted in a slot, and the transmit power function of the\nfading gain and the number of packets transmitted that is assumed. We identify\na case where the above behaviour of the tradeoff differs from that obtained\nfrom a previously considered approximate model, in which the random queue\nlength process is assumed to evolve on the non-negative real line, and the\ntransmit power function is strictly convex. We also consider a fading\npoint-to-point link, where the transmitter, in addition to controlling the\nnumber of packets served, can also control the number of packets admitted in\nevery slot. Our approach, which uses bounds on the stationary probability\ndistribution of the queue length, also leads to an intuitive explanation of the\nasymptotic behaviour of average delay in the regime where $V \\downarrow 0$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 02:48:00 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["S.", "Vineeth B.", ""], ["Mukherji", "Utpal", ""]]}, {"id": "1309.5802", "submitter": "Georges  Kaddoum GK", "authors": "Georges Kaddoum and Francois Gagnon", "title": "Lower Bound on the BER of a Decode-and-Forward Relay Network Under Chaos\n  Shift Keying Communication System", "comments": "This paper has been accepted for publication and will soon appear in\n  the IET Communications Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper carries out the first-ever investigation of the analysis of a\ncooperative Decode-and-Forward (DF) relay network with Chaos Shift Keying (CSK)\nmodulation. The performance analysis of DF-CSK in this paper takes into account\nthe dynamical nature of chaotic signal, which is not similar to a conventional\nbinary modulation performance computation methodology. The expression of a\nlower bound bit error rate (BER) is derived in order to investigate the\nperformance of the cooperative system under independently and identically\ndistributed (i.i.d.) Gaussian fading wireless environments. The effect of the\nnon-periodic nature of chaotic sequence leading to a non constant bit energy of\nthe considered modulation is also investigated. A computation approach of the\nBER expression based on the probability density function of the bit energy of\nthe chaotic sequence, channel distribution, and number of relays is presented.\nSimulation results prove the accuracy of our BER computation methodology.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 15:49:48 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2013 00:36:23 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Kaddoum", "Georges", ""], ["Gagnon", "Francois", ""]]}, {"id": "1309.7173", "submitter": "Priyanka Manchanda", "authors": "Priyanka Manchanda", "title": "Analysis of Optimization Techniques to Improve User Response Time of Web\n  Applications and Their Implementation for MOODLE", "comments": "The original final publication will be available at\n  http://www.springerlink.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of seven optimization techniques grouped under three categories\n(hardware, back-end, and front-end) is done to study the reduction in average\nuser response time for Modular Object Oriented Dynamic Learning Environment\n(Moodle), a Learning Management System which is scripted in PHP5, runs on\nApache web server and utilizes MySQL database software. Before the\nimplementation of these techniques, performance analysis of Moodle is performed\nfor varying number of concurrent users. The results obtained for each\noptimization technique are then reported in a tabular format. The maximum\nreduction in end user response time was achieved for hardware optimization\nwhich requires Moodle server and database to be installed on solid state disk.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 09:27:01 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2013 07:19:32 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Manchanda", "Priyanka", ""]]}]