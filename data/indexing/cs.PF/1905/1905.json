[{"id": "1905.00553", "submitter": "Renlord Yang", "authors": "Renlord Yang, Toby Murray, Paul Rimba, Udaya Parampalli", "title": "Empirically Analyzing Ethereum's Gas Mechanism", "comments": "Accepted at IEEE S&B 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethereum's Gas mechanism attempts to set transaction fees in accordance with\nthe computational cost of transaction execution: a cost borne by default by\nevery node on the network to ensure correct smart contract execution. Gas\nencourages users to author transactions that are efficient to execute and in so\ndoing encourages node diversity, allowing modestly resourced nodes to join and\ncontribute to the security of the network.\n  However, the effectiveness of this scheme relies on Gas costs being correctly\naligned with observed computational costs in reality. In this work, we\nperformed the first large scale empirical study to understand to what degree\nthis alignment exists in practice, by collecting and analyzing Tera-bytes worth\nof nanosecond-precision transaction execution traces. Besides confirming\npotential denial-of-service vectors, our results also shed light on the role of\nI/O in transaction costs which remains poorly captured by the current Gas cost\nmodel. Finally, our results suggest that under the current Gas cost model,\nnodes with modest computational resources are disadvantaged compared to their\nbetter resourced peers, which we identify as an ongoing threat to node\ndiversity and network decentralization.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 02:28:06 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Yang", "Renlord", ""], ["Murray", "Toby", ""], ["Rimba", "Paul", ""], ["Parampalli", "Udaya", ""]]}, {"id": "1905.00626", "submitter": "Eliza Wszola", "authors": "Eliza Wszola, Celestine Mendler-D\\\"unner, Martin Jaggi, Markus\n  P\\\"uschel", "title": "On Linear Learning with Manycore Processors", "comments": "Accepted to 2019 IEEE 26th International Conference on High\n  Performance Computing, Data, and Analytics (HiPC)", "journal-ref": null, "doi": "10.1109/HiPC.2019.00032", "report-no": null, "categories": "cs.PF cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new generation of manycore processors is on the rise that offers dozens and\nmore cores on a chip and, in a sense, fuses host processor and accelerator. In\nthis paper we target the efficient training of generalized linear models on\nthese machines. We propose a novel approach for achieving parallelism which we\ncall Heterogeneous Tasks on Homogeneous Cores (HTHC). It divides the problem\ninto multiple fundamentally different tasks, which themselves are parallelized.\nFor evaluation, we design a detailed, architecture-cognizant implementation of\nour scheme on a recent 72-core Knights Landing processor that is adaptive to\nthe cache, memory, and core structure. Our library efficiently supports dense\nand sparse datasets as well as 4-bit quantized data for further possible gains\nin performance. We show benchmarks for Lasso and SVM with different data sets\nagainst straightforward parallel implementations and prior software. In\nparticular, for Lasso on dense data, we improve the state-of-the-art by an\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 09:01:10 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 07:52:01 GMT"}, {"version": "v3", "created": "Fri, 26 Jul 2019 08:06:31 GMT"}, {"version": "v4", "created": "Thu, 14 Nov 2019 15:48:28 GMT"}, {"version": "v5", "created": "Wed, 22 Jan 2020 07:31:02 GMT"}, {"version": "v6", "created": "Wed, 5 Feb 2020 06:35:01 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Wszola", "Eliza", ""], ["Mendler-D\u00fcnner", "Celestine", ""], ["Jaggi", "Martin", ""], ["P\u00fcschel", "Markus", ""]]}, {"id": "1905.01135", "submitter": "Dominik Durner", "authors": "Dominik Durner, Viktor Leis, Thomas Neumann", "title": "On the Impact of Memory Allocation on High-Performance Query Processing", "comments": null, "journal-ref": "DaMoN 2019", "doi": "10.1145/3329785.3329918", "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Somewhat surprisingly, the behavior of analytical query engines is crucially\naffected by the dynamic memory allocator used. Memory allocators highly\ninfluence performance, scalability, memory efficiency and memory fairness to\nother processes. In this work, we provide the first comprehensive experimental\nanalysis on the impact of memory allocation for high-performance query engines.\nWe test five state-of-the-art dynamic memory allocators and discuss their\nstrengths and weaknesses within our DBMS. The right allocator can increase the\nperformance of TPC-DS (SF 100) by 2.7x on a 4-socket Intel Xeon server.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 12:10:02 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Durner", "Dominik", ""], ["Leis", "Viktor", ""], ["Neumann", "Thomas", ""]]}, {"id": "1905.01141", "submitter": "Fabrice Guillemin", "authors": "Veronica Quintuna Rodriguez and Fabrice Guillemin", "title": "Higher aggregation of gNodeBs in Cloud-RAN architectures via parallel\n  computing", "comments": "Appeared at ICIN'19, Paris, February 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the virtualization and the centralization of\nreal-time network functions, notably in the framework of Cloud RAN (C-RAN). We\nthoroughly analyze the required fronthaul capacity for the deployment of the\nproposed C-RAN architecture. We are specifically interested in the performance\nof the software based channel coding function. We develop a dynamic\nmulti-threading approach to achieve parallel computing on a multi-core\nplatform. Measurements from an OAI-based testbed show important gains in terms\nof latency; this enables the increase of the distance between the radio\nelements and the virtualized RAN functions and thus a higher aggregation of\ngNodeBs in edge data centers, referred to as Central Offices (COs).\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 13:08:01 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Rodriguez", "Veronica Quintuna", ""], ["Guillemin", "Fabrice", ""]]}, {"id": "1905.01294", "submitter": "Jeremy Kepner", "authors": "Pieter Cailliau, Tim Davis, Vijay Gadepally, Jeremy Kepner, Roi\n  Lipman, Jeffrey Lovitz, Keren Ouaknine", "title": "RedisGraph GraphBLAS Enabled Graph Database", "comments": "Accepted to IEEE IPDPS 2019 GrAPL workshop", "journal-ref": null, "doi": "10.1109/IPDPSW.2019.00054", "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RedisGraph is a Redis module developed by Redis Labs to add graph database\nfunctionality to the Redis database. RedisGraph represents connected data as\nadjacency matrices. By representing the data as sparse matrices and employing\nthe power of GraphBLAS (a highly optimized library for sparse matrix\noperations), RedisGraph delivers a fast and efficient way to store, manage and\nprocess graphs. Initial benchmarks indicate that RedisGraph is significantly\nfaster than comparable graph databases.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 23:39:42 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Cailliau", "Pieter", ""], ["Davis", "Tim", ""], ["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""], ["Lipman", "Roi", ""], ["Lovitz", "Jeffrey", ""], ["Ouaknine", "Keren", ""]]}, {"id": "1905.01472", "submitter": "Faissal El Bouanani", "authors": "Elmehdi Illi, Faissal El Bouanani, Ki-Hong Park, Fouad Ayoub,\n  Mohamed-Slim Alouini", "title": "An Improved Accurate Solver for the Time-Dependent RTE in Underwater\n  Optical Wireless Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an improved numerical solver to evaluate the time-dependent\nradiative transfer equation (RTE) for underwater optical wireless\ncommunications (UOWC) is investigated. The RTE evaluates the optical path-loss\nof light wave in an underwater channel in terms of the inherent optical\nproperties related to the environments, namely the absorption and scattering\ncoefficients as well as the phase scattering function (PSF). The proposed\nnumerical algorithm was improved based on the ones proposed in [1]-[4], by\nmodifying the finite difference scheme proposed in [1] as well as an\nenhancement of the quadrature method proposed in [2] by involving a more\naccurate 7-points quadrature scheme in order to calculate the quadrature weight\ncoefficients corresponding to the integral term of the RTE. Furthermore, the\nscattering angular discretization algorithm used in [3] and [4] was modified,\nbased on which the receiver's field of view discretization was adapted\ncorrespondingly. Interestingly, the RTE solver has been applied to three volume\nscattering functions, namely: the single-term HG phase function, the two-term\nHG phase function [5], and the Fournier-Forand phase function [6], over\nHarbor-I and Harbor-II water types. Based on the normalized received power\nevaluated through the proposed algorithm, the bit error rate performance of the\nUOWC system is investigated in terms of system and channel parameters. The\nenhanced algorithm gives a tightly close performance to its Monte Carlo\ncounterpart improved based on the simulations provided in [7], by adjusting the\nnumerical cumulative distribution function computation method as well as\noptimizing the number of scattering angles. Matlab codes for the proposed RTE\nsolver are presented in [8].\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 10:29:02 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Illi", "Elmehdi", ""], ["Bouanani", "Faissal El", ""], ["Park", "Ki-Hong", ""], ["Ayoub", "Fouad", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1905.01888", "submitter": "Leandro Indrusiak", "authors": "Leandro Soares Indrusiak, Robert I. Davis, Piotr Dziurzanski", "title": "Evolutionary Optimisation of Real-Time Systems and Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design space of networked embedded systems is very large, posing\nchallenges to the optimisation of such platforms when it comes to support\napplications with real-time guarantees. Recent research has shown that a number\nof inter-related optimisation problems have a critical influence over the\nschedulability of a system, i.e. whether all its application components can\nexecute and communicate by their respective deadlines. Examples of such\noptimization problems include task allocation and scheduling, communication\nrouting and arbitration, memory allocation, and voltage and frequency scaling.\nIn this paper, we advocate the use of evolutionary approaches to address such\noptimization problems, aiming to evolve individuals of increased fitness over\nmultiple generations of potential solutions. We refer to plentiful evidence\nthat existing real-time schedulability tests can be used effectively to guide\nevolutionary optimisation, either by themselves or in combination with other\nmetrics such as energy dissipation or hardware overheads. We then push that\nconcept one step further and consider the possibility of using evolutionary\ntechniques to evolve the schedulability tests themselves, aiming to support the\nverification and optimisation of systems which are too complex for\nstate-of-the-art (manual) derivation of schedulability tests.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 09:04:02 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 13:59:18 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 12:39:12 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Indrusiak", "Leandro Soares", ""], ["Davis", "Robert I.", ""], ["Dziurzanski", "Piotr", ""]]}, {"id": "1905.02334", "submitter": "Nick Feamster", "authors": "Nick Feamster, Jason Livingood", "title": "Internet Speed Measurement: Current Challenges and Future\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Government organizations, regulators, consumers, Internet service providers,\nand application providers alike all have an interest in measuring user Internet\n\"speed\". Access speeds have increased by an order of magnitude in past years,\nwith gigabit speeds available to tens of millions of homes. Approaches must\nevolve to accurately reflect the changing user experience and network speeds.\nThis paper offers historical and technical background on current speed testing\nmethods, highlights their limitations as access network speeds continue to\nincrease, and offers recommendations for the next generation of Internet\n\"speed\" measurement.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 02:43:56 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 15:25:08 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 20:22:49 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Feamster", "Nick", ""], ["Livingood", "Jason", ""]]}, {"id": "1905.02487", "submitter": "Zhangyu Chen", "authors": "Zhangyu Chen, Yu Hua, Pengfei Zuo, Yuanyuan Sun, Yuncheng Guo", "title": "Efficient Similarity-aware Compression to Reduce Bit-writes in\n  Non-Volatile Main Memory for Image-based Applications", "comments": "14 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image bitmaps have been widely used in in-memory applications, which consume\nlots of storage space and energy. Compared with legacy DRAM, non-volatile\nmemories (NVMs) are suitable for bitmap storage due to the salient features in\ncapacity and power savings. However, NVMs suffer from higher latency and energy\nconsumption in writes compared with reads. Although compressing data in write\naccesses to NVMs on-the-fly reduces the bit-writes in NVMs, existing precise or\napproximate compression schemes show limited performance improvements for data\nof bitmaps, due to the irregular data patterns and variance in data. We observe\nthat the data containing bitmaps show the pixel-level similarity due to the\nanalogous contents in adjacent pixels. By exploiting the pixel-level\nsimilarity, we propose SimCom, an efficient similarity-aware compression scheme\nin hardware layer, to compress data for each write access on-the-fly. The idea\nbehind SimCom is to compress continuous similar words into the pairs of base\nwords with runs. With the aid of domain knowledge of images, SimCom adaptively\nselects an appropriate compression mode to achieve an efficient trade-off\nbetween image quality and memory performance. We implement SimCom on GEM5 with\nNVMain and evaluate the performance with real-world workloads. Our results\ndemonstrate that SimCom reduces 33.0%, 34.8% write latency and saves 28.3%,\n29.0% energy than state-of-the-art FPC and BDI with minor quality loss of 3%.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 11:57:29 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Chen", "Zhangyu", ""], ["Hua", "Yu", ""], ["Zuo", "Pengfei", ""], ["Sun", "Yuanyuan", ""], ["Guo", "Yuncheng", ""]]}, {"id": "1905.03136", "submitter": "Georg Hager", "authors": "Dominik Ernst, Georg Hager, Jonas Thies, Gerhard Wellein", "title": "Performance Engineering for Real and Complex Tall & Skinny Matrix\n  Multiplication Kernels on GPUs", "comments": "12 pages, 22 figures. Extended version of arXiv:1905.03136v1 for\n  journal submission", "journal-ref": null, "doi": "10.1007/978-3-030-43229-4_43", "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General matrix-matrix multiplications with double-precision real and complex\nentries (DGEMM and ZGEMM) in vendor-supplied BLAS libraries are best optimized\nfor square matrices but often show bad performance for tall & skinny matrices,\nwhich are much taller than wide. NVIDIA's current CUBLAS implementation\ndelivers only a fraction of the potential performance as indicated by the\nroofline model in this case. We describe the challenges and key characteristics\nof an implementation that can achieve close to optimal performance. We further\nevaluate different strategies of parallelization and thread distribution, and\ndevise a flexible, configurable mapping scheme. To ensure flexibility and allow\nfor highly tailored implementations we use code generation combined with\nautotuning. For a large range of matrix sizes in the domain of interest we\nachieve at least 2/3 of the roofline performance and often substantially\noutperform state-of-the art CUBLAS results on an NVIDIA Volta GPGPU.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 15:11:46 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 15:54:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ernst", "Dominik", ""], ["Hager", "Georg", ""], ["Thies", "Jonas", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1905.03344", "submitter": "Danilo Guerrera", "authors": "Danilo Guerrera, Aur\\'elien Cavelan, Rub\\'en M. Cabez\\'on, David\n  Imbert, Jean-Guillaume Piccinali, Ali Mohammed, Lucio Mayer, Darren Reed,\n  Florina M. Ciorba", "title": "SPH-EXA: Enhancing the Scalability of SPH codes Via an Exascale-Ready\n  SPH Mini-App", "comments": "arXiv admin note: substantial text overlap with arXiv:1809.08013", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical simulations of fluids in astrophysics and computational fluid\ndynamics (CFD) are among the most computationally-demanding calculations, in\nterms of sustained floating-point operations per second, or FLOP/s. It is\nexpected that these numerical simulations will significantly benefit from the\nfuture Exascale computing infrastructures, that will perform 10^18 FLOP/s. The\nperformance of the SPH codes is, in general, adversely impacted by several\nfactors, such as multiple time-stepping, long-range interactions, and/or\nboundary conditions. In this work an extensive study of three SPH\nimplementations SPHYNX, ChaNGa, and XXX is performed, to gain insights and to\nexpose any limitations and characteristics of the codes. These codes are the\nstarting point of an interdisciplinary co-design project, SPH-EXA, for the\ndevelopment of an Exascale-ready SPH mini-app. We implemented a rotating square\npatch as a joint test simulation for the three SPH codes and analyzed their\nperformance on a modern HPC system, Piz Daint. The performance profiling and\nscalability analysis conducted on the three parent codes allowed to expose\ntheir performance issues, such as load imbalance, both in MPI and OpenMP.\nTwo-level load balancing has been successfully applied to SPHYNX to overcome\nits load imbalance. The performance analysis shapes and drives the design of\nthe SPH-EXA mini-app towards the use of efficient parallelization methods,\nfault-tolerance mechanisms, and load balancing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:44:13 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Guerrera", "Danilo", ""], ["Cavelan", "Aur\u00e9lien", ""], ["Cabez\u00f3n", "Rub\u00e9n M.", ""], ["Imbert", "David", ""], ["Piccinali", "Jean-Guillaume", ""], ["Mohammed", "Ali", ""], ["Mayer", "Lucio", ""], ["Reed", "Darren", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1905.03439", "submitter": "Isaac Grosof", "authors": "Isaac Grosof, Ziv Scully, Mor Harchol-Balter", "title": "Load Balancing Guardrails: Keeping Your Heavy Traffic on the Road to Low\n  Response Times", "comments": "31 pages. To appear in ACM SIGMETRICS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load balancing systems, comprising a central dispatcher and a scheduling\npolicy at each server, are widely used in practice, and their response time has\nbeen extensively studied in the theoretical literature. While much is known\nabout the scenario where the scheduling at the servers is\nFirst-Come-First-Served (FCFS), to minimize mean response time we must use\nShortest-Remaining-Processing-Time (SRPT) scheduling at the servers. Much less\nis known about dispatching polices when SRPT scheduling is used. Unfortunately,\ntraditional dispatching policies that are used in practice in systems with FCFS\nservers often have poor performance in systems with SRPT servers. In this\npaper, we devise a simple fix that can be applied to any dispatching policy.\nThis fix, called guardrails, ensures that the dispatching policy yields optimal\nmean response time under heavy traffic when used in a system with SRPT servers.\nAny dispatching policy, when augmented with guardrails, becomes heavy-traffic\noptimal. Our results yield the first analytical bounds on mean response time\nfor load balancing systems with SRPT scheduling at the servers.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 04:21:09 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Grosof", "Isaac", ""], ["Scully", "Ziv", ""], ["Harchol-Balter", "Mor", ""]]}, {"id": "1905.03518", "submitter": "Erik Sy", "authors": "Erik Sy, Tobias Mueller, Christian Burkert, Hannes Federrath, Mathias\n  Fischer", "title": "Enhanced Performance and Privacy for TLS over TCP Fast Open", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small TCP flows make up the majority of web flows. For them, the TCP\nthree-way handshake induces significant delay overhead. The TCP Fast Open (TFO)\nprotocol can significantly decrease this delay via zero round-trip time (0-RTT)\nhandshakes for all TCP handshakes that follow a full initial handshake to the\nsame host. However, this comes at the cost of privacy limitations and also has\nsome performance limitations. In this paper, we investigate the TFP deployment\non popular websites and browsers. We found that a client revisiting a web site\nfor the first time fails to use an abbreviated TFO handshake in 40% of all\ncases due to web server load-balancing using multiple IP addresses. Our\nanalysis further reveals significant privacy problems of the protocol design\nand implementation. Network-based attackers and online trackers can exploit TFO\nto track the online activities of users. As a countermeasure, we introduce a\nnovel protocol called TCP Fast Open Privacy (FOP). TCP FOP prevents tracking by\nnetwork attackers and impedes third-party tracking, while still allowing 0-RTT\nhandshakes as in TFO. As a proof-of-concept, we have implemented the proposed\nprotocol for the Linux kernel and a TLS library. Our measurements indicate that\nTCP FOP outperforms TLS over TFO when websites are served from multiple IP\naddresses.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 10:25:23 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 13:45:36 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Sy", "Erik", ""], ["Mueller", "Tobias", ""], ["Burkert", "Christian", ""], ["Federrath", "Hannes", ""], ["Fischer", "Mathias", ""]]}, {"id": "1905.03641", "submitter": "Andre Pacheco", "authors": "Andre G. C. Pacheco", "title": "Multiplica\\c{c}\\~ao de matrizes: uma compara\\c{c}\\~ao entre as\n  abordagens sequencial (CPU) e paralela (GPU)", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing problems using matrices is very important in Computer Science.\nFields like graph computer, graphs theory, and machine learning use matrices\nvery often to solve their own problems. The most often matrix operation is the\nmultiplication. It may be time-consuming if the matrices to be multiplied are\nlarge. For this reason, the parallel computer became a must to tackle this\nproblem. In this report, it is presented a comparison between sequential and\nparallel approaches to computing the matrix multiplication using CUDA and\nopenMP. The results show the importance of parallelizing mainly when the\nmatrices are large.\n  A modelagem de problemas utilizando matrizes \\'e de extrema import\\^ancia\npara Ci\\^encia da Computa\\c{c}\\~ao. \\'Areas como computa\\c{c}\\~ao gr\\'afica,\ngrafos e aprendizado de m\\'aquina utilizam matrizes com alta frequ\\^encia para\nsolucionar seus respectivos problemas. Dessa forma, operar matrizes de maneira\neficiente \\'e muito importante para o desempenho de algoritmos. Uma das\nopera\\c{c}\\~oes de matrizes mais utilizadas \\'e a multiplica\\c{c}\\~ao, que se\ntorna um empecilho para o desempenho computacional de algoritmos na medida que\no tamanho das matrizes a serem multiplicadas aumentam. Por conta disso, a\ncomputa\\c{c}\\~ao paralela se tornou uma solu\\c{c}\\~ao padr\\~ao para abordar tal\nproblema. Neste trabalho \\'e apresentado uma compara\\c{c}\\~ao entre as\nabordagens sequencial e paralela para multiplica\\c{c}\\~ao de matrizes\nutilizando CUDA e OpenMP. O resultado da an\\'alise realizada entre o tamanho da\nmatriz e o desempenho da multiplica\\c{c}\\~ao mostra a import\\^ancia da\nparaleliza\\c{c}\\~ao principalmente para matrizes de ordem elevada.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:05:10 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Pacheco", "Andre G. C.", ""]]}, {"id": "1905.03852", "submitter": "Jieru Zhao", "authors": "Jieru Zhao, Tingyuan Liang, Sharad Sinha and Wei Zhang", "title": "Machine Learning Based Routing Congestion Prediction in FPGA High-Level\n  Synthesis", "comments": "Preprint: to appear in Proceedings of Design, Automation and Test in\n  Europe (DATE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level synthesis (HLS) shortens the development time of hardware designs\nand enables faster design space exploration at a higher abstraction level.\nOptimization of complex applications in HLS is challenging due to the effects\nof implementation issues such as routing congestion. Routing congestion\nestimation is absent or inaccurate in existing HLS design methods and tools.\nEarly and accurate congestion estimation is of great benefit to guide the\noptimization in HLS and improve the efficiency of implementation. However,\nroutability, a serious concern in FPGA designs, has been difficult to evaluate\nin HLS without analyzing post-implementation details after Place and Route. To\nthis end, we propose a novel method to predict routing congestion in HLS using\nmachine learning and map the expected congested regions in the design to the\nrelevant high-level source code. This is greatly beneficial in early\nidentification of routability oriented bottlenecks in the high-level source\ncode without running time-consuming register-transfer level (RTL)\nimplementation flow. Experiments demonstrate that our approach accurately\nestimates vertical and horizontal routing congestion with errors of 6.71% and\n10.05% respectively. By presenting Face Detection application as a case study,\nwe show that by discovering the bottlenecks in high-level source code, routing\ncongestion can be easily and quickly resolved compared to the efforts involved\nin RTL implementation and design feedback.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 06:33:22 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Zhao", "Jieru", ""], ["Liang", "Tingyuan", ""], ["Sinha", "Sharad", ""], ["Zhang", "Wei", ""]]}, {"id": "1905.04068", "submitter": "Jaya Prakash Champati Dr", "authors": "Jaya Prakash Champati, Hussein Al-Zubaidy, James Gross", "title": "On the Distribution of AoI for the GI/GI/1/1 and GI/GI/1/2* Systems:\n  Exact Expressions and Bounds", "comments": "13 pages. This work appeared in IEEE INFOCOM, 2019. This manuscript\n  complements the INFOCOM version by including the missing proofs and some\n  additional explanations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Age of Information (AoI) has been proposed as a metric that quantifies\nthe freshness of information updates in a communication system, there has been\na constant effort in understanding and optimizing different statistics of the\nAoI process for classical queueing systems. In addition to classical queuing\nsystems, more recently, systems with no queue or a unit capacity queue storing\nthe latest packet have been gaining importance as storing and transmitting\nolder packets do not reduce AoI at the receiver. Following this line of\nresearch, we study the distribution of AoI for the GI/GI/1/1 and GI/GI/1/2*\nsystems, under non-preemptive scheduling. For any single-source-single-server\nqueueing system, we derive, using sample path analysis, a fundamental result\nthat characterizes the AoI violation probability, and use it to obtain\nclosed-form expressions for D/GI/1/1, M/GI/1/1 as well as systems that use\nzero-wait policy. Further, when exact results are not tractable, we present a\nsimple methodology for obtaining upper bounds for the violation probability for\nboth GI/GI/1/1 and GI/GI/1/2* systems. An interesting feature of the proposed\nupper bounds is that, if the departure rate is given, they overestimate the\nviolation probability by at most a value that decreases with the arrival rate.\nThus, given the departure rate and for a fixed average service, the bounds are\ntighter at higher utilization.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:00:18 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Champati", "Jaya Prakash", ""], ["Al-Zubaidy", "Hussein", ""], ["Gross", "James", ""]]}, {"id": "1905.04150", "submitter": "Pavlos Sermpezis", "authors": "Pavlos Sermpezis, Vasileios Kotronis", "title": "Inferring Catchment in Internet Routing", "comments": "ACM Sigmetrics 2019", "journal-ref": "Proceedings of the ACM on the Measurement and Analysis of\n  Computing Systems (POMACS), Vol. 3, No. 2, Article 30. Publication date: June\n  2019", "doi": "10.1145/3326145", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BGP is the de-facto Internet routing protocol for exchanging prefix\nreachability information between Autonomous Systems (AS). It is a dynamic,\ndistributed, path-vector protocol that enables rich expressions of network\npolicies (typically treated as secrets). In this regime, where complexity is\ninterwoven with information hiding, answering questions such as \"what is the\nexpected catchment of the anycast sites of a content provider on the AS-level,\nif new sites are deployed?\", or \"how will load-balancing behave if an ISP\nchanges its routing policy for a prefix?\", is a hard challenge. In this work,\nwe present a formal model and methodology that takes into account policy-based\nrouting and topological properties of the Internet graph, to predict the\nrouting behavior of networks. We design algorithms that provide new\ncapabilities for informative route inference (e.g., isolating the effect of\nrandomness that is present in prior simulation-based approaches). We analyze\nthe properties of these inference algorithms, and evaluate them using publicly\navailable routing datasets and real-world experiments. The proposed framework\ncan be useful in a number of applications: measurements, traffic engineering,\nnetwork planning, Internet routing models, etc. As a use case, we study the\nproblem of selecting a set of measurement vantage points to maximize route\ninference. Our methodology is general and can capture standard valley-free\nrouting, as well as more complex topological and routing setups appearing in\npractice.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:05:31 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Sermpezis", "Pavlos", ""], ["Kotronis", "Vasileios", ""]]}, {"id": "1905.04341", "submitter": "Philipp Grete", "authors": "Philipp Grete, Forrest W. Glines, Brian W. O'Shea", "title": "K-Athena: a performance portable structured grid finite volume\n  magnetohydrodynamics code", "comments": "13 pages, 6 figures, 2 tables; accepted for publication in IEEE\n  Transactions on Parallel and Distributed Systems (TPDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale simulations are a key pillar of modern research and require\never-increasing computational resources. Different novel manycore architectures\nhave emerged in recent years on the way towards the exascale era. Performance\nportability is required to prevent repeated non-trivial refactoring of a code\nfor different architectures. We combine Athena++, an existing\nmagnetohydrodynamics (MHD) CPU code, with Kokkos, a performance portable\non-node parallel programming paradigm, into K-Athena to allow efficient\nsimulations on multiple architectures using a single codebase. We present\nprofiling and scaling results for different platforms including Intel Skylake\nCPUs, Intel Xeon Phis, and NVIDIA GPUs. K-Athena achieves $>10^8$\ncell-updates/s on a single V100 GPU for second-order double precision MHD\ncalculations, and a speedup of 30 on up to 24,576 GPUs on Summit (compared to\n172,032 CPU cores), reaching $1.94\\times10^{12}$ total cell-updates/s at 76%\nparallel efficiency. Using a roofline analysis we demonstrate that the overall\nperformance is currently limited by DRAM bandwidth and calculate a performance\nportability metric of 62.8%. Finally, we present the implementation strategies\nused and the challenges encountered in maximizing performance. This will\nprovide other research groups with a straightforward approach to prepare their\nown codes for the exascale era. K-Athena is available at\nhttps://gitlab.com/pgrete/kathena .\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 18:47:25 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 18:06:16 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Grete", "Philipp", ""], ["Glines", "Forrest W.", ""], ["O'Shea", "Brian W.", ""]]}, {"id": "1905.05392", "submitter": "Long Gong", "authors": "Long Gong, Jun Xu, Liang Liu, Siva Theja Maguluri", "title": "QPS-r: A Cost-Effective Crossbar Scheduling Algorithm and Its Stability\n  and Delay Analysis", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an input-queued switch, a crossbar schedule, or a matching between the\ninput ports and the output ports needs to be computed in each switching cycle,\nor time slot. Designing switching algorithms with very low computational\ncomplexity, that lead to high throughput and small delay is a challenging\nproblem. There appears to be a fundamental tradeoff between the computational\ncomplexity of the switching algorithm and the resultants throughput and delay.\nParallel maximal matching algorithms (adapted for switching) appear to have\nstricken a sweet spot in this tradeoff, and prior work has shown the following\nperformance guarantees. Using maximal matchings in every time slot results in\nat least 50% switch throughput and order-optimal (i.e., independent of the\nswitch size N) average delay bounds for various traffic arrival processes. On\nthe other hand, their computational complexity can be as low as $O(log^2N)$ per\nport/processor, which is much lower than those of the algorithms such as\nmaximum weighted matching which ensures better throughput performance.\n  In this work, we propose QPS-r, a parallel iterative switching algorithm that\nhas the lowest possible computational complexity: O(1) per port. Using Lyapunov\nstability analysis, we show that the throughput and delay performance is\nidentical to that of maximal matching algorithm. Although QPS-r builds upon an\nexisting technique called Queue-Proportional Sampling (QPS), in this paper, we\nprovide analytical guarantees on its throughput and delay under i.i.d. traffic\nas well as a Markovian traffic model which can model many realistic traffic\npatterns. We also demonstrate that QPS-3 (running 3 iterations) has comparable\nempirical throughput and delay performances as iSLIP (running $log_2 N$\niterations), a refined and optimized representative maximal matching algorithm\nadapted for switching.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 04:45:10 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 18:24:49 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 06:35:34 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Gong", "Long", ""], ["Xu", "Jun", ""], ["Liu", "Liang", ""], ["Maguluri", "Siva Theja", ""]]}, {"id": "1905.05574", "submitter": "Albin Severinson", "authors": "Albin Severinson, Eirik Rosnes and Alexandre Graell i Amat", "title": "Coded Distributed Tracking", "comments": "Accepted for publication at IEEE GLOBECOM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.PF cs.RO cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of tracking the state of a process that evolves over\ntime in a distributed setting, with multiple observers each observing parts of\nthe state, which is a fundamental information processing problem with a wide\nrange of applications. We propose a cloud-assisted scheme where the tracking is\nperformed over the cloud. In particular, to provide timely and accurate\nupdates, and alleviate the straggler problem of cloud computing, we propose a\ncoded distributed computing approach where coded observations are distributed\nover multiple workers. The proposed scheme is based on a coded version of the\nKalman filter that operates on data encoded with an erasure correcting code,\nsuch that the state can be estimated from partial updates computed by a subset\nof the workers. We apply the proposed scheme to the problem of tracking\nmultiple vehicles. We show that replication achieves significantly higher\naccuracy than the corresponding uncoded scheme. The use of maximum distance\nseparable (MDS) codes further improves accuracy for larger update intervals. In\nboth cases, the proposed scheme approaches the accuracy of an ideal centralized\nscheme when the update interval is large enough. Finally, we observe a\ntrade-off between age-of-information and estimation accuracy for MDS codes.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 13:02:44 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 14:16:02 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Severinson", "Albin", ""], ["Rosnes", "Eirik", ""], ["Amat", "Alexandre Graell i", ""]]}, {"id": "1905.05595", "submitter": "Hussien Al-Hmood Dr", "authors": "Hussien Al-Hmood and H. S. Al-Raweshidy", "title": "Selection Combining Scheme over Non-identically Distributed\n  Fisher-Snedecor $\\mathcal{F}$ Fading Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the performance of the selection combining (SC) scheme over\nFisher-Snedecor $\\mathcal{F}$ fading channels with independent and\nnon-identically distributed (i.n.i.d.) branches is analysed. The probability\ndensity function (PDF) and the moment generating function (MGF) of the maximum\ni.n.i.d. Fisher-Snedecor $\\mathcal{F}$ variates are derived first in terms of\nthe multivariate Fox's $H$-function that has been efficiently implemented in\nthe technical literature by various software codes. Based on this, the average\nbit error probability (ABEP) and the average channel capacity (ACC) of SC\ndiversity with i.n.i.d. receivers are investigated. Moreover, we analyse the\nperformance of the energy detection that is widely employed to perform the\nspectrum sensing in cognitive radio networks via deriving the average detection\nprobability (ADP) and the average area under the receiver operating\ncharacteristics curve (AUC). To validate our analysis, the numerical results\nare affirmed by the Monte Carlo simulations.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 18:42:42 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Al-Hmood", "Hussien", ""], ["Al-Raweshidy", "H. S.", ""]]}, {"id": "1905.05822", "submitter": "Yichen Li", "authors": "Yichen Li, Dobroslav Tsonev and Harald Haas", "title": "Performance Analysis of Non-DC-Biased OFDM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance analysis of a novel optical modulation scheme is presented in\nthis paper. The basic concept is to transmit signs of modulated optical\northogonal frequency division multiplexing (O-OFDM) symbols and absolute values\nof the symbols separately by two information carrying units: 1) indices of two\nlight emitting diodes (LED) transmitters that represent positive and negative\nsigns separately; and 2) optical intensity symbols that carry the absolute\nvalues of signals. The new approach, referred as to non-DC-biased OFDM\n(NDC-OFDM), uses the optical spatial modulation (OSM) technique to eliminate\nthe effect of the clipping distortion in DC-biased optical OFDM (DCO-OFDM). In\naddition, it can achieve similar advantages as the conventional unipolar\nmodulation scheme, asymmetrically clipped optical OFDM (ACO-OFDM), without\nusing additional subcarriers. In this paper, the analytical BER performance is\ncompared with the Monte Carlo result in order to prove the reliability of the\nnew method. Moreover, the practical BER performance of NDC-OFDM with DCO-OFDM\nand ACO-OFDM is compared for different constellation sizes to verify the\nimprovement of NDC-OFDM on the spectral and power efficiencies.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:15:24 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Li", "Yichen", ""], ["Tsonev", "Dobroslav", ""], ["Haas", "Harald", ""]]}, {"id": "1905.06228", "submitter": "Andreas Thoma", "authors": "Andreas Thoma and Sridhar Ravi", "title": "Significance of parallel computing on the performance of Digital Image\n  Correlation algorithms in MATLAB", "comments": "17 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Image Correlation (DIC) is a powerful tool used to evaluate\ndisplacements and deformations in a non-intrusive manner. By comparing two\nimages, one of the undeformed reference state of a specimen and another of the\ndeformed target state, the relative displacement between those two states is\ndetermined. DIC is well known and often used for post-processing analysis of\nin-plane displacements and deformation of specimen. Increasing the analysis\nspeed to enable real-time DIC analysis will be beneficial and extend the field\nof use of this technique. Here we tested several combinations of the most\ncommon DIC methods in combination with different parallelization approaches in\nMATLAB and evaluated their performance to determine whether real-time analysis\nis possible with these methods. To reflect improvements in computing technology\ndifferent hardware settings were also analysed. We found that implementation\nproblems can reduce the efficiency of a theoretically superior algorithm such\nthat it becomes practically slower than a sub-optimal algorithm. The\nNewton-Raphson algorithm in combination with a modified Particle Swarm\nalgorithm in parallel image computation was found to be most effective. This is\ncontrary to theory, suggesting that the inverse-compositional Gauss-Newton\nalgorithm is superior. As expected, the Brute Force Search algorithm is the\nleast effective method. We also found that the correct choice of\nparallelization tasks is crucial to achieve improvements in computing speed. A\npoorly chosen parallelisation approach with high parallel overhead leads to\ninferior performance. Finally, irrespective of the computing mode the correct\nchoice of combinations of integer-pixel and sub-pixel search algorithms is\ndecisive for an efficient analysis. Using currently available hardware\nreal-time analysis at high framerates remains an aspiration.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 15:05:15 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Thoma", "Andreas", ""], ["Ravi", "Sridhar", ""]]}, {"id": "1905.06234", "submitter": "Karan Aggarwal", "authors": "Karan Aggarwal, Uday Bondhugula", "title": "Optimizing the Linear Fascicle Evaluation Algorithm for Multi-Core and\n  Many-Core Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-vector multiplication (SpMV) operations are commonly used in\nvarious scientific applications. The performance of the SpMV operation often\ndepends on exploiting regularity patterns in the matrix. Various\nrepresentations have been proposed to minimize the memory bandwidth bottleneck\narising from the irregular memory access pattern involved. Among recent\nrepresentation techniques, tensor decomposition is a popular one used for very\nlarge but sparse matrices. Post sparse-tensor decomposition, the new\nrepresentation involves indirect accesses, making it challenging to optimize\nfor multi-cores and GPUs.\n  Computational neuroscience algorithms often involve sparse datasets while\nstill performing long-running computations on them. The LiFE application is a\npopular neuroscience algorithm used for pruning brain connectivity graphs. The\ndatasets employed herein involve the Sparse Tucker Decomposition (STD), a\nwidely used tensor decomposition method. Using this decomposition leads to\nirregular array references, making it very difficult to optimize for both CPUs\nand GPUs. Recent codes of the LiFE algorithm show that its SpMV operations are\nthe key bottleneck for performance and scaling. In this work, we first propose\ntarget-independent optimizations to optimize these SpMV operations, followed by\ntarget-dependent optimizations for CPU and GPU systems. The target-independent\ntechniques include: (1) standard compiler optimizations, (2) data restructuring\nmethods, and (3) methods to partition computations among threads. Then we\npresent the optimizations for CPUs and GPUs to exploit platform-specific speed.\nOur highly optimized CPU code obtain a speedup of 27.12x over the original\nsequential CPU code running on 16-core Intel Xeon (Skylake-based) system, and\nour optimized GPU code achieves a speedup of 5.2x over a reference optimized\nGPU code version on NVIDIA's GeForce RTX 2080 Ti GPU.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 11:09:11 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 06:29:42 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Aggarwal", "Karan", ""], ["Bondhugula", "Uday", ""]]}, {"id": "1905.06302", "submitter": "Yichen Li", "authors": "Yichen Li, Majid Safari, Robert Henderson and Harald Haas", "title": "Performance Analysis of SPAD-based OFDM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an analytical approach for the nonlinear distorted bit error\nrate performance of optical orthogonal frequency division multiplexing (O-OFDM)\nwith single photon avalanche diode (SPAD) receivers is presented. Major\ndistortion effects of passive quenching (PQ) and active quenching (AQ) SPAD\nreceivers are analysed in this study. The performance analysis of DC-biased\nO-OFDM and asymmetrically clipped O-OFDM with PQ and AQ SPAD are derived. The\ncomparison results show the maximum optical irradiance caused by the nonlinear\ndistortion, which limits the transmission power and bit rate. The theoretical\nmaximum bit rate of SPAD-based OFDM is found which is up to 1~Gbits/s. This\napproach supplies a closed-form analytical solution for designing an optimal\nSPAD-based system.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 17:23:54 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Li", "Yichen", ""], ["Safari", "Majid", ""], ["Henderson", "Robert", ""], ["Haas", "Harald", ""]]}, {"id": "1905.06520", "submitter": "Dumitrel Loghin", "authors": "Dumitrel Loghin, Gang Chen, Tien Tuan Anh Dinh, Beng Chin Ooi, Yong\n  Meng Teo", "title": "Blockchain Goes Green? An Analysis of Blockchain on Low-Power Nodes", "comments": "17 pages, 13 pages paper, 4 pages appendix, 20 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.ET cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the massive energy usage of blockchain, on the one hand, and by\nsignificant performance improvements in low-power, wimpy systems, on the other\nhand, we perform an in-depth time-energy analysis of blockchain systems on\nlow-power nodes in comparison to high-performance nodes. We use three low-power\nsystems to represent a wide range of the performance-power spectrum, while\ncovering both x86/64 and ARM architectures. We show that low-end wimpy nodes\nare struggling to run full-fledged blockchains mainly due to their small and\nlow-bandwidth memory. On the other hand, wimpy systems with balanced\nperformance-to-power ratio achieve reasonable performance while saving\nsignificant amounts of energy. For example, Jetson TX2 nodes achieve around 80%\nand 30% of the throughput of Parity and Hyperledger, respectively, while using\n18x and 23x less energy compared to traditional brawny servers with Intel Xeon\nCPU.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 04:21:04 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 06:21:18 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Loghin", "Dumitrel", ""], ["Chen", "Gang", ""], ["Dinh", "Tien Tuan Anh", ""], ["Ooi", "Beng Chin", ""], ["Teo", "Yong Meng", ""]]}, {"id": "1905.07346", "submitter": "Stylianos Venieris", "authors": "Mario Almeida, Stefanos Laskaridis, Ilias Leontiadis, Stylianos I.\n  Venieris, Nicholas D. Lane", "title": "EmBench: Quantifying Performance Variations of Deep Neural Networks\n  across Modern Commodity Devices", "comments": "Accepted at MobiSys 2019: 3rd International Workshop on Embedded and\n  Mobile Deep Learning (EMDL), 2019", "journal-ref": null, "doi": "10.1145/3325413.3329793", "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, advances in deep learning have resulted in unprecedented\nleaps in diverse tasks spanning from speech and object recognition to context\nawareness and health monitoring. As a result, an increasing number of\nAI-enabled applications are being developed targeting ubiquitous and mobile\ndevices. While deep neural networks (DNNs) are getting bigger and more complex,\nthey also impose a heavy computational and energy burden on the host devices,\nwhich has led to the integration of various specialized processors in commodity\ndevices. Given the broad range of competing DNN architectures and the\nheterogeneity of the target hardware, there is an emerging need to understand\nthe compatibility between DNN-platform pairs and the expected performance\nbenefits on each platform. This work attempts to demystify this landscape by\nsystematically evaluating a collection of state-of-the-art DNNs on a wide\nvariety of commodity devices. In this respect, we identify potential\nbottlenecks in each architecture and provide important guidelines that can\nassist the community in the co-design of more efficient DNNs and accelerators.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 16:03:29 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Almeida", "Mario", ""], ["Laskaridis", "Stefanos", ""], ["Leontiadis", "Ilias", ""], ["Venieris", "Stylianos I.", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "1905.07641", "submitter": "George Kesidis", "authors": "George Kesidis, Nader Alfares, Xi Li, Bhuvan Urgaonkar, Mahmut\n  Kandemir, Takis Konstantopoulos", "title": "On a caching system with object sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a content-caching system thatis shared by a number of proxies.\nThe cache could belocated in an edge-cloud datacenter and the proxies couldeach\nserve a large population of mobile end-users. Eachproxy operates its own\nLRU-list of a certain capacity inthe shared cache. The length of objects\nsimultaneouslyappearing in plural LRU-lists is equally divided amongthem,i.e.,\nobject sharing among the LRUs. We provide a \"working-set\" approximation for\nthis system to quicklyestimate the cache-hit probabilities under such\nobjectsharing, which can be used to facilitate admission control.Also, a way to\nreduce ripple evictions,i.e.,setrequestoverhead, is suggested. We give\nnumerical results for ourMemCacheD with Object Sharing (MCD-OS) prototype.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 20:16:02 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 15:39:14 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Kesidis", "George", ""], ["Alfares", "Nader", ""], ["Li", "Xi", ""], ["Urgaonkar", "Bhuvan", ""], ["Kandemir", "Mahmut", ""], ["Konstantopoulos", "Takis", ""]]}, {"id": "1905.08073", "submitter": "Ali Mohammed", "authors": "Ali Mohammed, Aurelien Cavelan, and Florina M. Ciorba", "title": "rDLB: A Novel Approach for Robust Dynamic Load Balancing of Scientific\n  Applications with Parallel Independent Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications often contain large and computationally intensive\nparallel loops. Dynamic loop self scheduling (DLS) is used to achieve a\nbalanced load execution of such applications on high performance computing\n(HPC) systems. Large HPC systems are vulnerable to processors or node failures\nand perturbations in the availability of resources. Most self-scheduling\napproaches do not consider fault-tolerant scheduling or depend on failure or\nperturbation detection and react by rescheduling failed tasks. In this work, a\nrobust dynamic load balancing (rDLB) approach is proposed for the robust self\nscheduling of independent tasks. The proposed approach is proactive and does\nnot depend on failure or perturbation detection. The theoretical analysis of\nthe proposed approach shows that it is linearly scalable and its cost decrease\nquadratically by increasing the system size. rDLB is integrated into an MPI DLS\nlibrary to evaluate its performance experimentally with two computationally\nintensive scientific applications. Results show that rDLB enables the tolerance\nof up to (P minus one) processor failures, where P is the number of processors\nexecuting an application. In the presence of perturbations, rDLB boosted the\nrobustness of DLS techniques up to 30 times and decreased application execution\ntime up to 7 times compared to their counterparts without rDLB.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 13:01:09 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 10:37:12 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 15:30:44 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Mohammed", "Ali", ""], ["Cavelan", "Aurelien", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1905.08217", "submitter": "Iosif Meyerov", "authors": "Iosif Meyerov, Sergei Bastrakov, Aleksei Bashinov, Evgeny Efimenko,\n  Alexander Panov, Elena Panova, Igor Surmin, Valentin Volokitin, and Arkady\n  Gonoskov", "title": "Exploiting Parallelism on Shared Memory in the QED Particle-in-Cell Code\n  PICADOR with Greedy Load Balancing", "comments": "11 pages, 5 figures. Submitted to PPAM-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art numerical simulations of laser plasma by means of the\nParticle-in-Cell method are often extremely computationally intensive.\nTherefore there is a growing need for development of approaches for efficient\nutilization of resources of modern supercomputers. In this paper, we address\nthe problem of a substantially non-uniform and dynamically varying distribution\nof macroparticles in a computational area in simulating quantum electrodynamic\n(QED) cascades. We propose and evaluate a load balancing scheme for shared\nmemory systems, which allows subdividing individual cells of the computational\ndomain into work portions with subsequent dynamic distribution of these\nportions between OpenMP threads. Computational experiments on 1D, 2D, and 3D\nQED simulations show that the proposed scheme outperforms the previously\ndeveloped standard and custom schemes in the PICADOR code by 2.1 to 10 times\nwhen employing several Intel Cascade Lake CPUs.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 17:08:25 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Meyerov", "Iosif", ""], ["Bastrakov", "Sergei", ""], ["Bashinov", "Aleksei", ""], ["Efimenko", "Evgeny", ""], ["Panov", "Alexander", ""], ["Panova", "Elena", ""], ["Surmin", "Igor", ""], ["Volokitin", "Valentin", ""], ["Gonoskov", "Arkady", ""]]}, {"id": "1905.08386", "submitter": "Angel Beltre", "authors": "Pankaj Saha, Angel Beltre, Madhusudhan Govindaraju", "title": "Scylla: A Mesos Framework for Container Based MPI Jobs", "comments": null, "journal-ref": "MTAGS 2017: 10th Workshop on Many-Task Computing on Clouds, Grids,\n  and Supercomputers", "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open source cloud technologies provide a wide range of support for creating\ncustomized compute node clusters to schedule tasks and managing resources. In\ncloud infrastructures such as Jetstream and Chameleon, which are used for\nscientific research, users receive complete control of the Virtual Machines\n(VM) that are allocated to them. Importantly, users get root access to the VMs.\nThis provides an opportunity for HPC users to experiment with new resource\nmanagement technologies such as Apache Mesos that have proven scalability,\nflexibility, and fault tolerance. To ease the development and deployment of HPC\ntools on the cloud, the containerization technology has matured and is gaining\ninterest in the scientific community. In particular, several well known\nscientific code bases now have publicly available Docker containers. While\nMesos provides support for Docker containers to execute individually, it does\nnot provide support for container inter-communication or orchestration of the\ncontainers for a parallel or distributed application. In this paper, we present\nthe design, implementation, and performance analysis of a Mesos framework,\nScylla, which integrates Mesos with Docker Swarm to enable orchestration of MPI\njobs on a cluster of VMs acquired from the Chameleon cloud [1]. Scylla uses\nDocker Swarm for communication between containerized tasks (MPI processes) and\nApache Mesos for resource pooling and allocation. Scylla allows a policy-driven\napproach to determine how the containers should be distributed across the nodes\ndepending on the CPU, memory, and network throughput requirement for each\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 23:59:52 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Saha", "Pankaj", ""], ["Beltre", "Angel", ""], ["Govindaraju", "Madhusudhan", ""]]}, {"id": "1905.08387", "submitter": "Angel Beltre", "authors": "Pankaj Saha, Angel Beltre, Madhusudhan Govindaraju", "title": "Tromino: Demand and DRF Aware Multi-Tenant Queue Manager for Apache\n  Mesos Cluster", "comments": null, "journal-ref": "2018 IEEE/ACM 11th International Conference on Utility and Cloud\n  Computing (UCC) 63-72", "doi": "10.1109/UCC.2018.00015", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Mesos, a two-level resource scheduler, provides resource sharing\nacross multiple users in a multi-tenant cluster environment. Computational\nresources (i.e., CPU, memory, disk, etc. ) are distributed according to the\nDominant Resource Fairness (DRF) policy. Mesos frameworks (users) receive\nresources based on their current usage and are responsible for scheduling their\ntasks within the allocation. We have observed that multiple frameworks can\ncause fairness imbalance in a multiuser environment. For example, a greedy\nframework consuming more than its fair share of resources can deny resource\nfairness to others. The user with the least Dominant Share is considered first\nby the DRF module to get its resource allocation. However, the default DRF\nimplementation, in Apache Mesos' Master allocation module, does not consider\nthe overall resource demands of the tasks in the queue for each user/framework.\nThis lack of awareness can result in users without any pending task receiving\nmore resource offers while users with a queue of pending tasks starve due to\ntheir high dominant shares. We have developed a policy-driven queue manager,\nTromino, for an Apache Mesos cluster where tasks for individual frameworks can\nbe scheduled based on each framework's overall resource demands and current\nresource consumption. Dominant Share and demand awareness of Tromino and\nscheduling based on these attributes can reduce (1) the impact of unfairness\ndue to a framework specific configuration, and (2) unfair waiting time due to\nhigher resource demand in a pending task queue. In the best case, Tromino can\nsignificantly reduce the average waiting time of a framework by using the\nproposed Demand-DRF aware policy.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 00:00:04 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Saha", "Pankaj", ""], ["Beltre", "Angel", ""], ["Govindaraju", "Madhusudhan", ""]]}, {"id": "1905.08388", "submitter": "Angel Beltre", "authors": "Pankaj Saha, Angel Beltre, Madhusudhan Govindaraju", "title": "Exploring the Fairness and Resource Distribution in an Apache Mesos\n  Environment", "comments": null, "journal-ref": "2018 IEEE 11th International Conference on Cloud Computing (CLOUD)", "doi": "10.1109/CLOUD.2018.00061", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Mesos, a cluster-wide resource manager, is widely deployed in massive\nscale at several Clouds and Data Centers. Mesos aims to provide high cluster\nutilization via fine grained resource co-scheduling and resource fairness among\nmultiple users through Dominant Resource Fairness (DRF) based allocation. DRF\ntakes into account different resource types (CPU, Memory, Disk I/O) requested\nby each application and determines the share of each cluster resource that\ncould be allocated to the applications. Mesos has adopted a two-level\nscheduling policy: (1) DRF to allocate resources to competing frameworks and\n(2) task level scheduling by each framework for the resources allocated during\nthe previous step. We have conducted experiments in a local Mesos cluster when\nused with frameworks such as Apache Aurora, Marathon, and our own framework\nScylla, to study resource fairness and cluster utilization. Experimental\nresults show how informed decision regarding second level scheduling policy of\nframeworks and attributes like offer holding period, offer refusal cycle and\ntask arrival rate can reduce unfair resource distribution. Bin-Packing\nscheduling policy on Scylla with Marathon can reduce unfair allocation from\n38\\% to 3\\%. By reducing unused free resources in offers we bring down the\nunfairness from to 90\\% to 28\\%. We also show the effect of task arrival rate\nto reduce the unfairness from 23\\% to 7\\%.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 00:00:07 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Saha", "Pankaj", ""], ["Beltre", "Angel", ""], ["Govindaraju", "Madhusudhan", ""]]}, {"id": "1905.08415", "submitter": "Angel Beltre", "authors": "Pankaj Saha, Angel Beltre, Piotr Uminski, Madhusudhan Govindaraju", "title": "Evaluation of Docker Containers for Scientific Workloads in the Cloud", "comments": null, "journal-ref": "PEARC 2018 Proceedings of the Practice and Experience on Advanced\n  Research Computing", "doi": "10.1145/3219104.3229280", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HPC community is actively researching and evaluating tools to support\nexecution of scientific applications in cloud-based environments. Among the\nvarious technologies, containers have recently gained importance as they have\nsignificantly better performance compared to full-scale virtualization, support\nfor microservices and DevOps, and work seamlessly with workflow and\norchestration tools. Docker is currently the leader in containerization\ntechnology because it offers low overhead, flexibility, portability of\napplications, and reproducibility. Singularity is another container solution\nthat is of interest as it is designed specifically for scientific applications.\nIt is important to conduct performance and feature analysis of the container\ntechnologies to understand their applicability for each application and target\nexecution environment. This paper presents a (1) performance evaluation of\nDocker and Singularity on bare metal nodes in the Chameleon cloud (2) mechanism\nby which Docker containers can be mapped with InfiniBand hardware with RDMA\ncommunication and (3) analysis of mapping elements of parallel workloads to the\ncontainers for optimal resource management with container-ready orchestration\ntools. Our experiments are targeted toward application developers so that they\ncan make informed decisions on choosing the container technologies and\napproaches that are suitable for their HPC workloads on cloud infrastructure.\nOur performance analysis shows that scientific workloads for both Docker and\nSingularity based containers can achieve near-native performance. Singularity\nis designed specifically for HPC workloads. However, Docker still has\nadvantages over Singularity for use in clouds as it provides overlay networking\nand an intuitive way to run MPI applications with one container per rank for\nfine-grained resources allocation.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 03:03:37 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Saha", "Pankaj", ""], ["Beltre", "Angel", ""], ["Uminski", "Piotr", ""], ["Govindaraju", "Madhusudhan", ""]]}, {"id": "1905.08764", "submitter": "Yihui Ren", "authors": "Yihui Ren, Shinjae Yoo and Adolfy Hoisie", "title": "Performance Analysis of Deep Learning Workloads on Leading-edge Systems", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the performance of leading-edge systems designed for\nmachine learning computing, including the NVIDIA DGX-2, Amazon Web Services\n(AWS) P3, IBM Power System Accelerated Compute Server AC922, and a\nconsumer-grade Exxact TensorEX TS4 GPU server. Representative deep learning\nworkloads from the fields of computer vision and natural language processing\nare the focus of the analysis. Performance analysis is performed along with a\nnumber of important dimensions. Performance of the communication interconnects\nand large and high-throughput deep learning models are considered. Different\npotential use models for the systems as standalone and in the cloud also are\nexamined. The effect of various optimization of the deep learning models and\nsystem configurations is included in the analysis.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 17:33:19 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 20:59:22 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Ren", "Yihui", ""], ["Yoo", "Shinjae", ""], ["Hoisie", "Adolfy", ""]]}, {"id": "1905.08778", "submitter": "Yehia Arafa", "authors": "Yehia Arafa, Abdel-Hameed Badawy, Gopinath Chennupati, Nandakishore\n  Santhi, and Stephan Eidenbenz", "title": "Low Overhead Instruction Latency Characterization for NVIDIA GPGPUs", "comments": "Several typos in addition to paper tittle are updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen a shift in the computer systems industry where\nheterogeneous computing has become prevalent. Graphics Processing Units (GPUs)\nare now present in supercomputers to mobile phones and tablets. GPUs are used\nfor graphics operations as well as general-purpose computing (GPGPUs) to boost\nthe performance of compute-intensive applications. However, the percentage of\nundisclosed characteristics beyond what vendors provide is not small. In this\npaper, we introduce a very low overhead and portable analysis for exposing the\nlatency of each instruction executing in the GPU pipeline(s) and the access\noverhead of the various memory hierarchies found in GPUs at the\nmicro-architecture level. Furthermore, we show the impact of the various\noptimizations the CUDA compiler can perform over the various latencies. We\nperform our evaluation on seven different high-end NVIDIA GPUs from five\ndifferent generations/architectures: Kepler, Maxwell, Pascal, Volta, and\nTuring. The results in this paper can help architects to have an accurate\ncharacterization of the latencies of these GPUs, which will help in modeling\nthe hardware accurately. Also, software developers can perform informed\noptimizations to their applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 17:48:35 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 23:55:02 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Arafa", "Yehia", ""], ["Badawy", "Abdel-Hameed", ""], ["Chennupati", "Gopinath", ""], ["Santhi", "Nandakishore", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "1905.09063", "submitter": "Pravin Chandran", "authors": "Raghavendra Bhat, Pravin Chandran, Juby Jose, Viswanath Dibbur,\n  Prakash Sirra Ajith", "title": "NTP : A Neural Network Topology Profiler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of end-to-end neural networks on a given hardware platform is a\nfunction of its compute and memory signature, which in-turn, is governed by a\nwide range of parameters such as topology size, primitives used, framework\nused, batching strategy, latency requirements, precision etc. Current\nbenchmarking tools suffer from limitations such as a) being either too granular\nlike DeepBench [1] (or) b) mandate a working implementation that is either\nframework specific or hardware-architecture specific or both (or) c) provide\nonly high level benchmark metrics. In this paper, we present NTP (Neural Net\nTopology Profiler), a sophisticated benchmarking framework, to effectively\nidentify memory and compute signature of an end-to-end topology on multiple\nhardware architectures, without the need for an actual implementation. NTP is\ntightly integrated with hardware specific benchmarking tools to enable\nexhaustive data collection and analysis. Using NTP, a deep learning researcher\ncan quickly establish baselines needed to understand performance of an\nend-to-end neural network topology and make high level architectural decisions.\nFurther, integration of NTP with frameworks like Tensorflow, Pytorch, Intel\nOpenVINO etc. allows for performance comparison along several vectors like a)\nComparison of different frameworks on a given hardware b) Comparison of\ndifferent hardware using a given framework c) Comparison across different\nheterogeneous hardware configurations for given framework etc. These\ncapabilities empower a researcher to effortlessly make architectural decisions\nneeded for achieving optimized performance on any hardware platform. The paper\ndocuments the architectural approach of NTP and demonstrates the capabilities\nof the tool by benchmarking Mozilla DeepSpeech, a popular Speech Recognition\ntopology.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 10:55:22 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 01:51:25 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Bhat", "Raghavendra", ""], ["Chandran", "Pravin", ""], ["Jose", "Juby", ""], ["Dibbur", "Viswanath", ""], ["Ajith", "Prakash Sirra", ""]]}, {"id": "1905.09219", "submitter": "Shiqiang Wang", "authors": "Tiffany Tuor, Shiqiang Wang, Kin K. Leung, Bong Jun Ko", "title": "Online Collection and Forecasting of Resource Utilization in Large-Scale\n  Distributed Systems", "comments": "Accepted at IEEE International Conference on Distributed Computing\n  Systems (ICDCS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed computing systems often contain thousands of\ndistributed nodes (machines). Monitoring the conditions of these nodes is\nimportant for system management purposes, which, however, can be extremely\nresource demanding as this requires collecting local measurements of each\nindividual node and constantly sending those measurements to a central\ncontroller. Meanwhile, it is often useful to forecast the future system\nconditions for various purposes such as resource planning/allocation and\nanomaly detection, but it is usually too resource-consuming to have one\nforecasting model running for each node, which may also neglect correlations in\nobserved metrics across different nodes. In this paper, we propose a mechanism\nfor collecting and forecasting the resource utilization of machines in a\ndistributed computing system in a scalable manner. We present an algorithm that\nallows each local node to decide when to transmit its most recent measurement\nto the central node, so that the transmission frequency is kept below a given\nconstraint value. Based on the measurements received from local nodes, the\ncentral node summarizes the received data into a small number of clusters.\nSince the cluster partitioning can change over time, we also present a method\nto capture the evolution of clusters and their centroids. As an effective way\nto reduce the amount of computation, time-series forecasting models are trained\non the time-varying centroids of each cluster, to forecast the future resource\nutilizations of a group of local nodes. The effectiveness of our proposed\napproach is confirmed by extensive experiments using multiple real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 16:14:14 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Tuor", "Tiffany", ""], ["Wang", "Shiqiang", ""], ["Leung", "Kin K.", ""], ["Ko", "Bong Jun", ""]]}, {"id": "1905.09395", "submitter": "Dirk Van Essendelft", "authors": "Kyle Buchheit, Opeoluwa Owoyele, Terry Jordan, Dirk Van Essendelft", "title": "The Stabilized Explicit Variable-Load Solver with Machine Learning\n  Acceleration for the Rapid Solution of Stiff Chemical Kinetics", "comments": "25 pages, 14 Figures, 2 Tables, 56 Equations, Original research into\n  accelerating CFD with ML/AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.GR cs.LG cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this study, a fast and stable machine-learned hybrid algorithm implemented\nin TensorFlow for the integration of stiff chemical kinetics is introduced.\nNumerical solutions to differential equations are at the core of computational\nfluid dynamics calculations. As the size and complexity of the simulations\ngrow, so does the need for computational power and time. Many efforts have been\nmade to implement stiff chemistry solvers on GPUs but have not been highly\nsuccessful because of the logical divergence in traditional stiff solver\nalgorithms. Because of these constrains, a novel Explicit Stabilized\nVariable-load (STEV) solver has been developed. Overstepping due to the\nrelatively large time steps is prevented by introducing limits to the maximum\nchanges of chemical species per time step. To prevent oscillations, a discrete\nFourier transform is introduced to dampen ringing. In contrast to conventional\nexplicit approaches, a variable-load approach is used where each cell in the\ncomputational domain is advanced with its unique time step. This approach\nallows cells to be integrated simultaneously while maintaining warp convergence\nbut finish at different iterations and be removed from the workload. To improve\nthe computational performance of the introduced solver, specific thermodynamic\nquantities of interest were estimated using shallow neural networks in place of\npolynomial fits, leading to an additional 10% savings in clock time with\nminimal training and implementation requirements. However ML specific hardware\ncould increase the time savings to as much as 28%. While the complexity of\nthese particular machine learning models is not high by modern standards, the\nimpact on computational efficiency should not be ignored. The results show a\ndramatic decrease in total chemistry solution time (over 200 times) while\nmaintaining a similar degree of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 15:10:35 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 12:26:27 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 21:14:31 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Buchheit", "Kyle", ""], ["Owoyele", "Opeoluwa", ""], ["Jordan", "Terry", ""], ["Van Essendelft", "Dirk", ""]]}, {"id": "1905.09822", "submitter": "Vivek Seshadri", "authors": "Vivek Seshadri and Onur Mutlu", "title": "In-DRAM Bulk Bitwise Execution Engine", "comments": "arXiv admin note: substantial text overlap with arXiv:1605.06483,\n  arXiv:1610.09603, arXiv:1611.09988", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications heavily use bitwise operations on large bitvectors as part\nof their computation. In existing systems, performing such bulk bitwise\noperations requires the processor to transfer a large amount of data on the\nmemory channel, thereby consuming high latency, memory bandwidth, and energy.\nIn this paper, we describe Ambit, a recently-proposed mechanism to perform bulk\nbitwise operations completely inside main memory. Ambit exploits the internal\norganization and analog operation of DRAM-based memory to achieve low cost,\nhigh performance, and low energy. Ambit exposes a new bulk bitwise execution\nmodel to the host processor. Evaluations show that Ambit significantly improves\nthe performance of several applications that use bulk bitwise operations,\nincluding databases.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 06:17:39 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 05:17:50 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 14:08:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Seshadri", "Vivek", ""], ["Mutlu", "Onur", ""]]}, {"id": "1905.09914", "submitter": "Jan K\\v{r}et\\'insk\\'y", "authors": "Milan \\v{C}e\\v{s}ka and Jan K\\v{r}et\\'insk\\'y", "title": "Semi-Quantitative Abstraction and Analysis of Chemical Reaction Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.PF q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of large continuous-time stochastic systems is a computationally\nintensive task. In this work we focus on population models arising from\nchemical reaction networks (CRNs), which play a fundamental role in analysis\nand design of biochemical systems. Many relevant CRNs are particularly\nchallenging for existing techniques due to complex dynamics including\nstochasticity, stiffness or multimodal population distributions. We propose a\nnovel approach allowing not only to predict, but also to explain both the\ntransient and steady-state behaviour. It focuses on qualitative description of\nthe behaviour and aims at quantitative precision only in orders of magnitude.\nFirstly, we abstract the CRN into a compact model preserving rough timing\ninformation, distinguishing only signifcinatly different populations, but\ncapturing relevant sequences of behaviour. Secondly, we approximately analyse\nthe most probable temporal behaviours of the model through most probable\ntransitions. As demonstrated on complex CRNs from literature, our approach\nreproduces the known results, but in contrast to the state-of-the-art methods,\nit runs with virtually no computational cost and thus offers\nunprecedented~scalability.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 20:44:39 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["\u010ce\u0161ka", "Milan", ""], ["K\u0159et\u00ednsk\u00fd", "Jan", ""]]}, {"id": "1905.10603", "submitter": "Georg Hager", "authors": "Ayesha Afzal, Georg Hager, Gerhard Wellein", "title": "Propagation and Decay of Injected One-Off Delays on Clusters: A Case\n  Study", "comments": "10 pages, 9 figures; title changed", "journal-ref": null, "doi": "10.1109/CLUSTER.2019.8890995", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic, first-principles performance modeling of distributed-memory\napplications is difficult due to a wide spectrum of random disturbances caused\nby the application and the system. These disturbances (commonly called \"noise\")\ndestroy the assumptions of regularity that one usually employs when\nconstructing simple analytic models. Despite numerous efforts to quantify,\ncategorize, and reduce such effects, a comprehensive quantitative understanding\nof their performance impact is not available, especially for long delays that\nhave global consequences for the parallel application. In this work, we\ninvestigate various traces collected from synthetic benchmarks that mimic real\napplications on simulated and real message-passing systems in order to pinpoint\nthe mechanisms behind delay propagation. We analyze the dependence of the\npropagation speed of idle waves emanating from injected delays with respect to\nthe execution and communication properties of the application, study how such\ndelays decay under increased noise levels, and how they interact with each\nother. We also show how fine-grained noise can make a system immune against the\nadverse effects of propagating idle waves. Our results contribute to a better\nunderstanding of the collective phenomena that manifest themselves in\ndistributed-memory parallel applications.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 14:32:34 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 13:35:52 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 11:10:35 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Afzal", "Ayesha", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1905.11012", "submitter": "Qiang Wang", "authors": "Zhenheng Tang, Yuxin Wang, Qiang Wang, Xiaowen Chu", "title": "The Impact of GPU DVFS on the Energy and Performance of Deep Learning:\n  an Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, great progress has been made in improving the computing\npower of general-purpose graphics processing units (GPGPUs), which facilitates\nthe prosperity of deep neural networks (DNNs) in multiple fields like computer\nvision and natural language processing. A typical DNN training process\nrepeatedly updates tens of millions of parameters, which not only requires huge\ncomputing resources but also consumes significant energy. In order to train\nDNNs in a more energy-efficient way, we empirically investigate the impact of\nGPU Dynamic Voltage and Frequency Scaling (DVFS) on the energy consumption and\nperformance of deep learning. Our experiments cover a wide range of GPU\narchitectures, DVFS settings, and DNN configurations. We observe that, compared\nto the default core frequency settings of three tested GPUs, the optimal core\nfrequency can help conserve 8.7%$\\sim$23.1% energy consumption for different\nDNN training cases. Regarding the inference, the benefits vary from\n19.6%$\\sim$26.4%. Our findings suggest that GPU DVFS has great potentials to\nhelp develop energy efficient DNN training/inference schemes.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 07:25:52 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Tang", "Zhenheng", ""], ["Wang", "Yuxin", ""], ["Wang", "Qiang", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1905.11707", "submitter": "Igor Ivkic", "authors": "Roland Pellegrini, Igor Ivkic and Markus Tauber", "title": "Function-as-a-Service Benchmarking Framework", "comments": "https://www.scitepress.org/PublicationsDetail.aspx?ID=Df3810DssWw=&t=1", "journal-ref": null, "doi": "10.5220/0007757304790487", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Service Providers deliver their products in form of 'as-a-Service',\nwhich are typically categorized by the level of abstraction. This approach\nhides the implementation details and shows only functionality to the user.\nHowever, the problem is that it is hard to measure the performance of Cloud\nservices, because they behave like black boxes. Especially with\nFunction-as-a-Service it is even more difficult because it completely hides\nserver and infrastructure management from users by design. Cloud Service\nProdivers usually restrict the maximum size of code, memory and runtime of\nCloud Functions. Nevertheless, users need clarification if more ressources are\nneeded to deliver services in high quality. In this regard, we present the\narchitectural design of a new Function-as-a-Service benchmarking tool, which\nallows users to evaluate the performance of Cloud Functions. Furthermore, the\ncapabilities of the framework are tested on an isolated platform with a\nspecific workload. The results show that users are able to get insights into\nFunction-as-a-Service environments. This, in turn, allows users to identify\nfactors which may slow down or speed up the performance of Cloud Functions.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 09:41:17 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Pellegrini", "Roland", ""], ["Ivkic", "Igor", ""], ["Tauber", "Markus", ""]]}, {"id": "1905.12155", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher and Matteo Dell'Amico", "title": "The Supermarket Model with Known and Predicted Service Times", "comments": "Revision, draft version, 16 pages, future updates and corrections\n  possible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The supermarket model refers to a system with a large number of queues, where\narriving customers choose $d$ queues at random and join the queue with the\nfewest customers. The supermarket model demonstrates the power of even small\namounts of choice, as compared to simply joining a queue chosen uniformly at\nrandom, for load balancing systems. In this work we perform simulation-based\nstudies to consider variations where service times for a customer are\npredicted, as might be done in modern settings using machine learning\ntechniques or related mechanisms. Our primary takeaway is that using even\nseemingly weak predictions of service times can yield significant benefits over\nblind First In First Out queueing in this context. However, some care must be\ntaken when using predicted service time information to both choose a queue and\norder elements for service within a queue; while in many cases using the\ninformation for both choosing and ordering is beneficial, in many of our\nsimulation settings we find that simply using the number of jobs to choose a\nqueue is better when using predicted service times to order jobs in a queue.\nAlthough this study is simulation based, our study leaves many natural\ntheoretical open questions for future work.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 15:33:45 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 14:44:09 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Dell'Amico", "Matteo", ""]]}, {"id": "1905.12292", "submitter": "Sanket Tavarageri", "authors": "Sanket Tavarageri", "title": "Categorization of Program Regions for Agile Compilation using Machine\n  Learning and Hardware Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compiler processes the code written in a high level language and produces\nmachine executable code. The compiler writers often face the challenge of\nkeeping the compilation times reasonable. That is because aggressive\noptimization passes which potentially will give rise to high performance are\noften expensive in terms of running time and memory footprint. Consequently the\ncompiler designers arrive at a compromise where they either simplify the\noptimization algorithm which may decrease the performance of the produced code,\nor they will restrict the optimization to the subset of the overall input\nprogram in which case large parts of the input application will go\nun-optimized.\n  The problem we address in this paper is that of keeping the compilation times\nreasonable, and at the same time optimizing the input program to the fullest\nextent possible. Consequently, the performance of the produced code will match\nthe performance when all the aggressive optimization passes are applied over\nthe entire input program.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 09:46:49 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Tavarageri", "Sanket", ""]]}, {"id": "1905.12411", "submitter": "Vuong M. Ngo", "authors": "Vuong M. Ngo and Nhien-An Le-Khac and M-Tahar Kechadi", "title": "Designing and Implementing Data Warehouse for Agricultural Big Data", "comments": "Business intelligent, data warehouse, constellation schema, Big Data,\n  precision agriculture", "journal-ref": "BigData 2019", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, precision agriculture that uses modern information and\ncommunication technologies is becoming very popular. Raw and semi-processed\nagricultural data are usually collected through various sources, such as:\nInternet of Thing (IoT), sensors, satellites, weather stations, robots, farm\nequipment, farmers and agribusinesses, etc. Besides, agricultural datasets are\nvery large, complex, unstructured, heterogeneous, non-standardized, and\ninconsistent. Hence, the agricultural data mining is considered as Big Data\napplication in terms of volume, variety, velocity and veracity. It is a key\nfoundation to establishing a crop intelligence platform, which will enable\nresource efficient agronomy decision making and recommendations. In this paper,\nwe designed and implemented a continental level agricultural data warehouse by\ncombining Hive, MongoDB and Cassandra. Our data warehouse capabilities: (1)\nflexible schema; (2) data integration from real agricultural multi datasets;\n(3) data science and business intelligent support; (4) high performance; (5)\nhigh storage; (6) security; (7) governance and monitoring; (8) replication and\nrecovery; (9) consistency, availability and partition tolerant; (10)\ndistributed and cloud deployment. We also evaluate the performance of our data\nwarehouse.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:18:03 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Ngo", "Vuong M.", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1905.12720", "submitter": "Val\\'erie Hayot-Sasson", "authors": "Valerie Hayot-Sasson and Tristan Glatard", "title": "Evaluation of pilot jobs for Apache Spark applications on HPC clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Big Data has become prominent throughout many scientific fields and, as a\nresult, scientific communities have sought out Big Data frameworks to\naccelerate the processing of their increasingly data-intensive pipelines.\nHowever, while scientific communities typically rely on High-Performance\nComputing (HPC) clusters for the parallelization of their pipelines, many\npopular Big Data frameworks such as Hadoop and Apache Spark were primarily\ndesigned to be executed on dedicated commodity infrastructures. This paper\nevaluates the benefits of pilot jobs over traditional batch submission for\nApache Spark on HPC clusters. Surprisingly, our results show that the speed-up\nprovided by pilot jobs over batch scheduling is moderate to inexistent (0.98 on\naverage) despite the presence of long queuing times. In addition, pilot jobs\nprovide an extra layer of scheduling that complexifies debugging and\ndeployment. We conclude that traditional batch scheduling should remain the\ndefault strategy to deploy Apache Spark applications on HPC clusters.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 20:55:50 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Hayot-Sasson", "Valerie", ""], ["Glatard", "Tristan", ""]]}, {"id": "1905.13135", "submitter": "Katy Williams", "authors": "Katy Williams, Alex Bigelow, Katherine E. Isaacs", "title": "Visualizing a Moving Target: A Design Study on Task Parallel Programs in\n  the Presence of Evolving Data and Concerns", "comments": "IEEE VIS InfoVis 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934285", "report-no": null, "categories": "cs.HC cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common pitfalls in visualization projects include lack of data availability\nand the domain users' needs and focus changing too rapidly for the design\nprocess to complete. While it is often prudent to avoid such projects, we argue\nit can be beneficial to engage them in some cases as the visualization process\ncan help refine data collection, solving a \"chicken and egg\" problem of having\nthe data and tools to analyze it. We found this to be the case in the domain of\ntask parallel computing where such data and tooling is an open area of\nresearch. Despite these hurdles, we conducted a design study. Through a\ntightly-coupled iterative design process, we built Atria, a multi-view\nexecution graph visualization to support performance analysis. Atria simplifies\nthe initial representation of the execution graph by aggregating nodes as\nrelated to their line of code. We deployed Atria on multiple platforms, some\nrequiring design alteration. We describe how we adapted the design study\nmethodology to the \"moving target\" of both the data and the domain experts'\nconcerns and how this movement kept both the visualization and programming\nproject healthy. We reflect on our process and discuss what factors allow the\nproject to be successful in the presence of changing data and user needs.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:01:13 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 18:26:05 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 18:29:54 GMT"}, {"version": "v4", "created": "Tue, 15 Oct 2019 17:34:03 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Williams", "Katy", ""], ["Bigelow", "Alex", ""], ["Isaacs", "Katherine E.", ""]]}, {"id": "1905.13264", "submitter": "Isabel Wagner", "authors": "Isabel Wagner, Yuchen Zhao", "title": "Using Metrics Suites to Improve the Measurement of Privacy in Graphs", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social graphs are widely used in research (e.g., epidemiology) and business\n(e.g., recommender systems). However, sharing these graphs poses privacy risks\nbecause they contain sensitive information about individuals. Graph\nanonymization techniques aim to protect individual users in a graph, while\ngraph de-anonymization aims to re-identify users. The effectiveness of\nanonymization and de-anonymization algorithms is usually evaluated with privacy\nmetrics. However, it is unclear how strong existing privacy metrics are when\nthey are used in graph privacy. In this paper, we study 26 privacy metrics for\ngraph anonymization and de-anonymization and evaluate their strength in terms\nof three criteria: monotonicity indicates whether the metric indicates lower\nprivacy for stronger adversaries; for within-scenario comparisons, evenness\nindicates whether metric values are spread evenly; and for between-scenario\ncomparisons, shared value range indicates whether metrics use a consistent\nvalue range across scenarios. Our extensive experiments indicate that no single\nmetric fulfills all three criteria perfectly. We therefore use methods from\nmulti-criteria decision analysis to aggregate multiple metrics in a metrics\nsuite, and we show that these metrics suites improve monotonicity compared to\nthe best individual metric. This important result enables more monotonic, and\nthus more accurate, evaluations of new graph anonymization and de-anonymization\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 18:59:17 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Wagner", "Isabel", ""], ["Zhao", "Yuchen", ""]]}, {"id": "1905.13368", "submitter": "Rekha Singhal Dr.", "authors": "Rekha Singhal, Gautam Shroff, Mukund Kumar, Sharod Roy, Sanket\n  Kadarkar, Rupinder virk, Siddharth Verma and Vartika Tiwari", "title": "Fast Online \"Next Best Offers\" using Deep Learning", "comments": "7 Pages, Accepted in COMAD-CODS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present iPrescribe, a scalable low-latency architecture for\nrecommending 'next-best-offers' in an online setting. The paper presents the\ndesign of iPrescribe and compares its performance for implementations using\ndifferent real-time streaming technology stacks. iPrescribe uses an ensemble of\ndeep learning and machine learning algorithms for prediction. We describe the\nscalable real-time streaming technology stack and optimized machine-learning\nimplementations to achieve a 90th percentile recommendation latency of 38\nmilliseconds. Optimizations include a novel mechanism to deploy recurrent Long\nShort Term Memory (LSTM) deep learning networks efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 01:03:04 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Singhal", "Rekha", ""], ["Shroff", "Gautam", ""], ["Kumar", "Mukund", ""], ["Roy", "Sharod", ""], ["Kadarkar", "Sanket", ""], ["virk", "Rupinder", ""], ["Verma", "Siddharth", ""], ["Tiwari", "Vartika", ""]]}, {"id": "1905.13536", "submitter": "Christopher Canel", "authors": "Christopher Canel, Thomas Kim, Giulio Zhou, Conglong Li, Hyeontaek\n  Lim, David G. Andersen, Michael Kaminsky, Subramanya R. Dulloor", "title": "Scaling Video Analytics on Constrained Edge Nodes", "comments": "This paper is an extended version of a paper with the same title\n  published in the 2nd SysML Conference, SysML '19 (Canel et. al., 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As video camera deployments continue to grow, the need to process large\nvolumes of real-time data strains wide area network infrastructure. When\nper-camera bandwidth is limited, it is infeasible for applications such as\ntraffic monitoring and pedestrian tracking to offload high-quality video\nstreams to a datacenter. This paper presents FilterForward, a new edge-to-cloud\nsystem that enables datacenter-based applications to process content from\nthousands of cameras by installing lightweight edge filters that backhaul only\nrelevant video frames. FilterForward introduces fast and expressive\nper-application microclassifiers that share computation to simultaneously\ndetect dozens of events on computationally constrained edge nodes. Only\nmatching events are transmitted to the cloud. Evaluation on two real-world\ncamera feed datasets shows that FilterForward reduces bandwidth use by an order\nof magnitude while improving computational efficiency and event detection\naccuracy for challenging video content.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 23:11:07 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Canel", "Christopher", ""], ["Kim", "Thomas", ""], ["Zhou", "Giulio", ""], ["Li", "Conglong", ""], ["Lim", "Hyeontaek", ""], ["Andersen", "David G.", ""], ["Kaminsky", "Michael", ""], ["Dulloor", "Subramanya R.", ""]]}]