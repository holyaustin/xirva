[{"id": "1712.00790", "submitter": "Ziv Scully", "authors": "Ziv Scully, Mor Harchol-Balter, Alan Scheller-Wolf", "title": "SOAP: One Clean Analysis of All Age-Based Scheduling Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an extremely broad class of M/G/1 scheduling policies called\nSOAP: Schedule Ordered by Age-based Priority. The SOAP policies include almost\nall scheduling policies in the literature as well as an infinite number of\nvariants which have never been analyzed, or maybe not even conceived. SOAP\npolicies range from classic policies, like first-come, first-serve (FCFS),\nforeground-background (FB), class-based priority, and shortest remaining\nprocessing time (SRPT); to much more complicated scheduling rules, such as the\nfamously complex Gittins index policy and other policies in which a job's\npriority changes arbitrarily with its age. While the response time of policies\nin the former category is well understood, policies in the latter category have\nresisted response time analysis. We present a universal analysis of all SOAP\npolicies, deriving the mean and Laplace-Stieltjes transform of response time.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 16:14:29 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 18:47:58 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Scully", "Ziv", ""], ["Harchol-Balter", "Mor", ""], ["Scheller-Wolf", "Alan", ""]]}, {"id": "1712.01103", "submitter": "EPTCS", "authors": "Elizabeth Firman (Tel Aviv University), Shahar Maoz (Tel Aviv\n  University), Jan Oliver Ringert (Tel Aviv University)", "title": "Performance Heuristics for GR(1) Synthesis and Related Algorithms", "comments": "In Proceedings SYNT 2017, arXiv:1711.10224", "journal-ref": "EPTCS 260, 2017, pp. 62-80", "doi": "10.4204/EPTCS.260.7", "report-no": null, "categories": "cs.LO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactive synthesis for the GR(1) fragment of LTL has been implemented and\nstudied in many works. In this workshop paper we present and evaluate a list of\nheuristics to potentially reduce running times for GR(1) synthesis and related\nalgorithms. The list includes early detection of fixed-points and\nunrealizability, fixed-point recycling, and heuristics for unrealizable core\ncomputations. We evaluate the presented heuristics on SYNTECH15, a total of 78\nspecifications of 6 autonomous Lego robots, written by 3rd year undergraduate\ncomputer science students in a project class we have taught, as well as on\nseveral benchmarks from the literature. The evaluation investigates not only\nthe potential of the suggested heuristics to improve computation times, but\nalso the difference between existing benchmarks and the robot's specifications\nin terms of the effectiveness of the heuristics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 01:27:33 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Firman", "Elizabeth", "", "Tel Aviv University"], ["Maoz", "Shahar", "", "Tel Aviv\n  University"], ["Ringert", "Jan Oliver", "", "Tel Aviv University"]]}, {"id": "1712.01209", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu and Benjamin Paul Chamberlain", "title": "Speeding Up BigClam Implementation on SNAP", "comments": "To appear in 2018 Imperial College Computing Student Workshop\n  (ICCSW'18); 12 pages, 4 figures, and 3 tables", "journal-ref": "2018 Imperial College Computing Student Workshop (ICCSW 2018).\n  OpenAccess Series in Informatics (OASIcs), vol. 66, pp. 1:1-1:13", "doi": "10.4230/OASIcs.ICCSW.2018.1", "report-no": null, "categories": "cs.SI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a detailed analysis of the C++ implementation of the Cluster\nAffiliation Model for Big Networks (BigClam) on the Stanford Network Analysis\nProject (SNAP). BigClam is a popular graph mining algorithm that is capable of\nfinding overlapping communities in networks containing millions of nodes. Our\nanalysis shows a key stage of the algorithm - determining if a node belongs to\na community - dominates the runtime of the implementation, yet the computation\nis not parallelized. We show that by parallelizing computations across multiple\nthreads using OpenMP we can speed up the algorithm by 5.3 times when solving\nlarge networks for communities, while preserving the integrity of the program\nand the result.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 17:23:11 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 17:43:35 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Liu", "C. H. Bryan", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1712.01718", "submitter": "Ronny Tschueter", "authors": "Ronny Tsch\\\"uter, Johannes Ziegenbalg, Bert Wesarg, Matthias Weber,\n  Christian Herold, Sebastian D\\\"obel, Ronny Brendel", "title": "An LLVM Instrumentation Plug-in for Score-P", "comments": "8 pages", "journal-ref": "LLVM-HPC'17: Proceedings of the Fourth Workshop on the LLVM\n  Compiler Infrastructure in HPC, 2017, 2:1--2:8", "doi": "10.1145/3148173.3148187", "report-no": null, "categories": "cs.SE cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing application runtime, scaling parallel applications to higher numbers\nof processes/threads, and porting applications to new hardware architectures\nare tasks necessary in the software development process. Therefore, developers\nhave to investigate and understand application runtime behavior. Tools such as\nmonitoring infrastructures that capture performance relevant data during\napplication execution assist in this task. The measured data forms the basis\nfor identifying bottlenecks and optimizing the code. Monitoring infrastructures\nneed mechanisms to record application activities in order to conduct\nmeasurements. Automatic instrumentation of the source code is the preferred\nmethod in most application scenarios. We introduce a plug-in for the LLVM\ninfrastructure that enables automatic source code instrumentation at\ncompile-time. In contrast to available instrumentation mechanisms in\nLLVM/Clang, our plug-in can selectively include/exclude individual application\nfunctions. This enables developers to fine-tune the measurement to the required\nlevel of detail while avoiding large runtime overheads due to excessive\ninstrumentation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 13:15:49 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Tsch\u00fcter", "Ronny", ""], ["Ziegenbalg", "Johannes", ""], ["Wesarg", "Bert", ""], ["Weber", "Matthias", ""], ["Herold", "Christian", ""], ["D\u00f6bel", "Sebastian", ""], ["Brendel", "Ronny", ""]]}, {"id": "1712.02912", "submitter": "Fabien Andr\\'e", "authors": "Fabien Andr\\'e", "title": "Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search", "comments": "PhD Thesis, 123 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.MM cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many multimedia information retrieval or machine learning problems require\nefficient high-dimensional nearest neighbor search techniques. For instance,\nmultimedia objects (images, music or videos) can be represented by\nhigh-dimensional feature vectors. Finding two similar multimedia objects then\ncomes down to finding two objects that have similar feature vectors. In the\ncurrent context of mass use of social networks, large scale multimedia\ndatabases or large scale machine learning applications are more and more\ncommon, calling for efficient nearest neighbor search approaches.\n  This thesis builds on product quantization, an efficient nearest neighbor\nsearch technique that compresses high-dimensional vectors into short codes.\nThis makes it possible to store very large databases entirely in RAM, enabling\nlow response times. We propose several contributions that exploit the\ncapabilities of modern CPUs, especially SIMD and the cache hierarchy, to\nfurther decrease response times offered by product quantization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 02:14:17 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Andr\u00e9", "Fabien", ""]]}, {"id": "1712.03209", "submitter": "Zhuo Chen", "authors": "Zhuo Chen and Diana Marculescu", "title": "Task Scheduling for Heterogeneous Multicore Systems", "comments": "heterogeneous systems; scheduling; performance modeling; queueing\n  theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, as the demand for low energy and high performance computing\nhas steadily increased, heterogeneous computing has emerged as an important and\npromising solution. Because most workloads can typically run most efficiently\non certain types of cores, mapping tasks on the best available resources can\nnot only save energy but also deliver high performance. However, optimal task\nscheduling for performance and/or energy is yet to be solved for heterogeneous\nplatforms. The work presented herein mathematically formulates the optimal\nheterogeneous system task scheduling as an optimization problem using queueing\ntheory. We analytically solve for the common case of two processor types, e.g.,\nCPU+GPU, and give an optimal policy (CAB). We design the GrIn heuristic to\nefficiently solve for near-optimal policy for any number of processor types\n(within 1.6% of the optimal). Both policies work for any task size distribution\nand processing order, and are therefore, general and practical. We extensively\nsimulate and validate the theory, and implement the proposed policy in a\nCPU-GPU real platform to show the optimal throughput and energy improvement.\nComparing to classic policies like load-balancing, our results range from\n1.08x~2.24x better performance or 1.08x~2.26x better energy efficiency in\nsimulations, and 2.37x~9.07x better performance in experiments.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 18:34:15 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Chen", "Zhuo", ""], ["Marculescu", "Diana", ""]]}, {"id": "1712.03246", "submitter": "Zhuo Chen", "authors": "Zhuo Chen and Diana Marculescu", "title": "Priority-Aware Near-Optimal Scheduling for Heterogeneous Multi-Core\n  Systems with Specialized Accelerators", "comments": "heterogeneous systems, performance modeling, queueing theory, optimal\n  scheduling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To deliver high performance in power limited systems, architects have turned\nto using heterogeneous systems, either CPU+GPU or mixed CPU-hardware systems.\nHowever, in systems with different processor types and task affinities,\nscheduling tasks becomes more challenging than in homogeneous multi-core\nsystems or systems without task affinities. The problem is even more complex\nwhen specialized accelerators and task priorities are included. In this paper,\nwe provide a formal proof for the optimal scheduling policy for heterogeneous\nsystems with arbitrary number of resource types, including specialized\naccelerators, independent of the task arrival rate, task size distribution, and\nresource processing order. We transform the optimal scheduling policy to a\nnonlinear integer optimization problem and propose a fast, near-optimal\nalgorithm. An additional heuristic is proposed for the case of priority-aware\nscheduling. Our experimental results demonstrate that the proposed algorithm is\nonly 0.3% from the optimal and superior to conventional scheduling policies.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 19:02:48 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chen", "Zhuo", ""], ["Marculescu", "Diana", ""]]}, {"id": "1712.03530", "submitter": "Mohammad Noormohammadpour", "authors": "Mohammad Noormohammadpour and Cauligi S. Raghavendra", "title": "Datacenter Traffic Control: Understanding Techniques and Trade-offs", "comments": "Accepted for Publication in IEEE Communications Surveys and Tutorials", "journal-ref": "IEEE Communications Surveys Tutorials 20 (2018) 1492-1525", "doi": "10.1109/COMST.2017.2782753", "report-no": null, "categories": "cs.NI cs.DC cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datacenters provide cost-effective and flexible access to scalable compute\nand storage resources necessary for today's cloud computing needs. A typical\ndatacenter is made up of thousands of servers connected with a large network\nand usually managed by one operator. To provide quality access to the variety\nof applications and services hosted on datacenters and maximize performance, it\ndeems necessary to use datacenter networks effectively and efficiently.\nDatacenter traffic is often a mix of several classes with different priorities\nand requirements. This includes user-generated interactive traffic, traffic\nwith deadlines, and long-running traffic. To this end, custom transport\nprotocols and traffic management techniques have been developed to improve\ndatacenter network performance.\n  In this tutorial paper, we review the general architecture of datacenter\nnetworks, various topologies proposed for them, their traffic properties,\ngeneral traffic control challenges in datacenters and general traffic control\nobjectives. The purpose of this paper is to bring out the important\ncharacteristics of traffic control in datacenters and not to survey all\nexisting solutions (as it is virtually impossible due to massive body of\nexisting research). We hope to provide readers with a wide range of options and\nfactors while considering a variety of traffic control mechanisms. We discuss\nvarious characteristics of datacenter traffic control including management\nschemes, transmission control, traffic shaping, prioritization, load balancing,\nmultipathing, and traffic scheduling. Next, we point to several open challenges\nas well as new and interesting networking paradigms. At the end of this paper,\nwe briefly review inter-datacenter networks that connect geographically\ndispersed datacenters which have been receiving increasing attention recently\nand pose interesting and novel research problems.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 14:03:16 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Noormohammadpour", "Mohammad", ""], ["Raghavendra", "Cauligi S.", ""]]}, {"id": "1712.03565", "submitter": "Faheem Zafari", "authors": "Faheem Zafari, Jian Li, Kin K. Leung, Don Towsley, Ananthram Swami", "title": "Optimal Energy Consumption with Communication, Computation, Caching and\n  QoI-Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency is a fundamental requirement of modern data communication\nsystems, and its importance is reflected in much recent work on performance\nanalysis of system energy consumption. However, most works have only focused on\ncommunication and computation costs, but do not account for caching costs.\nGiven the increasing interest in cache networks, this is a serious deficiency.\n{In this paper, we consider the problem of energy consumption in data\ncommunication, compression and caching (C$3$) with a Quality of Information\n(QoI) guarantee in a communication network. {Our goal is to identify the\noptimal data compression rate and data placement over the network to minimize\nthe overall energy consumption in the network.} he formulated problem is a\n\\emph{Mixed Integer Non-Linear Programming} (MINLP) problem with non-convex\nfunctions, which is NP-hard in general. } {We} propose a variant of spatial\nbranch and bound algorithm (V-SBB), that can {provide} the $\\epsilon$-global\noptimal solution to {our problem}. {We numerically show that our C3\noptimization framework can improve the energy efficiency up to 88\\% compared to\nany C2 optimization between communication and computation or caching.\nFurthermore, for our energy consumption problem, V-SBB {provides comparatively\nbetter solution than some other MINLP solvers.}}\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 17:44:10 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 17:07:20 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 17:28:51 GMT"}, {"version": "v4", "created": "Thu, 24 Jan 2019 14:21:27 GMT"}, {"version": "v5", "created": "Sat, 26 Jan 2019 23:45:30 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Zafari", "Faheem", ""], ["Li", "Jian", ""], ["Leung", "Kin K.", ""], ["Towsley", "Don", ""], ["Swami", "Ananthram", ""]]}, {"id": "1712.04175", "submitter": "Wasiur R. KhudaBukhsh", "authors": "Wasiur R. KhudaBukhsh and Bastian Alt and Sounak Kar and Amr Rizk and\n  Heinz Koeppl", "title": "Collaborative Uploading in Heterogeneous Networks: Optimal and Adaptive\n  Strategies", "comments": "15 pages, 11 figures, extended version of a conference paper accepted\n  for publication in the Proceedings of the IEEE International Conference on\n  Computer Communications (INFOCOM), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative uploading describes a type of crowdsourcing scenario in\nnetworked environments where a device utilizes multiple paths over neighboring\ndevices to upload content to a centralized processing entity such as a cloud\nservice. Intermediate devices may aggregate and preprocess this data stream.\nSuch scenarios arise in the composition and aggregation of information, e.g.,\nfrom smartphones or sensors. We use a queuing theoretic description of the\ncollaborative uploading scenario, capturing the ability to split data into\nchunks that are then transmitted over multiple paths, and finally merged at the\ndestination. We analyze replication and allocation strategies that control the\nmapping of data to paths and provide closed-form expressions that pinpoint the\noptimal strategy given a description of the paths' service distributions.\nFinally, we provide an online path-aware adaptation of the allocation strategy\nthat uses statistical inference to sequentially minimize the expected waiting\ntime for the uploaded data. Numerical results show the effectiveness of the\nadaptive approach compared to the proportional allocation and a variant of the\njoin-the-shortest-queue allocation, especially for bursty path conditions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 08:42:59 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 18:13:33 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["KhudaBukhsh", "Wasiur R.", ""], ["Alt", "Bastian", ""], ["Kar", "Sounak", ""], ["Rizk", "Amr", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1712.06634", "submitter": "Liang Liu", "authors": "Liang Liu, Long Gong, Sen Yang, Jun Xu, Lance Fortnow", "title": "Better Algorithms for Hybrid Circuit and Packet Switching in Data\n  Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid circuit and packet switching for data center networking (DCN) has\nreceived considerable research attention recently. A hybrid-switched DCN\nemploys a much faster circuit switch that is reconfigurable with a nontrivial\ncost, and a much slower packet switch that is reconfigurable with no cost, to\ninterconnect its racks of servers. The research problem is, given a traffic\ndemand matrix (between the racks), how to compute a good circuit switch\nconfiguration schedule so that the vast majority of the traffic demand is\nremoved by the circuit switch, leaving a remaining demand matrix that contains\nonly small elements for the packet switch to handle. In this paper, we propose\ntwo new hybrid switch scheduling algorithms under two different scheduling\nconstraints. Our first algorithm, called 2-hop Eclipse, strikes a much better\ntradeoff between the resulting performance (of the hybrid switch) and the\ncomputational complexity (of the algorithm) than the state of the art solution\nEclipse/Eclipse++. Our second algorithm, called BFF (best first fit), is the\nfirst hybrid switching solution that exploits the potential partial\nreconfiguration capability of the circuit switch for performance gains.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 19:24:24 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Liu", "Liang", ""], ["Gong", "Long", ""], ["Yang", "Sen", ""], ["Xu", "Jun", ""], ["Fortnow", "Lance", ""]]}, {"id": "1712.07307", "submitter": "Jian Li", "authors": "Nitish K. Panigrahy, Jian Li, Don Towsley, Christopher V. Hollot", "title": "Network Cache Design under Stationary Requests: Exact Analysis and\n  Poisson Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of caching algorithms to maximize hit probability has been\nextensively studied. In this paper, we associate each content with a utility,\nwhich is a function of either the corresponding content hit rate or hit\nprobability. We formulate a cache optimization problem to maximize the sum of\nutilities over all contents under stationary and ergodic request processes.\nThis problem is non-convex in general but we reformulate it as a convex\noptimization problem when the inter-request time (irt) distribution has a\nnon-increasing hazard rate function. We provide explicit optimal solutions for\nsome irt distributions, and compare the solutions of the hit-rate based (HRB)\nand hit-probability based (HPB) problems. We formulate a reverse engineering\nbased dual implementation of LRU under stationary arrivals. We also propose\ndecentralized algorithms that can be implemented using limited information and\nuse a discrete time Lyapunov technique (DTLT) to correctly characterize their\nstability. We find that decentralized algorithms that solve HRB are more robust\nthan decentralized HPB algorithms. Informed by these results, we further\npropose lightweight Poisson approximate decentralized and online algorithms\nthat are accurate and efficient in achieving optimal hit rates and hit\nprobabilities.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 03:06:48 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 17:31:08 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 14:05:14 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Panigrahy", "Nitish K.", ""], ["Li", "Jian", ""], ["Towsley", "Don", ""], ["Hollot", "Christopher V.", ""]]}, {"id": "1712.08230", "submitter": "Albin Severinson", "authors": "Albin Severinson, Alexandre Graell i Amat, Eirik Rosnes", "title": "Block-Diagonal and LT Codes for Distributed Computing With Straggling\n  Servers", "comments": "To appear in IEEE Transactions on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two coded schemes for the distributed computing problem of\nmultiplying a matrix by a set of vectors. The first scheme is based on\npartitioning the matrix into submatrices and applying maximum distance\nseparable (MDS) codes to each submatrix. For this scheme, we prove that up to a\ngiven number of partitions the communication load and the computational delay\n(not including the encoding and decoding delay) are identical to those of the\nscheme recently proposed by Li et al., based on a single, long MDS code.\nHowever, due to the use of shorter MDS codes, our scheme yields a significantly\nlower overall computational delay when the delay incurred by encoding and\ndecoding is also considered. We further propose a second coded scheme based on\nLuby Transform (LT) codes under inactivation decoding. Interestingly, LT codes\nmay reduce the delay over the partitioned scheme at the expense of an increased\ncommunication load. We also consider distributed computing under a deadline and\nshow numerically that the proposed schemes outperform other schemes in the\nliterature, with the LT code-based scheme yielding the best performance for the\nscenarios considered.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 22:02:25 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 14:17:50 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 10:00:57 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Severinson", "Albin", ""], ["Amat", "Alexandre Graell i", ""], ["Rosnes", "Eirik", ""]]}, {"id": "1712.08285", "submitter": "Emanuel Onica", "authors": "Ciprian Amariei, Paul Diac, Emanuel Onica", "title": "Grand Challenge: Optimized Stage Processing for Anomaly Detection on\n  Numerical Data Streams", "comments": null, "journal-ref": "DEBS 2017, Proceedings of the 11th ACM International Conference on\n  Distributed and Event-based Systems, Pages 286-291", "doi": "10.1145/3093742.3095101", "report-no": null, "categories": "cs.PF cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2017 Grand Challenge focused on the problem of automatic detection of\nanomalies for manufacturing equipment. This paper reports the technical details\nof a solution focused on particular optimizations of the processing stages.\nThese included customized input parsing, fine tuning of a k-means clustering\nalgorithm and probability analysis using a lazy flavor of a Markov chain. We\nhave observed in our custom implementation that carefully tweaking these\nprocessing stages at single node level by leveraging various data stream\ncharacteristics can yield good performance results. We start the paper with\nseveral observations concerning the input data stream, following with our\nsolution description with details on particular optimizations, and we conclude\nwith evaluation and a discussion of obtained results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 02:31:16 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Amariei", "Ciprian", ""], ["Diac", "Paul", ""], ["Onica", "Emanuel", ""]]}, {"id": "1712.08555", "submitter": "Debankur Mukherjee", "authors": "Mark van der Boor, Sem C. Borst, Johan S.H. van Leeuwaarden, and\n  Debankur Mukherjee", "title": "Scalable Load Balancing in Networked Systems: Universality Properties\n  and Stochastic Coupling Methods", "comments": "Survey paper. Contribution to the Proceedings of the ICM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview of scalable load balancing algorithms which provide\nfavorable delay performance in large-scale systems, and yet only require\nminimal implementation overhead. Aimed at a broad audience, the paper starts\nwith an introduction to the basic load balancing scenario, consisting of a\nsingle dispatcher where tasks arrive that must immediately be forwarded to one\nof $N$ single-server queues.\n  A popular class of load balancing algorithms are so-called power-of-$d$ or\nJSQ($d$) policies, where an incoming task is assigned to a server with the\nshortest queue among $d$ servers selected uniformly at random. This class\nincludes the Join-the-Shortest-Queue (JSQ) policy as a special case ($d = N$),\nwhich has strong stochastic optimality properties and yields a mean waiting\ntime that vanishes as $N$ grows large for any fixed subcritical load. However,\na nominal implementation of the JSQ policy involves a prohibitive communication\nburden in large-scale deployments. In contrast, a random assignment policy ($d\n= 1$) does not entail any communication overhead, but the mean waiting time\nremains constant as $N$ grows large for any fixed positive load.\n  In order to examine the fundamental trade-off between performance and\nimplementation overhead, we consider an asymptotic regime where $d(N)$ depends\non $N$. We investigate what growth rate of $d(N)$ is required to match the\nperformance of the JSQ policy on fluid and diffusion scale. The results\ndemonstrate that the asymptotics for the JSQ($d(N)$) policy are insensitive to\nthe exact growth rate of $d(N)$, as long as the latter is sufficiently fast,\nimplying that the optimality of the JSQ policy can asymptotically be preserved\nwhile dramatically reducing the communication overhead. We additionally show\nhow the communication overhead can be reduced yet further by the so-called\nJoin-the-Idle-Queue scheme, leveraging memory at the dispatcher.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 16:36:54 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["van der Boor", "Mark", ""], ["Borst", "Sem C.", ""], ["van Leeuwaarden", "Johan S. H.", ""], ["Mukherjee", "Debankur", ""]]}, {"id": "1712.08644", "submitter": "Michael Bechtel", "authors": "Michael G. Bechtel, Elise McEllhiney, Minje Kim, Heechul Yun", "title": "DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car", "comments": "To be published as a conference paper at RTCSA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepPicar, a low-cost deep neural network based autonomous car\nplatform. DeepPicar is a small scale replication of a real self-driving car\ncalled DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN),\nwhich takes images from a front-facing camera as input and produces car\nsteering angles as output. DeepPicar uses the same network architecture---9\nlayers, 27 million connections and 250K parameters---and can drive itself in\nreal-time using a web camera and a Raspberry Pi 3 quad-core platform. Using\nDeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end\ndeep learning based real-time control of autonomous vehicles. We also\nsystematically compare other contemporary embedded computing platforms using\nthe DeepPicar's CNN-based real-time control workload. We find that all tested\nplatforms, including the Pi 3, are capable of supporting the CNN-based\nreal-time control, from 20 Hz up to 100 Hz, depending on hardware platform.\nHowever, we find that shared resource contention remains an important issue\nthat must be considered in applying CNN models on shared memory based embedded\ncomputing platforms; we observe up to 11.6X execution time increase in the CNN\nbased control loop due to shared resource contention. To protect the CNN\nworkload, we also evaluate state-of-the-art cache partitioning and memory\nbandwidth throttling techniques on the Pi 3. We find that cache partitioning is\nineffective, while memory bandwidth throttling is an effective solution.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 22:24:08 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 04:33:17 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 01:08:25 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 02:29:01 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Bechtel", "Michael G.", ""], ["McEllhiney", "Elise", ""], ["Kim", "Minje", ""], ["Yun", "Heechul", ""]]}, {"id": "1712.08738", "submitter": "Waqar Ali", "authors": "Waqar Ali and Heechul Yun", "title": "Protecting real-time GPU kernels on integrated CPU-GPU SoC platforms", "comments": "This paper will be published at ECRTS-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated CPU-GPU architecture provides excellent acceleration capabilities\nfor data parallel applications on embedded platforms while meeting the size,\nweight and power (SWaP) requirements. However, sharing of main memory between\nCPU applications and GPU kernels can severely affect the execution of GPU\nkernels and diminish the performance gain provided by GPU. For example, in the\nNVIDIA Tegra K1 platform which has the integrated CPU-GPU architecture, we\nnoticed that in the worst case scenario, the GPU kernels can suffer as much as\n4X slowdown in the presence of co-running memory intensive CPU applications\ncompared to their solo execution. In this paper, we propose a software\nmechanism, which we call BWLOCK++, to protect the performance of GPU kernels\nfrom co-scheduled memory intensive CPU applications.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 08:44:28 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 20:09:20 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2018 02:19:42 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Ali", "Waqar", ""], ["Yun", "Heechul", ""]]}, {"id": "1712.09431", "submitter": "David Rohr", "authors": "D. Rohr, G. Neskovic, M. Radtke, V. Lindenstruth", "title": "The L-CSC cluster: greenest supercomputer in the world in Green500 list\n  of November 2014", "comments": "4 pages, proceedings to Supercomputing Frontiers conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The L-CSC (Lattice Computer for Scientific Computing) is a general purpose\ncompute cluster built of commodity hardware installed at GSI. Its main\noperational purpose is Lattice QCD (LQCD) calculations for physics simulations.\nQuantum Chromo Dynamics (QCD) is the physical theory describing the strong\nforce, one of the four known fundamental interactions in the universe. L-CSC\nleverages a multi-GPU design accommodating the huge demand of LQCD for memory\nbandwidth. In recent years, heterogeneous clusters with accelerators such as\nGPUs have become more and more powerful while supercomputers in general have\nshown enormous increases in power consumption making electricity costs and\ncooling a significant factor in the total cost of ownership. Using mainly GPUs\nfor processing, L-CSC is very power efficient, and its architecture was\noptimized to provide the greatest possible power efficiency. This paper\npresents the cluster design as well as optimizations to improve the power\nefficiency. It examines the power measurements performed for the Green500 list\nof the most power efficient supercomputers in the world which led to the number\n1 position as the greenest supercomputer in November 2014.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 21:49:40 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Rohr", "D.", ""], ["Neskovic", "G.", ""], ["Radtke", "M.", ""], ["Lindenstruth", "V.", ""]]}, {"id": "1712.09624", "submitter": "Nicolas Le Scouarnec", "authors": "Nicolas Le Scouarnec", "title": "Cuckoo++ Hash Tables: High-Performance Hash Tables for Networking\n  Applications", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are an essential data-structure for numerous networking\napplications (e.g., connection tracking, firewalls, network address\ntranslators). Among these, cuckoo hash tables provide excellent performance by\nallowing lookups to be processed with very few memory accesses (2 to 3 per\nlookup). Yet, for large tables, cuckoo hash tables remain memory bound and each\nmemory access impacts performance. In this paper, we propose algorithmic\nimprovements to cuckoo hash tables allowing to eliminate some unnecessary\nmemory accesses; these changes are conducted without altering the properties of\nthe original cuckoo hash table so that all existing theoretical analysis remain\napplicable. On a single core, our hash table achieves 37M lookups per second\nfor positive lookups (i.e., when the key looked up is present in the table),\nand 60M lookups per second for negative lookups, a 50% improvement over the\nimplementation included into the DPDK. On a 18-core, with mostly positive\nlookups, our implementation achieves 496M lookups per second, a 45% improvement\nover DPDK.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 16:40:15 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Scouarnec", "Nicolas Le", ""]]}]