[{"id": "1602.00586", "submitter": "Mariza Ferro", "authors": "Mariza Ferro, Antonio R. Mury, Bruno Schulze", "title": "A Gain Function for Architectural Decision-Making in Scientific\n  Computing", "comments": "25 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific Computing typically requires large computational needs which have\nbeen addressed with High Performance Distributed Computing. It is essential to\nefficiently deploy a number of complex scientific applications, which have\ndifferent characteristics, and so require distinct computational resources too.\nHowever, in many research laboratories, this high performance architecture is\nnot dedicated. So, the architecture must be shared to execute a set of\nscientific applications, with so many different execution times and relative\nimportance to research. Also, the high performance architectures have different\ncharacteristics and costs. When a new infrastructure has to be acquired to meet\nthe needs of this scenario, the decision-making is hard and complex. In this\nwork, we present a Gain Function as a model of an utility function, with which\nit is possible a decision-making with confidence. With the function is possible\nto evaluate the best architectural option taking into account aspects of\napplications and architectures, including the executions time, cost of\narchitecture, the relative importance of each application and also the relative\nimportance of performance and cost on the final evaluation. This paper presents\nthe Gain Function, examples, and a real case showing their applicabilities.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 16:36:58 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Ferro", "Mariza", ""], ["Mury", "Antonio R.", ""], ["Schulze", "Bruno", ""]]}, {"id": "1602.01732", "submitter": "Ahlem Mifdaoui Ahlem Mifdaoui", "authors": "Ahlem Mifdaoui, Hamdi Ayed", "title": "Buffer-aware Worst Case Timing Analysis of Wormhole Network On Chip", "comments": "ISAE Technical report done during the master Thesis of Hamdi Ayed at\n  ISAE at 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A buffer-aware worst-case timing analysis of wormhole NoC is proposed in this\npaper to integrate the impact of buffer size on the different dependencies\nrelationship between flows, i.e. direct and indirect blocking flows, and\nconsequently the timing performance. First, more accurate definitions of direct\nand indirect blocking flows sets have been introduced to take into account the\nbuffer size impact. Then, the modeling and worst-case timing analysis of\nwormhole NoC have been detailed, based on Network Calculus formalism and the\nnewly defined blocking flows sets. This introduced approach has been\nillustrated in the case of a realistic NoC case study to show the trade off\nbetween latency and buffer size. The comparative analysis of our proposed\nBuffer-aware timing analysis with conventional approaches is conducted and\nnoticeable enhancements in terms of maximum latency have been proved.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 16:27:14 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 16:01:55 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Mifdaoui", "Ahlem", ""], ["Ayed", "Hamdi", ""]]}, {"id": "1602.02159", "submitter": "Yehia Elkhatib PhD", "authors": "Faiza Samreen, Yehia Elkhatib, Matthew Rowe, Gordon S. Blair", "title": "Daleel: Simplifying Cloud Instance Selection Using Machine Learning", "comments": "In the IEEE/IFIP Network Operations and Management Symposium (NOMS),\n  April 2016", "journal-ref": null, "doi": "10.1109/NOMS.2016.7502858", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making in cloud environments is quite challenging due to the\ndiversity in service offerings and pricing models, especially considering that\nthe cloud market is an incredibly fast moving one. In addition, there are no\nhard and fast rules, each customer has a specific set of constraints (e.g.\nbudget) and application requirements (e.g. minimum computational resources).\nMachine learning can help address some of the complicated decisions by carrying\nout customer-specific analytics to determine the most suitable instance type(s)\nand the most opportune time for starting or migrating instances. We employ\nmachine learning techniques to develop an adaptive deployment policy, providing\nan optimal match between the customer demands and the available cloud service\nofferings. We provide an experimental study based on extensive set of job\nexecutions over a major public cloud infrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 21:00:02 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Samreen", "Faiza", ""], ["Elkhatib", "Yehia", ""], ["Rowe", "Matthew", ""], ["Blair", "Gordon S.", ""]]}, {"id": "1602.02517", "submitter": "Jose Nunez-Yanez Dr", "authors": "Jose Nunez-Yanez, Tom Sun", "title": "Energy Efficient Video Fusion with Heterogeneous CPU-FPGA Devices", "comments": "Presented at HIP3ES, 2016", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2016/3", "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a complete video fusion system with hardware acceleration\nand investigates the energy trade-offs between computing in the CPU or the FPGA\ndevice. The video fusion application is based on the Dual-Tree Complex Wavelet\nTransforms (DT-CWT). In this work the transforms are mapped to a hardware\naccelerator using high-level synthesis tools for the FPGA and also vectorized\ncode for the single instruction multiple data (SIMD) engine available in the\nCPU. The accelerated system reduces computation time and energy by a factor of\n2. Moreover, the results show a key finding that the FPGA is not always the\nbest choice for acceleration, and the SIMD engine should be selected when the\nwavelet decomposition reduces the frame size below a certain threshold. This\ndependency on workload size means that an adaptive system that intelligently\nselects between the SIMD engine and the FPGA achieves the most energy and\nperformance efficiency point.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 10:28:44 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Nunez-Yanez", "Jose", ""], ["Sun", "Tom", ""]]}, {"id": "1602.04183", "submitter": "Ardavan Pedram", "authors": "Ardavan Pedram, Stephen Richardson, Sameh Galal, Shahar Kvatinsky, and\n  Mark A. Horowitz", "title": "Dark Memory and Accelerator-Rich System Optimization in the Dark Silicon\n  Era", "comments": "8 pages, To appear in IEEE Design and Test Journal", "journal-ref": "IEEE Design & Test ( Volume: 34, Issue: 2, April 2017 )", "doi": "10.1109/MDAT.2016.2573586", "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge to improving performance in the age of Dark Silicon is how\nto leverage transistors when they cannot all be used at the same time. In\nmodern SOCs, these transistors are often used to create specialized\naccelerators which improve energy efficiency for some applications by 10-1000X.\nWhile this might seem like the magic bullet we need, for most CPU applications\nmore energy is dissipated in the memory system than in the processor: these\nlarge gains in efficiency are only possible if the DRAM and memory hierarchy\nare mostly idle. We refer to this desirable state as Dark Memory, and it only\noccurs for applications with an extreme form of locality.\n  To show our findings, we introduce Pareto curves in the energy/op and\nmm$^2$/(ops/s) metric space for compute units, accelerators, and on-chip\nmemory/interconnect. These Pareto curves allow us to solve the power,\nperformance, area constrained optimization problem to determine which\naccelerators should be used, and how to set their design parameters to optimize\nthe system. This analysis shows that memory accesses create a floor to the\nachievable energy-per-op. Thus high performance requires Dark Memory, which in\nturn requires co-design of the algorithm for parallelism and locality, with the\nhardware.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 19:48:31 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 20:06:16 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 00:49:56 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Pedram", "Ardavan", ""], ["Richardson", "Stephen", ""], ["Galal", "Sameh", ""], ["Kvatinsky", "Shahar", ""], ["Horowitz", "Mark A.", ""]]}, {"id": "1602.04873", "submitter": "Hannah Morgan", "authors": "Hannah Morgan, Matthew G. Knepley, Patrick Sanan, L. Ridgway Scott", "title": "A Stochastic Performance Model for Pipelined Krylov Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pipelined Krylov methods seek to ameliorate the latency due to inner products\nnecessary for projection by overlapping it with the computation associated with\nsparse matrix-vector multiplication. We clarify a folk theorem that this can\nonly result in a speedup of $2\\times$ over the naive implementation. Examining\nmany repeated runs, we show that stochastic noise also contributes to the\nlatency, and we model this using an analytical probability distribution. Our\nanalysis shows that speedups greater than $2\\times$ are possible with these\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 00:39:33 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Morgan", "Hannah", ""], ["Knepley", "Matthew G.", ""], ["Sanan", "Patrick", ""], ["Scott", "L. Ridgway", ""]]}, {"id": "1602.06589", "submitter": "Edoardo Di Napoli", "authors": "Edoardo Di Napoli (1 and 4), Elmar Peise (2), Markus Hrywniak (3),\n  Paolo Bientinesi (2) ((1) J\\\"ulich Supercomputing Centre, (2) AICES, RWTH\n  Aachen University, (3) GRS, RWTH Aachen University, (4) J\\\"ulich Aachen\n  Research Alliance -- High-performance Computing)", "title": "High-performance generation of the Hamiltonian and Overlap matrices in\n  FLAPW methods", "comments": "Second revised version. Corrected notation. Added acknowledgment. 30\n  pages, 2 figures and two tables. Submitted to a Special Issue of Computer\n  Physics Communication", "journal-ref": null, "doi": "10.1016/j.cpc.2016.10.003", "report-no": null, "categories": "cs.CE cs.DS cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the greatest efforts of computational scientists is to translate the\nmathematical model describing a class of physical phenomena into large and\ncomplex codes. Many of these codes face the difficulty of implementing the\nmathematical operations in the model in terms of low level optimized kernels\noffering both performance and portability. Legacy codes suffer from the\nadditional curse of rigid design choices based on outdated performance metrics\n(e.g. minimization of memory footprint). Using a representative code from the\nMaterials Science community, we propose a methodology to restructure the most\nexpensive operations in terms of an optimized combination of dense linear\nalgebra kernels. The resulting algorithm guarantees an increased performance\nand an extended life span of this code enabling larger scale simulations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 22:09:30 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 08:24:02 GMT"}, {"version": "v3", "created": "Mon, 15 Aug 2016 19:40:16 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Di Napoli", "Edoardo", "", "1 and 4"], ["Peise", "Elmar", ""], ["Hrywniak", "Markus", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1602.06763", "submitter": "Elmar Peise", "authors": "Elmar Peise (1), Paolo Bientinesi (1) ((1) AICES, RWTH Aachen)", "title": "Recursive Algorithms for Dense Linear Algebra: The ReLAPACK Collection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To exploit both memory locality and the full performance potential of highly\ntuned kernels, dense linear algebra libraries such as LAPACK commonly implement\noperations as blocked algorithms. However, to achieve next-to-optimal\nperformance with such algorithms, significant tuning is required. On the other\nhand, recursive algorithms are virtually tuning free, and yet attain similar\nperformance. In this paper, we first analyze and compare blocked and recursive\nalgorithms in terms of performance, and then introduce ReLAPACK, an open-source\nlibrary of recursive algorithms to seamlessly replace most of LAPACK's blocked\nalgorithms. In many scenarios, ReLAPACK clearly outperforms reference LAPACK,\nand even improves upon the performance of optimizes libraries.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 13:21:05 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Peise", "Elmar", "", "AICES, RWTH Aachen"], ["Bientinesi", "Paolo", "", "AICES, RWTH Aachen"]]}, {"id": "1602.07104", "submitter": "Mehmet Karaca", "authors": "Mehmet Karaca, Saeed Bastani, Basuki Endah Priyanto, Mohammadhassan\n  Safavi and Bj\\\"orn Landfeldt", "title": "Resource Management for OFDMA based Next Generation 802.11ax WLANs", "comments": "accepted to Wireless and Mobile Networking Conference (WMNC), 2016\n  9th IFIP", "journal-ref": null, "doi": "10.1109/WMNC.2016.7543930", "report-no": null, "categories": "cs.NI cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, IEEE 802.11ax Task Group has adapted OFDMA as a new technique for\nenabling multi-user transmission. It has been also decided that the scheduling\nduration should be same for all the users in a multi-user OFDMA so that the\ntransmission of the users should end at the same time. In order to realize that\ncondition, the users with insufficient data should transmit null data (i.e.\npadding) to fill the duration. While this scheme offers strong features such as\nresilience to Overlapping Basic Service Set (OBSS) interference and ease of\nsynchronization, it also poses major side issues of degraded throughput\nperformance and waste of devices' energy. In this work, for OFDMA based 802.11\nWLANs we first propose practical algorithm in which the scheduling duration is\nfixed and does not change from time to time. In the second algorithm the\nscheduling duration is dynamically determined in a resource allocation\nframework by taking into account the padding overhead, airtime fairness and\nenergy consumption of the users. We analytically investigate our resource\nallocation problems through Lyapunov optimization techniques and show that our\nalgorithms are arbitrarily close to the optimal performance at the price of\nreduced convergence rate. We also calculate the overhead of our algorithms in a\nrealistic set-up and propose solutions for the implementation issues.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 09:53:13 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 08:52:26 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 09:50:41 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Karaca", "Mehmet", ""], ["Bastani", "Saeed", ""], ["Priyanto", "Basuki Endah", ""], ["Safavi", "Mohammadhassan", ""], ["Landfeldt", "Bj\u00f6rn", ""]]}, {"id": "1602.07623", "submitter": "Anastasios Giovanidis", "authors": "Anastasios Giovanidis and Apostolos Avranas", "title": "Spatial multi-LRU Caching for Wireless Networks with Coverage Overlaps", "comments": "14 pages, 2-column, 10 Figures (13 sub-figures in all), part of the\n  results in ACM SIGMETRICS/IFIP Performance 2016, Antibes, France", "journal-ref": null, "doi": "10.1145/2896377.2901483", "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a novel family of decentralised caching policies,\napplicable to wireless networks with finite storage at the edge-nodes\n(stations). These policies are based on the Least-Recently-Used replacement\nprinciple, and are, here, referred to as spatial multi-LRU. Based on these,\ncache inventories are updated in a way that provides content diversity to users\nwho are covered by, and thus have access to, more than one station. Two\nvariations are proposed, namely the multi-LRU-One and -All, which differ in the\nnumber of replicas inserted in the involved caches. By introducing spatial\napproximations, we propose a Che-like method to predict the hit probability,\nwhich gives very accurate results under the Independent Reference Model (IRM).\nIt is shown that the performance of multi-LRU increases the more the\nmulti-coverage areas increase, and it approaches the performance of other\nproposed centralised policies, when multi-coverage is sufficient. For IRM\ntraffic multi-LRU-One outperforms multi-LRU-All, whereas when the traffic\nexhibits temporal locality the -All variation can perform better.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 18:08:43 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Giovanidis", "Anastasios", ""], ["Avranas", "Apostolos", ""]]}, {"id": "1602.07978", "submitter": "Felix Poloczek", "authors": "Felix Poloczek, Florin Ciucu", "title": "Contrasting Effects of Replication in Parallel Systems: From Overload to\n  Underload and Back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task replication has recently been advocated as a practical solution to\nreduce latencies in parallel systems. In addition to several convincing\nempirical studies, some others provide analytical results, yet under some\nstrong assumptions such as Poisson arrivals, exponential service times, or\nindependent service times of the replicas themselves, which may lend themselves\nto some contrasting and perhaps contriving behavior. For instance, under the\nsecond assumption, an overloaded system can be stabilized by a replication\nfactor, but can be sent back in overload through further replication. In turn,\nunder the third assumption, strictly larger stability regions of replication\nsystems do not necessarily imply smaller delays.\n  Motivated by the need to dispense with such common and restricting\nassumptions, which may additionally cause unexpected behavior, we develop a\nunified and general theoretical framework to compute tight bounds on the\ndistribution of response times in general replication systems. These results\nimmediately lend themselves to the optimal number of replicas minimizing\nresponse time quantiles, depending on the parameters of the system (e.g., the\ndegree of correlation amongst replicas). As a concrete application of our\nframework, we design a novel replication policy which can improve the stability\nregion of classical fork-join queueing systems by $\\mathcal{O}(\\ln K)$, in the\nnumber of servers $K$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 16:13:23 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Poloczek", "Felix", ""], ["Ciucu", "Florin", ""]]}]