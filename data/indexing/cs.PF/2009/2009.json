[{"id": "2009.00304", "submitter": "S\\\"oren Henning", "authors": "S\\\"oren Henning, Wilhelm Hasselbring", "title": "Theodolite: Scalability Benchmarking of Distributed Stream Processing\n  Engines in Microservice Architectures", "comments": "28 pages", "journal-ref": "Big Data Research 25 (2021)", "doi": "10.1016/j.bdr.2021.100209", "report-no": null, "categories": "cs.SE cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed stream processing engines are designed with a focus on\nscalability to process big data volumes in a continuous manner. We present the\nTheodolite method for benchmarking the scalability of distributed stream\nprocessing engines. Core of this method is the definition of use cases that\nmicroservices implementing stream processing have to fulfill. For each use\ncase, our method identifies relevant workload dimensions that might affect the\nscalability of a use case. We propose to design one benchmark per use case and\nrelevant workload dimension. We present a general benchmarking framework, which\ncan be applied to execute the individual benchmarks for a given use case and\nworkload dimension. Our framework executes an implementation of the use case's\ndataflow architecture for different workloads of the given dimension and\nvarious numbers of processing instances. This way, it identifies how resources\ndemand evolves with increasing workloads. Within the scope of this paper, we\npresent 4 identified use cases, derived from processing Industrial Internet of\nThings data, and 7 corresponding workload dimensions. We provide\nimplementations of 4 benchmarks with Kafka Streams and Apache Flink as well as\nan implementation of our benchmarking framework to execute scalability\nbenchmarks in cloud environments. We use both for evaluating the Theodolite\nmethod and for benchmarking Kafka Streams' and Flink's scalability for\ndifferent deployment options.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 09:13:16 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 14:45:35 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 14:30:56 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Henning", "S\u00f6ren", ""], ["Hasselbring", "Wilhelm", ""]]}, {"id": "2009.00804", "submitter": "Zhihui Zhang", "authors": "Zhihui Zhang, Jingwen Leng, Lingxiao Ma, Youshan Miao, Chao Li, Minyi\n  Guo", "title": "Architectural Implications of Graph Neural Networks", "comments": "4 pages, published in IEEE Computer Architecture Letters (CAL) 2020", "journal-ref": "in IEEE Computer Architecture Letters, vol. 19, no. 1, pp. 59-62,\n  1 Jan.-June 2020", "doi": "10.1109/LCA.2020.2988991", "report-no": null, "categories": "cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNN) represent an emerging line of deep learning\nmodels that operate on graph structures. It is becoming more and more popular\ndue to its high accuracy achieved in many graph-related tasks. However, GNN is\nnot as well understood in the system and architecture community as its\ncounterparts such as multi-layer perceptrons and convolutional neural networks.\nThis work tries to introduce the GNN to our community. In contrast to prior\nwork that only presents characterizations of GCNs, our work covers a large\nportion of the varieties for GNN workloads based on a general GNN description\nframework. By constructing the models on top of two widely-used libraries, we\ncharacterize the GNN computation at inference stage concerning general-purpose\nand application-specific architectures and hope our work can foster more system\nand architecture research for GNNs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 03:36:24 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Zhang", "Zhihui", ""], ["Leng", "Jingwen", ""], ["Ma", "Lingxiao", ""], ["Miao", "Youshan", ""], ["Li", "Chao", ""], ["Guo", "Minyi", ""]]}, {"id": "2009.01598", "submitter": "Emina Soljanin", "authors": "Mehmet Aktas, Gauri Joshi, Swanand Kadhe, Fatemeh Kazemi, and Emina\n  Soljanin", "title": "Service Rate Region: A New Aspect of Coded Distributed System Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Erasure coding has been recognized as a powerful method to mitigate delays\ndue to slow or straggling nodes in distributed systems. This work shows that\nerasure coding of data objects can flexibly handle skews in the request rates.\nCoding can help boost the \\emph{service rate region}, that is, increase the\noverall volume of data access requests that the system can handle. This paper\naims to postulate the service rate region as an important consideration in the\ndesign of erasure-coded distributed systems. We highlight several open problems\nthat can be grouped into two broad threads: 1) characterizing the service rate\nregion of a given code and finding the optimal request allocation, and2)\ndesigning the underlying erasure code for a given service rate region. As\ncontributions along the first thread, we characterize the rate regions of\nmaximum-distance-separable, locally repairable, and Simplex codes. We show the\neffectiveness of hybrid codes that combine replication and erasure coding in\nterms of code design. We also discover fundamental connections between\nmulti-set batch codes and the problem of maximizing the service rate region.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 12:08:51 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 10:39:12 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Aktas", "Mehmet", ""], ["Joshi", "Gauri", ""], ["Kadhe", "Swanand", ""], ["Kazemi", "Fatemeh", ""], ["Soljanin", "Emina", ""]]}, {"id": "2009.01659", "submitter": "Dethie Dione", "authors": "Bakary Kone, Salimata Gueye Diagne, Dethie Dione, Coumba Diallo", "title": "Analysis of an M/G/1 system for the optimization of the RTG performances\n  in the delivery of containers in Abidjan Terminal", "comments": "15 pages, 3 figures", "journal-ref": "IJAAMM (2016)", "doi": null, "report-no": "17BK", "categories": "cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In front of the major challenges to increase its productivity while\nsatisfying its customer, it is today important to establish in advance the\noperational performances of the RTG Abidjan Terminal. In this article, by using\nan M/G/1 retrial queue system, we obtained the average number of parked\ndelivery trucks and as well as their waiting time. Finally, we used Matlab to\nrepresent them graphically then analyze the RTG performances according to the\ntraffic rate.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 22:09:18 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Kone", "Bakary", ""], ["Diagne", "Salimata Gueye", ""], ["Dione", "Dethie", ""], ["Diallo", "Coumba", ""]]}, {"id": "2009.01692", "submitter": "Yuyang Jin", "authors": "Yuyang Jin and Haojie Wang and Teng Yu and Xiongchao Tang and Torsten\n  Hoefler and Xu Liu and Jidong Zhai", "title": "ScalAna: Automating Scaling Loss Detection with Graph Analysis", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling a parallel program to modern supercomputers is challenging due to\ninter-process communication, Amdahl's law, and resource contention. Performance\nanalysis tools for finding such scaling bottlenecks either base on profiling or\ntracing. Profiling incurs low overheads but does not capture detailed\ndependencies needed for root-cause analysis. Tracing collects all information\nat prohibitive overheads. In this work, we design ScalAna that uses static\nanalysis techniques to achieve the best of both worlds - it enables the\nanalyzability of traces at a cost similar to profiling. ScalAna first leverages\nstatic compiler techniques to build a Program Structure Graph, which records\nthe main computation and communication patterns as well as the program's\ncontrol structures. At runtime, we adopt lightweight techniques to collect\nperformance data according to the graph structure and generate a Program\nPerformance Graph. With this graph, we propose a novel approach, called\nbacktracking root cause detection, which can automatically and efficiently\ndetect the root cause of scaling loss. We evaluate ScalAna with real\napplications. Results show that our approach can effectively locate the root\ncause of scaling loss for real applications and incurs 1.73% overhead on\naverage for up to 2,048 processes. We achieve up to 11.11% performance\nimprovement by fixing the root causes detected by ScalAna on 2,048 processes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 14:15:19 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Jin", "Yuyang", ""], ["Wang", "Haojie", ""], ["Yu", "Teng", ""], ["Tang", "Xiongchao", ""], ["Hoefler", "Torsten", ""], ["Liu", "Xu", ""], ["Zhai", "Jidong", ""]]}, {"id": "2009.02449", "submitter": "Charlene Yang", "authors": "Charlene Yang", "title": "Hierarchical Roofline Analysis: How to Collect Data using Performance\n  Tools on Intel CPUs and NVIDIA GPUs", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys a range of methods to collect necessary performance data\non Intel CPUs and NVIDIA GPUs for hierarchical Roofline analysis. As of\nmid-2020, two vendor performance tools, Intel Advisor and NVIDIA Nsight\nCompute, have integrated Roofline analysis into their supported feature set.\nThis paper fills the gap for when these tools are not available, or when users\nwould like a more customized workflow for certain analysis. Specifically, we\nwill discuss how to use Intel Advisor, RRZE LIKWID, Intel SDE and Intel\nAmplifier on Intel architectures, and nvprof, Nsight Compute metrics, and\nNsight Compute section files on NVIDIA architectures. These tools will be used\nto collect information for as many memory/cache levels in the memory hierarchy\nas possible in order to provide insights into application's data reuse and\ncache locality characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 03:14:42 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 05:27:51 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 20:23:56 GMT"}, {"version": "v4", "created": "Sun, 4 Oct 2020 17:04:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yang", "Charlene", ""]]}, {"id": "2009.02995", "submitter": "Markus Iser", "authors": "Markus Iser, Luca Springer, Carsten Sinz", "title": "Collaborative Management of Benchmark Instances and their Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental evaluation is an integral part in the design process of\nalgorithms. Publicly available benchmark instances are widely used to evaluate\nmethods in SAT solving. For the interpretation of results and the design of\nalgorithm portfolios their attributes are crucial. Capturing the interrelation\nof benchmark instances and their attributes is considerably simplified through\nour specification of a benchmark instance identifier. Thus, our tool increases\nthe availability of both by providing means to manage and retrieve benchmark\ninstances by their attributes and vice versa. Like this, it facilitates the\ndesign and analysis of SAT experiments and the exchange of results.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:23:08 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Iser", "Markus", ""], ["Springer", "Luca", ""], ["Sinz", "Carsten", ""]]}, {"id": "2009.03715", "submitter": "Amir Mirzaeinia", "authors": "Amir Mirzaeinnia, Mehdi Mirzaeinia, Abdelmounaam Rezgui", "title": "Latency and Throughput Optimization in Modern Networks: A Comprehensive\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern applications are highly sensitive to communication delays and\nthroughput. This paper surveys major attempts on reducing latency and\nincreasing the throughput. These methods are surveyed on different networks and\nsurroundings such as wired networks, wireless networks, application layer\ntransport control, Remote Direct Memory Access, and machine learning based\ntransport control.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 15:17:26 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Mirzaeinnia", "Amir", ""], ["Mirzaeinia", "Mehdi", ""], ["Rezgui", "Abdelmounaam", ""]]}, {"id": "2009.04061", "submitter": "Keren Zhou", "authors": "Keren Zhou, Xiaozhu Meng, Ryuichi Sai, John Mellor-Crummey", "title": "GPA: A GPU Performance Advisor Based on Instruction Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient GPU kernels can be difficult because of the complexity\nof GPU architectures and programming models. Existing performance tools only\nprovide coarse-grained suggestions at the kernel level, if any. In this paper,\nwe describe GPA, a performance advisor for NVIDIA GPUs that suggests potential\ncode optimization opportunities at a hierarchy of levels, including individual\nlines, loops, and functions. To relieve users of the burden of interpreting\nperformance counters and analyzing bottlenecks, GPA uses data flow analysis to\napproximately attribute measured instruction stalls to their root causes and\nuses information about a program's structure and the GPU to match inefficiency\npatterns with suggestions for optimization. To quantify each suggestion's\npotential benefits, we developed PC sampling-based performance models to\nestimate its speedup. Our experiments with benchmarks and applications show\nthat GPA provides an insightful report to guide performance optimization. Using\nGPA, we obtained speedups on a Volta V100 GPU ranging from 1.01$\\times$ to\n3.53$\\times$, with a geometric mean of 1.22$\\times$.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 01:35:08 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 17:05:59 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Zhou", "Keren", ""], ["Meng", "Xiaozhu", ""], ["Sai", "Ryuichi", ""], ["Mellor-Crummey", "John", ""]]}, {"id": "2009.04598", "submitter": "Charlene Yang", "authors": "Yunsong Wang, Charlene Yang, Steven Farrell, Yan Zhang, Thorsten\n  Kurth, Samuel Williams", "title": "Time-Based Roofline for Deep Learning Performance Analysis", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning applications are usually very compute-intensive and require a\nlong run time for training and inference. This has been tackled by researchers\nfrom both hardware and software sides, and in this paper, we propose a\nRoofline-based approach to performance analysis to facilitate the optimization\nof these applications. This approach is an extension of the Roofline model\nwidely used in traditional high-performance computing applications, and it\nincorporates both compute/bandwidth complexity and run time in its formulae to\nprovide insights into deep learning-specific characteristics. We take two sets\nof representative kernels, 2D convolution and long short-term memory, to\nvalidate and demonstrate the use of this new approach, and investigate how\narithmetic intensity, cache locality, auto-tuning, kernel launch overhead, and\nTensor Core usage can affect performance. Compared to the common ad-hoc\napproach, this study helps form a more systematic way to analyze code\nperformance and identify optimization opportunities for deep learning\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 23:29:04 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 07:11:36 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 21:51:45 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Wang", "Yunsong", ""], ["Yang", "Charlene", ""], ["Farrell", "Steven", ""], ["Zhang", "Yan", ""], ["Kurth", "Thorsten", ""], ["Williams", "Samuel", ""]]}, {"id": "2009.05257", "submitter": "Charlene Yang", "authors": "Charlene Yang, Yunsong Wang, Steven Farrell, Thorsten Kurth, Samuel\n  Williams", "title": "Hierarchical Roofline Performance Analysis for Deep Learning\n  Applications", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a practical methodology for collecting performance data\nnecessary to conduct hierarchical Roofline analysis on NVIDIA GPUs. It\ndiscusses the extension of the Empirical Roofline Toolkit for broader support\nof a range of data precisions and Tensor Core support and introduces a Nsight\nCompute based method to accurately collect application performance information.\nThis methodology allows for automated machine characterization and application\ncharacterization for Roofline analysis across the entire memory hierarchy on\nNVIDIA GPUs, and it is validated by a complex deep learning application used\nfor climate image segmentation. We use two versions of the code, in TensorFlow\nand PyTorch respectively, to demonstrate the use and effectiveness of this\nmethodology. We highlight how the application utilizes the compute and memory\ncapabilities on the GPU and how the implementation and performance differ in\ntwo deep learning frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 07:16:55 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 07:14:16 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 21:57:58 GMT"}, {"version": "v4", "created": "Wed, 25 Nov 2020 02:52:41 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Yang", "Charlene", ""], ["Wang", "Yunsong", ""], ["Farrell", "Steven", ""], ["Kurth", "Thorsten", ""], ["Williams", "Samuel", ""]]}, {"id": "2009.05314", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "Repeated Recursion Unfolding for Super-Linear Speedup within Bounds", "comments": "This is the full version of a paper presented at the 30th\n  International Symposium on Logic-Based Program Synthesis and Transformation\n  (LOPSTR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeated recursion unfolding is a new approach that repeatedly unfolds a\nrecursion with itself and simplifies it while keeping all unfolded rules. Each\nunfolding doubles the number of recursive steps covered. This reduces the\nnumber of recursive rule applications to its logarithm at the expense of\nintroducing a logarithmic number of unfolded rules to the program. Efficiency\ncrucially depends on the amount of simplification inside the unfolded rules. We\nprove a super-linear speedup theorem in the best case, i.e. speedup by more\nthan a constant factor. Our optimization can lower the time complexity class of\na program. In this paper, the super-linear speedup is within bounds: it holds\nup to an arbitrary but chosen upper bound on the number of recursive steps. We\nalso report on the first results with a prototype implementation of repeated\nrecursion unfolding. A simple program transformation completely removes\nrecursion up to the chosen bound. The actual runtime improvement quickly\nreaches several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 09:59:00 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "2009.05724", "submitter": "Andreas Festag", "authors": "Anupama Hegde and Andreas Festag", "title": "Artery-C -- An OMNeT++ Based Discrete Event Simulation Framework for\n  Cellular V2X", "comments": "12 pages", "journal-ref": null, "doi": "10.1145/3416010.3423240", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cellular Vehicle-to-X (Cellular V2X) is a communication technology that aims\nto facilitate the communication among vehicles and with the roadside\ninfrastructure. Introduced with LTE Release 14, Cellular V2X enables\ndevice-to-device communication to support road safety and traffic efficiency\napplications. We present Artery-C, a simulation framework for the performance\nevaluation of Cellular V2X protocols and V2X applications. Our simulator relies\non the simulation framework SimuLTE and substantially extends it by\nimplementing control and user planes. Besides the vehicle-to-network\ncommunication via the up-/downlink interface, it provides vehicle-to-vehicle\nand vehicle-infrastructure communication via the sidelink interface using the\nmanaged and the unmanaged mode of Cellular V2X (mode 3 and 4, respectively).\nThe simulator also implements advanced features of 5G mobile networks, such as\nvariable numerologies. For the transmission of of V2X messages, it adds a\nnon-IP interface. Artery-C integrates seamlessly into the simulation framework\nArtery, which enables the simulation of standardized V2X messages at the\nfacilities layer as well as the coupling to the mobility simulator SUMO. A\nspecific feature of Artery-C is the support of dynamic switching between all\nmodes of Cellular V2X. In order to demonstrate the capabilities of Artery-C, we\nevaluate V2X-based platooning as a representative use case and present results\nfor mode 3, mode 4 and mode switching in a highway scenario.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 04:13:55 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hegde", "Anupama", ""], ["Festag", "Andreas", ""]]}, {"id": "2009.06009", "submitter": "Karel Ad\\'amek", "authors": "Karel Ad\\'amek, Jan Novotn\\'y, Jeyarajan Thiyagalingam, Wesley Armour", "title": "Efficiency Near the Edge: Increasing the Energy Efficiency of FFTs on\n  GPUs for Real-time Edge Computing", "comments": "Submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Square Kilometre Array (SKA) is an international initiative for\ndeveloping the world's largest radio telescope with a total collecting area of\nover a million square meters. The scale of the operation, combined with the\nremote location of the telescope, requires the use of energy-efficient\ncomputational algorithms. This, along with the extreme data rates that will be\nproduced by the SKA and the requirement for a real-time observing capability,\nnecessitates in-situ data processing in an edge style computing solution. More\ngenerally, energy efficiency in the modern computing landscape is becoming of\nparamount concern. Whether it be the power budget that can limit some of the\nworld's largest supercomputers, or the limited power available to the smallest\nInternet-of-Things devices. In this paper, we study the impact of hardware\nfrequency scaling on the energy consumption and execution time of the Fast\nFourier Transform (FFT) on NVIDIA GPUs using the cuFFT library. The FFT is used\nin many areas of science and it is one of the key algorithms used in radio\nastronomy data processing pipelines. Through the use of frequency scaling, we\nshow that we can lower the power consumption of the NVIDIA V100 GPU when\ncomputing the FFT by up to 60% compared to the boost clock frequency, with less\nthan a 10% increase in the execution time. Furthermore, using one common core\nclock frequency for all tested FFT lengths, we show on average a 50% reduction\nin power consumption compared to the boost core clock frequency with an\nincrease in the execution time still below 10%. We demonstrate how these\nresults can be used to lower the power consumption of existing data processing\npipelines. These savings, when considered over years of operation, can yield\nsignificant financial savings, but can also lead to a significant reduction of\ngreenhouse gas emissions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 14:48:16 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ad\u00e1mek", "Karel", ""], ["Novotn\u00fd", "Jan", ""], ["Thiyagalingam", "Jeyarajan", ""], ["Armour", "Wesley", ""]]}, {"id": "2009.07400", "submitter": "Rafael Ravedutti Lucio Machado", "authors": "Rafael Ravedutti L. Machado (1), Jonas Schmitt (1), Sebastian Eibl\n  (1), Jan Eitzinger (2), Roland Lei{\\ss}a (3), Sebastian Hack (3), Ars\\`ene\n  P\\'erard-Gayot (3), Richard Membarth (3 and 4) and Harald K\\\"ostler (1) ((1)\n  Chair for System Simulation at University of Erlangen-N\\\"urnberg, (2)\n  Regional Computer Center Erlangen at University of Erlangen-N\\\"urnberg, (3)\n  Saarland Informatics Campus at Saarland University, (4) German Research\n  Center for Artificial Intelligence at Saarland Informatics Campus)", "title": "tinyMD: A Portable and Scalable Implementation for Pairwise Interactions\n  Simulations", "comments": "35 pages, 8 figures, submitted to Journal of Computational Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.PL physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the suitability of the AnyDSL partial evaluation\nframework to implement tinyMD: an efficient, scalable, and portable simulation\nof pairwise interactions among particles. We compare tinyMD with the miniMD\nproxy application that scales very well on parallel supercomputers. We discuss\nthe differences between both implementations and contrast miniMD's performance\nfor single-node CPU and GPU targets, as well as its scalability on SuperMUC-NG\nand Piz Daint supercomputers. Additionaly, we demonstrate tinyMD's flexibility\nby coupling it with the waLBerla multi-physics framework. This allow us to\nexecute tinyMD simulations using the load-balancing mechanism implemented in\nwaLBerla.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 00:29:13 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Machado", "Rafael Ravedutti L.", "", "3 and 4"], ["Schmitt", "Jonas", "", "3 and 4"], ["Eibl", "Sebastian", "", "3 and 4"], ["Eitzinger", "Jan", "", "3 and 4"], ["Lei\u00dfa", "Roland", "", "3 and 4"], ["Hack", "Sebastian", "", "3 and 4"], ["P\u00e9rard-Gayot", "Ars\u00e8ne", "", "3 and 4"], ["Membarth", "Richard", "", "3 and 4"], ["K\u00f6stler", "Harald", ""]]}, {"id": "2009.07929", "submitter": "Mark Blanco", "authors": "Mark Blanco, Tze Meng Low, Kyungjoo Kim", "title": "Exploration of Fine-Grained Parallelism for Load Balancing Eager K-truss\n  on GPU and CPU", "comments": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916473", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a performance exploration on Eager K-truss, a\nlinear-algebraic formulation of the K-truss graph algorithm. We address\nperformance issues related to load imbalance of parallel tasks in symmetric,\ntriangular graphs by presenting a fine-grained parallel approach to executing\nthe support computation. This approach also increases available parallelism,\nmaking it amenable to GPU execution. We demonstrate our fine-grained parallel\napproach using implementations in Kokkos and evaluate them on an Intel Skylake\nCPU and an Nvidia Tesla V100 GPU. Overall, we observe between a 1.261. 48x\nimprovement on the CPU and a 9.97-16.92x improvement on the GPU due to our\nfine-grained parallel formulation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 20:45:04 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Blanco", "Mark", ""], ["Low", "Tze Meng", ""], ["Kim", "Kyungjoo", ""]]}, {"id": "2009.07935", "submitter": "Mark Blanco", "authors": "Mark P. Blanco, Scott McMillan, Tze Meng Low", "title": "Towards an Objective Metric for the Performance of Exact Triangle Count", "comments": "6 Pages, 2020 IEEE High Performance Extreme Computing\n  Conference(HPEC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of graph algorithms is often measured in terms of the number\nof traversed edges per second (TEPS). However, this performance metric is\ninadequate for a graph operation such as exact triangle counting. In triangle\ncounting, execution times on graphs with a similar number of edges can be\ndistinctly different as demonstrated by results from the past Graph Challenge\nentries. We discuss the need for an objective performance metric for graph\noperations and the desired characteristics of such a metric such that it more\naccurately captures the interactions between the amount of work performed and\nthe capabilities of the hardware on which the code is executed. Using exact\ntriangle counting as an example, we derive a metric that captures how certain\ntechniques employed in many implementations improve performance. We demonstrate\nthat our proposed metric can be used to evaluate and compare multiple\napproaches for triangle counting, using a SIMD approach as a case study against\na scalar baseline.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 21:08:27 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 14:00:14 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Blanco", "Mark P.", ""], ["McMillan", "Scott", ""], ["Low", "Tze Meng", ""]]}, {"id": "2009.08228", "submitter": "Abhishek  Sinha", "authors": "Debjit Paria, Abhishek Sinha", "title": "LeadCache: Regret-Optimal Caching in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NI cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a set-valued online prediction problem in the context of network\ncaching. Assume that multiple users are connected to several caches via a\nbipartite network. At any time slot, each user requests an arbitrary file\nchosen from a large catalog. A user's request at a slot is met if the requested\nfile is cached in at least one of the caches connected to the user. Our\nobjective is to predict, prefetch, and optimally distribute the files on the\ncaches to maximize the total number of cache hits in an online setting. The\nproblem is non-trivial due to the non-convex and non-smooth nature of the\nobjective function. In this paper, we propose $\\texttt{LeadCache}$ - an online\ncaching policy based on the Follow-the-Perturbed-Leader paradigm. We show that\nthe policy is regret-optimal up to a factor of $\\tilde{O}(n^{3/8}),$ where $n$\nis the number of users. We design two efficient implementations of the\n$\\texttt{LeadCache}$ policy, one based on Pipage rounding and the other based\non Madow's sampling, each of which makes precisely one call to an LP-solver per\niteration. With a Strong-Law-type assumption, we show that the total number of\nfile fetches under $\\texttt{LeadCache}$ remains almost surely finite over an\ninfinite horizon. Finally, we derive a tight regret lower bound using results\nfrom graph coloring. We conclude that the learning-based $\\texttt{LeadCache}$\npolicy decisively outperforms the known caching policies both theoretically and\nempirically.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 12:13:26 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 19:26:22 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 17:05:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Paria", "Debjit", ""], ["Sinha", "Abhishek", ""]]}, {"id": "2009.09433", "submitter": "Sounak Kar", "authors": "Sounak Kar, Robin Rehrmann, Arpan Mukhopadhyay, Bastian Alt, Florin\n  Ciucu, Heinz Koeppl, Carsten Binnig and Amr Rizk", "title": "On the Throughput Optimization in Large-Scale Batch-Processing Systems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a data-processing system with $n$ clients producing jobs which are\nprocessed in \\textit{batches} by $m$ parallel servers; the system throughput\ncritically depends on the batch size and a corresponding sub-additive speedup\nfunction. In practice, throughput optimization relies on numerical searches for\nthe optimal batch size, a process that can take up to multiple days in existing\ncommercial systems. In this paper, we model the system in terms of a closed\nqueueing network; a standard Markovian analysis yields the optimal throughput\nin $\\omega\\left(n^4\\right)$ time. Our main contribution is a mean-field model\nof the system for the regime where the system size is large. We show that the\nmean-field model has a unique, globally attractive stationary point which can\nbe found in closed form and which characterizes the asymptotic throughput of\nthe system as a function of the batch size. Using this expression we find the\n\\textit{asymptotically} optimal throughput in $O(1)$ time. Numerical settings\nfrom a large commercial system reveal that this asymptotic optimum is accurate\nin practical finite regimes.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 14:07:53 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kar", "Sounak", ""], ["Rehrmann", "Robin", ""], ["Mukhopadhyay", "Arpan", ""], ["Alt", "Bastian", ""], ["Ciucu", "Florin", ""], ["Koeppl", "Heinz", ""], ["Binnig", "Carsten", ""], ["Rizk", "Amr", ""]]}, {"id": "2009.10365", "submitter": "Diego Alvarez-Estevez", "authors": "Diego Alvarez-Estevez and Roselyne M. Rijsman", "title": "Inter-database validation of a deep learning approach for automatic\n  sleep scoring", "comments": "Original submission manuscript, 19 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we describe a new deep learning approach for automatic sleep\nstaging, and carry out its validation by addressing its generalization\ncapabilities on a wide range of sleep staging databases. Prediction\ncapabilities are evaluated in the context of independent local and external\ngeneralization scenarios. Effectively, by comparing both procedures it is\npossible to better extrapolate the expected performance of the method on the\ngeneral reference task of sleep staging, regardless of data from a specific\ndatabase. In addition, we examine the suitability of a novel approach based on\nthe use of an ensemble of individual local models and evaluate its impact on\nthe resulting inter-database generalization performance. Validation results\nshow good general performance, as compared to the expected levels of human\nexpert agreement, as well as state-of-the-art automatic sleep staging\napproaches\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 07:46:43 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Alvarez-Estevez", "Diego", ""], ["Rijsman", "Roselyne M.", ""]]}, {"id": "2009.11067", "submitter": "Kiichi Tokuyama", "authors": "Yukang Jiang, Kiichi Tokuyama, Yuichiro Wada, and Moeko Yajima", "title": "Correlation Coefficient Analysis of the Age of Information in\n  Multi-Source Systems", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the age of information (AoI) on an information updating\nsystem such that multiple sources share one server to process packets of\nupdated information. In such systems, packets from different sources compete\nfor the server, and thus they may suffer from being interrupted, being\nbacklogged, and becoming stale. Therefore, in order to grasp structures of such\nsystems, it is crucially important to study a metric indicating a correlation\nof different sources. In this paper, we aim to analyze the correlation of AoIs\non a single-server queueing system with multiple sources. As our contribution,\nwe provide the closed-form expression of the correlation coefficient of the\nAoIs. To this end, we first derive the Laplace-Stieltjes transform of the\nstationary distribution of each AoI for the multiple sources. Some nontrivial\nproperties on the systems are revealed from our analysis results.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 11:40:38 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Jiang", "Yukang", ""], ["Tokuyama", "Kiichi", ""], ["Wada", "Yuichiro", ""], ["Yajima", "Moeko", ""]]}, {"id": "2009.11208", "submitter": "Mohamed Handaoui", "authors": "Mohamed Handaoui and Jean-Emile Dartois and Jalil Boukhobza and\n  Olivier Barais and Laurent d'Orazio", "title": "ReLeaSER: A Reinforcement Learning Strategy for Optimizing Utilization\n  Of Ephemeral Cloud Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud data center capacities are over-provisioned to handle demand peaks and\nhardware failures which leads to low resources' utilization. One way to improve\nresource utilization and thus reduce the total cost of ownership is to offer\nunused resources (referred to as ephemeral resources) at a lower price.\nHowever, reselling resources needs to meet the expectations of its customers in\nterms of Quality of Service. The goal is so to maximize the amount of reclaimed\nresources while avoiding SLA penalties. To achieve that, cloud providers have\nto estimate their future utilization to provide availability guarantees. The\nprediction should consider a safety margin for resources to react to\nunpredictable workloads. The challenge is to find the safety margin that\nprovides the best trade-off between the amount of resources to reclaim and the\nrisk of SLA violations. Most state-of-the-art solutions consider a fixed safety\nmargin for all types of metrics (e.g., CPU, RAM). However, a unique fixed\nmargin does not consider various workloads variations over time which may lead\nto SLA violations or/and poor utilization. In order to tackle these challenges,\nwe propose ReLeaSER, a Reinforcement Learning strategy for optimizing the\nephemeral resources' utilization in the cloud. ReLeaSER dynamically tunes the\nsafety margin at the host-level for each resource metric. The strategy learns\nfrom past prediction errors (that caused SLA violations). Our solution reduces\nsignificantly the SLA violation penalties on average by 2.7x and up to 3.4x. It\nalso improves considerably the CPs' potential savings by 27.6% on average and\nup to 43.6%.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:19:28 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:23:21 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 09:59:20 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2020 10:48:38 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Handaoui", "Mohamed", ""], ["Dartois", "Jean-Emile", ""], ["Boukhobza", "Jalil", ""], ["Barais", "Olivier", ""], ["d'Orazio", "Laurent", ""]]}, {"id": "2009.11224", "submitter": "Jacek Czaja", "authors": "Jacek Czaja, Michal Gallus, Joanna Wozna, Adam Grygielski, Luo Tao", "title": "Applying the Roofline model for Deep Learning performance optimizations", "comments": "oneDNN library analysis with roofline model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper We present a methodology for creating Roofline models\nautomatically for Non-Unified Memory Access (NUMA) using Intel Xeon as an\nexample. Finally, we present an evaluation of highly efficient deep learning\nprimitives as implemented in the Intel oneDNN Library.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:39:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Czaja", "Jacek", ""], ["Gallus", "Michal", ""], ["Wozna", "Joanna", ""], ["Grygielski", "Adam", ""], ["Tao", "Luo", ""]]}, {"id": "2009.11806", "submitter": "Adrian Jackson", "authors": "Adrian Jackson, Mich\\`ele Weiland, Nick Brown, Andrew Turner, Mark\n  Parsons", "title": "Investigating Applications on the A64FX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The A64FX processor from Fujitsu, being designed for computational simulation\nand machine learning applications, has the potential for unprecedented\nperformance in HPC systems. In this paper, we evaluate the A64FX by\nbenchmarking against a range of production HPC platforms that cover a number of\nprocessor technologies. We investigate the performance of complex scientific\napplications across multiple nodes, as well as single node and mini-kernel\nbenchmarks. This paper finds that the performance of the A64FX processor across\nour chosen benchmarks often significantly exceeds other platforms, even without\nspecific application optimisations for the processor instruction set or\nhardware. However, this is not true for all the benchmarks we have undertaken.\nFurthermore, the specific configuration of applications can have an impact on\nthe runtime and performance experienced.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:49:24 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Jackson", "Adrian", ""], ["Weiland", "Mich\u00e8le", ""], ["Brown", "Nick", ""], ["Turner", "Andrew", ""], ["Parsons", "Mark", ""]]}, {"id": "2009.12263", "submitter": "Tim Besard", "authors": "Thomas Faingnaert, Tim Besard, Bjorn De Sutter", "title": "Flexible Performant GEMM Kernels on GPUs", "comments": "This paper was submitted to IEEE TPDS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General Matrix Multiplication or GEMM kernels take center place in high\nperformance computing and machine learning. Recent NVIDIA GPUs include GEMM\naccelerators, such as NVIDIA's Tensor Cores. Their exploitation is hampered by\nthe two-language problem: it requires either low-level programming which\nimplies low programmer productivity or using libraries that only offer a\nlimited set of components. Because rephrasing algorithms in terms of\nestablished components often introduces overhead, the libraries' lack of\nflexibility limits the freedom to explore new algorithms. Researchers using\nGEMMs can hence not enjoy programming productivity, high performance, and\nresearch flexibility at once.\n  In this paper we solve this problem. We present three sets of abstractions\nand interfaces to program GEMMs within the scientific Julia programming\nlanguage. The interfaces and abstractions are co-designed for researchers'\nneeds and Julia's features to achieve sufficient separation of concerns and\nflexibility to easily extend basic GEMMs in many different ways without paying\na performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS\nand CUTLASS, we demonstrate that our performance is mostly on par with, and in\nsome cases even exceeds, the libraries, without having to write a single line\nof code in CUDA C++ or assembly, and without facing flexibility limitations.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 14:29:08 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 12:41:57 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 06:43:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Faingnaert", "Thomas", ""], ["Besard", "Tim", ""], ["De Sutter", "Bjorn", ""]]}, {"id": "2009.12299", "submitter": "Celine Comte", "authors": "C\\'eline Comte, Jan-Pieter Dorsman", "title": "Pass-and-Swap Queues", "comments": "44 pages, 15 figures", "journal-ref": null, "doi": "10.1007/s11134-021-09700-3", "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Order-independent (OI) queues, introduced by Berezner, Kriel, and Krzesinski\nin 1995, expanded the family of multi-class queues that are known to have a\nproduct-form stationary distribution by allowing for intricate class-dependent\nservice rates. This paper further broadens this family by introducing\npass-and-swap (P&S) queues, an extension of OI queues where, upon a service\ncompletion, the customer that completes service is not necessarily the one that\nleaves the system. More precisely, we supplement the OI queue model with an\nundirected graph on the customer classes, which we call a swapping graph, such\nthat there is an edge between two classes if customers of these classes can be\nswapped with one another. When a customer completes service, it passes over\ncustomers in the remainder of the queue until it finds a customer it can swap\npositions with, that is, a customer whose class is a neighbor in the graph. In\nits turn, the customer that is ejected from its position takes the position of\nthe next customer it can be swapped with, and so on. This is repeated until a\ncustomer can no longer find another customer to be swapped with; this customer\nis the one that leaves the queue. After proving that P&S queues have a\nproduct-form stationary distribution, we derive a necessary and sufficient\nstability condition for (open networks of) P&S queues that also applies to OI\nqueues. We then study irreducibility properties of closed networks of P&S\nqueues and derive the corresponding product-form stationary distribution.\nLastly, we demonstrate that closed networks of P&S queues can be applied to\ndescribe the dynamics of new and existing load-distribution and scheduling\nprotocols in clusters of machines in which jobs have assignment constraints.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 15:42:06 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 18:24:51 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 18:56:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Comte", "C\u00e9line", ""], ["Dorsman", "Jan-Pieter", ""]]}, {"id": "2009.12580", "submitter": "Mario Di Mauro", "authors": "Mario Di Mauro, Antonio Liotta", "title": "An experimental evaluation and characterization of VoIP over an LTE-A\n  network", "comments": "Data available at:\n  https://www.researchgate.net/publication/342276809_DATASETS", "journal-ref": "IEEE Transactions on Network and Service Management, vol. 17, no.\n  3, pp. 1626-1639, Sept. 2020", "doi": "10.1109/TNSM.2020.2995505", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile telecommunications are converging towards all-IP solutions. This is\nthe case of the Long Term Evolution (LTE) technology that, having no\ncircuit-switched bearer to support voice traffic, needs a dedicated VoIP\ninfrastructure, which often relies on the IP Multimedia Subsystem architecture.\nMost telecom operators implement LTE-A, an advanced version of LTE often\nmarketed as 4G+, which achieves data rate peaks of 300 Mbps. Yet, although such\nnovel technology boosts the access to advanced multimedia contents and\nservices, telco operators continue to consider the VoIP market as the major\nrevenue for their business. In this work, the authors propose a detailed\nperformance assessment of VoIP traffic by carrying out experimental trials\nacross a real LTE-A environment. The experimental campaign consists of two\nstages. First, we characterize VoIP calls between fixed and mobile terminals,\nbased on a dataset that includes more than 750,000 data-voice packets. We\nanalyze quality-of-service metrics such as round-trip time (RTT) and jitter, to\ncapture the influence of uncontrolled factors that typically appear in\nreal-world settings. In the second stage, we further consider VoIP flows across\na range of codecs, looking at the trade-offs between quality and bandwidth\nconsumption. Moreover, we propose a statistical characterization of jitter and\nRTT (representing the most critical parameters), identifying the optimal\napproximating distribution, namely the Generalized Extreme Value (GEV). The\nestimation of parameters through the Maximum Likelihood criterion, leads us to\nreveal both the short- and long-tail behaviour for jitter and RTT,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 12:22:49 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Di Mauro", "Mario", ""], ["Liotta", "Antonio", ""]]}, {"id": "2009.12922", "submitter": "Olga Poppe", "authors": "Olga Poppe, Tayo Amuneke, Dalitso Banda, Aritra De, Ari Green, Manon\n  Knoertzer, Ehi Nosakhare, Karthik Rajendran, Deepak Shankargouda, Meina Wang,\n  Alan Au, Carlo Curino, Qun Guo, Alekh Jindal, Ajay Kalhan, Morgan Oslake,\n  Sonia Parchani, Vijay Ramani, Raj Sellappan, Saikat Sen, Sheetal Shrotri,\n  Soundararajan Srinivasan, Ping Xia, Shize Xu, Alicia Yang, Yiwen Zhu", "title": "Seagull: An Infrastructure for Load Prediction and Optimized Resource\n  Allocation", "comments": "Technical report for the paper in VLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsoft Azure is dedicated to guarantee high quality of service to its\ncustomers, in particular, during periods of high customer activity, while\ncontrolling cost. We employ a Data Science (DS) driven solution to predict user\nload and leverage these predictions to optimize resource allocation. To this\nend, we built the Seagull infrastructure that processes per-server telemetry,\nvalidates the data, trains and deploys ML models. The models are used to\npredict customer load per server (24h into the future), and optimize service\noperations. Seagull continually re-evaluates accuracy of predictions, fallback\nto previously known good models and triggers alerts as appropriate. We deployed\nthis infrastructure in production for PostgreSQL and MySQL servers across all\nAzure regions, and applied it to the problem of scheduling server backups\nduring low-load time. This minimizes interference with user-induced load and\nimproves customer experience.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 18:41:32 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 19:22:57 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Poppe", "Olga", ""], ["Amuneke", "Tayo", ""], ["Banda", "Dalitso", ""], ["De", "Aritra", ""], ["Green", "Ari", ""], ["Knoertzer", "Manon", ""], ["Nosakhare", "Ehi", ""], ["Rajendran", "Karthik", ""], ["Shankargouda", "Deepak", ""], ["Wang", "Meina", ""], ["Au", "Alan", ""], ["Curino", "Carlo", ""], ["Guo", "Qun", ""], ["Jindal", "Alekh", ""], ["Kalhan", "Ajay", ""], ["Oslake", "Morgan", ""], ["Parchani", "Sonia", ""], ["Ramani", "Vijay", ""], ["Sellappan", "Raj", ""], ["Sen", "Saikat", ""], ["Shrotri", "Sheetal", ""], ["Srinivasan", "Soundararajan", ""], ["Xia", "Ping", ""], ["Xu", "Shize", ""], ["Yang", "Alicia", ""], ["Zhu", "Yiwen", ""]]}, {"id": "2009.13141", "submitter": "Mario Di Mauro", "authors": "Mario Di Mauro, Maurizio Longo, Fabio Postiglione", "title": "Availability Evaluation of Multi-tenant Service Function Chaining\n  Infrastructures by Multidimensional Universal Generating Function", "comments": null, "journal-ref": null, "doi": "10.1109/TSC.2018.2885748", "report-no": null, "categories": "cs.NI cs.PF stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Network Function Virtualization (NFV) paradigm has been devised as an\nenabler of next generation network infrastructures by speeding up the\nprovisioning and the composition of novel network services. The latter are\nimplemented via a chain of virtualized network functions, a process known as\nService Function Chaining. In this paper, we evaluate the availability of\nmulti-tenant SFC infrastructures, where every network function is modeled as a\nmulti-state system and is shared among different and independent tenants. To\nthis aim, we propose a Universal Generating Function (UGF) approach, suitably\nextended to handle performance vectors, that we call Multidimensional UGF. This\nnovel methodology is validated in a realistic multi-tenant telecommunication\nnetwork scenario, where the service chain is composed by the network elements\nof an IP Multimedia Subsystem implemented via NFV. A steady-state availability\nevaluation of such an exemplary system is presented and a redundancy\noptimization problem is solved, so providing the SFC infrastructure which\nminimizes deployment cost while respecting a given availability requirement.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:39:53 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Di Mauro", "Mario", ""], ["Longo", "Maurizio", ""], ["Postiglione", "Fabio", ""]]}, {"id": "2009.13159", "submitter": "Faissal El Bouanani", "authors": "Yassine Mouchtak, Faissal El Bouanani", "title": "New Accurate Approximation for Average Error Probability Under\n  $\\kappa-\\mu$ Shadowed Fading Channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new accurate approximations for average error probability\n(AEP) of a communication system employing either $M$-phase-shift keying (PSK)\nor differential quaternary PSK with Gray coding (GC-DQPSK) modulation schemes\nover $\\kappa-\\mu$ shadowed fading channel. Firstly, new accurate approximations\nof error probability (EP) of both modulation schemes are derived over additive\nwhite Gaussian noise (AWGN) channel. Leveraging the trapezoidal integral\nmethod, a tight approximate expression of symbol error probability for $M$-PSK\nmodulation is presented, while new upper and lower bounds for Marcum\n$Q$-function of the first order (MQF), and subsequently those for bit error\nprobability (BER) under DQPSK scheme, are proposed. Next, these bounds are\nlinearly combined to propose a highly refined and accurate BER's approximation.\nThe key idea manifested in the decrease property of modified Bessel function\n$I_{v}$, strongly related to MQF, with its argument $v$. Finally, theses\napproximations are used to tackle AEP's approximation under $\\kappa-\\mu$\nshadowed fading. Numerical results show the accuracy of the presented\napproximations compared to the exact ones.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 09:14:13 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Mouchtak", "Yassine", ""], ["Bouanani", "Faissal El", ""]]}, {"id": "2009.13254", "submitter": "Fabrice Guillemin", "authors": "Fabrice Guillemin and Alain Simonian and Ridha Nasri and Veronica\n  Quintuna Rodriguez", "title": "On the sojourn time of a batch in the $M^{[X]}/M/1$ Processor Sharing\n  Queue", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the sojourn of an entire batch in a processor\nsharing $M^{[X]}/M/1$ processor queue, where geometrically distributed batches\narrive according to a Poisson process and jobs require exponential service\ntimes. By conditioning on the number of jobs in the systems and the number of\njobs in a tagged batch, we establish recurrence relations between conditional\nsojourn times, which subsequently allow us to derive a partial differential\nequation for an associated bivariate generating function. This equation\ninvolves an unknown generating function, whose coefficients can be computed by\nsolving an infinite lower triangular linear system. Once this unknown function\nis determined, we compute the Laplace transform and the mean value of the\nsojourn time of a batch in the system.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 12:23:21 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Guillemin", "Fabrice", ""], ["Simonian", "Alain", ""], ["Nasri", "Ridha", ""], ["Rodriguez", "Veronica Quintuna", ""]]}, {"id": "2009.13701", "submitter": "Haosen Wen", "authors": "Haosen Wen, Wentao Cai, Mingzhe Du, Louis Jenkins, Benjamin Valpey,\n  Michael L. Scott", "title": "Montage: A General System for Buffered Durably Linearizable Data\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent emergence of fast, dense, nonvolatile main memory suggests that\ncertain long-lived data might remain in its natural pointer-rich format across\nprogram runs and hardware reboots. Operations on such data must be instrumented\nwith explicit write-back and fence instructions to ensure consistency in the\nwake of a crash. Techniques to minimize the cost of this instrumentation are an\nactive topic of research.\n  We present what we believe to be the first general-purpose approach to\nbuilding buffered durably linearizable persistent data structures, and a\nsystem, Montage, to support that approach. Montage is built on top of the\nRalloc nonblocking persistent allocator. It employs a slow-ticking epoch clock,\nand ensures that no operation appears to span an epoch boundary. It also\narranges to persist only that data minimally required to reconstruct the\nstructure after a crash. If a crash occurs in epoch $e$, all work performed in\nepochs $e$ and $e-1$ is lost, but work from prior epochs is preserved.\n  We describe the implementation of Montage, argue its correctness, and report\nunprecedented throughput for persistent queues, sets/mappings, and general\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 00:30:33 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wen", "Haosen", ""], ["Cai", "Wentao", ""], ["Du", "Mingzhe", ""], ["Jenkins", "Louis", ""], ["Valpey", "Benjamin", ""], ["Scott", "Michael L.", ""]]}, {"id": "2009.13903", "submitter": "Georg Hager", "authors": "Christie L. Alappat, Jan Laukemann, Thomas Gruber, Georg Hager,\n  Gerhard Wellein, Nils Meyer, Tilo Wettig", "title": "Performance Modeling of Streaming Kernels and Sparse Matrix-Vector\n  Multiplication on A64FX", "comments": "6 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The A64FX CPU powers the current number one supercomputer on the Top500 list.\nAlthough it is a traditional cache-based multicore processor, its peak\nperformance and memory bandwidth rival accelerator devices. Generating\nefficient code for such a new architecture requires a good understanding of its\nperformance features. Using these features, we construct the\nExecution-Cache-Memory (ECM) performance model for the A64FX processor in the\nFX700 supercomputer and validate it using streaming loops. We also identify\narchitectural peculiarities and derive optimization hints. Applying the ECM\nmodel to sparse matrix-vector multiplication (SpMV), we motivate why the CRS\nmatrix storage format is inappropriate and how the SELL-C-sigma format with\nsuitable code optimizations can achieve bandwidth saturation for SpMV.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 09:52:59 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Alappat", "Christie L.", ""], ["Laukemann", "Jan", ""], ["Gruber", "Thomas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""], ["Meyer", "Nils", ""], ["Wettig", "Tilo", ""]]}, {"id": "2009.14574", "submitter": "Federico Larroca", "authors": "Paola Bermolen, Matthieu Jonckheere, Federico Larroca, Manuel Saenz", "title": "Sequential Algorithms and Independent Sets Discovering on Large Sparse\n  Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the size of maximum independent sets is a NP-hard problem for fixed\ngraphs. Characterizing and designing efficient algorithms to estimate this\nindependence number for random graphs are notoriously difficult and still\nlargely open issues. In a companion paper, we showed that a low complexity\ndegree-greedy exploration is actually asymptotically optimal on a large class\nof sparse random graphs. Encouraged by this result, we present and study two\nvariants of sequential exploration algorithms: static and dynamic degree-aware\nexplorations. We derive hydrodynamic limits for both of them, which in turn\nallow us to compute the size of the resulting independent set. Whereas the\nformer is simpler to compute, the latter may be used to arbitrarily approximate\nthe degree-greedy algorithm. Both can be implemented in a distributed manner.\nThe corresponding hydrodynamic limits constitute an efficient method to compute\nor bound the independence number for a large class of sparse random graphs. As\nan application, we then show how our method may be used to estimate the\ncapacity of a large 802.11-based wireless network. We finally consider further\nindicators such as the fairness of the resulting configuration, and show how an\nunexpected trade-off between fairness and capacity can be achieved.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 11:36:08 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Bermolen", "Paola", ""], ["Jonckheere", "Matthieu", ""], ["Larroca", "Federico", ""], ["Saenz", "Manuel", ""]]}, {"id": "2009.14600", "submitter": "Orestis Zachariadis", "authors": "Orestis Zachariadis, Nitin Satpute, Juan G\\'omez-Luna, Joaqu\\'in\n  Olivares", "title": "Accelerating Sparse Matrix-Matrix Multiplication with GPU Tensor Cores", "comments": "Accepted in CAEE", "journal-ref": "Comput. Electr. Eng. 88 (2020) 106848", "doi": "10.1016/j.compeleceng.2020.106848", "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse general matrix-matrix multiplication (spGEMM) is an essential\ncomponent in many scientific and data analytics applications. However, the\nsparsity pattern of the input matrices and the interaction of their patterns\nmake spGEMM challenging. Modern GPUs include Tensor Core Units (TCUs), which\nspecialize in dense matrix multiplication. Our aim is to re-purpose TCUs for\nsparse matrices. The key idea of our spGEMM algorithm, tSparse, is to multiply\nsparse rectangular blocks using the mixed precision mode of TCUs. tSparse\npartitions the input matrices into tiles and operates only on tiles which\ncontain one or more elements. It creates a task list of the tiles, and performs\nmatrix multiplication of these tiles using TCUs. To the best of our knowledge,\nthis is the first time that TCUs are used in the context of spGEMM. We show\nthat spGEMM, with our tiling approach, benefits from TCUs. Our approach\nsignificantly improves the performance of spGEMM in comparison to cuSPARSE,\nCUSP, RMerge2, Nsparse, AC-SpGEMM and spECK.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 14:10:15 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Zachariadis", "Orestis", ""], ["Satpute", "Nitin", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Olivares", "Joaqu\u00edn", ""]]}]