[{"id": "1906.00142", "submitter": "Davood Mohajerani", "authors": "Alexander Brandt, Davood Mohajerani, Marc Moreno Maza, Jeeva Paudel,\n  Lin-Xiao Wang", "title": "A Technique for Finding Optimal Program Launch Parameters Targeting\n  Manycore Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new technique to dynamically determine the values\nof program parameters in order to optimize the performance of a multithreaded\nprogram P. To be precise, we describe a novel technique to statically build\nanother program, say, R, that can dynamically determine the optimal values of\nprogram parameters to yield the best program performance for P given values for\nits data and hardware parameters. While this technique can be applied to\nparallel programs in general, we are particularly interested in programs\ntargeting manycore accelerators. Our technique has successfully been employed\nfor GPU kernels using the MWP-CWP performance model for CUDA.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 03:32:57 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Brandt", "Alexander", ""], ["Mohajerani", "Davood", ""], ["Maza", "Marc Moreno", ""], ["Paudel", "Jeeva", ""], ["Wang", "Lin-Xiao", ""]]}, {"id": "1906.01060", "submitter": "Nestor Nahuel Deniz", "authors": "Nestor N. Deniz, Marina H. Murillo, Guido Sanchez, Lucas M. Genzelis\n  and Leonardo Giovanini", "title": "Robust stability of moving horizon estimation for nonlinear systems with\n  bounded disturbances using adaptive arrival cost", "comments": "Under revision at IET Control theory and applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.PF cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the robust stability and convergence to the true state of\nmoving horizon estimator based on an adaptive arrival cost are established for\nnonlinear detectable systems. Robust global asymptotic stability is shown for\nthe case of non-vanishing bounded disturbances whereas the convergence to the\ntrue state is proved for the case of vanishing disturbances. Several\nsimulations were made in order to show the estimator behaviour under different\noperational conditions and to compare it with the state of the art estimation\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 20:15:30 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Deniz", "Nestor N.", ""], ["Murillo", "Marina H.", ""], ["Sanchez", "Guido", ""], ["Genzelis", "Lucas M.", ""], ["Giovanini", "Leonardo", ""]]}, {"id": "1906.01128", "submitter": "Millad Ghane", "authors": "Millad Ghane, Sunita Chandrasekaran, Margaret S. Cheung", "title": "Assessing Performance Implications of Deep Copy Operations via\n  Microbenchmarking", "comments": "11 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As scientific frameworks become sophisticated, so do their data structures.\nCurrent data structures are no longer simple in design and they have been\nprogressively complicated. The typical trend in designing data structures in\nscientific applications are basically nested data structures: pointing to a\ndata structure within another one. Managing nested data structures on a modern\nheterogeneous system requires tremendous effort due to the separate memory\nspace design.\n  In this paper, we will discuss the implications of deep copy on data\ntransfers on current heterogeneous. Then, we will discuss the two options that\nare currently available to perform the memory copy operations on complex\nstructures and will introduce pointerchain directive that we proposed.\nAfterwards, we will introduce a set of extensive benchmarks to compare the\navailable approaches. Our goal is to make our proposed benchmarks a basis to\nexamine the efficiency of upcoming approaches that address the challenge of\ndeep copy operations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 23:44:02 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 21:52:30 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Ghane", "Millad", ""], ["Chandrasekaran", "Sunita", ""], ["Cheung", "Margaret S.", ""]]}, {"id": "1906.01878", "submitter": "Xingzhou Zhang", "authors": "Xingzhou Zhang, Yifan Wang, Weisong Shi", "title": "pCAMP: Performance Comparison of Machine Learning Packages on the Edges", "comments": "6 pages, 3 figures, USENIX HotEdge 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has changed the computing paradigm. Products today are built\nwith machine intelligence as a central attribute, and consumers are beginning\nto expect near-human interaction with the appliances they use. However, much of\nthe deep learning revolution has been limited to the cloud. Recently, several\nmachine learning packages based on edge devices have been announced which aim\nto offload the computing to the edges. However, little research has been done\nto evaluate these packages on the edges, making it difficult for end users to\nselect an appropriate pair of software and hardware. In this paper, we make a\nperformance comparison of several state-of-the-art machine learning packages on\nthe edges, including TensorFlow, Caffe2, MXNet, PyTorch, and TensorFlow Lite.\nWe focus on evaluating the latency, memory footprint, and energy of these tools\nwith two popular types of neural networks on different edge devices. This\nevaluation not only provides a reference to select appropriate combinations of\nhardware and software packages for end users but also points out possible\nfuture directions to optimize packages for developers.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 08:24:49 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 05:03:03 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhang", "Xingzhou", ""], ["Wang", "Yifan", ""], ["Shi", "Weisong", ""]]}, {"id": "1906.01992", "submitter": "Sabri Pllana", "authors": "Andre Viebke, Sabri Pllana, Suejb Memeti, Joanna Kolodziej", "title": "Performance Modelling of Deep Learning on Intel Many Integrated Core\n  Architectures", "comments": "Preprint, HPCS. arXiv admin note: substantial text overlap with\n  arXiv:1702.07908", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex problems, such as natural language processing or visual object\ndetection, are solved using deep learning. However, efficient training of\ncomplex deep convolutional neural networks for large data sets is\ncomputationally demanding and requires parallel computing resources. In this\npaper, we present two parameterized performance models for estimation of\nexecution time of training convolutional neural networks on the Intel many\nintegrated core architecture. While for the first performance model we\nminimally use measurement techniques for parameter value estimation, in the\nsecond model we estimate more parameters based on measurements. We evaluate the\nprediction accuracy of performance models in the context of training three\ndifferent convolutional neural network architectures on the Intel Xeon Phi. The\nachieved average performance prediction accuracy is about 15% for the first\nmodel and 11% for second model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 10:14:50 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Viebke", "Andre", ""], ["Pllana", "Sabri", ""], ["Memeti", "Suejb", ""], ["Kolodziej", "Joanna", ""]]}, {"id": "1906.02061", "submitter": "Oscar J. Romero", "authors": "Oscar J. Romero and Sushma A. Akoju", "title": "Adroitness: An Android-based Middleware for Fast Development of\n  High-performance Apps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As smartphones become increasingly more powerful, a new generation of highly\ninteractive user-centric mobile apps emerge to make user's life simpler and\nmore productive. Mobile phones applications have to sustain limited resource\navailability on mobile devices such as battery life, network connectivity while\nalso providing better responsiveness, lightweight interactions within the\napplication. Developers end up spending a considerable amount of time dealing\nwith the architecture constraints imposed by the wide variety of platforms,\ntools, and devices offered by the mobile ecosystem, thereby diverting them from\ntheir main goal of building such apps. Therefore, we propose a mobile-based\nmiddleware architecture that alleviates the burdensome task of dealing with\nlow-level architectural decisions and fine-grained implementation details. We\nachieve such a goal by focusing on the separation of concerns and abstracting\naway the complexity of orchestrating device sensors and effectors,\ndecision-making processes, and connection to remote services, while providing\nscaffolding for the development of higher-level functional features of\ninteractive high-performance mobile apps. We demonstrate the powerfulness of\nour approach vs. Android's conventional framework by comparing different\nsoftware metric\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 14:59:33 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Romero", "Oscar J.", ""], ["Akoju", "Sushma A.", ""]]}, {"id": "1906.02068", "submitter": "Oscar J. Romero", "authors": "Oscar J. Romero", "title": "Architectural Middleware that Supports Building High-performance,\n  Scalable, Ubiquitous, Intelligent Personal Assistants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent Personal Assistants (IPAs) are software agents that can perform\ntasks on behalf of individuals and assist them on many of their daily\nactivities. IPAs capabilities are expanding rapidly due to the recent advances\non areas such as natural language processing, machine learning, artificial\ncognition, and ubiquitous computing, which equip the agents with competences to\nunderstand what users say, collect information from everyday ubiquitous devices\n(e.g., smartphones, wearables, tablets, laptops, cars, household appliances,\netc.), learn user preferences, deliver data-driven search results, and make\ndecisions based on user's context. Apart from the inherent complexity of\nbuilding such IPAs, developers and researchers have to address many critical\narchitectural challenges (e.g., low-latency, scalability, concurrency,\nubiquity, code mobility, interoperability, support to cognitive services and\nreasoning, to name a few.), thereby diverting them from their main goal:\nbuilding IPAs. Thus, our contribution in this paper is twofold: 1) we propose\nan architecture for a platform-agnostic, high-performance, ubiquitous, and\ndistributed middleware that alleviates the burdensome task of dealing with\nlow-level implementation details when building IPAs by adding multiple\nabstraction layers that hide the underlying complexity; and 2) we present an\nimplementation of the middleware that concretizes the aforementioned\narchitecture and allows the development of high-level capabilities while\nscaling the system up to hundreds of thousands of IPAs with no extra effort. We\ndemonstrate the powerfulness of our middleware by analyzing software metrics\nfor complexity, effort, performance, cohesion and coupling when developing a\nconversational IPA.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 15:15:36 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Romero", "Oscar J.", ""]]}, {"id": "1906.03884", "submitter": "Karthee Sivalingam", "authors": "Karthee Sivalingam, Harvey Richardson, Adrian Tate, Martin Lafferty", "title": "LASSi: Metric based I/O analytics for HPC", "comments": "12 pages, 6 figures, 3 tables, SpringSim-HPC, 2019 April 29-May 2,\n  Tucson, AZ, 2019 Society for Modeling and Simulation International (SCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LASSi is a tool aimed at analyzing application usage and contention caused by\nuse of shared resources (filesystem or network) in a HPC system. LASSi was\ninitially developed to support the ARCHER system where there are large\nvariations in application requirements and occasional user complaints regarding\nfilesystem performance manifested by variation in job runtimes or poor\ninteractive response. LASSi takes an approach of defining derivative risk and\nops metrics that relate to unusually high application I/O behaviour. The\nmetrics are shown to correlate to applications that can experience variable\nperformance or that may impact the performance of other applications. LASSi\nuses I/O statistics over time to provide application I/O profiles and has been\nautomated to generate daily reports for ARCHER. We demonstrate how LASSi\nprovides holistic I/O analysis by monitoring filesystem I/O, generating coarse\nprofiles of filesystems and application runs and automating analysis of\napplication slowdown using metrics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 10:36:46 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sivalingam", "Karthee", ""], ["Richardson", "Harvey", ""], ["Tate", "Adrian", ""], ["Lafferty", "Martin", ""]]}, {"id": "1906.03891", "submitter": "Karthee Sivalingam", "authors": "Andrew Turner, Dominic Sloan-Murphy, Karthee Sivalingam, Harvey\n  Richardson, Julian Kunkel", "title": "Analysis of parallel I/O use on the UK national supercomputing service,\n  ARCHER using Cray LASSi and EPCC SAFE", "comments": "15 pages, 19 figures, 5 tables, 2019 Cray User Group Meeting (CUG) ,\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe how we have used a combination of the LASSi tool\n(developed by Cray) and the SAFE software (developed by EPCC) to collect and\nanalyse Lustre I/O performance data for all jobs running on the UK national\nsupercomputing service, ARCHER; and to provide reports on I/O usage for users\nin our standard reporting framework. We also present results from analysis of\nparallel I/O use on ARCHER and analysis on the potential impact of different\napplications on file system performance using metrics we have derived from the\nLASSi data. We show that the performance data from LASSi reveals how the same\napplication can stress different components of the file system depending on how\nit is run, and how the LASSi risk metrics allow us to identify use cases that\ncould potentially cause issues for global I/O performance and work with users\nto improve their I/O use. We use the IO-500 benchmark to help us understand how\nLASSi risk metrics correspond to observed performance on the ARCHER file\nsystems. We also use LASSi data imported into SAFE to identify I/O use patterns\nassociated with different research areas, understand how the research workflow\ngives rise to the observed patterns and project how this will affect I/O\nrequirements in the future. Finally, we provide an overview of likely future\ndirections for the continuation of this work.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 10:45:30 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Turner", "Andrew", ""], ["Sloan-Murphy", "Dominic", ""], ["Sivalingam", "Karthee", ""], ["Richardson", "Harvey", ""], ["Kunkel", "Julian", ""]]}, {"id": "1906.04278", "submitter": "Jiawen Liu", "authors": "Jie Liu, Jiawen Liu, Wan Du, Dong Li", "title": "Performance Analysis and Characterization of Training Deep Learning\n  Models on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep learning models on mobile devices recently becomes possible,\nbecause of increasing computation power on mobile hardware and the advantages\nof enabling high user experiences. Most of the existing work on machine\nlearning at mobile devices is focused on the inference of deep learning models\n(particularly convolutional neural network and recurrent neural network), but\nnot training. The performance characterization of training deep learning models\non mobile devices is largely unexplored, although understanding the performance\ncharacterization is critical for designing and implementing deep learning\nmodels on mobile devices.\n  In this paper, we perform a variety of experiments on a representative mobile\ndevice (the NVIDIA TX2) to study the performance of training deep learning\nmodels. We introduce a benchmark suite and tools to study performance of\ntraining deep learning models on mobile devices, from the perspectives of\nmemory consumption, hardware utilization, and power consumption. The tools can\ncorrelate performance results with fine-grained operations in deep learning\nmodels, providing capabilities to capture performance variance and problems at\na fine granularity. We reveal interesting performance problems and\nopportunities, including under-utilization of heterogeneous hardware, large\nenergy consumption of the memory, and high predictability of workload\ncharacterization. Based on the performance analysis, we suggest interesting\nresearch directions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 21:16:57 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 16:27:27 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Jie", ""], ["Liu", "Jiawen", ""], ["Du", "Wan", ""], ["Li", "Dong", ""]]}, {"id": "1906.04286", "submitter": "Angel Beltre", "authors": "Angel Beltre, Shehtab Zaman, Kenneth Chiu, Sudhakar Pamidighantam,\n  Xingye Qiao, Madhusudhan Govindaraju", "title": "Towards Run Time Estimation of the Gaussian Chemistry Code for SEAGrid\n  Science Gateway", "comments": "8 pages, 4 Figures, conference", "journal-ref": "ACM, PEARC 2019", "doi": "10.1145/3332186.3338101", "report-no": "126", "categories": "physics.comp-ph cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of the run time of computational codes has a number of\nsignificant advantages for scientific computing. It is required information for\noptimal resource allocation, improving turnaround times and utilization of\nscience gateways. Furthermore, it allows users to better plan and schedule\ntheir research, streamlining workflows and improving the overall productivity\nof cyberinfrastructure. Predicting run time is challenging, however. The inputs\nto scientific codes can be complex and high dimensional. Their relationship to\nthe run time may be highly non-linear, and, in the most general case is\ncompletely arbitrary and thus unpredictable (i.e., simply a random mapping from\ninputs to run time). Most codes are not so arbitrary, however, and there has\nbeen significant prior research on predicting the run time of applications and\nworkloads. Such predictions are generally application-specific, however. In\nthis paper, we focus on the Gaussian computational chemistry code. We\ncharacterize a data set of runs from the SEAGrid science gateway with a number\nof different studies. We also explore a number of different potential\nregression methods and present promising future directions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 04:00:03 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Beltre", "Angel", ""], ["Zaman", "Shehtab", ""], ["Chiu", "Kenneth", ""], ["Pamidighantam", "Sudhakar", ""], ["Qiao", "Xingye", ""], ["Govindaraju", "Madhusudhan", ""]]}, {"id": "1906.04624", "submitter": "Oksana Shadura", "authors": "Oksana Shadura (1), Brian Paul Bockelman (1) ((1) University of\n  Nebraska-Lincoln, (2) Morgridge Institute for Research)", "title": "ROOT I/O compression algorithms and their performance impact within Run\n  3", "comments": "Submitted to proceedings of ACAT 2019", "journal-ref": null, "doi": "10.1088/1742-6596/1525/1/012049", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LHCs Run3 will push the envelope on data-intensive workflows and, since\nat the lowest level this data is managed using the ROOT software framework,\npreparations for managing this data are starting already. At the beginning of\nLHC Run 1, all ROOT data was compressed with the ZLIB algorithm; since then,\nROOT has added support for additional algorithms such as LZMA and LZ4, each\nwith unique strengths. This work must continue as industry introduces new\ntechniques - ROOT can benefit saving disk space or reducing the I/O and\nbandwidth for online and offline needs of experiments by introducing better\ncompression algorithms. In addition to alternate algorithms, we have been\nexploring alternate techniques to improve parallelism and apply\npre-conditioners to the serialized data.\n  We have performed a survey of the performance of the new compression\ntechniques. Our survey includes various use cases of data compression of ROOT\nfiles provided by different LHC experiments. We also provide insight into\nsolutions applied to resolve bottlenecks in compression algorithms, resulting\nin improved ROOT performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 14:36:03 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 07:51:44 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Shadura", "Oksana", ""], ["Bockelman", "Brian Paul", ""]]}, {"id": "1906.04787", "submitter": "Marco Baiesi", "authors": "Marco Baiesi", "title": "Power Gradient Descent", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of machine learning is promoting the search for fast and\nstable minimization algorithms. To this end, we suggest a change in the current\ngradient descent methods that should speed up the motion in flat regions and\nslow it down in steep directions of the function to minimize. It is based on a\n\"power gradient\", in which each component of the gradient is replaced by its\nversus-preserving $H$-th power, with $0<H<1$. We test three modern gradient\ndescent methods fed by such variant and by standard gradients, finding the new\nversion to achieve significantly better performances for the Nesterov\naccelerated gradient and AMSGrad. We also propose an effective new take on the\nADAM algorithm, which includes power gradients with varying $H$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 19:48:01 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Baiesi", "Marco", ""]]}, {"id": "1906.05103", "submitter": "Gewu Bu", "authors": "Bruno Baynat (SU, LIP6, NPA), Gewu Bu (SU, LIP6, NPA), Maria\n  Potop-Butucaru (SU, LIP6, NPA, LINCS)", "title": "Markovian model for Broadcast in Wireless Body Area Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless body area networks became recently a vast field of investigation. A\nlarge amount of research in this field is dedicated to the evaluation of\nvarious communication protocols, e.g., broadcast or convergecast, against human\nbody mobility. Most of the time this evaluation is done via simulations and in\nmany situations only synthetic data is used for the human body mobility. In\nthis paper we propose for the first time in Wireless Body Area Networks a\nMarkovian analytical model specifically designed for WBAN networks. The main\nobjective of the model is to evaluate the efficiency of a multi-hop\ntransmission in the case of a diffusion-based broadcast protocol, with respect\nto various performance parameters (e.g., cover probability, average cover\nnumber, hitting probability or average cover time). We validate our model by\ncomparing its results to simulation and show its accuracy. Finally, but not\nleast, we show how our model can be used to analytically evaluate the trade-off\nbetween transmission power and redundancy, when the same message is broadcasted\nseveral times in order to increase the broadcast reliability while maintaining\na low transmission power.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 12:58:01 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Baynat", "Bruno", "", "SU, LIP6, NPA"], ["Bu", "Gewu", "", "SU, LIP6, NPA"], ["Potop-Butucaru", "Maria", "", "SU, LIP6, NPA, LINCS"]]}, {"id": "1906.05345", "submitter": "Mehmet Aktas", "authors": "Mehmet Fatih Aktas, Emina Soljanin", "title": "Optimizing Redundancy Levels in Master-Worker Compute Clusters for\n  Straggler Mitigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime variability in computing systems causes some tasks to straggle and\ntake much longer than expected to complete. These straggler tasks are known to\nsignificantly slowdown distributed computation. Job execution with speculative\nexecution of redundant tasks has been the most widely deployed technique for\nmitigating the impact of stragglers, and many recent theoretical papers have\nstudied the advantages and disadvantages of using redundancy under various\nsystem and service models. However, no clear guidelines could yet be found on\nwhen, for which jobs, and how much redundancy should be employed in\nMaster-Worker compute clusters, which is the most widely adopted architecture\nin modern compute systems. We are concerned with finding a strategy for\nscheduling jobs with redundancy that works well in practice. This is a complex\noptimization problem, which we address in stages. We first use Reinforcement\nLearning (RL) techniques to learn good scheduling principles from realistic\nexperience. Building on these principles, we derive a simple scheduling policy\nand present an approximate analysis of its performance. Specifically, we derive\nexpressions to decide when and which jobs should be scheduled with how much\nredundancy. We show that policy that we devise in this way performs as good as\nthe more complex policies that are derived by RL. Finally, we extend our\napproximate analysis to the case when system employs the other widely deployed\nremedy for stragglers, which is relaunching straggler tasks after waiting some\ntime. We show that scheduling with redundancy significantly outperforms\nstraggler relaunch policy when the offered load on the system is low or\nmoderate, and performs slightly worse when the offered load is very high.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:37:01 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Soljanin", "Emina", ""]]}, {"id": "1906.05737", "submitter": "Felix Thielke", "authors": "Felix Thielke and Arne Hasselbring", "title": "A JIT Compiler for Neural Network Inference", "comments": "8 pages, to appear in RoboCup 2019: Robot World Cup XXIII, Springer", "journal-ref": null, "doi": "10.1007/978-3-030-35699-6_36", "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a C++ library that compiles neural network models at\nruntime into machine code that performs inference. This approach in general\npromises to achieve the best performance possible since it is able to integrate\nstatically known properties of the network directly into the code. In our\nexperiments on the NAO V6 platform, it outperforms existing implementations\nsignificantly on small networks, while being inferior on large networks. The\nlibrary was already part of the B-Human code release 2018, but has been\nextended since and is now available as a standalone version that can be\nintegrated into any C++14 code base.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 14:59:17 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Thielke", "Felix", ""], ["Hasselbring", "Arne", ""]]}, {"id": "1906.06240", "submitter": "Mario Almeida", "authors": "Mario Almeida, Liang Wang, Jeremy Blackburn, Konstantina Papagiannaki,\n  Jon Crowcroft", "title": "Diffusing Your Mobile Apps: Extending In-Network Function Virtualization\n  to Mobile Function Offloading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the huge disparity between the limited battery capacity of user\ndevices and the ever-growing energy demands of modern mobile apps, we propose\nINFv. It is the first offloading system able to cache, migrate and dynamically\nexecute on demand functionality from mobile devices in ISP networks. It aims to\nbridge this gap by extending the promising NFV paradigm to mobile applications\nin order to exploit in-network resources. In this paper, we present the overall\ndesign, state-of-the-art technologies adopted, and various engineering details\nin the INFv system. We also carefully study the deployment configurations by\ninvestigating over 20K Google Play apps, as well as thorough evaluations with\nrealistic settings. In addition to a significant improvement in battery life\n(up to 6.9x energy reduction) and execution time (up to 4x faster), INFv has\ntwo distinct advantages over previous systems: 1) a non-intrusive offloading\nmechanism transparent to existing apps; 2) an inherent framework support to\neffectively balance computation load and exploit the proximity of in-network\nresources. Both advantages together enable a scalable and incremental\ndeployment of computation offloading framework in practical ISPs' networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 15:12:41 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Almeida", "Mario", ""], ["Wang", "Liang", ""], ["Blackburn", "Jeremy", ""], ["Papagiannaki", "Konstantina", ""], ["Crowcroft", "Jon", ""]]}, {"id": "1906.07105", "submitter": "Adones Rukundo Mr", "authors": "Adones Rukundo, Aras Atalar and Philippas Tsigas", "title": "Monotonically relaxing concurrent data-structure semantics for\n  performance: An efficient 2D design framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a significant amount of work in the literature proposing\nsemantic relaxation of concurrent data structures for improving scalability and\nperformance. By relaxing the semantics of a data structure, a bigger design\nspace, that allows weaker synchronization and more useful parallelism, is\nunveiled. Investigating new data structure designs, capable of trading\nsemantics for achieving better performance in a monotonic way, is a major\nchallenge in the area. We algorithmically address this challenge in this paper.\nWe present an efficient, lock-free, concurrent data structure design framework\nfor out-of-order semantic relaxation. Our framework introduces a new two\ndimensional algorithmic design, that uses multiple instances of a given data\nstructure. The first dimension of our design is the number of data structure\ninstances operations are spread to, in order to benefit from parallelism\nthrough disjoint memory access. The second dimension is the number of\nconsecutive operations that try to use the same data structure instance in\norder to benefit from data locality. Our design can flexibly explore this\ntwo-dimensional space to achieve the property of monotonically relaxing\nconcurrent data structure semantics for achieving better throughput performance\nwithin a tight deterministic relaxation bound, as we prove in the paper. We\nshow how our framework can instantiate lock-free out-of-order queues, stacks,\ncounters and dequeues. We provide implementations of these relaxed data\nstructures and evaluate their performance and behaviour on two parallel\narchitectures. Experimental evaluation shows that our two-dimensional data\nstructures significantly outperform the respected previous proposed ones with\nrespect to scalability and throughput performance. Moreover, their throughput\nincreases monotonically as relaxation increases.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 16:13:15 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Rukundo", "Adones", ""], ["Atalar", "Aras", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1906.07312", "submitter": "Pankaj Saha", "authors": "Pankaj Saha, Madhusudhan Govindaraju, Suresh Marru, Marlon Pierce", "title": "MultiCloud Resource Management using Apache Mesos for Planned\n  Integration with Apache Airavata", "comments": "Gateways 2016", "journal-ref": null, "doi": "10.6084/m9.figshare.4491629.v2", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss initial results and our planned approach for incorporating Apache\nMesos based resource management that will enable design and development of\nscheduling strategies for Apache Airavata jobs so that they can be launched on\nmultiple clouds, wherein several VMs do not have Public IP addresses. We\npresent initial work and next steps on the design of a meta-scheduler using\nApache Mesos. Apache Mesos presents a unified view of resources available\nacross several clouds and clusters. Our meta-scheduler can potentially examine\nand identify the cases where multiple small jobs have been submitted by the\nsame scientists and then redirect job from the same community account or user\nto different clusters. Our approach uses a NAT firewall to make nodes/VMs,\nwithout a Public IP, visible to Mesos for the unified view.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 00:06:46 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 22:23:12 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Saha", "Pankaj", ""], ["Govindaraju", "Madhusudhan", ""], ["Marru", "Suresh", ""], ["Pierce", "Marlon", ""]]}, {"id": "1906.08138", "submitter": "Julian Hammer", "authors": "Julian Hornich, Julian Hammer, Georg Hager, Thomas Gruber, Gerhard\n  Wellein", "title": "Collecting and Presenting Reproducible Intranode Stencil Performance:\n  INSPECT", "comments": null, "journal-ref": null, "doi": "10.14529/jsfi190301", "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stencil algorithms have been receiving considerable interest in HPC research\nfor decades. The techniques used to approach multi-core stencil performance\nmodeling and engineering span basic runtime measurements, elaborate performance\nmodels, detailed hardware counter analysis, and thorough scaling behavior\nevaluation. Due to the plurality of approaches and stencil patterns, we set out\nto develop a generalizable methodology for reproducible measurements\naccompanied by state-of-the-art performance models. Our open-source toolchain,\nand collected results are publicly available in the \"Intranode Stencil\nPerformance Evaluation Collection\" (INSPECT). We present the underlying\nmethodologies, models and tools involved in gathering and documenting the\nperformance behavior of a collection of typical stencil patterns across\nmultiple architectures and hardware configuration options. Our aim is to endow\nperformance-aware application developers with reproducible baseline performance\ndata and validated models to initiate a well-defined process of performance\nassessment and optimization.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 15:08:06 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 10:08:33 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Hornich", "Julian", ""], ["Hammer", "Julian", ""], ["Hager", "Georg", ""], ["Gruber", "Thomas", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1906.08689", "submitter": "Zheng Wang", "authors": "Lu Yuan, Jie Ren, Ling Gao, Zhanyong Tang, Zheng Wang", "title": "Using Machine Learning to Optimize Web Interactions on Heterogeneous\n  Mobile Multi-cores", "comments": "Accepted to be published in IEEE ACCESS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The web has become a ubiquitous application development platform for mobile\nsystems. Yet, web access on mobile devices remains an energy-hungry activity.\nPrior work in the field mainly focuses on the initial page loading stage, but\nfails to exploit the opportunities for energy-efficiency optimization while the\nuser is interacting with a loaded page. This paper presents a novel approach\nfor performing energy optimization for interactive mobile web browsing. At the\nheart of our approach is a set of machine learning models, which estimate\n\\emph{at runtime} the frames per second for a given user interaction input by\nrunning the computation-intensive web render engine on a specific processor\ncore under a given clock speed. We use the learned predictive models as a\nutility function to quickly search for the optimal processor setting to\ncarefully trade responsive time for reduced energy consumption. We integrate\nour techniques to the open-source Chromium browser and apply it to two\nrepresentative mobile user events: scrolling and pinching (i.e., zoom in and\nout). We evaluate the developed system on the landing pages of the top-100\nhottest websites and two big.LITTLE heterogeneous mobile platforms. Our\nextensive experiments show that the proposed approach reduces the system-wide\nenergy consumption by over 36\\% on average and up to 70\\%. This translates to\nan over 17\\% improvement on energy-efficiency over a state-of-the-art\nevent-based web browser scheduler, but with significantly fewer violations on\nthe quality of service.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 15:21:09 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 16:07:13 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 13:11:49 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yuan", "Lu", ""], ["Ren", "Jie", ""], ["Gao", "Ling", ""], ["Tang", "Zhanyong", ""], ["Wang", "Zheng", ""]]}, {"id": "1906.08911", "submitter": "Vivek Kale PhD", "authors": "Vivek Kale, Christian Iwainsky, Michael Klemm, Jonas H. Muller\n  Korndorfer and Florina M. Ciorba", "title": "Toward a Standard Interface for User-Defined Scheduling in OpenMP", "comments": "16 pages with references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel loops are an important part of OpenMP programs. Efficient scheduling\nof parallel loops can improve performance of the programs. The current OpenMP\nspecification only offers three options for loop scheduling, which are\ninsufficient in certain instances. Given the large number of other possible\nscheduling strategies, it is infeasible to standardize each one. A more viable\napproach is to extend the OpenMP standard to allow for users to define loop\nscheduling strategies. The approach will enable standard-compliant\napplication-specific scheduling. This work analyzes the principal components\nrequired by user-defined scheduling and proposes two competing interfaces as\ncandidates for the OpenMP standard. We conceptually compare the two proposed\ninterfaces with respect to the three host languages of OpenMP, i.e., C, C++,\nand Fortran. These interfaces serve the OpenMP community as a basis for\ndiscussion and prototype implementation for user-defined scheduling.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 01:47:32 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 20:29:35 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kale", "Vivek", ""], ["Iwainsky", "Christian", ""], ["Klemm", "Michael", ""], ["Korndorfer", "Jonas H. Muller", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1906.09037", "submitter": "Xintao Huan", "authors": "Xintao Huan, Kyeong Soo Kim, Sanghyuk Lee, Eng Gee Lim, Alan Marshall", "title": "A Beaconless Asymmetric Energy-Efficient Time Synchronization Scheme for\n  Resource-Constrained Multi-Hop Wireless Sensor Networks", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing number of WSN deployments based on a large number of\nbattery-powered, low-cost sensor nodes, which are limited in their computing\nand power resources, puts the focus of WSN time synchronization research on\nthree major aspects, i.e., accuracy, energy consumption and computational\ncomplexity. In the literature, the latter two aspects have not received much\nattention compared to the accuracy of WSN time synchronization. Especially in\nmulti-hop WSNs, intermediate gateway nodes are overloaded with tasks for not\nonly relaying messages but also a variety of computations for their offspring\nnodes as well as themselves. Therefore, not only minimizing the energy\nconsumption but also lowering the computational complexity while maintaining\nthe synchronization accuracy is crucial to the design of time synchronization\nschemes for resource-constrained sensor nodes. In this paper, focusing on the\nthree aspects of WSN time synchronization, we introduce a framework of reverse\nasymmetric time synchronization for resource-constrained multi-hop WSNs and\npropose a beaconless energy-efficient time synchronization scheme based on\nreverse one-way message dissemination. Experimental results with a WSN testbed\nbased on TelosB motes running TinyOS demonstrate that the proposed scheme\nconserves up to 95% energy consumption compared to the flooding time\nsynchronization protocol while achieving microsecond-level synchronization\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 10:01:11 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Huan", "Xintao", ""], ["Kim", "Kyeong Soo", ""], ["Lee", "Sanghyuk", ""], ["Lim", "Eng Gee", ""], ["Marshall", "Alan", ""]]}, {"id": "1906.09039", "submitter": "Xintao Huan", "authors": "Xintao Huan, Kyeong Soo Kim", "title": "Optimal Message Bundling with Delay and Synchronization Constraints in\n  Wireless Sensor Networks", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Message bundling is an effective way to reduce the energy consumption for\nmessage transmissions in wireless sensor networks. However, bundling more\nmessages could increase both end-to-end delay and message transmission\ninterval; the former needs to be maintained within a certain value for\ntime-sensitive applications like environmental monitoring, while the latter\naffects time synchronization accuracy when the bundling includes\nsynchronization messages as well. Taking as an example a novel time\nsynchronization scheme recently proposed for energy efficiency, we propose an\noptimal message bundling approach to reduce the message transmissions while\nmaintaining the user-defined requirements on end-to-end delay and time\nsynchronization accuracy. Through translating the objective of joint\nmaintenance to an integer linear programming problem, we compute a set of\noptimal bundling numbers for the sensor nodes to constrain their link-level\ndelays, thereby achieve and maintain the required end-to-end delay and\nsynchronization accuracy while the message transmission is minimized.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 10:09:05 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Huan", "Xintao", ""], ["Kim", "Kyeong Soo", ""]]}, {"id": "1906.09512", "submitter": "Jin-Yuan Wang", "authors": "Jin-Yuan Wang, Hong Ge, Min Lin, Jun-Bo Wang, Jianxin Dai, and\n  Mohamed-Slim Alouini", "title": "On the Secrecy Rate of Spatial Modulation Based Indoor Visible Light\n  Communications", "comments": "30 pages, 10 figures, accepted by IEEE Journal on Selected Areas in\n  Communications, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the physical-layer security for a spatial\nmodulation (SM) based indoor visible light communication (VLC) system, which\nincludes multiple transmitters, a legitimate receiver, and a passive\neavesdropper (Eve). At the transmitters, the SM scheme is employed, i.e., only\none transmitter is active at each time instant. To choose the active\ntransmitter, a uniform selection (US) scheme is utilized. Two scenarios are\nconsidered: one is with non-negativity and average optical intensity\nconstraints, the other is with non-negativity, average optical intensity and\npeak optical intensity constraints. Then, lower and upper bounds on the secrecy\nrate are derived for these two scenarios. Besides, the asymptotic behaviors for\nthe derived secrecy rate bounds at high signal-to-noise ratio (SNR) are\nanalyzed. To further improve the secrecy performance, a channel adaptive\nselection (CAS) scheme and a greedy selection (GS) scheme are proposed to\nselect the active transmitter. Numerical results show that the lower and upper\nbounds of the secrecy rate are tight. At high SNR, small asymptotic performance\ngaps exist between the derived lower and upper bounds. Moreover, the proposed\nGS scheme has the best performance, followed by the CAS scheme and the US\nscheme.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 22:22:50 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Wang", "Jin-Yuan", ""], ["Ge", "Hong", ""], ["Lin", "Min", ""], ["Wang", "Jun-Bo", ""], ["Dai", "Jianxin", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1906.09560", "submitter": "Tuan  Phung-Duc", "authors": "Tuan Phung-Duc", "title": "Retrial Queueing Models: A Survey on Theory and Applications", "comments": "31 pages. In: Stochastic Operations Research in Business and Industry\n  (eds. by Tadashi Dohi, Katsunori Ano and Shoji Kasahara)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrial phenomenon naturally arises in various systems such as call centers,\ncellular networks and random access protocols in local area networks. This\npaper gives a comprehensive survey on theory and applications of retrial queues\nin these systems. We investigate the state of the art of the theoretical\nresearches including exact solutions, stability, asymptotic analyses and\nmultidimensional models. We present an overview on retrial models arising from\nreal world applications. Some open problems and promising research directions\nare also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 06:53:15 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Phung-Duc", "Tuan", ""]]}, {"id": "1906.09799", "submitter": "Valerio Schiavoni Dr", "authors": "Julien Amacher and Valerio Schiavoni", "title": "On The Performance of ARM TrustZone", "comments": "25 pages, extended version for version appeared in IFIP DAIS 2019.\n  European Commission Project: LEGaTO - Low Energy Toolset for Heterogeneous\n  Computing (EC-H2020-780681)", "journal-ref": null, "doi": "10.1007/978-3-030-22496-7_9", "report-no": null, "categories": "cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TrustZone technology, available in the vast majority of recent ARM\nprocessors, allows the execution of code inside a so-called secure world. It\neffectively provides hardware-isolated areas of the processor for sensitive\ndata and code, i.e., a trusted execution environment (TEE). The OP-TEE\nframework provides a collection of toolchain, open-source libraries and secure\nkernel specifically geared to develop applications for TrustZone. This paper\npresents an in-depth performance- and energy-wise study of TrustZone using the\nOP-TEE framework, including secure storage and the cost of switching between\nsecure and unsecure worlds, using emulated and hardware measurements.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 09:11:18 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 15:24:10 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Amacher", "Julien", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "1906.10037", "submitter": "Ahsan Javed Awan Dr", "authors": "Stefano Corda, Gagandeep Singh, Ahsan Javed Awan, Roel Jordans and\n  Henk Corporaal", "title": "Platform Independent Software Analysis for Near Memory Computing", "comments": null, "journal-ref": "Euromicro Conference on Digital System Design (DSD) 2019", "doi": null, "report-no": null, "categories": "cs.PF cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-memory Computing (NMC) promises improved performance for the\napplications that can exploit the features of emerging memory technologies such\nas 3D-stacked memory. However, it is not trivial to find such applications and\nspecialized tools are needed to identify them. In this paper, we present\nPISA-NMC, which extends a state-of-the-art hardware agnostic profiling tool\nwith metrics concerning memory and parallelism, which are relevant for NMC. The\nmetrics include memory entropy, spatial locality, data-level, and\nbasic-block-level parallelism. By profiling a set of representative\napplications and correlating the metrics with the application's performance on\na simulated NMC system, we verify the importance of those metrics. Finally, we\ndemonstrate which metrics are useful in identifying applications suitable for\nNMC architectures.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 15:53:47 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Corda", "Stefano", ""], ["Singh", "Gagandeep", ""], ["Awan", "Ahsan Javed", ""], ["Jordans", "Roel", ""], ["Corporaal", "Henk", ""]]}, {"id": "1906.10081", "submitter": "Jie Ren", "authors": "Jie Ren, Kai Wu, Dong Li", "title": "EasyCrash: Exploring Non-Volatility of Non-Volatile Memory for High\n  Performance Computing Under Failures", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging non-volatile memory (NVM) is promising for building future HPC.\nLeveraging the non-volatility of NVM as main memory, we can restart the\napplication using data objects remaining on NVM when the application crashes.\nThis paper explores this solution to handle HPC under failures, based on the\nobservation that many HPC applications have good enough intrinsic fault\ntolerance. To improve the possibility of successful recomputation with correct\noutcomes and ignorable performance loss, we introduce EasyCrash, a framework to\ndecide how to selectively persist application data objects during application\nexecution. Our evaluation shows that EasyCrash transforms 54% of crashes that\ncannot correctly recompute into the correct computation while incurring a\nnegligible performance overhead (1.5% on average). Using EasyCrash and\napplication intrinsic fault tolerance, 82% of crashes can successfully\nrecompute. When EasyCrash is used with a traditional checkpoint scheme, it\nenables up to 24% improvement (15% on average) in system efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:59:07 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ren", "Jie", ""], ["Wu", "Kai", ""], ["Li", "Dong", ""]]}, {"id": "1906.10347", "submitter": "Edward Hu", "authors": "Bodun Hu, Christopher J. Rossbach", "title": "ALTIS: Modernizing GPGPU Benchmarking", "comments": "ISPASS 2020. Project: https://github.com/utcs-scea/altis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Altis, a benchmark suite for modern GPGPU computing.\nPrevious benchmark suites such as Rodinia and SHOC have served the research\ncommunity well, but were developed years ago when hardware was more limited,\nsoftware supported fewer features, and production hardware-accelerated\nworkloads were scarce. Since that time, GPU compute density and memory capacity\nhas grown exponentially, programmability features such as unified memory,\ndemand paging, and HyperQ have matured, and new workloads such as deep neural\nnetworks (DNNs), graph analytics, and crypto-currencies have emerged in\nproduction environments, stressing the hardware and software in ways that\nprevious benchmarks did not anticipate. Drawing inspiration from Rodinia and\nSHOC, Altis is a benchmark suite designed for modern GPU architectures and\nmodern GPU runtimes, representing a diverse set of application domains. By\nadopting and extending applications from Rodinia and SHOC, adding new\napplications, and focusing on CUDA platforms, Altis better represents modern\nGPGPU workloads to enable support GPGPU research in both architecture and\nsystem software.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 06:54:40 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 07:10:39 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Hu", "Bodun", ""], ["Rossbach", "Christopher J.", ""]]}, {"id": "1906.10468", "submitter": "Adam Lev-Libfeld", "authors": "Adam Lev-Libfeld, Alexander Margolin", "title": "Fast Data: Moving beyond from Big Data's map-reduce", "comments": null, "journal-ref": "journal of geopython no. 1, jun 20 2016", "doi": null, "report-no": null, "categories": "cs.DC cs.GL cs.NI cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data may not be the solution many are looking for. The latest rise of Big\nData methods and systems is partly due to the new abilities these techniques\nprovide, partly to the simplicity of the software design and partly because the\nbuzzword itself has value to investors and clients. That said, popularity is\nnot a measure for suitability and the Big Data approach might not be the best\nsolution, or even an applicable one, to many common problems. Namely, time\ndependent problems whose solution may be bound or cached in any manner can\nbenefit greatly from moving to partly stateless, flow oriented functions and\ndata models. This paper presents such a model to substitute the traditional\nmap-shuffle-reduce models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 12:13:27 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Lev-Libfeld", "Adam", ""], ["Margolin", "Alexander", ""]]}, {"id": "1906.10664", "submitter": "Mehmet Aktas", "authors": "Mehmet Fatih Aktas and Emina Soljanin", "title": "Straggler Mitigation at Scale", "comments": "To appear in Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime performance variability at the servers has been a major issue,\nhindering the predictable and scalable performance in modern distributed\nsystems. Executing requests or jobs redundantly over multiple servers has been\nshown to be effective for mitigating variability, both in theory and practice.\nSystems that employ redundancy has drawn significant attention, and numerous\npapers have analyzed the pain and gain of redundancy under various service\nmodels and assumptions on the runtime variability. This paper presents a cost\n(pain) vs. latency (gain) analysis of executing jobs of many tasks by employing\nreplicated or erasure coded redundancy. Tail heaviness of service time\nvariability is decisive on the pain and gain of redundancy and we quantify its\neffect by deriving expressions for the cost and latency. Specifically, we try\nto answer four questions: 1) How do replicated and coded redundancy compare in\nthe cost vs. latency tradeoff? 2) Can we introduce redundancy after waiting\nsome time and expect to reduce the cost? 3) Can relaunching the tasks that\nappear to be straggling after some time help to reduce cost and/or latency? 4)\nIs it effective to use redundancy and relaunching together? We validate the\nanswers we found for each of the questions via simulations that use empirical\ndistributions extracted from a Google cluster data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 16:58:02 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 14:07:57 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Soljanin", "Emina", ""]]}, {"id": "1906.10877", "submitter": "Volodymyr Sokolov", "authors": "Volodymyr Buriachok and Volodymyr Sokolov and Pavlo Skladannyi", "title": "Security Rating Metrics for Distributed Wireless Systems", "comments": null, "journal-ref": "Mathematics. Information Technologies. Education (MoMLeT&DS), 2019", "doi": "10.5281/zenodo.3256219", "report-no": null, "categories": "cs.CR cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The paper examines quantitative assessment of wireless distribution system\nsecurity, as well as an assessment of risks from attacks and security\nviolations. Furthermore, it describes typical security breach and formal attack\nmodels and five methods for assessing security. The proposed normalized method\nfor assessing the degree of security assurance operates with at least three\ncharacteristics, which allows comparatively analyze heterogeneous information\nsystems. The improved calculating formulas have been proposed for two security\nassessment methods, and the elements of functional-cost analysis have been\napplied to calculate the degree of security. To check the results of the\nanalysis, the coefficient of concordance was calculated, which gives\nopportunity to determine the quality of expert assessment. The simultaneous use\nof several models to describe attacks and the effectiveness of countering them\nallows us to create a comprehensive approach to countering modern security\nthreats to information networks at the commercial enterprises and critical\ninfrastructure facilities.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 07:19:23 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Buriachok", "Volodymyr", ""], ["Sokolov", "Volodymyr", ""], ["Skladannyi", "Pavlo", ""]]}, {"id": "1906.10970", "submitter": "Andreas Gocht", "authors": "Andreas Gocht, Robert Sch\\\"one, Mario Bielert", "title": "Q-Learning Inspired Self-Tuning for Energy Efficiency in HPC", "comments": "4 pages short paper, HPCS 2019, AHPC 2019, READEX, HAEC, Horizon2020,\n  H2020 grant agreement number 671657, DFG, CRC 912", "journal-ref": null, "doi": "10.1109/HPCS48598.2019.9188112", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System self-tuning is a crucial task to lower the energy consumption of\ncomputers. Traditional approaches decrease the processor frequency in idle or\nsynchronisation periods. However, in High-Performance Computing (HPC) this is\nnot sufficient: if the executed code is load balanced, there are neither idle\nnor synchronisation phases that can be exploited. Therefore, alternative\nself-tuning approaches are needed, which allow exploiting different compute\ncharacteristics of HPC programs.\n  The novel notion of application regions based on function call stacks,\nintroduced in the Horizon 2020 Project READEX, allows us to define such a\nself-tuning approach. In this paper, we combine these regions with the\nQ-Learning typical state-action maps, which save information about available\nstates, possible actions to take, and the expected rewards. By exploiting the\nexisting processor power interface, we are able to provide direct feedback to\nthe learning process. This approach allows us to save up to 15% energy, while\nonly adding a minor runtime overhead.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 11:00:16 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 14:45:28 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Gocht", "Andreas", ""], ["Sch\u00f6ne", "Robert", ""], ["Bielert", "Mario", ""]]}, {"id": "1906.11175", "submitter": "Yann Beilliard", "authors": "Yann Beilliard, Maxime Godard, Aggelos Ioannou, Astrinos Damianakis,\n  Michael Ligerakis, Iakovos Mavroidis, Pierre-Yves Martinez, David Danovitch,\n  Julien Sylvestre, Dominique Drouin", "title": "FPGA-based Multi-Chip Module for High-Performance Computing", "comments": "HiPEAC 2019 - ExaNoDe Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current integration, architectural design and manufacturing technologies are\nnot suited for the computing density and power efficiency requested by Exascale\ncomputing. New approaches in hardware architecture are thus needed to overcome\nthe technological barriers preventing the transition to the Exascale era. In\nthat scope, we report successful fabrication of first ExaNoDe's MCM prototypes\ndedicated to Exascale computing applications. Each MCM was composed of 2 Xilinx\nZynq Ultrascale+ MPSoC, assembled on advanced 68.5 mm x 55 mm laminate\nsubstrates specifically designed and fabricated for the project. Acoustic\nmicroscopy, x-ray, cross-section and Thermo-Moire investigations revealed no\nvoids, shorts, delamination, cracks or warpage issues. Two MCMs were mounted on\na daughter board by FORTH for testing purposes. The DDR memories on the 4\nSODIMMs of the daughter board were successfully tested by running extensive\nXilinx memory tests with clock frequencies of 1866 MHz and 2133 MHz. All 4\nFPGAs were programmed with the Xilinx integrated bit error ratio test (IBERT)\ntailored for this board for links testing. All intra-board high-speed links\nbetween all FPGAs were stable at 10 Gbps, even under the more demanding 31-bit\nPRBS (Pseudorandom Binary Sequence) tests.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 15:43:34 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Beilliard", "Yann", ""], ["Godard", "Maxime", ""], ["Ioannou", "Aggelos", ""], ["Damianakis", "Astrinos", ""], ["Ligerakis", "Michael", ""], ["Mavroidis", "Iakovos", ""], ["Martinez", "Pierre-Yves", ""], ["Danovitch", "David", ""], ["Sylvestre", "Julien", ""], ["Drouin", "Dominique", ""]]}, {"id": "1906.11204", "submitter": "S\\'ebastien Vaucher", "authors": "S\\'ebastien Vaucher and Valerio Schiavoni and Pascal Felber", "title": "Stress-SGX: Load and Stress your Enclaves for Fun and Profit", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": "in Networked Systems, Springer International Publishing, 2019, pp.\n  358-363", "doi": "10.1007/978-3-030-05529-5_24", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest generation of Intel processors supports Software Guard Extensions\n(SGX), a set of instructions that implements a Trusted Execution Environment\n(TEE) right inside the CPU, by means of so-called enclaves. This paper presents\nStress-SGX, an easy-to-use stress-test tool to evaluate the performance of\nSGX-enabled nodes. We build on top of the popular Stress-NG tool, while only\nkeeping the workload injectors (stressors) that are meaningful in the SGX\ncontext. We report on several insights and lessons learned about porting legacy\ncode to run inside an SGX enclave, as well as the limitations introduced by\nthis process. Finally, we use Stress-SGX to conduct a study comparing the\nperformance of different SGX-enabled machines.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 16:47:44 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Vaucher", "S\u00e9bastien", ""], ["Schiavoni", "Valerio", ""], ["Felber", "Pascal", ""]]}, {"id": "1906.11307", "submitter": "Vijay Janapa Reddi", "authors": "Matthew Halpern, Behzad Boroujerdian, Todd Mummert, Evelyn\n  Duesterwald, Vijay Janapa Reddi", "title": "One Size Does Not Fit All: Quantifying and Exposing the Accuracy-Latency\n  Trade-off in Machine Learning Cloud Service APIs via Tolerance Tiers", "comments": "2019 IEEE International Symposium on Performance Analysis of Systems\n  and Software (ISPASS)", "journal-ref": null, "doi": "10.1109/ISPASS.2019.00012", "report-no": null, "categories": "cs.LG cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's cloud service architectures follow a \"one size fits all\" deployment\nstrategy where the same service version instantiation is provided to the end\nusers. However, consumers are broad and different applications have different\naccuracy and responsiveness requirements, which as we demonstrate renders the\n\"one size fits all\" approach inefficient in practice. We use a production-grade\nspeech recognition engine, which serves several thousands of users, and an open\nsource computer vision based system, to explain our point. To overcome the\nlimitations of the \"one size fits all\" approach, we recommend Tolerance Tiers\nwhere each MLaaS tier exposes an accuracy/responsiveness characteristic, and\nconsumers can programmatically select a tier. We evaluate our proposal on the\nCPU-based automatic speech recognition (ASR) engine and cutting-edge neural\nnetworks for image classification deployed on both CPUs and GPUs. The results\nshow that our proposed approach provides an MLaaS cloud service architecture\nthat can be tuned by the end API user or consumer to outperform the\nconventional \"one size fits all\" approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 19:35:59 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Halpern", "Matthew", ""], ["Boroujerdian", "Behzad", ""], ["Mummert", "Todd", ""], ["Duesterwald", "Evelyn", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1906.12066", "submitter": "Pengfei Su", "authors": "Pengfei Su, Qingsen Wang, Milind Chabbi, Xu Liu", "title": "Pinpointing Performance Inefficiencies in Java", "comments": "This is a full-version of our ESEC/FSE'2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many performance inefficiencies such as inappropriate choice of algorithms or\ndata structures, developers' inattention to performance, and missed compiler\noptimizations show up as wasteful memory operations. Wasteful memory operations\nare those that produce/consume data to/from memory that may have been avoided.\nWe present, JXPerf, a lightweight performance analysis tool for pinpointing\nwasteful memory operations in Java programs. Traditional byte-code\ninstrumentation for such analysis (1) introduces prohibitive overheads and (2)\nmisses inefficiencies in machine code generation. JXPerf overcomes both of\nthese problems. JXPerf uses hardware performance monitoring units to sample\nmemory locations accessed by a program and uses hardware debug registers to\nmonitor subsequent accesses to the same memory. The result is a lightweight\nmeasurement at machine-code level with attribution of inefficiencies to their\nprovenance: machine and source code within full calling contexts. JXPerf\nintroduces only 7% runtime overhead and 7% memory overhead making it useful in\nproduction. Guided by JXPerf, we optimize several Java applications by\nimproving code generation and choosing superior data structures and algorithms,\nwhich yield significant speedups.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:25:24 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Su", "Pengfei", ""], ["Wang", "Qingsen", ""], ["Chabbi", "Milind", ""], ["Liu", "Xu", ""]]}]