[{"id": "2105.00039", "submitter": "Ahmad Hesam", "authors": "Ahmad Hesam, Lukas Breitwieser, Fons Rademakers, Zaid Al-Ars", "title": "GPU Acceleration of 3D Agent-Based Biological Simulations", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": "10.1109/IPDPSW52791.2021.00040", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researchers in biology are faced with the tough challenge of developing\nhigh-performance computer simulations of their increasingly complex agent-based\nmodels. BioDynaMo is an open-source agent-based simulation platform that aims\nto alleviate researchers from the intricacies that go into the development of\nhigh-performance computing. Through a high-level interface, researchers can\nimplement their models on top of BioDynaMo's multi-threaded core execution\nengine to rapidly develop simulations that effectively utilize parallel\ncomputing hardware. In biological agent-based modeling, the type of operations\nthat are typically the most compute-intensive are those that involve agents\ninteracting with their local neighborhood. In this work, we investigate the\ncurrently implemented method of handling neighborhood interactions of cellular\nagents in BioDynaMo, and ways to improve the performance to enable large-scale\nand complex simulations. We propose to replace the kd-tree implementation to\nfind and iterate over the neighborhood of each agent with a uniform grid method\nthat allows us to take advantage of the massively parallel architecture of\ngraphics processing units (GPUs). We implement the uniform grid method in both\nCUDA and OpenCL to address GPUs from all major vendors and evaluate several\ntechniques to further improve the performance. Furthermore, we analyze the\nperformance of our implementations for models with a varying density of\nneighboring agents. As a result, the performance of the mechanical interactions\nmethod improved by up to two orders of magnitude in comparison to the\nmultithreaded baseline version. The implementations are open-source and\npublicly available on Github.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:37:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hesam", "Ahmad", ""], ["Breitwieser", "Lukas", ""], ["Rademakers", "Fons", ""], ["Al-Ars", "Zaid", ""]]}, {"id": "2105.01677", "submitter": "Andr\\'e Luiz Vieira E Silva", "authors": "Andr\\'e Luiz Buarque Vieira e Silva, Caio Jos\\'e dos Santos Brito,\n  Francisco Paulo Magalh\\~aes Sim\\~oes, Veronica Teichrieb", "title": "A fluid simulation system based on the MPS method", "comments": null, "journal-ref": "Computer Physics Communications 258 (2021): 107572", "doi": "10.1016/j.cpc.2020.107572", "report-no": null, "categories": "physics.comp-ph cs.PF physics.flu-dyn", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Fluid flow simulation is a highly active area with applications in a wide\nrange of engineering problems and interactive systems. Meshless methods like\nthe Moving Particle Semi-implicit (MPS) are a great alternative to deal\nefficiently with large deformations and free-surface flow. However, mesh-based\napproaches can achieve higher numerical precision than particle-based\ntechniques with a performance cost. This paper presents a numerically stable\nand parallelized system that benefits from advances in the literature and\nparallel computing to obtain an adaptable MPS method. The proposed technique\ncan simulate liquids using different approaches, such as two ways to calculate\nthe particles' pressure, turbulent flow, and multiphase interaction. The method\nis evaluated under traditional test cases presenting comparable results to\nrecent techniques. This work integrates the previously mentioned advances into\na single solution, which can switch on improvements, such as better momentum\nconservation and less spurious pressure oscillations, through a graphical\ninterface. The code is entirely open-source under the GPLv3 free software\nlicense. The GPU-accelerated code reached speedups ranging from 3 to 43 times,\ndepending on the total number of particles. The simulation runs at one fps for\na case with approximately 200,000 particles. Code:\nhttps://github.com/andreluizbvs/VoxarMPS\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:00:15 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Silva", "Andr\u00e9 Luiz Buarque Vieira e", ""], ["Brito", "Caio Jos\u00e9 dos Santos", ""], ["Sim\u00f5es", "Francisco Paulo Magalh\u00e3es", ""], ["Teichrieb", "Veronica", ""]]}, {"id": "2105.01792", "submitter": "Xinlong Yin", "authors": "Ranjan Pal, Ziyuan Huang, Xinlong Yin, Sergey Lototsky, Swades De,\n  Sasu Tarkoma, Mingyan Liu, Jon Crowcroft, Nishanth Sastry", "title": "Aggregate Cyber-Risk Management in the IoT Age: Cautionary Statistics\n  for (Re)Insurers and Likes", "comments": "incrementally updated version to version in IEEE Internet of Things\n  Journal", "journal-ref": null, "doi": "10.1109/JIOT.2020.3039254", "report-no": null, "categories": "cs.PF cs.SY eess.SY q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we provide (i) a rigorous general theory to elicit conditions\non (tail-dependent) heavy-tailed cyber-risk distributions under which a risk\nmanagement firm might find it (non)sustainable to provide aggregate cyber-risk\ncoverage services for smart societies, and (ii)a real-data driven numerical\nstudy to validate claims made in theory assuming boundedly rational cyber-risk\nmanagers, alongside providing ideas to boost markets that aggregate dependent\ncyber-risks with heavy-tails.To the best of our knowledge, this is the only\ncomplete general theory till date on the feasibility of aggregate cyber-risk\nmanagement.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 23:10:43 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Pal", "Ranjan", ""], ["Huang", "Ziyuan", ""], ["Yin", "Xinlong", ""], ["Lototsky", "Sergey", ""], ["De", "Swades", ""], ["Tarkoma", "Sasu", ""], ["Liu", "Mingyan", ""], ["Crowcroft", "Jon", ""], ["Sastry", "Nishanth", ""]]}, {"id": "2105.01892", "submitter": "Liqiang Lu", "authors": "Liqiang Lu, Naiqing Guan, Yuyue Wang, Liancheng Jia, Zizhang Luo,\n  Jieming Yin, Jason Cong, Yun Liang", "title": "TENET: A Framework for Modeling Tensor Dataflow Based on\n  Relation-centric Notation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating tensor applications on spatial architectures provides high\nperformance and energy-efficiency, but requires accurate performance models for\nevaluating various dataflow alternatives. Such modeling relies on the notation\nof tensor dataflow and the formulation of performance metrics. Recent proposed\ncompute-centric and data-centric notations describe the dataflow using\nimperative directives. However, these two notations are less expressive and\nthus lead to limited optimization opportunities and inaccurate performance\nmodels.\n  In this paper, we propose a framework TENET that models hardware dataflow of\ntensor applications. We start by introducing a relation-centric notation, which\nformally describes the hardware dataflow for tensor computation. The\nrelation-centric notation specifies the hardware dataflow, PE interconnection,\nand data assignment in a uniform manner using relations. The relation-centric\nnotation is more expressive than the compute-centric and data-centric notations\nby using more sophisticated affine transformations. Another advantage of\nrelation-centric notation is that it inherently supports accurate metrics\nestimation, including data reuse, bandwidth, latency, and energy. TENET\ncomputes each performance metric by counting the relations using integer set\nstructures and operators. Overall, TENET achieves 37.4\\% and 51.4\\% latency\nreduction for CONV and GEMM kernels compared with the state-of-the-art\ndata-centric notation by identifying more sophisticated hardware dataflows.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 06:50:57 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Lu", "Liqiang", ""], ["Guan", "Naiqing", ""], ["Wang", "Yuyue", ""], ["Jia", "Liancheng", ""], ["Luo", "Zizhang", ""], ["Yin", "Jieming", ""], ["Cong", "Jason", ""], ["Liang", "Yun", ""]]}, {"id": "2105.02926", "submitter": "Tejas Bodas", "authors": "Tim Hellemans, Arti Yardi, Tejas Bodas", "title": "Download time analysis for distributed storage systems with node\n  failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed storage system which stores several hot (popular)\nand cold (less popular) data files across multiple nodes or servers. Hot files\nare stored using repetition codes while cold files are stored using erasure\ncodes. The nodes are prone to failure and hence at any given time, we assume\nthat only a fraction of the nodes are available. Using a cavity process based\nmean field framework, we analyze the download time for users accessing hot or\ncold data in the presence of failed nodes. Our work also illustrates the impact\nof the choice of the storage code on the download time performance of users in\nthe system.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 19:47:37 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hellemans", "Tim", ""], ["Yardi", "Arti", ""], ["Bodas", "Tejas", ""]]}, {"id": "2105.02972", "submitter": "Karla Vargas", "authors": "Carlos L\\'opez, Sergio Rajsbaum, Michel Raynal, Karla Vargas", "title": "Leader Election in Arbitrarily Connected Networks with Process Crashes\n  and Weak Channel Reliability", "comments": "24 pages, 4 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A channel from a process p to a process q satisfies the ADD property if there\nare constants K and D, unknown to the processes, such that in any sequence of K\nconsecutive messages sent by p to q, at least one of them is delivered to q at\nmost D time units after it has been sent. This paper studies implementations of\nan eventual leader, namely, an {\\Omega} failure detector, in an arbitrarily\nconnected network of eventual ADD channels, where processes may fail by\ncrashing. It first presents an algorithm that assumes that processes initially\nknow n, the total number of processes, sending messages of size O( log n).\nThen, it presents a second algorithm that does not assume the processes know n.\nEventually the size of the messages sent by this algorithm is also O( log n).\nThese are the first implementations of leader election in the ADD model. In\nthis model, only eventually perfect failure detectors were considered, sending\nmessages of size O(n log n).\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 21:21:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["L\u00f3pez", "Carlos", ""], ["Rajsbaum", "Sergio", ""], ["Raynal", "Michel", ""], ["Vargas", "Karla", ""]]}, {"id": "2105.03215", "submitter": "Yida Wang", "authors": "Zhi Chen, Cody Hao Yu, Trevor Morris, Jorn Tuyls, Yi-Hsiang Lai, Jared\n  Roesch, Elliott Delaye, Vin Sharma, Yida Wang", "title": "Bring Your Own Codegen to Deep Learning Compiler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been ubiquitously applied in many\napplications, and accelerators are emerged as an enabler to support the fast\nand efficient inference tasks of these applications. However, to achieve high\nmodel coverage with high performance, each accelerator vendor has to develop a\nfull compiler stack to ingest, optimize, and execute the DNNs. This poses\nsignificant challenges in the development and maintenance of the software\nstack. In addition, the vendors have to contiguously update their hardware\nand/or software to cope with the rapid evolution of the DNN model architectures\nand operators. To address these issues, this paper proposes an open source\nframework that enables users to only concentrate on the development of their\nproprietary code generation tools by reusing as many as possible components in\nthe existing deep learning compilers. Our framework provides users flexible and\neasy-to-use interfaces to partition their models into segments that can be\nexecuted on \"the best\" processors to take advantage of the powerful computation\ncapability of accelerators. Our case study shows that our framework has been\ndeployed in multiple commercial vendors' compiler stacks with only a few\nthousand lines of code.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:22:25 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Chen", "Zhi", ""], ["Yu", "Cody Hao", ""], ["Morris", "Trevor", ""], ["Tuyls", "Jorn", ""], ["Lai", "Yi-Hsiang", ""], ["Roesch", "Jared", ""], ["Delaye", "Elliott", ""], ["Sharma", "Vin", ""], ["Wang", "Yida", ""]]}, {"id": "2105.03592", "submitter": "Chen Wang", "authors": "Jian Chen, Xuxin Zhang, Rui Zhang, Chen Wang, Ling Liu", "title": "De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks", "comments": "To be published in IEEE Transactions on Information Forensics and\n  Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have been widely applied to various applications.\nHowever, they are potentially vulnerable to data poisoning attacks, where\nsophisticated attackers can disrupt the learning procedure by injecting a\nfraction of malicious samples into the training dataset. Existing defense\ntechniques against poisoning attacks are largely attack-specific: they are\ndesigned for one specific type of attacks but do not work for other types,\nmainly due to the distinct principles they follow. Yet few general defense\nstrategies have been developed. In this paper, we propose De-Pois, an\nattack-agnostic defense against poisoning attacks. The key idea of De-Pois is\nto train a mimic model the purpose of which is to imitate the behavior of the\ntarget model trained by clean samples. We take advantage of Generative\nAdversarial Networks (GANs) to facilitate informative training data\naugmentation as well as the mimic model construction. By comparing the\nprediction differences between the mimic model and the target model, De-Pois is\nthus able to distinguish the poisoned samples from clean ones, without explicit\nknowledge of any ML algorithms or types of poisoning attacks. We implement four\ntypes of poisoning attacks and evaluate De-Pois with five typical defense\nmethods on different realistic datasets. The results demonstrate that De-Pois\nis effective and efficient for detecting poisoned data against all the four\ntypes of poisoning attacks, with both the accuracy and F1-score over 0.9 on\naverage.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 04:47:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Jian", ""], ["Zhang", "Xuxin", ""], ["Zhang", "Rui", ""], ["Wang", "Chen", ""], ["Liu", "Ling", ""]]}, {"id": "2105.03725", "submitter": "Geraldo Francisco De Oliveira Junior", "authors": "Geraldo F. Oliveira and Juan G\\'omez-Luna and Lois Orosa and Saugata\n  Ghose and Nandita Vijaykumar and Ivan Fernandez and Mohammad Sadrosadati and\n  Onur Mutlu", "title": "DAMOV: A New Methodology and Benchmark Suite for Evaluating Data\n  Movement Bottlenecks", "comments": "Our open source software is available at\n  https://github.com/CMU-SAFARI/DAMOV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data movement between the CPU and main memory is a first-order obstacle\nagainst improving performance, scalability, and energy efficiency in modern\nsystems. Computer systems employ a range of techniques to reduce overheads tied\nto data movement, spanning from traditional mechanisms (e.g., deep multi-level\ncache hierarchies, aggressive hardware prefetchers) to emerging techniques such\nas Near-Data Processing (NDP), where some computation is moved close to memory.\nOur goal is to methodically identify potential sources of data movement over a\nbroad set of applications and to comprehensively compare traditional\ncompute-centric data movement mitigation techniques to more memory-centric\ntechniques, thereby developing a rigorous understanding of the best techniques\nto mitigate each source of data movement.\n  With this goal in mind, we perform the first large-scale characterization of\na wide variety of applications, across a wide range of application domains, to\nidentify fundamental program properties that lead to data movement to/from main\nmemory. We develop the first systematic methodology to classify applications\nbased on the sources contributing to data movement bottlenecks. From our\nlarge-scale characterization of 77K functions across 345 applications, we\nselect 144 functions to form the first open-source benchmark suite (DAMOV) for\nmain memory data movement studies. We select a diverse range of functions that\n(1) represent different types of data movement bottlenecks, and (2) come from a\nwide range of application domains. Using NDP as a case study, we identify new\ninsights about the different data movement bottlenecks and use these insights\nto determine the most suitable data movement mitigation mechanism for a\nparticular application. We open-source DAMOV and the complete source code for\nour new characterization methodology at https://github.com/CMU-SAFARI/DAMOV.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 16:02:53 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:03:34 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 15:57:09 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 17:05:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Oliveira", "Geraldo F.", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Orosa", "Lois", ""], ["Ghose", "Saugata", ""], ["Vijaykumar", "Nandita", ""], ["Fernandez", "Ivan", ""], ["Sadrosadati", "Mohammad", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.03814", "submitter": "Juan G\\'omez-Luna", "authors": "Juan G\\'omez-Luna, Izzat El Hajj, Ivan Fernandez, Christina Giannoula,\n  Geraldo F. Oliveira, Onur Mutlu", "title": "Benchmarking a New Paradigm: An Experimental Analysis of a Real\n  Processing-in-Memory Architecture", "comments": "Our open source software is available at\n  https://github.com/CMU-SAFARI/prim-benchmarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many modern workloads, such as neural networks, databases, and graph\nprocessing, are fundamentally memory-bound. For such workloads, the data\nmovement between main memory and CPU cores imposes a significant overhead in\nterms of both latency and energy. A major reason is that this communication\nhappens through a narrow bus with high latency and limited bandwidth, and the\nlow data reuse in memory-bound workloads is insufficient to amortize the cost\nof main memory access. Fundamentally addressing this data movement bottleneck\nrequires a paradigm where the memory system assumes an active role in computing\nby integrating processing capabilities. This paradigm is known as\nprocessing-in-memory (PIM).\n  Recent research explores different forms of PIM architectures, motivated by\nthe emergence of new 3D-stacked memory technologies that integrate memory with\na logic layer where processing elements can be easily placed. Past works\nevaluate these architectures in simulation or, at best, with simplified\nhardware prototypes. In contrast, the UPMEM company has designed and\nmanufactured the first publicly-available real-world PIM architecture.\n  This paper provides the first comprehensive analysis of the first\npublicly-available real-world PIM architecture. We make two key contributions.\nFirst, we conduct an experimental characterization of the UPMEM-based PIM\nsystem using microbenchmarks to assess various architecture limits such as\ncompute throughput and memory bandwidth, yielding new insights. Second, we\npresent PrIM, a benchmark suite of 16 workloads from different application\ndomains (e.g., linear algebra, databases, graph processing, neural networks,\nbioinformatics).\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:57:47 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:05:02 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 20:58:40 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 16:39:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["G\u00f3mez-Luna", "Juan", ""], ["Hajj", "Izzat El", ""], ["Fernandez", "Ivan", ""], ["Giannoula", "Christina", ""], ["Oliveira", "Geraldo F.", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.04151", "submitter": "Xinyu Chen", "authors": "Xinyu Chen, Hongshi Tan, Yao Chen, Bingsheng He, Weng-Fai Wong, Deming\n  Chen", "title": "Skew-Oblivious Data Routing for Data-Intensive Applications on FPGAs\n  with HLS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  FPGAs have become emerging computing infrastructures for accelerating\napplications in datacenters. Meanwhile, high-level synthesis (HLS) tools have\nbeen proposed to ease the programming of FPGAs. Even with HLS, irregular\ndata-intensive applications require explicit optimizations, among which\nmultiple processing elements (PEs) with each owning a private BRAM-based buffer\nare usually adopted to process multiple data per cycle. Data routing, which\ndynamically dispatches multiple data to designated PEs, avoids data replication\nin buffers compared to statically assigning data to PEs, hence saving BRAM\nusage. However, the workload imbalance among PEs vastly diminishes performance\nwhen processing skew datasets. In this paper, we propose a skew-oblivious data\nrouting architecture that allocates secondary PEs and schedules them to share\nthe workload of the overloaded PEs at run-time. In addition, we integrate the\nproposed architecture into a framework called Ditto to minimize the development\nefforts for applications that require skew handling. We evaluate Ditto on five\ncommonly used applications: histogram building, data partitioning, pagerank,\nheavy hitter detection and hyperloglog. The results demonstrate that the\ngenerated implementations are robust to skew datasets and outperform the\nstateof-the-art designs in both throughput and BRAM usage efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 07:14:43 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Xinyu", ""], ["Tan", "Hongshi", ""], ["Chen", "Yao", ""], ["He", "Bingsheng", ""], ["Wong", "Weng-Fai", ""], ["Chen", "Deming", ""]]}, {"id": "2105.04547", "submitter": "Chengdong Yao", "authors": "Chengdong Yao", "title": "Highly Efficient Memory Failure Prediction using Mcelog-based Data\n  Mining and Machine Learning", "comments": "11 pages, 2 figures, 1 table. Codes has been open source to\n  https://www.github.com/ycd2016/acaioc2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.PF cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the data center, unexpected downtime caused by memory failures can lead to\na decline in the stability of the server and even the entire information\ntechnology infrastructure, which harms the business. Therefore, whether the\nmemory failure can be accurately predicted in advance has become one of the\nmost important issues to be studied in the data center. However, for the memory\nfailure prediction in the production system, it is necessary to solve technical\nproblems such as huge data noise and extreme imbalance between positive and\nnegative samples, and at the same time ensure the long-term stability of the\nalgorithm. This paper compares and summarizes some commonly used skills and the\nimprovement they can bring. The single model we proposed won the top 14th in\nthe 2nd Alibaba Cloud AIOps Competition belonging to the 25th PAKDD conference.\nIt takes only 30 minutes to pass the online test, while most of the other\ncontestants' solution need more than 3 hours. Codes has been open source to\nhttps://www.github.com/ycd2016/acaioc2.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 11:38:05 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 05:38:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yao", "Chengdong", ""]]}, {"id": "2105.04555", "submitter": "Jaehoon Koo", "authors": "Jaehoon Koo, Prasanna Balaprakash, Michael Kruse, Xingfu Wu, Paul\n  Hovland, Mary Hall", "title": "Customized Monte Carlo Tree Search for LLVM/Polly's Composable Loop\n  Optimization Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Polly is the LLVM project's polyhedral loop nest optimizer. Recently,\nuser-directed loop transformation pragmas were proposed based on LLVM/Clang and\nPolly. The search space exposed by the transformation pragmas is a tree,\nwherein each node represents a specific combination of loop transformations\nthat can be applied to the code resulting from the parent node's loop\ntransformations. We have developed a search algorithm based on Monte Carlo tree\nsearch (MCTS) to find the best combination of loop transformations. Our\nalgorithm consists of two phases: exploring loop transformations at different\ndepths of the tree to identify promising regions in the tree search space and\nexploiting those regions by performing a local search. Moreover, a restart\nmechanism is used to avoid the MCTS getting trapped in a local solution. The\nbest and worst solutions are transferred from the previous phases of the\nrestarts to leverage the search history. We compare our approach with random,\ngreedy, and breadth-first search methods on PolyBench kernels and ECP proxy\napplications. Experimental results show that our MCTS algorithm finds pragma\ncombinations with a speedup of 2.3x over Polly's heuristic optimizations on\naverage.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:57:39 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Koo", "Jaehoon", ""], ["Balaprakash", "Prasanna", ""], ["Kruse", "Michael", ""], ["Wu", "Xingfu", ""], ["Hovland", "Paul", ""], ["Hall", "Mary", ""]]}, {"id": "2105.06619", "submitter": "Jianshen Liu", "authors": "Jianshen Liu, Carlos Maltzahn, Craig Ulmer, Matthew Leon Curry", "title": "Performance Characteristics of the BlueField-2 SmartNIC", "comments": "13 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance computing (HPC) researchers have long envisioned scenarios\nwhere application workflows could be improved through the use of programmable\nprocessing elements embedded in the network fabric. Recently, vendors have\nintroduced programmable Smart Network Interface Cards (SmartNICs) that enable\ncomputations to be offloaded to the edge of the network. There is great\ninterest in both the HPC and high-performance data analytics communities in\nunderstanding the roles these devices may play in the data paths of upcoming\nsystems.\n  This paper focuses on characterizing both the networking and computing\naspects of NVIDIA's new BlueField-2 SmartNIC when used in an Ethernet\nenvironment. For the networking evaluation we conducted multiple transfer\nexperiments between processors located at the host, the SmartNIC, and a remote\nhost. These tests illuminate how much processing headroom is available on the\nSmartNIC during transfers. For the computing evaluation we used the stress-ng\nbenchmark to compare the BlueField-2 to other servers and place realistic\nbounds on the types of offload operations that are appropriate for the\nhardware.\n  Our findings from this work indicate that while the BlueField-2 provides a\nflexible means of processing data at the network's edge, great care must be\ntaken to not overwhelm the hardware. While the host can easily saturate the\nnetwork link, the SmartNIC's embedded processors may not have enough computing\nresources to sustain more than half the expected bandwidth when using\nkernel-space packet processing. From a computational perspective, encryption\noperations, memory operations under contention, and on-card IPC operations on\nthe SmartNIC perform significantly better than the general-purpose servers used\nfor comparisons in our experiments. Therefore, applications that mainly focus\non these operations may be good candidates for offloading to the SmartNIC.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 02:25:04 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Liu", "Jianshen", ""], ["Maltzahn", "Carlos", ""], ["Ulmer", "Craig", ""], ["Curry", "Matthew Leon", ""]]}, {"id": "2105.07776", "submitter": "Julien Girard-Satabin", "authors": "Julien Girard-Satabin (LIST, TAU), Aymeric Varasse (LIST), Marc\n  Schoenauer (TAU), Guillaume Charpiat (TAU), Zakaria Chihani (LIST)", "title": "DISCO Verification: Division of Input Space into COnvex polytopes for\n  neural network verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impressive results of modern neural networks partly come from their non\nlinear behaviour. Unfortunately, this property makes it very difficult to apply\nformal verification tools, even if we restrict ourselves to networks with a\npiecewise linear structure. However, such networks yields subregions that are\nlinear and thus simpler to analyse independently. In this paper, we propose a\nmethod to simplify the verification problem by operating a partitionning into\nmultiple linear subproblems. To evaluate the feasibility of such an approach,\nwe perform an empirical analysis of neural networks to estimate the number of\nlinear regions, and compare them to the bounds currently known. We also present\nthe impact of a technique aiming at reducing the number of linear regions\nduring training.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:40:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Girard-Satabin", "Julien", "", "LIST, TAU"], ["Varasse", "Aymeric", "", "LIST"], ["Schoenauer", "Marc", "", "TAU"], ["Charpiat", "Guillaume", "", "TAU"], ["Chihani", "Zakaria", "", "LIST"]]}, {"id": "2105.08694", "submitter": "Zhujun Xiao", "authors": "Zhujun Xiao, Zhengxu Xia, Haitao Zheng, Ben Y. Zhao, Junchen Jiang", "title": "Towards Performance Clarity of Edge Video Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Edge video analytics is becoming the solution to many safety and management\ntasks. Its wide deployment, however, must first address the tension between\ninference accuracy and resource (compute/network) cost. This has led to the\ndevelopment of video analytics pipelines (VAPs), which reduce resource cost by\ncombining DNN compression/speedup techniques with video processing heuristics.\nOur measurement study on existing VAPs, however, shows that today's methods for\nevaluating VAPs are incomplete, often producing premature conclusions or\nambiguous results. This is because each VAP's performance varies substantially\nacross videos and time (even under the same scenario) and is sensitive to\ndifferent subsets of video content characteristics.\n  We argue that accurate VAP evaluation must first characterize the complex\ninteraction between VAPs and video characteristics, which we refer to as VAP\nperformance clarity. We design and implement Yoda, the first VAP benchmark to\nachieve performance clarity. Using primitive-based profiling and a carefully\ncurated benchmark video set, Yoda builds a performance clarity profile for each\nVAP to precisely define its accuracy/cost tradeoff and its relationship with\nvideo characteristics. We show that Yoda substantially improves VAP evaluations\nby (1) providing a comprehensive, transparent assessment of VAP performance and\nits dependencies on video characteristics; (2) explicitly identifying\nfine-grained VAP behaviors that were previously hidden by large performance\nvariance; and (3) revealing strengths/weaknesses among different VAPs and new\ndesign opportunities.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:29:58 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Xiao", "Zhujun", ""], ["Xia", "Zhengxu", ""], ["Zheng", "Haitao", ""], ["Zhao", "Ben Y.", ""], ["Jiang", "Junchen", ""]]}, {"id": "2105.08706", "submitter": "Gal Sela", "authors": "Gal Sela, Erez Petrank", "title": "Durable Queues: The Second Amendment", "comments": "Code: https://github.com/galysela/DurableQueues", "journal-ref": "Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms\n  and Architectures (SPAA '21), July 6-8, 2021, Virtual Event, USA. ACM, New\n  York, NY, USA, 385-397", "doi": "10.1145/3409964.3461791", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider durable data structures for non-volatile main memory, such as the\nnew Intel Optane memory architecture. Substantial recent work has concentrated\non making concurrent data structures durable with low overhead, by adding a\nminimal number of blocking persist operations (i.e., flushes and fences). In\nthis work we show that focusing on minimizing the number of persist\ninstructions is important, but not enough. We show that access to flushed\ncontent is of high cost due to cache invalidation in current architectures.\nGiven this finding, we present a design of the queue data structure that\nproperly takes care of minimizing blocking persist operations as well as\nminimizing access to flushed content. The proposed design outperforms\nstate-of-the-art durable queues.\n  We start by providing a durable version of the Michael Scott queue (MSQ). We\namend MSQ by adding a minimal number of persist instructions, fewer than in\navailable durable queues, and meeting the theoretical lower bound on the number\nof blocking persist operations. We then proceed with a second amendment to this\ndesign, that eliminates accesses to flushed data. Evaluation shows that the\nsecond amendment yields substantial performance improvement, outperforming the\nstate of the art and demonstrating the importance of reduced accesses to\nflushed content. The presented queues are durably linearizable and lock-free.\nFinally, we discuss the theoretical optimal number of accesses to flushed\ncontent.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:45:37 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 15:42:37 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Sela", "Gal", ""], ["Petrank", "Erez", ""]]}, {"id": "2105.09004", "submitter": "Mario Di Mauro", "authors": "Mario Di Mauro, Giovanni Galatro, Fabio Postiglione, Marco Tambasco", "title": "Performability of Network Service Chains: Stochastic Modeling and\n  Assessment of Softwarized IP Multimedia Subsystem", "comments": null, "journal-ref": null, "doi": "10.1109/TDSC.2021.3082626", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service provisioning mechanisms implemented across 5G infrastructures take\nbroadly into use the network service chain concept. Typically, it is coupled\nwith Network Function Virtualization (NFV) paradigm, and consists in defining a\npre-determined path traversed by a set of softwarized network nodes to provide\nspecific services. A well known chain-like framework is the IP Multimedia\nSubsystem (IMS), a key infrastructure of 5G networks, that we characterize both\nby a performance and an availability perspective. Precisely, supported by a\ndesigned from scratch testbed realized through Clearwater platform, we perform\na stochastic assessment of a softwarized IMS (softIMS) architecture where two\nmain stages stand out: i) a performance analysis, where, exploiting the\nqueueing network decomposition method, we formalize an optimization problem of\nresource allocation by modeling each softIMS node as an M/G/c system; ii) an\navailability assessment, where, adopting the Stochastic Reward Net methodology,\nwe are able to characterize the behavior of softIMS in terms of failure/repair\nevents, and to derive a set of optimal configurations satisfying a given\navailability requirement (e.g. five nines) while minimizing deployment costs.\nTwo routines dubbed OptCNT and OptSearchChain have been devised to govern the\nperformance and availability analyses, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:13:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Di Mauro", "Mario", ""], ["Galatro", "Giovanni", ""], ["Postiglione", "Fabio", ""], ["Tambasco", "Marco", ""]]}, {"id": "2105.09058", "submitter": "George Chernishev", "authors": "Alexander Slesarev, Evgeniy Klyuchikov, Kirill Smirnov, George\n  Chernishev", "title": "Revisiting Data Compression in Column-Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data compression is widely used in contemporary column-oriented DBMSes to\nlower space usage and to speed up query processing. Pioneering systems have\nintroduced compression to tackle the disk bandwidth bottleneck by trading CPU\nprocessing power for it. The main issue of this is a trade-off between the\ncompression ratio and the decompression CPU cost. Existing results state that\nlight-weight compression with small decompression costs outperforms\nheavy-weight compression schemes in column-stores. However, since the time\nthese results were obtained, CPU, RAM, and disk performance have advanced\nconsiderably. Moreover, novel compression algorithms have emerged.\n  In this paper, we revisit the problem of compression in disk-based\ncolumn-stores. More precisely, we study the I/O-RAM compression scheme which\nimplies that there are two types of pages of different size: disk pages\n(compressed) and in-memory pages (uncompressed). In this scheme, the buffer\nmanager is responsible for decompressing pages as soon as they arrive from\ndisk. This scheme is rather popular as it is easy to implement: several modern\ncolumn and row-stores use it.\n  We pose and address the following research questions: 1) Are heavy-weight\ncompression schemes still inappropriate for disk-based column-stores?, 2) Are\nnew light-weight compression algorithms better than the old ones?, 3) Is there\na need for SIMD-employing decompression algorithms in case of a disk-based\nsystem? We study these questions experimentally using a columnar query engine\nand Star Schema Benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:53:41 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Slesarev", "Alexander", ""], ["Klyuchikov", "Evgeniy", ""], ["Smirnov", "Kirill", ""], ["Chernishev", "George", ""]]}, {"id": "2105.09187", "submitter": "Adri\\'an Castell\\'o", "authors": "Adri\\'an Castell\\'o, Sergio Barrachina, Manuel F. Dolz, Enrique S.\n  Quintana-Ort\\'i, Pau San Juan", "title": "High performance and energy efficient inference for deep learning on ARM\n  processors", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We evolve PyDTNN, a framework for distributed parallel training of Deep\nNeural Networks (DNNs), into an efficient inference tool for convolutional\nneural networks. Our optimization process on multicore ARM processors involves\nseveral high-level transformations of the original framework, such as the\ndevelopment and integration of Cython routines to exploit thread-level\nparallelism; the design and development of micro-kernels for the matrix\nmultiplication, vectorized with ARMs NEON intrinsics, that can accommodate\nlayer fusion; and the appropriate selection of several cache configuration\nparameters tailored to the memory hierarchy of the target ARM processors. Our\nexperiments evaluate both inference throughput (measured in processed images/s)\nand inference latency (i.e., time-to-response) as well as energy consumption\nper image when varying the level of thread parallelism and the processor power\nmodes. The experiments with the new inference engine are reported for the\nResNet50 v1.5 model on the ImageNet dataset from the MLPerf suite using the ARM\nv8.2 cores in the NVIDIA Jetson AGX Xavier board. These results show superior\nperformance compared with the well-spread TFLite from Google and slightly\ninferior results when compared with ArmNN, the native library from ARM for DNN\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:05:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Castell\u00f3", "Adri\u00e1n", ""], ["Barrachina", "Sergio", ""], ["Dolz", "Manuel F.", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""], ["Juan", "Pau San", ""]]}, {"id": "2105.09508", "submitter": "Wentao Cai", "authors": "Wentao Cai, Haosen Wen, Vladimir Maksimovski, Mingzhe Du, Rafaello\n  Sanna, Shreif Abdallah, Michael L. Scott", "title": "Fast Nonblocking Persistence for Concurrent Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a fully lock-free variant of the recent Montage system for\npersistent data structures. Our variant, nbMontage, adds persistence to almost\nany nonblocking concurrent structure without introducing significant overhead\nor blocking of any kind. Like its predecessor, nbMontage is buffered durably\nlinearizable: it guarantees that the state recovered in the wake of a crash\nwill represent a consistent prefix of pre-crash execution. Unlike its\npredecessor, nbMontage ensures wait-free progress of the persistence frontier,\nthereby bounding the number of recent updates that may be lost on a crash, and\nallowing a thread to force an update of the frontier (i.e., to perform a sync\noperation) without the risk of blocking. As an extra benefit, the helping\nmechanism employed by our wait-free sync significantly reduces its latency.\n  Performance results for nonblocking queues, skip lists, trees, and hash\ntables rival custom data structures in the literature -- dramatically faster\nthan achieved with prior general-purpose systems, and generally within 50% of\nequivalent non-persistent structures placed in DRAM.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 04:24:58 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cai", "Wentao", ""], ["Wen", "Haosen", ""], ["Maksimovski", "Vladimir", ""], ["Du", "Mingzhe", ""], ["Sanna", "Rafaello", ""], ["Abdallah", "Shreif", ""], ["Scott", "Michael L.", ""]]}, {"id": "2105.10852", "submitter": "Mouhamed Abdulla Ph.D.", "authors": "Alvin Ramoutar and Zohreh Motamedi and Mouhamed Abdulla", "title": "Latency of Concatenating Unlicensed LPWAN with Cellular IoT: An\n  Experimental QoE Study", "comments": "Experimental dataset is openly available here:\n  https://dx.doi.org/10.21227/zzax-g919", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AR cs.IT cs.PF math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing low-power wide-area network (LPWAN) solutions that are efficient\nto adopt, deploy and maintain are vital for smart cities. The poor\nquality-of-service of unlicensed LPWAN, and the high service cost of\nLTE-M/NB-IoT are key disadvantages of these technologies. Concatenating\nunlicensed with licensed LPWANs can overcome these limitations and harness\ntheir benefits. However, a concatenated LPWAN architecture will inevitably\nresult in excess latency which may impact users' quality-of-experience (QoE).\nTo evaluate the real-life feasibility of this system, we first propose a\nconcatenated LPWAN architecture and experimentally measure the statistics of\nend-to-end (E2E) latencies. The concatenated delay margin is determined by\nbenchmarking the latencies with different LPWAN architecture schemes, namely\nwith unlicensed IoT (standalone LoRa), cellular IoT (standalone LTE-M), and\nconcatenated IoT (LoRa interfaced with LTE-M). Through extensive experimental\nmeasurement campaigns of 30,000 data points of E2E latencies, we show that the\nexcess delay due to LPWAN interfacing introduces on average less than 300\nmilliseconds. The proof-of-concept results suggest that the latency for\nconcatenating unlicensed LPWAN with cellular IoT is negligible for smart city\nuse cases where human perception and decision making is in the loop.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 03:53:37 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 19:06:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ramoutar", "Alvin", ""], ["Motamedi", "Zohreh", ""], ["Abdulla", "Mouhamed", ""]]}, {"id": "2105.11104", "submitter": "Ravi Suman", "authors": "Ravi Suman, Ananth Krishnamurthy", "title": "Conditional Waiting Time Analysis in Tandem Polling Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze a tandem network of polling queues with two product types and two\nstations. We assume that external arrivals to the network follow a Poisson\nprocess, and service times at each station are exponentially distributed. For\nthis system, we determine the mean conditional waiting time for an arriving\ncustomer using a sample path analysis approach. The approach classifies system\nstate upon arrival into scenarios and exploits an inherent structure in the\nsequence of events that occur till the customer departs to obtain conditional\nwaiting time estimates. We conduct numerical studies to show both the accuracy\nof our conditional waiting time estimates and their practical importance.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 05:55:03 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Suman", "Ravi", ""], ["Krishnamurthy", "Ananth", ""]]}, {"id": "2105.11367", "submitter": "Fan Lai", "authors": "Fan Lai, Yinwei Dai, Xiangfeng Zhu, Mosharaf Chowdhury", "title": "FedScale: Benchmarking Model and System Performance of Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FedScale, a diverse set of challenging and realistic benchmark\ndatasets to facilitate scalable, comprehensive, and reproducible federated\nlearning (FL) research. FedScale datasets are large-scale, encompassing a\ndiverse range of important FL tasks, such as image classification, object\ndetection, language modeling, speech recognition, and reinforcement learning.\nFor each dataset, we provide a unified evaluation protocol using realistic data\nsplits and evaluation metrics. To meet the pressing need for reproducing\nrealistic FL at scale, we have also built an efficient evaluation platform to\nsimplify and standardize the process of FL experimental setup and model\nevaluation. Our evaluation platform provides flexible APIs to implement new FL\nalgorithms and includes new execution backends with minimal developer efforts.\nFinally, we perform indepth benchmark experiments on these datasets. Our\nexperiments suggest fruitful opportunities in heterogeneity-aware\nco-optimizations of the system and statistical efficiency under realistic FL\ncharacteristics. FedScale is open-source with permissive licenses and actively\nmaintained,1 and we welcome feedback and contributions from the community.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:55:27 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 03:58:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lai", "Fan", ""], ["Dai", "Yinwei", ""], ["Zhu", "Xiangfeng", ""], ["Chowdhury", "Mosharaf", ""]]}, {"id": "2105.12301", "submitter": "Keichi Takahashi", "authors": "Keichi Takahashi (1), Wassapon Watanakeesuntorn (1), Kohei Ichikawa\n  (1), Joseph Park (2), Ryousei Takano (3), Jason Haga (3), George Sugihara\n  (4), Gerald M. Pao (5) ((1) Nara Institute of Science and Technology, (2)\n  U.S. Department of the Interior, (3) National Institute of Advanced\n  Industrial Science and Technology, (4) University of California San Diego,\n  (5) Salk Institute for Biological Studies)", "title": "kEDM: A Performance-portable Implementation of Empirical Dynamic\n  Modeling using Kokkos", "comments": "8 pages, 9 figures, accepted at Practice & Experience in Advanced\n  Research Computing (PEARC'21), corresponding authors: Keichi Takahashi,\n  Gerald M. Pao", "journal-ref": null, "doi": "10.1145/3437359.3465571", "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Dynamic Modeling (EDM) is a state-of-the-art non-linear time-series\nanalysis framework. Despite its wide applicability, EDM was not scalable to\nlarge datasets due to its expensive computational cost. To overcome this\nobstacle, researchers have attempted and succeeded in accelerating EDM from\nboth algorithmic and implementational aspects. In previous work, we developed a\nmassively parallel implementation of EDM targeting HPC systems (mpEDM).\nHowever, mpEDM maintains different backends for different architectures. This\ndesign becomes a burden in the increasingly diversifying HPC systems, when\nporting to new hardware. In this paper, we design and develop a\nperformance-portable implementation of EDM based on the Kokkos performance\nportability framework (kEDM), which runs on both CPUs and GPUs while based on a\nsingle codebase. Furthermore, we optimize individual kernels specifically for\nEDM computation, and use real-world datasets to demonstrate up to $5.5\\times$\nspeedup compared to mpEDM in convergent cross mapping computation.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 02:21:55 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Takahashi", "Keichi", ""], ["Watanakeesuntorn", "Wassapon", ""], ["Ichikawa", "Kohei", ""], ["Park", "Joseph", ""], ["Takano", "Ryousei", ""], ["Haga", "Jason", ""], ["Sugihara", "George", ""], ["Pao", "Gerald M.", ""]]}, {"id": "2105.12663", "submitter": "Maciej Besta", "authors": "Maciej Besta, Marcel Schneider, Salvatore Di Girolamo, Ankit Singla,\n  Torsten Hoefler", "title": "Towards Million-Server Network Simulations on Just a Laptop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing size of data center and HPC networks pose unprecedented\nrequirements on the scalability of simulation infrastructure. The ability to\nsimulate such large-scale interconnects on a simple PC would facilitate\nresearch efforts. Unfortunately, as we first show in this work, existing\nshared-memory packet-level simulators do not scale to the sizes of the largest\nnetworks considered today. We then illustrate a feasibility analysis and a set\nof enhancements that enable a simple packet-level htsim simulator to scale to\nthe unprecedented simulation sizes on a single PC. Our code is available online\nand can be used to design novel schemes in the coming era of omnipresent data\ncenters and HPC clusters.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:21:33 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Besta", "Maciej", ""], ["Schneider", "Marcel", ""], ["Di Girolamo", "Salvatore", ""], ["Singla", "Ankit", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2105.12676", "submitter": "Zhaoxia Deng", "authors": "Zhaoxia (Summer) Deng, Jongsoo Park, Ping Tak Peter Tang, Haixin Liu,\n  Jie (Amy) Yang, Hector Yuen, Jianyu Huang, Daya Khudia, Xiaohan Wei, Ellie\n  Wen, Dhruv Choudhary, Raghuraman Krishnamoorthi, Carole-Jean Wu, Satish\n  Nadathur, Changkyu Kim, Maxim Naumov, Sam Naghshineh, Mikhail Smelyanskiy", "title": "Low-Precision Hardware Architectures Meet Recommendation Model Inference\n  at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.IR cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous success of machine learning (ML) and the unabated growth in ML\nmodel complexity motivated many ML-specific designs in both CPU and accelerator\narchitectures to speed up the model inference. While these architectures are\ndiverse, highly optimized low-precision arithmetic is a component shared by\nmost. Impressive compute throughputs are indeed often exhibited by these\narchitectures on benchmark ML models. Nevertheless, production models such as\nrecommendation systems important to Facebook's personalization services are\ndemanding and complex: These systems must serve billions of users per month\nresponsively with low latency while maintaining high prediction accuracy,\nnotwithstanding computations with many tens of billions parameters per\ninference. Do these low-precision architectures work well with our production\nrecommendation systems? They do. But not without significant effort. We share\nin this paper our search strategies to adapt reference recommendation models to\nlow-precision hardware, our optimization of low-precision compute kernels, and\nthe design and development of tool chain so as to maintain our models' accuracy\nthroughout their lifespan during which topic trends and users' interests\ninevitably evolve. Practicing these low-precision technologies helped us save\ndatacenter capacities while deploying models with up to 5X complexity that\nwould otherwise not be deployed on traditional general-purpose CPUs. We believe\nthese lessons from the trenches promote better co-design between hardware\narchitecture and software engineering and advance the state of the art of ML in\nindustry.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:42:33 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhaoxia", "", "", "Summer"], ["Deng", "", "", "Amy"], ["Park", "Jongsoo", "", "Amy"], ["Tang", "Ping Tak Peter", "", "Amy"], ["Liu", "Haixin", "", "Amy"], ["Jie", "", "", "Amy"], ["Yang", "", ""], ["Yuen", "Hector", ""], ["Huang", "Jianyu", ""], ["Khudia", "Daya", ""], ["Wei", "Xiaohan", ""], ["Wen", "Ellie", ""], ["Choudhary", "Dhruv", ""], ["Krishnamoorthi", "Raghuraman", ""], ["Wu", "Carole-Jean", ""], ["Nadathur", "Satish", ""], ["Kim", "Changkyu", ""], ["Naumov", "Maxim", ""], ["Naghshineh", "Sam", ""], ["Smelyanskiy", "Mikhail", ""]]}, {"id": "2105.12842", "submitter": "Dan Zhang", "authors": "Dan Zhang, Safeen Huda, Ebrahim Songhori, Quoc Le, Anna Goldie, Azalia\n  Mirhoseini", "title": "A Full-stack Accelerator Search Technique for Vision Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly-changing ML model landscape presents a unique opportunity for\nbuilding hardware accelerators optimized for specific datacenter-scale\nworkloads. We propose Full-stack Accelerator Search Technique (FAST), a\nhardware accelerator search framework that defines a broad optimization\nenvironment covering key design decisions within the hardware-software stack,\nincluding hardware datapath, software scheduling, and compiler passes such as\noperation fusion and tensor padding. Although FAST can be used on any number\nand type of deep learning workload, in this paper we focus on optimizing for a\nsingle or small set of vision models, resulting in significantly faster and\nmore power-efficient designs relative to a general purpose ML accelerator. When\nevaluated on EfficientNet, ResNet50v2, and OCR inference performance relative\nto a TPU-v3, designs generated by FAST optimized for single workloads can\nimprove Perf/TDP (peak power) by over 6x in the best case and 4x on average. On\na limited workload subset, FAST improves Perf/TDP 2.85x on average, with a\nreduction to 2.35x for a single design optimized over the set of workloads. In\naddition, we demonstrate a potential 1.8x speedup opportunity for TPU-v3 with\nimproved scheduling.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:10:20 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhang", "Dan", ""], ["Huda", "Safeen", ""], ["Songhori", "Ebrahim", ""], ["Le", "Quoc", ""], ["Goldie", "Anna", ""], ["Mirhoseini", "Azalia", ""]]}, {"id": "2105.12880", "submitter": "Eli Dart", "authors": "Eli Dart, William Allcock, Wahid Bhimji, Tim Boerner, Ravinderjeet\n  Cheema, Andrew Cherry, Brent Draney, Salman Habib, Damian Hazen, Jason Hill,\n  Matt Kollross, Suzanne Parete-Koon, Daniel Pelfrey, Adrian Pope, Jeff Porter,\n  David Wheeler", "title": "The Petascale DTN Project: High Performance Data Transfer for HPC\n  Facilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The movement of large-scale (tens of Terabytes and larger) data sets between\nhigh performance computing (HPC) facilities is an important and increasingly\ncritical capability. A growing number of scientific collaborations rely on HPC\nfacilities for tasks which either require large-scale data sets as input or\nproduce large-scale data sets as output. In order to enable the transfer of\nthese data sets as needed by the scientific community, HPC facilities must\ndesign and deploy the appropriate data transfer capabilities to allow users to\ndo data placement at scale.\n  This paper describes the Petascale DTN Project, an effort undertaken by four\nHPC facilities, which succeeded in achieving routine data transfer rates of\nover 1PB/week between the facilities. We describe the design and configuration\nof the Data Transfer Node (DTN) clusters used for large-scale data transfers at\nthese facilities, the software tools used, and the performance tuning that\nenabled this capability.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 23:55:23 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dart", "Eli", ""], ["Allcock", "William", ""], ["Bhimji", "Wahid", ""], ["Boerner", "Tim", ""], ["Cheema", "Ravinderjeet", ""], ["Cherry", "Andrew", ""], ["Draney", "Brent", ""], ["Habib", "Salman", ""], ["Hazen", "Damian", ""], ["Hill", "Jason", ""], ["Kollross", "Matt", ""], ["Parete-Koon", "Suzanne", ""], ["Pelfrey", "Daniel", ""], ["Pope", "Adrian", ""], ["Porter", "Jeff", ""], ["Wheeler", "David", ""]]}, {"id": "2105.13738", "submitter": "Youri Raaijmakers", "authors": "Youri Raaijmakers and Sem Borst and Onno Boxma", "title": "Fork-join and redundancy systems with heavy-tailed job sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the tail asymptotics of the response time distribution for the\ncancel-on-start (c.o.s.) and cancel-on-completion (c.o.c.) variants of\nredundancy-$d$ scheduling and the fork-join model with heavy-tailed job sizes.\nWe present bounds, which only differ in the pre-factor, for the tail\nprobability of the response time in the case of the first-come first-served\n(FCFS) discipline. For the c.o.s. variant we restrict ourselves to\nredundancy-$d$ scheduling, which is a special case of the fork-join model. In\nparticular, for regularly varying job sizes with tail index $-\\nu$ the tail\nindex of the response time for the c.o.s. variant of redundancy-$d$ equals\n$-\\min\\{d_{\\mathrm{cap}}(\\nu-1),\\nu\\}$, where $d_{\\mathrm{cap}} =\n\\min\\{d,N-k\\}$, $N$ is the number of servers and $k$ is the integer part of the\nload. This result indicates that for $d_{\\mathrm{cap}} < \\frac{\\nu}{\\nu-1}$ the\nwaiting time component is dominant, whereas for $d_{\\mathrm{cap}} >\n\\frac{\\nu}{\\nu-1}$ the job size component is dominant. Thus, having $d = \\lceil\n\\min\\{\\frac{\\nu}{\\nu-1},N-k\\} \\rceil$ replicas is sufficient to achieve the\noptimal asymptotic tail behavior of the response time. For the c.o.c. variant\nof the fork-join($n_{\\mathrm{F}},n_{\\mathrm{J}}$) model the tail index of the\nresponse time, under some assumptions on the load, equals $1-\\nu$ and\n$1-(n_{\\mathrm{F}}+1-n_{\\mathrm{J}})\\nu$, for identical and i.i.d. replicas,\nrespectively; here the waiting time component is always dominant.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 11:04:04 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Raaijmakers", "Youri", ""], ["Borst", "Sem", ""], ["Boxma", "Onno", ""]]}, {"id": "2105.13894", "submitter": "Paulo Silva Feitosa", "authors": "Paulo Silva, Thiago Emmanuel Pereira", "title": "Performance Evaluation of Snapshot Methods to Warm the Serverless Cold\n  Start", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The serverless computing model strengthens the cloud computing tendency to\nabstract resource management. Serverless platforms are responsible for\ndeploying and scaling the developer's applications. Serverless also\nincorporated the pay-as-you-go billing model, which only considers the time\nspent processing client requests. Such a decision created a natural incentive\nfor improving the platform's efficient resource usage. This search for\nefficiency can lead to the cold start problem, which represents a delay to\nexecute serverless applications. Among the solutions proposed to deal with the\ncold start, those based on the snapshot method stand out. Despite the rich\nexploration of the technique, there is a lack of research that evaluates the\nsolution's trade-offs. In this direction, this work compares two solutions to\nmitigate the cold start: Prebaking and SEUSS. We analyzed the solution's\nperformance with functions of different levels of complexity: NoOp, a function\nthat renders Markdown to HTML, and a function that loads 41 MB of dependencies.\nPreliminary results indicated that Prebaking showed a 33% and 25% superior\nperformance to startup the NoOp and Markdown functions, respectively. Further\nanalysis also revealed that Prebaking's warmup mechanism reduced the Markdown\nfirst request processing time by 69%.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:57:49 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Silva", "Paulo", ""], ["Pereira", "Thiago Emmanuel", ""]]}, {"id": "2105.14069", "submitter": "Arman Dehpanah", "authors": "Arman Dehpanah, Muheeb Faizan Ghori, Jonathan Gemmell, Bamshad\n  Mobasher", "title": "The Evaluation of Rating Systems in Team-based Battle Royale Games", "comments": "Updated references -- 10 pages, 1 figure, Accepted in the 23rd\n  International Conference on Artificial Intelligence (ICAI'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online competitive games have become a mainstream entertainment platform. To\ncreate a fair and exciting experience, these games use rating systems to match\nplayers with similar skills. While there has been an increasing amount of\nresearch on improving the performance of these systems, less attention has been\npaid to how their performance is evaluated. In this paper, we explore the\nutility of several metrics for evaluating three popular rating systems on a\nreal-world dataset of over 25,000 team battle royale matches. Our results\nsuggest considerable differences in their evaluation patterns. Some metrics\nwere highly impacted by the inclusion of new players. Many could not capture\nthe real differences between certain groups of players. Among all metrics\nstudied, normalized discounted cumulative gain (NDCG) demonstrated more\nreliable performance and more flexibility. It alleviated most of the challenges\nfaced by the other metrics while adding the freedom to adjust the focus of the\nevaluations on different groups of players.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:22:07 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 20:40:26 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Dehpanah", "Arman", ""], ["Ghori", "Muheeb Faizan", ""], ["Gemmell", "Jonathan", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "2105.14450", "submitter": "Yang You", "authors": "Zhengda Bian and Qifan Xu and Boxiang Wang and Yang You", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks", "comments": "Technical Report of NUS HPC-AI Lab (https://ai.comp.nus.edu.sg). The\n  leading two authors have equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent Natural Language Processing techniques have been refreshing the\nstate-of-the-art performance at an incredible speed. Training huge language\nmodels is therefore an imperative demand in both industry and academy. However,\nhuge language models impose challenges to both hardware and software. Graphical\nprocessing units (GPUs) are iterated frequently to meet the exploding demand,\nand a variety of ASICs like TPUs are spawned. However, there is still a tension\nbetween the fast growth of the extremely huge models and the fact that Moore's\nlaw is approaching the end. To this end, many model parallelism techniques are\nproposed to distribute the model parameters to multiple devices, so as to\nalleviate the tension on both memory and computation. Our work is the first to\nintroduce a 3-dimensional model parallelism for expediting huge language\nmodels. By reaching a perfect load balance, our approach presents smaller\nmemory and communication cost than existing state-of-the-art 1-D and 2-D model\nparallelism. Our experiments on 64 TACC's V100 GPUs show that our 3-D\nparallelism outperforms the 1-D and 2-D parallelism with 2.32x and 1.57x\nspeedup, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 07:41:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bian", "Zhengda", ""], ["Xu", "Qifan", ""], ["Wang", "Boxiang", ""], ["You", "Yang", ""]]}, {"id": "2105.14845", "submitter": "Muhammad Bilal", "authors": "Muhammad Bilal and Marco Canini and Rodrigo Fonseca and Rodrigo\n  Rodrigues", "title": "With Great Freedom Comes Great Opportunity: Rethinking Resource\n  Allocation for Serverless Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current serverless offerings give users a limited degree of flexibility for\nconfiguring the resources allocated to their function invocations by either\ncoupling memory and CPU resources together or providing no knobs at all. These\nconfiguration choices simplify resource allocation decisions on behalf of\nusers, but at the same time, create deployments that are resource inefficient.\n  In this paper, we take a principled approach to the problem of resource\nallocation for serverless functions, allowing this choice to be made in an\nautomatic way that leads to the best combination of performance and cost. In\nparticular, we systematically explore the opportunities that come with\ndecoupling memory and CPU resource allocations and also enabling the use of\ndifferent VM types. We find a rich trade-off space between performance and\ncost. The provider can use this in a number of ways: from exposing all these\nparameters to the user, to eliciting preferences for performance and cost from\nusers, or by simply offering the same performance with lower cost. This\nflexibility can also enable the provider to optimize its resource utilization\nand enable a cost-effective service with predictable performance.\n  Our results show that, by decoupling memory and CPU allocation, there is\npotential to have up to 40% lower execution cost than the preset coupled\nconfigurations that are the norm in current serverless offerings. Similarly,\nmaking the correct choice of VM instance type can provide up to 50% better\nexecution time. Furthermore, we demonstrate that providers can utilize\ndifferent instance types for the same functions to maximize resource\nutilization while providing performance within 10-20% of the best resource\nconfiguration for each respective function.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:59:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bilal", "Muhammad", ""], ["Canini", "Marco", ""], ["Fonseca", "Rodrigo", ""], ["Rodrigues", "Rodrigo", ""]]}, {"id": "2105.15098", "submitter": "Yongxin Liu", "authors": "Yongxin Liu, Jian Wang, Jianqiang Li, Shuteng Niu, Houbing Song", "title": "Zero-bias Deep Learning Enabled Quick and Reliable Abnormality Detection\n  in IoT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormality detection is essential to the performance of safety-critical and\nlatency-constrained systems. However, as systems are becoming increasingly\ncomplicated with a large quantity of heterogeneous data, conventional\nstatistical change point detection methods are becoming less effective and\nefficient. Although Deep Learning (DL) and Deep Neural Networks (DNNs) are\nincreasingly employed to handle heterogeneous data, they still lack theoretic\nassurable performance and explainability. This paper integrates zero-bias DNN\nand Quickest Event Detection algorithms to provide a holistic framework for\nquick and reliable detection of both abnormalities and time-dependent abnormal\nevents in the Internet of Things (IoT). We first use the zero-bias dense layer\nto increase the explainability of DNN. We provide a solution to convert\nzero-bias DNN classifiers into performance assured binary abnormality\ndetectors. Using the converted abnormality detector, we then present a\nsequential quickest detection scheme that provides the theoretically assured\nlowest abnormal event detection delay under false alarm constraints. Finally,\nwe demonstrate the effectiveness of the framework using both massive signal\nrecords from real-world aviation communication systems and simulated data. Code\nand data of our work is available at\n\\url{https://github.com/pcwhy/AbnormalityDetectionInZbDNN}\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 03:31:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Yongxin", ""], ["Wang", "Jian", ""], ["Li", "Jianqiang", ""], ["Niu", "Shuteng", ""], ["Song", "Houbing", ""]]}]