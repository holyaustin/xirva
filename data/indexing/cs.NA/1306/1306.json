[{"id": "1306.0103", "submitter": "Rastislav Telgarsky", "authors": "Rastislav Telgarsky", "title": "Dominant Frequency Extraction", "comments": "12 pages, 5 figures, Matlab code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series are collected and studied extensively for the knowledge about the\ndata source characteristics such as the trend or the spectral landscape. Some\npeaks in the spectral landscape correspond to dominant frequencies. The\napproach here is empirical: all time series are discrete and finite. Contents:\nIntroduction. 1 Examples of periodic phenomena. 2 Algorithms and libraries. 3\nTime series analysis. 4 Dominant frequency in ladar data. Conclusion.\nReferences.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 12:55:26 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Telgarsky", "Rastislav", ""]]}, {"id": "1306.0627", "submitter": "Joshua Bodyfelt", "authors": "Enrico Gerlach, Siegfried Eggl, Charalampos Skokos, Joshua D.\n  Bodyfelt, and Georgios Papamikos", "title": "High Order Three Part Split Symplectic Integration Schemes", "comments": "Submitted for conference proceedings to the 10th HSTAM International\n  Congress on Mechanics (http://www.10hstam.tuc.gr/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symplectic integration methods based on operator splitting are well\nestablished in many branches of science. For Hamiltonian systems which split in\nmore than two parts, symplectic methods of higher order have been studied in\ndetail only for a few special cases. In this work, we present and compare\ndifferent ways to construct high order symplectic schemes for general\nHamiltonian systems that can be split in three integrable parts. We use these\ntechniques to numerically solve the equations of motion for a simple toy model,\nas well as the disordered discrete nonlinear Schr\\\"odinger equation. We thereby\ncompare the efficiency of symplectic and non-symplectic integration methods.\nOur results show that the new symplectic schemes are superior to the other\ntested methods, with respect to both long term energy conservation and\ncomputational time requirements.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 01:03:04 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Gerlach", "Enrico", ""], ["Eggl", "Siegfried", ""], ["Skokos", "Charalampos", ""], ["Bodyfelt", "Joshua D.", ""], ["Papamikos", "Georgios", ""]]}, {"id": "1306.0769", "submitter": "Denys Dutykh", "authors": "Ashkan Rafiee and Denys Dutykh (CNRS/LAMA) and Fr\\'ed\\'eric Dias\n  (CMLA)", "title": "Numerical simulation of wave impact on a rigid wall using a two-phase\n  compressible SPH method", "comments": "18 pages, 11 figures, 33 references. Other author's papers can be\n  downloaded at http://www.denys-dutykh.com/", "journal-ref": "Procedia IUTAM (2015), Vol. 18, pp. 123-137", "doi": "10.1016/j.piutam.2015.11.013", "report-no": null, "categories": "physics.flu-dyn cs.NA math.NA physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, an SPH method based on the SPH-ALE formulation is used for\nmodelling two-phase flows with large density ratios and realistic sound speeds.\nThe SPH scheme is further improved to circumvent the tensile instability that\nmay occur in the SPH simulations. The two-phase SPH solver is then used to\nmodel a benchmark problem of liquid impact on a rigid wall. The results are\ncompared with an incompressible Level Set solver. Furthermore, a wave impact on\na rigid wall with a large entrained air pocket is modelled. The SPH simulation\nis initialised by the output of a fully non-linear potential flow solver. The\npressure distribution, velocity field and impact pressure are then analysed.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 13:02:43 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 19:32:20 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Rafiee", "Ashkan", "", "CNRS/LAMA"], ["Dutykh", "Denys", "", "CNRS/LAMA"], ["Dias", "Fr\u00e9d\u00e9ric", "", "CMLA"]]}, {"id": "1306.2305", "submitter": "Alexandre Chapoutot", "authors": "Olivier Bouissou (LMeASI), Alexandre Chapoutot (U2IS), Samuel Mimram\n  (LMeASI)", "title": "Computing Flowpipe of Nonlinear Hybrid Systems with Numerical Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA cs.SY math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern control-command systems often include controllers that perform\nnonlinear computations to control a physical system, which can typically be\ndescribed by an hybrid automaton containing high-dimensional systems of\nnonlinear differential equations. To prove safety of such systems, one must\ncompute all the reachable sets from a given initial position, which might be\nuncertain (its value is not precisely known). On linear hybrid systems,\nefficient and precise techniques exist, but they fail to handle nonlinear flows\nor jump conditions. In this article, we present a new tool name HySon which\ncomputes the flowpipes of both linear and nonlinear hybrid systems using\nguaranteed generalization of classical efficient numerical simulation methods,\nincluding with variable integration step-size. In particular, we present an\nalgorithm for detecting discrete events based on guaranteed interpolation\npolynomials that turns out to be both precise and efficient. Illustrations of\nthe techniques developed in this article are given on representative examples.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 08:25:47 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Bouissou", "Olivier", "", "LMeASI"], ["Chapoutot", "Alexandre", "", "U2IS"], ["Mimram", "Samuel", "", "LMeASI"]]}, {"id": "1306.2663", "submitter": "Guoqiang Zhong", "authors": "Guoqiang Zhong and Mohamed Cheriet", "title": "Large Margin Low Rank Tensor Analysis", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Other than vector representations, the direct objects of human cognition are\ngenerally high-order tensors, such as 2D images and 3D textures. From this\nfact, two interesting questions naturally arise: How does the human brain\nrepresent these tensor perceptions in a \"manifold\" way, and how can they be\nrecognized on the \"manifold\"? In this paper, we present a supervised model to\nlearn the intrinsic structure of the tensors embedded in a high dimensional\nEuclidean space. With the fixed point continuation procedures, our model\nautomatically and jointly discovers the optimal dimensionality and the\nrepresentations of the low dimensional embeddings. This makes it an effective\nsimulation of the cognitive process of human brain. Furthermore, the\ngeneralization of our model based on similarity between the learned low\ndimensional embeddings can be viewed as counterpart of recognition of human\nbrain. Experiments on applications for object recognition and face recognition\ndemonstrate the superiority of our proposed model over state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 21:39:56 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Zhong", "Guoqiang", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1306.3343", "submitter": "Zheng Pan", "authors": "Zheng Pan, Changshui Zhang", "title": "Relaxed Sparse Eigenvalue Conditions for Sparse Estimation via\n  Non-convex Regularized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex regularizers usually improve the performance of sparse estimation\nin practice. To prove this fact, we study the conditions of sparse estimations\nfor the sharp concave regularizers which are a general family of non-convex\nregularizers including many existing regularizers. For the global solutions of\nthe regularized regression, our sparse eigenvalue based conditions are weaker\nthan that of L1-regularization for parameter estimation and sparseness\nestimation. For the approximate global and approximate stationary (AGAS)\nsolutions, almost the same conditions are also enough. We show that the desired\nAGAS solutions can be obtained by coordinate descent (CD) based methods.\nFinally, we perform some experiments to show the performance of CD methods on\ngiving AGAS solutions and the degree of weakness of the estimation conditions\nrequired by the sharp concave regularizers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 09:10:00 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 06:25:21 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 09:27:57 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Pan", "Zheng", ""], ["Zhang", "Changshui", ""]]}, {"id": "1306.3391", "submitter": "Laura Balzano", "authors": "Laura Balzano and Stephen J. Wright", "title": "Local Convergence of an Algorithm for Subspace Identification from\n  Partial Data", "comments": "29 pages. 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an iterative\nalgorithm for identifying a linear subspace of R^n from data consisting of\npartial observations of random vectors from that subspace. This paper examines\nlocal convergence properties of GROUSE, under assumptions on the randomness of\nthe observed vectors, the randomness of the subset of elements observed at each\niteration, and incoherence of the subspace with the coordinate directions.\nConvergence at an expected linear rate is demonstrated under certain\nassumptions. The case in which the full random vector is revealed at each\niteration allows for much simpler analysis, and is also described. GROUSE is\nrelated to incremental SVD methods and to gradient projection algorithms in\noptimization.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 13:28:49 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 15:46:26 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Balzano", "Laura", ""], ["Wright", "Stephen J.", ""]]}, {"id": "1306.4080", "submitter": "An Bian", "authors": "An Bian, Xiong Li, Yuncai Liu, Ming-Hsuan Yang", "title": "Parallel Coordinate Descent Newton Method for Efficient\n  $\\ell_1$-Regularized Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent years have witnessed advances in parallel algorithms for large\nscale optimization problems. Notwithstanding demonstrated success, existing\nalgorithms that parallelize over features are usually limited by divergence\nissues under high parallelism or require data preprocessing to alleviate these\nproblems. In this work, we propose a Parallel Coordinate Descent Newton\nalgorithm using multidimensional approximate Newton steps (PCDN), where the\noff-diagonal elements of the Hessian are set to zero to enable parallelization.\nIt randomly partitions the feature set into $b$ bundles/subsets with size of\n$P$, and sequentially processes each bundle by first computing the descent\ndirections for each feature in parallel and then conducting $P$-dimensional\nline search to obtain the step size. We show that: (1) PCDN is guaranteed to\nconverge globally despite increasing parallelism; (2) PCDN converges to the\nspecified accuracy $\\epsilon$ within the limited iteration number of\n$T_\\epsilon$, and $T_\\epsilon$ decreases with increasing parallelism (bundle\nsize $P$). Using the implementation technique of maintaining intermediate\nquantities, we minimize the data transfer and synchronization cost of the\n$P$-dimensional line search. For concreteness, the proposed PCDN algorithm is\napplied to $\\ell_1$-regularized logistic regression and $\\ell_2$-loss SVM.\nExperimental evaluations on six benchmark datasets show that the proposed PCDN\nalgorithm exploits parallelism well and outperforms the state-of-the-art\nmethods in speed without losing accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 07:03:16 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 08:41:37 GMT"}, {"version": "v3", "created": "Tue, 18 Mar 2014 14:55:49 GMT"}, {"version": "v4", "created": "Thu, 7 Dec 2017 09:16:27 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Bian", "An", ""], ["Li", "Xiong", ""], ["Liu", "Yuncai", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1306.4905", "submitter": "Martin Trnecka", "authors": "Radim Belohlavek, Martin Trnecka", "title": "From-Below Approximations in Boolean Matrix Factorization: Geometry and\n  New Algorithm", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcss.2015.06.002", "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new results on Boolean matrix factorization and a new algorithm\nbased on these results. The results emphasize the significance of\nfactorizations that provide from-below approximations of the input matrix.\nWhile the previously proposed algorithms do not consider the possibly different\nsignificance of different matrix entries, our results help measure such\nsignificance and suggest where to focus when computing factors. An experimental\nevaluation of the new algorithm on both synthetic and real data demonstrates\nits good performance in terms of good coverage by the first k factors as well\nas a small number of factors needed for exact decomposition and indicates that\nthe algorithm outperforms the available ones in these terms. We also propose\nfuture research topics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 15:19:22 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Belohlavek", "Radim", ""], ["Trnecka", "Martin", ""]]}, {"id": "1306.5013", "submitter": "David  Biagioni", "authors": "David J. Biagioni, Daniel Beylkin and Gregory Beylkin", "title": "Randomized Interpolative Decomposition of Separated Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce tensor Interpolative Decomposition (tensor ID) for the reduction\nof the separation rank of Canonical Tensor Decompositions (CTDs). Tensor ID\nselects, for a user-defined accuracy \\epsilon, a near optimal subset of terms\nof a CTD to represent the remaining terms via a linear combination of the\nselected terms. Tensor ID can be used as an alternative to or a step of the\nAlternating Least Squares (ALS) algorithm. In addition, we briefly discuss\nQ-factorization to reduce the size of components within an ALS iteration.\nCombined, tensor ID and Q-factorization lead to a new paradigm for the\nreduction of the separation rank of CTDs. In this context, we also discuss the\nspectral norm as a computational alternative to the Frobenius norm.\n  We reduce the problem of finding tensor IDs to that of constructing\nInterpolative Decompositions of certain matrices. These matrices are generated\nvia either randomized projection or randomized sampling of the given tensor. We\nprovide cost estimates and several examples of the new approach to the\nreduction of separation rank.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 22:30:09 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2013 00:31:35 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Biagioni", "David J.", ""], ["Beylkin", "Daniel", ""], ["Beylkin", "Gregory", ""]]}, {"id": "1306.5216", "submitter": "Kalyana Babu Nakshatrala", "authors": "J. Chang and K. B. Nakshatrala", "title": "Modification to Darcy model for high pressure and high velocity\n  applications and associated mixed finite element formulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Darcy model is based on a plethora of assumptions. One of the most\nimportant assumptions is that the Darcy model assumes the drag coefficient to\nbe constant. However, there is irrefutable experimental evidence that\nviscosities of organic liquids and carbon-dioxide depend on the pressure.\nExperiments have also shown that the drag varies nonlinearly with respect to\nthe velocity at high flow rates. In important technological applications like\nenhanced oil recovery and geological carbon-dioxide sequestration, one\nencounters both high pressures and high flow rates. It should be emphasized\nthat flow characteristics and pressure variation under varying drag are both\nquantitatively and qualitatively different from that of constant drag.\nMotivated by experimental evidence, we consider the drag coefficient to depend\non both the pressure and velocity. We consider two major modifications to the\nDarcy model based on the Barus formula and Forchheimer approximation. The\nproposed modifications to the Darcy model result in nonlinear partial\ndifferential equations, which are not amenable to analytical solutions. To this\nend, we present mixed finite element formulations based on least-squares\nformalism and variational multiscale formalism for the resulting governing\nequations. The proposed modifications to the Darcy model and its associated\nfinite element formulations are used to solve realistic problems with relevance\nto enhanced oil recovery. We also study the competition between the nonlinear\ndependence of drag on the velocity and the dependence of viscosity on the\npressure. To the best of the authors' knowledge such a systematic study has not\nbeen performed.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 19:15:57 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Chang", "J.", ""], ["Nakshatrala", "K. B.", ""]]}, {"id": "1306.5226", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal N. Chaudhury, Yuehaw Khoo, Amit Singer", "title": "Global registration of multiple point clouds using semidefinite\n  programming", "comments": "33 pages, 12 figures. To appear in SIAM Journal on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider $N$ points in $\\mathbb{R}^d$ and $M$ local coordinate systems that\nare related through unknown rigid transforms. For each point we are given\n(possibly noisy) measurements of its local coordinates in some of the\ncoordinate systems. Alternatively, for each coordinate system, we observe the\ncoordinates of a subset of the points. The problem of estimating the global\ncoordinates of the $N$ points (up to a rigid transform) from such measurements\ncomes up in distributed approaches to molecular conformation and sensor network\nlocalization, and also in computer vision and graphics.\n  The least-squares formulation of this problem, though non-convex, has a well\nknown closed-form solution when $M=2$ (based on the singular value\ndecomposition). However, no closed form solution is known for $M\\geq 3$.\n  In this paper, we demonstrate how the least-squares formulation can be\nrelaxed into a convex program, namely a semidefinite program (SDP). By setting\nup connections between the uniqueness of this SDP and results from rigidity\ntheory, we prove conditions for exact and stable recovery for the SDP\nrelaxation. In particular, we prove that the SDP relaxation can guarantee\nrecovery under more adversarial conditions compared to earlier proposed\nspectral relaxations, and derive error bounds for the registration error\nincurred by the SDP relaxation.\n  We also present results of numerical experiments on simulated data to confirm\nthe theoretical findings. We empirically demonstrate that (a) unlike the\nspectral relaxation, the relaxation gap is mostly zero for the semidefinite\nprogram (i.e., we are able to solve the original non-convex least-squares\nproblem) up to a certain noise threshold, and (b) the semidefinite program\nperforms significantly better than spectral and manifold-optimization methods,\nparticularly at large noise levels.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 19:55:40 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2013 15:36:47 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2013 01:51:11 GMT"}, {"version": "v4", "created": "Fri, 4 Jul 2014 18:41:18 GMT"}, {"version": "v5", "created": "Tue, 23 Dec 2014 08:01:28 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Chaudhury", "Kunal N.", ""], ["Khoo", "Yuehaw", ""], ["Singer", "Amit", ""]]}, {"id": "1306.5918", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Lin Xiao", "title": "A Randomized Nonmonotone Block Proximal Gradient Method for a Class of\n  Structured Nonlinear Programming", "comments": "The previous title was \"Randomized Block Coordinate Non-Monotone\n  Gradient Method for a Class of Nonlinear Programming\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized nonmonotone block proximal gradient (RNBPG) method\nfor minimizing the sum of a smooth (possibly nonconvex) function and a\nblock-separable (possibly nonconvex nonsmooth) function. At each iteration,\nthis method randomly picks a block according to any prescribed probability\ndistribution and solves typically several associated proximal subproblems that\nusually have a closed-form solution, until a certain progress on objective\nvalue is achieved. In contrast to the usual randomized block coordinate descent\nmethod [23,20], our method has a nonmonotone flavor and uses variable stepsizes\nthat can partially utilize the local curvature information of the smooth\ncomponent of objective function. We show that any accumulation point of the\nsolution sequence of the method is a stationary point of the problem {\\it\nalmost surely} and the method is capable of finding an approximate stationary\npoint with high probability. We also establish a sublinear rate of convergence\nfor the method in terms of the minimal expected squared norm of certain\nproximal gradients over the iterations. When the problem under consideration is\nconvex, we show that the expected objective values generated by RNBPG converge\nto the optimal value of the problem. Under some assumptions, we further\nestablish a sublinear and linear rate of convergence on the expected objective\nvalues generated by a monotone version of RNBPG. Finally, we conduct some\npreliminary experiments to test the performance of RNBPG on the\n$\\ell_1$-regularized least-squares problem and a dual SVM problem in machine\nlearning. The computational results demonstrate that our method substantially\noutperforms the randomized block coordinate {\\it descent} method with fixed or\nvariable stepsizes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 11:11:42 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2015 01:11:56 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Lu", "Zhaosong", ""], ["Xiao", "Lin", ""]]}, {"id": "1306.6615", "submitter": "Tomoaki Okayama", "authors": "Tomoaki Okayama", "title": "Explicit error bound for modified numerical iterated integration by\n  means of Sinc methods", "comments": "Keyword: Sinc quadrature, Sinc indefinite integration, repeated\n  integral, verified numerical integration, double-exponential transformation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reinforces numerical iterated integration developed by\nMuhammad--Mori in the following two points: 1) the approximation formula is\nmodified so that it can achieve a better convergence rate in more general\ncases, and 2) explicit error bound is given in a computable form for the\nmodified formula. The formula works quite efficiently, especially if the\nintegrand is of a product type. Numerical examples that confirm it are also\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 19:27:30 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2013 16:06:38 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2015 13:28:42 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Okayama", "Tomoaki", ""]]}]