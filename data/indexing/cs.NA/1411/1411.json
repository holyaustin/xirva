[{"id": "1411.0583", "submitter": "Philipp Hoffmann", "authors": "Philipp H. W. Hoffmann", "title": "A Hitchhiker's Guide to Automatic Differentiation", "comments": "39 pages, 10 figures, Numerical Algorithms (2015)", "journal-ref": "Numerical Algorithms, Vol. 72 No. 3 (2016), 775-811", "doi": "10.1007/s11075-015-0067-6", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides an overview of some of the mathematical principles of\nAutomatic Differentiation (AD). In particular, we summarise different\ndescriptions of the Forward Mode of AD, like the matrix-vector product based\napproach, the idea of lifting functions to the algebra of dual numbers, the\nmethod of Taylor series expansion on dual numbers and the application of the\npush-forward operator, and explain why they all reduce to the same actual chain\nof computations. We further give a short mathematical description of some\nmethods of higher-order Forward AD and, at the end of this paper, briefly\ndescribe the Reverse Mode of Automatic Differentiation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 17:52:50 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 18:02:24 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2015 22:48:11 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2015 16:09:11 GMT"}, {"version": "v5", "created": "Fri, 1 Jul 2016 21:30:10 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Hoffmann", "Philipp H. W.", ""]]}, {"id": "1411.0622", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani and George Atia", "title": "A Subspace Method for Array Covariance Matrix Estimation", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a subspace method for the estimation of an array\ncovariance matrix. It is shown that when the received signals are uncorrelated,\nthe true array covariance matrices lie in a specific subspace whose dimension\nis typically much smaller than the dimension of the full space. Based on this\nidea, a subspace based covariance matrix estimator is proposed. The estimator\nis obtained as a solution to a semi-definite convex optimization problem. While\nthe optimization problem has no closed-form solution, a nearly optimal\nclosed-form solution is proposed making it easy to implement. In comparison to\nthe conventional approaches, the proposed method yields higher estimation\naccuracy because it eliminates the estimation error which does not lie in the\nsubspace of the true covariance matrices. The numerical examples indicate that\nthe proposed covariance matrix estimator can significantly improve the\nestimation quality of the covariance matrix.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 02:00:58 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1411.0814", "submitter": "Yiguang Liu", "authors": "Yiguang Liu", "title": "A random algorithm for low-rank decomposition of large-scale matrices\n  with missing entries", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2458176", "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Random SubMatrix method (RSM) is proposed to calculate the low-rank\ndecomposition of large-scale matrices with known entry percentage \\rho. RSM is\nvery fast as the floating-point operations (flops) required are compared\nfavorably with the state-of-the-art algorithms. Meanwhile RSM is very\nmemory-saving. With known entries homogeneously distributed in the given\nmatrix, sub-matrices formed by known entries are randomly selected. According\nto the just proved theorem that subspace related to smaller singular values is\nless perturbed by noise, the null vectors or the right singular vectors\nassociated with the minor singular values are calculated for each submatrix.\nThe vectors are the null vectors of the corresponding submatrix in the ground\ntruth of the given large-scale matrix. If enough sub-matrices are randomly\nchosen, the low-rank decomposition is estimated. The experimental results on\nrandom synthetical matrices with sizes such as 131072X1024 and on real data\nsets indicate that RSM is much faster and memory-saving, and, meanwhile, has\nconsiderable high precision achieving or approximating to the best.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 07:43:15 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Liu", "Yiguang", ""]]}, {"id": "1411.1087", "submitter": "Praneeth Netrapalli", "authors": "Prateek Jain and Praneeth Netrapalli", "title": "Fast Exact Matrix Completion with Finite Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is the problem of recovering a low rank matrix by observing\na small fraction of its entries. A series of recent works [KOM12,JNS13,HW14]\nhave proposed fast non-convex optimization based iterative algorithms to solve\nthis problem. However, the sample complexity in all these results is\nsub-optimal in its dependence on the rank, condition number and the desired\naccuracy.\n  In this paper, we present a fast iterative algorithm that solves the matrix\ncompletion problem by observing $O(nr^5 \\log^3 n)$ entries, which is\nindependent of the condition number and the desired accuracy. The run time of\nour algorithm is $O(nr^7\\log^3 n\\log 1/\\epsilon)$ which is near linear in the\ndimension of the matrix. To the best of our knowledge, this is the first near\nlinear time algorithm for exact matrix completion with finite sample complexity\n(i.e. independent of $\\epsilon$).\n  Our algorithm is based on a well known projected gradient descent method,\nwhere the projection is onto the (non-convex) set of low rank matrices. There\nare two key ideas in our result: 1) our argument is based on a $\\ell_{\\infty}$\nnorm potential function (as opposed to the spectral norm) and provides a novel\nway to obtain perturbation bounds for it. 2) we prove and use a natural\nextension of the Davis-Kahan theorem to obtain perturbation bounds on the best\nlow rank approximation of matrices with good eigen-gap. Both of these ideas may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:16:23 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Jain", "Prateek", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1411.1124", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Lorenzo Orecchia", "title": "Nearly Linear-Time Packing and Covering LP Solvers", "comments": "journal version (to appear in Mathematical Programming)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing and covering linear programs (PC-LPs) form an important class of\nlinear programs (LPs) across computer science, operations research, and\noptimization. In 1993, Luby and Nisan constructed an iterative algorithm for\napproximately solving PC-LPs in nearly linear time, where the time complexity\nscales nearly linearly in $N$, the number of nonzero entries of the matrix, and\npolynomially in $\\varepsilon$, the (multiplicative) approximation error.\nUnfortunately, all existing nearly linear-time algorithms for solving PC-LPs\nrequire time at least proportional to $\\varepsilon^{-2}$.\n  In this paper, we break this longstanding barrier by designing a packing\nsolver that runs in time $\\tilde{O}(N \\varepsilon^{-1})$ and covering LP solver\nthat runs in time $\\tilde{O}(N \\varepsilon^{-1.5})$. Our packing solver can be\nextended to run in time $\\tilde{O}(N \\varepsilon^{-1})$ for a class of\nwell-behaved covering programs. In a follow-up work, Wang et al. showed that\nall covering LPs can be converted into well-behaved ones by a reduction that\nblows up the problem size only logarithmically.\n  At high level, these two algorithms can be described as linear couplings of\nseveral first-order descent steps. This is an application of our linear\ncoupling technique to problems that are not amenable to blackbox applications\nknown iterative algorithms in convex optimization.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 01:09:04 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2016 20:08:57 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 23:29:06 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1411.1503", "submitter": "Ran Pan", "authors": "Ran Pan", "title": "Tensor Transpose and Its Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor transpose is a higher order generalization of matrix transpose. In\nthis paper, we use permutations and symmetry group to define? the tensor\ntranspose. Then we discuss the classification and composition of tensor\ntransposes. Properties of tensor transpose are studied in relation to tensor\nmultiplication, tensor eigenvalues, tensor decompositions and tensor rank.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 05:38:02 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Pan", "Ran", ""]]}, {"id": "1411.1972", "submitter": "Victor Pan", "authors": "Victor Y. Pan", "title": "Better Late Than Never: Filling a Void in the History of Fast Matrix\n  Multiplication and Tensor Decompositions", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilinear and tensor decompositions are a popular tool in linear and\nmultilinear algebra and have a wide range of important applications to modern\ncomputing. Our paper of 1972 presented the first nontrivial application of such\ndecompositions to fundamental matrix computations and was also a landmark in\nthe history of the acceleration of matrix multiplication. Published in 1972 in\nRussian, it has never been translated into English. It has been very rarely\ncited in the Western literature on matrix multiplication and never in the works\non multilinear and tensor decompositions. This motivates us to present its\ntranslation into English, together with our brief comments on its impact on the\ntwo fields.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 17:09:03 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Pan", "Victor Y.", ""]]}, {"id": "1411.2253", "submitter": "Buyang Li", "authors": "Buyang Li", "title": "A bounded numerical solution with a small mesh size implies existence of\n  a smooth solution to the Navier-Stokes equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that for a given smooth initial value, if the finite element\nsolution of the three-dimensional Navier-Stokes equations is bounded in a\ncertain norm with a relatively small mesh size, then the solution of the\nNavier-Stokes equations with this given initial value must be smooth and\nunique, and is successfully approximated by the numerical solution.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 16:21:11 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 15:08:56 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 07:08:07 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Li", "Buyang", ""]]}, {"id": "1411.2377", "submitter": "Tomonori Kouya", "authors": "Tomonori Kouya", "title": "A Highly Efficient Implementation of Multiple Precision Sparse\n  Matrix-Vector Multiplication and Its Application to Product-type Krylov\n  Subspace Methods", "comments": null, "journal-ref": "International Journal of Numerical Methods and Applications,\n  Vol.7, Issue 2, 2012, Pages 107 - 119", "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the performance of the Krylov subspace method by using highly\nefficient multiple precision sparse matrix-vector multiplication (SpMV).\nBNCpack is our multiple precision numerical computation library based on\nMPFR/GMP, which is one of the most efficient arbitrary precision floating-point\narithmetic libraries. However, it does not include functions that can\nmanipulate multiple precision sparse matrices. Therefore, by using benchmark\ntests, we show that SpMV implemented in these functions can be more efficient.\nFinally, we also show that product-type Krylov subspace methods such as BiCG\nand GPBiCG in which we have embedded SpMV, can efficiently solve large-scale\nlinear systems of equations provided in the UF sparse matrix collections in a\nmemory-restricted computing environment.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 10:58:18 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Kouya", "Tomonori", ""]]}, {"id": "1411.2940", "submitter": "Andrew McRae", "authors": "Andrew T. T. McRae, Gheorghe-Teodor Bercea, Lawrence Mitchell, David\n  A. Ham, Colin J. Cotter", "title": "Automated generation and symbolic manipulation of tensor product finite\n  elements", "comments": "Submitted to SISC special issue on CSE Software. Updated version,\n  following reviewer comments", "journal-ref": "SIAM Journal on Scientific Computing 38(5):S25-S47 (2016)", "doi": "10.1137/15M1021167", "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and implement a symbolic algebra for scalar and vector-valued\nfinite elements, enabling the computer generation of elements with tensor\nproduct structure on quadrilateral, hexahedral and triangular prismatic cells.\nThe algebra is implemented as an extension to the domain-specific language UFL,\nthe Unified Form Language. This allows users to construct many finite element\nspaces beyond those supported by existing software packages. We have made\ncorresponding extensions to FIAT, the FInite element Automatic Tabulator, to\nenable numerical tabulation of such spaces. This tabulation is consequently\nused during the automatic generation of low-level code that carries out local\nassembly operations, within the wider context of solving finite element\nproblems posed over such function spaces. We have done this work within the\ncode-generation pipeline of the software package Firedrake; we make use of the\nfull Firedrake package to present numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 19:47:45 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 17:31:00 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2016 11:12:45 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["McRae", "Andrew T. T.", ""], ["Bercea", "Gheorghe-Teodor", ""], ["Mitchell", "Lawrence", ""], ["Ham", "David A.", ""], ["Cotter", "Colin J.", ""]]}, {"id": "1411.3406", "submitter": "Thomas Goldstein", "authors": "Tom Goldstein, Christoph Studer, Richard Baraniuk", "title": "A Field Guide to Forward-Backward Splitting with a FASTA Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-differentiable and constrained optimization play a key role in machine\nlearning, signal and image processing, communications, and beyond. For\nhigh-dimensional minimization problems involving large datasets or many\nunknowns, the forward-backward splitting method provides a simple, practical\nsolver. Despite its apparently simplicity, the performance of the\nforward-backward splitting is highly sensitive to implementation details.\n  This article is an introductory review of forward-backward splitting with a\nspecial emphasis on practical implementation concerns. Issues like stepsize\nselection, acceleration, stopping conditions, and initialization are\nconsidered. Numerical experiments are used to compare the effectiveness of\ndifferent approaches.\n  Many variations of forward-backward splitting are implemented in the solver\nFASTA (short for Fast Adaptive Shrinkage/Thresholding Algorithm). FASTA\nprovides a simple interface for applying forward-backward splitting to a broad\nrange of problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 00:38:52 GMT"}, {"version": "v2", "created": "Sun, 16 Nov 2014 22:34:37 GMT"}, {"version": "v3", "created": "Fri, 9 Jan 2015 02:56:53 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2016 23:52:27 GMT"}, {"version": "v5", "created": "Mon, 15 Feb 2016 23:24:09 GMT"}, {"version": "v6", "created": "Wed, 28 Dec 2016 03:25:36 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Goldstein", "Tom", ""], ["Studer", "Christoph", ""], ["Baraniuk", "Richard", ""]]}, {"id": "1411.4324", "submitter": "Yangyang Xu", "authors": "Yangyang Xu", "title": "Fast algorithms for Higher-order Singular Value Decomposition from\n  incomplete data", "comments": "To appear in Journal of Computational Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order singular value decomposition (HOSVD) is an efficient way for\ndata reduction and also eliciting intrinsic structure of multi-dimensional\narray data. It has been used in many applications, and some of them involve\nincomplete data. To obtain HOSVD of the data with missing values, one can first\nimpute the missing entries through a certain tensor completion method and then\nperform HOSVD to the reconstructed data. However, the two-step procedure can be\ninefficient and does not make reliable decomposition.\n  In this paper, we formulate an incomplete HOSVD problem and combine the two\nsteps into solving a single optimization problem, which simultaneously achieves\nimputation of missing values and also tensor decomposition. We also present two\nalgorithms for solving the problem based on block coordinate update. Global\nconvergence of both algorithms is shown under mild assumptions. The convergence\nof the second algorithm implies that of the popular higher-order orthogonality\niteration (HOOI) method, and thus we, for the first time, give global\nconvergence of HOOI.\n  In addition, we compare the proposed methods to state-of-the-art ones for\nsolving incomplete HOSVD and also low-rank tensor completion problems and\ndemonstrate the superior performance of our methods over other compared ones.\nFurthermore, we apply them to face recognition and MRI image reconstruction to\nshow their practical performance.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 23:26:22 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 14:22:24 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Xu", "Yangyang", ""]]}, {"id": "1411.5235", "submitter": "Garth Wells", "authors": "Sander Rhebergen, Garth N. Wells, Andrew J. Wathen, Richard F. Katz", "title": "Three-field block-preconditioners for models of coupled magma/mantle\n  dynamics", "comments": "To appear in SIAM Journal on Scientific Computing", "journal-ref": "SIAM J. Sci. Comput., 2015, 37(5), A2270-A2294", "doi": "10.1137/14099718X", "report-no": null, "categories": "math.NA cs.CE cs.NA physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a prescribed porosity, the coupled magma/mantle flow equations can be\nformulated as a two-field system of equations with velocity and pressure as\nunknowns. Previous work has shown that while optimal preconditioners for the\ntwo-field formulation can be obtained, the construction of preconditioners that\nare uniform with respect to model parameters is difficult. This limits the\napplicability of two-field preconditioners in certain regimes of practical\ninterest. We address this issue by reformulating the governing equations as a\nthree-field problem, which removes a term that was problematic in the two-field\nformulation in favour of an additional equation for a pressure-like field. For\nthe three-field problem, we develop and analyse new preconditioners and we show\nnumerically that they are optimal in terms of problem size and less sensitive\nto model parameters, compared to the two-field preconditioner. This extends the\napplicability of optimal preconditioners for coupled mantle/magma dynamics into\nparameter regimes of physical interest.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 14:28:02 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 18:51:24 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Rhebergen", "Sander", ""], ["Wells", "Garth N.", ""], ["Wathen", "Andrew J.", ""], ["Katz", "Richard F.", ""]]}, {"id": "1411.5668", "submitter": "Matthew Hirn", "authors": "Ariel Herbert-Voss, Matthew J. Hirn, and Frederick McCollum", "title": "Computing minimal interpolants in $C^{1,1}(\\mathbb{R}^d)$", "comments": "41 pages, 6 figures. Replaces arXiv:1307.3292. v2: Minor edits,\n  formatting changed. v3: Revised version, which includes numerous updates,\n  corrections and edits for clarification. v4: Minor edits. Software available\n  at: https://github.com/matthew-hirn/C-1-1-Interpolation", "journal-ref": "Revista Matem\\'atica Iberoamericana, volume 33, issue 1, pages\n  29-66, 2017", "doi": "10.4171/rmi/927", "report-no": null, "categories": "math.NA cs.DS cs.NA math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following interpolation problem. Suppose one is given a\nfinite set $E \\subset \\mathbb{R}^d$, a function $f: E \\rightarrow \\mathbb{R}$,\nand possibly the gradients of $f$ at the points of $E$. We want to interpolate\nthe given information with a function $F \\in C^{1,1}(\\mathbb{R}^d)$ with the\nminimum possible value of $\\mathrm{Lip} (\\nabla F)$. We present practical,\nefficient algorithms for constructing an $F$ such that $\\mathrm{Lip} (\\nabla\nF)$ is minimal, or for less computational effort, within a small dimensionless\nconstant of being minimal.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 20:41:10 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 19:24:42 GMT"}, {"version": "v3", "created": "Fri, 17 Jun 2016 14:11:02 GMT"}, {"version": "v4", "created": "Wed, 26 Oct 2016 20:57:24 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Herbert-Voss", "Ariel", ""], ["Hirn", "Matthew J.", ""], ["McCollum", "Frederick", ""]]}, {"id": "1411.5873", "submitter": "Zheng Qu", "authors": "Zheng Qu and Peter Richt\\'arik and Tong Zhang", "title": "Randomized Dual Coordinate Ascent with Arbitrary Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of minimizing the average of a large number of smooth\nconvex functions penalized with a strongly convex regularizer. We propose and\nanalyze a novel primal-dual method (Quartz) which at every iteration samples\nand updates a random subset of the dual variables, chosen according to an\narbitrary distribution. In contrast to typical analysis, we directly bound the\ndecrease of the primal-dual error (in expectation), without the need to first\nanalyze the dual error. Depending on the choice of the sampling, we obtain\nefficient serial, parallel and distributed variants of the method. In the\nserial case, our bounds match the best known bounds for SDCA (both with uniform\nand importance sampling). With standard mini-batching, our bounds predict\ninitial data-independent speedup as well as additional data-driven speedup\nwhich depends on spectral and sparsity properties of the data. We calculate\ntheoretical speedup factors and find that they are excellent predictors of\nactual speedup in practice. Moreover, we illustrate that it is possible to\ndesign an efficient mini-batch importance sampling. The distributed variant of\nQuartz is the first distributed SDCA-like method with an analysis for\nnon-separable data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 13:55:31 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""], ["Zhang", "Tong", ""]]}, {"id": "1411.6081", "submitter": "Cho-Jui Hsieh Cho-Jui Hsieh", "authors": "Cho-Jui Hsieh and Nagarajan Natarajan and Inderjit S. Dhillon", "title": "PU Learning for Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the matrix completion problem when the\nobservations are one-bit measurements of some underlying matrix M, and in\nparticular the observed samples consist only of ones and no zeros. This problem\nis motivated by modern applications such as recommender systems and social\nnetworks where only \"likes\" or \"friendships\" are observed. The problem of\nlearning from only positive and unlabeled examples, called PU\n(positive-unlabeled) learning, has been studied in the context of binary\nclassification. We consider the PU matrix completion problem, where an\nunderlying real-valued matrix M is first quantized to generate one-bit\nobservations and then a subset of positive entries is revealed. Under the\nassumption that M has bounded nuclear norm, we provide recovery guarantees for\ntwo different observation models: 1) M parameterizes a distribution that\ngenerates a binary matrix, 2) M is thresholded to obtain a binary matrix. For\nthe first case, we propose a \"shifted matrix completion\" method that recovers M\nusing only a subset of indices corresponding to ones, while for the second\ncase, we propose a \"biased matrix completion\" method that recovers the\n(thresholded) binary matrix. Both methods yield strong error bounds --- if M is\nn by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotes\nthe fraction of ones observed. This implies a sample complexity of O(n\\log n)\nones to achieve a small error, when M is dense and n is large. We extend our\nmethods and guarantees to the inductive matrix completion problem, where rows\nand columns of M have associated features. We provide efficient and scalable\noptimization procedures for both the methods and demonstrate the effectiveness\nof the proposed methods for link prediction (on real-world networks consisting\nof over 2 million nodes and 90 million links) and semi-supervised clustering\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 04:37:15 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Hsieh", "Cho-Jui", ""], ["Natarajan", "Nagarajan", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1411.6296", "submitter": "Joseph Vokt", "authors": "Charles Van Loan and Joseph Vokt", "title": "Approximating Matrices with Multiple Symmetries", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a tensor with various symmetries is properly unfolded, then the resulting\nmatrix inherits those symmetries. As tensor computations become increasingly\nimportant it is imperative that we develop efficient structure preserving\nmethods for matrices with multiple symmetries. In this paper we consider how to\nexploit and preserve structure in the pivoted Cholesky factorization when\napproximating a matrix $A$ that is both symmetric ($A=A^T$) and what we call\n{\\em perfect shuffle symmetric}, or {\\em perf-symmetric}. The latter property\nmeans that $A = \\Pi A\\Pi$ where $\\Pi$ is a permutation with the property that\n$\\Pi v = v$ if $v$ is the vec of a symmetric matrix and $\\Pi v = -v$ if $v$ is\nthe vec of a skew-symmetric matrix. Matrices with this structure can arise when\nan order-4 tensor $\\cal A$ is unfolded and its elements satisfy ${\\cal\nA}(i_{1},i_{2},i_{3},i_{4}) = {\\cal A}(i_{2},i_{1},i_{3},i_{4}) ={\\cal\nA}(i_{1},i_{2},i_{4},i_{3}) ={\\cal A}(i_{3},i_{4},i_{1},i_{2}).$ This is the\ncase in certain quantum chemistry applications where the tensor entries are\nelectronic repulsion integrals. Our technique involves a closed-form block\ndiagonalization followed by one or two half-sized pivoted Cholesky\nfactorizations. This framework allows for a lazy evaluation feature that is\nimportant if the entries in $\\cal A$ are expensive to compute. In addition to\nbeing a structure preserving rank reduction technique, we find that this\napproach for obtaining the Cholesky factorization reduces the work by up to a\nfactor of 4.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 20:46:19 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 02:14:57 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Van Loan", "Charles", ""], ["Vokt", "Joseph", ""]]}, {"id": "1411.6529", "submitter": "Tiancheng Li", "authors": "Tiancheng Li", "title": "The Optimal Arbitrary-Proportional Finite-Set-Partitioning", "comments": null, "journal-ref": "Frontiers of Information Technology & Electronic Engineering,\n  Volume 16, Issue 11, pp 969-984 (2015)", "doi": "10.1631/FITEE.1500199", "report-no": null, "categories": "cs.NA stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the arbitrary-proportional finite-set-partitioning\nproblem which involves partitioning a finite set into multiple subsets with\nrespect to arbitrary nonnegative proportions. This is the core art of many\nfundamental problems such as determining quotas for different individuals of\ndifferent weights or sampling from a discrete-valued weighted sample set to get\na new identically distributed but non-weighted sample set (e.g. the resampling\nneeded in the particle filter). The challenge raises as the size of each subset\nmust be an integer while its unbiased expectation is often not. To solve this\nproblem, a metric (cost function) is defined on their discrepancies and\ncorrespondingly a solution is proposed to determine the sizes of each subsets,\ngaining the minimal bias. Theoretical proof and simulation demonstrations are\nprovided to demonstrate the optimality of the scheme in the sense of the\nproposed metric.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:56:28 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Li", "Tiancheng", ""]]}, {"id": "1411.7018", "submitter": "Jun Liu", "authors": "Jun Liu and Brittany D. Froese and Adam M. Oberman and Mingqing Xiao", "title": "A multigrid scheme for 3D Monge-Amp\\`ere equations", "comments": "18 pages, 1 figure, 7 tables, 41 references. Accepted by\n  International Journal of Computer Mathematics (published online: 21 Nov 2016)", "journal-ref": null, "doi": "10.1080/00207160.2016.1247443", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elliptic Monge-Amp\\`ere equation is a fully nonlinear partial\ndifferential equation which has been the focus of increasing attention from the\nscientific computing community. Fast three dimensional solvers are needed, for\nexample in medical image registration but are not yet available. We build fast\nsolvers for smooth solutions in three dimensions using a nonlinear\nfull-approximation storage multigrid method. Starting from a second-order\naccurate centered finite difference approximation, we present a nonlinear\nGauss-Seidel iterative method which has a mechanism for selecting the convex\nsolution of the equation. The iterative method is used as an effective\nsmoother, combined with the full-approximation storage multigrid method.\nNumerical experiments are provided to validate the accuracy of the finite\ndifference scheme and illustrate the computational efficiency of the proposed\nmultigrid solver.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 20:47:00 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 21:47:56 GMT"}, {"version": "v3", "created": "Thu, 29 Dec 2016 15:55:00 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Liu", "Jun", ""], ["Froese", "Brittany D.", ""], ["Oberman", "Adam M.", ""], ["Xiao", "Mingqing", ""]]}, {"id": "1411.7245", "submitter": "Nicolas Gillis", "authors": "Arnaud Vandaele and Nicolas Gillis and Fran\\c{c}ois Glineur and Daniel\n  Tuyttens", "title": "Heuristics for Exact Nonnegative Matrix Factorization", "comments": "32 pages, 2 figures, 16 tables", "journal-ref": "Journal of Global Optimization 65 (2), pp 369-400, 2016", "doi": "10.1007/s10898-015-0350-z", "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exact nonnegative matrix factorization (exact NMF) problem is the\nfollowing: given an $m$-by-$n$ nonnegative matrix $X$ and a factorization rank\n$r$, find, if possible, an $m$-by-$r$ nonnegative matrix $W$ and an $r$-by-$n$\nnonnegative matrix $H$ such that $X = WH$. In this paper, we propose two\nheuristics for exact NMF, one inspired from simulated annealing and the other\nfrom the greedy randomized adaptive search procedure. We show that these two\nheuristics are able to compute exact nonnegative factorizations for several\nclasses of nonnegative matrices (namely, linear Euclidean distance matrices,\nslack matrices, unique-disjointness matrices, and randomly generated matrices)\nand as such demonstrate their superiority over standard multi-start strategies.\nWe also consider a hybridization between these two heuristics that allows us to\ncombine the advantages of both methods. Finally, we discuss the use of these\nheuristics to gain insight on the behavior of the nonnegative rank, i.e., the\nminimum factorization rank such that an exact NMF exists. In particular, we\ndisprove a conjecture on the nonnegative rank of a Kronecker product, propose a\nnew upper bound on the extension complexity of generic $n$-gons and conjecture\nthe exact value of (i) the extension complexity of regular $n$-gons and (ii)\nthe nonnegative rank of a submatrix of the slack matrix of the correlation\npolytope.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 14:33:59 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Vandaele", "Arnaud", ""], ["Gillis", "Nicolas", ""], ["Glineur", "Fran\u00e7ois", ""], ["Tuyttens", "Daniel", ""]]}, {"id": "1411.7663", "submitter": "Stephan Schmidt Dr.", "authors": "Stephan Schmidt", "title": "A Two Stage CVT / Eikonal Convection Mesh Deformation Approach for Large\n  Nodal Deformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two step mesh deformation approach for large nodal deformations, typically\narising from non-parametric shape optimization, fluid-structure interaction or\ncomputer graphics, is considered. Two major difficulties, collapsed cells and\nan undesirable parameterization, are overcome by considering a special form of\nray tracing paired with a centroid Voronoi reparameterization. The ray\ndirection is computed by solving an Eikonal equation. With respect to the\nHadamard form of the shape derivative, both steps are within the kernel of the\nobjective and have no negative impact on the minimizer. The paper concludes\nwith applications in 2D and 3D fluid dynamics and automatic code generation and\nmanages to solve these problems without any remeshing. The methodology is\navailable as a FEniCS shape optimization add-on at\nhttp://www.mathematik.uni-wuerzburg.de/~schmidt/femorph.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 17:44:25 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Schmidt", "Stephan", ""]]}, {"id": "1411.7801", "submitter": "Kirk Soodhalter", "authors": "Kirk M. Soodhalter", "title": "Stagnation of block GMRES and its relationship to block FOM", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the the convergence behavior of block GMRES and characterize the\nphenomenon of stagnation which is then related to the behavior of the block FOM\nmethod. We generalize the block FOM method to generate well-defined\napproximations in the case that block FOM would normally break down, and these\ngeneralized solutions are used in our analysis. This behavior is also related\nto the principal angles between the column-space of the previous block GMRES\nresidual and the current minimum residual constraint space. At iteration $j$,\nit is shown that the proper generalization of GMRES stagnation to the block\nsetting relates to the columnspace of the $j$th block Arnoldi vector. Our\nanalysis covers both the cases of normal iterations as well as block Arnoldi\nbreakdown wherein dependent basis vectors are replaced with random ones.\nNumerical examples are given to illustrate what we have proven, including a\nsmall application problem to demonstrate the validity of the analysis in a less\npathological case.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 10:27:42 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 21:43:40 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2015 13:07:23 GMT"}, {"version": "v4", "created": "Mon, 11 Jul 2016 12:42:06 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Soodhalter", "Kirk M.", ""]]}]