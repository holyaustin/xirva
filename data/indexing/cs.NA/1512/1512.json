[{"id": "1512.00933", "submitter": "Francois-Xavier Briol", "authors": "Fran\\c{c}ois-Xavier Briol, Chris. J. Oates, Mark Girolami, Michael A.\n  Osborne and Dino Sejdinovic", "title": "Probabilistic Integration: A Role in Statistical Computation?", "comments": "Several improvements suggested by reviewers, including additional\n  experiments on uncertainty quantification properties. Change of title:\n  previously \"Probabilistic Integration: A Role for Statisticians in Numerical\n  Analysis?\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A research frontier has emerged in scientific computation, wherein numerical\nerror is regarded as a source of epistemic uncertainty that can be modelled.\nThis raises several statistical challenges, including the design of statistical\nmethods that enable the coherent propagation of probabilities through a\n(possibly deterministic) computational work-flow. This paper examines the case\nfor probabilistic numerical methods in routine statistical computation. Our\nfocus is on numerical integration, where a probabilistic integrator is equipped\nwith a full distribution over its output that reflects the presence of an\nunknown numerical error. Our main technical contribution is to establish, for\nthe first time, rates of posterior contraction for these methods. These show\nthat probabilistic integrators can in principle enjoy the \"best of both\nworlds\", leveraging the sampling efficiency of Monte Carlo methods whilst\nproviding a principled route to assess the impact of numerical error on\nscientific conclusions. Several substantial applications are provided for\nillustration and critical evaluation, including examples from statistical\nmodelling, computer graphics and a computer model for an oil reservoir.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 02:52:33 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 22:29:14 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 06:09:18 GMT"}, {"version": "v4", "created": "Mon, 11 Apr 2016 09:15:20 GMT"}, {"version": "v5", "created": "Thu, 20 Oct 2016 08:44:17 GMT"}, {"version": "v6", "created": "Wed, 18 Oct 2017 14:15:40 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Briol", "Fran\u00e7ois-Xavier", ""], ["Oates", "Chris. J.", ""], ["Girolami", "Mark", ""], ["Osborne", "Michael A.", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1512.00984", "submitter": "Quanming Yao", "authors": "Quanming Yao, James T. Kwok, Wenliang Zhong", "title": "Fast Low-Rank Matrix Learning with Nonconvex Regularization", "comments": "Long version of conference paper appeared ICDM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank modeling has a lot of important applications in machine learning,\ncomputer vision and social network analysis. While the matrix rank is often\napproximated by the convex nuclear norm, the use of nonconvex low-rank\nregularizers has demonstrated better recovery performance. However, the\nresultant optimization problem is much more challenging. A very recent\nstate-of-the-art is based on the proximal gradient algorithm. However, it\nrequires an expensive full SVD in each proximal step. In this paper, we show\nthat for many commonly-used nonconvex low-rank regularizers, a cutoff can be\nderived to automatically threshold the singular values obtained from the\nproximal operator. This allows the use of power method to approximate the SVD\nefficiently. Besides, the proximal operator can be reduced to that of a much\nsmaller matrix projected onto this leading subspace. Convergence, with a rate\nof O(1/T) where T is the number of iterations, can be guaranteed. Extensive\nexperiments are performed on matrix completion and robust principal component\nanalysis. The proposed method achieves significant speedup over the\nstate-of-the-art. Moreover, the matrix solution obtained is more accurate and\nhas a lower rank than that of the traditional nuclear norm regularizer.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 08:32:17 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""], ["Zhong", "Wenliang", ""]]}, {"id": "1512.01110", "submitter": "Yang Song", "authors": "Yang Song, Jun Zhu", "title": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization", "comments": "Accepted to AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian matrix completion has been studied based on a low-rank matrix\nfactorization formulation with promising results. However, little work has been\ndone on Bayesian matrix completion based on the more direct spectral\nregularization formulation. We fill this gap by presenting a novel Bayesian\nmatrix completion method based on spectral regularization. In order to\ncircumvent the difficulties of dealing with the orthonormality constraints of\nsingular vectors, we derive a new equivalent form with relaxed constraints,\nwhich then leads us to design an adaptive version of spectral regularization\nfeasible for Bayesian inference. Our Bayesian method requires no parameter\ntuning and can infer the number of latent factors automatically. Experiments on\nsynthetic and real datasets demonstrate encouraging results on rank recovery\nand collaborative filtering, with notably good results for very sparse\nmatrices.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 15:16:19 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 02:51:22 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Song", "Yang", ""], ["Zhu", "Jun", ""]]}, {"id": "1512.01748", "submitter": "Ying Zhang", "authors": "Ying Zhang", "title": "Restricted Low-Rank Approximation via ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix low-rank approximation problem with additional convex constraints\ncan find many applications and has been extensively studied before. However,\nthis problem is shown to be nonconvex and NP-hard; most of the existing\nsolutions are heuristic and application-dependent. In this paper, we show that,\nother than tons of application in current literature, this problem can be used\nto recover a feasible solution for SDP relaxation. By some sophisticated\ntricks, it can be equivalently posed in an appropriate form for the Alternating\nDirection Method of Multipliers (ADMM) to solve. The two updates of ADMM\ninclude the basic matrix low-rank approximation and projection onto a convex\nset. Different from the general non-convex problems, the sub-problems in each\nstep of ADMM can be solved exactly and efficiently in spite of their\nnon-convexity. Moreover, the algorithm will converge exponentially under proper\nconditions. The simulation results confirm its superiority over existing\nsolutions. We believe that the results in this paper provide a useful tool for\nthis important problem and will help to extend the application of ADMM to the\nnon-convex regime.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 06:12:15 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Zhang", "Ying", ""]]}, {"id": "1512.01904", "submitter": "Chengtao Li", "authors": "Chengtao Li, Suvrit Sra and Stefanie Jegelka", "title": "Gauss quadrature for matrix inverse forms with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for accelerating a spectrum of machine learning\nalgorithms that require computation of bilinear inverse forms $u^\\top A^{-1}u$,\nwhere $A$ is a positive definite matrix and $u$ a given vector. Our framework\nis built on Gauss-type quadrature and easily scales to large, sparse matrices.\nFurther, it allows retrospective computation of lower and upper bounds on\n$u^\\top A^{-1}u$, which in turn accelerates several algorithms. We prove that\nthese bounds tighten iteratively and converge at a linear (geometric) rate. To\nour knowledge, ours is the first work to demonstrate these key properties of\nGauss-type quadrature, which is a classical and deeply studied topic. We\nillustrate empirical consequences of our results by using quadrature to\naccelerate machine learning tasks involving determinantal point processes and\nsubmodular optimization, and observe tremendous speedups in several instances.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 04:13:45 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 04:12:21 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Li", "Chengtao", ""], ["Sra", "Suvrit", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1512.01927", "submitter": "Haoran Chen", "authors": "Haoran Chen and Yanfeng Sun and Junbin Gao and Yongli Hu", "title": "Fast Optimization Algorithm on Riemannian Manifolds and Its Application\n  in Low-Rank Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of optimizing a class of composite functions\non Riemannian manifolds and a new first order optimization algorithm (FOA) with\na fast convergence rate is proposed. Through the theoretical analysis for FOA,\nit has been proved that the algorithm has quadratic convergence. The\nexperiments in the matrix completion task show that FOA has better performance\nthan other first order optimization methods on Riemannian manifolds. A fast\nsubspace pursuit method based on FOA is proposed to solve the low-rank\nrepresentation model based on augmented Lagrange method on the low rank matrix\nvariety. Experimental results on synthetic and real data sets are presented to\ndemonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in\nterms of faster convergence and higher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 06:44:23 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chen", "Haoran", ""], ["Sun", "Yanfeng", ""], ["Gao", "Junbin", ""], ["Hu", "Yongli", ""]]}, {"id": "1512.02390", "submitter": "J\\\"org Stiller", "authors": "Joerg Stiller", "title": "Nonuniformly weighted Schwarz smoothers for spectral element multigrid", "comments": "Multigrid method; Schwarz methods; spectral element method; p-version\n  finite element method", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid Schwarz/multigrid method for spectral element solvers to the Poisson\nequation in $\\mathbb R^2$ is presented. It extends the additive Schwarz method\nstudied by J. Lottes and P. Fischer (J. Sci. Comput. 24:45--78, 2005) by\nintroducing nonuniform weight distributions based on the smoothed sign\nfunction. Using a V-cycle with only one pre-smoothing, the new method attains\nlogarithmic convergence rates in the range from 1.2 to 1.9, which corresponds\nto residual reductions of almost two orders of magnitude. Compared to the\noriginal method, it reduces the iteration count by a factor of 1.5 to 3,\nleading to runtime savings of about 50 percent. In numerical experiments the\nmethod proved robust with respect to the mesh size and polynomial orders up to\n32. Used as a preconditioner for the (inexact) CG method it is also suited for\nanisotropic meshes and easily extended to diffusion problems with variable\ncoefficients.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 10:21:29 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 10:18:40 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Stiller", "Joerg", ""]]}, {"id": "1512.02671", "submitter": "Per-Gunnar Martinsson", "authors": "Per-Gunnar Martinsson, Gregorio Quintana-Orti, Nathan Heavner, Robert\n  van de Geijn", "title": "Householder QR Factorization with Randomization for Column Pivoting\n  (HQRRP). FLAME Working Note #78", "comments": null, "journal-ref": null, "doi": null, "report-no": "FLAME Working Note #78", "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem when adding column pivoting to the Householder QR\nfactorization is that only about half of the computation can be cast in terms\nof high performing matrix-matrix multiplications, which greatly limits the\nbenefits that can be derived from so-called blocking of algorithms. This paper\ndescribes a technique for selecting groups of pivot vectors by means of\nrandomized projections. It is demonstrated that the asymptotic flop count for\nthe proposed method is $2mn^2 - (2/3)n^3$ for an $m\\times n$ matrix, identical\nto that of the best classical unblocked Householder QR factorization algorithm\n(with or without pivoting). Experiments demonstrate acceleration in speed of\nclose to an order of magnitude relative to the {\\sc geqp3} function in LAPACK,\nwhen executed on a modern CPU with multiple cores. Further, experiments\ndemonstrate that the quality of the randomized pivot selection strategy is\nroughly the same as that of classical column pivoting. The described algorithm\nis made available under Open Source license and can be used with LAPACK or\nlibflame.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 21:51:53 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 04:14:33 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Martinsson", "Per-Gunnar", ""], ["Quintana-Orti", "Gregorio", ""], ["Heavner", "Nathan", ""], ["van de Geijn", "Robert", ""]]}, {"id": "1512.03224", "submitter": "Linxiao Yang", "authors": "Jun Fang, Linxiao Yang, and Hongbin Li", "title": "Spectral Compressed Sensing via CANDECOMP/PARAFAC Decomposition of\n  Incomplete Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the line spectral estimation problem which aims to recover a\nmixture of complex sinusoids from a small number of randomly observed time\ndomain samples. Compressed sensing methods formulates line spectral estimation\nas a sparse signal recovery problem by discretizing the continuous frequency\nparameter space into a finite set of grid points. Discretization, however,\ninevitably incurs errors and leads to deteriorated estimation performance. In\nthis paper, we propose a new method which leverages recent advances in tensor\ndecomposition. Specifically, we organize the observed data into a structured\ntensor and cast line spectral estimation as a CANDECOMP/PARAFAC (CP)\ndecomposition problem with missing entries. The uniqueness of the CP\ndecomposition allows the frequency components to be super-resolved with\ninfinite precision. Simulation results show that the proposed method provides a\ncompetitive estimate accuracy compared with existing state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 12:04:56 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Fang", "Jun", ""], ["Yang", "Linxiao", ""], ["Li", "Hongbin", ""]]}, {"id": "1512.03251", "submitter": "Evgeny Nikulchev", "authors": "V.N. Petrushin, E.V. Nikulchev, D.A. Korolev", "title": "Histogram Arithmetic under Uncertainty of Probability Density Function", "comments": "10 pages", "journal-ref": "Applied Mathematical Sciences 9(2015) 7043-7052", "doi": "10.12988/ams.2015.510644", "report-no": null, "categories": "cs.NA stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article we propose a method of performing arithmetic operations on\nvaria-bles with unknown distribution. The approach to the evaluation results of\narithme-tic operations can select probability intervals of the algebraic\nequations and their systems solutions, of differential equations and their\nsystems in case of histogram evaluation of the empirical density distributions\nof random parameters.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 13:42:35 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Petrushin", "V. N.", ""], ["Nikulchev", "E. V.", ""], ["Korolev", "D. A.", ""]]}, {"id": "1512.03335", "submitter": "Mario Fern\\'andez-Pend\\'as", "authors": "Mario Fern\\'andez-Pend\\'as, Elena Akhmatskaya, J. M. Sanz-Serna", "title": "Adaptive multi-stage integrators for optimal energy conservation in\n  molecular simulations", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2016.09.035", "report-no": null, "categories": "cs.NA physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new Adaptive Integration Approach (AIA) to be used in a wide\nrange of molecular simulations. Given a simulation problem and a step size, the\nmethod automatically chooses the optimal scheme out of an available family of\nnumerical integrators. Although we focus on two-stage splitting integrators,\nthe idea may be used with more general families. In each instance, the\nsystem-specific integrating scheme identified by our approach is optimal in the\nsense that it provides the best conservation of energy for harmonic forces. The\nAIA method has been implemented in the BCAM-modified GROMACS software package.\nNumerical tests in molecular dynamics and hybrid Monte Carlo simulations of\nconstrained and unconstrained physical systems show that the method\nsuccessfully realises the fail-safe strategy. In all experiments, and for each\nof the criteria employed, the AIA is at least as good as, and often\nsignificantly outperforms the standard Verlet scheme, as well as fixed\nparameter, optimized two-stage integrators. In particular, the sampling\nefficiency found in simulations using the AIA is up to 5 times better than the\none achieved with other tested schemes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 17:31:21 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 08:14:28 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Fern\u00e1ndez-Pend\u00e1s", "Mario", ""], ["Akhmatskaya", "Elena", ""], ["Sanz-Serna", "J. M.", ""]]}, {"id": "1512.03357", "submitter": "Thomas Dierkes", "authors": "Thomas Dierkes", "title": "Construction of ODE systems from time series data by a highly flexible\n  modelling approach", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a down-to-earth approach to purely data-based modelling of\nunknown dynamical systems is presented. Starting from a classical, explicit ODE\nformulation $y' = f(t,y)$ of a dynamical system, a method determining the\nunknown right-hand side $f(t,y)$ from some trajectory data $y_{k}(t_{j})$,\npossibly very sparse, is given. As illustrative examples, a semi-standard\npredator-prey model is reconstructed from a data set describing the population\nnumbers of hares and lynxes over a period of twenty years, and a simple damped\npendulum system with a highly non-linear right-hand side is recovered from some\nartificial but very sparse data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 18:14:33 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Dierkes", "Thomas", ""]]}, {"id": "1512.04468", "submitter": "Basil Bayati", "authors": "Basil S. Bayati", "title": "A Method to Calculate the Exit Time in Stochastic Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method is presented to compute the exit time for the stochastic\nsimulation algorithm. The method is based on the addition of a series of random\nvariables and is derived using the convolution theorem. The final distribution\nis derived and approximated in the frequency domain. The distribution for the\nfinal time is transformed back to the real domain and can be sampled from in a\nsimulation. The result is an approximation of the classical stochastic\nsimulation algorithm that requires fewer random variates. An analysis of the\nerror and speedup compared to the stochastic simulation algorithm is presented.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 19:40:17 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Bayati", "Basil S.", ""]]}, {"id": "1512.06500", "submitter": "Gang Wu", "authors": "Gang Wu, Ting-ting Feng, Li-jia Zhang, and Meng Yang", "title": "Inexact Krylov Subspace Algorithms for Large Matrix Exponential\n  Eigenproblem from Dimensionality Reduction", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matrix exponential discriminant analysis (EDA) is a generalized discriminant\nanalysis method based on matrix exponential. It can essentially overcome the\nintrinsic difficulty of small sample size problem that exists in the classical\nlinear discriminant analysis (LDA). However, for data with high dimension, one\nhas to solve a large matrix exponential eigenproblem in this method, and the\ntime complexity is dominated by the computation of exponential of large\nmatrices. In this paper, we propose two inexact Krylov subspace algorithms for\nsolving the large matrix exponential eigenproblem effectively. The contribution\nof this work is threefold. First, we consider how to compute matrix\nexponential-vector products efficiently, which is the key step in the Krylov\nsubspace method. Second, we compare the discriminant analysis criterion of EDA\nand that of LDA from a theoretical point of view. Third, we establish a\nrelationship between the accuracy of the approximate eigenvectors and the\ndistance to nearest neighbour classifier, and show why the matrix exponential\neigenproblem can be solved approximately in practice. Numerical experiments on\nsome real-world databases show superiority of our new algorithms over many\nstate-of-the-art algorithms for face recognition.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 05:47:12 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Wu", "Gang", ""], ["Feng", "Ting-ting", ""], ["Zhang", "Li-jia", ""], ["Yang", "Meng", ""]]}, {"id": "1512.06626", "submitter": "Mostafa Jani", "authors": "M. Jani, E. Babolian, S. Javadi, D. Bhatta", "title": "Banded operational matrices for Bernstein polynomials and application to\n  the fractional advection-dispersion equation", "comments": "19 pages submitted to Numerical Algorithms", "journal-ref": "Jani, M., Babolian, E., Javadi, S. et al. Numer Algor (2016).\n  doi:10.1007/s11075-016-0229-1", "doi": "10.1007/s11075-016-0229-1", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the papers dealing with derivation and applications of operational\nmatrices of Bernstein polynomials, a basis transformation, commonly a\ntransformation to power basis, is used. The main disadvantage of this method is\nthat the transformation may be ill-conditioned. Moreover, when applied to the\nnumerical simulation of a functional differential equation, it leads to dense\noperational matrices and so a dense coefficient matrix is obtained. In this\npaper, we present a new property for Bernstein polynomials. Using this\nproperty, we build exact banded operational matrices for derivatives of\nBernstein polynomials. Next, as an application, we propose a new numerical\nmethod based on a Petrov-Galerkin variational formulation and the new\noperational matrices utilizing the dual Bernstein basis for the time-fractional\nadvection-dispersion equation. Finally, we show that the proposed method leads\nto a narrow-banded linear system and so less computational effort is required\nto obtain the desired accuracy for the approximate solution. Some numerical\nexamples are provided to demonstrate the efficiency of the method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 13:53:15 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 07:24:52 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Jani", "M.", ""], ["Babolian", "E.", ""], ["Javadi", "S.", ""], ["Bhatta", "D.", ""]]}, {"id": "1512.06629", "submitter": "Mostafa Jani", "authors": "S. Javadi, M. Jani, E. Babolian", "title": "A numerical scheme for space-time fractional advection-dispersion\n  equation", "comments": "13 pages", "journal-ref": "International Journal of Nonlinear Analysis and Applications, Vol.\n  7, Issue 2, pp. 331-343 (2016)", "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a numerical resolution of the space-time fractional\nadvection-dispersion equation. After time discretization, we utilize\ncollocation technique and implement a product integration method in order to\nsimplify the evaluation of the terms involving spatial fractional order\nderivatives. Then utilizing Bernstein polynomials as basis, the problem is\ntransformed into a linear system of algebraic equations. Error analysis and\norder of convergence for the proposed method are also discussed. Some numerical\nexperiments are presented to demonstrate the effectiveness of the proposed\nmethod and to confirm the analytic results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 14:03:51 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 16:47:02 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Javadi", "S.", ""], ["Jani", "M.", ""], ["Babolian", "E.", ""]]}, {"id": "1512.06890", "submitter": "Peter Richtarik", "authors": "Robert Mansel Gower and Peter Richtarik", "title": "Stochastic Dual Ascent for Solving Linear Systems", "comments": "This is a slightly refreshed version of the paper originally\n  submitted on Dec 21, 2015. We have added a numerical experiment involving\n  randomized Kaczmarz for rank-deficient systems, added a few relevant\n  references, and corrected a few typos. Stats: 29 pages, 2 algorithms, 1\n  figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new randomized iterative algorithm---stochastic dual ascent\n(SDA)---for finding the projection of a given vector onto the solution space of\na linear system. The method is dual in nature: with the dual being a\nnon-strongly concave quadratic maximization problem without constraints. In\neach iteration of SDA, a dual variable is updated by a carefully chosen point\nin a subspace spanned by the columns of a random matrix drawn independently\nfrom a fixed distribution. The distribution plays the role of a parameter of\nthe method. Our complexity results hold for a wide family of distributions of\nrandom matrices, which opens the possibility to fine-tune the stochasticity of\nthe method to particular applications. We prove that primal iterates associated\nwith the dual process converge to the projection exponentially fast in\nexpectation, and give a formula and an insightful lower bound for the\nconvergence rate. We also prove that the same rate applies to dual function\nvalues, primal function values and the duality gap. Unlike traditional\niterative methods, SDA converges under no additional assumptions on the system\n(e.g., rank, diagonal dominance) beyond consistency. In fact, our lower bound\nimproves as the rank of the system matrix drops. Many existing randomized\nmethods for linear systems arise as special cases of SDA, including randomized\nKaczmarz, randomized Newton, randomized coordinate descent, Gaussian descent,\nand their variants. In special cases where our method specializes to a known\nalgorithm, we either recover the best known rates, or improve upon them.\nFinally, we show that the framework can be applied to the distributed average\nconsensus problem to obtain an array of new algorithms. The randomized gossip\nalgorithm arises as a special case.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 22:09:30 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 06:43:52 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Gower", "Robert Mansel", ""], ["Richtarik", "Peter", ""]]}, {"id": "1512.07349", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen, Baichuan Zhang, Mohammad Al Hasan, Alfred O. Hero", "title": "Incremental Method for Spectral Clustering of Increasing Orders", "comments": "in KDD workshop on mining and learning graph, 2016\n  http://www.mlgworkshop.org/2016/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)\nof a graph Laplacian matrix have been widely used for spectral clustering and\ncommunity detection. However, in real-life applications the number of clusters\nor communities (say, $K$) is generally unknown a-priori. Consequently, the\nmajority of the existing methods either choose $K$ heuristically or they repeat\nthe clustering method with different choices of $K$ and accept the best\nclustering result. The first option, more often, yields suboptimal result,\nwhile the second option is computationally expensive. In this work, we propose\nan incremental method for constructing the eigenspectrum of the graph Laplacian\nmatrix. This method leverages the eigenstructure of graph Laplacian matrix to\nobtain the $K$-th eigenpairs of the Laplacian matrix given a collection of all\nthe $K-1$ smallest eigenpairs. Our proposed method adapts the Laplacian matrix\nsuch that the batch eigenvalue decomposition problem transforms into an\nefficient sequential leading eigenpair computation problem. As a practical\napplication, we consider user-guided spectral clustering. Specifically, we\ndemonstrate that users can utilize the proposed incremental method for\neffective eigenpair computation and determining the desired number of clusters\nbased on multiple clustering metrics.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 03:55:24 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 21:16:01 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 03:21:30 GMT"}, {"version": "v4", "created": "Sat, 13 Aug 2016 20:30:35 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Zhang", "Baichuan", ""], ["Hasan", "Mohammad Al", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1512.08716", "submitter": "Ohad Asor", "authors": "Ohad Asor and Avishy Carmi", "title": "On Approximating Univariate NP-Hard Integrals", "comments": "Need to show more evidence to the claims", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating a definite integral of product of cosines to within an accuracy\nof n binary digits where the integrand depends on input integers x[k] given in\nbinary radix, is equivalent to counting the number of equal-sum partitions of\nthe integers and is thus a #P problem. Similarly, integrating this function\nfrom zero to infinity and deciding whether the result is either zero or\ninfinity is an NP-Complete problem. Efficient numerical integration methods\nsuch as the double exponential formula and the sinc approximation have been\naround since the mid 70's. Noting the hardness of approximating the integral we\nargue that the proven rates of convergence of such methods cannot possibly be\ncorrect since they give rise to an anomalous result as P=#P.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 22:20:24 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2016 07:12:23 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 09:56:48 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Asor", "Ohad", ""], ["Carmi", "Avishy", ""]]}, {"id": "1512.09156", "submitter": "Shashanka Ubaru", "authors": "Shashanka Ubaru, Arya Mazumdar and Yousef Saad", "title": "Low rank approximation and decomposition of large matrices using error\n  correcting codes", "comments": null, "journal-ref": "IEEE Transactions on Information Theory ( Volume: 63, Issue: 9,\n  Sept. 2017 ) Page(s): 5544 - 5558", "doi": "10.1109/TIT.2017.2723898", "report-no": null, "categories": "cs.IT cs.LG cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank approximation is an important tool used in many applications of\nsignal processing and machine learning. Recently, randomized sketching\nalgorithms were proposed to effectively construct low rank approximations and\nobtain approximate singular value decompositions of large matrices. Similar\nideas were used to solve least squares regression problems. In this paper, we\nshow how matrices from error correcting codes can be used to find such low rank\napproximations and matrix decompositions, and extend the framework to linear\nleast squares regression problems. The benefits of using these code matrices\nare the following: (i) They are easy to generate and they reduce randomness\nsignificantly. (ii) Code matrices with mild properties satisfy the subspace\nembedding property, and have a better chance of preserving the geometry of an\nentire subspace of vectors. (iii) For parallel and distributed applications,\ncode matrices have significant advantages over structured random matrices and\nGaussian random matrices. (iv) Unlike Fourier or Hadamard transform matrices,\nwhich require sampling $O(k\\log k)$ columns for a rank-$k$ approximation, the\nlog factor is not necessary for certain types of code matrices. That is,\n$(1+\\epsilon)$ optimal Frobenius norm error can be achieved for a rank-$k$\napproximation with $O(k/\\epsilon)$ samples. (v) Fast multiplication is possible\nwith structured code matrices, so fast approximations can be achieved for\ngeneral dense input matrices. (vi) For least squares regression problem\n$\\min\\|Ax-b\\|_2$ where $A\\in \\mathbb{R}^{n\\times d}$, the $(1+\\epsilon)$\nrelative error approximation can be achieved with $O(d/\\epsilon)$ samples, with\nhigh probability, when certain code matrices are used.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 21:29:57 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 18:14:33 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 22:54:25 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Ubaru", "Shashanka", ""], ["Mazumdar", "Arya", ""], ["Saad", "Yousef", ""]]}]