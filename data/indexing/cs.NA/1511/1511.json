[{"id": "1511.01080", "submitter": "Michel Rueher", "authors": "H\\'el\\`ene Collavizza, Claude Michel, Michel Rueher", "title": "Searching input values hitting suspicious Intervals in programs with\n  floating-point operations", "comments": null, "journal-ref": "28th International Conference on Software and Systems\n  (ICTSS-2016)., Oct 2016, Graz, Austria. 2016, LNCS", "doi": null, "report-no": null, "categories": "cs.PL cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs with floating-point computations are often derived from mathematical\nmodels or designed with the semantics of the real numbers in mind. However, for\na given input, the computed path with floating-point numbers may differ from\nthe path corresponding to the same computation with real numbers. A common\npractice when validating such programs consists in estimating the accuracy of\nfloating-point computations with respect to the same sequence of operations in\nan ide-alized semantics of real numbers. However, state-of-the-art tools\ncompute an over-approximation of the error introduced by floating-point\noperations. As a consequence, totally inappropriate behaviors of a program may\nbe dreaded but the developer does not know whether these behaviors will\nactually occur, or not. In this paper, we introduce a new constraint-based\napproach that searches for test cases in the part of the over-approximation\nwhere errors due to floating-point arithmetic would lead to inappropriate\nbehaviors.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 20:46:32 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 08:22:12 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Collavizza", "H\u00e9l\u00e8ne", ""], ["Michel", "Claude", ""], ["Rueher", "Michel", ""]]}, {"id": "1511.01306", "submitter": "J\\'er\\'emy Emile Cohen", "authors": "Jeremy E. Cohen", "title": "About Notations in Multiway Array Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper gives an overview of notations used in multiway array processing.\nWe redefine the vectorization and matricization operators to comply with some\nproperties of the Kronecker product. The tensor product and Kronecker product\nare also represented with two different symbols, and it is shown how these\nnotations lead to clearer expressions for multiway array operations. Finally,\nthe paper recalls the useful yet widely unknown properties of the array normal\nlaw with suggested notations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 12:38:56 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 16:46:59 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Cohen", "Jeremy E.", ""]]}, {"id": "1511.01353", "submitter": "Matt Challacombe", "authors": "Matt Challacombe", "title": "A N-Body Solver for Free Mesh Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization of the Gaussian RBF kernel is developed for free-mesh\ninterpolation in the flat, polynomial limit corresponding to Taylor expansion\nand the Vandermonde basis of geometric moments. With this spectral\napproximation, a top-down octree-scoping of an interpolant is found by\nrecursively decomposing the residual, similar to the work of Driscoll and\nHeryudono (2007), except that in the current approach the grid is decoupled\nfrom the low rank approximation, allowing partial separation of sampling errors\n(the mesh) from representation errors (the polynomial order). Then, it is\npossible to demonstrate roughly 5 orders of magnitude improvement in free-mesh\ninterpolation errors for the three-dimensional Franke function, relative to\nprevious benchmarks. As in related work on $N$-body methods for factorization\nby square root iteration (Challacombe 2015), some emphasis is placed on\nresolution of the identity.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 15:58:41 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Challacombe", "Matt", ""]]}, {"id": "1511.01593", "submitter": "Vishwas Rao", "authors": "Vishwas Rao, Adrian Sandu, Michael Ng, and Elias Nino-Ruiz", "title": "Robust data assimilation using $L_1$ and Huber norms", "comments": "25 pages, Submitted to SISC", "journal-ref": null, "doi": null, "report-no": "CSL-TR-15-21", "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation is the process to fuse information from priors,\nobservations of nature, and numerical models, in order to obtain best estimates\nof the parameters or state of a physical system of interest. Presence of large\nerrors in some observational data, e.g., data collected from a faulty\ninstrument, negatively affect the quality of the overall assimilation results.\n  This work develops a systematic framework for robust data assimilation. The\nnew algorithms continue to produce good analyses in the presence of observation\noutliers. The approach is based on replacing the traditional $\\L_2$ norm\nformulation of data assimilation problems with formulations based on $\\L_1$ and\nHuber norms. Numerical experiments using the Lorenz-96 and the shallow water on\nthe sphere models illustrate how the new algorithms outperform traditional data\nassimilation approaches in the presence of data outliers.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 03:17:42 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Rao", "Vishwas", ""], ["Sandu", "Adrian", ""], ["Ng", "Michael", ""], ["Nino-Ruiz", "Elias", ""]]}, {"id": "1511.01598", "submitter": "Haishan Ye", "authors": "Haishan Ye, Yujun Li, Zhihua Zhang", "title": "A Simple Approach to Optimal CUR Decomposition", "comments": "this work can be derived from other work easily", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Prior optimal CUR decomposition and near optimal column reconstruction\nmethods have been established by combining BSS sampling and adaptive sampling.\nIn this paper, we propose a new approach to the optimal CUR decomposition and\nnear optimal column reconstruction by just using leverage score sampling. In\nour approach, both the BSS sampling and adaptive sampling are not needed.\nMoreover, our approach is the first $O(\\mathrm{nnz}(\\A))$ optimal CUR algorithm\nwhere $\\A$ is a data matrix in question. We also extend our approach to the\nNystr{\\\"o}m method, obtaining a fast algorithm which runs $\\tilde{O}(n^{2})$ or\n$O(\\mathrm{\\nnz}(\\A))$\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 03:48:28 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 14:46:20 GMT"}, {"version": "v3", "created": "Mon, 30 May 2016 07:03:41 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 02:08:26 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Ye", "Haishan", ""], ["Li", "Yujun", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1511.02134", "submitter": "Huber Markus", "authors": "Bj\\\"orn Gmeiner and Markus Huber and Lorenz John and Ulrich R\\\"ude and\n  Barbara Wohlmuth", "title": "A quantitative performance analysis for Stokes solvers at the extreme\n  scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a systematic quantitative performance analysis for\nlarge finite element computations on extreme scale computing systems. Three\nparallel iterative solvers for the Stokes system, discretized by low order\ntetrahedral elements, are compared with respect to their numerical efficiency\nand their scalability running on up to $786\\,432$ parallel threads. A genuine\nmultigrid method for the saddle point system using an Uzawa-type smoother\nprovides the best overall performance with respect to memory consumption and\ntime-to-solution. The largest system solved on a Blue Gene/Q system has more\nthan ten trillion ($1.1 \\cdot 10 ^{13}$) unknowns and requires about 13 minutes\ncompute time. Despite the matrix free and highly optimized implementation, the\nmemory requirement for the solution vector and the auxiliary vectors is about\n200 TByte. Brandt's notion of \"textbook multigrid efficiency\" is employed to\nstudy the algorithmic performance of iterative solvers. A recent extension of\nthis paradigm to \"parallel textbook multigrid efficiency\" makes it possible to\nassess also the efficiency of parallel iterative solvers for a given hardware\narchitecture in absolute terms. The efficiency of the method is demonstrated\nfor simulating incompressible fluid flow in a pipe filled with spherical\nobstacles.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 16:07:04 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Gmeiner", "Bj\u00f6rn", ""], ["Huber", "Markus", ""], ["John", "Lorenz", ""], ["R\u00fcde", "Ulrich", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1511.02770", "submitter": "Denys Dutykh", "authors": "Gayaz Khakimzyanov, Denys Dutykh (LAMA)", "title": "On supraconvergence phenomenon for second order centered finite\n  differences on non-uniform grids", "comments": "26 pages, 2 figures, 2 tables, 37 references. Other author's papers\n  can be downloaded at http://www.denys-dutykh.com/", "journal-ref": "Journal of Computational and Applied Mathematics (2017), Vol. 326,\n  pp. 1-14", "doi": "10.1016/j.cam.2017.05.006", "report-no": null, "categories": "math.NA cs.NA math.CA physics.class-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present study we consider an example of a boundary value problem for a\nsimple second order ordinary differential equation, which may exhibit a\nboundary layer phenomenon. We show that usual central finite differences, which\nare second order accurate on a uniform grid, can be substantially upgraded to\nthe fourth order by a suitable choice of the underlying non-uniform grid. This\nexample is quite pedagogical and may give some ideas for more complex problems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 13:24:20 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 11:16:12 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 12:19:50 GMT"}, {"version": "v4", "created": "Fri, 3 Feb 2017 13:59:27 GMT"}, {"version": "v5", "created": "Mon, 10 Apr 2017 08:42:13 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Khakimzyanov", "Gayaz", "", "LAMA"], ["Dutykh", "Denys", "", "LAMA"]]}, {"id": "1511.02771", "submitter": "Denys Dutykh", "authors": "Gayaz Khakimzyanov, Denys Dutykh (LAMA), Dimitrios Mitsotakis, Nina\n  Shokina", "title": "Numerical simulation of conservation laws with moving grid nodes:\n  Application to tsunami wave modelling", "comments": "46 pages, 7 figures, 7 tables, 94 references. Accepted to\n  Geosciences. Other author's papers can be downloaded at\n  http://www.denys-dutykh.com/", "journal-ref": "Geosciences (2019), Vol. 9, Issue 5, p. 197", "doi": "10.3390/geosciences9050197", "report-no": "https://www.mdpi.com/2076-3263/9/5/197", "categories": "math.NA cs.NA physics.class-ph physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the present article we describe a few simple and efficient finite volume\ntype schemes on moving grids in one spatial dimension combined with appropriate\npredictor-corrector method to achieve higher resolution. The underlying finite\nvolume scheme is conservative and it is accurate up to the second order in\nspace. The main novelty consists in the motion of the grid. This new dynamic\naspect can be used to resolve better the areas with large solution gradients or\nany other special features. No interpolation procedure is employed, thus\nunnecessary solution smearing is avoided, and therefore, our method enjoys\nexcellent conservation properties. The resulting grid is completely\nredistributed according the choice of the so-called monitor function. Several\nmore or less universal choices of the monitor function are provided. Finally,\nthe performance of the proposed algorithm is illustrated on several examples\nstemming from the simple linear advection to the simulation of complex shallow\nwater waves. The exact well-balanced property is proven. We believe that the\ntechniques described in our paper can be beneficially used to model tsunami\nwave propagation and run-up.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:21:00 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 15:54:52 GMT"}, {"version": "v3", "created": "Thu, 20 Oct 2016 12:19:20 GMT"}, {"version": "v4", "created": "Fri, 3 Feb 2017 13:56:19 GMT"}, {"version": "v5", "created": "Mon, 8 Apr 2019 12:30:16 GMT"}, {"version": "v6", "created": "Wed, 15 May 2019 08:19:21 GMT"}, {"version": "v7", "created": "Sat, 23 May 2020 15:29:02 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Khakimzyanov", "Gayaz", "", "LAMA"], ["Dutykh", "Denys", "", "LAMA"], ["Mitsotakis", "Dimitrios", ""], ["Shokina", "Nina", ""]]}, {"id": "1511.03925", "submitter": "M Borkowski", "authors": "M. Borkowski", "title": "The scalability of the matrices in direct Trefftz method in 2D Laplace\n  problem", "comments": null, "journal-ref": null, "doi": "10.1016/j.enganabound.2015.11.010", "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an interesting property of the matrices that may be\nobtained with the use of direct Trefftz method. It is proved analytically for\n2D Laplace problem that values of the elements of matrices describing the\ncapacitance of two scaled domains are inversely proportional to the scalability\nfactor. As an example of the application the capacitance extraction problem is\nchosen. Concise description of the algorithm in which the scalability property\ncan be utilized is given. Furthermore some numerical results of the algorithm\nare presented.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 15:25:39 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Borkowski", "M.", ""]]}, {"id": "1511.04515", "submitter": "Hao Zhuang", "authors": "Hao Zhuang, Wenjian Yu, Ilgweon Kang, Xinan Wang, Chung-Kuan Cheng", "title": "An Algorithmic Framework for Efficient Large-Scale Circuit Simulation\n  Using Exponential Integrators", "comments": "6 pages; ACM/IEEE DAC 2015", "journal-ref": null, "doi": "10.1145/2744769.2744793", "report-no": null, "categories": "cs.CE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient algorithmic framework for time domain circuit\nsimulation using exponential integrator. This work addresses several critical\nissues exposed by previous matrix exponential based circuit simulation\nresearch, and makes it capable of simulating stiff nonlinear circuit system at\na large scale. In this framework, the system's nonlinearity is treated with\nexponential Rosenbrock-Euler formulation. The matrix exponential and vector\nproduct is computed using invert Krylov subspace method. Our proposed method\nhas several distinguished advantages over conventional formulations (e.g., the\nwell-known backward Euler with Newton-Raphson method). The matrix factorization\nis performed only for the conductance/resistance matrix G, without being\nperformed for the combinations of the capacitance/inductance matrix C and\nmatrix G, which are used in traditional implicit formulations. Furthermore, due\nto the explicit nature of our formulation, we do not need to repeat LU\ndecompositions when adjusting the length of time steps for error controls. Our\nalgorithm is better suited to solving tightly coupled post-layout circuits in\nthe pursuit for full-chip simulation. Our experimental results validate the\nadvantages of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 05:57:35 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhuang", "Hao", ""], ["Yu", "Wenjian", ""], ["Kang", "Ilgweon", ""], ["Wang", "Xinan", ""], ["Cheng", "Chung-Kuan", ""]]}, {"id": "1511.04519", "submitter": "Hao Zhuang", "authors": "Hao Zhuang, Shih-Hung Weng, Jeng-Hau Lin, Chung-Kuan Cheng", "title": "MATEX: A Distributed Framework for Transient Simulation of Power\n  Distribution Networks", "comments": "ACM/IEEE DAC 2014. arXiv admin note: substantial text overlap with\n  arXiv:1505.06699", "journal-ref": null, "doi": "10.1145/2593069.2593160", "report-no": null, "categories": "cs.CE cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed MATEX, a distributed framework for transient simulation of power\ndistribution networks (PDNs). MATEX utilizes matrix exponential kernel with\nKrylov subspace approximations to solve differential equations of linear\ncircuit. First, the whole simulation task is divided into subtasks based on\ndecompositions of current sources, in order to reduce the computational\noverheads. Then these subtasks are distributed to different computing nodes and\nprocessed in parallel. Within each node, after the matrix factorization at the\nbeginning of simulation, the adaptive time stepping solver is performed without\nextra matrix re-factorizations. MATEX overcomes the stiff-ness hinder of\nprevious matrix exponential-based circuit simulator by rational Krylov subspace\nmethod, which leads to larger step sizes with smaller dimensions of Krylov\nsubspace bases and highly accelerates the whole computation. MATEX outperforms\nboth traditional fixed and adaptive time stepping methods, e.g., achieving\naround 13X over the trapezoidal framework with fixed time step for the IBM\npower grid benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 06:29:51 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhuang", "Hao", ""], ["Weng", "Shih-Hung", ""], ["Lin", "Jeng-Hau", ""], ["Cheng", "Chung-Kuan", ""]]}, {"id": "1511.04695", "submitter": "Linxiao Yang", "authors": "Linxiao Yang and Jun Fang and Hongbin Li and Bing Zeng", "title": "An Iterative Reweighted Method for Tucker Decomposition of Incomplete\n  Multiway Tensors", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2572047", "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of low-rank decomposition of incomplete multiway\ntensors. Since many real-world data lie on an intrinsically low dimensional\nsubspace, tensor low-rank decomposition with missing entries has applications\nin many data analysis problems such as recommender systems and image\ninpainting. In this paper, we focus on Tucker decomposition which represents an\nNth-order tensor in terms of N factor matrices and a core tensor via\nmultilinear operations. To exploit the underlying multilinear low-rank\nstructure in high-dimensional datasets, we propose a group-based log-sum\npenalty functional to place structural sparsity over the core tensor, which\nleads to a compact representation with smallest core tensor. The method for\nTucker decomposition is developed by iteratively minimizing a surrogate\nfunction that majorizes the original objective function, which results in an\niterative reweighted process. In addition, to reduce the computational\ncomplexity, an over-relaxed monotone fast iterative shrinkage-thresholding\ntechnique is adapted and embedded in the iterative reweighted process. The\nproposed method is able to determine the model complexity (i.e. multilinear\nrank) in an automatic way. Simulation results show that the proposed algorithm\noffers competitive performance compared with other existing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 12:56:36 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Yang", "Linxiao", ""], ["Fang", "Jun", ""], ["Li", "Hongbin", ""], ["Zeng", "Bing", ""]]}, {"id": "1511.04870", "submitter": "Juergen Zechner", "authors": "Gernot Beer and Benjamin Marussig and J\\\"urgen Zechner and Christian\n  D\\\"unser and Thomas-Peter Fries", "title": "Isogeometric Boundary Element Analysis with elasto-plastic inclusions.\n  Part 1: Plane problems", "comments": "21 pages, 16 figures, 1 table", "journal-ref": null, "doi": "10.1016/j.cma.2016.03.035", "report-no": null, "categories": "cs.NA math.NA physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a novel approach is presented for the isogeometric Boundary\nElement analysis of domains that contain inclusions with different elastic\nproperties than the ones used for computing the fundamental solutions. In\naddition the inclusion may exhibit inelastic material behavior. In this paper\nonly plane stress/strain problems are considered.\n  In our approach the geometry of the inclusion is described using NURBS basis\nfunctions. The advantage over currently used methods is that no discretization\ninto cells is required in order to evaluate the arising volume integrals. The\nother difference to current approaches is that Kernels of lower singularity are\nused in the domain term. The implementation is verified on simple finite and\ninfinite domain examples with various boundary conditions. Finally a practical\napplication in geomechanics is presented.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 08:54:43 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Beer", "Gernot", ""], ["Marussig", "Benjamin", ""], ["Zechner", "J\u00fcrgen", ""], ["D\u00fcnser", "Christian", ""], ["Fries", "Thomas-Peter", ""]]}, {"id": "1511.05133", "submitter": "Canyi Lu", "authors": "Canyi Lu, Huan Li, Zhouchen Lin, Shuicheng Yan", "title": "Fast Proximal Linearized Alternating Direction Method of Multiplier with\n  Parallel Splitting", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Augmented Lagragian Method (ALM) and Alternating Direction Method of\nMultiplier (ADMM) have been powerful optimization methods for general convex\nprogramming subject to linear constraint. We consider the convex problem whose\nobjective consists of a smooth part and a nonsmooth but simple part. We propose\nthe Fast Proximal Augmented Lagragian Method (Fast PALM) which achieves the\nconvergence rate $O(1/K^2)$, compared with $O(1/K)$ by the traditional PALM. In\norder to further reduce the per-iteration complexity and handle the\nmulti-blocks problem, we propose the Fast Proximal ADMM with Parallel Splitting\n(Fast PL-ADMM-PS) method. It also partially improves the rate related to the\nsmooth part of the objective function. Experimental results on both synthesized\nand real world data demonstrate that our fast methods significantly improve the\nprevious PALM and ADMM.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 13:14:42 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Lu", "Canyi", ""], ["Li", "Huan", ""], ["Lin", "Zhouchen", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1511.05194", "submitter": "Lingchen Zhu", "authors": "Lingchen Zhu, Entao Liu, James H. McClellan", "title": "Sparse-promoting Full Waveform Inversion based on Online Orthonormal\n  Dictionary Learning", "comments": "This paper has already been accepted by Geophysics", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full waveform inversion (FWI) delivers high-resolution images of the\nsubsurface by minimizing iteratively the misfit between the recorded and\ncalculated seismic data. It has been attacked successfully with the\nGauss-Newton method and sparsity promoting regularization based on fixed\nmultiscale transforms that permit significant subsampling of the seismic data\nwhen the model perturbation at each FWI data-fitting iteration can be\nrepresented with sparse coefficients. Rather than using analytical transforms\nwith predefined dictionaries to achieve sparse representation, we introduce an\nadaptive transform called the Sparse Orthonormal Transform (SOT) whose\ndictionary is learned from many small training patches taken from the model\nperturbations in previous iterations. The patch-based dictionary is constrained\nto be orthonormal and trained with an online approach to provide the best\nsparse representation of the complex features and variations of the entire\nmodel perturbation. The complexity of the training method is proportional to\nthe cube of the number of samples in one small patch. By incorporating both\ncompressive subsampling and the adaptive SOT-based representation into the\nGauss-Newton least-squares problem for each FWI iteration, the model\nperturbation can be recovered after an l1-norm sparsity constraint is applied\non the SOT coefficients. Numerical experiments on synthetic models demonstrate\nthat the SOT-based sparsity promoting regularization can provide robust FWI\nresults with reduced computation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:50:48 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 02:04:52 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Zhu", "Lingchen", ""], ["Liu", "Entao", ""], ["McClellan", "James H.", ""]]}, {"id": "1511.05208", "submitter": "Arvind Saibaba", "authors": "Arvind K. Saibaba", "title": "HOID: Higher Order Interpolatory Decomposition for tensors based on\n  Tucker representation", "comments": "28 pages, 9 figures, minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a CUR-type factorization for tensors in the Tucker format based on\ninterpolatory decomposition, which we will denote as Higher Order Interpolatory\nDecomposition (HOID). Given a tensor $\\mathcal{X}$, the algorithm provides a\nset of column vectors $\\{ \\mathbf{C}_n\\}_{n=1}^d$ which are columns extracted\nfrom the mode-$n$ tensor unfolding, along with a core tensor $\\mathcal{G}$ and\ntogether, they satisfy some error bounds. Compared to the Higher Order SVD\n(HOSVD) algorithm, the HOID provides a decomposition that preserves certain\nimportant features of the original tensor such as sparsity, non-negativity,\ninteger values, etc. Error bounds along with detailed estimates of\ncomputational costs are provided. The algorithms proposed in this paper have\nbeen validated against carefully chosen numerical examples which highlight the\nfavorable properties of the algorithms. Related methods for subset selection\nproposed for matrix CUR decomposition, such as Discrete Empirical Interpolation\nmethod (DEIM) and leverage score sampling, have also been extended to tensors\nand are compared against our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 22:36:26 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 20:13:06 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 20:19:53 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Saibaba", "Arvind K.", ""]]}, {"id": "1511.05261", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Robust PCA via Nonconvex Rank Approximation", "comments": "IEEE International Conference on Data Mining", "journal-ref": null, "doi": "10.1109/ICDM.2015.15", "report-no": null, "categories": "cs.CV cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous applications in data mining and machine learning require recovering\na matrix of minimal rank. Robust principal component analysis (RPCA) is a\ngeneral framework for handling this kind of problems. Nuclear norm based convex\nsurrogate of the rank function in RPCA is widely investigated. Under certain\nassumptions, it can recover the underlying true low rank matrix with high\nprobability. However, those assumptions may not hold in real-world\napplications. Since the nuclear norm approximates the rank by adding all\nsingular values together, which is essentially a $\\ell_1$-norm of the singular\nvalues, the resulting approximation error is not trivial and thus the resulting\nmatrix estimator can be significantly biased. To seek a closer approximation\nand to alleviate the above-mentioned limitations of the nuclear norm, we\npropose a nonconvex rank approximation. This approximation to the matrix rank\nis tighter than the nuclear norm. To solve the associated nonconvex\nminimization problem, we develop an efficient augmented Lagrange multiplier\nbased optimization algorithm. Experimental results demonstrate that our method\noutperforms current state-of-the-art algorithms in both accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 03:00:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1511.05362", "submitter": "Yujun Li", "authors": "Yujun Li, Kaichun Mo, Haishan Ye", "title": "Accelerating Random Kaczmarz Algorithm Based on Clustering Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kaczmarz algorithm is an efficient iterative algorithm to solve\noverdetermined consistent system of linear equations. During each updating\nstep, Kaczmarz chooses a hyperplane based on an individual equation and\nprojects the current estimate for the exact solution onto that space to get a\nnew estimate. Many vairants of Kaczmarz algorithms are proposed on how to\nchoose better hyperplanes. Using the property of randomly sampled data in\nhigh-dimensional space, we propose an accelerated algorithm based on clustering\ninformation to improve block Kaczmarz and Kaczmarz via Johnson-Lindenstrauss\nlemma. Additionally, we theoretically demonstrate convergence improvement on\nblock Kaczmarz algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 11:58:24 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 02:46:52 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Li", "Yujun", ""], ["Mo", "Kaichun", ""], ["Ye", "Haishan", ""]]}, {"id": "1511.06029", "submitter": "Abtin Rahimian", "authors": "Eduardo Corona, Abtin Rahimian, Denis Zorin", "title": "A Tensor-Train accelerated solver for integral equations in complex\n  geometries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework using the Quantized Tensor Train (QTT) decomposition\nto accurately and efficiently solve volume and boundary integral equations in\nthree dimensions. We describe how the QTT decomposition can be used as a\nhierarchical compression and inversion scheme for matrices arising from the\ndiscretization of integral equations. For a broad range of problems,\ncomputational and storage costs of the inversion scheme are extremely modest\n$O(\\log N)$ and once the inverse is computed, it can be applied in $O(N \\log\nN)$.\n  We analyze the QTT ranks for hierarchically low rank matrices and discuss its\nrelationship to commonly used hierarchical compression techniques such as FMM\nand HSS. We prove that the QTT ranks are bounded for translation-invariant\nsystems and argue that this behavior extends to non-translation invariant\nvolume and boundary integrals.\n  For volume integrals, the QTT decomposition provides an efficient direct\nsolver requiring significantly less memory compared to other fast direct\nsolvers. We present results demonstrating the remarkable performance of the\nQTT-based solver when applied to both translation and non-translation invariant\nvolume integrals in 3D.\n  For boundary integral equations, we demonstrate that using a QTT\ndecomposition to construct preconditioners for a Krylov subspace method leads\nto an efficient and robust solver with a small memory footprint. We test the\nQTT preconditioners in the iterative solution of an exterior elliptic boundary\nvalue problem (Laplace) formulated as a boundary integral equation in complex,\nmultiply connected geometries.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 00:11:08 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 23:14:53 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Corona", "Eduardo", ""], ["Rahimian", "Abtin", ""], ["Zorin", "Denis", ""]]}, {"id": "1511.06033", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Athanasios N. Nikolakopoulos, Vassilis Kalantzis, Efstratios\n  Gallopoulos and John D. Garofalakis", "title": "EigenRec: Generalizing PureSVD for Effective and Efficient Top-N\n  Recommendations", "comments": "23 pages. Journal version of the conference paper \"Factored Proximity\n  Models for Top-N Recommendation\"", "journal-ref": null, "doi": "10.1007/s10115-018-1197-7", "report-no": null, "categories": "cs.IR cs.DB cs.DC cs.NA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce EigenRec; a versatile and efficient Latent-Factor framework for\nTop-N Recommendations that includes the well-known PureSVD algorithm as a\nspecial case. EigenRec builds a low dimensional model of an inter-item\nproximity matrix that combines a similarity component, with a scaling operator,\ndesigned to control the influence of the prior item popularity on the final\nmodel. Seeing PureSVD within our framework provides intuition about its inner\nworkings, exposes its inherent limitations, and also, paves the path towards\npainlessly improving its recommendation performance. A comprehensive set of\nexperiments on the MovieLens and the Yahoo datasets based on widely applied\nperformance metrics, indicate that EigenRec outperforms several\nstate-of-the-art algorithms, in terms of Standard and Long-Tail recommendation\naccuracy, exhibiting low susceptibility to sparsity, even in its most extreme\nmanifestations -- the Cold-Start problems. At the same time EigenRec has an\nattractive computational profile and it can apply readily in large-scale\nrecommendation settings.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 00:34:51 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 16:00:46 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 12:56:14 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Nikolakopoulos", "Athanasios N.", ""], ["Kalantzis", "Vassilis", ""], ["Gallopoulos", "Efstratios", ""], ["Garofalakis", "John D.", ""]]}, {"id": "1511.06227", "submitter": "Xinrui He", "authors": "Ran Wang, Xinrui He", "title": "Empirical Research and Automatic Processing Method of Precision-specific\n  Operation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Significant inaccuracy often occurs during the process of mathematical\ncalculation due to the digit limitation of floating point, which may lead to\ncatastrophic loss. Normally, people believe that adjustment of floating-point\nprecision is an effective way to solve this problem, since high-precision\nfloating-point has more digits to store information. Thus, it is a prevalent\nmethod to reduce the inaccuracy in much floating-point related research, that\nperforming all the operations with higher precision. However, we discover that\nsome operations may lead to larger error in higher precision. In this paper, we\ndefine this kind of operation that generates large error due to precision\nadjustment a precision-specific operation. Furthermore, we propose a\nlight-weight searching algorithm for detecting precision-specific operations\nand figure out an automatic processing method to fixing them. In addition, we\nconducted an experiment on the scientific mathematical library of GLIBC. The\nresult shows that there are many precision-specific operations, and our fixing\napproach can significantly reduce the inaccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:10:04 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 07:58:15 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Wang", "Ran", ""], ["He", "Xinrui", ""]]}, {"id": "1511.06324", "submitter": "Jinshan Zeng", "authors": "Yu Wang, Wotao Yin and Jinshan Zeng", "title": "Global Convergence of ADMM in Nonconvex Nonsmooth Optimization", "comments": "33 pages, 1 figure, Accepted by Journal of Scientific Computing", "journal-ref": "Journal of Scientific Computing, 2018", "doi": null, "report-no": "UCLA CAM Report 15-62", "categories": "math.OC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the convergence of the alternating direction method\nof multipliers (ADMM) for minimizing a nonconvex and possibly nonsmooth\nobjective function, $\\phi(x_0,\\ldots,x_p,y)$, subject to coupled linear\nequality constraints. Our ADMM updates each of the primal variables\n$x_0,\\ldots,x_p,y$, followed by updating the dual variable. We separate the\nvariable $y$ from $x_i$'s as it has a special role in our analysis.\n  The developed convergence guarantee covers a variety of nonconvex functions\nsuch as piecewise linear functions, $\\ell_q$ quasi-norm, Schatten-$q$\nquasi-norm ($0<q<1$), minimax concave penalty (MCP), and smoothly clipped\nabsolute deviation (SCAD) penalty. It also allows nonconvex constraints such as\ncompact manifolds (e.g., spherical, Stiefel, and Grassman manifolds) and linear\ncomplementarity constraints. Also, the $x_0$-block can be almost any lower\nsemi-continuous function.\n  By applying our analysis, we show, for the first time, that several ADMM\nalgorithms applied to solve nonconvex models in statistical learning,\noptimization on manifold, and matrix decomposition are guaranteed to converge.\n  Our results provide sufficient conditions for ADMM to converge on (convex or\nnonconvex) monotropic programs with three or more blocks, as they are special\ncases of our model.\n  ADMM has been regarded as a variant to the augmented Lagrangian method (ALM).\nWe present a simple example to illustrate how ADMM converges but ALM diverges\nwith bounded penalty parameter $\\beta$. Indicated by this example and other\nanalysis in this paper, ADMM might be a better choice than ALM for some\nnonconvex \\emph{nonsmooth} problems, because ADMM is not only easier to\nimplement, it is also more likely to converge for the concerned scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 09:25:16 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 23:54:20 GMT"}, {"version": "v3", "created": "Sat, 5 Mar 2016 09:59:56 GMT"}, {"version": "v4", "created": "Wed, 23 Nov 2016 08:20:14 GMT"}, {"version": "v5", "created": "Tue, 4 Jul 2017 01:17:32 GMT"}, {"version": "v6", "created": "Wed, 6 Dec 2017 12:31:00 GMT"}, {"version": "v7", "created": "Thu, 12 Apr 2018 03:12:56 GMT"}, {"version": "v8", "created": "Wed, 30 May 2018 07:08:35 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Wang", "Yu", ""], ["Yin", "Wotao", ""], ["Zeng", "Jinshan", ""]]}, {"id": "1511.06468", "submitter": "Di Wang", "authors": "Di Wang, Michael Mahoney, Nishanth Mohan, Satish Rao", "title": "Faster Parallel Solver for Positive Linear Programs via\n  Dynamically-Bucketed Selective Coordinate Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide improved parallel approximation algorithms for the important class\nof packing and covering linear programs. In particular, we present new parallel\n$\\epsilon$-approximate packing and covering solvers which run in\n$\\tilde{O}(1/\\epsilon^2)$ expected time, i.e., in expectation they take\n$\\tilde{O}(1/\\epsilon^2)$ iterations and they do $\\tilde{O}(N/\\epsilon^2)$\ntotal work, where $N$ is the size of the constraint matrix and $\\epsilon$ is\nthe error parameter, and where the $\\tilde{O}$ hides logarithmic factors. To\nachieve our improvement, we introduce an algorithmic technique of broader\ninterest: dynamically-bucketed selective coordinate descent (DB-SCD). At each\nstep of the iterative optimization algorithm, the DB-SCD method dynamically\nbuckets the coordinates of the gradient into those of roughly equal magnitude,\nand it updates all the coordinates in one of the buckets. This\ndynamically-bucketed updating permits us to take steps along several\ncoordinates with similar-sized gradients, thereby permitting more appropriate\nstep sizes at each step of the algorithm. In particular, this technique allows\nus to use in a straightforward manner the recent analysis from the breakthrough\nresults of Allen-Zhu and Orecchia [2] to achieve our still-further improved\nbounds. More generally, this method addresses \"interference\" among coordinates,\nby which we mean the impact of the update of one coordinate on the gradients of\nother coordinates. Such interference is a core issue in parallelizing\noptimization routines that rely on smoothness properties. Since our DB-SCD\nmethod reduces interference via updating a selective subset of variables at\neach iteration, we expect it may also have more general applicability in\noptimization.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 01:10:13 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Wang", "Di", ""], ["Mahoney", "Michael", ""], ["Mohan", "Nishanth", ""], ["Rao", "Satish", ""]]}, {"id": "1511.07042", "submitter": "Shuhao Cao", "authors": "James Brannick and Shuhao Cao", "title": "Bootstrap Multigrid for the Laplace-Beltrami Eigenvalue Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces bootstrap two-grid and multigrid finite element\napproximations to the Laplace-Beltrami (surface Laplacian) eigen-problem on a\nclosed surface. The proposed multigrid method is suitable for recovering\neigenvalues having large multiplicity, computing interior eigenvalues, and\napproximating the shifted indefinite eigen-problem. Convergence analysis is\ncarried out for a simplified two-grid algorithm and numerical experiments are\npresented to illustrate the basic components and ideas behind the overall\nbootstrap multigrid approach.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 18:05:43 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 19:04:36 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 18:23:46 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Brannick", "James", ""], ["Cao", "Shuhao", ""]]}, {"id": "1511.07305", "submitter": "Caterina Fenu", "authors": "Caterina Fenu and Desmond J. Higham", "title": "Block Matrix Formulations for Evolving Networks", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NA math.NA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many types of pairwise interaction take the form of a fixed set of nodes with\nedges that appear and disappear over time. In the case of discrete-time\nevolution, the resulting evolving network may be represented by a time-ordered\nsequence of adjacency matrices. We consider here the issue of representing the\nsystem as a single, higher dimensional block matrix, built from the individual\ntime-slices. We focus on the task of computing network centrality measures.\nFrom a modeling perspective, we show that there is a suitable block formulation\nthat allows us to recover dynamic centrality measures respecting time's arrow.\nFrom a computational perspective, we show that the new block formulation leads\nto the design of more effective numerical algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 16:47:13 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 10:03:39 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Fenu", "Caterina", ""], ["Higham", "Desmond J.", ""]]}, {"id": "1511.08062", "submitter": "Chen Xu", "authors": "Chen Xu, Zhouchen Lin, Zhenyu Zhao, Hongbin Zha", "title": "Relaxed Majorization-Minimization for Non-smooth and Non-convex\n  Optimization", "comments": "AAAI16", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new majorization-minimization (MM) method for non-smooth and\nnon-convex programs, which is general enough to include the existing MM\nmethods. Besides the local majorization condition, we only require that the\ndifference between the directional derivatives of the objective function and\nits surrogate function vanishes when the number of iterations approaches\ninfinity, which is a very weak condition. So our method can use a surrogate\nfunction that directly approximates the non-smooth objective function. In\ncomparison, all the existing MM methods construct the surrogate function by\napproximating the smooth component of the objective function. We apply our\nrelaxed MM methods to the robust matrix factorization (RMF) problem with\ndifferent regularizations, where our locally majorant algorithm shows\nadvantages over the state-of-the-art approaches for RMF. This is the first\nalgorithm for RMF ensuring, without extra assumptions, that any limit point of\nthe iterates is a stationary point.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 13:55:57 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Xu", "Chen", ""], ["Lin", "Zhouchen", ""], ["Zhao", "Zhenyu", ""], ["Zha", "Hongbin", ""]]}, {"id": "1511.08253", "submitter": "Stuart Hadfield", "authors": "Mihir K. Bhaskar, Stuart Hadfield, Anargyros Papageorgiou and Iasonas\n  Petras", "title": "Quantum Algorithms and Circuits for Scientific Computing", "comments": "43 pages, 11 figures", "journal-ref": "Quantum Information and Computation. 16, no. 3&4 (2016): 0197-0236", "doi": null, "report-no": null, "categories": "quant-ph cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum algorithms for scientific computing require modules implementing\nfundamental functions, such as the square root, the logarithm, and others. We\nrequire algorithms that have a well-controlled numerical error, that are\nuniformly scalable and reversible (unitary), and that can be implemented\nefficiently. We present quantum algorithms and circuits for computing the\nsquare root, the natural logarithm, and arbitrary fractional powers. We provide\nperformance guarantees in terms of their worst-case accuracy and cost. We\nfurther illustrate their performance by providing tests comparing them to the\nrespective floating point implementations found in widely used numerical\nsoftware.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 23:39:16 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Bhaskar", "Mihir K.", ""], ["Hadfield", "Stuart", ""], ["Papageorgiou", "Anargyros", ""], ["Petras", "Iasonas", ""]]}, {"id": "1511.08264", "submitter": "Przemys{\\l}aw Gospodarczyk Ph.D.", "authors": "Przemys{\\l}aw Gospodarczyk, Pawe{\\l} Wo\\'zny", "title": "Efficient degree reduction of B\\'ezier curves with box constraints using\n  dual bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give an efficient algorithm of degree reduction of B\\'ezier\ncurves with box constraints. The idea is to combine the previous iterative\napproach, that has been presented recently in (P. Gospodarczyk, Comput. Aided\nDes. 62 (2015), 143--151), with a fast method of construction of dual bases\nfrom (P. Wo\\'zny, J. Comput. Appl. Math. 260 (2014), 301--311) and a new\nefficient method of modification of dual bases.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 17:20:23 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 16:38:06 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Gospodarczyk", "Przemys\u0142aw", ""], ["Wo\u017any", "Pawe\u0142", ""]]}, {"id": "1511.08528", "submitter": "Ming Gu", "authors": "Christopher Melgaard, Ming Gu", "title": "Gaussian Elimination with Randomized Complete Pivoting", "comments": "5 figures, 33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian elimination with partial pivoting (GEPP) has long been among the\nmost widely used methods for computing the LU factorization of a given matrix.\nHowever, this method is also known to fail for matrices that induce large\nelement growth during the factorization process. In this paper, we propose a\nnew scheme, Gaussian elimination with randomized complete pivoting (GERCP) for\nthe efficient and reliable LU factorization of a given matrix. GERCP satisfies\nGECP (Gaussian elimination with complete pivoting) style element growth bounds\nwith high probability, yet costs only marginally higher than GEPP. Our\nnumerical experimental results strongly suggest that GERCP is as reliable as\nGECP and as efficient as GEPP for computing the LU factorization.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 23:25:44 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Melgaard", "Christopher", ""], ["Gu", "Ming", ""]]}, {"id": "1511.08547", "submitter": "Alex Druinsky", "authors": "Alex Druinsky, Eyal Carlebach, and Sivan Toledo", "title": "Wilkinson's Inertia-Revealing Factorization and Its Application to\n  Sparse Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new inertia-revealing factorization for sparse symmetric\nmatrices. The factorization scheme and the method for extracting the inertia\nfrom it were proposed in the 1960s for dense, banded, or tridiagonal matrices,\nbut they have been abandoned in favor of faster methods. We show that this\nscheme can be applied to any sparse symmetric matrix and that the fill in the\nfactorization is bounded by the fill in the sparse QR factorization of the same\nmatrix (but is usually much smaller). We describe our serial proof-of-concept\nimplementation, and present experimental results, studying the method's\nnumerical stability and performance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 03:11:17 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 16:17:33 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Druinsky", "Alex", ""], ["Carlebach", "Eyal", ""], ["Toledo", "Sivan", ""]]}, {"id": "1511.08739", "submitter": "Praveen Chandrashekar", "authors": "Praveen Chandrashekar, Markus Zenk", "title": "Well-balanced nodal discontinuous Galerkin method for Euler equations\n  with gravity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a well-balanced nodal discontinuous Galerkin (DG) scheme for\ncompressible Euler equations with gravity. The DG scheme makes use of\ndiscontinuous Lagrange basis functions supported at Gauss-Lobatto-Legendre\n(GLL) nodes together with GLL quadrature using the same nodes. The\nwell-balanced property is achieved by a specific form of source term\ndiscretization that depends on the nature of the hydrostatic solution, together\nwith the GLL nodes for quadrature of the source term. The scheme is able to\npreserve isothermal and polytropic stationary solutions upto machine precision\non any mesh composed of quadrilateral cells and for any gravitational\npotential. It is applied on several examples to demonstrate its well-balanced\nproperty and the improved resolution of small perturbations around the\nstationary solution.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 13:11:12 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 06:45:56 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Chandrashekar", "Praveen", ""], ["Zenk", "Markus", ""]]}, {"id": "1511.09159", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Ioannis Akrotirianakis, Amit Chakraborty", "title": "Proximal gradient method for huberized support vector machine", "comments": "in Pattern analysis and application, 2015", "journal-ref": null, "doi": "10.1007/s10044-015-0485-z", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Support Vector Machine (SVM) has been used in a wide variety of\nclassification problems. The original SVM uses the hinge loss function, which\nis non-differentiable and makes the problem difficult to solve in particular\nfor regularized SVMs, such as with $\\ell_1$-regularization. This paper\nconsiders the Huberized SVM (HSVM), which uses a differentiable approximation\nof the hinge loss function. We first explore the use of the Proximal Gradient\n(PG) method to solving binary-class HSVM (B-HSVM) and then generalize it to\nmulti-class HSVM (M-HSVM). Under strong convexity assumptions, we show that our\nalgorithm converges linearly. In addition, we give a finite convergence result\nabout the support of the solution, based on which we further accelerate the\nalgorithm by a two-stage method. We present extensive numerical experiments on\nboth synthetic and real datasets which demonstrate the superiority of our\nmethods over some state-of-the-art methods for both binary- and multi-class\nSVMs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 05:02:02 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Xu", "Yangyang", ""], ["Akrotirianakis", "Ioannis", ""], ["Chakraborty", "Amit", ""]]}]