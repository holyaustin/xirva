[{"id": "1711.00260", "submitter": "Andrew McRae", "authors": "Chris J. Budd, Andrew T. T. McRae, Colin J. Cotter", "title": "The scaling and skewness of optimally transported meshes on the sphere", "comments": "Updated following reviewer comments", "journal-ref": null, "doi": "10.1016/j.jcp.2018.08.028", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of numerical solution of PDEs, dynamic mesh redistribution\nmethods (r-adaptive methods) are an important procedure for increasing the\nresolution in regions of interest, without modifying the connectivity of the\nmesh. Key to the success of these methods is that the mesh should be\nsufficiently refined (locally) and flexible in order to resolve evolving\nsolution features, but at the same time not introduce errors through skewness\nand lack of regularity. Some state-of-the-art methods are bottom-up in that\nthey attempt to prescribe both the local cell size and the alignment to\nfeatures of the solution. However, the resulting problem is overdetermined,\nnecessitating a compromise between these conflicting requirements. An\nalternative approach, described in this paper, is to prescribe only the local\ncell size and augment this an optimal transport condition to provide global\nregularity. This leads to a robust and flexible algorithm for generating meshes\nfitted to an evolving solution, with minimal need for tuning parameters. Of\nparticular interest for geophysical modelling are meshes constructed on the\nsurface of the sphere. The purpose of this paper is to demonstrate that meshes\ngenerated on the sphere using this optimal transport approach have good\na-priori regularity and that the meshes produced are naturally aligned to\nvarious simple features. It is further shown that the sphere's intrinsic\ncurvature leads to more regular meshes than the plane. In addition to these\ngeneral results, we provide a wide range of examples relevant to practical\napplications, to showcase the behaviour of optimally transported meshes on the\nsphere. These range from axisymmetric cases that can be solved analytically to\nmore general examples that are tackled numerically. Evaluation of the singular\nvalues and singular vectors of the mesh transformation provides a quantitative\nmeasure of the mesh aniso...\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 09:17:45 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 22:57:20 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 14:45:40 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Budd", "Chris J.", ""], ["McRae", "Andrew T. T.", ""], ["Cotter", "Colin J.", ""]]}, {"id": "1711.00386", "submitter": "Nicolas Tremblay", "authors": "Luc LeMagoarou and Nicolas Tremblay and R\\'emi Gribonval", "title": "Analyzing the Approximation Error of the Fast Graph Fourier Transform", "comments": "ASILOMAR conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph Fourier transform (GFT) is in general dense and requires O(n^2)\ntime to compute and O(n^2) memory space to store. In this paper, we pursue our\nprevious work on the approximate fast graph Fourier transform (FGFT). The FGFT\nis computed via a truncated Jacobi algorithm, and is defined as the product of\nJ Givens rotations (very sparse orthogonal matrices). The truncation parameter,\nJ, represents a trade-off between precision of the transform and time of\ncomputation (and storage space). We explore further this trade-off and study,\non different types of graphs, how is the approximation error distributed along\nthe spectrum.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 15:09:10 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 12:45:26 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["LeMagoarou", "Luc", ""], ["Tremblay", "Nicolas", ""], ["Gribonval", "R\u00e9mi", ""]]}, {"id": "1711.00439", "submitter": "Shashanka Ubaru", "authors": "Shashanka Ubaru and Yousef Saad", "title": "Sampling and multilevel coarsening algorithms for fast matrix\n  approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses matrix approximation problems for matrices that are\nlarge, sparse and/or that are representations of large graphs. To tackle these\nproblems, we consider algorithms that are based primarily on coarsening\ntechniques, possibly combined with random sampling. A multilevel coarsening\ntechnique is proposed which utilizes a hypergraph associated with the data\nmatrix and a graph coarsening strategy based on column matching. Theoretical\nresults are established that characterize the quality of the dimension\nreduction achieved by a coarsening step, when a proper column matching strategy\nis employed. We consider a number of standard applications of this technique as\nwell as a few new ones. Among the standard applications we first consider the\nproblem of computing the partial SVD for which a combination of sampling and\ncoarsening yields significantly improved SVD results relative to sampling\nalone. We also consider the Column subset selection problem, a popular low rank\napproximation method used in data related applications, and show how multilevel\ncoarsening can be adapted for this problem. Similarly, we consider the problem\nof graph sparsification and show how coarsening techniques can be employed to\nsolve it. Numerical experiments illustrate the performances of the methods in\nvarious applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 16:57:51 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 18:06:41 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Ubaru", "Shashanka", ""], ["Saad", "Yousef", ""]]}, {"id": "1711.00603", "submitter": "Shunsuke Ono", "authors": "Shunsuke Ono, Takuma Kasai", "title": "Efficient Constrained Tensor Factorization by Alternating Optimization\n  with Primal-Dual Splitting", "comments": "5 pages, submitted to ICASSP2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorization with hard and/or soft constraints has played an\nimportant role in signal processing and data analysis. However, existing\nalgorithms for constrained tensor factorization have two drawbacks: (i) they\nrequire matrix-inversion; and (ii) they cannot (or at least is very difficult\nto) handle structured regularizations. We propose a new tensor factorization\nalgorithm that circumvents these drawbacks. The proposed method is built upon\nalternating optimization, and each subproblem is solved by a primal-dual\nsplitting algorithm, yielding an efficient and flexible algorithmic framework\nto constrained tensor factorization. The advantages of the proposed method over\na state-of-the-art constrained tensor factorization algorithm, called AO-ADMM,\nare demonstrated on regularized nonnegative tensor factorization.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 02:59:37 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Ono", "Shunsuke", ""], ["Kasai", "Takuma", ""]]}, {"id": "1711.00903", "submitter": "Katarzyna Swirydowicz", "authors": "Kasia \\'Swirydowicz, Noel Chalmers, Ali Karakus and Timothy Warburton", "title": "Acceleration of tensor-product operations for high-order finite element\n  methods", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to GPU kernel optimization and performance analysis of\nthree tensor-product operators arising in finite element methods. We provide a\nmathematical background to these operations and implementation details.\nAchieving close-to-the-peak performance for these operators requires extensive\noptimization because of the operators' properties: low arithmetic intensity,\ntiered structure, and the need to store intermediate results inside the kernel.\nWe give a guided overview of optimization strategies and we present a\nperformance model that allows us to compare the efficacy of these optimizations\nagainst an empirically calibrated roofline.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 19:45:33 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 15:18:19 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["\u015awirydowicz", "Kasia", ""], ["Chalmers", "Noel", ""], ["Karakus", "Ali", ""], ["Warburton", "Timothy", ""]]}, {"id": "1711.00954", "submitter": "Yuehaw Khoo", "authors": "Yuehaw Khoo, Jianfeng Lu, Lexing Ying", "title": "Efficient construction of tensor ring representations from sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an efficient method to compress a high dimensional\nfunction into a tensor ring format, based on alternating least-squares (ALS).\nSince the function has size exponential in $d$ where $d$ is the number of\ndimensions, we propose efficient sampling scheme to obtain $O(d)$ important\nsamples in order to learn the tensor ring. Furthermore, we devise an\ninitialization method for ALS that allows fast convergence in practice.\nNumerical examples show that to approximate a function with similar accuracy,\nthe tensor ring format provided by the proposed method has less parameters than\ntensor-train format and also better respects the structure of the original\nfunction.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 21:59:49 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 14:11:13 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Khoo", "Yuehaw", ""], ["Lu", "Jianfeng", ""], ["Ying", "Lexing", ""]]}, {"id": "1711.01322", "submitter": "Ju Liu", "authors": "Ju Liu and Alison L. Marsden", "title": "A unified continuum and variational multiscale formulation for fluids,\n  solids, and fluid-structure interaction", "comments": null, "journal-ref": "Computer Methods in Applied Mechanics and Engineering,\n  337:549-597, 2018", "doi": "10.1016/j.cma.2018.03.045", "report-no": null, "categories": "physics.comp-ph cs.NA math.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a unified continuum modeling framework for viscous fluids and\nhyperelastic solids using the Gibbs free energy as the thermodynamic potential.\nThis framework naturally leads to a pressure primitive variable formulation for\nthe continuum body, which is well-behaved in both compressible and\nincompressible regimes. Our derivation also provides a rational justification\nof the isochoric-volumetric additive split of free energies in nonlinear\ncontinuum mechanics. The variational multiscale analysis is performed for the\ncontinuum model to construct a foundation for numerical discretization. We\nfirst consider the continuum body instantiated as a hyperelastic material and\ndevelop a variational multiscale formulation for the hyper-elastodynamic\nproblem. The generalized-alpha method is applied for temporal discretization. A\nsegregated algorithm for the nonlinear solver is designed and carefully\nanalyzed. Second, we apply the new formulation to construct a novel unified\nformulation for fluid-solid coupled problems. The variational multiscale\nformulation is utilized for spatial discretization in both fluid and solid\nsubdomains. The generalized-alpha method is applied for the whole continuum\nbody, and optimal high-frequency dissipation is achieved in both fluid and\nsolid subproblems. A new predictor multi-corrector algorithm is developed based\non the segregated algorithm to attain a good balance between robustness and\nefficiency. The efficacy of the new formulations is examined in several\nbenchmark problems. The results indicate that the proposed modeling and\nnumerical methodologies constitute a promising technology for biomedical and\nengineering applications, particularly those necessitating incompressible\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 20:25:18 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:41:49 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 17:55:08 GMT"}, {"version": "v4", "created": "Wed, 12 Sep 2018 19:03:50 GMT"}, {"version": "v5", "created": "Mon, 1 Oct 2018 16:57:30 GMT"}, {"version": "v6", "created": "Sun, 28 Apr 2019 03:18:39 GMT"}, {"version": "v7", "created": "Sun, 1 Mar 2020 20:55:58 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Liu", "Ju", ""], ["Marsden", "Alison L.", ""]]}, {"id": "1711.01443", "submitter": "Shibabrat Naik", "authors": "Shibabrat Naik, Francois Lekien, Shane D. Ross", "title": "Computational Method for Phase Space Transport with Applications to Lobe\n  Dynamics and Rate of Escape", "comments": "33 pages, 17 figures", "journal-ref": "Regular and Chaotic Dynamics, 22(3): 272-297, 2017", "doi": "10.1134/S1560354717030078", "report-no": null, "categories": "math.DS cs.NA nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lobe dynamics and escape from a potential well are general frameworks\nintroduced to study phase space transport in chaotic dynamical systems. While\nthe former approach studies how regions of phase space are transported by\nreducing the flow to a two-dimensional map, the latter approach studies the\nphase space structures that lead to critical events by crossing periodic orbit\naround saddles. Both of these frameworks require computation with curves\nrepresented by millions of points-computing intersection points between these\ncurves and area bounded by the segments of these curves-for quantifying the\ntransport and escape rate. We present a theory for computing these intersection\npoints and the area bounded between the segments of these curves based on a\nclassification of the intersection points using equivalence class. We also\npresent an alternate theory for curves with nontransverse intersections and a\nmethod to increase the density of points on the curves for locating the\nintersection points accurately.The numerical implementation of the theory\npresented herein is available as an open source software called Lober. We used\nthis package to demonstrate the application of the theory to lobe dynamics that\narises in fluid mechanics, and rate of escape from a potential well that arises\nin ship dynamics.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 15:00:32 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Naik", "Shibabrat", ""], ["Lekien", "Francois", ""], ["Ross", "Shane D.", ""]]}, {"id": "1711.01521", "submitter": "Jing Qin", "authors": "Jing Qin, Shuang Li, Deanna Needell, Anna Ma, Rachel Grotheer, Chenxi\n  Huang, Natalie Durgin", "title": "Stochastic Greedy Algorithms For Multiple Measurement Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation of a single measurement vector (SMV) has been explored\nin a variety of compressive sensing applications. Recently, SMV models have\nbeen extended to solve multiple measurement vectors (MMV) problems, where the\nunderlying signal is assumed to have joint sparse structures. To circumvent the\nNP-hardness of the $\\ell_0$ minimization problem, many deterministic MMV\nalgorithms solve the convex relaxed models with limited efficiency. In this\npaper, we develop stochastic greedy algorithms for solving the joint sparse MMV\nreconstruction problem. In particular, we propose the MMV Stochastic Iterative\nHard Thresholding (MStoIHT) and MMV Stochastic Gradient Matching Pursuit\n(MStoGradMP) algorithms, and we also utilize the mini-batching technique to\nfurther improve their performance. Convergence analysis indicates that the\nproposed algorithms are able to converge faster than their SMV counterparts,\ni.e., concatenated StoIHT and StoGradMP, under certain conditions. Numerical\nexperiments have illustrated the superior effectiveness of the proposed\nalgorithms over their SMV counterparts.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 02:41:00 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 13:15:08 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Qin", "Jing", ""], ["Li", "Shuang", ""], ["Needell", "Deanna", ""], ["Ma", "Anna", ""], ["Grotheer", "Rachel", ""], ["Huang", "Chenxi", ""], ["Durgin", "Natalie", ""]]}, {"id": "1711.01851", "submitter": "Nicolas Papadakis", "authors": "Alexis Thibault and L\\'ena\\\"ic Chizat and Charles Dossal and Nicolas\n  Papadakis", "title": "Overrelaxed Sinkhorn-Knopp Algorithm for Regularized Optimal Transport", "comments": "NIPS'17 Workshop on Optimal Transport & Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a set of methods for quickly computing the solution to\nthe regularized optimal transport problem. It generalizes and improves upon the\nwidely-used iterative Bregman projections algorithm (or Sinkhorn--Knopp\nalgorithm). We first propose to rely on regularized nonlinear acceleration\nschemes. In practice, such approaches lead to fast algorithms, but their global\nconvergence is not ensured. Hence, we next propose a new algorithm with\nconvergence guarantees. The idea is to overrelax the Bregman projection\noperators, allowing for faster convergence. We propose a simple method for\nestablishing global convergence by ensuring the decrease of a Lyapunov function\nat each step. An adaptive choice of overrelaxation parameter based on the\nLyapunov function is constructed. We also suggest a heuristic to choose a\nsuitable asymptotic overrelaxation parameter, based on a local convergence\nanalysis. Our numerical experiments show a gain in convergence speed by an\norder of magnitude in certain regimes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 12:16:32 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 20:57:31 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Thibault", "Alexis", ""], ["Chizat", "L\u00e9na\u00efc", ""], ["Dossal", "Charles", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1711.01996", "submitter": "Brendan Keith", "authors": "Brendan Keith, Ali Vaziri Astaneh, Leszek Demkowicz", "title": "Goal-oriented adaptive mesh refinement for non-symmetric functional\n  settings", "comments": "32 pages", "journal-ref": "SIAM J. Numer. Anal. 57(4):1649-1676 (2019)", "doi": "10.1137/18M1181754", "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a new unified duality theory is developed for\nPetrov-Galerkin finite element methods. This novel theory is then used to\nmotivate goal-oriented adaptive mesh refinement strategies for use with\ndiscontinuous Petrov-Galerkin (DPG) methods. The focus of this article is\nmainly on broken ultraweak variational formulations of stationary boundary\nvalue problems, however, many of the ideas presented within are general enough\nthat they be extended to any such well-posed variational formulation. The\nproposed goal-oriented adaptive mesh refinement procedures require the\nconstruction of refinement indicators for both a primal problem and a dual\nproblem. In the DPG context, the primal problem is simply the system of linear\nequations coming from a standard DPG method and the dual problem is a similar\nsystem of equations, coming from a new method which is dual to DPG. This new\nmethod has the same coefficient matrix as the associated DPG method but has a\ndifferent load. We refer to this new finite element method as a DPG* method. A\nthorough analysis of DPG* methods, as stand-alone finite element methods, is\nnot given here but will be provided in subsequent articles. For DPG methods,\nthe current theory of a posteriori error estimation is reviewed and the\nreliability estimate in [13, Theorem 2.1] is improved on. For DPG* methods,\nthree different classes of refinement indicators are derived and several\ncontributions are made towards rigorous a posteriori error estimation. At the\nclosure of the article, results of numerical experiments with Poisson's\nboundary value problem in a three-dimensional domain are provided. These\nresults clearly demonstrate the utility of the goal-oriented adaptive mesh\nrefinement strategies for quantities of interest with either interior or\nboundary terms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 16:32:16 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 16:46:14 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 15:45:08 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Keith", "Brendan", ""], ["Vaziri Astaneh", "Ali", ""], ["Demkowicz", "Leszek", ""]]}, {"id": "1711.02213", "submitter": "Xin Wang", "authors": "Urs K\\\"oster, Tristan J. Webb, Xin Wang, Marcel Nassar, Arjun K.\n  Bansal, William H. Constable, O\\u{g}uz H. Elibol, Scott Gray, Stewart Hall,\n  Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J. Pai, Naveen Rao", "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep\n  Neural Networks", "comments": "14 pages, 5 figures, accepted in Neural Information Processing\n  Systems 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are commonly developed and trained in 32-bit floating\npoint format. Significant gains in performance and energy efficiency could be\nrealized by training and inference in numerical formats optimized for deep\nlearning. Despite advances in limited precision inference in recent years,\ntraining of neural networks in low bit-width remains a challenging problem.\nHere we present the Flexpoint data format, aiming at a complete replacement of\n32-bit floating point format training and inference, designed to support modern\ndeep network topologies without modifications. Flexpoint tensors have a shared\nexponent that is dynamically adjusted to minimize overflows and maximize\navailable dynamic range. We validate Flexpoint by training AlexNet, a deep\nresidual network and a generative adversarial network, using a simulator\nimplemented with the neon deep learning framework. We demonstrate that 16-bit\nFlexpoint closely matches 32-bit floating point in training all three models,\nwithout any need for tuning of model hyperparameters. Our results suggest\nFlexpoint as a promising numerical format for future hardware for training and\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 23:05:10 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 17:08:50 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["K\u00f6ster", "Urs", ""], ["Webb", "Tristan J.", ""], ["Wang", "Xin", ""], ["Nassar", "Marcel", ""], ["Bansal", "Arjun K.", ""], ["Constable", "William H.", ""], ["Elibol", "O\u011fuz H.", ""], ["Gray", "Scott", ""], ["Hall", "Stewart", ""], ["Hornof", "Luke", ""], ["Khosrowshahi", "Amir", ""], ["Kloss", "Carey", ""], ["Pai", "Ruby J.", ""], ["Rao", "Naveen", ""]]}, {"id": "1711.02271", "submitter": "Longhao Yuan", "authors": "Longhao Yuan, Qibin Zhao and Jianting Cao", "title": "High-order Tensor Completion for Data Recovery via Sparse Tensor-train\n  Optimization", "comments": "5 pages (include 1 page of reference) ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at the problem of tensor data completion. Tensor-train\ndecomposition is adopted because of its powerful representation ability and\nlinear scalability to tensor order. We propose an algorithm named Sparse\nTensor-train Optimization (STTO) which considers incomplete data as sparse\ntensor and uses first-order optimization method to find the factors of\ntensor-train decomposition. Our algorithm is shown to perform well in\nsimulation experiments at both low-order cases and high-order cases. We also\nemploy a tensorization method to transform data to a higher-order form to\nenhance the performance of our algorithm. The results of image recovery\nexperiments in various cases manifest that our method outperforms other\ncompletion algorithms. Especially when the missing rate is very high, e.g.,\n90\\% to 99\\%, our method is significantly better than the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 03:25:49 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 02:16:24 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Yuan", "Longhao", ""], ["Zhao", "Qibin", ""], ["Cao", "Jianting", ""]]}, {"id": "1711.02473", "submitter": "Mikl\\'os Homolya", "authors": "Mikl\\'os Homolya, Robert C. Kirby, David A. Ham", "title": "Exposing and exploiting structure: optimal code generation for\n  high-order finite element methods", "comments": "Submitted to ACM Transactions on Mathematical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code generation based software platforms, such as Firedrake, have become\npopular tools for developing complicated finite element discretisations of\npartial differential equations. We extended the code generation infrastructure\nin Firedrake with optimisations that can exploit the structure inherent to some\nfinite elements. This includes sum factorisation on cuboid cells for\ncontinuous, discontinuous, H(div) and H(curl) conforming elements. Our\nexperiments confirm optimal algorithmic complexity for high-order finite\nelement assembly. This is achieved through several novel contributions: the\nintroduction of a more powerful interface between the form compiler and the\nlibrary providing the finite elements; a more abstract, smarter library of\nfinite elements called FInAT that explicitly communicates the structure of\nelements; and form compiler algorithms to automatically exploit this exposed\nstructure.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 14:18:07 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Homolya", "Mikl\u00f3s", ""], ["Kirby", "Robert C.", ""], ["Ham", "David A.", ""]]}, {"id": "1711.02574", "submitter": "Andreas Van Barel", "authors": "Andreas Van Barel, Stefan Vandewalle", "title": "Robust Optimization of PDEs with Random Coefficients Using a Multilevel\n  Monte Carlo Method", "comments": "This work was presented at the IMG 2016 conference (Dec 5 - Dec 9,\n  2016), at the Copper Mountain conference (Mar 26 - Mar 30, 2017), and at the\n  FrontUQ conference (Sept 5 - Sept 8, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses optimization problems constrained by partial\ndifferential equations with uncertain coefficients. In particular, the robust\ncontrol problem and the average control problem are considered for a tracking\ntype cost functional with an additional penalty on the variance of the state.\nThe expressions for the gradient and Hessian corresponding to either problem\ncontain expected value operators. Due to the large number of uncertainties\nconsidered in our model, we suggest to evaluate these expectations using a\nmultilevel Monte Carlo (MLMC) method. Under mild assumptions, it is shown that\nthis results in the gradient and Hessian corresponding to the MLMC estimator of\nthe original cost functional. Furthermore, we show that the use of certain\ncorrelated samples yields a reduction in the total number of samples required.\nTwo optimization methods are investigated: the nonlinear conjugate gradient\nmethod and the Newton method. For both, a specific algorithm is provided that\ndynamically decides which and how many samples should be taken in each\niteration. The cost of the optimization up to some specified tolerance $\\tau$\nis shown to be proportional to the cost of a gradient evaluation with requested\nroot mean square error $\\tau$. The algorithms are tested on a model elliptic\ndiffusion problem with lognormal diffusion coefficient. An additional nonlinear\nterm is also considered.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 15:57:30 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Van Barel", "Andreas", ""], ["Vandewalle", "Stefan", ""]]}, {"id": "1711.03141", "submitter": "Fernando Gaspoz", "authors": "Martin Alk\\\"amper, Fernando Gaspoz, Robert Kl\\\"ofkorn", "title": "A Weak Compatibility Condition for Newest Vertex Bisection in any\n  dimension", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a weak compatibility condition for the Newest Vertex Bisection\nalgorithm on simplex grids of any dimension and show that using this condition\nthe iterative algorithm terminates successfully. Additionally we provide an\nO(n) algorithm that renumbers any simplex grid to fulfil this condition.\nFurthermore we conduct experiments to estimate the distance to the standard\ncompatibility and also the geometric quality of the produced meshes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:09:42 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Alk\u00e4mper", "Martin", ""], ["Gaspoz", "Fernando", ""], ["Kl\u00f6fkorn", "Robert", ""]]}, {"id": "1711.03232", "submitter": "Eric Mason", "authors": "Eric Mason, Birsen Yazici", "title": "Performance Analysis of Convex LRMR based Passive SAR Imaging", "comments": "Submitted to IEEE Transactions on Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passive synthetic aperture radar (SAR) uses existing signals of opportunity\nsuch as communication and broadcasting signals. In our prior work, we have\ndeveloped a low-rank matrix recovery (LRMR) method that can reconstruct scenes\nwith extended and densely distributed point targets, overcoming shortcomings of\nconventional methods. The approach is based on correlating two sets of bistatic\nmeasurements, which results in a linear mapping of the tensor product of the\nscene reflectivity with itself. Recognizing this tensor product as a rank-one\npositive semi-definite (PSD) operator, we pose passive SAR image reconstruction\nas a LRMR problem with convex relaxation. In this paper, we present a\nperformance analysis of the convex LRMR-based passive SAR image reconstruction\nmethod. We use the restricted isometry property (RIP) and show that exact\nreconstruction is guaranteed under the condition that the pixel spacing or\nresolution satisfies a certain lower bound. We show that for sufficiently large\ncenter frequencies, our method provides superior resolution than that of\nFourier based methods, making it a super-resolution technique. Additionally, we\nshow that phaseless imaging is a special case of our passive SAR imaging\nmethod. We present extensive numerical simulation to validate our analysis.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 02:21:33 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Mason", "Eric", ""], ["Yazici", "Birsen", ""]]}, {"id": "1711.03420", "submitter": "Pierre Lairez", "authors": "Pierre Lairez", "title": "Rigid continuation paths I. Quasilinear average complexity for solving\n  polynomial systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many operations do we need on the average to compute an approximate root\nof a random Gaussian polynomial system? Beyond Smale's 17th problem that asked\nwhether a polynomial bound is possible, we prove a quasi-optimal bound\n$\\text{(input size)}^{1+o(1)}$. This improves upon the previously known\n$\\text{(input size)}^{\\frac32 +o(1)}$ bound.\n  The new algorithm relies on numerical continuation along \\emph{rigid\ncontinuation paths}. The central idea is to consider rigid motions of the\nequations rather than line segments in the linear space of all polynomial\nsystems. This leads to a better average condition number and allows for bigger\nsteps. We show that on the average, we can compute one approximate root of a\nrandom Gaussian polynomial system of~$n$ equations of degree at most $D$ in\n$n+1$ homogeneous variables with $O(n^5 D^2)$ continuation steps. This is a\ndecisive improvement over previous bounds that prove no better than\n$\\sqrt{2}^{\\min(n, D)}$ continuation steps on the average.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:23:28 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 09:18:54 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Lairez", "Pierre", ""]]}, {"id": "1711.03590", "submitter": "Martin Kronbichler", "authors": "Martin Kronbichler and Katharina Kormann", "title": "Fast matrix-free evaluation of discontinuous Galerkin finite element\n  operators", "comments": null, "journal-ref": "ACM Transactions on Mathematical Software 45(3), 29/1-29/40, 2019", "doi": "10.1145/3325864", "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithmic framework for matrix-free evaluation of\ndiscontinuous Galerkin finite element operators based on sum factorization on\nquadrilateral and hexahedral meshes. We identify a set of kernels for fast\nquadrature on cells and faces targeting a wide class of weak forms originating\nfrom linear and nonlinear partial differential equations. Different algorithms\nand data structures for the implementation of operator evaluation are compared\nin an in-depth performance analysis. The sum factorization kernels are\noptimized by vectorization over several cells and faces and an even-odd\ndecomposition of the one-dimensional compute kernels. In isolation our\nimplementation then reaches up to 60\\% of arithmetic peak on Intel Haswell and\nBroadwell processors and up to 50\\% of arithmetic peak on Intel Knights\nLanding. The full operator evaluation reaches only about half that throughput\ndue to memory bandwidth limitations from loading the input and output vectors,\nMPI ghost exchange, as well as handling variable coefficients and the geometry.\nOur performance analysis shows that the results are often within 10\\% of the\navailable memory bandwidth for the proposed implementation, with the exception\nof the Cartesian mesh case where the cost of gather operations and MPI\ncommunication are more substantial.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 20:36:06 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Kronbichler", "Martin", ""], ["Kormann", "Katharina", ""]]}, {"id": "1711.04333", "submitter": "David Bolin", "authors": "David Bolin and Kristin Kirchner", "title": "The rational SPDE approach for Gaussian random fields with general\n  smoothness", "comments": "28 pages, 4 figures", "journal-ref": "J. Comput. Graph. Statist. (2019)", "doi": "10.1080/10618600.2019.1665537", "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach for modeling and inference in spatial statistics is to\nrepresent Gaussian random fields as solutions to stochastic partial\ndifferential equations (SPDEs) of the form $L^{\\beta}u = \\mathcal{W}$, where\n$\\mathcal{W}$ is Gaussian white noise, $L$ is a second-order differential\noperator, and $\\beta>0$ is a parameter that determines the smoothness of $u$.\nHowever, this approach has been limited to the case $2\\beta\\in\\mathbb{N}$,\nwhich excludes several important models and makes it necessary to keep $\\beta$\nfixed during inference.\n  We propose a new method, the rational SPDE approach, which in spatial\ndimension $d\\in\\mathbb{N}$ is applicable for any $\\beta>d/4$, and thus remedies\nthe mentioned limitation. The presented scheme combines a finite element\ndiscretization with a rational approximation of the function $x^{-\\beta}$ to\napproximate $u$. For the resulting approximation, an explicit rate of\nconvergence to $u$ in mean-square sense is derived. Furthermore, we show that\nour method has the same computational benefits as in the restricted case\n$2\\beta\\in\\mathbb{N}$. Several numerical experiments and a statistical\napplication are used to illustrate the accuracy of the method, and to show that\nit facilitates likelihood-based inference for all model parameters including\n$\\beta$.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 18:15:20 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 10:38:11 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 16:45:57 GMT"}, {"version": "v4", "created": "Sun, 1 Dec 2019 14:20:28 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bolin", "David", ""], ["Kirchner", "Kristin", ""]]}, {"id": "1711.04471", "submitter": "Wim Vanderbauwhede", "authors": "Wim Vanderbauwhede, Gavin Davidson", "title": "Domain-Specific Acceleration and Auto-Parallelization of Legacy\n  Scientific Code in FORTRAN 77 using Source-to-Source Compilation", "comments": "12 pages, 5 figures, submitted to \"Computers and Fluids\" as full\n  paper from ParCFD conference entry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively parallel accelerators such as GPGPUs, manycores and FPGAs represent\na powerful and affordable tool for scientists who look to speed up simulations\nof complex systems. However, porting code to such devices requires a detailed\nunderstanding of heterogeneous programming tools and effective strategies for\nparallelization. In this paper we present a source to source compilation\napproach with whole-program analysis to automatically transform single-threaded\nFORTRAN 77 legacy code into OpenCL-accelerated programs with parallelized\nkernels.\n  The main contributions of our work are: (1) whole-source refactoring to allow\nany subroutine in the code to be offloaded to an accelerator. (2) Minimization\nof the data transfer between the host and the accelerator by eliminating\nredundant transfers. (3) Pragmatic auto-parallelization of the code to be\noffloaded to the accelerator by identification of parallelizable maps and\nreductions.\n  We have validated the code transformation performance of the compiler on the\nNIST FORTRAN 78 test suite and several real-world codes: the Large Eddy\nSimulator for Urban Flows, a high-resolution turbulent flow model; the shallow\nwater component of the ocean model Gmodel; the Linear Baroclinic Model, an\natmospheric climate model and Flexpart-WRF, a particle dispersion simulator.\n  The automatic parallelization component has been tested on as 2-D Shallow\nWater model (2DSW) and on the Large Eddy Simulator for Urban Flows (UFLES) and\nproduces a complete OpenCL-enabled code base. The fully OpenCL-accelerated\nversions of the 2DSW and the UFLES are resp. 9x and 20x faster on GPU than the\noriginal code on CPU, in both cases this is the same performance as manually\nported code.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 08:56:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Vanderbauwhede", "Wim", ""], ["Davidson", "Gavin", ""]]}, {"id": "1711.05135", "submitter": "Zhuo Feng", "authors": "Zhuo Feng", "title": "Similarity-Aware Spectral Sparsification by Edge Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, spectral graph sparsification techniques that can compute\nultra-sparse graph proxies have been extensively studied for accelerating\nvarious numerical and graph-related applications. Prior nearly-linear-time\nspectral sparsification methods first extract low-stretch spanning tree from\nthe original graph to form the backbone of the sparsifier, and then recover\nsmall portions of spectrally-critical off-tree edges to the spanning tree to\nsignificantly improve the approximation quality. However, it is not clear how\nmany off-tree edges should be recovered for achieving a desired spectral\nsimilarity level within the sparsifier. Motivated by recent graph signal\nprocessing techniques, this paper proposes a similarity-aware spectral graph\nsparsification framework that leverages efficient spectral off-tree edge\nembedding and filtering schemes to construct spectral sparsifiers with\nguaranteed spectral similarity (relative condition number) level. An iterative\ngraph densification scheme is introduced to facilitate efficient and effective\nfiltering of off-tree edges for highly ill-conditioned problems. The proposed\nmethod has been validated using various kinds of graphs obtained from public\ndomain sparse matrix collections relevant to VLSI CAD, finite element analysis,\nas well as social and data networks frequently studied in many machine learning\nand data mining applications.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:58:41 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 06:26:55 GMT"}, {"version": "v3", "created": "Sat, 7 Apr 2018 23:37:42 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Feng", "Zhuo", ""]]}, {"id": "1711.05354", "submitter": "William Leeb", "authors": "William Leeb and Vladimir Rokhlin", "title": "On the Numerical Solution of Fourth-Order Linear Two-Point Boundary\n  Value Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fast and numerically stable algorithm for the\nsolution of fourth-order linear boundary value problems on an interval. This\ntype of equation arises in a variety of settings in physics and signal\nprocessing. Our method reformulates the equation as a collection of second-kind\nintegral equations defined on local subdomains. Each such equation can be\nstably discretized and solved. The boundary values of these local solutions are\nmatched by solving a banded linear system. The method of deferred corrections\nis then used to increase the accuracy of the scheme. Deferred corrections\nrequires applying the integral operator to a function on the entire domain, for\nwhich we provide an algorithm with linear cost. We illustrate the performance\nof our method on several numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 23:15:28 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 02:03:29 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 20:12:46 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Leeb", "William", ""], ["Rokhlin", "Vladimir", ""]]}, {"id": "1711.05895", "submitter": "Jie Chen", "authors": "Jie Chen and Michael L. Stein", "title": "Linear-Cost Covariance Functions for Gaussian Random Fields", "comments": "Code is available at https://github.com/jiechenjiechen/RLCM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random fields (GRF) are a fundamental stochastic model for\nspatiotemporal data analysis. An essential ingredient of GRF is the covariance\nfunction that characterizes the joint Gaussian distribution of the field.\nCommonly used covariance functions give rise to fully dense and unstructured\ncovariance matrices, for which required calculations are notoriously expensive\nto carry out for large data. In this work, we propose a construction of\ncovariance functions that result in matrices with a hierarchical structure.\nEmpowered by matrix algorithms that scale linearly with the matrix dimension,\nthe hierarchical structure is proved to be efficient for a variety of random\nfield computations, including sampling, kriging, and likelihood evaluation.\nSpecifically, with $n$ scattered sites, sampling and likelihood evaluation has\nan $O(n)$ cost and kriging has an $O(\\log n)$ cost after preprocessing,\nparticularly favorable for the kriging of an extremely large number of sites\n(e.g., predicting on more sites than observed). We demonstrate comprehensive\nnumerical experiments to show the use of the constructed covariance functions\nand their appealing computation time. Numerical examples on a laptop include\nsimulated data of size up to one million, as well as a climate data product\nwith over two million observations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 02:20:47 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 03:52:18 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 20:09:22 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chen", "Jie", ""], ["Stein", "Michael L.", ""]]}, {"id": "1711.06352", "submitter": "Saeid Karimi", "authors": "Saeid Karimi", "title": "Numerical time integration of lumped parameter systems governed by\n  implicit constitutive relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-integration for lumped parameter systems obeying implicit Bingham-Kelvin\nconstitutive models is studied. The governing system of equations describing\nthe lumped parameter system is a non-linear differential-algebraic equation and\nneeds to be solved numerically. The response of this system is non-smooth and\nthe kinematic variables can not be written in terms of the dynamic variables,\nexplicitly. To gain insight into numerical time-integration of this system, a\nnew time-integration scheme based on the trapezoidal method is derived. This\nmethod relies on two independent parameters to adjust for damping and is\nstable. Numerical examples showcase the performance of the proposed\ntime-integration method and compare it to a benchmark algorithm. Under this\nscheme, implicit-explicit integration of the governing equations is possible.\nUsing this new method, the limitations of the trapezoidal time-integration\nmethods when applied to a non-smooth differential-algebraic equation are\nhighlighted.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 23:30:01 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Karimi", "Saeid", ""]]}, {"id": "1711.06450", "submitter": "HongGuang Sun", "authors": "XiaoTing Liu, HongGuang Sun, Yong Zhang, Zhuojia Fu", "title": "A scale-dependent finite difference method for time fractional\n  derivative relaxation type equations", "comments": "26 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fractional derivative relaxation type equations (FREs) including fractional\ndiffusion equation and fractional relaxation equation, have been widely used to\ndescribe anomalous phenomena in physics. To utilize the characteristics of\nfractional dynamic systems, this paper proposes a scale-dependent finite\ndifference method (S-FDM) in which the non-uniform mesh depends on the time\nfractional derivative order of FRE. The purpose is to establish a stable\nnumerical method with low computation cost for FREs by making a bridge between\nthe fractional derivative order and space-time discretization steps. The\nproposed method is proved to be unconditional stable with (2-{\\alpha})-th\nconvergence rate. Moreover, three examples are carried out to make a comparison\namong the uniform difference method, common non-uniform method and S-FDM in\nterm of accuracy, convergence rate and computational costs. It has been\nconfirmed that the S-FDM method owns obvious advantages in computational\nefficiency compared with uniform mesh method, especially for long-time range\ncomputation (e.g. the CPU time of S-FDM is ~1/400 of uniform mesh method with\nbetter relative error for time T=500 and fractional derivative order\nalpha=0.4).\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 08:16:44 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Liu", "XiaoTing", ""], ["Sun", "HongGuang", ""], ["Zhang", "Yong", ""], ["Fu", "Zhuojia", ""]]}, {"id": "1711.06633", "submitter": "Angxiu Ni", "authors": "Angxiu Ni, Qiqi Wang, Pablo Fernandez, Chaitanya Talnikar", "title": "Sensitivity analysis on chaotic dynamical systems by Finite Difference\n  Non-Intrusive Least Squares Shadowing (FD-NILSS)", "comments": "20 pages, 8 figures", "journal-ref": "Journal of Computational Physics, Volume 394, Pages 615-631, 2019", "doi": "10.1016/j.jcp.2019.06.004", "report-no": null, "categories": "physics.comp-ph cs.NA math.NA nlin.CD physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Finite Difference Non-Intrusive Least Squares Shadowing\n(FD-NILSS) algorithm for computing sensitivities of long-time averaged\nquantities in chaotic dynamical systems. FD-NILSS does not require tangent\nsolvers, and can be implemented with little modification to existing numerical\nsimulation software. We also give a formula for solving the least-squares\nproblem in FD-NILSS, which can be applied in NILSS as well. Finally, we apply\nFD-NILSS for sensitivity analysis of a chaotic flow over a 3-D cylinder at\nReynolds number 525, where FD-NILSS computes accurate sensitivities and the\ncomputational cost is in the same order as the numerical simulation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 17:17:10 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 02:58:54 GMT"}, {"version": "v3", "created": "Sun, 6 May 2018 23:58:04 GMT"}, {"version": "v4", "created": "Mon, 23 Jul 2018 04:11:18 GMT"}, {"version": "v5", "created": "Tue, 22 Jan 2019 05:06:04 GMT"}, {"version": "v6", "created": "Sun, 23 Jun 2019 17:33:43 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ni", "Angxiu", ""], ["Wang", "Qiqi", ""], ["Fernandez", "Pablo", ""], ["Talnikar", "Chaitanya", ""]]}, {"id": "1711.06656", "submitter": "Palma London", "authors": "Palma London, Shai Vardi, Adam Wierman, Hanling Yi", "title": "A Parallelizable Acceleration Framework for Packing Linear Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an acceleration framework for packing linear programming\nproblems where the amount of data available is limited, i.e., where the number\nof constraints m is small compared to the variable dimension n. The framework\ncan be used as a black box to speed up linear programming solvers dramatically,\nby two orders of magnitude in our experiments. We present worst-case guarantees\non the quality of the solution and the speedup provided by the algorithm,\nshowing that the framework provides an approximately optimal solution while\nrunning the original solver on a much smaller problem. The framework can be\nused to accelerate exact solvers, approximate solvers, and parallel/distributed\nsolvers. Further, it can be used for both linear programs and integer linear\nprograms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:17:01 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["London", "Palma", ""], ["Vardi", "Shai", ""], ["Wierman", "Adam", ""], ["Yi", "Hanling", ""]]}, {"id": "1711.06785", "submitter": "Ming Yan", "authors": "Zhi Li and Ming Yan", "title": "New convergence analysis of a primal-dual algorithm with large stepsizes", "comments": null, "journal-ref": "Advances in Computational Mathematics 47, 9 (2021)", "doi": "10.1007/s10444-020-09840-9", "report-no": null, "categories": "math.OC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a primal-dual algorithm for minimizing $f(x)+h\\square l(Ax)$ with\nFr\\'echet differentiable $f$ and $l^*$. This primal-dual algorithm has two\nnames in literature: Primal-Dual Fixed-Point algorithm based on the Proximity\nOperator (PDFP$^2$O) and Proximal Alternating Predictor-Corrector (PAPC). In\nthis paper, we prove its convergence under a weaker condition on the stepsizes\nthan existing ones. With additional assumptions, we show its linear\nconvergence. In addition, we show that this condition (the upper bound of the\nstepsize) is tight and can not be weakened. This result also recovers a\nrecently proposed positive-indefinite linearized augmented Lagrangian method.\nIn addition, we apply this result to a decentralized consensus algorithm\nPG-EXTRA and derive the weakest convergence condition.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 01:31:05 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 03:20:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Li", "Zhi", ""], ["Yan", "Ming", ""]]}, {"id": "1711.07031", "submitter": "Petr N. Vabishchevich", "authors": "Petr N. Vabishchevich", "title": "Two-level schemes for the advection equation", "comments": "28 pages, 7 figures, 3 tables", "journal-ref": null, "doi": "10.1016/j.jcp.2018.02.044", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advection equation is the basis for mathematical models of continuum\nmechanics. In the approximate solution of nonstationary problems it is\nnecessary to inherit main properties of the conservatism and monotonicity of\nthe solution. In this paper, the advection equation is written in the symmetric\nform, where the advection operator is the half-sum of advection operators in\nconservative (divergent) and non-conservative (characteristic) forms. The\nadvection operator is skew-symmetric. Standard finite element approximations in\nspace are used. The standart explicit two-level scheme for the advection\nequation is absolutly unstable. New conditionally stable regularized schemes\nare constructed, on the basis of the general theory of stability\n(well-posedness) of operator-difference schemes, the stability conditions of\nthe explicit Lax-Wendroff scheme are established. Unconditionally stable and\nconservative schemes are implicit schemes of the second (Crank-Nicolson scheme)\nand fourth order. The conditionally stable implicit Lax-Wendroff scheme is\nconstructed. The accuracy of the investigated explicit and implicit two-level\nschemes for an approximate solution of the advection equation is illustrated by\nthe numerical results of a model two-dimensional problem.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 15:34:28 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Vabishchevich", "Petr N.", ""]]}, {"id": "1711.07038", "submitter": "Ganzhao Yuan", "authors": "Ganzhao Yuan, Haoxian Tan, Wei-Shi Zheng", "title": "A Coordinate-wise Optimization Algorithm for Sparse Inverse Covariance\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse inverse covariance selection is a fundamental problem for analyzing\ndependencies in high dimensional data. However, such a problem is difficult to\nsolve since it is NP-hard. Existing solutions are primarily based on convex\napproximation and iterative hard thresholding, which only lead to sub-optimal\nsolutions. In this work, we propose a coordinate-wise optimization algorithm to\nsolve this problem which is guaranteed to converge to a coordinate-wise minimum\npoint. The algorithm iteratively and greedily selects one variable or swaps two\nvariables to identify the support set, and then solves a reduced convex\noptimization problem over the support set to achieve the greatest descent. As a\nside contribution of this paper, we propose a Newton-like algorithm to solve\nthe reduced convex sub-problem, which is proven to always converge to the\noptimal solution with global linear convergence rate and local quadratic\nconvergence rate. Finally, we demonstrate the efficacy of our method on\nsynthetic data and real-world data sets. As a result, the proposed method\nconsistently outperforms existing solutions in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 16:04:51 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 07:14:58 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Yuan", "Ganzhao", ""], ["Tan", "Haoxian", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1711.08448", "submitter": "Francesco Tudisco", "authors": "Francesco Tudisco, Francesca Arrigo, Antoine Gautier", "title": "Node and layer eigenvector centralities for multiplex networks", "comments": "author's accepted version", "journal-ref": null, "doi": "10.1137/17M1137668", "report-no": null, "categories": "cs.SI cs.NA math.NA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigenvector-based centrality measures are among the most popular centrality\nmeasures in network science. The underlying idea is intuitive and the\nmathematical description is extremely simple in the framework of standard,\nmono-layer networks. Moreover, several efficient computational tools are\navailable for their computation. Moving up in dimensionality, several efforts\nhave been made in the past to describe an eigenvector-based centrality measure\nthat generalizes Bonacich index to the case of multiplex networks. In this\nwork, we propose a new definition of eigenvector centrality that relies on the\nPerron eigenvector of a multi-homogeneous map defined in terms of the tensor\ndescribing the network. We prove that existence and uniqueness of such\ncentrality are guaranteed under very mild assumptions on the multiplex network.\nExtensive numerical studies are proposed to test the newly introduced\ncentrality measure and to compare it to other existing eigenvector-based\ncentralities.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:51:16 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 13:50:09 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Tudisco", "Francesco", ""], ["Arrigo", "Francesca", ""], ["Gautier", "Antoine", ""]]}, {"id": "1711.08453", "submitter": "Samuel Potter", "authors": "Samuel F. Potter, Ramani Duraiswami", "title": "Fast and Stable Pascal Matrix Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive a family of fast and stable algorithms for\nmultiplying and inverting $n \\times n$ Pascal matrices that run in $O(n log^2\nn)$ time and are closely related to De Casteljau's algorithm for B\\'ezier curve\nevaluation. These algorithms use a recursive factorization of the triangular\nPascal matrices and improve upon the cripplingly unstable $O(n log n)$ fast\nFourier transform-based algorithms which involve a Toeplitz matrix\nfactorization. We conduct numerical experiments which establish the speed and\nstability of our algorithm, as well as the poor performance of the Toeplitz\nfactorization algorithm. As an example, we show how our formulation relates to\nB\\'ezier curve evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:55:48 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Potter", "Samuel F.", ""], ["Duraiswami", "Ramani", ""]]}, {"id": "1711.08732", "submitter": "Milan Hlad\\'ik", "authors": "Milan Hlad\\'ik", "title": "An Overview of Polynomially Computable Characteristics of Special\n  Interval Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that many problems in interval computation are intractable,\nwhich restricts our attempts to solve large problems in reasonable time. This\ndoes not mean, however, that all problems are computationally hard. Identifying\npolynomially solvable classes thus belongs to important current trends. The\npurpose of this paper is to review some of such classes. In particular, we\nfocus on several special interval matrices and investigate their convenient\nproperties. We consider tridiagonal matrices, {M,H,P,B}-matrices, inverse\nM-matrices, inverse nonnegative matrices, nonnegative matrices, totally\npositive matrices and some others. We focus in particular on computing the\nrange of the determinant, eigenvalues, singular values, and selected norms.\nWhenever possible, we state also formulae for determining the inverse matrix\nand the hull of the solution set of an interval system of linear equations. We\nsurvey not only the known facts, but we present some new views as well.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 15:10:49 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Hlad\u00edk", "Milan", ""]]}, {"id": "1711.09867", "submitter": "Anthony Yezzi", "authors": "Anthony Yezzi and Ganesh Sundaramoorthi", "title": "Accelerated Optimization in the PDE Framework: Formulations for the\n  Active Contour Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the seminal work of Nesterov, accelerated optimization methods have\nbeen used to powerfully boost the performance of first-order, gradient-based\nparameter estimation in scenarios where second-order optimization strategies\nare either inapplicable or impractical. Not only does accelerated gradient\ndescent converge considerably faster than traditional gradient descent, but it\nalso performs a more robust local search of the parameter space by initially\novershooting and then oscillating back as it settles into a final\nconfiguration, thereby selecting only local minimizers with a basis of\nattraction large enough to contain the initial overshoot. This behavior has\nmade accelerated and stochastic gradient search methods particularly popular\nwithin the machine learning community. In their recent PNAS 2016 paper,\nWibisono, Wilson, and Jordan demonstrate how a broad class of accelerated\nschemes can be cast in a variational framework formulated around the Bregman\ndivergence, leading to continuum limit ODE's. We show how their formulation may\nbe further extended to infinite dimension manifolds (starting here with the\ngeometric space of curves and surfaces) by substituting the Bregman divergence\nwith inner products on the tangent space and explicitly introducing a\ndistributed mass model which evolves in conjunction with the object of interest\nduring the optimization process. The co-evolving mass model, which is\nintroduced purely for the sake of endowing the optimization with helpful\ndynamics, also links the resulting class of accelerated PDE based optimization\nschemes to fluid dynamical formulations of optimal mass transport.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 18:27:24 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Yezzi", "Anthony", ""], ["Sundaramoorthi", "Ganesh", ""]]}, {"id": "1711.10014", "submitter": "Greg Roddick", "authors": "Greg Roddick", "title": "Computation of Scattering Matrices and their Derivatives for Waveguides", "comments": "Third Revision 28th November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the calculation of the stationary scattering matrix and\nits derivatives for Euclidean waveguides. This is an adaptation and extension\nto a procedure developed by Levitin and Strohmaier which was used to compute\nthe stationary scattering matrix \\cite{alexnew}. On Euclidean waveguides, the\nscattering matrix can be meromorphically continued from the complex plane to a\nRiemann surface with a countably infinite number of sheets. We describe in\ndetail how we have dealt with this. In addition, our algorithm is also able to\ncalculate arbitrarily high derivatives. In the final section, we will present\nthe results of some numerical calculations obtained using this method.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 21:52:08 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 22:15:39 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Roddick", "Greg", ""]]}, {"id": "1711.10128", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Fei Xue, Andreas Stathopoulos", "title": "TRPL+K: Thick-Restart Preconditioned Lanczos+K Method for Large\n  Symmetric Eigenvalue Problems", "comments": "27 pages, 6 figures, 7 tables. Submitted to SIAM Journal on\n  Scientific Computing, Minor Revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lanczos method is one of the standard approaches for computing a few\neigenpairs of a large, sparse, symmetric matrix. It is typically used with\nrestarting to avoid unbounded growth of memory and computational requirements.\nThick-restart Lanczos is a popular restarted variant because of its simplicity\nand numerically robustness. However, convergence can be slow for highly\nclustered eigenvalues so more effective restarting techniques and the use of\npreconditioning is needed. In this paper, we present a thick-restart\npreconditioned Lanczos method, TRPL+K, that combines the power of locally\noptimal restarting (+K) and preconditioning techniques with the efficiency of\nthe thick-restart Lanczos method. TRPL+K employs an inner-outer scheme where\nthe inner loop applies Lanczos on a preconditioned operator while the outer\nloop augments the resulting Lanczos subspace with certain vectors from the\nprevious restart cycle to obtain eigenvector approximations with which it thick\nrestarts the outer subspace. We first identify the differences from various\nrelevant methods in the literature. Then, based on an optimization perspective,\nwe show an asymptotic global quasi-optimality of a simplified TRPL+K method\ncompared to an unrestarted global optimal method. Finally, we present extensive\nexperiments showing that TRPL+K either outperforms or matches other\nstate-of-the-art eigenmethods in both matrix-vector multiplications and\ncomputational time.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 05:33:14 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 03:36:36 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Wu", "Lingfei", ""], ["Xue", "Fei", ""], ["Stathopoulos", "Andreas", ""]]}, {"id": "1711.10561", "submitter": "Maziar Raissi", "authors": "Maziar Raissi, Paris Perdikaris, and George Em Karniadakis", "title": "Physics Informed Deep Learning (Part I): Data-driven Solutions of\n  Nonlinear Partial Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NA math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce physics informed neural networks -- neural networks that are\ntrained to solve supervised learning tasks while respecting any given law of\nphysics described by general nonlinear partial differential equations. In this\ntwo part treatise, we present our developments in the context of solving two\nmain classes of problems: data-driven solution and data-driven discovery of\npartial differential equations. Depending on the nature and arrangement of the\navailable data, we devise two distinct classes of algorithms, namely continuous\ntime and discrete time models. The resulting neural networks form a new class\nof data-efficient universal function approximators that naturally encode any\nunderlying physical laws as prior information. In this first part, we\ndemonstrate how these networks can be used to infer solutions to partial\ndifferential equations, and obtain physics-informed surrogate models that are\nfully differentiable with respect to all input coordinates and free parameters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:21:59 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1711.10600", "submitter": "Bibek Kabi", "authors": "Bibek Kabi, Anand S Sahadevan, Tapan Pradhan", "title": "An Overflow Free Fixed-point Eigenvalue Decomposition Algorithm: Case\n  Study of Dimensionality Reduction in Hyperspectral Images", "comments": "9 Pages; 2017 Conference On Design And Architectures For Signal And\n  Image Processing (DASIP), Dresden, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of enabling robust range estimation of eigenvalue\ndecomposition (EVD) algorithm for a reliable fixed-point design. The simplicity\nof fixed-point circuitry has always been so tempting to implement EVD algo-\nrithms in fixed-point arithmetic. Working towards an effective fixed-point\ndesign, integer bit-width allocation is a significant step which has a crucial\nimpact on accuracy and hardware efficiency. This paper investigates the\nshortcomings of the existing range estimation methods while deriving bounds for\nthe variables of the EVD algorithm. In light of the circumstances, we introduce\na range estimation approach based on vector and matrix norm properties together\nwith a scaling procedure that maintains all the assets of an analytical method.\nThe method could derive robust and tight bounds for the variables of EVD\nalgorithm. The bounds derived using the proposed approach remain same for any\ninput matrix and are also independent of the number of iterations or size of\nthe problem. Some benchmark hyperspectral data sets have been used to evaluate\nthe efficiency of the proposed technique. It was found that by the proposed\nrange estimation approach, all the variables generated during the computation\nof Jacobi EVD is bounded within $\\pm1$.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 22:57:53 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Kabi", "Bibek", ""], ["Sahadevan", "Anand S", ""], ["Pradhan", "Tapan", ""]]}, {"id": "1711.10724", "submitter": "Sohrab Valizadeh", "authors": "Sohrab Valizadeh, Abdollah Borhanifar", "title": "General matrix transform method for the Riesz space fractional\n  advection-dispersion equations", "comments": "21 Pages, 8 figures, this work has not been presented or published as\n  an article right now", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a mixed high order finite difference scheme-Pad\\'{e}\napproximation method is applied to obtain numerical solution of the Riesz\nfractional advection-dispersion equation. This method is based on the high\norder finite difference scheme that derived from fractional centered difference\nand Pad\\'{e} approximation method for space and time integration, respectively.\nThe stability analysis of the proposed method is discussed via theoretical\nmatrix analysis. Numerical experiments are carried out to confirm the\ntheoretical results of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 08:35:49 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 13:36:40 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Valizadeh", "Sohrab", ""], ["Borhanifar", "Abdollah", ""]]}, {"id": "1711.10803", "submitter": "Jeremy Levesley Prof", "authors": "Simon Hubbert and Jeremy Levesley", "title": "Convergence of Multilevel Stationary Gaussian Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that polynomial reproduction is not possible when\napproximating with Gaussian kernels. Quasi-interpolation schemes have been\ndeveloped which use a finite number of Gaussians at different scales, which\nthen reproduce polynomials of low degree \\cite{beatson}, and thus achieve\npolynomial orders of convergence. At the same time, interpolation with kernels\nof fixed width suffers from an explosion in condition number, and information\nfrom all data points influences the approximation at any one data point (no\nlocalisation). In \\cite{HL1} the authors show that, for periodic convolution\nwith the Gaussian kernel, a multilevel scheme can give orders of approximation\nfaster than any polynomial. In this paper we present a new multilevel\nquasi-interpolation algorithm, the discrete version of the algorithm in\n\\cite{HL1}, which mimics the continuous algorithm well, to single precision\naccuracy, and gives excellent convergence rates for band limited periodic\nfunctions. In this paper we explain how the algorithm works, and why we achieve\nthe numerical results we do. The estimates developed have two parts, one\ninvolving the convergence of a low degree polynomial truncation term and one\ninvolving the control of the remainder of the truncation as the algorithm\nproceeds.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:57:04 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 16:48:40 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Hubbert", "Simon", ""], ["Levesley", "Jeremy", ""]]}, {"id": "1711.10885", "submitter": "Steffen M\\\"uthing", "authors": "Steffen M\\\"uthing and Marian Piatkowski and Peter Bastian", "title": "High-performance Implementation of Matrix-free High-order Discontinuous\n  Galerkin Methods", "comments": "submitted to SIAM SISC on 2017-11-29", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving a substantial part of peak performance on todays and future\nhigh-performance computing systems is a major challenge for simulation codes.\nIn this paper we address this question in the context of the numerical solution\nof partial differential equations with finite element methods, in particular\nthe discontinuous Galerkin method applied to a convection-diffusion-reaction\nmodel problem. Assuming tensor product structure of basis functions and\nquadrature on cuboid meshes in a matrix-free approach a substantial reduction\nin computational complexity can be achieved for operator application compared\nto a matrix-based implementation while at the same time enabling SIMD\nvectorization and the use of fused-multiply-add. Close to 60\\% of peak\nperformance are obtained for a full operator evaluation on a Xeon Haswell CPU\nwith 16 cores and speedups of several hundred (with respect to matrix-based\ncomputation) are achieved for polynomial degree seven. Excellent weak\nscalability on a single node as well as the roofline model demonstrate that the\nalgorithm is fully compute-bound with a high flop per byte ratio. Excellent\nscalability is also demonstrated on up to 6144 cores using message passing.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 14:49:15 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["M\u00fcthing", "Steffen", ""], ["Piatkowski", "Marian", ""], ["Bastian", "Peter", ""]]}, {"id": "1711.11075", "submitter": "Fabiana Zama", "authors": "Damiana Lazzaro and Elena Loli Piccolomini and Fabiana Zama", "title": "A fast nonconvex Compressed Sensing algorithm for highly low-sampled MR\n  images reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a fast and efficient method for the reconstruction\nof Magnetic Resonance Images (MRI) from severely under-sampled data. From the\nCompressed Sensing theory we have mathematically modeled the problem as a\nconstrained minimization problem with a family of non-convex regularizing\nobjective functions depending on a parameter and a least squares data fit\nconstraint. We propose a fast and efficient algorithm, named Fast NonConvex\nReweighting (FNCR) algorithm, based on an iterative scheme where the non-convex\nproblem is approximated by its convex linearization and the penalization\nparameter is automatically updated. The convex problem is solved by a\nForward-Backward procedure, where the Backward step is performed by a Split\nBregman strategy. Moreover, we propose a new efficient iterative solver for the\narising linear systems. We prove the convergence of the proposed FNCR method.\nThe results on synthetic phantoms and real images show that the algorithm is\nvery well performing and computationally efficient, even when compared to the\nbest performing methods proposed in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:39:08 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Lazzaro", "Damiana", ""], ["Piccolomini", "Elena Loli", ""], ["Zama", "Fabiana", ""]]}, {"id": "1711.11296", "submitter": "Daisuke Hotta", "authors": "Daisuke Hotta and Masashi Ujiie", "title": "A nestable, multigrid-friendly grid on a sphere for global spectral\n  models based on Clenshaw-Curtis quadrature", "comments": "37 pages, 8 figures; published in Quarterly Journal of the Royal\n  Meteorological Society", "journal-ref": "Q. J. Royal Meteorol. Soc. 144 (2018) 1382-1397", "doi": "10.1002/qj.3282", "report-no": null, "categories": "physics.ao-ph cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new grid system on a sphere is proposed that allows for straight-forward\nimplementation of both spherical-harmonics-based spectral methods and\ngridpoint-based multigrid methods. The latitudinal gridpoints in the new grid\nare equidistant and spectral transforms in the latitudinal direction are\nperformed using Clenshaw-Curtis quadrature. The spectral transforms with this\nnew grid and quadrature are shown to be exact within the machine precision\nprovided that the grid truncation is such that there are at least 2N + 1\nlatitudinal gridpoints for the total truncation wavenumber of N. The new grid\nand quadrature is implemented and tested on a shallow-water equations model and\nthe hydrostatic dry dynamical core of the global NWP model JMA-GSM. The\nintegration results obtained with the new quadrature are shown to be almost\nidentical to those obtained with the conventional Gaussian quadrature on\nGaussian grid. Only minor code changes are required to any Gaussian-based\nspectral models to employ the proposed quadrature.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 10:05:51 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 02:52:19 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hotta", "Daisuke", ""], ["Ujiie", "Masashi", ""]]}, {"id": "1711.11550", "submitter": "Kevin Carlberg", "authors": "Kevin Carlberg, Youngsoo Choi, Syuzanna Sargsyan", "title": "Conservative model reduction for finite-volume models", "comments": "Submitted of Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2018.05.019", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a method for model reduction of finite-volume models that\nguarantees the resulting reduced-order model is conservative, thereby\npreserving the structure intrinsic to finite-volume discretizations. The\nproposed reduced-order models associate with optimization problems\ncharacterized by a minimum-residual objective function and nonlinear equality\nconstraints that explicitly enforce conservation over subdomains. Conservative\nGalerkin projection arises from formulating this optimization problem at the\ntime-continuous level, while conservative least-squares Petrov--Galerkin (LSPG)\nprojection associates with a time-discrete formulation. We equip these\napproaches with hyper-reduction techniques in the case of nonlinear flux and\nsource terms, and also provide approaches for handling infeasibility. In\naddition, we perform analyses that include deriving conditions under which\nconservative Galerkin and conservative LSPG are equivalent, as well as deriving\na posteriori error bounds. Numerical experiments performed on a parameterized\nquasi-1D Euler equation demonstrate the ability of the proposed method to\nensure not only global conservation, but also significantly lower state-space\nerrors than nonconservative reduced-order models such as standard Galerkin and\nLSPG projection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:12:50 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 19:05:59 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 16:36:53 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Carlberg", "Kevin", ""], ["Choi", "Youngsoo", ""], ["Sargsyan", "Syuzanna", ""]]}]