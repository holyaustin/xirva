[{"id": "1405.0223", "submitter": "Sivaram Ambikasaran", "authors": "Sivaram Ambikasaran, Michael O'Neil, Karan Raj Singh", "title": "Fast symmetric factorization of hierarchical matrices with applications", "comments": "18 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.flu-dyn stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast direct algorithm for computing symmetric factorizations,\ni.e. $A = WW^T$, of symmetric positive-definite hierarchical matrices with\nweak-admissibility conditions. The computational cost for the symmetric\nfactorization scales as $\\mathcal{O}(n \\log^2 n)$ for hierarchically\noff-diagonal low-rank matrices. Once this factorization is obtained, the cost\nfor inversion, application, and determinant computation scales as\n$\\mathcal{O}(n \\log n)$. In particular, this allows for the near optimal\ngeneration of correlated random variates in the case where $A$ is a covariance\nmatrix. This symmetric factorization algorithm depends on two key ingredients.\nFirst, we present a novel symmetric factorization formula for low-rank updates\nto the identity of the form $I+UKU^T$. This factorization can be computed in\n$\\mathcal{O}(n)$ time if the rank of the perturbation is sufficiently small.\nSecond, combining this formula with a recursive divide-and-conquer strategy,\nnear linear complexity symmetric factorizations for hierarchically structured\nmatrices can be obtained. We present numerical results for matrices relevant to\nproblems in probability \\& statistics (Gaussian processes), interpolation\n(Radial basis functions), and Brownian dynamics calculations in fluid mechanics\n(the Rotne-Prager-Yamakawa tensor).\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 17:18:17 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 08:11:13 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Ambikasaran", "Sivaram", ""], ["O'Neil", "Michael", ""], ["Singh", "Karan Raj", ""]]}, {"id": "1405.0413", "submitter": "Renato J Cintra", "authors": "F. M. Bayer, R. J. Cintra, A. Madanayake, U. S. Potluri", "title": "Multiplierless Approximate 4-point DCT VLSI Architectures for Transform\n  Block Coding", "comments": "5 pages, 1 figure, corrected Figure 1 (published paper in EL is\n  incorrect)", "journal-ref": "Electronics Letters, vol. 49, no. 24, pp. 1532-1534, 2013", "doi": "10.1049/el.2013.1352", "report-no": null, "categories": "cs.AR cs.MM cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two multiplierless algorithms are proposed for 4x4 approximate-DCT for\ntransform coding in digital video. Computational architectures for 1-D/2-D\nrealisations are implemented using Xilinx FPGA devices. CMOS synthesis at the\n45 nm node indicate real-time operation at 1 GHz yielding 4x4 block rates of\n125 MHz at less than 120 mW of dynamic power consumption.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 14:29:02 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Madanayake", "A.", ""], ["Potluri", "U. S.", ""]]}, {"id": "1405.0560", "submitter": "Ashish Garg", "authors": "Vidit Sharma, Ashish Garg", "title": "Numerical Investigation of Effects of Compound Angle and Length to\n  Diameter Ratio on Adiabatic Film Cooling Effectiveness", "comments": "13 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A modification has been done in the normal injection hole of 35 degree, by\ninjecting the cold fluid at different angles(compound angle) in lateral\ndirection, providing a significant change in the shape of holes which later we\nfound in our numerical investigation giving good quality of effectiveness in\ncooling. Different L/D ratios are also studied for each compound angle. The\nnumerical simulation is performed based on Reynolds Averaged\nNavier-Stokes(RANS) equations with k-epsilon turbulence model by using\nFluent(Commercial Software). Adiabatic Film Cooling Effectiveness has been\nstudied for compound angles of (0, 30, 45 and 60 degrees) and L/D ratios of (1,\n2, 3 and 4) on a hole of 6mm diameter with blowing ratio 0.5. The findings are\nobtained from the results, concludes that the trend of laterally averaged\nadiabatic effectiveness is the function of L/D ratio and compound angle.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 08:15:32 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Sharma", "Vidit", ""], ["Garg", "Ashish", ""]]}, {"id": "1405.1250", "submitter": "Wilhelmiina H\\\"am\\\"al\\\"ainen", "authors": "Wilhelmiina H\\\"am\\\"al\\\"ainen", "title": "New tight approximations for Fisher's exact test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's exact test is often a preferred method to estimate the significance\nof statistical dependence. However, in large data sets the test is usually too\nworksome to be applied, especially in an exhaustive search (data mining). The\ntraditional solution is to approximate the significance with the\n$\\chi^2$-measure, but the accuracy is often unacceptable. As a solution, we\nintroduce a family of upper bounds, which are fast to calculate and approximate\nFisher's $p$-value accurately. In addition, the new approximations are not\nsensitive to the data size, distribution, or smallest expected counts like the\n$\\chi^2$-based approximation. According to both theoretical and experimental\nanalysis, the new approximations produce accurate results for all sufficiently\nstrong dependencies. The basic form of the approximation can fail with weak\ndependencies, but the general form of the upper bounds can be adjusted to be\narbitrarily accurate.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 12:58:39 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Wilhelmiina", ""]]}, {"id": "1405.1332", "submitter": "Rachel Ward", "authors": "Felix Krahmer and Rachel Ward", "title": "A unified framework for linear dimensionality reduction in L1", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.MG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a family of interpolation norms $\\| \\cdot \\|_{1,2,s}$ on $\\mathbb{R}^n$,\nwe provide a distribution over random matrices $\\Phi_s \\in \\mathbb{R}^{m \\times\nn}$ parametrized by sparsity level $s$ such that for a fixed set $X$ of $K$\npoints in $\\mathbb{R}^n$, if $m \\geq C s \\log(K)$ then with high probability,\n$\\frac{1}{2} \\| x \\|_{1,2,s} \\leq \\| \\Phi_s (x) \\|_1 \\leq 2 \\| x\\|_{1,2,s}$ for\nall $x\\in X$. Several existing results in the literature reduce to special\ncases of this result at different values of $s$: for $s=n$, $\\| x\\|_{1,2,n}\n\\equiv \\| x \\|_{1}$ and we recover that dimension reducing linear maps can\npreserve the $\\ell_1$-norm up to a distortion proportional to the dimension\nreduction factor, which is known to be the best possible such result. For\n$s=1$, $\\|x \\|_{1,2,1} \\equiv \\| x \\|_{2}$, and we recover an $\\ell_2 / \\ell_1$\nvariant of the Johnson-Lindenstrauss Lemma for Gaussian random matrices.\nFinally, if $x$ is $s$-sparse, then $\\| x \\|_{1,2,s} = \\| x \\|_1$ and we\nrecover that $s$-sparse vectors in $\\ell_1^n$ embed into $\\ell_1^{\\mathcal{O}(s\n\\log(n))}$ via sparse random matrix constructions.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 16:01:50 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 19:46:58 GMT"}, {"version": "v3", "created": "Fri, 15 Aug 2014 21:17:19 GMT"}, {"version": "v4", "created": "Tue, 20 Jan 2015 18:58:20 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2015 22:19:58 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Krahmer", "Felix", ""], ["Ward", "Rachel", ""]]}, {"id": "1405.1386", "submitter": "Giuseppe Romanazzi", "authors": "Isabel N. Figueiredo, Carlos Leal, Giuseppe Romanazzi, Bjorn Engquist", "title": "Homogenization Model for Aberrant Crypt Foci", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several explanations can be found in the literature about the origin of\ncolorectal cancer. There is however some agreement on the fact that the\ncarcinogenic process is a result of several genetic mutations of normal cells.\nThe colon epithelium is characterized by millions of invaginations, very small\ncavities, called crypts, where most of the cellular activity occurs. It is\nconsensual in the medical community, that a potential first manifestation of\nthe carcinogenic process, observed in conventional colonoscopy images, is the\nappearance of Aberrant Crypt Foci (ACF). These are clusters of abnormal crypts,\nmorphologically characterized by an atypical behavior of the cells that\npopulate the crypts. In this work an homogenization model is proposed, for\nrepresenting the cellular dynamics in the colon epithelium. The goal is to\nsimulate and predict, in silico, the spread and evolution of ACF, as it can be\nobserved in colonoscopy images. By assuming that the colon is an heterogeneous\nmedia, exhibiting a periodic distribution of crypts, we start this work by\ndescribing a periodic model, that represents the ACF cell-dynamics in a\ntwo-dimensional setting. Then, homogenization techniques are applied to this\nperiodic model, to find a simpler model, whose solution symbolizes the averaged\nbehavior of ACF at the tissue level. Some theoretical results concerning the\nexistence of solution of the homogenized model are proven, applying a fixed\npoint theorem. Numerical results showing the convergence of the periodic model\nto the homogenized model are presented.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 18:04:51 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 15:08:11 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2015 16:01:41 GMT"}, {"version": "v4", "created": "Mon, 7 Mar 2016 18:43:49 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Figueiredo", "Isabel N.", ""], ["Leal", "Carlos", ""], ["Romanazzi", "Giuseppe", ""], ["Engquist", "Bjorn", ""]]}, {"id": "1405.2168", "submitter": "Elham Shadkam", "authors": "Elham Shadkam and Mehdi Bijari", "title": "Evaluation The Efficiency Of Cuckoo Optimization Algorithm", "comments": "9 pages", "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vol.4, No.2, April 2014", "doi": null, "report-no": null, "categories": "cs.NE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new evolutionary algorithm, for continuous nonlinear\noptimization problems, is surveyed. This method is inspired by the life of a\nbird, called Cuckoo. The Cuckoo Optimization Algorithm (COA) is evaluated by\nusing the Rastrigin function. The problem is a non-linear continuous function\nwhich is used for evaluating optimization algorithms. The efficiency of the COA\nhas been studied by obtaining optimal solution of various dimensions Rastrigin\nfunction in this paper. The mentioned function also was solved by FA and ABC\nalgorithms. Comparing the results shows the COA has better performance than\nother algorithms. Application of algorithm to test function has proven its\ncapability to deal with difficult optimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 08:11:59 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Shadkam", "Elham", ""], ["Bijari", "Mehdi", ""]]}, {"id": "1405.2948", "submitter": "John Scales", "authors": "H. Lydia Deng and John A. Scales", "title": "Characterizing the Topography of Multi-dimensional Energy Landscapes", "comments": "14 figures and an appendix containing proof of main result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic issue in optimization, inverse theory,neural networks, computational\nchemistry and many other problems is the geometrical characterization of high\ndimensional functions. In inverse calculations one aims to characterize the set\nof models that fit the data (among other constraints). If the data misfit\nfunction is unimodal then one can find its peak by local optimization methods\nand characterize its width (related to the range of data-fitting models) by\nestimating derivatives at this peak. On the other hand, if there are local\nextrema, then a number of interesting and difficult problems arise. Are the\nlocal extrema important compared to the global or can they be eliminated (e.g.,\nby smoothing) without significant loss of information? Is there a sufficiently\nsmall number of local extrema that they can be enumerated via local\noptimization? What are the basins of attraction of these local extrema? Can two\nextrema be joined by a path that never goes uphill? Can the whole problem be\nreduced to one of enumerating the local extrema and their basins of attraction?\nFor locally ill-conditioned functions, premature convergence of local\noptimization can be confused with the presence of local extrema. Addressing any\nof these issues requires topographic information about the functions under\nstudy. But in many applications these functions may have hundreds or thousands\nof variables and can only be evaluated pointwise (by some numerical method for\ninstance). In this paper we describe systematic (but generic) methods of\nanalysing the topography of high dimensional functions using local optimization\nmethods applied to randomly chosen starting models. We provide a number of\nquantitative measures of function topography that have proven to be useful in\npractical problems along with error estimates.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2014 22:54:51 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Deng", "H. Lydia", ""], ["Scales", "John A.", ""]]}, {"id": "1405.3230", "submitter": "Kalyana Babu Nakshatrala", "authors": "S. Karimi, and K. B. Nakshatrala", "title": "A monolithic multi-time-step computational framework for first-order\n  transient systems with disparate scales", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2014.10.003", "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing robust simulation tools for problems involving multiple\nmathematical scales has been a subject of great interest in computational\nmathematics and engineering. A desirable feature to have in a numerical\nformulation for multiscale transient problems is to be able to employ different\ntime-steps (multi-time-step coupling), and different time integrators and\ndifferent numerical formulations (mixed methods) in different regions of the\ncomputational domain. We present two new monolithic multi-time-step mixed\ncoupling methods for first-order transient systems. We shall employ unsteady\nadvection-diffusion-reaction equation with linear decay as the model problem,\nwhich offers several unique challenges in terms of non-self-adjoint spatial\noperator and rich features in the solutions. We shall employ the dual Schur\ndomain decomposition technique to handle the decomposition of domain into\nsubdomains. Two different methods of enforcing compatibility along the\nsubdomain interface will be used in the time discrete setting. A systematic\ntheoretical analysis (which includes numerical stability, influence of\nperturbations, bounds on drift along the subdomain interface) will be\nperformed. The first coupling method ensures that there is no drift along the\nsubdomain interface but does not facilitate explicit/implicit coupling. The\nsecond coupling method allows explicit/implicit coupling with controlled (but\nnon-zero) drift in the solution along the subdomain interface. Several\ncanonical problems will be solved to numerically verify the theoretical\npredictions, and to illustrate the overall performance of the proposed coupling\nmethods. Finally, we shall illustrate the robustness of the proposed coupling\nmethods using a multi-time-step transient simulation of a fast bimolecular\nadvective-diffusive-reactive system.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:53:01 GMT"}, {"version": "v2", "created": "Thu, 28 Aug 2014 04:44:54 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Karimi", "S.", ""], ["Nakshatrala", "K. B.", ""]]}, {"id": "1405.3235", "submitter": "Chakir Tajani CT", "authors": "Chakir Tajani, Jaafar Abouchabaka", "title": "An Alternating KMF Algorithm to Solve the Cauchy Problem for Laplaces\n  Equation", "comments": "International Journal of Computer Applications Volume 38 8 January\n  2012", "journal-ref": null, "doi": "10.5120/4709-6876", "report-no": null, "categories": "cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This work concerns the use of the iterative algorithm (KMF algorithm)\nproposed by Kozlov, Mazya and Fomin to solve the Cauchy problem for Laplaces\nequation. This problem consists to recovering the lacking data on some part of\nthe boundary using the over specified conditions on the other part of the\nboundary. We describe an alternating formulation of the KMF algorithm and its\nrelationship with a classical formulation. The implementation of this algorithm\nfor a regular domain is performed by the finite element method using the\nsoftware Freefem. The numerical tests developed show the effectiveness of the\nproposed algorithm since it allows to have more accurate results as well as\nreducing the number of iterations needed for convergence.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 17:19:13 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Tajani", "Chakir", ""], ["Abouchabaka", "Jaafar", ""]]}, {"id": "1405.3468", "submitter": "Salvatore Cuomo", "authors": "S. Cuomo, R. Farina, A. Galletti, L. Marcellino", "title": "An error estimate of Gaussian Recursive Filter in 3Dvar problem", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational kernel of the three-dimensional variational data assimilation\n(3D-Var) problem is a linear system, generally solved by means of an iterative\nmethod. The most costly part of each iterative step is a matrix-vector product\nwith a very large covariance matrix having Gaussian correlation structure. This\noperation may be interpreted as a Gaussian convolution, that is a very\nexpensive numerical kernel. Recursive Filters (RFs) are a well known way to\napproximate the Gaussian convolution and are intensively applied in the\nmeteorology, in the oceanography and in forecast models. In this paper, we deal\nwith an oceanographic 3D-Var data assimilation scheme, named OceanVar, where\nthe linear system is solved by using the Conjugate Gradient (GC) method by\nreplacing, at each step, the Gaussian convolution with RFs. Here we give\ntheoretical issues on the discrete convolution approximation with a first order\n(1st-RF) and a third order (3rd-RF) recursive filters. Numerical experiments\nconfirm given error bounds and show the benefits, in terms of accuracy and\nperformance, of the 3-rd RF.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 12:29:28 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Cuomo", "S.", ""], ["Farina", "R.", ""], ["Galletti", "A.", ""], ["Marcellino", "L.", ""]]}, {"id": "1405.4644", "submitter": "Pavel Klav\\'ik", "authors": "Pavel Klav\\'ik, A. Cristiano I. Malossi, Constantin Bekas, and\n  Alessandro Curioni", "title": "Changing Computing Paradigms Towards Power Efficiency", "comments": null, "journal-ref": "Philosophical Transactions of the Royal Society A: Physical,\n  Mathematical and Engineering Sciences. 372(2018)", "doi": "10.1098/rsta.2013.0278", "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power awareness is fast becoming immensely important in computing, ranging\nfrom the traditional High Performance Computing applications, to the new\ngeneration of data centric workloads.\n  In this work we describe our efforts towards a power efficient computing\nparadigm that combines low precision and high precision arithmetic. We showcase\nour ideas for the widely used kernel of solving systems of linear equations\nthat finds numerous applications in scientific and engineering disciplines as\nwell as in large scale data analytics, statistics and machine learning.\n  Towards this goal we developed tools for the seamless power profiling of\napplications at a fine grain level. In addition, we verify here previous work\non post FLOPS/Watt metrics and show that these can shed much more light in the\npower/energy profile of important applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 09:03:58 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Klav\u00edk", "Pavel", ""], ["Malossi", "A. Cristiano I.", ""], ["Bekas", "Constantin", ""], ["Curioni", "Alessandro", ""]]}, {"id": "1405.4980", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck", "title": "Convex Optimization: Algorithms and Complexity", "comments": "A previous version of the manuscript was titled \"Theory of Convex\n  Optimization for Machine Learning\"", "journal-ref": "In Foundations and Trends in Machine Learning, Vol. 8: No. 3-4, pp\n  231-357, 2015", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This monograph presents the main complexity theorems in convex optimization\nand their corresponding algorithms. Starting from the fundamental theory of\nblack-box optimization, the material progresses towards recent advances in\nstructural optimization and stochastic optimization. Our presentation of\nblack-box optimization, strongly influenced by Nesterov's seminal book and\nNemirovski's lecture notes, includes the analysis of cutting plane methods, as\nwell as (accelerated) gradient descent schemes. We also pay special attention\nto non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror\ndescent, and dual averaging) and discuss their relevance in machine learning.\nWe provide a gentle introduction to structural optimization with FISTA (to\noptimize a sum of a smooth and a simple non-smooth term), saddle-point mirror\nprox (Nemirovski's alternative to Nesterov's smoothing), and a concise\ndescription of interior point methods. In stochastic optimization we discuss\nstochastic gradient descent, mini-batches, random coordinate descent, and\nsublinear algorithms. We also briefly touch upon convex relaxation of\ncombinatorial problems and the use of randomness to round solutions, as well as\nrandom walks based methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 07:50:56 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 18:52:04 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""]]}, {"id": "1405.5170", "submitter": "Martin Drohmann", "authors": "Martin Drohmann and Kevin Carlberg", "title": "The ROMES method for statistical modeling of reduced-order-model error", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification, Vol. 3, No. 1, p.\n  116-145 (2015)", "doi": "10.1137/140969841", "report-no": null, "categories": "cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a technique for statistically modeling errors introduced\nby reduced-order models. The method employs Gaussian-process regression to\nconstruct a mapping from a small number of computationally inexpensive `error\nindicators' to a distribution over the true error. The variance of this\ndistribution can be interpreted as the (epistemic) uncertainty introduced by\nthe reduced-order model. To model normed errors, the method employs existing\nrigorous error bounds and residual norms as indicators; numerical experiments\nshow that the method leads to a near-optimal expected effectivity in contrast\nto typical error bounds. To model errors in general outputs, the method uses\ndual-weighted residuals---which are amenable to uncertainty control---as\nindicators. Experiments illustrate that correcting the reduced-order-model\noutput with this surrogate can improve prediction accuracy by an order of\nmagnitude; this contrasts with existing `multifidelity correction' approaches,\nwhich often fail for reduced-order models and suffer from the curse of\ndimensionality. The proposed error surrogates also lead to a notion of\n`probabilistic rigor', i.e., the surrogate bounds the error with specified\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 17:52:01 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 19:36:38 GMT"}, {"version": "v3", "created": "Wed, 10 Dec 2014 22:40:58 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Drohmann", "Martin", ""], ["Carlberg", "Kevin", ""]]}, {"id": "1405.5245", "submitter": "Luca Bonaventura", "authors": "G. Tumolo and L. Bonaventura", "title": "An accurate and efficient numerical framework for adaptive numerical\n  weather prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA physics.ao-ph physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an accurate and efficient discretization approach for the adaptive\ndiscretization of typical model equations employed in numerical weather\nprediction. A semi-Lagrangian approach is combined with the TR-BDF2\nsemi-implicit time discretization method and with a spatial discretization\nbased on adaptive discontinuous finite elements. The resulting method has full\nsecond order accuracy in time and can employ polynomial bases of arbitrarily\nhigh degree in space, is unconditionally stable and can effectively adapt the\nnumber of degrees of freedom employed in each element, in order to balance\naccuracy and computational cost. The p-adaptivity approach employed does not\nrequire remeshing, therefore it is especially suitable for applications, such\nas numerical weather prediction, in which a large number of physical quantities\nare associated with a given mesh. Furthermore, although the proposed method can\nbe implemented on arbitrary unstructured and nonconforming meshes, even its\napplication on simple Cartesian meshes in spherical coordinates can cure\neffectively the pole problem by reducing the polynomial degree used in the\npolar elements. Numerical simulations of classical benchmarks for the shallow\nwater and for the fully compressible Euler equations validate the method and\ndemonstrate its capability to achieve accurate results also at large Courant\nnumbers, with time steps up to 100 times larger than those of typical explicit\ndiscretizations of the same problems, while reducing the computational cost\nthanks to the adaptivity algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 21:08:44 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Tumolo", "G.", ""], ["Bonaventura", "L.", ""]]}, {"id": "1405.6139", "submitter": "Andrey Stroganov Valentine", "authors": "Vladimir Aristov and Andrey Stroganov", "title": "Development of the method of computer analogy for studying and solving\n  complex nonlinear systems", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method of representation of a solution as segments of the series in powers\nof the step of the independent variable is expanded for solving complex systems\nof ordinary differential equations (ODE): the Lorenz system and other systems.\nA new procedure of reduction of the representation of the solution to a sum of\ntwo parts (regular and random) is performed. A shifting procedure is applied in\neach level of the independent variable to the random part and it acts as the\nfilter that extracts the values to the regular part. In certain cases it is\npossible to omit the random part and construct the approximation which does not\nconverge but still provides the qualitative information about the full solution\n(a linear approximation provides a simple exact solution). Evaluation of the\nerror for this case is performed. Constructing the analytical representation of\nthe solutions for these systems by the developed method is presented.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 18:01:37 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Aristov", "Vladimir", ""], ["Stroganov", "Andrey", ""]]}, {"id": "1405.7442", "submitter": "Andr\\'e Almeida", "authors": "G\\'erard Favier and Andr\\'e L.F. de Almeida", "title": "Overview of Constrained PARAFAC Models", "comments": null, "journal-ref": null, "doi": "10.1186/1687-6180-2014-142", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an overview of constrained PARAFAC models where the\nconstraints model linear dependencies among columns of the factor matrices of\nthe tensor decomposition, or alternatively, the pattern of interactions between\ndifferent modes of the tensor which are captured by the equivalent core tensor.\nSome tensor prerequisites with a particular emphasis on mode combination using\nKronecker products of canonical vectors that makes easier matricization\noperations, are first introduced. This Kronecker product based approach is also\nformulated in terms of the index notation, which provides an original and\nconcise formalism for both matricizing tensors and writing tensor models. Then,\nafter a brief reminder of PARAFAC and Tucker models, two families of\nconstrained tensor models, the co-called PARALIND/CONFAC and PARATUCK models,\nare described in a unified framework, for $N^{th}$ order tensors. New tensor\nmodels, called nested Tucker models and block PARALIND/CONFAC models, are also\nintroduced. A link between PARATUCK models and constrained PARAFAC models is\nthen established. Finally, new uniqueness properties of PARATUCK models are\ndeduced from sufficient conditions for essential uniqueness of their associated\nconstrained PARAFAC models.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 02:27:50 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 19:51:02 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Favier", "G\u00e9rard", ""], ["de Almeida", "Andr\u00e9 L. F.", ""]]}, {"id": "1405.7879", "submitter": "Alex Eftimiades", "authors": "Alex Eftimiades", "title": "Kahler: An Implementation of Discrete Exterior Calculus on Hermitian\n  Manifolds", "comments": "Added a link to the code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CG cs.MS math.DG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper details the techniques and algorithms implemented in Kahler, a\nPython library that implements discrete exterior calculus on arbitrary\nHermitian manifolds. Borrowing techniques and ideas first implemented in PyDEC,\nKahler provides a uniquely general framework for computation using discrete\nexterior calculus. Manifolds can have arbitrary dimension, topology, bilinear\nHermitian metrics, and embedding dimension. Kahler comes equipped with tools\nfor generating triangular meshes in arbitrary dimensions with arbitrary\ntopology. Kahler can also generate discrete sharp operators and implement de\nRham maps. Computationally intensive tasks are automatically parallelized over\nthe number of cores detected. The program itself is written in Cython--a\nsuperset of the Python language that is translated to C and compiled for extra\nspeed. Kahler is applied to several example problems: normal modes of a\nvibrating membrane, electromagnetic resonance in a cavity, the quantum harmonic\noscillator, and the Dirac-Kahler equation. Convergence is demonstrated on\nrandom meshes.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 14:36:33 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 14:42:41 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Eftimiades", "Alex", ""]]}]