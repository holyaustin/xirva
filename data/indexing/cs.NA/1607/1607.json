[{"id": "1607.00101", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Randomized block proximal damped Newton method for composite\n  self-concordant minimization", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the composite self-concordant (CSC) minimization\nproblem, which minimizes the sum of a self-concordant function $f$ and a\n(possibly nonsmooth) proper closed convex function $g$. The CSC minimization is\nthe cornerstone of the path-following interior point methods for solving a\nbroad class of convex optimization problems. It has also found numerous\napplications in machine learning. The proximal damped Newton (PDN) methods have\nbeen well studied in the literature for solving this problem that enjoy a nice\niteration complexity. Given that at each iteration these methods typically\nrequire evaluating or accessing the Hessian of $f$ and also need to solve a\nproximal Newton subproblem, the cost per iteration can be prohibitively high\nwhen applied to large-scale problems. Inspired by the recent success of block\ncoordinate descent methods, we propose a randomized block proximal damped\nNewton (RBPDN) method for solving the CSC minimization. Compared to the PDN\nmethods, the computational cost per iteration of RBPDN is usually significantly\nlower. The computational experiment on a class of regularized logistic\nregression problems demonstrate that RBPDN is indeed promising in solving\nlarge-scale CSC minimization problems. The convergence of RBPDN is also\nanalyzed in the paper. In particular, we show that RBPDN is globally convergent\nwhen $g$ is Lipschitz continuous. It is also shown that RBPDN enjoys a local\nlinear convergence. Moreover, we show that for a class of $g$ including the\ncase where $g$ is Lipschitz differentiable, RBPDN enjoys a global linear\nconvergence. As a striking consequence, it shows that the classical damped\nNewton methods [22,40] and the PDN [31] for such $g$ are globally linearly\nconvergent, which was previously unknown in the literature. Moreover, this\nresult can be used to sharpen the existing iteration complexity of these\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 03:16:57 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1607.00127", "submitter": "Kim Batselier", "authors": "Kim Batselier, Zhongming Chen, Ngai Wong", "title": "Tensor Network alternating linear scheme for MIMO Volterra system\n  identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces two Tensor Network-based iterative algorithms for the\nidentification of high-order discrete-time nonlinear multiple-input\nmultiple-output (MIMO) Volterra systems. The system identification problem is\nrewritten in terms of a Volterra tensor, which is never explicitly constructed,\nthus avoiding the curse of dimensionality. It is shown how each iteration of\nthe two identification algorithms involves solving a linear system of low\ncomputational complexity. The proposed algorithms are guaranteed to\nmonotonically converge and numerical stability is ensured through the use of\northogonal matrix factorizations. The performance and accuracy of the two\nidentification algorithms are illustrated by numerical experiments, where\naccurate degree-10 MIMO Volterra models are identified in about 1 second in\nMatlab on a standard desktop pc.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 07:01:01 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 12:54:39 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Batselier", "Kim", ""], ["Chen", "Zhongming", ""], ["Wong", "Ngai", ""]]}, {"id": "1607.00315", "submitter": "Javier Turek Mr.", "authors": "Eran Treister and Javier S. Turek and Irad Yavneh", "title": "A multilevel framework for sparse optimization with application to\n  inverse covariance estimation and logistic regression", "comments": "To appear on SISC journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving l1 regularized optimization problems is common in the fields of\ncomputational biology, signal processing and machine learning. Such l1\nregularization is utilized to find sparse minimizers of convex functions. A\nwell-known example is the LASSO problem, where the l1 norm regularizes a\nquadratic function. A multilevel framework is presented for solving such l1\nregularized sparse optimization problems efficiently. We take advantage of the\nexpected sparseness of the solution, and create a hierarchy of problems of\nsimilar type, which is traversed in order to accelerate the optimization\nprocess. This framework is applied for solving two problems: (1) the sparse\ninverse covariance estimation problem, and (2) l1-regularized logistic\nregression. In the first problem, the inverse of an unknown covariance matrix\nof a multivariate normal distribution is estimated, under the assumption that\nit is sparse. To this end, an l1 regularized log-determinant optimization\nproblem needs to be solved. This task is challenging especially for large-scale\ndatasets, due to time and memory limitations. In the second problem, the\nl1-regularization is added to the logistic regression classification objective\nto reduce overfitting to the data and obtain a sparse model. Numerical\nexperiments demonstrate the efficiency of the multilevel framework in\naccelerating existing iterative solvers for both of these problems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 16:59:13 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Treister", "Eran", ""], ["Turek", "Javier S.", ""], ["Yavneh", "Irad", ""]]}, {"id": "1607.00345", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien", "title": "Convergence Rate of Frank-Wolfe for Non-Convex Objectives", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple proof that the Frank-Wolfe algorithm obtains a stationary\npoint at a rate of $O(1/\\sqrt{t})$ on non-convex objectives with a Lipschitz\ncontinuous gradient. Our analysis is affine invariant and is the first, to the\nbest of our knowledge, giving a similar rate to what was already proven for\nprojected gradient methods (though on slightly different measures of\nstationarity).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 18:37:33 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Lacoste-Julien", "Simon", ""]]}, {"id": "1607.00346", "submitter": "Yingzhou Li", "authors": "Yingzhou Li and Lexing Ying", "title": "Distributed-memory Hierarchical Interpolative Factorization", "comments": null, "journal-ref": null, "doi": "10.1186/s40687-017-0100-6", "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hierarchical interpolative factorization (HIF) offers an efficient way\nfor solving or preconditioning elliptic partial differential equations. By\nexploiting locality and low-rank properties of the operators, the HIF achieves\nquasi-linear complexity for factorizing the discrete positive definite elliptic\noperator and linear complexity for solving the associated linear system. In\nthis paper, the distributed-memory HIF (DHIF) is introduced as a parallel and\ndistributed-memory implementation of the HIF. The DHIF organizes the processes\nin a hierarchical structure and keep the communication as local as possible.\nThe computation complexity is $O\\left(\\frac{N\\log N}{P}\\right)$ and\n$O\\left(\\frac{N}{P}\\right)$ for constructing and applying the DHIF,\nrespectively, where $N$ is the size of the problem and $P$ is the number of\nprocesses. The communication complexity is $O\\left(\\sqrt{P}\\log^3\nP\\right)\\alpha + O\\left(\\frac{N^{2/3}}{\\sqrt{P}}\\right)\\beta$ where $\\alpha$ is\nthe latency and $\\beta$ is the inverse bandwidth. Extensive numerical examples\nare performed on the NERSC Edison system with up to 8192 processes. The\nnumerical results agree with the complexity analysis and demonstrate the\nefficiency and scalability of the DHIF.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 18:37:34 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 19:48:33 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 07:30:09 GMT"}, {"version": "v4", "created": "Thu, 23 Feb 2017 07:42:28 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Li", "Yingzhou", ""], ["Ying", "Lexing", ""]]}, {"id": "1607.00514", "submitter": "Nicolo Colombo", "authors": "Nicolo Colombo and Nikos Vlassis", "title": "Approximate Joint Matrix Triangularization", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximate joint triangularization of a set of\nnoisy jointly diagonalizable real matrices. Approximate joint triangularizers\nare commonly used in the estimation of the joint eigenstructure of a set of\nmatrices, with applications in signal processing, linear algebra, and tensor\ndecomposition. By assuming the input matrices to be perturbations of\nnoise-free, simultaneously diagonalizable ground-truth matrices, the\napproximate joint triangularizers are expected to be perturbations of the exact\njoint triangularizers of the ground-truth matrices. We provide a priori and a\nposteriori perturbation bounds on the `distance' between an approximate joint\ntriangularizer and its exact counterpart. The a priori bounds are theoretical\ninequalities that involve functions of the ground-truth matrices and noise\nmatrices, whereas the a posteriori bounds are given in terms of observable\nquantities that can be computed from the input matrices. From a practical\nperspective, the problem of finding the best approximate joint triangularizer\nof a set of noisy matrices amounts to solving a nonconvex optimization problem.\nWe show that, under a condition on the noise level of the input matrices, it is\npossible to find a good initial triangularizer such that the solution obtained\nby any local descent-type algorithm has certain global guarantees. Finally, we\ndiscuss the application of approximate joint matrix triangularization to\ncanonical tensor decomposition and we derive novel estimation error bounds.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 14:25:58 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Colombo", "Nicolo", ""], ["Vlassis", "Nikos", ""]]}, {"id": "1607.00648", "submitter": "Tobias Weinzierl", "authors": "Marion Weinzierl, Tobias Weinzierl", "title": "Quasi-matrix-free hybrid multigrid on dynamically adaptive Cartesian\n  grids", "comments": null, "journal-ref": null, "doi": "10.1145/3165280", "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a family of spacetree-based multigrid realizations using the\ntree's multiscale nature to derive coarse grids. They align with matrix-free\ngeometric multigrid solvers as they never assemble the system matrices which is\ncumbersome for dynamically adaptive grids and full multigrid. The most\nsophisticated realizations use BoxMG to construct operator-dependent\nprolongation and restriction in combination with Galerkin/Petrov-Galerkin\ncoarse-grid operators. This yields robust solvers for nontrivial elliptic\nproblems. We embed the algebraic, problem- and grid-dependent multigrid\noperators as stencils into the grid and evaluate all matrix-vector products\nin-situ throughout the grid traversals. While such an approach is not literally\nmatrix-free---the grid carries the matrix---we propose to switch to a\nhierarchical representation of all operators. Only differences of algebraic\noperators to their geometric counterparts are held. These hierarchical\ndifferences can be stored and exchanged with small memory footprint. Our\nrealizations support arbitrary dynamically adaptive grids while they vertically\nintegrate the multilevel operations through spacetree linearization. This\nyields good memory access characteristics, while standard colouring of mesh\nentities with domain decomposition allows us to use parallel manycore clusters.\nAll realization ingredients are detailed such that they can be used by other\ncodes.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 14:54:45 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 06:44:08 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 10:00:41 GMT"}, {"version": "v4", "created": "Mon, 14 Nov 2016 08:31:26 GMT"}, {"version": "v5", "created": "Mon, 17 Jul 2017 20:45:30 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Weinzierl", "Marion", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "1607.00968", "submitter": "Eran Treister", "authors": "Eran Treister and Eldad Haber", "title": "Full waveform inversion guided by travel time tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full waveform inversion (FWI) is a process in which seismic numerical\nsimulations are fit to observed data by changing the wave velocity model of the\nmedium under investigation. The problem is non-linear, and therefore\noptimization techniques have been used to find a reasonable solution to the\nproblem. The main problem in fitting the data is the lack of low spatial\nfrequencies. This deficiency often leads to a local minimum and to\nnon-plausible solutions. In this work we explore how to obtain low frequency\ninformation for FWI. Our approach involves augmenting FWI with travel time\ntomography, which has low-frequency features. By jointly inverting these two\nproblems we enrich FWI with information that can replace low frequency data. In\naddition, we use high order regularization, in a preliminary inversion stage,\nto prevent high frequency features from polluting our model in the initial\nstages of the reconstruction. This regularization also promotes the\nnon-dominant low-frequency modes that exist in the FWI sensitivity. By applying\na joint FWI and travel time inversion we are able to obtain a smooth model than\ncan later be used to recover a good approximation for the true model. A second\ncontribution of this paper involves the acceleration of the main computational\nbottleneck in FWI--the solution of the Helmholtz equation. We show that the\nsolution time can be reduced by solving the equation for multiple right hand\nsides using block multigrid preconditioned Krylov methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 17:33:05 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 07:31:46 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 09:35:51 GMT"}, {"version": "v4", "created": "Mon, 5 Jun 2017 08:41:13 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Treister", "Eran", ""], ["Haber", "Eldad", ""]]}, {"id": "1607.00973", "submitter": "Eran Treister", "authors": "Eran Treister and Eldad Haber", "title": "A fast marching algorithm for the factored eikonal equation", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2016.08.012", "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eikonal equation is instrumental in many applications in several fields\nranging from computer vision to geoscience. This equation can be efficiently\nsolved using the iterative Fast Sweeping (FS) methods and the direct Fast\nMarching (FM) methods. However, when used for a point source, the original\neikonal equation is known to yield inaccurate numerical solutions, because of a\nsingularity at the source. In this case, the factored eikonal equation is often\npreferred, and is known to yield a more accurate numerical solution. One\napplication that requires the solution of the eikonal equation for point\nsources is travel time tomography. This inverse problem may be formulated using\nthe eikonal equation as a forward problem. While this problem has been solved\nusing FS in the past, the more recent choice for applying it involves FM\nmethods because of the efficiency in which sensitivities can be obtained using\nthem. However, while several FS methods are available for solving the factored\nequation, the FM method is available only for the original eikonal equation.\n  In this paper we develop a Fast Marching algorithm for the factored eikonal\nequation, using both first and second order finite-difference schemes. Our\nalgorithm follows the same lines as the original FM algorithm and requires the\nsame computational effort. In addition, we show how to obtain sensitivities\nusing this FM method and apply travel time tomography, formulated as an inverse\nfactored eikonal equation. Numerical results in two and three dimensions show\nthat our algorithm solves the factored eikonal equation efficiently, and\ndemonstrate the achieved accuracy for computing the travel time. We also\ndemonstrate a recovery of a 2D and 3D heterogeneous medium by travel time\ntomography using the eikonal equation for forward modelling and inversion by\nGauss-Newton.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 17:46:45 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 20:25:50 GMT"}, {"version": "v3", "created": "Sat, 27 Aug 2016 09:32:09 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Treister", "Eran", ""], ["Haber", "Eldad", ""]]}, {"id": "1607.01027", "submitter": "Tianbao Yang", "authors": "Yi Xu, Qihang Lin, Tianbao Yang", "title": "Accelerate Stochastic Subgradient Method by Leveraging Local Growth\n  Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new theory is developed for first-order stochastic convex\noptimization, showing that the global convergence rate is sufficiently\nquantified by a local growth rate of the objective function in a neighborhood\nof the optimal solutions. In particular, if the objective function $F(\\mathbf\nw)$ in the $\\epsilon$-sublevel set grows as fast as $\\|\\mathbf w - \\mathbf\nw_*\\|_2^{1/\\theta}$, where $\\mathbf w_*$ represents the closest optimal\nsolution to $\\mathbf w$ and $\\theta\\in(0,1]$ quantifies the local growth rate,\nthe iteration complexity of first-order stochastic optimization for achieving\nan $\\epsilon$-optimal solution can be $\\widetilde O(1/\\epsilon^{2(1-\\theta)})$,\nwhich is optimal at most up to a logarithmic factor. To achieve the faster\nglobal convergence, we develop two different accelerated stochastic subgradient\nmethods by iteratively solving the original problem approximately in a local\nregion around a historical solution with the size of the local region gradually\ndecreasing as the solution approaches the optimal set. Besides the theoretical\nimprovements, this work also includes new contributions towards making the\nproposed algorithms practical: (i) we present practical variants of accelerated\nstochastic subgradient methods that can run without the knowledge of\nmultiplicative growth constant and even the growth rate $\\theta$; (ii) we\nconsider a broad family of problems in machine learning to demonstrate that the\nproposed algorithms enjoy faster convergence than traditional stochastic\nsubgradient method. We also characterize the complexity of the proposed\nalgorithms for ensuring the gradient is small without the smoothness\nassumption.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 20:01:17 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 03:03:55 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 15:24:58 GMT"}, {"version": "v4", "created": "Sun, 21 Jul 2019 20:44:28 GMT"}, {"version": "v5", "created": "Wed, 6 May 2020 03:21:49 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Xu", "Yi", ""], ["Lin", "Qihang", ""], ["Yang", "Tianbao", ""]]}, {"id": "1607.01231", "submitter": "Shiqian Ma", "authors": "Xiao Wang, Shiqian Ma, Donald Goldfarb, Wei Liu", "title": "Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization", "comments": "published in SIAM Journal on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study stochastic quasi-Newton methods for nonconvex\nstochastic optimization, where we assume that noisy information about the\ngradients of the objective function is available via a stochastic first-order\noracle (SFO). We propose a general framework for such methods, for which we\nprove almost sure convergence to stationary points and analyze its worst-case\niteration complexity. When a randomly chosen iterate is returned as the output\nof such an algorithm, we prove that in the worst-case, the SFO-calls complexity\nis $O(\\epsilon^{-2})$ to ensure that the expectation of the squared norm of the\ngradient is smaller than the given accuracy tolerance $\\epsilon$. We also\npropose a specific algorithm, namely a stochastic damped L-BFGS (SdLBFGS)\nmethod, that falls under the proposed framework. {Moreover, we incorporate the\nSVRG variance reduction technique into the proposed SdLBFGS method, and analyze\nits SFO-calls complexity. Numerical results on a nonconvex binary\nclassification problem using SVM, and a multiclass classification problem using\nneural networks are reported.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 12:51:33 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 08:45:20 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 07:18:03 GMT"}, {"version": "v4", "created": "Sun, 21 May 2017 06:23:50 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Wang", "Xiao", ""], ["Ma", "Shiqian", ""], ["Goldfarb", "Donald", ""], ["Liu", "Wei", ""]]}, {"id": "1607.01404", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Eloy Romero, Andreas Stathopoulos", "title": "PRIMME_SVDS: A High-Performance Preconditioned SVD Solver for Accurate\n  Large-Scale Computations", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing number of applications requiring the solution of large scale\nsingular value problems have rekindled interest in iterative methods for the\nSVD. Some promising recent ad- vances in large scale iterative methods are\nstill plagued by slow convergence and accuracy limitations for computing\nsmallest singular triplets. Furthermore, their current implementations in\nMATLAB cannot address the required large problems. Recently, we presented a\npreconditioned, two-stage method to effectively and accurately compute a small\nnumber of extreme singular triplets. In this research, we present a\nhigh-performance software, PRIMME SVDS, that implements our hybrid method based\non the state-of-the-art eigensolver package PRIMME for both largest and\nsmallest singular values. PRIMME SVDS fills a gap in production level software\nfor computing the partial SVD, especially with preconditioning. The numerical\nexperiments demonstrate its superior performance compared to other\nstate-of-the-art software and its good parallel performance under strong and\nweak scaling.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 20:15:56 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 18:27:56 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Wu", "Lingfei", ""], ["Romero", "Eloy", ""], ["Stathopoulos", "Andreas", ""]]}, {"id": "1607.01477", "submitter": "Tim Moon", "authors": "Tim Moon and Jack Poulson", "title": "Accelerating eigenvector and pseudospectra computation using blocked\n  multi-shift triangular solves", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-shift triangular solves are basic linear algebra calculations with\napplications in eigenvector and pseudospectra computation. We propose blocked\nalgorithms that efficiently exploit Level 3 BLAS to perform multi-shift\ntriangular solves and safe multi-shift triangular solves. Numerical experiments\nindicate that computing triangular eigenvectors with a safe multi-shift\ntriangular solve achieves speedups by a factor of 60 relative to LAPACK. This\nalgorithm accelerates the calculation of general eigenvectors threefold. When\nusing multi-shift triangular solves to compute pseudospectra, we report\nninefold speedups relative to EigTool.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 04:19:04 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 16:52:39 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Moon", "Tim", ""], ["Poulson", "Jack", ""]]}, {"id": "1607.01668", "submitter": "Xiao Fu", "authors": "Nicholas D. Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang,\n  Evangelos E. Papalexakis, Christos Faloutsos", "title": "Tensor Decomposition for Signal Processing and Machine Learning", "comments": "revised version, overview article", "journal-ref": null, "doi": "10.1109/TSP.2017.2690524", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors or {\\em multi-way arrays} are functions of three or more indices\n$(i,j,k,\\cdots)$ -- similar to matrices (two-way arrays), which are functions\nof two indices $(r,c)$ for (row,column). Tensors have a rich history,\nstretching over almost a century, and touching upon numerous disciplines; but\nthey have only recently become ubiquitous in signal and data analytics at the\nconfluence of signal processing, statistics, data mining and machine learning.\nThis overview article aims to provide a good starting point for researchers and\npractitioners interested in learning about and working with tensors. As such,\nit focuses on fundamentals and motivation (using various application examples),\naiming to strike an appropriate balance of breadth {\\em and depth} that will\nenable someone having taken first graduate courses in matrix algebra and\nprobability to get started doing research and/or developing tensor algorithms\nand software. Some background in applied optimization is useful but not\nstrictly required. The material covered includes tensor rank and rank\ndecomposition; basic tensor factorization models and their relationships and\nproperties (including fairly good coverage of identifiability); broad coverage\nof algorithms ranging from alternating optimization to stochastic gradient;\nstatistical performance analysis; and applications ranging from source\nseparation to collaborative filtering, mixture and topic modeling,\nclassification, and multilinear subspace learning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 15:22:31 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 15:16:53 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Sidiropoulos", "Nicholas D.", ""], ["De Lathauwer", "Lieven", ""], ["Fu", "Xiao", ""], ["Huang", "Kejun", ""], ["Papalexakis", "Evangelos E.", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1607.02548", "submitter": "Peyman Tavallali", "authors": "Peyman Tavallali and Thomas Y. Hou", "title": "On the non-uniqueness of the instantaneous frequency", "comments": "This paper has been withdrawn by the authors because some of the\n  statements and conclusions made by this paper were not accurate and their\n  interpretations could be misleading based on some feedback from readers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate the debated Instantaneous Frequency (IF)\ntopic. Here, we show that IF is non-unique inherently. We explain how this\nnon-uniqueness can be quantified and explained from a mathematical perspective.\nThe non-uniqueness of the IF can also be observed if different methods of\nadaptive signal processing are used. We will also show that even if we know the\nphysical origin of an oscillatory signal, e.g. linear second order ordinary\ndifferential equation, the non-uniqueness is still present. All in all, we will\nend up with the conclusion that, without any a priori assumption about the\nrelationship of the envelope and phase function of an oscillatory signal, there\nis not any preferred neither best representation of the IF of such oscillatory\nsignal.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 23:27:47 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 06:30:07 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Tavallali", "Peyman", ""], ["Hou", "Thomas Y.", ""]]}, {"id": "1607.02584", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jiashi Feng, Shuicheng Yan, Zhouchen Lin", "title": "A Unified Alternating Direction Method of Multipliers by Majorization\n  Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accompanied with the rising popularity of compressed sensing, the Alternating\nDirection Method of Multipliers (ADMM) has become the most widely used solver\nfor linearly constrained convex problems with separable objectives. In this\nwork, we observe that many previous variants of ADMM update the primal variable\nby minimizing different majorant functions with their convergence proofs given\ncase by case. Inspired by the principle of majorization minimization, we\nrespectively present the unified frameworks and convergence analysis for the\nGauss-Seidel ADMMs and Jacobian ADMMs, which use different historical\ninformation for the current updating. Our frameworks further generalize\nprevious ADMMs to the ones capable of solving the problems with non-separable\nobjectives by minimizing their separable majorant surrogates. We also show that\nthe bound which measures the convergence speed of ADMMs depends on the\ntightness of the used majorant function. Then several techniques are introduced\nto improve the efficiency of ADMMs by tightening the majorant functions. In\nparticular, we propose the Mixed Gauss-Seidel and Jacobian ADMM (M-ADMM) which\nalleviates the slow convergence issue of Jacobian ADMMs by absorbing merits of\nthe Gauss-Seidel ADMMs. M-ADMM can be further improved by using backtracking,\nwise variable partition and fully exploiting the structure of the constraint.\nBeyond the guarantee in theory, numerical experiments on both synthesized and\nreal-world data further demonstrate the superiority of our new ADMMs in\npractice. Finally, we release a toolbox at https://github.com/canyilu/LibADMM\nthat implements efficient ADMMs for many problems in compressed sensing.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 08:15:50 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Lu", "Canyi", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1607.03081", "submitter": "Hiva Ghanbari", "authors": "Hiva Ghanbari, Katya Scheinberg", "title": "Proximal Quasi-Newton Methods for Regularized Convex Optimization with\n  Linear and Accelerated Sublinear Convergence Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [19], a general, inexact, efficient proximal quasi-Newton algorithm for\ncomposite optimization problems has been proposed and a sublinear global\nconvergence rate has been established. In this paper, we analyze the\nconvergence properties of this method, both in the exact and inexact setting,\nin the case when the objective function is strongly convex. We also investigate\na practical variant of this method by establishing a simple stopping criterion\nfor the subproblem optimization. Furthermore, we consider an accelerated\nvariant, based on FISTA [1], to the proximal quasi-Newton algorithm. A similar\naccelerated method has been considered in [7], where the convergence rate\nanalysis relies on very strong impractical assumptions. We present a modified\nanalysis while relaxing these assumptions and perform a practical comparison of\nthe accelerated proximal quasi- Newton algorithm and the regular one. Our\nanalysis and computational results show that acceleration may not bring any\nbenefit in the quasi-Newton setting.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:20:06 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 01:08:29 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ghanbari", "Hiva", ""], ["Scheinberg", "Katya", ""]]}, {"id": "1607.03092", "submitter": "Qingjiang Shi", "authors": "Qingjiang Shi, Haoran Sun, Songtao Lu, Mingyi Hong, Meisam Razaviyayn", "title": "Inexact Block Coordinate Descent Methods For Symmetric Nonnegative\n  Matrix Factorization", "comments": "Submitted to TSP", "journal-ref": null, "doi": "10.1109/TSP.2017.2731321", "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric nonnegative matrix factorization (SNMF) is equivalent to computing\na symmetric nonnegative low rank approximation of a data similarity matrix. It\ninherits the good data interpretability of the well-known nonnegative matrix\nfactorization technique and have better ability of clustering nonlinearly\nseparable data. In this paper, we focus on the algorithmic aspect of the SNMF\nproblem and propose simple inexact block coordinate decent methods to address\nthe problem, leading to both serial and parallel algorithms. The proposed\nalgorithms have guaranteed stationary convergence and can efficiently handle\nlarge-scale and/or sparse SNMF problems. Extensive simulations verify the\neffectiveness of the proposed algorithms compared to recent state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:48:41 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Shi", "Qingjiang", ""], ["Sun", "Haoran", ""], ["Lu", "Songtao", ""], ["Hong", "Mingyi", ""], ["Razaviyayn", "Meisam", ""]]}, {"id": "1607.03252", "submitter": "Ulrich Ruede", "authors": "Bj\\\"orn Gmeiner and Daniel Drzisga and Ulrich Ruede and Robert\n  Scheichl and Barbara Wohlmuth", "title": "Scheduling massively parallel multigrid for multilevel Monte Carlo\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of naive, sampling-based uncertainty\nquantification for 3D partial differential equations is extremely high.\nMultilevel approaches, such as multilevel Monte Carlo (MLMC), can reduce the\ncomplexity significantly, but to exploit them fully in a parallel environment,\nsophisticated scheduling strategies are needed. Often fast algorithms that are\nexecuted in parallel are essential to compute fine level samples in 3D, whereas\nto compute individual coarse level samples only moderate numbers of processors\ncan be employed efficiently. We make use of multiple instances of a parallel\nmultigrid solver combined with advanced load balancing techniques. In\nparticular, we optimize the concurrent execution across the three layers of the\nMLMC method: parallelization across levels, across samples, and across the\nspatial grid. The overall efficiency and performance of these methods will be\nanalyzed. Here the scalability window of the multigrid solver is revealed as\nbeing essential, i.e., the property that the solution can be computed with a\nrange of process numbers while maintaining good parallel efficiency. We\nevaluate the new scheduling strategies in a series of numerical tests, and\nconclude the paper demonstrating large 3D scaling experiments.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 07:47:45 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Gmeiner", "Bj\u00f6rn", ""], ["Drzisga", "Daniel", ""], ["Ruede", "Ulrich", ""], ["Scheichl", "Robert", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1607.03463", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "comments": "first circulated on May 20, 2016; this newer version improves writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-SVD that is to obtain the first $k$ singular vectors of a matrix\n$A$. Recently, a few breakthroughs have been discovered on $k$-SVD: Musco and\nMusco [1] proved the first gap-free convergence result using the block Krylov\nmethod, Shamir [2] discovered the first variance-reduction stochastic method,\nand Bhojanapalli et al. [3] provided the fastest $O(\\mathsf{nnz}(A) +\n\\mathsf{poly}(1/\\varepsilon))$-time algorithm using alternating minimization.\n  In this paper, we put forward a new and simple LazySVD framework to improve\nthe above breakthroughs. This framework leads to a faster gap-free method\noutperforming [1], and the first accelerated and stochastic method\noutperforming [2]. In the $O(\\mathsf{nnz}(A) + \\mathsf{poly}(1/\\varepsilon))$\nrunning-time regime, LazySVD outperforms [3] in certain parameter regimes\nwithout even using alternating minimization.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 18:41:52 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 18:55:31 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.03592", "submitter": "Ahmed Attia", "authors": "Ahmed Attia, Azam Moosavi, Adrian Sandu", "title": "Cluster Sampling Filters for Non-Gaussian Data Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fully non-Gaussian version of the Hamiltonian Monte\nCarlo (HMC) sampling filter. The Gaussian prior assumption in the original HMC\nfilter is relaxed. Specifically, a clustering step is introduced after the\nforecast phase of the filter, and the prior density function is estimated by\nfitting a Gaussian Mixture Model (GMM) to the prior ensemble. Using the data\nlikelihood function, the posterior density is then formulated as a mixture\ndensity, and is sampled using a HMC approach (or any other scheme capable of\nsampling multimodal densities in high-dimensional subspaces). The main filter\ndeveloped herein is named \"cluster HMC sampling filter\" (ClHMC). A multi-chain\nversion of the ClHMC filter, namely MC-ClHMC is also proposed to guarantee that\nsamples are taken from the vicinities of all probability modes of the\nformulated posterior. The new methodologies are tested using a\nquasi-geostrophic (QG) model with double-gyre wind forcing and bi-harmonic\nfriction. Numerical results demonstrate the usefulness of using GMMs to relax\nthe Gaussian prior assumption in the HMC filtering paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 04:37:31 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 15:14:18 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Attia", "Ahmed", ""], ["Moosavi", "Azam", ""], ["Sandu", "Adrian", ""]]}, {"id": "1607.03936", "submitter": "Johann Rudi", "authors": "Johann Rudi, Georg Stadler, Omar Ghattas", "title": "Weighted BFBT Preconditioner for Stokes Flow Problems with Highly\n  Heterogeneous Viscosity", "comments": "To appear in SIAM Journal on Scientific Computing", "journal-ref": "SIAM Journal on Scientific Computing, 39(5), S272-S297, 2017", "doi": "10.1137/16M108450X", "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a weighted BFBT approximation (w-BFBT) to the inverse Schur\ncomplement of a Stokes system with highly heterogeneous viscosity. When used as\npart of a Schur complement-based Stokes preconditioner, we observe robust fast\nconvergence for Stokes problems with smooth but highly varying (up to 10 orders\nof magnitude) viscosities, optimal algorithmic scalability with respect to mesh\nrefinement, and only a mild dependence on the polynomial order of high-order\nfinite element discretizations ($Q_k \\times P_{k-1}^{disc}$, order $k \\ge 2$).\nFor certain difficult problems, we demonstrate numerically that w-BFBT\nsignificantly improves Stokes solver convergence over the widely used inverse\nviscosity-weighted pressure mass matrix approximation of the Schur complement.\nIn addition, we derive theoretical eigenvalue bounds to prove spectral\nequivalence of w-BFBT. Using detailed numerical experiments, we discuss\nmodifications to w-BFBT at Dirichlet boundaries that decrease the number of\niterations. The overall algorithmic performance of the Stokes solver is\ngoverned by the efficacy of w-BFBT as a Schur complement approximation and, in\naddition, by our parallel hybrid spectral-geometric-algebraic multigrid (HMG)\nmethod, which we use to approximate the inverses of the viscous block and\nvariable-coefficient pressure Poisson operators within w-BFBT. Building on the\nscalability of HMG, our Stokes solver achieves a parallel efficiency of 90%\nwhile weak scaling over a more than 600-fold increase from 48 to all 30,000\ncores of TACC's Lonestar 5 supercomputer.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 21:27:55 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 03:35:30 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Rudi", "Johann", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1607.03944", "submitter": "Philippe G. LeFloch", "authors": "Jan Giesselmann and Philippe G. LeFloch", "title": "Formulation and convergence of the finite volume method for conservation\n  laws on spacetimes with boundary", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonlinear hyperbolic conservation laws posed on a differential\n(n+1)-manifold with boundary referred to as a spacetime, and defined from a\nprescribed flux field of n-forms depending on a parameter (the unknown\nvariable), a class of equations proposed by LeFloch and Okutmustur in 2008. Our\nmain result is a proof of the convergence of the finite volume method for weak\nsolutions satisfying suitable entropy inequalities. A main difference with\nprevious work is that we allow for slices with a boundary and, in addition,\nintroduce a new formulation of the finite volume method involving the notion of\ntotal flux functions. Under a natural global hyperbolicity condition on the\nflux field and the spacetime and by assuming that the spacetime admits a\nfoliation by compact slices with boundary, we establish an existence and\nuniqueness theory for the initial and boundary value problem, and we prove a\ncontraction property in a geometrically natural L1-type distance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 22:09:20 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 14:51:45 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Giesselmann", "Jan", ""], ["LeFloch", "Philippe G.", ""]]}, {"id": "1607.04499", "submitter": "Wayne Eberly", "authors": "Wayne Eberly", "title": "Selecting Algorithms for Black Box Matrices: Checking for Matrix\n  Properties That Can Simplify Computations", "comments": "Department of Computer Science Technical Report 2016-1085-04", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processes to automate the selection of appropriate algorithms for various\nmatrix computations are described. In particular, processes to check for, and\ncertify, various matrix properties of black box matrices are presented. These\ninclude sparsity patterns and structural properties that allow \"superfast\"\nalgorithms to be used in place of black-box algorithms. Matrix properties that\nhold generically, and allow the use of matrix preconditioning to be reduced or\neliminated, can also be checked for and certified - notably including in the\nsmall-field case, where this presently has the greatest impact on the\nefficiency of the computation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 13:17:30 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 20:08:14 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Eberly", "Wayne", ""]]}, {"id": "1607.04514", "submitter": "Wayne Eberly", "authors": "Wayne Eberly", "title": "Black Box Linear Algebra: Extending Wiedemann's Analysis of a Sparse\n  Matrix Preconditioner for Computations over Small Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wiedemann's paper, introducing his algorithm for sparse and structured matrix\ncomputations over arbitrary fields, also presented a pair of matrix\npreconditioners for computations over small fields. The analysis of the second\nof these is extended in order to provide more explicit statements of the\nexpected number of nonzero entries in the matrices obtained as well as bounds\non the probability that such matrices have maximal rank.\n  This is part of ongoing work to establish that this matrix preconditioner can\nalso be used to bound the number of nontrivial nilpotent blocks in the Jordan\nnormal form of a preconditioned matrix, in such a way that one can also sample\nuniformly from the null space of the originally given matrix. If successful\nthis will result in a black box algorithm for the type of matrix computation\nrequired when using the number field sieve for integer factorization that is\nprovably reliable and - by a small factor - asymptotically more efficient than\nalternative techniques that make use of other matrix preconditioners or require\ncomputations over field extensions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 14:03:49 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Eberly", "Wayne", ""]]}, {"id": "1607.05073", "submitter": "Christos Chatzichristos", "authors": "Christos Chatzichristos, Eleftherios Kofidis, Giannis Kopsinis,\n  Sergios Theodoridis", "title": "Higher-Order Block Term Decomposition for Spatially Folded fMRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing use of neuroimaging technologies generates a massive amount of\nbiomedical data that exhibit high dimensionality. Tensor-based analysis of\nbrain imaging data has been proved quite effective in exploiting their multiway\nnature. The advantages of tensorial methods over matrix-based approaches have\nalso been demonstrated in the characterization of functional magnetic resonance\nimaging (fMRI) data, where the spatial (voxel) dimensions are commonly grouped\n(unfolded) as a single way/mode of the 3-rd order array, the other two ways\ncorresponding to time and subjects. However, such methods are known to be\nineffective in more demanding scenarios, such as the ones with strong noise\nand/or significant overlapping of activated regions. This paper aims at\ninvestigating the possible gains from a better exploitation of the spatial\ndimension, through a higher- (4 or 5) order tensor modeling of the fMRI signal.\nIn this context, and in order to increase the degrees of freedom of the\nmodeling process, a higher-order Block Term Decomposition (BTD) is applied, for\nthe first time in fMRI analysis. Its effectiveness is demonstrated via\nextensive simulation results.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:28:48 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Chatzichristos", "Christos", ""], ["Kofidis", "Eleftherios", ""], ["Kopsinis", "Giannis", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1607.06303", "submitter": "Oded Schwartz", "authors": "Vadim Stotland, Oded Schwartz, and Sivan Toledo", "title": "High-Performance Algorithms for Computing the Sign Function of\n  Triangular Matrices", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithms and implementations for computing the sign function of a\ntriangular matrix are fundamental building blocks in algorithms for computing\nthe sign of arbitrary square real or complex matrices. We present novel\nrecursive and cache efficient algorithms that are based on Higham's stabilized\nspecialization of Parlett's substitution algorithm for computing the sign of a\ntriangular matrix. We show that the new recursive algorithms are asymptotically\noptimal in terms of the number of cache misses that they generate. One of the\nnovel algorithms that we present performs more arithmetic than the\nnon-recursive version, but this allows it to benefit from calling\nhighly-optimized matrix-multiplication routines; the other performs the same\nnumber of operations as the non-recursive version, but it uses custom\ncomputational kernels instead. We present implementations of both, as well as a\ncache-efficient implementation of a block version of Parlett's algorithm. Our\nexperiments show that the blocked and recursive versions are much faster than\nthe previous algorithms, and that the inertia strongly influences their\nrelative performance, as predicted by our analysis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 12:58:36 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Stotland", "Vadim", ""], ["Schwartz", "Oded", ""], ["Toledo", "Sivan", ""]]}, {"id": "1607.06834", "submitter": "Arash Sarshar", "authors": "Arash Sarshar, Paul Tranquilli, Brent Pickering, Andrew McCall, Adrian\n  Sandu and Christopher J. Roy", "title": "A Numerical Investigation of Matrix-Free Implicit Time-Stepping Methods\n  for Large CFD Simulations", "comments": null, "journal-ref": "Computers & Fluids, Volume 159, 15 Dec. 2017, PP. 53-63", "doi": "10.1016/j.compfluid.2017.09.014", "report-no": "Computational Science Lab CSL-TR-16-6", "categories": "cs.CE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the development and testing of advanced\ntime-stepping methods suited for the integration of time-accurate, real-world\napplications of computational fluid dynamics (CFD). The performance of several\ntime discretization methods is studied numerically with regards to\ncomputational efficiency, order of accuracy, and stability, as well as the\nability to treat effectively stiff problems. We consider matrix-free\nimplementations, a popular approach for time-stepping methods applied to large\nCFD applications due to its adherence to scalable matrix-vector operations and\na small memory footprint. We compare explicit methods with matrix-free\nimplementations of implicit, linearly-implicit, as well as Rosenbrock-Krylov\nmethods. We show that Rosenbrock-Krylov methods are competitive with existing\ntechniques excelling for a number of problem types and settings.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 20:27:06 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 21:06:33 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Sarshar", "Arash", ""], ["Tranquilli", "Paul", ""], ["Pickering", "Brent", ""], ["McCall", "Andrew", ""], ["Sandu", "Adrian", ""], ["Roy", "Christopher J.", ""]]}, {"id": "1607.07607", "submitter": "Gianna Maria Del Corso", "authors": "Gianna M. Del Corso and Francesco Romani", "title": "Adaptive Nonnegative Matrix Factorization and Measure Comparisons for\n  Recommender Systems", "comments": null, "journal-ref": "Applied Mathematics and Computation 354, pp. 164-179, 2019", "doi": "10.1016/j.amc.2019.01.047", "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nonnegative Matrix Factorization (NMF) of the rating matrix has shown to\nbe an effective method to tackle the recommendation problem. In this paper we\npropose new methods based on the NMF of the rating matrix and we compare them\nwith some classical algorithms such as the SVD and the regularized and\nunregularized non-negative matrix factorization approach. In particular a new\nalgorithm is obtained changing adaptively the function to be minimized at each\nstep, realizing a sort of dynamic prior strategy. Another algorithm is obtained\nmodifying the function to be minimized in the NMF formulation by enforcing the\nreconstruction of the unknown ratings toward a prior term. We then combine\ndifferent methods obtaining two mixed strategies which turn out to be very\neffective in the reconstruction of missing observations. We perform a\nthoughtful comparison of different methods on the basis of several evaluation\nmeasures. We consider in particular rating, classification and ranking measures\nshowing that the algorithm obtaining the best score for a given measure is in\ngeneral the best also when different measures are considered, lowering the\ninterest in designing specific evaluation measures. The algorithms have been\ntested on different datasets, in particular the 1M, and 10M MovieLens datasets\ncontaining ratings on movies, the Jester dataset with ranting on jokes and\nAmazon Fine Foods dataset with ratings on foods. The comparison of the\ndifferent algorithms, shows the good performance of methods employing both an\nexplicit and an implicit regularization scheme. Moreover we can get a boost by\nmixed strategies combining a fast method with a more accurate one.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 09:26:20 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 10:06:23 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 09:11:05 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Del Corso", "Gianna M.", ""], ["Romani", "Francesco", ""]]}, {"id": "1607.07702", "submitter": "J. Nathan Kutz", "authors": "Syuzanna Sargsyan, Steven L. Brunton, and J. Nathan Kutz", "title": "Online interpolation point refinement for reduced order models using a\n  genetic algorithm", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA nlin.PS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A genetic algorithm procedure is demonstrated that refines the selection of\ninterpolation points of the discrete empirical interpolation method (DEIM) when\nused for constructing reduced order models for time dependent and/or\nparametrized nonlinear partial differential equations (PDEs) with proper\northogonal decomposition. The method achieves nearly optimal interpolation\npoints with only a few generations of the search, making it potentially useful\nfor {\\em online} refinement of the sparse sampling used to construct a\nprojection of the nonlinear terms. With the genetic algorithm, points are\noptimized to jointly minimize reconstruction error and enable dynamic regime\nclassification. The efficiency of the method is demonstrated on two canonical\nnonlinear PDEs: the cubic-quintic Ginzburg-Landau equation and the\nNavier-Stokes equation for flow around a cylinder. Using the former model, the\nprocedure can be compared to the ground-truth optimal interpolation points,\nshowing that the genetic algorithm quickly achieves nearly optimal performance\nand reduced the reconstruction error by nearly an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 22:57:37 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Sargsyan", "Syuzanna", ""], ["Brunton", "Steven L.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1607.08012", "submitter": "Quanming Yao", "authors": "Quanming Yao and James T. Kwok", "title": "Learning of Generalized Low-Rank Models: A Greedy Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning of low-rank matrices is fundamental to many machine learning\napplications. A state-of-the-art algorithm is the rank-one matrix pursuit\n(R1MP). However, it can only be used in matrix completion problems with the\nsquare loss. In this paper, we develop a more flexible greedy algorithm for\ngeneralized low-rank models whose optimization objective can be smooth or\nnonsmooth, general convex or strongly convex. The proposed algorithm has low\nper-iteration time complexity and fast convergence rate. Experimental results\nshow that it is much faster than the state-of-the-art, with comparable or even\nbetter prediction performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 09:18:25 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""]]}, {"id": "1607.08083", "submitter": "Olivier Pironneau", "authors": "Olivier Pironneau (LJLL)", "title": "An Energy stable Monolithic Eulerian Fluid-Structure Numerical Scheme *", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA math.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conservation laws of continuum mechanic written in an Eulerian frame make\nno difference between fluids and solids except in the expression of the stress\ntensors, usually with Newton's hypothesis for the fluids and Helmholtz\npotentials of energy for hyperelastic solids. By taking the velocities as\nunknown , monolithic methods for fluid structure interactions (FSI) are built.\nIn this article such a formulation is analyzed when the fluid is compressible\nand the fluid is incompressible. The idea is not new but the progress of mesh\ngenerators and numerical schemes like the Characteristics-Galerkin method\nrender this approach feasible and reasonably robust. In this article the method\nand its discretization are presented, stability is discussed by through an\nenergy estimate. A numerical section discusses implementation issues and\npresents a few simple tests.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:23:05 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 07:43:33 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Pironneau", "Olivier", "", "LJLL"]]}, {"id": "1607.08376", "submitter": "Maria Charina", "authors": "Maria Charina, Costanza Conti, Mariantonia Cotronei, Mihai Putinar", "title": "System theory and orthogonal multi-wavelets", "comments": null, "journal-ref": "J. Approx. Theory, 238 (2019), 85-102", "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a complete and unifying characterization of\ncompactly supported univariate scalar orthogonal wavelets and vector-valued or\nmatrix-valued orthogonal multi-wavelets. This characterization is based on\nclassical results from system theory and basic linear algebra. In particular,\nwe show that the corresponding wavelet and multi-wavelet masks are identified\nwith a transfer function $$\n  F(z)=A+B z (I-Dz)^{-1} \\, C, \\quad z \\in \\mathbb{D}=\\{z \\in \\mathbb{C} \\ : \\\n|z| < 1\\}, $$ of a conservative linear system. The complex matrices $A,\\ B, \\\nC, \\ D$ define a block circulant unitary matrix. Our results show that there\nare no intrinsic differences between the elegant wavelet construction by\nDaubechies or any other construction of vector-valued or matrix-valued\nmulti-wavelets. The structure of the unitary matrix defined by $A,\\ B, \\ C, \\\nD$ allows us to parametrize in a systematic way all classes of possible wavelet\nand multi-wavelet masks together with the masks of the corresponding refinable\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 09:39:37 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 08:19:36 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Charina", "Maria", ""], ["Conti", "Costanza", ""], ["Cotronei", "Mariantonia", ""], ["Putinar", "Mihai", ""]]}, {"id": "1607.08712", "submitter": "Samrat Mukhopadhyay", "authors": "Samrat Mukhopadhyay, Prateek Vashishtha and, Mrityunjoy Chakraborty", "title": "Signal Recovery in Uncorrelated and Correlated Dictionaries Using\n  Orthogonal Least Squares", "comments": "18 Pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NA math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though the method of least squares has been used for a long time in solving\nsignal processing problems, in the recent field of sparse recovery from\ncompressed measurements, this method has not been given much attention. In this\npaper we show that a method in the least squares family, known in the\nliterature as Orthogonal Least Squares (OLS), adapted for compressed recovery\nproblems, has competitive recovery performance and computation complexity, that\nmakes it a suitable alternative to popular greedy methods like Orthogonal\nMatching Pursuit (OMP). We show that with a slight modification, OLS can\nexactly recover a $K$-sparse signal, embedded in an $N$ dimensional space\n($K<<N$) in $M=\\mathcal{O}(K\\log (N/K))$ no of measurements with Gaussian\ndictionaries. We also show that OLS can be easily implemented in such a way\nthat it requires $\\mathcal{O}(KMN)$ no of floating point operations similar to\nthat of OMP. In this paper performance of OLS is also studied with sensing\nmatrices with correlated dictionary, in which algorithms like OMP does not\nexhibit good recovery performance. We study the recovery performance of OLS in\na specific dictionary called \\emph{generalized hybrid dictionary}, which is\nshown to be a correlated dictionary, and show numerically that OLS has is far\nsuperior to OMP in these kind of dictionaries in terms of recovery performance.\nFinally we provide analytical justifications that corroborate the findings in\nthe numerical illustrations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 07:42:09 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Mukhopadhyay", "Samrat", ""], ["and", "Prateek Vashishtha", ""], ["Chakraborty", "Mrityunjoy", ""]]}]