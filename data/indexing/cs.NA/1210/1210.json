[{"id": "1210.0577", "submitter": "Scott Field", "authors": "Harbir Antil, Scott E. Field, Frank Herrmann, Ricardo H. Nochetto,\n  Manuel Tiglio", "title": "Two-step greedy algorithm for reduced order quadratures", "comments": "27 pages, 9 figures, uses svjour3", "journal-ref": "Journal of Scientific Computing, December 2013, Volume 57, Issue\n  3, pp 604-637", "doi": "10.1007/s10915-013-9722-z", "report-no": null, "categories": "cs.NA gr-qc math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to generate application-specific, global reduced\norder quadratures (ROQ) for multiple fast evaluations of weighted inner\nproducts between parameterized functions. If a reduced basis (RB) or any other\nprojection-based model reduction technique is applied, the dimensionality of\nintegrands is reduced dramatically; however, the cost of approximating the\nintegrands by projection still scales as the size of the original problem. In\ncontrast, using discrete empirical interpolation (DEIM) points as ROQ nodes\nleads to a computational cost which depends linearly on the dimension of the\nreduced space. Generation of a reduced basis via a greedy procedure requires a\ntraining set, which for products of functions can be very large. Since this\ndirect approach can be impractical in many applications, we propose instead a\ntwo-step greedy targeted towards approximation of such products. We present\nnumerical experiments demonstrating the accuracy and the efficiency of the\ntwo-step approach. The presented ROQ are expected to display very fast\nconvergence whenever there is regularity with respect to parameter variation.\nWe find that for the particular application here considered, one driven by\ngravitational wave physics, the two-step approach speeds up the offline\ncomputations to build the ROQ by more than two orders of magnitude.\nFurthermore, the resulting ROQ rule is found to converge exponentially with the\nnumber of nodes, and a factor of ~50 savings, without loss of accuracy, is\nobserved in evaluations of inner products when ROQ are used as a downsampling\nstrategy for equidistant samples using the trapezoidal rule. While the primary\nfocus of this paper is on quadrature rules for inner products of parameterized\nfunctions, our method can be easily adapted to integrations of single\nparameterized functions, and some examples of this type are considered.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 21:07:13 GMT"}, {"version": "v2", "created": "Sun, 19 May 2013 07:13:39 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Antil", "Harbir", ""], ["Field", "Scott E.", ""], ["Herrmann", "Frank", ""], ["Nochetto", "Ricardo H.", ""], ["Tiglio", "Manuel", ""]]}, {"id": "1210.1885", "submitter": "Varun Shankar", "authors": "Varun Shankar, Grady B. Wright, Aaron L. Fogelson and R. M. Kirby", "title": "A Study of Different Modeling Choices For Simulating Platelets Within\n  the Immersed Boundary Method", "comments": "33 pages, 17 figures, Accepted (in press) by APNUM", "journal-ref": null, "doi": "10.1016/j.apnum.2012.09.006", "report-no": null, "categories": "math.NA cs.NA math.DG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Immersed Boundary (IB) method is a widely-used numerical methodology for\nthe simulation of fluid-structure interaction problems. The IB method utilizes\nan Eulerian discretization for the fluid equations of motion while maintaining\na Lagrangian representation of structural objects. Operators are defined for\ntransmitting information (forces and velocities) between these two\nrepresentations. Most IB simulations represent their structures with\npiecewise-linear approximations and utilize Hookean spring models to\napproximate structural forces. Our specific motivation is the modeling of\nplatelets in hemodynamic flows. In this paper, we study two alternative\nrepresentations - radial basis functions (RBFs) and Fourier-based\n(trigonometric polynomials and spherical harmonics) representations - for the\nmodeling of platelets in two and three dimensions within the IB framework, and\ncompare our results with the traditional piecewise-linear approximation\nmethodology. For different representative shapes, we examine the geometric\nmodeling errors (position and normal vectors), force computation errors, and\ncomputational cost and provide an engineering trade-off strategy for when and\nwhy one might select to employ these different representations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 22:56:13 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Shankar", "Varun", ""], ["Wright", "Grady B.", ""], ["Fogelson", "Aaron L.", ""], ["Kirby", "R. M.", ""]]}, {"id": "1210.3039", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Sequential Convex Programming Methods for A Class of Structured\n  Nonlinear Programming", "comments": "This paper has been withdrawn by the author due to major revision and\n  corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a broad class of structured nonlinear programming\n(SNLP) problems. In particular, we first establish the first-order optimality\nconditions for them. Then we propose sequential convex programming (SCP)\nmethods for solving them in which each iteration is obtained by solving a\nconvex programming problem exactly or inexactly. Under some suitable\nassumptions, we establish that any accumulation point of the sequence generated\nby the methods is a KKT point of the SNLP problems. In addition, we propose a\nvariant of the exact SCP method for SNLP in which nonmonotone scheme and\n\"local\" Lipschitz constants of the associated functions are used. And a similar\nconvergence result as mentioned above is established.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 20:01:29 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 17:31:12 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1210.3709", "submitter": "Weimin Miao", "authors": "Weimin Miao, Shaohua Pan and Defeng Sun", "title": "A Rank-Corrected Procedure for Matrix Completion with Fixed Basis\n  Coefficients", "comments": "51 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problems of low-rank matrix completion, the efficiency of the\nwidely-used nuclear norm technique may be challenged under many circumstances,\nespecially when certain basis coefficients are fixed, for example, the low-rank\ncorrelation matrix completion in various fields such as the financial market\nand the low-rank density matrix completion from the quantum state tomography.\nTo seek a solution of high recovery quality beyond the reach of the nuclear\nnorm, in this paper, we propose a rank-corrected procedure using a nuclear\nsemi-norm to generate a new estimator. For this new estimator, we establish a\nnon-asymptotic recovery error bound. More importantly, we quantify the\nreduction of the recovery error bound for this rank-corrected procedure.\nCompared with the one obtained for the nuclear norm penalized least squares\nestimator, this reduction can be substantial (around 50%). We also provide\nnecessary and sufficient conditions for rank consistency in the sense of Bach\n(2008). Very interestingly, these conditions are highly related to the concept\nof constraint nondegeneracy in matrix optimization. As a byproduct, our results\nprovide a theoretical foundation for the majorized penalty method of Gao and\nSun (2010) and Gao (2010) for structured low-rank matrix optimization problems.\nExtensive numerical experiments demonstrate that our proposed rank-corrected\nprocedure can simultaneously achieve a high recovery accuracy and capture the\nlow-rank structure.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2012 14:22:27 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 21:44:16 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 17:14:03 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Miao", "Weimin", ""], ["Pan", "Shaohua", ""], ["Sun", "Defeng", ""]]}, {"id": "1210.4081", "submitter": "Bogdan Savchynskyy", "authors": "Bogdan Savchynskyy and Stefan Schmidt", "title": "Getting Feasible Variable Estimates From Infeasible Ones: MRF Local\n  Polytope Study", "comments": "20 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for construction of approximate feasible primal\nsolutions from dual ones for large-scale optimization problems possessing\ncertain separability properties. Whereas infeasible primal estimates can\ntypically be produced from (sub-)gradients of the dual function, it is often\nnot easy to project them to the primal feasible set, since the projection\nitself has a complexity comparable to the complexity of the initial problem. We\npropose an alternative efficient method to obtain feasibility and show that its\nproperties influencing the convergence to the optimum are similar to the\nproperties of the Euclidean projection. We apply our method to the local\npolytope relaxation of inference problems for Markov Random Fields and\ndemonstrate its superiority over existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 15:55:34 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Savchynskyy", "Bogdan", ""], ["Schmidt", "Stefan", ""]]}, {"id": "1210.4539", "submitter": "Michael Baudin", "authors": "Michael Baudin, Robert L. Smith", "title": "A Robust Complex Division in Scilab", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most widely used algorithm for floating point complex division, known as\nSmith's method, may fail more often than expected. This document presents two\nimproved complex division algorithms. We present a proof of the robustness of\nthe first improved algorithm. Numerical simulations show that this algorithm\nperforms well in practice and is significantly more robust than other known\nimplementations. By combining additionnal scaling methods with this first\nalgorithm, we were able to create a second algorithm, which rarely fails.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 19:52:16 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2012 17:23:43 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Baudin", "Michael", ""], ["Smith", "Robert L.", ""]]}, {"id": "1210.4883", "submitter": "Leonard K. M. Poon", "authors": "Leonard K. M. Poon, April H. Liu, Tengfei Liu, Nevin Lianwen Zhang", "title": "A Model-Based Approach to Rounding in Spectral Clustering", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-685-694", "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spectral clustering, one defines a similarity matrix for a collection of\ndata points, transforms the matrix to get the Laplacian matrix, finds the\neigenvectors of the Laplacian matrix, and obtains a partition of the data using\nthe leading eigenvectors. The last step is sometimes referred to as rounding,\nwhere one needs to decide how many leading eigenvectors to use, to determine\nthe number of clusters, and to partition the data points. In this paper, we\npropose a novel method for rounding. The method differs from previous methods\nin three ways. First, we relax the assumption that the number of clusters\nequals the number of eigenvectors used. Second, when deciding the number of\nleading eigenvectors to use, we not only rely on information contained in the\nleading eigenvectors themselves, but also use subsequent eigenvectors. Third,\nour method is model-based and solves all the three subproblems of rounding\nusing a class of graphical models called latent tree models. We evaluate our\nmethod on both synthetic and real-world data. The results show that our method\nworks correctly in the ideal case where between-clusters similarity is 0, and\ndegrades gracefully as one moves away from the ideal case.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:45:11 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Poon", "Leonard K. M.", ""], ["Liu", "April H.", ""], ["Liu", "Tengfei", ""], ["Zhang", "Nevin Lianwen", ""]]}, {"id": "1210.5034", "submitter": "Pierre Machart", "authors": "Pierre Machart (LIF, LSIS), Sandrine Anthoine (LATP), Luca Baldassarre\n  (EPFL)", "title": "Optimal Computational Trade-Off of Inexact Proximal Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the trade-off between convergence rate and\ncomputational cost when minimizing a composite functional with\nproximal-gradient methods, which are popular optimisation tools in machine\nlearning. We consider the case when the proximity operator is computed via an\niterative procedure, which provides an approximation of the exact proximity\noperator. In that case, we obtain algorithms with two nested loops. We show\nthat the strategy that minimizes the computational cost to reach a solution\nwith a desired accuracy in finite time is to set the number of inner iterations\nto a constant, which differs from the strategy indicated by a convergence rate\nanalysis. In the process, we also present a new procedure called SIP (that is\nSpeedy Inexact Proximal-gradient algorithm) that is both computationally\nefficient and easy to implement. Our numerical experiments confirm the\ntheoretical findings and suggest that SIP can be a very competitive alternative\nto the standard procedure.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 06:27:10 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2012 06:17:08 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Machart", "Pierre", "", "LIF, LSIS"], ["Anthoine", "Sandrine", "", "LATP"], ["Baldassarre", "Luca", "", "EPFL"]]}, {"id": "1210.5290", "submitter": "Kalyana Babu Nakshatrala", "authors": "K. B. Nakshatrala, M. K. Mudunuru, A. J. Valocchi", "title": "A numerical framework for diffusion-controlled bimolecular-reactive\n  systems to enforce maximum principles and non-negative constraint", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2013.07.010", "report-no": null, "categories": "cs.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel computational framework for diffusive-reactive systems\nthat satisfies the non-negative constraint and maximum principles on general\ncomputational grids. The governing equations for the concentration of reactants\nand product are written in terms of tensorial diffusion-reaction equations. %\nWe restrict our studies to fast irreversible bimolecular reactions. If one\nassumes that the reaction is diffusion-limited and all chemical species have\nthe same diffusion coefficient, one can employ a linear transformation to\nrewrite the governing equations in terms of invariants, which are unaffected by\nthe reaction. This results in two uncoupled tensorial diffusion equations in\nterms of these invariants, which are solved using a novel non-negative solver\nfor tensorial diffusion-type equations. The concentrations of the reactants and\nthe product are then calculated from invariants using algebraic manipulations.\nThe novel aspect of the proposed computational framework is that it will always\nproduce physically meaningful non-negative values for the concentrations of all\nchemical species. Several representative numerical examples are presented to\nillustrate the robustness, convergence, and the numerical performance of the\nproposed computational framework. We will also compare the proposed framework\nwith other popular formulations. In particular, we will show that the Galerkin\nformulation (which is the standard single-field formulation) does not produce\nreliable solutions, and the reason can be attributed to the fact that the\nsingle-field formulation does not guarantee non-negative solutions. We will\nalso show that the clipping procedure (which produces non-negative solutions\nbut is considered as a variational crime) does not give accurate results when\ncompared with the proposed computational framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 01:06:02 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2012 19:02:39 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2013 21:20:21 GMT"}, {"version": "v4", "created": "Sat, 27 Jul 2013 22:21:28 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Nakshatrala", "K. B.", ""], ["Mudunuru", "M. K.", ""], ["Valocchi", "A. J.", ""]]}, {"id": "1210.5844", "submitter": "Nelly Pustelnik", "authors": "Giovanni Chierchia, Nelly Pustelnik, Jean-Christophe Pesquet, and\n  B\\'eatrice Pesquet-Popescu", "title": "Epigraphical splitting for solving constrained convex formulations of\n  inverse problems with proximal tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a proximal approach to deal with a class of convex variational\nproblems involving nonlinear constraints. A large family of constraints, proven\nto be effective in the solution of inverse problems, can be expressed as the\nlower level set of a sum of convex functions evaluated over different, but\npossibly overlapping, blocks of the signal. For such constraints, the\nassociated projection operator generally does not have a simple form. We\ncircumvent this difficulty by splitting the lower level set into as many\nepigraphs as functions involved in the sum. A closed half-space constraint is\nalso enforced, in order to limit the sum of the introduced epigraphical\nvariables to the upper bound of the original lower level set. In this paper, we\nfocus on a family of constraints involving linear transforms of distance\nfunctions to a convex set or $\\ell_{1,p}$ norms with $p\\in \\{1,2,\\infty\\}$. In\nthese cases, the projection onto the epigraph of the involved function has a\nclosed form expression.\n  The proposed approach is validated in the context of image restoration with\nmissing samples, by making use of constraints based on Non-Local Total\nVariation. Experiments show that our method leads to significant improvements\nin term of convergence speed over existing algorithms for solving similar\nconstrained problems. A second application to a pulse shape design problem is\nprovided in order to illustrate the flexibility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 09:12:48 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2013 08:27:08 GMT"}, {"version": "v3", "created": "Thu, 20 Mar 2014 12:32:02 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Chierchia", "Giovanni", ""], ["Pustelnik", "Nelly", ""], ["Pesquet", "Jean-Christophe", ""], ["Pesquet-Popescu", "B\u00e9atrice", ""]]}, {"id": "1210.6266", "submitter": "Rahul Sampath", "authors": "Rahul S. Sampath, Bobby Philip, Srikanth Allu and Srdjan Simunovic", "title": "Recursive Schur Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a parallel recursive algorithm based on\nmulti-level domain decomposition that can be used as a precondtioner to a\nKrylov subspace method to solve sparse linear systems of equations arising from\nthe discretization of partial differential equations (PDEs). We tested the\neffectiveness of the algorithm on several PDEs using different number of\nsub-domains (ranging from 8 to 32768) and various problem sizes (ranging from\nabout 2000 to over a billion degrees of freedom). We report the results from\nthese tests; the results show that the algorithm scales very well with the\nnumber of sub-domains.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 15:19:13 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Sampath", "Rahul S.", ""], ["Philip", "Bobby", ""], ["Allu", "Srikanth", ""], ["Simunovic", "Srdjan", ""]]}, {"id": "1210.7292", "submitter": "Matthias Messner", "authors": "Matthias Messner (INRIA Bordeaux - Sud-Ouest), B\\'erenger Bramas\n  (INRIA Bordeaux - Sud-Ouest), Olivier Coulaud (INRIA Bordeaux - Sud-Ouest),\n  Eric Darve", "title": "Optimized M2L Kernels for the Chebyshev Interpolation based Fast\n  Multipole Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CE cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast multipole method (FMM) for asymptotically smooth kernel functions\n(1/r, 1/r^4, Gauss and Stokes kernels, radial basis functions, etc.) based on a\nChebyshev interpolation scheme has been introduced in [Fong et al., 2009]. The\nmethod has been extended to oscillatory kernels (e.g., Helmholtz kernel) in\n[Messner et al., 2012]. Beside its generality this FMM turns out to be\nfavorable due to its easy implementation and its high performance based on\nintensive use of highly optimized BLAS libraries. However, one of its\nbottlenecks is the precomputation of the multiple-to-local (M2L) operator, and\nits higher number of floating point operations (flops) compared to other FMM\nformulations. Here, we present several optimizations for that operator, which\nis known to be the costliest FMM operator. The most efficient ones do not only\nreduce the precomputation time by a factor up to 340 but they also speed up the\nmatrix-vector product. We conclude with comparisons and numerical validations\nof all presented optimizations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2012 05:46:13 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2012 17:46:07 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Messner", "Matthias", "", "INRIA Bordeaux - Sud-Ouest"], ["Bramas", "B\u00e9renger", "", "INRIA Bordeaux - Sud-Ouest"], ["Coulaud", "Olivier", "", "INRIA Bordeaux - Sud-Ouest"], ["Darve", "Eric", ""]]}, {"id": "1210.7858", "submitter": "Bahman Kalantari", "authors": "Bahman Kalantari", "title": "Solving Linear System of Equations Via A Convex Hull Algorithm", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new iterative algorithms for solving a square linear system $Ax=b$\nin dimension $n$ by employing the {\\it Triangle Algorithm} \\cite{kal12}, a\nfully polynomial-time approximation scheme for testing if the convex hull of a\nfinite set of points in a Euclidean space contains a given point. By converting\n$Ax=b$ into a convex hull problem and solving via the Triangle Algorithm,\ntogether with a {\\it sensitivity theorem}, we compute in $O(n^2\\epsilon^{-2})$\narithmetic operations an approximate solution satisfying $\\Vert Ax_\\epsilon - b\n\\Vert \\leq \\epsilon \\rho$, where $\\rho= \\max \\{\\Vert a_1 \\Vert,..., \\Vert a_n\n\\Vert, \\Vert b \\Vert \\}$, and $a_i$ is the $i$-th column of $A$. In another\napproach we apply the Triangle Algorithm incrementally, solving a sequence of\nconvex hull problems while repeatedly employing a {\\it distance duality}. The\nsimplicity and theoretical complexity bounds of the proposed algorithms,\nrequiring no structural restrictions on the matrix $A$, suggest their potential\npracticality, offering alternatives to the existing exact and iterative\nmethods, especially for large scale linear systems. The assessment of\ncomputational performance however is the subject of future experimentations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 22:46:02 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Kalantari", "Bahman", ""]]}, {"id": "1210.8072", "submitter": "{\\L}ukasz Struski", "authors": "{\\L}ukasz Struski and Jacek Tabor", "title": "Strict localization of eigenvectors and eigenvalues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we show and implement a simple and effcient method to\nstrictly locate eigenvectors and eigenvalues of a given matrix, based on the\nmodified cone condition. As a consequence we can also effectively localize\nzeros of complex polynomials.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 16:31:08 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Struski", "\u0141ukasz", ""], ["Tabor", "Jacek", ""]]}]