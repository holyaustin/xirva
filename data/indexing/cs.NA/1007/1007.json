[{"id": "1007.0380", "submitter": "Mithun Das Gupta", "authors": "Mithun Das Gupta", "title": "Additive Non-negative Matrix Factorization for Missing Data", "comments": "General extension of the NMF framework", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) has previously been shown to be a\nuseful decomposition for multivariate data. We interpret the factorization in a\nnew way and use it to generate missing attributes from test data. We provide a\njoint optimization scheme for the missing attributes as well as the NMF\nfactors. We prove the monotonic convergence of our algorithms. We present\nclassification results for cases with missing attributes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2010 17:40:01 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Gupta", "Mithun Das", ""]]}, {"id": "1007.3753", "submitter": "Allen Yang", "authors": "Allen Y. Yang, Zihan Zhou, Arvind Ganesh, S. Shankar Sastry, and Yi Ma", "title": "Fast L1-Minimization Algorithms For Robust Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L1-minimization refers to finding the minimum L1-norm solution to an\nunderdetermined linear system b=Ax. Under certain conditions as described in\ncompressive sensing theory, the minimum L1-norm solution is also the sparsest\nsolution. In this paper, our study addresses the speed and scalability of its\nalgorithms. In particular, we focus on the numerical implementation of a\nsparsity-based classification framework in robust face recognition, where\nsparse representation is sought to recover human identities from very\nhigh-dimensional facial images that may be corrupted by illumination, facial\ndisguise, and pose variation. Although the underlying numerical problem is a\nlinear program, traditional algorithms are known to suffer poor scalability for\nlarge-scale applications. We investigate a new solution based on a classical\nconvex optimization framework, known as Augmented Lagrangian Methods (ALM). The\nnew convex solvers provide a viable solution to real-world, time-critical\napplications such as face recognition. We conduct extensive experiments to\nvalidate and compare the performance of the ALM algorithms against several\npopular L1-minimization solvers, including interior-point method, Homotopy,\nFISTA, SESOP-PCD, approximate message passing (AMP) and TFOCS. To aid peer\nevaluation, the code for all the algorithms has been made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 20:26:26 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2010 10:15:09 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2012 05:22:51 GMT"}, {"version": "v4", "created": "Sun, 26 Aug 2012 23:17:25 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Yang", "Allen Y.", ""], ["Zhou", "Zihan", ""], ["Ganesh", "Arvind", ""], ["Sastry", "S. Shankar", ""], ["Ma", "Yi", ""]]}, {"id": "1007.3760", "submitter": "Satish  Karra", "authors": "Satish Karra and K. R. Rajagopal", "title": "Development of three dimensional constitutive theories based on lower\n  dimensional experimental data", "comments": "23 pages, 6 figures", "journal-ref": "Appl. Math. 54 (2009) 147-176", "doi": "10.1007/s10492-009-0010-z", "report-no": null, "categories": "cs.NA math-ph math.MP physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most three dimensional constitutive relations that have been developed to\ndescribe the behavior of bodies are correlated against one dimensional and two\ndimensional experiments. What is usually lost sight of is the fact that\ninfinity of such three dimensional models may be able to explain these\nexperiments that are lower dimensional. Recently, the notion of maximization of\nthe rate of entropy production has been used to obtain constitutive relations\nbased on the choice of the stored energy and rate of entropy production, etc.\nIn this paper we show different choices for the manner in which the body stores\nenergy and dissipates energy and satisfies the requirement of maximization of\nthe rate of entropy production that leads to many three dimensional models. All\nof these models, in one dimension, reduce to the model proposed by Burgers to\ndescribe the viscoelastic behavior of bodies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 21:04:51 GMT"}], "update_date": "2010-07-23", "authors_parsed": [["Karra", "Satish", ""], ["Rajagopal", "K. R.", ""]]}, {"id": "1007.3764", "submitter": "Satish  Karra", "authors": "Satish Karra and K. R. Rajagopal", "title": "A thermodynamic framework to develop rate-type models for fluids without\n  instantaneous elasticity", "comments": "18 pages, 5 figures", "journal-ref": "Acta Mech. 205(1-4) (2009) 105-119", "doi": "10.1007/s00707-009-0167-2", "report-no": null, "categories": "cs.NA math-ph math.MP physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply the thermodynamic framework recently put into place\nby Rajagopal and co-workers, to develop rate-type models for viscoelastic\nfluids which do not possess instantaneous elasticity. To illustrate the\ncapabilities of such models we make a specific choice for the specific\nHelmholtz potential and the rate of dissipation and consider the creep and\nstress relaxation response associated with the model. Given specific forms for\nthe Helmholtz potential and the rate of dissipation, the rate of dissipation is\nmaximized with the constraint that the difference between the stress power and\nthe rate of change of Helmholtz potential is equal to the rate of dissipation\nand any other constraint that may be applicable such as incompressibility. We\nshow that the model that is developed exhibits fluid-like characteristics and\nis incapable of instantaneous elastic response. It also includes Maxwell-like\nand Kelvin-Voigt-like viscoelastic materials (when certain material moduli take\nspecial values).\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 21:28:38 GMT"}], "update_date": "2010-07-23", "authors_parsed": [["Karra", "Satish", ""], ["Rajagopal", "K. R.", ""]]}, {"id": "1007.3881", "submitter": "Vasil Kolev", "authors": "Vasil Kolev", "title": "Orthogonal multifilters image processing of astronomical images from\n  scanned photographic plates", "comments": "6 pages, The ACM proceedings of CompSysTech 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper orthogonal multifilters for astronomical image processing are\npresented. We obtained new orthogonal multifilters based on the orthogonal\nwavelet of Haar and Daubechies. Recently, multiwavelets have been introduced as\na more powerful multiscale analysis tool. It adds several degrees of freedom in\nmultifilter design and makes it possible to have several useful properties such\nas symmetry, orthogonality, short support, and a higher number of vanishing\nmoments simultaneously. Multifilter decomposition of scanned photographic\nplates with astronomical images is made.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2010 13:22:50 GMT"}, {"version": "v2", "created": "Sat, 16 Oct 2010 17:01:02 GMT"}], "update_date": "2010-10-19", "authors_parsed": [["Kolev", "Vasil", ""]]}, {"id": "1007.4518", "submitter": "Michael Cullinan Dr", "authors": "M. P. Cullinan", "title": "Piecewise Convex-Concave Approximation in the $\\ell_{\\infty}$ Norm", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that $\\ff \\in \\reals^{n}$ is a vector of $n$ error-contaminated\nmeasurements of $n$ smooth values measured at distinct and strictly ascending\nabscissae. The following projective technique is proposed for obtaining a\nvector of smooth approximations to these values. Find \\yy\\ minimizing $\\| \\yy -\n\\ff \\|_{\\infty}$ subject to the constraints that the second order consecutive\ndivided differences of the components of \\yy\\ change sign at most $q$ times.\nThis optimization problem (which is also of general geometrical interest) does\nnot suffer from the disadvantage of the existence of purely local minima and\nallows a solution to be constructed in $O(nq)$ operations. A new algorithm for\ndoing this is developed and its effectiveness is proved. Some of the results of\napplying it to undulating and peaky data are presented, showing that it is\neconomical and can give very good results, particularly for large\ndensely-packed data, even when the errors are quite large.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2010 17:50:16 GMT"}], "update_date": "2010-07-27", "authors_parsed": [["Cullinan", "M. P.", ""]]}, {"id": "1007.5510", "submitter": "Mark Tygert", "authors": "Nathan Halko, Per-Gunnar Martinsson, Yoel Shkolnisky, and Mark Tygert", "title": "An algorithm for the principal component analysis of large data sets", "comments": "17 pages, 3 figures (each with 2 or 3 subfigures), 2 tables (each\n  with 2 subtables)", "journal-ref": "SIAM Journal on Scientific Computing, 33 (5): 2580-2594, 2011", "doi": null, "report-no": null, "categories": "stat.CO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently popularized randomized methods for principal component analysis\n(PCA) efficiently and reliably produce nearly optimal accuracy --- even on\nparallel processors --- unlike the classical (deterministic) alternatives. We\nadapt one of these randomized methods for use with data sets that are too large\nto be stored in random-access memory (RAM). (The traditional terminology is\nthat our procedure works efficiently \"out-of-core.\") We illustrate the\nperformance of the algorithm via several numerical examples. For example, we\nreport on the PCA of a data set stored on disk that is so large that less than\na hundredth of it can fit in our computer's RAM.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jul 2010 18:24:23 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2011 20:04:21 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Halko", "Nathan", ""], ["Martinsson", "Per-Gunnar", ""], ["Shkolnisky", "Yoel", ""], ["Tygert", "Mark", ""]]}]