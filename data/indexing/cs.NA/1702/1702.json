[{"id": "1702.00108", "submitter": "Yusaku Yamamoto Dr.", "authors": "Yusaku Yamamoto", "title": "On the optimality and sharpness of Laguerre's lower bound on the\n  smallest eigenvalue of a symmetric positive definite matrix", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lower bounds on the smallest eigenvalue of a symmetric positive definite\nmatrices $A\\in\\mathbb{R}^{m\\times m}$ play an important role in condition\nnumber estimation and in iterative methods for singular value computation. In\nparticular, the bounds based on ${\\rm Tr}(A^{-1})$ and ${\\rm Tr}(A^{-2})$\nattract attention recently because they can be computed in $O(m)$ work when $A$\nis tridiagonal. In this paper, we focus on these bounds and investigate their\nproperties in detail. First, we consider the problem of finding the optimal\nbound that can be computed solely from ${\\rm Tr}(A^{-1})$ and ${\\rm\nTr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one\nin terms of sharpness. Next, we study the gap between the Laguerre bound and\nthe smallest eigenvalue. We characterize the situation in which the gap becomes\nlargest in terms of the eigenvalue distribution of $A$ and show that the gap\nbecomes smallest when ${\\rm Tr}(A^{-2})/\\{{\\rm Tr}(A^{-1})\\}^2$ approaches 1 or\n$\\frac{1}{m}$. These results will be useful, for example, in designing\nefficient shift strategies for singular value computation algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 03:00:08 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Yamamoto", "Yusaku", ""]]}, {"id": "1702.00269", "submitter": "Maria Charina", "authors": "Maria Charina, Vladimir Yu. Protasov", "title": "Regularity of anisotropic refinable functions", "comments": null, "journal-ref": null, "doi": "10.1016/j.acha.2017.12.003", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a detailed regularity analysis of anisotropic wavelet\nframes and subdivision. In the univariate setting, the smoothness of wavelet\nframes and subdivision is well understood by means of the matrix approach. In\nthe multivariate setting, this approach has been extended only to the special\ncase of isotropic refinement with the dilation matrix all of whose eigenvalues\nare equal in the absolute value. The general anisotropic case has resisted to\nbe fully understood: the matrix approach can determine whether a refinable\nfunction belongs to $C(\\mathbb{R}^s)$ or $L_p(\\mathbb{R}^s)$, $1 \\le p <\n\\infty$, but its H\\\"older regularity remained mysteriously unattainable.\n  It this paper we show how to compute the H\\\"older regularity in\n$C(\\mathbb{R}^s)$ or $L_p(\\mathbb{R}^s)$, $1 \\le p < \\infty$. In the\nanisotropic case, our expression for the exact H\\\"older exponent of a refinable\nfunction reflects the impact of the variable moduli of the eigenvalues of the\ncorresponding dilation matrix. In the isotropic case, our results reduce to the\nwell-known facts from the literature. We provide an efficient algorithm for\ndetermining the finite set of the restricted transition matrices whose spectral\nproperties characterize the H\\\"older exponent of the corresponding refinable\nfunction. We also analyze the higher regularity, the local regularity, the\ncorresponding moduli of continuity, and the rate of convergence of the\ncorresponding subdivision schemes. We illustrate our results with several\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 14:19:17 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 08:33:13 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Charina", "Maria", ""], ["Protasov", "Vladimir Yu.", ""]]}, {"id": "1702.02111", "submitter": "Rachel Slaybaugh", "authors": "R.N. Slaybaugh, T.M. Evans, G.G. Davidson, P.P.H. Wilson", "title": "Rayleigh Quotient Iteration with a Multigrid in Energy Preconditioner\n  for Massively Parallel Neutron Transport", "comments": "arXiv admin note: text overlap with arXiv:1612.00907", "journal-ref": "ANS MC2015 Joint International Conference on Mathematics and\n  Computation, Supercomputing in Nuclear Applications and the Monte Carlo\n  Method, Nashville, Tennessee, April 19-23, 2015", "doi": null, "report-no": null, "categories": "cs.CE cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three complementary methods have been implemented in the code Denovo that\naccelerate neutral particle transport calculations with methods that use\nleadership-class computers fully and effectively: a multigroup block (MG)\nKrylov solver, a Rayleigh quotient iteration (RQI) eigenvalue solver, and a\nmultigrid in energy preconditioner. The multigroup Krylov solver converges more\nquickly than Gauss Seidel and enables energy decomposition such that Denovo can\nscale to hundreds of thousands of cores. The new multigrid in energy\npreconditioner reduces iteration count for many problem types and takes\nadvantage of the new energy decomposition such that it can scale efficiently.\nThese two tools are useful on their own, but together they enable the RQI\neigenvalue solver to work. Each individual method has been described before,\nbut this is the first time they have been demonstrated to work together\neffectively.\n  RQI should converge in fewer iterations than power iteration (PI) for large\nand challenging problems. RQI creates shifted systems that would not be\ntractable without the MG Krylov solver. It also creates ill-conditioned\nmatrices that cannot converge without the multigrid in energy preconditioner.\nUsing these methods together, RQI converged in fewer iterations and in less\ntime than all PI calculations for a full pressurized water reactor core. It\nalso scaled reasonably well out to 275,968 cores.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 17:34:39 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Slaybaugh", "R. N.", ""], ["Evans", "T. M.", ""], ["Davidson", "G. G.", ""], ["Wilson", "P. P. H.", ""]]}, {"id": "1702.02912", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson and Lionel Mathelin and Steven L. Brunton and J.\n  Nathan Kutz", "title": "Randomized Dynamic Mode Decomposition", "comments": null, "journal-ref": "SIAM Journal on Applied Dynamical Systems 2019 18:4, 1867-1891", "doi": "10.1137/18M1215013", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a randomized algorithm for computing the near-optimal\nlow-rank dynamic mode decomposition (DMD). Randomized algorithms are emerging\ntechniques to compute low-rank matrix approximations at a fraction of the cost\nof deterministic algorithms, easing the computational challenges arising in the\narea of `big data'. The idea is to derive a small matrix from the\nhigh-dimensional data, which is then used to efficiently compute the dynamic\nmodes and eigenvalues. The algorithm is presented in a modular probabilistic\nframework, and the approximation quality can be controlled via oversampling and\npower iterations. The effectiveness of the resulting randomized DMD algorithm\nis demonstrated on several benchmark examples of increasing complexity,\nproviding an accurate and efficient approach to extract spatiotemporal coherent\nstructures from big data in a framework that scales with the intrinsic rank of\nthe data, rather than the ambient measurement dimension. For this work we\nassume that the dynamics of the problem under consideration is evolving on a\nlow-dimensional subspace that is well characterized by a fast decaying singular\nvalue spectrum.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 18:26:28 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 19:19:52 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 00:34:51 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Mathelin", "Lionel", ""], ["Brunton", "Steven L.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1702.03673", "submitter": "Jon Cockayne", "authors": "Jon Cockayne, Chris Oates, Tim Sullivan, Mark Girolami", "title": "Bayesian Probabilistic Numerical Methods", "comments": null, "journal-ref": "SIAM Review 61(4):756--789, 2019", "doi": "10.1137/17M1139357", "report-no": null, "categories": "stat.ME cs.NA math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergent field of probabilistic numerics has thus far lacked clear\nstatistical principals. This paper establishes Bayesian probabilistic numerical\nmethods as those which can be cast as solutions to certain inverse problems\nwithin the Bayesian framework. This allows us to establish general conditions\nunder which Bayesian probabilistic numerical methods are well-defined,\nencompassing both non-linear and non-Gaussian models. For general computation,\na numerical approximation scheme is proposed and its asymptotic convergence\nestablished. The theoretical development is then extended to pipelines of\ncomputation, wherein probabilistic numerical methods are composed to solve more\nchallenging numerical tasks. The contribution highlights an important research\nfrontier at the interface of numerical analysis and uncertainty quantification,\nwith a challenging industrial application presented.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 08:52:58 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 13:58:53 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Cockayne", "Jon", ""], ["Oates", "Chris", ""], ["Sullivan", "Tim", ""], ["Girolami", "Mark", ""]]}, {"id": "1702.04077", "submitter": "Rachelle Rivero", "authors": "Tsuyoshi Kato and Rachelle Rivero", "title": "Mutual Kernel Matrix Completion", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the huge influx of various data nowadays, extracting knowledge from them\nhas become an interesting but tedious task among data scientists, particularly\nwhen the data come in heterogeneous form and have missing information. Many\ndata completion techniques had been introduced, especially in the advent of\nkernel methods. However, among the many data completion techniques available in\nthe literature, studies about mutually completing several incomplete kernel\nmatrices have not been given much attention yet. In this paper, we present a\nnew method, called Mutual Kernel Matrix Completion (MKMC) algorithm, that\ntackles this problem of mutually inferring the missing entries of multiple\nkernel matrices by combining the notions of data fusion and kernel matrix\ncompletion, applied on biological data sets to be used for classification task.\nWe first introduced an objective function that will be minimized by exploiting\nthe EM algorithm, which in turn results to an estimate of the missing entries\nof the kernel matrices involved. The completed kernel matrices are then\ncombined to produce a model matrix that can be used to further improve the\nobtained estimates. An interesting result of our study is that the E-step and\nthe M-step are given in closed form, which makes our algorithm efficient in\nterms of time and memory. After completion, the (completed) kernel matrices are\nthen used to train an SVM classifier to test how well the relationships among\nthe entries are preserved. Our empirical results show that the proposed\nalgorithm bested the traditional completion techniques in preserving the\nrelationships among the data points, and in accurately recovering the missing\nkernel matrix entries. By far, MKMC offers a promising solution to the problem\nof mutual estimation of a number of relevant incomplete kernel matrices.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 04:30:03 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 05:16:10 GMT"}, {"version": "v3", "created": "Wed, 10 May 2017 09:59:32 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Kato", "Tsuyoshi", ""], ["Rivero", "Rachelle", ""]]}, {"id": "1702.04274", "submitter": "Cl\\'audio Gomes", "authors": "Cl\\'audio Gomes, Yentl Van Tendeloo, Joachim Denil, Paul De\n  Meulenaere, Hans Vangheluwe", "title": "Hybrid System Modelling and Simulation with Dirac Deltas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a wide variety of problems, creating detailed continuous models of\n(continuous) physical systems is, at the very least, impractical. Hybrid models\ncan abstract away short transient behaviour (thus introducing discontinuities)\nin order to simplify the study of such systems. For example, when modelling a\nbouncing ball, the bounce can be abstracted as a discontinuous change of the\nvelocity, instead of resorting to the physics of the ball (de-)compression to\nkeep the velocity signal continuous. Impulsive differential equations can be\nused to model and simulate hybrid systems such as the bouncing ball. In this\napproach, the force acted on the ball by the floor is abstracted as an\ninfinitely large function in an infinitely small interval of time, that is, an\nimpulse. Current simulators cannot handle such approximations well due to the\nlimitations of machine precision.\n  In this paper, we explore the simulation of impulsive differential equations,\nwhere impulses are first class citizens. We present two approaches for the\nsimulation of impulses: symbolic and numerical. Our contribution is a\ntheoretically founded description of the implementation of both approaches in a\nCausal Block Diagram modelling and simulation tool. Furthermore, we investigate\nthe conditions for which one approach is better than the other.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 16:04:20 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Gomes", "Cl\u00e1udio", ""], ["Van Tendeloo", "Yentl", ""], ["Denil", "Joachim", ""], ["De Meulenaere", "Paul", ""], ["Vangheluwe", "Hans", ""]]}, {"id": "1702.04464", "submitter": "Jingwei Liu", "authors": "Jingwei Liu, Yi Liu", "title": "Deleting Items and Disturbing Mesh Theorems for Riemann Definite\n  Integral and Their Applications", "comments": "14", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the definition of Riemann definite integral,deleting items and\ndisturbing mesh theorems on Riemann sums are given. After deleting some items\nor disturbing the mesh of partition, the limit of Riemann sums still converges\nto Riemann definite integral under specific conditions. These theorems can deal\nwith a class of complicate limitation of sum and product of series of a\nfunction, and demonstrate that the geometric intuition of Riemann definite\nintegral is more profound than ordinary thinking of area of curved trapezoid.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 05:14:54 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Liu", "Jingwei", ""], ["Liu", "Yi", ""]]}, {"id": "1702.04837", "submitter": "Shusen Wang", "authors": "Shusen Wang and Alex Gittens and Michael W. Mahoney", "title": "Sketched Ridge Regression: Optimization Perspective, Statistical\n  Perspective, and Model Averaging", "comments": "To appear in Journal of Machine Learning Research, 2018. A short\n  version has appeared in International Conference on Machine Learning (ICML),\n  2017", "journal-ref": "Journal of Machine Learning Research, 19, pp1-50, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the statistical and optimization impacts of the classical sketch\nand Hessian sketch used to approximately solve the Matrix Ridge Regression\n(MRR) problem. Prior research has quantified the effects of classical sketch on\nthe strictly simpler least squares regression (LSR) problem. We establish that\nclassical sketch has a similar effect upon the optimization properties of MRR\nas it does on those of LSR: namely, it recovers nearly optimal solutions. By\ncontrast, Hessian sketch does not have this guarantee, instead, the\napproximation error is governed by a subtle interplay between the \"mass\" in the\nresponses and the optimal objective value.\n  For both types of approximation, the regularization in the sketched MRR\nproblem results in significantly different statistical properties from those of\nthe sketched LSR problem. In particular, there is a bias-variance trade-off in\nsketched MRR that is not present in sketched LSR. We provide upper and lower\nbounds on the bias and variance of sketched MRR, these bounds show that\nclassical sketch significantly increases the variance, while Hessian sketch\nsignificantly increases the bias. Empirically, sketched MRR solutions can have\nrisks that are higher by an order-of-magnitude than those of the optimal MRR\nsolutions.\n  We establish theoretically and empirically that model averaging greatly\ndecreases the gap between the risks of the true and sketched solutions to the\nMRR problem. Thus, in parallel or distributed settings, sketching combined with\nmodel averaging is a powerful technique that quickly obtains near-optimal\nsolutions to the MRR problem while greatly mitigating the increased statistical\nrisk incurred by sketching.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 02:01:26 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 02:59:41 GMT"}, {"version": "v3", "created": "Sat, 10 Jun 2017 17:52:18 GMT"}, {"version": "v4", "created": "Sat, 5 May 2018 20:58:25 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Wang", "Shusen", ""], ["Gittens", "Alex", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1702.05423", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Shuzhong Zhang", "title": "Accelerated Primal-Dual Proximal Block Coordinate Updating Methods for\n  Constrained Convex Optimization", "comments": "Accepted to Computational Optimization and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block Coordinate Update (BCU) methods enjoy low per-update computational\ncomplexity because every time only one or a few block variables would need to\nbe updated among possibly a large number of blocks. They are also easily\nparallelized and thus have been particularly popular for solving problems\ninvolving large-scale dataset and/or variables. In this paper, we propose a\nprimal-dual BCU method for solving linearly constrained convex program in\nmulti-block variables. The method is an accelerated version of a primal-dual\nalgorithm proposed by the authors, which applies randomization in selecting\nblock variables to update and establishes an $O(1/t)$ convergence rate under\nweak convexity assumption. We show that the rate can be accelerated to\n$O(1/t^2)$ if the objective is strongly convex. In addition, if one block\nvariable is independent of the others in the objective, we then show that the\nalgorithm can be modified to achieve a linear rate of convergence. The\nnumerical experiments show that the accelerated method performs stably with a\nsingle set of parameters while the original method needs to tune the parameters\nfor different datasets in order to achieve a comparable level of performance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 16:29:00 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 17:36:26 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 19:55:18 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Xu", "Yangyang", ""], ["Zhang", "Shuzhong", ""]]}, {"id": "1702.06166", "submitter": "Tammo Rukat", "authors": "Tammo Rukat and Chris C. Holmes and Michalis K. Titsias and\n  Christopher Yau", "title": "Bayesian Boolean Matrix Factorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA q-bio.GN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean matrix factorisation aims to decompose a binary data matrix into an\napproximate Boolean product of two low rank, binary matrices: one containing\nmeaningful patterns, the other quantifying how the observations can be\nexpressed as a combination of these patterns. We introduce the OrMachine, a\nprobabilistic generative model for Boolean matrix factorisation and derive a\nMetropolised Gibbs sampler that facilitates efficient parallel posterior\ninference. On real world and simulated data, our method outperforms all\ncurrently existing approaches for Boolean matrix factorisation and completion.\nThis is the first method to provide full posterior inference for Boolean Matrix\nfactorisation which is relevant in applications, e.g. for controlling false\npositive rates in collaborative filtering and, crucially, improves the\ninterpretability of the inferred patterns. The proposed algorithm scales to\nlarge datasets as we demonstrate by analysing single cell gene expression data\nin 1.3 million mouse brain cells across 11 thousand genes on commodity\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 20:31:39 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 14:17:44 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Rukat", "Tammo", ""], ["Holmes", "Chris C.", ""], ["Titsias", "Michalis K.", ""], ["Yau", "Christopher", ""]]}, {"id": "1702.06365", "submitter": "Florian Zwicke", "authors": "Florian Zwicke, Philipp Knechtges, Marek Behr, Stefanie Elgeti", "title": "Automatic implementation of material laws: Jacobian calculation in a\n  finite element code with TAPENADE", "comments": "17 pages, 9 figures", "journal-ref": "Computers & Mathematics with Applications, 72 (2016) 2808-2822", "doi": "10.1016/j.camwa.2016.10.010", "report-no": null, "categories": "cs.NA cs.DS physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to increase the versatility of finite element codes, we explore\nthe possibility of automatically creating the Jacobian matrix necessary for the\ngradient-based solution of nonlinear systems of equations. Particularly, we aim\nto assess the feasibility of employing the automatic differentiation tool\nTAPENADE for this purpose on a large Fortran codebase that is the result of\nmany years of continuous development. As a starting point we will describe the\nspecial structure of finite element codes and the implications that this code\ndesign carries for an efficient calculation of the Jacobian matrix. We will\nalso propose a first approach towards improving the efficiency of such a\nmethod. Finally, we will present a functioning method for the automatic\nimplementation of the Jacobian calculation in a finite element software, but\nwill also point out important shortcomings that will have to be addressed in\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:08:06 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Zwicke", "Florian", ""], ["Knechtges", "Philipp", ""], ["Behr", "Marek", ""], ["Elgeti", "Stefanie", ""]]}, {"id": "1702.07199", "submitter": "Rafa{\\l} Nowak", "authors": "Rafa{\\l} Nowak", "title": "Convergence acceleration of alternating series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new simple convergence acceleration method for wide range class\nof convergent alternating series. It has some common features with Smith's and\nFord's modification of Levin's and Weniger's sequence transformations, but its\ncomputational and memory cost is lower. We compare all three methods and give\nsome common theoretical results. Numerical examples confirm a similar\nperformance of all of them.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 12:51:51 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 08:00:13 GMT"}, {"version": "v3", "created": "Sun, 29 Apr 2018 20:17:46 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Nowak", "Rafa\u0142", ""]]}, {"id": "1702.07367", "submitter": "Matthias Chung", "authors": "Julianne Chung, Matthias Chung, J. Tanner Slagel, and Luis Tenorio", "title": "Stochastic Newton and Quasi-Newton Methods for Large Linear\n  Least-squares Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe stochastic Newton and stochastic quasi-Newton approaches to\nefficiently solve large linear least-squares problems where the very large data\nsets present a significant computational burden (e.g., the size may exceed\ncomputer memory or data are collected in real-time). In our proposed framework,\nstochasticity is introduced in two different frameworks as a means to overcome\nthese computational limitations, and probability distributions that can exploit\nstructure and/or sparsity are considered. Theoretical results on consistency of\nthe approximations for both the stochastic Newton and the stochastic\nquasi-Newton methods are provided. The results show, in particular, that\nstochastic Newton iterates, in contrast to stochastic quasi-Newton iterates,\nmay not converge to the desired least-squares solution. Numerical examples,\nincluding an example from extreme learning machines, demonstrate the potential\napplications of these methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 19:09:19 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Chung", "Julianne", ""], ["Chung", "Matthias", ""], ["Slagel", "J. Tanner", ""], ["Tenorio", "Luis", ""]]}, {"id": "1702.07834", "submitter": "Weiran Wang", "authors": "Jialei Wang, Weiran Wang, Dan Garber, Nathan Srebro", "title": "Efficient coordinate-wise leading eigenvector computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze efficient \"coordinate-wise\" methods for finding the\nleading eigenvector, where each step involves only a vector-vector product. We\nestablish global convergence with overall runtime guarantees that are at least\nas good as Lanczos's method and dominate it for slowly decaying spectrum. Our\nmethods are based on combining a shift-and-invert approach with coordinate-wise\nalgorithms for linear regression.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 05:11:25 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Wang", "Jialei", ""], ["Wang", "Weiran", ""], ["Garber", "Dan", ""], ["Srebro", "Nathan", ""]]}, {"id": "1702.08124", "submitter": "Haishan Ye", "authors": "Haishan Ye, Luo Luo, Zhihua Zhang", "title": "Approximate Newton Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many machine learning models involve solving optimization problems. Thus, it\nis important to deal with a large-scale optimization problem in big data\napplications. Recently, subsampled Newton methods have emerged to attract much\nattention due to their efficiency at each iteration, rectified a weakness in\nthe ordinary Newton method of suffering a high cost in each iteration while\ncommanding a high convergence rate. Other efficient stochastic second order\nmethods are also proposed. However, the convergence properties of these methods\nare still not well understood. There are also several important gaps between\nthe current convergence theory and the performance in real applications. In\nthis paper, we aim to fill these gaps. We propose a unifying framework to\nanalyze both local and global convergence properties of second order methods.\nBased on this framework, we present our theoretical results which match the\nperformance in real applications well.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 02:07:39 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 04:26:22 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ye", "Haishan", ""], ["Luo", "Luo", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1702.08142", "submitter": "Mahito Sugiyama", "authors": "Mahito Sugiyama and Hiroyuki Nakahara and Koji Tsuda", "title": "Tensor Balancing on Statistical Manifold", "comments": "19 pages, 5 figures, accepted to the 34th International Conference on\n  Machine Learning (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve tensor balancing, rescaling an Nth order nonnegative tensor by\nmultiplying N tensors of order N - 1 so that every fiber sums to one. This\ngeneralizes a fundamental process of matrix balancing used to compare matrices\nin a wide range of applications from biology to economics. We present an\nefficient balancing algorithm with quadratic convergence using Newton's method\nand show in numerical experiments that the proposed algorithm is several orders\nof magnitude faster than existing ones. To theoretically prove the correctness\nof the algorithm, we model tensors as probability distributions in a\nstatistical manifold and realize tensor balancing as projection onto a\nsubmanifold. The key to our algorithm is that the gradient of the manifold,\nused as a Jacobian matrix in Newton's method, can be analytically obtained\nusing the Moebius inversion formula, the essential of combinatorial\nmathematics. Our model is not limited to tensor balancing, but has a wide\napplicability as it includes various statistical and machine learning models\nsuch as weighted DAGs and Boltzmann machines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 04:30:06 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:43:01 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 15:18:28 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Sugiyama", "Mahito", ""], ["Nakahara", "Hiroyuki", ""], ["Tsuda", "Koji", ""]]}]