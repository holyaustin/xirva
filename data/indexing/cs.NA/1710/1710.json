[{"id": "1710.00087", "submitter": "Thomas Trogdon", "authors": "Thomas Trogdon", "title": "On spectral and numerical properties of random butterfly matrices", "comments": "Fixed a few typos and added some additional comments", "journal-ref": null, "doi": "10.1016/j.aml.2019.03.024", "report-no": null, "categories": "math.NA cs.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral and numerical properties of classes of random orthogonal butterfly\nmatrices, as introduced by Parker (1995), are discussed, including the\nuniformity of eigenvalue distributions. These matrices are important because\nthe matrix-vector product with an $N$-dimensional vector can be performed in\n$O(N \\log N)$ operations. And in the simplest situation, these random matrices\ncoincide with Haar measure on a subgroup of the orthogonal group. We discuss\nother implications in the context of randomized linear algebra.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 20:53:00 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 18:43:59 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Trogdon", "Thomas", ""]]}, {"id": "1710.00246", "submitter": "Antoine Tambue", "authors": "Jean Daniel Mukam, Antoine Tambue", "title": "Optimal strong convergence rates of numerical methods for semilinear\n  parabolic SPDE driven by Gaussian noise and Poisson random measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the numerical approximation of semilinear parabolic\nstochastic partial differential equation (SPDE) driven simultaneously by\nGaussian noise and Poisson random measure, more realistic in modeling real\nworld phenomena. The SPDE is discretized in space with the standard finite\nelement method and in time with the linear implicit Euler method or an\nexponential integrator, more efficient and stable for stiff problems. We prove\nthe strong convergence of the fully discrete schemes toward the mild solution.\nThe results reveal how convergence orders depend on the regularity of the noise\nand the initial data.In addition, we exceed the classical orders $1/2$ in time\nand $1$ in space achieved in the literature when dealing with SPDE driven by\nPoisson measure with less regularity assumptions on the nonlinear drift\nfunction. In particular, for trace class multiplicative Gaussian noise we\nachieve convergence order $\\mathcal{O}(h^2+\\Delta t^{1/2})$.For additive trace\nclass Gaussian noise and an appropriate jump function, we achieve convergence\norder $\\mathcal{O}(h^2+\\Delta t)$. Numerical experiments to sustain the\ntheoretical results are provided.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 18:59:53 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 19:23:46 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 01:21:01 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Mukam", "Jean Daniel", ""], ["Tambue", "Antoine", ""]]}, {"id": "1710.00387", "submitter": "Tomohiko Mizutani", "authors": "Tomohiko Mizutani and Mirai Tanaka", "title": "Efficient Preconditioning for Noisy Separable NMFs by Successive\n  Projection Based Low-Rank Approximations", "comments": "32 pages, 4 figures", "journal-ref": "Machine Learning, 107(4), pages 643-673, 2018", "doi": null, "report-no": null, "categories": "cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The successive projection algorithm (SPA) can quickly solve a nonnegative\nmatrix factorization problem under a separability assumption. Even if noise is\nadded to the problem, SPA is robust as long as the perturbations caused by the\nnoise are small. In particular, robustness against noise should be high when\nhandling the problems arising from real applications. The preconditioner\nproposed by Gillis and Vavasis (2015) makes it possible to enhance the noise\nrobustness of SPA. Meanwhile, an additional computational cost is required. The\nconstruction of the preconditioner contains a step to compute the top-$k$\ntruncated singular value decomposition of an input matrix. It is known that the\ndecomposition provides the best rank-$k$ approximation to the input matrix; in\nother words, a matrix with the smallest approximation error among all matrices\nof rank less than $k$. This step is an obstacle to an efficient implementation\nof the preconditioned SPA.\n  To address the cost issue, we propose a modification of the algorithm for\nconstructing the preconditioner. Although the original algorithm uses the best\nrank-$k$ approximation, instead of it, our modification uses an alternative.\nIdeally, this alternative should have high approximation accuracy and low\ncomputational cost. To ensure this, our modification employs a rank-$k$\napproximation produced by an SPA based algorithm. We analyze the accuracy of\nthe approximation and evaluate the computational cost of the algorithm. We then\npresent an empirical study revealing the actual performance of the SPA based\nrank-$k$ approximation algorithm and the modified preconditioned SPA.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 18:20:57 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Mizutani", "Tomohiko", ""], ["Tanaka", "Mirai", ""]]}, {"id": "1710.00620", "submitter": "Yuzhen Lu", "authors": "Yuzhen Lu", "title": "Out-of-focus Blur: Image De-blurring", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image de-blurring is important in many cases of imaging a real scene or\nobject by a camera. This project focuses on de-blurring an image distorted by\nan out-of-focus blur through a simulation study. A pseudo-inverse filter is\nfirst explored but it fails because of severe noise amplification. Then\nTikhonov regularization methods are employed, which produce greatly improved\nresults compared to the pseudo-inverse filter. In Tikhonov regularization, the\nchoice of the regularization parameter plays a critical rule in obtaining a\nhigh-quality image, and the regularized solutions possess a semi-convergence\nproperty. The best result, with the relative restoration error of 8.49%, is\nachieved when the prescribed discrepancy principle is used to decide an optimal\nvalue. Furthermore, an iterative method, Conjugated Gradient, is employed for\nimage de-blurring, which is fast in computation and leads to an even better\nresult with the relative restoration error of 8.22%. The number of iteration in\nCG acts as a regularization parameter, and the iterates have a semi-convergence\nproperty as well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 13:08:12 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 03:11:32 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Lu", "Yuzhen", ""]]}, {"id": "1710.01106", "submitter": "Charles Pierre", "authors": "Thomas Roy, Yves Bourgault, Charles Pierre (LMAP)", "title": "Analysis of time-stepping methods for the monodomain model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To a large extent, the stiffness of the bidomain and monodomain models\ndepends on the choice of the ionic model, which varies in terms of complexity\nand realism. In this paper, we compare and analyze a variety of time-stepping\nmethods: explicit or semi-implicit, operator splitting, exponential, and\ndeferred correction methods. We compare these methods for solving the bidomain\nmodel coupled with three ionic models of varying complexity and stiffness: the\nphenomenological Mitchell-Schaeffer model, the more realistic Beeler-Reuter\nmodel, and the stiff and very complex ten Tuscher-Noble-Noble-Panfilov (TNNP)\nmodel. For each method, we derive absolute stability criteria of the spatially\ndiscretized monodomain model and verify that the theoretical critical\ntime-steps obtained closely match the ones in numerical experiments. We also\nverify that the numerical methods achieve an optimal order of convergence on\nthe model variables and derived quantities (such as speed of the wave,\ndepolarization time), and this in spite of the local non-differentiability of\nsome of the ionic models. The efficiency of the different methods is also\nconsidered by comparing computational times for similar accuracy. Conclusions\nare drawn on the methods to be used to solve the monodomain model based on the\nmodel stiffness and complexity, measured respectively by the eigenvalues of the\nmodel's Jacobian and the number of variables, and based on strict stability and\naccuracy criteria.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 12:25:12 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 12:11:39 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Roy", "Thomas", "", "LMAP"], ["Bourgault", "Yves", "", "LMAP"], ["Pierre", "Charles", "", "LMAP"]]}, {"id": "1710.01133", "submitter": "Cosmin Bonchis", "authors": "Cosmin Bonchis, Eva Kaslik and Florin Rosu", "title": "HPC optimal parallel communication algorithm for the simulation of\n  fractional-order systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel numerical simulation algorithm is presented for fractional-order\nsystems involving Caputo-type derivatives, based on the Adams-Bashforth-Moulton\n(ABM) predictor-corrector scheme. The parallel algorithm is implemented using\nseveral different approaches: a pure MPI version, a combination of MPI with\nOpenMP optimization and a memory saving speedup approach. All tests run on a\nBlueGene/P cluster, and comparative improvement results for the running time\nare provided. As an applied experiment, the solutions of a fractional-order\nversion of a system describing a forced series LCR circuit are numerically\ncomputed, depicting cascades of period-doubling bifurcations which lead to the\nonset of chaotic behavior.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 14:36:55 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Bonchis", "Cosmin", ""], ["Kaslik", "Eva", ""], ["Rosu", "Florin", ""]]}, {"id": "1710.01386", "submitter": "Antoine Tambue", "authors": "Antoine Tambue, Jean Daniel Mukam", "title": "Strong Convergence of the Linear Implicit Euler Method for the Finite\n  Element Discretization of Semilinear SPDEs Driven by Multiplicative and\n  Additive Noise", "comments": "arXiv admin note: text overlap with arXiv:1610.06790 and\n  arXiv:1710.00246", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to investigate the numerical approximation of a general\nsecond order parabolic stochastic partial differential equation(SPDE) driven by\nmultiplicative and additive noise under more relaxed conditions. The SPDE is\ndiscretized in space by the finite element method and in time by the linear\nimplicit Euler method. This extends the current results in the literature to\nnot necessary self-adjoint operator with more general boundary conditions. As a\nconsequence key part of the proof does not rely on the spectral decomposition\nof the linear operator. We achieve optimal convergence orders which depend on\nthe regularity of the noise and the initial data. In particular, for\nmultiplicative noise we achieve optimal order $\\mathcal{O}(h^2+\\Delta t^{1/2})$\nand for additive noise, we achieve optimal order $\\mathcal{O}(h^2+\\Delta t)$.\nIn contrast to current work in the literature, where the optimal convergence\norders are achieved for additive noise by incorporating further regularity\nassumptions on the nonlinear drift function, our optimal convergence orders are\nobtained under only the standard Lipschitz condition of the nonlinear drift\nterm. Numerical experiments to sustain our theoretical results are provided.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 19:13:07 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 20:26:50 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Tambue", "Antoine", ""], ["Mukam", "Jean Daniel", ""]]}, {"id": "1710.01493", "submitter": "Ruben H\\\"uhnerbein", "authors": "Ruben H\\\"uhnerbein, Fabrizio Savarino, Freddie \\r{A}str\\\"om, Christoph\n  Schn\\\"orr", "title": "Image Labeling Based on Graphical Models Using Wasserstein Messages and\n  Geometric Assignment", "comments": null, "journal-ref": null, "doi": "10.1137/17M1150669", "report-no": null, "categories": "cs.LG cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to Maximum A Posteriori inference based on\ndiscrete graphical models. By utilizing local Wasserstein distances for\ncoupling assignment measures across edges of the underlying graph, a given\ndiscrete objective function is smoothly approximated and restricted to the\nassignment manifold. A corresponding multiplicative update scheme combines in a\nsingle process (i) geometric integration of the resulting Riemannian gradient\nflow and (ii) rounding to integral solutions that represent valid labelings.\nThroughout this process, local marginalization constraints known from the\nestablished LP relaxation are satisfied, whereas the smooth geometric setting\nresults in rapidly converging iterations that can be carried out in parallel\nfor every edge.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 08:00:50 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 08:54:37 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["H\u00fchnerbein", "Ruben", ""], ["Savarino", "Fabrizio", ""], ["\u00c5str\u00f6m", "Freddie", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1710.02261", "submitter": "Sejoon Oh", "authors": "Sejoon Oh, Namyong Park, Lee Sael, and U Kang", "title": "Scalable Tucker Factorization for Sparse Tensors - Algorithms and\n  Discoveries", "comments": "IEEE International Conference on Data Engineering (ICDE 2018)", "journal-ref": null, "doi": "10.1109/ICDE.2018.00104", "report-no": null, "categories": "cs.NA cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given sparse multi-dimensional data (e.g., (user, movie, time; rating) for\nmovie recommendations), how can we discover latent concepts/relations and\npredict missing values? Tucker factorization has been widely used to solve such\nproblems with multi-dimensional data, which are modeled as tensors. However,\nmost Tucker factorization algorithms regard and estimate missing entries as\nzeros, which triggers a highly inaccurate decomposition. Moreover, few methods\nfocusing on an accuracy exhibit limited scalability since they require huge\nmemory and heavy computational costs while updating factor matrices. In this\npaper, we propose P-Tucker, a scalable Tucker factorization method for sparse\ntensors. P-Tucker performs alternating least squares with a row-wise update\nrule in a fully parallel way, which significantly reduces memory requirements\nfor updating factor matrices. Furthermore, we offer two variants of P-Tucker: a\ncaching algorithm P-Tucker-Cache and an approximation algorithm\nP-Tucker-Approx, both of which accelerate the update process. Experimental\nresults show that P-Tucker exhibits 1.7-14.1x speed-up and 1.4-4.8x less error\ncompared to the state-of-the-art. In addition, P-Tucker scales near linearly\nwith the number of observable entries in a tensor and number of threads. Thanks\nto P-Tucker, we successfully discover hidden concepts and relations in a\nlarge-scale real-world tensor, while existing methods cannot reveal latent\nfeatures due to their limited scalability or low accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 02:54:44 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 05:24:52 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Oh", "Sejoon", ""], ["Park", "Namyong", ""], ["Sael", "Lee", ""], ["Kang", "U", ""]]}, {"id": "1710.02774", "submitter": "Nir Sharon", "authors": "Roy Mitz and Nir Sharon and Yoel Shkolnisky", "title": "Symmetric rank-one updates from partial spectrum with an application to\n  out-of-sample extension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank-one update of the spectrum of a matrix is a fundamental problem in\nclassical perturbation theory. In this paper, we consider its variant where\nonly part of the spectrum is known. We address this variant using an efficient\nscheme for updating the known eigenpairs with guaranteed error bounds. Then, we\napply our scheme to the extension of the top eigenvectors of the graph\nLaplacian to a new data sample. In particular, we model this extension as a\nperturbation problem and show how to solve it using our rank-one updating\nscheme. We provide a theoretical analysis of this extension method, and back it\nup with numerical results that illustrate its advantages.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 03:20:47 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 18:25:20 GMT"}, {"version": "v3", "created": "Sat, 29 Jun 2019 09:34:40 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 11:01:51 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Mitz", "Roy", ""], ["Sharon", "Nir", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1710.02812", "submitter": "Ramakrishna Mokkapati", "authors": "Vinita Vasudevan, M. Ramakrishna", "title": "A Hierarchical Singular Value Decomposition Algorithm for Low Rank\n  Matrices", "comments": "eight Pages, fourteen figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular value decomposition (SVD) is a widely used technique for\ndimensionality reduction and computation of basis vectors. In many\napplications, especially in fluid mechanics and image processing the matrices\nare dense, but low-rank matrices. In these cases, a truncated SVD corresponding\nto the most significant singular values is sufficient. In this paper, we\npropose a tree based merge-and-truncate algorithm to obtain an approximate\ntruncated SVD of the matrix. Unlike previous methods, our technique is not\nlimited to \"tall and skinny\" or \"short and fat\" matrices and it can be used for\nmatrices of arbitrary size. The matrix is partitioned into blocks and the\ntruncated SVDs of blocks are merged to obtain the final SVD. If the matrices\nare low rank, this algorithm gives significant speedup over finding the\ntruncated SVD, even when run on a single core. The error is typically less than\n3\\%.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 09:28:07 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 09:37:09 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Vasudevan", "Vinita", ""], ["Ramakrishna", "M.", ""]]}, {"id": "1710.02874", "submitter": "Roy R. Lederman", "authors": "Roy R. Lederman", "title": "Numerical Algorithms for the Computation of Generalized Prolate\n  Spheroidal Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Prolate Spheroidal Functions (GPSF) are the eigenfunctions of the\ntruncated Fourier transform, restricted to D-dimensional balls in the spatial\ndomain and frequency domain. Despite their useful properties in many\napplications, GPSFs are often replaced by crude approximations. The purpose of\nthis paper is to review the elements of computing GPSFs and associated\neigenvalues. This paper is accompanied by open-source code.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 19:41:55 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Lederman", "Roy R.", ""]]}, {"id": "1710.03608", "submitter": "Jungwoo Lee", "authors": "Jungwoo Lee, Dongjin Choi, and Lee Sael", "title": "CTD: Fast, Accurate, and Interpretable Method for Static and Dynamic\n  Tensor Decompositions", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0200579", "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we find patterns and anomalies in a tensor, or multi-dimensional\narray, in an efficient and directly interpretable way? How can we do this in an\nonline environment, where a new tensor arrives each time step? Finding patterns\nand anomalies in a tensor is a crucial problem with many applications,\nincluding building safety monitoring, patient health monitoring, cyber\nsecurity, terrorist detection, and fake user detection in social networks.\nStandard PARAFAC and Tucker decomposition results are not directly\ninterpretable. Although a few sampling-based methods have previously been\nproposed towards better interpretability, they need to be made faster, more\nmemory efficient, and more accurate.\n  In this paper, we propose CTD, a fast, accurate, and directly interpretable\ntensor decomposition method based on sampling. CTD-S, the static version of\nCTD, provably guarantees a high accuracy that is 17 ~ 83x more accurate than\nthat of the state-of-the-art method. Also, CTD-S is made 5 ~ 86x faster, and 7\n~ 12x more memory-efficient than the state-of-the-art method by removing\nredundancy. CTD-D, the dynamic version of CTD, is the first interpretable\ndynamic tensor decomposition method ever proposed. Also, it is made 2 ~ 3x\nfaster than already fast CTD-S by exploiting factors at previous time step and\nby reordering operations. With CTD, we demonstrate how the results can be\neffectively interpreted in the online distributed denial of service (DDoS)\nattack detection.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 09:44:41 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Lee", "Jungwoo", ""], ["Choi", "Dongjin", ""], ["Sael", "Lee", ""]]}, {"id": "1710.03940", "submitter": "Denis Demidov", "authors": "Denis Demidov and Riccardo Rossi", "title": "Subdomain Deflation Combined with Local AMG: a Case Study Using AMGCL\n  Library", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": "10.1134/S1995080220040071", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a combination of the subdomain deflation method and local\nalgebraic multigrid as a scalable distributed memory preconditioner that is\nable to solve large linear systems of equations. The implementation of the\nalgorithm is made available for the community as part of an open source AMGCL\nlibrary. The solution targets both homogeneous (CPU-only) and heterogeneous\n(CPU/GPU) systems, employing hybrid MPI/OpenMP approach in the former and a\ncombination of MPI, OpenMP, and CUDA in the latter cases. The use of OpenMP\nminimizes the number of MPI processes, thus reducing the communication overhead\nof the deflation method and improving both weak and strong scalability of the\npreconditioner. The examples of scalar, Poisson-like, systems as well as\nnon-scalar problems, stemming out of the discretization of the Navier-Stokes\nequations, are considered in order to estimate performance of the implemented\nalgorithm. A comparison with a traditional global AMG preconditioner based on a\nwell-established Trilinos ML package is provided.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 07:25:16 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 10:54:14 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 08:19:38 GMT"}, {"version": "v4", "created": "Fri, 26 Oct 2018 06:20:18 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Demidov", "Denis", ""], ["Rossi", "Riccardo", ""]]}, {"id": "1710.04265", "submitter": "David Casillas-Perez", "authors": "David Casillas-Perez, Daniel Pizarro, Manuel Mazo and Adrien Bartoli", "title": "Solutions of Quadratic First-Order ODEs applied to Computer Vision\n  Problems", "comments": "The version 2: New change of variable. Maximal Curve Maximal Solution\n  Convergence Cones The version 3: modifies the author's list and the abstract\n  in metadata", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is a study about the existence and the uniqueness of solutions\nof a specific quadratic first-order ODE that frequently appears in multiple\nreconstruction problems. It is called the \\emph{planar-perspective equation}\ndue to the duality with the geometric problem of reconstruction of\nplanar-perspective curves from their modulus. Solutions of the\n\\emph{planar-perspective equation} are related with planar curves parametrized\nwith perspective parametrization due to this geometric interpretation. The\narticle proves the existence of only two local solutions to the \\emph{initial\nvalue problem} with \\emph{regular initial conditions} and a maximum of two\nanalytic solutions with \\emph{critical initial conditions}. The article also\ngives theorems to extend the local definition domain where the existence of\nboth solutions are guaranteed. It introduces the \\emph{maximal depth function}\nas a function that upper-bound all possible solutions of the\n\\emph{planar-perspective equation} and contains all its possible \\emph{critical\npoints}. Finally, the article describes the \\emph{maximal-depth solution\nproblem} that consists of finding the solution of the referred equation that\nhas maximum the depth and proves its uniqueness. It is an important problem as\nit does not need initial conditions to obtain the unique solution and its the\nfrequent solution that practical algorithms of the state-of-the-art give.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 19:29:42 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 08:24:15 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 09:33:14 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Casillas-Perez", "David", ""], ["Pizarro", "Daniel", ""], ["Mazo", "Manuel", ""], ["Bartoli", "Adrien", ""]]}, {"id": "1710.04576", "submitter": "Martin Wilhelm", "authors": "Martin Wilhelm", "title": "Balancing expression dags for more efficient lazy adaptive evaluation", "comments": "MACIS 2017 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arithmetic expression dags are widely applied in robust geometric computing.\nIn this paper we restructure expression dags by balancing consecutive additions\nor multiplications. We predict an asymptotic improvement in running time and\nexperimentally confirm the theoretical results. Finally, we discuss some\npitfalls of the approach resulting from changes in evaluation order.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 15:48:45 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Wilhelm", "Martin", ""]]}, {"id": "1710.04688", "submitter": "Shadrokh Samavi", "authors": "Shadrokh Samavi and Mohammad Reza Jahangir", "title": "Reduction of Look Up Tables for Computation of Reciprocal of Square\n  Roots", "comments": "6 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among many existing algorithms, convergence methods are the most popular\nmeans of computing square root and the reciprocal of square root of numbers. An\ninitial approximation is required in these methods. Look up tables (LUT) are\nemployed to produce the initial approximation. In this paper a number of\nmethods are suggested to reduce the size of the look up tables. The precision\nof the initial approximation plays an important role in the quality of the\nfinal result. There are constraints for the use of a LUT in terms of its size\nand its access time. Therefore, the optimization of the LUTs must be done in a\nway to minimize hardware while offering acceptable convergence speed and\nexactitude.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 19:00:24 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Samavi", "Shadrokh", ""], ["Jahangir", "Mohammad Reza", ""]]}, {"id": "1710.05513", "submitter": "Ziping Zhao", "authors": "Ziping Zhao and Daniel P. Palomar", "title": "Robust Maximum Likelihood Estimation of Sparse Vector Error Correction\n  Model", "comments": "5 pages, 3 figures, to appear in Proc. of the 2017 5th IEEE Global\n  Conference on Signal and Information Processing (GlobalSIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA q-fin.ST stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In econometrics and finance, the vector error correction model (VECM) is an\nimportant time series model for cointegration analysis, which is used to\nestimate the long-run equilibrium variable relationships. The traditional\nanalysis and estimation methodologies assume the underlying Gaussian\ndistribution but, in practice, heavy-tailed data and outliers can lead to the\ninapplicability of these methods. In this paper, we propose a robust model\nestimation method based on the Cauchy distribution to tackle this issue. In\naddition, sparse cointegration relations are considered to realize feature\nselection and dimension reduction. An efficient algorithm based on the\nmajorization-minimization (MM) method is applied to solve the proposed\nnonconvex problem. The performance of this algorithm is shown through numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 05:38:27 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhao", "Ziping", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1710.05705", "submitter": "Leon Bungert", "authors": "Leon Bungert, David A. Coomes, Matthias J. Ehrhardt, Jennifer Rasch,\n  Rafael Reisenhofer, Carola-Bibiane Sch\\\"onlieb", "title": "Blind Image Fusion for Hyperspectral Imaging with the Directional Total\n  Variation", "comments": "24 pages, 18 figures, published in Inverse Problems, typo corrected,\n  figure added", "journal-ref": null, "doi": "10.1088/1361-6420/aaaf63", "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging is a cutting-edge type of remote sensing used for\nmapping vegetation properties, rock minerals and other materials. A major\ndrawback of hyperspectral imaging devices is their intrinsic low spatial\nresolution. In this paper, we propose a method for increasing the spatial\nresolution of a hyperspectral image by fusing it with an image of higher\nspatial resolution that was obtained with a different imaging modality. This is\naccomplished by solving a variational problem in which the regularization\nfunctional is the directional total variation. To accommodate for possible\nmis-registrations between the two images, we consider a non-convex blind\nsuper-resolution problem where both a fused image and the corresponding\nconvolution kernel are estimated. Using this approach, our model can realign\nthe given images if needed. Our experimental results indicate that the\nnon-convexity is negligible in practice and that reliable solutions can be\ncomputed using a variety of different optimization algorithms. Numerical\nresults on real remote sensing data from plant sciences and urban monitoring\nshow the potential of the proposed method and suggests that it is robust with\nrespect to the regularization parameters, mis-registration and the shape of the\nkernel.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 15:18:13 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 13:50:06 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 23:10:45 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 14:40:20 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Bungert", "Leon", ""], ["Coomes", "David A.", ""], ["Ehrhardt", "Matthias J.", ""], ["Rasch", "Jennifer", ""], ["Reisenhofer", "Rafael", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1710.05961", "submitter": "Hassan Mansour", "authors": "Hassan Mansour", "title": "A Short Note on Improved ROSETA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents a more efficient formulation of the robust online subspace\nestimation and tracking algorithm (ROSETA) that is capable of identifying and\ntracking a time-varying low dimensional subspace from incomplete measurements\nand in the presence of sparse outliers. The algorithm minimizes a robust l1\nnorm cost function between the observed measurements and their projection onto\nthe estimated subspace. The projection coefficients and sparse outliers are\ncomputed using a LASSO solver and the subspace estimate is updated using a\nproximal point iteration with adaptive parameter selection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 18:51:03 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Mansour", "Hassan", ""]]}, {"id": "1710.06647", "submitter": "Tom Tirer", "authors": "Tom Tirer, Raja Giryes", "title": "Image Restoration by Iterative Denoising and Backward Projections", "comments": "To appear in IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems appear in many applications, such as image deblurring and\ninpainting. The common approach to address them is to design a specific\nalgorithm for each problem. The Plug-and-Play (P&P) framework, which has been\nrecently introduced, allows solving general inverse problems by leveraging the\nimpressive capabilities of existing denoising algorithms. While this fresh\nstrategy has found many applications, a burdensome parameter tuning is often\nrequired in order to obtain high-quality results. In this work, we propose an\nalternative method for solving inverse problems using off-the-shelf denoisers,\nwhich requires less parameter tuning. First, we transform a typical cost\nfunction, composed of fidelity and prior terms, into a closely related, novel\noptimization problem. Then, we propose an efficient minimization scheme with a\nplug-and-play property, i.e., the prior term is handled solely by a denoising\noperation. Finally, we present an automatic tuning mechanism to set the\nmethod's parameters. We provide a theoretical analysis of the method, and\nempirically demonstrate its competitiveness with task-specific techniques and\nthe P&P approach for image inpainting and deblurring.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 09:39:30 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 14:20:16 GMT"}, {"version": "v3", "created": "Thu, 4 Jan 2018 12:54:02 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 20:35:55 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}, {"id": "1710.06965", "submitter": "Art Owen", "authors": "Art B. Owen and Yury Maximov and Michael Chertkov", "title": "Importance sampling the union of rare events with an application to\n  power systems analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider importance sampling to estimate the probability $\\mu$ of a union\nof $J$ rare events $H_j$ defined by a random variable $\\boldsymbol{x}$. The\nsampler we study has been used in spatial statistics, genomics and\ncombinatorics going back at least to Karp and Luby (1983). It works by sampling\none event at random, then sampling $\\boldsymbol{x}$ conditionally on that event\nhappening and it constructs an unbiased estimate of $\\mu$ by multiplying an\ninverse moment of the number of occuring events by the union bound. We prove\nsome variance bounds for this sampler. For a sample size of $n$, it has a\nvariance no larger than $\\mu(\\bar\\mu-\\mu)/n$ where $\\bar\\mu$ is the union\nbound. It also has a coefficient of variation no larger than\n$\\sqrt{(J+J^{-1}-2)/(4n)}$ regardless of the overlap pattern among the $J$\nevents. Our motivating problem comes from power system reliability, where the\nphase differences between connected nodes have a joint Gaussian distribution\nand the $J$ rare events arise from unacceptably large phase differences. In the\ngrid reliability problems even some events defined by $5772$ constraints in\n$326$ dimensions, with probability below $10^{-22}$, are estimated with a\ncoefficient of variation of about $0.0024$ with only $n=10{,}000$ sample\nvalues.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 00:09:36 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 01:35:32 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 09:39:26 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 00:41:21 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Owen", "Art B.", ""], ["Maximov", "Yury", ""], ["Chertkov", "Michael", ""]]}, {"id": "1710.07067", "submitter": "Alessio De Angelis", "authors": "A. De Angelis, J. Schoukens, K. R. Godfrey, P. Carbone", "title": "Best Linear Approximation of Wiener Systems Using Multilevel Signals:\n  Theory and Experiments", "comments": "Accepted for Publication in IEEE Transactions on Instrumentation and\n  Measurement", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of measuring the best linear approximation of a nonlinear system\nby means of multilevel excitation sequences is analyzed. A comparison between\ndifferent types of sequences applied at the input of Wiener systems is provided\nby numerical simulations and by experiments on a practical circuit including an\nanalog filter and a clipping nonlinearity. The performance of the sequences is\ncompared with a white Gaussian noise signal for reference purposes. The\ntheoretical characterization of the best linear approximation when using\nrandomized constrained sequences is derived analytically for the cubic\nnonlinearity case. Numerical and experimental results show that the randomized\nconstrained approach for designing ternary sequences has a low sensitivity to\nboth even and odd order nonlinearities, resulting in a response close to the\nactual response of the underlying linear system.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 10:19:59 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["De Angelis", "A.", ""], ["Schoukens", "J.", ""], ["Godfrey", "K. R.", ""], ["Carbone", "P.", ""]]}, {"id": "1710.07185", "submitter": "Suleyman Cengizci PhD Cand.", "authors": "S\\\"uleyman Cengizci", "title": "A comparison between MMAE and SCEM for solving singularly perturbed\n  linear boundary layer problems", "comments": "3 Tables, 8 Figures", "journal-ref": "Filomat 33:7 (2019), 2135--2148", "doi": "10.2298/FIL1907135C", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose an efficient method so-called Successive\nComplementary Expansion Method (SCEM), that is based on generalized asymptotic\nexpansions, for approximating to the solutions of singularly perturbed\ntwo-point boundary value problems. In this easy-applicable asymptotic method,\nin contrast to the well-known method the Method of Matched Asymptotic\nExpansions (MMAE), the matching process is not necessary to obtain uniformly\nvalid approximations. The key point: A uniformly valid approximation is adopted\nfirst, and complementary functions are obtained imposing the corresponding\nboundary conditions. An illustrative and two numerical experiments are provided\nto show the implementation and numerical properties of the present method.\nFurthermore, MMAE results are also given in order to compare the numerical\nrobustness of the methods. Numerical results and the comparisons demonstrate\nabsolute superiority of SCEM over MMAE for linear problems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 10:11:50 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 16:04:22 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 22:40:14 GMT"}, {"version": "v4", "created": "Mon, 30 Dec 2019 18:30:27 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Cengizci", "S\u00fcleyman", ""]]}, {"id": "1710.07205", "submitter": "Quanming Yao", "authors": "Quanming Yao and James T. Kwok", "title": "Scalable Robust Matrix Factorization with Nonconvex Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust matrix factorization (RMF), which uses the $\\ell_1$-loss, often\noutperforms standard matrix factorization using the $\\ell_2$-loss, particularly\nwhen outliers are present. The state-of-the-art RMF solver is the RMF-MM\nalgorithm, which, however, cannot utilize data sparsity. Moreover, sometimes\neven the (convex) $\\ell_1$-loss is not robust enough. In this paper, we propose\nthe use of nonconvex loss to enhance robustness. To address the resultant\ndifficult optimization problem, we use majorization-minimization (MM)\noptimization and propose a new MM surrogate. To improve scalability, we exploit\ndata sparsity and optimize the surrogate via its dual with the accelerated\nproximal gradient algorithm. The resultant algorithm has low time and space\ncomplexities and is guaranteed to converge to a critical point. Extensive\nexperiments demonstrate its superiority over the state-of-the-art in terms of\nboth accuracy and scalability.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 15:55:27 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 07:19:26 GMT"}, {"version": "v3", "created": "Sat, 19 May 2018 02:38:13 GMT"}, {"version": "v4", "created": "Sun, 23 Sep 2018 15:08:23 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""]]}, {"id": "1710.07282", "submitter": "H{\\aa}kon Hoel", "authors": "Alexey Chernov, H{\\aa}kon Hoel, Kody J. H. Law, Fabio Nobile, Raul\n  Tempone", "title": "Multilevel ensemble Kalman filtering for spatio-temporal processes", "comments": "Version 1: 39 pages, 4 figures.arXiv admin note: substantial text\n  overlap with arXiv:1608.08558 . Version 2 (this version): 52 pages, 6\n  figures. Revision primarily of the introduction and the numerical examples\n  section", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and analyse the performance of a multilevel ensemble Kalman filter\nmethod (MLEnKF) for filtering settings where the underlying state-space model\nis an infinite-dimensional spatio-temporal process. We consider underlying\nmodels that needs to be simulated by numerical methods, with discretization in\nboth space and time. The multilevel Monte Carlo (MLMC) sampling strategy,\nachieving variance reduction through pairwise coupling of ensemble particles on\nneighboring resolutions, is used in the sample-moment step of MLEnKF to produce\nan efficient hierarchical filtering method for spatio-temporal models. Under\nsufficient regularity, MLEnKF is proven to be more efficient for weak\napproximations than EnKF, asymptotically in the large-ensemble and\nfine-numerical-resolution limit. Numerical examples support our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 16:11:28 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 14:45:17 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Chernov", "Alexey", ""], ["Hoel", "H\u00e5kon", ""], ["Law", "Kody J. H.", ""], ["Nobile", "Fabio", ""], ["Tempone", "Raul", ""]]}, {"id": "1710.07428", "submitter": "Philipp Wacker", "authors": "Philipp Wacker and Peter Knabner", "title": "Wavelet-based priors accelerate maximum-a-posteriori optimization in\n  Bayesian inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelet (Besov) priors are a promising way of reconstructing indirectly\nmeasured fields in a regularized manner. We demonstrate how wavelets can be\nused as a localized basis for reconstructing permeability fields with sharp\ninterfaces from noisy pointwise pressure field measurements in the context of\nthe elliptic inverse problem. For this we derive the adjoint method of\nminimizing the Besov-norm-regularized misfit functional (this corresponds to\ndetermining the maximum a posteriori point in the Bayesian point of view) in\nthe Haar wavelet setting. As it turns out, choosing a wavelet--based prior\nallows for accelerated optimization compared to established\ntrigonometrically--based priors.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 06:43:52 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 19:40:09 GMT"}, {"version": "v3", "created": "Sun, 7 Jul 2019 18:10:32 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Wacker", "Philipp", ""], ["Knabner", "Peter", ""]]}, {"id": "1710.07634", "submitter": "Anthony Torres", "authors": "A. Torres-Hernandez, F. Brambila-Paz", "title": "Fractional Newton-Raphson Method", "comments": null, "journal-ref": "Applied Mathematics and Sciences: An International Journal\n  (MathSJ), 8:1-13, 2021", "doi": "10.5121/mathsj.2021.8101", "report-no": "https://airccse.com/mathsj/papers/8121mathsj01.pdf", "categories": "math.NA cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Newton-Raphson (N-R) method is useful to find the roots of a polynomial\nof degree n. However, this method is limited since it diverges for the case in\nwhich polynomials only have complex roots if a real initial condition is taken.\nIn the present work, we explain an iterative method that is created using the\nfractional calculus, which we will call the Fractional Newton-Raphson (F N-R)\nMethod, which has the ability to enter the space of complex numbers given a\nreal initial condition, which allows us to find both the real and complex roots\nof a polynomial unlike the classical Newton-Raphson method.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 00:31:53 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 03:45:42 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 06:31:04 GMT"}, {"version": "v4", "created": "Sat, 4 May 2019 17:22:29 GMT"}, {"version": "v5", "created": "Sat, 26 Oct 2019 07:18:49 GMT"}, {"version": "v6", "created": "Sat, 2 Jan 2021 06:21:30 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Torres-Hernandez", "A.", ""], ["Brambila-Paz", "F.", ""]]}, {"id": "1710.07769", "submitter": "Fahreddin Sukru Torun", "authors": "F. Sukru Torun, Murat Manguoglu and Cevdet Aykanat", "title": "A Novel Partitioning Method for Accelerating the Block Cimmino Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel block-row partitioning method in order to improve the\nconvergence rate of the block Cimmino algorithm for solving general sparse\nlinear systems of equations. The convergence rate of the block Cimmino\nalgorithm depends on the orthogonality among the block rows obtained by the\npartitioning method. The proposed method takes numerical orthogonality among\nblock rows into account by proposing a row inner-product graph model of the\ncoefficient matrix. In the graph partitioning formulation defined on this graph\nmodel, the partitioning objective of minimizing the cutsize directly\ncorresponds to minimizing the sum of inter-block inner products between block\nrows thus leading to an improvement in the eigenvalue spectrum of the iteration\nmatrix. This in turn leads to a significant reduction in the number of\niterations required for convergence. Extensive experiments conducted on a large\nset of matrices confirm the validity of the proposed method against a\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 07:26:32 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 13:58:21 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Torun", "F. Sukru", ""], ["Manguoglu", "Murat", ""], ["Aykanat", "Cevdet", ""]]}, {"id": "1710.07771", "submitter": "Konrad Kollnig", "authors": "Konrad Kollnig", "title": "Constrained Optimisation of Rational Functions for Accelerating Subspace\n  Iteration", "comments": "Bachelor's thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earlier this decade, the so-called FEAST algorithm was released for computing\nthe eigenvalues of a matrix in a given interval. Previously, rational filter\nfunctions have been examined as a parameter of FEAST. In this thesis, we expand\non existing work with the following contributions: (i) Obtaining\nwell-performing rational filter functions via standard minimisation algorithms,\n(ii) Obtaining constrained rational filter functions efficiently, and (iii)\nImproving existing rational filter functions algorithmically. Using our new\nrational filter functions, FEAST requires up to one quarter fewer iterations on\naverage compared to state-of-art rational filter functions.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 08:12:20 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Kollnig", "Konrad", ""]]}, {"id": "1710.08308", "submitter": "Xiao-Yang Liu", "authors": "Cuiping Li, Xiao-Yang Liu, and Yue Sun", "title": "Tensor Matched Subspace Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing whether a signal lies within a given subspace, also\nnamed matched subspace detection, has been well studied when the signal is\nrepresented as a vector. However, the matched subspace detection methods based\non vectors can not be applied to the situations that signals are naturally\nrepresented as multi-dimensional data arrays or tensors. Considering that\ntensor subspaces and orthogonal projections onto these subspaces are well\ndefined in the recently proposed transform-based tensor model, which motivates\nus to investigate the problem of matched subspace detection in high dimensional\ncase. In this paper, we propose an approach for tensor matched subspace\ndetection based on the transform-based tensor model with tubal-sampling and\nelementwise-sampling, respectively. First, we construct estimators based on\ntubal-sampling and elementwise-sampling to estimate the energy of a signal\noutside a given subspace of a third-order tensor and then give the probability\nbounds of our estimators, which show that our estimators work effectively when\nthe sample size is greater than a constant. Secondly, the detectors both for\nnoiseless data and noisy data are given, and the corresponding detection\nperformance analyses are also provided. Finally, based on discrete Fourier\ntransform (DFT) and discrete cosine transform (DCT), the performance of our\nestimators and detectors are evaluated by several simulations, and simulation\nresults verify the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 14:42:08 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 12:19:22 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 09:47:18 GMT"}, {"version": "v4", "created": "Sun, 22 Apr 2018 02:43:33 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Li", "Cuiping", ""], ["Liu", "Xiao-Yang", ""], ["Sun", "Yue", ""]]}, {"id": "1710.08812", "submitter": "Stephane Chretien", "authors": "Stephane Chretien, Nathalie Herr, Jean-Marc Nicod and Christophe\n  Varnier", "title": "Post-Prognostics Decision for Optimizing the Commitment of Fuel Cell\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a post-prognostics decision context, this paper addresses the problem of\nmaximizing the useful life of a platform composed of several parallel machines\nunder service constraint. Application on multi-stack fuel cell systems is\nconsidered. In order to propose a solution to the insufficient durability of\nfuel cells, the purpose is to define a commitment strategy by determining at\neach time the contribution of each fuel cell stack to the global output so as\nto satisfy the demand as long as possible. A relaxed version of the problem is\nintroduced, which makes it potentially solvable for very large instances.\nResults based on computational experiments illustrate the efficiency of the new\napproach, based on the Mirror Prox algorithm, when compared with a simple\nmethod of successive projections onto the constraint sets associated with the\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 08:43:01 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Chretien", "Stephane", ""], ["Herr", "Nathalie", ""], ["Nicod", "Jean-Marc", ""], ["Varnier", "Christophe", ""]]}, {"id": "1710.08883", "submitter": "Saeed Soori", "authors": "Saeed Soori, Aditya Devarakonda, James Demmel, Mert Gurbuzbalaban,\n  Maryam Mehri Dehnavi", "title": "Avoiding Communication in Proximal Methods for Convex Optimization\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast iterative soft thresholding algorithm (FISTA) is used to solve\nconvex regularized optimization problems in machine learning. Distributed\nimplementations of the algorithm have become popular since they enable the\nanalysis of large datasets. However, existing formulations of FISTA communicate\ndata at every iteration which reduces its performance on modern distributed\narchitectures. The communication costs of FISTA, including bandwidth and\nlatency costs, is closely tied to the mathematical formulation of the\nalgorithm. This work reformulates FISTA to communicate data at every k\niterations and reduce data communication when operating on large data sets. We\nformulate the algorithm for two different optimization methods on the Lasso\nproblem and show that the latency cost is reduced by a factor of k while\nbandwidth and floating-point operation costs remain the same. The convergence\nrates and stability properties of the reformulated algorithms are similar to\nthe standard formulations. The performance of communication-avoiding FISTA and\nProximal Newton methods is evaluated on 1 to 1024 nodes for multiple benchmarks\nand demonstrate average speedups of 3-10x with scaling properties that\noutperform the classical algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 16:47:23 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Soori", "Saeed", ""], ["Devarakonda", "Aditya", ""], ["Demmel", "James", ""], ["Gurbuzbalaban", "Mert", ""], ["Dehnavi", "Maryam Mehri", ""]]}, {"id": "1710.09090", "submitter": "Yumiharu Nakano", "authors": "Yumiharu Nakano", "title": "Kernel-based collocation methods for Zakai equations", "comments": "The corrected version of the paper published in Stochastics and\n  Partial Differential Equations: Analysis and Computations (2019), Vol. 7,\n  476-494", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine an application of the kernel-based interpolation to numerical\nsolutions for Zakai equations in nonlinear filtering, and aim to prove its\nrigorous convergence. To this end, we find the class of kernels and the\nstructure of collocation points explicitly under which the process of iterative\ninterpolation is stable. This result together with standard argument in error\nestimation shows that the approximation error is bounded by the order of the\nsquare root of the time step and the error that comes from a single step\ninterpolation. Our theorem is well consistent with the results of numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 07:01:45 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 13:40:37 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 23:56:44 GMT"}, {"version": "v4", "created": "Wed, 7 Nov 2018 05:35:20 GMT"}, {"version": "v5", "created": "Tue, 15 Jan 2019 07:18:17 GMT"}, {"version": "v6", "created": "Wed, 23 Jan 2019 12:33:34 GMT"}, {"version": "v7", "created": "Mon, 28 Oct 2019 06:33:42 GMT"}, {"version": "v8", "created": "Tue, 17 Dec 2019 06:30:13 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Nakano", "Yumiharu", ""]]}, {"id": "1710.09356", "submitter": "Erik Schnetter", "authors": "Alexander B. Atanasov, Erik Schnetter", "title": "Sparse Grid Discretizations based on a Discontinuous Galerkin Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine and extend Sparse Grids as a discretization method for partial\ndifferential equations (PDEs). Solving a PDE in $D$ dimensions has a cost that\ngrows as $O(N^D)$ with commonly used methods. Even for moderate $D$ (e.g.\n$D=3$), this quickly becomes prohibitively expensive for increasing problem\nsize $N$. This effect is known as the Curse of Dimensionality. Sparse Grids\noffer an alternative discretization method with a much smaller cost of $O(N\n\\log^{D-1}N)$. In this paper, we introduce the reader to Sparse Grids, and\nextend the method via a Discontinuous Galerkin approach. We then solve the\nscalar wave equation in up to $6+1$ dimensions, comparing cost and accuracy\nbetween full and sparse grids. Sparse Grids perform far superior, even in three\ndimensions. Our code is freely available as open source, and we encourage the\nreader to reproduce the results we show.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 17:31:06 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Atanasov", "Alexander B.", ""], ["Schnetter", "Erik", ""]]}, {"id": "1710.09854", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni, Jialei Wang, Ji Liu, Tong Zhang", "title": "Gradient Sparsification for Communication-Efficient Distributed\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern large scale machine learning applications require stochastic\noptimization algorithms to be implemented on distributed computational\narchitectures. A key bottleneck is the communication overhead for exchanging\ninformation such as stochastic gradients among different workers. In this\npaper, to reduce the communication cost we propose a convex optimization\nformulation to minimize the coding length of stochastic gradients. To solve the\noptimal sparsification efficiently, several simple and fast algorithms are\nproposed for approximate solution, with theoretical guaranteed for sparseness.\nExperiments on $\\ell_2$ regularized logistic regression, support vector\nmachines, and convolutional neural networks validate our sparsification\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 18:26:43 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Wangni", "Jianqiao", ""], ["Wang", "Jialei", ""], ["Liu", "Ji", ""], ["Zhang", "Tong", ""]]}, {"id": "1710.10364", "submitter": "Jeff Calder", "authors": "Jeff Calder", "title": "Consistency of Lipschitz learning with infinite unlabeled data and\n  finite labeled data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the consistency of Lipschitz learning on graphs in the limit of\ninfinite unlabeled data and finite labeled data. Previous work has conjectured\nthat Lipschitz learning is well-posed in this limit, but is insensitive to the\ndistribution of the unlabeled data, which is undesirable for semi-supervised\nlearning. We first prove that this conjecture is true in the special case of a\nrandom geometric graph model with kernel-based weights. Then we go on to show\nthat on a random geometric graph with self-tuning weights, Lipschitz learning\nis in fact highly sensitive to the distribution of the unlabeled data, and we\nshow how the degree of sensitivity can be adjusted by tuning the weights. In\nboth cases, our results follow from showing that the sequence of learned\nfunctions converges to the viscosity solution of an $\\infty$-Laplace type\nequation, and studying the structure of the limiting equation.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 00:56:14 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 03:42:25 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2019 02:29:10 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Calder", "Jeff", ""]]}, {"id": "1710.10543", "submitter": "Norikazu Saito", "authors": "Norikazu Saito", "title": "Variational analysis of the discontinuous Galerkin time-stepping method\n  for parabolic equations", "comments": "20 pages; The original version was submitted on 29 October 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discontinuous Galerkin (DG) time-stepping method applied to abstract\nevolution equation of parabolic type is studied using a variational approach.\nWe establish the inf-sup condition or Babu\\v{s}ka--Brezzi condition for the DG\nbilinear form. Then, a nearly best approximation property and a nearly\nsymmetric error estimate are obtained as corollaries. Moreover, the optimal\norder error estimates under appropriate regularity assumption on the solution\nare derived as direct applications of the standard interpolation error\nestimates. Our method of analysis is new for the DG time-stepping method; it\ndiffers from previous works by which the method is formulated as the one-step\nmethod. We apply our abstract results to the finite element approximation of a\nsecond order parabolic equation with space-time variable coefficient functions\nin a polyhedral domain, and derive the optimal order error estimates in several\nnorms.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 00:43:38 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 06:09:10 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 01:41:37 GMT"}, {"version": "v4", "created": "Tue, 14 Jan 2020 12:51:09 GMT"}, {"version": "v5", "created": "Thu, 23 Jan 2020 12:32:34 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Saito", "Norikazu", ""]]}, {"id": "1710.10737", "submitter": "Nicolas Loizou", "authors": "Nicolas Loizou, Peter Richt\\'arik", "title": "Linearly convergent stochastic heavy ball method for minimizing\n  generalization error", "comments": "NIPS 2017, Workshop on Optimization for Machine Learning (camera\n  ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we establish the first linear convergence result for the\nstochastic heavy ball method. The method performs SGD steps with a fixed\nstepsize, amended by a heavy ball momentum term. In the analysis, we focus on\nminimizing the expected loss and not on finite-sum minimization, which is\ntypically a much harder problem. While in the analysis we constrain ourselves\nto quadratic loss, the overall objective is not necessarily strongly convex.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 01:49:34 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 02:18:06 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Loizou", "Nicolas", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1710.10781", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai", "title": "Stochastic variance reduced multiplicative update for nonnegative matrix\n  factorization", "comments": "IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF), a dimensionality reduction and factor\nanalysis method, is a special case in which factor matrices have low-rank\nnonnegative constraints. Considering the stochastic learning in NMF, we\nspecifically address the multiplicative update (MU) rule, which is the most\npopular, but which has slow convergence property. This present paper introduces\non the stochastic MU rule a variance-reduced technique of stochastic gradient.\nNumerical comparisons suggest that our proposed algorithms robustly outperform\nstate-of-the-art algorithms across different synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 06:14:17 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 21:45:46 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Kasai", "Hiroyuki", ""]]}, {"id": "1710.10899", "submitter": "Michael Lass", "authors": "Michael Lass and Stephan Mohr and Hendrik Wiebeler and Thomas D.\n  K\\\"uhne and Christian Plessl", "title": "A Massively Parallel Algorithm for the Approximate Calculation of\n  Inverse p-th Roots of Large Sparse Matrices", "comments": null, "journal-ref": null, "doi": "10.1145/3218176.3218231", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the submatrix method, a highly parallelizable method for the\napproximate calculation of inverse p-th roots of large sparse symmetric\nmatrices which are required in different scientific applications. We follow the\nidea of Approximate Computing, allowing imprecision in the final result in\norder to be able to utilize the sparsity of the input matrix and to allow\nmassively parallel execution. For an n x n matrix, the proposed algorithm\nallows to distribute the calculations over n nodes with only little\ncommunication overhead. The approximate result matrix exhibits the same\nsparsity pattern as the input matrix, allowing for efficient reuse of allocated\ndata structures.\n  We evaluate the algorithm with respect to the error that it introduces into\ncalculated results, as well as its performance and scalability. We demonstrate\nthat the error is relatively limited for well-conditioned matrices and that\nresults are still valuable for error-resilient applications like\npreconditioning even for ill-conditioned matrices. We discuss the execution\ntime and scaling of the algorithm on a theoretical level and present a\ndistributed implementation of the algorithm using MPI and OpenMP. We\ndemonstrate the scalability of this implementation by running it on a\nhigh-performance compute cluster comprised of 1024 CPU cores, showing a speedup\nof 665x compared to single-threaded execution.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 12:33:35 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 13:18:53 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 13:50:40 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Lass", "Michael", ""], ["Mohr", "Stephan", ""], ["Wiebeler", "Hendrik", ""], ["K\u00fchne", "Thomas D.", ""], ["Plessl", "Christian", ""]]}, {"id": "1710.11016", "submitter": "Alexandre Marques", "authors": "Alexandre Noll Marques, Jean-Christophe Nave, and Rodolfo Ruben\n  Rosales", "title": "Imposing jump conditions on nonconforming interfaces for the Correction\n  Function Method: a least squares approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a technique that simplifies the problem of imposing jump\nconditions on interfaces that are not aligned with a computational grid in the\ncontext of the Correction Function Method (CFM). The CFM offers a general\nframework to solve Poisson's equation in the presence of discontinuities to\nhigh order of accuracy, while using a compact discretization stencil. A key\nconcept behind the CFM is enforcing the jump conditions in a least squares\nsense. This concept requires computing integrals over sections of the\ninterface, which is a challenge in 3-D when only an implicit representation of\nthe interface is available (e.g., the zero contour of a level set function).\nThe technique introduced here is based on a new formulation of the least\nsquares procedure that relies only on integrals over domains that are amenable\nto simple quadrature after local coordinate transformations. We incorporate\nthis technique into a fourth order accurate implementation of the CFM, and show\nexamples of solutions to Poisson's equation computed in 2-D and 3-D.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 15:37:12 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 04:11:58 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 17:32:49 GMT"}, {"version": "v4", "created": "Wed, 10 Jul 2019 09:52:44 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Marques", "Alexandre Noll", ""], ["Nave", "Jean-Christophe", ""], ["Rosales", "Rodolfo Ruben", ""]]}, {"id": "1710.11284", "submitter": "Athena Picarelli", "authors": "Athena Picarelli, Christoph Reisinger, Julen Rotaetxe Arto", "title": "Some regularity and convergence results for parabolic\n  Hamilton-Jacobi-Bellman equations in bounded domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximation of parabolic Hamilton-Jacobi-Bellman (HJB)\nequations in bounded domains with strong Dirichlet boundary conditions. We work\nunder the assumption of the existence of a sufficiently regular barrier\nfunction for the problem to obtain well-posedness and regularity of a related\nswitching system and the convergence of its components to the HJB equation. In\nparticular, we show existence of a viscosity solution to the switching system\nby a novel construction of sub- and supersolutions and application of Perron's\nmethod. Error bounds for monotone schemes for the HJB equation are then derived\nfrom estimates near the boundary, where the standard regularisation procedure\nfor viscosity solutions is not applicable, and are found to be of the same\norder as known results for the whole space. We deduce error bounds for some\ncommon finite difference and truncated semi-Lagrangian schemes.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 00:53:55 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 09:25:48 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 13:57:22 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Picarelli", "Athena", ""], ["Reisinger", "Christoph", ""], ["Arto", "Julen Rotaetxe", ""]]}, {"id": "1710.11298", "submitter": "Dong Xia", "authors": "Dong Xia and Ming Yuan", "title": "Effective Tensor Sketching via Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate effective sketching schemes via sparsification\nfor high dimensional multilinear arrays or tensors. More specifically, we\npropose a novel tensor sparsification algorithm that retains a subset of the\nentries of a tensor in a judicious way, and prove that it can attain a given\nlevel of approximation accuracy in terms of tensor spectral norm with a much\nsmaller sample complexity when compared with existing approaches. In\nparticular, we show that for a $k$th order $d\\times\\cdots\\times d$ cubic tensor\nof {\\it stable rank} $r_s$, the sample size requirement for achieving a\nrelative error $\\varepsilon$ is, up to a logarithmic factor, of the order\n$r_s^{1/2} d^{k/2} /\\varepsilon$ when $\\varepsilon$ is relatively large, and\n$r_s d /\\varepsilon^2$ and essentially optimal when $\\varepsilon$ is\nsufficiently small. It is especially noteworthy that the sample size\nrequirement for achieving a high accuracy is of an order independent of $k$. To\nfurther demonstrate the utility of our techniques, we also study how higher\norder singular value decomposition (HOSVD) of large tensors can be efficiently\napproximated via sparsification.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 02:04:39 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 15:31:16 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 14:20:51 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Xia", "Dong", ""], ["Yuan", "Ming", ""]]}]