[{"id": "1503.00021", "submitter": "Alex Gorodetsky", "authors": "Alex A. Gorodetsky and Youssef M. Marzouk", "title": "Mercer kernels and integrated variance experimental design: connections\n  between Gaussian process regression and polynomial approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines experimental design procedures used to develop surrogates\nof computational models, exploring the interplay between experimental designs\nand approximation algorithms. We focus on two widely used approximation\napproaches, Gaussian process (GP) regression and non-intrusive polynomial\napproximation. First, we introduce algorithms for minimizing a posterior\nintegrated variance (IVAR) design criterion for GP regression. Our formulation\ntreats design as a continuous optimization problem that can be solved with\ngradient-based methods on complex input domains, without resorting to greedy\napproximations. We show that minimizing IVAR in this way yields point sets with\ngood interpolation properties, and that it enables more accurate GP regression\nthan designs based on entropy minimization or mutual information maximization.\nSecond, using a Mercer kernel/eigenfunction perspective on GP regression, we\nidentify conditions under which GP regression coincides with pseudospectral\npolynomial approximation. Departures from these conditions can be understood as\nchanges either to the kernel or to the experimental design itself. We then show\nhow IVAR-optimal designs, while sacrificing discrete orthogonality of the\nkernel eigenfunctions, can yield lower approximation error than orthogonalizing\npoint sets. Finally, we compare the performance of adaptive Gaussian process\nregression and adaptive pseudospectral approximation for several classes of\ntarget functions, identifying features that are favorable to the GP + IVAR\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 21:50:03 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 19:58:16 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 18:41:42 GMT"}, {"version": "v4", "created": "Fri, 29 Apr 2016 03:05:27 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Gorodetsky", "Alex A.", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1503.00034", "submitter": "Varun Shankar", "authors": "Varun Shankar and Sarah D. Olson", "title": "Radial Basis Function (RBF)-based Parametric Models for Closed and Open\n  Curves within the Method of Regularized Stokeslets", "comments": "23 pages, 11 figures, 1 table", "journal-ref": null, "doi": "10.1002/fld.4048", "report-no": null, "categories": "math.NA cs.NA q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of regularized Stokeslets (MRS) is a numerical approach using\nregularized fundamental solutions to compute the flow due to an object in a\nviscous fluid where inertial effects can be neglected. The elastic object is\nrepresented as a Lagrangian structure, exerting point forces on the fluid. The\nforces on the structure are often determined by a bending or tension model,\npreviously calculated using finite difference approximations. In this paper, we\nstudy Spherical Basis Function (SBF), Radial Basis Function (RBF) and\nLagrange-Chebyshev parametric models to represent and calculate forces on\nelastic structures that can be represented by an open curve, motivated by the\nstudy of cilia and flagella. The evaluation error for static open curves for\nthe different interpolants, as well as errors for calculating normals and\nsecond derivatives using different types of clustered parametric nodes, are\ngiven for the case of an open planar curve. We determine that SBF and RBF\ninterpolants built on clustered nodes are competitive with Lagrange-Chebyshev\ninterpolants for modeling twice-differentiable open planar curves. We propose\nusing SBF and RBF parametric models within the MRS for evaluating and updating\nthe elastic structure. Results for open and closed elastic structures immersed\nin a 2D fluid are presented, showing the efficacy of the RBF-Stokeslets method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 23:35:59 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Shankar", "Varun", ""], ["Olson", "Sarah D.", ""]]}, {"id": "1503.01375", "submitter": "Tamara Kolda", "authors": "Tamara G. Kolda", "title": "Symmetric Orthogonal Tensor Decomposition is Trivial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decomposing a real-valued symmetric tensor as the\nsum of outer products of real-valued, pairwise orthogonal vectors. Such\ndecompositions do not generally exist, but we show that some symmetric tensor\ndecomposition problems can be converted to orthogonal problems following the\nwhitening procedure proposed by Anandkumar et al. (2012). If an orthogonal\ndecomposition of an $m$-way $n$-dimensional symmetric tensor exists, we propose\na novel method to compute it that reduces to an $n \\times n$ symmetric matrix\neigenproblem. We provide numerical results demonstrating the effectiveness of\nthe method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 16:39:51 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Kolda", "Tamara G.", ""]]}, {"id": "1503.01752", "submitter": "Yin Tat Lee", "authors": "Yin Tat Lee and Aaron Sidford", "title": "Efficient Inverse Maintenance and Faster Algorithms for Linear\n  Programming", "comments": "In an older version of this paper, we mistakenly claimed an improved\n  running time for Dikin walk by noting solely the improved running time for\n  linear system solving and ignoring the determinant computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the following inverse maintenance problem: given\n$A \\in \\mathbb{R}^{n\\times d}$ and a number of rounds $r$, we receive a\n$n\\times n$ diagonal matrix $D^{(k)}$ at round $k$ and we wish to maintain an\nefficient linear system solver for $A^{T}D^{(k)}A$ under the assumption\n$D^{(k)}$ does not change too rapidly. This inverse maintenance problem is the\ncomputational bottleneck in solving multiple optimization problems. We show how\nto solve this problem with $\\tilde{O}(nnz(A)+d^{\\omega})$ preprocessing time\nand amortized $\\tilde{O}(nnz(A)+d^{2})$ time per round, improving upon previous\nrunning times for solving this problem.\n  Consequently, we obtain the fastest known running times for solving multiple\nproblems including, linear programming and computing a rounding of a polytope.\nIn particular given a feasible point in a linear program with $d$ variables,\n$n$ constraints, and constraint matrix $A\\in\\mathbb{R}^{n\\times d}$, we show\nhow to solve the linear program in time\n$\\tilde{O}(nnz(A)+d^{2})\\sqrt{d}\\log(\\epsilon^{-1}))$. We achieve our results\nthrough a novel combination of classic numerical techniques of low rank update,\npreconditioning, and fast matrix multiplication as well as recent work on\nsubspace embeddings and spectral sparsification that we hope will be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 20:12:13 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 19:48:25 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 13:06:14 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""]]}, {"id": "1503.01889", "submitter": "Julian Hall Dr", "authors": "Q. Huangfu and J. A. J. Hall", "title": "Parallelizing the dual revised simplex method", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": "ERGO-14-011", "categories": "math.OC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the design and implementation of two parallel dual\nsimplex solvers for general large scale sparse linear programming problems. One\napproach, called PAMI, extends a relatively unknown pivoting strategy called\nsuboptimization and exploits parallelism across multiple iterations. The other,\ncalled SIP, exploits purely single iteration parallelism by overlapping\ncomputational components when possible. Computational results show that the\nperformance of PAMI is superior to that of the leading open-source simplex\nsolver, and that SIP complements PAMI in achieving speedup when PAMI results in\nslowdown. One of the authors has implemented the techniques underlying PAMI\nwithin the FICO Xpress simplex solver and this paper presents computational\nresults demonstrating their value. This performance increase is sufficiently\nvaluable for the achievement to be used as the basis of promotional material by\nFICO. In developing the first parallel revised simplex solver of general\nutility and commercial importance, this work represents a significant\nachievement in computational optimization.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 09:40:41 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Huangfu", "Q.", ""], ["Hall", "J. A. J.", ""]]}, {"id": "1503.02552", "submitter": "Avram Sidi", "authors": "Avram Sidi", "title": "Minimal Polynomial and Reduced Rank Extrapolation Methods Are Related", "comments": null, "journal-ref": "Advances in Computational Mathematics, 43:151--170, 2017", "doi": null, "report-no": "CS-2015-02", "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimal Polynomial Extrapolation (MPE) and Reduced Rank Extrapolation (RRE)\nare two polynomial methods used for accelerating the convergence of sequences\nof vectors $\\{{x}_m\\}$. They are applied successfully in conjunction with\nfixed-point iterative schemes in the solution of large and sparse systems of\nlinear and nonlinear equations in different disciplines of science and\nengineering. Both methods produce approximations $s_k$ to the limit or\nantilimit of $\\{{x}_m\\}$ that are of the form ${s}_k=\\sum^k_{i=0}\\gamma_i{x}_i$\nwith $\\sum^k_{i=0}\\gamma_i=1$, for some scalars $\\gamma_i$. The way the two\nmethods are derived suggests that they might, somehow, be related to each\nother; this has not been explored so far, however. In this work, we tackle this\nissue and show that the vectors $s_{k}^\\text{MPE}$ and $s_k^\\text{RRE}$\nproduced by the two methods are related in more than one way, and independently\nof the way the $x_m$ are generated. One of our results states that RRE\nstagnates, in the sense that ${s}_k^\\text{RRE}={s}_{k-1}^\\text{RRE}$, if and\nonly if ${s}_{k}^\\text{MPE}$ does not exist. Another result states that, when\n${s}_{k}^\\text{MPE}$ exists, there holds $$\\mu_k{s}_k^\\text{RRE} =\n\\mu_{k-1}{s}_{k-1}^\\text{RRE} + \\nu_k{s}_{k}^\\text{MPE} \\quad \\text{with} \\quad\n\\mu_k = \\mu_{k-1} + \\nu_k,$$ for some positive scalars $\\mu_k$, $\\mu_{k-1}$,\nand $\\nu_k$ that depend only on ${s}_k^\\text{RRE}$, ${s}_{k-1}^\\text{RRE}$, and\n${s}_{k}^\\text{MPE}$, respectively. Our results are valid when MPE and RRE are\ndefined in any weighted inner product and the norm induced by it. They also\ncontain as special cases the known results pertaining to the connection between\nthe method of Arnoldi and the method of generalized minimal residuals, two\nimportant Krylov subspace methods for solving nonsingular linear systems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 16:37:50 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 09:18:46 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Sidi", "Avram", ""]]}, {"id": "1503.02737", "submitter": "Art Owen", "authors": "K. Basu and A. B. Owen", "title": "Scrambled geometric net integration over general product spaces", "comments": "29 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Monte Carlo (QMC) sampling has been developed for integration over\n$[0,1]^s$ where it has superior accuracy to Monte Carlo (MC) for integrands of\nbounded variation. Scrambled net quadrature gives allows replication based\nerror estimation for QMC with at least the same accuracy and for smooth enough\nintegrands even better accuracy than plain QMC. Integration over triangles,\nspheres, disks and Cartesian products of such spaces is more difficult for QMC\nbecause the induced integrand on a unit cube may fail to have the desired\nregularity. In this paper, we present a construction of point sets for\nnumerical integration over Cartesian products of $s$ spaces of dimension $d$,\nwith triangles ($d=2$) being of special interest. The point sets are\ntransformations of randomized $(t,m,s)$-nets using recursive geometric\npartitions. The resulting integral estimates are unbiased and their variance is\n$o(1/n)$ for any integrand in $L^2$ of the product space. Under smoothness\nassumptions on the integrand, our randomized QMC algorithm has variance\n$O(n^{-1 - 2/d} (\\log n)^{s-1})$, for integration over $s$-fold Cartesian\nproducts of $d$-dimensional domains, compared to $O(n^{-1})$ for ordinary Monte\nCarlo.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 00:12:57 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Basu", "K.", ""], ["Owen", "A. B.", ""]]}, {"id": "1503.02828", "submitter": "Mingkui Tan", "authors": "Mingkui Tan and Shijie Xiao and Junbin Gao and Dong Xu and Anton Van\n  Den Hengel and Qinfeng Shi", "title": "Scalable Nuclear-norm Minimization by Subspace Pursuit Proximal\n  Riemannian Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear-norm regularization plays a vital role in many learning tasks, such\nas low-rank matrix recovery (MR), and low-rank representation (LRR). Solving\nthis problem directly can be computationally expensive due to the unknown rank\nof variables or large-rank singular value decompositions (SVDs). To address\nthis, we propose a proximal Riemannian gradient (PRG) scheme which can\nefficiently solve trace-norm regularized problems defined on real-algebraic\nvariety $\\mMLr$ of real matrices of rank at most $r$. Based on PRG, we further\npresent a simple and novel subspace pursuit (SP) paradigm for general\ntrace-norm regularized problems without the explicit rank constraint $\\mMLr$.\nThe proposed paradigm is very scalable by avoiding large-rank SVDs. Empirical\nstudies on several tasks, such as matrix completion and LRR based subspace\nclustering, demonstrate the superiority of the proposed paradigms over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 09:42:17 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 02:20:44 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Tan", "Mingkui", ""], ["Xiao", "Shijie", ""], ["Gao", "Junbin", ""], ["Xu", "Dong", ""], ["Hengel", "Anton Van Den", ""], ["Shi", "Qinfeng", ""]]}, {"id": "1503.03004", "submitter": "German Ros", "authors": "German Ros and Julio Guerrero", "title": "Fast and Robust Fixed-Rank Matrix Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of efficient sparse fixed-rank (S-FR) matrix\ndecomposition, i.e., splitting a corrupted matrix $M$ into an uncorrupted\nmatrix $L$ of rank $r$ and a sparse matrix of outliers $S$. Fixed-rank\nconstraints are usually imposed by the physical restrictions of the system\nunder study. Here we propose a method to perform accurate and very efficient\nS-FR decomposition that is more suitable for large-scale problems than existing\napproaches. Our method is a grateful combination of geometrical and algebraical\ntechniques, which avoids the bottleneck caused by the Truncated SVD (TSVD).\nInstead, a polar factorization is used to exploit the manifold structure of\nfixed-rank problems as the product of two Stiefel and an SPD manifold, leading\nto a better convergence and stability. Then, closed-form projectors help to\nspeed up each iteration of the method. We introduce a novel and fast projector\nfor the $\\text{SPD}$ manifold and a proof of its validity. Further acceleration\nis achieved using a Nystrom scheme. Extensive experiments with synthetic and\nreal data in the context of robust photometric stereo and spectral clustering\nshow that our proposals outperform the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 17:35:46 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 21:43:00 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2015 09:26:04 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Ros", "German", ""], ["Guerrero", "Julio", ""]]}, {"id": "1503.03355", "submitter": "Evangelos Papalexakis", "authors": "Evangelos E. Papalexakis", "title": "Automatic Unsupervised Tensor Mining with Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular tool for unsupervised modelling and mining multi-aspect data is\ntensor decomposition. In an exploratory setting, where and no labels or ground\ntruth are available how can we automatically decide how many components to\nextract? How can we assess the quality of our results, so that a domain expert\ncan factor this quality measure in the interpretation of our results? In this\npaper, we introduce AutoTen, a novel automatic unsupervised tensor mining\nalgorithm with minimal user intervention, which leverages and improves upon\nheuristics that assess the result quality. We extensively evaluate AutoTen's\nperformance on synthetic data, outperforming existing baselines on this very\nhard problem. Finally, we apply AutoTen on a variety of real datasets,\nproviding insights and discoveries. We view this work as a step towards a fully\nautomated, unsupervised tensor mining tool that can be easily adopted by\npractitioners in academia and industry.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 14:34:46 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Papalexakis", "Evangelos E.", ""]]}, {"id": "1503.03903", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail", "title": "Approximating Sparse PCA from Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how well one can recover sparse principal components of a data\nmatrix using a sketch formed from a few of its elements. We show that for a\nwide class of optimization problems, if the sketch is close (in the spectral\nnorm) to the original data matrix, then one can recover a near optimal solution\nto the optimization problem by using the sketch. In particular, we use this\napproach to obtain sparse principal components and show that for \\math{m} data\npoints in \\math{n} dimensions, \\math{O(\\epsilon^{-2}\\tilde k\\max\\{m,n\\})}\nelements gives an \\math{\\epsilon}-additive approximation to the sparse PCA\nproblem (\\math{\\tilde k} is the stable rank of the data matrix). We demonstrate\nour algorithms extensively on image, text, biological and financial data. The\nresults show that not only are we able to recover the sparse PCAs from the\nincomplete data, but by using our sparse sketch, the running time drops by a\nfactor of five or more.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 22:16:55 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Kundu", "Abhisek", ""], ["Drineas", "Petros", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1503.04500", "submitter": "Zhongxiao Jia", "authors": "Zhongxiao Jia and Wenjie Kang", "title": "A Residual Based Sparse Approximate Inverse Preconditioning Procedure\n  for Large Sparse Linear Systems", "comments": "18 pages, 1 figure", "journal-ref": "Numerical Linear Algebra with Applications, 24 (2) (2017): e2080", "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SPAI algorithm, a sparse approximate inverse preconditioning technique\nfor large sparse linear systems, proposed by Grote and Huckle [SIAM J. Sci.\nComput., 18 (1997), pp.~838--853.], is based on the F-norm minimization and\ncomputes a sparse approximate inverse $M$ of a large sparse matrix $A$\nadaptively. However, SPAI may be costly to seek the most profitable indices at\neach loop and $M$ may be ineffective for preconditioning. In this paper, we\npropose a residual based sparse approximate inverse preconditioning procedure\n(RSAI), which, unlike SPAI, is based on only the {\\em dominant} rather than all\ninformation on the current residual and augments sparsity patterns adaptively\nduring the loops. RSAI is less costly to seek indices and is more effective to\ncapture a good approximate sparsity pattern of $A^{-1}$ than SPAI. To control\nthe sparsity of $M$ and reduce computational cost, we develop a practical\nRSAI($tol$) algorithm that drops small nonzero entries adaptively during the\nprocess. Numerical experiments are reported to demonstrate that RSAI($tol$) is\nat least competitive with SPAI and can be considerably more efficient and\neffective than SPAI. They also indicate that RSAI($tol$) is comparable to the\nPSAI($tol$) algorithm proposed by one of the authors in 2009.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 02:05:26 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 02:14:12 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2015 02:30:07 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2015 02:05:46 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Jia", "Zhongxiao", ""], ["Kang", "Wenjie", ""]]}, {"id": "1503.05214", "submitter": "Tarek Elgamal", "authors": "Tarek Elgamal, Mohamed Hefeeda", "title": "Analysis of PCA Algorithms in Distributed Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical machine learning algorithms often face scalability bottlenecks when\nthey are applied to large-scale data. Such algorithms were designed to work\nwith small data that is assumed to fit in the memory of one machine. In this\nreport, we analyze different methods for computing an important machine learing\nalgorithm, namely Principal Component Analysis (PCA), and we comment on its\nlimitations in supporting large datasets. The methods are analyzed and compared\nacross two important metrics: time complexity and communication complexity. We\nconsider the worst-case scenarios for both metrics, and we identify the\nsoftware libraries that implement each method. The analysis in this report\nhelps researchers and engineers in (i) understanding the main bottlenecks for\nscalability in different PCA algorithms, (ii) choosing the most appropriate\nmethod and software library for a given application and data set\ncharacteristics, and (iii) designing new scalable PCA algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 20:38:15 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 12:05:02 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Elgamal", "Tarek", ""], ["Hefeeda", "Mohamed", ""]]}, {"id": "1503.05468", "submitter": "Kazuaki Tanaka", "authors": "Kazuaki Tanaka, Kouta Sekine, Makoto Mizuguchi, and Shin'ichi Oishi", "title": "Sharp numerical inclusion of the best constant for embedding\n  $H_{0}^{1}(\\Omega) \\hookrightarrow L^{p}(\\Omega)$ on bounded convex domain", "comments": "12 pages, 1 figure", "journal-ref": "J. Comput. Appl. Math. 311 (2017) 306-313", "doi": "10.1016/j.cam.2016.07.021", "report-no": null, "categories": "math.NA cs.NA math.DG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a verified numerical method for obtaining a sharp\ninclusion of the best constant for the embedding $H_{0}^{1}(\\Omega)\n\\hookrightarrow L^{p}(\\Omega)$ on bounded convex domain in $\\mathbb{R}^{2}$. We\nestimate the best constant by computing the corresponding extremal function\nusing a verified numerical computation. Verified numerical inclusions of the\nbest constant on a square domain are presented.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 16:21:30 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2015 12:11:42 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 08:18:32 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Tanaka", "Kazuaki", ""], ["Sekine", "Kouta", ""], ["Mizuguchi", "Makoto", ""], ["Oishi", "Shin'ichi", ""]]}, {"id": "1503.05947", "submitter": "Yanlai Chen", "authors": "Yanlai Chen", "title": "Reduced Basis Decomposition: a Certified and Fast Lossy Data Compression\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.AI cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is often needed in the area of data mining. The goal of\nthese methods is to map the given high-dimensional data into a low-dimensional\nspace preserving certain properties of the initial data. There are two kinds of\ntechniques for this purpose. The first, projective methods, builds an explicit\nlinear projection from the high-dimensional space to the low-dimensional one.\nOn the other hand, the nonlinear methods utilizes nonlinear and implicit\nmapping between the two spaces. In both cases, the methods considered in\nliterature have usually relied on computationally very intensive matrix\nfactorizations, frequently the Singular Value Decomposition (SVD). The\ncomputational burden of SVD quickly renders these dimension reduction methods\ninfeasible thanks to the ever-increasing sizes of the practical datasets.\n  In this paper, we present a new decomposition strategy, Reduced Basis\nDecomposition (RBD), which is inspired by the Reduced Basis Method (RBM). Given\n$X$ the high-dimensional data, the method approximates it by $Y \\, T (\\approx\nX)$ with $Y$ being the low-dimensional surrogate and $T$ the transformation\nmatrix. $Y$ is obtained through a greedy algorithm thus extremely efficient. In\nfact, it is significantly faster than SVD with comparable accuracy. $T$ can be\ncomputed on the fly. Moreover, unlike many compression algorithms, it easily\nfinds the mapping for an arbitrary ``out-of-sample'' vector and it comes with\nan ``error indicator'' certifying the accuracy of the compression. Numerical\nresults are shown validating these claims.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 21:10:57 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Chen", "Yanlai", ""]]}, {"id": "1503.06394", "submitter": "Insu Han", "authors": "Insu Han, Dmitry Malioutov, Jinwoo Shin", "title": "Large-scale Log-determinant Computation through Stochastic Chebyshev\n  Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Logarithms of determinants of large positive definite matrices appear\nubiquitously in machine learning applications including Gaussian graphical and\nGaussian process models, partition functions of discrete graphical models,\nminimum-volume ellipsoids, metric learning and kernel learning. Log-determinant\ncomputation involves the Cholesky decomposition at the cost cubic in the number\nof variables, i.e., the matrix dimension, which makes it prohibitive for\nlarge-scale applications. We propose a linear-time randomized algorithm to\napproximate log-determinants for very large-scale positive definite and general\nnon-singular matrices using a stochastic trace approximation, called the\nHutchinson method, coupled with Chebyshev polynomial expansions that both rely\non efficient matrix-vector multiplications. We establish rigorous additive and\nmultiplicative approximation error bounds depending on the condition number of\nthe input matrix. In our experiments, the proposed algorithm can provide very\nhigh accuracy solutions at orders of magnitude faster time than the Cholesky\ndecomposition and Schur completion, and enables us to compute log-determinants\nof matrices involving tens of millions of variables.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 06:55:12 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Han", "Insu", ""], ["Malioutov", "Dmitry", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1503.06561", "submitter": "Ankit Gupta", "authors": "Ankit Gupta, Ashish Oberoi", "title": "A Comparative Analysis of Tensor Decomposition Models Using Hyper\n  Spectral Image", "comments": "7 pages, 3 figures,1 table", "journal-ref": "International Journal of Computer Science Trends and Technology\n  (IJCST) V3(2): Page(5-11) Mar-Apr 2015. ISSN: 2347-8578", "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper spectral imaging is a remote sensing technology, providing variety of\napplications such as material identification, space object identification,\nplanetary exploitation etc. It deals with capturing continuum of images of the\nearth surface from different angles. Due to the multidimensional nature of the\nimage, multi-way arrays are one of the possible solutions for analyzing hyper\nspectral data. This multi-way array is called tensor. Our approach deals with\nimplementing three decomposition models LMLRA, BTD and CPD to the sample data\nfor choosing the best decomposition of the data set. The results have proved\nthat Block Term Decomposition (BTD) is the best tensor model for decomposing\nthe hyper spectral image in to resultant factor matrices.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:08:50 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Gupta", "Ankit", ""], ["Oberoi", "Ashish", ""]]}, {"id": "1503.08360", "submitter": "Kalyana Babu Nakshatrala", "authors": "S. Karimi and K. B. Nakshatrala", "title": "Do current lattice Boltzmann methods for diffusion and diffusion-type\n  equations respect maximum principles and the non-negative constraint?", "comments": null, "journal-ref": null, "doi": "10.4208/cicp.181015.270416a", "report-no": null, "categories": "cs.NA math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lattice Boltzmann method (LBM) has established itself as a valid\nnumerical method in computational fluid dynamics. Recently,\nmultiple-relaxation-time LBM has been proposed to simulate anisotropic\nadvection-diffusion processes. The governing differential equations of\nadvective-diffusive systems are known to satisfy maximum principles, comparison\nprinciples, the non-negative constraint, and the decay property. In this paper,\nit will be shown that current single- and multiple-relaxation-time lattice\nBoltzmann methods fail to preserve these mathematical properties for transient\ndiffusion-type equations. It will also be shown that the discretization of\nDirichlet boundary conditions will affect the performance of lattice Boltzmann\nmethods in meeting these mathematical principles. A new way of discretizing the\nDirichlet boundary conditions is also proposed. Several benchmark problems have\nbeen solved to illustrate the performance of lattice Boltzmann methods and the\neffect of discretization of boundary conditions with respect to the\naforementioned mathematical properties for transient diffusion and\nadvection-diffusion equations.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 22:16:11 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 03:03:44 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Karimi", "S.", ""], ["Nakshatrala", "K. B.", ""]]}, {"id": "1503.08462", "submitter": "Hehu Xie", "authors": "Ning Zhang, Xiaole Han, Yunhui He, Hehu Xie and Chun'guang You", "title": "An Algebraic Multigrid Method for Eigenvalue Problems in Some Different\n  Cases", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to develop an algebraic multigrid method to solve\neigenvalue problems based on the combination of the multilevel correction\nscheme and the algebraic multigrid method for linear equations. Our approach\nuses the algebraic multigrid method setup procedure to construct the hierarchy\nand the intergrid transfer operators. In this algebraic multigrid scheme, a\nlarge scale eigenvalue problem is solved by some algebraic multigrid smoothing\nsteps in the hierarchy and very small-dimensional eigenvalue problems. To\nemphasize the efficiency and flexibility of the proposed method, here we\nconsider a set of test eigenvalue problems, discretized on unstructured meshes,\nwith different shape of domain, singularity, and discontinuous parameters.\nMoreover, global convergence independent of the number of desired eigenvalues\nis obtained.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 17:26:12 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 10:08:58 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Zhang", "Ning", ""], ["Han", "Xiaole", ""], ["He", "Yunhui", ""], ["Xie", "Hehu", ""], ["You", "Chun'guang", ""]]}, {"id": "1503.08509", "submitter": "Luke Olson", "authors": "Natalie N. Beams, Luke N. Olson, Jonathan B. Freund", "title": "A Finite Element Based P3M Method for N-body Problems", "comments": "20 pages, submitted to SISC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a fast mesh-based method for computing N-body interactions that\nis both scalable and accurate. The method is founded on a\nparticle-particle--particle-mesh P3M approach, which decomposes a potential\ninto rapidly decaying short-range interactions and smooth, mesh-resolvable\nlong-range interactions. However, in contrast to the traditional approach of\nusing Gaussian screen functions to accomplish this decomposition, our method\nemploys specially designed polynomial bases to construct the screened\npotentials. Because of this form of the screen, the long-range component of the\npotential is then solved exactly with a finite element method, leading\nultimately to a sparse matrix problem that is solved efficiently with standard\nmultigrid methods. Moreover, since this system represents an exact\ndiscretization, the optimal resolution properties of the FFT are unnecessary,\nthough the short-range calculation is now more involved than P3M/PME methods.\nWe introduce the method, analyze its key properties, and demonstrate the\naccuracy of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 00:12:51 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Beams", "Natalie N.", ""], ["Olson", "Luke N.", ""], ["Freund", "Jonathan B.", ""]]}, {"id": "1503.08601", "submitter": "Tasuku Soma", "authors": "Yuji Nakatsukasa, Tasuku Soma, and Andr\\'e Uschmajew", "title": "Finding a low-rank basis in a matrix subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given matrix subspace, how can we find a basis that consists of\nlow-rank matrices? This is a generalization of the sparse vector problem. It\nturns out that when the subspace is spanned by rank-1 matrices, the matrices\ncan be obtained by the tensor CP decomposition. For the higher rank case, the\nsituation is not as straightforward. In this work we present an algorithm based\non a greedy process applicable to higher rank problems. Our algorithm first\nestimates the minimum rank by applying soft singular value thresholding to a\nnuclear norm relaxation, and then computes a matrix with that rank using the\nmethod of alternating projections. We provide local convergence results, and\ncompare our algorithm with several alternative approaches. Applications include\ndata compression beyond the classical truncated SVD, computing accurate\neigenvectors of a near-multiple eigenvalue, image separation and graph\nLaplacian eigenproblems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 09:04:40 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 02:10:39 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Nakatsukasa", "Yuji", ""], ["Soma", "Tasuku", ""], ["Uschmajew", "Andr\u00e9", ""]]}, {"id": "1503.09079", "submitter": "Gino Montecinos", "authors": "Gino I. Montecinos", "title": "Analytic solutions for the Burgers equation with source terms", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic solutions for Burgers equations with source terms, possibly stiff,\nrepresent an important element to assess numerical schemes. Here we present a\nprocedure, based on the characteristic technique to obtain analytic solutions\nfor these equations with smooth initial conditions.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 22:00:19 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Montecinos", "Gino I.", ""]]}]