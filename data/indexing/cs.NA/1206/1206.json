[{"id": "1206.0129", "submitter": "Yair Censor", "authors": "Yair Censor and Alexander J. Zaslavski", "title": "Convergence and Perturbation Resilience of Dynamic String-Averaging\n  Projection Methods", "comments": "Computational Optimization and Applications, accepted for publication", "journal-ref": null, "doi": "10.1007/s10589-012-9491-x", "report-no": null, "categories": "math.OC cs.NA physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the convex feasibility problem (CFP) in Hilbert space and\nconcentrate on the study of string-averaging projection (SAP) methods for the\nCFP, analyzing their convergence and their perturbation resilience. In the\npast, SAP methods were formulated with a single predetermined set of strings\nand a single predetermined set of weights. Here we extend the scope of the\nfamily of SAP methods to allow iteration-index-dependent variable strings and\nweights and term such methods dynamic string-averaging projection (DSAP)\nmethods. The bounded perturbation resilience of DSAP methods is relevant and\nimportant for their possible use in the framework of the recently developed\nsuperiorization heuristic methodology for constrained minimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2012 09:36:39 GMT"}], "update_date": "2012-06-04", "authors_parsed": [["Censor", "Yair", ""], ["Zaslavski", "Alexander J.", ""]]}, {"id": "1206.0701", "submitter": "Kalyana Babu Nakshatrala", "authors": "K. B. Nakshatrala, H. Nagarajan, and M. Shabouei", "title": "A numerical methodology for enforcing maximum principles and the\n  non-negative constraint for transient diffusion equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient diffusion equations arise in many branches of engineering and\napplied sciences (e.g., heat transfer and mass transfer), and are parabolic\npartial differential equations. It is well-known that, under certain\nassumptions on the input data, these equations satisfy important mathematical\nproperties like maximum principles and the non-negative constraint, which have\nimplications in mathematical modeling. However, existing numerical formulations\nfor these types of equations do not, in general, satisfy maximum principles and\nthe non-negative constraint. In this paper, we present a methodology for\nenforcing maximum principles and the non-negative constraint for transient\nanisotropic diffusion equation. The method of horizontal lines (also known as\nthe Rothe method) is applied in which the time is discretized first. This\nresults in solving steady anisotropic diffusion equation with decay equation at\nevery discrete time level. The proposed methodology for transient anisotropic\ndiffusion equation will satisfy maximum principles and the non-negative\nconstraint on general computational grids, and with no additional restrictions\non the time step. We illustrate the performance and accuracy of the proposed\nformulation using representative numerical examples. We also perform numerical\nconvergence of the proposed methodology. For comparison, we also present the\nresults from the standard single-field semi-discrete formulation and the\nresults from a popular software package, which all will violate maximum\nprinciples and the non-negative constraint.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2012 18:50:29 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2012 02:03:57 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2013 21:29:13 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Nakshatrala", "K. B.", ""], ["Nagarajan", "H.", ""], ["Shabouei", "M.", ""]]}, {"id": "1206.1187", "submitter": "Gleb Beliakov", "authors": "Gleb Beliakov, Michael Johnstone, Doug Creighton, Tim Wilkin", "title": "Parallel random variates generator for GPUs based on normal numbers", "comments": "preprint, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudorandom number generators are required for many computational tasks,\nsuch as stochastic modelling and simulation. This paper investigates the serial\nCPU and parallel GPU implementation of a Linear Congruential Generator based on\nthe binary representation of the normal number $\\alpha_{2,3}$. We adapted two\nmethods of modular reduction which allowed us to perform most operations in\n64-bit integer arithmetic, improving on the original implementation based on\n106-bit double-double operations. We found that our implementation is faster\nthan existing methods in literature, and our generation rate is close to the\nlimiting rate imposed by the efficiency of writing to a GPU's global memory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 11:30:42 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2012 04:42:05 GMT"}], "update_date": "2012-06-25", "authors_parsed": [["Beliakov", "Gleb", ""], ["Johnstone", "Michael", ""], ["Creighton", "Doug", ""], ["Wilkin", "Tim", ""]]}, {"id": "1206.1623", "submitter": "Jason Lee", "authors": "Jason D. Lee, Yuekai Sun, Michael A. Saunders", "title": "Proximal Newton-type methods for minimizing composite functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize Newton-type methods for minimizing smooth functions to handle a\nsum of two convex functions: a smooth function and a nonsmooth function with a\nsimple proximal mapping. We show that the resulting proximal Newton-type\nmethods inherit the desirable convergence behavior of Newton-type methods for\nminimizing smooth functions, even when search directions are computed\ninexactly. Many popular methods tailored to problems arising in bioinformatics,\nsignal processing, and statistical learning are special cases of proximal\nNewton-type methods, and our analysis yields new convergence results for some\nof these methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 21:31:23 GMT"}, {"version": "v10", "created": "Thu, 16 May 2013 15:30:05 GMT"}, {"version": "v11", "created": "Mon, 3 Jun 2013 16:19:20 GMT"}, {"version": "v12", "created": "Wed, 25 Dec 2013 19:55:21 GMT"}, {"version": "v13", "created": "Mon, 17 Mar 2014 22:08:25 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2012 15:14:21 GMT"}, {"version": "v3", "created": "Sun, 14 Oct 2012 21:54:57 GMT"}, {"version": "v4", "created": "Sun, 11 Nov 2012 19:21:12 GMT"}, {"version": "v5", "created": "Mon, 15 Apr 2013 18:28:57 GMT"}, {"version": "v6", "created": "Tue, 16 Apr 2013 16:38:48 GMT"}, {"version": "v7", "created": "Wed, 17 Apr 2013 17:40:34 GMT"}, {"version": "v8", "created": "Mon, 22 Apr 2013 16:24:55 GMT"}, {"version": "v9", "created": "Mon, 29 Apr 2013 18:57:33 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Yuekai", ""], ["Saunders", "Michael A.", ""]]}, {"id": "1206.2061", "submitter": "M. Emre Celebi", "authors": "M. Emre Celebi, Hassan A. Kingravi, Fatih Celiker", "title": "Comments on \"On Approximating Euclidean Metrics by Weighted t-Cost\n  Distances in Arbitrary Dimension\"", "comments": "7 pages, 1 figure, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:1008.4870", "journal-ref": "Pattern Recognition Letters 33 (2012) 1422--1425", "doi": "10.1016/j.patrec.2012.03.002", "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mukherjee (Pattern Recognition Letters, vol. 32, pp. 824-831, 2011) recently\nintroduced a class of distance functions called weighted t-cost distances that\ngeneralize m-neighbor, octagonal, and t-cost distances. He proved that weighted\nt-cost distances form a family of metrics and derived an approximation for the\nEuclidean norm in $\\mathbb{Z}^n$. In this note we compare this approximation to\ntwo previously proposed Euclidean norm approximations and demonstrate that the\nempirical average errors given by Mukherjee are significantly optimistic in\n$\\mathbb{R}^n$. We also propose a simple normalization scheme that improves the\naccuracy of his approximation substantially with respect to both average and\nmaximum relative errors.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2012 22:13:45 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Celebi", "M. Emre", ""], ["Kingravi", "Hassan A.", ""], ["Celiker", "Fatih", ""]]}, {"id": "1206.2812", "submitter": "Jasper Kreeft", "authors": "Jasper Kreeft and Marc Gerritsma", "title": "A priori error estimates for compatible spectral discretization of the\n  Stokes problem for all admissible boundary conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the recently developed mixed mimetic spectral element\nmethod for the Stokes problem in the vorticity-velocity-pressure formulation.\nThis compatible discretization method relies on the construction of a\nconforming discrete Hodge decomposition, that is based on a bounded projection\noperator that commutes with the exterior derivative. The projection operator is\nthe composition of a reduction and a reconstruction step. The reconstruction in\nterms of mimetic spectral element basis-functions are tensor-based\nconstructions and therefore hold for curvilinear quadrilateral and hexahedral\nmeshes. For compatible discretization methods that contain a conforming\ndiscrete Hodge decomposition, we derive optimal a priori error estimates which\nare valid for all admissible boundary conditions on both Cartesian and\ncurvilinear meshes. These theoretical results are confirmed by numerical\nexperiments. These clearly show that the mimetic spectral elements outperform\nthe commonly used H(div)-compatible Raviart-Thomas elements.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 14:09:43 GMT"}], "update_date": "2012-06-14", "authors_parsed": [["Kreeft", "Jasper", ""], ["Gerritsma", "Marc", ""]]}, {"id": "1206.2948", "submitter": "Nathan Collier", "authors": "Nathan Collier and Lisandro Dalcin and David Pardo and V. M. Calo", "title": "The cost of continuity: performance of iterative solvers on isogeometric\n  finite elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study how the use of a more continuous set of basis\nfunctions affects the cost of solving systems of linear equations resulting\nfrom a discretized Galerkin weak form. Specifically, we compare performance of\nlinear solvers when discretizing using $C^0$ B-splines, which span traditional\nfinite element spaces, and $C^{p-1}$ B-splines, which represent maximum\ncontinuity. We provide theoretical estimates for the increase in cost of the\nmatrix-vector product as well as for the construction and application of\nblack-box preconditioners. We accompany these estimates with numerical results\nand study their sensitivity to various grid parameters such as element size $h$\nand polynomial order of approximation $p$. Finally, we present timing results\nfor a range of preconditioning options for the Laplace problem. We conclude\nthat the matrix-vector product operation is at most $\\slfrac{33p^2}{8}$ times\nmore expensive for the more continuous space, although for moderately low $p$,\nthis number is significantly reduced. Moreover, if static condensation is not\nemployed, this number further reduces to at most a value of 8, even for high\n$p$. Preconditioning options can be up to $p^3$ times more expensive to setup,\nalthough this difference significantly decreases for some popular\npreconditioners such as Incomplete LU factorization.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 21:54:08 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Collier", "Nathan", ""], ["Dalcin", "Lisandro", ""], ["Pardo", "David", ""], ["Calo", "V. M.", ""]]}, {"id": "1206.3177", "submitter": "Dohy Hong", "authors": "Dohy Hong and Philippe Jacquet", "title": "Optimizing the eigenvector computation algorithm with diffusion approach", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply the ideas of the matrix column based diffusion\napproach to define a new eigenvector computation algorithm of a stationary\nprobability of a Markov chain.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2012 16:44:10 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Hong", "Dohy", ""], ["Jacquet", "Philippe", ""]]}, {"id": "1206.4329", "submitter": "Sudarshan Nandy", "authors": "Sudarshan Nandy, Partha Pratim Sarkar and Achintya Das", "title": "An Improved Gauss-Newtons Method based Back-propagation Algorithm for\n  Fast Convergence", "comments": "7 pages, 6 figures,2 tables, Published with International Journal of\n  Computer Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 39(8):1-7, February\n  2012. Published by Foundation of Computer Science, New York, USA", "doi": "10.5120/4837-7097", "report-no": null, "categories": "cs.AI cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work deals with an improved back-propagation algorithm based on\nGauss-Newton numerical optimization method for fast convergence. The steepest\ndescent method is used for the back-propagation. The algorithm is tested using\nvarious datasets and compared with the steepest descent back-propagation\nalgorithm. In the system, optimization is carried out using multilayer neural\nnetwork. The efficacy of the proposed method is observed during the training\nperiod as it converges quickly for the dataset used in test. The requirement of\nmemory for computing the steps of algorithm is also analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2012 20:20:56 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Nandy", "Sudarshan", ""], ["Sarkar", "Partha Pratim", ""], ["Das", "Achintya", ""]]}, {"id": "1206.4481", "submitter": "Mathieu Fauvel", "authors": "M. Fauvel, A. Villa, J. Chanussot and J. A. Benediktsson", "title": "Parsimonious Mahalanobis Kernel for the Classification of High\n  Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of high dimensional data with kernel methods is considered\nin this article. Exploit- ing the emptiness property of high dimensional\nspaces, a kernel based on the Mahalanobis distance is proposed. The computation\nof the Mahalanobis distance requires the inversion of a covariance matrix. In\nhigh dimensional spaces, the estimated covariance matrix is ill-conditioned and\nits inversion is unstable or impossible. Using a parsimonious statistical\nmodel, namely the High Dimensional Discriminant Analysis model, the specific\nsignal and noise subspaces are estimated for each considered class making the\ninverse of the class specific covariance matrix explicit and stable, leading to\nthe definition of a parsimonious Mahalanobis kernel. A SVM based framework is\nused for selecting the hyperparameters of the parsimonious Mahalanobis kernel\nby optimizing the so-called radius-margin bound. Experimental results on three\nhigh dimensional data sets show that the proposed kernel is suitable for\nclassifying high dimensional data, providing better classification accuracies\nthan the conventional Gaussian kernel.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 12:49:48 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2012 13:01:47 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Fauvel", "M.", ""], ["Villa", "A.", ""], ["Chanussot", "J.", ""], ["Benediktsson", "J. A.", ""]]}, {"id": "1206.4602", "submitter": "Philipp Hennig", "authors": "Philipp Hennig (MPI Intelligent Systems), Martin Kiefel (MPI for\n  Intelligent Systems)", "title": "Quasi-Newton Methods: A New Direction", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Four decades after their invention, quasi-Newton methods are still state of\nthe art in unconstrained numerical optimization. Although not usually\ninterpreted thus, these are learning algorithms that fit a local quadratic\napproximation to the objective function. We show that many, including the most\npopular, quasi-Newton methods can be interpreted as approximations of Bayesian\nlinear regression under varying prior assumptions. This new notion elucidates\nsome shortcomings of classical algorithms, and lights the way to a novel\nnonparametric quasi-Newton method, which is able to make more efficient use of\navailable information at computational cost similar to its predecessors.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:41:11 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Hennig", "Philipp", "", "MPI Intelligent Systems"], ["Kiefel", "Martin", "", "MPI for\n  Intelligent Systems"]]}, {"id": "1206.4608", "submitter": "Soeren Laue", "authors": "Soeren Laue (Friedrich-Schiller-University)", "title": "A Hybrid Algorithm for Convex Semidefinite Optimization", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid algorithm for optimizing a convex, smooth function over\nthe cone of positive semidefinite matrices. Our algorithm converges to the\nglobal optimal solution and can be used to solve general large-scale\nsemidefinite programs and hence can be readily applied to a variety of machine\nlearning problems. We show experimental results on three machine learning\nproblems (matrix completion, metric learning, and sparse PCA) . Our approach\noutperforms state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:44:28 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Laue", "Soeren", "", "Friedrich-Schiller-University"]]}, {"id": "1206.4640", "submitter": "Huan Xu", "authors": "Yu-Xiang Wang (National University of Singapore), Huan Xu (National\n  University of Singapore)", "title": "Stability of matrix factorization for collaborative filtering", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stability vis a vis adversarial noise of matrix factorization\nalgorithm for matrix completion. In particular, our results include: (I) we\nbound the gap between the solution matrix of the factorization method and the\nground truth in terms of root mean square error; (II) we treat the matrix\nfactorization as a subspace fitting problem and analyze the difference between\nthe solution subspace and the ground truth; (III) we analyze the prediction\nerror of individual users based on the subspace stability. We apply these\nresults to the problem of collaborative filtering under manipulator attack,\nwhich leads to useful insights and guidelines for collaborative filtering\nsystem design.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:18:05 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Wang", "Yu-Xiang", "", "National University of Singapore"], ["Xu", "Huan", "", "National\n  University of Singapore"]]}, {"id": "1206.4645", "submitter": "Lauren Hannah", "authors": "Lauren Hannah (Duke University), David Dunson (Duke University)", "title": "Ensemble Methods for Convex Regression with Applications to Geometric\n  Programming Based Circuit Design", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex regression is a promising area for bridging statistical estimation and\ndeterministic convex optimization. New piecewise linear convex regression\nmethods are fast and scalable, but can have instability when used to\napproximate constraints or objective functions for optimization. Ensemble\nmethods, like bagging, smearing and random partitioning, can alleviate this\nproblem and maintain the theoretical properties of the underlying estimator. We\nempirically examine the performance of ensemble methods for prediction and\noptimization, and then apply them to device modeling and constraint\napproximation for geometric programming based circuit design.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:19:58 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Hannah", "Lauren", "", "Duke University"], ["Dunson", "David", "", "Duke University"]]}, {"id": "1206.4676", "submitter": "Zhirong Yang", "authors": "Zhirong Yang (Aalto University), Erkki Oja (Aalto University)", "title": "Clustering by Low-Rank Doubly Stochastic Matrix Decomposition", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering analysis by nonnegative low-rank approximations has achieved\nremarkable progress in the past decade. However, most approximation approaches\nin this direction are still restricted to matrix factorization. We propose a\nnew low-rank learning method to improve the clustering performance, which is\nbeyond matrix factorization. The approximation is based on a two-step bipartite\nrandom walk through virtual cluster nodes, where the approximation is formed by\nonly cluster assigning probabilities. Minimizing the approximation error\nmeasured by Kullback-Leibler divergence is equivalent to maximizing the\nlikelihood of a discriminative model, which endows our method with a solid\nprobabilistic interpretation. The optimization is implemented by a relaxed\nMajorization-Minimization algorithm that is advantageous in finding good local\nminima. Furthermore, we point out that the regularized algorithm with Dirichlet\nprior only serves as initialization. Experimental results show that the new\nmethod has strong performance in clustering purity for various datasets,\nespecially for large-scale manifold data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:36:49 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Yang", "Zhirong", "", "Aalto University"], ["Oja", "Erkki", "", "Aalto University"]]}, {"id": "1206.5986", "submitter": "Joel Andersson Mr.", "authors": "Joel Andersson and Jan-Olov Str\\\"omberg", "title": "On the Theorem of Uniform Recovery of Random Sampling Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two theorems from the theory of compressive sensing. Mainly a\ntheorem concerning uniform recovery of random sampling matrices, where the\nnumber of samples needed in order to recover an $s$-sparse signal from linear\nmeasurements (with high probability) is known to be $m\\gtrsim s(\\ln s)^3\\ln N$.\nWe present new and improved constants together with what we consider to be a\nmore explicit proof. A proof that also allows for a slightly larger class of\n$m\\times N$-matrices, by considering what we call \\emph{low entropy}. We also\npresent an improved condition on the so-called restricted isometry constants,\n$\\delta_s$, ensuring sparse recovery via $\\ell^1$-minimization. We show that\n$\\delta_{2s}<4/\\sqrt{41}$ is sufficient and that this can be improved further\nto almost allow for a sufficient condition of the type $\\delta_{2s}<2/3$.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2012 13:37:49 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2013 10:56:28 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2013 10:34:25 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Andersson", "Joel", ""], ["Str\u00f6mberg", "Jan-Olov", ""]]}, {"id": "1206.6470", "submitter": "Franz Kiraly", "authors": "Franz Kiraly (TU Berlin), Ryota Tomioka (University of Tokyo)", "title": "A Combinatorial Algebraic Approach for the Identifiability of Low-Rank\n  Matrix Completion", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we review the problem of matrix completion and expose its\nintimate relations with algebraic geometry, combinatorics and graph theory. We\npresent the first necessary and sufficient combinatorial conditions for\nmatrices of arbitrary rank to be identifiable from a set of matrix entries,\nyielding theoretical constraints and new algorithms for the problem of matrix\ncompletion. We conclude by algorithmically evaluating the tightness of the\ngiven conditions and algorithms for practically relevant matrix sizes, showing\nthat the algebraic-combinatoric approach can lead to improvements over\nstate-of-the-art matrix completion methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kiraly", "Franz", "", "TU Berlin"], ["Tomioka", "Ryota", "", "University of Tokyo"]]}, {"id": "1206.6474", "submitter": "Pierre-Andre Savalle", "authors": "Emile Richard (ENS Cachan), Pierre-Andre Savalle (Ecole Centrale de\n  Paris), Nicolas Vayatis (ENS Cachan)", "title": "Estimation of Simultaneously Sparse and Low Rank Matrices", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a penalized matrix estimation procedure aiming at\nsolutions which are sparse and low-rank at the same time. Such structures arise\nin the context of social networks or protein interactions where underlying\ngraphs have adjacency matrices which are block-diagonal in the appropriate\nbasis. We introduce a convex mixed penalty which involves $\\ell_1$-norm and\ntrace norm simultaneously. We obtain an oracle inequality which indicates how\nthe two effects interact according to the nature of the target matrix. We bound\ngeneralization error in the link prediction problem. We also develop proximal\ndescent strategies to solve the optimization problem efficiently and evaluate\nperformance on synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Richard", "Emile", "", "ENS Cachan"], ["Savalle", "Pierre-Andre", "", "Ecole Centrale de\n  Paris"], ["Vayatis", "Nicolas", "", "ENS Cachan"]]}, {"id": "1206.6833", "submitter": "Inmar Givoni", "authors": "Inmar Givoni, Vincent Cheung, Brendan J. Frey", "title": "Matrix Tile Analysis", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-200-207", "categories": "cs.LG cs.CE cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks require finding groups of elements in a matrix of numbers, symbols\nor class likelihoods. One approach is to use efficient bi- or tri-linear\nfactorization techniques including PCA, ICA, sparse matrix factorization and\nplaid analysis. These techniques are not appropriate when addition and\nmultiplication of matrix elements are not sensibly defined. More directly,\nmethods like bi-clustering can be used to classify matrix elements, but these\nmethods make the overly-restrictive assumption that the class of each element\nis a function of a row class and a column class. We introduce a general\ncomputational problem, `matrix tile analysis' (MTA), which consists of\ndecomposing a matrix into a set of non-overlapping tiles, each of which is\ndefined by a subset of usually nonadjacent rows and columns. MTA does not\nrequire an algebra for combining tiles, but must search over discrete\ncombinations of tile assignments. Exact MTA is a computationally intractable\ninteger programming problem, but we describe an approximate iterative technique\nand a computationally efficient sum-product relaxation of the integer program.\nWe compare the effectiveness of these methods to PCA and plaid on hundreds of\nrandomly generated tasks. Using double-gene-knockout data, we show that MTA\nfinds groups of interacting yeast genes that have biologically-related\nfunctions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:18:05 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Givoni", "Inmar", ""], ["Cheung", "Vincent", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1206.6857", "submitter": "Dongryeol Lee", "authors": "Dongryeol Lee, Alexander G. Gray", "title": "Faster Gaussian Summation: Theory and Experiment", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-281-288", "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide faster algorithms for the problem of Gaussian summation, which\noccurs in many machine learning methods. We develop two new extensions - an\nO(Dp) Taylor expansion for the Gaussian kernel with rigorous error bounds and a\nnew error control scheme integrating any arbitrary approximation method -\nwithin the best discretealgorithmic framework using adaptive hierarchical data\nstructures. We rigorously evaluate these techniques empirically in the context\nof optimal bandwidth selection in kernel density estimation, revealing the\nstrengths and weaknesses of current state-of-the-art approaches for the first\ntime. Our results demonstrate that the new error control scheme yields improved\nperformance, whereas the series expansion approach is only effective in low\ndimensions (five or less).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:26:27 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Lee", "Dongryeol", ""], ["Gray", "Alexander G.", ""]]}]