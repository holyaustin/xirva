[{"id": "1610.00199", "submitter": "Laura Balzano", "authors": "Dejiao Zhang, Laura Balzano", "title": "Convergence of a Grassmannian Gradient Descent Algorithm for Subspace\n  Estimation From Undersampled Data", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace learning and matrix factorization problems have great many\napplications in science and engineering, and efficient algorithms are critical\nas dataset sizes continue to grow. Many relevant problem formulations are\nnon-convex, and in a variety of contexts it has been observed that solving the\nnon-convex problem directly is not only efficient but reliably accurate. We\ndiscuss convergence theory for a particular method: first order incremental\ngradient descent constrained to the Grassmannian. The output of the algorithm\nis an orthonormal basis for a $d$-dimensional subspace spanned by an input\nstreaming data matrix. We study two sampling cases: where each data vector of\nthe streaming matrix is fully sampled, or where it is undersampled by a\nsampling matrix $A_t\\in \\mathbb{R}^{m\\times n}$ with $m\\ll n$. Our results\ncover two cases, where $A_t$ is Gaussian or a subset of rows of the identity\nmatrix. We propose an adaptive stepsize scheme that depends only on the sampled\ndata and algorithm outputs. We prove that with fully sampled data, the stepsize\nscheme maximizes the improvement of our convergence metric at each iteration,\nand this method converges from any random initialization to the true subspace,\ndespite the non-convex formulation and orthogonality constraints. For the case\nof undersampled data, we establish monotonic expected improvement on the\ndefined convergence metric for each iteration with high probability.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 22:19:02 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 19:17:34 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Zhang", "Dejiao", ""], ["Balzano", "Laura", ""]]}, {"id": "1610.00251", "submitter": "Ajinkya Kadu", "authors": "Ajinkya Kadu, Tristan van Leeuwen, Wim A. Mulder", "title": "Salt Reconstruction in Full Waveform Inversion with a Parametric\n  Level-Set Method", "comments": null, "journal-ref": "IEEE Transactions on Computational Imaging ( Volume: 3 , Issue: 2\n  , June 2017 )", "doi": "10.1109/TCI.2016.2640761", "report-no": null, "categories": "cs.CE cs.NA math.NA math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Seismic full-waveform inversion tries to estimate subsurface medium\nparameters from seismic data. Areas with subsurface salt bodies are of\nparticular interest because they often have hydrocarbon reservoirs on their\nsides or underneath. Accurate reconstruction of their geometry is a challenge\nfor current techniques. This paper presents a parametric level-set method for\nthe reconstruction of salt-bodies in seismic full-waveform inversion. We split\nthe subsurface model in two parts: a background velocity model and the salt\nbody with known velocity but undetermined shape. The salt geometry is\nrepresented by a level-set function that evolves during the inversion. We\nchoose radial basis functions to represent the level-set function, leading to\nan optimization problem with a modest number of parameters. A common problem\nwith level-set methods is to fine tune the width of the level-set boundary for\noptimal sensitivity. We propose a robust algorithm that dynamically adapts the\nwidth of the level-set boundary to ensure faster convergence. Tests on a suite\nof idealized salt geometries show that the proposed method is stable against a\nmodest amount of noise. We also extend the method to joint inversion of both\nthe background velocity model and the salt-geometry.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 09:33:35 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 15:35:55 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Kadu", "Ajinkya", ""], ["van Leeuwen", "Tristan", ""], ["Mulder", "Wim A.", ""]]}, {"id": "1610.00625", "submitter": "Ferran Vidal-Codina", "authors": "Ferran Vidal-Codina, Joel Saa-Seoane, Ngoc-Cuong Nguyen and Jaime\n  Peraire", "title": "A multiscale continuous Galerkin method for stochastic simulation and\n  robust design of photonic crystals", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.jcpx.2019.100016", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multiscale continuous Galerkin (MSCG) method for the fast and\naccurate stochastic simulation and optimization of time-harmonic wave\npropagation through photonic crystals. The MSCG method exploits repeated\npatterns in the geometry to drastically decrease computational cost and\nincorporates the following ingredients: (1) a reference domain formulation that\nallows us to treat geometric variability resulting from manufacturing\nuncertainties; (2) a reduced basis approximation to solve the parametrized\nlocal subproblems; (3) a gradient computation of the objective function; and\n(4) a model and variance reduction technique that enables the accelerated\ncomputation of statistical outputs by exploiting the statistical correlation\nbetween the MSCG solution and the reduced basis approximation. The proposed\nmethod is thus well suited for both deterministic and stochastic simulations,\nas well as robust design of photonic crystals. We provide convergence and cost\nanalysis of the MSCG method, as well as a simulation results for a waveguide\nT-splitter and a Z-bend to illustrate its advantages for stochastic simulation\nand robust design.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 16:37:00 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 21:34:36 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Vidal-Codina", "Ferran", ""], ["Saa-Seoane", "Joel", ""], ["Nguyen", "Ngoc-Cuong", ""], ["Peraire", "Jaime", ""]]}, {"id": "1610.00835", "submitter": "Kui Ren", "authors": "Kui Ren, Rongting Zhang and Yimin Zhong", "title": "A fast algorithm for radiative transport in isotropic media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA nucl-th physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this work a fast numerical algorithm for solving the equation\nof radiative transfer (ERT) in isotropic media. The algorithm has two steps. In\nthe first step, we derive an integral equation for the angularly averaged ERT\nsolution by taking advantage of the isotropy of the scattering kernel, and\nsolve the integral equation with a fast multipole method (FMM). In the second\nstep, we solve a scattering-free transport equation to recover the original ERT\nsolution. Numerical simulations are presented to demonstrate the performance of\nthe algorithm for both homogeneous and inhomogeneous media.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 03:34:10 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 21:32:33 GMT"}, {"version": "v3", "created": "Sat, 6 Jul 2019 05:44:23 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Ren", "Kui", ""], ["Zhang", "Rongting", ""], ["Zhong", "Yimin", ""]]}, {"id": "1610.01039", "submitter": "Alessandro Castagnotto", "authors": "Alessandro Castagnotto, Christopher Beattie, Serkan Gugercin", "title": "Interpolatory methods for $\\mathcal{H}_\\infty$ model reduction of\n  multi-input/multi-output systems", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-58786-8_22", "report-no": null, "categories": "math.NA cs.NA math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop here a computationally effective approach for producing\nhigh-quality $\\mathcal{H}_\\infty$-approximations to large scale linear\ndynamical systems having multiple inputs and multiple outputs (MIMO). We extend\nan approach for $\\mathcal{H}_\\infty$ model reduction introduced by Flagg,\nBeattie, and Gugercin for the single-input/single-output (SISO) setting, which\ncombined ideas originating in interpolatory $\\mathcal{H}_2$-optimal model\nreduction with complex Chebyshev approximation. Retaining this framework, our\napproach to the MIMO problem has its principal computational cost dominated by\n(sparse) linear solves, and so it can remain an effective strategy in many\nlarge-scale settings. We are able to avoid computationally demanding\n$\\mathcal{H}_\\infty$ norm calculations that are normally required to monitor\nprogress within each optimization cycle through the use of \"data-driven\"\nrational approximations that are built upon previously computed function\nsamples. Numerical examples are included that illustrate our approach. We\nproduce high fidelity reduced models having consistently better\n$\\mathcal{H}_\\infty$ performance than models produced via balanced truncation;\nthese models often are as good as (and occasionally better than) models\nproduced using optimal Hankel norm approximation as well. In all cases\nconsidered, the method described here produces reduced models at far lower cost\nthan is possible with either balanced truncation or optimal Hankel norm\napproximation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 15:12:08 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Castagnotto", "Alessandro", ""], ["Beattie", "Christopher", ""], ["Gugercin", "Serkan", ""]]}, {"id": "1610.01694", "submitter": "Tahsin Khajah", "authors": "Tahsin Khajah, Xavier Antoine, St\\'ephane P.A. Bordas", "title": "Isogeometric finite element analysis of time-harmonic exterior acoustic\n  scattering problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an isogeometric analysis of time-harmonic exterior acoustic\nproblems. The infinite space is truncated by a fictitious boundary and (simple)\nabsorbing boundary conditions are applied. The truncation error is included in\nthe exact solution so that the reported error is an indicator of the\nperformance of the isogeometric analysis, in particular of the related\npollution error. Numerical results performed with high-order basis functions\n(third or fourth orders) showed no visible pollution error even for very high\nfrequencies. This property combined with exact geometrical representation makes\nisogeometric analysis a very promising platform to solve high-frequency\nacoustic problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 00:11:05 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Khajah", "Tahsin", ""], ["Antoine", "Xavier", ""], ["Bordas", "St\u00e9phane P. A.", ""]]}, {"id": "1610.02570", "submitter": "Huu Phuoc Bui", "authors": "Huu Phuoc Bui (1 and 2), Satyendra Tomar (2), Hadrien Courtecuisse\n  (1), St\\'ephane Cotin (3), St\\'ephane Bordas (2 and 4) ((1) University of\n  Strasbourg, CNRS, ICube, Strasbourg, France, (2) University of Luxembourg,\n  Luxembourg, (3) Inria Nancy Grand Est, Villers-les-Nancy, France, (4) Cardiff\n  University, Wales, UK)", "title": "Real-time Error Control for Surgical Simulation", "comments": "12 pages, 16 figures, change of the title, submitted to IEEE TBME", "journal-ref": "IEEE Transactions on Biomedical Engineering ( Volume: 65 , Issue:\n  3 , March 2018 )", "doi": "10.1109/TBME.2017.2695587", "report-no": null, "categories": "cs.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To present the first real-time a posteriori error-driven adaptive\nfinite element approach for real-time simulation and to demonstrate the method\non a needle insertion problem. Methods: We use corotational elasticity and a\nfrictional needle/tissue interaction model. The problem is solved using finite\nelements within SOFA. The refinement strategy relies upon a hexahedron-based\nfinite element method, combined with a posteriori error estimation driven local\n$h$-refinement, for simulating soft tissue deformation. Results: We control the\nlocal and global error level in the mechanical fields (e.g. displacement or\nstresses) during the simulation. We show the convergence of the algorithm on\nacademic examples, and demonstrate its practical usability on a percutaneous\nprocedure involving needle insertion in a liver. For the latter case, we\ncompare the force displacement curves obtained from the proposed adaptive\nalgorithm with that obtained from a uniform refinement approach. Conclusions:\nError control guarantees that a tolerable error level is not exceeded during\nthe simulations. Local mesh refinement accelerates simulations. Significance:\nOur work provides a first step to discriminate between discretization error and\nmodeling error by providing a robust quantification of discretization error\nduring simulations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 19:50:07 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 08:40:16 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 14:47:24 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Bui", "Huu Phuoc", "", "1 and 2"], ["Tomar", "Satyendra", "", "2 and 4"], ["Courtecuisse", "Hadrien", "", "2 and 4"], ["Cotin", "St\u00e9phane", "", "2 and 4"], ["Bordas", "St\u00e9phane", "", "2 and 4"]]}, {"id": "1610.02889", "submitter": "Dirk Lorenz", "authors": "Frank Sch\\\"opfer, Dirk A. Lorenz", "title": "Linear convergence of the Randomized Sparse Kaczmarz Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized version of the Kaczmarz method for the solution of linear\nsystems is known to converge linearly in expectation. In this work we extend\nthis result and show that the recently proposed Randomized Sparse Kaczmarz\nmethod for recovery of sparse solutions, as well as many variants, also\nconverges linearly in expectation. The result is achieved in the framework of\nsplit feasibility problems and their solution by randomized Bregman projections\nwith respect to strongly convex functions. To obtain the expected convergence\nrates we prove extensions of error bounds for projections. The convergence\nresult is shown to hold in more general settings involving smooth convex\nfunctions, piecewise linear-quadratic functions and also the regularized\nnuclear norm, which is used in the area of low rank matrix problems. Numerical\nexperiments indicate that the Randomized Sparse Kaczmarz method provides\nadvantages over both the non-randomized and the non-sparse Kaczmarz methods for\nthe solution of over- and under-determined linear systems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 12:46:43 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Sch\u00f6pfer", "Frank", ""], ["Lorenz", "Dirk A.", ""]]}, {"id": "1610.02962", "submitter": "Patrick Heas", "authors": "Patrick H\\'eas and C\\'edric Herzet", "title": "Low-Rank Dynamic Mode Decomposition: Optimal Solution in Polynomial-Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the linear approximation of high-dimensional dynamical\nsystems using low-rank dynamic mode decomposition (DMD). Searching this\napproximation in a data-driven approach is formalised as attempting to solve a\nlow-rank constrained optimisation problem. This problem is non-convex and\nstate-of-the-art algorithms are all sub-optimal. This paper shows that there\nexists a closed-form solution, which is computed in polynomial time, and\ncharacterises the l2-norm of the optimal approximation error. The paper also\nproposes low-complexity algorithms building reduced models from this optimal\nsolution, based on singular value decomposition or eigen value decomposition.\nThe algorithms are evaluated by numerical simulations using synthetic and\nphysical data benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 15:29:12 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 16:19:42 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 12:15:56 GMT"}, {"version": "v4", "created": "Mon, 16 Oct 2017 14:36:29 GMT"}, {"version": "v5", "created": "Thu, 17 May 2018 13:12:09 GMT"}, {"version": "v6", "created": "Fri, 21 Dec 2018 10:58:47 GMT"}, {"version": "v7", "created": "Fri, 21 Feb 2020 14:03:19 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["H\u00e9as", "Patrick", ""], ["Herzet", "C\u00e9dric", ""]]}, {"id": "1610.02967", "submitter": "Soeren Laue", "authors": "Joachim Giesen and S\\\"oren Laue", "title": "Distributed Convex Optimization with Many Convex Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of solving convex optimization problems with many\nconvex constraints in a distributed setting. Our approach is based on an\nextension of the alternating direction method of multipliers (ADMM) that\nrecently gained a lot of attention in the Big Data context. Although it has\nbeen invented decades ago, ADMM so far can be applied only to unconstrained\nproblems and problems with linear equality or inequality constraints. Our\nextension can handle arbitrary inequality constraints directly. It combines the\nability of ADMM to solve convex optimization problems in a distributed setting\nwith the ability of the Augmented Lagrangian method to solve constrained\noptimization problems, and as we show, it inherits the convergence guarantees\nof ADMM and the Augmented Lagrangian method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 12:59:46 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 10:52:50 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Giesen", "Joachim", ""], ["Laue", "S\u00f6ren", ""]]}, {"id": "1610.03764", "submitter": "Mark Iwen", "authors": "Jade Larriva-Latt, Angela Morrison, Alison Radgowski, Joseph Tobin,\n  Aditya Viswanathan, and Mark Iwen", "title": "Technical Report: Improved Fourier Reconstruction using Jump Information\n  with Applications to MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain applications such as Magnetic Resonance Imaging (MRI) require the\nreconstruction of functions from Fourier spectral data. When the underlying\nfunctions are piecewise-smooth, standard Fourier approximation methods suffer\nfrom the Gibbs phenomenon - with associated oscillatory artifacts in the\nvicinity of edges and an overall reduced order of convergence in the\napproximation. This paper proposes an edge-augmented Fourier reconstruction\nprocedure which uses only the first few Fourier coefficients of an underlying\npiecewise-smooth function to accurately estimate jump information and then\nincorporate it into a Fourier partial sum approximation. We provide both\ntheoretical and empirical results showing the improved accuracy of the proposed\nmethod, as well as comparisons demonstrating superior performance over existing\nstate-of-the-art sparse optimization-based methods. Extensions of the proposed\ntechniques to functions of several variables are also addressed preliminarily.\nAll code used to generate the results in this report are made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 15:59:52 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Larriva-Latt", "Jade", ""], ["Morrison", "Angela", ""], ["Radgowski", "Alison", ""], ["Tobin", "Joseph", ""], ["Viswanathan", "Aditya", ""], ["Iwen", "Mark", ""]]}, {"id": "1610.04272", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Kim Batselier, Haotian Liu, Luca Daniel, Ngai Wong", "title": "Tensor Computation: A New Framework for High-Dimensional Problems in EDA", "comments": "14 figures. Accepted by IEEE Trans. CAD of Integrated Circuits and\n  Systems", "journal-ref": null, "doi": "10.1109/TCAD.2016.2618879", "report-no": null, "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many critical EDA problems suffer from the curse of dimensionality, i.e. the\nvery fast-scaling computational burden produced by large number of parameters\nand/or unknown variables. This phenomenon may be caused by multiple spatial or\ntemporal factors (e.g. 3-D field solvers discretizations and multi-rate circuit\nsimulation), nonlinearity of devices and circuits, large number of design or\noptimization parameters (e.g. full-chip routing/placement and circuit sizing),\nor extensive process variations (e.g. variability/reliability analysis and\ndesign for manufacturability). The computational challenges generated by such\nhigh dimensional problems are generally hard to handle efficiently with\ntraditional EDA core algorithms that are based on matrix and vector\ncomputation. This paper presents \"tensor computation\" as an alternative general\nframework for the development of efficient EDA algorithms and tools. A tensor\nis a high-dimensional generalization of a matrix and a vector, and is a natural\nchoice for both storing and solving efficiently high-dimensional EDA problems.\nThis paper gives a basic tutorial on tensors, demonstrates some recent examples\nof EDA applications (e.g., nonlinear circuit modeling and high-dimensional\nuncertainty quantification), and suggests further open EDA problems where the\nuse of tensor computation could be of advantage.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 21:56:14 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Zheng", ""], ["Batselier", "Kim", ""], ["Liu", "Haotian", ""], ["Daniel", "Luca", ""], ["Wong", "Ngai", ""]]}, {"id": "1610.05427", "submitter": "Dimitri Bertsekas", "authors": "Dimitri P. Bertsekas", "title": "Proximal Algorithms and Temporal Differences for Large Linear Systems:\n  Extrapolation, Approximation, and Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large linear and nonlinear fixed point problems, and solution\nwith proximal algorithms. We show that there is a close connection between two\nseemingly different types of methods from distinct fields: 1) Proximal\niterations for linear systems of equations, which are prominent in numerical\nanalysis and convex optimization, and 2) Temporal difference (TD) type methods,\nsuch as TD(lambda), LSTD(lambda), and LSPE(lambda), which are central in\nsimulation-based approximate dynamic programming/reinforcement learning\n(DP/RL), and its recent prominent successes in large-scale game contexts, among\nothers.\n  One benefit of this connection is a new and simple way to accelerate the\nstandard proximal algorithm by extrapolation towards the TD iteration, which\ngenerically has a faster convergence rate. Another benefit is the potential\nintegration into the proximal algorithmic context of several new ideas that\nhave emerged in the DP/RL context. We discuss some of the possibilities, and in\nparticular, algorithms that project each proximal iterate onto the subspace\nspanned by a small number of basis functions, using low-dimensional\ncalculations and simulation. A third benefit is that insights and analysis from\nproximal algorithms can be brought to bear on the enhancement of TD methods.\n  The linear fixed point methodology can be extended to nonlinear fixed point\nproblems involving a contraction, thus providing guaranteed and potentially\nsubstantial acceleration of the proximal and forward backward splitting\nalgorithms at no extra cost. Moreover, the connection of proximal and TD\nmethods can be extended to nonlinear (nondifferentiable) fixed point problems\nthrough new proximal-like algorithms that involve successive linearization,\nsimilar to policy iteration in DP.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 04:16:15 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 00:02:52 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 19:20:27 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 06:52:42 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Bertsekas", "Dimitri P.", ""]]}, {"id": "1610.05525", "submitter": "Antoine Tambue", "authors": "Jean Daniel Mukam, Antoine Tambue", "title": "A note on exponential Rosenbrock-Euler method for the finite element\n  discretization of a semilinear parabolic partial differential equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the numerical approximation of a general second\norder semi-linear parabolic partial differential equation. Equations of this\ntype arise in many contexts, such as transport in porous media. Using finite\nelement method for space discretization and the exponential Rosenbrock-Euler\nmethod for time discretization, we provide a rigorous convergence proof in\nspace and time under only the standard Lipschitz condition of the nonlinear\npart for both smooth and nonsmooth initial solution. This is in contrast to\nvery restrictive assumptions made in the literature, where the authors have\nconsidered only approximation in time so far in their convergence proofs. The\noptimal orders of convergence in space and in time are achieved for smooth and\nnonsmooth initial solution.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 10:43:20 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 17:30:37 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Mukam", "Jean Daniel", ""], ["Tambue", "Antoine", ""]]}, {"id": "1610.05621", "submitter": "Kassem Mustapha", "authors": "Kassem Mustapha", "title": "FEM for time-fractional diffusion equations, novel optimal error\n  analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semidiscrete Galerkin finite element method applied to time-fractional\ndiffusion equations with time-space dependent diffusivity on bounded convex\nspatial domains will be studied. The main focus is on achieving optimal error\nresults with respect to both the convergence order of the approximate solution\nand the regularity of the initial data. By using novel energy arguments, for\neach fixed time $t$, optimal error bounds in the spatial $L^2$- and $H^1$-norms\nare derived for both cases: smooth and nonsmooth initial data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 13:59:08 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 16:56:05 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Mustapha", "Kassem", ""]]}, {"id": "1610.05838", "submitter": "Xiaolong Xie", "authors": "Xiaolong Xie, Wei Tan, Liana L. Fong, Yun Liang", "title": "CuMF_SGD: Fast and Scalable Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) has been widely used in e.g., recommender systems,\ntopic modeling and word embedding. Stochastic gradient descent (SGD) is popular\nin solving MF problems because it can deal with large data sets and is easy to\ndo incremental learning. We observed that SGD for MF is memory bound.\nMeanwhile, single-node CPU systems with caching performs well only for small\ndata sets; distributed systems have higher aggregated memory bandwidth but\nsuffer from relatively slow network connection. This observation inspires us to\naccelerate MF by utilizing GPUs's high memory bandwidth and fast intra-node\nconnection. We present cuMF_SGD, a CUDA-based SGD solution for large-scale MF\nproblems. On a single CPU, we design two workload schedule schemes, i.e.,\nbatch-Hogwild! and wavefront-update that fully exploit the massive amount of\ncores. Especially, batch-Hogwild! as a vectorized version of Hogwild! overcomes\nthe issue of memory discontinuity. We also develop highly-optimized kernels for\nSGD update, leveraging cache, warp-shuffle instructions and half-precision\nfloats. We also design a partition scheme to utilize multiple GPUs while\naddressing the well-known convergence issue when parallelizing SGD. On three\ndata sets with only one Maxwell or Pascal GPU, cuMF_SGD runs 3.1X-28.2X as fast\ncompared with state-of-art CPU solutions on 1-64 CPU nodes. Evaluations also\nshow that cuMF_SGD scales well on multiple GPUs in large data sets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 01:28:11 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 13:38:34 GMT"}, {"version": "v3", "created": "Thu, 10 Nov 2016 01:16:40 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Xie", "Xiaolong", ""], ["Tan", "Wei", ""], ["Fong", "Liana L.", ""], ["Liang", "Yun", ""]]}, {"id": "1610.06049", "submitter": "Michael Breu{\\ss}", "authors": "Martin B\\\"ahr, Michael Breu{\\ss}, Yvain Qu\\'eau, Ali Sharifi\n  Boroujerdi, Jean-Denis Durou", "title": "Fast and Accurate Surface Normal Integration on Non-Rectangular Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of surface normals for the purpose of computing the shape of\na surface in 3D space is a classic problem in computer vision. However, even\nnowadays it is still a challenging task to devise a method that combines the\nflexibility to work on non-trivial computational domains with high accuracy,\nrobustness and computational efficiency. By uniting a classic approach for\nsurface normal integration with modern computational techniques we construct a\nsolver that fulfils these requirements. Building upon the Poisson integration\nmodel we propose to use an iterative Krylov subspace solver as a core step in\ntackling the task. While such a method can be very efficient, it may only show\nits full potential when combined with a suitable numerical preconditioning and\na problem-specific initialisation. We perform a thorough numerical study in\norder to identify an appropriate preconditioner for our purpose. To address the\nissue of a suitable initialisation we propose to compute this initial state via\na recently developed fast marching integrator. Detailed numerical experiments\nilluminate the benefits of this novel combination. In addition, we show on\nreal-world photometric stereo datasets that the developed numerical framework\nis flexible enough to tackle modern computer vision applications.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:01:09 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["B\u00e4hr", "Martin", ""], ["Breu\u00df", "Michael", ""], ["Qu\u00e9au", "Yvain", ""], ["Boroujerdi", "Ali Sharifi", ""], ["Durou", "Jean-Denis", ""]]}, {"id": "1610.06086", "submitter": "Moritz August", "authors": "Moritz August, Mari Carmen Ba\\~nuls and Thomas Huckle", "title": "On the Approximation of Functionals of Very Large Hermitian Matrices\n  represented as Matrix Product Operators", "comments": "12 pages, 6 figures, comments are welcome", "journal-ref": "ETNA 46, 215 (2017)", "doi": null, "report-no": null, "categories": "cs.NA quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to approximate functionals $\\text{Tr} \\, f(A)$ of very\nhigh-dimensional hermitian matrices $A$ represented as Matrix Product Operators\n(MPOs). Our method is based on a reformulation of a block Lanczos algorithm in\ntensor network format. We state main properties of the method and show how to\nadapt the basic Lanczos algorithm to the tensor network formalism to allow for\nhigh-dimensional computations. Additionally, we give an analysis of the\ncomplexity of our method and provide numerical evidence that it yields good\napproximations of the entropy of density matrices represented by MPOs while\nbeing robust against truncations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 16:10:51 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["August", "Moritz", ""], ["Ba\u00f1uls", "Mari Carmen", ""], ["Huckle", "Thomas", ""]]}, {"id": "1610.06685", "submitter": "Tomoaki Okayama", "authors": "Tomoaki Okayama", "title": "Error estimates with explicit constants for the Sinc approximation over\n  infinite intervals", "comments": "Keywords: Sinc approximation, conformal map, double-exponential\n  transformation, infinite interval, error bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sinc approximation is a function approximation formula that attains\nexponential convergence for rapidly decaying functions defined on the whole\nreal axis. Even for other functions, the Sinc approximation works accurately\nwhen combined with a proper variable transformation. The convergence rate has\nbeen analyzed for typical cases including finite, semi-infinite, and infinite\nintervals. Recently, for verified numerical computations, a more explicit,\n\"computable\" error bound has been given in the case of a finite interval. In\nthis paper, such explicit error bounds are derived for other cases.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 07:16:27 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 16:04:40 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 02:29:48 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Okayama", "Tomoaki", ""]]}, {"id": "1610.06790", "submitter": "Antoine Tambue", "authors": "Jean Daniel Mukam, Antoine Tambue", "title": "Strong convergence analysis of the stochastic exponential Rosenbrock\n  scheme for the finite element discretization of semilinear SPDEs driven by\n  multiplicative and additive noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the numerical approximation of a general second\norder semilinear stochastic partial differential equation (SPDE) driven by\nmultiplicative and additive noise. Our main interest is on such SPDEs where the\nnonlinear part is stronger than the linear part also called stochastic reactive\ndominated transport equations. Most numerical techniques, including current\nstochastic exponential integrators lose their good stability properties on such\nequations. Using finite element for space discretization, we propose a new\nscheme appropriated on such equations, called stochastic exponential Rosenbrock\nscheme (SERS) based on local linearization at every time step of the\nsemi-discrete equation obtained after space discretization. We consider noise\nthat is in a trace class and give a strong convergence proof of the new scheme\ntoward the exact solution in the root-mean-square $L^2$ norm. Numerical\nexperiments to sustain theoretical results are provided.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 14:00:14 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 01:30:15 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Mukam", "Jean Daniel", ""], ["Tambue", "Antoine", ""]]}, {"id": "1610.07038", "submitter": "Victor Magron", "authors": "Alexandre Rocca and Victor Magron and Thao Dang", "title": "Certified Roundoff Error Bounds using Bernstein Expansions and Sparse\n  Krivine-Stengle Representations", "comments": "20 pages, 2 tables", "journal-ref": null, "doi": "10.1109/ARITH.2017.36", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floating point error is an inevitable drawback of embedded systems\nimplementation. Computing rigorous upper bounds of roundoff errors is\nabsolutely necessary to the validation of critical software. This problem is\neven more challenging when addressing non-linear programs. In this paper, we\npropose and compare two new methods based on Bernstein expansions and sparse\nKrivine-Stengle representations, adapted from the field of the global\noptimization to compute upper bounds of roundoff errors for programs\nimplementing polynomial functions. We release two related software package\nFPBern and FPKiSten, and compare them with state of the art tools. We show that\nthese two methods achieve competitive performance, while computing accurate\nupper bounds by comparison with other tools.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 12:04:12 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 14:09:48 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Rocca", "Alexandre", ""], ["Magron", "Victor", ""], ["Dang", "Thao", ""]]}, {"id": "1610.07722", "submitter": "Ioakeim Perros", "authors": "Ioakeim Perros and Robert Chen and Richard Vuduc and Jimeng Sun", "title": "Sparse Hierarchical Tucker Factorization and its Application to\n  Healthcare", "comments": "This is an extended version of a paper presented at the 15th IEEE\n  International Conference on Data Mining (ICDM 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new tensor factorization method, called the Sparse\nHierarchical-Tucker (Sparse H-Tucker), for sparse and high-order data tensors.\nSparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker\nmethod, which aims to compute a tree-structured factorization of an input data\nset that may be readily interpreted by a domain expert. However, Sparse\nH-Tucker uses a nested sampling technique to overcome a key scalability problem\nin Hierarchical Tucker, which is the creation of an unwieldy intermediate dense\ncore tensor; the result of our approach is a faster, more space-efficient, and\nmore accurate method. We extensively test our method on a real healthcare\ndataset, which is collected from 30K patients and results in an 18th order\nsparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the\nfull data set on a single multi-threaded machine. It can also do so more\naccurately and in less time than the state-of-the-art: on a 12th order subset\nof the input data, Sparse H-Tucker is 18x more accurate and 7.5x faster than a\npreviously state-of-the-art method. Even for analyzing low order tensors (e.g.,\n4-order), our method requires close to an order of magnitude less time and over\ntwo orders of magnitude less memory, as compared to traditional tensor\nfactorization methods such as CP and Tucker. Moreover, we observe that Sparse\nH-Tucker scales nearly linearly in the number of non-zero tensor elements. The\nresulting model also provides an interpretable disease hierarchy, which is\nconfirmed by a clinical expert.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 04:08:11 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Perros", "Ioakeim", ""], ["Chen", "Robert", ""], ["Vuduc", "Richard", ""], ["Sun", "Jimeng", ""]]}, {"id": "1610.08127", "submitter": "Thomas Brouwer", "authors": "Thomas Brouwer, Jes Frellsen, Pietro Lio'", "title": "Fast Bayesian Non-Negative Matrix Factorisation and Tri-Factorisation", "comments": "NIPS 2016 Workshop on Advances in Approximate Bayesian Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast variational Bayesian algorithm for performing non-negative\nmatrix factorisation and tri-factorisation. We show that our approach achieves\nfaster convergence per iteration and timestep (wall-clock) than Gibbs sampling\nand non-probabilistic approaches, and do not require additional samples to\nestimate the posterior. We show that in particular for matrix tri-factorisation\nconvergence is difficult, but our variational Bayesian approach offers a fast\nsolution, allowing the tri-factorisation approach to be used more effectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 00:10:44 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Brouwer", "Thomas", ""], ["Frellsen", "Jes", ""], ["Lio'", "Pietro", ""]]}, {"id": "1610.08417", "submitter": "Onur Teymur", "authors": "Onur Teymur, Konstantinos Zygalakis, Ben Calderhead", "title": "Probabilistic Linear Multistep Methods", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": "Advances in Neural Information Processing Systems 29 (2016) pp.\n  4321-4328", "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a derivation and theoretical investigation of the Adams-Bashforth\nand Adams-Moulton family of linear multistep methods for solving ordinary\ndifferential equations, starting from a Gaussian process (GP) framework. In the\nlimit, this formulation coincides with the classical deterministic methods,\nwhich have been used as higher-order initial value problem solvers for over a\ncentury. Furthermore, the natural probabilistic framework provided by the GP\nformulation allows us to derive probabilistic versions of these methods, in the\nspirit of a number of other probabilistic ODE solvers presented in the recent\nliterature. In contrast to higher-order Runge-Kutta methods, which require\nmultiple intermediate function evaluations per step, Adams family methods make\nuse of previous function evaluations, so that increased accuracy arising from a\nhigher-order multistep approach comes at very little additional computational\ncost. We show that through a careful choice of covariance function for the GP,\nthe posterior mean and standard deviation over the numerical solution can be\nmade to exactly coincide with the value given by the deterministic method and\nits local truncation error respectively. We provide a rigorous proof of the\nconvergence of these new methods, as well as an empirical investigation (up to\nfifth order) demonstrating their convergence rates in practice.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:53:32 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Teymur", "Onur", ""], ["Zygalakis", "Konstantinos", ""], ["Calderhead", "Ben", ""]]}, {"id": "1610.08881", "submitter": "Hao Ji", "authors": "Hao Ji, Seth H. Weinberg, and Yaohang Li", "title": "A Revisit of Block Power Methods for Finite State Markov Chain\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the generalized block power methods for\napproximating the eigenvector associated with $\\lambda_1 = 1$ of a Markov chain\ntransition matrix. Our analysis of the block power method shows that when $s$\nlinearly independent probability vectors are used as the initial block, the\nconvergence of the block power method to the stationary distribution depends on\nthe magnitude of the $(s+1)$th dominant eigenvalue $\\lambda_{s+1}$ of $P$\ninstead of that of $\\lambda_2$ in the power method. Therefore, the block power\nmethod with block size $s$ is particularly effective for transition matrices\nwhere $|\\lambda_{s+1}|$ is well separated from $\\lambda_1 = 1$ but\n$|\\lambda_2|$ is not. This approach is particularly useful when visiting the\nelements of a large transition matrix is the main computational bottleneck over\nmatrix--vector multiplications, where the block power method can effectively\nreduce the total number of times to pass over the matrix. To further reduce the\noverall computational cost, we combine the block power method with a sliding\nwindow scheme, taking advantage of the subsequent vectors of the latest $s$\niterations to assemble the block matrix. The sliding window scheme correlates\nvectors in the sliding window to quickly remove the influences from the\neigenvalues whose magnitudes are smaller than $|\\lambda_{s}|$ to reduce the\noverall number of matrix--vector multiplications to reach convergence. Finally,\nwe compare the effectiveness of these methods in a Markov chain model\nrepresenting a stochastic luminal calcium release site.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 16:55:27 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Ji", "Hao", ""], ["Weinberg", "Seth H.", ""], ["Li", "Yaohang", ""]]}, {"id": "1610.09049", "submitter": "Kevin Carlberg", "authors": "Kevin Carlberg, Lukas Brencher, Bernard Haasdonk, Andrea Barth", "title": "Data-driven time parallelism via forecasting", "comments": "Submitted to the SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a data-driven method for enabling the efficient, stable\ntime-parallel numerical solution of systems of ordinary differential equations\n(ODEs). The method assumes that low-dimensional bases that accurately capture\nthe time evolution of the state are available. The method adopts the parareal\nframework for time parallelism, which is defined by an initialization method, a\ncoarse propagator, and a fine propagator. Rather than employing usual\napproaches for initialization and coarse propagation, we propose novel\ndata-driven techniques that leverage the available time-evolution bases. The\ncoarse propagator is defined by a forecast (proposed in Ref. [12]) applied\nlocally within each coarse time interval, which comprises the following steps:\n(1) apply the fine propagator for a small number of time steps, (2) approximate\nthe state over the entire coarse time interval using gappy POD with the local\ntime-evolution bases, and (3) select the approximation at the end of the time\ninterval as the propagated state. We also propose both local-forecast and\nglobal-forecast initialization. The method is particularly well suited for\nPOD-based reduced-order models (ROMs). In this case, spatial parallelism\nquickly saturates, as the ROM dynamical system is low dimensional; thus, time\nparallelism is needed to enable lower wall times. Further, the time-evolution\nbases can be extracted from the (readily available) right singular vectors\narising during POD computation. In addition to performing analyses related to\nthe method's accuracy, speedup, stability, and convergence, we also numerically\ndemonstrate the method's performance. Here, numerical experiments on ROMs for a\nnonlinear convection-reaction problem demonstrate the method's ability to\nrealize near-ideal speedups; global-forecast initialization with a\nlocal-forecast coarse propagator leads to the best performance.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 01:17:22 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 19:19:51 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Carlberg", "Kevin", ""], ["Brencher", "Lukas", ""], ["Haasdonk", "Bernard", ""], ["Barth", "Andrea", ""]]}, {"id": "1610.09079", "submitter": "Taras Lakoba", "authors": "Taras I. Lakoba, Zihao Deng", "title": "Stability analysis of the numerical Method of characteristics applied to\n  a class of energy-preserving systems. Part I: Periodic boundary conditions", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study numerical (in)stability of the Method of characteristics (MoC)\napplied to a system of non-dissipative hyperbolic partial differential\nequations (PDEs) with periodic boundary conditions. We consider three different\nsolvers along the characteristics: simple Euler (SE), modified Euler (ME), and\nLeap-frog (LF). The two former solvers are well known to exhibit a mild, but\nunconditional, numerical instability for non-dissipative ordinary differential\nequations (ODEs). They are found to have a similar (or stronger, for the\nMoC-ME) instability when applied to non-dissipative PDEs. On the other hand,\nthe LF solver is known to be stable when applied to non-dissipative ODEs.\nHowever, when applied to non-dissipative PDEs within the MoC framework, it was\nfound to have by far the strongest instability among all three solvers. We also\ncomment on the use of the fourth-order Runge--Kutta solver within the MoC\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 04:29:38 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 18:21:32 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Lakoba", "Taras I.", ""], ["Deng", "Zihao", ""]]}, {"id": "1610.09080", "submitter": "Taras Lakoba", "authors": "Taras I. Lakoba, Zihao Deng", "title": "Stability analysis of the numerical Method of characteristics applied to\n  a class of energy-preserving systems. Part II: Nonreflecting boundary\n  conditions", "comments": "38 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that imposition of non-periodic, in place of periodic, boundary\nconditions (BC) can alter stability of modes in the Method of characteristics\n(MoC) employing certain ordinary-differential equation (ODE) numerical solvers.\nThus, using non-periodic BC may render some of the MoC schemes stable for most\npractical computations, even though they are unstable for periodic BC. This\nfact contradicts a statement, found in some literature, that an instability\ndetected by the von Neumann analysis for a given numerical scheme implies an\ninstability of that scheme with arbitrary (i.e., non-periodic) BC. We explain\nthe mechanism behind this contradiction. We also show that, and explain why,\nfor the MoC employing some other ODE solvers, stability of the modes may be\nunaffected by the BC.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 04:47:59 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 18:28:22 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Lakoba", "Taras I.", ""], ["Deng", "Zihao", ""]]}, {"id": "1610.09456", "submitter": "Thomas Flynn", "authors": "Thomas Flynn", "title": "Forward sensitivity analysis for contracting stochastic systems", "comments": "Manuscript version of published work", "journal-ref": "Forward sensitivity analysis for contracting stochastic systems.\n  T. Flynn, Advances in Applied Probability vol. 50 p. 102-130, 2018", "doi": null, "report-no": null, "categories": "math.OC cs.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate gradient estimation for a class of contracting\nstochastic systems on a continuous state space. We find conditions on the\none-step transitions, namely differentiability and contraction in a Wasserstein\ndistance, that guarantee differentiability of stationary costs. Then we show\nhow to estimate the derivatives, deriving an estimator that can be seen as a\ngeneralization of the forward sensitivity analysis method used in deterministic\nsystems. We apply the results to examples, including a neural network model.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 05:04:35 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 23:38:10 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 01:06:44 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Flynn", "Thomas", ""]]}, {"id": "1610.09461", "submitter": "Quanming Yao", "authors": "Quanming Yao, James T. Kwok, Xiawei Guo", "title": "Fast Learning with Nonconvex L1-2 Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex regularizers are often used for sparse learning. They are easy to\noptimize, but can lead to inferior prediction performance. The difference of\n$\\ell_1$ and $\\ell_2$ ($\\ell_{1-2}$) regularizer has been recently proposed as\na nonconvex regularizer. It yields better recovery than both $\\ell_0$ and\n$\\ell_1$ regularizers on compressed sensing. However, how to efficiently\noptimize its learning problem is still challenging. The main difficulty is that\nboth the $\\ell_1$ and $\\ell_2$ norms in $\\ell_{1-2}$ are not differentiable,\nand existing optimization algorithms cannot be applied. In this paper, we show\nthat a closed-form solution can be derived for the proximal step associated\nwith this regularizer. We further extend the result for low-rank matrix\nlearning and the total variation model. Experiments on both synthetic and real\ndata sets show that the resultant accelerated proximal gradient algorithm is\nmore efficient than other noncovex optimization algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 06:02:17 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 09:07:03 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 16:51:03 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""], ["Guo", "Xiawei", ""]]}, {"id": "1610.10022", "submitter": "Dirk Lorenz", "authors": "Christoph Brauer, Dirk A. Lorenz, Andreas M. Tillmann", "title": "A Primal-Dual Homotopy Algorithm for $\\ell_{1}$-Minimization with\n  $\\ell_{\\infty}$-Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a primal-dual homotopy method for\n$\\ell_1$-minimization problems with infinity norm constraints in the context of\nsparse reconstruction. The natural homotopy parameter is the value of the bound\nfor the constraints and we show that there exists a piecewise linear solution\npath with finitely many break points for the primal problem and a respective\npiecewise constant path for the dual problem. We show that by solving a small\nlinear program, one can jump to the next primal break point and then, solving\nanother small linear program, a new optimal dual solution is calculated which\nenables the next such jump in the subsequent iteration. Using a theorem of the\nalternative, we show that the method never gets stuck and indeed calculates the\nwhole path in a finite number of steps.\n  Numerical experiments demonstrate the effectiveness of our algorithm. In many\ncases, our method significantly outperforms commercial LP solvers; this is\npossible since our approach employs a sequence of considerably simpler\nauxiliary linear programs that can be solved efficiently with specialized\nactive-set strategies.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 17:15:23 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Brauer", "Christoph", ""], ["Lorenz", "Dirk A.", ""], ["Tillmann", "Andreas M.", ""]]}]