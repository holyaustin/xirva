[{"id": "1303.0167", "submitter": "Stephen Becker", "authors": "Stephen Becker, Volkan Cevher, Anastasios Kyrillidis", "title": "Randomized Low-Memory Singular Value Projection", "comments": "13 pages. This version has a revised theorem and new numerical\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affine rank minimization algorithms typically rely on calculating the\ngradient of a data error followed by a singular value decomposition at every\niteration. Because these two steps are expensive, heuristic approximations are\noften used to reduce computational burden. To this end, we propose a recovery\nscheme that merges the two steps with randomized approximations, and as a\nresult, operates on space proportional to the degrees of freedom in the\nproblem. We theoretically establish the estimation guarantees of the algorithm\nas a function of approximation tolerance. While the theoretical approximation\nrequirements are overly pessimistic, we demonstrate that in practice the\nalgorithm performs well on the quantum tomography recovery problem.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 13:42:38 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 08:12:46 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2013 08:37:59 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Becker", "Stephen", ""], ["Cevher", "Volkan", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "1303.0738", "submitter": "AbdelRahman Karawia Dr.", "authors": "A. A. Karawia", "title": "New Symbolic Algorithms For Solving A General Bordered Tridiagonal\n  Linear System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the author present reliable symbolic algorithms for solving a\ngeneral bordered tridiagonal linear system. The first algorithm is based on the\nLU decomposition of the coefficient matrix and the computational cost of it is\nO(n). The second is based on The Sherman-Morrison-Woodbury formula. The\nalgorithms are implementable to the Computer Algebra System (CAS) such as\nMAPLE, MATLAB and MATHEMATICA. Three examples are presented for the sake of\nillustration.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 15:52:39 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Karawia", "A. A.", ""]]}, {"id": "1303.1264", "submitter": "Radim Belohlavek", "authors": "Radim Belohlavek and Vilem Vychodil", "title": "Discovery of factors in matrices with grades", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to decomposition and factor analysis of matrices with\nordinal data. The matrix entries are grades to which objects represented by\nrows satisfy attributes represented by columns, e.g. grades to which an image\nis red, a product has a given feature, or a person performs well in a test. We\nassume that the grades form a bounded scale equipped with certain aggregation\noperators and conforms to the structure of a complete residuated lattice. We\npresent a greedy approximation algorithm for the problem of decomposition of\nsuch matrix in a product of two matrices with grades under the restriction that\nthe number of factors be small. Our algorithm is based on a geometric insight\nprovided by a theorem identifying particular rectangular-shaped submatrices as\noptimal factors for the decompositions. These factors correspond to formal\nconcepts of the input data and allow an easy interpretation of the\ndecomposition. We present illustrative examples and experimental evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 07:58:14 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Belohlavek", "Radim", ""], ["Vychodil", "Vilem", ""]]}, {"id": "1303.1849", "submitter": "Alex Gittens", "authors": "Alex Gittens and Michael W. Mahoney", "title": "Revisiting the Nystrom Method for Improved Large-Scale Machine Learning", "comments": "60 pages, 15 color figures; updated proof of Frobenius norm bounds,\n  added comparison to projection-based low-rank approximations, and an analysis\n  of the power method applied to SPSD sketches", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconsider randomized algorithms for the low-rank approximation of\nsymmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel\nmatrices that arise in data analysis and machine learning applications. Our\nmain results consist of an empirical evaluation of the performance quality and\nrunning time of sampling and projection methods on a diverse suite of SPSD\nmatrices. Our results highlight complementary aspects of sampling versus\nprojection methods; they characterize the effects of common data preprocessing\nsteps on the performance of these algorithms; and they point to important\ndifferences between uniform sampling and nonuniform sampling methods based on\nleverage scores. In addition, our empirical results illustrate that existing\ntheory is so weak that it does not provide even a qualitative guide to\npractice. Thus, we complement our empirical results with a suite of worst-case\ntheoretical bounds for both random sampling and random projection methods.\nThese bounds are qualitatively superior to existing bounds---e.g. improved\nadditive-error bounds for spectral and Frobenius norm error and relative-error\nbounds for trace norm error---and they point to future directions to make these\nalgorithms useful in even larger-scale machine learning applications.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 23:16:16 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 20:07:19 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Gittens", "Alex", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1303.2033", "submitter": "Vilnis Liepins", "authors": "Vilnis Liepins", "title": "Extended Fourier analysis of signals", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This summary of the doctoral thesis is created to emphasize the close\nconnection of the proposed spectral analysis method with the Discrete Fourier\nTransform (DFT), the most extensively studied and frequently used approach in\nthe history of signal processing. It is shown that in a typical application\ncase, where uniform data readings are transformed to the same number of\nuniformly spaced frequencies, the results of the classical DFT and proposed\napproach coincide. The difference in performance appears when the length of the\nDFT is selected to be greater than the length of the data. The DFT solves the\nunknown data problem by padding readings with zeros up to the length of the\nDFT, while the proposed Extended DFT (EDFT) deals with this situation in a\ndifferent way, it uses the Fourier integral transform as a target and optimizes\nthe transform basis in the extended frequency range without putting such\nrestrictions on the time domain. Consequently, the Inverse DFT (IDFT) applied\nto the result of EDFT returns not only known readings, but also the\nextrapolated data, where classical DFT is able to give back just zeros, and\nhigher resolution are achieved at frequencies where the data has been\nsuccessfully extended. It has been demonstrated that EDFT able to process data\nwith missing readings or gaps inside or even nonuniformly distributed data.\nThus, EDFT significantly extends the usability of the DFT-based methods, where\npreviously these approaches have been considered as not applicable. The EDFT\nfounds the solution in an iterative way and requires repeated calculations to\nget the adaptive basis, and this makes it numerical complexity much higher\ncompared to DFT. This disadvantage was a serious problem in the 1990s, when the\nmethod has been proposed. Fortunately, since then the power of computers has\nincreased so much that nowadays EDFT application could be considered as a real\nalternative.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 15:47:28 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 14:58:43 GMT"}, {"version": "v3", "created": "Fri, 31 May 2013 13:15:18 GMT"}, {"version": "v4", "created": "Tue, 15 Oct 2013 08:47:43 GMT"}, {"version": "v5", "created": "Thu, 23 Jan 2014 08:39:21 GMT"}, {"version": "v6", "created": "Fri, 12 Sep 2014 11:43:40 GMT"}, {"version": "v7", "created": "Mon, 23 Feb 2015 06:29:25 GMT"}, {"version": "v8", "created": "Thu, 28 Sep 2017 07:26:38 GMT"}, {"version": "v9", "created": "Tue, 12 Mar 2019 09:20:17 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Liepins", "Vilnis", ""]]}, {"id": "1303.2238", "submitter": "Jean-Philippe Braeunig", "authors": "Jean-Philippe Braeunig (CEA-DAM-DIF), Nicolas Crouseilles (IRMAR,\n  INRIA - IRMAR), Virginie Grandgirard (IRFM), Guillaume Latu (IRFM), Michel\n  Mehrenberger (INRIA Nancy - Grand Est / IECN / LSIIT / IRMA, IRMA), Eric\n  Sonnendr\\\"ucker (INRIA Nancy - Grand Est / IECN / LSIIT / IRMA, IRMA)", "title": "Some numerical aspects of the conservative PSM scheme in a 4D\n  drift-kinetic code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is simulation of magnetised plasmas in the ITER\nproject framework. In this context, kinetic Vlasov-Poisson like models are used\nto simulate core turbulence in the tokamak in a toroidal geometry. This leads\nto heavy simulations because a 6D dimensional problem has to be solved, even if\nreduced to a 5D in so called gyrokinetic models. Accurate schemes, parallel\nalgorithms need to be designed to bear these simulations. This paper describes\nthe numerical studies to improve robustness of the conservative PSM scheme in\nthe context of its development in the GYSELA code. In this paper, we only\nconsider the 4D drift-kinetic model which is the backbone of the 5D gyrokinetic\nmodels and relevant to build a robust and accurate numerical method.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 16:55:30 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Braeunig", "Jean-Philippe", "", "CEA-DAM-DIF"], ["Crouseilles", "Nicolas", "", "IRMAR,\n  INRIA - IRMAR"], ["Grandgirard", "Virginie", "", "IRFM"], ["Latu", "Guillaume", "", "IRFM"], ["Mehrenberger", "Michel", "", "INRIA Nancy - Grand Est / IECN / LSIIT / IRMA, IRMA"], ["Sonnendr\u00fccker", "Eric", "", "INRIA Nancy - Grand Est / IECN / LSIIT / IRMA, IRMA"]]}, {"id": "1303.3182", "submitter": "Henricus Bouwmeester", "authors": "Henricus Bouwmeester", "title": "Tiled Algorithms for Matrix Computations on Multicore Architectures", "comments": "PhD Thesis, 2012 http://math.ucdenver.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current computer architecture has moved towards the multi/many-core\nstructure. However, the algorithms in the current sequential dense numerical\nlinear algebra libraries (e.g. LAPACK) do not parallelize well on\nmulti/many-core architectures. A new family of algorithms, the tile algorithms,\nhas recently been introduced to circumvent this problem. Previous research has\nshown that it is possible to write efficient and scalable tile algorithms for\nperforming a Cholesky factorization, a (pseudo) LU factorization, and a QR\nfactorization. The goal of this thesis is to study tiled algorithms in a\nmulti/many-core setting and to provide new algorithms which exploit the current\narchitecture to improve performance relative to current state-of-the-art\nlibraries while maintaining the stability and robustness of these libraries.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 15:04:05 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Bouwmeester", "Henricus", ""]]}, {"id": "1303.3614", "submitter": "Tae-Hyuk Ahn", "authors": "Tae-Hyuk Ahn and Adrian Sandu and Xiaoying Han", "title": "Implicit Simulation Methods for Stochastic Chemical Kinetics", "comments": "Prepare submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biochemical systems some of the chemical species are present with only\nsmall numbers of molecules. In this situation discrete and stochastic\nsimulation approaches are more relevant than continuous and deterministic ones.\nThe fundamental Gillespie's stochastic simulation algorithm (SSA) accounts for\nevery reaction event, which occurs with a probability determined by the\nconfiguration of the system. This approach requires a considerable\ncomputational effort for models with many reaction channels and chemical\nspecies. In order to improve efficiency, tau-leaping methods represent multiple\nfirings of each reaction during a simulation step by Poisson random variables.\nFor stiff systems the mean of this variable is treated implicitly in order to\nensure numerical stability.\n  This paper develops fully implicit tau-leaping-like algorithms that treat\nimplicitly both the mean and the variance of the Poisson variables. The\nconstruction is based on adapting weakly convergent discretizations of\nstochastic differential equations to stochastic chemical kinetic systems.\nTheoretical analyses of accuracy and stability of the new methods are performed\non a standard test problem. Numerical results demonstrate the performance of\nthe proposed tau-leaping methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 21:09:50 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Ahn", "Tae-Hyuk", ""], ["Sandu", "Adrian", ""], ["Han", "Xiaoying", ""]]}, {"id": "1303.4207", "submitter": "Shusen Wang", "authors": "Shusen Wang, Zhihua Zhang", "title": "Improving CUR Matrix Decomposition and the Nystr\\\"{o}m Approximation via\n  Adaptive Sampling", "comments": null, "journal-ref": "Journal of Machine Learning Research, 14: 2549-2589, 2013", "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CUR matrix decomposition and the Nystr\\\"{o}m approximation are two\nimportant low-rank matrix approximation techniques. The Nystr\\\"{o}m method\napproximates a symmetric positive semidefinite matrix in terms of a small\nnumber of its columns, while CUR approximates an arbitrary data matrix by a\nsmall number of its columns and rows. Thus, CUR decomposition can be regarded\nas an extension of the Nystr\\\"{o}m approximation.\n  In this paper we establish a more general error bound for the adaptive\ncolumn/row sampling algorithm, based on which we propose more accurate CUR and\nNystr\\\"{o}m algorithms with expected relative-error bounds. The proposed CUR\nand Nystr\\\"{o}m algorithms also have low time complexity and can avoid\nmaintaining the whole data matrix in RAM. In addition, we give theoretical\nanalysis for the lower error bounds of the standard Nystr\\\"{o}m method and the\nensemble Nystr\\\"{o}m method. The main theoretical results established in this\npaper are novel, and our analysis makes no special assumption on the data\nmatrices.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 11:17:55 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2013 04:20:45 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2013 08:28:43 GMT"}, {"version": "v4", "created": "Tue, 28 May 2013 07:17:40 GMT"}, {"version": "v5", "created": "Fri, 31 May 2013 05:12:41 GMT"}, {"version": "v6", "created": "Thu, 12 Sep 2013 06:56:13 GMT"}, {"version": "v7", "created": "Tue, 1 Oct 2013 06:31:11 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Wang", "Shusen", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1303.4434", "submitter": "Pinghua Gong", "authors": "Pinghua Gong, Changshui Zhang, Zhaosong Lu, Jianhua Huang, Jieping Ye", "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex\n  Regularized Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex sparsity-inducing penalties have recently received considerable\nattentions in sparse learning. Recent theoretical investigations have\ndemonstrated their superiority over the convex counterparts in several sparse\nlearning settings. However, solving the non-convex optimization problems\nassociated with non-convex penalties remains a big challenge. A commonly used\napproach is the Multi-Stage (MS) convex relaxation (or DC programming), which\nrelaxes the original non-convex problem to a sequence of convex problems. This\napproach is usually not very practical for large-scale problems because its\ncomputational cost is a multiple of solving a single convex problem. In this\npaper, we propose a General Iterative Shrinkage and Thresholding (GIST)\nalgorithm to solve the nonconvex optimization problem for a large class of\nnon-convex penalties. The GIST algorithm iteratively solves a proximal operator\nproblem, which in turn has a closed-form solution for many commonly used\npenalties. At each outer iteration of the algorithm, we use a line search\ninitialized by the Barzilai-Borwein (BB) rule that allows finding an\nappropriate step size quickly. The paper also presents a detailed convergence\nanalysis of the GIST algorithm. The efficiency of the proposed algorithm is\ndemonstrated by extensive experiments on large-scale data sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 21:41:53 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Gong", "Pinghua", ""], ["Zhang", "Changshui", ""], ["Lu", "Zhaosong", ""], ["Huang", "Jianhua", ""], ["Ye", "Jieping", ""]]}, {"id": "1303.4942", "submitter": "Kumar Eswaran Dr.", "authors": "K. Eswaran", "title": "A Novel Algorithm for Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimizing a linear objective function,given a number of\nlinear constraints has been a long standing problem ever since the times of\nKantorovich, Dantzig and von Neuman. These developments have been followed by a\ndifferent approach pioneered by Khachiyan and Karmarkar.\n  In this paper we present an entirely new method for solving an old\noptimization problem in a novel manner, a technique that reduces the dimension\nof the problem step by step and interestingly is recursive. A theorem which\nproves the correctness of the approach is given.\n  The method can be extended to other types of optimization problems in convex\nspace, e.g. for solving a linear optimization problem subject to nonlinear\nconstraints in a convex region.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 13:57:11 GMT"}], "update_date": "2013-03-21", "authors_parsed": [["Eswaran", "K.", ""]]}, {"id": "1303.6370", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka, Taiji Suzuki", "title": "Convex Tensor Decomposition via Structured Schatten Norm Regularization", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss structured Schatten norms for tensor decomposition that includes\ntwo recently proposed norms (\"overlapped\" and \"latent\") for\nconvex-optimization-based tensor decomposition, and connect tensor\ndecomposition with wider literature on structured sparsity. Based on the\nproperties of the structured Schatten norms, we mathematically analyze the\nperformance of \"latent\" approach for tensor decomposition, which was\nempirically found to perform better than the \"overlapped\" approach in some\nsettings. We show theoretically that this is indeed the case. In particular,\nwhen the unknown true tensor is low-rank in a specific mode, this approach\nperforms as good as knowing the mode with the smallest rank. Along the way, we\nshow a novel duality result for structures Schatten norms, establish the\nconsistency, and discuss the identifiability of this approach. We confirm\nthrough numerical simulations that our theoretical prediction can precisely\npredict the scaling behavior of the mean squared error.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 02:36:49 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Tomioka", "Ryota", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1303.6683", "submitter": "Josildo Silva", "authors": "Josildo Pereira da Silva and Ant\\^onio Lopes Apolin\\'ario J\\'unior and\n  Gilson A. Giraldi", "title": "A Review of Dynamic NURBS Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.NA physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Dynamic NURBS, also called D-NURBS, is a known dynamic version of the\nnonuniform rational B-spline (NURBS) which integrates free-form shape\nrepresentation and a physically-based model in a unified framework. More\nrecently, computer aided design (CAD) and finite element (FEM) community\nrealized the need to unify CAD and FEM descriptions which motivates a review of\nD-NURBS concepts. Therefore, in this paper we describe D-NURBS theory in the\ncontext of 1D shape deformations. We start with a revision of NURBS for\nparametric representation of curve spaces. Then, the Lagrangian mechanics is\nintroduced in order to complete the theoretical background. Next, the D-NURBS\nframework for 1D curve spaces is presented as well as some details about\nconstraints and numerical implementations. In the experimental results, we\nfocus on parameters choice and computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 22:14:22 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["da Silva", "Josildo Pereira", ""], ["J\u00fanior", "Ant\u00f4nio Lopes Apolin\u00e1rio", ""], ["Giraldi", "Gilson A.", ""]]}, {"id": "1303.6796", "submitter": "Tomasz Tyranowski", "authors": "Tomasz M. Tyranowski, Mathieu Desbrun", "title": "R-adaptive multisymplectic and variational integrators", "comments": "65 pages, 13 figures", "journal-ref": "Mathematics 2019, 7(7), 642", "doi": "10.3390/math7070642", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving mesh methods (also called r-adaptive methods) are space-adaptive\nstrategies used for the numerical simulation of time-dependent partial\ndifferential equations. These methods keep the total number of mesh points\nfixed during the simulation, but redistribute them over time to follow the\nareas where a higher mesh point density is required. There are a very limited\nnumber of moving mesh methods designed for solving field-theoretic partial\ndifferential equations, and the numerical analysis of the resulting schemes is\nchallenging. In this paper we present two ways to construct r-adaptive\nvariational and multisymplectic integrators for (1+1)-dimensional Lagrangian\nfield theories. The first method uses a variational discretization of the\nphysical equations and the mesh equations are then coupled in a way typical of\nthe existing r-adaptive schemes. The second method treats the mesh points as\npseudo-particles and incorporates their dynamics directly into the variational\nprinciple. A user-specified adaptation strategy is then enforced through\nLagrange multipliers as a constraint on the dynamics of both the physical field\nand the mesh points. We discuss the advantages and limitations of our methods.\nNumerical results for the Sine-Gordon equation are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 11:40:47 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 13:37:31 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Tyranowski", "Tomasz M.", ""], ["Desbrun", "Mathieu", ""]]}]