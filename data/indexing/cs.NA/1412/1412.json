[{"id": "1412.0393", "submitter": "Kirk Soodhalter", "authors": "Kirk M. Soodhalter", "title": "Block Krylov subspace recycling for shifted systems with unrelated\n  right-hand sides", "comments": "24 pages, 4 figures, 2 tables", "journal-ref": "SIAM J. Sci. Comput., 38(1), A302-A324, 2016", "doi": "10.1137/140998214", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Krylov subspace methods for shifted linear systems take advantage of the\ninvariance of the Krylov subspace under a shift of the matrix. However,\nexploiting this fact in the non-Hermitian case introduces restrictions; e.g.,\ninitial residuals must be collinear and this collinearity must be maintained at\nrestart. Thus we cannot simultaneously solve shifted systems with unrelated\nright-hand sides using this strategy, and all shifted residuals cannot be\nsimultaneously minimized over a Krylov subspace such that collinearity is\nmaintained. It has been shown that this renders them generally incompatible\nwith techniques of subspace recycling [Soodhalter et al. APNUM '14].\n  This problem, however, can be overcome. By interpreting a family of shifted\nsystems as one Sylvester equation, we can take advantage of the known \"shift\ninvariance\" of the Krylov subspace generated by the Sylvester operator. Thus we\ncan simultaneously solve all systems over one block Krylov subspace using FOM\nor GMRES type methods, even when they have unrelated right-hand sides. Because\nresidual collinearity is no longer a requirement at restart, these methods are\nfully compatible with subspace recycling techniques. Furthermore, we realize\nthe benefits of block sparse matrix operations which arise in the context of\nhigh-performance computing applications.\n  In this paper, we discuss exploiting this Sylvester equation point of view\nwhich has yielded methods for shifted systems which are compatible with\nunrelated right-hand sides. From this, we propose a recycled GMRES method for\nsimultaneous solution of shifted systems.Numerical experiments demonstrate the\neffectiveness of the methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 09:33:58 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 12:55:13 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Soodhalter", "Kirk M.", ""]]}, {"id": "1412.0904", "submitter": "Kai Sun", "authors": "Nan Duan and Kai Sun", "title": "Finding Semi-Analytic Solutions of Power System Differential-Algebraic\n  Equations for Fast Transient Stability Simulation", "comments": "An extension of this work has been published as:Nan Duan, Kai Sun,\n  \"Power System Simulation Using the Multi-stage Adomian Decomposition Method,\n  IEEE Transactions on Power Systems,\" vol. 32, no. 1, pp. 430-441, January\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.NA cs.SC cs.SY math.CA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper studies the semi-analytic solution (SAS) of a power system's\ndifferential-algebraic equation. A SAS is a closed-form function of symbolic\nvariables including time, the initial state and the parameters on system\noperating conditions, and hence able to directly give trajectories on system\nstate variables, which are accurate for at least a certain time window. A\ntwo-stage SAS-based approach for fast transient stability simulation is\nproposed, which offline derives the SAS by the Adomian Decomposition Method and\nonline evaluates the SAS for each of sequential time windows until making up a\ndesired simulation period. When applied to fault simulation, the new approach\nemploys numerical integration only for the fault-on period to determine the\npost-disturbance initial state of the SAS. The paper further analyzes the\nmaximum length of a time window for a SAS to keep its accuracy, and\naccordingly, introduces a divergence indicator for adaptive time windows. The\nproposed SAS-based new approach is validated on the IEEE 10-machine, 39-bus\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 13:28:12 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 14:00:56 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Duan", "Nan", ""], ["Sun", "Kai", ""]]}, {"id": "1412.1687", "submitter": "Alexey Smirnov", "authors": "A. V. Smirnov", "title": "The Approximate Bilinear Algorithm of Length 46 for Multiplication of 4\n  x 4 Matrices", "comments": "PDF, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the arbitrary precision approximate (APA) bilinear algorithm of\nlength 46 for multiplication of 4 x 4 and 4 x 4 matrices. The algorithm has\npolynomial order 3 and 352 nonzero coefficients from total 2208.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 15:30:47 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Smirnov", "A. V.", ""]]}, {"id": "1412.1717", "submitter": "Dhagash Mehta", "authors": "Joseph Cleveland, Jeffrey Dzugan, Jonathan D. Hauenstein, Ian Haywood,\n  Dhagash Mehta, Anthony Morse, Leonardo Robol and Taylor Schlenk", "title": "Certified counting of roots of random univariate polynomials", "comments": "28 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA hep-th nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenging problem in computational mathematics is to compute roots of a\nhigh-degree univariate random polynomial. We combine an efficient\nmultiprecision implementation for solving high-degree random polynomials with\ntwo certification methods, namely Smale's $\\alpha$-theory and one based on\nGerschgorin's theorem, for showing that a given numerical approximation is in\nthe quadratic convergence region of Newton's method of some exact solution.\nWith this combination, we can certifiably count the number of real roots of\nrandom polynomials. We quantify the difference between the two certification\nprocedures and list the salient features of both of them. After benchmarking on\nrandom polynomials where the coefficients are drawn from the Gaussian\ndistribution, we obtain novel experimental results for the Cauchy distribution\ncase.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 16:20:06 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Cleveland", "Joseph", ""], ["Dzugan", "Jeffrey", ""], ["Hauenstein", "Jonathan D.", ""], ["Haywood", "Ian", ""], ["Mehta", "Dhagash", ""], ["Morse", "Anthony", ""], ["Robol", "Leonardo", ""], ["Schlenk", "Taylor", ""]]}, {"id": "1412.1885", "submitter": "Guoxu Zhou", "authors": "Guoxu Zhou and Andrzej Cichocki and Shengli Xie", "title": "Decomposition of Big Tensors With Low Multilinear Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decompositions are promising tools for big data analytics as they\nbring multiple modes and aspects of data to a unified framework, which allows\nus to discover complex internal structures and correlations of data.\nUnfortunately most existing approaches are not designed to meet the major\nchallenges posed by big data analytics. This paper attempts to improve the\nscalability of tensor decompositions and provides two contributions: A flexible\nand fast algorithm for the CP decomposition (FFCP) of tensors based on their\nTucker compression; A distributed randomized Tucker decomposition approach for\narbitrarily big tensors but with relatively low multilinear rank. These two\nalgorithms can deal with huge tensors, even if they are dense. Extensive\nsimulations provide empirical evidence of the validity and efficiency of the\nproposed algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 03:04:32 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 02:39:53 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Zhou", "Guoxu", ""], ["Cichocki", "Andrzej", ""], ["Xie", "Shengli", ""]]}, {"id": "1412.2231", "submitter": "Canyi Lu", "authors": "Canyi Lu, Changbo Zhu, Chunyan Xu, Shuicheng Yan, Zhouchen Lin", "title": "Generalized Singular Value Thresholding", "comments": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the Generalized Singular Value Thresholding (GSVT) operator\n${\\text{Prox}}_{g}^{{\\sigma}}(\\cdot)$, \\begin{equation*}\n  {\\text{Prox}}_{g}^{{\\sigma}}(B)=\\arg\\min\\limits_{X}\\sum_{i=1}^{m}g(\\sigma_{i}(X))\n+ \\frac{1}{2}||X-B||_{F}^{2}, \\end{equation*} associated with a nonconvex\nfunction $g$ defined on the singular values of $X$. We prove that GSVT can be\nobtained by performing the proximal operator of $g$ (denoted as\n$\\text{Prox}_g(\\cdot)$) on the singular values since $\\text{Prox}_g(\\cdot)$ is\nmonotone when $g$ is lower bounded. If the nonconvex $g$ satisfies some\nconditions (many popular nonconvex surrogate functions, e.g., $\\ell_p$-norm,\n$0<p<1$, of $\\ell_0$-norm are special cases), a general solver to find\n$\\text{Prox}_g(b)$ is proposed for any $b\\geq0$. GSVT greatly generalizes the\nknown Singular Value Thresholding (SVT) which is a basic subroutine in many\nconvex low rank minimization methods. We are able to solve the nonconvex low\nrank minimization problem by using GSVT in place of SVT.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 13:08:29 GMT"}, {"version": "v2", "created": "Sun, 27 May 2018 05:56:25 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Lu", "Canyi", ""], ["Zhu", "Changbo", ""], ["Xu", "Chunyan", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1412.2276", "submitter": "Saul Teukolsky", "authors": "Saul A. Teukolsky", "title": "Short note on the mass matrix for Gauss-Lobatto grid points", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2014.12.012", "report-no": null, "categories": "math.NA cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mass matrix for Gauss-Lobatto grid points is usually approximated by\nGauss-Lobatto quadrature because this leads to a diagonal matrix that is easy\nto invert. The exact mass matrix and its inverse are full. We show that the\nexact mass matrix \\emph{and} its inverse differ from the approximate diagonal\nones by a simple rank-1 update (outer product). They can thus be applied to an\narbitrary vector in $O(N)$ operations instead of $O(N^2)$.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 20:33:49 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Teukolsky", "Saul A.", ""]]}, {"id": "1412.2675", "submitter": "Yilun Wang", "authors": "Yaru Fan, Yilun Wang and Tingzhu Huang", "title": "Enhanced joint sparsity via Iterative Support Detection", "comments": "arXiv admin note: text overlap with arXiv:1008.4348 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint sparsity has attracted considerable attention in recent years in many\nfields including sparse signal recovery in compressed sensing (CS), statistics,\nand machine learning. Traditional convex models suffer from the suboptimal\nperformance though enjoying tractable computation. In this paper, we propose a\nnew non-convex joint sparsity model, and develop a corresponding multi-stage\nadaptive convex relaxation algorithm. This method extends the idea of iterative\nsupport detection (ISD) from the single vector estimation to the multi-vector\nestimation by considering the joint sparsity prior. We provide some preliminary\ntheoretical analysis including convergence analysis and a sufficient recovery\ncondition. Numerical experiments from both compressive sensing and feature\nlearning show the better performance of the proposed method in comparison with\nseveral state-of-the-art alternatives. Moreover, we demonstrate that the\nextension of ISD from the single vector to multi-vector estimation is not\ntrivial. In particular, while ISD does not work well for reconstructing the\nsignal channel sparse Bernoulli signal, it does achieve significantly improved\nperformance when recovering the multi-channel sparse Bernoulli signal thanks to\nits ability of natural incorporation of the joint sparsity structure.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 17:42:58 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 19:02:25 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2015 02:25:56 GMT"}, {"version": "v4", "created": "Sun, 25 Jun 2017 09:42:10 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Fan", "Yaru", ""], ["Wang", "Yilun", ""], ["Huang", "Tingzhu", ""]]}, {"id": "1412.2700", "submitter": "Sampurna Biswas", "authors": "Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai\n  and Mathews Jacob", "title": "Subspace based low rank and joint sparse matrix recovery", "comments": "5 pages, 5 figures, Asilomar 2014 conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recovery of a low rank and jointly sparse matrix from under\nsampled measurements of its columns. This problem is highly relevant in the\nrecovery of dynamic MRI data with high spatio-temporal resolution, where each\ncolumn of the matrix corresponds to a frame in the image time series; the\nmatrix is highly low-rank since the frames are highly correlated. Similarly the\nnon-zero locations of the matrix in appropriate transform/frame domains (e.g.\nwavelet, gradient) are roughly the same in different frame. The superset of the\nsupport can be safely assumed to be jointly sparse. Unlike the classical\nmultiple measurement vector (MMV) setup that measures all the snapshots using\nthe same matrix, we consider each snapshot to be measured using a different\nmeasurement matrix. We show that this approach reduces the total number of\nmeasurements, especially when the rank of the matrix is much smaller than than\nits sparsity. Our experiments in the context of dynamic imaging shows that this\napproach is very useful in realizing free breathing cardiac MRI.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 18:42:14 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 18:36:17 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Biswas", "Sampurna", ""], ["Poddar", "Sunrita", ""], ["Dasgupta", "Soura", ""], ["Mudumbai", "Raghuraman", ""], ["Jacob", "Mathews", ""]]}, {"id": "1412.3335", "submitter": "Narendra Karmarkar Dr", "authors": "Narendra Karmarkar", "title": "Towards a Broader View of Theory of Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beginning with the projectively invariant method for linear programming,\ninterior point methods have led to powerful algorithms for many difficult\ncomputing problems, in combinatorial optimization, logic, number theory and\nnon-convex optimization. Algorithms for convex optimization benefitted from\nmany pre-established ideas from classical mathematics, but non-convex problems\nrequire new concepts. Lecture series I am presenting at the conference on\nFoundations of Computational Mathematics, 2014, outlines some of these\nconcepts{computational models based on the concept of the continuum, algorithms\ninvariant w.r.t. projective, bi-rational, and bi-holomorphic transformations on\nco-ordinate representation, extended proof systems for more efficient\ncertificates of optimality, extensions of Grassmanns extension theory,\nefficient evaluation methods for the effect of exponential number of\nconstraints, theory of connected sets based on graded connectivity, theory of\ncurved spaces adapted to the problem data, and concept of relatively algebraic\nsets in curved space. Since this conference does not have a proceedings, the\npurpose of this article is to provide the material being presented at the\nconference in more widely accessible form.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 15:16:23 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Karmarkar", "Narendra", ""]]}, {"id": "1412.4044", "submitter": "Jun He", "authors": "Jun He, Yue Zhang", "title": "Adaptive Stochastic Gradient Descent on the Grassmannian for Robust\n  Low-Rank Subspace Recovery and Clustering", "comments": "13 pages, 12 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradient\nfor $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm to\nrobustly recover the low-rank subspace from a large matrix. In the presence of\ncolumn outliers, we reformulate the batch mode matrix $L_{2,1}$ norm\nminimization with rank constraint problem as a stochastic optimization approach\nconstrained on Grassmann manifold. For each observed data vector, the low-rank\nsubspace $\\mathcal{S}$ is updated by taking a gradient step along the geodesic\nof Grassmannian. In order to accelerate the convergence rate of the stochastic\ngradient method, we choose to adaptively tune the constant step-size by\nleveraging the consecutive gradients. Furthermore, we demonstrate that with\nproper initialization, the K-subspaces extension, K-GASG21, can robustly\ncluster a large number of corrupted data vectors into a union of subspaces.\nNumerical experiments on synthetic and real data demonstrate the efficiency and\naccuracy of the proposed algorithms even with heavy column outliers corruption.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 16:32:48 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2015 08:37:45 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["He", "Jun", ""], ["Zhang", "Yue", ""]]}, {"id": "1412.5706", "submitter": "Petr Vabishchevich N.", "authors": "Petr N. Vabishchevich", "title": "Numerical solution of nonstationary problems for a space-fractional\n  diffusion equation", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsteady problem is considered for a space-fractional diffusion equation\nin a bounded domain. A first-order evolutionary equation containing a\nfractional power of an elliptic operator of second order is studied for general\nboundary conditions of Robin type. Finite element approximation in space is\nemployed. To construct approximation in time, regularized two-level schemes are\nused. The numerical implementation is based on solving the equation with the\nfractional power of the elliptic operator using an auxiliary Cauchy problem for\na pseudo-parabolic equation. The results of numerical experiments are presented\nfor a model two-dimensional problem.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 02:26:59 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Vabishchevich", "Petr N.", ""]]}, {"id": "1412.6291", "submitter": "Maciej Wielgus", "authors": "Maciek Wielgus", "title": "Perona-Malik equation and its numerical properties", "comments": "My bachelor thesis, Faculty of Mathematics, Informatics and\n  Mechanics, University of Warsaw, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work concerns the Perona-Malik equation, which plays essential role in\nimage processing. The first part gives a survey of results on existance,\nuniqueness and stability of solutions, the second part introduces\ndiscretisations of equation and deals with an analysis of discrete problem. In\nthe last part I present some numerical results, in particular with algorithms\napplied to real images.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 11:17:38 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Wielgus", "Maciek", ""]]}, {"id": "1412.6293", "submitter": "Zheng Qu", "authors": "Jakub Kone\\v{c}n\\'y and Zheng Qu and Peter Richt\\'arik", "title": "Semi-Stochastic Coordinate Descent", "comments": "14 pages. The paper was accepted for presentation at the 2014 NIPS\n  Optimization for Machine Learning workshop in a peer reviewed process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel stochastic gradient method---semi-stochastic coordinate\ndescent (S2CD)---for the problem of minimizing a strongly convex function\nrepresented as the average of a large number of smooth convex functions:\n$f(x)=\\tfrac{1}{n}\\sum_i f_i(x)$. Our method first performs a deterministic\nstep (computation of the gradient of $f$ at the starting point), followed by a\nlarge number of stochastic steps. The process is repeated a few times, with the\nlast stochastic iterate becoming the new starting point where the deterministic\nstep is taken. The novelty of our method is in how the stochastic steps are\nperformed. In each such step, we pick a random function $f_i$ and a random\ncoordinate $j$---both using nonuniform distributions---and update a single\ncoordinate of the decision vector only, based on the computation of the\n$j^{th}$ partial derivative of $f_i$ at two different points. Each random step\nof the method constitutes an unbiased estimate of the gradient of $f$ and\nmoreover, the squared norm of the steps goes to zero in expectation, meaning\nthat the stochastic estimate of the gradient progressively improves. The\ncomplexity of the method is the sum of two terms: $O(n\\log(1/\\epsilon))$\nevaluations of gradients $\\nabla f_i$ and $O(\\hat{\\kappa}\\log(1/\\epsilon))$\nevaluations of partial derivatives $\\nabla_j f_i$, where $\\hat{\\kappa}$ is a\nnovel condition number.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 11:20:13 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1412.6407", "submitter": "Pierre de Buyl", "authors": "Robert Cimrman", "title": "Enhancing SfePy with Isogeometric Analysis", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-11", "categories": "cs.MS cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In the paper a recent enhancement to the open source package SfePy (Simple\nFinite Elements in Python, http://sfepy.org) is introduced, namely the addition\nof another numerical discretization scheme, the isogeometric analysis, to the\noriginal implementation based on the nowadays standard and well-established\nnumerical solution technique, the finite element method. The isogeometric\nremoves the need of the solution domain approximation by a piece-wise polygonal\ndomain covered by the finite element mesh, and allows approximation of unknown\nfields with a higher smoothness then the finite element method, which can be\nadvantageous in many applications. Basic numerical examples illustrating the\nimplementation and use of the isogeometric analysis in SfePy are shown.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 16:03:26 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Cimrman", "Robert", ""]]}, {"id": "1412.7364", "submitter": "David Gleich", "authors": "David F. Gleich and Ananth Grama and Yao Zhu", "title": "Erasure coding for fault oblivious linear system solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with hardware and software faults is an important problem as parallel\nand distributed systems scale to millions of processing cores and wide area\nnetworks. Traditional methods for dealing with faults include\ncheckpoint-restart, active replicas, and deterministic replay. Each of these\ntechniques has associated resource overheads and constraints. In this paper, we\npropose an alternate approach to dealing with faults, based on input\naugmentation. This approach, which is an algorithmic analog of erasure coded\nstorage, applies a minimally modified algorithm on the augmented input to\nproduce an augmented output. The execution of such an algorithm proceeds\ncompletely oblivious to faults in the system. In the event of one or more\nfaults, the real solution is recovered using a rapid reconstruction method from\nthe augmented output. We demonstrate this approach on the problem of solving\nsparse linear systems using a conjugate gradient solver. We present input\naugmentation and output recovery techniques. Through detailed experiments, we\nshow that our approach can be made oblivious to a large number of faults with\nlow computational overhead. Specifically, we demonstrate cases where a single\nfault can be corrected with less than 10% overhead in time, and even in extreme\ncases (fault rates of 20%), our approach is able to compute a solution with\nreasonable overhead. These results represent a significant improvement over the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 14:04:34 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Gleich", "David F.", ""], ["Grama", "Ananth", ""], ["Zhu", "Yao", ""]]}, {"id": "1412.7552", "submitter": "Danial Sadeghi", "authors": "Danial Sadeghi, Azim Rivaz", "title": "An introduction to the Equiangular algorithm", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a generalization of the Gram-Schmidt Algorithm is presented.\nActually we provide an algorithm to construct a set of equiangular vectors with\na given angle $\\theta\\in(0,\\arccos(\\frac{-1}{n-1}))$ using a set of input\nindependent vectors in $\\mathbb{R}^n$. Therefore a usual type of matrix\ndecomposition is derived. Then we discuss some properties of matrices derived\nfrom the new algorithm. The inverse and eigenvalue problems of these matrices\nif there exist are studied. Also, we derive some canonical forms based on the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 21:30:26 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 13:56:54 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 15:47:28 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 07:06:55 GMT"}, {"version": "v5", "created": "Sat, 17 Oct 2020 17:00:06 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sadeghi", "Danial", ""], ["Rivaz", "Azim", ""]]}, {"id": "1412.8045", "submitter": "Robert Gower", "authors": "Robert Mansel Gower, Jacek Gondzio", "title": "Action constrained quasi-Newton methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of Newton based optimization methods is a sequence of symmetric\nlinear systems. Each consecutive system in this sequence is similar to the\nnext, so solving them separately is a waste of computational effort. Here we\ndescribe automatic preconditioning techniques for iterative methods for solving\nsuch sequences of systems by maintaining an estimate of the inverse system\nmatrix. We update the estimate of the inverse system matrix with quasi-Newton\ntype formulas based on what we call an action constraint instead of the secant\nequation. We implement the estimated inverses as preconditioners in a Newton-CG\nmethod and prove quadratic termination. Our implementation is the first\nparallel quasi-Newton preconditioners, in full and limited memory variants.\nTests on logistic Support Vector Machine problems reveal that our method is\nvery efficient, converging in wall clock time before a Newton-CG method without\npreconditioning. Further tests on a set of classic test problems reveal that\nthe method is robust. The action constraint makes these updates flexible enough\nto mesh with trust-region and active set methods, a flexibility that is not\npresent in classic quasi-Newton methods.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 13:40:10 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Gower", "Robert Mansel", ""], ["Gondzio", "Jacek", ""]]}, {"id": "1412.8060", "submitter": "Zheng Qu", "authors": "Zheng Qu and Peter Richt\\'arik", "title": "Coordinate Descent with Arbitrary Sampling I: Algorithms and Complexity", "comments": "32 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of minimizing the sum of a smooth convex function and a\nconvex block-separable regularizer and propose a new randomized coordinate\ndescent method, which we call ALPHA. Our method at every iteration updates a\nrandom subset of coordinates, following an arbitrary distribution. No\ncoordinate descent methods capable to handle an arbitrary sampling have been\nstudied in the literature before for this problem. ALPHA is a remarkably\nflexible algorithm: in special cases, it reduces to deterministic and\nrandomized methods such as gradient descent, coordinate descent, parallel\ncoordinate descent and distributed coordinate descent -- both in nonaccelerated\nand accelerated variants. The variants with arbitrary (or importance) sampling\nare new. We provide a complexity analysis of ALPHA, from which we deduce as a\ndirect corollary complexity bounds for its many variants, all matching or\nimproving best known bounds.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 15:28:26 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 16:09:21 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1412.8063", "submitter": "Zheng Qu", "authors": "Zheng Qu and Peter Richt\\'arik", "title": "Coordinate Descent with Arbitrary Sampling II: Expected Separable\n  Overapproximation", "comments": "29 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and complexity analysis of randomized coordinate descent methods,\nand in particular of variants which update a random subset (sampling) of\ncoordinates in each iteration, depends on the notion of expected separable\noverapproximation (ESO). This refers to an inequality involving the objective\nfunction and the sampling, capturing in a compact way certain smoothness\nproperties of the function in a random subspace spanned by the sampled\ncoordinates. ESO inequalities were previously established for special classes\nof samplings only, almost invariably for uniform samplings. In this paper we\ndevelop a systematic technique for deriving these inequalities for a large\nclass of functions and for arbitrary samplings. We demonstrate that one can\nrecover existing ESO results using our general approach, which is based on the\nstudy of eigenvalues associated with samplings and the data describing the\nfunction.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 15:39:30 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 19:49:20 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1412.8185", "submitter": "Yuliya Boyarinova", "authors": "Yakiv O. Kalinovsky, Yuliya E. Boyarinova, Alina S. Turenko, Yana V.\n  Khitsko", "title": "Generalized quaternions and their relations with Grassmann-Clifford\n  procedure of doubling", "comments": "arXiv admin note: substantial text overlap with arXiv:1409.3193", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of non-commutative hypercomplex number systems (HNS) of\n4-dimension, constructed by using of non-commutative Grassmann-Clifford\nprocedure of doubling of 2-dimensional systems is investigated in the article\nand established here are their relationships with the generalized quaternions.\nAlgorithms of performance of operations and methods of algebraic\ncharacteristics calculation in them, such as conjugation, normalization, a type\nof zero divisors are investigated. The considered arithmetic and algebraic\noperations and procedures in this class HNS allow to use these HNS in\nmathematical modeling.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 16:44:30 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Kalinovsky", "Yakiv O.", ""], ["Boyarinova", "Yuliya E.", ""], ["Turenko", "Alina S.", ""], ["Khitsko", "Yana V.", ""]]}, {"id": "1412.8447", "submitter": "Sergey Voronin", "authors": "Sergey Voronin and Per-Gunnar Martinsson", "title": "Efficient Algorithms for CUR and Interpolative Matrix Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The manuscript describes efficient algorithms for the computation of the CUR\nand ID decompositions. The methods used are based on simple modifications to\nthe classical truncated pivoted QR decomposition, which means that highly\noptimized library codes can be utilized for implementation. For certain\napplications, further acceleration can be attained by incorporating techniques\nbased on randomized projections. Numerical experiments demonstrate advantageous\nperformance compared to existing techniques for computing CUR factorizations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 20:27:33 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 20:19:10 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 17:19:53 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Voronin", "Sergey", ""], ["Martinsson", "Per-Gunnar", ""]]}]