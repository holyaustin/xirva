[{"id": "1606.00251", "submitter": "Daniel Sorin", "authors": "Ralph Nathan, Helia Naeimi, Daniel J. Sorin, Xiaobai Sun", "title": "Profile-Driven Automated Mixed Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scheme to automatically set the precision of floating point\nvariables in an application. We design a framework that profiles applications\nto measure undesirable numerical behavior at the floating point operation\nlevel. We use this framework to perform mixed precision analysis to\nheuristically set the precision of all variables in an application based on\ntheir numerical profiles. We experimentally evaluate the mixed precision\nanalysis to show that it can generate a range of results with different\naccuracy and performance characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:27:54 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Nathan", "Ralph", ""], ["Naeimi", "Helia", ""], ["Sorin", "Daniel J.", ""], ["Sun", "Xiaobai", ""]]}, {"id": "1606.00602", "submitter": "Xiyu Yu PhD", "authors": "Xiyu Yu, Dacheng Tao", "title": "Variance-Reduced Proximal Stochastic Gradient Descent for Non-convex\n  Composite optimization", "comments": "This paper has been withdrawn by the author due to an error in the\n  proof of the convergence rate. They will modify this proof as soon as\n  possible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we study non-convex composite optimization: first, a finite-sum of\nsmooth but non-convex functions, and second, a general function that admits a\nsimple proximal mapping. Most research on stochastic methods for composite\noptimization assumes convexity or strong convexity of each function. In this\npaper, we extend this problem into the non-convex setting using variance\nreduction techniques, such as prox-SVRG and prox-SAGA. We prove that, with a\nconstant step size, both prox-SVRG and prox-SAGA are suitable for non-convex\ncomposite optimization, and help the problem converge to a stationary point\nwithin $O(1/\\epsilon)$ iterations. That is similar to the convergence rate seen\nwith the state-of-the-art RSAG method and faster than stochastic gradient\ndescent. Our analysis is also extended into the min-batch setting, which\nlinearly accelerates the convergence. To the best of our knowledge, this is the\nfirst analysis of convergence rate of variance-reduced proximal stochastic\ngradient for non-convex composite optimization.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 09:59:16 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 04:15:04 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Yu", "Xiyu", ""], ["Tao", "Dacheng", ""]]}, {"id": "1606.00785", "submitter": "Sebastian Ewert", "authors": "Sebastian Ewert, Mark Sandler", "title": "Piano Transcription in the Studio Using an Extensible Alternating\n  Directions Framework", "comments": "IEEE/ACM Transactions on Audio, Speech, and Language Processing", "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol. 24, no. 11, pp. 1983-1997, 2016", "doi": "10.1109/TASLP.2016.2593801", "report-no": null, "categories": "cs.SD cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a musical audio recording, the goal of automatic music transcription is\nto determine a score-like representation of the piece underlying the recording.\nDespite significant interest within the research community, several studies\nhave reported on a 'glass ceiling' effect, an apparent limit on the\ntranscription accuracy that current methods seem incapable of overcoming. In\nthis paper, we explore how much this effect can be mitigated by focusing on a\nspecific instrument class and making use of additional information on the\nrecording conditions available in studio or home recording scenarios. In\nparticular, exploiting the availability of single note recordings for the\ninstrument in use we develop a novel signal model employing variable-length\nspectro-temporal patterns as its central building blocks - tailored for pitched\npercussive instruments such as the piano. Temporal dependencies between\nspectral templates are modeled, resembling characteristics of factorial scaled\nhidden Markov models (FS-HMM) and other methods combining Non-Negative Matrix\nFactorization with Markov processes. In contrast to FS-HMMs, our parameter\nestimation is developed in a global, relaxed form within the extensible\nalternating direction method of multipliers (ADMM) framework, which enables the\nsystematic combination of basic regularizers propagating sparsity and local\nstationarity in note activity with more complex regularizers imposing temporal\nsemantics. The proposed method achieves an f-measure of 93-95% for note onsets\non pieces recorded on a Yamaha Disklavier (MAPS DB).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 18:15:34 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 11:43:29 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Ewert", "Sebastian", ""], ["Sandler", "Mark", ""]]}, {"id": "1606.00803", "submitter": "Guillaume Aupy", "authors": "Guillaume Aupy and JeongHyung Park and Padma Raghavan", "title": "Locality-Aware Laplacian Mesh Smoothing", "comments": "Accepted to ICPP'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel reordering scheme to improve the\nperformance of a Laplacian Mesh Smoothing (LMS). While the Laplacian smoothing\nalgorithm is well optimized and studied, we show how a simple reordering of the\nvertices of the mesh can greatly improve the execution time of the smoothing\nalgorithm. The idea of our reordering is based on (i) the postulate that cache\nmisses are a very time consuming part of the execution of LMS, and (ii) the\nstudy of the reuse distance patterns of various executions of the LMS\nalgorithm.\n  Our reordering algorithm is very simple but allows for huge performance\nimprovement. We ran it on a Westmere-EX platform and obtained a speedup of 75\non 32 cores compared to the single core execution without reordering, and a\ngain in execution of 32% on 32 cores compared to state of the art reordering.\nFinally, we show that we leave little room for a better ordering by reducing\nthe L2 and L3 cache misses to a bare minimum.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 18:58:53 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Aupy", "Guillaume", ""], ["Park", "JeongHyung", ""], ["Raghavan", "Padma", ""]]}, {"id": "1606.00807", "submitter": "Elias David Nino Ruiz", "authors": "Elias D. Nino, Adrian Sandu and Xinwei Deng", "title": "A Parallel Implementation of the Ensemble Kalman Filter Based on\n  Modified Cholesky Decomposition", "comments": "arXiv admin note: text overlap with arXiv:1605.08875", "journal-ref": null, "doi": null, "report-no": "CSTR-3/2016", "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses an efficient parallel implementation of the ensemble\nKalman filter based on the modified Cholesky decomposition. The proposed\nimplementation starts with decomposing the domain into sub-domains. In each\nsub-domain a sparse estimation of the inverse background error covariance\nmatrix is computed via a modified Cholesky decomposition; the estimates are\ncomputed concurrently on separate processors. The sparsity of this estimator is\ndictated by the conditional independence of model components for some radius of\ninfluence. Then, the assimilation step is carried out in parallel without the\nneed of inter-processor communication. Once the local analysis states are\ncomputed, the analysis sub-domains are mapped back onto the global domain to\nobtain the analysis ensemble. Computational experiments are performed using the\nAtmospheric General Circulation Model (SPEEDY) with the T-63 resolution on the\nBlueridge cluster at Virginia Tech. The number of processors used in the\nexperiments ranges from 96 to 2,048. The proposed implementation outperforms in\nterms of accuracy the well-known local ensemble transform Kalman filter (LETKF)\nfor all the model variables. The computational time of the proposed\nimplementation is similar to that of the parallel LETKF method (where no\ncovariance estimation is performed). Finally, for the largest number of\nprocessors, the proposed parallel implementation is 400 times faster than the\nserial version of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 14:33:06 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Nino", "Elias D.", ""], ["Sandu", "Adrian", ""], ["Deng", "Xinwei", ""]]}, {"id": "1606.01245", "submitter": "Fanhua Shang", "authors": "Fanhua Shang and Yuanyuan Liu and James Cheng", "title": "Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization", "comments": "16 pages, 5 figures, Appears in Proceedings of the 30th AAAI\n  Conference on Artificial Intelligence (AAAI), Phoenix, Arizona, USA, pp.\n  2016--2022, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Schatten-p quasi-norm $(0<p<1)$ is usually used to replace the standard\nnuclear norm in order to approximate the rank function more accurately.\nHowever, existing Schatten-p quasi-norm minimization algorithms involve\nsingular value decomposition (SVD) or eigenvalue decomposition (EVD) in each\niteration, and thus may become very slow and impractical for large-scale\nproblems. In this paper, we first define two tractable Schatten quasi-norms,\ni.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove\nthat they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively,\nwhich lead to the design of very efficient algorithms that only need to update\ntwo much smaller factor matrices. We also design two efficient proximal\nalternating linearized minimization algorithms for solving representative\nmatrix completion problems. Finally, we provide the global convergence and\nperformance guarantees for our algorithms, which have better convergence\nproperties than existing algorithms. Experimental results on synthetic and\nreal-world data show that our algorithms are more accurate than the\nstate-of-the-art methods, and are orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 03:28:41 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Cheng", "James", ""]]}, {"id": "1606.01289", "submitter": "Darren Engwirda", "authors": "Darren Engwirda", "title": "Conforming restricted Delaunay mesh generation for piecewise smooth\n  complexes", "comments": "To appear at the 25th International Meshing Roundtable", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CE cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Frontal-Delaunay refinement algorithm for mesh generation in piecewise\nsmooth domains is described. Built using a restricted Delaunay framework, this\nnew algorithm combines a number of novel features, including: (i) an\nunweighted, conforming restricted Delaunay representation for domains specified\nas a (non-manifold) collection of piecewise smooth surface patches and curve\nsegments, (ii) a protection strategy for domains containing curve segments that\nsubtend sharply acute angles, and (iii) a new class of off-centre refinement\nrules designed to achieve high-quality point-placement along embedded curve\nfeatures. Experimental comparisons show that the new Frontal-Delaunay algorithm\noutperforms a classical (statically weighted) restricted Delaunay-refinement\ntechnique for a number of three-dimensional benchmark problems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 22:06:58 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 18:06:16 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Engwirda", "Darren", ""]]}, {"id": "1606.01316", "submitter": "Anastasios Kyrillidis", "authors": "Dohyung Park, Anastasios Kyrillidis, Srinadh Bhojanapalli, Constantine\n  Caramanis, Sujay Sanghavi", "title": "Provable Burer-Monteiro factorization for a class of norm-constrained\n  matrix problems", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the projected gradient descent method on low-rank matrix problems\nwith a strongly convex objective. We use the Burer-Monteiro factorization\napproach to implicitly enforce low-rankness; such factorization introduces\nnon-convexity in the objective. We focus on constraint sets that include both\npositive semi-definite (PSD) constraints and specific matrix norm-constraints.\nSuch criteria appear in quantum state tomography and phase retrieval\napplications.\n  We show that non-convex projected gradient descent favors local linear\nconvergence in the factored space. We build our theory on a novel descent\nlemma, that non-trivially extends recent results on the unconstrained problem.\nThe resulting algorithm is Projected Factored Gradient Descent, abbreviated as\nProjFGD, and shows superior performance compared to state of the art on quantum\nstate tomography and sparse phase retrieval applications.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 02:12:13 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 02:35:04 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2016 22:47:53 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Park", "Dohyung", ""], ["Kyrillidis", "Anastasios", ""], ["Bhojanapalli", "Srinadh", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1606.01396", "submitter": "Victor Pan", "authors": "Remi Imbach, Victor Y. Pan, Chee Yap, Ilias S. Kotsireas and Vitaly\n  Zaderman", "title": "Root-finding with Implicit Deflation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional iterations such as Newton's are a popular tool for polynomial\nroot-finding. We consider realistic situation where some (e.g.,\nbetter-conditioned) roots have already been approximated and where further\ncomputations is directed to approximation of the remaining roots. Such\nsituation is also realistic for root-finding by means of subdivision. A natural\napproach of applying explicit deflation has been much studied and recently\nadvanced by one of the authors of this paper, but presently we contribute to\nthe alternative approach of applying implicit deflation, which we combine with\nmapping the variable and reversion of an input polynomial.\n  We also show another unexplored direction for substantial further progress in\nthis long and extensively studied area. Namely we dramatically increase their\nlocal efficiency by means of the incorporation of fast algorithms for\nmultipoint polynomial evaluation and Fast Multipole Method.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 16:58:37 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 15:50:24 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 00:01:47 GMT"}, {"version": "v4", "created": "Wed, 30 May 2018 15:40:01 GMT"}, {"version": "v5", "created": "Mon, 8 Apr 2019 15:03:31 GMT"}, {"version": "v6", "created": "Tue, 21 May 2019 19:29:43 GMT"}, {"version": "v7", "created": "Wed, 29 May 2019 02:47:54 GMT"}, {"version": "v8", "created": "Sat, 6 Jul 2019 12:05:14 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Imbach", "Remi", ""], ["Pan", "Victor Y.", ""], ["Yap", "Chee", ""], ["Kotsireas", "Ilias S.", ""], ["Zaderman", "Vitaly", ""]]}, {"id": "1606.01500", "submitter": "Johann Bengua", "authors": "Johann A. Bengua, Ho N. Phien, Hoang D. Tuan and Minh N. Do", "title": "Efficient tensor completion for color image and video recovery: Low-rank\n  tensor train", "comments": "Submitted to the IEEE Transactions on Image Processing. arXiv admin\n  note: substantial text overlap with arXiv:1601.01083", "journal-ref": null, "doi": "10.1109/TIP.2017.2672439", "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to tensor completion, which recovers\nmissing entries of data represented by tensors. The approach is based on the\ntensor train (TT) rank, which is able to capture hidden information from\ntensors thanks to its definition from a well-balanced matricization scheme.\nAccordingly, new optimization formulations for tensor completion are proposed\nas well as two new algorithms for their solution. The first one called simple\nlow-rank tensor completion via tensor train (SiLRTC-TT) is intimately related\nto minimizing a nuclear norm based on TT rank. The second one is from a\nmultilinear matrix factorization model to approximate the TT rank of a tensor,\nand is called tensor completion by parallel matrix factorization via tensor\ntrain (TMac-TT). A tensor augmentation scheme of transforming a low-order\ntensor to higher-orders is also proposed to enhance the effectiveness of\nSiLRTC-TT and TMac-TT. Simulation results for color image and video recovery\nshow the clear advantage of our method over all other methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 12:09:19 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Bengua", "Johann A.", ""], ["Phien", "Ho N.", ""], ["Tuan", "Hoang D.", ""], ["Do", "Minh N.", ""]]}, {"id": "1606.01753", "submitter": "Zvi Kedem", "authors": "Zvi M. Kedem and Kirthi Krishna Muntimadugu", "title": "Mathematical Modeling of General Inaccurate Adders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inaccurate circuits make possible the conservation of limited resources, such\nas energy. But effective design of such circuits requires an understanding of\nresulting tradeoffs between accuracy and design parameters, such as voltages\nand speed of execution. Although studies of tradeoffs have been done on\nspecific circuits, the applicability of those studies is narrow. This paper\npresents a comprehensive and mathematically rigorous method for analyzing a\nlarge class of inaccurate circuits for addition. Furthermore, it presents new,\nfast algorithms for the computation of key statistical measures of inaccuracy\nin such adders, thus helping hardware architects explore the design space with\ngreater confidence.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 14:09:30 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Kedem", "Zvi M.", ""], ["Muntimadugu", "Kirthi Krishna", ""]]}, {"id": "1606.01889", "submitter": "Richard Kleeman", "authors": "Richard Kleeman", "title": "Multitimescale method for approximating the path action relevant to\n  non-equilibrium statistical physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A path integral formalism has been proposed recently for non-equilibrium\nstatistical physics applications by the author. In this contribution we outline\nan efficient method for its numerical evaluation. The method used is based on\nthe multiscale MCMC method of Ceperley and co-workers in quantum applications.\nA significant new feature of the method proposed is that the time endpoint is\nnot fixed and indeed the endpoint sample is the principle object of interest.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 15:25:02 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Kleeman", "Richard", ""]]}, {"id": "1606.02424", "submitter": "Yassine Hachaichi", "authors": "Imen Ben Saad, Younes Lahbib, Yassine Hacha\\\"ichi (LAMSIN), Sonia\n  Mami, Abdelkader Mami", "title": "Generic-Precision algorithm for DCT-Cordic architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DM cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a generic algorithm to calculate the rotation\nparameters of CORDIC angles required for the Discrete Cosine Transform\nalgorithm (DCT). This leads us to increase the precision of calculation meeting\nany accuracy.Our contribution is to use this decomposition in CORDIC based DCT\nwhich is appropriate for domains which require high quality and top precision.\nWe then propose a hardware implementation of the novel transformation, and as\nexpected, a substantial improvement in PSNR quality is found.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 07:08:10 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Saad", "Imen Ben", "", "LAMSIN"], ["Lahbib", "Younes", "", "LAMSIN"], ["Hacha\u00efchi", "Yassine", "", "LAMSIN"], ["Mami", "Sonia", ""], ["Mami", "Abdelkader", ""]]}, {"id": "1606.02433", "submitter": "Huiming Chen Mr", "authors": "Huiming Chen", "title": "Training Design and Two-stage Channel Estimation for Correlated Two-way\n  MIMO Relay Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the training signal design for the channel estimation in\ntwo-way multiple-input-and-multipleoutput (MIMO) relay systems, where the\nchannels are correlated. We first derive the backward channel estimator with\nthe optimal training signal sent by the relay node. Given the estimated\nbackward channels and the probabilistic knowledge of the estimation error, we\nmainly focus on the forward channel estimation and the related training signal\ndesign. We further propose a novel training signal. The design criterion is to\nminimize the relaxation of the total mean square error (MSE) of the forward\nchannel estimators, which is conditioned on the estimated backward channels.\nFinally, the numerical results show that the proposed training signal can\nimprove the MSE performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 07:47:14 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Chen", "Huiming", ""]]}, {"id": "1606.02468", "submitter": "Yassine Hachaichi", "authors": "Yassine Hacha\\\"ichi (LAMSIN), Younes Lahbib", "title": "An efficient mathematically correct scale free CORDIC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to approximate transandental functions, several algorithms were\nproposed.Historically, polynomial interpolation, infinite series, $\\cdots$ and\nother$+,\\times, -$ and $/$ based algorithms were studied for this purpose.The\nCORDIC (COordinate Rotation DIgital Computer)introduced by Jack E. Volder in\n1959, and generalized by J. S. Walther a few years later, is a hardware based\nalgorithmfor the approximation of trigonometric, hyperbolic andlogarithmic\nfunctions.As a consequence, CORDIC is used for applications indiverse areas\nsuch as signal and image processing.For these reasons, several modified\nversions were proposed.In this article, we present anoverview of the CORDIC\nalgorithm for the computation of the circular functions, essentially the\nscaling free version,and we will give a substential improvement to the commonly\nused one.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 09:36:11 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Hacha\u00efchi", "Yassine", "", "LAMSIN"], ["Lahbib", "Younes", ""]]}, {"id": "1606.02959", "submitter": "Gang Xu", "authors": "Gang Xu, Tsz-Ho Kwok, Charlie C.L. Wang", "title": "Isogeometric computation reuse method for complex objects with\n  topology-consistent volumetric parameterization", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric spline parameterization and computational efficiency are two main\nchallenges in isogeometric analysis (IGA). To tackle this problem, we propose a\nframework of computation reuse in IGA on a set of three-dimensional models with\nsimilar semantic features. Given a template domain, B-spline based consistent\nvolumetric parameterization is first constructed for a set of models with\nsimilar semantic features. An efficient quadrature-free method is investigated\nin our framework to compute the entries of stiffness matrix by Bezier\nextraction and polynomial approximation. In our approach, evaluation on the\nstiffness matrix and imposition of the boundary conditions can be pre-computed\nand reused during IGA on a set of CAD models. Examples with complex geometry\nare presented to show the effectiveness of our methods, and efficiency similar\nto the computation in linear finite element analysis can be achieved for IGA\ntaken on a set of models.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 13:27:47 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 00:46:08 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2016 22:08:43 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Xu", "Gang", ""], ["Kwok", "Tsz-Ho", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "1606.03032", "submitter": "Gino Montecinos", "authors": "Gino I. Montecinos", "title": "A strategy to implement Dirichlet boundary conditions in the context of\n  ADER finite volume schemes. One-dimensional conservation laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ADER schemes are numerical methods, which can reach an arbitrary order of\naccuracy in both space and time. They are based on a reconstruction procedure\nand the solution of generalized Riemann problems. However, for general boundary\nconditions, in particular of Dirichlet type, a lack of accuracy might occur if\na suitable treatment of boundaries conditions is not properly carried out. In\nthis work the treatment of Dirichlet boundary conditions for conservation laws\nin the context of ADER schemes, is concerned. The solution of generalized\nRiemann problems at the extremes of the computational domain, provides the\ncorrect influence of boundaries. The reconstruction procedure, for data near to\nthe boundaries, demands for information outside the computational domain, which\nis carried out in terms of ghost cells, which are provided by using the\nnumerical solution of auxiliary problems. These auxiliary problems are\nhyperbolic and they are constructed from the conservation laws and the\ninformation at boundaries, which may be partially or totally known in terms of\nprescribed functions. The evolution of these problems, unlike to the usual\nmanner, is done in space rather than in time due to that these problems are\nnamed here, {\\it reverse problems}. The methodology can be considered as a\nnumerical counterpart of the inverse Lax-Wendroff procedure for filling ghost\ncells. However, the use of Taylor series expansions, as well as, Lax-Wendroff\nprocedure, are avoided. For the scalar case is shown that the present procedure\npreserve the accuracy of the scheme which is reinforced with some numerical\nresults. Expected orders of accuracy for solving conservation laws by using the\nproposed strategy at boundaries, are obtained up to fifth-order in both space\nand time.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 17:39:43 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Montecinos", "Gino I.", ""]]}, {"id": "1606.03135", "submitter": "Varun Shankar", "authors": "Varun Shankar", "title": "The Overlapped Radial Basis Function-Finite Difference (RBF-FD) Method:\n  A Generalization of RBF-FD", "comments": "11 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2017.04.037", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of the RBF-FD method that computes RBF-FD weights\nin finite-sized neighborhoods around the centers of RBF-FD stencils by\nintroducing an overlap parameter $\\delta \\in [0,1]$ such that $\\delta=1$\nrecovers the standard RBF-FD method and $\\delta=0$ results in a full decoupling\nof stencils. We provide experimental evidence to support this generalization,\nand develop an automatic stabilization procedure based on local Lebesgue\nfunctions for the stable selection of stencil weights over a wide range of\n$\\delta$ values. We provide an a priori estimate for the speedup of our method\nover RBF-FD that serves as a good predictor for the true speedup. We apply our\nmethod to parabolic partial differential equations with time-dependent\ninhomogeneous boundary conditions-- Neumann in 2D, and Dirichlet in 3D. Our\nresults show that our method can achieve as high as a 60x speedup in 3D over\nexisting RBF-FD methods in the task of forming differentiation matrices.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 22:20:50 GMT"}, {"version": "v2", "created": "Sun, 25 Dec 2016 22:48:33 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2017 20:41:11 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Shankar", "Varun", ""]]}, {"id": "1606.03168", "submitter": "Anastasios Kyrillidis", "authors": "Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, Sujay\n  Sanghavi", "title": "Finding Low-Rank Solutions via Non-Convex Matrix Factorization,\n  Efficiently and Provably", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.IT cs.LG cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rank-$r$ matrix $X \\in \\mathbb{R}^{m \\times n}$ can be written as a product\n$U V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n\n\\times r}$. One could exploit this observation in optimization: e.g., consider\nthe minimization of a convex function $f(X)$ over rank-$r$ matrices, where the\nset of rank-$r$ matrices is modeled via the factorization $UV^\\top$. Though\nsuch parameterization reduces the number of variables, and is more\ncomputationally efficient (of particular interest is the case $r \\ll \\min\\{m,\nn\\}$), it comes at a cost: $f(UV^\\top)$ becomes a non-convex function w.r.t.\n$U$ and $V$.\n  We study such parameterization for optimization of generic convex objectives\n$f$, and focus on first-order, gradient descent algorithmic solutions. We\npropose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient\nfirst-order method that operates on the $U, V$ factors. We show that when $f$\nis (restricted) smooth, BFGD has local sublinear convergence, and linear\nconvergence when $f$ is both (restricted) smooth and (restricted) strongly\nconvex. For several key applications, we provide simple and efficient\ninitialization schemes that provide approximate solutions good enough for the\nabove convergence results to hold.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 03:18:01 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 20:55:56 GMT"}, {"version": "v3", "created": "Sat, 29 Oct 2016 21:03:47 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Park", "Dohyung", ""], ["Kyrillidis", "Anastasios", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1606.03918", "submitter": "Takuya Tsuchiya", "authors": "Kenta Kobayashi and Takuya Tsuchiya", "title": "Error Analysis of Lagrange Interpolation on Tetrahedrons", "comments": "To appear in Journal of Approximation Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the analysis of Lagrange interpolation errors on\ntetrahedrons. In many textbooks, the error analysis of Lagrange interpolation\nis conducted under geometric assumptions such as shape regularity or the\n(generalized) maximum angle condition. In this paper, we present a new\nestimation in which the error is bounded in terms of the diameter and projected\ncircumradius of the tetrahedron. Because we do not impose any geometric\nrestrictions on the tetrahedron itself, our error estimation may be applied to\nany tetrahedralizations of domains including very thin tetrahedrons.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 12:37:20 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 02:54:19 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Kobayashi", "Kenta", ""], ["Tsuchiya", "Takuya", ""]]}, {"id": "1606.04022", "submitter": "Huiming Chen Mr", "authors": "Huiming Chen, Xiaohan Zhong", "title": "Joint Channel Estimation and Training Signal Design for Two-way MIMO\n  Relay Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:1606.02433", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a two-stage channel estimation scheme for two-way MIMO relay\nsystems with a single relay antenna is proposed. The backward channel is\nestimated by using linear minimum mean square estimator (LMMSE) at the first\nstage, where the optimal training signal is designed. We then mainly focus on\nthe forward channel estimation by using singular value decomposition (SVD)\nbased maximum likelihood method, and the related training signal is proposed.\nWe note that the forward channel estimator is nonlinear and by analyzing the\nasymptotic Bayesian Cramer-rao Lower Bound (BCRLB), we seek BCRLB as the\ncriterion for training signal design. Finally, the numerical results show that\nthe proposed training signal can improve the MSE performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 16:31:04 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Chen", "Huiming", ""], ["Zhong", "Xiaohan", ""]]}, {"id": "1606.04970", "submitter": "Nicolas Boumal", "authors": "Nicolas Boumal, Vladislav Voroninski and Afonso S. Bandeira", "title": "The non-convex Burer-Monteiro approach works on smooth semidefinite\n  programs", "comments": "19 pages, in the proceedings of NIPS 2016", "journal-ref": "In proceedings of NIPS 2016:\n  https://papers.nips.cc/paper/6517-the-non-convex-burer-monteiro-approach-works-on-smooth-semidefinite-programs", "doi": null, "report-no": null, "categories": "math.OC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semidefinite programs (SDPs) can be solved in polynomial time by interior\npoint methods, but scalability can be an issue. To address this shortcoming,\nover a decade ago, Burer and Monteiro proposed to solve SDPs with few equality\nconstraints via rank-restricted, non-convex surrogates. Remarkably, for some\napplications, local optimization methods seem to converge to global optima of\nthese non-convex surrogates reliably. Although some theory supports this\nempirical success, a complete explanation of it remains an open question. In\nthis paper, we consider a class of SDPs which includes applications such as\nmax-cut, community detection in the stochastic block model, robust PCA, phase\nretrieval and synchronization of rotations. We show that the low-rank\nBurer--Monteiro formulation of SDPs in that class almost never has any spurious\nlocal optima.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 20:31:29 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 21:54:03 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 20:29:02 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Boumal", "Nicolas", ""], ["Voroninski", "Vladislav", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "1606.05535", "submitter": "Qibin Zhao Dr", "authors": "Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej\n  Cichocki", "title": "Tensor Ring Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor networks have in recent years emerged as the powerful tools for\nsolving the large-scale optimization problems. One of the most popular tensor\nnetwork is tensor train (TT) decomposition that acts as the building blocks for\nthe complicated tensor networks. However, the TT decomposition highly depends\non permutations of tensor dimensions, due to its strictly sequential\nmultilinear products over latent cores, which leads to difficulties in finding\nthe optimal TT representation. In this paper, we introduce a fundamental tensor\ndecomposition model to represent a large dimensional tensor by a circular\nmultilinear products over a sequence of low dimensional cores, which can be\ngraphically interpreted as a cyclic interconnection of 3rd-order tensors, and\nthus termed as tensor ring (TR) decomposition. The key advantage of TR model is\nthe circular dimensional permutation invariance which is gained by employing\nthe trace operation and treating the latent cores equivalently. TR model can be\nviewed as a linear combination of TT decompositions, thus obtaining the\npowerful and generalized representation abilities. For optimization of latent\ncores, we present four different algorithms based on the sequential SVDs, ALS\nscheme, and block-wise ALS techniques. Furthermore, the mathematical properties\nof TR model are investigated, which shows that the basic multilinear algebra\ncan be performed efficiently by using TR representaions and the classical\ntensor decompositions can be conveniently transformed into the TR\nrepresentation. Finally, the experiments on both synthetic signals and\nreal-world datasets were conducted to evaluate the performance of different\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 14:40:18 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Zhao", "Qibin", ""], ["Zhou", "Guoxu", ""], ["Xie", "Shengli", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1606.05556", "submitter": "Alexandros Syrakos", "authors": "Alexandros Syrakos, Stylianos Varchanis, Yannis Dimakopoulos,\n  Apostolos Goulas, John Tsamopoulos", "title": "A critical analysis of some popular methods for the discretisation of\n  the gradient operator in finite volume methods", "comments": "Minor corrections compared to the previous version", "journal-ref": "Physics of Fluids 29, 127103 (2017)", "doi": "10.1063/1.4997682", "report-no": null, "categories": "cs.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite volume methods (FVMs) constitute a popular class of methods for the\nnumerical simulation of fluid flows. Among the various components of these\nmethods, the discretisation of the gradient operator has received less\nattention despite its fundamental importance with regards to the accuracy of\nthe FVM. The most popular gradient schemes are the divergence theorem (DT) (or\nGreen-Gauss) scheme, and the least-squares (LS) scheme. Both are widely\nbelieved to be second-order accurate, but the present study shows that in fact\nthe common variant of the DT gradient is second-order accurate only on\nstructured meshes whereas it is zeroth-order accurate on general unstructured\nmeshes, and the LS gradient is second-order and first-order accurate,\nrespectively. This is explained through a theoretical analysis and is confirmed\nby numerical tests. The schemes are then used within a FVM to solve a simple\ndiffusion equation on unstructured grids generated by several methods; the\nresults reveal that the zeroth-order accuracy of the DT gradient is inherited\nby the FVM as a whole, and the discretisation error does not decrease with grid\nrefinement. On the other hand, use of the LS gradient leads to second-order\naccurate results, as does the use of alternative, consistent, DT gradient\nschemes, including a new iterative scheme that makes the common DT gradient\nconsistent at almost no extra cost. The numerical tests are performed using\nboth an in-house code and the popular public domain PDE solver OpenFOAM.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 15:18:08 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 15:29:54 GMT"}, {"version": "v3", "created": "Sat, 3 Jun 2017 21:53:08 GMT"}, {"version": "v4", "created": "Mon, 19 Jun 2017 19:59:48 GMT"}, {"version": "v5", "created": "Tue, 25 Jul 2017 10:44:42 GMT"}, {"version": "v6", "created": "Fri, 29 Dec 2017 19:59:15 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Syrakos", "Alexandros", ""], ["Varchanis", "Stylianos", ""], ["Dimakopoulos", "Yannis", ""], ["Goulas", "Apostolos", ""], ["Tsamopoulos", "John", ""]]}, {"id": "1606.05562", "submitter": "Renato J Cintra", "authors": "T. L. T. da Silveira, F. M. Bayer, R. J. Cintra, S. Kulasekera, A.\n  Madanayake, A. J. Kozakevicius", "title": "An Orthogonal 16-point Approximate DCT for Image and Video Compression", "comments": "18 pages, 7 figures, 6 tables", "journal-ref": "Multidimensional Systems and Signal Processing, vol. 27, no. 1,\n  pp. 87-104, 2016", "doi": "10.1007/s11045-014-0291-6", "report-no": null, "categories": "cs.IT cs.AR cs.NA math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-complexity orthogonal multiplierless approximation for the 16-point\ndiscrete cosine transform (DCT) was introduced. The proposed method was\ndesigned to possess a very low computational cost. A fast algorithm based on\nmatrix factorization was proposed requiring only 60~additions. The proposed\narchitecture outperforms classical and state-of-the-art algorithms when\nassessed as a tool for image and video compression. Digital VLSI hardware\nimplementations were also proposed being physically realized in FPGA technology\nand implemented in 45 nm up to synthesis and place-route levels. Additionally,\nthe proposed method was embedded into a high efficiency video coding (HEVC)\nreference software for actual proof-of-concept. Obtained results show\nnegligible video degradation when compared to Chen DCT algorithm in HEVC.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 01:19:46 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["da Silveira", "T. L. T.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""], ["Kozakevicius", "A. J.", ""]]}, {"id": "1606.06035", "submitter": "Harald van Brummelen", "authors": "E.H. van Brummelen and C.H. Venner", "title": "Multilevel Evaluation of Multidimensional Integral Transforms with\n  Asymptotically Smooth Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical applications of numerical methods a substantial increase in\nefficiency can be obtained by using local grid refinement, since the solution\nis generally smooth in large parts of the domain and large gradients occur only\nlocally. Fast evaluation of integral transforms on such an adaptive grid\nrequires an algorithm that relies on the smoothness of the continuum kernel\nonly, independent of its discrete form. A multilevel algorithm with this\nproperty was presented in [A. Brandt and C.H. Venner, SIAM J. Sci. Stat.\nComput. 19 (1998) pp.468-492] [Bra1998]. Ref. [Bra1998] shows that already on a\nuniform grid the new algorithm is more efficient than earlier fast evaluation\nalgorithms, and elaborates the application to one-dimensional transforms. The\npresent work analyses the extension and implementation of the algorithm for\nmultidimensional transforms. The analysis conveys that the multidimensional\nextension is nontrivial, on account of the occurence of nonlocal corrections.\nHowever, by virtue of the asymptotic smoothness properties of the continuum\nkernel, these corrections can again be evaluated fast. By recursion, it is then\npossible to obtain the optimal work estimates indicated in [Bra1998].\nCurrently, only uniform grids are considered. Detailed numerical results will\nbe presented for a two dimensional model problem. The results demonstrate that\nwith the new algorithm the evaluation of multidimensional transforms is also\nmore efficient than with previous algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 09:41:46 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["van Brummelen", "E. H.", ""], ["Venner", "C. H.", ""]]}, {"id": "1606.06511", "submitter": "Kishore Kumar Naraparaju", "authors": "N. Kishore Kumar, Jan Shneider", "title": "Literature survey on low rank approximation of matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank approximation of matrices has been well studied in literature.\nSingular value decomposition, QR decomposition with column pivoting, rank\nrevealing QR factorization (RRQR), Interpolative decomposition etc are\nclassical deterministic algorithms for low rank approximation. But these\ntechniques are very expensive $(O(n^{3})$ operations are required for $n\\times\nn$ matrices). There are several randomized algorithms available in the\nliterature which are not so expensive as the classical techniques (but the\ncomplexity is not linear in n). So, it is very expensive to construct the low\nrank approximation of a matrix if the dimension of the matrix is very large.\nThere are alternative techniques like Cross/Skeleton approximation which gives\nthe low-rank approximation with linear complexity in n . In this article we\nreview low rank approximation techniques briefly and give extensive references\nof many techniques.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 11:02:01 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Kumar", "N. Kishore", ""], ["Shneider", "Jan", ""]]}, {"id": "1606.06604", "submitter": "Dmitry Kulyabov PhD", "authors": "M. N. Gevorkyan, T. R. Velieva, A. V. Korolkova, D. S. Kulyabov, L. A.\n  Sevastyanov", "title": "Stochastic Runge-Kutta Software Package for Stochastic Differential\n  Equations", "comments": "in English, in Russian. M.N. Gevorkyan, T.R. Velieva, A.V. Korolkova,\n  D.S. Kulyabov, L.A. Sevastyanov, Stochastic Runge-Kutta Software Package for\n  Stochastic Differential Equations, in Dependability Engineering and Complex\n  Systems, Vol. 470, 2016, pp. 169-179", "journal-ref": null, "doi": "10.1007/978-3-319-39639-2_15", "report-no": null, "categories": "physics.comp-ph cs.MS cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As a result of the application of a technique of multistep processes\nstochastic models construction the range of models, implemented as a\nself-consistent differential equations, was obtained. These are partial\ndifferential equations (master equation, the Fokker--Planck equation) and\nstochastic differential equations (Langevin equation). However, analytical\nmethods do not always allow to research these equations adequately. It is\nproposed to use the combined analytical and numerical approach studying these\nequations. For this purpose the numerical part is realized within the framework\nof symbolic computation. It is recommended to apply stochastic Runge--Kutta\nmethods for numerical study of stochastic differential equations in the form of\nthe Langevin. Under this approach, a program complex on the basis of analytical\ncalculations metasystem Sage is developed. For model verification logarithmic\nwalks and Black--Scholes two-dimensional model are used. To illustrate the\nstochastic \"predator--prey\" type model is used. The utility of the combined\nnumerical-analytical approach is demonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 14:51:11 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Gevorkyan", "M. N.", ""], ["Velieva", "T. R.", ""], ["Korolkova", "A. V.", ""], ["Kulyabov", "D. S.", ""], ["Sevastyanov", "L. A.", ""]]}, {"id": "1606.06977", "submitter": "Fredrik Johansson", "authors": "Fredrik Johansson", "title": "Computing hypergeometric functions rigorously", "comments": "v2: corrected example in section 3.1; corrected timing data for case\n  E-G in section 8.5 (table 6, figure 2); adjusted paper size", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient implementation of hypergeometric functions in\narbitrary-precision interval arithmetic. The functions ${}_0F_1$, ${}_1F_1$,\n${}_2F_1$ and ${}_2F_0$ (or the Kummer $U$-function) are supported for\nunrestricted complex parameters and argument, and by extension, we cover\nexponential and trigonometric integrals, error functions, Fresnel integrals,\nincomplete gamma and beta functions, Bessel functions, Airy functions, Legendre\nfunctions, Jacobi polynomials, complete elliptic integrals, and other special\nfunctions. The output can be used directly for interval computations or to\ngenerate provably correct floating-point approximations in any format.\nPerformance is competitive with earlier arbitrary-precision software, and\nsometimes orders of magnitude faster. We also partially cover the generalized\nhypergeometric function ${}_pF_q$ and computation of high-order parameter\nderivatives.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 15:07:11 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 11:58:14 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Johansson", "Fredrik", ""]]}, {"id": "1606.07315", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Kartik Gupta, Prateek Jain", "title": "Nearly-optimal Robust Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of Robust Matrix Completion (RMC)\nwhere the goal is to recover a low-rank matrix by observing a small number of\nits entries out of which a few can be arbitrarily corrupted. We propose a\nsimple projected gradient descent method to estimate the low-rank matrix that\nalternately performs a projected gradient descent step and cleans up a few of\nthe corrupted entries using hard-thresholding. Our algorithm solves RMC using\nnearly optimal number of observations as well as nearly optimal number of\ncorruptions. Our result also implies significant improvement over the existing\ntime complexity bounds for the low-rank matrix completion problem. Finally, an\napplication of our result to the robust PCA problem (low-rank+sparse matrix\nseparation) leads to nearly linear time (in matrix dimensions) algorithm for\nthe same; existing state-of-the-art methods require quadratic time. Our\nempirical results corroborate our theoretical results and show that even for\nmoderate sized problems, our method for robust PCA is an an order of magnitude\nfaster than the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 13:57:56 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 20:05:13 GMT"}, {"version": "v3", "created": "Thu, 8 Dec 2016 19:48:40 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Gupta", "Kartik", ""], ["Jain", "Prateek", ""]]}, {"id": "1606.07407", "submitter": "Bosu Choi", "authors": "Bosu Choi, Andrew Christlieb, and Yang Wang", "title": "High-Dimensional Sparse Fourier Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the development of a sublinear sparse Fourier\nalgorithm for high-dimensional data. In ``Adaptive Sublinear Time Fourier\nAlgorithm\" by D. Lawlor, Y. Wang and A. Christlieb (2013), an efficient\nalgorithm with $\\Theta(k\\log k)$ average-case runtime and $\\Theta(k)$\naverage-case sampling complexity for the one-dimensional sparse FFT was\ndeveloped for signals of bandwidth $N$, where $k$ is the number of significant\nmodes such that $k\\ll N$.\n  In this work we develop an efficient algorithm for sparse FFT for higher\ndimensional signals, extending some of the ideas in the paper mentioned above.\nNote a higher dimensional signal can always be unwrapped into a one dimensional\nsignal, but when the dimension gets large, unwrapping a higher dimensional\nsignal into a one dimensional array is far too expensive to be realistic. Our\napproach here introduces two new concepts: ``partial unwrapping'' and\n``tilting''. These two ideas allow us to efficiently compute the sparse FFT of\nhigher dimensional signals.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 18:56:06 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 14:20:53 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 22:14:06 GMT"}, {"version": "v4", "created": "Mon, 1 Jul 2019 02:30:19 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Choi", "Bosu", ""], ["Christlieb", "Andrew", ""], ["Wang", "Yang", ""]]}, {"id": "1606.07414", "submitter": "Renato J Cintra", "authors": "T. L. T. Silveira, R. S. Oliveira, F. M. Bayer, R. J. Cintra, A.\n  Madanayake", "title": "Multiplierless 16-point DCT Approximation for Low-complexity Image and\n  Video Coding", "comments": "12 pages, 5 figures, 3 tables", "journal-ref": null, "doi": "10.1007/s11760-016-0923-4", "report-no": null, "categories": "cs.CV cs.MM cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An orthogonal 16-point approximate discrete cosine transform (DCT) is\nintroduced. The proposed transform requires neither multiplications nor\nbit-shifting operations. A fast algorithm based on matrix factorization is\nintroduced, requiring only 44 additions---the lowest arithmetic cost in\nliterature. To assess the introduced transform, computational complexity,\nsimilarity with the exact DCT, and coding performance measures are computed.\nClassical and state-of-the-art 16-point low-complexity transforms were used in\na comparative analysis. In the context of image compression, the proposed\napproximation was evaluated via PSNR and SSIM measurements, attaining the best\ncost-benefit ratio among the competitors. For video encoding, the proposed\napproximation was embedded into a HEVC reference software for direct comparison\nwith the original HEVC standard. Physically realized and tested using FPGA\nhardware, the proposed transform showed 35% and 37% improvements of area-time\nand area-time-squared VLSI metrics when compared to the best competing\ntransform in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 19:26:01 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Silveira", "T. L. T.", ""], ["Oliveira", "R. S.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Madanayake", "A.", ""]]}, {"id": "1606.07643", "submitter": "Sebastian Sch\\\"ops", "authors": "Ulrich R\\\"omer, Sebastian Sch\\\"ops, Thomas Weiland", "title": "Stochastic Modeling and Regularity of the Nonlinear Elliptic Curl-Curl\n  Equation", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification 2016 4:1, 952-979", "doi": "10.1137/15M1026535", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the nonlinear elliptic curl-curl equation with\nuncertainties in the material law. It is frequently employed in the numerical\nevaluation of magnetostatic fields, where the uncertainty is ascribed to the\nso-called B-H curve. A truncated Karhunen-Lo\\`eve approximation of the\nstochastic B-H curve is presented and analyzed with regard to monotonicity\nconstraints. A stochastic non-linear curl-curl formulation is introduced and\nnumerically approximated by a finite element and collocation method in the\ndeterministic and stochastic variable, respectively. The stochastic regularity\nis analyzed by a higher order sensitivity analysis. It is shown that, unlike to\nlinear and several nonlinear elliptic problems, the solution is not analytic\nwith respect to the random variables and an algebraic decay of the stochastic\nerror is obtained. Numerical results for both the Karhunen-Lo\\`eve expansion\nand the stochastic curl-curl equation are given for illustration.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 11:24:43 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["R\u00f6mer", "Ulrich", ""], ["Sch\u00f6ps", "Sebastian", ""], ["Weiland", "Thomas", ""]]}, {"id": "1606.08381", "submitter": "Murat Uzunca", "authors": "Sinem Kozp{\\i}nar, Murat Uzunca, B\\\"ulent Karas\\\"ozen", "title": "Pricing European and American Options under Heston Model using\n  Discontinuous Galerkin Finite Elements", "comments": null, "journal-ref": "Mathematics and Computers in Simulation, 177, 568-587 (2020)", "doi": "10.1016/j.matcom.2020.05.022", "report-no": null, "categories": "q-fin.CP cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with pricing of European and American options, when the\nunderlying asset price follows Heston model, via the interior penalty\ndiscontinuous Galerkin finite element method (dGFEM). The advantages of dGFEM\nspace discretization with Rannacher smoothing as time integrator with nonsmooth\ninitial and boundary conditions are illustrated for European vanilla options,\ndigital call and American put options. The convection dominated Heston model\nfor vanishing volatility is efficiently solved utilizing the adaptive dGFEM.\nFor fast solution of the linear complementary problem of the American options,\na projected successive over relaxation (PSOR) method is developed with the norm\npreconditioned dGFEM. We show the efficiency and accuracy of dGFEM for option\npricing by conducting comparison analysis with other methods and numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 01:00:28 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 19:51:35 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 11:41:18 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Kozp\u0131nar", "Sinem", ""], ["Uzunca", "Murat", ""], ["Karas\u00f6zen", "B\u00fclent", ""]]}, {"id": "1606.08743", "submitter": "Jesus Bonilla", "authors": "Santiago Badia, Jes\\'us Bonilla", "title": "Monotonicity-preserving finite element schemes based on differentiable\n  nonlinear stabilization", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2016.09.035", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a nonlinear stabilization technique for scalar\nconservation laws with implicit time stepping. The method relies on an\nartificial diffusion method, based on a graph-Laplacian operator. It is\nnonlinear, since it depends on a shock detector. The same shock detector is\nused to gradually lump the mass matrix. The resulting method is LED, positivity\npreserving, linearity preserving, and also satisfies a global DMP. Lipschitz\ncontinuity has also been proved. However, the resulting scheme is highly\nnonlinear, leading to very poor nonlinear convergence rates. We propose a\nsmooth version of the scheme, which leads to twice differentiable nonlinear\nstabilization schemes. It allows one to straightforwardly use Newton's method\nand obtain quadratic convergence. In the numerical experiments, steady and\ntransient linear transport, and transient Burgers' equation have been\nconsidered in 2D. Using the Newton method with a smooth version of the scheme\nwe can reduce 10 to 20 times the number of iterations of Anderson acceleration\nwith the original non-smooth scheme. In any case, these properties are only\ntrue for the converged solution, but not for iterates. In this sense, we have\nalso proposed the concept of projected nonlinear solvers, where a projection\nstep is performed at the end of every nonlinear iteration onto a FE space of\nadmissible solutions. The space of admissible solutions is the one that\nsatisfies the desired monotonic properties (maximum principle or positivity).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 14:46:50 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 17:00:02 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Badia", "Santiago", ""], ["Bonilla", "Jes\u00fas", ""]]}, {"id": "1606.09155", "submitter": "Yangyang Xu", "authors": "Yangyang Xu", "title": "Accelerated first-order primal-dual proximal methods for linearly\n  constrained composite convex programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by big data applications, first-order methods have been extremely\npopular in recent years. However, naive gradient methods generally converge\nslowly. Hence, much efforts have been made to accelerate various first-order\nmethods. This paper proposes two accelerated methods towards solving structured\nlinearly constrained convex programming, for which we assume composite convex\nobjective.\n  The first method is the accelerated linearized augmented Lagrangian method\n(LALM). At each update to the primal variable, it allows linearization to the\ndifferentiable function and also the augmented term, and thus it enables easy\nsubproblems. Assuming merely weak convexity, we show that LALM owns $O(1/t)$\nconvergence if parameters are kept fixed during all the iterations and can be\naccelerated to $O(1/t^2)$ if the parameters are adapted, where $t$ is the\nnumber of total iterations.\n  The second method is the accelerated linearized alternating direction method\nof multipliers (LADMM). In addition to the composite convexity, it further\nassumes two-block structure on the objective. Different from classic ADMM, our\nmethod allows linearization to the objective and also augmented term to make\nthe update simple. Assuming strong convexity on one block variable, we show\nthat LADMM also enjoys $O(1/t^2)$ convergence with adaptive parameters. This\nresult is a significant improvement over that in [Goldstein et. al, SIIMS'14],\nwhich requires strong convexity on both block variables and no linearization to\nthe objective or augmented term.\n  Numerical experiments are performed on quadratic programming, image\ndenoising, and support vector machine. The proposed accelerated methods are\ncompared to nonaccelerated ones and also existing accelerated methods. The\nresults demonstrate the validness of acceleration and superior performance of\nthe proposed methods over existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 15:26:55 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Xu", "Yangyang", ""]]}, {"id": "1606.09402", "submitter": "Wenjian Yu Prof.", "authors": "Wenjian Yu, Yu Gu, and Yaohang Li", "title": "Efficient Randomized Algorithms for the Fixed-Precision Low-Rank Matrix\n  Approximation", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms for low-rank matrix approximation are investigated,\nwith the emphasis on the fixed-precision problem and computational efficiency\nfor handling large matrices. The algorithms are based on the so-called QB\nfactorization, where Q is an orthonormal matrix. Firstly, a mechanism for\ncalculating the approximation error in Frobenius norm is proposed, which\nenables efficient adaptive rank determination for large and/or sparse matrix.\nIt can be combined with any QB-form factorization algorithm in which B's rows\nare incrementally generated. Based on the blocked randQB algorithm by P.-G.\nMartinsson and S. Voronin, this results in an algorithm called randQB EI. Then,\nwe further revise the algorithm to obtain a pass-efficient algorithm, randQB\nFP, which is mathematically equivalent to the existing randQB algorithms and\nalso suitable for the fixed-precision problem. Especially, randQB FP can serve\nas a single-pass algorithm for calculating leading singular values, under\ncertain condition. With large and/or sparse test matrices, we have empirically\nvalidated the merits of the proposed techniques, which exhibit remarkable\nspeedup and memory saving over the blocked randQB algorithm. We have also\ndemonstrated that the single-pass algorithm derived by randQB FP is much more\naccurate than an existing single-pass algorithm. And with data from a scenic\nimage and an information retrieval application, we have shown the advantages of\nthe proposed algorithms over the adaptive range finder algorithm for solving\nthe fixed-precision problem.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 09:14:53 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 08:06:04 GMT"}, {"version": "v3", "created": "Sat, 10 Feb 2018 06:44:17 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Yu", "Wenjian", ""], ["Gu", "Yu", ""], ["Li", "Yaohang", ""]]}]