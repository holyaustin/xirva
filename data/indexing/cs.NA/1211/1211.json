[{"id": "1211.0517", "submitter": "Prathapasinghe Dharmawansa", "authors": "Prathapasinghe Dharmawansa, Matthew McKay, and Yang Chen", "title": "Distributions of Demmel and Related Condition Numbers", "comments": "To appear in SIAM Journal on Matrix Analysis and Applications (SIMAX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random matrix $\\mathbf{A}\\in\\mathbb{C}^{m\\times n}$ ($m \\geq n$)\ncontaining independent complex Gaussian entries with zero mean and unit\nvariance, and let $0<\\lambda_1\\leq \\lambda_{2}\\leq ...\\leq \\lambda_n<\\infty$\ndenote the eigenvalues of $\\mathbf{A}^{*}\\mathbf{A}$ where $(\\cdot)^*$\nrepresents conjugate-transpose. This paper investigates the distribution of the\nrandom variables $\\frac{\\sum_{j=1}^n \\lambda_j}{\\lambda_k}$, for $k = 1$ and $k\n= 2$. These two variables are related to certain condition number metrics,\nincluding the so-called Demmel condition number, which have been shown to arise\nin a variety of applications. For both cases, we derive new exact expressions\nfor the probability densities, and establish the asymptotic behavior as the\nmatrix dimensions grow large. In particular, it is shown that as $n$ and $m$\ntend to infinity with their difference fixed, both densities scale on the order\nof $n^3$. After suitable transformations, we establish exact expressions for\nthe asymptotic densities, obtaining simple closed-form expressions in some\ncases. Our results generalize the work of Edelman on the Demmel condition\nnumber for the case $m = n$.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 18:38:29 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Dharmawansa", "Prathapasinghe", ""], ["McKay", "Matthew", ""], ["Chen", "Yang", ""]]}, {"id": "1211.1550", "submitter": "Bamdev Mishra", "authors": "B. Mishra, K. Adithya Apuroop and R. Sepulchre", "title": "A Riemannian geometry for low-rank matrix completion", "comments": "Title modified, Typos removed. arXiv admin note: text overlap with\n  arXiv:1209.0430", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Riemannian geometry for fixed-rank matrices that is\nspecifically tailored to the low-rank matrix completion problem. Exploiting the\ndegree of freedom of a quotient space, we tune the metric on our search space\nto the particular least square cost function. At one level, it illustrates in a\nnovel way how to exploit the versatile framework of optimization on quotient\nmanifold. At another level, our algorithm can be considered as an improved\nversion of LMaFit, the state-of-the-art Gauss-Seidel algorithm. We develop\nnecessary tools needed to perform both first-order and second-order\noptimization. In particular, we propose gradient descent schemes (steepest\ndescent and conjugate gradient) and trust-region algorithms. We also show that,\nthanks to the simplicity of the cost function, it is numerically cheap to\nperform an exact linesearch given a search direction, which makes our\nalgorithms competitive with the state-of-the-art on standard low-rank matrix\ncompletion instances.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 13:49:26 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2012 17:50:39 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Mishra", "B.", ""], ["Apuroop", "K. Adithya", ""], ["Sepulchre", "R.", ""]]}, {"id": "1211.2379", "submitter": "Emmanuelle Gouillart", "authors": "Emmanuelle Gouillart (SVI), Florent Krzakala (LPCT), Marc Mezard\n  (LPTMS), Lenka Zdeborov\\'a (IPHT)", "title": "Belief Propagation Reconstruction for Discrete Tomography", "comments": null, "journal-ref": "Inverse Problems 29, 3 (2013) 035003", "doi": "10.1088/0266-5611/29/3/035003", "report-no": null, "categories": "cs.NA cond-mat.stat-mech cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the reconstruction of a two-dimensional discrete image from a set\nof tomographic measurements corresponding to the Radon projection. Assuming\nthat the image has a structure where neighbouring pixels have a larger\nprobability to take the same value, we follow a Bayesian approach and introduce\na fast message-passing reconstruction algorithm based on belief propagation.\nFor numerical results, we specialize to the case of binary tomography. We test\nthe algorithm on binary synthetic images with different length scales and\ncompare our results against a more usual convex optimization approach. We\ninvestigate the reconstruction error as a function of the number of tomographic\nmeasurements, corresponding to the number of projection angles. The belief\npropagation algorithm turns out to be more efficient than the\nconvex-optimization algorithm, both in terms of recovery bounds for noise-free\nprojections, and in terms of reconstruction quality when moderate Gaussian\nnoise is added to the projections.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 07:19:16 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 18:44:19 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Gouillart", "Emmanuelle", "", "SVI"], ["Krzakala", "Florent", "", "LPCT"], ["Mezard", "Marc", "", "LPTMS"], ["Zdeborov\u00e1", "Lenka", "", "IPHT"]]}, {"id": "1211.2517", "submitter": "Yanchuang Cao", "authors": "Yanchuang Cao, Lihua Wen, Junjie Rong", "title": "A SVD accelerated kernel-independent fast multipole method and its\n  application to BEM", "comments": "19 pages, 4 figures", "journal-ref": "Boundary Elements and Other Mesh Reduction Methods XXXVI. 431-443.\n  2013", "doi": "10.2495/BEM360351", "report-no": null, "categories": "cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The kernel-independent fast multipole method (KIFMM) proposed in [1] is of\nalmost linear complexity. In the original KIFMM the time-consuming M2L\ntranslations are accelerated by FFT. However, when more equivalent points are\nused to achieve higher accuracy, the efficiency of the FFT approach tends to be\nlower because more auxiliary volume grid points have to be added. In this\npaper, all the translations of the KIFMM are accelerated by using the singular\nvalue decomposition (SVD) based on the low-rank property of the translating\nmatrices. The acceleration of M2L is realized by first transforming the\nassociated translating matrices into more compact form, and then using low-rank\napproximations. By using the transform matrices for M2L, the orders of the\ntranslating matrices in upward and downward passes are also reduced. The\nimproved KIFMM is then applied to accelerate BEM. The performance of the\nproposed algorithms are demonstrated by three examples. Numerical results show\nthat, compared with the original KIFMM, the present method can reduce about 40%\nof the iterating time and 25% of the memory requirement.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 06:27:22 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2013 01:07:17 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Cao", "Yanchuang", ""], ["Wen", "Lihua", ""], ["Rong", "Junjie", ""]]}, {"id": "1211.2521", "submitter": "Razvan Stefanescu", "authors": "Razvan Stefanescu and Ionel Michael Navon", "title": "POD/DEIM Nonlinear model order reduction of an ADI implicit shallow\n  water equations model", "comments": "41 pages, 16 figures, 12 tables, one appendix", "journal-ref": null, "doi": "10.1016/j.jcp.2012.11.035", "report-no": null, "categories": "physics.comp-ph cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider a 2-D shallow-water equations (SWE) model on\na $\\beta$-plane solved using an alternating direction fully implicit (ADI)\nfinite-difference scheme on a rectangular domain. The scheme was shown to be\nunconditionally stable for the linearized equations.\n  The discretization yields a number of nonlinear systems of algebraic\nequations. We then use a proper orthogonal decomposition (POD) to reduce the\ndimension of the SWE model. Due to the model nonlinearities, the computational\ncomplexity of the reduced model still depends on the number of variables of the\nfull shallow - water equations model. By employing the discrete empirical\ninterpolation method (DEIM) we reduce the computational complexity of the\nreduced order model due to its depending on the nonlinear full dimension model\nand regain the full model reduction expected from the POD model.\n  To emphasize the CPU gain in performance due to use of POD/DEIM, we also\npropose testing an explicit Euler finite difference scheme (EE) as an\nalternative to the ADI implicit scheme for solving the swallow water equations\nmodel.\n  We then proceed to assess the efficiency of POD/DEIM as a function of number\nof spatial discretization points, time steps, and POD basis functions. As was\nexpected, our numerical experiments showed that the CPU time performances of\nPOD/DEIM schemes are proportional to the number of mesh points. Once the number\nof spatial discretization points exceeded 10000 and for 90 DEIM interpolation\npoints, the CPU time was decreased by a factor of 10 in case of POD/DEIM\nimplicit SWE scheme and by a factor of 15 for the POD/DEIM explicit SWE scheme\nin comparison with the corresponding POD SWE schemes. Our numerical tests\nrevealed that if the number of points selected by DEIM algorithm reached 50,\nthe approximation errors due to POD/DEIM and POD reduced systems have the same\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 06:50:21 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Stefanescu", "Razvan", ""], ["Navon", "Ionel Michael", ""]]}, {"id": "1211.3056", "submitter": "Mourad Gouicem", "authors": "Pierre Fortin (LIP6), Mourad Gouicem (LIP6), Stef Graillat (LIP6)", "title": "GPU-accelerated generation of correctly-rounded elementary functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IEEE 754-2008 standard recommends the correct rounding of some elementary\nfunctions. This requires to solve the Table Maker's Dilemma which implies a\nhuge amount of CPU computation time. We consider in this paper accelerating\nsuch computations, namely Lefe'vre algorithm on Graphics Processing Units\n(GPUs) which are massively parallel architectures with a partial SIMD execution\n(Single Instruction Multiple Data). We first propose an analysis of the\nLef\\`evre hard-to-round argument search using the concept of continued\nfractions. We then propose a new parallel search algorithm much more efficient\non GPU thanks to its more regular control flow. We also present an efficient\nhybrid CPU-GPU deployment of the generation of the polynomial approximations\nrequired in Lef\\`evre algorithm. In the end, we manage to obtain overall\nspeedups up to 53.4x on one GPU over a sequential CPU execution, and up to 7.1x\nover a multi-core CPU, which enable a much faster solving of the Table Maker's\nDilemma for the double precision format.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 17:28:03 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2013 11:51:55 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Fortin", "Pierre", "", "LIP6"], ["Gouicem", "Mourad", "", "LIP6"], ["Graillat", "Stef", "", "LIP6"]]}, {"id": "1211.3211", "submitter": "Kensuke Sekihara", "authors": "Kensuke Sekihara, Hagai Attias, Julia P. Owen, Srikantan S. Nagarajan", "title": "Effectiveness of sparse Bayesian algorithm for MVAR coefficient\n  estimation in MEG/EEG source-space causality analysis", "comments": "Proceedings of the 8th Annual Conference of Non-invasive Functional\n  Source Imaging held at Banff, May 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the effectiveness of a sparse Bayesian algorithm to\nestimate multivariate autoregressive coefficients when a large amount of\nbackground interference exists. This paper employs computer experiments to\ncompare two methods in the source-space causality analysis: the conventional\nleast-squares method and a sparse Bayesian method. Results of our computer\nexperiments show that the interference affects the least-squares method in a\nvery severe manner. It produces large false-positive results, unless the\nsignal-to-interference ratio is very high. On the other hand, the sparse\nBayesian method is relatively insensitive to the existence of interference.\nHowever, this robustness of the sparse Bayesian method is attained on the\nscarifies of the detectability of true causal relationship. Our experiments\nalso show that the surrogate data bootstrapping method tends to give a\nstatistical threshold that are too low for the sparse method.\n  The permutation-test-based method gives a higher (more conservative)\nthreshold and it should be used with the sparse Bayesian method whenever the\ncontrol period is available.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 06:36:15 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Sekihara", "Kensuke", ""], ["Attias", "Hagai", ""], ["Owen", "Julia P.", ""], ["Nagarajan", "Srikantan S.", ""]]}, {"id": "1211.3500", "submitter": "Guoxu Zhou", "authors": "Guoxu Zhou, Andrzej Cichocki, and Shengli Xie", "title": "Accelerated Canonical Polyadic Decomposition by Using Mode Reduction", "comments": "12 pages. Accepted by TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2013.2271507", "report-no": null, "categories": "cs.NA cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical Polyadic (or CANDECOMP/PARAFAC, CP) decompositions (CPD) are widely\napplied to analyze high order tensors. Existing CPD methods use alternating\nleast square (ALS) iterations and hence need to unfold tensors to each of the\n$N$ modes frequently, which is one major bottleneck of efficiency for\nlarge-scale data and especially when $N$ is large. To overcome this problem, in\nthis paper we proposed a new CPD method which converts the original $N$th\n($N>3$) order tensor to a 3rd-order tensor first. Then the full CPD is realized\nby decomposing this mode reduced tensor followed by a Khatri-Rao product\nprojection procedure. This way is quite efficient as unfolding to each of the\n$N$ modes are avoided, and dimensionality reduction can also be easily\nincorporated to further improve the efficiency. We show that, under mild\nconditions, any $N$th-order CPD can be converted into a 3rd-order case but\nwithout destroying the essential uniqueness, and theoretically gives the same\nresults as direct $N$-way CPD methods. Simulations show that, compared with\nstate-of-the-art CPD methods, the proposed method is more efficient and escape\nfrom local solutions more easily.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 05:50:30 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 03:06:52 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Zhou", "Guoxu", ""], ["Cichocki", "Andrzej", ""], ["Xie", "Shengli", ""]]}, {"id": "1211.3567", "submitter": "Nikola Mirkov Mr.", "authors": "Nikola Mirkov and Bosko Rasuo", "title": "A Bernstein Polynomial Collocation Method for the Solution of Elliptic\n  Boundary Value Problems", "comments": "21 page, 12 figures, 5tables, Python code listings in the Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a formulation of a point-collocation method in which the\nunknown function is approximated using global expansion in tensor product\nBernstein polynomial basis is presented. Bernstein polynomials used in this\nstudy are defined over general interval [a,b]. Method incorporates several\nideas that enable higher numerical efficiency compared to Bernstein polynomial\nmethods that have been previously presented. The approach is illustrated by a\nsolution of Poisson, Helmholtz and Biharmonic equations with Dirichlet and\nNeumann type boundary conditions. Comparisons with analytical solutions are\ngiven to demonstrate the accuracy and convergence properties of the current\nprocedure. The method is implemented in an open-source code, and a library for\nmanipulation of Bernstein polynomials bernstein-poly, developed by the authors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 10:38:46 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Mirkov", "Nikola", ""], ["Rasuo", "Bosko", ""]]}, {"id": "1211.3603", "submitter": "Francois Orieux", "authors": "F. Orieux, J.-F. Giovannelli, T. Rodet and A. Abergel", "title": "Estimating hyperparameters and instrument parameters in regularized\n  inversion. Illustration for SPIRE/Herschel map making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.NA stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe regularized methods for image reconstruction and focus on the\nquestion of hyperparameter and instrument parameter estimation, i.e.\nunsupervised and myopic problems. We developed a Bayesian framework that is\nbased on the \\post density for all unknown quantities, given the observations.\nThis density is explored by a Markov Chain Monte-Carlo sampling technique based\non a Gibbs loop and including a Metropolis-Hastings step. The numerical\nevaluation relies on the SPIRE instrument of the Herschel observatory. Using\nsimulated and real observations, we show that the hyperparameters and\ninstrument parameters are correctly estimated, which opens up many perspectives\nfor imaging in astrophysics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 13:37:04 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Orieux", "F.", ""], ["Giovannelli", "J. -F.", ""], ["Rodet", "T.", ""], ["Abergel", "A.", ""]]}, {"id": "1211.3796", "submitter": "Anh Huy Phan", "authors": "Anh Huy Phan, Petr Tichavsky and Andrzej Cichocki", "title": "CANDECOMP/PARAFAC Decomposition of High-order Tensors Through Tensor\n  Reshaping", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2269046", "report-no": null, "categories": "math.NA cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, algorithms for order-3 CANDECOMP/-PARAFAC (CP), also coined\ncanonical polyadic decomposition (CPD), are easily to implement and can be\nextended to higher order CPD. Unfortunately, the algorithms become\ncomputationally demanding, and they are often not applicable to higher order\nand relatively large scale tensors. In this paper, by exploiting the uniqueness\nof CPD and the relation of a tensor in Kruskal form and its unfolded tensor, we\npropose a fast approach to deal with this problem. Instead of directly\nfactorizing the high order data tensor, the method decomposes an unfolded\ntensor with lower order, e.g., order-3 tensor. On basis of the order-3\nestimated tensor, a structured Kruskal tensor of the same dimension as the data\ntensor is then generated, and decomposed to find the final solution using fast\nalgorithms for the structured CPD. In addition, strategies to unfold tensors\nare suggested and practically verified in the paper.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 04:48:33 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Phan", "Anh Huy", ""], ["Tichavsky", "Petr", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1211.3821", "submitter": "Enrique Nadal Soriano", "authors": "Enrique Nadal Soriano (DIMM), Octavio Andr\\'es Gonz\\'alez Estrada\n  (IMAM), Juan Jos\\'e R\\'odenas Garc\\'ia (DIMM), Francisco Javier Fuenmayor\n  Fern\\'andez (DIMM)", "title": "Report: Error estimation of recovered solution in FE analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recovery type error estimators introduced by Zienkiewicz and Zhu use a\nrecovered stress field evaluated from the Finite Element (FE) solution. Their\naccuracy depends on the quality of the recovered field. In this sense, accurate\nresults are obtained using recovery procedures based on the Superconvergent\nPatch recovery technique (SPR). These error estimators can be easily\nimplemented and provide accurate estimates. Another important feature is that\nthe recovered solution is of a better quality than the FE solution and can\ntherefore be used as an enhanced solution. We have developed an SPR-type\nrecovery technique that considers equilibrium and displacements constraints to\nobtain a very accurate recovered displacements field from which a recovered\nstress field can also be evaluated. We propose the use of these recovered\nfields as the standard output of the FE code instead of the raw FE solution.\nTechniques to quantify the error of the recovered solution are therefore\nneeded. In this report we present an error estimation technique that accurately\nevaluates the error of the recovered solution both at global and local levels\nin the FEM and XFEM frameworks. We have also developed an h-adaptive mesh\nrefinement strategy based on the error of the recovered solution. As the\nconverge rate of the error of the recovered solution is higher than that of the\nFE one, the computational cost required to obtain a solution with a prescribed\naccuracy is smaller than for traditional h-adaptive processes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 08:28:29 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Soriano", "Enrique Nadal", "", "DIMM"], ["Estrada", "Octavio Andr\u00e9s Gonz\u00e1lez", "", "IMAM"], ["Garc\u00eda", "Juan Jos\u00e9 R\u00f3denas", "", "DIMM"], ["Fern\u00e1ndez", "Francisco Javier Fuenmayor", "", "DIMM"]]}, {"id": "1211.4047", "submitter": "Garth Wells", "authors": "Martin S. Alnaes and Anders Logg and Kristian B. Oelgaard and Marie E.\n  Rognes and Garth N. Wells", "title": "Unified Form Language: A domain-specific language for weak formulations\n  of partial differential equations", "comments": "To appear in ACM Transactions on Mathematical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Unified Form Language (UFL), which is a domain-specific\nlanguage for representing weak formulations of partial differential equations\nwith a view to numerical approximation. Features of UFL include support for\nvariational forms and functionals, automatic differentiation of forms and\nexpressions, arbitrary function space hierarchies for multi-field problems,\ngeneral differential operators and flexible tensor algebra. With these\nfeatures, UFL has been used to effortlessly express finite element methods for\ncomplex systems of partial differential equations in near-mathematical\nnotation, resulting in compact, intuitive and readable programs. We present in\nthis work the language and its construction. An implementation of UFL is freely\navailable as an open-source software library. The library generates abstract\nsyntax tree representations of variational problems, which are used by other\nsoftware libraries to generate concrete low-level implementations. Some\napplication examples are presented and libraries that support UFL are\nhighlighted.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 21:56:02 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2013 20:18:09 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Alnaes", "Martin S.", ""], ["Logg", "Anders", ""], ["Oelgaard", "Kristian B.", ""], ["Rognes", "Marie E.", ""], ["Wells", "Garth N.", ""]]}, {"id": "1211.4116", "submitter": "Louis Theran", "authors": "Franz J. Kir\\'aly, Louis Theran, Ryota Tomioka", "title": "The Algebraic Combinatorial Approach for Low-Rank Matrix Completion", "comments": "37 pages, with an appendix by Takeaki Uno", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.AG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algebraic combinatorial view on low-rank matrix completion\nbased on studying relations between a few entries with tools from algebraic\ngeometry and matroid theory. The intrinsic locality of the approach allows for\nthe treatment of single entries in a closed theoretical and practical\nframework. More specifically, apart from introducing an algebraic combinatorial\ntheory of low-rank matrix completion, we present probability-one algorithms to\ndecide whether a particular entry of the matrix can be completed. We also\ndescribe methods to complete that entry from a few others, and to estimate the\nerror which is incurred by any method completing that entry. Furthermore, we\nshow how known results on matrix completion and their sampling assumptions can\nbe related to our new perspective and interpreted in terms of a completability\nphase transition.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2012 12:23:36 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2012 00:07:26 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2013 06:43:36 GMT"}, {"version": "v4", "created": "Tue, 19 Aug 2014 15:00:30 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Theran", "Louis", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1211.4332", "submitter": "Ye Liang", "authors": "Ye Liang", "title": "Real root refinements for univariate polynomial equations", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real root finding of polynomial equations is a basic problem in computer\nalgebra. This task is usually divided into two parts: isolation and refinement.\nIn this paper, we propose two algorithms LZ1 and LZ2 to refine real roots of\nunivariate polynomial equations. Our algorithms combine Newton's method and the\nsecant method to bound the unique solution in an interval of a monotonic convex\nisolation (MCI) of a polynomial, and have quadratic and cubic convergence\nrates, respectively. To avoid the swell of coefficients and speed up the\ncomputation, we implement the two algorithms by using the floating-point\ninterval method in Maple15 with the package intpakX. Experiments show that our\nmethods are effective and much faster than the function RefineBox in the\nsoftware Maple15 on benchmark polynomials.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 08:30:30 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Liang", "Ye", ""]]}, {"id": "1211.4461", "submitter": "Siegfried Cools", "authors": "Siegfried Cools, Bram Reps, Wim Vanroose", "title": "An efficient multigrid calculation of the far field map for Helmholtz\n  and Schr\\\"odinger equations", "comments": "SIAM Journal on Scientific Computing, 29 pages, 10 figures, 5 tables", "journal-ref": "SIAM Journal on Scientific Computing, 36:3(2014), p. 367-395", "doi": "10.1137/130921064", "report-no": null, "categories": "math.NA cs.NA math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new highly efficient calculation method for the\nfar field amplitude pattern that arises from scattering problems governed by\nthe d-dimensional Helmholtz equation and, by extension, Schr\\\"odinger's\nequation. The new technique is based upon a reformulation of the classical\nreal-valued Green's function integral for the far field amplitude to an\nequivalent integral over a complex domain. It is shown that the scattered wave,\nwhich is essential for the calculation of the far field integral, can be\ncomputed very efficiently along this complex contour (or manifold, in multiple\ndimensions). Using the iterative multigrid method as a solver for the\ndiscretized damped scattered wave system, the proposed approach results in a\nfast and scalable calculation method for the far field map. The complex contour\nmethod is successfully validated on Helmholtz and Schr\\\"odinger model problems\nin two and three spatial dimensions, and multigrid convergence results are\nprovided to substantiate the wavenumber scalability and overall performance of\nthe method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 15:23:31 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2012 15:11:29 GMT"}, {"version": "v3", "created": "Wed, 15 May 2013 13:25:12 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2013 16:35:33 GMT"}, {"version": "v5", "created": "Wed, 19 Mar 2014 13:29:54 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Cools", "Siegfried", ""], ["Reps", "Bram", ""], ["Vanroose", "Wim", ""]]}, {"id": "1211.4516", "submitter": "Lasha Ephremidze", "authors": "Nika Salia, Alexander Gamkrelidze, and Lasha Ephremidze", "title": "Numerical comparison of different algorithms for construction of wavelet\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization of compact wavelet matrices into primitive ones has been known\nfor more than 20 years. This method makes it possible to generate wavelet\nmatrix coefficients and also to specify them by their first row. Recently, a\nnew parametrization of compact wavelet matrices of the same order and degree\nhas been introduced by the last author. This method also enables us to fulfill\nthe above mentioned tasks of matrix constructions. In the present paper, we\nbriefly describe the corresponding algorithms based on two different methods,\nand numerically compare their performance\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 17:37:59 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Salia", "Nika", ""], ["Gamkrelidze", "Alexander", ""], ["Ephremidze", "Lasha", ""]]}, {"id": "1211.4974", "submitter": "Martin Ziegler", "authors": "Akitoshi Kawamura and Norbert Th. M\\\"uller and Carsten R\\\"osnick and\n  Martin Ziegler", "title": "Parameterized Uniform Complexity in Numerics: from Smooth to Analytic,\n  from NP-hard to Polytime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synthesis of classical Computational Complexity Theory with Recursive\nAnalysis provides a quantitative foundation to reliable numerics. Here the\noperators of maximization, integration, and solving ordinary differential\nequations are known to map (even high-order differentiable) polynomial-time\ncomputable functions to instances which are `hard' for classical complexity\nclasses NP, #P, and CH; but, restricted to analytic functions, map\npolynomial-time computable ones to polynomial-time computable ones --\nnon-uniformly!\n  We investigate the uniform parameterized complexity of the above operators in\nthe setting of Weihrauch's TTE and its second-order extension due to\nKawamura&Cook (2010). That is, we explore which (both continuous and discrete,\nfirst and second order) information and parameters on some given f is\nsufficient to obtain similar data on Max(f) and int(f); and within what running\ntime, in terms of these parameters and the guaranteed output precision 2^(-n).\n  It turns out that Gevrey's hierarchy of functions climbing from analytic to\nsmooth corresponds to the computational complexity of maximization growing from\npolytime to NP-hard. Proof techniques involve mainly the Theory of (discrete)\nComputation, Hard Analysis, and Information-Based Complexity.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 09:23:14 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Kawamura", "Akitoshi", ""], ["M\u00fcller", "Norbert Th.", ""], ["R\u00f6snick", "Carsten", ""], ["Ziegler", "Martin", ""]]}, {"id": "1211.5052", "submitter": "Ricardo Monge", "authors": "Osvaldo Skliar, Ricardo E. Monge, Sherry Gapper, Guillermo Oviedo", "title": "A Mathematical Random Number Generator (MRNG)", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel Mathematical Random Number Generator (MRNG) is presented here. In\nthis case, \"mathematical\" refers to the fact that to construct that generator\nit is not necessary to resort to a physical phenomenon, such as the thermal\nnoise of an electronic device, but rather to a mathematical procedure. The MRNG\ngenerates binary strings - in principle, as long as desired - which may be\nconsidered genuinely random in the sense that they pass the statistical tests\ncurrently accepted to evaluate the randomness of those strings. From those\nstrings, the MRNG also generates random numbers expressed in base 10. An MRNG\nhas been installed as a facility on the following web page:\nhttp://www.appliedmathgroup.org. This generator may be used for applications in\ntasks in: a) computational simulation of probabilistic-type systems, and b) the\nrandom selection of samples of different populations. Users interested in\napplications in cryptography can build another MRNG, but they would have to\nwithhold information - specified in section 5 - from people who are not\nauthorized to decode messages encrypted using that resource.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 15:06:41 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Skliar", "Osvaldo", ""], ["Monge", "Ricardo E.", ""], ["Gapper", "Sherry", ""], ["Oviedo", "Guillermo", ""]]}, {"id": "1211.5082", "submitter": "Marianne Clausel", "authors": "Marianne Clausel and Thomas Oberlin and Val\\'erie Perrier", "title": "The Monogenic Synchrosqueezed Wavelet Transform: A tool for the\n  Decomposition/Demodulation of AM-FM images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synchrosqueezing method aims at decomposing 1D functions as\nsuperpositions of a small number of \"Intrinsic Modes\", supposed to be well\nseparated both in time and frequency. Based on the unidimensional wavelet\ntransform and its reconstruction properties, the synchrosqueezing transform\nprovides a powerful representation of multicomponent signals in the\ntime-frequency plane, together with a reconstruction of each mode.\n  In this paper, a bidimensional version of the synchrosqueezing transform is\ndefined, by considering a well-adapted extension of the concept of analytic\nsignal to images: the monogenic signal. The natural bidimensional counterpart\nof the notion of Intrinsic Mode is then the concept of \"Intrinsic Monogenic\nMode\" that we define. Thereafter, we investigate the properties of its\nassociated Monogenic Wavelet Decomposition. This leads to a natural bivariate\nextension of the Synchrosqueezed Wavelet Transform, for decomposing and\nprocessing multicomponent images. Numerical tests validate the effectiveness of\nthe method for different examples.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 08:23:30 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Clausel", "Marianne", ""], ["Oberlin", "Thomas", ""], ["Perrier", "Val\u00e9rie", ""]]}, {"id": "1211.5414", "submitter": "Daniel Hsu", "authors": "Daniel Hsu and Sham M. Kakade and Tong Zhang", "title": "Analysis of a randomized approximation scheme for matrix multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note gives a simple analysis of a randomized approximation scheme for\nmatrix multiplication proposed by Sarlos (2006) based on a random rotation\nfollowed by uniform column sampling. The result follows from a matrix version\nof Bernstein's inequality and a tail inequality for quadratic forms in\nsubgaussian random vectors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 06:11:54 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Zhang", "Tong", ""]]}, {"id": "1211.5904", "submitter": "Paolo Bientinesi", "authors": "Diego Fabregat-Traver (1), Paolo Bientinesi (1), ((1) AICES, RWTH\n  Aachen)", "title": "Application-tailored Linear Algebra Algorithms: A search-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of automatically generating algorithms\nfor linear algebra operations by taking advantage of problem-specific\nknowledge. In most situations, users possess much more information about the\nproblem at hand than what current libraries and computing environments accept;\nevidence shows that if properly exploited, such information leads to\nuncommon/unexpected speedups. We introduce a knowledge-aware linear algebra\ncompiler that allows users to input matrix equations together with properties\nabout the operands and the problem itself; for instance, they can specify that\nthe equation is part of a sequence, and how successive instances are related to\none another. The compiler exploits all this information to guide the generation\nof algorithms, to limit the size of the search space, and to avoid redundant\ncomputations. We applied the compiler to equations arising as part of\nsensitivity and genome studies; the algorithms produced exhibit, respectively,\n100- and 1000-fold speedups.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 10:15:29 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Fabregat-Traver", "Diego", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1211.5969", "submitter": "Petr Tichy", "authors": "J\\\"org Liesen, Petr Tich\\'y", "title": "The field of values bound on ideal GMRES", "comments": "made some corrections and improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely known result of Elman, and its improvements due to Starke, Eiermann\nand Ernst, gives a bound on the worst-case GMRES residual norm using quantities\nrelated to the field of values of the given matrix and its inverse. We prove\nthat these bounds also hold for the ideal GMRES approximation, and we derive\nand discuss some improvements of the bounds.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 14:36:46 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2013 06:27:05 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 09:34:03 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Liesen", "J\u00f6rg", ""], ["Tich\u00fd", "Petr", ""]]}, {"id": "1211.6687", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "Robustness Analysis of Hottopixx, a Linear Programming Model for\n  Factoring Nonnegative Matrices", "comments": "23 pages; new numerical results; Comparison with Arora et al.;\n  Accepted in SIAM J. Mat. Anal. Appl", "journal-ref": "SIAM J. Matrix Anal. & Appl. 34 (3), pp. 1189-1212, 2013", "doi": "10.1137/120900629", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although nonnegative matrix factorization (NMF) is NP-hard in general, it has\nbeen shown very recently that it is tractable under the assumption that the\ninput nonnegative data matrix is close to being separable (separability\nrequires that all columns of the input matrix belongs to the cone spanned by a\nsmall subset of these columns). Since then, several algorithms have been\ndesigned to handle this subclass of NMF problems. In particular, Bittorf,\nRecht, R\\'e and Tropp (`Factoring nonnegative matrices with linear programs',\nNIPS 2012) proposed a linear programming model, referred to as Hottopixx. In\nthis paper, we provide a new and more general robustness analysis of their\nmethod. In particular, we design a provably more robust variant using a\npost-processing strategy which allows us to deal with duplicates and near\nduplicates in the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 18:05:56 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 16:06:55 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2013 08:53:06 GMT"}, {"version": "v4", "created": "Fri, 31 May 2013 15:06:57 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1211.6719", "submitter": "Thakshila Wimalajeewa", "authors": "Thakshila Wimalajeewa and Pramod K. Varshney", "title": "Cooperative Sparsity Pattern Recovery in Distributed Networks Via\n  Distributed-OMP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of collaboratively estimating the\nsparsity pattern of a sparse signal with multiple measurement data in\ndistributed networks. We assume that each node makes Compressive Sensing (CS)\nbased measurements via random projections regarding the same sparse signal. We\npropose a distributed greedy algorithm based on Orthogonal Matching Pursuit\n(OMP), in which the sparse support is estimated iteratively while fusing\nindices estimated at distributed nodes. In the proposed distributed framework,\neach node has to perform less number of iterations of OMP compared to the\nsparsity index of the sparse signal. Thus, with each node having a very small\nnumber of compressive measurements, a significant performance gain in support\nrecovery is achieved via the proposed collaborative scheme compared to the case\nwhere each node estimates the sparsity pattern independently and then fusion is\nperformed to get a global estimate. We further extend the algorithm to estimate\nthe sparsity pattern in a binary hypothesis testing framework, where the\nalgorithm first detects the presence of a sparse signal collaborating among\nnodes with a fewer number of iterations of OMP and then increases the number of\niterations to estimate the sparsity pattern only if the signal is detected.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 19:55:24 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Wimalajeewa", "Thakshila", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1211.6822", "submitter": "Tamio Koyama", "authors": "Tamio Koyama, Akimichi Takemura", "title": "Calculation of orthant probabilities by the holonomic gradient method", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the holonomic gradient method (HGM) introduced by [9] to the\ncalculation of orthant probabilities of multivariate normal distribution. The\nholonomic gradient method applied to orthant probabilities is found to be a\nvariant of Plackett's recurrence relation ([14]). However an implementation of\nthe method yields recurrence relations more suitable for numerical computation\nthan Plackett's recurrence relation. We derive some theoretical results on the\nholonomic system for the orthant probabilities. These results show that\nmultivariate normal orthant probabilities possess some remarkable properties\nfrom the viewpoint of holonomic systems. Finally we show that numerical\nperformance of our method is comparable or superior compared to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 07:03:24 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2013 06:31:39 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Koyama", "Tamio", ""], ["Takemura", "Akimichi", ""]]}]