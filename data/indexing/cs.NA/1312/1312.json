[{"id": "1312.0707", "submitter": "Farbod Roosta-Khorasani", "authors": "Farbod Roosta-Khorasani, Kees van den Doel and Uri Ascher", "title": "Data completion and stochastic algorithms for PDE inversion problems\n  with many measurements", "comments": null, "journal-ref": "Electronic Transactions on Numerical Analysis. 42 (2014) 177-196", "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems involving systems of partial differential equations (PDEs)\nwith many measurements or experiments can be very expensive to solve\nnumerically. In a recent paper we examined dimensionality reduction methods,\nboth stochastic and deterministic, to reduce this computational burden,\nassuming that all experiments share the same set of receivers. In the present\narticle we consider the more general and practically important case where\nreceivers are not shared across experiments. We propose a data completion\napproach to alleviate this problem. This is done by means of an approximation\nusing an appropriately restricted gradient or Laplacian regularization,\nextending existing data for each experiment to the union of all receiver\nlocations. Results using the method of simultaneous sources (SS) with the\ncompleted data are then compared to those obtained by a more general but slower\nrandom subset (RS) method which requires no modifications.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 06:05:01 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 19:39:29 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Roosta-Khorasani", "Farbod", ""], ["Doel", "Kees van den", ""], ["Ascher", "Uri", ""]]}, {"id": "1312.1254", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Ruru Hao, Wotao Yin, Zhixun Su", "title": "Parallel matrix factorization for low-rank tensor completion", "comments": "25 pages, 12 figures", "journal-ref": "Inverse Problems and Imaging. Volume 9, No.2, 601-624, 2015", "doi": "10.3934/ipi.2015.9.601", "report-no": null, "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order low-rank tensors naturally arise in many applications including\nhyperspectral data recovery, video inpainting, seismic data recon- struction,\nand so on. We propose a new model to recover a low-rank tensor by\nsimultaneously performing low-rank matrix factorizations to the all-mode ma-\ntricizations of the underlying tensor. An alternating minimization algorithm is\napplied to solve the model, along with two adaptive rank-adjusting strategies\nwhen the exact rank is not known.\n  Phase transition plots reveal that our algorithm can recover a variety of\nsynthetic low-rank tensors from significantly fewer samples than the compared\nmethods, which include a matrix completion method applied to tensor recovery\nand two state-of-the-art tensor completion methods. Further tests on real-\nworld data show similar advantages. Although our model is non-convex, our\nalgorithm performs consistently throughout the tests and give better results\nthan the compared methods, some of which are based on convex models. In\naddition, the global convergence of our algorithm can be established in the\nsense that the gradient of Lagrangian function converges to zero.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 17:36:49 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2015 21:10:10 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Xu", "Yangyang", ""], ["Hao", "Ruru", ""], ["Yin", "Wotao", ""], ["Su", "Zhixun", ""]]}, {"id": "1312.1431", "submitter": "Miles Lubin", "authors": "Miles Lubin and Iain Dunning", "title": "Computing in Operations Research using Julia", "comments": "Source code included in supplement", "journal-ref": null, "doi": "10.1287/ijoc.2014.0623", "report-no": null, "categories": "math.OC cs.NA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of numerical computing is currently characterized by a divide\nbetween highly efficient yet typically cumbersome low-level languages such as\nC, C++, and Fortran and highly expressive yet typically slow high-level\nlanguages such as Python and MATLAB. This paper explores how Julia, a modern\nprogramming language for numerical computing which claims to bridge this divide\nby incorporating recent advances in language and compiler design (such as\njust-in-time compilation), can be used for implementing software and algorithms\nfundamental to the field of operations research, with a focus on mathematical\noptimization. In particular, we demonstrate algebraic modeling for linear and\nnonlinear optimization and a partial implementation of a practical simplex\ncode. Extensive cross-language benchmarks suggest that Julia is capable of\nobtaining state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 04:29:20 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Lubin", "Miles", ""], ["Dunning", "Iain", ""]]}, {"id": "1312.1613", "submitter": "Jim Wang J-Y", "authors": "Jim Jing-Yan Wang", "title": "Max-Min Distance Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) has been a popular representation\nmethod for pattern classification problem. It tries to decompose a nonnegative\nmatrix of data samples as the product of a nonnegative basic matrix and a\nnonnegative coefficient matrix, and the coefficient matrix is used as the new\nrepresentation. However, traditional NMF methods ignore the class labels of the\ndata samples. In this paper, we proposed a supervised novel NMF algorithm to\nimprove the discriminative ability of the new representation. Using the class\nlabels, we separate all the data sample pairs into within-class pairs and\nbetween-class pairs. To improve the discriminate ability of the new NMF\nrepresentations, we hope that the maximum distance of the within-class pairs in\nthe new NMF space could be minimized, while the minimum distance of the\nbetween-class pairs pairs could be maximized. With this criterion, we construct\nan objective function and optimize it with regard to basic and coefficient\nmatrices and slack variables alternatively, resulting in a iterative algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 16:49:05 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Wang", "Jim Jing-Yan", ""]]}, {"id": "1312.1638", "submitter": "Freddie Witherden", "authors": "Freddie D Witherden and Antony M Farrington and Peter E Vincent", "title": "PyFR: An Open Source Framework for Solving Advection-Diffusion Type\n  Problems on Streaming Architectures using the Flux Reconstruction Approach", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2014.07.011", "report-no": null, "categories": "physics.comp-ph cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  High-order numerical methods for unstructured grids combine the superior\naccuracy of high-order spectral or finite difference methods with the geometric\nflexibility of low-order finite volume or finite element schemes. The Flux\nReconstruction (FR) approach unifies various high-order schemes for\nunstructured grids within a single framework. Additionally, the FR approach\nexhibits a significant degree of element locality, and is thus able to run\nefficiently on modern streaming architectures, such as Graphical Processing\nUnits (GPUs). The aforementioned properties of FR mean it offers a promising\nroute to performing affordable, and hence industrially relevant,\nscale-resolving simulations of hitherto intractable unsteady flows within the\nvicinity of real-world engineering geometries. In this paper we present PyFR,\nan open-source Python based framework for solving advection-diffusion type\nproblems on streaming architectures using the FR approach. The framework is\ndesigned to solve a range of governing systems on mixed unstructured grids\ncontaining various element types. It is also designed to target a range of\nhardware platforms via use of an in-built domain specific language based on the\nMako templating engine. The current release of PyFR is able to solve the\ncompressible Euler and Navier-Stokes equations on grids of quadrilateral and\ntriangular elements in two dimensions, and hexahedral elements in three\ndimensions, targeting clusters of CPUs, and NVIDIA GPUs. Results are presented\nfor various benchmark flow problems, single-node performance is discussed, and\nscalability of the code is demonstrated on up to 104 NVIDIA M2090 GPUs. The\nsoftware is freely available under a 3-Clause New Style BSD license (see\nwww.pyfr.org).\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 18:39:40 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 05:12:45 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Witherden", "Freddie D", ""], ["Farrington", "Antony M", ""], ["Vincent", "Peter E", ""]]}, {"id": "1312.1666", "submitter": "Jakub Kone\\v{c}n\\'y", "authors": "Jakub Kone\\v{c}n\\'y and Peter Richt\\'arik", "title": "Semi-Stochastic Gradient Descent Methods", "comments": "19 pages, 3 figures, 2 algorithms, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of minimizing the average of a large\nnumber ($n$) of smooth convex loss functions. We propose a new method, S2GD\n(Semi-Stochastic Gradient Descent), which runs for one or several epochs in\neach of which a single full gradient and a random number of stochastic\ngradients is computed, following a geometric law. The total work needed for the\nmethod to output an $\\varepsilon$-accurate solution in expectation, measured in\nthe number of passes over data, or equivalently, in units equivalent to the\ncomputation of a single gradient of the loss, is\n$O((\\kappa/n)\\log(1/\\varepsilon))$, where $\\kappa$ is the condition number.\nThis is achieved by running the method for $O(\\log(1/\\varepsilon))$ epochs,\nwith a single gradient evaluation and $O(\\kappa)$ stochastic gradient\nevaluations in each. The SVRG method of Johnson and Zhang arises as a special\ncase. If our method is limited to a single epoch only, it needs to evaluate at\nmost $O((\\kappa/\\varepsilon)\\log(1/\\varepsilon))$ stochastic gradients. In\ncontrast, SVRG requires $O(\\kappa/\\varepsilon^2)$ stochastic gradients. To\nillustrate our theoretical results, S2GD only needs the workload equivalent to\nabout 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution for\na problem with $n=10^9$ and $\\kappa=10^3$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 20:04:52 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 05:05:40 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1312.2266", "submitter": "Timo Heister", "authors": "Wolfgang Bangerth, Timo Heister, Luca Heltai, Guido Kanschat, Martin\n  Kronbichler, Matthias Maier, Bruno Turcksin, Toby D. Young", "title": "The deal.II Library, Version 8.1", "comments": "v4: for deal.II version 8.1 v3: minor fixes. v2: correct the citation\n  inside the article", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an overview of the new features of the finite element\nlibrary deal.II version 8.1.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 21:31:14 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2013 19:25:04 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2013 10:00:08 GMT"}, {"version": "v4", "created": "Tue, 31 Dec 2013 12:47:20 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bangerth", "Wolfgang", ""], ["Heister", "Timo", ""], ["Heltai", "Luca", ""], ["Kanschat", "Guido", ""], ["Kronbichler", "Martin", ""], ["Maier", "Matthias", ""], ["Turcksin", "Bruno", ""], ["Young", "Toby D.", ""]]}, {"id": "1312.2333", "submitter": "James Elliott", "authors": "James Elliott, Mark Hoemmen, Frank Mueller", "title": "Exploiting Data Representation for Fault Tolerance", "comments": null, "journal-ref": null, "doi": "10.1016/j.jocs.2015.12.002", "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the link between data representation and soft errors in dot\nproducts. We present an analytic model for the absolute error introduced should\na soft error corrupt a bit in an IEEE-754 floating-point number. We show how\nthis finding relates to the fundamental linear algebra concepts of\nnormalization and matrix equilibration. We present a case study illustrating\nthat the probability of experiencing a large error in a dot product is\nminimized when both vectors are normalized. Furthermore, when data is\nnormalized we show that the absolute error is less than one or very large,\nwhich allows us to detect large errors. We demonstrate how this finding can be\nused by instrumenting the GMRES iterative solver. We count all possible errors\nthat can be introduced through faults in arithmetic in the computationally\nintensive orthogonalization phase, and show that when scaling is used the\nabsolute error can be bounded above by one.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 08:11:12 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Elliott", "James", ""], ["Hoemmen", "Mark", ""], ["Mueller", "Frank", ""]]}, {"id": "1312.2674", "submitter": "Austin Benson", "authors": "Austin R. Benson, Sven Schmit, Robert Schreiber", "title": "Silent error detection in numerical time-stepping schemes", "comments": null, "journal-ref": "The International Journal of High Performance Computing\n  Applications, 29(4), 2015", "doi": "10.1177/1094342014532297", "report-no": null, "categories": "cs.NA cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Errors due to hardware or low level software problems, if detected, can be\nfixed by various schemes, such as recomputation from a checkpoint. Silent\nerrors are errors in application state that have escaped low-level error\ndetection. At extreme scale, where machines can perform astronomically many\noperations per second, silent errors threaten the validity of computed results.\n  We propose a new paradigm for detecting silent errors at the application\nlevel. Our central idea is to frequently compare computed values to those\nprovided by a cheap checking computation, and to build error detectors based on\nthe difference between the two output sequences. Numerical analysis provides us\nwith usable checking computations for the solution of initial-value problems in\nODEs and PDEs, arguably the most common problems in computational science.\nHere, we provide, optimize, and test methods based on Runge-Kutta and linear\nmultistep methods for ODEs, and on implicit and explicit finite difference\nschemes for PDEs. We take the heat equation and Navier-Stokes equations as\nexamples. In tests with artificially injected errors, this approach effectively\ndetects almost all meaningful errors, without significant slowdown.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 05:31:23 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Benson", "Austin R.", ""], ["Schmit", "Sven", ""], ["Schreiber", "Robert", ""]]}, {"id": "1312.3300", "submitter": "Nathalie Revol", "authors": "Nathalie Revol (Inria Grenoble Rh\\^one-Alpes / LIP Laboratoire de\n  l'Informatique du Parall\\'elisme), Philippe Th\\'eveny (Inria Grenoble\n  Rh\\^one-Alpes / LIP Laboratoire de l'Informatique du Parall\\'elisme, LIP)", "title": "Numerical Reproducibility and Parallel Computations: Issues for Interval\n  Algorithms", "comments": "submitted to IEEE Transactions on Computers", "journal-ref": "IEEE Transactions on Computers (2014)", "doi": "10.1109/TC.2014.2322593", "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is called \"numerical reproducibility\" is the problem of getting the same\nresult when the scientific computation is run several times, either on the same\nmachine or on different machines, with different types and numbers of\nprocessing units, execution environments, computational loads etc. This problem\nis especially stringent for HPC numerical simulations. In what follows, the\nfocus is on parallel implementations of interval arithmetic using\nfloating-point arithmetic. For interval computations, numerical reproducibility\nis of course an issue for testing and debugging purposes. However, as long as\nthe computed result encloses the exact and unknown result, the inclusion\nproperty, which is the main property of interval arithmetic, is satisfied and\ngetting bit for bit identical results may not be crucial. Still, implementation\nissues may invalidate the inclusion property. Several ways to preserve the\ninclusion property are presented, on the example of the product of matrices\nwith interval coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 20:09:33 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Revol", "Nathalie", "", "Inria Grenoble Rh\u00f4ne-Alpes / LIP Laboratoire de\n  l'Informatique du Parall\u00e9lisme"], ["Th\u00e9veny", "Philippe", "", "Inria Grenoble\n  Rh\u00f4ne-Alpes / LIP Laboratoire de l'Informatique du Parall\u00e9lisme, LIP"]]}, {"id": "1312.3618", "submitter": "James Bellamy", "authors": "James Bellamy", "title": "Randomness of D Sequences via Diehard Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comparison of the quality of randomness of D sequences\nbased on diehard tests. Since D sequences can model any random sequence, this\ncomparison is of value beyond this specific class.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 20:40:13 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Bellamy", "James", ""]]}, {"id": "1312.4587", "submitter": "Jingwei Lu", "authors": "Jingwei Lu, Pengwen Chen, Chin-Chih Chang, Lu Sha, Dennis Jen-Hsin\n  Huang, Chin-Chi Teng, Chung-Kuan Cheng", "title": "FFTPL: An Analytic Placement Algorithm Using Fast Fourier Transform for\n  Density Equalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flat nonlinear placement algorithm FFTPL using fast Fourier\ntransform for density equalization. The placement instance is modeled as an\nelectrostatic system with the analogy of density cost to the potential energy.\nA well-defined Poisson's equation is proposed for gradient and cost\ncomputation. Our placer outperforms state-of-the-art placers with better\nsolution quality and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 23:01:46 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Lu", "Jingwei", ""], ["Chen", "Pengwen", ""], ["Chang", "Chin-Chih", ""], ["Sha", "Lu", ""], ["Huang", "Dennis Jen-Hsin", ""], ["Teng", "Chin-Chi", ""], ["Cheng", "Chung-Kuan", ""]]}, {"id": "1312.4917", "submitter": "Pierre Lescanne", "authors": "Pierre Lescanne (LIP)", "title": "An exercise on streams: convergence acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents convergence acceleration, a method for computing\nefficiently the limit of numerical sequences as a typical application of\nstreams and higher-order functions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 19:53:28 GMT"}, {"version": "v2", "created": "Mon, 3 Mar 2014 19:43:27 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Lescanne", "Pierre", "", "LIP"]]}, {"id": "1312.5799", "submitter": "Peter Richtarik", "authors": "Olivier Fercoq and Peter Richt\\'arik", "title": "Accelerated, Parallel and Proximal Coordinate Descent", "comments": "25 pages, 2 algorithms, 6 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new stochastic coordinate descent method for minimizing the sum\nof convex functions each of which depends on a small number of coordinates\nonly. Our method (APPROX) is simultaneously Accelerated, Parallel and PROXimal;\nthis is the first time such a method is proposed. In the special case when the\nnumber of processors is equal to the number of coordinates, the method\nconverges at the rate $2\\bar{\\omega}\\bar{L} R^2/(k+1)^2 $, where $k$ is the\niteration counter, $\\bar{\\omega}$ is an average degree of separability of the\nloss function, $\\bar{L}$ is the average of Lipschitz constants associated with\nthe coordinates and individual functions in the sum, and $R$ is the distance of\nthe initial point from the minimizer. We show that the method can be\nimplemented without the need to perform full-dimensional vector operations,\nwhich is the major bottleneck of existing accelerated coordinate descent\nmethods. The fact that the method depends on the average degree of\nseparability, and not on the maximum degree of separability, can be attributed\nto the use of new safe large stepsizes, leading to improved expected separable\noverapproximation (ESO). These are of independent interest and can be utilized\nin all existing parallel stochastic coordinate descent algorithms based on the\nconcept of ESO.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 03:00:15 GMT"}, {"version": "v2", "created": "Sat, 1 Mar 2014 17:04:59 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Fercoq", "Olivier", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1312.6155", "submitter": "Stefan Ratschan", "authors": "Milan Hlad\\'ik, Stefan Ratschan", "title": "Efficient Solution of a Class of Quantified Constraints with Quantifier\n  Prefix Exists-Forall", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various applications the search for certificates for certain properties\n(e.g., stability of dynamical systems, program termination) can be formulated\nas a quantified constraint solving problem with quantifier prefix\nexists-forall. In this paper, we present an algorithm for solving a certain\nclass of such problems based on interval techniques in combination with\nconservative linear programming approximation. In comparison with previous\nwork, the method is more general - allowing general Boolean structure in the\ninput constraint, and more efficient - using splitting heuristics that learn\nfrom the success of previous linear programming approximations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:41:18 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 15:02:33 GMT"}, {"version": "v3", "created": "Tue, 24 Jun 2014 20:07:13 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Hlad\u00edk", "Milan", ""], ["Ratschan", "Stefan", ""]]}, {"id": "1312.6182", "submitter": "Weifeng Liu", "authors": "W. Liu, H. Zhang, D. Tao, Y. Wang, K. Lu", "title": "Large-Scale Paralleled Sparse Principal Component Analysis", "comments": "submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a statistical technique commonly used\nin multivariate data analysis. However, PCA can be difficult to interpret and\nexplain since the principal components (PCs) are linear combinations of the\noriginal variables. Sparse PCA (SPCA) aims to balance statistical fidelity and\ninterpretability by approximating sparse PCs whose projections capture the\nmaximal variance of original data. In this paper we present an efficient and\nparalleled method of SPCA using graphics processing units (GPUs), which can\nprocess large blocks of data in parallel. Specifically, we construct parallel\nimplementations of the four optimization formulations of the generalized power\nmethod of SPCA (GP-SPCA), one of the most efficient and effective SPCA\napproaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS)\nis up to eleven times faster than the corresponding CPU implementation (using\nCBLAS), and up to 107 times faster than a MatLab implementation. Extensive\ncomparative experiments in several real-world datasets confirm that SPCA offers\na practical advantage.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 00:38:02 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Liu", "W.", ""], ["Zhang", "H.", ""], ["Tao", "D.", ""], ["Wang", "Y.", ""], ["Lu", "K.", ""]]}, {"id": "1312.6488", "submitter": "Zheng Li", "authors": "Zheng Li and Liam O'Brien and Rajiv Ranjan and Miranda Zhang", "title": "Early Observations on Performance of Google Compute Engine for\n  Scientific Computing", "comments": "Proceedings of the 5th International Conference on Cloud Computing\n  Technologies and Science (CloudCom 2013), pp. 1-8, Bristol, UK, December 2-5,\n  2013", "journal-ref": null, "doi": "10.1109/CloudCom.2013.7", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Cloud computing emerged for business applications in industry,\npublic Cloud services have been widely accepted and encouraged for scientific\ncomputing in academia. The recently available Google Compute Engine (GCE) is\nclaimed to support high-performance and computationally intensive tasks, while\nlittle evaluation studies can be found to reveal GCE's scientific capabilities.\nConsidering that fundamental performance benchmarking is the strategy of\nearly-stage evaluation of new Cloud services, we followed the Cloud Evaluation\nExperiment Methodology (CEEM) to benchmark GCE and also compare it with Amazon\nEC2, to help understand the elementary capability of GCE for dealing with\nscientific problems. The experimental results and analyses show both potential\nadvantages of, and possible threats to applying GCE to scientific computing.\nFor example, compared to Amazon's EC2 service, GCE may better suit applications\nthat require frequent disk operations, while it may not be ready yet for single\nVM-based parallel computing. Following the same evaluation methodology,\ndifferent evaluators can replicate and/or supplement this fundamental\nevaluation of GCE. Based on the fundamental evaluation results, suitable GCE\nenvironments can be further established for case studies of solving real\nscience problems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 09:12:09 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Li", "Zheng", ""], ["O'Brien", "Liam", ""], ["Ranjan", "Rajiv", ""], ["Zhang", "Miranda", ""]]}, {"id": "1312.6872", "submitter": "Anupriya Gogna", "authors": "Anupriya Gogna, Ankita Shukla and Angshul Majumdar", "title": "Matrix recovery using Split Bregman", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of recovering a matrix, with inherent\nlow rank structure, from its lower dimensional projections. This problem is\nfrequently encountered in wide range of areas including pattern recognition,\nwireless sensor networks, control systems, recommender systems, image/video\nreconstruction etc. Both in theory and practice, the most optimal way to solve\nthe low rank matrix recovery problem is via nuclear norm minimization. In this\npaper, we propose a Split Bregman algorithm for nuclear norm minimization. The\nuse of Bregman technique improves the convergence speed of our algorithm and\ngives a higher success rate. Also, the accuracy of reconstruction is much\nbetter even for cases where small number of linear measurements are available.\nOur claim is supported by empirical results obtained using our algorithm and\nits comparison to other existing methods for matrix recovery. The algorithms\nare compared on the basis of NMSE, execution time and success rate for varying\nranks and sampling ratios.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 15:12:48 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Gogna", "Anupriya", ""], ["Shukla", "Ankita", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1312.7279", "submitter": "Pierre-Jean Spaenlehauer", "authors": "\\'Eric Schost and Pierre-Jean Spaenlehauer", "title": "A Quadratically Convergent Algorithm for Structured Low-Rank\n  Approximation", "comments": "37 pages, Maple package available at\n  http://www.pjspaenlehauer.net/data/software/NewtonSLRA_notes.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.SC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured Low-Rank Approximation is a problem arising in a wide range of\napplications in Numerical Analysis and Engineering Sciences. Given an input\nmatrix $M$, the goal is to compute a matrix $M'$ of given rank $r$ in a linear\nor affine subspace $E$ of matrices (usually encoding a specific structure) such\nthat the Frobenius distance $\\lVert M-M'\\rVert$ is small. We propose a\nNewton-like iteration for solving this problem, whose main feature is that it\nconverges locally quadratically to such a matrix under mild transversality\nassumptions between the manifold of matrices of rank $r$ and the linear/affine\nsubspace $E$. We also show that the distance between the limit of the iteration\nand the optimal solution of the problem is quadratic in the distance between\nthe input matrix and the manifold of rank $r$ matrices in $E$. To illustrate\nthe applicability of this algorithm, we propose a Maple implementation and give\nexperimental results for several applicative problems that can be modeled by\nStructured Low-Rank Approximation: univariate approximate GCDs (Sylvester\nmatrices), low-rank Matrix completion (coordinate spaces) and denoising\nprocedures (Hankel matrices). Experimental results give evidence that this\nall-purpose algorithm is competitive with state-of-the-art numerical methods\ndedicated to these problems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 15:17:39 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 09:20:30 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Schost", "\u00c9ric", ""], ["Spaenlehauer", "Pierre-Jean", ""]]}, {"id": "1312.7637", "submitter": "Bijeesh Tv", "authors": "B. Premjith, S. Sachin Kumar, Akhil Manikkoth, T V Bijeesh, K P Soman", "title": "Insight into Primal Augmented Lagrangian Multilplier Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simplified form of Primal Augmented Lagrange Multiplier\nalgorithm. We intend to fill the gap in the steps involved in the mathematical\nderivations of the algorithm so that an insight into the algorithm is made. The\nexperiment is focused to show the reconstruction done using this algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 06:09:28 GMT"}], "update_date": "2014-01-02", "authors_parsed": [["Premjith", "B.", ""], ["Kumar", "S. Sachin", ""], ["Manikkoth", "Akhil", ""], ["Bijeesh", "T V", ""], ["Soman", "K P", ""]]}, {"id": "1312.7852", "submitter": "Christiaan Erdbrink", "authors": "C.D. Erdbrink, V.V. Krzhizhanovskaya, P.M.A. Sloot", "title": "Evolutionary Design of Numerical Methods: Generating Finite Difference\n  and Integration Schemes by Differential Evolution", "comments": "19 pages, 7 figures, 10 tables, 4 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical and new numerical schemes are generated using evolutionary\ncomputing. Differential Evolution is used to find the coefficients of finite\ndifference approximations of function derivatives, and of single and multi-step\nintegration methods. The coefficients are reverse engineered based on samples\nfrom a target function and its derivative used for training. The Runge-Kutta\nschemes are trained using the order condition equations. An appealing feature\nof the evolutionary method is the low number of model parameters. The\npopulation size, termination criterion and number of training points are\ndetermined in a sensitivity analysis. Computational results show good agreement\nbetween evolved and analytical coefficients. In particular, a new fifth-order\nRunge-Kutta scheme is computed which adheres to the order conditions with a sum\nof absolute errors of order 10^-14. Execution of the evolved schemes proved the\nintended orders of accuracy. The outcome of this study is valuable for future\ndevelopments in the design of complex numerical methods that are out of reach\nby conventional means.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:21:44 GMT"}], "update_date": "2014-01-02", "authors_parsed": [["Erdbrink", "C. D.", ""], ["Krzhizhanovskaya", "V. V.", ""], ["Sloot", "P. M. A.", ""]]}]