[{"id": "1407.0013", "submitter": "Martin Sundin", "authors": "Martin Sundin, Saikat Chatterjee, Magnus Jansson and Cristian R. Rojas", "title": "Relevance Singular Vector Machine for low-rank matrix sensing", "comments": "International Conference on Signal Processing and Communications\n  (SPCOM), 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a new Bayesian inference method for low rank matrix\nreconstruction. We call the new method the Relevance Singular Vector Machine\n(RSVM) where appropriate priors are defined on the singular vectors of the\nunderlying matrix to promote low rank. To accelerate computations, a\nnumerically efficient approximation is developed. The proposed algorithms are\napplied to matrix completion and matrix reconstruction problems and their\nperformance is studied numerically.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 12:19:17 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Sundin", "Martin", ""], ["Chatterjee", "Saikat", ""], ["Jansson", "Magnus", ""], ["Rojas", "Cristian R.", ""]]}, {"id": "1407.0286", "submitter": "Hoai An Le Thi", "authors": "Hoai An Le Thi, Tao Pham Dinh, Hoai Minh Le, Xuan Thanh Vo", "title": "DC approximation approaches for sparse optimization", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse optimization refers to an optimization problem involving the zero-norm\nin objective or constraints. In this paper, nonconvex approximation approaches\nfor sparse optimization have been studied with a unifying point of view in DC\n(Difference of Convex functions) programming framework. Considering a common DC\napproximation of the zero-norm including all standard sparse inducing penalty\nfunctions, we studied the consistency between global minimums (resp. local\nminimums) of approximate and original problems. We showed that, in several\ncases, some global minimizers (resp. local minimizers) of the approximate\nproblem are also those of the original problem. Using exact penalty techniques\nin DC programming, we proved stronger results for some particular\napproximations, namely, the approximate problem, with suitable parameters, is\nequivalent to the original problem. The efficiency of several sparse inducing\npenalty functions have been fully analyzed. Four DCA (DC Algorithm) schemes\nwere developed that cover all standard algorithms in nonconvex sparse\napproximation approaches as special versions. They can be viewed as, an $\\ell\n_{1}$-perturbed algorithm / reweighted-$\\ell _{1}$ algorithm / reweighted-$\\ell\n_{1}$ algorithm. We offer a unifying nonconvex approximation approach, with\nsolid theoretical tools as well as efficient algorithms based on DC programming\nand DCA, to tackle the zero-norm and sparse optimization. As an application, we\nimplemented our methods for the feature selection in SVM (Support Vector\nMachine) problem and performed empirical comparative numerical experiments on\nthe proposed algorithms with various approximation functions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 15:45:05 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 08:28:33 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Thi", "Hoai An Le", ""], ["Dinh", "Tao Pham", ""], ["Le", "Hoai Minh", ""], ["Vo", "Xuan Thanh", ""]]}, {"id": "1407.0898", "submitter": "Franck Iutzeler", "authors": "Pascal Bianchi, Walid Hachem and Franck Iutzeler", "title": "A Coordinate Descent Primal-Dual Algorithm and Application to\n  Distributed Asynchronous Optimization", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the idea of randomized coordinate descent of $\\alpha$-averaged\noperators, a randomized primal-dual optimization algorithm is introduced, where\na random subset of coordinates is updated at each iteration. The algorithm\nbuilds upon a variant of a recent (deterministic) algorithm proposed by V\\~u\nand Condat that includes the well known ADMM as a particular case. The obtained\nalgorithm is used to solve asynchronously a distributed optimization problem. A\nnetwork of agents, each having a separate cost function containing a\ndifferentiable term, seek to find a consensus on the minimum of the aggregate\nobjective. The method yields an algorithm where at each iteration, a random\nsubset of agents wake up, update their local estimates, exchange some data with\ntheir neighbors, and go idle. Numerical results demonstrate the attractive\nperformance of the method. The general approach can be naturally adapted to\nother situations where coordinate descent convex optimization algorithms are\nused with a random choice of the coordinates.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 12:51:58 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 17:03:02 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2015 17:50:25 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Bianchi", "Pascal", ""], ["Hachem", "Walid", ""], ["Iutzeler", "Franck", ""]]}, {"id": "1407.1061", "submitter": "John Jakeman", "authors": "John D. Jakeman and Timothy Wildey", "title": "Enhancing adaptive sparse grid approximations and improving refinement\n  strategies using adjoint-based a posteriori error estimates", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2014.09.014", "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an algorithm for adaptive sparse grid approximations\nof quantities of interest computed from discretized partial differential\nequations. We use adjoint-based a posteriori error estimates of the physical\ndiscretization error and the interpolation error in the sparse grid to enhance\nthe sparse grid approximation and to drive adaptivity of the sparse grid.\nUtilizing these error estimates provides significantly more accurate functional\nvalues for random samples of the sparse grid approximation. We also demonstrate\nthat alternative refinement strategies based upon a posteriori error estimates\ncan lead to further increases in accuracy in the approximation over traditional\nhierarchical surplus based strategies. Throughout this paper we also provide\nand test a framework for balancing the physical discretization error with the\nstochastic interpolation error of the enhanced sparse grid approximation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 21:07:54 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Jakeman", "John D.", ""], ["Wildey", "Timothy", ""]]}, {"id": "1407.1399", "submitter": "Fanhua Shang", "authors": "Fanhua Shang and Yuanyuan Liu and James Cheng", "title": "Generalized Higher-Order Tensor Decomposition via Parallel ADMM", "comments": "9 pages, 5 figures, AAAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order tensors are becoming prevalent in many scientific areas such as\ncomputer vision, social network analysis, data mining and neuroscience.\nTraditional tensor decomposition approaches face three major challenges: model\nselecting, gross corruptions and computational efficiency. To address these\nproblems, we first propose a parallel trace norm regularized tensor\ndecomposition method, and formulate it as a convex optimization problem. This\nmethod does not require the rank of each mode to be specified beforehand, and\ncan automatically determine the number of factors in each mode through our\noptimization scheme. By considering the low-rank structure of the observed\ntensor, we analyze the equivalent relationship of the trace norm between a\nlow-rank tensor and its core tensor. Then, we cast a non-convex tensor\ndecomposition model into a weighted combination of multiple much smaller-scale\nmatrix trace norm minimization. Finally, we develop two parallel alternating\ndirection methods of multipliers (ADMM) to solve our problems. Experimental\nresults verify that our regularized formulation is effective, and our methods\nare robust to noise or outliers.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jul 2014 11:58:30 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Cheng", "James", ""]]}, {"id": "1407.1537", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Lorenzo Orecchia", "title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent", "comments": "A new section added; polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order methods play a central role in large-scale machine learning. Even\nthough many variations exist, each suited to a particular problem, almost all\nsuch methods fundamentally rely on two types of algorithmic steps: gradient\ndescent, which yields primal progress, and mirror descent, which yields dual\nprogress.\n  We observe that the performances of gradient and mirror descent are\ncomplementary, so that faster algorithms can be designed by LINEARLY COUPLING\nthe two. We show how to reconstruct Nesterov's accelerated gradient methods\nusing linear coupling, which gives a cleaner interpretation than Nesterov's\noriginal proofs. We also discuss the power of linear coupling by extending it\nto many other settings that Nesterov's methods cannot apply to.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:11:48 GMT"}, {"version": "v2", "created": "Sat, 9 Aug 2014 01:48:01 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 06:59:10 GMT"}, {"version": "v4", "created": "Fri, 2 Jan 2015 17:41:24 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 19:30:37 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1407.1572", "submitter": "Sivaram Ambikasaran", "authors": "Sivaram Ambikasaran and Eric Darve", "title": "The Inverse Fast Multipole Method", "comments": "25 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a new fast direct solver for linear systems arising\nout of wide range of applications, integral equations, multivariate statistics,\nradial basis interpolation, etc., to name a few. \\emph{The highlight of this\nnew fast direct solver is that the solver scales linearly in the number of\nunknowns in all dimensions.} The solver, termed as Inverse Fast Multipole\nMethod (abbreviated as IFMM), works on the same data-structure as the Fast\nMultipole Method (abbreviated as FMM). More generally, the solver can be\nimmediately extended to the class of hierarchical matrices, denoted as\n$\\mathcal{H}^2$ matrices with strong admissibility criteria (weak low-rank\nstructure), i.e., \\emph{the interaction between neighboring cluster of\nparticles is full-rank whereas the interaction between particles corresponding\nto well-separated clusters can be efficiently represented as a low-rank\nmatrix}. The algorithm departs from existing approaches in the fact that\nthroughout the algorithm the interaction corresponding to neighboring clusters\nare always treated as full-rank interactions. Our approach relies on two major\nideas: (i) The $N \\times N$ matrix arising out of FMM (from now on termed as\nFMM matrix) can be represented as an extended sparser matrix of size $M \\times\nM$, where $M \\approx 3N$. (ii) While solving the larger extended sparser\nmatrix, \\emph{the fill-in's that arise in the matrix blocks corresponding to\nwell-separated clusters are hierarchically compressed}. The ordering of the\nequations and the unknowns in the extended sparser matrix is strongly related\nto the local and multipole coefficients in the FMM~\\cite{greengard1987fast} and\n\\emph{the order of elimination is different from the usual nested dissection\napproach}. Numerical benchmarks on $2$D manifold confirm the linear scaling of\nthe algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 03:16:40 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Ambikasaran", "Sivaram", ""], ["Darve", "Eric", ""]]}, {"id": "1407.1593", "submitter": "Kim Batselier", "authors": "Kim Batselier, Haotian Liu, Ngai Wong", "title": "A Constructive Algorithm for Decomposing a Tensor into a Finite Sum of\n  Orthonormal Rank-1 Terms", "comments": "Added subsection on orthogonal complement tensors. Added constructive\n  proof of maximal CP-rank of a 2x2x2 tensor. Added perturbation of singular\n  values result. Added conversion of the TTr1 decomposition to the Tucker\n  decomposition. Added example that demonstrates how the rank behaves when\n  subtracting rank-1 terms. Added example with exponential decaying singular\n  values", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a constructive algorithm that decomposes an arbitrary real tensor\ninto a finite sum of orthonormal rank-1 outer products. The algorithm, named\nTTr1SVD, works by converting the tensor into a tensor-train rank-1 (TTr1)\nseries via the singular value decomposition (SVD). TTr1SVD naturally\ngeneralizes the SVD to the tensor regime with properties such as uniqueness for\na fixed order of indices, orthogonal rank-1 outer product terms, and easy\ntruncation error quantification. Using an outer product column table it also\nallows, for the first time, a complete characterization of all tensors\northogonal with the original tensor. Incidentally, this leads to a strikingly\nsimple constructive proof showing that the maximum rank of a real $2 \\times 2\n\\times 2$ tensor over the real field is 3. We also derive a conversion of the\nTTr1 decomposition into a Tucker decomposition with a sparse core tensor.\nNumerical examples illustrate each of the favorable properties of the TTr1\ndecomposition.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 06:42:57 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 03:27:11 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Batselier", "Kim", ""], ["Liu", "Haotian", ""], ["Wong", "Ngai", ""]]}, {"id": "1407.1723", "submitter": "Michael Moeller", "authors": "Thomas M\\\"ollenhoff and Evgeny Strekalovskiy and Michael Moeller and\n  Daniel Cremers", "title": "The Primal-Dual Hybrid Gradient Method for Semiconvex Splittings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the analysis of a recent reformulation of the\nprimal-dual hybrid gradient method [Zhu and Chan 2008, Pock, Cremers, Bischof\nand Chambolle 2009, Esser, Zhang and Chan 2010, Chambolle and Pock 2011], which\nallows to apply it to nonconvex regularizers as first proposed for truncated\nquadratic penalization in [Strekalovskiy and Cremers 2014]. Particularly, it\ninvestigates variational problems for which the energy to be minimized can be\nwritten as $G(u) + F(Ku)$, where $G$ is convex, $F$ semiconvex, and $K$ is a\nlinear operator. We study the method and prove convergence in the case where\nthe nonconvexity of $F$ is compensated by the strong convexity of the $G$. The\nconvergence proof yields an interesting requirement for the choice of algorithm\nparameters, which we show to not only be sufficient, but necessary.\nAdditionally, we show boundedness of the iterates under much weaker conditions.\nFinally, we demonstrate effectiveness and convergence of the algorithm beyond\nthe theoretical guarantees in several numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 14:12:25 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["M\u00f6llenhoff", "Thomas", ""], ["Strekalovskiy", "Evgeny", ""], ["Moeller", "Michael", ""], ["Cremers", "Daniel", ""]]}, {"id": "1407.1925", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Lorenzo Orecchia", "title": "Using Optimization to Solve Positive LPs Faster in Parallel", "comments": "polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive linear programs (LP), also known as packing and covering linear\nprograms, are an important class of problems that bridges computer science,\noperations research, and optimization. Despite the consistent efforts on this\nproblem, all known nearly-linear-time algorithms require\n$\\tilde{O}(\\varepsilon^{-4})$ iterations to converge to $1\\pm \\varepsilon$\napproximate solutions. This $\\varepsilon^{-4}$ dependence has not been improved\nsince 1993, and limits the performance of parallel implementations for such\nalgorithms. Moreover, previous algorithms and their analyses rely on update\nsteps and convergence arguments that are combinatorial in nature and do not\nseem to arise naturally from an optimization viewpoint.\n  In this paper, we leverage new insights from optimization theory to construct\na novel algorithm that breaks the longstanding $\\varepsilon^{-4}$ barrier. Our\nalgorithm has a simple analysis and a clear motivation. Our work introduces a\nnumber of novel techniques, such as the combined application of gradient\ndescent and mirror descent, and a truncated, smoothed version of the standard\nmultiplicative weight update, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 01:58:11 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 07:03:14 GMT"}, {"version": "v3", "created": "Sun, 13 Nov 2016 05:23:54 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1407.2337", "submitter": "Hong Zhang", "authors": "Hong Zhang, Adrian Sandu, Sebastien Blaise", "title": "High Order Implicit-Explicit General Linear Methods with Optimized\n  Stability Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the numerical solution of partial differential equations using a\nmethod-of-lines approach, the availability of high order spatial discretization\nschemes motivates the development of sophisticated high order time integration\nmethods. For multiphysics problems with both stiff and non-stiff terms\nimplicit-explicit (IMEX) time stepping methods attempt to combine the lower\ncost advantage of explicit schemes with the favorable stability properties of\nimplicit schemes. Existing high order IMEX Runge Kutta or linear multistep\nmethods, however, suffer from accuracy or stability reduction.\n  This work shows that IMEX general linear methods (GLMs) are competitive\nalternatives to classic IMEX schemes for large problems arising in practice.\nHigh order IMEX-GLMs are constructed in the framework developed by the authors\n[34]. The stability regions of the new schemes are optimized numerically. The\nresulting IMEX-GLMs have similar stability properties as IMEX Runge-Kutta\nmethods, but they do not suffer from order reduction, and are superior in terms\nof accuracy and efficiency. Numerical experiments with two and three\ndimensional test problems illustrate the potential of the new schemes to speed\nup complex applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 02:21:46 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Zhang", "Hong", ""], ["Sandu", "Adrian", ""], ["Blaise", "Sebastien", ""]]}, {"id": "1407.2802", "submitter": "Marc Mezzarobba", "authors": "Alexandre Benoit, Mioara Joldes (LAAS), Marc Mezzarobba (LIP6)", "title": "Rigorous uniform approximation of D-finite functions using Chebyshev\n  expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of numerical methods exists for computing polynomial\napproximations of solutions of ordinary differential equations based on\nChebyshev series expansions or Chebyshev interpolation polynomials. We consider\nthe application of such methods in the context of rigorous computing (where we\nneed guarantees on the accuracy of the result), and from the complexity point\nof view. It is well-known that the order-n truncation of the Chebyshev\nexpansion of a function over a given interval is a near-best uniform polynomial\napproximation of the function on that interval. In the case of solutions of\nlinear differential equations with polynomial coefficients, the coefficients of\nthe expansions obey linear recurrence relations with polynomial coefficients.\nUnfortunately, these recurrences do not lend themselves to a direct recursive\ncomputation of the coefficients, owing among other things to a lack of initial\nconditions. We show how they can nevertheless be used, as part of a validated\nprocess, to compute good uniform approximations of D-finite functions together\nwith rigorous error bounds, and we study the complexity of the resulting\nalgorithms. Our approach is based on a new view of a classical numerical method\ngoing back to Clenshaw, combined with a functional enclosure method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 14:27:23 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Benoit", "Alexandre", "", "LAAS"], ["Joldes", "Mioara", "", "LAAS"], ["Mezzarobba", "Marc", "", "LIP6"]]}, {"id": "1407.3124", "submitter": "Andrzej  Cichocki", "authors": "Andrzej Cichocki", "title": "Tensor Networks for Big Data Analytics and Large-Scale Optimization\n  Problems", "comments": "arXiv admin note: text overlap with arXiv:1403.2048", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review basic and emerging models and associated algorithms\nfor large-scale tensor networks, especially Tensor Train (TT) decompositions\nusing novel mathematical and graphical representations. We discus the concept\nof tensorization (i.e., creating very high-order tensors from lower-order\noriginal data) and super compression of data achieved via quantized tensor\ntrain (QTT) networks. The purpose of a tensorization and quantization is to\nachieve, via low-rank tensor approximations \"super\" compression, and\nmeaningful, compact representation of structured data. The main objective of\nthis paper is to show how tensor networks can be used to solve a wide class of\nbig data optimization problems (that are far from tractable by classical\nnumerical methods) by applying tensorization and performing all operations\nusing relatively small size matrices and tensors and applying iteratively\noptimized and approximative tensor contractions.\n  Keywords: Tensor networks, tensor train (TT) decompositions, matrix product\nstates (MPS), matrix product operators (MPO), basic tensor operations,\ntensorization, distributed representation od data optimization problems for\nvery large-scale problems: generalized eigenvalue decomposition (GEVD),\nPCA/SVD, canonical correlation analysis (CCA).\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 12:08:14 GMT"}, {"version": "v2", "created": "Fri, 22 Aug 2014 11:31:02 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Cichocki", "Andrzej", ""]]}, {"id": "1407.3189", "submitter": "Lukas Einkemmer", "authors": "Lukas Einkemmer", "title": "A modern resistive magnetohydrodynamics solver using C++ and the Boost\n  library", "comments": null, "journal-ref": "Computer Physics Communications, Volume 206, September 2016, Pages\n  69-77", "doi": "10.1016/j.cpc.2016.04.015", "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the implementation of our C++ resistive\nmagnetohydrodynamics solver. The framework developed facilitates the separation\nof the code implementing the specific numerical method and the physical model,\non the one hand, from the handling of boundary conditions and the management of\nthe computational domain, on the other hand. In particular, this will allow us\nto use finite difference stencils which are only defined in the interior of the\ndomain (the boundary conditions are handled automatically). We will discuss\nthis and other design considerations and their impact on performance in some\ndetail. In addition, we provide a documentation of the code developed and\ndemonstrate that a performance comparable to Fortran can be achieved, while\nstill maintaining a maximum of code readability and extensibility.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 15:05:33 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Einkemmer", "Lukas", ""]]}, {"id": "1407.3374", "submitter": "Yuhan Jia", "authors": "Yuhan Jia, Jianping Wu, Yiman Du", "title": "An improved car-following model considering variable safety headway\n  distance", "comments": null, "journal-ref": null, "doi": "10.4006/0836-1398-27.4.616", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering high speed following on expressway or highway, an improved\ncar-following model is developed in this paper by introducing variable safety\nheadway distance. Stability analysis of the new model is carried out using the\ncontrol theory method. Finally, numerical simulations are implemented and the\nresults show good consistency with theoretical study.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 11:26:44 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Jia", "Yuhan", ""], ["Wu", "Jianping", ""], ["Du", "Yiman", ""]]}, {"id": "1407.5107", "submitter": "David Gleich", "authors": "David F. Gleich", "title": "PageRank beyond the Web", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CE cs.NA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google's PageRank method was developed to evaluate the importance of\nweb-pages via their link structure. The mathematics of PageRank, however, are\nentirely general and apply to any graph or network in any domain. Thus,\nPageRank is now regularly used in bibliometrics, social and information network\nanalysis, and for link prediction and recommendation. It's even used for\nsystems analysis of road networks, as well as biology, chemistry, neuroscience,\nand physics. We'll see the mathematics and ideas that unite these diverse\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 20:20:07 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Gleich", "David F.", ""]]}, {"id": "1407.5183", "submitter": "Manda Winlaw", "authors": "Hans De Sterck and Manda Winlaw", "title": "A Nonlinearly Preconditioned Conjugate Gradient Algorithm for Rank-R\n  Canonical Tensor Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating least squares (ALS) is often considered the workhorse algorithm\nfor computing the rank-R canonical tensor approximation, but for certain\nproblems its convergence can be very slow. The nonlinear conjugate gradient\n(NCG) method was recently proposed as an alternative to ALS, but the results\nindicated that NCG is usually not faster than ALS. To improve the convergence\nspeed of NCG, we consider a nonlinearly preconditioned nonlinear conjugate\ngradient (PNCG) algorithm for computing the rank-R canonical tensor\ndecomposition. Our approach uses ALS as a nonlinear preconditioner in the NCG\nalgorithm. Alternatively, NCG can be viewed as an acceleration process for ALS.\nWe demonstrate numerically that the convergence acceleration mechanism in PNCG\noften leads to important pay-offs for difficult tensor decomposition problems,\nwith convergence that is significantly faster and more robust than for the\nstand-alone NCG or ALS algorithms. We consider several approaches for\nincorporating the nonlinear preconditioner into the NCG algorithm that have\nbeen described in the literature previously and have met with success in\ncertain application areas. However, it appears that the nonlinearly\npreconditioned NCG approach has received relatively little attention in the\nbroader community and remains underexplored both theoretically and\nexperimentally. Thus, this paper serves several additional functions, by\nproviding in one place a concise overview of several PNCG variants and their\nproperties that have only been described in a few places scattered throughout\nthe literature, by systematically comparing the performance of these PNCG\nvariants for the tensor decomposition problem, and by drawing further attention\nto the usefulness of nonlinearly preconditioned NCG as a general tool. In\naddition, we briefly discuss the convergence of the PNCG algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jul 2014 13:23:08 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["De Sterck", "Hans", ""], ["Winlaw", "Manda", ""]]}, {"id": "1407.5516", "submitter": "Mark Embree", "authors": "D. C. Sorensen and M. Embree", "title": "A DEIM Induced CUR Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": "Rice University, Department of Computational and Applied Mathematics\n  Report TR 14-04", "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a CUR matrix factorization based on the Discrete Empirical\nInterpolation Method (DEIM). For a given matrix $A$, such a factorization\nprovides a low rank approximate decomposition of the form $A \\approx C U R$,\nwhere $C$ and $R$ are subsets of the columns and rows of $A$, and $U$ is\nconstructed to make $CUR$ a good approximation. Given a low-rank singular value\ndecomposition $A \\approx V S W^T$, the DEIM procedure uses $V$ and $W$ to\nselect the columns and rows of $A$ that form $C$ and $R$. Through an error\nanalysis applicable to a general class of CUR factorizations, we show that the\naccuracy tracks the optimal approximation error within a factor that depends on\nthe conditioning of submatrices of $V$ and $W$. For large-scale problems, $V$\nand $W$ can be approximated using an incremental QR algorithm that makes one\npass through $A$. Numerical examples illustrate the favorable performance of\nthe DEIM-CUR method, compared to CUR approximations based on leverage scores.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 14:48:27 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 21:23:07 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Sorensen", "D. C.", ""], ["Embree", "M.", ""]]}, {"id": "1407.5593", "submitter": "T. Wallace", "authors": "Tim Wallace, Ali Sekmen", "title": "Deterministic Versus Randomized Kaczmarz Iterative Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kaczmarz's alternating projection method has been widely used for solving a\nconsistent (mostly over-determined) linear system of equations Ax=b. Because of\nits simple iterative nature with light computation, this method was\nsuccessfully applied in computerized tomography. Since tomography generates a\nmatrix A with highly coherent rows, randomized Kaczmarz algorithm is expected\nto provide faster convergence as it picks a row for each iteration at random,\nbased on a certain probability distribution. It was recently shown that picking\na row at random, proportional with its norm, makes the iteration converge\nexponentially in expectation with a decay constant that depends on the scaled\ncondition number of A and not the number of equations. Since Kaczmarz's method\nis a subspace projection method, the convergence rate for simple Kaczmarz\nalgorithm was developed in terms of subspace angles. This paper provides\nanalyses of simple and randomized Kaczmarz algorithms and explain the link\nbetween them. It also propose new versions of randomization that may speed up\nconvergence.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 18:38:38 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Wallace", "Tim", ""], ["Sekmen", "Ali", ""]]}, {"id": "1407.5965", "submitter": "Steven Smith", "authors": "Steven Thomas Smith", "title": "Optimization Techniques on Riemannian Manifolds", "comments": "Hamiltonian and Gradient Flows, Algorithms, and Control, Fields\n  Institute Communications, Volume 3, AMS (1994)", "journal-ref": "Fields Institute Communications; Volume: 3; 1994", "doi": null, "report-no": "ISBN 978-0-8218-0255-7", "categories": "math.OC cs.CG cs.NA math.DG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The techniques and analysis presented in this paper provide new methods to\nsolve optimization problems posed on Riemannian manifolds. A new point of view\nis offered for the solution of constrained optimization problems. Some\nclassical optimization techniques on Euclidean space are generalized to\nRiemannian manifolds. Several algorithms are presented and their convergence\nproperties are analyzed employing the Riemannian structure of the manifold.\nSpecifically, two apparently new algorithms, which can be thought of as\nNewton's method and the conjugate gradient method on Riemannian manifolds, are\npresented and shown to possess, respectively, quadratic and superlinear\nconvergence. Examples of each method on certain Riemannian manifolds are given\nwith the results of numerical experiments. Rayleigh's quotient defined on the\nsphere is one example. It is shown that Newton's method applied to this\nfunction converges cubically, and that the Rayleigh quotient iteration is an\nefficient approximation of Newton's method. The Riemannian version of the\nconjugate gradient method applied to this function gives a new algorithm for\nfinding the eigenvectors corresponding to the extreme eigenvalues of a\nsymmetric matrix. Another example arises from extremizing the function\n$\\mathop{\\rm tr} {\\Theta}^{\\scriptscriptstyle\\rm T}Q{\\Theta}N$ on the special\northogonal group. In a similar example, it is shown that Newton's method\napplied to the sum of the squares of the off-diagonal entries of a symmetric\nmatrix converges cubically.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 18:29:37 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Smith", "Steven Thomas", ""]]}, {"id": "1407.6202", "submitter": "Svetlana Matculevich", "authors": "Svetlana Matculevich and Sergey Repin", "title": "Estimates of the distance to the exact solution of parabolic problems\n  based on local Poincar\\'e type inequalities", "comments": "Error in the specification of the area of research", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the paper is to derive two-sided bounds of the distance between\nthe exact solution of the evolutionary reaction-diffusion problem with mixed\nDirichlet--Robin boundary conditions and any function in the admissible energy\nspace.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 13:16:38 GMT"}, {"version": "v2", "created": "Fri, 25 Jul 2014 11:51:17 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Matculevich", "Svetlana", ""], ["Repin", "Sergey", ""]]}, {"id": "1407.6486", "submitter": "Daniel Ruprecht", "authors": "Michael Minion, Robert Speck, Matthias Bolten, Matthew Emmett, Daniel\n  Ruprecht", "title": "Interweaving PFASST and Parallel Multigrid", "comments": null, "journal-ref": "SIAM Journal on Scientific Computing 37(5), pp. S244-S263, 2015", "doi": "10.1137/14097536X", "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parallel full approximation scheme in space and time (PFASST) introduced\nby Emmett and Minion in 2012 is an iterative strategy for the temporal\nparallelization of ODEs and discretized PDEs. As the name suggests, PFASST is\nsimilar in spirit to a space-time FAS multigrid method performed over multiple\ntime-steps in parallel. However, since the original focus of PFASST has been on\nthe performance of the method in terms of time parallelism, the solution of any\nspatial system arising from the use of implicit or semi-implicit temporal\nmethods within PFASST have simply been assumed to be solved to some desired\naccuracy completely at each sub-step and each iteration by some unspecified\nprocedure. It hence is natural to investigate how iterative solvers in the\nspatial dimensions can be interwoven with the PFASST iterations and whether\nthis strategy leads to a more efficient overall approach. This paper presents\nan initial investigation on the relative performance of different strategies\nfor coupling PFASST iterations with multigrid methods for the implicit\ntreatment of diffusion terms in PDEs. In particular, we compare full accuracy\nmultigrid solves at each sub-step with a small fixed number of multigrid\nV-cycles. This reduces the cost of each PFASST iteration at the possible\nexpense of a corresponding increase in the number of PFASST iterations needed\nfor convergence. Parallel efficiency of the resulting methods is explored\nthrough numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 08:38:07 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 12:39:33 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Minion", "Michael", ""], ["Speck", "Robert", ""], ["Bolten", "Matthias", ""], ["Emmett", "Matthew", ""], ["Ruprecht", "Daniel", ""]]}, {"id": "1407.7299", "submitter": "Carl Meyer Dr.", "authors": "Amy N. Langville, Carl D. Meyer, Russell Albright, James Cox, David\n  Duling", "title": "Algorithms, Initializations, and Convergence for the Nonnegative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that good initializations can improve the speed and accuracy\nof the solutions of many nonnegative matrix factorization (NMF) algorithms.\nMany NMF algorithms are sensitive with respect to the initialization of W or H\nor both. This is especially true of algorithms of the alternating least squares\n(ALS) type, including the two new ALS algorithms that we present in this paper.\nWe compare the results of six initialization procedures (two standard and four\nnew) on our ALS algorithms. Lastly, we discuss the practical issue of choosing\nan appropriate convergence criterion.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 00:41:12 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Langville", "Amy N.", ""], ["Meyer", "Carl D.", ""], ["Albright", "Russell", ""], ["Cox", "James", ""], ["Duling", "David", ""]]}, {"id": "1407.7618", "submitter": "Matthew Zahr", "authors": "Matthew J. Zahr and Charbel Farhat", "title": "Progressive construction of a parametric reduced-order model for\n  PDE-constrained optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adaptive approach to using reduced-order models as surrogates in\nPDE-constrained optimization is introduced that breaks the traditional\noffline-online framework of model order reduction. A sequence of optimization\nproblems constrained by a given Reduced-Order Model (ROM) is defined with the\ngoal of converging to the solution of a given PDE-constrained optimization\nproblem. For each reduced optimization problem, the constraining ROM is trained\nfrom sampling the High-Dimensional Model (HDM) at the solution of some of the\nprevious problems in the sequence. The reduced optimization problems are\nequipped with a nonlinear trust-region based on a residual error indicator to\nkeep the optimization trajectory in a region of the parameter space where the\nROM is accurate. A technique for incorporating sensitivities into a\nReduced-Order Basis (ROB) is also presented, along with a methodology for\ncomputing sensitivities of the reduced-order model that minimizes the distance\nto the corresponding HDM sensitivity, in a suitable norm. The proposed reduced\noptimization framework is applied to subsonic aerodynamic shape optimization\nand shown to reduce the number of queries to the HDM by a factor of 4-5,\ncompared to the optimization problem solved using only the HDM, with errors in\nthe optimal solution far less than 0.1%.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 02:36:41 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Zahr", "Matthew J.", ""], ["Farhat", "Charbel", ""]]}, {"id": "1407.8078", "submitter": "Eric Polizzi", "authors": "Stefan Guettel, Eric Polizzi, Ping Tak Peter Tang, Gautier Viaud", "title": "Zolotarev Quadrature Rules and Load Balancing for the FEAST Eigensolver", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FEAST method for solving large sparse eigenproblems is equivalent to\nsubspace iteration with an approximate spectral projector and implicit\northogonalization. This relation allows to characterize the convergence of this\nmethod in terms of the error of a certain rational approximant to an indicator\nfunction. We propose improved rational approximants leading to FEAST variants\nwith faster convergence, in particular, when using rational approximants based\non the work of Zolotarev. Numerical experiments demonstrate the possible\ncomputational savings especially for pencils whose eigenvalues are not well\nseparated and when the dimension of the search space is only slightly larger\nthan the number of wanted eigenvalues. The new approach improves both\nconvergence robustness and load balancing when FEAST runs on multiple search\nintervals in parallel.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 15:13:03 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Guettel", "Stefan", ""], ["Polizzi", "Eric", ""], ["Tang", "Ping Tak Peter", ""], ["Viaud", "Gautier", ""]]}, {"id": "1407.8093", "submitter": "John Jakeman", "authors": "John D. Jakeman, Michael S. Eldred, Khachik Sargsyan", "title": "Enhancing $\\ell_1$-minimization estimates of polynomial chaos expansions\n  using basis selection", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2015.02.025", "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a basis selection method that can be used with\n$\\ell_1$-minimization to adaptively determine the large coefficients of\npolynomial chaos expansions (PCE). The adaptive construction produces\nanisotropic basis sets that have more terms in important dimensions and limits\nthe number of unimportant terms that increase mutual coherence and thus degrade\nthe performance of $\\ell_1$-minimization. The important features and the\naccuracy of basis selection are demonstrated with a number of numerical\nexamples. Specifically, we show that for a given computational budget, basis\nselection produces a more accurate PCE than would be obtained if the basis is\nfixed a priori. We also demonstrate that basis selection can be applied with\nnon-uniform random variables and can leverage gradient information.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 15:32:28 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Jakeman", "John D.", ""], ["Eldred", "Michael S.", ""], ["Sargsyan", "Khachik", ""]]}, {"id": "1407.8154", "submitter": "Lukas Einkemmer", "authors": "Lukas Einkemmer and Alexander Ostermann", "title": "A splitting approach for the Kadomtsev--Petviashvili equation", "comments": null, "journal-ref": "Journal of Computational Physics, Volume 299, 15 October 2015,\n  Pages 716-730", "doi": "10.1016/j.jcp.2015.07.024", "report-no": null, "categories": "physics.comp-ph cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a splitting approach for the Kadomtsev--Petviashvili equation\nwith periodic boundary conditions and show that the necessary interpolation\nprocedure can be efficiently implemented. The error made by this numerical\nscheme is compared to exponential integrators which have been shown in Klein\nand Roidot (SIAM J. Sci. Comput., 2011) to perform best for stiff solutions of\nthe Kadomtsev--Petviashvili equation. Since many classic high order splitting\nmethods do not perform well, we propose a stable extrapolation method in order\nto construct an efficient numerical scheme of order four. In addition, the\nconservation properties and the possibility of order reduction for certain\ninitial values for the numerical schemes under consideration is investigated.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 18:47:03 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 20:05:40 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Einkemmer", "Lukas", ""], ["Ostermann", "Alexander", ""]]}, {"id": "1407.8168", "submitter": "Elizabeth Michel", "authors": "Daniel Kimball, Elizabeth Michel, Paul Keltcher, and Michael M. Wolf", "title": "Quantifying the Effect of Matrix Structure on Multithreaded Performance\n  of the SpMV Kernel", "comments": "6 pages, 7 figures. IEEE HPEC 2014", "journal-ref": null, "doi": "10.1109/HPEC.2014.7040991", "report-no": null, "categories": "cs.DC cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-vector multiplication (SpMV) is the core operation in many\ncommon network and graph analytics, but poor performance of the SpMV kernel\nhandicaps these applications. This work quantifies the effect of matrix\nstructure on SpMV performance, using Intel's VTune tool for the Sandy Bridge\narchitecture. Two types of sparse matrices are considered: finite difference\n(FD) matrices, which are structured, and R-MAT matrices, which are\nunstructured. Analysis of cache behavior and prefetcher activity reveals that\nthe SpMV kernel performs far worse with R-MAT matrices than with FD matrices,\ndue to the difference in matrix structure. To address the problems caused by\nunstructured matrices, novel architecture improvements are proposed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 19:37:10 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kimball", "Daniel", ""], ["Michel", "Elizabeth", ""], ["Keltcher", "Paul", ""], ["Wolf", "Michael M.", ""]]}]