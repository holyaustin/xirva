[{"id": "0909.0777", "submitter": "Arian Maleki", "authors": "Arian Maleki, David L. Donoho", "title": "Optimally Tuned Iterative Reconstruction Algorithms for Compressed\n  Sensing", "comments": "12 pages, 14 figures", "journal-ref": null, "doi": "10.1109/JSTSP.2009.2039176", "report-no": null, "categories": "cs.NA cs.IT cs.MS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conducted an extensive computational experiment, lasting multiple\nCPU-years, to optimally select parameters for two important classes of\nalgorithms for finding sparse solutions of underdetermined systems of linear\nequations. We make the optimally tuned implementations available at {\\tt\nsparselab.stanford.edu}; they run `out of the box' with no user tuning: it is\nnot necessary to select thresholds or know the likely degree of sparsity. Our\nclass of algorithms includes iterative hard and soft thresholding with or\nwithout relaxation, as well as CoSaMP, subspace pursuit and some natural\nextensions. As a result, our optimally tuned algorithms dominate such\nproposals. Our notion of optimality is defined in terms of phase transitions,\ni.e. we maximize the number of nonzeros at which the algorithm can successfully\noperate. We show that the phase transition is a well-defined quantity with our\nsuite of random underdetermined linear systems. Our tuning gives the highest\ntransition possible within each class of algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2009 22:26:32 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Maleki", "Arian", ""], ["Donoho", "David L.", ""]]}, {"id": "0909.1305", "submitter": "Christian Mercat", "authors": "Alexander I. Bobenko (IM TU-B), Christian Mercat (LIRMM, I3M), Markus\n  Schmies (IM TU-B)", "title": "Conformal Structures and Period Matrices of Polyhedral Surfaces", "comments": "1--13", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recall the theory of linear discrete Riemann surfaces and show how to use\nit in order to interpret a surface embedded in R^3 as a discrete Riemann\nsurface and compute its basis of holomorphic forms on it. We present numerical\nexamples, recovering known results to test the numerics and giving the yet\nunknown period matrix of the Lawson genus-2 surface.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 18:46:03 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Bobenko", "Alexander I.", "", "IM TU-B"], ["Mercat", "Christian", "", "LIRMM, I3M"], ["Schmies", "Markus", "", "IM TU-B"]]}, {"id": "0909.2793", "submitter": "Ge Di", "authors": "D. Ge, J. Idier and E. Le Carpentier", "title": "Enhanced sampling schemes for MCMC based blind Bernoulli-Gaussian\n  deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and compares two new sampling schemes for sparse\ndeconvolution using a Bernoulli-Gaussian model. To tackle such a deconvolution\nproblem in a blind and unsupervised context, the Markov Chain Monte Carlo\n(MCMC) framework is usually adopted, and the chosen sampling scheme is most\noften the Gibbs sampler. However, such a sampling scheme fails to explore the\nstate space efficiently. Our first alternative, the $K$-tuple Gibbs sampler, is\nsimply a grouped Gibbs sampler. The second one, called partially marginalized\nsampler, is obtained by integrating the Gaussian amplitudes out of the target\ndistribution. While the mathematical validity of the first scheme is obvious as\na particular instance of the Gibbs sampler, a more detailed analysis is\nprovided to prove the validity of the second scheme.\n  For both methods, optimized implementations are proposed in terms of\ncomputation and storage cost. Finally, simulation results validate both schemes\nas more efficient in terms of convergence time compared with the plain Gibbs\nsampler. Benchmark sequence simulations show that the partially marginalized\nsampler takes fewer iterations to converge than the $K$-tuple Gibbs sampler.\nHowever, its computation load per iteration grows almost quadratically with\nrespect to the data length, while it only grows linearly for the $K$-tuple\nGibbs sampler.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2009 12:29:26 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2009 14:58:05 GMT"}], "update_date": "2009-09-18", "authors_parsed": [["Ge", "D.", ""], ["Idier", "J.", ""], ["Carpentier", "E. Le", ""]]}, {"id": "0909.4101", "submitter": "Gregorio Malajovich", "authors": "Felipe Cucker, Teresa Krick, Gregorio Malajovich, Mario Wschebor", "title": "A Numerical Algorithm for Zero Counting. II: Distance to Ill-posedness\n  and Smoothed Analysis", "comments": null, "journal-ref": "Journal of Fixed Point Theory and Applications 6 No 2, pp 285-294\n  (Dec. 2009)", "doi": "10.1007/s11784-009-0127-4", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a Condition Number Theorem for the condition number of zero counting\nfor real polynomial systems. That is, we show that this condition number equals\nthe inverse of the normalized distance to the set of ill-posed systems (i.e.,\nthose having multiple real zeros). As a consequence, a smoothed analysis of\nthis condition number follows.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2009 21:42:51 GMT"}], "update_date": "2010-07-12", "authors_parsed": [["Cucker", "Felipe", ""], ["Krick", "Teresa", ""], ["Malajovich", "Gregorio", ""], ["Wschebor", "Mario", ""]]}, {"id": "0909.4888", "submitter": "Mugurel Ionut Andreica", "authors": "Andrei-Horia Mogos, Mugurel Ionut Andreica", "title": "Approximating Mathematical Semantic Web Services Using Approximation\n  Formulas and Numerical Methods", "comments": "The International Workshop on Multi-Agent Systems Technology and\n  Semantics - MASTS 2009", "journal-ref": "Proc. of the 17th Intl. Conf. on Control Systems and Computer\n  Science (CSCS), vol. 2, pp. 533-538, Bucharest, Romania, 26-29 May, 2009", "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical semantic web services are very useful in practice, but only a\nsmall number of research results are reported in this area. In this paper we\npresent a method of obtaining an approximation of a mathematical semantic web\nservice, from its semantic description, using existing mathematical semantic\nweb services, approximation formulas, and numerical methods techniques. We also\ngive a method for automatic comparison of two complexity functions. In\naddition, we present a method for classifying the numerical methods\nmathematical semantic web services from a library.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2009 18:52:59 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Mogos", "Andrei-Horia", ""], ["Andreica", "Mugurel Ionut", ""]]}, {"id": "0909.5000", "submitter": "Hrushikesh Mhaskar", "authors": "H. N. Mhaskar", "title": "Eignets for function approximation on manifolds", "comments": "28 pages. Articles in press; Applied and Computational Harmonic\n  Analysis, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\XX$ be a compact, smooth, connected, Riemannian manifold without\nboundary, $G:\\XX\\times\\XX\\to \\RR$ be a kernel. Analogous to a radial basis\nfunction network, an eignet is an expression of the form $\\sum_{j=1}^M\na_jG(\\circ,y_j)$, where $a_j\\in\\RR$, $y_j\\in\\XX$, $1\\le j\\le M$. We describe a\ndeterministic, universal algorithm for constructing an eignet for approximating\nfunctions in $L^p(\\mu;\\XX)$ for a general class of measures $\\mu$ and kernels\n$G$. Our algorithm yields linear operators. Using the minimal separation\namongst the centers $y_j$ as the cost of approximation, we give modulus of\nsmoothness estimates for the degree of approximation by our eignets, and show\nby means of a converse theorem that these are the best possible for every\n\\emph{individual function}. We also give estimates on the coefficients $a_j$ in\nterms of the norm of the eignet. Finally, we demonstrate that if any sequence\nof eignets satisfies the optimal estimates for the degree of approximation of a\nsmooth function, measured in terms of the minimal separation, then the\nderivatives of the eignets also approximate the corresponding derivatives of\nthe target function in an optimal manner.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 04:25:03 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Mhaskar", "H. N.", ""]]}, {"id": "0909.5413", "submitter": "Rio Yokota Dr.", "authors": "Rio Yokota, L. A. Barba, Matthew G. Knepley", "title": "PetRBF--A parallel O(N) algorithm for radial basis function\n  interpolation", "comments": "Submitted to Computer Methods in Applied Mechanics and Engineering", "journal-ref": "Computer Methods in Applied Mechanics and Engineering, 199(25-28),\n  pp. 1793-1804, 2010", "doi": "10.1016/j.cma.2010.02.008", "report-no": null, "categories": "cs.MS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a parallel algorithm for radial basis function (RBF)\ninterpolation that exhibits O(N) complexity,requires O(N) storage, and scales\nexcellently up to a thousand processes. The algorithm uses a GMRES iterative\nsolver with a restricted additive Schwarz method (RASM) as a preconditioner and\na fast matrix-vector algorithm. Previous fast RBF methods, --,achieving at most\nO(NlogN) complexity,--, were developed using multiquadric and polyharmonic\nbasis functions. In contrast, the present method uses Gaussians with a small\nvariance (a common choice in particle methods for fluid simulation, our main\ntarget application). The fast decay of the Gaussian basis function allows rapid\nconvergence of the iterative solver even when the subdomains in the RASM are\nvery small. The present method was implemented in parallel using the PETSc\nlibrary (developer version). Numerical experiments demonstrate its capability\nin problems of RBF interpolation with more than 50 million data points, timing\nat 106 seconds (19 iterations for an error tolerance of 10^-15 on 1024\nprocessors of a Blue Gene/L (700 MHz PowerPC processors). The parallel code is\nfreely available in the open-source model.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 19:27:51 GMT"}], "update_date": "2011-09-21", "authors_parsed": [["Yokota", "Rio", ""], ["Barba", "L. A.", ""], ["Knepley", "Matthew G.", ""]]}]