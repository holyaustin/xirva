[{"id": "1708.00140", "submitter": "Levent Erkok", "authors": "Warren E. Ferguson Jr, Jesse Bingham, Levent Erk\\\"ok, John R.\n  Harrison, and Joe Leslie-Hurd", "title": "Digit Serial Methods with Applications to Division and Square Root (with\n  mechanically checked correctness proofs)", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic digit serial method (DSM) to compute the digits of a\nreal number $V$ . Bounds on these digits, and on the errors in the associated\nestimates of $V$ formed from these digits, are derived. To illustrate our\nresults, we derive such bounds for a parameterized family of high-radix\nalgorithms for division and square root. These bounds enable a DSM designer to\ndetermine, for example, whether a given choice of parameters allows rapid\nformation and rounding of its approximation to $V$. All our claims are\nmechanically verified using the HOL-Light theorem prover, and are included in\nthe appendix with commentary.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 02:42:37 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Ferguson", "Warren E.", "Jr"], ["Bingham", "Jesse", ""], ["Erk\u00f6k", "Levent", ""], ["Harrison", "John R.", ""], ["Leslie-Hurd", "Joe", ""]]}, {"id": "1708.00745", "submitter": "Emmanuel Soubies", "authors": "Emmanuel Soubies, Thanh-An Pham and Michael Unser", "title": "Efficient Inversion of Multiple-Scattering Model for Optical Diffraction\n  Tomography", "comments": null, "journal-ref": "Opt. Express 25, 21786-21800 (2017)", "doi": "10.1364/OE.25.021786", "report-no": null, "categories": "cs.CE cs.NA physics.data-an physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical diffraction tomography relies on solving an inverse scattering\nproblem governed by the wave equation. Classical reconstruction algorithms are\nbased on linear approximations of the forward model (Born or Rytov), which\nlimits their applicability to thin samples with low refractive-index contrasts.\nMore recent works have shown the benefit of adopting nonlinear models. They\naccount for multiple scattering and reflections, improving the quality of\nreconstruction. To reduce the complexity and memory requirements of these\nmethods, we derive an explicit formula for the Jacobian matrix of the nonlinear\nLippmann-Schwinger model which lends itself to an efficient evaluation of the\ngradient of the data- fidelity term. This allows us to deploy efficient methods\nto solve the corresponding inverse problem subject to sparsity constraints.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 09:46:32 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 09:40:55 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Soubies", "Emmanuel", ""], ["Pham", "Thanh-An", ""], ["Unser", "Michael", ""]]}, {"id": "1708.01244", "submitter": "Yury Korolev", "authors": "Yury Korolev and Jan Lellmann", "title": "Image reconstruction with imperfect forward models and applications in\n  deblurring", "comments": null, "journal-ref": null, "doi": "10.1137/17M1141965", "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyse an approach to image reconstruction problems with\nimperfect forward models based on partially ordered spaces - Banach lattices.\nIn this approach, errors in the data and in the forward models are described\nusing order intervals. The method can be characterised as the lattice analogue\nof the residual method, where the feasible set is defined by linear inequality\nconstraints. The study of this feasible set is the main contribution of this\npaper. Convexity of this feasible set is examined in several settings and\nmodifications for introducing additional information about the forward operator\nare considered. Numerical examples demonstrate the performance of the method in\ndeblurring with errors in the blurring kernel.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 17:49:31 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 12:49:18 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 08:11:10 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Korolev", "Yury", ""], ["Lellmann", "Jan", ""]]}, {"id": "1708.01793", "submitter": "Wai-Tong Louis Fan", "authors": "Wai-Tong Louis Fan", "title": "Stochastic PDEs on graphs as scaling limits of discrete interacting\n  systems", "comments": "39 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.NA q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic partial differential equations (SPDE) on graphs were introduced by\nCerrai and Freidlin [Ann. Inst. Henri Poincar\\'e Probab. Stat. 53 (2017)\n865-899]. This class of stochastic equations in infinite dimensions provides a\nminimal framework for the study of the effective dynamics of much more complex\nsystems. However, how they emerge from microscopic individual-based models is\nstill poorly understood, partly due to complications near vertex singularities.\nIn this work, motivated by the study of the dynamics and the genealogies of\nexpanding populations in spatially structured environments, we obtain a new\nclass of SPDE on graphs of Wright-Fisher type which have nontrivial boundary\nconditions on the vertex set. We show that these SPDE arise as scaling limits\nof suitably defined biased voter models (BVM), which extends the scaling limits\nof Durrett and Fan [Ann. Appl. Probab. 26 (2016) 456-490]. We further obtain a\nconvergent simulation scheme for each of these SPDE in terms of a system of\nIt\\^o SDEs, which is useful when the size of the BVM is too large for\nstochastic simulations. These give the first rigorous connection between SPDE\non graphs and more discrete models, specifically, interacting particle systems\nand interacting SDEs. Uniform heat kernel estimates for symmetric random walks\napproximating diffusions on graphs are the keys to our proofs.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 17:33:58 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 18:32:53 GMT"}, {"version": "v3", "created": "Sat, 28 Sep 2019 05:06:59 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 21:58:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Fan", "Wai-Tong Louis", ""]]}, {"id": "1708.01945", "submitter": "Shusen Wang", "authors": "Miles E. Lopes and Shusen Wang and Michael W. Mahoney", "title": "A Bootstrap Method for Error Estimation in Randomized Matrix\n  Multiplication", "comments": null, "journal-ref": "Journal of Machine Learning Research, 20(39): 1-40, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, randomized methods for numerical linear algebra have\nreceived growing interest as a general approach to large-scale problems.\nTypically, the essential ingredient of these methods is some form of randomized\ndimension reduction, which accelerates computations, but also creates random\napproximation error. In this way, the dimension reduction step encodes a\ntradeoff between cost and accuracy. However, the exact numerical relationship\nbetween cost and accuracy is typically unknown, and consequently, it may be\ndifficult for the user to precisely know (1) how accurate a given solution is,\nor (2) how much computation is needed to achieve a given level of accuracy. In\nthe current paper, we study randomized matrix multiplication (sketching) as a\nprototype setting for addressing these general problems. As a solution, we\ndevelop a bootstrap method for \\emph{directly estimating} the accuracy as a\nfunction of the reduced dimension (as opposed to deriving worst-case bounds on\nthe accuracy in terms of the reduced dimension). From a computational\nstandpoint, the proposed method does not substantially increase the cost of\nstandard sketching methods, and this is made possible by an \"extrapolation\"\ntechnique. In addition, we provide both theoretical and empirical results to\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 22:20:13 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 01:20:24 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Lopes", "Miles E.", ""], ["Wang", "Shusen", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1708.02207", "submitter": "Alexander Litvinenko", "authors": "Alexander Litvinenko", "title": "Partial inversion of the elliptic operator to speed up computation of\n  likelihood in Bayesian inference", "comments": "35 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, when solving forward, inverse or data assimilation problems, only a\npart of the solution is needed. As a model, we consider the stationary\ndiffusion problem. We demonstrate an algorithm that can compute only a part or\na functional of the solution, without calculating the full inversion operator\nand the complete solution. It is a well-known fact about partial differential\nequations that the solution at each discretisation point depends on the\nsolutions at all other discretisation points. Therefore, it is impossible to\ncompute the solution only at one point, without calculating the solution at all\nother points. The standard numerical methods like a conjugate gradient or Gauss\nelimination compute the whole solution and/or the complete inverse operator. We\nsuggest a method which can compute the solution of the given partial\ndifferential equation 1) at a point; 2) at few points; 3) on an interface; or a\nfunctional of the solution, without computing the solution at all points. The\nrequired storage cost and computational resources will be lower as in the\nstandard approach.\n  With this new method, we can speed up, for instance, computation of the\ninnovation in filtering or the likelihood distribution, which measures the data\nmisfit (mismatch). Further, we can speed up the solution of the regression,\nBayesian inversion, data assimilation, and Kalman filter update problems.\n  Applying additionally the hierarchical matrix approximation, we reduce the\ncubic computational cost to almost linear $\\mathcal{O}(k^2n \\log^2 n)$, where\n$k\\ll n$ and $n$ is the number of degrees of freedom.\n  Up to the hierarchical matrix approximation error, the computed solution is\nexact. One of the disadvantages of this method is the need to modify the\nexisting deterministic solver.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 17:13:24 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 21:27:37 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Litvinenko", "Alexander", ""]]}, {"id": "1708.02276", "submitter": "Jacob Schroder", "authors": "Jacob B. Schroder", "title": "Parallelizing Over Artificial Neural Network Training Runs with\n  Multigrid", "comments": "Version 2: - Added more complete references to basic neural network\n  literature - Corrected typos - Condensed results in Section 3 to be more\n  concise - 22 pages", "journal-ref": null, "doi": null, "report-no": "LLNL-JRNL-736173", "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are a popular and effective machine learning\ntechnique. Great progress has been made parallelizing the expensive training\nphase of an individual network, leading to highly specialized pieces of\nhardware, many based on GPU-type architectures, and more concurrent algorithms\nsuch as synthetic gradients. However, the training phase continues to be a\nbottleneck, where the training data must be processed serially over thousands\nof individual training runs. This work considers a multigrid reduction in time\n(MGRIT) algorithm that is able to parallelize over the thousands of training\nruns and converge to the exact same solution as traditional training would\nprovide. MGRIT was originally developed to provide parallelism for time\nevolution problems that serially step through a finite number of time-steps.\nThis work recasts the training of a neural network similarly, treating neural\nnetwork training as an evolution equation that evolves the network weights from\none step to the next. Thus, this work concerns distributed computing approaches\nfor neural networks, but is distinct from other approaches which seek to\nparallelize only over individual training runs. The work concludes with\nsupporting numerical results for two model problems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 19:42:24 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 18:14:19 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Schroder", "Jacob B.", ""]]}, {"id": "1708.03438", "submitter": "Alejandro Ortiz-Bernardin", "authors": "Alejandro Ortiz-Bernardin, Catalina Alvarez, Nancy Hitschfeld-Kahler,\n  Alessandro Russo, Rodrigo Silva-Valenzuela and Edgardo Olate-Sanzana", "title": "Veamy: an extensible object-oriented C++ library for the virtual element\n  method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the development of Veamy, an object-oriented C++\nlibrary for the virtual element method (VEM) on general polygonal meshes, whose\nmodular design is focused on its extensibility. The linear elastostatic and\nPoisson problems in two dimensions have been chosen as the starting stage for\nthe development of this library. The theory of the VEM, upon which Veamy is\nbuilt, is presented using a notation and a terminology that resemble the\nlanguage of the finite element method (FEM) in engineering analysis. Several\nexamples are provided to demonstrate the usage of Veamy, and in particular, one\nof them features the interaction between Veamy and the polygonal mesh generator\nPolyMesher. A computational performance comparison between VEM and FEM is also\nconducted. Veamy is free and open source software.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 05:39:36 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 20:07:42 GMT"}, {"version": "v3", "created": "Sat, 24 Feb 2018 17:49:06 GMT"}, {"version": "v4", "created": "Mon, 24 Dec 2018 01:02:58 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Ortiz-Bernardin", "Alejandro", ""], ["Alvarez", "Catalina", ""], ["Hitschfeld-Kahler", "Nancy", ""], ["Russo", "Alessandro", ""], ["Silva-Valenzuela", "Rodrigo", ""], ["Olate-Sanzana", "Edgardo", ""]]}, {"id": "1708.03519", "submitter": "Frits De Prenter", "authors": "Frits de Prenter, Clemens Verhoosel, Harald van Brummelen", "title": "Preconditioning immersed isogeometric finite element methods with\n  application to flow problems", "comments": null, "journal-ref": "Computer Methods in Applied Mechanics and Engineering 2019", "doi": "10.1016/j.cma.2019.01.030", "report-no": null, "categories": "cs.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersed finite element methods generally suffer from conditioning problems\nwhen cut elements intersect the physical domain only on a small fraction of\ntheir volume. De Prenter et al. [Computer Methods in Applied Mechanics and\nEngineering, 316 (2017) pp. 297-327] present an analysis for symmetric positive\ndefinite (SPD) immersed problems, and for this class of problems an algebraic\npreconditioner is developed. In this contribution the conditioning analysis is\nextended to immersed finite element methods for systems that are not SPD and\nthe preconditioning technique is generalized to a connectivity-based\npreconditioner inspired by Additive-Schwarz preconditioning. This\nConnectivity-based Additive-Schwarz (CbAS) preconditioner is applicable to\nproblems that are not SPD and to mixed problems, such as the Stokes and\nNavier-Stokes equations. A detailed numerical investigation of the effectivity\nof the CbAS preconditioner to a range of flow problems is presented.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 12:29:11 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["de Prenter", "Frits", ""], ["Verhoosel", "Clemens", ""], ["van Brummelen", "Harald", ""]]}, {"id": "1708.03570", "submitter": "Gottfried Hastermann", "authors": "Gottfried Hastermann, Maria Reinhardt, Rupert Klein, Sebastian Reich", "title": "Balanced data assimilation for highly-oscillatory mechanical systems", "comments": null, "journal-ref": "Commun. Appl. Math. Comput. Sci. 16 (2021) 119-154", "doi": "10.2140/camcos.2021.16.119", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation algorithms are used to estimate the states of a dynamical\nsystem using partial and noisy observations. The ensemble Kalman filter has\nbecome a popular data assimilation scheme due to its simplicity and robustness\nfor a wide range of application areas. Nevertheless, the ensemble Kalman filter\nalso has limitations due to its inherent Gaussian and linearity assumptions.\nThese limitations can manifest themselves in dynamically inconsistent state\nestimates. We investigate this issue in this paper for highly oscillatory\nHamiltonian systems with a dynamical behavior which satisfies certain balance\nrelations. We first demonstrate that the standard ensemble Kalman filter can\nlead to estimates which do not satisfy those balance relations, ultimately\nleading to filter divergence. We also propose two remedies for this phenomenon\nin terms of blended time-stepping schemes and ensemble-based penalty methods.\nThe effect of these modifications to the standard ensemble Kalman filter are\ndiscussed and demonstrated numerically for two model scenarios. First, we\nconsider balanced motion for highly oscillatory Hamiltonian systems and,\nsecond, we investigate thermally embedded highly oscillatory Hamiltonian\nsystems. The first scenario is relevant for applications from meteorology while\nthe second scenario is relevant for applications of data assimilation to\nmolecular dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 15:23:43 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 09:05:54 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 09:17:02 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Hastermann", "Gottfried", ""], ["Reinhardt", "Maria", ""], ["Klein", "Rupert", ""], ["Reich", "Sebastian", ""]]}, {"id": "1708.03599", "submitter": "Micol Pennacchio", "authors": "Silvia Bertoluzza, Micol Pennacchio, Daniele Prada", "title": "BDDC and FETI-DP for the Virtual Element Method", "comments": null, "journal-ref": "Calcolo, 54:1565-1593 (2017)", "doi": "10.1007/s10092-017-0242-3", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build and analyze Balancing Domain Decomposition by Constraint (BDDC) and\nFinite Element Tearing and Interconnecting Dual Primal (FETI-DP)\npreconditioners for elliptic problems discretized by the virtual element method\n(VEM). We prove polylogarithmic condition number bounds, independent of the\nnumber of subdomains, the mesh size, and jumps in the diffusion coefficients.\nNumerical experiments confirm the theory.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 16:21:36 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 09:12:57 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 12:20:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bertoluzza", "Silvia", ""], ["Pennacchio", "Micol", ""], ["Prada", "Daniele", ""]]}, {"id": "1708.04332", "submitter": "Qin Li", "authors": "Qin Li, Jian-Guo Liu and Ruiwen Shu", "title": "Sensitivity analysis of Burgers' equation with shocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized polynomial chaos (gPC) method has been extensively used in\nuncertainty quantification problems where equations contain random variables.\nFor gPC to achieve high accuracy, PDE solutions need to have high regularity in\nthe random space, but this is what hyperbolic type problems cannot provide. We\nprovide a counter-argument in this paper, and show that even though the\nsolution profile develops singularities in the random space, which destroys the\nspectral accuracy of gPC, the physical quantities (such as the shock emergence\ntime, the shock location, and the shock strength) are all smooth functions of\nthe uncertainties coming from both initial data and the wave speed: with proper\nshifting, the solution's polynomial interpolation approximates the real\nsolution accurately, and the error decays as the order of the polynomial\nincreases. Therefore this work provides a new perspective to \"quantify\nuncertainties\" and significantly improves the accuracy of the gPC method with a\nslight reformulation. We use the Burgers' equation as an example for the\nthorough analysis, and the analysis could be extended to general conservation\nlaws with convex fluxes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 21:18:28 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 08:18:30 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 07:11:37 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 17:59:36 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Li", "Qin", ""], ["Liu", "Jian-Guo", ""], ["Shu", "Ruiwen", ""]]}, {"id": "1708.04539", "submitter": "Mathias Jacquelin", "authors": "Mathias Jacquelin, Lin Lin, Chao Yang", "title": "PSelInv - A Distributed Memory Parallel Algorithm for Selected\n  Inversion: the non-symmetric Case", "comments": "arXiv admin note: text overlap with arXiv:1404.0447", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper generalizes the parallel selected inversion algorithm called\nPSelInv to sparse non- symmetric matrices. We assume a general sparse matrix A\nhas been decomposed as PAQ = LU on a distributed memory parallel machine, where\nL, U are lower and upper triangular matrices, and P, Q are permutation\nmatrices, respectively. The PSelInv method computes selected elements of A-1.\nThe selection is confined by the sparsity pattern of the matrix AT . Our\nalgorithm does not assume any symmetry properties of A, and our parallel\nimplementation is memory efficient, in the sense that the computed elements of\nA-T overwrites the sparse matrix L+U in situ. PSelInv involves a large number\nof collective data communication activities within different processor groups\nof various sizes. In order to minimize idle time and improve load balancing,\ntree-based asynchronous communication is used to coordinate all such collective\ncommunication. Numerical results demonstrate that PSelInv can scale efficiently\nto 6,400 cores for a variety of matrices.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 00:18:41 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Jacquelin", "Mathias", ""], ["Lin", "Lin", ""], ["Yang", "Chao", ""]]}, {"id": "1708.04928", "submitter": "Rachel Slaybaugh", "authors": "R.N. Slaybaugh, M. Ramirez-Zweiger, Tara Pandya, Steven Hamilton, and\n  T.M. Evans", "title": "Eigenvalue Solvers for Modeling Nuclear Reactors on Leadership Class\n  Machines", "comments": "arXiv admin note: substantial text overlap with arXiv:1702.02111,\n  arXiv:1612.00907", "journal-ref": null, "doi": "10.1080/00295639.2017.1413875", "report-no": null, "categories": "cs.NA physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Three complementary methods have been implemented in the code Denovo that\naccelerate neutral particle transport calculations with methods that use\nleadership-class computers fully and effectively: a multigroup block (MG)\nKrylov solver, a Rayleigh Quotient Iteration (RQI) eigenvalue solver, and a\nmultigrid in energy (MGE) preconditioner. The MG Krylov solver converges more\nquickly than Gauss Seidel and enables energy decomposition such that Denovo can\nscale to hundreds of thousands of cores. RQI should converge in fewer\niterations than power iteration (PI) for large and challenging problems. RQI\ncreates shifted systems that would not be tractable without the MG Krylov\nsolver. It also creates ill-conditioned matrices. The MGE preconditioner\nreduces iteration count significantly when used with RQI and takes advantage of\nthe new energy decomposition such that it can scale efficiently. Each\nindividual method has been described before, but this is the first time they\nhave been demonstrated to work together effectively.\n  The combination of solvers enables the RQI eigenvalue solver to work better\nthan the other available solvers for large reactors problems on leadership\nclass machines. Using these methods together, RQI converged in fewer iterations\nand in less time than PI for a full pressurized water reactor core. These\nsolvers also performed better than an Arnoldi eigenvalue solver for a reactor\nbenchmark problem when energy decomposition is needed. The MG Krylov, MGE\npreconditioner, and RQI solver combination also scales well in energy. This\nsolver set is a strong choice for very large and challenging problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 22:48:24 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 19:54:24 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Slaybaugh", "R. N.", ""], ["Ramirez-Zweiger", "M.", ""], ["Pandya", "Tara", ""], ["Hamilton", "Steven", ""], ["Evans", "T. M.", ""]]}, {"id": "1708.05163", "submitter": "Shahar Tsiper Mr.", "authors": "Shahar Tsiper and Yonina C. Eldar", "title": "RAPToR: A Resampling Algorithm for Pseudo-Polar based Tomographic\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stable and fast reconstruction technique for parallel-beam (PB)\ntomographic X-ray imaging, relying on the discrete pseudo-polar (PP) Radon\ntransform. Our main contribution is a resampling method, based on modern\nsampling theory, that transforms the acquired PB measurements to a PP grid. The\nresampling process is both fast and accurate, and in addition, simultaneously\ndenoises the measurements, by exploiting geometrical properties of the\ntomographic scan. The transformed measurements are then reconstructed using an\niterative solver with total variation (TV) regularization. We show that\nreconstructing from measurements on the PP grid, leads to improved recovery,\ndue to the inherent stability and accuracy of the PP Radon transform, compared\nwith the PB Radon transform. We also demonstrate recovery from a reduced number\nof PB acquisition angles, and high noise levels. Our approach is shown to\nachieve superior results over other state-of-the-art solutions, that operate\ndirectly on the given PB measurements. The proposed method can benefit fan-beam\nand/or cone-beam projections by coupling it with a rebinning process.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 07:47:08 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 04:46:25 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Tsiper", "Shahar", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1708.05832", "submitter": "Omar Lakkis", "authors": "Emmanuil H. Georgoulis and Omar Lakkis and Thomas P. Wihler", "title": "A posteriori error bounds for fully-discrete hp-discontinuous Galerkin\n  timestepping methods for parabolic problems", "comments": "submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider fully discrete time-space approximations of abstract linear\nparabolic partial differential equations (PDEs) consisting of an $hp$-version\ndiscontinuous Galerkin (DG) time stepping scheme in conjunction with standard\n(conforming) Galerkin discretizations in space. We derive abstract computable a\nposteriori error bounds resulting, for instance, in concrete bounds in\n$L_{\\infty}(I;L_2(\\Omega))$- and $L_{2}(I;H^{1}(\\Omega))$-type norms when $I$\nis the temporal and $\\Omega$ the spatial domain for the PDE. We base our\nmethodology for the analysis on a novel space-time reconstruction approach. Our\napproach is flexible as it works for any type of elliptic error estimator and\nleaves their choice of up to the user. It also allows exhibits mesh-change\nestimators in a clear an concise way. We also show how our approach allows the\nderivation of such bounds in the $H^1(I;H^{-1}(\\Omega))$ norm.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 11:11:44 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 17:45:12 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Georgoulis", "Emmanuil H.", ""], ["Lakkis", "Omar", ""], ["Wihler", "Thomas P.", ""]]}, {"id": "1708.05850", "submitter": "Qiya Hu", "authors": "Qiya Hu", "title": "Convergence of HX Preconditioner for Maxwell's Equations with Jump\n  Coefficients (i): Various Extensions of The Regular Helmholtz Decomposition", "comments": "with 27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first one of two serial articles, whose goal is to prove\nconvergence of HX Preconditioner (proposed by Hiptmair and Xu 2007) for\nMaxwell's equations with jump coefficients. In this paper we establish various\nextensions of the regular Helmholtz decomposition for edge finite element\nfunctions defined in three dimensional domains. The functions defined by the\nregular Helmholtz decompositions can preserve the zero tangential complement on\nfaces and edges of polyhedral domains and some non-Lipchitz domains, and\npossess stability estimates with only a $logarithm$ factor. These regular\nHelmholtz decompositions will be used to prove convergence of the HX\npreconditioner for Maxwell's equations with jump coefficients in another paper\n(Hu 2017).\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 14:15:41 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 07:46:14 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 01:52:14 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Hu", "Qiya", ""]]}, {"id": "1708.06018", "submitter": "Shin Harase", "authors": "Shin Harase", "title": "Conversion of Mersenne Twister to double-precision floating-point\n  numbers", "comments": null, "journal-ref": "Mathematics and Computers in Simulation, Volume 161, July 2019,\n  Pages 76-83", "doi": "10.1016/j.matcom.2018.08.006", "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 32-bit Mersenne Twister generator MT19937 is a widely used random number\ngenerator. To generate numbers with more than 32 bits in bit length, and\nparticularly when converting into 53-bit double-precision floating-point\nnumbers in $[0,1)$ in the IEEE 754 format, the typical implementation\nconcatenates two successive 32-bit integers and divides them by a power of $2$.\nIn this case, the 32-bit MT19937 is optimized in terms of its equidistribution\nproperties (the so-called dimension of equidistribution with $v$-bit accuracy)\nunder the assumption that one will mainly be using 32-bit output values, and\nhence the concatenation sometimes degrades the dimension of equidistribution\ncompared with the simple use of 32-bit outputs. In this paper, we analyze such\nphenomena by investigating hidden $\\mathbb{F}_2$-linear relations among the\nbits of high-dimensional outputs. Accordingly, we report that MT19937 with a\nspecific lag set fails several statistical tests, such as the overlapping\ncollision test, matrix rank test, and Hamming independence test.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 20:55:19 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 13:42:43 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 13:53:13 GMT"}, {"version": "v4", "created": "Sun, 2 Sep 2018 16:59:58 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Harase", "Shin", ""]]}, {"id": "1708.06290", "submitter": "Nela Bosner", "authors": "Nela Bosner, Zvonimir Bujanovi\\'c, Zlatko Drma\\v{c}", "title": "Parallel solver for shifted systems in a hybrid CPU-GPU framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a combination of a hybrid CPU--GPU and a pure GPU\nsoftware implementation of a direct algorithm for solving shifted linear\nsystems $(A - \\sigma I)X = B$ with large number of complex shifts $\\sigma$ and\nmultiple right-hand sides. Such problems often appear e.g. in control theory\nwhen evaluating the transfer function, or as a part of an algorithm performing\ninterpolatory model reduction, as well as when computing pseudospectra and\nstructured pseudospectra, or solving large linear systems of ordinary\ndifferential equations. The proposed algorithm first jointly reduces the\ngeneral full $n\\times n$ matrix $A$ and the $n\\times m$ full right-hand side\nmatrix $B$ to the controller Hessenberg canonical form that facilitates\nefficient solution: $A$ is transformed to a so-called $m$-Hessenberg form and\n$B$ is made upper-triangular. This is implemented as blocked highly parallel\nCPU--GPU hybrid algorithm; individual blocks are reduced by the CPU, and the\nnecessary updates of the rest of the matrix are split among the cores of the\nCPU and the GPU. To enhance parallelization, the reduction and the updates are\noverlapped. In the next phase, the reduced $m$-Hessenberg--triangular systems\nare solved entirely on the GPU, with shifts divided into batches. The benefits\nof such load distribution are demonstrated by numerical experiments. In\nparticular, we show that our proposed implementation provides an excellent\nbasis for efficient implementations of computational methods in systems and\ncontrol theory, from evaluation of transfer function to the interpolatory model\nreduction.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 15:39:14 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Bosner", "Nela", ""], ["Bujanovi\u0107", "Zvonimir", ""], ["Drma\u010d", "Zlatko", ""]]}, {"id": "1708.06714", "submitter": "Sathya N. Ravi", "authors": "Sathya N. Ravi, Maxwell D. Collins, Vikas Singh", "title": "A Deterministic Nonsmooth Frank Wolfe Algorithm with Coreset Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Frank-Wolfe (FW) type algorithm that is applicable to\nminimization problems with a nonsmooth convex objective. We provide convergence\nbounds and show that the scheme yields so-called coreset results for various\nMachine Learning problems including 1-median, Balanced Development, Sparse PCA,\nGraph Cuts, and the $\\ell_1$-norm-regularized Support Vector Machine (SVM)\namong others. This means that the algorithm provides approximate solutions to\nthese problems in time complexity bounds that are not dependent on the size of\nthe input problem. Our framework, motivated by a growing body of work on\nsublinear algorithms for various data analysis problems, is entirely\ndeterministic and makes no use of smoothing or proximal operators. Apart from\nthese theoretical results, we show experimentally that the algorithm is very\npractical and in some cases also offers significant computational advantages on\nlarge problem instances. We provide an open source implementation that can be\nadapted for other problems that fit the overall structure.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 16:43:33 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Ravi", "Sathya N.", ""], ["Collins", "Maxwell D.", ""], ["Singh", "Vikas", ""]]}, {"id": "1708.07471", "submitter": "Adam Jermyn", "authors": "Adam S. Jermyn", "title": "Efficient Decomposition of High-Rank Tensors", "comments": "10 pages, 15 figures. Updated to match published version in J Comp\n  Phys", "journal-ref": "J Comp Phys Volume 377, 15 January 2019, Pages 142-154", "doi": "10.1016/j.jcp.2018.10.026", "report-no": null, "categories": "physics.comp-ph cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors are a natural way to express correlations among many physical\nvariables, but storing tensors in a computer naively requires memory which\nscales exponentially in the rank of the tensor. This is not optimal, as the\nrequired memory is actually set not by the rank but by the mutual information\namongst the variables in question. Representations such as the tensor tree\nperform near-optimally when the tree decomposition is chosen to reflect the\ncorrelation structure in question, but making such a choice is non-trivial and\ngood heuristics remain highly context-specific. In this work I present two new\nalgorithms for choosing efficient tree decompositions, independent of the\nphysical context of the tensor. The first is a brute-force algorithm which\nproduces optimal decompositions up to truncation error but is generally\nimpractical for high-rank tensors, as the number of possible choices grows\nexponentially in rank. The second is a greedy algorithm, and while it is not\noptimal it performs extremely well in numerical experiments while having\nruntime which makes it practical even for tensors of very high rank.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 15:56:49 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 16:32:07 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 15:37:46 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 19:50:06 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Jermyn", "Adam S.", ""]]}, {"id": "1708.07622", "submitter": "Maarten de Jong", "authors": "M. de Jong", "title": "On the repeated inversion of a covariance matrix", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases, the values of some model parameters are determined by\nmaximising the likelihood of a set of data points given the parameter values.\nThe presence of outliers in the data and correlations between data points\ncomplicate this procedure. An efficient procedure for the elimination of\noutliers is presented which takes the correlations between data points into\naccount.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 06:25:56 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["de Jong", "M.", ""]]}, {"id": "1708.07627", "submitter": "Gouranga Mallik", "authors": "Carsten Carstensen, Gouranga Mallik, Neela Nataraj", "title": "Nonconforming Finite Element Discretisation for Semilinear Problems with\n  Trilinear Nonlinearity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Morley finite element method (FEM) is attractive for semilinear problems\nwith the biharmonic operator as a leading term in the stream function vorticity\nformulation of 2D Navier-Stokes problem and in the von K\\'{a}rm\\'{a}n\nequations. This paper establishes a best-approximation a~priori error analysis\nand an a~posteriori error analysis of discrete solutions close to an arbitrary\nregular solution on the continuous level to semilinear problems with a\ntrilinear nonlinearity. The analysis avoids any smallness assumptions on the\ndata and so has to provide discrete stability by a perturbation analysis before\nthe Newton-Kantorovic theorem can provide the existence of discrete solutions.\nAn abstract framework for the stability analysis in terms of discrete operators\nfrom the medius analysis leads to new results on the nonconforming\nCrouzeix-Raviart FEM for second-order linear non-selfadjoint and indefinite\nelliptic problems with $L^\\infty$ coefficients. The paper identifies six\nparameters and sufficient conditions for the local a~priori and a~posteriori\nerror control of conforming and nonconforming discretisations of a class of\nsemilinear elliptic problems first in an abstract framework and then in the two\nsemilinear applications. This leads to new best-approximation error estimates\nand to a~posteriori error estimates in terms of explicit residual-based error\ncontrol for the conforming and Morley FEM.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 06:44:39 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 05:02:08 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 13:25:47 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Carstensen", "Carsten", ""], ["Mallik", "Gouranga", ""], ["Nataraj", "Neela", ""]]}, {"id": "1708.07670", "submitter": "Simon Telen", "authors": "Simon Telen and Marc Van Barel", "title": "A Stabilized Normal Form Algorithm for Generic Systems of Polynomial\n  Equations", "comments": "This work is presented at the ILAS 2017 conference at Iowa State\n  University (July 24 - July 28, 2017) and the SIAM conference on Applied\n  Algebraic Geometry (AG17) at Georgia Institute of Technology (July 31 - Aug\n  4, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.AC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a numerical linear algebra based method to find the multiplication\noperators of the quotient ring $\\mathbb{C}[x]/I$ associated to a\nzero-dimensional ideal $I$ generated by $n$ $\\mathbb{C}$-polynomials in $n$\nvariables. We assume that the polynomials are generic in the sense that the\nnumber of solutions in $\\mathbb{C}^n$ equals the B\\'ezout number. The main\ncontribution of this paper is an automated choice of basis for\n$\\mathbb{C}[x]/I$, which is crucial for the feasibility of normal form methods\nin finite precision arithmetic. This choice is based on numerical linear\nalgebra techniques and governed by the numerical properties of the given\ngenerators of $I$.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 09:53:51 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 08:33:48 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Telen", "Simon", ""], ["Van Barel", "Marc", ""]]}, {"id": "1708.07718", "submitter": "William Smith", "authors": "Silvia Tozza, William A. P. Smith, Dizhong Zhu, Ravi Ramamoorthi and\n  Edwin R. Hancock", "title": "Linear Differential Constraints for Photo-polarimetric Height Estimation", "comments": "To appear at International Conference on Computer Vision (ICCV),\n  Venice, Italy, October 22-29, 2017", "journal-ref": null, "doi": null, "report-no": "Roma01.Math.NA", "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a differential approach to photo-polarimetric shape\nestimation. We propose several alternative differential constraints based on\npolarisation and photometric shading information and show how to express them\nin a unified partial differential system. Our method uses the image ratios\ntechnique to combine shading and polarisation information in order to directly\nreconstruct surface height, without first computing surface normal vectors.\nMoreover, we are able to remove the non-linearities so that the problem reduces\nto solving a linear differential problem. We also introduce a new method for\nestimating a polarisation image from multichannel data and, finally, we show it\nis possible to estimate the illumination directions in a two source setup,\nextending the method into an uncalibrated scenario. From a numerical point of\nview, we use a least-squares formulation of the discrete version of the\nproblem. To the best of our knowledge, this is the first work to consider a\nunified differential approach to solve photo-polarimetric shape estimation\ndirectly for height. Numerical results on synthetic and real-world data confirm\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 13:01:05 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Tozza", "Silvia", ""], ["Smith", "William A. P.", ""], ["Zhu", "Dizhong", ""], ["Ramamoorthi", "Ravi", ""], ["Hancock", "Edwin R.", ""]]}, {"id": "1708.07733", "submitter": "Zhe Yu", "authors": "Mang Liao, Di Shi, Zhe Yu, Wendong Zhu, Zhiwei Wang, and Yingmeng\n  Xiang", "title": "Recover the lost Phasor Measurement Unit Data Using Alternating\n  Direction Multipliers Method", "comments": "5 pages, 3 figures. Accepted by 2018 IEEE/PES Transmission and\n  Distribution Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel algorithm for recovering missing data of phasor\nmeasurement units (PMUs). Due to the low-rank property of PMU data, missing\nmeasurement recovery can be formulated as a low-rank matrix-completion problem.\nBased on maximum-margin matrix factorization, we propose an efficient algorithm\nbased on alternating direction method of multipliers (ADMM) for solving the\nmatrix completion problem. Comparing to existing approaches, the proposed ADMM\nbased algorithm does not need to estimate the rank of the target data matrix\nand provides better performance in computation complexity. In addition, we\nconsider the case of measurements missing from all PMU channels and provide a\nstrategy of reshaping the matrix which contains the received PMU data for\nrecovery. Numerical results using PMU measurements from IEEE 68-bus power\nsystem model illustrate the effectiveness and efficiency of the proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 20:48:29 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 18:17:21 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Liao", "Mang", ""], ["Shi", "Di", ""], ["Yu", "Zhe", ""], ["Zhu", "Wendong", ""], ["Wang", "Zhiwei", ""], ["Xiang", "Yingmeng", ""]]}, {"id": "1708.07850", "submitter": "Benjamin Haeffele", "authors": "Benjamin D. Haeffele and Rene Vidal", "title": "Structured Low-Rank Matrix Factorization: Global Optimality, Algorithms,\n  and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convex formulations of low-rank matrix factorization problems have\nreceived considerable attention in machine learning. However, such formulations\noften require solving for a matrix of the size of the data matrix, making it\nchallenging to apply them to large scale datasets. Moreover, in many\napplications the data can display structures beyond simply being low-rank,\ne.g., images and videos present complex spatio-temporal structures that are\nlargely ignored by standard low-rank methods. In this paper we study a matrix\nfactorization technique that is suitable for large datasets and captures\nadditional structure in the factors by using a particular form of\nregularization that includes well-known regularizers such as total variation\nand the nuclear norm as particular cases. Although the resulting optimization\nproblem is non-convex, we show that if the size of the factors is large enough,\nunder certain conditions, any local minimizer for the factors yields a global\nminimizer. A few practical algorithms are also provided to solve the matrix\nfactorization problem, and bounds on the distance from a given approximate\nsolution of the optimization problem to the global optimum are derived.\nExamples in neural calcium imaging video segmentation and hyperspectral\ncompressed recovery show the advantages of our approach on high-dimensional\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 18:14:44 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Haeffele", "Benjamin D.", ""], ["Vidal", "Rene", ""]]}, {"id": "1708.08035", "submitter": "Bin Shi Mr", "authors": "Bin Shi", "title": "A Conservation Law Method in Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose some algorithms to find local minima in nonconvex optimization and\nto obtain global minima in some degree from the Newton Second Law without\nfriction. With the key observation of the velocity observable and controllable\nin the motion, the algorithms simulate the Newton Second Law without friction\nbased on symplectic Euler scheme. From the intuitive analysis of analytical\nsolution, we give a theoretical analysis for the high-speed convergence in the\nalgorithm proposed. Finally, we propose the experiments for strongly convex\nfunction, non-strongly convex function and nonconvex function in\nhigh-dimension.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 01:55:49 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 04:43:48 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 22:26:51 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Shi", "Bin", ""]]}, {"id": "1708.08354", "submitter": "Andrew Knyazev", "authors": "Andrew Knyazev", "title": "Recent implementations, applications, and extensions of the Locally\n  Optimal Block Preconditioned Conjugate Gradient method (LOBPCG)", "comments": "4 pages. Householder Symposium on Numerical Linear Algebra, June 2017", "journal-ref": null, "doi": null, "report-no": "MERL TR2017-078", "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since introduction [A. Knyazev, Toward the optimal preconditioned\neigensolver: Locally optimal block preconditioned conjugate gradient method,\nSISC (2001) DOI:10.1137/S1064827500366124] and efficient parallel\nimplementation [A. Knyazev et al., Block locally optimal preconditioned\neigenvalue xolvers (BLOPEX) in HYPRE and PETSc, SISC (2007)\nDOI:10.1137/060661624], LOBPCG has been used is a wide range of applications in\nmechanics, material sciences, and data sciences. We review its recent\nimplementations and applications, as well as extensions of the local optimality\nidea beyond standard eigenvalue problems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:53:30 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Knyazev", "Andrew", ""]]}, {"id": "1708.08552", "submitter": "Xuanqing Liu", "authors": "Xuanqing Liu, Cho-Jui Hsieh, Jason D. Lee and Yuekai Sun", "title": "An inexact subsampled proximal Newton-type method for large-scale\n  machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast proximal Newton-type algorithm for minimizing regularized\nfinite sums that returns an $\\epsilon$-suboptimal point in\n$\\tilde{\\mathcal{O}}(d(n + \\sqrt{\\kappa d})\\log(\\frac{1}{\\epsilon}))$ FLOPS,\nwhere $n$ is number of samples, $d$ is feature dimension, and $\\kappa$ is the\ncondition number. As long as $n > d$, the proposed method is more efficient\nthan state-of-the-art accelerated stochastic first-order methods for non-smooth\nregularizers which requires $\\tilde{\\mathcal{O}}(d(n + \\sqrt{\\kappa\nn})\\log(\\frac{1}{\\epsilon}))$ FLOPS. The key idea is to form the subsampled\nNewton subproblem in a way that preserves the finite sum structure of the\nobjective, thereby allowing us to leverage recent developments in stochastic\nfirst-order methods to solve the subproblem. Experimental results verify that\nthe proposed algorithm outperforms previous algorithms for $\\ell_1$-regularized\nlogistic regression on real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 22:47:48 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Liu", "Xuanqing", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Jason D.", ""], ["Sun", "Yuekai", ""]]}, {"id": "1708.08640", "submitter": "Dongjin Choi", "authors": "Dongjin Choi, Jun-Gi Jang, U Kang", "title": "Fast, Accurate, and Scalable Method for Sparse Coupled Matrix-Tensor\n  Factorization", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we capture the hidden properties from a tensor and a matrix data\nsimultaneously in a fast, accurate, and scalable way? Coupled matrix-tensor\nfactorization (CMTF) is a major tool to extract latent factors from a tensor\nand matrices at once. Designing an accurate and efficient CMTF method has\nbecome more crucial as the size and dimension of real-world data are growing\nexplosively. However, existing methods for CMTF suffer from lack of accuracy,\nslow running time, and limited scalability. In this paper, we propose S3CMTF, a\nfast, accurate, and scalable CMTF method. S3CMTF achieves high speed by\nexploiting the sparsity of real-world tensors, and high accuracy by capturing\ninter-relations between factors. Also, S3CMTF accomplishes additional speed-up\nby lock-free parallel SGD update for multi-core shared memory systems. We\npresent two methods, S3CMTF-naive and S3CMTF-opt. S3CMTF-naive is a basic\nversion of S3CMTF, and S3CMTF-opt improves its speed by exploiting intermediate\ndata. We theoretically and empirically show that S3CMTF is the fastest,\noutperforming existing methods. Experimental results show that S3CMTF is 11~43\ntimes faster, and 2.1~4.1 times more accurate than existing methods. S3CMTF\nshows linear scalability on the number of data entries and the number of cores.\nIn addition, we apply S3CMTF to Yelp recommendation tensor data coupled with 3\nadditional matrices to discover interesting properties.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 08:38:06 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 12:44:15 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 02:12:32 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 09:18:31 GMT"}, {"version": "v5", "created": "Thu, 2 Nov 2017 07:14:29 GMT"}, {"version": "v6", "created": "Tue, 5 Dec 2017 16:16:24 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Choi", "Dongjin", ""], ["Jang", "Jun-Gi", ""], ["Kang", "U", ""]]}, {"id": "1708.08736", "submitter": "Stefan Steinerberger", "authors": "Stefan Steinerberger", "title": "Spectral Limitations of Quadrature Rules and Generalized Spherical\n  Designs", "comments": "to appear in IMRN", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SP cs.NA math.AP math.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study manifolds $M$ equipped with a quadrature rule $$ \\int_{M}{\\phi(x)\ndx} \\simeq \\sum_{i=1}^{n}{a_i \\phi(x_i)}.$$ We show that $n-$point quadrature\nrules with nonnegative weights on a compact $d-$dimensional manifold cannot\nintegrate more than at most the first $c_{d}n + o(n)$ Laplacian eigenfunctions\nexactly. The constants $c_d$ are explicitly computed and $c_2 = 4$. The result\nis new even on $\\mathbb{S}^2$ where it generalizes results on spherical\ndesigns.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 13:26:24 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 11:57:40 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Steinerberger", "Stefan", ""]]}, {"id": "1708.09165", "submitter": "Andrzej Cichocki", "authors": "A. Cichocki, A-H. Phan, Q. Zhao, N. Lee, I.V. Oseledets, M. Sugiyama,\n  D. Mandic", "title": "Tensor Networks for Dimensionality Reduction and Large-Scale\n  Optimizations. Part 2 Applications and Future Perspectives", "comments": "232 pages", "journal-ref": "Foundations and Trends in Machine Learning: Vol. 9: No. 6, pp\n  431-673, 2017", "doi": "10.1561/2200000067", "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part 2 of this monograph builds on the introduction to tensor networks and\ntheir operations presented in Part 1. It focuses on tensor network models for\nsuper-compressed higher-order representation of data/parameters and related\ncost functions, while providing an outline of their applications in machine\nlearning and data analytics. A particular emphasis is on the tensor train (TT)\nand Hierarchical Tucker (HT) decompositions, and their physically meaningful\ninterpretations which reflect the scalability of the tensor network approach.\nThrough a graphical approach, we also elucidate how, by virtue of the\nunderlying low-rank tensor approximations and sophisticated contractions of\ncore tensors, tensor networks have the ability to perform distributed\ncomputations on otherwise prohibitively large volumes of data/parameters,\nthereby alleviating or even eliminating the curse of dimensionality. The\nusefulness of this concept is illustrated over a number of applied areas,\nincluding generalized regression and classification (support tensor machines,\ncanonical correlation analysis, higher order partial least squares),\ngeneralized eigenvalue decomposition, Riemannian optimization, and in the\noptimization of deep neural networks. Part 1 and Part 2 of this work can be\nused either as stand-alone separate texts, or indeed as a conjoint\ncomprehensive review of the exciting field of low-rank tensor networks and\ntensor decompositions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 08:37:36 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Cichocki", "A.", ""], ["Phan", "A-H.", ""], ["Zhao", "Q.", ""], ["Lee", "N.", ""], ["Oseledets", "I. V.", ""], ["Sugiyama", "M.", ""], ["Mandic", "D.", ""]]}, {"id": "1708.09699", "submitter": "Robert Martin", "authors": "Herbert Baaser and Robert J. Martin and Patrizio Neff", "title": "Inconsistency of uhyper and umat in Abaqus for compressible hyperelastic\n  materials", "comments": "The article does not sufficiently distinguish between the behaviour\n  of Abaqus for incremental and absolute stress updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we revisited Ba\\v{z}ant's comments on the implementation of\nhyperelastic material models in commercial finite element software. We would\nlike to clarify that our assertions only apply if the material models are\nimplemented as hypoelastic, i.e. by incremental stress updates, in common\ninterfaces (including, in particular, umat in Abaqus). This assumption was not\nmade sufficiently clear in the article.\n  If, on the other hand, the stress calculations are implemented using the umat\ninterface with absolute (or \"total\") stress updates, as is also assumed in the\nuhyper interface, there is no difference in the internal processes or the\nresults between the umat and the uhyper implementation. This applies to highly\ncompressible formulations as well, where the Kirchhoff and Cauchy stress\ntensors are clearly distinguished.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 13:17:18 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 13:14:58 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Baaser", "Herbert", ""], ["Martin", "Robert J.", ""], ["Neff", "Patrizio", ""]]}, {"id": "1708.09707", "submitter": "Peter Zaspel", "authors": "Peter Zaspel", "title": "Algorithmic patterns for $\\mathcal{H}$-matrices on many-core processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the reformulation of hierarchical ($\\mathcal{H}$)\nmatrix algorithms for many-core processors with a model implementation on\ngraphics processing units (GPUs). $\\mathcal{H}$ matrices approximate specific\ndense matrices, e.g., from discretized integral equations or kernel ridge\nregression, leading to log-linear time complexity in dense matrix-vector\nproducts. The parallelization of $\\mathcal{H}$ matrix operations on many-core\nprocessors is difficult due to the complex nature of the underlying algorithms.\nWhile previous algorithmic advances for many-core hardware focused on\naccelerating existing $\\mathcal{H}$ matrix CPU implementations by many-core\nprocessors, we here aim at totally relying on that processor type. As main\ncontribution, we introduce the necessary parallel algorithmic patterns allowing\nto map the full $\\mathcal{H}$ matrix construction and the fast matrix-vector\nproduct to many-core hardware. Here, crucial ingredients are space filling\ncurves, parallel tree traversal and batching of linear algebra operations. The\nresulting model GPU implementation hmglib is the, to the best of the authors\nknowledge, first entirely GPU-based Open Source $\\mathcal{H}$ matrix library of\nthis kind. We conclude this work by an in-depth performance analysis and a\ncomparative performance study against a standard $\\mathcal{H}$ matrix library,\nhighlighting profound speedups of our many-core parallel approach.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 13:50:42 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Zaspel", "Peter", ""]]}]