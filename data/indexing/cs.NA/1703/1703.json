[{"id": "1703.00008", "submitter": "Murat Uzunca", "authors": "B\\\"ulent Karas\\\"ozen, Murat Uzunca, Tu\\u{g}ba K\\\"u\\c{c}\\\"ukseyhan", "title": "Reduced Order Optimal Control of the Convective FitzHugh-Nagumo Equation", "comments": null, "journal-ref": "Computers and Mathematics with Applications, 79, 982-995 (2020)", "doi": "10.1016/j.camwa.2019.08.009", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare three model order reduction methods: the proper\northogonal decomposition (POD), discrete empirical interpolation method (DEIM)\nand dynamic mode decomposition (DMD) for the optimal control of the convective\nFitzHugh-Nagumo (FHN) equations. The convective FHN equations consists of the\nsemi-linear activator and the linear inhibitor equations, modeling blood\ncoagulation in moving excitable media. The semilinear activator equation leads\nto a non-convex optimal control problem (OCP). The most commonly used method in\nreduced optimal control is POD. We use DEIM and DMD to approximate efficiently\nthe nonlinear terms in reduced order models. We compare the accuracy and\ncomputational times of three reduced-order optimal control solutions with the\nfull order discontinuous Galerkin finite element solution of the convection\ndominated FHN equations with terminal controls. Numerical results show that POD\nis the most accurate whereas POD-DMD is the fastest.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 11:29:36 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 15:52:30 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Karas\u00f6zen", "B\u00fclent", ""], ["Uzunca", "Murat", ""], ["K\u00fc\u00e7\u00fckseyhan", "Tu\u011fba", ""]]}, {"id": "1703.00279", "submitter": "Henrik Barthels M.Sc.", "authors": "Henrik Barthels", "title": "Systematic Generation of Algorithms for Iterative Methods", "comments": "Master's Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FLAME methodology makes it possible to derive provably correct algorithms\nfrom a formal description of a linear algebra problem. So far, the methodology\nhas been successfully used to automate the derivation of direct algorithms such\nas the Cholesky decomposition and the solution of Sylvester equations. In this\nthesis, we present an extension of the FLAME methodology to tackle iterative\nmethods such as Conjugate Gradient. As a starting point, we use a formal\ndescription of the iterative method in matrix form. The result is a family of\nprovably correct pseudocode algorithms. We argue that all the intermediate\nsteps are sufficiently systematic to be fully automated.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 13:09:19 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 13:35:16 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Barthels", "Henrik", ""]]}, {"id": "1703.00663", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "Introduction to Nonnegative Matrix Factorization", "comments": "18 pages, 4 figures", "journal-ref": "SIAG/OPT Views and News 25 (1), pp. 7-16 (2017)", "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and provide a short overview of nonnegative\nmatrix factorization (NMF). Several aspects of NMF are discussed, namely, the\napplication in hyperspectral imaging, geometry and uniqueness of NMF solutions,\ncomplexity, algorithms, and its link with extended formulations of polyhedra.\nIn order to put NMF into perspective, the more general problem class of\nconstrained low-rank matrix approximation problems is first briefly introduced.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 08:23:04 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1703.00734", "submitter": "Xiangju Qin", "authors": "Xiangju Qin, Paul Blomstedt, Eemeli Lepp\\\"aaho, Pekka Parviainen,\n  Samuel Kaski", "title": "Distributed Bayesian Matrix Factorization with Limited Communication", "comments": "28 pages, 8 figures. The paper is published in Machine Learning\n  journal. An implementation of the method is is available in SMURFF software\n  on github (bmfpp branch): https://github.com/ExaScience/smurff", "journal-ref": "Machine Learning, 2019", "doi": "10.1007/s10994-019-05778-2", "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian matrix factorization (BMF) is a powerful tool for producing low-rank\nrepresentations of matrices and for predicting missing values and providing\nconfidence intervals. Scaling up the posterior inference for massive-scale\nmatrices is challenging and requires distributing both data and computation\nover many workers, making communication the main computational bottleneck.\nEmbarrassingly parallel inference would remove the communication needed, by\nusing completely independent computations on different data subsets, but it\nsuffers from the inherent unidentifiability of BMF solutions. We introduce a\nhierarchical decomposition of the joint posterior distribution, which couples\nthe subset inferences, allowing for embarrassingly parallel computations in a\nsequence of at most three stages. Using an efficient approximate\nimplementation, we show improvements empirically on both real and simulated\ndata. Our distributed approach is able to achieve a speed-up of almost an order\nof magnitude over the full posterior, with a negligible effect on predictive\naccuracy. Our method outperforms state-of-the-art embarrassingly parallel MCMC\nmethods in accuracy, and achieves results competitive to other available\ndistributed and parallel implementations of BMF.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 11:48:24 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 09:47:09 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 18:58:59 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 17:07:21 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Qin", "Xiangju", ""], ["Blomstedt", "Paul", ""], ["Lepp\u00e4aho", "Eemeli", ""], ["Parviainen", "Pekka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1703.01202", "submitter": "Chao Yang", "authors": "Ying Wei and Chao Yang and Jizu Huang", "title": "Parallel energy-stable phase field crystal simulations based on domain\n  decomposition methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a parallel numerical algorithm for solving the\nphase field crystal equation. In the algorithm, a semi-implicit finite\ndifference scheme is derived based on the discrete variational derivative\nmethod. Theoretical analysis is provided to show that the scheme is\nunconditionally energy stable and can achieve second-order accuracy in both\nspace and time. An adaptive time step strategy is adopted such that the time\nstep size can be flexibly controlled based on the dynamical evolution of the\nproblem. At each time step, a nonlinear algebraic system is constructed from\nthe discretization of the phase field crystal equation and solved by a domain\ndecomposition based, parallel Newton--Krylov--Schwarz method with improved\nboundary conditions for subdomain problems. Numerical experiments with several\ntwo and three dimensional test cases show that the proposed algorithm is\nsecond-order accurate in both space and time, energy stable with large time\nsteps, and highly scalable to over ten thousands processor cores on the Sunway\nTaihuLight supercomputer.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 15:27:32 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Wei", "Ying", ""], ["Yang", "Chao", ""], ["Huang", "Jizu", ""]]}, {"id": "1703.01325", "submitter": "Bo Yang", "authors": "Bo Yang, Hui Liu, He Zhong and Zhangxin Chen", "title": "Decoupled Block-Wise ILU(k) Preconditioner on GPU", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research investigates the implementation mechanism of block-wise ILU(k)\npreconditioner on GPU. The block-wise ILU(k) algorithm requires both the level\nk and the block size to be designed as variables. A decoupled ILU(k) algorithm\nconsists of a symbolic phase and a factorization phase. In the symbolic phase,\na ILU(k) nonzero pattern is established from the point-wise structure extracted\nfrom a block-wise matrix. In the factorization phase, the block-wise matrix\nwith a variable block size is factorized into a block lower triangular matrix\nand a block upper triangular matrix. And a further diagonal factorization is\nrequired to perform on the block upper triangular matrix for adapting a\nparallel triangular solver on GPU.We also present the numerical experiments to\nstudy the preconditioner actions on different k levels and block sizes.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 20:08:02 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Yang", "Bo", ""], ["Liu", "Hui", ""], ["Zhong", "He", ""], ["Chen", "Zhangxin", ""]]}, {"id": "1703.01920", "submitter": "Tom Tirer", "authors": "Tom Tirer and Raja Giryes", "title": "Generalizing CoSaMP to Signals from a Union of Low Dimensional Linear\n  Subspaces", "comments": null, "journal-ref": "Applied and Computational Harmonic Analysis, 2018", "doi": "10.1016/j.acha.2018.11.005", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea that signals reside in a union of low dimensional subspaces subsumes\nmany low dimensional models that have been used extensively in the recent\ndecade in many fields and applications. Until recently, the vast majority of\nworks have studied each one of these models on its own. However, a recent\napproach suggests providing general theory for low dimensional models using\ntheir Gaussian mean width, which serves as a measure for the intrinsic low\ndimensionality of the data. In this work we use this novel approach to study a\ngeneralized version of the popular compressive sampling matching pursuit\n(CoSaMP) algorithm, and to provide general recovery guarantees for signals from\na union of low dimensional linear subspaces, under the assumption that the\nmeasurement matrix is Gaussian. We discuss the implications of our results for\nspecific models, and use the generalized algorithm as an inspiration for a new\ngreedy method for signal reconstruction in a combined sparse-synthesis and\ncosparse-analysis model. We perform experiments that demonstrate the usefulness\nof the proposed strategy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 15:20:18 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}, {"id": "1703.02283", "submitter": "Christian Plessl", "authors": "Michael Lass and Thomas D. K\\\"uhne and Christian Plessl", "title": "Using Approximate Computing for the Calculation of Inverse Matrix p-th\n  Roots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate computing has shown to provide new ways to improve performance\nand power consumption of error-resilient applications. While many of these\napplications can be found in image processing, data classification or machine\nlearning, we demonstrate its suitability to a problem from scientific\ncomputing. Utilizing the self-correcting behavior of iterative algorithms, we\nshow that approximate computing can be applied to the calculation of inverse\nmatrix p-th roots which are required in many applications in scientific\ncomputing. Results show great opportunities to reduce the computational effort\nand bandwidth required for the execution of the discussed algorithm, especially\nwhen targeting special accelerator hardware.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 08:57:53 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Lass", "Michael", ""], ["K\u00fchne", "Thomas D.", ""], ["Plessl", "Christian", ""]]}, {"id": "1703.02311", "submitter": "Olivier Pironneau", "authors": "S\\'ebastien Geeraert, Charles-Albert Lehalle, Barak Pearlmutter,\n  Olivier Pironneau (LJLL), Adil Reghai", "title": "Mini-symposium on automatic differentiation and its applications in the\n  financial industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic differentiation is involved for long in applied mathematics as an\nalternative to finite difference to improve the accuracy of numerical\ncomputation of derivatives. Each time a numerical minimization is involved,\nautomatic differentiation can be used. In between formal derivation and\nstandard numerical schemes, this approach is based on software solutions\napplying mechanically the chain rule to obtain an exact value for the desired\nderivative. It has a cost in memory and cpu consumption. For participants of\nfinancial markets (banks, insurances, financial intermediaries, etc), computing\nderivatives is needed to obtain the sensitivity of its exposure to well-defined\npotential market moves. It is a way to understand variations of their balance\nsheets in specific cases. Since the 2008 crisis, regulation demand to compute\nthis kind of exposure to many different case, to be sure market participants\nare aware and ready to face a wide spectrum of configurations. This paper shows\nhow automatic differentiation provides a partial answer to this recent\nexplosion of computation to perform. One part of the answer is a\nstraightforward application of Adjoint Algorithmic Differentiation (AAD), but\nit is not enough. Since financial sensitivities involves specific functions and\nmix differentiation with Monte-Carlo simulations, dedicated tools and\nassociated theoretical results are needed. We give here short introductions to\ntypical cases arising when one use AAD on financial markets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 10:18:41 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 14:35:41 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Geeraert", "S\u00e9bastien", "", "LJLL"], ["Lehalle", "Charles-Albert", "", "LJLL"], ["Pearlmutter", "Barak", "", "LJLL"], ["Pironneau", "Olivier", "", "LJLL"], ["Reghai", "Adil", ""]]}, {"id": "1703.02422", "submitter": "Xuefeng Xu", "authors": "Xuefeng Xu", "title": "New upper bounds for the spectral variation of a general matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $A\\in\\mathbb{C}^{n\\times n}$ be a normal matrix with spectrum\n$\\{\\lambda_{i}\\}_{i=1}^{n}$, and let $\\widetilde{A}=A+E\\in\\mathbb{C}^{n\\times\nn}$ be a perturbed matrix with spectrum\n$\\{\\widetilde{\\lambda}_{i}\\}_{i=1}^{n}$. If $\\widetilde{A}$ is still normal,\nthe celebrated Hoffman--Wielandt theorem states that there exists a permutation\n$\\pi$ of $\\{1,\\ldots,n\\}$ such that\n$\\big(\\sum_{i=1}^{n}|\\widetilde{\\lambda}_{\\pi(i)}-\\lambda_{i}|^{2}\\big)^{1/2}\\leq\\|E\\|_{F}$,\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm of a matrix. This theorem\nreveals the strong stability of the spectrum of a normal matrix. However, if\n$A$ or $\\widetilde{A}$ is non-normal, the Hoffman--Wielandt theorem does not\nhold in general. In this paper, we present new upper bounds for\n$\\big(\\sum_{i=1}^{n}|\\widetilde{\\lambda}_{\\pi(i)}-\\lambda_{i}|^{2}\\big)^{1/2}$,\nprovided that both $A$ and $\\widetilde{A}$ are general matrices. Some of our\nestimates improve or generalize the existing ones.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:06:44 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 02:55:12 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 02:28:29 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Xu", "Xuefeng", ""]]}, {"id": "1703.03680", "submitter": "Han Cheng Lie", "authors": "H. C. Lie and A. M. Stuart and T. J. Sullivan", "title": "Strong convergence rates of probabilistic integrators for ordinary\n  differential equations", "comments": "25 pages", "journal-ref": "Statistics and Computing (2019)", "doi": "10.1007/s11222-019-09898-6", "report-no": null, "categories": "math.NA cs.NA math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic integration of a continuous dynamical system is a way of\nsystematically introducing model error, at scales no larger than errors\nintroduced by standard numerical discretisation, in order to enable thorough\nexploration of possible responses of the system to inputs. It is thus a\npotentially useful approach in a number of applications such as forward\nuncertainty quantification, inverse problems, and data assimilation. We extend\nthe convergence analysis of probabilistic integrators for deterministic\nordinary differential equations, as proposed by Conrad et al.\\ (\\textit{Stat.\\\nComput.}, 2017), to establish mean-square convergence in the uniform norm on\ndiscrete- or continuous-time solutions under relaxed regularity assumptions on\nthe driving vector fields and their induced flows. Specifically, we show that\nrandomised high-order integrators for globally Lipschitz flows and randomised\nEuler integrators for dissipative vector fields with polynomially-bounded local\nLipschitz constants all have the same mean-square convergence rate as their\ndeterministic counterparts, provided that the variance of the integration noise\nis not of higher order than the corresponding deterministic integrator. These\nand similar results are proven for probabilistic integrators where the random\nperturbations may be state-dependent, non-Gaussian, or non-centred random\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 13:43:06 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 09:21:47 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 15:23:43 GMT"}, {"version": "v4", "created": "Thu, 27 Sep 2018 13:50:16 GMT"}, {"version": "v5", "created": "Tue, 8 Jan 2019 21:19:24 GMT"}, {"version": "v6", "created": "Sat, 26 Oct 2019 18:01:13 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lie", "H. C.", ""], ["Stuart", "A. M.", ""], ["Sullivan", "T. J.", ""]]}, {"id": "1703.03722", "submitter": "Nematollah Zarmehi", "authors": "Nematollah Zarmehi and Farokh Marvasti", "title": "Recovery of Sparse and Low Rank Components of Matrices Using Iterative\n  Method with Adaptive Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we propose an algorithm for recovery of sparse and low rank\ncomponents of matrices using an iterative method with adaptive thresholding. In\neach iteration, the low rank and sparse components are obtained using a\nthresholding operator. This algorithm is fast and can be implemented easily. We\ncompare it with one of the most common fast methods in which the rank and\nsparsity are approximated by $\\ell_1$ norm. We also apply it to some real\napplications where the noise is not so sparse. The simulation results show that\nit has a suitable performance with low run-time.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 17:06:21 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 04:37:45 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Zarmehi", "Nematollah", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1703.04219", "submitter": "Ioakeim Perros", "authors": "Ioakeim Perros and Evangelos E. Papalexakis and Fei Wang and Richard\n  Vuduc and Elizabeth Searles and Michael Thompson and Jimeng Sun", "title": "SPARTan: Scalable PARAFAC2 for Large & Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exploratory tensor mining, a common problem is how to analyze a set of\nvariables across a set of subjects whose observations do not align naturally.\nFor example, when modeling medical features across a set of patients, the\nnumber and duration of treatments may vary widely in time, meaning there is no\nmeaningful way to align their clinical records across time points for analysis\npurposes. To handle such data, the state-of-the-art tensor model is the\nso-called PARAFAC2, which yields interpretable and robust output and can\nnaturally handle sparse data. However, its main limitation up to now has been\nthe lack of efficient algorithms that can handle large-scale datasets.\n  In this work, we fill this gap by developing a scalable method to compute the\nPARAFAC2 decomposition of large and sparse datasets, called SPARTan. Our method\nexploits special structure within PARAFAC2, leading to a novel algorithmic\nreformulation that is both fast (in absolute time) and more memory-efficient\nthan prior work. We evaluate SPARTan on both synthetic and real datasets,\nshowing 22X performance gains over the best previous implementation and also\nhandling larger problem instances for which the baseline fails. Furthermore, we\nare able to apply SPARTan to the mining of temporally-evolving phenotypes on\ndata taken from real and medically complex pediatric patients. The clinical\nmeaningfulness of the phenotypes identified in this process, as well as their\ntemporal evolution over time for several patients, have been endorsed by\nclinical experts.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 01:38:56 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Perros", "Ioakeim", ""], ["Papalexakis", "Evangelos E.", ""], ["Wang", "Fei", ""], ["Vuduc", "Richard", ""], ["Searles", "Elizabeth", ""], ["Thompson", "Michael", ""], ["Sun", "Jimeng", ""]]}, {"id": "1703.04560", "submitter": "Kevin Carlberg", "authors": "Youngsoo Choi and Kevin Carlberg", "title": "Space-time least-squares Petrov-Galerkin projection for nonlinear model\n  reduction", "comments": "Accepted to the SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a space-time least-squares Petrov-Galerkin (ST-LSPG)\nprojection method for model reduction of nonlinear dynamical systems. In\ncontrast to typical nonlinear model-reduction methods that first apply\n(Petrov-)Galerkin projection in the spatial dimension and subsequently apply\ntime integration to numerically resolve the resulting low-dimensional dynamical\nsystem, the proposed method applies projection in space and time\nsimultaneously. To accomplish this, the method first introduces a\nlow-dimensional space-time trial subspace, which can be obtained by computing\ntensor decompositions of state-snapshot data. The method then computes\ndiscrete-optimal approximations in this space-time trial subspace by minimizing\nthe residual arising after time discretization over all space and time in a\nweighted $\\ell^2$-norm. This norm can be defined to enable complexity reduction\n(i.e., hyper-reduction) in time, which leads to space-time collocation and\nspace-time GNAT variants of the ST-LSPG method. Advantages of the approach\nrelative to typical spatial-projection-based nonlinear model reduction methods\nsuch as Galerkin projection and least-squares Petrov-Galerkin projection\ninclude: (1) a reduction of both the spatial and temporal dimensions of the\ndynamical system, (2) the removal of spurious temporal modes (e.g., unstable\ngrowth) from the state space, and (3) error bounds that exhibit slower growth\nin time. Numerical examples performed on model problems in fluid dynamics\ndemonstrate the ability of the method to generate orders-of-magnitude\ncomputational savings relative to spatial-projection-based reduced-order models\nwithout sacrificing accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 01:27:37 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 01:05:41 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 19:05:54 GMT"}, {"version": "v4", "created": "Mon, 15 Oct 2018 16:39:41 GMT"}, {"version": "v5", "created": "Mon, 12 Nov 2018 20:21:06 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Choi", "Youngsoo", ""], ["Carlberg", "Kevin", ""]]}, {"id": "1703.04679", "submitter": "Thomas Ranner", "authors": "Charles M. Elliott and Thomas Ranner", "title": "A unified theory for continuous in time evolving finite element space\n  approximations to partial differential equations in evolving domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a unified theory for continuous in time finite element\ndiscretisations of partial differential equations posed in evolving domains\nincluding the consideration of equations posed on evolving surfaces and bulk\ndomains as well coupled surface bulk systems. We use an abstract variational\nsetting with time dependent function spaces and abstract time dependent finite\nelement spaces. Optimal a priori bounds are shown under usual assumptions on\nperturbations of bilinear forms and approximation properties of the abstract\nfinite element spaces. The abstract theory is applied to evolving finite\nelements in both flat and curved spaces. Evolving bulk and surface\nisoparametric finite element spaces defined on evolving triangulations are\ndefined and developed. These spaces are used to define approximations to\nparabolic equations in general domains for which the abstract theory is shown\nto apply. Numerical experiments are described which confirm the rates of\nconvergence.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 19:30:22 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 19:21:26 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 13:25:13 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 15:15:07 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Elliott", "Charles M.", ""], ["Ranner", "Thomas", ""]]}, {"id": "1703.04890", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai and Hiroyuki Sato and Bamdev Mishra", "title": "Riemannian stochastic quasi-Newton algorithm with variance reduction and\n  its convergence analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variance reduction algorithms have recently become popular for\nminimizing the average of a large, but finite number of loss functions. The\npresent paper proposes a Riemannian stochastic quasi-Newton algorithm with\nvariance reduction (R-SQN-VR). The key challenges of averaging, adding, and\nsubtracting multiple gradients are addressed with notions of retraction and\nvector transport. We present convergence analyses of R-SQN-VR on both\nnon-convex and retraction-convex functions under retraction and vector\ntransport operators. The proposed algorithm is evaluated on the Karcher mean\ncomputation on the symmetric positive-definite manifold and the low-rank matrix\ncompletion on the Grassmann manifold. In all cases, the proposed algorithm\noutperforms the state-of-the-art Riemannian batch and stochastic gradient\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 02:34:39 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 15:52:38 GMT"}, {"version": "v3", "created": "Sat, 16 Sep 2017 09:47:22 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kasai", "Hiroyuki", ""], ["Sato", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1703.05487", "submitter": "Quanming Yao", "authors": "Quanming Yao and James T. Kwok", "title": "Accelerated and Inexact Soft-Impute for Large-Scale Matrix and Tensor\n  Completion", "comments": "Journal version of previous conference paper 'Accelerated inexact\n  soft-impute for fast large-scale matrix completion' appeared at IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix and tensor completion aim to recover a low-rank matrix / tensor from\nlimited observations and have been commonly used in applications such as\nrecommender systems and multi-relational data mining. A state-of-the-art matrix\ncompletion algorithm is Soft-Impute, which exploits the special \"sparse plus\nlow-rank\" structure of the matrix iterates to allow efficient SVD in each\niteration. Though Soft-Impute is a proximal algorithm, it is generally believed\nthat acceleration destroys the special structure and is thus not useful. In\nthis paper, we show that Soft-Impute can indeed be accelerated without\ncomprising this structure. To further reduce the iteration time complexity, we\npropose an approximate singular value thresholding scheme based on the power\nmethod. Theoretical analysis shows that the proposed algorithm still enjoys the\nfast $O(1/T^2)$ convergence rate of accelerated proximal algorithms. We further\nextend the proposed algorithm to tensor completion with the scaled latent\nnuclear norm regularizer. We show that a similar \"sparse plus low-rank\"\nstructure also exists, leading to low iteration complexity and fast $O(1/T^2)$\nconvergence rate. Extensive experiments demonstrate that the proposed algorithm\nis much faster than Soft-Impute and other state-of-the-art matrix and tensor\ncompletion algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 06:50:00 GMT"}, {"version": "v2", "created": "Sat, 25 Aug 2018 07:23:05 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""]]}, {"id": "1703.06327", "submitter": "Sewoong Oh", "authors": "Ashish Khetan, Sewoong Oh", "title": "Spectrum Estimation from a Few Entries", "comments": "52 pages; 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular values of a data in a matrix form provide insights on the structure\nof the data, the effective dimensionality, and the choice of hyper-parameters\non higher-level data analysis tools. However, in many practical applications\nsuch as collaborative filtering and network analysis, we only get a partial\nobservation. Under such scenarios, we consider the fundamental problem of\nrecovering spectral properties of the underlying matrix from a sampling of its\nentries. We are particularly interested in directly recovering the spectrum,\nwhich is the set of singular values, and also in sample-efficient approaches\nfor recovering a spectral sum function, which is an aggregate sum of the same\nfunction applied to each of the singular values. We propose first estimating\nthe Schatten $k$-norms of a matrix, and then applying Chebyshev approximation\nto the spectral sum function or applying moment matching in Wasserstein\ndistance to recover the singular values. The main technical challenge is in\naccurately estimating the Schatten norms from a sampling of a matrix. We\nintroduce a novel unbiased estimator based on counting small structures in a\ngraph and provide guarantees that match its empirical performance. Our\ntheoretical analysis shows that Schatten norms can be recovered accurately from\nstrictly smaller number of samples compared to what is needed to recover the\nunderlying low-rank matrix. Numerical experiments suggest that we significantly\nimprove upon a competing approach of using matrix completion methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 18:12:17 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Khetan", "Ashish", ""], ["Oh", "Sewoong", ""]]}, {"id": "1703.06359", "submitter": "Toni Karvonen", "authors": "Toni Karvonen and Simo S\\\"arkk\\\"a", "title": "Fully symmetric kernel quadrature", "comments": "Accepted for publication in SIAM Journal on Scientific Computing.\n  Minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel quadratures and other kernel-based approximation methods typically\nsuffer from prohibitive cubic time and quadratic space complexity in the number\nof function evaluations. The problem arises because a system of linear\nequations needs to be solved. In this article we show that the weights of a\nkernel quadrature rule can be computed efficiently and exactly for up to tens\nof millions of nodes if the kernel, integration domain, and measure are fully\nsymmetric and the node set is a union of fully symmetric sets. This is based on\nthe observations that in such a setting there are only as many distinct weights\nas there are fully symmetric sets and that these weights can be solved from a\nlinear system of equations constructed out of row sums of certain submatrices\nof the full kernel matrix. We present several numerical examples that show\nfeasibility, both for a large number of nodes and in high dimensions, of the\ndeveloped fully symmetric kernel quadrature rules. Most prominent of the fully\nsymmetric kernel quadrature rules we propose are those that use sparse grids.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 22:12:08 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 13:23:32 GMT"}, {"version": "v3", "created": "Sat, 6 Jan 2018 12:17:21 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Karvonen", "Toni", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1703.06463", "submitter": "Lennard Kamenski", "authors": "Weizhang Huang, Lennard Kamenski, Jens Lang", "title": "Conditioning of implicit Runge-Kutta integration for finite element\n  approximation of linear diffusion equations on anisotropic meshes", "comments": "Accepted manuscript. \\copyright 2019. Licensed under CC-BY-NC-ND 4.0\n  (https://creativecommons.org/licenses/by-nc-nd/4.0)", "journal-ref": "J. Comput. Appl. Math. 387, 112497 (2021)", "doi": "10.1016/j.cam.2019.112497", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditioning of implicit Runge-Kutta (RK) integration for linear finite\nelement approximation of diffusion equations on general anisotropic meshes is\ninvestigated. Bounds are established for the condition number of the resulting\nlinear system with and without diagonal preconditioning for the implicit Euler\nand general implicit RK methods. Two solution strategies are considered for the\nlinear system resulting from general implicit RK integration: the simultaneous\nsolution (the system is solved as a whole) and a successive solution which\nfollows the commonly used implementation of implicit RK methods to first\ntransform the system into smaller systems using the Jordan normal form of the\nRK matrix and then solve them successively.\n  For the simultaneous solution in case of a positive semidefinite symmetric\npart of the RK coefficient matrix and for the successive solution it is shown\nthat . If the smallest eigenvalue of the symmetric part of the RK coefficient\nmatrix is negative and the simultaneous solution strategy is used, an upper\nbound on the time step is given so that the system matrix is positive definite.\n  The obtained bounds for the condition number have explicit geometric\ninterpretations and take the interplay between the diffusion matrix and the\nmesh geometry into full consideration. They show that there are three\nmesh-dependent factors that can affect the conditioning: the number of\nelements, the mesh nonuniformity measured in the Euclidean metric, and the mesh\nnonuniformity with respect to the inverse of the diffusion matrix. They also\nreveal that the preconditioning using the diagonal of the system matrix, the\nmass matrix, or the lumped mass matrix can effectively eliminate the effects of\nthe mesh nonuniformity measured in the Euclidean metric. Numerical examples are\ngiven.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 16:32:59 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 19:38:43 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 17:40:03 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Huang", "Weizhang", ""], ["Kamenski", "Lennard", ""], ["Lang", "Jens", ""]]}, {"id": "1703.06494", "submitter": "Pavel Kus", "authors": "Pavel K\\r{u}s, Jakub \\v{S}\\'istek", "title": "Coupling parallel adaptive mesh refinement with a nonoverlapping domain\n  decomposition solver", "comments": null, "journal-ref": "Advances in Engineering Software 110 (2017), 34-54", "doi": "10.1016/j.advengsoft.2017.03.012", "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of adaptive mesh refinement on a parallel domain\ndecomposition solver of a linear system of algebraic equations. These concepts\nneed to be combined within a parallel adaptive finite element software. A\nprototype implementation is presented for this purpose. It uses adaptive mesh\nrefinement with one level of hanging nodes. Two and three-level versions of the\nBalancing Domain Decomposition based on Constraints (BDDC) method are used to\nsolve the arising system of algebraic equations. The basic concepts are\nrecalled and components necessary for the combination are studied in detail. Of\nparticular interest is the effect of disconnected subdomains, a typical output\nof the employed mesh partitioning based on space-filling curves, on the\nconvergence and solution time of the BDDC method. It is demonstrated using a\nlarge set of experiments that while both refined meshes and disconnected\nsubdomains have a negative effect on the convergence of BDDC, the number of\niterations remains acceptable. In addition, scalability of the three-level BDDC\nsolver remains good on up to a few thousands of processor cores. The largest\npresented problem using adaptive mesh refinement has over 10^9 unknowns and is\nsolved on 2048 cores.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 19:25:42 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["K\u016fs", "Pavel", ""], ["\u0160\u00edstek", "Jakub", ""]]}, {"id": "1703.06495", "submitter": "Avram Sidi", "authors": "Avram Sidi", "title": "Acceleration of Convergence of Some Infinite Sequences\n  $\\boldsymbol{\\{A_n\\}}$ Whose Asymptotic Expansions Involve Fractional Powers\n  of $\\boldsymbol{n}$ via the $\\tilde{d}^{(m)}$ transformation", "comments": null, "journal-ref": "Numerical Algorithms, 85:1409--1445, 2020", "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the application of the author's $\\tilde{d}^{(m)}$\ntransformation to accelerate the convergence of infinite series\n$\\sum^\\infty_{n=1}a_n$ when the terms $a_n$ have asymptotic expansions that can\nbe expressed in the form $$\na_n\\sim(n!)^{s/m}\\exp\\left[\\sum^{m}_{i=0}q_in^{i/m}\\right]\\sum^\\infty_{i=0}w_i\nn^{\\gamma-i/m}\\quad\\text{as $n\\to\\infty$},\\quad s\\ \\text{integer.}$$ We discuss\nthe implementation of the $\\tilde{d}^{(m)}$ transformation via the recursive\nW-algorithm of the author. We show how to apply this transformation and how to\nassess in a reliable way the accuracies of the approximations it produces,\nwhether the series converge or they diverge. We classify the different cases\nthat exhibit unique numerical stability issues in floating-point arithmetic. We\nshow that the $\\tilde{d}^{(m)}$ transformation can also be used efficiently to\naccelerate the convergence of infinite products\n  $\\prod^\\infty_{n=1}(1+v_n)$, where\n  $v_n\\sim \\sum^\\infty_{i=0}e_in^{-t/m-i/m}$ as $n\\to\\infty$,\\ $t\\geq m+1$ an\ninteger. Finally, we give several numerical examples that attest the high\nefficiency of the $\\tilde{d}^{(m)}$ transformation for the different cases.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 19:27:44 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 14:08:01 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Sidi", "Avram", ""]]}, {"id": "1703.07206", "submitter": "Carlos M\\'alaga Dr.", "authors": "J. T. Becerra-Sagredo, F. Mandujano and C. Malaga", "title": "A GPU-based Multi-level Algorithm for Boundary Value Problems", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel and scalable geometric multi-level algorithm is presented for the\nnumerical solution of elliptic partial differential equations, specially\ndesigned to run with high occupancy of streaming processors inside Graphics\nProcessing Units(GPUs). The algorithm consists of iterative, superposed\noperations on a single grid, and it is composed of two simple full-grid\nroutines: a restriction and a coarsened interpolation-relaxation. The\nrestriction is used to collect sources using recursive coarsened averages, and\nthe interpolation-relaxation simultaneously applies coarsened finite-difference\noperators and interpolations. The routines are scheduled in a saw-like refining\ncycle. Convergence to machine precision is achieved repeating the full cycle\nusing accumulated residuals and successively collecting the solution. Its total\nnumber of operations scale linearly with the number of nodes. It provides an\nattractive fast solver for Boundary Value Problems (BVPs), specially for\nsimulations running entirely in the GPU. Applications shown in this work\ninclude the deformation of two-dimensional grids, the computation of\nthree-dimensional streamlines for a singular trifoil-knot vortex and the\ncalculation of three-dimensional electric potentials in heterogeneous\ndielectric media.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 15:35:08 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Becerra-Sagredo", "J. T.", ""], ["Mandujano", "F.", ""], ["Malaga", "C.", ""]]}, {"id": "1703.07307", "submitter": "Andreas Varga", "authors": "Andreas Varga", "title": "On recursive computation of coprime factorizations of rational matrices", "comments": "18 pages; article accepted for publication in the Linear Algebra and\n  Its Applications", "journal-ref": null, "doi": "10.1016/j.laa.2020.01.030", "report-no": null, "categories": "cs.SY cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General computational methods based on descriptor state-space realizations\nare proposed to compute coprime factorizations of rational matrices with\nminimum degree denominators. The new methods rely on recursive pole dislocation\ntechniques, which allow to successively place all poles of the factors into a\n\"good\" region of the complex plane. The resulting McMillan degree of the\ndenominator factor is equal to the number of poles lying in the complementary\n\"bad\" region and therefore is minimal. The developed pole dislocation\ntechniques are instrumental for devising numerically reliable procedures for\nthe computation of coprime factorizations with proper and stable factors of\narbitrary improper rational matrices and coprime factorizations with inner\ndenominators. Implementation aspects of the proposed algorithms are discussed\nand illustrative examples are given.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 16:40:12 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 12:03:42 GMT"}, {"version": "v3", "created": "Sat, 8 Feb 2020 09:41:10 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Varga", "Andreas", ""]]}, {"id": "1703.07810", "submitter": "Andrey Tremba", "authors": "Boris Polyak and Andrey Tremba", "title": "New versions of Newton method: step-size choice, convergence domain and\n  under-determined equations", "comments": "Authors' version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newton method is one of the most powerful methods for finding solutions of\nnonlinear equations and for proving their existence. In its \"pure\" form it has\nfast convergence near the solution, but small convergence domain. On the other\nhand damped Newton method has slower convergence rate, but weaker conditions on\nthe initial point. We provide new versions of Newton-like algorithms, resulting\nin combinations of Newton and damped Newton method with special step-size\nchoice, and estimate its convergence domain. Under some assumptions the\nconvergence is global. Explicit complexity results are also addressed. The\nadaptive version of the algorithm (with no a priori constants knowledge) is\npresented. The method is applicable for under-determined equations (with $m<n$,\n$m$ being the number of equations and $n$ being the number of variables). The\nresults are specified for systems of quadratic equations, for composite\nmappings and for one-dimensional equations and inequalities.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 18:32:53 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 11:18:30 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Polyak", "Boris", ""], ["Tremba", "Andrey", ""]]}, {"id": "1703.07865", "submitter": "Tor Anderson", "authors": "Tor Anderson, Chin-Yao Chang, and Sonia Martinez", "title": "Weight Design of Distributed Approximate Newton Algorithms for\n  Constrained Optimization", "comments": "Submitted to CCTA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by economic dispatch and linearly-constrained resource allocation\nproblems, this paper proposes a novel Distributed Approx-Newton algorithm that\napproximates the standard Newton optimization method. A main property of this\ndistributed algorithm is that it only requires agents to exchange constant-size\ncommunication messages. The convergence of this algorithm is discussed and\nrigorously analyzed. In addition, we aim to address the problem of designing\ncommunication topologies and weightings that are optimal for second-order\nmethods. To this end, we propose an effective approximation which is loosely\nbased on completing the square to address the NP-hard bilinear optimization\ninvolved in the design. Simulations demonstrate that our proposed weight design\napplied to the Distributed Approx-Newton algorithm has a superior convergence\nproperty compared to existing weighted and distributed first-order gradient\ndescent methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 21:29:38 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Anderson", "Tor", ""], ["Chang", "Chin-Yao", ""], ["Martinez", "Sonia", ""]]}, {"id": "1703.08370", "submitter": "Ivano Notarnicola", "authors": "Ivano Notarnicola and Giuseppe Notarstefano", "title": "A randomized primal distributed algorithm for partitioned and big-data\n  non-convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a distributed optimization scenario in which the\naggregate objective function to minimize is partitioned, big-data and possibly\nnon-convex. Specifically, we focus on a set-up in which the dimension of the\ndecision variable depends on the network size as well as the number of local\nfunctions, but each local function handled by a node depends only on a (small)\nportion of the entire optimization variable. This problem set-up has been shown\nto appear in many interesting network application scenarios. As main paper\ncontribution, we develop a simple, primal distributed algorithm to solve the\noptimization problem, based on a randomized descent approach, which works under\nasynchronous gossip communication. We prove that the proposed asynchronous\nalgorithm is a proper, ad-hoc version of a coordinate descent method and thus\nconverges to a stationary point. To show the effectiveness of the proposed\nalgorithm, we also present numerical simulations on a non-convex quadratic\nprogram, which confirm the theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 11:43:51 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Notarnicola", "Ivano", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1703.09074", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson and Krithika Manohar and Steven L. Brunton and J.\n  Nathan Kutz", "title": "Randomized CP Tensor Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular\ndimensionality-reduction method for multiway data. Dimensionality reduction is\noften sought after since many high-dimensional tensors have low intrinsic rank\nrelative to the dimension of the ambient measurement space. However, the\nemergence of `big data' poses significant computational challenges for\ncomputing this fundamental tensor decomposition. By leveraging modern\nrandomized algorithms, we demonstrate that coherent structures can be learned\nfrom a smaller representation of the tensor in a fraction of the time. Thus,\nthis simple but powerful algorithm enables one to compute the approximate CP\ndecomposition even for massive tensors. The approximation error can thereby be\ncontrolled via oversampling and the computation of power iterations. In\naddition to theoretical results, several empirical results demonstrate the\nperformance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 13:41:00 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 06:27:21 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Manohar", "Krithika", ""], ["Brunton", "Steven L.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1703.09085", "submitter": "Steffen B\\\"orm", "authors": "Steffen B\\\"orm", "title": "Hierarchical matrix arithmetic with accumulated updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical matrices can be used to construct efficient preconditioners for\npartial differential and integral equations by taking advantage of low-rank\nstructures in triangular factorizations and inverses of the corresponding\nstiffness matrices.\n  The setup phase of these preconditioners relies heavily on low-rank updates\nthat are responsible for a large part of the algorithm's total run-time,\nparticularly for matrices resulting from three-dimensional problems.\n  This article presents a new algorithm that significantly reduces the number\nof low-rank updates and can reduce the setup time by 50 percent or more.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 14:00:24 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 13:43:46 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 13:53:32 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["B\u00f6rm", "Steffen", ""]]}, {"id": "1703.09268", "submitter": "Curt Da Silva", "authors": "Curt Da Silva, Felix J. Herrmann", "title": "A Unified 2D/3D Large Scale Software Environment for Nonlinear Inverse\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale parameter estimation problems are among some of the most\ncomputationally demanding problems in numerical analysis. An academic\nresearcher's domain-specific knowledge often precludes that of software design,\nwhich results in inversion frameworks that are technically correct, but not\nscalable to realistically-sized problems. On the other hand, the computational\ndemands for realistic problems result in industrial codebases that are geared\nsolely for high performance, rather than comprehensibility or flexibility. We\npropose a new software design for inverse problems constrained by partial\ndifferential equations that bridges the gap between these two seemingly\ndisparate worlds. A hierarchical and modular design allows a user to delve into\nas much detail as she desires, while exploiting high performance primitives at\nthe lower levels. Our code has the added benefit of actually reflecting the\nunderlying mathematics of the problem, which lowers the cognitive load on user\nusing it and reduces the initial startup period before a researcher can be\nfully productive. We also introduce a new preconditioner for the 3D Helmholtz\nequation that is suitable for fault-tolerant distributed systems. Numerical\nexperiments on a variety of 2D and 3D test problems demonstrate the\neffectiveness of this approach on scaling algorithms from small to large scale\nproblems with minimal code changes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 19:01:10 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 00:19:08 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Da Silva", "Curt", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "1703.09499", "submitter": "Yangyang Li", "authors": "Yangyang Li and Ruqian Lu", "title": "Locality preserving projection on SPD matrix Lie group: algorithm and\n  analysis", "comments": "15 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric positive definite (SPD) matrices used as feature descriptors in\nimage recognition are usually high dimensional. Traditional manifold learning\nis only applicable for reducing the dimension of high-dimensional vector-form\ndata. For high-dimensional SPD matrices, directly using manifold learning\nalgorithms to reduce the dimension of matrix-form data is impossible. The SPD\nmatrix must first be transformed into a long vector, and then the dimension of\nthis vector must be reduced. However, this approach breaks the spatial\nstructure of the SPD matrix space. To overcome this limitation, we propose a\nnew dimension reduction algorithm on SPD matrix space to transform\nhigh-dimensional SPD matrices into low-dimensional SPD matrices. Our work is\nbased on the fact that the set of all SPD matrices with the same size has a Lie\ngroup structure, and we aim to transform the manifold learning to the SPD\nmatrix Lie group. We use the basic idea of the manifold learning algorithm\ncalled locality preserving projection (LPP) to construct the corresponding\nLaplacian matrix on the SPD matrix Lie group. Thus, we call our approach\nLie-LPP to emphasize its Lie group character. We present a detailed algorithm\nanalysis and show through experiments that Lie-LPP achieves effective results\non human action recognition and human face recognition.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 10:38:22 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:47:32 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Li", "Yangyang", ""], ["Lu", "Ruqian", ""]]}, {"id": "1703.10230", "submitter": "Maziar Raissi", "authors": "Maziar Raissi, Paris Perdikaris, and George Em Karniadakis", "title": "Numerical Gaussian Processes for Time-dependent and Non-linear Partial\n  Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA math.AP math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of numerical Gaussian processes, which we define as\nGaussian processes with covariance functions resulting from temporal\ndiscretization of time-dependent partial differential equations. Numerical\nGaussian processes, by construction, are designed to deal with cases where: (1)\nall we observe are noisy data on black-box initial conditions, and (2) we are\ninterested in quantifying the uncertainty associated with such noisy data in\nour solutions to time-dependent partial differential equations. Our method\ncircumvents the need for spatial discretization of the differential operators\nby proper placement of Gaussian process priors. This is an attempt to construct\nstructured and data-efficient learning machines, which are explicitly informed\nby the underlying physics that possibly generated the observed data. The\neffectiveness of the proposed approach is demonstrated through several\nbenchmark problems involving linear and nonlinear time-dependent operators. In\nall examples, we are able to recover accurate approximations of the latent\nsolutions, and consistently propagate uncertainty, even in cases involving very\nlong time integration.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 20:17:30 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1703.10297", "submitter": "Mary Pugh", "authors": "David Yan, M. C. Pugh, F. P. Dawson", "title": "Adaptive Time-stepping Schemes for the Solution of the\n  Poisson-Nernst-Planck Equations", "comments": "The earlier version, arXiv:1703.10297v1, contained a detailed study\n  of a \"toy model\" and of the boundary conditions. Those studies are not\n  included in this version. This version is the first of a pair of companion\n  articles. The second article is entitled \"A Study of the Numerical Stability\n  of an ImEx Scheme with Application to the Poisson-Nernst-Planck Equations\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.chem-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Poisson-Nernst-Planck equations with generalized Frumkin-Butler-Volmer\nboundary conditions (PNP-FBV) describe ion transport with Faradaic reactions,\nand have applications in a number of fields. In this article, we develop an\nadaptive time-stepping scheme for the solution of the PNP-FBV equations based\non two time-stepping methods: a fully implicit (BDF2) method, and an\nimplicit-explicit (SBDF2) method. We present simulations under both current and\nvoltage boundary conditions and demonstrate the ability to simulate a large\nrange of parameters, including any value of the singular perturbation parameter\n$\\epsilon$. When the underlying dynamics is one that would have the solutions\nconverge to a steady-state solution, we observe that the adaptive time-stepper\nbased on the SBDF2 method produces solutions that ``nearly'' converge to the\nsteady state and that, simultaneously, the time-step sizes stabilize to a\nlimiting size $dt_\\infty$. In the companion to this article \\cite{YPD_Part2},\nwe linearize the SBDF2 scheme about the steady-state solution and demonstrate\nthat the linearized scheme is conditionally stable. This conditional stability\nis the cause of the adaptive time-stepper's behaviour. While the adaptive\ntime-stepper based on the fully-implicit (BDF2) method is not subject to such\ntime-step constraints, the required nonlinear solve yields run times that are\nsignificantly longer.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 03:03:04 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 21:34:05 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 21:55:37 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Yan", "David", ""], ["Pugh", "M. C.", ""], ["Dawson", "F. P.", ""]]}, {"id": "1703.10523", "submitter": "Rodrigo de Lamare", "authors": "R. C. de Lamare", "title": "Report on Two-Step Knowledge-Aided Iterative ESPRIT Algorithm", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a subspace-based algorithm for direction-of-arrival\n(DOA) estimation, referred to as two-step knowledge-aided iterative estimation\nof signal parameters via rotational invariance techniques (ESPRIT) method\n(Two-Step KAI-ESPRIT), which achieves more accurate estimates than those of\nprior art. The proposed Two-Step KAI-ESPRIT improves the estimation of the\ncovariance matrix of the input data by incorporating prior knowledge of signals\nand by exploiting knowledge of the structure of the covariance matrix and its\nperturbation terms. Simulation results illustrate the improvement achieved by\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 01:26:28 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["de Lamare", "R. C.", ""]]}, {"id": "1703.10740", "submitter": "Morteza Ashraphijuo", "authors": "Morteza Ashraphijuo, Xiaodong Wang", "title": "Fundamental Conditions for Low-CP-Rank Tensor Completion", "comments": "arXiv admin note: text overlap with arXiv:1703.07698", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of low canonical polyadic (CP) rank tensor\ncompletion. A completion is a tensor whose entries agree with the observed\nentries and its rank matches the given CP rank. We analyze the manifold\nstructure corresponding to the tensors with the given rank and define a set of\npolynomials based on the sampling pattern and CP decomposition. Then, we show\nthat finite completability of the sampled tensor is equivalent to having a\ncertain number of algebraically independent polynomials among the defined\npolynomials. Our proposed approach results in characterizing the maximum number\nof algebraically independent polynomials in terms of a simple geometric\nstructure of the sampling pattern, and therefore we obtain the deterministic\nnecessary and sufficient condition on the sampling pattern for finite\ncompletability of the sampled tensor. Moreover, assuming that the entries of\nthe tensor are sampled independently with probability $p$ and using the\nmentioned deterministic analysis, we propose a combinatorial method to derive a\nlower bound on the sampling probability $p$, or equivalently, the number of\nsampled entries that guarantees finite completability with high probability. We\nalso show that the existing result for the matrix completion problem can be\nused to obtain a loose lower bound on the sampling probability $p$. In\naddition, we obtain deterministic and probabilistic conditions for unique\ncompletability. It is seen that the number of samples required for finite or\nunique completability obtained by the proposed analysis on the CP manifold is\norders-of-magnitude lower than that is obtained by the existing analysis on the\nGrassmannian manifold.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 03:21:32 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Ashraphijuo", "Morteza", ""], ["Wang", "Xiaodong", ""]]}]