[{"id": "1608.00075", "submitter": "Renbo Zhao", "authors": "Renbo Zhao, Vincent Y. F. Tan, Huan Xu", "title": "Online Nonnegative Matrix Factorization with General Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a unified and systematic framework for performing online\nnonnegative matrix factorization under a wide variety of important divergences.\nThe online nature of our algorithm makes it particularly amenable to\nlarge-scale data. We prove that the sequence of learned dictionaries converges\nalmost surely to the set of critical points of the expected loss function. We\ndo so by leveraging the theory of stochastic approximations and projected\ndynamical systems. This result substantially generalizes the previous results\nobtained only for the squared-$\\ell_2$ loss. Moreover, the novel techniques\ninvolved in our analysis open new avenues for analyzing similar matrix\nfactorization problems. The computational efficiency and the quality of the\nlearned dictionary of our algorithm are verified empirically on both synthetic\nand real datasets. In particular, on the tasks of topic learning, shadow\nremoval and image denoising, our algorithm achieves superior trade-offs between\nthe quality of learned dictionary and running time over the batch and other\nonline NMF algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 06:07:38 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 02:36:50 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Zhao", "Renbo", ""], ["Tan", "Vincent Y. F.", ""], ["Xu", "Huan", ""]]}, {"id": "1608.00117", "submitter": "Jack Fitzsimons", "authors": "J. K. Fitzsimons, M. A. Osborne, S. J. Roberts and J. F. Fitzsimons", "title": "Improved stochastic trace estimation using mutually unbiased bases", "comments": "5 pages, 1 figure, 2 tables. Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS math.NA quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of estimating the trace of a matrix $A$ when given\naccess to an oracle which computes $x^\\dagger A x$ for an input vector $x$. We\nmake use of the basis vectors from a set of mutually unbiased bases, widely\nstudied in the field of quantum information processing, in the selection of\nprobing vectors $x$. This approach offers a new state of the art single shot\nsampling variance while requiring only $O(\\log(n))$ random bits to generate\neach vector. This significantly improves on traditional methods such as\nHutchinson's and Gaussian estimators in terms of the number of random bits\nrequired and worst case sample variance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 13:06:05 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Fitzsimons", "J. K.", ""], ["Osborne", "M. A.", ""], ["Roberts", "S. J.", ""], ["Fitzsimons", "J. F.", ""]]}, {"id": "1608.00135", "submitter": "Amaury Pouly", "authors": "Amaury Pouly", "title": "Computational complexity of solving polynomial differential equations\n  over unbounded domains with non-rational coefficients", "comments": "This is a note that extends arXiv:1409.0451", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we extend the result of \\cite{PoulyG16} about the complexity of\nsolving polynomial differential equations over unbounded domains to work with\nnon-rational input. In order to deal with arbitrary input, we phrase the result\nin framework of Conputable Analysis \\cite{Ko91}. As a side result, we also get\na uniform result about complexity of the operator, and not just about the\nsolution.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 15:57:16 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Pouly", "Amaury", ""]]}, {"id": "1608.00351", "submitter": "Tengfei Ma", "authors": "Tengfei Ma", "title": "Accelerated Kaczmarz Algorithms using History Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kaczmarz algorithm is a well known iterative method for solving\noverdetermined linear systems. Its randomized version yields provably\nexponential convergence in expectation. In this paper, we propose two new\nmethods to speed up the randomized Kaczmarz algorithm by utilizing the past\nestimates in the iterations. The first one utilize the past estimates to get a\npreconditioner. The second one combines the stochastic average gradient (SAG)\nmethod with the randomized Kaczmarz algorithm. It takes advantage of past\ngradients to improve the convergence speed. Numerical experiments indicate that\nthe new algorithms can dramatically outperform the standard randomized Kaczmarz\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 08:38:12 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ma", "Tengfei", ""]]}, {"id": "1608.00451", "submitter": "Avanti Athreya", "authors": "Avanti Athreya, Michael Kane, Bryan Lewis, Zachary Lubberts, Vince\n  Lyzinski, Youngser Park, Carey E. Priebe, and Minh Tang", "title": "Numerical tolerance for spectral decompositions of random matrices", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We precisely quantify the impact of statistical error in the quality of a\nnumerical approximation to a random matrix eigendecomposition, and under mild\nconditions, we use this to introduce an optimal numerical tolerance for\nresidual error in spectral decompositions of random matrices. We demonstrate\nthat terminating an eigendecomposition algorithm when the numerical error and\nstatistical error are of the same order results in computational savings with\nno loss of accuracy. We also repair a flaw in a ubiquitous termination\ncondition, one in wide employ in several computational linear algebra\nimplementations. We illustrate the practical consequences of our stopping\ncriterion with an analysis of simulated and real networks. Our theoretical\nresults and real-data examples establish that the tradeoff between statistical\nand numerical error is of significant import for data science.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 14:46:02 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 16:37:56 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 00:09:09 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Athreya", "Avanti", ""], ["Kane", "Michael", ""], ["Lewis", "Bryan", ""], ["Lubberts", "Zachary", ""], ["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""], ["Tang", "Minh", ""]]}, {"id": "1608.00514", "submitter": "Alireza Davoudi", "authors": "Alireza Davoudi, Saeed Shiry Ghidary, Khadijeh Sadatnejad", "title": "Dimensionality reduction based on Distance Preservation to Local Mean\n  (DPLM) for SPD matrices and its application in BCI", "comments": null, "journal-ref": null, "doi": "10.1088/1741-2552/aa61bb", "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a nonlinear dimensionality reduction algorithm for\nthe manifold of Symmetric Positive Definite (SPD) matrices that considers the\ngeometry of SPD matrices and provides a low dimensional representation of the\nmanifold with high class discrimination. The proposed algorithm, tries to\npreserve the local structure of the data by preserving distance to local mean\n(DPLM) and also provides an implicit projection matrix. DPLM is linear in terms\nof the number of training samples and may use the label information when they\nare available in order to performance improvement in classification tasks. We\nperformed several experiments on the multi-class dataset IIa from BCI\ncompetition IV. The results show that our approach as dimensionality reduction\ntechnique - leads to superior results in comparison with other competitor in\nthe related literature because of its robustness against outliers. The\nexperiments confirm that the combination of DPLM with FGMDM as the classifier\nleads to the state of the art performance on this dataset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 15:17:16 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Davoudi", "Alireza", ""], ["Ghidary", "Saeed Shiry", ""], ["Sadatnejad", "Khadijeh", ""]]}, {"id": "1608.00644", "submitter": "Yangang Chen", "authors": "Yangang Chen, Justin W. L. Wan, Jessey Lin", "title": "Monotone Mixed Finite Difference Scheme for Monge-Amp\\`ere Equation", "comments": null, "journal-ref": "Journal of Scientific Computing, 76(3), pp.1839-1867 (2018)", "doi": "10.1007/s10915-018-0685-y", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a monotone mixed finite difference scheme for\nsolving the two-dimensional Monge-Amp\\`ere equation. In order to accomplish\nthis, we convert the Monge-Amp\\`ere equation to an equivalent\nHamilton-Jacobi-Bellman (HJB) equation. Based on the HJB formulation, we apply\nthe standard 7-point stencil discretization, which is second order accurate, to\nthe grid points wherever monotonicity holds, and apply semi-Lagrangian wide\nstencil discretization elsewhere to ensure monotonicity on the entire\ncomputational domain. By dividing the admissible control set into six regions\nand optimizing the sub-problem in each region, the computational cost of the\noptimization problem at each grid point is reduced from $O(M^2)$ to $O(1)$ when\nthe standard 7-point stencil discretization is applied and to $O(M)$ otherwise,\nwhere the discretized control set is $M \\times M$. We prove that our numerical\nscheme satisfies consistency, stability, monotonicity and strong comparison\nprinciple, and hence is convergent to the viscosity solution of the\nMonge-Amp\\`ere equation. In the numerical results, second order convergence\nrate is achieved when the standard 7-point stencil discretization is applied\nmonotonically on the entire computation domain, and up to order one convergence\nis achieved otherwise. The proposed mixed scheme yields a smaller\ndiscretization error and a faster convergence rate compared to the pure\nsemi-Lagrangian wide stencil scheme.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 23:51:18 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 22:53:32 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 05:55:05 GMT"}, {"version": "v4", "created": "Mon, 7 Oct 2019 02:52:00 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chen", "Yangang", ""], ["Wan", "Justin W. L.", ""], ["Lin", "Jessey", ""]]}, {"id": "1608.01372", "submitter": "Giovanni Barbarino", "authors": "Giovanni Barbarino", "title": "Permutation NMF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization(NMF) is a common used technique in machine\nlearning to extract features out of data such as text documents and images\nthanks to its natural clustering properties. In particular, it is popular in\nimage processing since it can decompose several pictures and recognize common\nparts if they're located in the same position over the photos. This paper's aim\nis to present a way to add the translation invariance to the classical NMF,\nthat is, the algorithms presented are able to detect common features, even when\nthey're shifted, in different original images.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 21:59:44 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Barbarino", "Giovanni", ""]]}, {"id": "1608.02166", "submitter": "Ricardo Monge", "authors": "Osvaldo Skliar, Ricardo E. Monge, Sherry Gapper", "title": "Analysis of time series and signals using the Square Wave Method", "comments": "22 pages, 6 figures. Supplementary data is available here:\n  http://www.appliedmathgroup.org/en/data.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Square Wave Method (SWM), previously introduced for the analysis of\nsignals and images, is presented here as a mathematical tool suitable for the\nanalysis of time series and signals. To show the potential that the SWM has to\nanalyze many different types of time series, the results of the analysis of a\ntime series composed of a sequence of 10,000 numerical values are presented\nhere. These values were generated by using the Mathematical Random Number\nGenerator (MRNG).\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 00:48:57 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 22:34:02 GMT"}, {"version": "v3", "created": "Tue, 16 Aug 2016 14:51:16 GMT"}, {"version": "v4", "created": "Fri, 26 Aug 2016 12:32:22 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Skliar", "Osvaldo", ""], ["Monge", "Ricardo E.", ""], ["Gapper", "Sherry", ""]]}, {"id": "1608.02461", "submitter": "Huda Ibeid", "authors": "Huda Ibeid, Rio Yokota, David Keyes", "title": "A Matrix-free Preconditioner for the Helmholtz Equation based on the\n  Fast Multipole Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast multipole methods (FMM) were originally developed for accelerating\n$N$-body problems for particle-based methods. FMM is more than an $N$-body\nsolver, however. Recent efforts to view the FMM as an elliptic Partial\nDifferential Equation (PDE) solver have opened the possibility to use it as a\npreconditioner for a broader range of applications. FMM can solve Helmholtz\nproblems with optimal $\\mathcal{O}(N \\log N)$ complexity, has compute-bound\ninner kernels, and highly asynchronous communication patterns. The combination\nof these features makes FMM an interesting candidate as a preconditioner for\nsparse solvers on architectures of the future. The use of FMM as a\npreconditioner allows us to use lower order multipole expansions than would be\nrequired as a solver because individual solves need not be accurate. This\nreduces the amount of computation and communication significantly and makes the\ntime-to-solution competitive with state-of-the-art preconditioners.\nFurthermore, the high asynchronicity of FMM allows it to scale to much larger\ncore counts than factorization-based and multilevel methods. We describe our\ntests in reproducible details with freely available codes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 14:48:25 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Ibeid", "Huda", ""], ["Yokota", "Rio", ""], ["Keyes", "David", ""]]}, {"id": "1608.02567", "submitter": "Nathan Roberts", "authors": "Nathan V. Roberts and Jesse Chan", "title": "A Geometric Multigrid Preconditioning Strategy for DPG System Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discontinuous Petrov-Galerkin (DPG) methodology of Demkowicz and\nGopalakrishnan [15,17] guarantees the optimality of the solution in an energy\nnorm, and provides several features facilitating adaptive schemes. A key\nquestion that has not yet been answered in general - though there are some\nresults for Poisson, e.g. - is how best to precondition the DPG system matrix,\nso that iterative solvers may be used to allow solution of large-scale\nproblems.\n  In this paper, we detail a strategy for preconditioning the DPG system matrix\nusing geometric multigrid which we have implemented as part of Camellia [26],\nand demonstrate through numerical experiments its effectiveness in the context\nof several variational formulations. We observe that in some of our\nexperiments, the behavior of the preconditioner is closely tied to the discrete\ntest space enrichment.\n  We include experiments involving adaptive meshes with hanging nodes for\nlid-driven cavity flow, demonstrating that the preconditioners can be applied\nin the context of challenging problems. We also include a scalability study\ndemonstrating that the approach - and our implementation - scales well to many\nMPI ranks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 19:38:12 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Roberts", "Nathan V.", ""], ["Chan", "Jesse", ""]]}, {"id": "1608.02702", "submitter": "Boris Landa", "authors": "Boris Landa and Yoel Shkolnisky", "title": "Steerable Principal Components for Space-Frequency Localized Images", "comments": null, "journal-ref": null, "doi": "10.1137/16M1085334", "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a fast and accurate method for obtaining steerable\nprincipal components from a large dataset of images, assuming the images are\nwell localized in space and frequency. The obtained steerable principal\ncomponents are optimal for expanding the images in the dataset and all of their\nrotations. The method relies upon first expanding the images using a series of\ntwo-dimensional Prolate Spheroidal Wave Functions (PSWFs), where the expansion\ncoefficients are evaluated using a specially designed numerical integration\nscheme. Then, the expansion coefficients are used to construct a\nrotationally-invariant covariance matrix which admits a block-diagonal\nstructure, and the eigen-decomposition of its blocks provides us with the\ndesired steerable principal components. The proposed method is shown to be\nfaster then existing methods, while providing appropriate error bounds which\nguarantee its accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 06:35:09 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 06:12:48 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Landa", "Boris", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1608.02875", "submitter": "Haishan Ye", "authors": "Haishan Ye, Luo Luo and Zhihua Zhang", "title": "Revisiting Sub-sampled Newton Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many machine learning models depend on solving a large scale optimization\nproblem. Recently, sub-sampled Newton methods have emerged to attract much\nattention for optimization due to their efficiency at each iteration, rectified\na weakness in the ordinary Newton method of suffering a high cost at each\niteration while commanding a high convergence rate. In this work we propose two\nnew efficient Newton-type methods, Refined Sub-sampled Newton and Refined\nSketch Newton. Our methods exhibit a great advantage over existing sub-sampled\nNewton methods, especially when Hessian-vector multiplication can be calculated\nefficiently. Specifically, the proposed methods are shown to converge\nsuperlinearly in general case and quadratically under a little stronger\nassumption. The proposed methods can be generalized to a unifying framework for\nthe convergence proof of several existing sub-sampled Newton methods, revealing\nnew convergence properties. Finally, we empirically evaluate the performance of\nour methods on several standard datasets and the results show consistent\nimprovement in computational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 03:30:12 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 03:57:35 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Ye", "Haishan", ""], ["Luo", "Luo", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1608.04361", "submitter": "Tao Wu", "authors": "Tao Wu, David F. Gleich", "title": "Multi-way Monte Carlo Method for Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Monte Carlo method for solving a linear system of the form $x =\nH x + b$. A sufficient condition for the method to work is $\\| H \\| < 1$, which\ngreatly limits the usability of this method. We improve this condition by\nproposing a new multi-way Markov random walk, which is a generalization of the\nstandard Markov random walk. Under our new framework we prove that the\nnecessary and sufficient condition for our method to work is the spectral\nradius $\\rho(H^{+}) < 1$, which is a weaker requirement than $\\| H \\| < 1$. In\naddition to solving more problems, our new method can work faster than the\nstandard algorithm. In numerical experiments on both synthetic and real world\nmatrices, we demonstrate the effectiveness of our new method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:45:08 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Wu", "Tao", ""], ["Gleich", "David F.", ""]]}, {"id": "1608.04571", "submitter": "Alessandro Cultrera", "authors": "Alessandro Cultrera and Luca Callegaro", "title": "A simple algorithm to find the L-curve corner in the regularisation of\n  ill-posed inverse problems", "comments": "11 pages, 6 figures. Keywords: inverse problem, ill-posed problem,\n  regularization parameter, L-curve, golden section search, Menger curvature,\n  electrical resistance tomography", "journal-ref": "IOP SciNotes, 1 (2020) 025004", "doi": "10.1088/2633-1357/abad0d", "report-no": null, "categories": "math.NA cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simple algorithm to locate the \"corner\" of an L-curve, a\nfunction often used to select the regularisation parameter for the solution of\nill-posed inverse problems. The algorithm involves the Menger curvature of a\ncircumcircle and the golden section search method. It efficiently finds the\nregularisation parameter value corresponding to the maximum positive curvature\nregion of the L-curve. The algorithm is applied to some commonly available test\nproblems and compared to the typical way of locating the l-curve corner by\nmeans of its analytical curvature. The application of the algorithm to the data\nprocessing of an electrical resistance tomography experiment on thin conductive\nfilms is also reported.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 12:32:47 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 11:22:30 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 10:56:42 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Cultrera", "Alessandro", ""], ["Callegaro", "Luca", ""]]}, {"id": "1608.04834", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Asymptotic approximation of central binomial coefficients with rigorous\n  error bounds", "comments": "11 pages, 1 table; added more references in v3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.CA math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a well-known asymptotic series for the logarithm of the central\nbinomial coefficient is strictly enveloping in the sense of P\\'olya and\nSzeg\\\"o, so the error incurred in truncating the series is of the same sign as\nthe next term, and is bounded in magnitude by that term. We consider closely\nrelated asymptotic series for Binet's function, for $\\ln\\Gamma(z+1/2)$, and for\nthe Riemann-Siegel theta function, and make some historical remarks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 02:12:14 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 03:06:28 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 13:28:21 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1608.05230", "submitter": "Hiroki Sumi", "authors": "Hiroki Sumi", "title": "Negativity of Lyapunov Exponents and Convergence of Generic Random\n  Polynomial Dynamical Systems and Random Relaxed Newton's Methods", "comments": "Published in Communications in Mathematical Physics, 384, 1513--1583,\n  2021. 63 pages. See also http://www.math.h.kyoto-u.ac.jp/~sumi/index.html", "journal-ref": "Communications in Mathematical Physics, 384, 1513--1583, 2021", "doi": "10.1007/s00220-021-04070-6", "report-no": null, "categories": "math.DS cs.NA math.CV math.GT math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate i.i.d. random complex dynamical systems generated by\nprobability measures on finite unions of the loci of holomorphic families of\nrational maps on the Riemann sphere. We show that under certain conditions on\nthe families, for a generic system, (especially, for a generic random\npolynomial dynamical system,) for all but countably many initial values $z$ in\nthe Riemann sphere, for almost every sequence of maps $\\gamma\n=(\\gamma_{1},\\gamma_{2},\\ldots )$, the Lyapunov exponent of $\\gamma $ at $z$ is\nnegative. Also, we show that for a generic system, for every initial value $z$\nin the Riemann sphere, the orbit of the Dirac measure at $z$ under the\niteration of the dual map of the transition operator tends to a periodic cycle\nof measures in the space of probability measures on the Riemann sphere. Note\nthat these are new phenomena in random complex dynamics which cannot hold in\ndeterministic complex dynamical systems. We apply the above theory and results\nof random complex dynamical systems to finding roots of any polynomials by\nrandom relaxed Newton's methods and we show that for any polynomial $g$, for\nany initial value $z$ in the complex plane which is not a root of $g'$, the\nrandom orbit starting with $z$ tends to a root of $g$ almost surely, which is\nthe virtue of the effect of randomness.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 10:30:15 GMT"}, {"version": "v10", "created": "Sat, 5 Sep 2020 13:19:27 GMT"}, {"version": "v11", "created": "Tue, 17 Nov 2020 07:43:14 GMT"}, {"version": "v12", "created": "Thu, 4 Mar 2021 08:34:32 GMT"}, {"version": "v13", "created": "Tue, 25 May 2021 04:14:57 GMT"}, {"version": "v14", "created": "Mon, 21 Jun 2021 05:42:22 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 12:15:48 GMT"}, {"version": "v3", "created": "Fri, 21 Oct 2016 06:31:03 GMT"}, {"version": "v4", "created": "Mon, 5 Jun 2017 12:09:19 GMT"}, {"version": "v5", "created": "Mon, 20 Nov 2017 11:27:41 GMT"}, {"version": "v6", "created": "Sat, 2 Jun 2018 14:33:08 GMT"}, {"version": "v7", "created": "Thu, 18 Jul 2019 10:37:50 GMT"}, {"version": "v8", "created": "Tue, 27 Aug 2019 11:30:08 GMT"}, {"version": "v9", "created": "Wed, 28 Aug 2019 06:26:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sumi", "Hiroki", ""]]}, {"id": "1608.05754", "submitter": "Shashanka Ubaru", "authors": "Shashanka Ubaru, Yousef Saad, Abd-Krim Seghouane", "title": "Fast estimation of approximate matrix ranks using spectral densities", "comments": null, "journal-ref": "Neural Computation, Vol. 29, No. 5, pp. 1317-1351 (May 2017)", "doi": "10.1162/NECO_a_00951", "report-no": null, "categories": "cs.NA cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning and data related applications, it is required to\nhave the knowledge of approximate ranks of large data matrices at hand. In this\npaper, we present two computationally inexpensive techniques to estimate the\napproximate ranks of such large matrices. These techniques exploit approximate\nspectral densities, popular in physics, which are probability density\ndistributions that measure the likelihood of finding eigenvalues of the matrix\nat a given point on the real line. Integrating the spectral density over an\ninterval gives the eigenvalue count of the matrix in that interval. Therefore\nthe rank can be approximated by integrating the spectral density over a\ncarefully selected interval. Two different approaches are discussed to estimate\nthe approximate rank, one based on Chebyshev polynomials and the other based on\nthe Lanczos algorithm. In order to obtain the appropriate interval, it is\nnecessary to locate a gap between the eigenvalues that correspond to noise and\nthe relevant eigenvalues that contribute to the matrix rank. A method for\nlocating this gap and selecting the interval of integration is proposed based\non the plot of the spectral density. Numerical experiments illustrate the\nperformance of these techniques on matrices from typical applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 23:07:08 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Ubaru", "Shashanka", ""], ["Saad", "Yousef", ""], ["Seghouane", "Abd-Krim", ""]]}, {"id": "1608.05787", "submitter": "Martin Ziegler", "authors": "Sewon Park, Franz Brau{\\ss}e, Pieter Collins, SunYoung Kim, Michal\n  Kone\\v{c}n\\'y, Gyesik Lee, Norbert M\\\"uller, Eike Neumann, Norbert Preining,\n  Martin Ziegler", "title": "Foundation of Computer (Algebra) ANALYSIS Systems: Semantics, Logic,\n  Programming, Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a semantics of operating on real numbers that is sound,\nTuring-complete, and practical. It modifies the intuitive but super-recursive\nBlum-Shub-Smale model (formalizing Computer ALGEBRA Systems), to coincide in\npower with the realistic but inconvenient Type-2 Turing machine underlying\nComputable Analysis: reconciling both as foundation to a Computer ANALYSIS\nSystem.\n  Several examples illustrate the elegance of rigorous numerical coding in this\nframework, formalized as a simple imperative programming language ERC with\ndenotational semantics for REALIZING a real function $f$: arguments $x$ are\ngiven as exact real numbers, while values $y=f(x)$ suffice to be returned\napproximately up to absolute error $2^p$ with respect to an additionally given\ninteger parameter $p\\to-\\infty$. Real comparison (necessarily) becomes partial,\npossibly 'returning' the lazy Kleenean value UNKNOWN (subtly different from\n$\\bot$ for classically undefined expressions like 1/0). This asserts closure\nunder composition, and in fact 'Turing-completeness over the reals': All and\nonly functions computable in the sense of Computable Analysis can be realized\nin ERC. Programs thus operate on a many-sorted structure involving real numbers\nand integers, the latter connected via the 'error' embedding $Z\\ni p\\mapsto\n2^p\\in R$, whose first-order theory is proven decidable and model-complete.\nThis logic serves for formally specifying and formally verifying correctness of\nERC programs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 06:12:45 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 19:12:18 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2018 14:24:05 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 06:25:18 GMT"}, {"version": "v5", "created": "Mon, 14 Sep 2020 01:59:03 GMT"}, {"version": "v6", "created": "Mon, 7 Jun 2021 15:55:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Park", "Sewon", ""], ["Brau\u00dfe", "Franz", ""], ["Collins", "Pieter", ""], ["Kim", "SunYoung", ""], ["Kone\u010dn\u00fd", "Michal", ""], ["Lee", "Gyesik", ""], ["M\u00fcller", "Norbert", ""], ["Neumann", "Eike", ""], ["Preining", "Norbert", ""], ["Ziegler", "Martin", ""]]}, {"id": "1608.06473", "submitter": "Simon Bauer", "authors": "Simon Bauer, Marcus Mohr, Ulrich R\\\"ude, Jens Weism\\\"uller, Markus\n  Wittmann, Barbara Wohlmuth", "title": "A two-scale approach for efficient on-the-fly operator assembly in\n  massively parallel high performance multigrid codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-free finite element implementations of massively parallel geometric\nmultigrid save memory and are often significantly faster than implementations\nusing classical sparse matrix techniques. They are especially well suited for\nhierarchical hybrid grids on polyhedral domains. In the case of constant\ncoefficients all fine grid node stencils in the interior of a coarse macro\nelement are equal. However, for non-polyhedral domains the situation changes.\nThen even for the Laplace operator, the non-linear element mapping leads to\nfine grid stencils that can vary from grid point to grid point. This\nobservation motivates a new two-scale approach that exploits a piecewise\npolynomial approximation of the fine grid operator with respect to the coarse\nmesh size. The low-cost evaluation of these surrogate polynomials results in an\nefficient stencil assembly on-the-fly for non-polyhedral domains that can be\nsignificantly more efficient than matrix-free techniques that are based on an\nelement-wise assembly. The performance analysis and additional hardware-aware\ncode optimizations are based on the Execution-Cache-Memory model. Several\naspects such as two-scale a priori error bounds and double discretization\ntechniques are presented. Weak and strong scaling results illustrate the\nbenefits of the new technique when used within large scale PDE solvers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 11:43:20 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Bauer", "Simon", ""], ["Mohr", "Marcus", ""], ["R\u00fcde", "Ulrich", ""], ["Weism\u00fcller", "Jens", ""], ["Wittmann", "Markus", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1608.06691", "submitter": "Guangning Tan", "authors": "Guangning Tan, Nedialko S. Nedialkov, John D. Pryce", "title": "Conversion Methods for Improving Structural Analysis of\n  Differential-Algebraic Equation Systems", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential-algebraic equation systems (DAEs) are generated routinely by\nsimulation and modeling environments. Before a simulation starts and a\nnumerical method is applied, some kind of structural analysis (SA) is used to\ndetermine which equations to be differentiated, and how many times. Both\nPantelides's algorithm and Pryce's $\\Sigma$-method are equivalent: if one of\nthem finds correct structural information, the other does also. Nonsingularity\nof the Jacobian produced by SA indicates a success, which occurs on many\nproblems of interest. However, these methods can fail on simple, solvable DAEs\nand give incorrect structural information including the index. This article\ninvestigates $\\Sigma$-method's failures and presents two conversion methods for\nfixing them. Both methods convert a DAE on which the $\\Sigma$-method fails to\nan equivalent problem on which this SA is more likely to succeed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 02:20:55 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Tan", "Guangning", ""], ["Nedialkov", "Nedialko S.", ""], ["Pryce", "John D.", ""]]}, {"id": "1608.06693", "submitter": "Guangning Tan", "authors": "Guangning Tan, Nedialko S. Nedialkov, John D. Pryce", "title": "Conversion Methods, Block Triangularization, and Structural Analysis of\n  Differential-Algebraic Equation Systems", "comments": "25 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous article, the authors developed two conversion methods to\nimprove the $\\Sigma$-method for structural analysis (SA) of\ndifferential-algebraic equations (DAEs). These methods reformulate a DAE on\nwhich the $\\Sigma$-method fails into an equivalent problem on which this SA is\nmore likely to succeed with a generically nonsingular Jacobian. The basic\nversion of these methods processes the DAE as a whole. This article presents\nthe block version that exploits block triangularization of a DAE. Using a block\ntriangular form of a Jacobian sparsity pattern, we identify which diagonal\nblocks of the Jacobian are identically singular and then perform a conversion\non each such block. This approach improves the efficiency of finding a suitable\nconversion for fixing SA's failures. All of our conversion methods can be\nimplemented in a computer algebra system so that every conversion can be\nautomated.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 02:32:50 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Tan", "Guangning", ""], ["Nedialkov", "Nedialko S.", ""], ["Pryce", "John D.", ""]]}, {"id": "1608.07641", "submitter": "Deanna Needell", "authors": "Deanna Needell and Rachel Ward", "title": "Batched Stochastic Gradient Descent with Weighted Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a batched variant of Stochastic Gradient Descent (SGD) with\nweighted sampling distribution for smooth and non-smooth objective functions.\nWe show that by distributing the batches computationally, a significant speedup\nin the convergence rate is provably possible compared to either batched\nsampling or weighted sampling alone. We propose several computationally\nefficient schemes to approximate the optimal weights, and compute proposed\nsampling distributions explicitly for the least squares and hinge loss\nproblems. We show both analytically and experimentally that substantial gains\ncan be obtained.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 00:51:16 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 21:47:13 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Needell", "Deanna", ""], ["Ward", "Rachel", ""]]}]