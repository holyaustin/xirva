[{"id": "1510.00844", "submitter": "Aydin Buluc", "authors": "Ariful Azad, Grey Ballard, Aydin Buluc, James Demmel, Laura Grigori,\n  Oded Schwartz, Sivan Toledo, Samuel Williams", "title": "Exploiting Multiple Levels of Parallelism in Sparse Matrix-Matrix\n  Multiplication", "comments": null, "journal-ref": "SIAM Journal of Scientific Computing, Volume 38, Number 6, pp.\n  C624-C651, 2016", "doi": "10.1137/15M104253X", "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many\nhigh-performance graph algorithms as well as for some linear solvers, such as\nalgebraic multigrid. The scaling of existing parallel implementations of SpGEMM\nis heavily bound by communication. Even though 3D (or 2.5D) algorithms have\nbeen proposed and theoretically analyzed in the flat MPI model on Erdos-Renyi\nmatrices, those algorithms had not been implemented in practice and their\ncomplexities had not been analyzed for the general case. In this work, we\npresent the first ever implementation of the 3D SpGEMM formulation that also\nexploits multiple (intra-node and inter-node) levels of parallelism, achieving\nsignificant speedups over the state-of-the-art publicly available codes at all\nlevels of concurrencies. We extensively evaluate our implementation and\nidentify bottlenecks that should be subject to further research.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 16:32:43 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 17:59:17 GMT"}, {"version": "v3", "created": "Wed, 16 Nov 2016 23:19:52 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Azad", "Ariful", ""], ["Ballard", "Grey", ""], ["Buluc", "Aydin", ""], ["Demmel", "James", ""], ["Grigori", "Laura", ""], ["Schwartz", "Oded", ""], ["Toledo", "Sivan", ""], ["Williams", "Samuel", ""]]}, {"id": "1510.01025", "submitter": "Anthony Man-Cho So", "authors": "Huikang Liu and Weijie Wu and Anthony Man-Cho So", "title": "Quadratic Optimization with Orthogonality Constraints: Explicit\n  Lojasiewicz Exponent and Linear Convergence of Line-Search Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental class of matrix optimization problems that arise in many areas\nof science and engineering is that of quadratic optimization with orthogonality\nconstraints. Such problems can be solved using line-search methods on the\nStiefel manifold, which are known to converge globally under mild conditions.\nTo determine the convergence rate of these methods, we give an explicit\nestimate of the exponent in a Lojasiewicz inequality for the (non-convex) set\nof critical points of the aforementioned class of problems. By combining such\nan estimate with known arguments, we are able to establish the linear\nconvergence of a large class of line-search methods. A key step in our proof is\nto establish a local error bound for the set of critical points, which may be\nof independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 04:14:22 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Liu", "Huikang", ""], ["Wu", "Weijie", ""], ["So", "Anthony Man-Cho", ""]]}, {"id": "1510.01077", "submitter": "Guy Gilboa", "authors": "Guy Gilboa, Michael Moeller, Martin Burger", "title": "Nonlinear Spectral Analysis via One-homogeneous Functionals - Overview\n  and Future Prospects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SP cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper the motivation and theory of nonlinear spectral\nrepresentations, based on convex regularizing functionals. Some comparisons and\nanalogies are drawn to the fields of signal processing, harmonic analysis and\nsparse representations. The basic approach, main results and initial\napplications are shown. A discussion of open problems and future directions\nconcludes this work.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 09:44:13 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Gilboa", "Guy", ""], ["Moeller", "Michael", ""], ["Burger", "Martin", ""]]}, {"id": "1510.01118", "submitter": "Sokolov Dmitry", "authors": "Nicolas Ray (ALICE), Sokolov Dmitry (ALICE)", "title": "Illustration of iterative linear solver behavior on simple 1D and 2D\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In geometry processing, numerical optimization methods often involve solving\nsparse linear systems of equations. These linear systems have a structure that\nstrongly resembles to adjacency graphs of the underlying mesh. We observe how\nclassic linear solvers behave on this specific type of problems. For the sake\nof simplicity, we minimise either the squared gradient or the squared\nLaplacian, evaluated by finite differences on a regular 1D or 2D grid. We\nobserved the evolution of the solution for both energies, in 1D and 2D, and\nwith different solvers: Jacobi, Gauss-Seidel, SSOR (Symmetric successive\nover-relaxation) and CG (conjugate gradient [She94]). Plotting results at\ndifferent iterations allows to have an intuition of the behavior of these\nclassic solvers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 12:11:43 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Ray", "Nicolas", "", "ALICE"], ["Dmitry", "Sokolov", "", "ALICE"]]}, {"id": "1510.01145", "submitter": "Daniel Sorin", "authors": "Yaqi Zhang, Ralph Nathan, Daniel J. Sorin", "title": "Reduced Precision Checking to Detect Errors in Floating Point Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use reduced precision checking (RPC) to detect errors in\nfloating point arithmetic. Prior work explored RPC for addition and\nmultiplication. In this work, we extend RPC to a complete floating point unit\n(FPU), including division and square root, and we present precise analyses of\nthe errors undetectable with RPC that show bounds that are smaller than prior\nwork. We implement RPC for a complete FPU in RTL and experimentally evaluate\nits error coverage and cost.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 13:38:23 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Zhang", "Yaqi", ""], ["Nathan", "Ralph", ""], ["Sorin", "Daniel J.", ""]]}, {"id": "1510.01728", "submitter": "Mouhacine Benosman", "authors": "Mouhacine Benosman, Boris Kramer, Petros Boufounos, Piyush Grover", "title": "Learning-based Reduced Order Model Stabilization for Partial\n  Differential Equations: Application to the Coupled Burgers Equation", "comments": null, "journal-ref": null, "doi": "10.1109/ACC.2016.7525157", "report-no": null, "categories": "cs.SY cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results on stabilization for reduced order models (ROM) of partial\ndifferential equations using learning. Stabilization is achieved via closure\nmodels for ROMs, where we use a model-free extremum seeking (ES) dither-based\nalgorithm to learn the best closure models' parameters, for optimal ROM\nstabilization. We first propose to auto-tune linear closure models using ES,\nand then extend the results to a closure model combining linear and nonlinear\nterms, for better stabilization performance. The coupled Burgers' equation is\nemployed as a test-bed for the proposed tuning method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 19:53:40 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Benosman", "Mouhacine", ""], ["Kramer", "Boris", ""], ["Boufounos", "Petros", ""], ["Grover", "Piyush", ""]]}, {"id": "1510.01873", "submitter": "Zhihui Liu", "authors": "Yanzhao Cao, Jialin Hong, and Zhihui Liu", "title": "Well-posedness and Finite Element Approximations for Elliptic SPDEs with\n  Gaussian Noises", "comments": null, "journal-ref": "Commun. Math. Res. 36 (2020), no. 2, 113--127", "doi": "10.4208/cmr.2020-0006", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies the well-posedness and optimal error estimates of spectral\nfinite element approximations for the boundary value problems of semi-linear\nelliptic SPDEs driven by white or colored Gaussian noises. The noise term is\napproximated through the spectral projection of the covariance operator, which\nis not required to be commutative with the Laplacian operator. Through the\nconvergence analysis of SPDEs with the noise terms replaced by the projected\nnoises, the well-posedness of the SPDE is established under certain covariance\noperator-dependent conditions. These SPDEs with projected noises are then\nnumerically approximated with the finite element method. A general error\nestimate framework is established for the finite element approximations. Based\non this framework, optimal error estimates of finite element approximations for\nelliptic SPDEs driven by power-law noises are obtained. It is shown that with\nthe proposed approach, convergence order of white noise driven SPDEs is\nimproved by half for one-dimensional problems, and by an infinitesimal factor\nfor higher-dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 09:43:53 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 06:52:57 GMT"}, {"version": "v3", "created": "Tue, 5 Apr 2016 06:11:37 GMT"}, {"version": "v4", "created": "Fri, 20 Jan 2017 04:20:47 GMT"}, {"version": "v5", "created": "Tue, 30 Jan 2018 05:17:39 GMT"}, {"version": "v6", "created": "Sun, 24 May 2020 03:05:07 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Cao", "Yanzhao", ""], ["Hong", "Jialin", ""], ["Liu", "Zhihui", ""]]}, {"id": "1510.02975", "submitter": "Guillermo Gallego", "authors": "Daniel Berj\\'on, Guillermo Gallego, Carlos Cuevas, Francisco Mor\\'an\n  and Narciso Garc\\'ia", "title": "Optimal Piecewise Linear Function Approximation for GPU-based\n  Applications", "comments": "12 pages, 12 figures, post-print, IEEE Transactions on Cybernetics,\n  Oct. 2015", "journal-ref": "IEEE Transactions on Cybernetics, vol. 46, no. 11, pp. 2584-2595,\n  Nov. 2016", "doi": "10.1109/TCYB.2015.2482365", "report-no": null, "categories": "math.OC cs.CV cs.DC cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision and human-computer interaction applications developed in\nrecent years need evaluating complex and continuous mathematical functions as\nan essential step toward proper operation. However, rigorous evaluation of this\nkind of functions often implies a very high computational cost, unacceptable in\nreal-time applications. To alleviate this problem, functions are commonly\napproximated by simpler piecewise-polynomial representations. Following this\nidea, we propose a novel, efficient, and practical technique to evaluate\ncomplex and continuous functions using a nearly optimal design of two types of\npiecewise linear approximations in the case of a large budget of evaluation\nsubintervals. To this end, we develop a thorough error analysis that yields\nasymptotically tight bounds to accurately quantify the approximation\nperformance of both representations. It provides an improvement upon previous\nerror estimates and allows the user to control the trade-off between the\napproximation error and the number of evaluation subintervals. To guarantee\nreal-time operation, the method is suitable for, but not limited to, an\nefficient implementation in modern Graphics Processing Units (GPUs), where it\noutperforms previous alternative approaches by exploiting the fixed-function\ninterpolation routines present in their texture units. The proposed technique\nis a perfect match for any application requiring the evaluation of continuous\nfunctions, we have measured in detail its quality and efficiency on several\nfunctions, and, in particular, the Gaussian function because it is extensively\nused in many areas of computer vision and cybernetics, and it is expensive to\nevaluate.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 20:49:17 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Berj\u00f3n", "Daniel", ""], ["Gallego", "Guillermo", ""], ["Cuevas", "Carlos", ""], ["Mor\u00e1n", "Francisco", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "1510.03588", "submitter": "Marie Doumic Jauffret", "authors": "Marie Doumic (MAMBA, LJLL), Miguel Escobedo", "title": "Time Asymptotics for a Critical Case in Fragmentation and\n  Growth-Fragmentation Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fragmentation and growth-fragmentation equations is a family of problems with\nvaried and wide applications. This paper is devoted to description of the long\ntime time asymptotics of two critical cases of these equations, when the\ndivision rate is constant and the growth rate is linear or zero. The study of\nthese cases may be reduced to the study of the following fragmentation\nequation:$$\\frac{\\partial}{\\partial t} u(t,x) + u(t,x)=\\int\\limits\\_x^\\infty\nk\\_0(\\frac{x}{y}) u(t,y) dy.$$Using the Mellin transform of the equation, we\ndetermine the long time behavior of the solutions. Our results show in\nparticular the strong dependence of this asymptotic behavior with respect to\nthe initial data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 09:27:42 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Doumic", "Marie", "", "MAMBA, LJLL"], ["Escobedo", "Miguel", ""]]}, {"id": "1510.04121", "submitter": "Igor Potapov", "authors": "Oleksiy Kurganskyy and Igor Potapov", "title": "Reachability problems for PAMs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise affine maps (PAMs) are frequently used as a reference model to show\nthe openness of the reachability questions in other systems. The reachability\nproblem for one-dimentional PAM is still open even if we define it with only\ntwo intervals. As the main contribution of this paper we introduce new\ntechniques for solving reachability problems based on p-adic norms and weights\nas well as showing decidability for two classes of maps. Then we show the\nconnections between topological properties for PAM's orbits, reachability\nproblems and representation of numbers in a rational base system. Finally we\nshow a particular instance where the uniform distribution of the original orbit\nmay not remain uniform or even dense after making regular shifts and taking a\nfractional part in that sequence.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 14:40:59 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Kurganskyy", "Oleksiy", ""], ["Potapov", "Igor", ""]]}, {"id": "1510.04591", "submitter": "Shengguo Li", "authors": "Shengguo Li, Xiangke Liao, Jie Liu, Hao Jiang", "title": "New fast divide-and-conquer algorithms for the symmetric tridiagonal\n  eigenvalue problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two accelerated divide-and-conquer algorithms are proposed for\nthe symmetric tridiagonal eigenvalue problem, which cost $O(N^2r)$ {flops} in\nthe worst case, where $N$ is the dimension of the matrix and $r$ is a modest\nnumber depending on the distribution of eigenvalues. Both of these algorithms\nuse hierarchically semiseparable (HSS) matrices to approximate some\nintermediate eigenvector matrices which are Cauchy-like matrices and are\noff-diagonally low-rank. The difference of these two versions lies in using\ndifferent HSS construction algorithms, one (denoted by {ADC1}) uses a\nstructured low-rank approximation method and the other ({ADC2}) uses a\nrandomized HSS construction algorithm. For the ADC2 algorithm, a method is\nproposed to estimate the off-diagonal rank. Numerous experiments have been done\nto show their stability and efficiency. These algorithms are implemented in\nparallel in a shared memory environment, and some parallel implementation\ndetails are included. Comparing the ADCs with highly optimized multithreaded\nlibraries such as Intel MKL, we find that ADCs could be more than 6x times\nfaster for some large matrices with few deflations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 15:42:18 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Li", "Shengguo", ""], ["Liao", "Xiangke", ""], ["Liu", "Jie", ""], ["Jiang", "Hao", ""]]}, {"id": "1510.04632", "submitter": "Tshilidzi Marwala", "authors": "Daniel J Joubert, Tshilidzi Marwala", "title": "Monte Carlo Dynamically Weighted Importance Sampling For Finite Element\n  Model Updating", "comments": "Submitted to the IMAC-XXXIV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Finite Element Method (FEM) is generally unable to accurately predict\nnatural frequencies and mode shapes of structures (eigenvalues and\neigenvectors). Engineers develop numerical methods and a variety of techniques\nto compensate for this misalignment of modal properties, between experimentally\nmeasured data and the computed result from the FEM of structures. In this paper\nwe compare two indirect methods of updating namely, the Adaptive Metropolis\nHastings and a newly applied algorithm called Monte Carlo Dynamically Weighted\nImportance Sampling (MCDWIS). The approximation of a posterior predictive\ndistribution is based on Bayesian inference of continuous multivariate Gaussian\nprobability density functions, defining the variability of physical properties\naffected by forced vibration. The motivation behind applying MCDWIS is in the\ncomplexity of computing normalizing constants in higher dimensional or\nmultimodal systems. The MCDWIS accounts for this intractability by analytically\ncomputing importance sampling estimates at each time step of the algorithm. In\naddition, a dynamic weighting step with an Adaptive Pruned Enriched Population\nControl Scheme (APEPCS) allows for further control over weighted samples and\npopulation size. The performance of the MCDWIS simulation is graphically\nillustrated for all algorithm dependent parameters and show unbiased, stable\nsample estimates.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 17:11:12 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Joubert", "Daniel J", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "1510.04658", "submitter": "James Fairbanks", "authors": "James P. Fairbanks and Geoffrey D. Sanders and David A. Bader", "title": "Spectral Partitioning with Blends of Eigenvectors", "comments": "32 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many common methods for data analysis rely on linear algebra. We provide new\nresults connecting data analysis error to numerical accuracy, which leads to\nthe first meaningful stopping criterion for two way spectral partitioning. More\ngenerally, we provide pointwise convergence guarantees so that blends (linear\ncombinations) of eigenvectors can be employed to solve data analysis problems\nwith confidence in their accuracy. We demonstrate this theory on an accessible\nmodel problem, the Ring of Cliques, by deriving the relevant eigenpairs and\ncomparing the predicted results to numerical solutions. These results bridge\nthe gap between linear algebra based data analysis methods and the convergence\ntheory of iterative approximation methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 18:36:51 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 18:00:34 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Fairbanks", "James P.", ""], ["Sanders", "Geoffrey D.", ""], ["Bader", "David A.", ""]]}, {"id": "1510.04853", "submitter": "Milan Hlad\\'ik", "authors": "Marzieh Dehghani-Madiseh and Milan Hlad\\'ik", "title": "Efficient Approaches for Enclosing the United Solution Set of the\n  Interval Generalized Sylvester Matrix Equation", "comments": null, "journal-ref": null, "doi": "10.1016/j.apnum.2017.12.003", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the interval generalized Sylvester matrix\nequation ${\\bf{A}}X{\\bf{B}}+{\\bf{C}}X{\\bf{D}}={\\bf{F}}$ and develop some\ntechniques for obtaining outer estimations for the so-called united solution\nset of this interval system. First, we propose a modified variant of the\nKrawczyk operator which causes reducing computational complexity to cubic,\ncompared to Kronecker product form. We then propose an iterative technique for\nenclosing the solution set. These approaches are based on spectral\ndecompositions of the midpoints of ${\\bf{A}}$, ${\\bf{B}}$, ${\\bf{C}}$ and\n${\\bf{D}}$ and in both of them we suppose that the midpoints of ${\\bf{A}}$ and\n${\\bf{C}}$ are simultaneously diagonalizable as well as for the midpoints of\nthe matrices ${\\bf{B}}$ and ${\\bf{D}}$. Some numerical experiments are given to\nillustrate the performance of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 11:59:50 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Dehghani-Madiseh", "Marzieh", ""], ["Hlad\u00edk", "Milan", ""]]}, {"id": "1510.04895", "submitter": "Andreas Alvermann", "authors": "Andreas Pieper, Moritz Kreutzer, Andreas Alvermann, Martin Galgon,\n  Holger Fehske, Georg Hager, Bruno Lang, Gerhard Wellein", "title": "High-performance implementation of Chebyshev filter diagonalization for\n  interior eigenvalue computations", "comments": "31 pages, 14 figures, 4 tables. Extended version", "journal-ref": "J. Comput. Phys. 325, 226 (2016)", "doi": "10.1016/j.jcp.2016.08.027", "report-no": null, "categories": "math.NA cond-mat.mes-hall cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Chebyshev filter diagonalization as a tool for the computation of\nmany interior eigenvalues of very large sparse symmetric matrices. In this\ntechnique the subspace projection onto the target space of wanted eigenvectors\nis approximated with filter polynomials obtained from Chebyshev expansions of\nwindow functions. After the discussion of the conceptual foundations of\nChebyshev filter diagonalization we analyze the impact of the choice of the\ndamping kernel, search space size, and filter polynomial degree on the\ncomputational accuracy and effort, before we describe the necessary steps\ntowards a parallel high-performance implementation. Because Chebyshev filter\ndiagonalization avoids the need for matrix inversion it can deal with matrices\nand problem sizes that are presently not accessible with rational function\nmethods based on direct or iterative linear solvers. To demonstrate the\npotential of Chebyshev filter diagonalization for large-scale problems of this\nkind we include as an example the computation of the $10^2$ innermost\neigenpairs of a topological insulator matrix with dimension $10^9$ derived from\nquantum physics applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 14:25:46 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 11:19:20 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Pieper", "Andreas", ""], ["Kreutzer", "Moritz", ""], ["Alvermann", "Andreas", ""], ["Galgon", "Martin", ""], ["Fehske", "Holger", ""], ["Hager", "Georg", ""], ["Lang", "Bruno", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1510.05231", "submitter": "Tarek Lahlou", "authors": "Tarek A. Lahlou and Thomas A. Baran", "title": "Signal Processing Structures for Solving Conservative Constraint\n  Satisfaction Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This primary purpose of this paper is to succinctly state a number of\nverifiable and tractable sufficient conditions under which a particular class\nof conservative signal processing structures may be readily used to solve a\ncompanion class of constraint satisfaction problems using both synchronous and\nasynchronous implementation protocols. In particular, the mentioned class of\nstructures is shown to have desirable convergence and robustness properties\nwith respect to various uncertainties involving communication and processing\ndelays. Essential ingredients to the arguments herein involve blending together\nfunctional composition methods, conservation principles, asynchronous signal\nprocessing implementation protocols, and methods of homotopy. Numerical\nexperiments complement the theoretical presentation and connections to\noptimization theory are made.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 12:21:48 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Lahlou", "Tarek A.", ""], ["Baran", "Thomas A.", ""]]}, {"id": "1510.05237", "submitter": "Vijay Gadepally", "authors": "Brendan Gavin and Vijay Gadepally and Jeremy Kepner", "title": "Large Enforced Sparse Non-Negative Matrix Factorization", "comments": "9 pages", "journal-ref": null, "doi": "10.1109/IPDPSW.2016.58", "report-no": null, "categories": "cs.LG cs.NA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is a common method for generating\ntopic models from text data. NMF is widely accepted for producing good results\ndespite its relative simplicity of implementation and ease of computation. One\nchallenge with applying NMF to large datasets is that intermediate matrix\nproducts often become dense, stressing the memory and compute elements of a\nsystem. In this article, we investigate a simple but powerful modification of a\ncommon NMF algorithm that enforces the generation of sparse intermediate and\noutput matrices. This method enables the application of NMF to large datasets\nthrough improved memory and compute performance. Further, we demonstrate\nempirically that this method of enforcing sparsity in the NMF either preserves\nor improves both the accuracy of the resulting topic model and the convergence\nrate of the underlying algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 12:53:38 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Gavin", "Brendan", ""], ["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1510.06689", "submitter": "Grey Ballard", "authors": "Woody Austin and Grey Ballard and Tamara G. Kolda", "title": "Parallel Tensor Compression for Large-Scale Scientific Data", "comments": null, "journal-ref": "IPDPS'16: Proceedings of the 30th IEEE International Parallel and\n  Distributed Processing Symposium, pp. 912-922, May 2016", "doi": "10.1109/IPDPS.2016.67", "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As parallel computing trends towards the exascale, scientific data produced\nby high-fidelity simulations are growing increasingly massive. For instance, a\nsimulation on a three-dimensional spatial grid with 512 points per dimension\nthat tracks 64 variables per grid point for 128 time steps yields 8~TB of data,\nassuming double precision. By viewing the data as a dense five-way tensor, we\ncan compute a Tucker decomposition to find inherent low-dimensional multilinear\nstructure, achieving compression ratios of up to 5000 on real-world data sets\nwith negligible loss in accuracy. So that we can operate on such massive data,\nwe present the first-ever distributed-memory parallel implementation for the\nTucker decomposition, whose key computations correspond to parallel linear\nalgebra operations, albeit with nonstandard data layouts. Our approach\nspecifies a data distribution for tensors that avoids any tensor data\nredistribution, either locally or in parallel. We provide accompanying analysis\nof the computation and communication costs of the algorithms. To demonstrate\nthe compression and accuracy of the method, we apply our approach to real-world\ndata sets from combustion science simulations. We also provide detailed\nperformance results, including parallel performance in both weak and strong\nscaling experiments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 17:06:12 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 20:55:59 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Austin", "Woody", ""], ["Ballard", "Grey", ""], ["Kolda", "Tamara G.", ""]]}, {"id": "1510.06895", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin", "title": "Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted\n  Nuclear Norm", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2511584", "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nuclear norm is widely used as a convex surrogate of the rank function in\ncompressive sensing for low rank matrix recovery with its applications in image\nrecovery and signal processing. However, solving the nuclear norm based relaxed\nconvex problem usually leads to a suboptimal solution of the original rank\nminimization problem. In this paper, we propose to perform a family of\nnonconvex surrogates of $L_0$-norm on the singular values of a matrix to\napproximate the rank function. This leads to a nonconvex nonsmooth minimization\nproblem. Then we propose to solve the problem by Iteratively Reweighted Nuclear\nNorm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value\nThresholding (WSVT) problem, which has a closed form solution due to the\nspecial properties of the nonconvex surrogate functions. We also extend IRNN to\nsolve the nonconvex problem with two or more blocks of variables. In theory, we\nprove that IRNN decreases the objective function value monotonically, and any\nlimit point is a stationary point. Extensive experiments on both synthesized\ndata and real images demonstrate that IRNN enhances the low-rank matrix\nrecovery compared with state-of-the-art convex algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 11:28:06 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Lu", "Canyi", ""], ["Tang", "Jinhui", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1510.07363", "submitter": "Hadi Pouransari", "authors": "Hadi Pouransari, Pieter Coulier, Eric Darve", "title": "Fast hierarchical solvers for sparse matrices using extended\n  sparsification and low-rank approximation", "comments": null, "journal-ref": null, "doi": "10.1137/15M1046939", "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inversion of sparse matrices with standard direct solve schemes is robust,\nbut computationally expensive. Iterative solvers, on the other hand,\ndemonstrate better scalability; but, need to be used with an appropriate\npreconditioner (e.g., ILU, AMG, Gauss-Seidel, etc.) for proper convergence. The\nchoice of an effective preconditioner is highly problem dependent. We propose a\nnovel fully algebraic sparse matrix solve algorithm, which has linear\ncomplexity with the problem size. Our scheme is based on the Gauss elimination.\nFor a given matrix, we approximate the LU factorization with a tunable accuracy\ndetermined a priori. This method can be used as a stand-alone direct solver\nwith linear complexity and tunable accuracy, or it can be used as a black-box\npreconditioner in conjunction with iterative methods such as GMRES. The\nproposed solver is based on the low-rank approximation of fill-ins generated\nduring the elimination. Similar to H-matrices, fill-ins corresponding to blocks\nthat are well-separated in the adjacency graph are represented via a\nhierarchical structure. The linear complexity of the algorithm is guaranteed if\nthe blocks corresponding to well-separated clusters of variables are\nnumerically low-rank.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 04:39:37 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 19:44:50 GMT"}, {"version": "v3", "created": "Thu, 15 Dec 2016 02:24:36 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Pouransari", "Hadi", ""], ["Coulier", "Pieter", ""], ["Darve", "Eric", ""]]}, {"id": "1510.08297", "submitter": "Petr Vabishchevich N.", "authors": "Petr N. Vabishchevich", "title": "Numerical solving unsteady space-fractional problems with the square\n  root of an elliptic operator", "comments": "21 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1412.5706", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsteady problem is considered for a space-fractional equation in a\nbounded domain. A first-order evolutionary equation involves the square root of\nan elliptic operator of second order. Finite element approximation in space is\nemployed. To construct approximation in time, regularized two-level schemes are\nused. The numerical implementation is based on solving the equation with the\nsquare root of the elliptic operator using an auxiliary Cauchy problem for a\npseudo-parabolic equation. The scheme of the second-order accuracy in time is\nbased on a regularization of the three-level explicit Adams scheme. More\ngeneral problems for the equation with convective terms are considered, too.\nThe results of numerical experiments are presented for a model two-dimensional\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 13:17:01 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Vabishchevich", "Petr N.", ""]]}, {"id": "1510.08345", "submitter": "Michael B Hynes", "authors": "Michael B Hynes and Hans De Sterck", "title": "A polynomial expansion line search for large-scale unconstrained\n  minimization of smooth L2-regularized loss functions, with implementation in\n  Apache Spark", "comments": "9 pages, 8 figures, 2 tables. Preprint appearing in SIAM Conf on Data\n  Mining, Miami, FL, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale unconstrained optimization algorithms such as limited memory\nBFGS (LBFGS), a common subproblem is a line search minimizing the loss function\nalong a descent direction. Commonly used line searches iteratively find an\napproximate solution for which the Wolfe conditions are satisfied, typically\nrequiring multiple function and gradient evaluations per line search, which is\nexpensive in parallel due to communication requirements. In this paper we\npropose a new line search approach for cases where the loss function is\nanalytic, as in least squares regression, logistic regression, or low rank\nmatrix factorization. We approximate the loss function by a truncated Taylor\npolynomial, whose coefficients may be computed efficiently in parallel with\nless communication than evaluating the gradient, after which this polynomial\nmay be minimized with high accuracy in a neighbourhood of the expansion point.\nOur Polynomial Expansion Line Search (PELS) was implemented in the Apache Spark\nframework and used to accelerate the training of a logistic regression model on\nbinary classification datasets from the LIBSVM repository with LBFGS and the\nNonlinear Conjugate Gradient (NCG) method. In large-scale numerical experiments\nin parallel on a 16-node cluster with 256 cores using the URL, KDDA, and KDDB\ndatasets, the PELS approach produced significant convergence improvements\ncompared to the use of classical Wolfe line searches. For example, to reach the\nfinal training label prediction accuracies, LBFGS using PELS had speedup\nfactors of 1.8--2 over LBFGS using a Wolfe line search, measured by both the\nnumber of iterations and the time required, due to the better accuracy of step\nsizes computed in the line search. PELS has the potential to significantly\naccelerate large-scale regression and factorization computations, and is\napplicable to continuous optimization problems with smooth loss functions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 15:27:26 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 07:01:03 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Hynes", "Michael B", ""], ["De Sterck", "Hans", ""]]}, {"id": "1510.08642", "submitter": "Tomonori Kouya", "authors": "Tomonori Kouya", "title": "Performance evaluation of multiple precision matrix multiplications\n  using parallelized Strassen and Winograd algorithms", "comments": null, "journal-ref": "JSIAM Letters Vol. 8 (2016) p. 21-24", "doi": "10.14495/jsiaml.8.21", "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Strassen and Winograd algorithms can reduce the\ncomputational costs associated with dense matrix multiplication. We have\nalready shown that they are also very effective for software-based multiple\nprecision floating-point arithmetic environments such as the MPFR/GMP library.\nIn this paper, we show that we can obtain the same effectiveness for\ndouble-double (DD) and quadruple-double (QD) environments supported by the QD\nlibrary, and that parallelization can increase the speed of these multiple\nprecision matrix multiplications. Finally, we demonstrate that our implemented\nparallelized Strassen and Winograd algorithms can increase the speed of\nparallelized LU decomposition.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 10:58:55 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Kouya", "Tomonori", ""]]}]