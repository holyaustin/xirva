[{"id": "1506.00059", "submitter": "Martin Arjovsky", "authors": "Martin Arjovsky", "title": "Saddle-free Hessian-free Optimization", "comments": "NIPS 2016 Workshop on Nonconvex Optimization for Machine Learning:\n  Theory and Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonconvex optimization problems such as the ones in training deep neural\nnetworks suffer from a phenomenon called saddle point proliferation. This means\nthat there are a vast number of high error saddle points present in the loss\nfunction. Second order methods have been tremendously successful and widely\nadopted in the convex optimization community, while their usefulness in deep\nlearning remains limited. This is due to two problems: computational complexity\nand the methods being driven towards the high error saddle points. We introduce\na novel algorithm specially designed to solve these two issues, providing a\ncrucial first step to take the widely known advantages of Newton's method to\nthe nonconvex optimization community, especially in high dimensional settings.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 02:42:21 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 18:59:41 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 22:37:12 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Arjovsky", "Martin", ""]]}, {"id": "1506.00289", "submitter": "Paulo Laerte Natti", "authors": "Cibele A. Ladeia, Neyva M.L. Romeiro, Paulo L. Natti and Eliandro R.\n  Cirilo", "title": "Semi-Discrete Formulations for 1D Burgers Equation", "comments": "14 pages, 2 figures, in Portuguese", "journal-ref": "TEMA - Tend\\^encias em Matem\\'atica Aplicada e Computacional 14,\n  N.3, 319-331, 2013", "doi": "10.5540/tema.2013.014.03.0319", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we compare semi-discrete formulations to obtain numerical\nsolutions for the 1D Burgers equation. The formulations consist in the\ndiscretization of the time-domain via multi-stage methods of second and fourth\norder: R_{11} and R_{22} Pad\\'e approximants, and of the spatial-domain via\nfinite element methods: least-squares (MEFMQ), Galerkin (MEFG) and\nStreamline-Upwind Petrov-Galerkin (SUPG). Knowing the analytical solutions of\nthe 1D Burgues equation, for different initial and boundary conditions,\nanalyzes were performed for numerical errors from L_{2} and L_{\\infinity} norm.\nWe found that the R_{22} Pad\\'e approximants, added to the MEFMQ, MEFG, and\nSUPG formulations, increased the region of convergence of the numerical\nsolutions, and showed greater accuracy when compared to the solutions obtained\nby the R_{11} Pad\\'e approximants. We note that the R_{22} Pad\\'e approximants\nsoftened the oscillations of the numerical solutions associated to the MEFG and\nSUPG formulations.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 20:53:57 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Ladeia", "Cibele A.", ""], ["Romeiro", "Neyva M. L.", ""], ["Natti", "Paulo L.", ""], ["Cirilo", "Eliandro R.", ""]]}, {"id": "1506.01460", "submitter": "Kui Ren", "authors": "Kui Ren and Rongting Zhang and Yimin Zhong", "title": "Inverse transport problems in quantitative PAT for molecular imaging", "comments": null, "journal-ref": null, "doi": "10.1088/0266-5611/31/12/125012", "report-no": null, "categories": "math.AP cs.NA math.NA math.OC physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence photoacoustic tomography (fPAT) is a molecular imaging modality\nthat combines photoacoustic tomography (PAT) with fluorescence imaging to\nobtain high-resolution imaging of fluorescence distributions inside\nheterogeneous media. The objective of this work is to study inverse problems in\nthe quantitative step of fPAT where we intend to reconstruct physical\ncoefficients in a coupled system of radiative transport equations using\ninternal data recovered from ultrasound measurements. We derive uniqueness and\nstability results on the inverse problems and develop some efficient algorithms\nfor image reconstructions. Numerical simulations based on synthetic data are\npresented to validate the theoretical analysis. The results we present here\ncomplement these in [Ren-Zhao, SIAM J. Imag. Sci., 2013] on the same problem\nbut in the diffusive regime.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 04:00:39 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 00:58:49 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 09:05:11 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Ren", "Kui", ""], ["Zhang", "Rongting", ""], ["Zhong", "Yimin", ""]]}, {"id": "1506.01701", "submitter": "Yuliya Boyarinova", "authors": "Yakiv O. Kalinovsky, Yuliya E. Boyarinova, Iana V. Khitsko", "title": "Reversible Digital Filters Total Parametric Sensitivity Optimization\n  using Non-canonical Hypercomplex Number Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital filter construction method, which is optimal by parametric\nsensitivity, based on using of non-canonical hypercomplex number systems is\nproposed and investigated. It is shown that the use of non-canonical\nhypercomplex number system with greater number of non-zero structure constants\nin multiplication table can significantly improve the sensitivity of the\ndigital filter.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 18:38:09 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Kalinovsky", "Yakiv O.", ""], ["Boyarinova", "Yuliya E.", ""], ["Khitsko", "Iana V.", ""]]}, {"id": "1506.01762", "submitter": "Daisuke Ishii", "authors": "Daisuke Ishii, Naoki Yonezaki, Alexandre Goldsztejn", "title": "Monitoring Bounded LTL Properties Using Interval Analysis", "comments": "Appeared in NSV'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of temporal logic properties plays a crucial role in proving the\ndesired behaviors of hybrid systems. In this paper, we propose an interval\nmethod for verifying the properties described by a bounded linear temporal\nlogic. We relax the problem to allow outputting an inconclusive result when\nverification process cannot succeed with a prescribed precision, and present an\nefficient and rigorous monitoring algorithm that demonstrates that the problem\nis decidable. This algorithm performs a forward simulation of a hybrid\nautomaton, detects a set of time intervals in which the atomic propositions\nhold, and validates the property by propagating the time intervals. A\ncontinuous state at a certain time computed in each step is enclosed by an\ninterval vector that is proven to contain a unique solution. In the\nexperiments, we show that the proposed method provides a useful tool for formal\nanalysis of nonlinear and complex hybrid systems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 01:19:18 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 14:38:49 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Ishii", "Daisuke", ""], ["Yonezaki", "Naoki", ""], ["Goldsztejn", "Alexandre", ""]]}, {"id": "1506.01959", "submitter": "Namgil Lee", "authors": "Namgil Lee, Andrzej Cichocki", "title": "Regularized Computation of Approximate Pseudoinverse of Large Matrices\n  Using Low-Rank Tensor Train Decompositions", "comments": "28 pages", "journal-ref": "SIAM. J. Matrix Anal. Appl. 37(2):598-623, 2016", "doi": "10.1137/15M1028479", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for low-rank approximation of Moore-Penrose\npseudoinverses (MPPs) of large-scale matrices using tensor networks. The\ncomputed pseudoinverses can be useful for solving or preconditioning of\nlarge-scale overdetermined or underdetermined systems of linear equations. The\ncomputation is performed efficiently and stably based on the modified\nalternating least squares (MALS) scheme using low-rank tensor train (TT)\ndecompositions and tensor network contractions. The formulated large-scale\noptimization problem is reduced to sequential smaller-scale problems for which\nany standard and stable algorithms can be applied. Regularization technique is\nincorporated in order to alleviate ill-posedness and obtain robust low-rank\napproximations. Numerical simulation results illustrate that the regularized\npseudoinverses of a wide class of non-square or nonsymmetric matrices admit\ngood approximate low-rank TT representations. Moreover, we demonstrated that\nthe computational cost of the proposed method is only logarithmic in the matrix\nsize given that the TT-ranks of a data matrix and its approximate pseudoinverse\nare bounded. It is illustrated that a strongly nonsymmetric\nconvection-diffusion problem can be efficiently solved by using the\npreconditioners computed by the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 16:23:37 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 07:40:16 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2015 03:51:32 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2015 13:11:59 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Lee", "Namgil", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1506.02159", "submitter": "Bamdev Mishra", "authors": "Hiroyuki Kasai and Bamdev Mishra", "title": "Riemannian preconditioning for tensor completion", "comments": "Supplementary material included in the paper. An extension of the\n  paper is in arXiv:1605.08257", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Riemannian preconditioning approach for the tensor\ncompletion problem with rank constraint. A Riemannian metric or inner product\nis proposed that exploits the least-squares structure of the cost function and\ntakes into account the structured symmetry in Tucker decomposition. The\nspecific metric allows to use the versatile framework of Riemannian\noptimization on quotient manifolds to develop a preconditioned nonlinear\nconjugate gradient algorithm for the problem. To this end, concrete matrix\nrepresentations of various optimization-related ingredients are listed.\nNumerical comparisons suggest that our proposed algorithm robustly outperforms\nstate-of-the-art algorithms across different problem instances encompassing\nvarious synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 14:52:13 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 17:28:32 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Kasai", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1506.02618", "submitter": "Jan Verschelde", "authors": "Nathan Bliss and Jeff Sommars and Jan Verschelde and Xiangcheng Yu", "title": "Solving Polynomial Systems in the Cloud with Polynomial Homotopy\n  Continuation", "comments": "Accepted for publication in the Proceedings of CASC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA cs.SC math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial systems occur in many fields of science and engineering.\nPolynomial homotopy continuation methods apply symbolic-numeric algorithms to\nsolve polynomial systems. We describe the design and implementation of our web\ninterface and reflect on the application of polynomial homotopy continuation\nmethods to solve polynomial systems in the cloud. Via the graph isomorphism\nproblem we organize and classify the polynomial systems we solved. The\nclassification with the canonical form of a graph identifies newly submitted\nsystems with systems that have already been solved.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:05:49 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Bliss", "Nathan", ""], ["Sommars", "Jeff", ""], ["Verschelde", "Jan", ""], ["Yu", "Xiangcheng", ""]]}, {"id": "1506.02649", "submitter": "Alon Gonen", "authors": "Alon Gonen, Shai Shalev-Shwartz", "title": "Faster SGD Using Sketched Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for speeding up stochastic optimization algorithms\nvia sketching methods, which recently became a powerful tool for accelerating\nalgorithms for numerical linear algebra. We revisit the method of conditioning\nfor accelerating first-order methods and suggest the use of sketching methods\nfor constructing a cheap conditioner that attains a significant speedup with\nrespect to the Stochastic Gradient Descent (SGD) algorithm. While our\ntheoretical guarantees assume convexity, we discuss the applicability of our\nmethod to deep neural networks, and experimentally demonstrate its merits.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:08:37 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Gonen", "Alon", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1506.02857", "submitter": "Assal\\'e Adj\\'e", "authors": "Assal\\'e Adj\\'e", "title": "Overapproximating the Reachable Values Set of Piecewise Affine Systems\n  Coupling Policy Iterations with Piecewise Quadratic Lyapunov Functions", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently constructed a piecewise quadratic Lyapunov function to prove\nthe boundedness of the reachable values set of piecewise affine discrete-time\nsystems. The method developed also provided an overapproximation of the\nreachable values set. In this paper, we refine the latter overapproximation\nextending previous works combining policy iterations with quadratic Lyapunov\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 10:47:54 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 12:56:55 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Adj\u00e9", "Assal\u00e9", ""]]}, {"id": "1506.03771", "submitter": "Javier V. G\\'omez", "authors": "Javier V. Gomez, David Alvarez, Santiago Garrido, Luis Moreno", "title": "Fast Methods for Eikonal Equations: an Experimental Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.RO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Fast Marching Method is a very popular algorithm to compute\ntimes-of-arrival maps (distances map measured in time units). Since their\nproposal in 1995, it has been applied to many different applications such as\nrobotics, medical computer vision, fluid simulation, etc. Many alternatives\nhave been proposed with two main objectives: to reduce its computational time\nand to improve its accuracy. In this paper, we collect the main approaches\nwhich improve the computational time of the standard Fast Marching Method,\nfocusing on single-threaded methods and isotropic environments. 9 different\nmethods are studied under a common mathematical framework and experimentally in\nrepresentative environments: Fast Marching Method with binary heap, Fast\nMarching Method with Fibonacci Heap, Simplified Fast Marching Method, Untidy\nFast Marching Method, Fast Iterative Method, Group Marching Method, Fast\nSweeping Method, Lock Sweeping Method and Double Dynamic Queue Method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 18:47:11 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Gomez", "Javier V.", ""], ["Alvarez", "David", ""], ["Garrido", "Santiago", ""], ["Moreno", "Luis", ""]]}, {"id": "1506.03914", "submitter": "Juergen Zechner", "authors": "J\\\"urgen Zechner and Benjamin Marussig and Gernot Beer and\n  Thomas-Peter Fries", "title": "The Isogeometric Nystr\\\"om Method", "comments": "21 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the isogeometric Nystr\\\"om method is presented. It's\noutstanding features are: it allows the analysis of domains described by many\ndifferent geometrical mapping methods in computer aided geometric design and it\nrequires only pointwise function evaluations just like isogeometric collocation\nmethods. The analysis of the computational domain is carried out by means of\nboundary integral equations, therefor only the boundary representation is\nrequired. The method is thoroughly integrated into the isogeometric framework.\nFor example, the regularization of the arising singular integrals performed\nwith local correction as well as the interpolation of the pointwise existing\nresults are carried out by means of Bezier elements.\n  The presented isogeometric Nystr\\\"om method is applied to practical problems\nsolved by the Laplace and the Lame-Navier equation. Numerical tests show higher\norder convergence in two and three dimensions. It is concluded that the\npresented approach provides a simple and flexible alternative to currently used\nmethods for solving boundary integral equations, but has some limitations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 06:52:00 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Zechner", "J\u00fcrgen", ""], ["Marussig", "Benjamin", ""], ["Beer", "Gernot", ""], ["Fries", "Thomas-Peter", ""]]}, {"id": "1506.03963", "submitter": "Xiaolin Qin", "authors": "Xiaolin Qin, Juan Tang, Yong Feng, Bernhard Bachmann, Peter Fritzson", "title": "Efficient algorithm for computing large scale systems of differential\n  algebraic equations", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In many mathematical models of physical phenomenons and engineering fields,\nsuch as electrical circuits or mechanical multibody systems, which generate the\ndifferential algebraic equations (DAEs) systems naturally. In general, the\nfeature of DAEs is a sparse large scale system of fully nonlinear and high\nindex. To make use of its sparsity, this paper provides a simple and efficient\nalgorithm for computing the large scale DAEs system. We exploit the shortest\naugmenting path algorithm for finding maximum value transversal (MVT) as well\nas block triangular forms (BTF). We also present the extended signature matrix\nmethod with the block fixed point iteration and its complexity results.\nFurthermore, a range of nontrivial problems are demonstrated by our algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 09:59:39 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Qin", "Xiaolin", ""], ["Tang", "Juan", ""], ["Feng", "Yong", ""], ["Bachmann", "Bernhard", ""], ["Fritzson", "Peter", ""]]}, {"id": "1506.04268", "submitter": "Tomasz Waclawczyk PhD", "authors": "Tomasz Waclawczyk", "title": "A consistent solution of the reinitialization equation in the\n  conservative level-set method", "comments": "the paper is currently under review after the first revision in the\n  Journal of Computational Physics (72 pages)", "journal-ref": "J. Comput. Phys. 299 (2015) 487-525", "doi": "10.1016/j.jcp.2015.06.029", "report-no": null, "categories": "cs.NA physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new re-initialization method for the conservative level-set\nfunction is put forward. First, it has been shown that the re-initialization\nand advection equations of the conservative level-set function are\nmathematically equivalent to the re-initialization and advection equations of\nthe localized signed distance function. Next, a new discretization for the\nspatial derivatives of the conservative level-set function has been proposed.\nThis new discretization is consistent with the re-initialization procedure and\nit guarantees a second-order convergence rate of the interface curvature on\ngradually refined grids. The new re-initialization method does not introduce\nartificial deformations to stationary and non-stationary interfaces, even when\nthe number of re-initialization steps is large.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 13:26:55 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Waclawczyk", "Tomasz", ""]]}, {"id": "1506.04462", "submitter": "Ross Adelman", "authors": "Ross Adelman and Nail A. Gumerov and Ramani Duraiswami", "title": "Accurate computation of Galerkin double surface integrals in the 3-D\n  boundary element method", "comments": null, "journal-ref": null, "doi": "10.1109/TAP.2016.2546951", "report-no": null, "categories": "physics.comp-ph cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many boundary element integral equation kernels are based on the Green's\nfunctions of the Laplace and Helmholtz equations in three dimensions. These\ninclude, for example, the Laplace, Helmholtz, elasticity, Stokes, and Maxwell's\nequations. Integral equation formulations lead to more compact, but dense\nlinear systems. These dense systems are often solved iteratively via Krylov\nsubspace methods, which may be accelerated via the fast multipole method. There\nare advantages to Galerkin formulations for such integral equations, as they\ntreat problems associated with kernel singularity, and lead to symmetric and\nbetter conditioned matrices. However, the Galerkin method requires each entry\nin the system matrix to be created via the computation of a double surface\nintegral over one or more pairs of triangles. There are a number of\nsemi-analytical methods to treat these integrals, which all have some issues,\nand are discussed in this paper. We present novel methods to compute all the\nintegrals that arise in Galerkin formulations involving kernels based on the\nLaplace and Helmholtz Green's functions to any specified accuracy. Integrals\ninvolving completely geometrically separated triangles are non-singular and are\ncomputed using a technique based on spherical harmonics and multipole\nexpansions and translations, which results in the integration of polynomial\nfunctions over the triangles. Integrals involving cases where the triangles\nhave common vertices, edges, or are coincident are treated via scaling and\nsymmetry arguments, combined with automatic recursive geometric decomposition\nof the integrals. Example results are presented, and the developed software is\navailable as open source.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 02:49:10 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Adelman", "Ross", ""], ["Gumerov", "Nail A.", ""], ["Duraiswami", "Ramani", ""]]}, {"id": "1506.04463", "submitter": "Eric Polizzi", "authors": "James Kestyn, Eric Polizzi, Ping Tak Peter Tang", "title": "FEAST Eigensolver for non-Hermitian Problems", "comments": "22 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A detailed new upgrade of the FEAST eigensolver targeting non-Hermitian\neigenvalue problems is presented and thoroughly discussed. It aims at\nbroadening the class of eigenproblems that can be addressed within the\nframework of the FEAST algorithm. The algorithm is ideally suited for computing\nselected interior eigenvalues and their associated right/left bi-orthogonal\neigenvectors,located within a subset of the complex plane. It combines subspace\niteration with efficient contour integration techniques that approximate the\nleft and right spectral projectors. We discuss the various algorithmic choices\nthat have been made to improve the stability and usability of the new\nnon-Hermitian eigensolver. The latter retains the convergence property and\nmulti-level parallelism of Hermitian FEAST, making it a valuable new software\ntool for the scientific community.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 02:55:29 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Kestyn", "James", ""], ["Polizzi", "Eric", ""], ["Tang", "Ping Tak Peter", ""]]}, {"id": "1506.04631", "submitter": "Ivan Yu. Tyukin", "authors": "Alexander N. Gorban, Ivan Yu. Tyukin, Danil V. Prokhorov, Konstantin\n  I. Sofeikov", "title": "Approximation with Random Bases: Pro et Contra", "comments": "arXiv admin note: text overlap with arXiv:0905.0677", "journal-ref": "Information Sciences 364-365, 10 October 2016, Pages 129-145", "doi": "10.1016/j.ins.2015.09.021", "report-no": null, "categories": "cs.NA cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we discuss the problem of selecting suitable approximators from\nfamilies of parameterized elementary functions that are known to be dense in a\nHilbert space of functions. We consider and analyze published procedures, both\nrandomized and deterministic, for selecting elements from these families that\nhave been shown to ensure the rate of convergence in $L_2$ norm of order\n$O(1/N)$, where $N$ is the number of elements. We show that both randomized and\ndeterministic procedures are successful if additional information about the\nfamilies of functions to be approximated is provided. In the absence of such\nadditional information one may observe exponential growth of the number of\nterms needed to approximate the function and/or extreme sensitivity of the\noutcome of the approximation to parameters. Implications of our analysis for\napplications of neural networks in modeling and control are illustrated with\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 15:22:06 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 07:59:25 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 11:57:54 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2015 15:18:28 GMT"}, {"version": "v5", "created": "Sat, 24 Oct 2015 16:07:26 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Gorban", "Alexander N.", ""], ["Tyukin", "Ivan Yu.", ""], ["Prokhorov", "Danil V.", ""], ["Sofeikov", "Konstantin I.", ""]]}, {"id": "1506.04651", "submitter": "Max Duarte", "authors": "St\\'ephane Descombes, Max Duarte (LBNL), Thierry Dumont (ICJ), Thomas\n  Guillet, Violaine Louvet (ICJ), Marc Massot (EM2C)", "title": "Task-based adaptive multiresolution for time-space multi-scale\n  reaction-diffusion systems on multi-core architectures", "comments": null, "journal-ref": "SMAI Journal of Computational Mathematics, Vol. 3 (2017) 29-51", "doi": "10.5802/smai-jcm.19", "report-no": null, "categories": "cs.NA cs.DC math.AP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new solver featuring time-space adaptation and error control has been\nrecently introduced to tackle the numerical solution of stiff\nreaction-diffusion systems. Based on operator splitting, finite volume adaptive\nmultiresolution and high order time integrators with specific stability\nproperties for each operator, this strategy yields high computational\nefficiency for large multidimensional computations on standard architectures\nsuch as powerful workstations. However, the data structure of the original\nimplementation, based on trees of pointers, provides limited opportunities for\nefficiency enhancements, while posing serious challenges in terms of parallel\nprogramming and load balancing. The present contribution proposes a new\nimplementation of the whole set of numerical methods including Radau5 and\nROCK4, relying on a fully different data structure together with the use of a\nspecific library, TBB, for shared-memory, task-based parallelism with\nwork-stealing. The performance of our implementation is assessed in a series of\ntest-cases of increasing difficulty in two and three dimensions on multi-core\nand many-core architectures, demonstrating high scalability.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 12:53:48 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 07:58:57 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 12:28:12 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Descombes", "St\u00e9phane", "", "LBNL"], ["Duarte", "Max", "", "LBNL"], ["Dumont", "Thierry", "", "ICJ"], ["Guillet", "Thomas", "", "ICJ"], ["Louvet", "Violaine", "", "ICJ"], ["Massot", "Marc", "", "EM2C"]]}, {"id": "1506.04954", "submitter": "Sara Soltani", "authors": "Sara Soltani, Misha E. Kilmer, and Per Christian Hansen", "title": "A Tensor-Based Dictionary Learning Approach to Tomographic Image\n  Reconstruction", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider tomographic reconstruction using priors in the form of a\ndictionary learned from training images. The reconstruction has two stages:\nfirst we construct a tensor dictionary prior from our training data, and then\nwe pose the reconstruction problem in terms of recovering the expansion\ncoefficients in that dictionary. Our approach differs from past approaches in\nthat a) we use a third-order tensor representation for our images and b) we\nrecast the reconstruction problem using the tensor formulation. The dictionary\nlearning problem is presented as a non-negative tensor factorization problem\nwith sparsity constraints. The reconstruction problem is formulated in a convex\noptimization framework by looking for a solution with a sparse representation\nin the tensor dictionary. Numerical results show that our tensor formulation\nleads to very sparse representations of both the training images and the\nreconstructions due to the ability of representing repeated features compactly\nin the dictionary.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 08:55:28 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Soltani", "Sara", ""], ["Kilmer", "Misha E.", ""], ["Hansen", "Per Christian", ""]]}, {"id": "1506.04971", "submitter": "Anh Huy Phan", "authors": "Anh-Huy Phan, Petr Tichavsky and Andrzej Cichocki", "title": "Tensor Deflation for CANDECOMP/PARAFAC. Part 3: Rank Splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  CANDECOMP/PARAFAC (CPD) approximates multiway data by sum of rank-1 tensors.\nOur recent study has presented a method to rank-1 tensor deflation, i.e.\nsequential extraction of the rank-1 components. In this paper, we extend the\nmethod to block deflation problem. When at least two factor matrices have full\ncolumn rank, one can extract two rank-1 tensors simultaneously, and rank of the\ndata tensor is reduced by 2. For decomposition of order-3 tensors of size R x R\nx R and rank-R, the block deflation has a complexity of O(R^3) per iteration\nwhich is lower than the cost O(R^4) of the ALS algorithm for the overall CPD.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 13:51:38 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Phan", "Anh-Huy", ""], ["Tichavsky", "Petr", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1506.04972", "submitter": "Yang Yang", "authors": "Yang Yang, Marius Pesavento", "title": "A Unified Successive Pseudo-Convex Approximation Framework", "comments": "submitted to IEEE Transactions on Signal Processing; original title:\n  A Novel Iterative Convex Approximation Method", "journal-ref": null, "doi": "10.1109/TSP.2017.2684748", "report-no": null, "categories": "math.OC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a successive pseudo-convex approximation algorithm\nto efficiently compute stationary points for a large class of possibly\nnonconvex optimization problems. The stationary points are obtained by solving\na sequence of successively refined approximate problems, each of which is much\neasier to solve than the original problem. To achieve convergence, the\napproximate problem only needs to exhibit a weak form of convexity, namely,\npseudo-convexity. We show that the proposed framework not only includes as\nspecial cases a number of existing methods, for example, the gradient method\nand the Jacobi algorithm, but also leads to new algorithms which enjoy easier\nimplementation and faster convergence speed. We also propose a novel line\nsearch method for nondifferentiable optimization problems, which is carried out\nover a properly constructed differentiable function with the benefit of a\nsimplified implementation as compared to state-of-the-art line search\ntechniques that directly operate on the original nondifferentiable objective\nfunction. The advantages of the proposed algorithm are shown, both\ntheoretically and numerically, by several example applications, namely, MIMO\nbroadcast channel capacity computation, energy efficiency maximization in\nmassive MIMO systems and LASSO in sparse signal recovery.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 13:54:00 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 20:45:53 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Yang", "Yang", ""], ["Pesavento", "Marius", ""]]}, {"id": "1506.05317", "submitter": "Murugesan Venkatapathi", "authors": "Hariprasad M. and Murugesan Venkatapathi", "title": "Semi-analytical solutions for eigenvalue problems of chains and periodic\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first show the existence and nature of convergence to a limiting set of\nroots for polynomials in a three-term recurrence of the form $p_{n+1}(z) =\nQ_k(z)p_{n}(z)+ \\gamma p_{n-1}(z)$ as $n$ $\\rightarrow$ $\\infty$, where the\ncoefficient $Q_k(z)$ is a $k^{th}$ degree polynomial, and $z,\\gamma \\in\n\\mathbb{C}$. We extend these results to relations for numerically approximating\nroots of such polynomials for any given $n$. General solutions for the\nevaluation are motivated by large computational efforts and errors in the\niterative numerical methods. Later, we apply this solution to the eigenvalue\nproblems represented by tridiagonal matrices with a periodicity $k$ in its\nentries, providing a more accurate numerical method for evaluation of spectra\nof chains and a reduction in computational effort from $\\mathcal{O}(n^2)$ to\n$\\mathcal{O}(n)$. We also show that these results along with the spectral rules\nof Kronecker products allow an efficient and accurate evaluation of spectra of\nmany spatial lattices and other periodic graphs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 13:14:43 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 07:15:27 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2015 13:50:36 GMT"}, {"version": "v4", "created": "Fri, 11 Aug 2017 13:37:54 GMT"}, {"version": "v5", "created": "Thu, 17 Dec 2020 10:56:56 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["M.", "Hariprasad", ""], ["Venkatapathi", "Murugesan", ""]]}, {"id": "1506.05996", "submitter": "Rajesh Gandham", "authors": "J.-F. Remacle, R. Gandham, T. Warburton", "title": "GPU accelerated spectral finite elements on all-hex meshes", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2016.08.005", "report-no": null, "categories": "cs.CE cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a spectral element finite element scheme that efficiently\nsolves elliptic problems on unstructured hexahedral meshes. The discrete\nequations are solved using a matrix-free preconditioned conjugate gradient\nalgorithm. An additive Schwartz two-scale preconditioner is employed that\nallows h-independence convergence. An extensible multi-threading programming\nAPI is used as a common kernel language that allows runtime selection of\ndifferent computing devices (GPU and CPU) and different threading interfaces\n(CUDA, OpenCL and OpenMP). Performance tests demonstrate that problems with\nover 50 million degrees of freedom can be solved in a few seconds on an\noff-the-shelf GPU.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 13:27:05 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Remacle", "J. -F.", ""], ["Gandham", "R.", ""], ["Warburton", "T.", ""]]}, {"id": "1506.06040", "submitter": "Esin Karahan", "authors": "Esin Karahan, Pedro A. Rojas-Lopez, Maria L. Bringas-Vega, Pedro A.\n  Valdes-Hernandez, Pedro A. Valdes-Sosa", "title": "Tensor Analysis and Fusion of Multimodal Brain Images", "comments": "23 pages, 15 figures, submitted to Proceedings of the IEEE", "journal-ref": null, "doi": "10.1109/JPROC.2015.2455028", "report-no": null, "categories": "stat.ME cs.NA stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current high-throughput data acquisition technologies probe dynamical systems\nwith different imaging modalities, generating massive data sets at different\nspatial and temporal resolutions posing challenging problems in multimodal data\nfusion. A case in point is the attempt to parse out the brain structures and\nnetworks that underpin human cognitive processes by analysis of different\nneuroimaging modalities (functional MRI, EEG, NIRS etc.). We emphasize that the\nmultimodal, multi-scale nature of neuroimaging data is well reflected by a\nmulti-way (tensor) structure where the underlying processes can be summarized\nby a relatively small number of components or \"atoms\". We introduce\nMarkov-Penrose diagrams - an integration of Bayesian DAG and tensor network\nnotation in order to analyze these models. These diagrams not only clarify\nmatrix and tensor EEG and fMRI time/frequency analysis and inverse problems,\nbut also help understand multimodal fusion via Multiway Partial Least Squares\nand Coupled Matrix-Tensor Factorization. We show here, for the first time, that\nGranger causal analysis of brain networks is a tensor regression problem, thus\nallowing the atomic decomposition of brain networks. Analysis of EEG and fMRI\nrecordings shows the potential of the methods and suggests their use in other\nscientific domains.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 15:03:22 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Karahan", "Esin", ""], ["Rojas-Lopez", "Pedro A.", ""], ["Bringas-Vega", "Maria L.", ""], ["Valdes-Hernandez", "Pedro A.", ""], ["Valdes-Sosa", "Pedro A.", ""]]}, {"id": "1506.06099", "submitter": "Kalyana Babu Nakshatrala", "authors": "M.K. Mudunuru and K. B. Nakshatrala", "title": "On enforcing maximum principles and achieving element-wise species\n  balance for advection-diffusion-reaction equations under the finite element\n  method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust computational framework for advective-diffusive-reactive\nsystems that satisfies maximum principles, the non-negative constraint, and\nelement-wise species balance property. The proposed methodology is valid on\ngeneral computational grids, can handle heterogeneous anisotropic media, and\nprovides accurate numerical solutions even for very high P\\'eclet numbers. The\nsignificant contribution of this paper is to incorporate advection (which makes\nthe spatial part of the differential operator non-self-adjoint) into the\nnon-negative computational framework, and overcome numerical challenges\nassociated with advection. We employ low-order mixed finite element\nformulations based on least-squares formalism, and enforce explicit constraints\non the discrete problem to meet the desired properties. The resulting\nconstrained discrete problem belongs to convex quadratic programming for which\na unique solution exists. Maximum principles and the non-negative constraint\ngive rise to bound constraints while element-wise species balance gives rise to\nequality constraints. The resulting convex quadratic programming problems are\nsolved using an interior-point algorithm. Several numerical results pertaining\nto advection-dominated problems are presented to illustrate the robustness,\nconvergence, and the overall performance of the proposed computational\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 17:59:16 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 19:21:50 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Mudunuru", "M. K.", ""], ["Nakshatrala", "K. B.", ""]]}, {"id": "1506.06185", "submitter": "Huber Markus", "authors": "Markus Huber, Bj\\\"orn Gmeiner, Ulrich R\\\"ude, Barbara Wohlmuth", "title": "Resilience for Multigrid Software at the Extreme Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault tolerant algorithms for the numerical approximation of elliptic partial\ndifferential equations on modern supercomputers play a more and more important\nrole in the future design of exa-scale enabled iterative solvers. Here, we\ncombine domain partitioning with highly scalable geometric multigrid schemes to\nobtain fast and fault-robust solvers in three dimensions. The recovery strategy\nis based on a hierarchical hybrid concept where the values on lower dimensional\nprimitives such as faces are stored redundantly and thus can be recovered\neasily in case of a failure. The lost volume unknowns in the faulty region are\nre-computed approximately with multigrid cycles by solving a local Dirichlet\nproblem on the faulty subdomain. Different strategies are compared and\nevaluated with respect to performance, computational cost, and speed up.\nEspecially effective are strategies in which the local recovery in the faulty\nregion is executed in parallel with global solves and when the local recovery\nis additionally accelerated. This results in an asynchronous multigrid\niteration that can fully compensate faults. Excellent parallel performance on a\ncurrent peta-scale system is demonstrated.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 00:03:16 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Huber", "Markus", ""], ["Gmeiner", "Bj\u00f6rn", ""], ["R\u00fcde", "Ulrich", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1506.07405", "submitter": "Dejiao Zhang", "authors": "Dejiao Zhang, Laura Balzano", "title": "Global Convergence of a Grassmannian Gradient Descent Algorithm for\n  Subspace Estimation", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been observed in a variety of contexts that gradient descent methods\nhave great success in solving low-rank matrix factorization problems, despite\nthe relevant problem formulation being non-convex. We tackle a particular\ninstance of this scenario, where we seek the $d$-dimensional subspace spanned\nby a streaming data matrix. We apply the natural first order incremental\ngradient descent method, constraining the gradient method to the Grassmannian.\nIn this paper, we propose an adaptive step size scheme that is greedy for the\nnoiseless case, that maximizes the improvement of our metric of convergence at\neach data index $t$, and yields an expected improvement for the noisy case. We\nshow that, with noise-free data, this method converges from any random\ninitialization to the global minimum of the problem. For noisy data, we provide\nthe expected convergence rate of the proposed algorithm per iteration.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 14:59:27 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 18:31:20 GMT"}, {"version": "v3", "created": "Sat, 25 Jun 2016 01:13:37 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Zhang", "Dejiao", ""], ["Balzano", "Laura", ""]]}, {"id": "1506.07540", "submitter": "Benjamin Haeffele", "authors": "Benjamin D. Haeffele and Rene Vidal", "title": "Global Optimality in Tensor Factorization, Deep Learning, and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques involving factorization are found in a wide range of applications\nand have enjoyed significant empirical success in many fields. However, common\nto a vast majority of these problems is the significant disadvantage that the\nassociated optimization problems are typically non-convex due to a multilinear\nform or other convexity destroying transformation. Here we build on ideas from\nconvex relaxations of matrix factorizations and present a very general\nframework which allows for the analysis of a wide range of non-convex\nfactorization problems - including matrix factorization, tensor factorization,\nand deep neural network training formulations. We derive sufficient conditions\nto guarantee that a local minimum of the non-convex optimization problem is a\nglobal minimum and show that if the size of the factorized variables is large\nenough then from any initialization it is possible to find a global minimizer\nusing a purely local descent algorithm. Our framework also provides a partial\ntheoretical justification for the increasingly common use of Rectified Linear\nUnits (ReLUs) in deep neural networks and offers guidance on deep network\narchitectures and regularization strategies to facilitate efficient\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 20:08:47 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Haeffele", "Benjamin D.", ""], ["Vidal", "Rene", ""]]}, {"id": "1506.07615", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, Zhouchen Lin, Chao Zhang", "title": "Completing Low-Rank Matrices with Corrupted Samples from Few\n  Coefficients in General Basis", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2016.2573311", "report-no": null, "categories": "cs.IT cs.LG cs.NA math.IT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace recovery from corrupted and missing data is crucial for various\napplications in signal processing and information theory. To complete missing\nvalues and detect column corruptions, existing robust Matrix Completion (MC)\nmethods mostly concentrate on recovering a low-rank matrix from few corrupted\ncoefficients w.r.t. standard basis, which, however, does not apply to more\ngeneral basis, e.g., Fourier basis. In this paper, we prove that the range\nspace of an $m\\times n$ matrix with rank $r$ can be exactly recovered from few\ncoefficients w.r.t. general basis, though $r$ and the number of corrupted\nsamples are both as high as $O(\\min\\{m,n\\}/\\log^3 (m+n))$. Our model covers\nprevious ones as special cases, and robust MC can recover the intrinsic matrix\nwith a higher rank. Moreover, we suggest a universal choice of the\nregularization parameter, which is $\\lambda=1/\\sqrt{\\log n}$. By our\n$\\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we can\nfurther reduce the computational cost of our model. As an application, we also\nfind that the solutions to extended robust Low-Rank Representation and to our\nextended robust MC are mutually expressible, so both our theory and algorithm\ncan be applied to the subspace clustering problem with missing values under\ncertain conditions. Experiments verify our theories.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 05:11:44 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 17:59:24 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Hongyang", ""], ["Lin", "Zhouchen", ""], ["Zhang", "Chao", ""]]}, {"id": "1506.07849", "submitter": "Youngsoo Choi", "authors": "Youngsoo Choi, Gabriele Boncoraglio, Spenser Anderson, David Amsallem,\n  Charbel Farhat", "title": "Gradient-based Constrained Optimization Using a Database of Linear\n  Reduced-Order Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A methodology grounded in model reduction is presented for accelerating the\ngradient-based solution of a family of linear or nonlinear constrained\noptimization problems where the constraints include at least one linear Partial\nDifferential Equation (PDE). A key component of this methodology is the\nconstruction, during an offline phase, of a database of pointwise, linear,\nProjection-based Reduced-Order Models (PROM)s associated with a design\nparameter space and the linear PDE(s). A parameter sampling procedure based on\nan appropriate saturation assumption is proposed to maximize the efficiency of\nsuch a database of PROMs. A real-time method is also presented for\ninterpolating at any queried but unsampled parameter vector in the design\nparameter space the relevant sensitivities of a PROM. The practical\nfeasibility, computational advantages, and performance of the proposed\nmethodology are demonstrated for several realistic, nonlinear, aerodynamic\nshape optimization problems governed by linear aeroelastic constraints.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 19:12:09 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 22:48:30 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Choi", "Youngsoo", ""], ["Boncoraglio", "Gabriele", ""], ["Anderson", "Spenser", ""], ["Amsallem", "David", ""], ["Farhat", "Charbel", ""]]}, {"id": "1506.08187", "submitter": "Yin Tat Lee", "authors": "S\\'ebastien Bubeck, Yin Tat Lee, Mohit Singh", "title": "A geometric alternative to Nesterov's accelerated gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for unconstrained optimization of a smooth and\nstrongly convex function, which attains the optimal rate of convergence of\nNesterov's accelerated gradient descent. The new algorithm has a simple\ngeometric interpretation, loosely inspired by the ellipsoid method. We provide\nsome numerical evidence that the new method can be superior to Nesterov's\naccelerated gradient descent.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 19:39:50 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Lee", "Yin Tat", ""], ["Singh", "Mohit", ""]]}, {"id": "1506.08272", "submitter": "Xiangru Lian", "authors": "Xiangru Lian, Yijun Huang, Yuncheng Li, Ji Liu", "title": "Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous parallel implementations of stochastic gradient (SG) have been\nbroadly used in solving deep neural network and received many successes in\npractice recently. However, existing theories cannot explain their convergence\nand speedup properties, mainly due to the nonconvexity of most deep learning\nformulations and the asynchronous parallel mechanism. To fill the gaps in\ntheory and provide theoretical supports, this paper studies two asynchronous\nparallel implementations of SG: one is on the computer network and the other is\non the shared memory system. We establish an ergodic convergence rate\n$O(1/\\sqrt{K})$ for both algorithms and prove that the linear speedup is\nachievable if the number of workers is bounded by $\\sqrt{K}$ ($K$ is the total\nnumber of iterations). Our results generalize and improve existing analysis for\nconvex minimization.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 08:41:50 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 15:39:14 GMT"}, {"version": "v3", "created": "Sat, 20 May 2017 07:35:06 GMT"}, {"version": "v4", "created": "Sat, 10 Jun 2017 05:04:18 GMT"}, {"version": "v5", "created": "Thu, 18 Apr 2019 18:25:04 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Lian", "Xiangru", ""], ["Huang", "Yijun", ""], ["Li", "Yuncheng", ""], ["Liu", "Ji", ""]]}, {"id": "1506.08350", "submitter": "Yadong Mu", "authors": "Yadong Mu and Wei Liu and Wei Fan", "title": "Stochastic Gradient Made Stable: A Manifold Propagation Approach for\n  Large-Scale Optimization", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) holds as a classical method to build large\nscale machine learning models over big data. A stochastic gradient is typically\ncalculated from a limited number of samples (known as mini-batch), so it\npotentially incurs a high variance and causes the estimated parameters bounce\naround the optimal solution. To improve the stability of stochastic gradient,\nrecent years have witnessed the proposal of several semi-stochastic gradient\ndescent algorithms, which distinguish themselves from standard SGD by\nincorporating global information into gradient computation. In this paper we\ncontribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm\nto this nascent research area, accelerating the optimization of a large family\nof composite convex functions. Though theoretically converging faster, prior\nsemi-stochastic algorithms are found to suffer from high iteration complexity,\nwhich makes them even slower than SGD in practice on many datasets. In our\nproposed S3GD, the semi-stochastic gradient is calculated based on efficient\nmanifold propagation, which can be numerically accomplished by sparse matrix\nmultiplications. This way S3GD is able to generate a highly-accurate estimate\nof the exact gradient from each mini-batch with largely-reduced computational\ncomplexity. Theoretic analysis reveals that the proposed S3GD elegantly\nbalances the geometric algorithmic convergence rate against the space and time\ncomplexities during the optimization. The efficacy of S3GD is also\nexperimentally corroborated on several large-scale benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 03:33:38 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 21:30:08 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Mu", "Yadong", ""], ["Liu", "Wei", ""], ["Fan", "Wei", ""]]}, {"id": "1506.08435", "submitter": "Kalyana Babu Nakshatrala", "authors": "J. Chang, S. Karra and K. B. Nakshatrala", "title": "Large-scale Optimization-based Non-negative Computational Framework for\n  Diffusion Equations: Parallel Implementation and Performance Studies", "comments": null, "journal-ref": null, "doi": "10.1007/s10915-016-0250-5", "report-no": null, "categories": "cs.NA cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the standard Galerkin formulation, which is often the\nformulation of choice under the finite element method for solving self-adjoint\ndiffusion equations, does not meet maximum principles and the non-negative\nconstraint for anisotropic diffusion equations. Recently, optimization-based\nmethodologies that satisfy maximum principles and the non-negative constraint\nfor steady-state and transient diffusion-type equations have been proposed. To\ndate, these methodologies have been tested only on small-scale academic\nproblems. The purpose of this paper is to systematically study the performance\nof the non-negative methodology in the context of high performance computing\n(HPC). PETSc and TAO libraries are, respectively, used for the parallel\nenvironment and optimization solvers. For large-scale problems, it is important\nfor computational scientists to understand the computational performance of\ncurrent algorithms available in these scientific libraries. The numerical\nexperiments are conducted on the state-of-the-art HPC systems, and a\nsingle-core performance model is used to better characterize the efficiency of\nthe solvers. Our studies indicate that the proposed non-negative computational\nframework for diffusion-type equations exhibits excellent strong scaling for\nreal-world large-scale problems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 18:57:25 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2015 21:45:18 GMT"}, {"version": "v3", "created": "Sat, 9 Apr 2016 18:44:40 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Chang", "J.", ""], ["Karra", "S.", ""], ["Nakshatrala", "K. B.", ""]]}, {"id": "1506.08938", "submitter": "Nguyen Duy Khuong", "authors": "Duy-Khuong Nguyen and Tu-Bao Ho", "title": "Accelerated Parallel and Distributed Algorithm using Limited Internal\n  Memory for Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a powerful technique for dimension\nreduction, extracting latent factors and learning part-based representation.\nFor large datasets, NMF performance depends on some major issues: fast\nalgorithms, fully parallel distributed feasibility and limited internal memory.\nThis research aims to design a fast fully parallel and distributed algorithm\nusing limited internal memory to reach high NMF performance for large datasets.\nIn particular, we propose a flexible accelerated algorithm for NMF with all its\n$L_1$ $L_2$ regularized variants based on full decomposition, which is a\ncombination of an anti-lopsided algorithm and a fast block coordinate descent\nalgorithm. The proposed algorithm takes advantages of both these algorithms to\nachieve a linear convergence rate of $\\mathcal{O}(1-\\frac{1}{||Q||_2})^k$ in\noptimizing each factor matrix when fixing the other factor one in the sub-space\nof passive variables, where $r$ is the number of latent components; where\n$\\sqrt{r} \\leq ||Q||_2 \\leq r$. In addition, the algorithm can exploit the data\nsparseness to run on large datasets with limited internal memory of machines.\nFurthermore, our experimental results are highly competitive with 7\nstate-of-the-art methods about three significant aspects of convergence,\noptimality and average of the iteration number. Therefore, the proposed\nalgorithm is superior to fast block coordinate descent methods and accelerated\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 04:58:10 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Nguyen", "Duy-Khuong", ""], ["Ho", "Tu-Bao", ""]]}, {"id": "1506.08988", "submitter": "Francisco Igual", "authors": "Sandra Catal\\'an, Francisco D. Igual, Rafael Mayo, Rafael\n  Rodr\\'iguez-S\\'anchez and Enrique S. Quintana-Ort\\'i", "title": "Architecture-Aware Configuration and Scheduling of Matrix Multiplication\n  on Asymmetric Multicore Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetric multicore processors (AMPs) have recently emerged as an appealing\ntechnology for severely energy-constrained environments, especially in mobile\nappliances where heterogeneity in applications is mainstream. In addition,\ngiven the growing interest for low-power high performance computing, this type\nof architectures is also being investigated as a means to improve the\nthroughput-per-Watt of complex scientific applications.\n  In this paper, we design and embed several architecture-aware optimizations\ninto a multi-threaded general matrix multiplication (gemm), a key operation of\nthe BLAS, in order to obtain a high performance implementation for ARM\nbig.LITTLE AMPs. Our solution is based on the reference implementation of gemm\nin the BLIS library, and integrates a cache-aware configuration as well as\nasymmetric--static and dynamic scheduling strategies that carefully tune and\ndistribute the operation's micro-kernels among the big and LITTLE cores of the\ntarget processor. The experimental results on a Samsung Exynos 5422, a\nsystem-on-chip with ARM Cortex-A15 and Cortex-A7 clusters that implements the\nbig.LITTLE model, expose that our cache-aware versions of gemm with asymmetric\nscheduling attain important gains in performance with respect to its\narchitecture-oblivious counterparts while exploiting all the resources of the\nAMP to deliver considerable energy efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 08:35:15 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Catal\u00e1n", "Sandra", ""], ["Igual", "Francisco D.", ""], ["Mayo", "Rafael", ""], ["Rodr\u00edguez-S\u00e1nchez", "Rafael", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""]]}, {"id": "1506.09016", "submitter": "Th\\'eo Trouillon", "authors": "Guillaume Bouchard, Th\\'eo Trouillon, Julien Perez, Adrien Gaidon", "title": "Online Learning to Sample", "comments": "Update: removed convergence theorem and proof as there is an error.\n  Submitted to UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is one of the most widely used techniques\nfor online optimization in machine learning. In this work, we accelerate SGD by\nadaptively learning how to sample the most useful training examples at each\ntime step. First, we show that SGD can be used to learn the best possible\nsampling distribution of an importance sampling estimator. Second, we show that\nthe sampling distribution of an SGD algorithm can be estimated online by\nincrementally minimizing the variance of the gradient. The resulting algorithm\n- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to\noptimize, as well as a set of parameters to sample learning examples. We show\nthat AWSGD yields faster convergence in three different applications: (i) image\nclassification with deep features, where the sampling of images depends on\ntheir labels, (ii) matrix factorization, where rows and columns are not sampled\nuniformly, and (iii) reinforcement learning, where the optimized and\nexploration policies are estimated at the same time, where our approach\ncorresponds to an off-policy gradient algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 10:08:35 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 16:08:56 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Bouchard", "Guillaume", ""], ["Trouillon", "Th\u00e9o", ""], ["Perez", "Julien", ""], ["Gaidon", "Adrien", ""]]}]