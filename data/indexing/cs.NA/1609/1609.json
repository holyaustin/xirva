[{"id": "1609.00048", "submitter": "Joel Tropp", "authors": "Joel A. Tropp, Alp Yurtsever, Madeleine Udell, Volkan Cevher", "title": "Practical sketching algorithms for low-rank matrix approximation", "comments": null, "journal-ref": "SIAM J. Matrix Analysis and Applications, Vol. 38, num. 4, pp.\n  1454-1485, Dec. 2017", "doi": "10.1137/17M1111590", "report-no": null, "categories": "cs.NA cs.DS math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a suite of algorithms for constructing low-rank\napproximations of an input matrix from a random linear image of the matrix,\ncalled a sketch. These methods can preserve structural properties of the input\nmatrix, such as positive-semidefiniteness, and they can produce approximations\nwith a user-specified rank. The algorithms are simple, accurate, numerically\nstable, and provably correct. Moreover, each method is accompanied by an\ninformative error bound that allows users to select parameters a priori to\nachieve a given approximation quality. These claims are supported by numerical\nexperiments with real and synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 21:30:26 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 18:13:40 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Tropp", "Joel A.", ""], ["Yurtsever", "Alp", ""], ["Udell", "Madeleine", ""], ["Cevher", "Volkan", ""]]}, {"id": "1609.00324", "submitter": "Fabiana Zama", "authors": "V. Bortolotti, R. J. S. Brown, P. Fantazzini, G. Landi, F. Zama", "title": "Uniform Penalty inversion of two-dimensional NMR Relaxation data", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/33/1/015003", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inversion of two-dimensional NMR data is an ill-posed problem related to\nthe numerical computation of the inverse Laplace transform. In this paper we\npresent the 2DUPEN algorithm that extends the Uniform Penalty (UPEN) algorithm\n[Borgia, Brown, Fantazzini, {\\em Journal of Magnetic Resonance}, 1998] to\ntwo-dimensional data. The UPEN algorithm, defined for the inversion of\none-dimensional NMR relaxation data, uses Tikhonov-like regularization and\noptionally non-negativity constraints in order to implement locally adapted\nregularization. In this paper, we analyze the regularization properties of this\napproach. Moreover, we extend the one-dimensional UPEN algorithm to the\ntwo-dimensional case and present an efficient implementation based on the\nNewton Projection method. Without any a-priori information on the noise norm,\n2DUPEN automatically computes the locally adapted regularization parameters and\nthe distribution of the unknown NMR parameters by using variable smoothing.\nResults of numerical experiments on simulated and real data are presented in\norder to illustrate the potential of the proposed method in reconstructing\npeaks and flat regions with the same accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 17:33:33 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Bortolotti", "V.", ""], ["Brown", "R. J. S.", ""], ["Fantazzini", "P.", ""], ["Landi", "G.", ""], ["Zama", "F.", ""]]}, {"id": "1609.00671", "submitter": "Eugenia-Maria Kontopoulou", "authors": "Petros Drineas and Ilse Ipsen and Eugenia-Maria Kontopoulou and Malik\n  Magdon-Ismail", "title": "Structural Convergence Results for Approximation of Dominant Subspaces\n  from Block Krylov Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with approximating the dominant left singular vector\nspace of a real matrix $A$ of arbitrary dimension, from block Krylov spaces\ngenerated by the matrix $AA^T$ and the block vector $AX$. Two classes of\nresults are presented. First are bounds on the distance, in the two and\nFrobenius norms, between the Krylov space and the target space. The distance is\nexpressed in terms of principal angles. Second are quality of approximation\nbounds, relative to the best approximation in the Frobenius norm. For starting\nguesses $X$ of full column-rank, the bounds depend on the tangent of the\nprincipal angles between $X$ and the dominant right singular vector space of\n$A$. The results presented here form the structural foundation for the analysis\nof randomized Krylov space methods. The innovative feature is a combination of\ntraditional Lanczos convergence analysis with optimal approximations via least\nsquares problems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 17:20:46 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 01:42:17 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Drineas", "Petros", ""], ["Ipsen", "Ilse", ""], ["Kontopoulou", "Eugenia-Maria", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1609.00829", "submitter": "Javier Segura", "authors": "A. Gil, J. Segura, N. M. Temme", "title": "Efficient computation of Laguerre polynomials", "comments": "To appear in Computer Physics Communications", "journal-ref": null, "doi": "10.1016/j.cpc.2016.09.002", "report-no": null, "categories": "cs.NA cs.MS math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient algorithm and a Fortran 90 module (LaguerrePol) for computing\nLaguerre polynomials $L^{(\\alpha)}_n(z)$ are presented. The standard three-term\nrecurrence relation satisfied by the polynomials and different types of\nasymptotic expansions valid for $n$ large and $\\alpha$ small, are used\ndepending on the parameter region.\n  Based on tests of contiguous relations in the parameter $\\alpha$ and the\ndegree $n$ satisfied by the polynomials, we claim that a relative accuracy\nclose or better than $10^{-12}$ can be obtained using the module LaguerrePol\nfor computing the functions $L^{(\\alpha)}_n(z)$ in the parameter range $z \\ge\n0$, $-1 < \\alpha \\le 5$, $n \\ge 0$.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 13:59:43 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Gil", "A.", ""], ["Segura", "J.", ""], ["Temme", "N. M.", ""]]}, {"id": "1609.00893", "submitter": "Andrzej Cichocki", "authors": "A. Cichocki, N. Lee, I.V. Oseledets, A.-H. Phan, Q. Zhao, D. Mandic", "title": "Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale\n  Optimization Problems: Perspectives and Challenges PART 1", "comments": "176 pages", "journal-ref": "Foundations and Trends in Machine Learning, vol. 9, no. 4-5, pp.\n  249-429, 2016", "doi": "10.1561/2200000059", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and data mining algorithms are becoming increasingly\nimportant in analyzing large volume, multi-relational and multi--modal\ndatasets, which are often conveniently represented as multiway arrays or\ntensors. It is therefore timely and valuable for the multidisciplinary research\ncommunity to review tensor decompositions and tensor networks as emerging tools\nfor large-scale data analysis and data mining. We provide the mathematical and\ngraphical representations and interpretation of tensor networks, with the main\nfocus on the Tucker and Tensor Train (TT) decompositions and their extensions\nor generalizations.\n  Keywords: Tensor networks, Function-related tensors, CP decomposition, Tucker\nmodels, tensor train (TT) decompositions, matrix product states (MPS), matrix\nproduct operators (MPO), basic tensor operations, multiway component analysis,\nmultilinear blind source separation, tensor completion, linear/multilinear\ndimensionality reduction, large-scale optimization problems, symmetric\neigenvalue decomposition (EVD), PCA/SVD, huge systems of linear equations,\npseudo-inverse of very large matrices, Lasso and Canonical Correlation Analysis\n(CCA) (This is Part 1)\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 05:12:43 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 16:35:24 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 10:04:39 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Cichocki", "A.", ""], ["Lee", "N.", ""], ["Oseledets", "I. V.", ""], ["Phan", "A. -H.", ""], ["Zhao", "Q.", ""], ["Mandic", "D.", ""]]}, {"id": "1609.01689", "submitter": "Meiyue Shao", "authors": "Meiyue Shao, Hasan Metin Aktulga, Chao Yang, Esmond G. Ng, Pieter\n  Maris, James P. Vary", "title": "Accelerating Nuclear Configuration Interaction Calculations through a\n  Preconditioned Block Iterative Eigensolver", "comments": "Accepted for publication in Computer Physics Communications", "journal-ref": "Computer Physics Communications, 222:1--13, 2018", "doi": "10.1016/j.cpc.2017.09.004", "report-no": null, "categories": "cs.NA cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a number of recently developed techniques for improving the\nperformance of large-scale nuclear configuration interaction calculations on\nhigh performance parallel computers. We show the benefit of using a\npreconditioned block iterative method to replace the Lanczos algorithm that has\ntraditionally been used to perform this type of computation. The rapid\nconvergence of the block iterative method is achieved by a proper choice of\nstarting guesses of the eigenvectors and the construction of an effective\npreconditioner. These acceleration techniques take advantage of special\nstructure of the nuclear configuration interaction problem which we discuss in\ndetail. The use of a block method also allows us to improve the concurrency of\nthe computation, and take advantage of the memory hierarchy of modern\nmicroprocessors to increase the arithmetic intensity of the computation\nrelative to data movement. We also discuss implementation details that are\ncritical to achieving high performance on massively parallel multi-core\nsupercomputers, and demonstrate that the new block iterative solver is two to\nthree times faster than the Lanczos based algorithm for problems of moderate\nsizes on a Cray XC30 system.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 18:32:36 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 19:16:49 GMT"}, {"version": "v3", "created": "Fri, 8 Sep 2017 22:09:38 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Shao", "Meiyue", ""], ["Aktulga", "Hasan Metin", ""], ["Yang", "Chao", ""], ["Ng", "Esmond G.", ""], ["Maris", "Pieter", ""], ["Vary", "James P.", ""]]}, {"id": "1609.02258", "submitter": "Haishan Ye", "authors": "Haishan Ye, Qiaoming Ye, Zhihua Zhang", "title": "Tighter bound of Sketched Generalized Matrix Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalized matrix approximation plays a fundamental role in many machine\nlearning problems, such as CUR decomposition, kernel approximation, and matrix\nlow rank approximation. Especially with today's applications involved in larger\nand larger dataset, more and more efficient generalized matrix approximation\nalgorithems become a crucially important research issue. In this paper, we find\nnew sketching techniques to reduce the size of the original data matrix to\ndevelop new matrix approximation algorithms. Our results derive a much tighter\nbound for the approximation than previous works: we obtain a $(1+\\epsilon)$\napproximation ratio with small sketched dimensions which implies a more\nefficient generalized matrix approximation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 04:01:02 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Ye", "Haishan", ""], ["Ye", "Qiaoming", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1609.02302", "submitter": "Axel Flinth", "authors": "Axel Flinth", "title": "Soft Recovery Through $\\ell_{1,2}$ Minimization with Applications in\n  Recovery of Simultaneously Sparse and Low-Rank Matrice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a new type of analysis of a compressed-sensing based\ntechnique for recovering column-sparse matrices, namely minimization of the\n$\\ell_{1,2}$-norm. Rather than providing conditions on the measurement matrix\nwhich guarantees the solution of the program to be exactly equal to the ground\ntruth signal (which already has been thoroughly investigated), it presents a\ncondition which guarantees that the solution is approximately equal to the\nground truth. Soft recovery statements of this kind are to the best knowledge\nof the author a novelty in Compressed Sensing. Apart from the theoretical\nanalysis, we present two heuristic proposes how this property of the\n$\\ell_{1,2}$-program can be utilized to design algorithms for recovery of\nmatrices which are sparse and have low rank at the same time.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 07:28:41 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Flinth", "Axel", ""]]}, {"id": "1609.02393", "submitter": "Hendrik Ranocha", "authors": "Philipp \\\"Offner, Jan Glaubitz, Hendrik Ranocha", "title": "Analysis of Artificial Dissipation of Explicit and Implicit\n  Time-Integration Methods", "comments": null, "journal-ref": "International Journal of Numerical Analysis and Modeling, 17.3:\n  332-349, 2020", "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability is an important aspect of numerical methods for hyperbolic\nconservation laws and has received much interest. However, continuity in time\nis often assumed and only semidiscrete stability is studied. Thus, it is\ninteresting to investigate the influence of explicit and implicit time\nintegration methods on the stability of numerical schemes. If an explicit time\nintegration method is applied, spacially stable numerical schemes for\nhyperbolic conservation laws can result in unstable fully discrete schemes.\nFocusing on the explicit Euler method (and convex combinations thereof),\nundesired terms in the energy balance trigger this phenomenon and introduce an\nerroneous growth of the energy over time. In this work, we study the influence\nof artificial dissipation and modal filtering in the context of discontinuous\nspectral element methods to remedy these issues. In particular, lower bounds on\nthe strength of both artificial dissipation and modal filtering operators are\ngiven and an adaptive procedure to conserve the (discrete) $\\mathbf{L}_2$ norm\nof the numerical solution in time is derived. This might be beneficial in\nregions where the solution is smooth and for long time simulations. Moreover,\nthis approach is used to study the connections between explicit and implicit\ntime integration methods and the associated energy production. By adjusting the\nadaptive procedure, we demonstrate that filtering in explicit time integration\nmethods is able to mimic the dissipative behavior inherent in implicit time\nintegration methods. This contribution leads to a better understanding of\nexisting algorithms and numerical techniques, in particular the application of\nartificial dissipation as well as modal filtering in the context of numerical\nmethods for hyperbolic conservation laws together with the selection of\nexplicit or implicit time integration methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 12:30:17 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 14:24:36 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["\u00d6ffner", "Philipp", ""], ["Glaubitz", "Jan", ""], ["Ranocha", "Hendrik", ""]]}, {"id": "1609.04167", "submitter": "Laurent Jacques", "authors": "V. Abrol, O. Absil, P.-A. Absil, S. Anthoine, P. Antoine, T. Arildsen,\n  N. Bertin, F. Bleichrodt, J. Bobin, A. Bol, A. Bonnefoy, F. Caltagirone, V.\n  Cambareri, C. Chenot, V. Crnojevi\\'c, M. Da\\v{n}kov\\'a, K. Degraux, J.\n  Eisert, J. M. Fadili, M. Gabri\\'e, N. Gac, D. Giacobello, A. Gonzalez, C. A.\n  Gomez Gonzalez, A. Gonz\\'alez, P.-Y. Gousenbourger, M. Gr{\\ae}sb{\\o}ll\n  Christensen, R. Gribonval, S. Gu\\'erit, S. Huang, P. Irofti, L. Jacques, U.\n  S. Kamilov, S. Kitic\\'c, M. Kliesch, F. Krzakala, J. A. Lee, W. Liao, T.\n  Lindstr{\\o}m Jensen, A. Manoel, H. Mansour, A. Mohammad-Djafari, A.\n  Moshtaghpour, F. Ngol\\`e, B. Pairet, M. Pani\\'c, G. Peyr\\'e, A. Pi\\v{z}urica,\n  P. Rajmic, M. Roblin, I. Roth, A. K. Sao, P. Sharma, J.-L. Starck, E. W.\n  Tramel, T. van Waterschoot, D. Vukobratovic, L. Wang, B. Wirth, G. Wunder, H.\n  Zhang", "title": "Proceedings of the third \"international Traveling Workshop on\n  Interactions between Sparse models and Technology\" (iTWIST'16)", "comments": "69 pages, 22 extended abstracts, iTWIST'16 website:\n  http://www.itwist16.es.aau.dk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The third edition of the \"international - Traveling Workshop on Interactions\nbetween Sparse models and Technology\" (iTWIST) took place in Aalborg, the 4th\nlargest city in Denmark situated beautifully in the northern part of the\ncountry, from the 24th to 26th of August 2016. The workshop venue was at the\nAalborg University campus. One implicit objective of this biennial workshop is\nto foster collaboration between international scientific teams by disseminating\nideas through both specific oral/poster presentations and free discussions. For\nthis third edition, iTWIST'16 gathered about 50 international participants and\nfeatures 8 invited talks, 12 oral presentations, and 12 posters on the\nfollowing themes, all related to the theory, application and generalization of\nthe \"sparsity paradigm\": Sparsity-driven data sensing and processing (e.g.,\noptics, computer vision, genomics, biomedical, digital communication, channel\nestimation, astronomy); Application of sparse models in non-convex/non-linear\ninverse problems (e.g., phase retrieval, blind deconvolution, self\ncalibration); Approximate probabilistic inference for sparse problems; Sparse\nmachine learning and inference; \"Blind\" inverse problems and dictionary\nlearning; Optimization for sparse modelling; Information theory, geometry and\nrandomness; Sparsity? What's next? (Discrete-valued signals; Union of\nlow-dimensional spaces, Cosparsity, mixed/group norm, model-based,\nlow-complexity models, ...); Matrix/manifold sensing/processing (graph,\nlow-rank approximation, ...); Complexity/accuracy tradeoffs in numerical\nmethods/optimization; Electronic/optical compressive sensors (hardware).\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 08:27:11 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Abrol", "V.", ""], ["Absil", "O.", ""], ["Absil", "P. -A.", ""], ["Anthoine", "S.", ""], ["Antoine", "P.", ""], ["Arildsen", "T.", ""], ["Bertin", "N.", ""], ["Bleichrodt", "F.", ""], ["Bobin", "J.", ""], ["Bol", "A.", ""], ["Bonnefoy", "A.", ""], ["Caltagirone", "F.", ""], ["Cambareri", "V.", ""], ["Chenot", "C.", ""], ["Crnojevi\u0107", "V.", ""], ["Da\u0148kov\u00e1", "M.", ""], ["Degraux", "K.", ""], ["Eisert", "J.", ""], ["Fadili", "J. M.", ""], ["Gabri\u00e9", "M.", ""], ["Gac", "N.", ""], ["Giacobello", "D.", ""], ["Gonzalez", "A.", ""], ["Gonzalez", "C. A. Gomez", ""], ["Gonz\u00e1lez", "A.", ""], ["Gousenbourger", "P. -Y.", ""], ["Christensen", "M. Gr\u00e6sb\u00f8ll", ""], ["Gribonval", "R.", ""], ["Gu\u00e9rit", "S.", ""], ["Huang", "S.", ""], ["Irofti", "P.", ""], ["Jacques", "L.", ""], ["Kamilov", "U. S.", ""], ["Kitic\u0107", "S.", ""], ["Kliesch", "M.", ""], ["Krzakala", "F.", ""], ["Lee", "J. A.", ""], ["Liao", "W.", ""], ["Jensen", "T. Lindstr\u00f8m", ""], ["Manoel", "A.", ""], ["Mansour", "H.", ""], ["Mohammad-Djafari", "A.", ""], ["Moshtaghpour", "A.", ""], ["Ngol\u00e8", "F.", ""], ["Pairet", "B.", ""], ["Pani\u0107", "M.", ""], ["Peyr\u00e9", "G.", ""], ["Pi\u017eurica", "A.", ""], ["Rajmic", "P.", ""], ["Roblin", "M.", ""], ["Roth", "I.", ""], ["Sao", "A. K.", ""], ["Sharma", "P.", ""], ["Starck", "J. -L.", ""], ["Tramel", "E. W.", ""], ["van Waterschoot", "T.", ""], ["Vukobratovic", "D.", ""], ["Wang", "L.", ""], ["Wirth", "B.", ""], ["Wunder", "G.", ""], ["Zhang", "H.", ""]]}, {"id": "1609.04585", "submitter": "Daniel Langr", "authors": "Daniel Langr", "title": "On Memory Footprints of Partitioned Sparse Matrices", "comments": null, "journal-ref": "Annals of Computer Science and Information Systems, Volume 11,\n  2017", "doi": "10.15439/2017F70", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime characteristics of sparse matrix computations and related processes\nmay be often improved by reducing memory footprints of involved matrices. Such\na reduction can be usually achieved when matrices are processed in a block-wise\nmanner. The presented study analysed memory footprints of 563 representative\nbenchmark sparse matrices with respect to their partitioning into\nuniformly-sized blocks. Different block sizes and different ways of storing\nblocks in memory were considered and statistically evaluated. Memory footprints\nof partitioned matrices were additionally compared with lower bounds and with\nthe CSR storage format. The average measured memory savings against CSR in case\nof single and double precision were 42.3 and 28.7 percents, the corresponding\nworst-case savings 25.5 and 17.1 percents. Moreover, memory footprints of\npartitioned matrices were in average 5 times closer to their lower bounds than\nCSR. Based on the obtained results, generic suggestions for efficient\npartitioning and storage of sparse matrices in a computer memory are provided.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 11:54:58 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 16:16:48 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Langr", "Daniel", ""]]}, {"id": "1609.05434", "submitter": "Yoni Choukroun", "authors": "Alex Bronstein, Yoni Choukroun, Ron Kimmel, Matan Sela", "title": "Consistent Discretization and Minimization of the L1 Norm on Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The L1 norm has been tremendously popular in signal and image processing in\nthe past two decades due to its sparsity-promoting properties. More recently,\nits generalization to non-Euclidean domains has been found useful in shape\nanalysis applications. For example, in conjunction with the minimization of the\nDirichlet energy, it was shown to produce a compactly supported quasi-harmonic\northonormal basis, dubbed as compressed manifold modes. The continuous L1 norm\non the manifold is often replaced by the vector l1 norm applied to sampled\nfunctions. We show that such an approach is incorrect in the sense that it does\nnot consistently discretize the continuous norm and warn against its\nsensitivity to the specific sampling. We propose two alternative\ndiscretizations resulting in an iteratively-reweighed l2 norm. We demonstrate\nthe proposed strategy on the compressed modes problem, which reduces to a\nsequence of simple eigendecomposition problems not requiring non-convex\noptimization on Stiefel manifolds and producing more stable and accurate\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 06:56:57 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Bronstein", "Alex", ""], ["Choukroun", "Yoni", ""], ["Kimmel", "Ron", ""], ["Sela", "Matan", ""]]}, {"id": "1609.05587", "submitter": "Vaneet Aggarwal", "authors": "Wenqi Wang and Vaneet Aggarwal and Shuchin Aeron", "title": "Tensor Completion by Alternating Minimization under the Tensor Train\n  (TT) Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the matrix product state (MPS) representation of tensor train\ndecompositions, in this paper we propose a tensor completion algorithm which\nalternates over the matrices (tensors) in the MPS representation. This\ndevelopment is motivated in part by the success of matrix completion algorithms\nwhich alternate over the (low-rank) factors. We comment on the computational\ncomplexity of the proposed algorithm and numerically compare it with existing\nmethods employing low rank tensor train approximation for data completion as\nwell as several other recently proposed methods. We show that our method is\nsuperior to existing ones for a variety of real settings.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 03:25:33 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Wang", "Wenqi", ""], ["Aggarwal", "Vaneet", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1609.06431", "submitter": "Nicolas Bousserez", "authors": "Nicolas Bousserez and Daven K. Henze", "title": "Optimal and scalable methods to approximate the solutions of large-scale\n  Bayesian problems: Theory and application to atmospheric inversions and data\n  assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.NA math.NA physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a detailed theoretical analysis of methods to approximate\nthe solutions of high-dimensional (>10^6) linear Bayesian problems. An optimal\nlow-rank projection that maximizes the information content of the Bayesian\ninversion is proposed and efficiently constructed using a scalable randomized\nSVD algorithm. Useful optimality results are established for the associated\nposterior error covariance matrix and posterior mean approximations, which are\nfurther investigated in a numerical experiment consisting of a large-scale\natmospheric tracer transport source-inversion problem. This method proves to be\na robust and efficient approach to dimension reduction, as well as a natural\nframework to analyze the information content of the inversion. Possible\nextensions of this approach to the non-linear framework in the context of\noperational numerical weather forecast data assimilation systems based on the\nincremental 4D-Var technique are also discussed, and a detailed implementation\nof a new Randomized Incremental Optimal Technique (RIOT) for 4D-Var algorithms\nleveraging our theoretical results is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 06:44:35 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Bousserez", "Nicolas", ""], ["Henze", "Daven K.", ""]]}, {"id": "1609.06762", "submitter": "Jaehyun Park", "authors": "Jaehyun Park", "title": "Sparsity-Preserving Difference of Positive Semidefinite Matrix\n  Representation of Indefinite Matrices", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of writing an arbitrary symmetric matrix as the\ndifference of two positive semidefinite matrices. We start with simple ideas\nsuch as eigenvalue decomposition. Then, we develop a simple adaptation of the\nCholesky that returns a difference-of-Cholesky representation of indefinite\nmatrices. Heuristics that promote sparsity can be applied directly to this\nmodification.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 21:38:06 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Park", "Jaehyun", ""]]}, {"id": "1609.07086", "submitter": "Arvind Saibaba", "authors": "Jiani Zhang, Arvind K. Saibaba, Misha Kilmer, Shuchin Aeron", "title": "A Randomized Tensor Singular Value Decomposition based on the t-product", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tensor Singular Value Decomposition (t-SVD) for third order tensors that\nwas proposed by Kilmer and Martin~\\cite{2011kilmer} has been applied\nsuccessfully in many fields, such as computed tomography, facial recognition,\nand video completion. In this paper, we propose a method that extends a\nwell-known randomized matrix method to the t-SVD. This method can produce a\nfactorization with similar properties to the t-SVD, but is more computationally\nefficient on very large datasets. We present details of the algorithm,\ntheoretical results, and provide numerical results that show the promise of our\napproach for compressing and analyzing datasets. We also present an improved\nanalysis of the randomized subspace iteration for matrices, which may be of\nindependent interest to the scientific community.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 17:55:21 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Zhang", "Jiani", ""], ["Saibaba", "Arvind K.", ""], ["Kilmer", "Misha", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1609.07373", "submitter": "Tuomo Valkonen", "authors": "Tuomo Valkonen", "title": "Block-proximal methods with spatially adapted acceleration", "comments": null, "journal-ref": null, "doi": "10.1553/etna_vol51s15", "report-no": null, "categories": "math.OC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study and develop (stochastic) primal--dual block-coordinate descent\nmethods for convex problems based on the method due to Chambolle and Pock. Our\nmethods have known convergence rates for the iterates and the ergodic gap:\n$O(1/N^2)$ if each block is strongly convex, $O(1/N)$ if no convexity is\npresent, and more generally a mixed rate $O(1/N^2)+O(1/N)$ for strongly convex\nblocks, if only some blocks are strongly convex. Additional novelties of our\nmethods include blockwise-adapted step lengths and acceleration, as well as the\nability to update both the primal and dual variables randomly in blocks under a\nvery light compatibility condition. In other words, these variants of our\nmethods are doubly-stochastic. We test the proposed methods on various image\nprocessing problems, where we employ pixelwise-adapted acceleration.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 14:18:25 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 16:42:44 GMT"}, {"version": "v3", "created": "Tue, 26 Dec 2017 13:22:21 GMT"}, {"version": "v4", "created": "Sun, 18 Nov 2018 19:10:05 GMT"}, {"version": "v5", "created": "Thu, 3 Jan 2019 15:23:40 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Valkonen", "Tuomo", ""]]}, {"id": "1609.07429", "submitter": "Greg Ongie", "authors": "Greg Ongie, Mathews Jacob", "title": "A Fast Algorithm for Convolutional Structured Low-Rank Matrix Recovery", "comments": "Accepted for publication in IEEE Transactions on Computational\n  Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier domain structured low-rank matrix priors are emerging as powerful\nalternatives to traditional image recovery methods such as total variation and\nwavelet regularization. These priors specify that a convolutional structured\nmatrix, i.e., Toeplitz, Hankel, or their multi-level generalizations, built\nfrom Fourier data of the image should be low-rank. The main challenge in\napplying these schemes to large-scale problems is the computational complexity\nand memory demand resulting from lifting the image data to a large scale\nmatrix. We introduce a fast and memory efficient approach called the Generic\nIterative Reweighted Annihilation Filter (GIRAF) algorithm that exploits the\nconvolutional structure of the lifted matrix to work in the original un-lifted\ndomain, thus considerably reducing the complexity. Our experiments on the\nrecovery of images from undersampled Fourier measurements show that the\nresulting algorithm is considerably faster than previously proposed algorithms,\nand can accommodate much larger problem sizes than previously studied.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 16:58:11 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 19:16:56 GMT"}, {"version": "v3", "created": "Sun, 25 Jun 2017 21:15:57 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Ongie", "Greg", ""], ["Jacob", "Mathews", ""]]}, {"id": "1609.07444", "submitter": "Art Owen", "authors": "Kinjal Basu and Art B. Owen", "title": "Quasi-Monte Carlo for an Integrand with a Singularity along a Diagonal\n  in the Square", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Monte Carlo methods are designed for integrands of bounded variation,\nand this excludes singular integrands. Several methods are known for integrands\nthat become singular on the boundary of the unit cube $[0,1]^d$ or at isolated\npossibly unknown points within $[0,1]^d$. Here we consider functions on the\nsquare $[0,1]^2$ that may become singular as the point approaches the diagonal\nline $x_1=x_2$, and we study three quadrature methods. The first method splits\nthe square into two triangles separated by a region around the line of\nsingularity, and applies recently developed triangle QMC rules to the two\ntriangular parts. For functions with a singularity `no worse than\n$|x_1-x_2|^{-A}$ for $0<A<1$ that method yields an error of $O(\n(\\log(n)/n)^{(1-A)/2})$. We also consider methods extending the integrand into\na region containing the singularity and show that method will not improve up on\nusing two triangles. Finally, we consider transforming the integrand to have a\nmore QMC-friendly singularity along the boundary of the square. This then leads\nto error rates of $O(n^{-1+\\epsilon+A})$ when combined with some\ncorner-avoiding Halton points or with randomized QMC, but it requires some\nstronger assumptions on the original singular integrand.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 17:39:36 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 22:26:44 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 18:14:17 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Basu", "Kinjal", ""], ["Owen", "Art B.", ""]]}, {"id": "1609.08063", "submitter": "Philippe Dreesen", "authors": "Philippe Dreesen and David Westwick and Johan Schoukens and Mariya\n  Ishteva", "title": "Modeling Parallel Wiener-Hammerstein Systems Using Tensor Decomposition\n  of Volterra Kernels", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing flexibility and user-interpretability in nonlinear system\nidentification can be achieved by means of block-oriented methods. One of such\nblock-oriented system structures is the parallel Wiener-Hammerstein system,\nwhich is a sum of Wiener-Hammerstein branches, consisting of static\nnonlinearities sandwiched between linear dynamical blocks. Parallel\nWiener-Hammerstein models have more descriptive power than their single-branch\ncounterparts, but their identification is a non-trivial task that requires\ntailored system identification methods. In this work, we will tackle the\nidentification problem by performing a tensor decomposition of the Volterra\nkernels obtained from the nonlinear system. We illustrate how the parallel\nWiener-Hammerstein block-structure gives rise to a joint tensor decomposition\nof the Volterra kernels with block-circulant structured factors. The\ncombination of Volterra kernels and tensor methods is a fruitful way to tackle\nthe parallel Wiener-Hammerstein system identification task. In simulation\nexperiments, we were able to reconstruct very accurately the underlying blocks\nunder noisy conditions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 16:44:16 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Dreesen", "Philippe", ""], ["Westwick", "David", ""], ["Schoukens", "Johan", ""], ["Ishteva", "Mariya", ""]]}, {"id": "1609.08251", "submitter": "Anil Damle", "authors": "Anil Damle, Victor Minden and Lexing Ying", "title": "Robust and efficient multi-way spectral clustering", "comments": "23 pages, 4 figures; slight algorithm update, does not change\n  existing results; new real data example; expanded theory section with\n  connections to relevant models", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for spectral clustering based on a column-pivoted\nQR factorization that may be directly used for cluster assignment or to provide\nan initial guess for k-means. Our algorithm is simple to implement, direct, and\nrequires no initial guess. Furthermore, it scales linearly in the number of\nnodes of the graph and a randomized variant provides significant computational\ngains. Provided the subspace spanned by the eigenvectors used for clustering\ncontains a basis that resembles the set of indicator vectors on the clusters,\nwe prove that both our deterministic and randomized algorithms recover a basis\nclose to the indicators in Frobenius norm. We also experimentally demonstrate\nthat the performance of our algorithm tracks recent information theoretic\nbounds for exact recovery in the stochastic block model. Finally, we explore\nthe performance of our algorithm when applied to a real world graph.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 03:56:23 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 06:56:06 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Damle", "Anil", ""], ["Minden", "Victor", ""], ["Ying", "Lexing", ""]]}, {"id": "1609.08438", "submitter": "Raz Z. Nossek", "authors": "Raz Z. Nossek and Guy Gilboa", "title": "Flows Generating Nonlinear Eigenfunctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear variational methods have become very powerful tools for many image\nprocessing tasks. Recently a new line of research has emerged, dealing with\nnonlinear eigenfunctions induced by convex functionals. This has provided new\ninsights and better theoretical understanding of convex regularization and\nintroduced new processing methods. However, the theory of nonlinear eigenvalue\nproblems is still at its infancy. We present a new flow that can generate\nnonlinear eigenfunctions of the form $T(u)=\\lambda u$, where $T(u)$ is a\nnonlinear operator and $\\lambda \\in \\mathbb{R} $ is the eigenvalue. We develop\nthe theory where $T(u)$ is a subgradient element of a regularizing\none-homogeneous functional, such as total-variation (TV) or\ntotal-generalized-variation (TGV). We introduce two flows: a forward flow and\nan inverse flow; for which the steady state solution is a nonlinear\neigenfunction. The forward flow monotonically smooths the solution (with\nrespect to the regularizer) and simultaneously increases the $L^2$ norm. The\ninverse flow has the opposite characteristics. For both flows, the steady state\ndepends on the initial condition, thus different initial conditions yield\ndifferent eigenfunctions. This enables a deeper investigation into the space of\nnonlinear eigenfunctions, allowing to produce numerically diverse examples,\nwhich may be unknown yet. In addition we suggest an indicator to measure the\naffinity of a function to an eigenfunction and relate it to\npseudo-eigenfunctions in the linear case.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:42:13 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Nossek", "Raz Z.", ""], ["Gilboa", "Guy", ""]]}, {"id": "1609.09154", "submitter": "Grey Ballard", "authors": "Ramakrishnan Kannan, Grey Ballard, Haesun Park", "title": "MPI-FAUN: An MPI-Based Framework for Alternating-Updating Nonnegative\n  Matrix Factorization", "comments": "arXiv admin note: text overlap with arXiv:1509.09313", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is the problem of determining two\nnon-negative low rank factors $W$ and $H$, for the given input matrix $A$, such\nthat $A \\approx W H$. NMF is a useful tool for many applications in different\ndomains such as topic modeling in text mining, background separation in video\nanalysis, and community detection in social networks. Despite its popularity in\nthe data mining community, there is a lack of efficient parallel algorithms to\nsolve the problem for big data sets.\n  The main contribution of this work is a new, high-performance parallel\ncomputational framework for a broad class of NMF algorithms that iteratively\nsolves alternating non-negative least squares (NLS) subproblems for $W$ and\n$H$. It maintains the data and factor matrices in memory (distributed across\nprocessors), uses MPI for interprocessor communication, and, in the dense case,\nprovably minimizes communication costs (under mild assumptions). The framework\nis flexible and able to leverage a variety of NMF and NLS algorithms, including\nMultiplicative Update, Hierarchical Alternating Least Squares, and Block\nPrincipal Pivoting. Our implementation allows us to benchmark and compare\ndifferent algorithms on massive dense and sparse data matrices of size that\nspans for few hundreds of millions to billions. We demonstrate the scalability\nof our algorithm and compare it with baseline implementations, showing\nsignificant performance improvements. The code and the datasets used for\nconducting the experiments are available online.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 23:31:45 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Kannan", "Ramakrishnan", ""], ["Ballard", "Grey", ""], ["Park", "Haesun", ""]]}, {"id": "1609.09230", "submitter": "Anh Huy Phan", "authors": "Anh-Huy Phan, Andrzej Cichocki, Andre Uschmajew, Petr Tichavsky,\n  George Luta and Danilo Mandic", "title": "Tensor Networks for Latent Variable Analysis. Part I: Algorithms for\n  Tensor Train Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decompositions of tensors into factor matrices, which interact through a core\ntensor, have found numerous applications in signal processing and machine\nlearning. A more general tensor model which represents data as an ordered\nnetwork of sub-tensors of order-2 or order-3 has, so far, not been widely\nconsidered in these fields, although this so-called tensor network\ndecomposition has been long studied in quantum physics and scientific\ncomputing. In this study, we present novel algorithms and applications of\ntensor network decompositions, with a particular focus on the tensor train\ndecomposition and its variants. The novel algorithms developed for the tensor\ntrain decomposition update, in an alternating way, one or several core tensors\nat each iteration, and exhibit enhanced mathematical tractability and\nscalability to exceedingly large-scale data tensors. The proposed algorithms\nare tested in classic paradigms of blind source separation from a single\nmixture, denoising, and feature extraction, and achieve superior performance\nover the widely used truncated algorithms for tensor train decomposition.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 07:22:21 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Phan", "Anh-Huy", ""], ["Cichocki", "Andrzej", ""], ["Uschmajew", "Andre", ""], ["Tichavsky", "Petr", ""], ["Luta", "George", ""], ["Mandic", "Danilo", ""]]}, {"id": "1609.09841", "submitter": "Arturo Vargas", "authors": "Arturo Vargas, Jesse Chan, Thomas Hagstrom, and Timothy Warburton", "title": "GPU Acceleration of Hermite Methods for the Simulation of Wave\n  Propagation", "comments": "12 pages. Submitted to ICOSAHOM 2016 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hermite methods of Goodrich, Hagstrom, and Lorenz (2006) use Hermite\ninterpolation to construct high order numerical methods for hyperbolic initial\nvalue problems. The structure of the method has several favorable features for\nparallel computing. In this work, we propose algorithms that take advantage of\nthe many-core architecture of Graphics Processing Units. The algorithm exploits\nthe compact stencil of Hermite methods and uses data structures that allow for\nefficient data load and stores. Additionally the highly localized evolution\noperator of Hermite methods allows us to combine multi-stage time-stepping\nmethods within the new algorithms incurring minimal accesses of global memory.\nUsing a scalar linear wave equation, we study the algorithm by considering\nHermite interpolation and evolution as individual kernels and alternatively\ncombined them into a monolithic kernel. For both approaches we demonstrate\nstrategies to increase performance. Our numerical experiments show that\nalthough a two kernel approach allows for better performance on the hardware, a\nmonolithic kernel can offer a comparable time to solution with less global\nmemory usage.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 18:02:54 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Vargas", "Arturo", ""], ["Chan", "Jesse", ""], ["Hagstrom", "Thomas", ""], ["Warburton", "Timothy", ""]]}]