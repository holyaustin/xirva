[{"id": "1004.0202", "submitter": "Alexandre Chapoutot", "authors": "Alexandre Chapoutot (LIP6)", "title": "Interval Slopes as Numerical Abstract Domain for Floating-Point\n  Variables", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-15769-1_12", "report-no": null, "categories": "cs.PL cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of embedded control systems is mainly done with model-based tools\nsuch as Matlab/Simulink. Numerical simulation is the central technique of\ndevelopment and verification of such tools. Floating-point arithmetic, that is\nwell-known to only provide approximated results, is omnipresent in this\nactivity. In order to validate the behaviors of numerical simulations using\nabstract interpretation-based static analysis, we present, theoretically and\nwith experiments, a new partially relational abstract domain dedicated to\nfloating-point variables. It comes from interval expansion of non-linear\nfunctions using slopes and it is able to mimic all the behaviors of the\nfloating-point arithmetic. Hence it is adapted to prove the absence of run-time\nerrors or to analyze the numerical precision of embedded control systems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2010 18:43:10 GMT"}, {"version": "v2", "created": "Mon, 31 May 2010 09:12:36 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2010 09:17:40 GMT"}, {"version": "v4", "created": "Fri, 18 Jun 2010 07:50:43 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Chapoutot", "Alexandre", "", "LIP6"]]}, {"id": "1004.1220", "submitter": "Ilya Safro", "authors": "Dorit Ron and Ilya Safro and Achi Brandt", "title": "Relaxation-based coarsening and multiscale graph organization", "comments": null, "journal-ref": null, "doi": null, "report-no": "ANL/MCS-P1696-1009", "categories": "cs.DS cs.NA", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper we generalize and improve the multiscale organization of graphs\nby introducing a new measure that quantifies the \"closeness\" between two nodes.\nThe calculation of the measure is linear in the number of edges in the graph\nand involves just a small number of relaxation sweeps. A similar notion of\ndistance is then calculated and used at each coarser level. We demonstrate the\nuse of this measure in multiscale methods for several important combinatorial\noptimization problems and discuss the multiscale graph organization.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 02:05:02 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Ron", "Dorit", ""], ["Safro", "Ilya", ""], ["Brandt", "Achi", ""]]}, {"id": "1004.1253", "submitter": "Ravindran Kannan", "authors": "Ravindran Kannan", "title": "Spectral Methods for Matrices and Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Spectral Methods have long been used for Principal Component Analysis,\nthis survey focusses on work over the last 15 years with three salient\nfeatures: (i) Spectral methods are useful not only for numerical problems, but\nalso discrete optimization problems (Constraint Optimization Problems - CSP's)\nlike the max. cut problem and similar mathematical considerations underlie both\nareas. (ii) Spectral methods can be extended to tensors. The theory and\nalgorithms for tensors are not as simple/clean as for matrices, but the survey\ndescribes methods for low-rank approximation which extend to tensors. These\ntensor approximations help us solve Max-$r$-CSP's for $r>2$ as well as\nnumerical tensor problems. (iii) Sampling on the fly plays a prominent role in\nthese methods. A primary result is that for any matrix, a random submatrix of\nrows/columns picked with probabilities proportional to the squared lengths (of\nrows/columns), yields estimates of the singular values as well as an\napproximation to the whole matrix.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 06:36:48 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Kannan", "Ravindran", ""]]}, {"id": "1004.1986", "submitter": "Dmitry Savostyanov V.", "authors": "S. A. Goreinov, I. V. Oseledets and D. V. Savostyanov", "title": "Wedderburn rank reduction and Krylov subspace method for tensor\n  approximation. Part 1: Tucker case", "comments": "34 pages, 3 tables, 5 figures. Submitted to SIAM J. Scientific\n  Computing", "journal-ref": "SIAM J. Sci Comp, V 34(1), pp. A1-A27, 2012", "doi": "10.1137/100792056", "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New algorithms are proposed for the Tucker approximation of a 3-tensor, that\naccess it using only the tensor-by-vector-by-vector multiplication subroutine.\nIn the matrix case, Krylov methods are methods of choice to approximate the\ndominant column and row subspaces of a sparse or structured matrix given\nthrough the matrix-by-vector multiplication subroutine. Using the Wedderburn\nrank reduction formula, we propose an algorithm of matrix approximation that\ncomputes Krylov subspaces and allows generalization to the tensor case. Several\nvariants of proposed tensor algorithms differ by pivoting strategies, overall\ncost and quality of approximation. By convincing numerical experiments we show\nthat the proposed methods are faster and more accurate than the minimal Krylov\nrecursion, proposed recently by Elden and Savas.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 15:07:43 GMT"}, {"version": "v2", "created": "Tue, 19 Oct 2010 13:14:41 GMT"}], "update_date": "2012-02-06", "authors_parsed": [["Goreinov", "S. A.", ""], ["Oseledets", "I. V.", ""], ["Savostyanov", "D. V.", ""]]}, {"id": "1004.3374", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "On the precision attainable with various floating-point number systems", "comments": "Corrected version of an old paper (predating the IEEE floating point\n  standard). For details see http://wwwmaths.anu.edu.au/~brent/pub/pub017.html", "journal-ref": "IEEE Transactions on Computers C-22 (1973), 601-607", "doi": null, "report-no": "Report TR RC 3751, IBM Research, Yorktown Heights, New York\n  (February 1972), 28 pages", "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For scientific computations on a digital computer the set of real number is\nusually approximated by a finite set F of \"floating-point\" numbers. We compare\nthe numerical accuracy possible with difference choices of F having\napproximately the same range and requiring the same word length. In particular,\nwe compare different choices of base (or radix) in the usual floating-point\nsystems. The emphasis is on the choice of F, not on the details of the number\nrepresentation or the arithmetic, but both rounded and truncated arithmetic are\nconsidered. Theoretical results are given, and some simulations of typical\nfloating-point computations (forming sums, solving systems of linear equations,\nfinding eigenvalues) are described. If the leading fraction bit of a normalized\nbase 2 number is not stored explicitly (saving a bit), and the criterion is to\nminimize the mean square roundoff error, then base 2 is best. If unnormalized\nnumbers are allowed, so the first bit must be stored explicitly, then base 4\n(or sometimes base 8) is the best of the usual systems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 08:17:24 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3412", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Multiple-precision zero-finding methods and the complexity of elementary\n  function evaluation", "comments": "An old (1975) paper with a postscript describing more recent\n  developments. See also http://wwwmaths.anu.edu.au/~brent/pub/pub028.html", "journal-ref": "Analytic Computational Complexity (edited by J. F. Traub),\n  Academic Press, New York, 1975, 151-176", "doi": null, "report-no": "Interim Report ADA014059, Department of Computer Science,\n  Carnegie-Mellon University (July 1975), ii+26 pages", "categories": "cs.NA cs.CC cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider methods for finding high-precision approximations to simple zeros\nof smooth functions. As an application, we give fast methods for evaluating the\nelementary functions log(x), exp(x), sin(x) etc. to high precision. For\nexample, if x is a positive floating-point number with an n-bit fraction, then\n(under rather weak assumptions) an n-bit approximation to log(x) or exp(x) may\nbe computed in time asymptotically equal to 13M(n)lg(n), where M(n) is the time\nrequired to multiply floating-point numbers with n-bit fractions. Similar\nresults are given for the other elementary functions. Some analogies with\noperations on formal power series (over a field of characteristic zero) are\ndiscussed. In particular, it is possible to compute the first n terms in log(1\n+ a_1.x + ...) or exp(a_1.x + ...) in time O(M(n)), where M(n) is the time\nrequired to multiply two polynomials of degree n - 1. It follows that the first\nn terms in a q-th power (1 + a_1.x + ...)^q can be computed in time O(M(n)),\nindependent of q. One of the results of this paper is the \"Gauss-Legendre\" or\n\"Brent-Salamin\" algorithm for computing pi. This is the first quadratically\nconvergent algorithm for pi. It was also published in Brent [J. ACM 23 (1976),\n242-251], and independently by Salamin [Math. Comp. 30 (1976), 565-570].\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 11:27:07 GMT"}, {"version": "v2", "created": "Sun, 30 May 2010 02:59:53 GMT"}], "update_date": "2010-06-01", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3486", "submitter": "Sheng-Gwo Chen", "authors": "Jyh-Yang Wu, Mei-Hsiu Chi and Sheng-Gwo Chen", "title": "Convergent discrete Laplace-Beltrami operators over surfaces", "comments": "13 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convergence problem of the Laplace-Beltrami operators plays an essential\nrole in the convergence analysis of the numerical simulations of some important\ngeometric partial differential equations which involve the operator. In this\nnote we present a new effective and convergent algorithm to compute discrete\nLaplace-Beltrami operators acting on functions over surfaces. We prove a\nconvergence theorem for our discretization. To our knowledge, this is the first\nconvergent algorithm of discrete Laplace-Beltrami operators over surfaces for\nfunctions on general surfaces. Our algorithm is conceptually simple and easy to\ncompute. Indeed, the convergence rate of our new algorithm of discrete\nLaplace-Beltrami operators over surfaces is $O(r)$ where r represents the size\nof the mesh of discretization of the surface.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 15:51:24 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Wu", "Jyh-Yang", ""], ["Chi", "Mei-Hsiu", ""], ["Chen", "Sheng-Gwo", ""]]}, {"id": "1004.3608", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "The complexity of multiple-precision arithmetic", "comments": "An old (1976) paper with a postscript (1999) describing more recent\n  developments. 30 pages. For further details, see\n  http://wwwmaths.anu.edu.au/~brent/pub/pub032.html. Typos corrected in v2", "journal-ref": "The Complexity of Computational Problem Solving (edited by R. S.\n  Anderssen and R. P. Brent), University of Queensland Press, Brisbane, 1976,\n  126-165", "doi": null, "report-no": null, "categories": "cs.CC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In studying the complexity of iterative processes it is usually assumed that\nthe arithmetic operations of addition, multiplication, and division can be\nperformed in certain constant times. This assumption is invalid if the\nprecision required increases as the computation proceeds. We give upper and\nlower bounds on the number of single-precision operations required to perform\nvarious multiple-precision operations, and deduce some interesting consequences\nconcerning the relative efficiencies of methods for solving nonlinear equations\nusing variable-length multiple-precision arithmetic. A postscript describes\nmore recent developments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 02:15:20 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 10:45:24 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3621", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Unrestricted algorithms for elementary and special functions", "comments": "Corrected and updated version of a paper first published in 1980. 13\n  pages. For further details see\n  http://wwwmaths.anu.edu.au/~brent/pub/pub052.html", "journal-ref": "Information Processing 80 (edited by S. H. Lavington),\n  North-Holland, Amsterdam, 1980, 613-619", "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe some \"unrestricted\" algorithms which are useful for the\ncomputation of elementary and special functions when the precision required is\nnot known in advance. Several general classes of algorithms are identified and\nillustrated by examples. The topics include: power series methods, use of\nhalving identities, asymptotic expansions, continued fractions, recurrence\nrelations, Newton's method, numerical contour integration, and the\narithmetic-geometric mean. Most of the algorithms discussed are implemented in\nthe MP package (arXiv:1004.3173).\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 05:31:42 GMT"}], "update_date": "2010-04-22", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.4329", "submitter": "Joseph Shtok", "authors": "Joseph Shtok and Michael Elad", "title": "Analysis of Basis Pursuit Via Capacity Sets", "comments": null, "journal-ref": "Journal of Fourier Analysis and Applications, Volume 14, Numbers\n  5-6, December 2008, pp. 688-711", "doi": "10.1007/s00041-008-9036-y", "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the sparsest solution $\\alpha$ for an under-determined linear system\nof equations $D\\alpha=s$ is of interest in many applications. This problem is\nknown to be NP-hard. Recent work studied conditions on the support size of\n$\\alpha$ that allow its recovery using L1-minimization, via the Basis Pursuit\nalgorithm. These conditions are often relying on a scalar property of $D$\ncalled the mutual-coherence. In this work we introduce an alternative set of\nfeatures of an arbitrarily given $D$, called the \"capacity sets\". We show how\nthose could be used to analyze the performance of the basis pursuit, leading to\nimproved bounds and predictions of performance. Both theoretical and numerical\nmethods are presented, all using the capacity values, and shown to lead to\nimproved assessments of the basis pursuit success in finding the sparest\nsolution of $D\\alpha=s$.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2010 06:28:58 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Shtok", "Joseph", ""], ["Elad", "Michael", ""]]}, {"id": "1004.4622", "submitter": "Akitoshi Kawamura", "authors": "Akitoshi Kawamura", "title": "Lipschitz Continuous Ordinary Differential Equations are\n  Polynomial-Space Complete", "comments": "22 pages, 9 figures; preliminary version presented at CCC 2009", "journal-ref": "Computational Complexity 19(2):305-332, May 2010", "doi": "10.1007/s00037-010-0286-0", "report-no": null, "categories": "cs.CC cs.NA math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In answer to Ko's question raised in 1983, we show that an initial value\nproblem given by a polynomial-time computable, Lipschitz continuous function\ncan have a polynomial-space complete solution. The key insight is simple: the\nLipschitz condition means that the feedback in the differential equation is\nweak. We define a class of polynomial-space computation tableaux with equally\nweak feedback, and show that they are still polynomial-space complete. The same\ntechnique also settles Ko's two later questions on Volterra integral equations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 19:52:25 GMT"}], "update_date": "2010-07-19", "authors_parsed": [["Kawamura", "Akitoshi", ""]]}, {"id": "1004.4710", "submitter": "Richard Brent", "authors": "Richard P. Brent and Paul Zimmermann", "title": "Modern Computer Arithmetic (version 0.5.1)", "comments": "Preliminary version of a book to be published by Cambridge University\n  Press. xvi+247 pages. Cite as \"Modern Computer Arithmetic, Version 0.5.1, 5\n  March 2010\". For further details, updates and errata see\n  http://wwwmaths.anu.edu.au/~brent/pub/pub226.html or\n  http://www.loria.fr/~zimmerma/mca/pub226.html", "journal-ref": "Cambridge Monographs on Computational and Applied Mathematics (No.\n  18), Cambridge University Press, November 2010, 236 pages", "doi": "10.1017/cbo9780511921698.001", "report-no": null, "categories": "cs.DS cs.NA math.NA math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a draft of a book about algorithms for performing arithmetic, and\ntheir implementation on modern computers. We are concerned with software more\nthan hardware - we do not cover computer architecture or the design of computer\nhardware. Instead we focus on algorithms for efficiently performing arithmetic\noperations such as addition, multiplication and division, and their connections\nto topics such as modular arithmetic, greatest common divisors, the Fast\nFourier Transform (FFT), and the computation of elementary and special\nfunctions. The algorithms that we present are mainly intended for\narbitrary-precision arithmetic. They are not limited by the computer word size,\nonly by the memory and time available for the computation. We consider both\ninteger and real (floating-point) computations. The book is divided into four\nmain chapters, plus an appendix. Our aim is to present the latest developments\nin a concise manner. At the same time, we provide a self-contained introduction\nfor the reader who is not an expert in the field, and exercises at the end of\neach chapter. Chapter titles are: 1, Integer Arithmetic; 2, Modular Arithmetic\nand the FFT; 3, Floating-Point Arithmetic; 4, Elementary and Special Function\nEvaluation; 5 (Appendix), Implementations and Pointers. The book also contains\na bibliography of 236 entries, index, summary of notation, and summary of\ncomplexities.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 04:42:33 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Brent", "Richard P.", ""], ["Zimmermann", "Paul", ""]]}, {"id": "1004.4769", "submitter": "Petr Vabishchevich N.", "authors": "Nikolay P. Vabishchevich and Petr N. Vabishchevich", "title": "VAGO method for the solution of elliptic second-order boundary value\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical physics problems are often formulated using differential\noprators of vector analysis - invariant operators of first order, namely,\ndivergence, gradient and rotor operators. In approximate solution of such\nproblems it is natural to employ similar operator formulations for grid\nproblems, too. The VAGO (Vector Analysis Grid Operators) method is based on\nsuch a methodology. In this paper the vector analysis difference operators are\nconstructed using the Delaunay triangulation and the Voronoi diagrams. Further\nthe VAGO method is used to solve approximately boundary value problems for the\ngeneral elliptic equation of second order. In the convection-diffusion-reaction\nequation the diffusion coefficient is a symmetric tensor of second order.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 11:55:56 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Vabishchevich", "Nikolay P.", ""], ["Vabishchevich", "Petr N.", ""]]}, {"id": "1004.5128", "submitter": "Gabriel Silva", "authors": "Brian P. Sprouse, Christopher L. MacDonald, Gabriel A. Silva", "title": "Computational efficiency of fractional diffusion using adaptive time\n  step memory", "comments": "6 pages and 5 figures. Submitted to 4th IFAC Workshop on Fractional\n  Differentiation and Its Applications.", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph cs.NA math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical solutions to fractional differential equations can be extremely\ncomputationally intensive due to the effect of non-local derivatives in which\nall previous time points contribute to the current iteration. In finite\ndifference methods this has been approximated using the 'short memory effect'\nwhere it is assumed that previous events prior to some certain time point are\ninsignificant and thus not calculated. Here we present an adaptive time method\nfor smooth functions that is computationally efficient and results in smaller\nerrors during numerical simulations. Sampled points along the system's history\nat progressively longer intervals are assumed to reflect the values of\nneighboring time points. By including progressively fewer points as a function\nof time, a temporally 'weighted' history is computed that includes\ncontributions from the entire past of the system, resulting in increased\naccuracy, but with fewer points actually calculated, which ensures\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2010 20:04:48 GMT"}], "update_date": "2010-04-30", "authors_parsed": [["Sprouse", "Brian P.", ""], ["MacDonald", "Christopher L.", ""], ["Silva", "Gabriel A.", ""]]}, {"id": "1004.5437", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Parallel algorithms in linear algebra", "comments": "17 pages. An old Technical Report, submitted for archival purposes.\n  For further details see http://wwwmaths.anu.edu.au/~brent/pub/pub128.html", "journal-ref": "Algorithms and Architectures: Proceedings of the Second NEC\n  Research Symposium (edited by T. Ishiguro), SIAM, Philadelphia, 1993, 54-72", "doi": null, "report-no": "Technical Report TR-CS-91-06, Computer Sciences Laboratory,\n  Australian National University, Canberra, August 1991, 17 pages", "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an introduction to algorithms for fundamental linear\nalgebra problems on various parallel computer architectures, with the emphasis\non distributed-memory MIMD machines. To illustrate the basic concepts and key\nissues, we consider the problem of parallel solution of a nonsingular linear\nsystem by Gaussian elimination with partial pivoting. This problem has come to\nbe regarded as a benchmark for the performance of parallel machines. We\nconsider its appropriateness as a benchmark, its communication requirements,\nand schemes for data distribution to facilitate communication and load\nbalancing. In addition, we describe some parallel algorithms for orthogonal\n(QR) factorization and the singular value decomposition (SVD).\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 02:30:36 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.5510", "submitter": "Richard Brent", "authors": "Adam W. Bojanczyk, Richard P. Brent, Frank R. de Hoog and Douglas R.\n  Sweet", "title": "On the stability of the Bareiss and related Toeplitz factorization\n  algorithms", "comments": "18 pages. An old Technical Report, submitted for archival purposes.\n  For further details, see http://wwwmaths.anu.edu.au/~brent/pub/pub144.html", "journal-ref": "SIAM J. Matrix Analysis and Applications 16 (1995), 40-57", "doi": "10.1137/S0895479891221563", "report-no": "Technical Report TR-CS-93-14, Computer Sciences Laboratory,\n  Australian National University, November 1993, 18 pages", "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report contains a numerical stability analysis of factorization\nalgorithms for computing the Cholesky decomposition of symmetric positive\ndefinite matrices of displacement rank 2. The algorithms in the class can be\nexpressed as sequences of elementary downdating steps. The stability of the\nfactorization algorithms follows directly from the numerical properties of\nalgorithms for realizing elementary downdating operations. It is shown that the\nBareiss algorithm for factorizing a symmetric positive definite Toeplitz matrix\nis in the class and hence the Bareiss algorithm is stable. Some numerical\nexperiments that compare behavior of the Bareiss algorithm and the Levinson\nalgorithm are presented. These experiments indicate that in general (when the\nreflection coefficients are not all positive) the Levinson algorithm is not\nstable; certainly it can give much larger residuals than the Bareiss algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 12:01:45 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Bojanczyk", "Adam W.", ""], ["Brent", "Richard P.", ""], ["de Hoog", "Frank R.", ""], ["Sweet", "Douglas R.", ""]]}]