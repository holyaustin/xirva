[{"id": "1509.00206", "submitter": "Roel Matthysen", "authors": "Roel Matthysen, Daan Huybrechs", "title": "Fast Algorithms for the computation of Fourier Extensions of arbitrary\n  length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier series of smooth, non-periodic functions on $[-1,1]$ are known to\nexhibit the Gibbs phenomenon, and exhibit overall slow convergence. One way of\novercoming these problems is by using a Fourier series on a larger domain, say\n$[-T,T]$ with $T>1$, a technique called Fourier extension or Fourier\ncontinuation. When constructed as the discrete least squares minimizer in\nequidistant points, the Fourier extension has been shown shown to converge\ngeometrically in the truncation parameter $N$. A fast ${\\mathcal O}(N \\log^2\nN)$ algorithm has been described to compute Fourier extensions for the case\nwhere $T=2$, compared to ${\\mathcal O}(N^3)$ for solving the dense discrete\nleast squares problem. We present two ${\\mathcal O}(N\\log^2 N )$ algorithms for\nthe computation of these approximations for the case of general $T$, made\npossible by exploiting the connection between Fourier extensions and Prolate\nSpheroidal Wave theory. The first algorithm is based on the explicit\ncomputation of so-called periodic discrete prolate spheroidal sequences, while\nthe second algorithm is purely algebraic and only implicitly based on the\ntheory.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 10:02:12 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Matthysen", "Roel", ""], ["Huybrechs", "Daan", ""]]}, {"id": "1509.00728", "submitter": "Florian Bernard", "authors": "Johan Thunberg, Florian Bernard, Jorge Goncalves", "title": "On Transitive Consistency for Linear Invertible Transformations between\n  Euclidean Coordinate Systems", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.MA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transitive consistency is an intrinsic property for collections of linear\ninvertible transformations between Euclidean coordinate frames. In practice,\nwhen the transformations are estimated from data, this property is lacking.\nThis work addresses the problem of synchronizing transformations that are not\ntransitively consistent. Once the transformations have been synchronized, they\nsatisfy the transitive consistency condition - a transformation from frame $A$\nto frame $C$ is equal to the composite transformation of first transforming A\nto B and then transforming B to C. The coordinate frames correspond to nodes in\na graph and the transformations correspond to edges in the same graph. Two\ndirect or centralized synchronization methods are presented for different graph\ntopologies; the first one for quasi-strongly connected graphs, and the second\none for connected graphs. As an extension of the second method, an iterative\nGauss-Newton method is presented, which is later adapted to the case of affine\nand Euclidean transformations. Two distributed synchronization methods are also\npresented for orthogonal matrices, which can be seen as distributed versions of\nthe two direct or centralized methods; they are similar in nature to standard\nconsensus protocols used for distributed averaging. When the transformations\nare orthogonal matrices, a bound on the optimality gap can be computed.\nSimulations show that the gap is almost right, even for noise large in\nmagnitude. This work also contributes on a theoretical level by providing\nlinear algebraic relationships for transitively consistent transformations. One\nof the benefits of the proposed methods is their simplicity - basic linear\nalgebraic methods are used, e.g., the Singular Value Decomposition (SVD). For a\nwide range of parameter settings, the methods are numerically validated.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 14:57:16 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Thunberg", "Johan", ""], ["Bernard", "Florian", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1509.00778", "submitter": "Hugo Jim\\'enez-P\\'erez", "authors": "Hugo Jim\\'enez-P\\'erez", "title": "Towards exact symplectic integrators from Liouvillian forms", "comments": "16 pages, 3 figures. Cosmetic modifications and some additional\n  clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SG cs.NA math-ph math.MP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we introduce a low order implicit symplectic integrator\ndesigned to follow the Hamiltonian flow as close as possible. This integrator\nis obtained by the method of Liouvillian forms and does not require particular\nhypotheses on the Hamiltonian.\n  The numerical scheme introduced in this paper is a modification of the\nsymplectic mid-point rule, it is symmetric and it is obtained by an isotopy of\nthe deformation of the exact Hamiltonian flow to the straight line passing by\ntwo consecutive points of the discretized flow. This isotopy generates an\nalternative vector field on the flow lines transversal to the Hamiltonian\nvector field. We consider only the line arising from the mid-point to construct\nthe symplectic integrator.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 16:43:11 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 14:32:16 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2015 13:51:48 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 23:08:02 GMT"}, {"version": "v5", "created": "Mon, 2 Nov 2020 20:51:36 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Jim\u00e9nez-P\u00e9rez", "Hugo", ""]]}, {"id": "1509.00888", "submitter": "Albert Fannjiang", "authors": "Pengwen Chen and Albert Fannjiang", "title": "Fourier Phase Retrieval with a Single Mask by Douglas-Rachford Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.data-an physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Douglas-Rachford (DR) algorithm is analyzed for Fourier phase retrieval with\na single random phase mask. Local, geometric convergence to a unique fixed\npoint is proved with numerical demonstration of global convergence.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 22:04:39 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Chen", "Pengwen", ""], ["Fannjiang", "Albert", ""]]}, {"id": "1509.01208", "submitter": "Da Kuang", "authors": "Da Kuang, Barry Drake, Haesun Park", "title": "Fast Clustering and Topic Modeling Based on Rank-2 Nonnegative Matrix\n  Factorization", "comments": "This paper has been withdrawn by the author to clarify the authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of unsupervised clustering and topic modeling is well\nrecognized with ever-increasing volumes of text data. In this paper, we propose\na fast method for hierarchical clustering and topic modeling called HierNMF2.\nOur method is based on fast Rank-2 nonnegative matrix factorization (NMF) that\nperforms binary clustering and an efficient node splitting rule. Further\nutilizing the final leaf nodes generated in HierNMF2 and the idea of\nnonnegative least squares fitting, we propose a new clustering/topic modeling\nmethod called FlatNMF2 that recovers a flat clustering/topic modeling result in\na very simple yet significantly more effective way than any other existing\nmethods. We implement highly optimized open source software in C++ for both\nHierNMF2 and FlatNMF2 for hierarchical and partitional clustering/topic\nmodeling of document data sets.\n  Substantial experimental tests are presented that illustrate significant\nimprovements both in computational time as well as quality of solutions. We\ncompare our methods to other clustering methods including K-means, standard\nNMF, and CLUTO, and also topic modeling methods including latent Dirichlet\nallocation (LDA) and recently proposed algorithms for NMF with separability\nconstraints. Overall, we present efficient tools for analyzing large-scale data\nsets, and techniques that can be generalized to many other data analytics\nproblem domains.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 18:55:28 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 04:29:04 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2015 18:06:13 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Kuang", "Da", ""], ["Drake", "Barry", ""], ["Park", "Haesun", ""]]}, {"id": "1509.01347", "submitter": "Pablo De Oliveira Castro", "authors": "Christophe Denis (CMLA), Pablo De Oliveira Castro (LI-PaRAD, UVSQ),\n  Eric Petit (UVSQ)", "title": "Verificarlo: checking floating point accuracy through Monte Carlo\n  Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical accuracy of floating point computation is a well studied topic\nwhich has not made its way to the end-user in scientific computing. Yet, it has\nbecome a critical issue with the recent requirements for code modernization to\nharness new highly parallel hardware and perform higher resolution computation.\nTo democratize numerical accuracy analysis, it is important to propose tools\nand methodologies to study large use cases in a reliable and automatic way. In\nthis paper, we propose verificarlo, an extension to the LLVM compiler to\nautomatically use Monte Carlo Arithmetic in a transparent way for the end-user.\nIt supports all the major languages including C, C++, and Fortran. Unlike\nsource-to-source approaches, our implementation captures the influence of\ncompiler optimizations on the numerical accuracy. We illustrate how Monte Carlo\nArithmetic using the verificarlo tool outperforms the existing approaches on\nvarious use cases and is a step toward automatic numerical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 06:20:18 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 09:46:12 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2015 12:53:31 GMT"}, {"version": "v4", "created": "Fri, 9 Nov 2018 07:55:49 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Denis", "Christophe", "", "CMLA"], ["Castro", "Pablo De Oliveira", "", "LI-PaRAD, UVSQ"], ["Petit", "Eric", "", "UVSQ"]]}, {"id": "1509.01404", "submitter": "Nicolas Gillis", "authors": "Arnaud Vandaele, Nicolas Gillis, Qi Lei, Kai Zhong, Inderjit Dhillon", "title": "Coordinate Descent Methods for Symmetric Nonnegative Matrix\n  Factorization", "comments": "25 pages, 5 figures, 7 tables. Main changes: comparison with another\n  symNMF algorithm (namely, BetaSNMF), and correction of an error in the\n  convergence proof", "journal-ref": "IEEE Transactions on Signal Processing 64 (21), pp. 5571-5584,\n  2016", "doi": "10.1109/TSP.2016.2591510", "report-no": null, "categories": "cs.NA cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix\nfactorization (symNMF) is the problem of finding a nonnegative matrix $H$,\nusually with much fewer columns than $A$, such that $A \\approx HH^T$. SymNMF\ncan be used for data analysis and in particular for various clustering tasks.\nIn this paper, we propose simple and very efficient coordinate descent schemes\nto solve this problem, and that can handle large and sparse input matrices. The\neffectiveness of our methods is illustrated on synthetic and real-world data\nsets, and we show that they perform favorably compared to recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 11:19:35 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 12:50:38 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Vandaele", "Arnaud", ""], ["Gillis", "Nicolas", ""], ["Lei", "Qi", ""], ["Zhong", "Kai", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1509.02157", "submitter": "Yao Yang", "authors": "Yao Yang", "title": "Detecting Potential Instabilities of Numerical Algorithms", "comments": "This paper has a different perspectives about stability analysis\n  axioms (forward stability, backward stability and mixed stability axioms as\n  in Demmel, Kahan and Parlett's papers and teaching at Berkeley for numerical\n  algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been the standard teaching of today that backward stability analysis\nis taught as absolute, just as in Newtonian physics time is taught absolute\ntime. We will prove it is not true in general. It depends on algorithms. We\nwill prove that forward and mixed stability anlaysis are absolutely invalid\nstability analysis in the sense that they have absolutely wrong reference\npoints for detecting huge element growth of any algoritms(if any), even an\n\"ideal\" or \"desirable\" backward stability analysis is not so \"ideal\" or\n\"desirable\" in general. Any of forward stable, backward stable and mixed stable\nalgorihms as in Demmel, Kahan , Parlett and other's papers and text books, see\nDemmel(6) and Higham(8)may not be really stable at all because they may fail to\ndetect and expose any potential instabilities of the algorithm in corresponding\nstability analysis. Therefore, it is impossible to prove an algorithm is stable\naccording to the standard teachin of today, just as it is impossible to prove a\nmathematical equuation(Maxwell's) is a law of physics according to the standard\nteaching in Newtonian physics.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 09:45:34 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Yang", "Yao", ""]]}, {"id": "1509.02223", "submitter": "Tuomo Valkonen", "authors": "Artur Gorokh, Yury Korolev, Tuomo Valkonen", "title": "Diffusion tensor imaging with deterministic error bounds", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-016-0639-7", "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Errors in the data and the forward operator of an inverse problem can be\nhandily modelled using partial order in Banach lattices. We present some\nexisting results of the theory of regularisation in this novel framework, where\nerrors are represented as bounds by means of the appropriate partial order.\n  We apply the theory to Diffusion Tensor Imaging, where correct noise\nmodelling is challenging: it involves the Rician distribution and the nonlinear\nStejskal-Tanner equation. Linearisation of the latter in the statistical\nframework would complicate the noise model even further. We avoid this using\nthe error bounds approach, which preserves simple error structure under\nmonotone transformations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 23:15:51 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 10:13:27 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Gorokh", "Artur", ""], ["Korolev", "Yury", ""], ["Valkonen", "Tuomo", ""]]}, {"id": "1509.02314", "submitter": "Shenjian Zhao", "authors": "Shenjian Zhao, Cong Xie, Zhihua Zhang", "title": "A Scalable and Extensible Framework for Superposition-Structured Models", "comments": null, "journal-ref": "AAAI 2016: 2372-2378", "doi": null, "report-no": null, "categories": "cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many learning tasks, structural models usually lead to better\ninterpretability and higher generalization performance. In recent years,\nhowever, the simple structural models such as lasso are frequently proved to be\ninsufficient. Accordingly, there has been a lot of work on\n\"superposition-structured\" models where multiple structural constraints are\nimposed. To efficiently solve these \"superposition-structured\" statistical\nmodels, we develop a framework based on a proximal Newton-type method.\nEmploying the smoothed conic dual approach with the LBFGS updating formula, we\npropose a scalable and extensible proximal quasi-Newton (SEP-QN) framework.\nEmpirical analysis on various datasets shows that our framework is potentially\npowerful, and achieves super-linear convergence rate for optimizing some\npopular \"superposition-structured\" statistical models such as the fused sparse\ngroup lasso.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 10:33:27 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 04:29:43 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Zhao", "Shenjian", ""], ["Xie", "Cong", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1509.02465", "submitter": "Akshay Gadde", "authors": "Akshay Gadde, Andrew Knyazev, Dong Tian, Hassan Mansour", "title": "Guided Signal Reconstruction with Application to Image Magnification", "comments": "5 pages, 6 figures; Accepted to IEEE GlobalSIP 2015", "journal-ref": "2015 IEEE Global Conference on Signal and Information Processing\n  (GlobalSIP), Orlando, FL, 14-16 Dec.2015, pp. 938 - 942", "doi": "10.1109/GlobalSIP.2015.7418335", "report-no": "MERL-TR2015-141", "categories": "cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of reconstructing a signal from its projection on a\nsubspace. The proposed signal reconstruction algorithms utilize a guiding\nsubspace that represents desired properties of reconstructed signals. We show\nthat optimal reconstructed signals belong to a convex bounded set, called the\n\"reconstruction\" set. We also develop iterative algorithms, based on conjugate\ngradient methods, to approximate optimal reconstructions with low memory and\ncomputational costs. The effectiveness of the proposed approach is demonstrated\nfor image magnification, where the reconstructed image quality is shown to\nexceed that of consistent and generalized reconstruction schemes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 17:48:06 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Gadde", "Akshay", ""], ["Knyazev", "Andrew", ""], ["Tian", "Dong", ""], ["Mansour", "Hassan", ""]]}, {"id": "1509.03590", "submitter": "Yaroslav Sergeyev", "authors": "Daniela Lera and Yaroslav D. Sergeyev", "title": "Deterministic global optimization using space-filling curves and\n  multiple estimates of Lipschitz and Holder constants", "comments": "26 pages, 10 figures, 4 tables", "journal-ref": "Communications in Nonlinear Science and Numerical Simulation,\n  Volume 23, Issues 1-3, 2015, Pages 328-342", "doi": "10.1016/j.cnsns.2014.11.015", "report-no": null, "categories": "math.OC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the global optimization problem $\\min_{y\\in S} F(y)$ with $S$\nbeing a hyperinterval in $\\Re^N$ and $F(y)$ satisfying the Lipschitz condition\nwith an unknown Lipschitz constant is considered. It is supposed that the\nfunction $F(y)$ can be multiextremal, non-differentiable, and given as a\n`black-box'. To attack the problem, a new global optimization algorithm based\non the following two ideas is proposed and studied both theoretically and\nnumerically. First, the new algorithm uses numerical approximations to\nspace-filling curves to reduce the original Lipschitz multi-dimensional problem\nto a univariate one satisfying the H\\\"{o}lder condition. Second, the algorithm\nat each iteration applies a new geometric technique working with a number of\npossible H\\\"{o}lder constants chosen from a set of values varying from zero to\ninfinity showing so that ideas introduced in a popular DIRECT method can be\nused in the H\\\"{o}lder global optimization. Convergence conditions of the\nresulting deterministic global optimization method are established. Numerical\nexperiments carried out on several hundreds of test functions show quite a\npromising performance of the new algorithm in comparison with its direct\ncompetitors.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 17:44:38 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Lera", "Daniela", ""], ["Sergeyev", "Yaroslav D.", ""]]}, {"id": "1509.03917", "submitter": "Anastasios Kyrillidis", "authors": "Srinadh Bhojanapalli, Anastasios Kyrillidis, Sujay Sanghavi", "title": "Dropping Convexity for Faster Semi-definite Optimization", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimization of a convex function $f(X)$ over the set of\n$n\\times n$ positive semi-definite matrices, but when the problem is recast as\n$\\min_U g(U) := f(UU^\\top)$, with $U \\in \\mathbb{R}^{n \\times r}$ and $r \\leq\nn$. We study the performance of gradient descent on $g$---which we refer to as\nFactored Gradient Descent (FGD)---under standard assumptions on the original\nfunction $f$.\n  We provide a rule for selecting the step size and, with this choice, show\nthat the local convergence rate of FGD mirrors that of standard gradient\ndescent on the original $f$: i.e., after $k$ steps, the error is $O(1/k)$ for\nsmooth $f$, and exponentially small in $k$ when $f$ is (restricted) strongly\nconvex. In addition, we provide a procedure to initialize FGD for (restricted)\nstrongly convex objectives and when one only has access to $f$ via a\nfirst-order oracle; for several problem instances, such proper initialization\nleads to global convergence guarantees.\n  FGD and similar procedures are widely used in practice for problems that can\nbe posed as matrix factorization. To the best of our knowledge, this is the\nfirst paper to provide precise convergence rate guarantees for general convex\nfunctions under standard convex assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 00:40:11 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 01:45:02 GMT"}, {"version": "v3", "created": "Sat, 16 Apr 2016 03:17:46 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Kyrillidis", "Anastasios", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1509.03946", "submitter": "Yoshinobu Kawahara", "authors": "Yoshinobu Kawahara and Yutaro Yamaguchi", "title": "Parametric Maxflows for Structured Sparse Learning with Convex\n  Relaxations of Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proximal problem for structured penalties obtained via convex relaxations\nof submodular functions is known to be equivalent to minimizing separable\nconvex functions over the corresponding submodular polyhedra. In this paper, we\nreveal a comprehensive class of structured penalties for which penalties this\nproblem can be solved via an efficiently solvable class of parametric maxflow\noptimization. We then show that the parametric maxflow algorithm proposed by\nGallo et al. and its variants, which runs, in the worst-case, at the cost of\nonly a constant factor of a single computation of the corresponding maxflow\noptimization, can be adapted to solve the proximal problems for those\npenalties. Several existing structured penalties satisfy these conditions;\nthus, regularized learning with these penalties is solvable quickly using the\nparametric maxflow algorithm. We also investigate the empirical runtime\nperformance of the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 04:11:02 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Kawahara", "Yoshinobu", ""], ["Yamaguchi", "Yutaro", ""]]}, {"id": "1509.04252", "submitter": "Andreas Kreienbuehl", "authors": "Andreas Kreienbuehl and Arne Naegel and Daniel Ruprecht and Andreas\n  Vogel and Gabriel Wittum and Rolf Krause", "title": "Parareal convergence for 2D unsteady flow around a cylinder", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report we study the convergence of Parareal for 2D\nincompressible flow around a cylinder for different viscosities. Two methods\nare used as fine integrator: backward Euler and a fractional step method. It is\nfound that Parareal converges better for the implicit Euler, likely because it\nunder-resolves the fine-scale dynamics as a result of numerical diffusion.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 19:29:41 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Kreienbuehl", "Andreas", ""], ["Naegel", "Arne", ""], ["Ruprecht", "Daniel", ""], ["Vogel", "Andreas", ""], ["Wittum", "Gabriel", ""], ["Krause", "Rolf", ""]]}, {"id": "1509.04321", "submitter": "Marco Secondini", "authors": "Stella Civelli, Luigi Barletti, Marco Secondini", "title": "Numerical Methods for the Inverse Nonlinear Fourier Transform", "comments": "To be presented at the Tyrrhenian International Workshop on Digital\n  Communications (TIWDC) 2015", "journal-ref": "in Tyrrhenian International Workshop on Digital Communications\n  (TIWDC) 2015 , pp.13-16, 22 Sept. 2015", "doi": "10.1109/TIWDC.2015.7323325", "report-no": null, "categories": "math.NA cs.IT cs.NA math.IT physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new numerical method for the computation of the inverse\nnonlinear Fourier transform and compare its computational complexity and\naccuracy to those of other methods available in the literature. For a given\naccuracy, the proposed method requires the lowest number of operations\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 16:51:15 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Civelli", "Stella", ""], ["Barletti", "Luigi", ""], ["Secondini", "Marco", ""]]}, {"id": "1509.04706", "submitter": "Daniil Kazantsev", "authors": "Daniil Kazantsev, Evgueni Ovtchinnikov, William R. B. Lionheart,\n  Philip J. Withers, Peter D. Lee", "title": "Direct high-order edge-preserving regularization for tomographic image\n  reconstruction", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MS cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a new two-level iterative algorithm for tomographic\nimage reconstruction. The algorithm uses a regularization technique, which we\ncall edge-preserving Laplacian, that preserves sharp edges between objects\nwhile damping spurious oscillations in the areas where the reconstructed image\nis smooth. Our numerical simulations demonstrate that the proposed method\noutperforms total variation (TV) regularization and it is competitive with the\ncombined TV-L2 penalty. Obtained reconstructed images show increased\nsignal-to-noise ratio and visually appealing structural features. Computer\nimplementation and parameter control of the proposed technique is\nstraightforward, which increases the feasibility of it across many tomographic\napplications. In this paper, we applied our method to the under-sampled\ncomputed tomography (CT) projection data and also considered a case of\nreconstruction in emission tomography The MATLAB code is provided to support\nobtained results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 18:23:56 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Kazantsev", "Daniil", ""], ["Ovtchinnikov", "Evgueni", ""], ["Lionheart", "William R. B.", ""], ["Withers", "Philip J.", ""], ["Lee", "Peter D.", ""]]}, {"id": "1509.05009", "submitter": "Nadav Cohen", "authors": "Nadav Cohen, Or Sharir, Amnon Shashua", "title": "On the Expressive Power of Deep Learning: A Tensor Analysis", "comments": null, "journal-ref": "29th Annual Conference on Learning Theory, pp. 698-728, 2016", "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been conjectured that hypotheses spaces suitable for data that is\ncompositional in nature, such as text or images, may be more efficiently\nrepresented with deep hierarchical networks than with shallow ones. Despite the\nvast empirical evidence supporting this belief, theoretical justifications to\ndate are limited. In particular, they do not account for the locality, sharing\nand pooling constructs of convolutional networks, the most successful deep\nlearning architecture to date. In this work we derive a deep network\narchitecture based on arithmetic circuits that inherently employs locality,\nsharing and pooling. An equivalence between the networks and hierarchical\ntensor factorizations is established. We show that a shallow network\ncorresponds to CP (rank-1) decomposition, whereas a deep network corresponds to\nHierarchical Tucker decomposition. Using tools from measure theory and matrix\nalgebra, we prove that besides a negligible set, all functions that can be\nimplemented by a deep network of polynomial size, require exponential size in\norder to be realized (or even approximated) by a shallow network. Since\nlog-space computation transforms our networks into SimNets, the result applies\ndirectly to a deep learning architecture demonstrating promising empirical\nperformance. The construction and theory developed in this paper shed new light\non various practices and ideas employed by the deep learning community.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 19:32:54 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 16:31:49 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:07:22 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Cohen", "Nadav", ""], ["Sharir", "Or", ""], ["Shashua", "Amnon", ""]]}, {"id": "1509.05647", "submitter": "Dan Garber", "authors": "Dan Garber and Elad Hazan", "title": "Fast and Simple PCA via Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of principle component analysis (PCA) is traditionally solved by\nspectral or algebraic methods. We show how computing the leading principal\ncomponent could be reduced to solving a \\textit{small} number of\nwell-conditioned {\\it convex} optimization problems. This gives rise to a new\nefficient method for PCA based on recent advances in stochastic methods for\nconvex optimization.\n  In particular we show that given a $d\\times d$ matrix $\\X =\n\\frac{1}{n}\\sum_{i=1}^n\\x_i\\x_i^{\\top}$ with top eigenvector $\\u$ and top\neigenvalue $\\lambda_1$ it is possible to: \\begin{itemize} \\item compute a unit\nvector $\\w$ such that $(\\w^{\\top}\\u)^2 \\geq 1-\\epsilon$ in\n$\\tilde{O}\\left({\\frac{d}{\\delta^2}+N}\\right)$ time, where $\\delta = \\lambda_1\n- \\lambda_2$ and $N$ is the total number of non-zero entries in\n$\\x_1,...,\\x_n$,\n  \\item compute a unit vector $\\w$ such that $\\w^{\\top}\\X\\w \\geq\n\\lambda_1-\\epsilon$ in $\\tilde{O}(d/\\epsilon^2)$ time. \\end{itemize} To the\nbest of our knowledge, these bounds are the fastest to date for a wide regime\nof parameters. These results could be further accelerated when $\\delta$ (in the\nfirst case) and $\\epsilon$ (in the second case) are smaller than $\\sqrt{d/N}$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 15:03:03 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 12:50:14 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2015 19:10:31 GMT"}, {"version": "v4", "created": "Wed, 25 Nov 2015 12:07:31 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Garber", "Dan", ""], ["Hazan", "Elad", ""]]}, {"id": "1509.05669", "submitter": "Varun Shankar", "authors": "Edward J. Fuselier, Varun Shankar and Grady B. Wright", "title": "A High-Order Radial Basis Function (RBF) Leray Projection Method for the\n  Solution of the Incompressible Unsteady Stokes Equations", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new projection method based on radial basis functions (RBFs) is presented\nfor discretizing the incompressible unsteady Stokes equations in irregular\ngeometries. The novelty of the method comes from the application of a new\ntechnique for computing the Leray-Helmholtz projection of a vector field using\ngeneralized interpolation with divergence-free and curl-free RBFs. Unlike\ntraditional projection methods, this new method enables matching both\ntangential and normal components of divergence-free vector fields on the domain\nboundary. This allows incompressibility of the velocity field to be enforced\nwithout any time-splitting or pressure boundary conditions. Spatial derivatives\nare approximated using collocation with global RBFs so that the method only\nrequires samples of the field at (possibly scattered) nodes over the domain.\nNumerical results are presented demonstrating high-order convergence in both\nspace (between 5th and 6th order) and time (up to 4th order) for some model\nproblems in two dimensional irregular geometries.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 15:39:17 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Fuselier", "Edward J.", ""], ["Shankar", "Varun", ""], ["Wright", "Grady B.", ""]]}, {"id": "1509.05696", "submitter": "Tarek Lahlou", "authors": "Tarek A. Lahlou and Anuran Makur", "title": "Transient Signal Spaces and Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.NA cs.SY math.CA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of transient signal analysis. A\nsignal-dependent algorithm is proposed which sequentially identifies the\ncountable sets of decay rates and expansion coefficients present in a given\nsignal. We qualitatively compare our method to existing techniques such as\northogonal exponential transforms generated from orthogonal polynomial classes.\nThe presented algorithm has immediate utility to signal processing applications\nwherein the decay rates and expansion coefficients associated with a transient\nsignal convey information. We also provide a functional interpretation of our\nparameter extraction method via signal approximation using monomials over the\nunit interval from the perspective of biorthogonal constraint satisfaction.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 18:13:57 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Lahlou", "Tarek A.", ""], ["Makur", "Anuran", ""]]}, {"id": "1509.05895", "submitter": "Tarek Lahlou", "authors": "Tarek A. Lahlou and Alan V. Oppenheim", "title": "Trading Accuracy for Numerical Stability: Orthogonalization,\n  Biorthogonalization and Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two novel regularization methods motivated in part by the\ngeometric significance of biorthogonal bases in signal processing applications.\nThese methods, in particular, draw upon the structural relevance of\northogonality and biorthogonality principles and are presented from the\nperspectives of signal processing, convex programming, continuation methods and\nnonlinear projection operators. Each method is specifically endowed with either\na homotopy or tuning parameter to facilitate tradeoff analysis between accuracy\nand numerical stability. An example involving a basis comprised of real\nexponential signals illustrates the utility of the proposed methods on an\nill-conditioned inverse problem and the results are compared to standard\nregularization techniques from the signal processing literature.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 14:10:31 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 03:19:45 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Lahlou", "Tarek A.", ""], ["Oppenheim", "Alan V.", ""]]}, {"id": "1509.06231", "submitter": "Ruben Becker", "authors": "Ruben Becker, Michael Sagraloff, Vikram Sharma, Chee Yap", "title": "A Near-Optimal Subdivision Algorithm for Complex Root Isolation based on\n  the Pellet Test and Newton Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.SC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a subdivision algorithm for isolating the complex roots of a\npolynomial $F\\in\\mathbb{C}[x]$. Given an oracle that provides approximations of\neach of the coefficients of $F$ to any absolute error bound and given an\narbitrary square $\\mathcal{B}$ in the complex plane containing only simple\nroots of $F$, our algorithm returns disjoint isolating disks for the roots of\n$F$ in $\\mathcal{B}$. Our complexity analysis bounds the absolute error to\nwhich the coefficients of $F$ have to be provided, the total number of\niterations, and the overall bit complexity. It further shows that the\ncomplexity of our algorithm is controlled by the geometry of the roots in a\nnear neighborhood of the input square $\\mathcal{B}$, namely, the number of\nroots, their absolute values and pairwise distances. The number of subdivision\nsteps is near-optimal. For the \\emph{benchmark problem}, namely, to isolate all\nthe roots of a polynomial of degree $n$ with integer coefficients of bit size\nless than $\\tau$, our algorithm needs $\\tilde O(n^3+n^2\\tau)$ bit operations,\nwhich is comparable to the record bound of Pan (2002). It is the first time\nthat such a bound has been achieved using subdivision methods, and independent\nof divide-and-conquer techniques such as Sch\\\"onhage's splitting circle\ntechnique. Our algorithm uses the quadtree construction of Weyl (1924) with two\nkey ingredients: using Pellet's Theorem (1881) combined with Graeffe iteration,\nwe derive a \"soft-test\" to count the number of roots in a disk. Using\nSchr\\\"oder's modified Newton operator combined with bisection, in a form\ninspired by the quadratic interval method from Abbot (2006), we achieve\nquadratic convergence towards root clusters. Relative to the divide-conquer\nalgorithms, our algorithm is quite simple with the potential of being\npractical. This paper is self-contained: we provide pseudo-code for all\nsubroutines used by our algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 14:13:19 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 11:26:58 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 09:59:16 GMT"}, {"version": "v4", "created": "Tue, 8 Nov 2016 09:29:38 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Becker", "Ruben", ""], ["Sagraloff", "Michael", ""], ["Sharma", "Vikram", ""], ["Yap", "Chee", ""]]}, {"id": "1509.06265", "submitter": "Sarmen Keshishzadeh", "authors": "Sarmen Keshishzadeh, Jan Friso Groote", "title": "Exact Real Arithmetic with Perturbation Analysis and Proof of\n  Correctness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we consider a simple representation for real numbers and\npropose top-down procedures to approximate various algebraic and transcendental\noperations with arbitrary precision. Detailed algorithms and proofs are\nprovided to guarantee the correctness of the approximations. Moreover, we\ndevelop and apply a perturbation analysis method to show that our approximation\nprocedures only recompute expressions when unavoidable.\n  In the last decade, various theories have been developed and implemented to\nrealize real computations with arbitrary precision. Proof of correctness for\nexisting approaches typically consider basic algebraic operations, whereas\ndetailed arguments about transcendental operations are not available. Another\nimportant observation is that in each approach some expressions might require\niterative computations to guarantee the desired precision. However, no formal\nreasoning is provided to prove that such iterative calculations are essential\nin the approximation procedures. In our approximations of real functions, we\nexplicitly relate the precision of the inputs to the guaranteed precision of\nthe output, provide full proofs and a precise analysis of the necessity of\niterations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 15:20:26 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Keshishzadeh", "Sarmen", ""], ["Groote", "Jan Friso", ""]]}, {"id": "1509.06935", "submitter": "Daniel Ruprecht", "authors": "Daniel Ruprecht", "title": "Shared Memory Pipelined Parareal", "comments": null, "journal-ref": "In: Rivera F., Pena T., Cabaleiro J. (eds) Euro-Par 2017: Parallel\n  Processing. Lecture Notes in Computer Science, vol 10417. Springer", "doi": "10.1007/978-3-319-64203-1_48", "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the parallel-in-time integration method Parareal, pipelining can be used\nto hide some of the cost of the serial correction step and improve its\nefficiency. The paper introduces a basic OpenMP implementation of pipelined\nParareal and compares it to a standard MPI-based variant. Both versions yield\nalmost identical runtimes, but, depending on the compiler, the OpenMP variant\nconsumes about 7% less energy and has a significantly smaller memory footprint.\nHowever, its higher implementation complexity might make it difficult to use in\nlegacy codes and in combination with spatial parallelisation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 12:04:23 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 09:57:47 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 15:20:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Ruprecht", "Daniel", ""]]}, {"id": "1509.07919", "submitter": "Ang Li", "authors": "Ang Li, Radu Serban, Dan Negrut", "title": "Analysis of A Splitting Approach for the Parallel Solution of Linear\n  Systems on GPU Cards", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an approach for solving sparse or dense banded linear systems\n${\\bf A} {\\bf x} = {\\bf b}$ on a Graphics Processing Unit (GPU) card. The\nmatrix ${\\bf A} \\in {\\mathbb{R}}^{N \\times N}$ is possibly nonsymmetric and\nmoderately large; i.e., $10000 \\leq N \\leq 500000$. The ${\\it split\\ and\\\nparallelize}$ (${\\tt SaP}$) approach seeks to partition the matrix ${\\bf A}$\ninto diagonal sub-blocks ${\\bf A}_i$, $i=1,\\ldots,P$, which are independently\nfactored in parallel. The solution may choose to consider or to ignore the\nmatrices that couple the diagonal sub-blocks ${\\bf A}_i$. This approach, along\nwith the Krylov subspace-based iterative method that it preconditions, are\nimplemented in a solver called ${\\tt SaP::GPU}$, which is compared in terms of\nefficiency with three commonly used sparse direct solvers: ${\\tt PARDISO}$,\n${\\tt SuperLU}$, and ${\\tt MUMPS}$. ${\\tt SaP::GPU}$, which runs entirely on\nthe GPU except several stages involved in preliminary row-column permutations,\nis robust and compares well in terms of efficiency with the aforementioned\ndirect solvers. In a comparison against Intel's ${\\tt MKL}$, ${\\tt SaP::GPU}$\nalso fares well when used to solve dense banded systems that are close to being\ndiagonally dominant. ${\\tt SaP::GPU}$ is publicly available and distributed as\nopen source under a permissive BSD3 license.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 23:04:17 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Li", "Ang", ""], ["Serban", "Radu", ""], ["Negrut", "Dan", ""]]}, {"id": "1509.08323", "submitter": "J. M. Landsberg", "authors": "J.M. Landsberg and Nicholas Ryder", "title": "On the geometry of border rank algorithms for n x 2 by 2 x 2 matrix\n  multiplication", "comments": "19 pages, two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make an in-depth study of the known border rank (i.e. approximate)\nalgorithms for the matrix multiplication tensor encoding the multiplication of\nan n x 2 matrix by a 2 x 2 matrix.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 14:03:33 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Landsberg", "J. M.", ""], ["Ryder", "Nicholas", ""]]}, {"id": "1509.08581", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Optimization over Sparse Symmetric Sets via a Nonmonotone Projected\n  Gradient Method", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a Lipschitz differentiable function\nover a class of sparse symmetric sets that has wide applications in engineering\nand science. For this problem, it is known that any accumulation point of the\nclassical projected gradient (PG) method with a constant stepsize $1/L$\nsatisfies the $L$-stationarity optimality condition that was introduced in [3].\nIn this paper we introduce a new optimality condition that is stronger than the\n$L$-stationarity optimality condition. We also propose a nonmonotone projected\ngradient (NPG) method for this problem by incorporating some support-changing\nand coordintate-swapping strategies into a projected gradient method with\nvariable stepsizes. It is shown that any accumulation point of NPG satisfies\nthe new optimality condition and moreover it is a coordinatewise stationary\npoint. Under some suitable assumptions, we further show that it is a global or\na local minimizer of the problem. Numerical experiments are conducted to\ncompare the performance of PG and NPG. The computational results demonstrate\nthat NPG has substantially better solution quality than PG, and moreover, it is\nat least comparable to, but sometimes can be much faster than PG in terms of\nspeed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 03:39:01 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 22:19:11 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2015 18:47:57 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1509.08863", "submitter": "Nicolas Tremblay", "authors": "Nicolas Tremblay, Gilles Puy, Pierre Borgnat, Remi Gribonval, Pierre\n  Vandergheynst", "title": "Accelerated Spectral Clustering Using Graph Filtering Of Random Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build upon recent advances in graph signal processing to propose a faster\nspectral clustering algorithm. Indeed, classical spectral clustering is based\non the computation of the first k eigenvectors of the similarity matrix'\nLaplacian, whose computation cost, even for sparse matrices, becomes\nprohibitive for large datasets. We show that we can estimate the spectral\nclustering distance matrix without computing these eigenvectors: by graph\nfiltering random signals. Also, we take advantage of the stochasticity of these\nrandom vectors to estimate the number of clusters k. We compare our method to\nclassical spectral clustering on synthetic data, and show that it reaches equal\nperformance while being faster by a factor at least two for large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 17:32:48 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Tremblay", "Nicolas", ""], ["Puy", "Gilles", ""], ["Borgnat", "Pierre", ""], ["Gribonval", "Remi", ""], ["Vandergheynst", "Pierre", ""]]}]