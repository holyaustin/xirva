[{"id": "1006.0392", "submitter": "EPTCS", "authors": "Stefano Galatolo (Dipartiento di matematica applicata, Universita di\n  Pisa), Mathieu Hoyrup (LORIA, Vandoeuvre-l es-Nancy, France), Crist\\'obal\n  Rojas (Fields Institute, Toronto, Canada)", "title": "Computing the speed of convergence of ergodic averages and pseudorandom\n  points in computable dynamical systems", "comments": null, "journal-ref": "EPTCS 24, 2010, pp. 7-18", "doi": "10.4204/EPTCS.24.6", "report-no": null, "categories": "cs.NA cs.CE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pseudorandom point in an ergodic dynamical system over a computable metric\nspace is a point which is computable but its dynamics has the same statistical\nbehavior as a typical point of the system.\n  It was proved in [Avigad et al. 2010, Local stability of ergodic averages]\nthat in a system whose dynamics is computable the ergodic averages of\ncomputable observables converge effectively. We give an alternative, simpler\nproof of this result.\n  This implies that if also the invariant measure is computable then the\npseudorandom points are a set which is dense (hence nonempty) on the support of\nthe invariant measure.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 14:29:58 GMT"}], "update_date": "2010-06-03", "authors_parsed": [["Galatolo", "Stefano", "", "Dipartiento di matematica applicata, Universita di\n  Pisa"], ["Hoyrup", "Mathieu", "", "LORIA, Vandoeuvre-l es-Nancy, France"], ["Rojas", "Crist\u00f3bal", "", "Fields Institute, Toronto, Canada"]]}, {"id": "1006.0401", "submitter": "EPTCS", "authors": "Norbert Th. M\\\"uller (Universit\\\"at Trier, Germany), Margarita\n  Korovina (University Manchester, UK)", "title": "Making big steps in trajectories", "comments": null, "journal-ref": "EPTCS 24, 2010, pp. 106-119", "doi": "10.4204/EPTCS.24.15", "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the solution of initial value problems within the context of\nhybrid systems and emphasise the use of high precision approximations (in\nsoftware for exact real arithmetic). We propose a novel algorithm for the\ncomputation of trajectories up to the area where discontinuous jumps appear,\napplicable for holomorphic flow functions. Examples with a prototypical\nimplementation illustrate that the algorithm might provide results with higher\nprecision than well-known ODE solvers at a similar computation time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 14:30:40 GMT"}], "update_date": "2010-06-03", "authors_parsed": [["M\u00fcller", "Norbert Th.", "", "Universit\u00e4t Trier, Germany"], ["Korovina", "Margarita", "", "University Manchester, UK"]]}, {"id": "1006.0402", "submitter": "EPTCS", "authors": "Robert Rettinger", "title": "A Local to Global Principle for the Complexity of Riemann Mappings\n  (Extended Abstract)", "comments": null, "journal-ref": "EPTCS 24, 2010, pp. 120-129", "doi": "10.4204/EPTCS.24.16", "report-no": null, "categories": "cs.CC cs.LO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the computational complexity of Riemann mappings can be bounded\nby the complexity needed to compute conformal mappings locally at boundary\npoints. As a consequence we get first formally proven upper bounds for\nSchwarz-Christoffel mappings and, more generally, Riemann mappings of domains\nwith piecewise analytic boundaries.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 14:30:45 GMT"}], "update_date": "2010-06-03", "authors_parsed": [["Rettinger", "Robert", ""]]}, {"id": "1006.0405", "submitter": "EPTCS", "authors": "Thomas Steinke, Raazesh Sainudiin", "title": "A Rigorous Extension of the Sch\\\"onhage-Strassen Integer Multiplication\n  Algorithm Using Complex Interval Arithmetic", "comments": null, "journal-ref": "EPTCS 24, 2010, pp. 151-159", "doi": "10.4204/EPTCS.24.19", "report-no": null, "categories": "cs.NA cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication of n-digit integers by long multiplication requires O(n^2)\noperations and can be time-consuming. In 1970 A. Schoenhage and V. Strassen\npublished an algorithm capable of performing the task with only O(n log(n))\narithmetic operations over the complex field C; naturally, finite-precision\napproximations to C are used and rounding errors need to be accounted for.\nOverall, using variable-precision fixed-point numbers, this results in an\nO(n(log(n))^(2+Epsilon))-time algorithm. However, to make this algorithm more\nefficient and practical we need to make use of hardware-based floating-point\nnumbers. How do we deal with rounding errors? and how do we determine the\nlimits of the fixed-precision hardware? Our solution is to use interval\narithmetic to guarantee the correctness of results and determine the hardware's\nlimits. We examine the feasibility of this approach and are able to report that\n75,000-digit base-256 integers can be handled using double-precision\ncontainment sets. This clearly demonstrates that our approach has practical\npotential; however, at this stage, our implementation does not yet compete with\ncommercial ones, but we are able to demonstrate the feasibility of this\ntechnique.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 14:31:02 GMT"}], "update_date": "2010-06-03", "authors_parsed": [["Steinke", "Thomas", ""], ["Sainudiin", "Raazesh", ""]]}, {"id": "1006.1043", "submitter": "Ognyan Kounchev", "authors": "Ognyan Kounchev, Damyan Kalaglarsky", "title": "Polyharmonic Daubechies type wavelets in Image Processing and Astronomy,\n  I", "comments": "6 pages, prepared for the ACM proceedings of CompSysTech 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of multivariate wavelets which are obtained by\n\"polyharmonic subdivision\". They generalize directly the original compactly\nsupported Daubechies wavelets.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 13:25:19 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Kounchev", "Ognyan", ""], ["Kalaglarsky", "Damyan", ""]]}, {"id": "1006.1282", "submitter": "Jun Zhao", "authors": "Jun Zhao, Elizabeth Mansfield", "title": "Discrete Variational Calculus for B-spline Approximated Curves", "comments": "14 pages, 19 figures, prepare for LMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study variational problems for curves approximated by B-spline curves. We\nshow that, one can obtain discrete Euler-Lagrange equations, for the data\ndescribing the approximated curves. Our main application is to the curve\ncompletion problem in 2D and 3D. In this case, the aim is to find various\naesthetically pleasing solutions as opposed to a solution of a physical\nproblem. The Lagrangians of interest are invariant under the special Euclidean\ngroup action for which B-spline approximated curves are well suited. Smooth\nLagrangians with special Euclidean symmetries involve curvature, torsion, and\narc length. Expressions in these, in the original coordinates, are highly\ncomplex. We show that, by contrast, relatively simple discrete Lagrangians\noffer excellent results for the curve completion problem. The methods we\ndevelop for the discrete curve completion problem are general and can be used\nto solve other discrete variational problems for B-spline curves.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2010 22:14:15 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2012 17:32:35 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Zhao", "Jun", ""], ["Mansfield", "Elizabeth", ""]]}, {"id": "1006.1296", "submitter": "Yago Ascasibar", "authors": "Yago Ascasibar (UAM, Spain)", "title": "Estimating multidimensional probability fields using the Field Estimator\n  for Arbitrary Spaces (FiEstAS) with applications to astrophysics", "comments": "15 pages, 4 figures, accepted in Comp. Phys. Comm", "journal-ref": "Computer Physics Communications, Volume 181, Issue 8, August 2010,\n  Pages 1438-1443", "doi": "10.1016/j.cpc.2010.04.011", "report-no": null, "categories": "astro-ph.IM cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Field Estimator for Arbitrary Spaces (FiEstAS) computes the continuous\nprobability density field underlying a given discrete data sample in multiple,\nnon-commensurate dimensions. The algorithm works by constructing a\nmetric-independent tessellation of the data space based on a recursive binary\nsplitting. Individual, data-driven bandwidths are assigned to each point,\nscaled so that a constant \"mass\" M0 is enclosed. Kernel density estimation may\nthen be performed for different kernel shapes, and a combination of balloon and\nsample point estimators is proposed as a compromise between resolution and\nvariance. A bias correction is evaluated for the particular (yet common) case\nwhere the density is computed exactly at the locations of the data points\nrather than at an uncorrelated set of locations. By default, the algorithm\ncombines a top-hat kernel with M0=2.0 with the balloon estimator and applies\nthe corresponding bias correction. These settings are shown to yield reasonable\nresults for a simple test case, a two-dimensional ring, that illustrates the\nperformance for oblique distributions, as well as for a six-dimensional\nHernquist sphere, a fairly realistic model of the dynamical structure of\nstellar bulges in galaxies and dark matter haloes in cosmological N-body\nsimulations. Results for different parameter settings are discussed in order to\nprovide a guideline to select an optimal configuration in other cases. Source\ncode is available upon request.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 16:49:05 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Ascasibar", "Yago", "", "UAM, Spain"]]}, {"id": "1006.1307", "submitter": "Sraban Mohanty", "authors": "Sraban Kumar Mohanty", "title": "I/O Efficient Algorithms for Matrix Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse some QR decomposition algorithms, and show that the I/O complexity\nof the tile based algorithm is asymptotically the same as that of matrix\nmultiplication. This algorithm, we show, performs the best when the tile size\nis chosen so that exactly one tile fits in the main memory. We propose a\nconstant factor improvement, as well as a new recursive cache oblivious\nalgorithm with the same asymptotic I/O complexity. We design Hessenberg,\ntridiagonal, and bidiagonal reductions that use banded intermediate forms, and\nperform only asymptotically optimal numbers of I/Os; these are the first I/O\noptimal algorithms for these problems. In particular, we show that known slab\nbased algorithms for two sided reductions all have suboptimal asymptotic I/O\nperformances, even though they have been reported to do better than the\ntraditional algorithms on the basis of empirical evidence.\n  We propose new tile based variants of multishift QR and QZ algorithms that\nunder certain conditions on the number of shifts, have better seek and I/O\ncomplexities than all known variants.\n  We show that techniques like rescheduling of computational steps, appropriate\nchoosing of the blocking parameters and incorporating of more matrix-matrix\noperations, can be used to improve the I/O and seek complexities of matrix\ncomputations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 17:42:53 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Mohanty", "Sraban Kumar", ""]]}, {"id": "1006.2183", "submitter": "Aydin Buluc", "authors": "Ayd{\\i}n Bulu\\c{c} and John R. Gilbert", "title": "Highly Parallel Sparse Matrix-Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCSB technical report CS-2010-10", "categories": "cs.DC cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized sparse matrix-matrix multiplication is a key primitive for many\nhigh performance graph algorithms as well as some linear solvers such as\nmultigrid. We present the first parallel algorithms that achieve increasing\nspeedups for an unbounded number of processors. Our algorithms are based on\ntwo-dimensional block distribution of sparse matrices where serial sections use\na novel hypersparse kernel for scalability. We give a state-of-the-art MPI\nimplementation of one of our algorithms. Our experiments show scaling up to\nthousands of processors on a variety of test scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 02:10:58 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Bulu\u00e7", "Ayd\u0131n", ""], ["Gilbert", "John R.", ""]]}, {"id": "1006.2269", "submitter": "Stefan Engblom", "authors": "Stefan Engblom", "title": "On well-separated sets and fast multipole methods", "comments": null, "journal-ref": "Appl. Numer. Math. 61(10):1096--1102, 2011", "doi": "10.1016/j.apnum.2011.06.011", "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of well-separated sets is crucial in fast multipole methods as the\nmain idea is to approximate the interaction between such sets via cluster\nexpansions. We revisit the one-parameter multipole acceptance criterion in a\ngeneral setting and derive a relative error estimate. This analysis benefits\nasymmetric versions of the method, where the division of the multipole boxes is\nmore liberal than in conventional codes. Such variants offer a particularly\nelegant implementation with a balanced multipole tree, a feature which might be\nvery favorable on modern computer architectures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 11:45:07 GMT"}, {"version": "v2", "created": "Tue, 8 Feb 2011 17:30:52 GMT"}, {"version": "v3", "created": "Wed, 10 Aug 2011 07:53:15 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Engblom", "Stefan", ""]]}, {"id": "1006.3159", "submitter": "Alexandre Chapoutot", "authors": "Olivier Bouissou (LMeASI), Yassamine Seladji (LMeASI), Alexandre\n  Chapoutot (LIP6)", "title": "Abstract Fixpoint Computations with Numerical Acceleration Methods", "comments": null, "journal-ref": "Electronic Notes in Theoretical Computer Science (2010) 29-42", "doi": "10.1016/j.entcs.2010.09.004", "report-no": null, "categories": "cs.PL cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static analysis by abstract interpretation aims at automatically proving\nproperties of computer programs. To do this, an over-approximation of program\nsemantics, defined as the least fixpoint of a system of semantic equations,\nmust be computed. To enforce the convergence of this computation, widening\noperator is used but it may lead to coarse results. We propose a new method to\naccelerate the computation of this fixpoint by using standard techniques of\nnumerical analysis. Our goal is to automatically and dynamically adapt the\nwidening operator in order to maintain precision.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2010 08:39:12 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Bouissou", "Olivier", "", "LMeASI"], ["Seladji", "Yassamine", "", "LMeASI"], ["Chapoutot", "Alexandre", "", "LIP6"]]}, {"id": "1006.3962", "submitter": "Pedro Gonnet", "authors": "Pedro Gonnet", "title": "Increasing the Reliability of Adaptive Quadrature Using Explicit\n  Interpolants", "comments": "32 pages, submitted to ACM Transactions on Mathematical Software", "journal-ref": "ACM Transactions on Mathematical Software (TOMS), Volume 37 Issue\n  3, September 2010", "doi": "10.1145/1824801.1824804", "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new adaptive quadrature routines. Both routines differ from\npreviously published algorithms in many aspects, most significantly in how they\nrepresent the integrand, how they treat non-numerical values of the integrand,\nhow they deal with improper divergent integrals and how they estimate the\nintegration error. The main focus of these improvements is to increase the\nreliability of the algorithms without significantly impacting their efficiency.\nBoth algorithms are implemented in Matlab and tested using both the \"families\"\nsuggested by Lyness and Kaganove and the battery test used by Gander and\nGautschi and Kahaner. They are shown to be more reliable, albeit in some cases\nless efficient, than other commonly-used adaptive integrators.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2010 21:52:01 GMT"}], "update_date": "2010-11-09", "authors_parsed": [["Gonnet", "Pedro", ""]]}, {"id": "1006.4425", "submitter": "EPTCS", "authors": "Aleksandr Andreychenko, Pepijn Crouzen, Linar Mikeev, Verena Wolf", "title": "On-the-fly Uniformization of Time-Inhomogeneous Infinite Markov\n  Population Models", "comments": null, "journal-ref": "EPTCS 57, 2011, pp. 1-15", "doi": "10.4204/EPTCS.57.1", "report-no": null, "categories": "math.PR cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an on-the-fly uniformization technique for the analysis\nof time-inhomogeneous Markov population models. This technique is applicable to\nmodels with infinite state spaces and unbounded rates, which are, for instance,\nencountered in the realm of biochemical reaction networks. To deal with the\ninfinite state space, we dynamically maintain a finite subset of the states\nwhere most of the probability mass is located. This approach yields an\nunderapproximation of the original, infinite system. We present experimental\nresults to show the applicability of our technique.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 06:44:20 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 07:54:20 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2011 21:43:40 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Andreychenko", "Aleksandr", ""], ["Crouzen", "Pepijn", ""], ["Mikeev", "Linar", ""], ["Wolf", "Verena", ""]]}, {"id": "1006.5104", "submitter": "EPTCS", "authors": "Anton Stefanek (Imperial College London), Richard Hayden (Imperial\n  College London), Jeremy Bradley (Imperial College London)", "title": "A new tool for the performance analysis of massively parallel computer\n  systems", "comments": null, "journal-ref": "EPTCS 28, 2010, pp. 159-181", "doi": "10.4204/EPTCS.28.11", "report-no": null, "categories": "cs.PF cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new tool, GPA, that can generate key performance measures for\nvery large systems. Based on solving systems of ordinary differential equations\n(ODEs), this method of performance analysis is far more scalable than\nstochastic simulation. The GPA tool is the first to produce higher moment\nanalysis from differential equation approximation, which is essential, in many\ncases, to obtain an accurate performance prediction. We identify so-called\nswitch points as the source of error in the ODE approximation. We investigate\nthe switch point behaviour in several large models and observe that as the\nscale of the model is increased, in general the ODE performance prediction\nimproves in accuracy. In the case of the variance measure, we are able to\njustify theoretically that in the limit of model scale, the ODE approximation\ncan be expected to tend to the actual variance of the model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2010 03:03:55 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Stefanek", "Anton", "", "Imperial College London"], ["Hayden", "Richard", "", "Imperial\n  College London"], ["Bradley", "Jeremy", "", "Imperial College London"]]}, {"id": "1006.5111", "submitter": "Timothy Matisziw", "authors": "Timothy C. Matisziw, Alan T. Murray", "title": "Modeling s-t Path Availability to Support Disaster Vulnerability\n  Assessment of Network Infrastructure", "comments": null, "journal-ref": "Matisziw, T.C. and A.T. Murray. 2009. Modeling s-t Path\n  Availability to Support Disaster Vulnerability Assessment of Network\n  Infrastructure. Computers & Operations Research. 36(1), 16-26", "doi": null, "report-no": null, "categories": "physics.data-an cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maintenance of system flow is critical for effective network operation.\nAny type of disruption to network facilities (arcs/nodes) potentially risks\nloss of service, leaving users without access to important resources. It is\ntherefore an important goal of planners to assess infrastructures for\nvulnerabilities, identifying those vital nodes/arcs whose debilitation would\ncompromise the most source-sink (s-t) interaction or system flow. Due to the\nbudgetary limitations of disaster management agencies, protection/fortification\nand planning for the recovery of these vital infrastructure facilities is a\nlogical and efficient proactive approach to reducing worst-case risk of service\ndisruption. Given damage to a network, evaluating the potential for flow\nbetween s-t pairs requires assessing the availability of an operational s-t\npath. Recent models proposed for identifying infrastructure vital to system\nflow have relied on enumeration of all s-t paths to support this task. This\npaper proposes an alternative model constraint structure that does not require\ncomplete enumeration of s-t paths, providing computational benefits over\nexisting models. To illustrate the model, an application to a practical\ninfrastructure planning problem is presented.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2010 04:07:23 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Matisziw", "Timothy C.", ""], ["Murray", "Alan T.", ""]]}, {"id": "1006.5252", "submitter": "Samuel Cheng", "authors": "Rick Ma and Samuel Cheng", "title": "Decomposition Approach for Low-rank Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a low-rank matrix completion method based on\nmatrix decomposition. An incomplete matrix is decomposed into submatrices which\nare filled with a proposed trimming step and then are recombined to form a\nlow-rank completed matrix. The divide-and-conquer approach can significantly\nreduce computation complexity and storage requirement. Moreover, the proposed\ndecomposition method can be naturally incorporated into any existing matrix\ncompletion methods to attain further gain. Unlike most existing approaches, the\nproposed method is not based on norm minimization nor SVD decomposition. This\nmakes it possible to be applied beyond real domain and can be used in arbitrary\nfields including finite fields.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2010 01:36:15 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Ma", "Rick", ""], ["Cheng", "Samuel", ""]]}, {"id": "1006.5311", "submitter": "Nicolas Goze", "authors": "Nicolas Goze", "title": "Linear Algebra in the vector space of intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous paper, we have given an algebraic model to the set of\nintervals. Here, we apply this model in a linear frame. We define a notion of\ndiagonalization of square matrices whose coefficients are intervals. But in\nthis case, with respect to the real case, a matrix of order $n$ could have more\nthan $n$ eigenvalues (the set of intervals is not factorial). We consider a\nnotion of central eigenvalues permits to describe criterium of diagonalization.\nAs application, we define a notion of Exponential mapping.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2010 10:41:39 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Goze", "Nicolas", ""]]}, {"id": "1006.5748", "submitter": "Adam Oberman", "authors": "Brittany D. Froese and Adam M. Oberman", "title": "Fast finite difference solvers for singular solutions of the elliptic\n  Monge-Amp\\`ere equation", "comments": "23 pages, 4 figures, 4 tables; added arxiv links to references, added\n  coments", "journal-ref": null, "doi": "10.1016/j.jcp.2010.10.020", "report-no": null, "categories": "math.NA cs.NA math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elliptic Monge-Ampere equation is a fully nonlinear Partial Differential\nEquation which originated in geometric surface theory, and has been applied in\ndynamic meteorology, elasticity, geometric optics, image processing and image\nregistration. Solutions can be singular, in which case standard numerical\napproaches fail. In this article we build a finite difference solver for the\nMonge-Ampere equation, which converges even for singular solutions. Regularity\nresults are used to select a priori between a stable, provably convergent\nmonotone discretization and an accurate finite difference discretization in\ndifferent regions of the computational domain. This allows singular solutions\nto be computed using a stable method, and regular solutions to be computed more\naccurately. The resulting nonlinear equations are then solved by Newton's\nmethod. Computational results in two and three dimensions validate the claims\nof accuracy and solution speed. A computational example is presented which\ndemonstrates the necessity of the use of the monotone scheme near\nsingularities.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 00:21:59 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2010 21:16:04 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2010 02:35:29 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Froese", "Brittany D.", ""], ["Oberman", "Adam M.", ""]]}]