[{"id": "1507.00272", "submitter": "Alex Townsend", "authors": "Vanni Noferini and Alex Townsend", "title": "Numerical instability of resultant methods for multidimensional\n  rootfinding", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden-variable resultant methods are a class of algorithms for solving\nmultidimensional polynomial rootfinding problems. In two dimensions, when\nsignificant care is taken, they are competitive practical rootfinders. However,\nin higher dimensions they are known to miss zeros, calculate roots to low\nprecision, and introduce spurious solutions. We show that the hidden variable\nresultant method based on the Cayley (Dixon or B\\'ezout) matrix is inherently\nand spectacularly numerically unstable by a factor that grows exponentially\nwith the dimension. We also show that the Sylvester matrix for solving\nbivariate polynomial systems can square the condition number of the problem. In\nother words, two popular hidden variable resultant methods are numerically\nunstable, and this mathematically explains the difficulties that are frequently\nreported by practitioners. Regardless of how the constructed polynomial\neigenvalue problem is solved, severe numerical difficulties will be present.\nAlong the way, we prove that the Cayley resultant is a generalization of\nCramer's rule for solving linear systems and generalize Clenshaw's algorithm to\nan evaluation scheme for polynomials expressed in a degree-graded polynomial\nbasis.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 16:00:40 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2016 20:35:33 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 16:17:09 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Noferini", "Vanni", ""], ["Townsend", "Alex", ""]]}, {"id": "1507.00333", "submitter": "Jie Yang", "authors": "Yuan Lu and Jie Yang", "title": "Notes on Low-rank Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix factorization (MF) is an important technique in data science.\nThe key idea of MF is that there exists latent structures in the data, by\nuncovering which we could obtain a compressed representation of the data. By\nfactorizing an original matrix to low-rank matrices, MF provides a unified\nmethod for dimension reduction, clustering, and matrix completion. In this\narticle we review several important variants of MF, including: Basic MF,\nNon-negative MF, Orthogonal non-negative MF. As can be told from their names,\nnon-negative MF and orthogonal non-negative MF are variants of basic MF with\nnon-negativity and/or orthogonality constraints. Such constraints are useful in\nspecific senarios. In the first part of this article, we introduce, for each of\nthese models, the application scenarios, the distinctive properties, and the\noptimizing method. By properly adapting MF, we can go beyond the problem of\nclustering and matrix completion. In the second part of this article, we will\nextend MF to sparse matrix compeletion, enhance matrix compeletion using\nvarious regularization methods, and make use of MF for (semi-)supervised\nlearning by introducing latent space reinforcement and transformation. We will\nsee that MF is not only a useful model but also as a flexible framework that is\napplicable for various prediction problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 20:47:34 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 20:44:46 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 10:35:36 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Lu", "Yuan", ""], ["Yang", "Jie", ""]]}, {"id": "1507.00421", "submitter": "Yao Xie", "authors": "Yang Cao, Yao Xie", "title": "Categorical Matrix Completion", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of completing a matrix with categorical-valued\nentries from partial observations. This is achieved by extending the\nformulation and theory of one-bit matrix completion. We recover a low-rank\nmatrix $X$ by maximizing the likelihood ratio with a constraint on the nuclear\nnorm of $X$, and the observations are mapped from entries of $X$ through\nmultiple link functions. We establish theoretical upper and lower bounds on the\nrecovery error, which meet up to a constant factor $\\mathcal{O}(K^{3/2})$ where\n$K$ is the fixed number of categories. The upper bound in our case depends on\nthe number of categories implicitly through a maximization of terms that\ninvolve the smoothness of the link functions. In contrast to one-bit matrix\ncompletion, our bounds for categorical matrix completion are optimal up to a\nfactor on the order of the square root of the number of categories, which is\nconsistent with an intuition that the problem becomes harder when the number of\ncategories increases. By comparing the performance of our method with the\nconventional matrix completion method on the MovieLens dataset, we demonstrate\nthe advantage of our method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 03:58:47 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1507.00438", "submitter": "Alain Rakotomamonjy", "authors": "Alain Rakotomamonjy (LITIS), Remi Flamary (LAGRANGE, OCA), Gilles\n  Gasso (LITIS)", "title": "DC Proximal Newton for Non-Convex Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel algorithm for solving learning problems where both the\nloss function and the regularizer are non-convex but belong to the class of\ndifference of convex (DC) functions. Our contribution is a new general purpose\nproximal Newton algorithm that is able to deal with such a situation. The\nalgorithm consists in obtaining a descent direction from an approximation of\nthe loss function and then in performing a line search to ensure sufficient\ndescent. A theoretical analysis is provided showing that the iterates of the\nproposed algorithm {admit} as limit points stationary points of the DC\nobjective function. Numerical experiments show that our approach is more\nefficient than current state of the art for a problem with a convex loss\nfunctions and non-convex regularizer. We have also illustrated the benefit of\nour algorithm in high-dimensional transductive learning problem where both loss\nfunction and regularizers are non-convex.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 06:41:32 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Rakotomamonjy", "Alain", "", "LITIS"], ["Flamary", "Remi", "", "LAGRANGE, OCA"], ["Gasso", "Gilles", "", "LITIS"]]}, {"id": "1507.00644", "submitter": "Kevin Houston", "authors": "Kevin Houston", "title": "Compressed Manifold Modes: Fast Calculation and Natural Ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed manifold modes are locally supported analogues of eigenfunctions\nof the Laplace-Beltrami operator of a manifold. In this paper we describe an\nalgorithm for the calculation of modes for discrete manifolds that, in\nexperiments, requires on average 47% fewer iterations and 44% less time than\nthe previous algorithm. We show how to naturally order the modes in an\nanalogous way to eigenfunctions, that is we define a compressed eigenvalue.\nFurthermore, in contrast to the previous algorithm we permit unlumped mass\nmatrices for the operator and we show, unlike the case of eigenfunctions, that\nmodes can, in general, be oriented.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 16:06:26 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 13:40:58 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Houston", "Kevin", ""]]}, {"id": "1507.00687", "submitter": "Alex Druinsky", "authors": "Grey Ballard, Austin R. Benson, Alex Druinsky, Benjamin Lipshitz, Oded\n  Schwartz", "title": "Improving the numerical stability of fast matrix multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast algorithms for matrix multiplication, namely those that perform\nasymptotically fewer scalar operations than the classical algorithm, have been\nconsidered primarily of theoretical interest. Apart from Strassen's original\nalgorithm, few fast algorithms have been efficiently implemented or used in\npractical applications. However, there exist many practical alternatives to\nStrassen's algorithm with varying performance and numerical properties. Fast\nalgorithms are known to be numerically stable, but because their error bounds\nare slightly weaker than the classical algorithm, they are not used even in\ncases where they provide a performance benefit.\n  We argue in this paper that the numerical sacrifice of fast algorithms,\nparticularly for the typical use cases of practical algorithms, is not\nprohibitive, and we explore ways to improve the accuracy both theoretically and\nempirically. The numerical accuracy of fast matrix multiplication depends on\nproperties of the algorithm and of the input matrices, and we consider both\ncontributions independently. We generalize and tighten previous error analyses\nof fast algorithms and compare their properties. We discuss algorithmic\ntechniques for improving the error guarantees from two perspectives:\nmanipulating the algorithms, and reducing input anomalies by various forms of\ndiagonal scaling. Finally, we benchmark performance and demonstrate our\nimproved numerical accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 18:42:28 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 01:55:33 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Ballard", "Grey", ""], ["Benson", "Austin R.", ""], ["Druinsky", "Alex", ""], ["Lipshitz", "Benjamin", ""], ["Schwartz", "Oded", ""]]}, {"id": "1507.00702", "submitter": "Dimitri Bertsekas", "authors": "Dimitri P. Bertsekas", "title": "Centralized and Distributed Newton Methods for Network Optimization and\n  Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": "Report LIDS - 2866, Lab. for Information and Decision Systems,\n  Massachusetts Institute of Technology, Cambridge, MA", "categories": "cs.NA math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider Newton methods for common types of single commodity and\nmulti-commodity network flow problems. Despite the potentially very large\ndimension of the problem, they can be implemented using the conjugate gradient\nmethod and low-dimensional network operations, as shown nearly thirty years\nago. We revisit these methods, compare them to more recent proposals, and\ndescribe how they can be implemented in a distributed computing system. We also\ndiscuss generalizations, including the treatment of arc gains, linear side\nconstraints, and related special structures.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 19:06:47 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Bertsekas", "Dimitri P.", ""]]}, {"id": "1507.00887", "submitter": "Ting Kei Pong", "authors": "Guoyin Li, Tianxiang Liu, Ting Kei Pong", "title": "Peaceman-Rachford splitting for a class of nonconvex optimization\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the applicability of the Peaceman-Rachford (PR) splitting method for\nsolving nonconvex optimization problems. When applied to minimizing the sum of\na strongly convex Lipschitz differentiable function and a proper closed\nfunction, we show that if the strongly convex function has a large enough\nstrong convexity modulus and the step-size parameter is chosen below a\nthreshold that is computable, then any cluster point of the sequence generated,\nif exists, will give a stationary point of the optimization problem. We also\ngive sufficient conditions guaranteeing boundedness of the sequence generated.\nWe then discuss one way to split the objective so that the proposed method can\nbe suitably applied to solving optimization problems with a coercive objective\nthat is the sum of a (not necessarily strongly) convex Lipschitz differentiable\nfunction and a proper closed function; this setting covers a large class of\nnonconvex feasibility problems and constrained least squares problems. Finally,\nwe illustrate the proposed algorithm numerically.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 12:27:09 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 04:18:27 GMT"}, {"version": "v3", "created": "Mon, 9 Jan 2017 07:39:36 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Li", "Guoyin", ""], ["Liu", "Tianxiang", ""], ["Pong", "Ting Kei", ""]]}, {"id": "1507.03173", "submitter": "Jinshan Zeng", "authors": "Jinshan Zeng, Zhimin Peng, Shaobo Lin", "title": "A Gauss-Seidel Iterative Thresholding Algorithm for lq Regularized Least\n  Squares Regression", "comments": "35 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent studies on sparse modeling, $l_q$ ($0<q<1$) regularized least\nsquares regression ($l_q$LS) has received considerable attention due to its\nsuperiorities on sparsity-inducing and bias-reduction over the convex\ncounterparts. In this paper, we propose a Gauss-Seidel iterative thresholding\nalgorithm (called GAITA) for solution to this problem. Different from the\nclassical iterative thresholding algorithms using the Jacobi updating rule,\nGAITA takes advantage of the Gauss-Seidel rule to update the coordinate\ncoefficients. Under a mild condition, we can justify that the support set and\nsign of an arbitrary sequence generated by GAITA will converge within finite\niterations. This convergence property together with the Kurdyka-{\\L}ojasiewicz\nproperty of ($l_q$LS) naturally yields the strong convergence of GAITA under\nthe same condition as above, which is generally weaker than the condition for\nthe convergence of the classical iterative thresholding algorithms.\nFurthermore, we demonstrate that GAITA converges to a local minimizer under\ncertain additional conditions. A set of numerical experiments are provided to\nshow the effectiveness, particularly, much faster convergence of GAITA as\ncompared with the classical iterative thresholding algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 00:47:51 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Zeng", "Jinshan", ""], ["Peng", "Zhimin", ""], ["Lin", "Shaobo", ""]]}, {"id": "1507.03194", "submitter": "Ali Caner T\\\"urkmen", "authors": "Ali Caner T\\\"urkmen", "title": "A Review of Nonnegative Matrix Factorization Methods for Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) was first introduced as a low-rank\nmatrix approximation technique, and has enjoyed a wide area of applications.\nAlthough NMF does not seem related to the clustering problem at first, it was\nshown that they are closely linked. In this report, we provide a gentle\nintroduction to clustering and NMF before reviewing the theoretical\nrelationship between them. We then explore several NMF variants, namely Sparse\nNMF, Projective NMF, Nonnegative Spectral Clustering and Cluster-NMF, along\nwith their clustering interpretations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 07:14:16 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 13:43:28 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["T\u00fcrkmen", "Ali Caner", ""]]}, {"id": "1507.03331", "submitter": "Victor Magron", "authors": "Victor Magron, George Constantinides, Alastair Donaldson", "title": "Certified Roundoff Error Bounds Using Semidefinite Programming", "comments": "32 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roundoff errors cannot be avoided when implementing numerical programs with\nfinite precision. The ability to reason about rounding is especially important\nif one wants to explore a range of potential representations, for instance for\nFPGAs or custom hardware implementations. This problem becomes challenging when\nthe program does not employ solely linear operations, and non-linearities are\ninherent to many interesting computational problems in real-world applications.\n  Existing solutions to reasoning possibly lead to either inaccurate bounds or\nhigh analysis time in the presence of nonlinear correlations between variables.\nFurthermore, while it is easy to implement a straightforward method such as\ninterval arithmetic, sophisticated techniques are less straightforward to\nimplement in a formal setting. Thus there is a need for methods which output\ncertificates that can be formally validated inside a proof assistant.\n  We present a framework to provide upper bounds on absolute roundoff errors of\nfloating-point nonlinear programs. This framework is based on optimization\ntechniques employing semidefinite programming and sums of squares certificates,\nwhich can be checked inside the Coq theorem prover to provide formal roundoff\nerror bounds for polynomial programs. Our tool covers a wide range of nonlinear\nprograms, including polynomials and transcendental operations as well as\nconditional statements. We illustrate the efficiency and precision of this tool\non non-trivial programs coming from biology, optimization and space control.\nOur tool produces more accurate error bounds for 23% of all programs and yields\nbetter performance in 66% of all programs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 06:21:01 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 18:18:00 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2016 23:11:14 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 18:44:50 GMT"}, {"version": "v5", "created": "Fri, 5 Aug 2016 15:25:50 GMT"}, {"version": "v6", "created": "Sun, 6 Nov 2016 16:59:31 GMT"}, {"version": "v7", "created": "Fri, 25 Nov 2016 11:11:56 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Magron", "Victor", ""], ["Constantinides", "George", ""], ["Donaldson", "Alastair", ""]]}, {"id": "1507.04396", "submitter": "Risi Kondor", "authors": "Risi Kondor, Nedelina Teneva, Pramod K. Mudrakarta", "title": "Parallel MMF: a Multiresolution Approach to Matrix Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiresolution Matrix Factorization (MMF) was recently introduced as a\nmethod for finding multiscale structure and defining wavelets on\ngraphs/matrices. In this paper we derive pMMF, a parallel algorithm for\ncomputing the MMF factorization. Empirically, the running time of pMMF scales\nlinearly in the dimension for sparse matrices. We argue that this makes pMMF a\nvaluable new computational primitive in its own right, and present experiments\non using pMMF for two distinct purposes: compressing matrices and\npreconditioning large sparse linear systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 21:19:25 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Kondor", "Risi", ""], ["Teneva", "Nedelina", ""], ["Mudrakarta", "Pramod K.", ""]]}, {"id": "1507.04417", "submitter": "Bishnu Lamichhane", "authors": "Bishnu P. Lamichhane", "title": "A quadrilateral 'mini' finite element for the Stokes problem using a\n  single bubble function", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a quadrilateral 'mini' finite element for approximating the\nsolution of Stokes equations using a quadrilateral mesh. We use the standard\nbilinear finite element space enriched with element-wise defined bubble\nfunctions for the velocity and the standard bilinear finite element space for\nthe pressure space. With a simple modification of the standard bubble function\nwe show that a single bubble function is sufficient to ensure the inf-sup\ncondition. We have thus improved an earlier result on the quadrilateral 'mini'\nelement, where more than one bubble function are used to get the stability.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 00:16:02 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Lamichhane", "Bishnu P.", ""]]}, {"id": "1507.05281", "submitter": "Hidekazu Yoshioka", "authors": "Hidekazu Yoshioka", "title": "On Dual-Finite Volume Methods for Extended Porous Medium Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article shows that the unconditional stability of the Dual-Finite Volume\nMethod, which is at least valid for linear problems, is not true for generic\nnonlinear differential equations including the PMEs unless the coefficient\nappearing in the numerical fluxes are appropriately evaluated. This article\nprovides a theoretically truly isotone numerical fluxes specialized for solving\nthe PMEs presented, which is still as simple as the conventional fully-upwind\ncounterpart.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 12:37:13 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yoshioka", "Hidekazu", ""]]}, {"id": "1507.05366", "submitter": "Hau-tieng Wu", "authors": "Ingrid Daubechies, Yi Wang, Hau-tieng Wu", "title": "ConceFT: Concentration of Frequency and Time via a multitapered\n  synchrosqueezed transform", "comments": null, "journal-ref": null, "doi": "10.1098/rsta.2015.0193", "report-no": null, "categories": "math.ST cs.NA stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed to determine the time-frequency content of\ntime-dependent signals consisting of multiple oscillatory components, with\ntime-varying amplitudes and instantaneous frequencies. Numerical experiments as\nwell as a theoretical analysis are presented to assess its effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:05:14 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Daubechies", "Ingrid", ""], ["Wang", "Yi", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1507.05372", "submitter": "Hau-tieng Wu", "authors": "Yu-Ting Lin, Patrick Flandrin, Hau-tieng Wu", "title": "When interpolation-induced reflection artifact meets time-frequency\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While extracting the temporal dynamical features based on the time-frequency\nanalyses, like the reassignment and synchrosqueezing transform, attracts more\nand more interest in bio-medical data analysis, we should be careful about\nartifacts generated by interpolation schemes, in particular when the sampling\nrate is not significantly higher than the frequency of the oscillatory\ncomponent we are interested in. In this study, we formulate the problem called\nthe reflection effect and provide a theoretical justification of the statement.\nWe also show examples in the anesthetic depth analysis with clear but\nundesirable artifacts. The results show that the artifact associated with the\nreflection effect exists not only theoretically but practically. Its influence\nis pronounced when we apply the time-frequency analyses to extract the\ntime-varying dynamics hidden inside the signal. In conclusion, we have to\ncarefully deal with the artifact associated with the reflection effect by\nchoosing a proper interpolation scheme.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:47:36 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 03:26:46 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Lin", "Yu-Ting", ""], ["Flandrin", "Patrick", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1507.05593", "submitter": "David Bindel", "authors": "Jeffrey N. Chadwick and David S. Bindel", "title": "An Efficient Solver for Sparse Linear Systems Based on Rank-Structured\n  Cholesky Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct factorization methods for the solution of large, sparse linear systems\nthat arise from PDE discretizations are robust, but typically show poor time\nand memory scalability for large systems. In this paper, we describe an\nefficient sparse, rank-structured Cholesky algorithm for solution of the\npositive definite linear system $A x = b$ when $A$ comes from a discretized\npartial-differential equation. Our approach combines the efficient memory\naccess patterns of conventional supernodal Cholesky algorithms with the memory\nefficiency of rank-structured direct solvers. For several test problems arising\nfrom PDE discretizations, our method takes less memory than standard sparse\nCholesky solvers and less wall-clock time than standard preconditioned\niterations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 19:01:30 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Chadwick", "Jeffrey N.", ""], ["Bindel", "David S.", ""]]}, {"id": "1507.05854", "submitter": "Chi Jin", "authors": "Prateek Jain, Chi Jin, Sham M. Kakade and Praneeth Netrapalli", "title": "Global Convergence of Non-Convex Gradient Descent for Computing Matrix\n  Squareroot", "comments": "Appear in AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been a significant amount of work studying gradient descent\ntechniques for non-convex optimization problems over the last few years, all\nexisting results establish either local convergence with good rates or global\nconvergence with highly suboptimal rates, for many problems of interest. In\nthis paper, we take the first step in getting the best of both worlds --\nestablishing global convergence and obtaining a good rate of convergence for\nthe problem of computing squareroot of a positive definite (PD) matrix, which\nis a widely studied problem in numerical linear algebra with applications in\nmachine learning and statistics among others. Given a PD matrix $M$ and a PD\nstarting point $U_0$, we show that gradient descent with appropriately chosen\nstep-size finds an $\\epsilon$-accurate squareroot of $M$ in $O(\\alpha \\log\n(\\|M-U_0^2\\|_F /\\epsilon))$ iterations, where $\\alpha =\n(\\max\\{\\|U_0\\|_2^2,\\|M\\|_2\\} / \\min \\{\\sigma_{\\min}^2(U_0),\\sigma_{\\min}(M) \\}\n)^{3/2}$. Our result is the first to establish global convergence for this\nproblem and that it is robust to errors in each iteration. A key contribution\nof our work is the general proof technique which we believe should further\nexcite research in understanding deterministic and stochastic variants of\nsimple non-convex gradient descent algorithms with good global convergence\nrates for other problems in machine learning and numerical linear algebra.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 14:42:45 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 17:46:28 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Jain", "Prateek", ""], ["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1507.06174", "submitter": "Gil Shabat", "authors": "Amir Averbuch, Gil Shabat and Yoel Shkolnisky", "title": "Direct Inversion of the 3D Pseudo-polar Fourier Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo-polar Fourier transform is a specialized non-equally spaced\nFourier transform, which evaluates the Fourier transform on a near-polar grid,\nknown as the pseudo-polar grid. The advantage of the pseudo-polar grid over\nother non-uniform sampling geometries is that the transformation, which samples\nthe Fourier transform on the pseudo-polar grid, can be inverted using a fast\nand stable algorithm. For other sampling geometries, even if the non-equally\nspaced Fourier transform can be inverted, the only known algorithms are\niterative. The convergence speed of these algorithms as well as their accuracy\nare difficult to control, as they depend both on the sampling geometry as well\nas on the unknown reconstructed object. In this paper, we present a direct\ninversion algorithm for the three-dimensional pseudo-polar Fourier transform.\nThe algorithm is based only on one-dimensional resampling operations, and is\nshown to be significantly faster than existing iterative inversion algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 13:19:27 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2016 16:54:26 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Averbuch", "Amir", ""], ["Shabat", "Gil", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1507.07227", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Jesse Laeuchli, Vassilis Kalantzis, Andreas Stathopoulos,\n  and Efstratios Gallopoulos", "title": "Estimating the Trace of the Matrix Inverse by Interpolating from the\n  Diagonal of an Approximate Inverse", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2016.09.001", "report-no": null, "categories": "cs.NA math.NA quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of applications require the computation of the trace of a matrix\nthat is implicitly available through a function. A common example of a function\nis the inverse of a large, sparse matrix, which is the focus of this paper.\nWhen the evaluation of the function is expensive, the task is computationally\nchallenging because the standard approach is based on a Monte Carlo method\nwhich converges slowly. We present a different approach that exploits the\npattern correlation, if present, between the diagonal of the inverse of the\nmatrix and the diagonal of some approximate inverse that can be computed\ninexpensively. We leverage various sampling and fitting techniques to fit the\ndiagonal of the approximation to the diagonal of the inverse. Depending on the\nquality of the approximate inverse, our method may serve as a standalone kernel\nfor providing a fast trace estimate with a small number of samples.\nFurthermore, the method can be used as a variance reduction method for Monte\nCarlo in some cases. This is decided dynamically by our algorithm. An extensive\nset of experiments with various technique combinations on several matrices from\nsome real applications demonstrate the potential of our method.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 17:43:26 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 17:58:14 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2016 02:15:44 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Wu", "Lingfei", ""], ["Laeuchli", "Jesse", ""], ["Kalantzis", "Vassilis", ""], ["Stathopoulos", "Andreas", ""], ["Gallopoulos", "Efstratios", ""]]}, {"id": "1507.08243", "submitter": "Yanqiu Wang", "authors": "Weizhang Huang and Yanqiu Wang", "title": "Anisotropic mesh quality measures and adaptation for polygonal meshes", "comments": "43 figures", "journal-ref": "J. Comput. Phys. 410 (2020), 109368", "doi": "10.1016/j.jcp.2020.109368", "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anisotropic mesh quality measures and anisotropic mesh adaptation are studied\nfor polygonal meshes. Three sets of alignment and equidistribution measures are\ndeveloped, one based on least squares fitting, one based on generalized\nbarycentric mapping, and the other based on singular value decomposition of\nedge matrices. Numerical tests show that all three sets of mesh quality\nmeasures provide good measurements for the quality of polygonal meshes under\ngiven metrics. Based on one of these sets of quality measures and using a\nmoving mesh partial differential equation, an anisotropic adaptive polygonal\nmesh method is constructed for the numerical solution of second order elliptic\nequations. Numerical examples are presented to demonstrate the effectiveness of\nthe method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 18:05:10 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 11:08:55 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Huang", "Weizhang", ""], ["Wang", "Yanqiu", ""]]}, {"id": "1507.08751", "submitter": "Frank Ong", "authors": "Frank Ong and Michael Lustig", "title": "Beyond Low Rank + Sparse: Multi-scale Low Rank Matrix Decomposition", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2016.2545518", "report-no": null, "categories": "cs.SY cs.IT cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a natural generalization of the recent low rank + sparse matrix\ndecomposition and consider the decomposition of matrices into components of\nmultiple scales. Such decomposition is well motivated in practice as data\nmatrices often exhibit local correlations in multiple scales. Concretely, we\npropose a multi-scale low rank modeling that represents a data matrix as a sum\nof block-wise low rank matrices with increasing scales of block sizes. We then\nconsider the inverse problem of decomposing the data matrix into its\nmulti-scale low rank components and approach the problem via a convex\nformulation. Theoretically, we show that under various incoherence conditions,\nthe convex program recovers the multi-scale low rank components \\revised{either\nexactly or approximately}. Practically, we provide guidance on selecting the\nregularization parameters and incorporate cycle spinning to reduce blocking\nartifacts. Experimentally, we show that the multi-scale low rank decomposition\nprovides a more intuitive decomposition than conventional low rank methods and\ndemonstrate its effectiveness in four applications, including illumination\nnormalization for face images, motion separation for surveillance videos,\nmulti-scale modeling of the dynamic contrast enhanced magnetic resonance\nimaging and collaborative filtering exploiting age information.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 04:46:25 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 18:05:24 GMT"}, {"version": "v3", "created": "Wed, 3 Aug 2016 18:00:06 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Ong", "Frank", ""], ["Lustig", "Michael", ""]]}, {"id": "1507.08788", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and\n  Convexity", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence properties of the VR-PCA algorithm introduced by\n\\cite{shamir2015stochastic} for fast computation of leading singular vectors.\nWe prove several new results, including a formal analysis of a block version of\nthe algorithm, and convergence from random initialization. We also make a few\nobservations of independent interest, such as how pre-initializing with just a\nsingle exact power iteration can significantly improve the runtime of\nstochastic methods, and what are the convexity and non-convexity properties of\nthe underlying optimization problem.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 07:57:18 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1507.08805", "submitter": "Kim Batselier", "authors": "Kim Batselier and Ngai Wong", "title": "A constructive arbitrary-degree Kronecker product decomposition of\n  tensors", "comments": "Rewrote the paper completely and generalized everything to tensors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the tensor Kronecker product singular value decomposition~(TKPSVD)\nthat decomposes a real $k$-way tensor $\\mathcal{A}$ into a linear combination\nof tensor Kronecker products with an arbitrary number of $d$ factors\n$\\mathcal{A} = \\sum_{j=1}^R \\sigma_j\\, \\mathcal{A}^{(d)}_j \\otimes \\cdots\n\\otimes \\mathcal{A}^{(1)}_j$. We generalize the matrix Kronecker product to\ntensors such that each factor $\\mathcal{A}^{(i)}_j$ in the TKPSVD is a $k$-way\ntensor. The algorithm relies on reshaping and permuting the original tensor\ninto a $d$-way tensor, after which a polyadic decomposition with orthogonal\nrank-1 terms is computed. We prove that for many different structured tensors,\nthe Kronecker product factors $\\mathcal{A}^{(1)}_j,\\ldots,\\mathcal{A}^{(d)}_j$\nare guaranteed to inherit this structure. In addition, we introduce the new\nnotion of general symmetric tensors, which includes many different structures\nsuch as symmetric, persymmetric, centrosymmetric, Toeplitz and Hankel tensors.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 09:19:47 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2015 09:37:08 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 12:25:34 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Batselier", "Kim", ""], ["Wong", "Ngai", ""]]}, {"id": "1507.08847", "submitter": "Fei  Guo", "authors": "Jiachen Yanga, Zhiyong Dinga, Fei Guoa, Huogen Wanga, Nick Hughesb", "title": "A novel multivariate performance optimization method based on sparse\n  coding and hyper-predictor learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2015.07.011", "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of optimization multivariate\nperformance measures, and propose a novel algorithm for it. Different from\ntraditional machine learning methods which optimize simple loss functions to\nlearn prediction function, the problem studied in this paper is how to learn\neffective hyper-predictor for a tuple of data points, so that a complex loss\nfunction corresponding to a multivariate performance measure can be minimized.\nWe propose to present the tuple of data points to a tuple of sparse codes via a\ndictionary, and then apply a linear function to compare a sparse code against a\ngive candidate class label. To learn the dictionary, sparse codes, and\nparameter of the linear function, we propose a joint optimization problem. In\nthis problem, the both the reconstruction error and sparsity of sparse code,\nand the upper bound of the complex loss function are minimized. Moreover, the\nupper bound of the loss function is approximated by the sparse codes and the\nlinear function parameter. To optimize this problem, we develop an iterative\nalgorithm based on descent gradient methods to learn the sparse codes and\nhyper-predictor parameter alternately. Experiment results on some benchmark\ndata sets show the advantage of the proposed methods over other\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 12:14:35 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Yanga", "Jiachen", ""], ["Dinga", "Zhiyong", ""], ["Guoa", "Fei", ""], ["Wanga", "Huogen", ""], ["Hughesb", "Nick", ""]]}]