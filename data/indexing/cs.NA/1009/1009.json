[{"id": "1009.0881", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis, Fran\\c{c}ois Glineur", "title": "A Multilevel Approach For Nonnegative Matrix Factorization", "comments": "23 pages, 10 figures. Section 6 added discussing limitations of the\n  method. Accepted in Journal of Computational and Applied Mathematics", "journal-ref": "Journal of Computational and Applied Mathematics 236 (7), pp.\n  1708-1723, 2012", "doi": "10.1016/j.cam.2011.10.002", "report-no": null, "categories": "math.OC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) is the problem of approximating a\nnonnegative matrix with the product of two low-rank nonnegative matrices and\nhas been shown to be particularly useful in many applications, e.g., in text\nmining, image processing, computational biology, etc. In this paper, we explain\nhow algorithms for NMF can be embedded into the framework of multilevel methods\nin order to accelerate their convergence. This technique can be applied in\nsituations where data admit a good approximate representation in a lower\ndimensional space through linear transformations preserving nonnegativity. A\nsimple multilevel strategy is described and is experimentally shown to speed up\nsignificantly three popular NMF algorithms (alternating nonnegative least\nsquares, multiplicative updates and hierarchical alternating least squares) on\nseveral standard image datasets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 22:55:34 GMT"}, {"version": "v2", "created": "Sun, 12 Sep 2010 16:47:01 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2011 00:02:21 GMT"}], "update_date": "2012-08-13", "authors_parsed": [["Gillis", "Nicolas", ""], ["Glineur", "Fran\u00e7ois", ""]]}, {"id": "1009.0938", "submitter": "Pablo Garc\\'ia-Risue\\~no", "authors": "Pablo Garc\\'ia-Risue\\~no, Pablo Echenique", "title": "Linearly scaling direct method for accurately inverting sparse banded\n  matrices", "comments": "24 pages, 5 figures, submitted to J. Comp. Phys", "journal-ref": "Journal of Physics A: Mathematical and Theoretical 45 (2012)\n  065204", "doi": "10.1088/1751-8113/45/6/065204", "report-no": null, "categories": "physics.comp-ph cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems in Computational Physics and Chemistry, one finds a special\nkind of sparse matrices, termed \"banded matrices\". These matrices, which are\ndefined as having non-zero entries only within a given distance from the main\ndiagonal, need often to be inverted in order to solve the associated linear\nsystem of equations. In this work, we introduce a new O(n) algorithm for\nsolving such a system, being n X n the size of the matrix. We produce the\nanalytical recursive expressions that allow to directly obtain the solution, as\nwell as the pseudocode for its computer implementation. Moreover, we review the\ndifferent options for possibly parallelizing the method, we describe the\nextension to deal with matrices that are banded plus a small number of non-zero\nentries outside the band, and we use the same ideas to produce a method for\nobtaining the full inverse matrix. Finally, we show that the New Algorithm is\ncompetitive, both in accuracy and in numerical efficiency, when compared to a\nstandard method based in Gaussian elimination. We do this using sets of large\nrandom banded matrices, as well as the ones that appear when one tries to solve\nthe 1D Poisson equation by finite differences.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 18:05:22 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2011 23:13:54 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Garc\u00eda-Risue\u00f1o", "Pablo", ""], ["Echenique", "Pablo", ""]]}, {"id": "1009.2738", "submitter": "Jan Cieslinski L.", "authors": "Jan L. Cie\\'sli\\'nski, Bogus{\\l}aw Ratkiewicz", "title": "Energy-preserving numerical schemes of high accuracy for one-dimensional\n  Hamiltonian systems", "comments": "15 pages, 6 figures. Presented at the conference \"BIT 50 - Trends in\n  Numerical Computing\" (Lund, 17-20 June 2010)", "journal-ref": "Journal of Physics A: Mathematical and Theoretical 44 (15) (2011)\n  155206 (14 pp)", "doi": "10.1088/1751-8113/44/15/155206", "report-no": null, "categories": "cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a class of non-standard numerical schemes which are modifications\nof the discrete gradient method. They preserve the energy integral exactly (up\nto the round-off error). The considered class contains locally exact discrete\ngradient schemes and integrators of arbitrary high order. In numerical\nexperiments we compare our integrators with some other numerical schemes,\nincluding the standard discrete gradient method, the leap-frog scheme and a\nsymplectic scheme of 4th order. We study the error accumulation for very long\ntime and the conservation of the energy integral.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 19:29:49 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Cie\u015bli\u0144ski", "Jan L.", ""], ["Ratkiewicz", "Bogus\u0142aw", ""]]}, {"id": "1009.4647", "submitter": "Stephen Adler", "authors": "Stephen L. Adler", "title": "Parameterized Adaptive Multidimensional Integration Routines (PAMIR):\n  Localization by Repeated 2^p Subdivision", "comments": "84 pages Latex, figures included; minor changes to program\n  descriptions and tildes added to Eqs. (63) and (65)", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book draft gives the theory of a new method for p dimensional adaptive\nintegration by repeated 2^p subdivision of simplexes and hypercubes. A new\nmethod of constructing high order integration routines for these geometries\npermits adjustable samplings of the integration region controlled by user\nsupplied parameters. An outline of the programs and use instructions are also\nincluded in the draft. The fortran programs are not included, but will be\npublished with this draft as a book.\n", "versions": [{"version": "v1", "created": "Thu, 23 Sep 2010 15:54:20 GMT"}, {"version": "v2", "created": "Thu, 28 Oct 2010 18:21:33 GMT"}], "update_date": "2010-10-29", "authors_parsed": [["Adler", "Stephen L.", ""]]}, {"id": "1009.4677", "submitter": "Ioana Dumitriu", "authors": "Ioana Dumitriu", "title": "Smallest eigenvalue distributions for two classes of $\\beta$-Jacobi\n  ensembles", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC cs.NA math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute the exact and limiting smallest eigenvalue distributions for two\nclasses of $\\beta$-Jacobi ensembles not covered by previous studies. In the\ngeneral $\\beta$ case, these distributions are given by multivariate\nhypergeometric ${}_2F_{1}^{2/\\beta}$ functions, whose behavior can be analyzed\nasymptotically for special values of $\\beta$ which include $\\beta \\in\n2\\mathbb{N}_{+}$ as well as for $\\beta = 1$. Interest in these objects stems\nfrom their connections (in the $\\beta = 1,2$ cases) to principal submatrices of\nHaar-distributed (orthogonal, unitary) matrices appearing in randomized,\ncommunication-optimal, fast, and stable algorithms for eigenvalue computations\n\\cite{DDH07}, \\cite{BDD10}.\n", "versions": [{"version": "v1", "created": "Thu, 23 Sep 2010 18:31:26 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2011 21:53:36 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Dumitriu", "Ioana", ""]]}, {"id": "1009.5055", "submitter": "Zhouchen Lin", "authors": "Zhouchen Lin, Minming Chen, Yi Ma", "title": "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted\n  Low-Rank Matrices", "comments": "Please cite \"Zhouchen Lin, Risheng Liu, and Zhixun Su, Linearized\n  Alternating Direction Method with Adaptive Penalty for Low Rank\n  Representation, NIPS 2011.\" (available at arXiv:1109.0367) instead for a more\n  general method called Linearized Alternating Direction Method This manuscript\n  first appeared as University of Illinois at Urbana-Champaign technical report\n  #UILU-ENG-09-2215 in October 2009 Zhouchen Lin, Risheng Liu, and Zhixun Su,\n  Linearized Alternating Direction Method with Adaptive Penalty for Low Rank\n  Representation, NIPS 2011. (available at http://arxiv.org/abs/1109.0367)", "journal-ref": null, "doi": "10.1016/j.jsb.2012.10.010", "report-no": null, "categories": "math.OC cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes scalable and fast algorithms for solving the Robust PCA\nproblem, namely recovering a low-rank matrix with an unknown fraction of its\nentries being arbitrarily corrupted. This problem arises in many applications,\nsuch as image processing, web data ranking, and bioinformatic data analysis. It\nwas recently shown that under surprisingly broad conditions, the Robust PCA\nproblem can be exactly solved via convex optimization that minimizes a\ncombination of the nuclear norm and the $\\ell^1$-norm . In this paper, we apply\nthe method of augmented Lagrange multipliers (ALM) to solve this convex\nprogram. As the objective function is non-smooth, we show how to extend the\nclassical analysis of ALM to such new objective functions and prove the\noptimality of the proposed algorithms and characterize their convergence rate.\nEmpirically, the proposed new algorithms can be more than five times faster\nthan the previous state-of-the-art algorithms for Robust PCA, such as the\naccelerated proximal gradient (APG) algorithm. Moreover, the new algorithms\nachieve higher precision, yet being less storage/memory demanding. We also show\nthat the ALM technique can be used to solve the (related but somewhat simpler)\nmatrix completion problem and obtain rather promising results too. We further\nprove the necessary and sufficient condition for the inexact ALM to converge\nglobally. Matlab code of all algorithms discussed are available at\nhttp://perception.csl.illinois.edu/matrix-rank/home.html\n", "versions": [{"version": "v1", "created": "Sun, 26 Sep 2010 03:42:27 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2011 04:19:52 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2013 13:42:57 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Lin", "Zhouchen", ""], ["Chen", "Minming", ""], ["Ma", "Yi", ""]]}]