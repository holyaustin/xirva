[{"id": "1109.0545", "submitter": "Jan Verschelde", "authors": "Jan Verschelde and Genady Yoffe", "title": "Quality Up in Polynomial Homotopy Continuation by Multithreaded Path\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA cs.SC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speedup measures how much faster we can solve the same problem using many\ncores. If we can afford to keep the execution time fixed, then quality up\nmeasures how much better the solution will be computed using many cores. In\nthis paper we describe our multithreaded implementation to track one solution\npath defined by a polynomial homotopy. Limiting quality to accuracy and\nconfusing accuracy with precision, we strive to offset the cost of\nmultiprecision arithmetic running multithreaded code on many cores.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 20:06:55 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Verschelde", "Jan", ""], ["Yoffe", "Genady", ""]]}, {"id": "1109.0910", "submitter": "Marcus Mendenhall", "authors": "Marcus H. Mendenhall and Robert A. Weller", "title": "A probability-conserving cross-section biasing mechanism for variance\n  reduction in Monte Carlo particle transport calculations", "comments": null, "journal-ref": null, "doi": "10.1016/j.nima.2011.11.084", "report-no": null, "categories": "physics.comp-ph cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Monte Carlo particle transport codes, it is often important to adjust\nreaction cross sections to reduce the variance of calculations of relatively\nrare events, in a technique known as non-analogous Monte Carlo. We present the\ntheory and sample code for a Geant4 process which allows the cross section of a\nG4VDiscreteProcess to be scaled, while adjusting track weights so as to\nmitigate the effects of altered primary beam depletion induced by the cross\nsection change. This makes it possible to increase the cross section of nuclear\nreactions by factors exceeding 10^4 (in appropriate cases), without distorting\nthe results of energy deposition calculations or coincidence rates. The\nprocedure is also valid for bias factors less than unity, which is useful, for\nexample, in problems that involve computation of particle penetration deep into\na target, such as occurs in atmospheric showers or in shielding.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2011 14:20:58 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2011 15:15:18 GMT"}], "update_date": "2011-12-09", "authors_parsed": [["Mendenhall", "Marcus H.", ""], ["Weller", "Robert A.", ""]]}, {"id": "1109.1027", "submitter": "Ronald Caplan", "authors": "R.M. Caplan and R. Carretero", "title": "A Two-Step High-Order Compact Scheme for the Laplacian Operator and its\n  Implementation in an Explicit Method for Integrating the Nonlinear\n  Schr\u007f\\\"odinger Equation", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and test an easy-to-implement two-step high-order compact (2SHOC)\nscheme for the Laplacian operator and its implementation into an explicit\nfinite-difference scheme for simulating the nonlinear Schr\u007f\\\"odinger equation\n(NLSE). Our method relies on a compact `double-differencing' which is shown to\nbe computationally equivalent to standard fourth-order non-compact schemes.\nThrough numerical simulations of the NLSE using fourth-order Runge-Kutta, we\nconfirm that our scheme shows the desired fourth-order accuracy. A computation\nand storage requirement comparison is made between the 2SHOC scheme and the\nnon-compact equivalent scheme for both the Laplacian operator alone, as well as\nwhen implemented in the NLSE simulations. Stability bounds are also shown in\norder to get maximum efficiency out of the method. We conclude that the modest\nincrease in storage and computation of the 2SHOC schemes are well worth the\nadvantages of having the schemes compact, and their ease of implementation\nmakes their use very useful for practical implementations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2011 22:38:05 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2013 21:51:04 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Caplan", "R. M.", ""], ["Carretero", "R.", ""]]}, {"id": "1109.1342", "submitter": "ZiQiang Shi", "authors": "Ziqiang Shi, Tieran Zheng, Jiqing Han", "title": "Trace Norm Regularized Tensor Classification and Its Online Learning\n  Approaches", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an algorithm to classify tensor data. Our\nmethodology is built on recent studies about matrix classification with the\ntrace norm constrained weight matrix and the tensor trace norm. Similar to\nmatrix classification, the tensor classification is formulated as a convex\noptimization problem which can be solved by using the off-the-shelf accelerated\nproximal gradient (APG) method. However, there are no analytic solutions as the\nmatrix case for the updating of the weight tensors via the proximal gradient.\nTo tackle this problem, the Douglas-Rachford splitting technique and the\nalternating direction method of multipliers (ADM) used in tensor completion are\nadapted to update the weight tensors. Further more, due to the demand of real\napplications, we also propose its online learning approaches. Experiments\ndemonstrate the efficiency of the methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 02:46:40 GMT"}], "update_date": "2011-09-08", "authors_parsed": [["Shi", "Ziqiang", ""], ["Zheng", "Tieran", ""], ["Han", "Jiqing", ""]]}, {"id": "1109.1693", "submitter": "Oded Schwartz", "authors": "Grey Ballard, James Demmel, Olga Holtz, Oded Schwartz", "title": "Graph Expansion and Communication Costs of Fast Matrix Multiplication", "comments": null, "journal-ref": "Proceedings of the 23rd annual symposium on parallelism in\n  algorithms and architectures. ACM, 1-12. 2011 (a shorter conference version)", "doi": "10.1145/1989493.1989495", "report-no": "UCB/EECS-2011-40", "categories": "cs.DS cs.CC cs.DC cs.NA math.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The communication cost of algorithms (also known as I/O-complexity) is shown\nto be closely related to the expansion properties of the corresponding\ncomputation graphs. We demonstrate this on Strassen's and other fast matrix\nmultiplication algorithms, and obtain first lower bounds on their communication\ncosts.\n  In the sequential case, where the processor has a fast memory of size $M$,\ntoo small to store three $n$-by-$n$ matrices, the lower bound on the number of\nwords moved between fast and slow memory is, for many of the matrix\nmultiplication algorithms, $\\Omega((\\frac{n}{\\sqrt M})^{\\omega_0}\\cdot M)$,\nwhere $\\omega_0$ is the exponent in the arithmetic count (e.g., $\\omega_0 = \\lg\n7$ for Strassen, and $\\omega_0 = 3$ for conventional matrix multiplication).\nWith $p$ parallel processors, each with fast memory of size $M$, the lower\nbound is $p$ times smaller.\n  These bounds are attainable both for sequential and for parallel algorithms\nand hence optimal. These bounds can also be attained by many fast algorithms in\nlinear algebra (e.g., algorithms for LU, QR, and solving the Sylvester\nequation).\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 11:36:00 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Ballard", "Grey", ""], ["Demmel", "James", ""], ["Holtz", "Olga", ""], ["Schwartz", "Oded", ""]]}, {"id": "1109.3411", "submitter": "Markus Hartikainen", "authors": "Markus Hartikainen and Vesa Ojalehto", "title": "Demonstrating the Applicability of PAINT to Computationally Expensive\n  Real-life Multiobjective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the applicability of a new PAINT method to speed up iterations\nof interactive methods in multiobjective optimization. As our test case, we\nsolve a computationally expensive non-linear, five-objective problem of\ndesigning and operating a wastewater treatment plant. The PAINT method\ninterpolates between a given set of Pareto optimal outcomes and constructs a\ncomputationally inexpensive mixed integer linear surrogate problem for the\noriginal problem. We develop an IND-NIMBUS(R) PAINT module to combine the\ninteractive NIMBUS method and the PAINT method and to find a preferred solution\nto the original problem. With the PAINT method, the solution process with the\nNIMBUS method take a comparatively short time even though the original problem\nis computationally expensive.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 17:39:56 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Hartikainen", "Markus", ""], ["Ojalehto", "Vesa", ""]]}, {"id": "1109.3739", "submitter": "Aydin Buluc", "authors": "Aydin Buluc and John Gilbert", "title": "Parallel Sparse Matrix-Matrix Multiplication and Indexing:\n  Implementation and Experiments", "comments": null, "journal-ref": "SIAM J. Sci. Comput., 34(4), 170 - 191, 2012", "doi": "10.1137/110848244", "report-no": null, "categories": "cs.DC cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized sparse matrix-matrix multiplication (or SpGEMM) is a key\nprimitive for many high performance graph algorithms as well as for some linear\nsolvers, such as algebraic multigrid. Here we show that SpGEMM also yields\nefficient algorithms for general sparse-matrix indexing in distributed memory,\nprovided that the underlying SpGEMM implementation is sufficiently flexible and\nscalable. We demonstrate that our parallel SpGEMM methods, which use\ntwo-dimensional block data distributions with serial hypersparse kernels, are\nindeed highly flexible, scalable, and memory-efficient in the general case.\nThis algorithm is the first to yield increasing speedup on an unbounded number\nof processors; our experiments show scaling up to thousands of processors in a\nvariety of test scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 23:25:28 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2012 20:41:41 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Buluc", "Aydin", ""], ["Gilbert", "John", ""]]}, {"id": "1109.5100", "submitter": "Mikhail Botchev (Bochev)", "authors": "Mikhail A. Botchev", "title": "Block Krylov subspace exact time integration of linear ODE systems. Part\n  1: algorithm description", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a time-exact Krylov-subspace-based method for solving linear ODE\n(ordinary differential equation) systems of the form $y'=-Ay + g(t)$, where\n$y(t)$ is the unknown function. The method consists of two stages. The first\nstage is an accurate polynomial approximation of the source term $g(t)$,\nconstructed with the help of the truncated SVD (singular value decomposition).\nThe second stage is a special residual-based block Krylov subspace method.\n  The accuracy of the method is only restricted by the accuracy of the\npolynomial approximation and by the error of the block Krylov process. Since\nboth errors can, in principle, be made arbitrarily small, this yields, at some\ncosts, a time-exact method.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2011 14:57:51 GMT"}], "update_date": "2011-09-26", "authors_parsed": [["Botchev", "Mikhail A.", ""]]}, {"id": "1109.5752", "submitter": "Arash Fahim", "authors": "Erhan Bayraktar and Arash Fahim", "title": "A Stochastic Approximation for Fully Nonlinear Free Boundary Parabolic\n  Problems", "comments": "To appear in the journal \"Numerical Methods for Partial Differential\n  Equations\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stochastic numerical method for solving fully non-linear free\nboundary problems of parabolic type and provide a rate of convergence under\nreasonable conditions on the non-linearity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 00:20:30 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 20:03:29 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2013 15:13:13 GMT"}, {"version": "v4", "created": "Fri, 8 Nov 2013 01:29:27 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Bayraktar", "Erhan", ""], ["Fahim", "Arash", ""]]}, {"id": "1109.5913", "submitter": "Pierre Kerfriden", "authors": "Olivier Allix (LMT), Pierre Kerfriden (LMT), Pierre Gosselet (LMT)", "title": "On the control of the load increments for a proper description of\n  multiple delamination in a domain decomposition framework", "comments": null, "journal-ref": "International Journal for Numerical Methods in Engineering 83, 11\n  (2010) 1518-1540", "doi": "10.1002/nme.2884", "report-no": null, "categories": "cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In quasi-static nonlinear time-dependent analysis, the choice of the time\ndiscretization is a complex issue. The most basic strategy consists in\ndetermining a value of the load increment that ensures the convergence of the\nsolution with respect to time on the base of preliminary simulations. In more\nadvanced applications, the load increments can be controlled for instance by\nprescribing the number of iterations of the nonlinear resolution procedure, or\nby using an arc-length algorithm. These techniques usually introduce a\nparameter whose correct value is not easy to obtain. In this paper, an\nalternative procedure is proposed. It is based on the continuous control of the\nresidual of the reference problem over time, whose measure is easy to\ninterpret. This idea is applied in the framework of a multiscale domain\ndecomposition strategy in order to perform 3D delamination analysis.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 14:47:03 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Allix", "Olivier", "", "LMT"], ["Kerfriden", "Pierre", "", "LMT"], ["Gosselet", "Pierre", "", "LMT"]]}, {"id": "1109.5981", "submitter": "Xiangrui Meng", "authors": "Xiangrui Meng and Michael A. Saunders, and Michael W. Mahoney", "title": "LSRN: A Parallel Iterative Solver for Strongly Over- or Under-Determined\n  Systems", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a parallel iterative least squares solver named \\texttt{LSRN}\nthat is based on random normal projection. \\texttt{LSRN} computes the\nmin-length solution to $\\min_{x \\in \\mathbb{R}^n} \\|A x - b\\|_2$, where $A \\in\n\\mathbb{R}^{m \\times n}$ with $m \\gg n$ or $m \\ll n$, and where $A$ may be\nrank-deficient. Tikhonov regularization may also be included. Since $A$ is only\ninvolved in matrix-matrix and matrix-vector multiplications, it can be a dense\nor sparse matrix or a linear operator, and \\texttt{LSRN} automatically speeds\nup when $A$ is sparse or a fast linear operator. The preconditioning phase\nconsists of a random normal projection, which is embarrassingly parallel, and a\nsingular value decomposition of size $\\lceil \\gamma \\min(m,n) \\rceil \\times\n\\min(m,n)$, where $\\gamma$ is moderately larger than 1, e.g., $\\gamma = 2$. We\nprove that the preconditioned system is well-conditioned, with a strong\nconcentration result on the extreme singular values, and hence that the number\nof iterations is fully predictable when we apply LSQR or the Chebyshev\nsemi-iterative method. As we demonstrate, the Chebyshev method is particularly\nefficient for solving large problems on clusters with high communication cost.\nNumerical results demonstrate that on a shared-memory machine, \\texttt{LSRN}\noutperforms LAPACK's DGELSD on large dense problems, and MATLAB's backslash\n(SuiteSparseQR) on sparse problems. Further experiments demonstrate that\n\\texttt{LSRN} scales well on an Amazon Elastic Compute Cloud cluster.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 18:06:44 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2012 20:39:18 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Meng", "Xiangrui", ""], ["Saunders", "Michael A.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1109.5993", "submitter": "Jakob Lemvig", "authors": "Gitta Kutyniok, Jakob Lemvig, and Wang-Q Lim", "title": "Optimally sparse approximations of 3D functions by compactly supported\n  shearlet frames", "comments": "56 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study efficient and reliable methods of capturing and sparsely\nrepresenting anisotropic structures in 3D data. As a model class for\nmultidimensional data with anisotropic features, we introduce generalized\nthree-dimensional cartoon-like images. This function class will have two\nsmoothness parameters: one parameter \\beta controlling classical smoothness and\none parameter \\alpha controlling anisotropic smoothness. The class then\nconsists of piecewise C^\\beta-smooth functions with discontinuities on a\npiecewise C^\\alpha-smooth surface. We introduce a pyramid-adapted, hybrid\nshearlet system for the three-dimensional setting and construct frames for\nL^2(R^3) with this particular shearlet structure. For the smoothness range\n1<\\alpha =< \\beta =< 2 we show that pyramid-adapted shearlet systems provide a\nnearly optimally sparse approximation rate within the generalized cartoon-like\nimage model class measured by means of non-linear N-term approximations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 19:20:02 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2012 13:56:10 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Kutyniok", "Gitta", ""], ["Lemvig", "Jakob", ""], ["Lim", "Wang-Q", ""]]}, {"id": "1109.6279", "submitter": "Michael Sagraloff", "authors": "Michael Sagraloff", "title": "When Newton meets Descartes: A Simple and Fast Algorithm to Isolate the\n  Real Roots of a Polynomial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm denoted DSC2 to isolate the real roots of a\nunivariate square-free polynomial f with integer coefficients. The algorithm\niteratively subdivides an initial interval which is known to contain all real\nroots of f. The main novelty of our approach is that we combine Descartes' Rule\nof Signs and Newton iteration. More precisely, instead of using a fixed\nsubdivision strategy such as bisection in each iteration, a Newton step based\non the number of sign variations for an actual interval is considered, and,\nonly if the Newton step fails, we fall back to bisection. Following this\napproach, our analysis shows that, for most iterations, we can achieve\nquadratic convergence towards the real roots. In terms of complexity, our\nmethod induces a recursion tree of almost optimal size O(nlog(n tau)), where n\ndenotes the degree of the polynomial and tau the bitsize of its coefficients.\nThe latter bound constitutes an improvement by a factor of tau upon all\nexisting subdivision methods for the task of isolating the real roots. In\naddition, we provide a bit complexity analysis showing that DSC2 needs only\n\\tilde{O}(n^3tau) bit operations to isolate all real roots of f. This matches\nthe best bound known for this fundamental problem. However, in comparison to\nthe much more involved algorithms by Pan and Sch\\\"onhage (for the task of\nisolating all complex roots) which achieve the same bit complexity, DSC2\nfocuses on real root isolation, is very easy to access and easy to implement.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 17:39:07 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Sagraloff", "Michael", ""]]}]