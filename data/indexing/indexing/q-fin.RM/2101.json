[{"id": "2101.00327", "submitter": "Min Shu", "authors": "Ruiqiang Song, Min Shu, Wei Zhu", "title": "The 2020 Global Stock Market Crash: Endogenous or Exogenous?", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.GN q-fin.EC q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting on February 20, 2020, the global stock markets began to suffer the\nworst decline since the Great Recession in 2008, and the COVID-19 has been\nwidely blamed on the stock market crashes. In this study, we applied the\nlog-periodic power law singularity (LPPLS) methodology based on multilevel time\nseries to unravel the underlying mechanisms of the 2020 global stock market\ncrash by analyzing the trajectories of 10 major stock market indexes from both\ndeveloped and emergent stock markets, including the S&P 500, DJIA, NASDAQ,\nFTSE, DAX, NIKKEI, CSI 300, HSI, BSESN, and BOVESPA. In order to effectively\ndistinguish between endogenous crash and exogenous crash, we proposed using the\nLPPLS confidence indicator as a classification proxy. The results show that the\napparent LPPLS bubble patterns of the super-exponential increase, corrected by\nthe accelerating logarithm-periodic oscillations, have indeed presented in the\nprice trajectories of the seven indexes: S&P 500, DJIA, NASDAQ, DAX, CSI 300,\nBSESN, and BOVESPA, indicating that the large positive bubbles have formed\nendogenously prior to the 2020 stock market crash, and the subsequent crashes\nfor the seven indexes are endogenous, stemming from the increasingly systemic\ninstability of the stock markets, while the well-known external shocks such as\nthe COVID-19 pandemic etc. only acted as sparks during the 2020 global stock\nmarket crash. In contrast, the obvious signatures of the LPPLS model have not\nbeen observed in the price trajectories of the three remaining indexes: FTSE,\nNIKKEI, and HSI, signifying that the crashes in these three indexes are\nexogenous, stemming from external shocks. The novel classification method of\ncrash types proposed in this study can also be used to analyze regime changes\nof any price trajectories in global financial markets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 22:31:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Song", "Ruiqiang", ""], ["Shu", "Min", ""], ["Zhu", "Wei", ""]]}, {"id": "2101.01261", "submitter": "Bin Zou", "authors": "Carol Alexander, Jun Deng, Bin Zou", "title": "Optimal Hedging with Margin Constraints and Default Aversion and its\n  Application to Bitcoin Perpetual Futures", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF q-fin.PM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider a futures hedging problem subject to a budget constraint that\nlimits the ability of a hedger with default aversion to meet margin\nrequirements. We derive a semi-closed form for an optimal hedging strategy with\ndual objectives -- to minimize both the variance of the hedged portfolio and\nthe probability of forced liquidations due to margin calls. An empirical\nanalysis of bitcoin shows that the optimal strategy not only achieves superior\nhedge effectiveness, but also reduces the probability of forced liquidations to\nan acceptable level. We also compare how the hedger's default aversion impacts\nthe performance of optimal hedging based on minute-level data across major\nbitcoin spot and perpetual futures markets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 22:24:56 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Alexander", "Carol", ""], ["Deng", "Jun", ""], ["Zou", "Bin", ""]]}, {"id": "2101.02110", "submitter": "Thierry Roncalli", "authors": "Thierry Roncalli, Fatma Karray-Meziou, Fran\\c{c}ois Pan, Margaux\n  Regnault", "title": "Liquidity Stress Testing in Asset Management -- Part 1. Modeling the\n  Liability Liquidity Risk", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.14893.72165", "report-no": null, "categories": "q-fin.RM q-fin.CP stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article is part of a comprehensive research project on liquidity risk in\nasset management, which can be divided into three dimensions. The first\ndimension covers liability liquidity risk (or funding liquidity) modeling, the\nsecond dimension focuses on asset liquidity risk (or market liquidity)\nmodeling, and the third dimension considers asset-liability liquidity risk\nmanagement (or asset-liability matching). The purpose of this research is to\npropose a methodological and practical framework in order to perform liquidity\nstress testing programs, which comply with regulatory guidelines (ESMA, 2019)\nand are useful for fund managers. The review of the academic literature and\nprofessional research studies shows that there is a lack of standardized and\nanalytical models. The aim of this research project is then to fill the gap\nwith the goal to develop mathematical and statistical approaches, and provide\nappropriate answers.\n  In this first part that focuses on liability liquidity risk modeling, we\npropose several statistical models for estimating redemption shocks. The\nhistorical approach must be complemented by an analytical approach based on\nzero-inflated models if we want to understand the true parameters that\ninfluence the redemption shocks. Moreover, we must also distinguish aggregate\npopulation models and individual-based models if we want to develop behavioral\napproaches. Once these different statistical models are calibrated, the second\nbig issue is the risk measure to assess normal and stressed redemption shocks.\nFinally, the last issue is to develop a factor model that can translate stress\nscenarios on market risk factors into stress scenarios on fund liabilities.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:08:27 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Roncalli", "Thierry", ""], ["Karray-Meziou", "Fatma", ""], ["Pan", "Fran\u00e7ois", ""], ["Regnault", "Margaux", ""]]}, {"id": "2101.02287", "submitter": "Arash Mohammadi", "authors": "Farnoush Ronaghi, Mohammad Salimibeni, Farnoosh Naderkhani, and Arash\n  Mohammadi", "title": "COVID19-HPSMP: COVID-19 Adopted Hybrid and Parallel Deep Information\n  Fusion Framework for Stock Price Movement Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG eess.SP q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel of coronavirus (COVID-19) has suddenly and abruptly changed the\nworld as we knew at the start of the 3rd decade of the 21st century.\nParticularly, COVID-19 pandemic has negatively affected financial econometrics\nand stock markets across the globe. Artificial Intelligence (AI) and Machine\nLearning (ML)-based prediction models, especially Deep Neural Network (DNN)\narchitectures, have the potential to act as a key enabling factor to reduce the\nadverse effects of the COVID-19 pandemic and future possible ones on financial\nmarkets. In this regard, first, a unique COVID-19 related PRIce MOvement\nprediction (COVID19 PRIMO) dataset is introduced in this paper, which\nincorporates effects of social media trends related to COVID-19 on stock market\nprice movements. Afterwards, a novel hybrid and parallel DNN-based framework is\nproposed that integrates different and diversified learning architectures.\nReferred to as the COVID-19 adopted Hybrid and Parallel deep fusion framework\nfor Stock price Movement Prediction (COVID19-HPSMP), innovative fusion\nstrategies are used to combine scattered social media news related to COVID-19\nwith historical mark data. The proposed COVID19-HPSMP consists of two parallel\npaths (hence hybrid), one based on Convolutional Neural Network (CNN) with\nLocal/Global Attention modules, and one integrated CNN and Bi-directional Long\nShort term Memory (BLSTM) path. The two parallel paths are followed by a\nmultilayer fusion layer acting as a fusion centre that combines localized\nfeatures. Performance evaluations are performed based on the introduced COVID19\nPRIMO dataset illustrating superior performance of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 15:55:19 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 17:59:17 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ronaghi", "Farnoush", ""], ["Salimibeni", "Mohammad", ""], ["Naderkhani", "Farnoosh", ""], ["Mohammadi", "Arash", ""]]}, {"id": "2101.02760", "submitter": "Peter  Forsyth", "authors": "Peter A. Forsyth and Kenneth R. Vetzal and Graham Westmacott", "title": "Optimal control of the decumulation of a retirement portfolio with\n  variable spending and dynamic asset allocation", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PM q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We extend the Annually Recalculated Virtual Annuity (ARVA) spending rule for\nretirement savings decumulation to include a cap and a floor on withdrawals.\nWith a minimum withdrawal constraint, the ARVA strategy runs the risk of\ndepleting the investment portfolio. We determine the dynamic asset allocation\nstrategy which maximizes a weighted combination of expected total withdrawals\n(EW) and expected shortfall (ES), defined as the average of the worst five per\ncent of the outcomes of real terminal wealth. We compare the performance of our\ndynamic strategy to simpler alternatives which maintain constant asset\nallocation weights over time accompanied by either our same modified ARVA\nspending rule or withdrawals that are constant over time in real terms. Tests\nare carried out using both a parametric model of historical asset returns as\nwell as bootstrap resampling of historical data. Consistent with previous\nliterature that has used different measures of reward and risk than EW and ES,\nwe find that allowing some variability in withdrawals leads to large\nimprovements in efficiency. However, unlike the prior literature, we also\ndemonstrate that further significant enhancements are possible through\nincorporating a dynamic asset allocation strategy rather than simply keeping\nasset allocation weights constant throughout retirement.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 20:50:30 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Forsyth", "Peter A.", ""], ["Vetzal", "Kenneth R.", ""], ["Westmacott", "Graham", ""]]}, {"id": "2101.03625", "submitter": "Min Shu", "authors": "Min Shu, Ruiqiang Song, Wei Zhu", "title": "The 'COVID' Crash of the 2020 U.S. Stock Market", "comments": "19 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2101.00327", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.GN q-fin.EC q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employed the log-periodic power law singularity (LPPLS) methodology to\nsystematically investigate the 2020 stock market crash in the U.S. equities\nsectors with different levels of total market capitalizations through four\nmajor U.S. stock market indexes, including the Wilshire 5000 Total Market\nindex, the S&P 500 index, the S&P MidCap 400 index, and the Russell 2000 index,\nrepresenting the stocks overall, the large capitalization stocks, the middle\ncapitalization stocks and the small capitalization stocks, respectively. During\nthe 2020 U.S. stock market crash, all four indexes lost more than a third of\ntheir values within five weeks, while both the middle capitalization stocks and\nthe small capitalization stocks have suffered much greater losses than the\nlarge capitalization stocks and stocks overall. Our results indicate that the\nprice trajectories of these four stock market indexes prior to the 2020 stock\nmarket crash have clearly featured the obvious LPPLS bubble pattern and were\nindeed in a positive bubble regime. Contrary to the popular belief that the\nCOVID-19 led to the 2020 stock market crash, the 2020 U.S. stock market crash\nwas endogenous, stemming from the increasingly systemic instability of the\nstock market itself. We also performed the complementary post-mortem analysis\nof the 2020 U.S. stock market crash. Our analyses indicate that the 2020 U.S.\nstock market crash originated from a bubble which began to form as early as\nSeptember 2018; and the bubbles in stocks with different levels of total market\ncapitalizations have significantly different starting time profiles. This study\nnot only sheds new light on the making of the 2020 U.S. stock market crash but\nalso creates a novel pipeline for future real-time crash detection and\nmechanism dissection of any financial market and/or economic index.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 20:48:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Shu", "Min", ""], ["Song", "Ruiqiang", ""], ["Zhu", "Wei", ""]]}, {"id": "2101.03954", "submitter": "Bin Zou", "authors": "Yang Shen and Bin Zou", "title": "Mean-Variance Investment and Risk Control Strategies -- A\n  Time-Consistent Approach via A Forward Auxiliary Process", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC q-fin.MF q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider an optimal investment and risk control problem for an insurer\nunder the mean-variance (MV) criterion. By introducing a deterministic\nauxiliary process defined forward in time, we formulate an alternative\ntime-consistent problem related to the original MV problem, and obtain the\noptimal strategy and the value function to the new problem in closed-form. We\ncompare our formulation and optimal strategy to those under the precommitment\nand game-theoretic framework. Numerical studies show that, when the financial\nmarket is negatively correlated with the risk process, optimal investment may\ninvolve short selling the risky asset and, if that happens, a less risk averse\ninsurer short sells more risky asset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:11:07 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Shen", "Yang", ""], ["Zou", "Bin", ""]]}, {"id": "2101.04975", "submitter": "Matteo Brachetta", "authors": "Matteo Brachetta and Claudia Ceci", "title": "Optimal reinsurance problem under fixed cost and exponential preferences", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF math.OC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an optimal reinsurance problem for an insurance company facing\na constant fixed cost when the reinsurance contract is signed. The insurer\nneeds to optimally choose both the starting time of the reinsurance contract\nand the retention level in order to maximize the expected utility of terminal\nwealth. This leads to a mixed optimal control/optimal stopping time problem,\nwhich is solved by a two-step procedure: first considering the pure reinsurance\nstochastic control problem and next discussing a time-inhomogeneous optimal\nstopping problem with discontinuous reward. Using the classical\nCram\\'er-Lundberg approximation risk model, we prove that the optimal strategy\nis deterministic and depends on the model parameters. In particular, we show\nthat there exists a maximum fixed cost that the insurer is willing to pay for\nthe contract activation. Finally, we provide some economical interpretations\nand numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:07:36 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Brachetta", "Matteo", ""], ["Ceci", "Claudia", ""]]}, {"id": "2101.06077", "submitter": "Simon Hochgerner", "authors": "Florian Gach, Simon Hochgerner", "title": "Estimation of future discretionary benefits in traditional life\n  insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of traditional life insurance, the future discretionary\nbenefits ($FDB$), which are a central item for Solvency~II reporting, are\ngenerally calculated by computationally expensive Monte Carlo algorithms. We\nderive analytic formulas for lower and upper bounds for the $FDB$. This yields\nan estimation interval for the $FDB$, and the average of lower and upper bound\nis a simple estimator. These formulae are designed for real world applications,\nand we compare the results to publicly available reporting data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:52:57 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 13:48:56 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gach", "Florian", ""], ["Hochgerner", "Simon", ""]]}, {"id": "2101.06690", "submitter": "Selin \\\"Ozen", "authors": "Selin \\\"Ozen and \\c{S}ule \\c{S}ahin", "title": "A Two-Population Mortality Model to Assess Longevity Basis Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Index-based hedging solutions are used to transfer the longevity risk to the\ncapital markets. However, mismatches between the liability of the hedger and\nthe hedging instrument cause longevity basis risk. Therefore, an appropriate\ntwo-population model to measure and assess the longevity basis risk is\nrequired. In this paper, we aim to construct a two-population mortality model\nto provide an effective hedge against the longevity basis risk. The reference\npopulation is modelled by using the Lee-Carter model with the renewal process\nand exponential jumps proposed by \\\"Ozen and \\c{S}ahin (2020) and the dynamics\nof the book population are specified. The analysis based on the UK mortality\ndata indicates that the proposed model for the reference population and the\ncommon age effect model for the book population provide a better fit compared\nto the other models considered in the paper. Different two-population models\nare used to investigate the impact of the sampling risk on the index-based\nhedge as well as to analyse the risk reduction regarding hedge effectiveness.\nThe results show that the proposed model provides a significant risk reduction\nwhen mortality jumps and the sampling risk are taken into account.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 15:22:59 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["\u00d6zen", "Selin", ""], ["\u015eahin", "\u015eule", ""]]}, {"id": "2101.08559", "submitter": "Victor Olkhov", "authors": "Victor Olkhov", "title": "To VaR, or Not to VaR, That is the Question", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC q-fin.GN q-fin.PM q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the value-at-risk (VaR) concept and assesses the\nfinancial adequacy of the price probability determined by frequency of trades\nat price p. We take the price definition as the ratio of executed trade value\nto volume and show that it leads to price statistical moments, which differ\nfrom those, generated by frequency price probability. We derive the price n th\nstatistical moments as ratio of n th statistical moments of the value and the\nvolume of executed transactions. We state that the price probability determined\nby frequency of trades at price p does not describe probability of executed\ntrade prices and VaR based on frequency price probability may be origin for\nunexpected and excessive losses. We explain the need to replace frequency price\nprobability by frequency probabilities of the value and the volume of executed\ntransactions and derive price characteristic function. After 50 years of the\nVaR usage main problems of the VaR concept are still open. We believe that VaR\ncommitment to forecast the price probability for the time horizon T seems to be\none of the most tough and expensive puzzle of modern finance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 11:32:30 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Olkhov", "Victor", ""]]}, {"id": "2101.08964", "submitter": "Petar Jevtic", "authors": "Petar Jevtic and Nicolas Lanchier", "title": "Probabilistic Framework For Loss Distribution Of Smart Contract Risk", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Smart contract risk can be defined as a financial risk of loss due to cyber\nattacks on or contagious failures of smart contracts. Its quantification is of\nparamount importance to technology platform providers as well as companies and\nindividuals when considering the deployment of this new technology. That is\nwhy, as our primary contribution, we propose a structural framework of\naggregate loss distribution for smart contract risk under the assumption of a\ntree-stars graph topology representing the network of interactions among smart\ncontracts and their users. Up to our knowledge, there exist no theoretical\nframeworks or models of an aggregate loss distribution for smart contracts in\nthis setting. To achieve our goal, we contextualize the problem in the\nprobabilistic graph-theoretical framework using bond percolation models. We\nassume that the smart contract network topology is represented by a random tree\ngraph of finite size, and that each smart contract is the center of a {random}\nstar graph whose leaves represent the users of the smart contract. We allow for\nheterogeneous loss topology superimposed on this smart contract and user\ntopology and provide analytical results and instructive numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 06:24:08 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Jevtic", "Petar", ""], ["Lanchier", "Nicolas", ""]]}, {"id": "2101.10293", "submitter": "Nikolaos Athanasios Anagnostopoulos", "authors": "Nikolaos Athanasios Anagnostopoulos", "title": "The Role of Cost in the Integration of Security Features in Integrated\n  Circuits for Smart Cards", "comments": "Submission to Research Topics in University of Twente under the\n  auspices of the EIT ICT Labs Master School in the academic year 2013-14", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.CR econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This essay investigates the role of cost in the development and production of\nsecure integrated circuits. Initially, I make a small introduction on hardware\nattacks on smart cards and some of the reasons behind them. Subsequently, I\nintroduce the production phases of chips that are integrated to smart cards and\ntry to identify the costs affecting each one of them. I proceed to identify how\nadding security features on such integrated circuits may affect the costs of\ntheir development and production. I then make a more thorough investigation on\nthe costs of developing a hardware attack for such chips and try to estimate\nthe potential damages and losses of such an attack. I also go on to examine\npotential ways of reducing the cost of production for secure chips, while\nidentifying the difficulties in adopting them.\n  This essay ends with the conclusion that adding security features to chips\nmeant to be used for secure applications is well worth it, because the costs of\ndeveloping attacks are of comparable amounts to the costs of developing and\nproducing a chip and the potential damages and losses caused by such attacks\ncan be way higher than these costs. Therefore, although the production and\ndevelopment of integrated circuits come at a certain cost and security\nintroduces further additional costs, security is inherently unavoidable in such\nchips. Finally, I additionally identify that security is an evolving concept\nand does not aim to make a chip totally impenetrable, as this may be\nimpossible, but to lower the potential risks, including that of being\ncompromised, to acceptable levels. Thus, a balance needs be found between the\nlevel of security and the levels of cost and risk.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:22:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Anagnostopoulos", "Nikolaos Athanasios", ""]]}, {"id": "2101.10635", "submitter": "Thierry Roncalli", "authors": "Th\\'eo Roncalli, Th\\'eo Le Guenedal, Fr\\'ed\\'eric Lepetit, Thierry\n  Roncalli, Takaya Sekine", "title": "The Market Measure of Carbon Risk and its Impact on the Minimum Variance\n  Portfolio", "comments": "15 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:2008.13198", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM econ.GN q-fin.EC q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Like ESG investing, climate change is an important concern for asset managers\nand owners, and a new challenge for portfolio construction. Until now,\ninvestors have mainly measured carbon risk using fundamental approaches, such\nas with carbon intensity metrics. Nevertheless, it has not been proven that\nasset prices are directly impacted by these fundamental-based measures. In this\npaper, we focus on another approach, which consists in measuring the\nsensitivity of stock prices with respect to a carbon risk factor. In our\nopinion, carbon betas are market-based measures that are complementary to\ncarbon intensities or fundamental-based measures when managing investment\nportfolios, because carbon betas may be viewed as an extension or\nforward-looking measure of the current carbon footprint. In particular, we show\nhow this new metric can be used to build minimum variance strategies and how\nthey impact their portfolio construction.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 08:52:15 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Roncalli", "Th\u00e9o", ""], ["Guenedal", "Th\u00e9o Le", ""], ["Lepetit", "Fr\u00e9d\u00e9ric", ""], ["Roncalli", "Thierry", ""], ["Sekine", "Takaya", ""]]}, {"id": "2101.11590", "submitter": "Mark Kiermayer", "authors": "Mark Kiermayer", "title": "Modeling surrender risk in life insurance: theoretical and experimental\n  insight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrender poses one of the major risks to life insurance and a sound modeling\nof its true probability has direct implication on the risk capital demanded by\nthe Solvency II directive. We add to the existing literature by performing\nextensive experiments that present highly practical results for various\nmodeling approaches, including XGBoost and neural networks. Further, we detect\nshortcomings of prevalent model assessments, which are in essence based on a\nconfusion matrix. Our results indicate that accurate label predictions and a\nsound modeling of the true probability can be opposing objectives. We\nillustrate this with the example of resampling. While resampling is capable of\nimproving label prediction in rare event settings, such as surrender, and thus\nis commonly applied, we show theoretically and numerically that models trained\non resampled data predict significantly biased event probabilities. Following a\nprobabilistic perspective on surrender, we further propose time-dependent\nconfidence bands on predicted mean surrender rates as a complementary\nassessment and demonstrate its benefit. This evaluation takes a very practical,\ngoing concern perspective, which respects that the composition of a portfolio\nmight change over time.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:36:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kiermayer", "Mark", ""]]}, {"id": "2101.12262", "submitter": "Takaaki Koike", "authors": "Takaaki Koike, Shogo Kato, Marius Hofert", "title": "Tail concordance measures: A fair assessment of tail dependence", "comments": "42 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of measures of bivariate tail dependence called tail concordance\nmeasures (TCMs) is proposed, which is defined as the limit of a measure of\nconcordance of the underlying copula restricted to the tail region of interest.\nTCMs captures the extremal relationship between random variables not only along\nthe diagonal but also along all angles weighted by a tail generating measure.\nAxioms of tail dependence measures are introduced, and TCMs are shown to\ncharacterize linear tail dependence measures. The infimum and supremum of TCMs\nover all generating measures are considered to investigate the issue of under-\nand overestimation of the degree of extreme co-movements. The infimum is shown\nto be attained by the classical tail dependence coefficient, and thus the\nclassical notion always underestimates the degree of tail dependence. A formula\nfor the supremum TCM is derived and shown to overestimate the degree of extreme\nco-movements. Estimators of the proposed measures are studied, and their\nperformance is demonstrated in numerical experiments. For a fair assessment of\ntail dependence and stability of the estimation under small sample sizes, TCMs\nweighted over all angles are suggested, with tail Spearman's rho and tail\nGini's gamma being interesting novel special cases of TCMs.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 20:17:49 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 06:20:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Koike", "Takaaki", ""], ["Kato", "Shogo", ""], ["Hofert", "Marius", ""]]}, {"id": "2101.12402", "submitter": "Yiqiang Zhao", "authors": "Suman Thapa and Yiqiang Q. Zhao", "title": "Estimating value at risk and conditional tail expectation for extreme\n  and aggregate risks", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate risk measures such as value at risk (VaR) and\nthe conditional tail expectation (CTE) of the extreme (maximum and minimum) and\nthe aggregate (total) of two dependent risks. In finance, insurance and the\nother fields, when people invest their money in two or more dependent or\nindependent markets, it is very important to know the extreme and total risk\nbefore the investment. To find these risk measures for dependent cases is quite\nchallenging, which has not been reported in the literature to the best of our\nknowledge. We use the FGM copula for modelling the dependence as it is\nrelatively simple for computational purposes and has empirical successes. The\nmarginal of the risks are considered as exponential and pareto, separately, for\nthe case of extreme risk and as exponential for the case of the total risk. The\neffect of the degree of dependency on the VaR and CTE of the extreme and total\nrisks is analyzed. We also make comparisons for the dependent and independent\nrisks. Moreover, we propose a new risk measure called median of tail (MoT) and\ninvestigate MoT for the extreme and aggregate dependent risks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 04:45:15 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Thapa", "Suman", ""], ["Zhao", "Yiqiang Q.", ""]]}]