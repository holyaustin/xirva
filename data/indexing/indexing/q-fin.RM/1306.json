[{"id": "1306.0887", "submitter": "Damiano  Brigo", "authors": "Damiano Brigo, Jan-Frederik Mai, Matthias Scherer", "title": "Consistent iterated simulation of multi-variate default times: a\n  Markovian indicators characterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate under which conditions a single simulation of joint default\ntimes at a final time horizon can be decomposed into a set of simulations of\njoint defaults on subsequent adjacent sub-periods leading to that final\nhorizon. Besides the theoretical interest, this is also a practical problem as\npart of the industry has been working under the misleading assumption that the\ntwo approaches are equivalent for practical purposes. As a reasonable trade-off\nbetween realistic stylized facts, practical demands, and mathematical\ntractability, we propose models leading to a Markovian multi-variate\nsurvival--indicator process, and we investigate two instances of static models\nfor the vector of default times from the statistical literature that fall into\nthis class. On the one hand, the \"looping default\" case is known to be equipped\nwith this property, and we point out that it coincides with the classical\n\"Freund distribution\" in the bivariate case. On the other hand, if all\nsub-vectors of the survival indicator process are Markovian, this constitutes a\nnew characterization of the Marshall--Olkin distribution, and hence of\nmulti-variate lack-of-memory. A paramount property of the resulting model is\nstability of the type of multi-variate distribution with respect to elimination\nor insertion of a new marginal component with marginal distribution from the\nsame family. The practical implications of this \"nested margining\" property are\nenormous. To implement this distribution we present an efficient and unbiased\nsimulation algorithm based on the L\\'evy-frailty construction. We highlight\ndifferent pitfalls in the simulation of dependent default times and examine,\nwithin a numerical case study, the effect of inadequate simulation practices.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 19:42:56 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2013 14:03:31 GMT"}, {"version": "v3", "created": "Thu, 1 May 2014 16:46:57 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Brigo", "Damiano", ""], ["Mai", "Jan-Frederik", ""], ["Scherer", "Matthias", ""]]}, {"id": "1306.0966", "submitter": "Novriana Sumarti", "authors": "Novriana Sumarti and Rafki Hidayat", "title": "A Financial Risk Analysis: Does the 2008 Financial Crisis Give Impact on\n  Weekends Returns of the U.S. Movie Box Office?", "comments": "6 pages, 10 figures", "journal-ref": "IAENG International Journal of Applied Mathematics, 41 vol. 4,\n  2011 pp: 343-348", "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Financial Crisis of 2008 is a worldwide financial crisis causing a\nworldwide economic decline that is the most severe since the 1930s. According\nto the International Monetary Fund (IMF), the global financial crisis gave\nimpact on USD 3.4 trillion losses from financial institutions around the world\nbetween 2007 and 2010. Does the crisis give impact on the returns of the U.S.\nmovie Box Office? It will be answered by doing an analysis on the financial\nrisk model based on Extreme Value Theory (EVT) and calculations of Value at\nRisk (VaR) and Expected Shortfall (ES). The values of VaR and ES from 2\nperiods, 1982 to 1995 and 1996 to 2010, are compared. Results show that the\npossibility of loss for an investment in the movie industry is relatively lower\nthan the possibility of gain for both periods of time. The values of VaR and ES\nfor the second period are higher than the first period. We are able to conclude\nthat the 2008 financial crisis gave no significant effect on these measurement\nvalues in the second period. This result describes the high potential\nopportunity in the investment of the U.S. movie makers.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 03:03:51 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Sumarti", "Novriana", ""], ["Hidayat", "Rafki", ""]]}, {"id": "1306.1882", "submitter": "Pavel Shevchenko V", "authors": "Pavel V. Shevchenko and Gareth W. Peters", "title": "Loss Distribution Approach for Operational Risk Capital Modelling under\n  Basel II: Combining Different Data Sources for Risk Estimation", "comments": null, "journal-ref": "The Journal of Governance and Regulation 2(3), pages 33-57, (2013)", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The management of operational risk in the banking industry has undergone\nsignificant changes over the last decade due to substantial changes in\noperational risk environment. Globalization, deregulation, the use of complex\nfinancial products and changes in information technology have resulted in\nexposure to new risks very different from market and credit risks. In response,\nBasel Committee for banking Supervision has developed a regulatory framework,\nreferred to as Basel II, that introduced operational risk category and\ncorresponding capital requirements. Over the past five years, major banks in\nmost parts of the world have received accreditation under the Basel II Advanced\nMeasurement Approach (AMA) by adopting the loss distribution approach (LDA)\ndespite there being a number of unresolved methodological challenges in its\nimplementation. Different approaches and methods are still under hot debate. In\nthis paper, we review methods proposed in the literature for combining\ndifferent data sources (internal data, external data and scenario analysis)\nwhich is one of the regulatory requirement for AMA.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 04:58:09 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Shevchenko", "Pavel V.", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1306.2719", "submitter": "M. H. A. Davis", "authors": "M. H. A. Davis, M. R. Pistorius", "title": "Explicit solution of an inverse first-passage time problem for L\\'{e}vy\n  processes and counterparty credit risk", "comments": "Published at http://dx.doi.org/10.1214/14-AAP1051 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2015, Vol. 25, No. 5, 2383-2415", "doi": "10.1214/14-AAP1051", "report-no": "IMS-AAP-AAP1051", "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given Markov process $X$ and survival function $\\overline{H}$ on\n$\\mathbb{R}^+$, the inverse first-passage time problem (IFPT) is to find a\nbarrier function $b:\\mathbb{R}^+\\to[-\\infty,+\\infty]$ such that the survival\nfunction of the first-passage time $\\tau_b=\\inf \\{t\\ge0:X(t)<b(t)\\}$ is given\nby $\\overline{H}$. In this paper, we consider a version of the IFPT problem\nwhere the barrier is fixed at zero and the problem is to find an initial\ndistribution $\\mu$ and a time-change $I$ such that for the time-changed process\n$X\\circ I$ the IFPT problem is solved by a constant barrier at the level zero.\nFor any L\\'{e}vy process $X$ satisfying an exponential moment condition, we\nderive the solution of this problem in terms of $\\lambda$-invariant\ndistributions of the process $X$ killed at the epoch of first entrance into the\nnegative half-axis. We provide an explicit characterization of such\ndistributions, which is a result of independent interest. For a given\nmulti-variate survival function $\\overline{H}$ of generalized frailty type, we\nconstruct subsequently an explicit solution to the corresponding IFPT with the\nbarrier level fixed at zero. We apply these results to the valuation of\nfinancial contracts that are subject to counterparty credit risk.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 05:55:19 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 17:55:03 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2015 05:01:17 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Davis", "M. H. A.", ""], ["Pistorius", "M. R.", ""]]}, {"id": "1306.2834", "submitter": "Ghislaine Gayraud", "authors": "Mauro Bernardi, Ghislaine Gayraud, Lea Petrella", "title": "Bayesian inference for CoVaR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent financial disasters emphasised the need to investigate the consequence\nassociated with the tail co-movements among institutions; episodes of contagion\nare frequently observed and increase the probability of large losses affecting\nmarket participants' risk capital. Commonly used risk management tools fail to\naccount for potential spillover effects among institutions because they provide\nindividual risk assessment. We contribute to analyse the interdependence\neffects of extreme events providing an estimation tool for evaluating the\nConditional Value-at-Risk (CoVaR) defined as the Value-at-Risk of an\ninstitution conditioned on another institution being under distress. In\nparticular, our approach relies on Bayesian quantile regression framework. We\npropose a Markov chain Monte Carlo algorithm exploiting the Asymmetric Laplace\ndistribution and its representation as a location-scale mixture of Normals.\nMoreover, since risk measures are usually evaluated on time series data and\nreturns typically change over time, we extend the CoVaR model to account for\nthe dynamics of the tail behaviour. Application on U.S. companies belonging to\ndifferent sectors of the Standard and Poor's Composite Index (S&P500) is\nconsidered to evaluate the marginal contribution to the overall systemic risk\nof each individual institution\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 14:26:07 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 15:56:32 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2013 16:46:12 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Bernardi", "Mauro", ""], ["Gayraud", "Ghislaine", ""], ["Petrella", "Lea", ""]]}, {"id": "1306.3479", "submitter": "Helena Jasiulewicz", "authors": "Helena Jasiulewicz, Wojciech Kordecki", "title": "Ruin probability of a discrete-time risk process with proportional\n  reinsurance and investment for exponential and Pareto distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a quantitative analysis of the ruin probability in finite time\nof discrete risk process with proportional reinsurance and investment of\nfinance surplus is focused on. It is assumed that the total loss on a unit\ninterval has a light-tailed distribution -- exponential distribution and a\nheavy-tailed distribution -- Pareto distribution. The ruin probability for\nfinite-horizon 5 and 10 was determined from recurrence equations. Moreover for\nexponential distribution the upper bound of ruin probability by Lundberg\nadjustment coefficient is given. For Pareto distribution the adjustment\ncoefficient does not exist, hence an asymptotic approximation of the ruin\nprobability if an initial capital tends to infinity is given. Obtained\nnumerical results are given as tables and they are illustrated as graphs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 18:35:49 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2015 17:31:59 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Jasiulewicz", "Helena", ""], ["Kordecki", "Wojciech", ""]]}, {"id": "1306.3531", "submitter": "Argyn Kuketayev", "authors": "Argyn Kuketayev", "title": "The convergence of regional house prices in the USA in the context of\n  the stress testing of financial institutions", "comments": "38 pages, 7 tables, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  I studied the convergence of regional house prices to national prices in USA\nby analyzing time-series of house price indices of 9 Census Divisions. I found\nthe evidence of the convergence in some parts of the country using asymmetric\nunit root tests. The fact that the evidence of the convergence is not present\nin large parts of the country raises an issue of execution and interpretation\nof results of Federal Reserve Bank's annual stress testing of the US banking\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 23:57:54 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Kuketayev", "Argyn", ""]]}, {"id": "1306.3856", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist and Peter Sarlin", "title": "From Text to Bank Interrelation Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the wake of the ongoing global financial crisis, interdependencies among\nbanks have come into focus in trying to assess systemic risk. To date, such\nanalysis has largely been based on numerical data. By contrast, this study\nattempts to gain further insight into bank interconnections by tapping into\nfinancial discussion. Co-mentions of bank names are turned into a network,\nwhich can be visualized and analyzed quantitatively, in order to illustrate\ncharacteristics of individual banks and the network as a whole. The approach\nallows for the study of temporal dynamics of the network, to highlight changing\npatterns of discussion that reflect real-world events, the current financial\ncrisis in particular. For instance, it depicts how connections from distressed\nbanks to other banks and supervisory authorities have emerged and faded over\ntime, as well as how global shifts in network structure coincide with severe\ncrisis episodes. The usage of textual data holds an additional advantage in the\npossibility of gaining a more qualitative understanding of an observed\ninterrelation, through its context. We illustrate our approach using a case\nstudy on Finnish banks and financial institutions. The data set comprises 3.9M\nposts from online, financial and business-related discussion, during the years\n2004 to 2012. Future research includes analyzing European news articles with a\nbroader perspective, and a focus on improving semantic description of\nrelations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 13:47:35 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1306.4619", "submitter": "Jean-Francois Renaud", "authors": "Jean-Fran\\c{c}ois Renaud", "title": "On the time spent in the red by a refracted L\\'evy risk process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an insurance ruin model with adaptive premium\nrate, thereafter refered to as restructuring/refraction, in which classical\nruin and bankruptcy are distinguished. In this model, the premium rate is\nincreased as soon as the wealth process falls into the red zone and is brought\nback to its regular level when the process recovers. The analysis is mainly\nfocused on the time a refracted L\\'evy risk process spends in the red zone\n(analogous to the duration of the negative surplus). Building on results from\nKyprianou and Loeffen (2010) and Loeffen et al. (2012), we identify the\ndistribution of various functionals related to occupation times of refracted\nspectrally negative L\\'evy processes. For example, these results are used to\ncompute the probability of bankruptcy and the probability of Parisian ruin in\nthis model with restructuring.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 17:10:56 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Renaud", "Jean-Fran\u00e7ois", ""]]}, {"id": "1306.5198", "submitter": "Igor Cialenco", "authors": "Tomasz R. Bielecki, Igor Cialenco, Samuel Drapeau, Martin Karliczek", "title": "Dynamic Assessment Indices", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a unified framework, which allows, in particular, to\nstudy the structure of dynamic monetary risk measures and dynamic acceptability\nindices. The main mathematical tool, which we use here, and which allows us to\nsignificantly generalize existing results is the theory of $L^0$-modules. In\nthe first part of the paper we develop the general theory and provide a robust\nrepresentation of conditional assessment indices, and in the second part we\napply this theory to dynamic acceptability indices acting on stochastic\nprocesses.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 17:23:53 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 00:21:33 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Bielecki", "Tomasz R.", ""], ["Cialenco", "Igor", ""], ["Drapeau", "Samuel", ""], ["Karliczek", "Martin", ""]]}, {"id": "1306.5302", "submitter": "Diane Wilcox PhD", "authors": "Diane Wilcox, Tim Gebbie", "title": "Factorising equity returns in an emerging market through exogenous\n  shocks and capital flows", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique from stochastic portfolio theory [Fernholz, 1998] is applied to\nanalyse equity returns of Small, Mid and Large cap portfolios in an emerging\nmarket through periods of growth and regional crises, up to the onset of the\nglobal financial crisis. In particular, we factorize portfolios in the South\nAfrican market in terms of distribution of capital, change of stock ranks in\nportfolios, and the effect due to dividends for the period Nov 1994 to May\n2007. We discuss the results in the context of broader economic thinking to\nconsider capital flows as risk factors, turning around more established\napproaches which use macroeconomic and socio-economic conditions to explain\nForeign Direct Investment (into the economy) and Net Portfolio Investment (into\nequity and bond markets).\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 09:14:00 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2013 12:32:48 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Wilcox", "Diane", ""], ["Gebbie", "Tim", ""]]}, {"id": "1306.5510", "submitter": "Nadia Saad", "authors": "Beno\\^it Collins, David McDonald and Nadia Saad", "title": "Compound Wishart Matrices and Noisy Covariance Matrices: Risk\n  Underestimation", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST q-fin.PM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain a property of the expectation of the inverse of\ncompound Wishart matrices which results from their orthogonal invariance. Using\nthis property as well as results from random matrix theory (RMT), we derive the\nasymptotic effect of the noise induced by estimating the covariance matrix on\ncomputing the risk of the optimal portfolio. This in turn enables us to get an\nasymptotically unbiased estimator of the risk of the optimal portfolio not only\nfor the case of independent observations but also in the case of correlated\nobservations. This improvement provides a new approach to estimate the risk of\na portfolio based on covariance matrices estimated from exponentially weighted\nmoving averages of stock returns.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 04:50:04 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Collins", "Beno\u00eet", ""], ["McDonald", "David", ""], ["Saad", "Nadia", ""]]}, {"id": "1306.5705", "submitter": "Babacar Seck", "authors": "Babacar Seck, Robert J. Elliott, Jean-Pierre Gueyie", "title": "Computational Dynamic Market Risk Measures in Discrete Time Setting", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different approaches to defining dynamic market risk measures are available\nin the literature. Most are focused or derived from probability theory,\neconomic behavior or dynamic programming. Here, we propose an approach to\ndefine and implement dynamic market risk measures based on recursion and state\neconomy representation. The proposed approach is to be implementable and to\ninherit properties from static market risk measures.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 18:40:35 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Seck", "Babacar", ""], ["Elliott", "Robert J.", ""], ["Gueyie", "Jean-Pierre", ""]]}, {"id": "1306.6588", "submitter": "Pierre Nyquist", "authors": "Pierre Nyquist", "title": "Moderate deviations for importance sampling estimators of risk measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling has become an important tool for the computation of\ntail-based risk measures. Since such quantities are often determined mainly by\nrare events standard Monte Carlo can be inefficient and importance sampling\nprovides a way to speed up computations. This paper considers moderate\ndeviations for the weighted empirical process, the process analogue of the\nweighted empirical measure, arising in importance sampling. The moderate\ndeviation principle is established as an extension of existing results. Using a\ndelta method for large deviations established by Gao and Zhao (Ann. Statist.,\n2011) together with classical large deviation techniques, the moderate\ndeviation principle for the weighted empirical process is extended to\nfunctionals of the weighted empirical process which correspond to risk\nmeasures. The main results are moderate deviation principles for importance\nsampling estimators of the quantile function of a distribution and Expected\nShortfall.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 18:04:10 GMT"}], "update_date": "2013-06-29", "authors_parsed": [["Nyquist", "Pierre", ""]]}, {"id": "1306.6715", "submitter": "David Chisholm", "authors": "David Chisholm, Graham Andersen", "title": "The Meaning of Probability of Default for Asset-backed Loans", "comments": "28 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors examine the concept of probability of default for asset-backed\nloans. In contrast to unsecured loans it is shown that probability of default\ncan be defined as either a measure of the likelihood of the borrower failing to\nmake required payments, or as the likelihood of an insufficiency of collateral\nvalue on foreclosure. Assuming expected loss is identical under either\ndefinition, this implies a corresponding pair of definitions for loss given\ndefault. Industry treatment of probability of default for asset-backed loans\nappears to inconsistently blend the two types of definition.\n  The authors develop a mathematical treatment of asset-backed loans which\nconsistently applies each type of definition in a framework to produce the same\nexpected loss and allows translation between the two frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 05:08:53 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Chisholm", "David", ""], ["Andersen", "Graham", ""]]}]