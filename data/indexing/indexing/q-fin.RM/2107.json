[{"id": "2107.00066", "submitter": "Antoine Jacquier Dr.", "authors": "Paul Bilokon and Antoine Jacquier and Conor McIndoe", "title": "Market regime classification with signatures", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a data-driven algorithm to classify market regimes for time\nseries. We utilise the path signature, encoding time series into\neasy-to-describe objects, and provide a metric structure which establishes a\nconnection between separation of regimes and clustering of points.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:22:22 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bilokon", "Paul", ""], ["Jacquier", "Antoine", ""], ["McIndoe", "Conor", ""]]}, {"id": "2107.01065", "submitter": "Silvana Pesenti", "authors": "Silvana M. Pesenti", "title": "Reverse Sensitivity Analysis for Risk Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem where a modeller conducts sensitivity analysis of a\nmodel consisting of random input factors, a corresponding random output of\ninterest, and a baseline probability measure. The modeller seeks to understand\nhow the model (the distribution of the input factors as well as the output)\nchanges under a stress on the output's distribution. Specifically, for a stress\non the output random variable, we derive the unique stressed distribution of\nthe output that is closest in the Wasserstein distance to the baseline output's\ndistribution and satisfies the stress. We further derive the stressed model,\nincluding the stressed distribution of the inputs, which can be calculated in a\nnumerically efficient way from a set of baseline Monte Carlo samples.\n  The proposed reverse sensitivity analysis framework is model-free and allows\nfor stresses on the output such as (a) the mean and variance, (b) any\ndistortion risk measure including the Value-at-Risk and Expected-Shortfall, and\n(c) expected utility type constraints, thus making the reverse sensitivity\nanalysis framework suitable for risk models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:19:27 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pesenti", "Silvana M.", ""]]}, {"id": "2107.01611", "submitter": "Jianfei Zhang", "authors": "Mathieu Rosenbaum and Jianfei Zhang", "title": "Deep calibration of the quadratic rough Heston model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.MF q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quadratic rough Heston model provides a natural way to encode Zumbach\neffect in the rough volatility paradigm. We apply multi-factor approximation\nand use deep learning methods to build an efficient calibration procedure for\nthis model. We show that the model is able to reproduce very well both SPX and\nVIX implied volatilities. We typically obtain VIX option prices within the\nbid-ask spread and an excellent fit of the SPX at-the-money skew. Moreover, we\nalso explain how to use the trained neural networks for hedging with\ninstantaneous computation of hedging quantities.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 12:52:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rosenbaum", "Mathieu", ""], ["Zhang", "Jianfei", ""]]}, {"id": "2107.01730", "submitter": "Roger Laeven", "authors": "Thomas Knispel, Roger J. A. Laeven, Gregor Svindland", "title": "Asymptotic Analysis of Risk Premia Induced by Law-Invariant Risk\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the limiting behavior of the risk premium associated with the\nPareto optimal risk sharing contract in an infinitely expanding pool of risks\nunder a general class of law-invariant risk measures encompassing\nrank-dependent utility preferences. We show that the corresponding convergence\nrate is typically only $n^{1/2}$ instead of the conventional $n$, with $n$ the\nmultiplicity of risks in the pool, depending upon the precise risk preferences.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 20:59:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Knispel", "Thomas", ""], ["Laeven", "Roger J. A.", ""], ["Svindland", "Gregor", ""]]}, {"id": "2107.02537", "submitter": "Alfredo Egidio dos Reis", "authors": "Yacine Koucha and Alfredo D. Egidio dos Reis", "title": "Approximations to ultimate ruin probabilities with a Wienner process\n  perturbation", "comments": "Master dissertation work, 18 pages, 4 figures, 8 numerical tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR q-fin.ST stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we adapt the classic Cram\\'er-Lundberg collective risk theory\nmodel to a perturbed model by adding a Wiener process to the compound Poisson\nprocess, which can be used to incorporate premium income uncertainty, interest\nrate fluctuations and changes in the number of policyholders. Our study is part\nof a Master dissertation, our aim is to make a short overview and present\nadditionally some new approximation methods for the infinite time ruin\nprobabilities for the perturbed risk model. We present four different\napproximation methods for the perturbed risk model. The first method is based\non iterative upper and lower approximations to the maximal aggregate loss\ndistribution. The second method relies on a four-moment exponential De Vylder\napproximation. The third method is based on the first-order Pad\\'e\napproximation of the Renyi and De Vylder approximations. The last method is the\nsecond order Pad\\'e-Ramsay approximation. These are generated by fitting one,\ntwo, three or four moments of the claim amount distribution, which greatly\ngeneralizes the approximations. We test the precision of approximations using a\ncombination of light and heavy tailed distributions for the individual claim\namount. We assess the ultimate ruin probability and present numerical results\nfor the exponential, gamma, and mixed exponential claim distributions,\ndemonstrating the high accuracy of these four methods. Analytical and numerical\nmethods are used to highlight the practical implications of our findings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:06:05 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Koucha", "Yacine", ""], ["Reis", "Alfredo D. Egidio dos", ""]]}, {"id": "2107.02656", "submitter": "Ruodu Wang", "authors": "Xiaoqing Liang, Ruodu Wang, Virginia Young", "title": "Optimal Insurance to Maximize RDEU Under a Distortion-Deviation Premium\n  Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an optimal insurance problem for a risk-averse\nindividual who seeks to maximize the rank-dependent expected utility (RDEU) of\nher terminal wealth, and insurance is priced via a general distortion-deviation\npremium principle. We prove necessary and sufficient conditions satisfied by\nthe optimal solution and consider three ambiguity orders to further determine\nthe optimal indemnity. Finally, we analyze examples under three\ndistortion-deviation premium principles to explore the specific conditions\nunder which no insurance or deductible insurance is optimal.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 14:50:14 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liang", "Xiaoqing", ""], ["Wang", "Ruodu", ""], ["Young", "Virginia", ""]]}, {"id": "2107.02764", "submitter": "Arthur Charpentier", "authors": "Arthur Charpentier and Lariosse Kouakou and Matthias L\\\"owe and\n  Philipp Ratz and Franck Vermet", "title": "Collaborative Insurance Sustainability and Network Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.SI econ.GN q-fin.CP q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The peer-to-peer (P2P) economy has been growing with the advent of the\nInternet, with well known brands such as Uber or Airbnb being examples thereof.\nIn the insurance sector the approach is still in its infancy, but some\ncompanies have started to explore P2P-based collaborative insurance products\n(eg. Lemonade in the U.S. or Inspeer in France). The actuarial literature only\nrecently started to consider those risk sharing mechanisms, as in Denuit and\nRobert (2021) or Feng et al. (2021). In this paper, describe and analyse such a\nP2P product, with some reciprocal risk sharing contracts. Here, we consider the\ncase where policyholders still have an insurance contract, but the first\nself-insurance layer, below the deductible, can be shared with friends. We\nstudy the impact of the shape of the network (through the distribution of\ndegrees) on the risk reduction. We consider also some optimal setting of the\nreciprocal commitments, and discuss the introduction of contracts with friends\nof friends to mitigate some possible drawbacks of having people without enough\nconnections to exchange risks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:29:38 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Charpentier", "Arthur", ""], ["Kouakou", "Lariosse", ""], ["L\u00f6we", "Matthias", ""], ["Ratz", "Philipp", ""], ["Vermet", "Franck", ""]]}, {"id": "2107.03340", "submitter": "Wing Fung Chong", "authors": "Wing Fung Chong and Haoen Cui and Yuxuan Li", "title": "Pseudo-Model-Free Hedging for Variable Annuities via Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper applies a deep reinforcement learning approach to revisit the\nhedging problem of variable annuities. Instead of assuming actuarial and\nfinancial dual-market model a priori, the reinforcement learning agent learns\nhow to hedge by collecting anchor-hedging reward signals through interactions\nwith the market. By the recently advanced proximal policy optimization, the\npseudo-model-free reinforcement learning agent performs equally well as the\ncorrect Delta, while outperforms the misspecified Deltas. The reinforcement\nlearning agent is also integrated with online learning to demonstrate its full\nadaptive capability to the market.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:31:06 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Chong", "Wing Fung", ""], ["Cui", "Haoen", ""], ["Li", "Yuxuan", ""]]}, {"id": "2107.03712", "submitter": "Emmanuel Coffie", "authors": "Emmanuel Coffie", "title": "Numerical approximation of hybrid Poisson-jump Ait-Sahalia-type interest\n  rate model with delay", "comments": "this article supersedes arXiv:2103.07651", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the original Ait-Sahalia interest rate model has been found\nconsiderable use as a model for describing time series evolution of interest\nrates, it may not possess adequate specifications to explain responses of\ninterest rates to empirical phenomena such as volatility 'skews' and 'smiles',\njump behaviour, market regulatory lapses, economic crisis, financial clashes,\npolitical instability, among others collectively. The aim of this paper is to\npropose a modified version of this model by incorporating additional features\nto collectively describe these empirical phenomena adequately. Moreover, due to\nlack of a closed-form solution to the proposed model, we employ several new\ntruncated EM techniques to examine this model and justify the scheme within\nMonte Carlo framework to compute expected payoffs of some financial quantities\nsuch as a bond and a barrier option.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 09:42:41 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 19:01:54 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 07:28:11 GMT"}, {"version": "v4", "created": "Wed, 28 Jul 2021 08:14:39 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Coffie", "Emmanuel", ""]]}, {"id": "2107.03979", "submitter": "Daniel Hadley", "authors": "Daniel Hadley, Harry Joe, Natalia Nolde", "title": "On the Selection of Loss Severity Distributions to Model Operational\n  Risk", "comments": "Submitted to Journal of Operational Risk on October 19, 2018;\n  Accepted May 2, 2019", "journal-ref": "Journal of Operational Risk, 14(3):73-94 (2019)", "doi": "10.21314/JOP.2019.229", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate modeling of operational risk is important for a bank and the finance\nindustry as a whole to prepare for potentially catastrophic losses. One\napproach to modeling operational is the loss distribution approach, which\nrequires a bank to group operational losses into risk categories and select a\nloss frequency and severity distribution for each category. This approach\nestimates the annual operational loss distribution, and a bank must set aside\ncapital, called regulatory capital, equal to the 0.999 quantile of this\nestimated distribution. In practice, this approach may produce unstable\nregulatory capital calculations from year-to-year as selected loss severity\ndistribution families change. This paper presents truncation probability\nestimates for loss severity data and a consistent quantile scoring function on\nannual loss data as useful severity distribution selection criteria that may\nlead to more stable regulatory capital. Additionally, the Sinh-arcSinh\ndistribution is another flexible candidate family for modeling loss severities\nthat can be easily estimated using the maximum likelihood approach. Finally, we\nrecommend that loss frequencies below the minimum reporting threshold be\ncollected so that loss severity data can be treated as censored data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:15:41 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Hadley", "Daniel", ""], ["Joe", "Harry", ""], ["Nolde", "Natalia", ""]]}, {"id": "2107.04698", "submitter": "Alex Garivaltis", "authors": "Alex Garivaltis", "title": "Waiting to Borrow From a 457(b) Plan", "comments": "52 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formulates and solves the optimal stopping problem for a loan made\nto one's self from a tax-advantaged retirement account such as a 401(k),\n403(b), or 457(b) plan. If the plan participant has access to an external asset\nwith a higher expected rate of return than the investment funds and indices\nthat are available within the retirement account, then he must decide how long\nto wait before exercising the loan option. On the one hand, taking the loan\nquickly will result in many years of exponential capital growth at the higher\n(external) rate; on the other hand, if we wait to accumulate more funds in the\n457(b), then we can make a larger deposit into the external asset (albeit for a\nshorter period of time). I derive a variety of cutoff rules for optimal loan\ncontrol; in general, the investor must wait until he accumulates a certain\namount of money (measured in contribution-years) that depends on the disparate\nyields, the loan parameters, and the date certain at which he will liquidate\nthe retirement account. Letting the horizon tend to infinity, the optimal\n(horizon-free) policy gains in elegance, simplicity, and practical robustness\nto different life outcomes. When asset prices and returns are stochastic, the\n(continuous time) cutoff rule turns into a \"wait region,\" whereby the mean of\nterminal wealth is rising and the variance of terminal wealth is falling. After\nhis sojourn through the wait region is over, the participant finds himself on\nthe mean-variance frontier, at which point his subsequent behavior is a matter\nof personal risk preference.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 22:19:34 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Garivaltis", "Alex", ""]]}, {"id": "2107.05201", "submitter": "Dong Zhou", "authors": "Hengxu Lin, Dong Zhou, Weiqing Liu, Jiang Bian", "title": "Deep Risk Model: A Deep Learning Solution for Mining Latent Risk Factors\n  to Improve Covariance Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling and managing portfolio risk is perhaps the most important step to\nachieve growing and preserving investment performance. Within the modern\nportfolio construction framework that built on Markowitz's theory, the\ncovariance matrix of stock returns is required to model the portfolio risk.\nTraditional approaches to estimate the covariance matrix are based on human\ndesigned risk factors, which often requires tremendous time and effort to\ndesign better risk factors to improve the covariance estimation. In this work,\nwe formulate the quest of mining risk factors as a learning problem and propose\na deep learning solution to effectively \"design\" risk factors with neural\nnetworks. The learning objective is carefully set to ensure the learned risk\nfactors are effective in explaining stock returns as well as have desired\northogonality and stability. Our experiments on the stock market data\ndemonstrate the effectiveness of the proposed method: our method can obtain\n$1.9\\%$ higher explained variance measured by $R^2$ and also reduce the risk of\na global minimum variance portfolio. Incremental analysis further supports our\ndesign of both the architecture and the learning objective.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 05:30:50 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Lin", "Hengxu", ""], ["Zhou", "Dong", ""], ["Liu", "Weiqing", ""], ["Bian", "Jiang", ""]]}, {"id": "2107.05359", "submitter": "P\\'al Andr\\'as Papp", "authors": "P\\'al Andr\\'as Papp, Roger Wattenhofer", "title": "Debt Swapping for Risk Mitigation in Financial Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3465456.3467638", "report-no": null, "categories": "q-fin.RM cs.CE q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study financial networks where banks are connected by debt contracts. We\nconsider the operation of debt swapping when two creditor banks decide to\nexchange an incoming payment obligation, thus leading to a locally different\nnetwork structure. We say that a swap is positive if it is beneficial for both\nof the banks involved; we can interpret this notion either with respect to the\namount of assets received by the banks, or their exposure to different shocks\nthat might hit the system.\n  We analyze various properties of these swapping operations in financial\nnetworks. We first show that there can be no positive swap for any pair of\nbanks in a static financial system, or when a shock hits each bank in the\nnetwork proportionally. We then study worst-case shock models, when a shock of\ngiven size is distributed in the worst possible way for a specific bank. If the\ngoal of banks is to minimize their losses in such a worst-case setting, then a\npositive swap can indeed exist. We analyze the effects of such a positive swap\non other banks of the system, the computational complexity of finding a swap,\nand special cases where a swap can be found efficiently. Finally, we also\npresent some results for more complex swapping operations when the banks swap\nmultiple contracts, or when more than two banks participate in the swap.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 12:42:34 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Papp", "P\u00e1l Andr\u00e1s", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2107.06349", "submitter": "Ashish Kumar", "authors": "Ashish Kumar, Laszlo Markus, Norbert Hari", "title": "Arbitrage-free pricing of CVA for cross-currency swap with wrong-way\n  risk under stochastic correlation modeling framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A positive correlation between exposure and counterparty credit risk gives\nrise to the so-called Wrong-Way Risk (WWR). Even after a decade of the\nfinancial crisis, addressing WWR in both sound and tractable ways remains\nchallenging. Academicians have proposed arbitrage-free set-ups through copula\nmethods but those are computationally expensive and hard to use in practice.\nResampling methods are proposed by the industry but they lack mathematical\nfoundations. The purpose of this article is to bridge this gap between the\napproaches used by academicians and industry. To this end, we propose a\nstochastic correlation approach to asses WWR. The methods based on constant\ncorrelation to model the dependency between exposure and counterparty credit\nrisk assume a linear dependency, thus fail to capture the tail dependence.\nUsing a stochastic correlation we move further away from the Gaussian copula\nand can capture the tail risk. This effect is reflected in the results where\nthe impact of stochastic correlation on calculated CVA is substantial when\ncompared to the case when a high constant correlation is assumed between\nexposure and credit. Given the uncertainty inherent to CVA, the proposed method\nis believed to provide a promising way to model WWR.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 19:16:53 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Kumar", "Ashish", ""], ["Markus", "Laszlo", ""], ["Hari", "Norbert", ""]]}, {"id": "2107.06518", "submitter": "Siddhartha Chakrabarty", "authors": "Suryadeepto Nag, Siddhartha P. Chakrabarty, Sankarshan Basu", "title": "From Carbon-transition Premium to Carbon-transition Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Investor awareness about impending regulations requiring firms to reduce\ntheir carbon footprint has introduced a carbon transition risk premium in the\nstocks of firms. On performing a cross-section analysis, a significant premium\nwas estimated among large caps in the US markets. The existence of a risk\npremium indicates investor awareness about future exposure to low-carbon\ntransition. A new measure, the Single Event Transition Risk (SETR), was\ndeveloped to model the maximum exposure of a firm to carbon transition risk,\nand a functional form for the same was determined, in terms of risk premia.\nDifferent classes of distributions for arrival processes of transition events\nwere considered and the respective SETRs were determined and studied. The\ntrade-off between higher premia and higher risks was studied for the different\nprocesses, and it was observed that, based on the distributions of arrival\ntimes, investors could have a lower, equal or higher probability of positive\nreturns (from the premium-risk trade-off), and that despite a fair pricing of\nthe carbon premium, decisions by investors to take long or short positions on a\nstock could still be biased.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 07:27:38 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Nag", "Suryadeepto", ""], ["Chakrabarty", "Siddhartha P.", ""], ["Basu", "Sankarshan", ""]]}, {"id": "2107.06623", "submitter": "Panagiotis Kanellopoulos", "authors": "Panagiotis Kanellopoulos, Maria Kyropoulou, Hao Zhou", "title": "Financial Network Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CE q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study financial systems from a game-theoretic standpoint. A financial\nsystem is represented by a network, where nodes correspond to firms, and\ndirected labeled edges correspond to debt contracts between them. The existence\nof cycles in the network indicates that a payment of a firm to one of its\nlenders might result to some incoming payment. So, if a firm cannot fully repay\nits debt, then the exact (partial) payments it makes to each of its creditors\ncan affect the cash inflow back to itself. We naturally assume that the firms\nare interested in their financial well-being (utility) which is aligned with\nthe amount of incoming payments they receive from the network. This defines a\ngame among the firms, that can be seen as utility-maximizing agents who can\nstrategize over their payments.\n  We are the first to study financial network games that arise under a natural\nset of payment strategies called priority-proportional payments. We investigate\nthe existence and (in)efficiency of equilibrium strategies, under different\nassumptions on how the firms' utility is defined, on the types of debt\ncontracts allowed between the firms, and on the presence of other financial\nfeatures that commonly arise in practice. Surprisingly, even if all firms'\nstrategies are fixed, the existence of a unique payment profile is not\nguaranteed. So, we also investigate the existence and computation of valid\npayment profiles for fixed payment strategies.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 11:50:54 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kanellopoulos", "Panagiotis", ""], ["Kyropoulou", "Maria", ""], ["Zhou", "Hao", ""]]}, {"id": "2107.06839", "submitter": "Natalie Packham", "authors": "N. Packham, F. Woebbeking", "title": "Correlation scenarios and correlation stress testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We develop a general approach for stress testing correlations of financial\nasset portfolios. The correlation matrix of asset returns is specified in a\nparametric form, where correlations are represented as a function of risk\nfactors, such as country and industry factors. A sparse factor structure\nlinking assets and risk factors is built using Bayesian variable selection\nmethods. Regular calibration yields a joint distribution of economically\nmeaningful stress scenarios of the factors. As such, the method also lends\nitself as a reverse stress testing framework: using the Mahalanobis distance or\nhighest density regions (HDR) on the joint risk factor distribution allows to\ninfer worst-case correlation scenarios. We give examples of stress tests on a\nlarge portfolio of European and North American stocks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:54:47 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Packham", "N.", ""], ["Woebbeking", "F.", ""]]}, {"id": "2107.06841", "submitter": "Chongrui Zhu", "authors": "Chongrui Zhu", "title": "The threshold strategy for spectrally negative Levy processes and a\n  terminal value at creeping ruin in the objective function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a dividend optimization problem with a terminal value at\ncreeping ruin for Levy risk models has been investigated. We consider an\ninsurance company whose surplus process evolves as a spectrally negative Levy\nprocess with a Gaussian part and its objective function is given by cumulative\ndiscounted dividend payments and a terminal value at creeping ruin. In views of\nidentities from fluctuation theory, under the restriction on the negative\nterminal value, we show that the threshold strategy turns out to be the optimal\none with threshold level at zero over an admissible class with restricted\ndividend rates. Furthermore, some sufficient conditions for the positive one\nalso have been given.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:56:21 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Zhu", "Chongrui", ""]]}, {"id": "2107.07556", "submitter": "Renata Gomes Alcoforado", "authors": "R. G. Alcoforado, W. Bernardino, A. D. Eg\\'idio dos Reis and J. A. C.\n  Santos", "title": "Modelling risk for commodities in Brazil: An application to live cattle\n  spot and futures prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study analysed a series of live cattle spot and futures prices from the\nBoi Gordo Index (BGI) in Brazil. The objective was to develop a model that best\nportrays this commodity's behaviour to estimate futures prices more accurately.\nThe database created contained 2,010 daily entries in which trade in futures\ncontracts occurred, as well as BGI spot sales in the market, from 1 December\n2006 to 30 April 2015. One of the most important reasons why this type of risk\nneeds to be measured is to set loss limits. To identify patterns in price\nbehaviour in order to improve future transactions' results, investors must\nanalyse fluctuations in assets' value for longer periods. Bibliographic\nresearch revealed that no other study has conducted a comprehensive analysis of\nthis commodity using this approach. Cattle ranching is big business in Brazil\ngiven that in 2017, this sector moved 523.25 billion Brazilian reals (about\n130.5 billion United States dollars). In that year, agribusiness contributed\n22% of Brazil's total gross domestic product. Using the proposed risk modelling\ntechnique, economic agents can make the best decision about which options\nwithin these investors' reach produce more effective risk management. The\nmethodology was based on Holt-Winters exponential smoothing algorithm,\nautoregressive integrated moving average (ARIMA), ARIMA with exogenous inputs,\ngeneralised autoregressive conditionally heteroskedastic and generalised\nautoregressive moving average (GARMA) models. More specifically, 5 different\nmethods were applied that allowed a comparison of 12 different models as ways\nto portray and predict the BGI commodity's behaviour. The results show that\nGARMA with order c(2,1) and without intercept is the best model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 18:36:40 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Alcoforado", "R. G.", ""], ["Bernardino", "W.", ""], ["Reis", "A. D. Eg\u00eddio dos", ""], ["Santos", "J. A. C.", ""]]}, {"id": "2107.08109", "submitter": "Niushan Gao", "authors": "Shengzhong Chen, Niushan Gao, Denny Leung, Lei Li", "title": "Automatic Fatou Property of Law-invariant Risk Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.FA q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that, on classical model spaces including Orlicz\nspaces, every real-valued, law-invariant, coherent risk measure automatically\nhas the Fatou property at every point whose negative part has a thin tail.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 20:32:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Shengzhong", ""], ["Gao", "Niushan", ""], ["Leung", "Denny", ""], ["Li", "Lei", ""]]}, {"id": "2107.08827", "submitter": "Gustav Sourek", "authors": "Matej Uhr\\'in, Gustav \\v{S}ourek, Ond\\v{r}ej Hub\\'a\\v{c}ek, Filip\n  \\v{Z}elezn\\'y", "title": "Optimal sports betting strategies in practice: an experimental review", "comments": "Accepted to IMA Journal of Management Mathematics where it, however,\n  appeared with swapped names and surnames - putting the correct version here\n  for reference", "journal-ref": "IMA Journal of Management Mathematics (2021) 00", "doi": "10.1093/imaman/dpaa029", "report-no": null, "categories": "q-fin.PM cs.CE q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the most popular approaches to the problem of sports betting\ninvestment based on modern portfolio theory and the Kelly criterion. We define\nthe problem setting, the formal investment strategies, and review their common\nmodifications used in practice. The underlying purpose of the reviewed\nmodifications is to mitigate the additional risk stemming from the unrealistic\nmathematical assumptions of the formal strategies. We test the resulting\nmethods using a unified evaluation protocol for three sports: horse racing,\nbasketball and soccer. The results show the practical necessity of the\nadditional risk-control methods and demonstrate their individual benefits.\nParticularly, we show that an adaptive variant of the popular ``fractional\nKelly'' method is a very suitable choice across a wide range of settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 15:09:47 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Uhr\u00edn", "Matej", ""], ["\u0160ourek", "Gustav", ""], ["Hub\u00e1\u010dek", "Ond\u0159ej", ""], ["\u017delezn\u00fd", "Filip", ""]]}, {"id": "2107.09048", "submitter": "Anton Josef Heckens", "authors": "Anton J. Heckens and Thomas Guhr", "title": "A New Attempt to Identify Long-term Precursors for Financial Crisis in\n  the Market Correlation Structures", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of events in financial markets is every investor's dream and,\nusually, wishful thinking. From a more general, economic and societal\nviewpoint, the identification of indicators for large events is highly\ndesirable to assess systemic risks. Unfortunately, the very nature of financial\nmarkets, particularly the predominantly non-Markovian character as well as\nnon-stationarity, make this challenge a formidable one, leaving little hope for\nfully fledged answers. Nevertheless, it is called for to collect pieces of\nevidence in a variety of observables to be assembled like the pieces of a\npuzzle that eventually might help to catch a glimpse of long-term indicators or\nprecursors for large events - if at all in a statistical sense. Here, we\npresent a new piece for this puzzle. We use the quasi-stationary market states\nwhich exist in the time evolution of the correlation structure in financial\nmarkets. Recently, we identified such market states relative to the collective\nmotion of the market as a whole. We study their precursor properties in the US\nstock markets over 16 years, including two crises, the dot-com bubble burst and\nthe pre-phase of the Lehman Brothers crash. We identify certain interesting\nfeatures and critically discuss their suitability as indicators.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 17:56:05 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Heckens", "Anton J.", ""], ["Guhr", "Thomas", ""]]}, {"id": "2107.10226", "submitter": "Wen Su", "authors": "Wen Su", "title": "Default Distances Based on the KMV-CEV Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents a new method to assess default risk based on applying non\nconstant volatility to the KMV model, taking the CEV model as an instance. We\nfind the evidence that the classical KMV model could not distinguish ST\ncompanies in China stock market. Aiming at improve the accuracy of the KMV\nmodel, we assume the firm's asset value dynamics are given by the CEV process\n$\\frac{dV_A}{V_A} = \\mu_A dt + \\delta V_A^{\\beta-1}dB$ and use fixed effects\nmodel and equivalent volatility method to estimate parameters. The estimation\nresults show the $\\beta>1$ for non ST companies while $\\beta<1$ for ST\ncompanies and the equivalent volatility method estimate the parameters much\nmore precisely. Compared with the classical KMV model, our CEV-KMV model fits\nthe market better in forecasting the default probability. We also provide an\ninsight that other volatility model can be applied, too.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 17:23:57 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Su", "Wen", ""]]}, {"id": "2107.10306", "submitter": "Dan Wang", "authors": "Dan Wang, Zhi Chen, Ionut Florescu", "title": "A Sparsity Algorithm with Applications to Corporate Credit Rating", "comments": "16 pages, 11 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Artificial Intelligence, interpreting the results of a Machine Learning\ntechnique often termed as a black box is a difficult task. A counterfactual\nexplanation of a particular \"black box\" attempts to find the smallest change to\nthe input values that modifies the prediction to a particular output, other\nthan the original one. In this work we formulate the problem of finding a\ncounterfactual explanation as an optimization problem. We propose a new\n\"sparsity algorithm\" which solves the optimization problem, while also\nmaximizing the sparsity of the counterfactual explanation. We apply the\nsparsity algorithm to provide a simple suggestion to publicly traded companies\nin order to improve their credit ratings. We validate the sparsity algorithm\nwith a synthetically generated dataset and we further apply it to quarterly\nfinancial statements from companies in financial, healthcare and IT sectors of\nthe US market. We provide evidence that the counterfactual explanation can\ncapture the nature of the real statement features that changed between the\ncurrent quarter and the following quarter when ratings improved. The empirical\nresults show that the higher the rating of a company the greater the \"effort\"\nrequired to further improve credit rating.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 18:47:35 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wang", "Dan", ""], ["Chen", "Zhi", ""], ["Florescu", "Ionut", ""]]}, {"id": "2107.10377", "submitter": "Marco Bianchetti", "authors": "Lorenzo Silotto, Marco Scaringi, Marco Bianchetti", "title": "Everything You Always Wanted to Know About XVA Model Risk but Were\n  Afraid to Ask", "comments": "59 pages, 15 figures, 16 tables, 43 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Valuation adjustments, collectively named XVA, play an important role in\nmodern derivatives pricing. XVA are an exotic pricing component since they\nrequire the forward simulation of multiple risk factors in order to compute the\nportfolio exposure including collateral, leading to a significant model risk\nand computational effort, even in case of plain vanilla trades. This work\nanalyses the most critical model risk factors, meant as those to which XVA are\nmost sensitive, finding an acceptable compromise between accuracy and\nperformance. This task has been conducted in a complete context including a\nmarket standard multi-curve G2++ model calibrated on real market data, both\nVariation Margin and ISDA-SIMM dynamic Initial Margin, different\ncollateralization schemes, and the most common linear and non-linear interest\nrates derivatives. Moreover, we considered an alternative analytical approach\nfor XVA in case of uncollateralized Swaps. We show that a crucial element is\nthe construction of a parsimonious time grid capable of capturing all\nperiodical spikes arising in collateralized exposure during the Margin Period\nof Risk. To this end, we propose a workaround to efficiently capture all\nspikes. Moreover, we show that there exists a parameterization which allows to\nobtain accurate results in a reasonable time, which is a very important feature\nfor practical applications. In order to address the valuation uncertainty\nlinked to the existence of a range of different parameterizations, we calculate\nthe Model Risk AVA (Additional Valuation Adjustment) for XVA according to the\nprovisions of the EU Prudent Valuation regulation. Finally, this work can serve\nas an handbook containing step-by-step instructions for the implementation of a\ncomplete, realistic and robust modelling framework of collateralized exposure\nand XVA.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 22:24:32 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Silotto", "Lorenzo", ""], ["Scaringi", "Marco", ""], ["Bianchetti", "Marco", ""]]}, {"id": "2107.10635", "submitter": "Cosimo Munari", "authors": "Cosimo Munari, Lutz Wilhelmy, Stefan Weber", "title": "Capital Requirements and Claims Recovery: A New Perspective on Solvency\n  Regulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Protection of creditors is a key objective of financial regulation. Where the\nprotection needs are high, i.e., in banking and insurance, regulatory solvency\nrequirements are an instrument to prevent that creditors incur losses on their\nclaims. The current regulatory requirements based on Value at Risk and Average\nValue at Risk limit the probability of default of financial institutions, but\nthey fail to control the size of recovery on creditors' claims in the case of\ndefault. We resolve this failure by developing a novel risk measure, Recovery\nValue at Risk. Our conceptual approach can flexibly be extended and allows the\nconstruction of general recovery risk measures for various risk management\npurposes. By design, these risk measures control recovery on creditors' claims\nand integrate the protection needs of creditors into the incentive structure of\nthe management. We provide detailed case studies and applications: We analyze\nhow recovery risk measures react to the joint distributions of assets and\nliabilities on firms' balance sheets and compare the corresponding capital\nrequirements with the current regulatory benchmarks based on Value at Risk and\nAverage Value at Risk. We discuss how to calibrate recovery risk measures to\nhistoric regulatory standards. Finally, we show that recovery risk measures can\nbe applied to performance-based management of business divisions of firms and\nthat they allow for a tractable characterization of optimal tradeoffs between\nrisk and return in the context of investment management.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 13:16:54 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Munari", "Cosimo", ""], ["Wilhelmy", "Lutz", ""], ["Weber", "Stefan", ""]]}, {"id": "2107.10891", "submitter": "Francesco Della Corte", "authors": "Gian Paolo Clemente, Francesco Della Corte, Nino Savelli", "title": "A bridge between Local GAAP and Solvency II frameworks to quantify\n  Capital Requirement for demographic risk", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper provides a stochastic model useful for assessing the capital\nrequirement for demographic risk. The model extends to the market consistent\ncontext classical methodologies developed in a local accounting framework. In\nparticular we provide a unique formulation for different non-participating life\ninsurance contracts and we prove analytically that the valuation of demographic\nprofit can be significantly affected by the financial conditions in the market.\nA case study has been also developed considering a portfolio of life insurance\ncontracts. Results prove the effectiveness of the model in highlighting main\ndrivers of capital requirement evaluation, also compared to local GAAP\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 19:21:44 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Clemente", "Gian Paolo", ""], ["Della Corte", "Francesco", ""], ["Savelli", "Nino", ""]]}, {"id": "2107.11340", "submitter": "Alexandre Carbonneau", "authors": "Alexandre Carbonneau and Fr\\'ed\\'eric Godin", "title": "Deep equal risk pricing of financial derivatives with non-translation\n  invariant risk measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of non-translation invariant risk measures within the equal risk\npricing (ERP) methodology for the valuation of financial derivatives is\ninvestigated. The ability to move beyond the class of convex risk measures\nconsidered in several prior studies provides more flexibility within the\npricing scheme. In particular, suitable choices for the risk measure embedded\nin the ERP framework such as the semi-mean-square-error (SMSE) are shown herein\nto alleviate the price inflation phenomenon observed under Tail Value-at-Risk\nbased ERP as documented for instance in Carbonneau and Godin (2021b). The\nnumerical implementation of non-translation invariant ERP is performed through\ndeep reinforcement learning, where a slight modification is applied to the\nconventional deep hedging training algorithm (see Buehler et al., 2019) so as\nto enable obtaining a price through a single training run for the two neural\nnetworks associated with the respective long and short hedging strategies. The\naccuracy of the neural network training procedure is shown in simulation\nexperiments not to be materially impacted by such modification of the training\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 16:36:46 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Carbonneau", "Alexandre", ""], ["Godin", "Fr\u00e9d\u00e9ric", ""]]}]