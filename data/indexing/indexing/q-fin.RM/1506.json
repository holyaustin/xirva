[{"id": "1506.00937", "submitter": "Zachary Feinstein", "authors": "Zachary Feinstein", "title": "Financial Contagion and Asset Liquidation Strategies", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a framework for modeling the financial system with\nmultiple illiquid assets during a crisis. This work generalizes the paper by\nAmini, Filipovic and Minca (2016) by allowing for differing liquidation\nstrategies. The main result is a proof of sufficient conditions for the\nexistence of an equilibrium liquidation strategy with corresponding unique\nclearing payments and liquidation prices. An algorithm for computing the\nmaximal clearing payments and prices is provided.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 16:08:07 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 18:39:12 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Feinstein", "Zachary", ""]]}, {"id": "1506.02013", "submitter": "Eric Bax", "authors": "James Li, Eric Bax, Nilanjan Roy, Andrea Leistra", "title": "VCG Payments for Portfolio Allocations in Online Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some online advertising offers pay only when an ad elicits a response.\nRandomness and uncertainty about response rates make showing those ads a risky\ninvestment for online publishers. Like financial investors, publishers can use\nportfolio allocation over multiple advertising offers to pursue revenue while\ncontrolling risk. Allocations over multiple offers do not have a distinct\nwinner and runner-up, so the usual second-price mechanism does not apply. This\npaper develops a pricing mechanism for portfolio allocations. The mechanism is\nefficient, truthful, and rewards offers that reduce risk.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 19:17:46 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Li", "James", ""], ["Bax", "Eric", ""], ["Roy", "Nilanjan", ""], ["Leistra", "Andrea", ""]]}, {"id": "1506.02020", "submitter": "Eric Bax", "authors": "Ragavendran Gopalakrishnan, Eric Bax, Krishna Prasad Chitrapura,\n  Sachin Garg", "title": "Portfolio Allocation for Sellers in Online Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In markets for online advertising, some advertisers pay only when users\nrespond to ads. So publishers estimate ad response rates and multiply by\nadvertiser bids to estimate expected revenue for showing ads. Since these\nestimates may be inaccurate, the publisher risks not selecting the ad for each\nad call that would maximize revenue. The variance of revenue can be decomposed\ninto two components -- variance due to `uncertainty' because the true response\nrate is unknown, and variance due to `randomness' because realized response\nstatistics fluctuate around the true response rate. Over a sequence of many ad\ncalls, the variance due to randomness nearly vanishes due to the law of large\nnumbers. However, the variance due to uncertainty doesn't diminish.\n  We introduce a technique for ad selection that augments existing estimation\nand explore-exploit methods. The technique uses methods from portfolio\noptimization to produce a distribution over ads rather than selecting the\nsingle ad that maximizes estimated expected revenue. Over a sequence of similar\nad calls, ads are selected according to the distribution. This approach\ndecreases the effects of uncertainty and increases revenue.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 19:26:53 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Gopalakrishnan", "Ragavendran", ""], ["Bax", "Eric", ""], ["Chitrapura", "Krishna Prasad", ""], ["Garg", "Sachin", ""]]}, {"id": "1506.03564", "submitter": "Fabio Derendinger", "authors": "Fabio Derendinger", "title": "Copula based hierarchical risk aggregation - Tree dependent sampling and\n  the space of mild tree dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to adequately model risks is crucial for insurance companies. The\nmethod of \"Copula-based hierarchical risk aggregation\" by Arbenz et al. offers\na flexible way in doing so and has attracted much attention recently. We\nbriefly introduce the aggregation tree model as well as the sampling algorithm\nproposed by they authors.\n  An important characteristic of the model is that the joint distribution of\nall risk is not fully specified unless an additional assumption (known as\n\"conditional independence assumption\") is added. We show that there is\nnumerical evidence that the sampling algorithm yields an approximation of the\ndistribution uniquely specified by the conditional independence assumption. We\npropose a modified algorithm and provide a proof that under certain conditions\nthe said distribution is indeed approximated by our algorithm.\n  We further determine the space of feasible distributions for a given\naggregation tree model in case we drop the conditional independence assumption.\nWe study the impact of the input parameters and the tree structure, which\nallows conclusions of the way the aggregation tree should be designed.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 06:54:20 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 18:27:36 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Derendinger", "Fabio", ""]]}, {"id": "1506.04125", "submitter": "Khalil Said", "authors": "V\\'eronique Maume-Deschamps (ICJ), Didier Rulli\\`ere (SAF), Khalil\n  Said (SAF)", "title": "A risk management approach to capital allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The European insurance sector will soon be faced with the application of\nSolvency 2 regulation norms. It will create a real change in risk management\npractices. The ORSA approach of the second pillar makes the capital allocation\nan important exercise for all insurers and specially for groups. Considering\nmulti-branches firms, capital allocation has to be based on a multivariate risk\nmodeling. Several allocation methods are present in the literature and insurers\npractices. In this paper, we present a new risk allocation method, we study its\ncoherence using an axiomatic approach, and we try to define what the best\nallocation choice for an insurance group is.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 19:42:30 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Maume-Deschamps", "V\u00e9ronique", "", "ICJ"], ["Rulli\u00e8re", "Didier", "", "SAF"], ["Said", "Khalil", "", "SAF"]]}, {"id": "1506.04663", "submitter": "Vahan Nanumyan", "authors": "Vahan Nanumyan, Antonios Garas, Frank Schweitzer", "title": "The Network of Counterparty Risk: Analysing Correlations in OTC\n  Derivatives", "comments": "36 pages, 18 figures, 2 tables", "journal-ref": "PLoS ONE 10(9): e0136638, 2015", "doi": "10.1371/journal.pone.0136638", "report-no": null, "categories": "q-fin.RM physics.soc-ph q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterparty risk denotes the risk that a party defaults in a bilateral\ncontract. This risk not only depends on the two parties involved, but also on\nthe risk from various other contracts each of these parties holds. In rather\ninformal markets, such as the OTC (over-the-counter) derivative market,\ninstitutions only report their aggregated quarterly risk exposure, but no\ndetails about their counterparties. Hence, little is known about the\ndiversification of counterparty risk. In this paper, we reconstruct the\nweighted and time-dependent network of counterparty risk in the OTC derivatives\nmarket of the United States between 1998 and 2012. To proxy unknown bilateral\nexposures, we first study the co-occurrence patterns of institutions based on\ntheir quarterly activity and ranking in the official report. The network\nobtained this way is further analysed by a weighted k-core decomposition, to\nreveal a core-periphery structure. This allows us to compare the activity-based\nranking with a topology-based ranking, to identify the most important\ninstitutions and their mutual dependencies. We also analyse correlations in\nthese activities, to show strong similarities in the behavior of the core\ninstitutions. Our analysis clearly demonstrates the clustering of counterparty\nrisk in a small set of about a dozen US banks. This not only increases the\ndefault risk of the central institutions, but also the default risk of\nperipheral institutions which have contracts with the central ones. Hence, all\ninstitutions indirectly have to bear (part of) the counterparty risk of all\nothers, which needs to be better reflected in the price of OTC derivatives.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 16:49:13 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 15:07:52 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Nanumyan", "Vahan", ""], ["Garas", "Antonios", ""], ["Schweitzer", "Frank", ""]]}, {"id": "1506.06664", "submitter": "Antonios Garas", "authors": "Rebekka Burkholz, Matt V. Leduc, Antonios Garas, Frank Schweitzer", "title": "Systemic risk in multiplex networks with asymmetric coupling and\n  threshold feedback", "comments": "18 pages, 5 figures", "journal-ref": "Physica D: Nonlinear Phenomena, Vol. 323--324, 64--72 (2016)", "doi": "10.1016/j.physd.2015.10.004", "report-no": null, "categories": "physics.soc-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study cascades on a two-layer multiplex network, with asymmetric feedback\nthat depends on the coupling strength between the layers. Based on an\nanalytical branching process approximation, we calculate the systemic risk\nmeasured by the final fraction of failed nodes on a reference layer. The\nresults are compared with the case of a single layer network that is an\naggregated representation of the two layers. We find that systemic risk in the\ntwo-layer network is smaller than in the aggregated one only if the coupling\nstrength between the two layers is small. Above a critical coupling strength,\nsystemic risk is increased because of the mutual amplification of cascades in\nthe two layers. We even observe sharp phase transitions in the cascade size\nthat are less pronounced on the aggregated layer. Our insights can be applied\nto a scenario where firms decide whether they want to split their business into\na less risky core business and a more risky subsidiary business. In most cases,\nthis may lead to a drastic increase of systemic risk, which is underestimated\nin an aggregated approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 16:01:52 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Burkholz", "Rebekka", ""], ["Leduc", "Matt V.", ""], ["Garas", "Antonios", ""], ["Schweitzer", "Frank", ""]]}, {"id": "1506.06975", "submitter": "Johan Dahlin", "authors": "Johan Dahlin, Mattias Villani and Thomas B. Sch\\\"on", "title": "Bayesian optimisation for fast approximate inference in state-space\n  models with intractable likelihoods", "comments": "24 pages, 7 figures. Submitted to journal for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximate Bayesian parameter inference in\nnon-linear state-space models with intractable likelihoods. Sequential Monte\nCarlo with approximate Bayesian computations (SMC-ABC) is one approach to\napproximate the likelihood in this type of models. However, such approximations\ncan be noisy and computationally costly which hinders efficient implementations\nusing standard methods based on optimisation and Monte Carlo methods. We\npropose a computationally efficient novel method based on the combination of\nGaussian process optimisation and SMC-ABC to create a Laplace approximation of\nthe intractable posterior. We exemplify the proposed algorithm for inference in\nstochastic volatility models with both synthetic and real-world data as well as\nfor estimating the Value-at-Risk for two portfolios using a copula model. We\ndocument speed-ups of between one and two orders of magnitude compared to\nstate-of-the-art algorithms for posterior inference.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 13:04:03 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 07:54:19 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 13:20:12 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Dahlin", "Johan", ""], ["Villani", "Mattias", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1506.08595", "submitter": "Stephane Crepey", "authors": "Yannick Armenti (LaMME), St\\'ephane Cr\\'epey (LaMME)", "title": "Central Clearing Valuation Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an XVA (costs) analysis of centrally cleared trading,\nparallel to the one that has been developed in the last years for bilateral\ntransactions. We introduce a dynamic framework that incorporates the sequence\nof cash-flows involved in the waterfall of resources of a clearing house. The\ntotal cost of the clearance framework for a clearing member, called CCVA for\ncentral clearing valuation adjustment, is decomposed into a CVA corresponding\nto the cost of its losses on the default fund in case of defaults of other\nmember, an MVA corresponding to the cost of funding its margins and a KVA\ncorresponding to the cost of the regulatory capital and also of the capital at\nrisk that the member implicitly provides to the CCP through its default fund\ncontribution. In the end the structure of the XVA equations for bilateral and\ncleared portfolios is similar, but the input data to these equations are not\nthe same, reflecting different financial network structures. The resulting XVA\nnumbers differ, but, interestingly enough, they become comparable after scaling\nby a suitable netting ratio.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 11:57:55 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 15:11:16 GMT"}, {"version": "v3", "created": "Fri, 3 Feb 2017 14:56:00 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Armenti", "Yannick", "", "LaMME"], ["Cr\u00e9pey", "St\u00e9phane", "", "LaMME"]]}]