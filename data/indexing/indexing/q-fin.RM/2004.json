[{"id": "2004.00047", "submitter": "Ladislav Kristoufek", "authors": "Ladislav Kristoufek", "title": "Grandpa, grandpa, tell me the one about Bitcoin being a safe haven:\n  Evidence from the COVID-19 pandemics", "comments": "8 pages, 4 figures", "journal-ref": "Frontiers in Physics 8:296 (2020)", "doi": "10.3389/fphy.2020.00296", "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin being a safe haven asset is one of the traditional stories in the\ncryptocurrency community. However, during its existence and relevant presence,\ni.e. approximately since 2013, there has been no severe situation on the\nfinancial markets globally to prove or disprove this story until the COVID-19\npandemics. We study the quantile correlations of Bitcoin and two benchmarks --\nS\\&P500 and VIX -- and we make comparison with gold as the traditional safe\nhaven asset. The Bitcoin safe haven story is shown and discussed to be\nunsubstantiated and far-fetched, while gold comes out as a clear winner in this\ncontest.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:16:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kristoufek", "Ladislav", ""]]}, {"id": "2004.01838", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi and Hayden Lau and Bernard Wong", "title": "Optimal periodic dividend strategies for spectrally negative L\\'evy\n  processes with fixed transaction costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximising dividends is one classical stability criterion in actuarial risk\ntheory. Motivated by the fact that dividends are paid periodically in real\nlife, $\\textit{periodic}$ dividend strategies were recently introduced\n(Albrecher, Gerber and Shiu, 2011). In this paper, we incorporate fixed\ntransaction costs into the model and study the optimal periodic dividend\nstrategy with fixed transaction costs for spectrally negative L\\'evy processes.\n  The value function of a periodic $(b_u,b_l)$ strategy is calculated by means\nof exiting identities and It\\^o's excusion when the surplus process is of\nunbounded variation. We show that a sufficient condition for optimality is that\nthe L\\'evy measure admits a density which is completely monotonic. Under such\nassumptions, a periodic $(b_u,b_l)$ strategy is confirmed to be optimal.\n  Results are illustrated.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 02:41:26 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 05:10:50 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Lau", "Hayden", ""], ["Wong", "Bernard", ""]]}, {"id": "2004.02312", "submitter": "Richard Martin", "authors": "Richard J. Martin", "title": "Fixed income portfolio optimisation: Interest rates, credit, and the\n  efficient frontier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed income has received far less attention than equity portfolio\noptimisation since Markowitz' original work of 1952, partly as a result of the\nneed to model rates and credit risk. We argue that the shape of the efficient\nfrontier is mainly controlled by linear constraints, with the standard\ndeviation relatively unimportant, and propose a two-factor model for its time\nevolution.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 20:38:16 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Martin", "Richard J.", ""]]}, {"id": "2004.03165", "submitter": "Christian Bongiorno", "authors": "Christian Bongiorno", "title": "Bootstraps Regularize Singular Correlation Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.SP q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I show analytically that the average of $k$ bootstrapped correlation matrices\nrapidly becomes positive-definite as $k$ increases, which provides a simple\napproach to regularize singular Pearson correlation matrices. If $n$ is the\nnumber of objects and $t$ the number of features, the averaged correlation\nmatrix is almost surely positive-definite if $k> \\frac{e}{e-1}\\frac{n}{t}\\simeq\n1.58\\frac{n}{t}$ in the limit of large $t$ and $n$. The probability of\nobtaining a positive-definite correlation matrix with $k$ bootstraps is also\nderived for finite $n$ and $t$. Finally, I demonstrate that the number of\nrequired bootstraps is always smaller than $n$. This method is particularly\nrelevant in fields where $n$ is orders of magnitude larger than the size of\ndata points $t$, e.g., in finance, genetics, social science, or image\nprocessing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 07:26:35 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bongiorno", "Christian", ""]]}, {"id": "2004.03190", "submitter": "Zhi-Qiang Jiang", "authors": "Wei-Zhen Li (ECUST), Jin-Rui Zhai (ECUST), Zhi-Qiang Jiang (ECUST),\n  Gang-Jin Wang (HNU), Wei-Xing Zhou (ECUST)", "title": "Predicting tail events in a RIA-EVT-Copula framework", "comments": "14 pages, 5 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the occurrence of tail events is of great importance in financial\nrisk management. By employing the method of peak-over-threshold (POT) to\nidentify the financial extremes, we perform a recurrence interval analysis\n(RIA) on these extremes. We find that the waiting time between consecutive\nextremes (recurrence interval) follow a $q$-exponential distribution and the\nsizes of extremes above the thresholds (exceeding size) conform to a\ngeneralized Pareto distribution. We also find that there is a significant\ncorrelation between recurrence intervals and exceeding sizes. We thus model the\njoint distribution of recurrence intervals and exceeding sizes through\nconnecting the two corresponding marginal distributions with the Frank and AMH\ncopula functions, and apply this joint distribution to estimate the hazard\nprobability to observe another extreme in $\\Delta t$ time since the last\nextreme happened $t$ time ago. Furthermore, an extreme predicting model based\non RIA-EVT-Copula is proposed by applying a decision-making algorithm on the\nhazard probability. Both in-sample and out-of-sample tests reveal that this new\nextreme forecasting framework has better performance in prediction comparing\nwith the forecasting model based on the hazard probability only estimated from\nthe distribution of recurrence intervals. Our results not only shed a new light\non understanding the occurring pattern of extremes in financial markets, but\nalso improve the accuracy to predict financial extremes for risk management.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:20:34 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 01:47:53 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Wei-Zhen", "", "ECUST"], ["Zhai", "Jin-Rui", "", "ECUST"], ["Jiang", "Zhi-Qiang", "", "ECUST"], ["Wang", "Gang-Jin", "", "HNU"], ["Zhou", "Wei-Xing", "", "ECUST"]]}, {"id": "2004.04501", "submitter": "Sander Willems", "authors": "Sander Willems", "title": "SABR smiles for RFR caplets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.CP q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a natural extension of the SABR model to price both backward and\nforward-looking RFR caplets in a post-Libor world. Forward-looking RFR caplets\ncan be priced using the market standard approximations of Hagan et al. (2002).\nWe provide closed-form effective SABR parameters for pricing backward-looking\nRFR caplets. These results are useful for smile interpolation and for analyzing\nbackward and forward-looking smiles in normalized units.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 12:01:53 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 14:31:07 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 07:23:13 GMT"}, {"version": "v4", "created": "Wed, 6 May 2020 07:19:23 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Willems", "Sander", ""]]}, {"id": "2004.05367", "submitter": "Giuseppe Brandi", "authors": "Giuseppe Brandi and T. Di Matteo", "title": "A new multilayer network construction via Tensor learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer networks proved to be suitable in extracting and providing\ndependency information of different complex systems. The construction of these\nnetworks is difficult and is mostly done with a static approach, neglecting\ntime delayed interdependences. Tensors are objects that naturally represent\nmultilayer networks and in this paper, we propose a new methodology based on\nTucker tensor autoregression in order to build a multilayer network directly\nfrom data. This methodology captures within and between connections across\nlayers and makes use of a filtering procedure to extract relevant information\nand improve visualization. We show the application of this methodology to\ndifferent stationary fractionally differenced financial data. We argue that our\nresult is useful to understand the dependencies across three different aspects\nof financial risk, namely market risk, liquidity risk, and volatility risk.\nIndeed, we show how the resulting visualization is a useful tool for risk\nmanagers depicting dependency asymmetries between different risk factors and\naccounting for delayed cross dependencies. The constructed multilayer network\nshows a strong interconnection between the volumes and prices layers across all\nthe stocks considered while a lower number of interconnections between the\nuncertainty measures is identified.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 11:06:33 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Brandi", "Giuseppe", ""], ["Di Matteo", "T.", ""]]}, {"id": "2004.05894", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb", "title": "What You See and What You Don't See: The Hidden Moments of a Probability\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical distributions have their in-sample maxima as natural censoring. We\nlook at the \"hidden tail\", that is, the part of the distribution in excess of\nthe maximum for a sample size of $n$. Using extreme value theory, we examine\nthe properties of the hidden tail and calculate its moments of order $p$. The\nmethod is useful in showing how large a bias one can expect, for a given $n$,\nbetween the visible in-sample mean and the true statistical mean (or higher\nmoments), which is considerable for $\\alpha$ close to 1. Among other\nproperties, we note that the \"hidden\" moment of order $0$, that is, the\nexceedance probability for power law distributions, follows an exponential\ndistribution and has for expectation $\\frac{1}{n}$ regardless of the\nparametrization of the scale and tail index.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 19:43:13 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "2004.06144", "submitter": "Youssef Nassef", "authors": "Youssef Nassef", "title": "The PCL Framework: A strategic approach to comprehensive risk management\n  in response to climate change impacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PCL framework provides a comprehensive climate risk management approach\ngrounded in the assessment of societal values of financial and non-financial\nloss tolerability. The framework optimizes response action across three main\nclusters, namely preemptive adaptation (P) or risk reduction, contingent\narrangements (C), and loss acceptance (L); without a predetermined hierarchy\nacross them. The PCL Framework aims at including the three clusters of outlay\nwithin a single continuum, and with the main policy outcome being a balanced\nportfolio of actions across the three clusters by way of an optimization\nmodule, such that the aggregate outlay is optimized in the long-term. It is\nproposed that the approach be applied separately for each hazard to which the\ntarget community is exposed. While it is currently applied to climate-related\nrisk management, the methodology can be repurposed for use in other contexts\nwhere societal buy-in is central.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 18:24:34 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Nassef", "Youssef", ""]]}, {"id": "2004.06420", "submitter": "Tomaso Aste", "authors": "Tomaso Aste", "title": "Stress testing and systemic risk measures using multivariate conditional\n  probability", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate conditional probability distribution models the effects of a\nset of variables onto the statistical properties of another set of variables.\nIn the study of systemic risk in a financial system, the multivariate\nconditional probability distribution can be used for stress-testing by\nquantifying the propagation of losses from a set of `stressing' variables to\nanother set of `stressed' variables. In this paper I describe how to compute\nsuch conditional probability distributions for the vast family of multivariate\nelliptical distributions, and in particular for the multivariate Student-t and\nthe multivariate Normal distributions. Measures of stress impact and systemic\nrisk are proposed. An application to the US equity market illustrates the\npotentials of this approach.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 11:04:49 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 14:52:45 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 14:27:11 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Aste", "Tomaso", ""]]}, {"id": "2004.06880", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi, Gregory Clive Taylor, Phuong Anh Vu, and Bernard Wong", "title": "A multivariate evolutionary generalised linear model framework with\n  adaptive estimation for claims reserving", "comments": "Accepted for publication in Insurance: Mathematics and Economics", "journal-ref": "Insurance: Mathematics and Economics, Volume 93, July 2020, Pages\n  50-71", "doi": "10.1016/j.insmatheco.2020.04.007", "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a multivariate evolutionary generalised linear\nmodel (GLM) framework for claims reserving, which allows for dynamic features\nof claims activity in conjunction with dependency across business lines to\naccurately assess claims reserves. We extend the traditional GLM reserving\nframework on two fronts: GLM fixed factors are allowed to evolve in a recursive\nmanner, and dependence is incorporated in the specification of these factors\nusing a common shock approach.\n  We consider factors that evolve across accident years in conjunction with\nfactors that evolve across calendar years. This two-dimensional evolution of\nfactors is unconventional as a traditional evolutionary model typically\nconsiders the evolution in one single time dimension. This creates challenges\nfor the estimation process, which we tackle in this paper. We develop the\nformulation of a particle filtering algorithm with parameter learning\nprocedure. This is an adaptive estimation approach which updates evolving\nfactors of the framework recursively over time.\n  We implement and illustrate our model with a simulated data set, as well as a\nset of real data from a Canadian insurer.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 04:49:26 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Taylor", "Gregory Clive", ""], ["Vu", "Phuong Anh", ""], ["Wong", "Bernard", ""]]}, {"id": "2004.08204", "submitter": "Tam Tran-The", "authors": "Tam Tran-The", "title": "Modeling Institutional Credit Risk with Financial News", "comments": "Accepted to the AAAI-20 Workshop on Knowledge Discovery from\n  Unstructured Data in Financial Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Credit risk management, the practice of mitigating losses by understanding\nthe adequacy of a borrower's capital and loan loss reserves, has long been\nimperative to any financial institution's long-term sustainability and growth.\nMassMutual is no exception. The company is keen on effectively monitoring\ndowngrade risk, or the risk associated with the event when credit rating of a\ncompany deteriorates. Current work in downgrade risk modeling depends on\nmultiple variations of quantitative measures provided by third-party rating\nagencies and risk management consultancy companies. As these structured\nnumerical data become increasingly commoditized among institutional investors,\nthere has been a wide push into using alternative sources of data, such as\nfinancial news, earnings call transcripts, or social media content, to possibly\ngain a competitive edge in the industry. The volume of qualitative information\nor unstructured text data has exploded in the past decades and is now available\nfor due diligence to supplement quantitative measures of credit risk. This\npaper proposes a predictive downgrade model using solely news data represented\nby neural network embeddings. The model standalone achieves an Area Under the\nReceiver Operating Characteristic Curve (AUC) of more than 80 percent. The\noutput probability from this news model, as an additional feature, improves the\nperformance of our benchmark model using only quantitative measures by more\nthan 5 percent in terms of both AUC and recall rate. A qualitative evaluation\nalso indicates that news articles related to our predicted downgrade events are\nspecially relevant and high-quality in our business context.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:52:07 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Tran-The", "Tam", ""]]}, {"id": "2004.08240", "submitter": "Samudra Dasgupta", "authors": "Samudra Dasgupta, Kathleen E. Hamilton, and Arnab Banerjee", "title": "Designing a NISQ reservoir with maximal memory capacity for volatility\n  forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the CBOE volatility index (VIX) is a highly non-linear and\nmemory-intensive task. In this paper, we use quantum reservoir computing to\nforecast the VIX using S&P500 (SPX) time-series. Our reservoir is a hybrid\nquantum-classical system executed on IBM's 53-qubit Rochester chip. We encode\nthe SPX values in the rotation angles and linearly combine the average spin of\nthe six-qubit register to predict the value of VIX at the next time step. Our\nresults demonstrate a potential application of noisy intermediate scale quantum\n(NISQ) devices to complex, real world applications.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 21:21:36 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 18:00:07 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 18:44:49 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 02:24:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Dasgupta", "Samudra", ""], ["Hamilton", "Kathleen E.", ""], ["Banerjee", "Arnab", ""]]}, {"id": "2004.08891", "submitter": "Weiguan Wang", "authors": "Johannes Ruf, Weiguan Wang", "title": "Hedging with Linear Regressions and Neural Networks", "comments": "Forthcoming in the Journal of Business & Economic Statistics", "journal-ref": null, "doi": "10.1080/07350015.2021.1931241", "report-no": null, "categories": "q-fin.RM cs.LG q-fin.MF q-fin.ST stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study neural networks as nonparametric estimation tools for the hedging of\noptions. To this end, we design a network, named HedgeNet, that directly\noutputs a hedging strategy. This network is trained to minimise the hedging\nerror instead of the pricing error. Applied to end-of-day and tick prices of\nS&P 500 and Euro Stoxx 50 options, the network is able to reduce the mean\nsquared hedging error of the Black-Scholes benchmark significantly. However, a\nsimilar benefit arises by simple linear regressions that incorporate the\nleverage effect.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 16:07:45 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 15:23:09 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 08:11:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ruf", "Johannes", ""], ["Wang", "Weiguan", ""]]}, {"id": "2004.09042", "submitter": "Misha Van Beek", "authors": "Misha van Beek", "title": "Consistent Calibration of Economic Scenario Generators: The Case for\n  Conditional Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic Scenario Generators (ESGs) simulate economic and financial variables\nforward in time for risk management and asset allocation purposes. It is often\nnot feasible to calibrate the dynamics of all variables within the ESG to\nhistorical data alone. Calibration to forward-information such as future\nscenarios and return expectations is needed for stress testing and portfolio\noptimization, but no generally accepted methodology is available. This paper\nintroduces the Conditional Scenario Simulator, which is a framework for\nconsistently calibrating simulations and projections of economic and financial\nvariables both to historical data and forward-looking information. The\nframework can be viewed as a multi-period, multi-factor generalization of the\nBlack-Litterman model, and can embed a wide array of financial and\nmacroeconomic models. Two practical examples demonstrate this in a frequentist\nand Bayesian setting.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 03:44:47 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["van Beek", "Misha", ""]]}, {"id": "2004.09963", "submitter": "Nick James", "authors": "Arjun Prakash, Nick James, Max Menzies, Gilad Francis", "title": "Structural clustering of volatility regimes for dynamic trading\n  strategies", "comments": "Expression edits and small methodological changes relative to v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new method to find the number of volatility regimes in a\nnonstationary financial time series by applying unsupervised learning to its\nvolatility structure. We use change point detection to partition a time series\ninto locally stationary segments and then compute a distance matrix between\nsegment distributions. The segments are clustered into a learned number of\ndiscrete volatility regimes via an optimization routine. Using this framework,\nwe determine a volatility clustering structure for financial indices, large-cap\nequities, exchange-traded funds and currency pairs. Our method overcomes the\nrigid assumptions necessary to implement many parametric regime-switching\nmodels, while effectively distilling a time series into several characteristic\nbehaviours. Our results provide significant simplification of these time series\nand a strong descriptive analysis of prior behaviours of volatility. This\nempirical analysis could be used with other regime-switching implementations,\njustifying the parametric structure encoded in any candidate model. Finally, we\ncreate and validate a dynamic trading strategy that learns the optimal match\nbetween the current distribution of a time series and its past regimes, thereby\nmaking online risk-avoidance decisions in the present.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 12:54:23 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 10:54:34 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Prakash", "Arjun", ""], ["James", "Nick", ""], ["Menzies", "Max", ""], ["Francis", "Gilad", ""]]}, {"id": "2004.11169", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi, Gregory Clive Taylor, Bernard Wong, and Xinda Yang", "title": "On the modelling of multivariate counts with Cox processes and dependent\n  shot noise intensities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a method to model and estimate several, _dependent_\ncount processes, using granular data. Specifically, we develop a multivariate\nCox process with shot noise intensities to jointly model the arrival process of\ncounts (e.g. insurance claims). The dependency structure is introduced via\nmultivariate shot noise _intensity_ processes which are connected with the help\nof L\\'evy copulas. In aggregate, our approach allows for (i) over-dispersion\nand auto-correlation within each line of business; (ii) realistic features\ninvolving time-varying, known covariates; and (iii) parsimonious dependence\nbetween processes without requiring simultaneous primary (e.g. accidents)\nevents.\n  The explicit incorporation of time-varying, known covariates can accommodate\ncharacteristics of real data and hence facilitate implementation in practice.\nIn an insurance context, these could be changes in policy volumes over time, as\nwell as seasonality patterns and trends, which may explain some of the\nrelationship (dependence) between multiple claims processes, or at least help\ntease out those relationships.\n  Finally, we develop a filtering algorithm based on the reversible-jump Markov\nChain Monte Carlo (RJMCMC) method to estimate the latent stochastic intensities\nand illustrate model calibration using real data from the AUSI data set.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:59:44 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 05:30:58 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Taylor", "Gregory Clive", ""], ["Wong", "Bernard", ""], ["Yang", "Xinda", ""]]}, {"id": "2004.12848", "submitter": "Chung-Han Hsieh", "authors": "Chung-Han Hsieh", "title": "Generalization of Affine Feedback Stock Trading Results to Include\n  Stop-Loss Orders", "comments": "SIAM Journal on Control and Optimization (SICON)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY eess.SY q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The takeoff point of this paper is to generalize the existing stock trading\nresults for a class of affine feedback controller to include consideration of a\nstop-loss order. Using the geometric Brownian motion as the underlying stock\nprice model, our main result is to provide a closed-form expression for the\ncumulative distribution function for the trading profit or loss. In addition,\nwe show that the affine feedback controller with stop-loss order indeed\ngeneralizes the result without stop order in the sense of distribution\nfunction. Some simulations and illustrative examples are also provided as\nsupporting evidence of the theory. Moreover, we provide some technical results\naimed at addressing the issues about survivability, cash-financing\nconsiderations, long-only property, and lower bound of the expected gain or\nloss.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:57:17 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hsieh", "Chung-Han", ""]]}, {"id": "2004.13235", "submitter": "Rodrigo S. Targino", "authors": "Takaaki Koike and Yuri F. Saporito and Rodrigo S. Targino", "title": "Avoiding zero probability events when computing Value at Risk\n  contributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the process of risk allocation for a generic\nmultivariate model when the risk measure is chosen as the Value-at-Risk (VaR).\nWe recast the traditional Euler contributions from an expectation conditional\non an event of zero probability to a ratio involving conditional expectations\nwhose conditioning events have stricktly positive probability. We derive an\nanalytical form of the proposed representation of VaR contributions for various\nparametric models. Our numerical experiments show that the estimator using this\nnovel representation outperforms the standard Monte Carlo estimator in terms of\nbias and variance. Moreover, unlike the existing estimators, the proposed\nestimator is free from hyperparameters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 01:23:44 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 16:51:08 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Koike", "Takaaki", ""], ["Saporito", "Yuri F.", ""], ["Targino", "Rodrigo S.", ""]]}, {"id": "2004.14149", "submitter": "Lucio Fernandez-Arjona", "authors": "Lucio Fernandez-Arjona (University of Zurich), Damir Filipovi\\'c (EPFL\n  and Swiss Finance Institute)", "title": "A machine learning approach to portfolio pricing and risk management for\n  high-dimensional problems", "comments": "30 pages (main), 10 pages (appendix), 3 figures, 22 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for portfolio risk management in discrete\ntime, based on a replicating martingale. This martingale is learned from a\nfinite sample in a supervised setting. The model learns the features necessary\nfor an effective low-dimensional representation, overcoming the curse of\ndimensionality common to function approximation in high-dimensional spaces. We\nshow results based on polynomial and neural network bases. Both offer superior\nresults to naive Monte Carlo methods and other existing methods like\nleast-squares Monte Carlo and replicating portfolios.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:51:02 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 08:11:35 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 17:52:47 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Fernandez-Arjona", "Lucio", "", "University of Zurich"], ["Filipovi\u0107", "Damir", "", "EPFL\n  and Swiss Finance Institute"]]}, {"id": "2004.14862", "submitter": "Indranil SenGupta", "authors": "Humayra Shoshi and Indranil SenGupta", "title": "Hedging and machine learning driven crude oil data analysis using a\n  refined Barndorff-Nielsen and Shephard model", "comments": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1911.13300", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a refined Barndorff-Nielsen and Shephard (BN-S) model is\nimplemented to find an optimal hedging strategy for commodity markets. The\nrefinement of the BN-S model is obtained with various machine and deep learning\nalgorithms. The refinement leads to the extraction of a deterministic parameter\nfrom the empirical data set. The problem is transformed to an appropriate\nclassification problem with a couple of different approaches: the volatility\napproach and the duration approach. The analysis is implemented to the Bakken\ncrude oil data and the aforementioned deterministic parameter is obtained for a\nwide range of data sets. With the implementation of this parameter in the\nrefined model, the resulting model performs much better than the classical BN-S\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:45:58 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 16:57:29 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 17:00:41 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Shoshi", "Humayra", ""], ["SenGupta", "Indranil", ""]]}]