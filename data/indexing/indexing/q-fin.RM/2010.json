[{"id": "2010.02511", "submitter": "Saul Jacka", "authors": "Kevin Engelbrecht and Saul Jacka", "title": "Pricing and Hedging the No-Negative-Equity Guarantee in Equity-Release\n  Mortgages", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a practical superhedging strategy for the pricing and hedging of\nthe No-Negative-Equity-Guarantee (NNEG) found in Equity-Release Mortgages\n(ERMs), or reverse mortgages, using a discrete-time model. In contrast to many\npapers on the NNEG and industry practice we work in an incomplete market\nsetting so that deaths and property prices are not independent under most\npricing measures. We give theoretical results and numerical illustrations to\nshow that the assumption of market completeness leads to a considerable\nundervaluation of the NNEG. By introducing an Excess-of-Loss reinsurance asset,\nwe show that it is possible to reduce the cost of the superhedge for a\nportfolio of ERMs with the average cost decreasing rapidly as the number of\nlives in the portfolio increases. All the hedging assets, with the exception of\ncash, have a term of one year making the availability of a property hedging\nasset from over-the-counter derivative providers more realistic. We outline how\na practical multi-period ERM pricing and hedging model can be built. Although\nthe prices identified by this model will be higher than prices under the\ncompleteness assumption, they are considerably lower than those under the\nEquivalent Value Test mandated by the UK's Prudential Regulatory Authority.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 06:27:34 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Engelbrecht", "Kevin", ""], ["Jacka", "Saul", ""]]}, {"id": "2010.03315", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Bruno Spilak, Wolfgang Karl H\\\"ardle", "title": "Tail-risk protection: Machine Learning meets modern Econometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tail risk protection is in the focus of the financial industry and requires\nsolid mathematical and statistical tools, especially when a trading strategy is\nderived. Recent hype driven by machine learning (ML) mechanisms has raised the\nnecessity to display and understand the functionality of ML tools. In this\npaper, we present a dynamic tail risk protection strategy that targets a\nmaximum predefined level of risk measured by Value-At-Risk while controlling\nfor participation in bull market regimes. We propose different weak\nclassifiers, parametric and non-parametric, that estimate the exceedance\nprobability of the risk level from which we derive trading signals in order to\nhedge tail events. We then compare the different approaches both with\nstatistical and trading strategy performance, finally we propose an ensemble\nclassifier that produces a meta tail risk protection strategy improving both\ngeneralization and trading performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 10:22:30 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 14:17:14 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 13:24:10 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Spilak", "Bruno", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2010.05601", "submitter": "Conrad Beyers", "authors": "Arno Botha, Conrad Beyers, Pieter de Villiers", "title": "The loss optimisation of loan recovery decision times using forecast\n  cash flows", "comments": "29 pages (including appendix), 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theoretical method is empirically illustrated in finding the best time to\nforsake a loan such that the overall credit loss is minimised. This is\npredicated by forecasting the future cash flows of a loan portfolio up to the\ncontractual term, as a remedy to the inherent right-censoring of real-world\n`incomplete' portfolios. Two techniques, a simple probabilistic model as well\nas an eight-state Markov chain, are used to forecast these cash flows\nindependently. We train both techniques from different segments within\nresidential mortgage data, provided by a large South African bank, as part of a\ncomparative experimental framework. As a result, the recovery decision's\nimplied timing is empirically illustrated as a multi-period optimisation\nproblem across uncertain cash flows and competing costs. Using a delinquency\nmeasure as a central criterion, our procedure helps to find a loss-optimal\nthreshold at which loan recovery should ideally occur for a given portfolio.\nFurthermore, both the portfolio's historical risk profile and forecasting\nthereof are shown to influence the timing of the recovery decision. This work\ncan therefore facilitate the revision of relevant bank policies or strategies\ntowards optimising the loan collections process, especially that of secured\nlending.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 11:12:39 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Botha", "Arno", ""], ["Beyers", "Conrad", ""], ["de Villiers", "Pieter", ""]]}, {"id": "2010.06227", "submitter": "Jonathan Berrisch", "authors": "Jonathan Berrisch, Florian Ziel", "title": "Modeling and Probababilistic Forecasting of Natural Gas Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the problem of modeling and forecasting European\nDay-Ahead and Month-Ahead natural gas prices. For this, we propose two distinct\nprobabilistic models that can be utilized in risk- and portfolio management. We\nuse daily pricing data ranging from 2011 to 2020. Extensive descriptive data\nanalysis shows that both time series feature heavy tails, conditional\nheteroscedasticity, and show asymmetric behavior in their differences. We\npropose state-space time series models under skewed, heavy-tailed distribution\nto capture all stylized facts in the data. They include the impact of\nautocorrelation, seasonality, risk premia, temperature, storage levels, the\nprice of European Emission Allowances, and related fuel prices of oil, coal,\nand electricity. We provide a rigorous model diagnostic and interpret all model\ncomponents in detail. Additionally, we conduct a probabilistic forecasting\nstudy with significance test and compare the predictive performance against\nliterature benchmarks. The proposed Day-Ahead (Month-Ahead) model leads to a\n$13\\%$ ($9$\\%) reduction in out of sample CRPS compared to the best performing\nbenchmark model, mainly due to adequate modeling of the volatility and heavy\ntails.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:22:00 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Berrisch", "Jonathan", ""], ["Ziel", "Florian", ""]]}, {"id": "2010.07220", "submitter": "Alexander Glauner", "authors": "Nicole B\\\"auerle and Alexander Glauner", "title": "Markov Decision Processes with Recursive Risk Measures", "comments": null, "journal-ref": "European Journal of Operational Research 2021", "doi": "10.1016/j.ejor.2021.04.030", "report-no": null, "categories": "math.OC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider risk-sensitive Markov Decision Processes (MDPs)\nwith Borel state and action spaces and unbounded cost under both finite and\ninfinite planning horizons. Our optimality criterion is based on the recursive\napplication of static risk measures. This is motivated by recursive utilities\nin the economic literature, has been studied before for the entropic risk\nmeasure and is extended here to an axiomatic characterization of suitable risk\nmeasures. We derive a Bellman equation and prove the existence of Markovian\noptimal policies. For an infinite planning horizon, the model is shown to be\ncontractive and the optimal policy to be stationary. Moreover, we establish a\nconnection to distributionally robust MDPs, which provides a global\ninterpretation of the recursively defined objective function. Monotone models\nare studied in particular.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:32:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["B\u00e4uerle", "Nicole", ""], ["Glauner", "Alexander", ""]]}, {"id": "2010.07383", "submitter": "Mario Ghossoub", "authors": "Corina Birghila and Tim J. Boonen and Mario Ghossoub", "title": "Optimal Insurance under Maxmin Expected Utility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.TH q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a problem of demand for insurance indemnification, when the\ninsured is sensitive to ambiguity and behaves according to the Maxmin-Expected\nUtility model of Gilboa and Schmeidler (1989), whereas the insurer is a\n(risk-averse or risk-neutral) Expected-Utility maximizer. We characterize\noptimal indemnity functions both with and without the customary ex ante\nno-sabotage requirement on feasible indemnities, and for both concave and\nlinear utility functions for the two agents. This allows us to provide a\nunifying framework in which we examine the effects of the no-sabotage\ncondition, marginal utility of wealth, belief heterogeneity, as well as\nambiguity (multiplicity of priors) on the structure of optimal indemnity\nfunctions. In particular, we show how the singularity in beliefs leads to an\noptimal indemnity function that involves full insurance on an event to which\nthe insurer assigns zero probability, while the decision maker assigns a\npositive probability. We examine several illustrative examples, and we provide\nnumerical studies for the case of a Wasserstein and a Renyi ambiguity set.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 20:06:04 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Birghila", "Corina", ""], ["Boonen", "Tim J.", ""], ["Ghossoub", "Mario", ""]]}, {"id": "2010.08263", "submitter": "Qi Wu", "authors": "Xing Yan, Weizhong Zhang, Lin Ma, Wei Liu, Qi Wu", "title": "Parsimonious Quantile Regression of Financial Asset Tail Dynamics via\n  Sequential Learning", "comments": "NeurIPS 2018:1582-1592", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious quantile regression framework to learn the dynamic\ntail behaviors of financial asset returns. Our model captures well both the\ntime-varying characteristic and the asymmetrical heavy-tail property of\nfinancial time series. It combines the merits of a popular sequential neural\nnetwork model, i.e., LSTM, with a novel parametric quantile function that we\nconstruct to represent the conditional distribution of asset returns. Our model\nalso captures individually the serial dependences of higher moments, rather\nthan just the volatility. Across a wide range of asset classes, the\nout-of-sample forecasts of conditional quantiles or VaR of our model outperform\nthe GARCH family. Further, the proposed approach does not suffer from the issue\nof quantile crossing, nor does it expose to the ill-posedness comparing to the\nparametric probability density function approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:35:52 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yan", "Xing", ""], ["Zhang", "Weizhong", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Wu", "Qi", ""]]}, {"id": "2010.08601", "submitter": "Honggao Cao", "authors": "Feng Zhang, Ruite Guo and Honggao Cao", "title": "Information Coefficient as a Performance Measure of Stock Selection\n  Models", "comments": "15 pages, 2 figures, and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information coefficient (IC) is a widely used metric for measuring investment\nmanagers' skills in selecting stocks. However, its adequacy and effectiveness\nfor evaluating stock selection models has not been clearly understood, as IC\nfrom a realistic stock selection model can hardly be materially different from\nzero and is often accompanies with high volatility. In this paper, we\ninvestigate the behavior of IC as a performance measure of stick selection\nmodels. Through simulation and simple statistical modeling, we examine the IC\nbehavior both statically and dynamically. The examination helps us propose two\npractical procedures that one may use for IC-based ongoing performance\nmonitoring of stock selection models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 19:40:49 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhang", "Feng", ""], ["Guo", "Ruite", ""], ["Cao", "Honggao", ""]]}, {"id": "2010.08890", "submitter": "Giuseppe Brandi", "authors": "Ioannis P. Antoniades, Giuseppe Brandi, L. G. Magafas, T. Di Matteo", "title": "The use of scaling properties to detect relevant changes in financial\n  time series: a new visual warning tool", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2020.125561", "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The dynamical evolution of multiscaling in financial time series is\ninvestigated using time-dependent Generalized Hurst Exponents (GHE), $H_q$, for\nvarious values of the parameter $q$. Using $H_q$, we introduce a new visual\nmethodology to algorithmically detect critical changes in the scaling of the\nunderlying complex time-series. The methodology involves the degree of\nmultiscaling at a particular time instance, the multiscaling trend which is\ncalculated by the Change-Point Analysis method, and a rigorous evaluation of\nthe statistical significance of the results. Using this algorithm, we have\nidentified particular patterns in the temporal co-evolution of the different\n$H_q$ time-series. These GHE patterns, distinguish in a statistically robust\nway, not only between time periods of uniscaling and multiscaling, but also\namong different types of multiscaling: symmetric multiscaling (M) and\nasymmetric multiscaling (A). We apply the visual methodology to time-series\ncomprising of daily close prices of four stock market indices: two major ones\n(S\\&P~500 and NIKKEI) and two peripheral ones (Athens Stock Exchange general\nIndex and Bombay-SENSEX). Results show that multiscaling varies greatly with\ntime: time periods of strong multiscaling behavior and time periods of\nuniscaling behavior are interchanged while transitions from uniscaling to\nmultiscaling behavior occur before critical market events, such as stock market\nbubbles. Moreover, particular asymmetric multiscaling patterns appear during\ncritical stock market eras and provide useful information about market\nconditions. In particular, they can be used as 'fingerprints' of a turbulent\nmarket period as well as provide warning signals for an upcoming stock market\n'bubble'. The applied visual methodology also appears to distinguish between\nexogenous and endogenous stock market crises, based on the observed patterns\nbefore the actual events.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 00:01:33 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 10:19:28 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 17:14:52 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 10:47:50 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Antoniades", "Ioannis P.", ""], ["Brandi", "Giuseppe", ""], ["Magafas", "L. G.", ""], ["Di Matteo", "T.", ""]]}, {"id": "2010.09937", "submitter": "Marcin Pitera", "authors": "Marcin Pitera and Thorsten Schmidt", "title": "Unbiased estimation and backtesting of risk in the context of heavy\n  tails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the estimation of risk is an important question in the daily business\nof banks and insurances, many existing plug-in estimation procedures suffer\nfrom an unnecessary bias. This often leads to the underestimation of risk and\nnegatively impacts backtesting results, especially in small sample cases. In\nthis article we show that the link between estimation bias and backtesting can\nbe traced back to the dual relationship between risk measures and the\ncorresponding performance measures, and discuss this in reference to\nvalue-at-risk and expected shortfall frameworks. Motivated by this finding, we\npropose a new algorithm for bias correction and show how to apply it for\ngeneralized Pareto distributions. In particular, we consider value-at-risk and\nexpected shortfall plug-in estimators, and show that the application of our\nalgorithm leads to gain in efficiency when heavy tails exist in the data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:37:52 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 17:34:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pitera", "Marcin", ""], ["Schmidt", "Thorsten", ""]]}, {"id": "2010.10794", "submitter": "Andrew Lim", "authors": "Jun-ya Gotoh, Michael Jong Kim, Andrew E.B.Lim", "title": "Worst-case sensitivity", "comments": "27 Pages + 11 page Appendix, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.SY eess.SY q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of Worst-Case Sensitivity, defined as the worst-case\nrate of increase in the expected cost of a Distributionally Robust Optimization\n(DRO) model when the size of the uncertainty set vanishes. We show that\nworst-case sensitivity is a Generalized Measure of Deviation and that a large\nclass of DRO models are essentially mean-(worst-case) sensitivity problems when\nuncertainty sets are small, unifying recent results on the relationship between\nDRO and regularized empirical optimization with worst-case sensitivity playing\nthe role of the regularizer. More generally, DRO solutions can be sensitive to\nthe family and size of the uncertainty set, and reflect the properties of its\nworst-case sensitivity. We derive closed-form expressions of worst-case\nsensitivity for well known uncertainty sets including smooth $\\phi$-divergence,\ntotal variation, \"budgeted\" uncertainty sets, uncertainty sets corresponding to\na convex combination of expected value and CVaR, and the Wasserstein metric.\nThese can be used to select the uncertainty set and its size for a given\napplication.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 07:19:53 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Gotoh", "Jun-ya", ""], ["Kim", "Michael Jong", ""], ["Lim", "Andrew E. B.", ""]]}, {"id": "2010.12043", "submitter": "Robert Ceres Jr", "authors": "Robert L. Ceres, Chris E. Forest, Klaus Keller", "title": "Trade-offs and synergies in managing coastal flood risk: A case study\n  for New York City", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decisions on how to manage future flood risks are frequently informed by both\nsophisticated and computationally expensive models. This complexity often\nlimits the representation of uncertainties and the consideration of strategies.\nHere, we use an intermediate complexity model framework that enables us to\nanalyze a rich set of strategies, objectives, and uncertainties. We find that\nallowing for more combinations of risk mitigation strategies can expand the\nsolution set, help explain synergies and trade-offs, and point to strategies\nthat can improve outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 20:32:48 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ceres", "Robert L.", ""], ["Forest", "Chris E.", ""], ["Keller", "Klaus", ""]]}, {"id": "2010.12577", "submitter": "Hansjoerg Albrecher", "authors": "Hansjoerg Albrecher and Pierre-Olivier Goffard", "title": "On the profitability of selfish blockchain mining under consideration of\n  ruin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.OC math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining blocks on a blockchain equipped with a proof of work consensus\nprotocol is well-known to be resource-consuming. A miner bears the operational\ncost, mainly electricity consumption and IT gear, of mining, and is compensated\nby a capital gain when a block is discovered. This paper aims at quantifying\nthe profitability of mining when the possible event of ruin is also considered.\nThis is done by formulating a tractable stochastic model and using tools from\napplied probability and analysis, including the explicit solution of a certain\ntype of advanced functional differential equation. The expected profit at a\nfuture time point is determined for the situation when the miner follows the\nprotocol as well as when he/she withholds blocks. The obtained explicit\nexpressions allow to analyze the sensitivity with respect to the different\nmodel ingredients and to identify conditions under which selfish mining is a\nstrategic advantage.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 15:21:45 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Albrecher", "Hansjoerg", ""], ["Goffard", "Pierre-Olivier", ""]]}, {"id": "2010.12651", "submitter": "Aurelien Alfonsi", "authors": "Aur\\'elien Alfonsi and Adel Cherchali and Jose Arturo Infante Acevedo", "title": "Multilevel Monte-Carlo for computing the SCR with the standard formula\n  and other stress tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the multilevel Monte-Carlo estimator for the expectation\nof a maximum of conditional expectations. This problem arises naturally when\nconsidering many stress tests and appears in the calculation of the interest\nrate module of the standard formula for the SCR. We obtain theoretical\nconvergence results that complements the recent work of Giles and Goda and\ngives some additional tractability through a parameter that somehow describes\nregularity properties around the maximum. We then apply the MLMC estimator to\nthe calculation of the SCR at future dates with the standard formula for an ALM\nsavings business on life insurance. We compare it with estimators obtained with\nLeast Square Monte-Carlo or Neural Networks. We find that the MLMC estimator is\ncomputationally more efficient and has the main advantage to avoid regression\nissues, which is particularly significant in the context of projection of a\nbalance sheet by an insurer due to the path dependency. Last, we discuss the\npotentiality of this numerical method and analyze in particular the effect of\nthe portfolio allocation on the SCR at future~dates.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:29:59 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 09:25:18 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Alfonsi", "Aur\u00e9lien", ""], ["Cherchali", "Adel", ""], ["Acevedo", "Jose Arturo Infante", ""]]}, {"id": "2010.13843", "submitter": "Kristoffer Andersson", "authors": "Kristoffer Andersson and Cornelis W. Oosterlee", "title": "Deep learning for CVA computations of large portfolios of financial\n  derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a neural network-based method for CVA computations\nof a portfolio of derivatives. In particular, we focus on portfolios consisting\nof a combination of derivatives, with and without true optionality,\n\\textit{e.g.,} a portfolio of a mix of European- and Bermudan-type derivatives.\nCVA is computed, with and without netting, for different levels of WWR and for\ndifferent levels of credit quality of the counterparty. We show that the CVA is\noverestimated with up to 25\\% by using the standard procedure of not adjusting\nthe exercise strategy for the default-risk of the counterparty. For the\nExpected Shortfall of the CVA dynamics, the overestimation was found to be more\nthan 100\\% in some non-extreme cases.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:56:47 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Andersson", "Kristoffer", ""], ["Oosterlee", "Cornelis W.", ""]]}, {"id": "2010.14673", "submitter": "Mario Ghossoub", "authors": "Mario Ghossoub and Jesse Hall and David Saunders", "title": "Maximum Spectral Measures of Risk with given Risk Factor Marginal\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of determining an upper bound for the value of a\nspectral risk measure of a loss that is a general nonlinear function of two\nfactors whose marginal distributions are known, but whose joint distribution is\nunknown. The factors may take values in complete separable metric spaces. We\nintroduce the notion of Maximum Spectral Measure (MSP), as a worst-case\nspectral risk measure of the loss with respect to the dependence between the\nfactors. The MSP admits a formulation as a solution to an optimization problem\nthat has the same constraint set as the optimal transport problem, but with a\nmore general objective function. We present results analogous to the\nKantorovich duality, and we investigate the continuity properties of the\noptimal value function and optimal solution set with respect to perturbation of\nthe marginal distributions. Additionally, we provide an asymptotic result\ncharacterizing the limiting distribution of the optimal value function when the\nfactor distributions are simulated from finite sample spaces. The special case\nof Expected Shortfall and the resulting Maximum Expected Shortfall is also\nexamined.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 23:55:05 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Ghossoub", "Mario", ""], ["Hall", "Jesse", ""], ["Saunders", "David", ""]]}, {"id": "2010.15254", "submitter": "Zachary Feinstein", "authors": "Zachary Feinstein and Andreas Sojmark", "title": "Dynamic Default Contagion in Heterogeneous Interbank Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we provide a simple setting that connects the structural\nmodelling approach of Gai-Kapadia interbank networks with the mean-field\napproach to default contagion. To accomplish this we make two key\ncontributions. First, we propose a dynamic default contagion model with\nendogenous early defaults for a finite set of banks, generalising the\nGai-Kapadia framework. Second, we reformulate this system as a stochastic\nparticle system leading to a limiting mean-field problem. We study the\nexistence of these clearing systems and, for the mean-field problem, the\ncontinuity of the system response.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 22:05:28 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 12:48:53 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 12:45:22 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Feinstein", "Zachary", ""], ["Sojmark", "Andreas", ""]]}, {"id": "2010.15709", "submitter": "Dietmar Pfeifer Prof. Dr.", "authors": "Dietmar Pfeifer, Doreen Strassburger, Joerg Philipps", "title": "Modelling and simulation of dependence structures in nonlife insurance\n  with Bernstein copulas", "comments": "paper presented on the International ASTIN Colloquium 2009, Helsinki", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review Bernstein and grid-type copulas for arbitrary\ndimensions and general grid resolutions in connection with discrete random\nvectors possessing uniform margins. We further suggest a pragmatic way to fit\nthe dependence structure of multivariate data to Bernstein copulas via\ngrid-type copulas and empirical contingency tables. Finally, we discuss a Monte\nCarlo study for the simulation and PML estimation for aggregate dependent\nlosses form observed windstorm and flooding data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:50:27 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Pfeifer", "Dietmar", ""], ["Strassburger", "Doreen", ""], ["Philipps", "Joerg", ""]]}, {"id": "2010.16009", "submitter": "Thomas Bernhardt", "authors": "Thomas Bernhardt, Catherine Donnelly", "title": "Quantifying the trade-off between income stability and the number of\n  members in a pooled annuity fund", "comments": null, "journal-ref": null, "doi": "10.1017/asb.2020.33", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of people who receive a stable income for life from a closed\npooled annuity fund is studied. Income stability is defined as keeping the\nincome within a specified tolerance of the initial income in a fixed proportion\nof future scenarios. The focus is on quantifying the effect of the number of\nmembers, which drives the level of idiosyncratic longevity risk in the fund, on\nthe income stability. To do this, investment returns are held constant and\nsystematic longevity risk is omitted. An analytical expression that closely\napproximates the number of fund members who receive a stable income is derived\nand is seen to be independent of the mortality model. An application of the\nresult is to calculate the length of time for which the pooled annuity fund can\nprovide the desired level of income stability\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 00:56:04 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Bernhardt", "Thomas", ""], ["Donnelly", "Catherine", ""]]}]