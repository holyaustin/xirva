[{"id": "1912.01280", "submitter": "Christian P\\\"otz", "authors": "Kathrin Glau, Ricardo Pachon and Christian P\\\"otz", "title": "Speed-up credit exposure calculations for pricing and risk management", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.00238", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method to calculate the credit exposure of European and\npath-dependent options. The proposed method is able to calculate accurate\nexpected exposure and potential future exposure profiles under the risk-neutral\nand the real-world measure. Key advantage of is that it delivers an accuracy\ncomparable to a full re-evaluation and at the same time it is faster than a\nregression-based method. Core of the approach is solving a dynamic programming\nproblem by function approximation. This yields a closed form approximation\nalong the paths together with the option's delta and gamma. The simple\nstructure allows for highly efficient evaluation of the exposures, even for a\nlarge number of simulated paths. The approach is flexible in the model choice,\npayoff profiles and asset classes. We validate the accuracy of the method\nnumerically for three different equity products and a Bermudan interest rate\nswaption. Benchmarking against the popular least-squares Monte Carlo approach\nshows that our method is able to deliver a higher accuracy in a faster runtime.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 10:19:35 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Glau", "Kathrin", ""], ["Pachon", "Ricardo", ""], ["P\u00f6tz", "Christian", ""]]}, {"id": "1912.02423", "submitter": "Kevin Kuo", "authors": "Kevin Kuo", "title": "Generative Synthesis of Insurance Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the impediments in advancing actuarial research and developing open\nsource assets for insurance analytics is the lack of realistic publicly\navailable datasets. In this work, we develop a workflow for synthesizing\ninsurance datasets leveraging CTGAN, a recently proposed neural network\narchitecture for generating tabular data. Applying the proposed workflow to\npublicly available data in the domains of general insurance pricing and life\ninsurance shock lapse modeling, we evaluate the synthesized datasets from a few\nperspectives: machine learning efficacy, distributions of variables, and\nstability of model parameters. This workflow is implemented via an R interface\nto promote adoption by researchers and data owners.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 07:49:31 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 03:46:00 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Kuo", "Kevin", ""]]}, {"id": "1912.04086", "submitter": "Yinzhi Wang", "authors": "Erik B{\\o}lviken and Yinzhi Wang", "title": "Optimal reinsurance for risk over surplus ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal reinsurance when Value at Risk and expected surplus is balanced\nthrough their ratio is studied, and it is demonstrated how results for\nrisk-adjusted surplus can be utilized. Simplifications for large portfolios are\nderived, and this large-portfolio study suggests a new condition on the\nreinsurance pricing regime which is crucial for the results obtained. One or\ntwo-layer contracts now become optimal for both risk-adjusted surplus and the\nrisk over expected surplus ratio, but there is no second layer when portfolios\nare large or when reinsurance prices are below some threshold. Simple\napproximations of the optimum portfolio are considered, and their degree of\ndegradation compared to the optimum is studied which leads to theoretical\ndegradation rates as the number of policies grows. The theory is supported by\nnumerical experiments which suggest that the shape of the claim severity\ndistributions may not be of primary importance when designing an optimal\nreinsurance program. It is argued that the approach can be applied to\nConditional Value at Risk as well.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 14:40:04 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["B\u00f8lviken", "Erik", ""], ["Wang", "Yinzhi", ""]]}, {"id": "1912.04175", "submitter": "Yinzhi Wang", "authors": "Yinzhi Wang and Erik B{\\o}lviken", "title": "How much is optimal reinsurance degraded by error?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on optimal reinsurance does not deal with how much the\neffectiveness of such solutions is degraded by errors in parameters and models.\nThe issue is investigated through both asymptotics and numerical studies. It is\nshown that the rate of degradation is often $O(1/n)$ as the sample size $n$ of\nhistorical observations becomes infinite. Criteria based on Value at Risk are\nexceptions that may achieve only $O(1/\\sqrt{n})$. These theoretical results are\nsupported by numerical studies. A Bayesian perspective on how to integrate risk\ncaused by parameter error is offered as well.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:50:35 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Wang", "Yinzhi", ""], ["B\u00f8lviken", "Erik", ""]]}, {"id": "1912.04308", "submitter": "Regis Houssou", "authors": "R\\'egis Houssou, J\\'er\\^ome Bovay, Stephan Robert", "title": "Adaptive Financial Fraud Detection in Imbalanced Data with Time-Varying\n  Poisson Processes", "comments": "Accepted for publication in the Journal Of Financial Risk Management\n  (JFRM). Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses financial fraud detection in imbalanced dataset using\nhomogeneous and non-homogeneous Poisson processes. The probability of\npredicting fraud on the financial transaction is derived. Applying our\nmethodology to the financial dataset shows a better predicting power than a\nbaseline approach, especially in the case of higher imbalanced data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 19:00:13 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Houssou", "R\u00e9gis", ""], ["Bovay", "J\u00e9r\u00f4me", ""], ["Robert", "Stephan", ""]]}, {"id": "1912.05228", "submitter": "Junjie Hu", "authors": "Junjie Hu, Wolfgang Karl H\\\"ardle, Weiyu Kuo", "title": "Risk of Bitcoin Market: Volatility, Jumps, and Forecasts", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among all the emerging markets, the cryptocurrency market is considered the\nmost controversial and simultaneously the most interesting one. The visibly\nsignificant market capitalization of cryptos motivates modern financial\ninstruments such as futures and options. Those will depend on the dynamics,\nvolatility, or even the jumps of cryptos. In this paper, the risk\ncharacteristics for Bitcoin are analyzed from a realized volatility dynamics\nview. The realized variance is estimated with the corrected threshold jump\ncomponents, realized semi-variance, and signed jumps. Our empirical results\nshow that the BTC is far riskier than any of the other developed financial\nmarkets. Up to 68% of the days are identified to be entangled with jumps.\nHowever, the discontinuities do not contribute to the variance significantly.\nThe full-sample fitting suggests that future realized variance has a positive\nrelationship with downside risk and a negative relationship with the positive\njump. The rolling-window out-of-sample forecasting results reveal that the\nforecasting horizon plays an important role in choosing forecasting models. For\nthe long horizon risk forecast, explicitly modeling jumps and signed estimators\nimprove forecasting accuracy and give extra utility up to 19 bps annually,\nwhile the HAR model without accounting jumps or signed estimators suits the\nshort horizon case best. Lastly, a simple equal-weighted portfolio of BTC not\nonly significantly reduces the size and quantity of jumps but also gives\ninvestors higher utility in short horizon case.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:05:15 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Hu", "Junjie", ""], ["H\u00e4rdle", "Wolfgang Karl", ""], ["Kuo", "Weiyu", ""]]}, {"id": "1912.06410", "submitter": "Pierre-Jean Tisserand", "authors": "P-J. Tisserand (LMT), M. Ragueneau", "title": "A mechanical and economical based framework to help decision-makers for\n  natural hazards and malicious events impact on infrastructure prevention", "comments": null, "journal-ref": "WCRR 2019, 12th World Congress on Railway Research, Oct 2019,\n  Tokyo, France", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies in economics deal with the non-reliability cost to assess\ninsurance fees or investment analyses, but none takes into consideration the\nmechanical aspect of reliability analysis. Other studies in mechanics give some\ntools and methods to carry out reliability analyses and fragility study. This\nstudy developed a framework where economical and mechanical considerations for\ninfrastructure investment decision-making. The theoretical reasoning is here\ndeveloped to couple mechanical reliability analyses, which are composed of\nfragility curves, and economical reliability analyses, which is based on\nresilience cost functions. This coupling is carried out with some probabilistic\nconsiderations, giving the concept of \"probable cost of failure\". The strength\nof this framework is that it can be used to analyze all possible critical\ncomponents in a network with all possible natural hazards or malicious event or\nother undesired events which it is possible to assess its probability of\noccurrence. The results of the analysis are indicators of probable cost of\nfailure of an infrastructure, which represents the insurance fee. These\nindicators can be computed for railway lines, for critical components, for\nevents. This tool enables decision-makers to prioritize safety investments and\nto guide strategic choices. The next step of this study will be to develop\nsmart data analysis tools, because of this framework needs and produces a lot\nof data, which must be smartly analyzed and presented.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:17:37 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Tisserand", "P-J.", "", "LMT"], ["Ragueneau", "M.", ""]]}, {"id": "1912.06916", "submitter": "\\c{C}a\\u{g}{\\i}n Ararat", "authors": "\\c{C}a\\u{g}{\\i}n Ararat, Zachary Feinstein", "title": "Set-Valued Risk Measures as Backward Stochastic Difference Inclusions\n  and Equations", "comments": "27 pages", "journal-ref": "Finance and Stochastics 25 (1), 43-76, (2021)", "doi": "10.1007/s00780-020-00445-0", "report-no": null, "categories": "q-fin.RM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalar dynamic risk measures for univariate positions in continuous time are\ncommonly represented as backward stochastic differential equations. In the\nmultivariate setting, dynamic risk measures have been defined and studied as\nfamilies of set-valued functionals in the recent literature. There are two\npossible extensions of scalar backward stochastic differential equations for\nthe set-valued framework: (1) backward stochastic differential inclusions,\nwhich evaluate the risk dynamics on the selectors of acceptable capital\nallocations; or (2) set-valued backward stochastic differential equations,\nwhich evaluate the risk dynamics on the full set of acceptable capital\nallocations as a singular object. In this work, the discrete time setting is\ninvestigated with difference inclusions and difference equations in order to\nprovide insights for such differential representations for set-valued dynamic\nrisk measures in continuous time.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 20:13:16 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 15:59:17 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 19:32:09 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ararat", "\u00c7a\u011f\u0131n", ""], ["Feinstein", "Zachary", ""]]}, {"id": "1912.08695", "submitter": "Zachary Feinstein", "authors": "Zachary Feinstein and Andreas Sojmark", "title": "A Dynamic Default Contagion Model: From Eisenberg-Noe to the Mean Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a model of default contagion that combines the\napproaches of Eisenberg-Noe interbank networks and dynamic mean field\ninteractions. The proposed contagion mechanism provides an endogenous rule for\nearly defaults in a network of financial institutions. The main result is to\ndemonstrate a mean field interaction that can be found as the limit of the\nfinite bank system generated from a finite Eisenberg-Noe style network. In this\nway, we connect two previously disparate frameworks for systemic risk, and in\nturn we provide a bridge for exploiting recent advances in mean field analysis\nwhen modelling systemic risk. The mean field limit is shown to be well-posed\nand is identified as a certain conditional McKean-Vlasov type problem that\nrespects the original network topology under suitable assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 16:15:54 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Feinstein", "Zachary", ""], ["Sojmark", "Andreas", ""]]}, {"id": "1912.09273", "submitter": "Ali Reza Fallahi", "authors": "Safoora Zarei and Ali R. Fallahi", "title": "Pay-As-You-Drive Insurance Pricing Model", "comments": null, "journal-ref": "American Journal of Statistics and Actuarial Science, Vol.2, Issue\n  1, pp 1 - 9, 2020", "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every time drivers take to the road, and with each mile that they drive,\nexposes themselves and others to the risk of an accident. Insurance premiums\nare only weakly linked to mileage, however, and have lump-sum characteristics\nlargely. The result is too much driving, and too many accidents. In this paper,\nwe introduce some useful theoretical results for Pay-As-You-Drive in Automobile\ninsurances. We consider a counting process and also find the distribution of\ndiscounted collective risk model when the counting process is non-homogeneous\nPoisson.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 23:58:46 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Zarei", "Safoora", ""], ["Fallahi", "Ali R.", ""]]}, {"id": "1912.09764", "submitter": "Claudio Nordio", "authors": "Angela Rita Provenzano, Daniele Trifir\\`o, Nicola Jean, Giacomo Le\n  Pera, Maurizio Spadaccino, Luca Massaron and Claudio Nordio", "title": "An Artificial Intelligence approach to Shadow Rating", "comments": "18 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the effectiveness of modern deep learning techniques in predicting\ncredit ratings over a universe of thousands of global corporate entities\nobligations when compared to most popular, traditional machine-learning\napproaches such as linear models and tree-based classifiers. Our results show a\nadequate accuracy over different rating classes when applying categorical\nembeddings to artificial neural networks (ANN) architectures.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 11:23:32 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Provenzano", "Angela Rita", ""], ["Trifir\u00f2", "Daniele", ""], ["Jean", "Nicola", ""], ["Pera", "Giacomo Le", ""], ["Spadaccino", "Maurizio", ""], ["Massaron", "Luca", ""], ["Nordio", "Claudio", ""]]}, {"id": "1912.09964", "submitter": "Mark Kiermayer", "authors": "Mark Kiermayer, Christian Wei{\\ss}", "title": "Grouping of Contracts in Insurance using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the high importance of grouping in practice, there exists little\nresearch on the respective topic. The present work presents a complete\nframework for grouping and a novel method to optimize model points. Model\npoints are used to substitute clusters of contracts in an insurance portfolio\nand thus yield a smaller, computationally less burdensome portfolio. This\ngrouped portfolio is controlled to have similar characteristics as the original\nportfolio. We provide numerical results for term life insurance and defined\ncontribution plans, which indicate the superiority of our approach compared to\nK-means clustering, a common baseline algorithm for grouping. Lastly, we show\nthat the presented concept can optimize a fixed number of model points for the\nentire portfolio simultaneously. This eliminates the need for any\npre-clustering of the portfolio, e.g. by K-means clustering, and therefore\npresents our method as an entirely new and independent methodology.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 17:39:26 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Kiermayer", "Mark", ""], ["Wei\u00df", "Christian", ""]]}, {"id": "1912.10380", "submitter": "Qiang Liu", "authors": "Shuxin Guo and Qiang Liu", "title": "The Black-Scholes-Merton dual equation", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the Black-Scholes-Merton dual equation, which has exactly the same\nform as the Black-Scholes-Merton equation. The new equation is general and\nworks for European, American, Bermudan, Asian, barrier, lookback, etc. options\nand leads to new insights into pricing and hedging. Trivially, a put-call\nequality emerges - all the above-mentioned put (call) options can be priced as\ntheir corresponding calls (puts) by simply swapping stock price (dividend\nyield) for strike price (risk-free rate) simultaneously. More importantly,\ndeltas (gammas) of such puts and calls are linked via analytic formulas. As one\napplication in hedging, the dual equation is utilized to improve the accuracy\nof the recently proposed approach of hedging options statically with\nshort-maturity contracts.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 04:12:10 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Guo", "Shuxin", ""], ["Liu", "Qiang", ""]]}, {"id": "1912.10526", "submitter": "Gary Venter", "authors": "Gary Venter and Kailan Shang", "title": "Building and Testing Yield Curve Generators for P&C Insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest-rate risk is a key factor for property-casualty insurer capital. P&C\ncompanies tend to be highly leveraged, with bond holdings much greater than\ncapital. For GAAP capital, bonds are marked to market but liabilities are not,\nso shifts in the yield curve can have a significant impact on capital.\nYield-curve scenario generators are one approach to quantifying this risk. They\nproduce many future simulated evolutions of the yield curve, which can be used\nto quantify the probabilities of bond-value changes that would result from\nvarious maturity-mix strategies. Some of these generators are provided as\nblack-box models where the user gets only the projected scenarios. One focus of\nthis paper is to provide methods for testing generated scenarios from such\nmodels by comparing to known distributional properties of yield curves.\n  P&C insurers hold bonds to maturity and manage cash-flow risk by matching\nasset and liability flows. Derivative pricing and stochastic volatility are of\nlittle concern over the relevant time frames. This requires different models\nand model testing than what is common in the broader financial markets.\n  To complicate things further, interest rates for the last decade have not\nbeen following the patterns established in the sixty years following WWII. We\nare now coming out of the period of very low rates, yet are still not returning\nto what had been thought of as normal before that. Modeling and model testing\nare in an evolving state while new patterns emerge.\n  Our analysis starts with a review of the literature on interest-rate model\ntesting, with a P&C focus, and an update of the tests for current market\nbehavior. We then discuss models, and use them to illustrate the fitting and\ntesting methods. The testing discussion does not require the model-building\nsection.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 20:13:23 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Venter", "Gary", ""], ["Shang", "Kailan", ""]]}, {"id": "1912.10866", "submitter": "Andrea Macrina", "authors": "Holly Brannelly, Andrea Macrina, Gareth W. Peters", "title": "Quantile Diffusions", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a novel approach for the construction of quantile processes\ngoverning the stochastic dynamics of quantiles in continuous time. Two classes\nof quantile diffusions are identified: The first, which we largely focus on,\nfeatures a random quantile level and allows for direct interpretation of model\nparameters such as skewness and kurtosis. The second type are function-valued\nquantile diffusions and are driven by stochastic parameter processes, which\ndetermine the entire quantile function at each point in time. The quantile\nprocesses are obtained by transforming the marginals of a diffusion process\nunder a composite map consisting of a distribution and a quantile function.\nSuch maps, analogous to rank transmutation maps, produce the marginals of the\nresulting quantile process. We discuss the relationship and differences between\nour approach and existing methods and characterisations of quantile processes\nin discrete and continuous time. As an example of an application of quantile\ndiffusions, we show how measure distortions, often found in financial\nmathematics and actuarial science, may be induced.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 14:27:51 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 12:09:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Brannelly", "Holly", ""], ["Macrina", "Andrea", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1912.11172", "submitter": "Tianyi Liu", "authors": "Tianyi Liu and Enlu Zhou", "title": "Online Quantification of Input Model Uncertainty by Two-Layer Importance\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulation has been widely used to analyze the performance of\ncomplex stochastic systems and facilitate decision making in those systems.\nStochastic simulation is driven by the input model, which is a collection of\nprobability distributions that model the stochasticity in the system. The input\nmodel is usually estimated using a finite amount of data, which introduces the\nso-called input model uncertainty to the simulation output. How to quantify\ninput uncertainty has been studied extensively, and many methods have been\nproposed for the batch data setting, i.e., when all the data are available at\nonce. However, methods for \"streaming data\" arriving sequentially in time are\nstill in demand, despite that streaming data have become increasingly prevalent\nin modern applications. To fill this gap, we propose a two-layer importance\nsampling framework that incorporates streaming data for online input\nuncertainty quantification. Under this framework, we develop two algorithms\nthat suit different application scenarios: the first scenario is when data come\nat a fast speed and there is no time for any new simulation in between updates;\nthe second is when data come at a moderate speed and a few but limited\nsimulations are allowed at each time stage. We prove the consistency and\nasymptotic convergence rate results, which theoretically show the efficiency of\nour proposed approach. We further demonstrate the proposed algorithms on a\nnumerical example of the news vendor problem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 02:02:01 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 21:13:36 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Liu", "Tianyi", ""], ["Zhou", "Enlu", ""]]}, {"id": "1912.11858", "submitter": "Julia Eisenberg", "authors": "M. Carmen Boado-Penas, Julia Eisenberg and Paul Kr\\\"uhner", "title": "Maximising with-profit pensions without guarantees", "comments": null, "journal-ref": null, "doi": "10.1007/s13385-019-00220-2", "report-no": null, "categories": "q-fin.RM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, pension providers are running into trouble mainly due to the\nultra-low interest rates and the guarantees associated to some pension\nbenefits. With the aim of reducing the pension volatility and providing\nadequate pension levels with no guarantees, we carry out mathematical analysis\nof a new pension design in the accumulation phase. The individual's premium is\nsplit into the individual and collective part and invested in funds. In times\nwhen the return from the individual fund exits a predefined corridor, a certain\nnumber of units is transferred to or from the collective account smoothing in\nthis way the volatility of the individual fund. The target is to maximise the\ntotal accumulated capital, consisting of the individual account and a portion\nof the collective account due to a so-called redistribution index, at\nretirement by controlling the corridor width. We also discuss the necessary and\nsufficient conditions that have to be put on the redistribution index in order\nto avoid arbitrage opportunities for contributors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 13:16:30 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Boado-Penas", "M. Carmen", ""], ["Eisenberg", "Julia", ""], ["Kr\u00fchner", "Paul", ""]]}, {"id": "1912.12329", "submitter": "Julia Eisenberg", "authors": "M. Carmen Boado-Penas, Julia Eisenberg, Ralf Korn", "title": "Transforming public pensions: A mixed scheme with a credit granted by\n  the state", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Birth rates have dramatically decreased and, with continuous improvements in\nlife expectancy, pension expenditure is on an irreversibly increasing path.\nThis will raise serious concerns for the sustainability of the public pension\nsystems usually financed on a pay-as-you-go (PAYG) basis where current\ncontributions cover current pension expenditure. With this in mind, the aim of\nthis paper is to propose a mixed pension system that consists of a combination\nof a classical PAYG scheme and an increase of the contribution rate invested in\na funding scheme. The investment of the funding part is designed so that the\nPAYG pension system is financially sustainable at a particular level of\nprobability and at the same time provide some gains to individuals. In this\nsense, we make the individuals be an active part to face the demographic risks\ninherent in the PAYG and re-establish its financial sustainability.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 20:37:05 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Boado-Penas", "M. Carmen", ""], ["Eisenberg", "Julia", ""], ["Korn", "Ralf", ""]]}, {"id": "1912.13275", "submitter": "Valentina Macchiati", "authors": "V. Macchiati, G. Brandi, G. Cimini, G. Caldarelli, D. Paolotti, T. Di\n  Matteo", "title": "Systemic liquidity contagion in the European interbank market", "comments": "32 pages, 24 figures Submitted to Journal of Economic Interaction and\n  Coordination", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systemic liquidity risk, defined by the IMF as \"the risk of simultaneous\nliquidity difficulties at multiple financial institutions\", is a key topic in\nmacroprudential policy and financial stress analysis. Specialized models to\nsimulate funding liquidity risk and contagion are available but they require\nnot only banks' bilateral exposures data but also balance sheet data with\nsufficient granularity, which are hardly available. Alternatively, risk\nanalyses on interbank networks have been done via centrality measures of the\nunderlying graph capturing the most interconnected and hence more prone to risk\nspreading banks. In this paper, we propose a model which relies on an epidemic\nmodel which simulate a contagion on the interbank market using the funding\nliquidity shortage mechanism as contagion process. The model is enriched with\ncountry and bank risk features which take into account the heterogeneity of the\ninterbank market. The proposed model is particularly useful when full set of\ndata necessary to run specialized models is not available. Since the interbank\nnetwork is not fully available, an economic driven reconstruction method is\nalso proposed to retrieve the interbank network by constraining the standard\nreconstruction methodology to real financial indicators. We show that the\ncontagion model is able to reproduce systemic liquidity risk across different\nyears and countries. This result suggests that the proposed model can be\nsuccessfully used as a valid alternative to more complex ones.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 11:42:34 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 15:30:20 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Macchiati", "V.", ""], ["Brandi", "G.", ""], ["Cimini", "G.", ""], ["Caldarelli", "G.", ""], ["Paolotti", "D.", ""], ["Di Matteo", "T.", ""]]}]