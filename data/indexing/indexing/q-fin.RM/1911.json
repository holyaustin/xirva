[{"id": "1911.00919", "submitter": "Denis Grebenkov", "authors": "Sebastien Valeyre, Denis S. Grebenkov, and Sofiane Aboura", "title": "The Reactive Beta Model", "comments": null, "journal-ref": "J. Finan. Res. 42, 71-113 (2019)", "doi": "10.1111/jfir.1217", "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a reactive beta model that includes the leverage effect to allow\nhedge fund managers to target a near-zero beta for market neutral strategies.\nFor this purpose, we derive a metric of correlation with leverage effect to\nidentify the relation between the market beta and volatility changes. An\nempirical test based on the most popular market neutral strategies is run from\n2000 to 2015 with exhaustive data sets including 600 US stocks and 600 European\nstocks. Our findings confirm the ability of the reactive beta model to withdraw\nan important part of the bias from the beta estimation and from most popular\nmarket neutral strategies.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 15:56:53 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Valeyre", "Sebastien", ""], ["Grebenkov", "Denis S.", ""], ["Aboura", "Sofiane", ""]]}, {"id": "1911.02261", "submitter": "Damiano Rossello", "authors": "Christos E. Kountzakis, Damiano Rossello", "title": "Acceptability Indices of Performance for Bounded C\\`adl\\`ag Processes", "comments": "22 pages, 6 Figures, 2 Tables, 2 Appendixes", "journal-ref": null, "doi": "10.1080/17442508.2019.1687705", "report-no": null, "categories": "q-fin.MF q-fin.RM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indices of acceptability are well suited to frame the axiomatic features of\nmany performance measures, associated to terminal random cash flows.We extend\nthis notion to classes of c\\`adl\\`ag processes modelling cash flows over a\nfixed investment horizon.We provide a representation result for bounded paths.\nWe suggest an acceptability index based both on the static Average\nValue-at-Risk functional and the running minimum of the paths, which eventually\nrepresents a RAROC-type model. Some numerical comparisons clarify the magnitude\nof performance evaluation for processes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 09:14:11 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Kountzakis", "Christos E.", ""], ["Rossello", "Damiano", ""]]}, {"id": "1911.03245", "submitter": "Samuel Drapeau", "authors": "Samuel Drapeau and Mekonnen Tadese", "title": "Dual Representation of Expectile based Expected Shortfall and Its\n  Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectile can be considered as a generalization of quantile. While\nexpected shortfall is a quantile based risk measure, we study its counterpart\n-- the expectile based expected shortfall -- where expectile takes the place of\nquantile. We provide its dual representation in terms of Bochner integral.\nAmong other properties, we show that it is bounded from below in terms of\nconvex combinations of expected shortfalls, and also from above by the smallest\nlaw invariant, coherent and comonotonic risk measure, for which we give the\nexplicit formulation of the corresponding distortion function. As a benchmark\nto the industry standard expected shortfall we further provide its comparative\nasymptotic behavior in terms of extreme value distributions. Based on these\nresults, we finally compute explicitly the expectile based expected shortfall\nfor some selected class of distributions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 13:27:11 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Drapeau", "Samuel", ""], ["Tadese", "Mekonnen", ""]]}, {"id": "1911.05116", "submitter": "Robert MacKay", "authors": "Nicholas Beale, Heather Battey, Anthony C. Davison, and Robert S.\n  MacKay", "title": "An Unethical Optimization Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If an artificial intelligence aims to maximise risk-adjusted return, then\nunder mild conditions it is disproportionately likely to pick an unethical\nstrategy unless the objective function allows sufficiently for this risk. Even\nif the proportion ${\\eta}$ of available unethical strategies is small, the\nprobability ${p_U}$ of picking an unethical strategy can become large; indeed\nunless returns are fat-tailed ${p_U}$ tends to unity as the strategy space\nbecomes large. We define an Unethical Odds Ratio Upsilon (${\\Upsilon}$) that\nallows us to calculate ${p_U}$ from ${\\eta}$, and we derive a simple formula\nfor the limit of ${\\Upsilon}$ as the strategy space becomes large. We give an\nalgorithm for estimating ${\\Upsilon}$ and ${p_U}$ in finite cases and discuss\nhow to deal with infinite strategy spaces. We show how this principle can be\nused to help detect unethical strategies and to estimate ${\\eta}$. Finally we\nsketch some policy implications of this work.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 19:41:46 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Beale", "Nicholas", ""], ["Battey", "Heather", ""], ["Davison", "Anthony C.", ""], ["MacKay", "Robert S.", ""]]}, {"id": "1911.05620", "submitter": "Weiguan Wang", "authors": "Johannes Ruf, Weiguan Wang", "title": "Neural networks for option pricing and hedging: a literature review", "comments": "Minor changes. Accepted for publications in Journal of Computational\n  Finance", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG q-fin.RM q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been used as a nonparametric method for option pricing\nand hedging since the early 1990s. Far over a hundred papers have been\npublished on this topic. This note intends to provide a comprehensive review.\nPapers are compared in terms of input features, output variables, benchmark\nmodels, performance measures, data partition methods, and underlying assets.\nFurthermore, related work and regularisation techniques are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 17:01:36 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 12:45:34 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ruf", "Johannes", ""], ["Wang", "Weiguan", ""]]}, {"id": "1911.06126", "submitter": "Giuseppe Brandi", "authors": "Giuseppe Brandi, Ruggero Gramatica, Tiziana Di Matteo", "title": "Unveil stock correlation via a new tensor-based decomposition method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio allocation and risk management make use of correlation matrices and\nheavily rely on the choice of a proper correlation matrix to be used. In this\nregard, one important question is related to the choice of the proper sample\nperiod to be used to estimate a stable correlation matrix. This paper addresses\nthis question and proposes a new methodology to estimate the correlation matrix\nwhich doesn't depend on the chosen sample period. This new methodology is based\non tensor factorization techniques. In particular, combining and normalizing\nfactor components, we build a correlation matrix which shows emerging\nstructural dependency properties not affected by the sample period. To retrieve\nthe factor components, we propose a new tensor decomposition (which we name\nSlice-Diagonal Tensor (SDT) factorization) and compare it to the two most used\ntensor decompositions, the Tucker and the PARAFAC. We have that the new\nfactorization is more parsimonious than the Tucker decomposition and more\nflexible than the PARAFAC. Moreover, this methodology applied to both simulated\nand empirical data shows results which are robust to two non-parametric tests,\nnamely Kruskal-Wallis and Kolmogorov-Smirnov tests. Since the resulting\ncorrelation matrix features stability and emerging structural dependency\nproperties, it can be used as alternative to other correlation matrices type of\nmeasures, including the Person correlation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 14:32:29 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:44:21 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Brandi", "Giuseppe", ""], ["Gramatica", "Ruggero", ""], ["Di Matteo", "Tiziana", ""]]}, {"id": "1911.06698", "submitter": "Sergejs Solovyov", "authors": "Oleg Kolesnikov, Alexander Markov, Daulet Smagulov, Sergejs Solovjovs", "title": "Cyber bonds and their pricing models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the developments in cyber risk treatment in the finance\nindustry, we propose a general framework of cyber bond, whose main purpose is\nto insure (compensate) losses of a cyber attack. Based on a database of\npublicly available cyber events, we determine cyber loss distribution\nparameters and use them to numerically simulate cyber bond price, yield, and\nother characteristics. We also consider two possible approaches to cyber bond\ncoupon calculation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 15:28:25 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Kolesnikov", "Oleg", ""], ["Markov", "Alexander", ""], ["Smagulov", "Daulet", ""], ["Solovjovs", "Sergejs", ""]]}, {"id": "1911.07313", "submitter": "Daniel Ritter", "authors": "Daniel Ritter", "title": "Mathematical Modeling of Systemic Risk in Financial Networks: Managing\n  Default Contagion and Fire Sales", "comments": "PhD Thesis", "journal-ref": "Dissertation, LMU M\\\"unchen: Fakult\\\"at f\\\"ur Mathematik,\n  Informatik und Statistik, 2019:\n  http://nbn-resolving.de/urn:nbn:de:bvb:19-241619", "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As impressively shown by the financial crisis in 2007/08, contagion effects\nin financial networks harbor a great threat for the stability of the entire\nsystem. Without sufficient capital requirements for banks and other financial\ninstitutions, shocks that are locally confined at first can spread through the\nentire system and be significantly amplified by various contagion channels. The\naim of this thesis is thus to investigate in detail two selected contagion\nchannels of this so-called systemic risk, provide mathematical models and\nderive consequences for the systemic risk management of financial institutions.\nThe first contagion channel we consider is default contagion. The underlying\neffect is here that insolvent institutions cannot service their debt or other\nfinancial obligations anymore - at least partially. Debtors and other directly\nimpacted parties in the system are thus forced to write off their losses and\ncan possibly be driven into insolvency themselves due to their incurred\nfinancial losses. This on the other hand starts a new round in the default\ncontagion process. In our model we simplistically describe each institution by\nall the financial positions it is exposed to as well as its initial capital. In\ndoing so, our starting point is the work of Detering et al. (2017) - a model\nfor contagion in unweighted networks - which particularly considers the exact\nnetwork configuration to be random and derives asymptotic results for large\nnetworks. We extend this model such that weighted networks can be considered\nand an application to financial networks becomes possible. More precisely, for\nany given initial shock we deduce an explicit asymptotic expression for the\ntotal damage caused in the system by contagion and provide a necessary and\nsufficient criterion for an unshocked financial system to be stable against\nsmall shocks. Moreover, ...\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 18:25:00 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ritter", "Daniel", ""]]}, {"id": "1911.08448", "submitter": "Ivan Cherednik", "authors": "Ivan Cherednik", "title": "Artificial intelligence approach to momentum risk-taking", "comments": "60 pages including 5 figures and various tables; v2: profit-taking,\n  Bessel and hypergeometric functions were added, editing; v3: connections to\n  fBM, random processes, references were added, editing; v4: further editing", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mathematical model of momentum risk-taking, which is essentially\nreal-time risk management focused on short-term volatility of stock markets.\nIts implementation, our fully automated momentum equity trading system\npresented systematically, proved to be successful in extensive historical and\nreal-time experiments. Momentum risk-taking is one of the key components of\ngeneral decision-making, a challenge for artificial intelligence and machine\nlearning with deep roots in cognitive science; its variants beyond stock\nmarkets are discussed. We begin with a new algebraic-type theory of news impact\non share-prices, which describes well their power growth, periodicity, and the\nmarket phenomena like price targets and profit-taking. This theory generally\nrequires Bessel and hypergeometric functions. Its discretization results in\nsome tables of bids, which are basically expected returns for main investment\nhorizons, the key in our trading system. The ML procedures we use are similar\nto those in neural networking. A preimage of our approach is the new contract\ncard game provided at the end, a combination of bridge and poker. Relations to\nrandom processes and the fractional Brownian motion are outlined.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 18:14:58 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 14:56:52 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 14:21:58 GMT"}, {"version": "v4", "created": "Tue, 17 Mar 2020 16:06:58 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Cherednik", "Ivan", ""]]}, {"id": "1911.09858", "submitter": "Sheikh Rabiul Islam", "authors": "Sheikh Rabiul Islam, William Eberle, Sheikh K. Ghafoor, Sid C. Bundy,\n  Douglas A. Talbert, and Ambareen Siraj", "title": "Investigating bankruptcy prediction models in the presence of extreme\n  class imbalance and multiple stages of economy", "comments": "Under review in Expert Systems with Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of credit risk analytics, current Bankruptcy Prediction Models\n(BPMs) struggle with (a) the availability of comprehensive and real-world data\nsets and (b) the presence of extreme class imbalance in the data (i.e., very\nfew samples for the minority class) that degrades the performance of the\nprediction model. Moreover, little research has compared the relative\nperformance of well-known BPM's on public datasets addressing the class\nimbalance problem. In this work, we apply eight classes of well-known BPMs, as\nsuggested by a review of decades of literature, on a new public dataset named\nFreddie Mac Single-Family Loan-Level Dataset with resampling (i.e., adding\nsynthetic minority samples) of the minority class to tackle class imbalance.\nAdditionally, we apply some recent AI techniques (e.g., tree-based ensemble\ntechniques) that demonstrate potentially better results on models trained with\nresampled data. In addition, from the analysis of 19 years (1999-2017) of data,\nwe discover that models behave differently when presented with sudden changes\nin the economy (e.g., a global financial crisis) resulting in abrupt\nfluctuations in the national default rate. In summary, this study should aid\npractitioners/researchers in determining the appropriate model with respect to\ndata that contains a class imbalance and various economic stages.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 05:00:09 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Islam", "Sheikh Rabiul", ""], ["Eberle", "William", ""], ["Ghafoor", "Sheikh K.", ""], ["Bundy", "Sid C.", ""], ["Talbert", "Douglas A.", ""], ["Siraj", "Ambareen", ""]]}, {"id": "1911.10104", "submitter": "Sheikh Rabiul Islam", "authors": "Sheikh Rabiul Islam, William Eberle, Sheikh K. Ghafoor", "title": "Towards Quantification of Explainability in Explainable Artificial\n  Intelligence Methods", "comments": "Submitted to FLAIRS-33", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) has become an integral part of domains such as\nsecurity, finance, healthcare, medicine, and criminal justice. Explaining the\ndecisions of AI systems in human terms is a key challenge--due to the high\ncomplexity of the model, as well as the potential implications on human\ninterests, rights, and lives . While Explainable AI is an emerging field of\nresearch, there is no consensus on the definition, quantification, and\nformalization of explainability. In fact, the quantification of explainability\nis an open challenge. In our previous work, we incorporated domain knowledge\nfor better explainability, however, we were unable to quantify the extent of\nexplainability. In this work, we (1) briefly analyze the definitions of\nexplainability from the perspective of different disciplines (e.g., psychology,\nsocial science), properties of explanation, explanation methods, and\nhuman-friendly explanations; and (2) propose and formulate an approach to\nquantify the extent of explainability. Our experimental result suggests a\nreasonable and model-agnostic way to quantify explainability\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:03:52 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Islam", "Sheikh Rabiul", ""], ["Eberle", "William", ""], ["Ghafoor", "Sheikh K.", ""]]}, {"id": "1911.10254", "submitter": "Eric Benhamou", "authors": "Eric Benhamou, Beatrice Guez and Nicolas Paris1", "title": "Omega and Sharpe ratio", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omega ratio, defined as the probability-weighted ratio of gains over losses\nat a given level of expected return, has been advocated as a better performance\nindicator compared to Sharpe and Sortino ratio as it depends on the full return\ndistribution and hence encapsulates all information about risk and return. We\ncompute Omega ratio for the normal distribution and show that under some\ndistribution symmetry assumptions, the Omega ratio is oversold as it does not\nprovide any additional information compared to Sharpe ratio. Indeed, for\nreturns that have elliptic distributions, we prove that the optimal portfolio\naccording to Omega ratio is the same as the optimal portfolio according to\nSharpe ratio. As elliptic distributions are a weak form of symmetric\ndistributions that generalized Gaussian distributions and encompass many fat\ntail distributions, this reduces tremendously the potential interest for the\nOmega ratio.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 21:46:58 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Benhamou", "Eric", ""], ["Guez", "Beatrice", ""], ["Paris1", "Nicolas", ""]]}, {"id": "1911.10948", "submitter": "Mariano Zeron", "authors": "Mariano Zeron-Medina Laris, Ignacio Ruiz", "title": "Denting the FRTB IMA computational challenge via Orthogonal Chebyshev\n  Sliding Technique", "comments": "A version of this paper has been peer-reviewed and will be published\n  in Wilmott Magazine in January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new technique based on high-dimensional\nChebyshev Tensors that we call \\emph{Orthogonal Chebyshev Sliding Technique}.\nWe implemented this technique inside the systems of a tier-one bank, and used\nit to approximate Front Office pricing functions in order to reduce the\nsubstantial computational burden associated with the capital calculation as\nspecified by FRTB IMA. In all cases, the computational burden reductions\nobtained were of more than $90\\%$, while keeping high degrees of accuracy, the\nlatter obtained as a result of the mathematical properties enjoyed by Chebyshev\nTensors.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 14:40:58 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 15:10:18 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Laris", "Mariano Zeron-Medina", ""], ["Ruiz", "Ignacio", ""]]}, {"id": "1911.12469", "submitter": "Kenji Shiohara", "authors": "Koichi Miyamoto, Kenji Shiohara", "title": "Reduction of Qubits in Quantum Algorithm for Monte Carlo Simulation by\n  Pseudo-random Number Generator", "comments": "14 pages, 12 figures", "journal-ref": "Phys. Rev. A 102, 022424 (2020)", "doi": "10.1103/PhysRevA.102.022424", "report-no": "EPHOU-19-0017", "categories": "quant-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that quantum computers can speed up Monte Carlo simulation\ncompared to classical counterparts. There are already some proposals of\napplication of the quantum algorithm to practical problems, including\nquantitative finance. In many problems in finance to which Monte Carlo\nsimulation is applied, many random numbers are required to obtain one sample\nvalue of the integrand, since those problems are extremely high-dimensional\nintegrations, for example, risk measurement of credit portfolio. This leads to\nthe situation that the required qubit number is too large in the naive\nimplementation where a quantum register is allocated per random number. In this\npaper, we point out that we can reduce qubits keeping quantum speed up if we\nperform calculation similar to classical one, that is, estimate the average of\nintegrand values sampled by a pseudo-random number generator (PRNG) implemented\non a quantum circuit. We present not only the overview of the idea but also\nconcrete implementation of PRNG and application to credit risk measurement.\nActually, reduction of qubits is a trade-off against increase of circuit depth.\nTherefore full reduction might be impractical, but such a trade-off between\nspeed and memory space will be important in adjustment of calculation setting\nconsidering machine specs, if large-scale Monte Carlo simulation by quantum\ncomputer is in operation in the future.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 00:39:46 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 09:29:41 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Miyamoto", "Koichi", ""], ["Shiohara", "Kenji", ""]]}]