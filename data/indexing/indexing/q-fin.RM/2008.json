[{"id": "2008.00391", "submitter": "Zuo Quan Xu Dr.", "authors": "Chonghu Guan, Zuo Quan Xu, Rui Zhou", "title": "Dynamic optimal reinsurance and dividend-payout in finite time horizon", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a dynamic optimal reinsurance and dividend-payout problem\nfor an insurer in a finite time horizon. The goal of the insurer is to maximize\nits expected cumulative discounted dividend payouts until bankruptcy or\nmaturity which comes earlier. The insurer is allowed to dynamically choose\nreinsurance contracts over the whole time horizon. This is a mixed\nsingular-classical control problem and the corresponding\nHamilton-Jacobi-Bellman equation is a variational inequality with fully\nnonlinear operator and with gradient constraint. The $C^{2,1}$ smoothness of\nthe value function and a comparison principle for its gradient function are\nestablished by penalty approximation method. We find that the surplus-time\nspace can be divided into three non-overlapping regions by a\nrisk-magnitude-and-time-dependent reinsurance barrier and a time-dependent\ndividend-payout barrier. The insurer should be exposed to higher risk as\nsurplus increases; exposed to all the risks once surplus upward crosses the\nreinsurance barrier; and pay out all reserves in excess of the dividend-payout\nbarrier. The localities of these regions are explicitly estimated.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 03:35:58 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 01:55:52 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Guan", "Chonghu", ""], ["Xu", "Zuo Quan", ""], ["Zhou", "Rui", ""]]}, {"id": "2008.00392", "submitter": "Zuo Quan Xu Dr.", "authors": "Zuo Quan Xu, Harry Zheng", "title": "Optimal Investment, Heterogeneous Consumption and Best Time for\n  Retirement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies an optimal investment and consumption problem with\nheterogeneous consumption of basic and luxury goods, together with the choice\nof time for retirement. The utility for luxury goods is not necessarily a\nconcave function. The optimal heterogeneous consumption strategies for a class\nof non-homothetic utility maximizer are shown to consume only basic goods when\nthe wealth is small, to consume basic goods and make savings when the wealth is\nintermediate, and to consume small portion in basic goods and large portion in\nluxury goods when the wealth is large. The optimal retirement policy is shown\nto be both universal, in the sense that all individuals should retire at the\nsame level of marginal utility that is determined only by income, labor cost,\ndiscount factor as well as market parameters, and not universal, in the sense\nthat all individuals can achieve the same marginal utility with different\nutility and wealth. It is also shown that individuals prefer to retire as time\ngoes by if the marginal labor cost increases faster than that of income. The\nmain tools used in analysing the problem are from PDE and stochastic control\ntheory including viscosity solution, variational inequality and dual\ntransformation.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 03:47:14 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Xu", "Zuo Quan", ""], ["Zheng", "Harry", ""]]}, {"id": "2008.01277", "submitter": "ShaoPeng Hong", "authors": "Hong Shaopeng", "title": "Generalized Autoregressive Score asymmetric Laplace Distribution and\n  Extreme Downward Risk Prediction", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the skessed distribution, high peak and thick tail and asymmetry of\nfinancial return data, it is difficult to describe the traditional\ndistribution. In recent years, generalized autoregressive score (GAS) has been\nused in many fields and achieved good results. In this paper, under the\nframework of generalized autoregressive score (GAS), the asymmetric Laplace\ndistribution (ALD) is improved, and the GAS-ALD model is proposed, which has\nthe characteristics of time-varying parameters, can describe the peak thick\ntail, biased and asymmetric distribution. The model is used to study the\nShanghai index, Shenzhen index and SME board index. It is found that: 1) the\ndistribution parameters and moments of the three indexes have obvious\ntime-varying characteristics and aggregation characteristics. 2) Compared with\nthe commonly used models for calculating VaR and ES, the GAS-ALD model has a\nhigh prediction effect.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 02:11:57 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 08:36:03 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 05:41:29 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Shaopeng", "Hong", ""]]}, {"id": "2008.01687", "submitter": "Claudio Nordio", "authors": "A. R. Provenzano, D. Trifir\\`o, A. Datteo, L. Giada, N. Jean, A.\n  Riciputi, G. Le Pera, M. Spadaccino, L. Massaron and C. Nordio", "title": "Machine Learning approach for Credit Scoring", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we build a stack of machine learning models aimed at composing a\nstate-of-the-art credit rating and default prediction system, obtaining\nexcellent out-of-sample performances. Our approach is an excursion through the\nmost recent ML / AI concepts, starting from natural language processes (NLP)\napplied to economic sectors' (textual) descriptions using embedding and\nautoencoders (AE), going through the classification of defaultable firms on the\nbase of a wide range of economic features using gradient boosting machines\n(GBM) and calibrating their probabilities paying due attention to the treatment\nof unbalanced samples. Finally we assign credit ratings through genetic\nalgorithms (differential evolution, DE). Model interpretability is achieved by\nimplementing recent techniques such as SHAP and LIME, which explain predictions\nlocally in features' space.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:29:06 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Provenzano", "A. R.", ""], ["Trifir\u00f2", "D.", ""], ["Datteo", "A.", ""], ["Giada", "L.", ""], ["Jean", "N.", ""], ["Riciputi", "A.", ""], ["Pera", "G. Le", ""], ["Spadaccino", "M.", ""], ["Massaron", "L.", ""], ["Nordio", "C.", ""]]}, {"id": "2008.02420", "submitter": "Zuo Quan Xu Dr.", "authors": "Xiangyu Wang, Jianming Xia, Zuo Quan Xu, Zhou Yang", "title": "Minimal Quantile Functions Subject to Stochastic Dominance Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of finding an SSD-minimal quantile function subject to\nthe mixture of multiple first-order stochastic dominance (FSD) and second-order\nstochastic dominance (SSD) constraints. The solution is explicitly worked out\nand has a closed relation to the Skorokhod problem. We then apply this result\nto solve an expenditure minimization problem with the mixture of an FSD\nconstraint and an SSD constraint in financial economics.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 01:41:29 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Wang", "Xiangyu", ""], ["Xia", "Jianming", ""], ["Xu", "Zuo Quan", ""], ["Yang", "Zhou", ""]]}, {"id": "2008.03123", "submitter": "Veronique Maume-Deschamps", "authors": "Weihong Ni, Corina Constantinescu, Alfredo Eg\\'idio dos Reis,\n  V\\'eronique Maume-Deschamps (ICJ, PSPM)", "title": "Pricing foreseeable and unforeseeable risks in insurance portfolios", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.07157", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we propose a method for pricing insurance products that\ncover not only traditional risks, but also unforeseen ones. By considering the\nPoisson process parameter to be a mixed random variable, we capture the\nheterogeneity of foreseeable and unforeseeable risks. To illustrate, we\nestimate the weights for the two risk streams for a real dataset from a\nPortuguese insurer. To calculate the premium, we set the frequency and severity\nas distributions that belong to the linear exponential family. Under a Bayesian\nsetup , we show that when working with a finite mixture of conjugate priors,\nthe premium can be estimated by a mixture of posterior means, with updated\nparameters, depending on claim histories. We emphasise the riskiness of the\nunforeseeable trend, by choosing heavy-tailed distributions. After estimating\ndistribution parameters involved using the Expectation-Maximization algorithm,\nwe found that Bayesian premiums derived are more reactive to claim trends than\ntraditional ones.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:25:27 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Ni", "Weihong", "", "ICJ, PSPM"], ["Constantinescu", "Corina", "", "ICJ, PSPM"], ["Reis", "Alfredo Eg\u00eddio dos", "", "ICJ, PSPM"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ, PSPM"]]}, {"id": "2008.03672", "submitter": "Abootaleb Shirvani", "authors": "Thilini V. Mahanama and Abootaleb Shirvani", "title": "A Natural Disasters Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural disasters, such as tornadoes, floods, and wildfire pose risks to life\nand property, requiring the intervention of insurance corporations. One of the\nmost visible consequences of changing climate is an increase in the intensity\nand frequency of extreme weather events. The relative strengths of these\ndisasters are far beyond the habitual seasonal maxima, often resulting in\nsubsequent increases in property losses. Thus, insurance policies should be\nmodified to endure increasingly volatile catastrophic weather events. We\npropose a Natural Disasters Index (NDI) for the property losses caused by\nnatural disasters in the United States based on the \"Storm Data\" published by\nthe National Oceanic and Atmospheric Administration. The proposed NDI is an\nattempt to construct a financial instrument for hedging the intrinsic risk. The\nNDI is intended to forecast the degree of future risk that could forewarn the\ninsurers and corporations allowing them to transfer insurance risk to capital\nmarket investors. This index could also be modified to other regions and\ncountries.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 06:37:10 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Mahanama", "Thilini V.", ""], ["Shirvani", "Abootaleb", ""]]}, {"id": "2008.04110", "submitter": "Xian-Min Jin", "authors": "Hao Tang, Anurag Pal, Lu-Feng Qiao, Tian-Yu Wang, Jun Gao, Xian-Min\n  Jin", "title": "Quantum Computation for Pricing the Collateralized Debt Obligations", "comments": "19 pages, 18 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collateralized debt obligation (CDO) has been one of the most commonly used\nstructured financial products and is intensively studied in quantitative\nfinance. By setting the asset pool into different tranches, it effectively\nworks out and redistributes credit risks and returns to meet the risk\npreferences for different tranche investors. The copula models of various kinds\nare normally used for pricing CDOs, and the Monte Carlo simulations are\nrequired to get their numerical solution. Here we implement two typical CDO\nmodels, the single-factor Gaussian copula model and Normal Inverse Gaussian\ncopula model, and by applying the conditional independence approach, we manage\nto load each model of distribution in quantum circuits. We then apply quantum\namplitude estimation as an alternative to Monte Carlo simulation for CDO\npricing. We demonstrate the quantum computation results using IBM Qiskit. Our\nwork addresses a useful task in finance instrument pricing, significantly\nbroadening the application scope for quantum computing in finance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 18:00:01 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:41:41 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Tang", "Hao", ""], ["Pal", "Anurag", ""], ["Qiao", "Lu-Feng", ""], ["Wang", "Tian-Yu", ""], ["Gao", "Jun", ""], ["Jin", "Xian-Min", ""]]}, {"id": "2008.04782", "submitter": "Adit Chopra", "authors": "Adit Chopra, Abhi Bansal, Aryaman Wadhwa", "title": "Evidence of Predicting Early Signs of Corporate Bankruptcy Using\n  Financial Ratios in the Indian Landscape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corporate bankruptcy impacts the functioning of the economy as it impacts its\nvarious stakeholders: Shareholders, financial and operational lenders, and the\ngovernment. This paper aims to study the impact of a wide array of\nprofitability, leverage and efficiency ratios to predict early signs of\nbankruptcy in public listed companies in India using a logistic regression\nconsidering impacts at two levels: one year and two years before the filing of\nbankruptcy with the NCLT during the year 2019. The study proves that the\naccuracies of the classification model are 81.4% and 85.1% respectively for one\nyear and two years before the bankruptcy.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 18:56:09 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Chopra", "Adit", ""], ["Bansal", "Abhi", ""], ["Wadhwa", "Aryaman", ""]]}, {"id": "2008.05147", "submitter": "Vica Tendenan", "authors": "Vica Tendenan, Richard Gerlach, and Chao Wang", "title": "Tail risk forecasting using Bayesian realized EGARCH models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a Bayesian framework for the realized exponential\ngeneralized autoregressive conditional heteroskedasticity (realized EGARCH)\nmodel, which can incorporate multiple realized volatility measures for the\nmodelling of a return series. The realized EGARCH model is extended by adopting\na standardized Student-t and a standardized skewed Student-t distribution for\nthe return equation. Different types of realized measures, such as sub-sampled\nrealized variance, sub-sampled realized range, and realized kernel, are\nconsidered in the paper. The Bayesian Markov chain Monte Carlo (MCMC)\nestimation employs the robust adaptive Metropolis algorithm (RAM) in the burn\nin period and the standard random walk Metropolis in the sample period. The\nBayesian estimators show more favourable results than maximum likelihood\nestimators in a simulation study. We test the proposed models with several\nindices to forecast one-step-ahead Value at Risk (VaR) and Expected Shortfall\n(ES) over a period of 1000 days. Rigorous tail risk forecast evaluations show\nthat the realized EGARCH models employing the standardized skewed Student-t\ndistribution and incorporating sub-sampled realized range are favored, compared\nto a range of models.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 07:36:57 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 11:37:50 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Tendenan", "Vica", ""], ["Gerlach", "Richard", ""], ["Wang", "Chao", ""]]}, {"id": "2008.05653", "submitter": "Feng Fu", "authors": "Jonathan Meng and Feng Fu", "title": "Understanding Gambling Behavior and Risk Attitudes Using\n  Cryptocurrency-based Casino Blockchain Data", "comments": "21 pages. Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical concept of Gambler's Ruin suggests that gambling has a large\namount of risk. Nevertheless, gambling at casinos and gambling on the Internet\nare both hugely popular activities. In recent years, both prospect theory and\nlab-controlled experiments have been used to improve our understanding of risk\nattitudes associated with gambling. Despite theoretical progress, collecting\nreal-life gambling data, which is essential to validate predictions and\nexperimental findings, remains a challenge. To address this issue, we collect\npublicly available betting data from a \\emph{DApp} (decentralized application)\non the Ethereum Blockchain, which instantly publishes the outcome of every\nsingle bet (consisting of each bet's timestamp, wager, probability of winning,\nuserID, and profit). This online casino is a simple dice game that allows\ngamblers to tune their own winning probabilities. Thus the dataset is well\nsuited for studying gambling strategies and the complex dynamic of risk\nattitudes involved in betting decisions. We analyze the dataset through the\nlens of current probability-theoretic models and discover empirical examples of\ngambling systems. Our results shed light on understanding the role of risk\npreferences in human financial behavior and decision-makings beyond gambling.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 02:45:10 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 13:00:22 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Meng", "Jonathan", ""], ["Fu", "Feng", ""]]}, {"id": "2008.05693", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi, Gregory Clive Taylor, Melantha Wang, Bernard Wong", "title": "SynthETIC: an individual insurance claim simulator with feature control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen rapid increase in the application of machine learning\nto insurance loss reserving. They yield most value when applied to large data\nsets, such as individual claims, or large claim triangles. In short, they are\nlikely to be useful in the analysis of any data set whose volume is sufficient\nto obscure a naked-eye view of its features. Unfortunately, such large data\nsets are in short supply in the actuarial literature. Accordingly, one needs to\nturn to synthetic data. Although the ultimate objective of these methods is\napplication to real data, the use of synthetic data containing features\ncommonly observed in real data is also to be encouraged.\n  While there are a number of claims simulators in existence, each valuable\nwithin its own context, the inclusion of a number of desirable (but\ncomplicated) data features requires further development. Accordingly, in this\npaper we review those desirable features, and propose a new simulator of\nindividual claim experience called SynthETIC.\n  Our simulator is publicly available, open source, and fills a gap in the\nnon-life actuarial toolkit. The simulator specifically allows for desirable\n(but optionally complicated) data features typically occurring in practice,\nsuch as variations in rates of settlements and development patterns; as with\nsuperimposed inflation, and various discontinuities, and also enables various\ndependencies between variables. The user has full control of the mechanics of\nthe evolution of an individual claim. As a result, the complexity of the data\nset generated (meaning the level of difficulty of analysis) may be dialled\nanywhere from extremely simple to extremely complex.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 05:24:34 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 03:32:31 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 13:18:00 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Taylor", "Gregory Clive", ""], ["Wang", "Melantha", ""], ["Wong", "Bernard", ""]]}, {"id": "2008.05824", "submitter": "M. Andrea Arias-Serna", "authors": "M. Andrea Arias-Serna, Jean-Michel Loubes, Francisco J. Caro-Lopera", "title": "Risk Measures Estimation Under Wasserstein Barycenter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomness in financial markets requires modern and robust multivariate\nmodels of risk measures. This paper proposes a new approach for modeling\nmultivariate risk measures under Wasserstein barycenters of probability\nmeasures supported on location-scatter families. Simple and advanced copulas\nmultivariate Value at Risk models are compared with the derived technique. The\nperformance of the model is also checked in market indices of United States\ngenerated by the financial crisis due to COVID-19. The introduced model behaves\nsatisfactory in both common and volatile periods of asset prices, providing\nrealistic VaR forecast in this era of social distancing.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 11:23:20 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Arias-Serna", "M. Andrea", ""], ["Loubes", "Jean-Michel", ""], ["Caro-Lopera", "Francisco J.", ""]]}, {"id": "2008.07082", "submitter": "Zuo Quan Xu Dr.", "authors": "Chonghu Guan, Jing Peng, Zuo Quan Xu", "title": "A free boundary problem arising from a multi-state regime-switching\n  stock trading model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a free boundary problem, which arises from an optimal\ntrading problem of a stock that is driven by a uncertain market status process.\nThe free boundary problem is a variational inequality system of three functions\nwith a degenerate operator. The main contribution of this paper is that we not\nonly prove all the four switching free boundaries are no-overlapping, monotonic\nand $C^{\\infty}$-smooth, but also completely determine their relative\nlocalities and provide the optimal trading strategies for the stock trading\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 04:34:29 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Guan", "Chonghu", ""], ["Peng", "Jing", ""], ["Xu", "Zuo Quan", ""]]}, {"id": "2008.07103", "submitter": "Shengchao Zhuang", "authors": "Yichun Chi, Xun Yu Zhou and Sheng Chao Zhuang", "title": "Variance Contracts", "comments": "42 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of an optimal insurance contract in which the insured\nmaximizes her expected utility and the insurer limits the variance of his risk\nexposure while maintaining the principle of indemnity and charging the premium\naccording to the expected value principle. We derive the optimal policy\nsemi-analytically, which is coinsurance above a deductible when the variance\nbound is binding. This policy automatically satisfies the incentive-compatible\ncondition, which is crucial to rule out ex post moral hazard. We also find that\nthe deductible is absent if and only if the contract pricing is actuarially\nfair. Focusing on the actuarially fair case, we carry out comparative statics\non the effects of the insured's initial wealth and the variance bound on\ninsurance demand. Our results indicate that the expected coverage is always\nlarger for a wealthier insured, implying that the underlying insurance is a\nnormal good, which supports certain recent empirical findings. Moreover, as the\nvariance constraint tightens, the insured who is prudent cedes less losses,\nwhile the insurer is exposed to less tail risk.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 05:58:59 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Chi", "Yichun", ""], ["Zhou", "Xun Yu", ""], ["Zhuang", "Sheng Chao", ""]]}, {"id": "2008.07564", "submitter": "Eduardo Ramos", "authors": "Eduardo Ramos-P\\'erez, Pablo J. Alonso-Gonz\\'alez, Jos\\'e Javier\n  N\\'u\\~nez-Vel\\'azquez", "title": "Stochastic reserving with a stacked model based on a hybridized\n  Artificial Neural Network", "comments": null, "journal-ref": "Expert Systems with Applications, Volume 163, January 2021", "doi": "10.1016/j.eswa.2020.113782", "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, legal requirements demand that insurance companies increase their\nemphasis on monitoring the risks linked to the underwriting and asset\nmanagement activities. Regarding underwriting risks, the main uncertainties\nthat insurers must manage are related to the premium sufficiency to cover\nfuture claims and the adequacy of the current reserves to pay outstanding\nclaims. Both risks are calibrated using stochastic models due to their nature.\nThis paper introduces a reserving model based on a set of machine learning\ntechniques such as Gradient Boosting, Random Forest and Artificial Neural\nNetworks. These algorithms and other widely used reserving models are stacked\nto predict the shape of the runoff. To compute the deviation around a former\nprediction, a log-normal approach is combined with the suggested model. The\nempirical results demonstrate that the proposed methodology can be used to\nimprove the performance of the traditional reserving techniques based on\nBayesian statistics and a Chain Ladder, leading to a more accurate assessment\nof the reserving risk.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 18:26:05 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ramos-P\u00e9rez", "Eduardo", ""], ["Alonso-Gonz\u00e1lez", "Pablo J.", ""], ["N\u00fa\u00f1ez-Vel\u00e1zquez", "Jos\u00e9 Javier", ""]]}, {"id": "2008.08733", "submitter": "Zachary Feinstein", "authors": "Hamed Amini and Zachary Feinstein", "title": "Optimal Network Compression", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a formulation of the optimal network compression\nproblem for financial systems. This general formulation is presented for\ndifferent levels of network compression or rerouting allowed from the initial\ninterbank network. We prove that this problem is, generically, NP-hard. We\nfocus on objective functions generated by systemic risk measures under\nsystematic shocks to the financial network. We conclude by studying the optimal\ncompression problem for specific networks; this permits us to study the\nso-called robust fragility of certain network topologies more generally as well\nas the potential benefits and costs of network compression. In particular,\nunder systematic shocks and heterogeneous financial networks the typical\nheuristics of robust fragility no longer hold generally.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 02:11:23 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 20:21:21 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 10:04:59 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Amini", "Hamed", ""], ["Feinstein", "Zachary", ""]]}, {"id": "2008.09108", "submitter": "Konstantin E. Feldman", "authors": "K.E. Feldman", "title": "Analytic Calibration in Andreasen-Huge SABR Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP math.PR q-fin.MF q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive analytic formulae which link $\\alpha$, $\\nu$ and $\\rho$ parameters\nin Andreasen-Huge style SABR model to the ATM price and option prices at four\nstrikes close to ATM. Based on these formulae we give a characterisation for\nthe SABR parameters in terms of derivatives of the swap rate forward\nprobability density function. We test the analytic result in the application to\nthe interest rate futures option market.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:51:47 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 07:03:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Feldman", "K. E.", ""]]}, {"id": "2008.09818", "submitter": "Anand Deo", "authors": "Anand Deo and Karthyek Murthy", "title": "Optimizing tail risks using an importance sampling based extrapolation\n  for heavy-tailed objectives", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the prominence of Conditional Value-at-Risk (CVaR) as a measure\nfor tail risk in settings affected by uncertainty, we develop a new formula for\napproximating CVaR based optimization objectives and their gradients from\nlimited samples. A key difficulty that limits the widespread practical use of\nthese optimization formulations is the large amount of data required by the\nstate-of-the-art sample average approximation schemes to approximate the CVaR\nobjective with high fidelity. Unlike the state-of-the-art sample average\napproximations which require impractically large amounts of data in tail\nprobability regions, the proposed approximation scheme exploits the\nself-similarity of heavy-tailed distributions to extrapolate data from suitable\nlower quantiles. The resulting approximations are shown to be statistically\nconsistent and are amenable for optimization by means of conventional gradient\ndescent. The approximation is guided by means of a systematic\nimportance-sampling scheme whose asymptotic variance reduction properties are\nrigorously examined. Numerical experiments demonstrate the superiority of the\nproposed approximations and the ease of implementation points to the\nversatility of settings to which the approximation scheme can be applied.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 11:46:58 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Deo", "Anand", ""], ["Murthy", "Karthyek", ""]]}, {"id": "2008.12427", "submitter": "Stephen Mildenhall", "authors": "John A. Major, Stephen J. Mildenhall", "title": "Pricing and Capital Allocation for Multiline Insurance Firms With Finite\n  Assets in an Imperfect Market", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze multiline pricing and capital allocation in equilibrium\nno-arbitrage markets. Existing theories often assume a perfect complete market,\nbut when pricing is linear, there is no diversification benefit from risk\npooling and therefore no role for insurance companies. Instead of a perfect\nmarket, we assume a non-additive distortion pricing functional and the\nprinciple of equal priority of payments in default. Under these assumptions, we\nderive a canonical allocation of premium and margin, with properties that merit\nthe name the natural allocation. The natural allocation gives non-negative\nmargins to all independent lines for default-free insurance but can exhibit\nnegative margins for low-risk lines under limited liability. We introduce novel\nconditional expectation measures of relative risk within a portfolio and use\nthem to derive simple, intuitively appealing expressions for risk margins and\ncapital allocations. We give a unique capital allocation consistent with our\nlaw invariant pricing functional. Such allocations produce returns that vary by\nline, in contrast to many other approaches. Our model provides a bridge between\nthe theoretical perspective that there should be no compensation for bearing\ndiversifiable risk and the empirical observation that more risky lines fetch\nhigher margins relative to subjective expected values.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 01:26:23 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Major", "John A.", ""], ["Mildenhall", "Stephen J.", ""]]}, {"id": "2008.13087", "submitter": "Mingbin Feng", "authors": "Mingbin Ben Feng and Eunhye Song", "title": "Optimal Nested Simulation Experiment Design via Likelihood Ratio Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested simulation arises frequently in {risk management} or uncertainty\nquantification problems, where the performance measure is a function of the\nsimulation output mean conditional on the outer scenario. The standard nested\nsimulation samples $M$ outer scenarios and runs $N$ inner replications at each.\nWe propose a new experiment design framework for a problem whose inner\nreplication's inputs are generated from distributions parameterized by the\nouter scenario. This structure lets us pool replications from an outer scenario\nto estimate another scenario's conditional mean via the likelihood ratio\nmethod. We formulate a bi-level optimization problem to decide not only which\nof $M$ outer scenarios to simulate and how many times to replicate at each, but\nalso how to pool these replications such that the total simulation effort is\nminimized while achieving a target level of {precision}. The resulting optimal\ndesign requires far less simulation effort than $MN$. We provide asymptotic\nanalyses on the convergence rates of the performance measure estimators\ncomputed from the experiment design. Empirical results show that our experiment\ndesign reduces the simulation effort by orders of magnitude compared to the\nstandard nested simulation and outperforms a state-of-the-art regression-based\ndesign that pools replications via regression.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 04:19:39 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 04:09:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Feng", "Mingbin Ben", ""], ["Song", "Eunhye", ""]]}, {"id": "2008.13198", "submitter": "Thierry Roncalli", "authors": "Th\\'eo Roncalli, Th\\'eo Le Guenedal, Fr\\'ed\\'eric Lepetit, Thierry\n  Roncalli, Takaya Sekine", "title": "Measuring and Managing Carbon Risk in Investment Portfolios", "comments": "58 pages, 32 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the impact of carbon risk on stock pricing. To address\nthis, we consider the seminal approach of G\\\"orgen \\textsl{et al.} (2019), who\nproposed estimating the carbon financial risk of equities by their carbon beta.\nTo achieve this, the primary task is to develop a brown-minus-green (or BMG)\nrisk factor, similar to Fama and French (1992). Secondly, we must estimate the\ncarbon beta using a multi-factor model. While G\\\"orgen \\textsl{et al.} (2019)\nconsidered that the carbon beta is constant, we propose a time-varying\nestimation model to assess the dynamics of the carbon risk. Moreover, we test\nseveral specifications of the BMG factor to understand which climate\nchange-related dimensions are priced in by the stock market. In the second part\nof the article, we focus on the carbon risk management of investment\nportfolios. First, we analyze how carbon risk impacts the construction of a\nminimum variance portfolio. As the goal of this portfolio is to reduce\nunrewarded financial risks of an investment, incorporating the carbon risk into\nthis approach fulfils this objective. Second, we propose a new framework for\nbuilding enhanced index portfolios with a lower exposure to carbon risk than\ncapitalization-weighted stock indices. Finally, we explore how carbon\nsensitivities can improve the robustness of factor investing portfolios.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 15:41:02 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Roncalli", "Th\u00e9o", ""], ["Guenedal", "Th\u00e9o Le", ""], ["Lepetit", "Fr\u00e9d\u00e9ric", ""], ["Roncalli", "Thierry", ""], ["Sekine", "Takaya", ""]]}, {"id": "2008.13309", "submitter": "Jian Wu", "authors": "Jian Wu, William B. Haskell, Wenjie Huang, and Huifu Xu", "title": "Preference Robust Optimization for Quasi-concave Choice Functions", "comments": "53 pages, 7 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In behavioural economics, a decision maker's preferences are expressed by\nchoice functions. Preference robust optimization (PRO) is concerned with\nproblems where the decision maker's preferences are ambiguous, and the optimal\ndecision is based on a robust choice function with respect to a preference\nambiguity set. In this paper, we propose a PRO model to support choice\nfunctions that are: (i) monotonic (prefer more to less), (ii) quasi-concave\n(prefer diversification), and (iii) multi-attribute (have multiple\nobjectives/criteria). As our main result, we show that the robust choice\nfunction can be constructed efficiently by solving a sequence of linear\nprogramming problems. Then, the robust choice function can be optimized\nefficiently by solving a sequence of convex optimization problems. Our\nnumerical experiments for the portfolio optimization and capital allocation\nproblems show that our method is practical and scalable.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 01:08:40 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 18:08:03 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 02:22:48 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Wu", "Jian", ""], ["Haskell", "William B.", ""], ["Huang", "Wenjie", ""], ["Xu", "Huifu", ""]]}]