[{"id": "2005.01160", "submitter": "Piero Mazzarisi", "authors": "Piero Mazzarisi, Silvia Zaoli, Carlo Campajola, Fabrizio Lillo", "title": "Tail Granger causalities and where to find them: extreme risk spillovers\n  vs. spurious linkages", "comments": "22 pages, 7 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying risk spillovers in financial markets is of great importance for\nassessing systemic risk and portfolio management. Granger causality in tail (or\nin risk) tests whether past extreme events of a time series help predicting\nfuture extreme events of another time series. The topology and connectedness of\nnetworks built with Granger causality in tail can be used to measure systemic\nrisk and to identify risk transmitters. Here we introduce a novel test of\nGranger causality in tail which adopts the likelihood ratio statistic and is\nbased on the multivariate generalization of a discrete autoregressive process\nfor binary time series describing the sequence of extreme events of the\nunderlying price dynamics. The proposed test has very good size and power in\nfinite samples, especially for large sample size, allows inferring the correct\ntime scale at which the causal interaction takes place, and it is flexible\nenough for multivariate extension when more than two time series are considered\nin order to decrease false detections as spurious effect of neglected\nvariables. An extensive simulation study shows the performances of the proposed\nmethod with a large variety of data generating processes and it introduces also\nthe comparison with the test of Granger causality in tail by [Hong et al.,\n2009]. We report both advantages and drawbacks of the different approaches,\npointing out some crucial aspects related to the false detections of Granger\ncausality for tail events. An empirical application to high frequency data of a\nportfolio of US stocks highlights the merits of our novel approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 18:27:50 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 16:46:15 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Mazzarisi", "Piero", ""], ["Zaoli", "Silvia", ""], ["Campajola", "Carlo", ""], ["Lillo", "Fabrizio", ""]]}, {"id": "2005.01686", "submitter": "Andreas Hoepner PhD", "authors": "Alexander Arimond, Damian Borth, Andreas Hoepner, Michael Klawunn and\n  Stefan Weisheit", "title": "Neural Networks and Value at Risk", "comments": "2019 Financial Data Science Association Paper, San Francisco", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilizing a generative regime switching framework, we perform Monte-Carlo\nsimulations of asset returns for Value at Risk threshold estimation. Using\nequity markets and long term bonds as test assets in the global, US, Euro area\nand UK setting over an up to 1,250 weeks sample horizon ending in August 2018,\nwe investigate neural networks along three design steps relating (i) to the\ninitialization of the neural network, (ii) its incentive function according to\nwhich it has been trained and (iii) the amount of data we feed. First, we\ncompare neural networks with random seeding with networks that are initialized\nvia estimations from the best-established model (i.e. the Hidden Markov). We\nfind latter to outperform in terms of the frequency of VaR breaches (i.e. the\nrealized return falling short of the estimated VaR threshold). Second, we\nbalance the incentive structure of the loss function of our networks by adding\na second objective to the training instructions so that the neural networks\noptimize for accuracy while also aiming to stay in empirically realistic regime\ndistributions (i.e. bull vs. bear market frequencies). In particular this\ndesign feature enables the balanced incentive recurrent neural network (RNN) to\noutperform the single incentive RNN as well as any other neural network or\nestablished approach by statistically and economically significant levels.\nThird, we half our training data set of 2,000 days. We find our networks when\nfed with substantially less data (i.e. 1,000 days) to perform significantly\nworse which highlights a crucial weakness of neural networks in their\ndependence on very large data sets ...\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:41:59 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 10:22:13 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Arimond", "Alexander", ""], ["Borth", "Damian", ""], ["Hoepner", "Andreas", ""], ["Klawunn", "Michael", ""], ["Weisheit", "Stefan", ""]]}, {"id": "2005.02318", "submitter": "Lucio Fernandez-Arjona", "authors": "Lucio Fernandez-Arjona", "title": "A neural network model for solvency calculations in life insurance", "comments": "20 pages, 6 figures, 4 tables", "journal-ref": null, "doi": "10.1017/S1748499520000330", "report-no": null, "categories": "q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insurance companies make extensive use of Monte Carlo simulations in their\ncapital and solvency models. To overcome the computational problems associated\nwith Monte Carlo simulations, most large life insurance companies use proxy\nmodels such as replicating portfolios.\n  In this paper, we present an example based on a variable annuity guarantee,\nshowing the main challenges faced by practitioners in the construction of\nreplicating portfolios: the feature engineering step and subsequent basis\nfunction selection problem.\n  We describe how neural networks can be used as a proxy model and how to apply\nrisk-neutral pricing on a neural network to integrate such a model into a\nmarket risk framework. The proposed model naturally solves the feature\nengineering and feature selection problems of replicating portfolios.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 16:22:09 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Fernandez-Arjona", "Lucio", ""]]}, {"id": "2005.02633", "submitter": "Alessandro Gnoatto", "authors": "Alessandro Gnoatto, Athena Picarelli, Christoph Reisinger", "title": "Deep xVA solver -- A neural network based counterparty credit risk\n  management framework", "comments": "25 pages. Error bounds added", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.CP q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel computational framework for portfolio-wide\nrisk management problems, where the presence of a potentially large number of\nrisk factors makes traditional numerical techniques ineffective. The new method\nutilises a coupled system of BSDEs for the valuation adjustments (xVA) and\nsolves these by a recursive application of a neural network based BSDE solver.\nThis not only makes the computation of xVA for high-dimensional problems\nfeasible, but also produces hedge ratios and dynamic risk measures for xVA, and\nallows simulations of the collateral account.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 07:36:31 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 05:40:25 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Gnoatto", "Alessandro", ""], ["Picarelli", "Athena", ""], ["Reisinger", "Christoph", ""]]}, {"id": "2005.02950", "submitter": "Takaaki Koike", "authors": "Takaaki Koike and Marius Hofert", "title": "Modality for Scenario Analysis and Maximum Likelihood Allocation", "comments": "41 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the variability of a risk from the statistical viewpoint of\nmultimodality of the conditional loss distribution given that the aggregate\nloss equals an exogenously provided capital. This conditional distribution\nserves as a building block for calculating risk allocations such as the Euler\ncapital allocation of Value-at-Risk. A superlevel set of this conditional\ndistribution can be interpreted as a set of severe and plausible stress\nscenarios the given capital is supposed to cover. We show that various\ndistributional properties of this conditional distribution, such as modality,\ndependence and tail behavior, are inherited from those of the underlying joint\nloss distribution. Among these properties, we find that modality of the\nconditional distribution is an important feature in risk assessment related to\nthe variety of risky scenarios likely to occur in a stressed situation. Under\nunimodality, we introduce a novel risk allocation method called maximum\nlikelihood allocation (MLA), defined as the mode of the conditional\ndistribution given the total capital. Under multimodality, a single vector of\nallocations can be less sound. To overcome this issue, we investigate the\nso-called multimodalty adjustment to increase the soundness of risk\nallocations. Properties of the conditional distribution, MLA and multimodality\nadjustment are demonstrated in numerical experiments. In particular, we observe\nthat negative dependence among losses typically leads to multimodality, and\nthus a higher multimodality adjustment can be required.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:43:17 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 02:08:18 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Koike", "Takaaki", ""], ["Hofert", "Marius", ""]]}, {"id": "2005.03500", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi and Gregory Clive Taylor and Phuong Anh Vu and Bernard\n  Wong", "title": "On unbalanced data and common shock models in stochastic loss reserving", "comments": null, "journal-ref": "Ann. actuar. sci. 15 (2021) 173-203", "doi": "10.1017/S1748499520000196", "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introducing common shocks is a popular dependence modelling approach, with\nsome recent applications in loss reserving. The main advantage of this approach\nis the ability to capture structural dependence coming from known\nrelationships. In addition, it helps with the parsimonious construction of\ncorrelation matrices of large dimensions. However, complications arise in the\npresence of \"unbalanced data\", that is, when (expected) magnitude of\nobservations over a single triangle, or between triangles, can vary\nsubstantially. Specifically, if a single common shock is applied to all of\nthese cells, it can contribute insignificantly to the larger values and/or\nswamp the smaller ones, unless careful adjustments are made. This problem is\nfurther complicated in applications involving negative claim amounts. In this\npaper, we address this problem in the loss reserving context using a common\nshock Tweedie approach for unbalanced data. We show that the solution not only\nprovides a much better balance of the common shock proportions relative to the\nunbalanced data, but it is also parsimonious. Finally, the common shock Tweedie\nmodel also provides distributional tractability.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:03:34 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 09:01:40 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Taylor", "Gregory Clive", ""], ["Vu", "Phuong Anh", ""], ["Wong", "Bernard", ""]]}, {"id": "2005.03554", "submitter": "Yerkin Kitapbayev", "authors": "Yerkin Kitapbayev, Scott Robertson", "title": "Mortgage Contracts and Underwater Default", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze recently proposed mortgage contracts that aim to eliminate\nselective borrower default when the loan balance exceeds the house price (the\n``underwater'' effect). We show contracts that automatically reduce the\noutstanding balance in the event of house price decline remove the default\nincentive, but may induce prepayment in low price states. However, low state\nprepayments vanish if the benefit from home ownership is sufficiently high. We\nalso show that capital gain sharing features, such as prepayment penalties in\nhigh house price states, are ineffective as they virtually eliminate\nprepayment. For observed foreclosure costs, we find that contracts with\nautomatic balance adjustments become preferable to the traditional fixed-rate\ncontracts at mortgage rate spreads between 50-100 basis points. We obtain these\nresults for perpetual versions of the contracts using American options pricing\nmethodology, in a continuous-time model with diffusive home prices. The\ncontracts' values and optimal decision rules are associated with free boundary\nproblems, which admit semi-explicit solutions.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:40:57 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 20:43:01 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 16:56:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kitapbayev", "Yerkin", ""], ["Robertson", "Scott", ""]]}, {"id": "2005.03698", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Proving prediction prudence", "comments": "23 pages, some typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to perform tests on samples of pairs of observations and\npredictions in order to assess whether or not the predictions are prudent.\nPrudence requires that that the mean of the difference of the\nobservation-prediction pairs can be shown to be significantly negative. For\nsafe conclusions, we suggest testing both unweighted (or equally weighted) and\nweighted means and explicitly taking into account the randomness of individual\npairs. The test methods presented are mainly specified as bootstrap and normal\napproximation algorithms. The tests are general but can be applied in\nparticular in the area of credit risk, both for regulatory and accounting\npurposes.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:47:12 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 17:19:01 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "2005.04868", "submitter": "Chao Wang Dr", "authors": "Giuseppe Storti and Chao Wang", "title": "Nonparametric Expected Shortfall Forecasting Incorporating Weighted\n  Quantiles", "comments": "38 pages, 2 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new semi-parametric Expected Shortfall (ES) estimation and forecasting\nframework is proposed. The proposed approach is based on a two-step estimation\nprocedure. The first step involves the estimation of Value-at-Risk (VaR) at\ndifferent quantile levels through a set of quantile time series regressions.\nThen, the ES is computed as a weighted average of the estimated quantiles. The\nquantiles weighting structure is parsimoniously parameterized by means of a\nBeta weight function whose coefficients are optimized by minimizing a joint VaR\nand ES loss function of the Fissler-Ziegel class. The properties of the\nproposed approach are first evaluated with an extensive simulation study using\ntwo data generating processes. Two forecasting studies with different\nout-of-sample sizes are then conducted, one of which focuses on the 2008 Global\nFinancial Crisis (GFC) period. The proposed models are applied to 7 stock\nmarket indices and their forecasting performances are compared to those of a\nrange of parametric, non-parametric and semi-parametric models, including\nGARCH, Conditional AutoRegressive Expectile (CARE), joint VaR and ES quantile\nregression models and simple average of quantiles. The results of the\nforecasting experiments provide clear evidence in support of proposed models.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 05:33:38 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 00:30:47 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 04:05:29 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Storti", "Giuseppe", ""], ["Wang", "Chao", ""]]}, {"id": "2005.05364", "submitter": "Zachary Feinstein", "authors": "Maxim Bichuch, Zachary Feinstein", "title": "A Repo Model of Fire Sales with VWAP and LOB Pricing Mechanisms", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a network of banks that optimally choose a strategy of asset\nliquidations and borrowing in order to cover short term obligations. The\nborrowing is done in the form of collateralized repurchase agreements, the\nhaircut level of which depends on the total liquidations of all the banks.\nSimilarly the fire-sale price of the asset obtained by each of the banks\ndepends on the amount of assets liquidated by the bank itself and by other\nbanks. By nature of this setup, banks' behavior is considered as a Nash\nequilibrium. This paper provides two forms for market clearing to occur:\nthrough a common closing price and through an application of the limit order\nbook. The main results of this work are providing the existence of maximal and\nminimal clearing solutions (i.e., liquidations, borrowing, fire sale prices,\nand haircut levels) as well as sufficient conditions for uniqueness of the\nclearing solutions.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 18:13:01 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 21:43:54 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 12:17:02 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bichuch", "Maxim", ""], ["Feinstein", "Zachary", ""]]}, {"id": "2005.05428", "submitter": "Vsevolod Malinovskii", "authors": "Vsevolod Malinovskii", "title": "Value-at-Risk substitute for non-ruin capital is fallacious and\n  redundant", "comments": "21 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This seemed impossible to use a theoretically adequate but too sophisticated\nrisk measure called non-ruin capital, whence its widespread (including\nregulatory documents) replacement with an inadequate, but simple risk measure\ncalled Value-at-Risk. Conflicting with the idea by Albert Einstein that\n\"everything should be made as simple as possible, but not simpler\", this led to\nfallacious, and even deceitful (but generally accepted) standards and\nrecommendations. Arguing from the standpoint of mathematical theory of risk, we\naim to break this impasse.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:52:02 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Malinovskii", "Vsevolod", ""]]}, {"id": "2005.06015", "submitter": "Bin Zou", "authors": "Jun Deng and Bin Zou", "title": "Quadratic Hedging for Sequential Claims with Random Weights in Discrete\n  Time", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.PM q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a quadratic hedging problem for a sequence of contingent claims with\nrandom weights in discrete time. We obtain the optimal hedging strategy\nexplicitly in a recursive representation, without imposing the non-degeneracy\n(ND) condition on the model and square integrability on hedging strategies. We\nrelate the general results to hedging under random horizon and fair pricing in\nthe quadratic sense. We illustrate the significance of our results in an\nexample in which the ND condition fails.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 19:12:13 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 15:59:12 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Deng", "Jun", ""], ["Zou", "Bin", ""]]}, {"id": "2005.07967", "submitter": "Shintaro Mori", "authors": "Masato Hisakado, Shintaro Mori", "title": "Parameter estimation of default portfolios using the Merton model and\n  Phase transition", "comments": "19 pages, 5 figures", "journal-ref": "Physica A,vol. 563, 1 February, 2021, 125435", "doi": "10.1016/j.physa.2020.125435", "report-no": null, "categories": "q-fin.RM cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the parameter estimation of the probability of default (PD), the\ncorrelation between the obligors, and a phase transition. In our previous work,\nwe studied the problem using the beta-binomial distribution. A non-equilibrium\nphase transition with an order parameter occurs when the temporal correlation\ndecays by power law. In this article, we adopt the Merton model, which uses an\nasset correlation as the default correlation, and find that a phase transition\noccurs when the temporal correlation decays by power law. When the power index\nis less than one, the PD estimator converges slowly. Thus, it is difficult to\nestimate PD with limited historical data. Conversely, when the power index is\ngreater than one, the convergence speed is inversely proportional to the number\nof samples. We investigate the empirical default data history of several rating\nagencies. The estimated power index is in the slow convergence range when we\nuse long history data. This suggests that PD could have a long memory and that\nit is difficult to estimate parameters due to slow convergence.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 12:27:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Hisakado", "Masato", ""], ["Mori", "Shintaro", ""]]}, {"id": "2005.08929", "submitter": "Christian Wagner", "authors": "Marco Pagano, Christian Wagner, Josef Zechner", "title": "Disaster Resilience and Asset Prices", "comments": "40 pages, 11 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN econ.GN q-fin.EC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates whether security markets price the effect of social\ndistancing on firms' operations. We document that firms that are more resilient\nto social distancing significantly outperformed those with lower resilience\nduring the COVID-19 outbreak, even after controlling for the standard risk\nfactors. Similar cross-sectional return differentials already emerged before\nthe COVID-19 crisis: the 2014-19 cumulative return differential between more\nand less resilient firms is of similar size as during the outbreak, suggesting\ngrowing awareness of pandemic risk well in advance of its materialization.\nFinally, we use stock option prices to infer the market's return expectations\nafter the onset of the pandemic: even at a two-year horizon, stocks of more\npandemic-resilient firms are expected to yield significantly lower returns than\nless resilient ones, reflecting their lower exposure to disaster risk. Hence,\ngoing forward, markets appear to price exposure to a new risk factor, namely,\npandemic risk.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:56:02 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 11:52:06 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Pagano", "Marco", ""], ["Wagner", "Christian", ""], ["Zechner", "Josef", ""]]}, {"id": "2005.09974", "submitter": "Sergio Maffra", "authors": "Sergio Alvares Maffra, John Armstrong, Teemu Pennanen", "title": "Stochastic modeling of assets and liabilities with mortality risk", "comments": null, "journal-ref": null, "doi": "10.1111/j.1365-2966.2005.09974.x", "report-no": null, "categories": "q-fin.RM econ.EM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a general approach for stochastic modeling of assets\nreturns and liability cash-flows of a typical pensions insurer. On the asset\nside, we model the investment returns on equities and various classes of\nfixed-income instruments including short- and long-maturity fixed-rate bonds as\nwell as index-linked and corporate bonds. On the liability side, the risks are\ndriven by future mortality developments as well as price and wage inflation.\nAll the risk factors are modeled as a multivariate stochastic process that\ncaptures the dynamics and the dependencies across different risk factors. The\nmodel is easy to interpret and to calibrate to both historical data and to\nforecasts or expert views concerning the future. The simple structure of the\nmodel allows for efficient computations. The construction of a million\nscenarios takes only a few minutes on a personal computer. The approach is\nillustrated with an asset-liability analysis of a defined benefit pension fund.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 11:37:08 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Maffra", "Sergio Alvares", ""], ["Armstrong", "John", ""], ["Pennanen", "Teemu", ""]]}, {"id": "2005.10488", "submitter": "Takanobu Mizuta", "authors": "Takanobu Mizuta", "title": "Does an artificial intelligence perform market manipulation with its own\n  discretion? -- A genetic algorithm learns in an artificial market simulation", "comments": null, "journal-ref": "2020 IEEE Symposium Series on Computational Intelligence (SSCI)", "doi": "10.1109/SSCI47803.2020.9308349", "report-no": null, "categories": "q-fin.TR q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Who should be charged with responsibility for an artificial intelligence\nperforming market manipulation have been discussed. In this study, I\nconstructed an artificial intelligence using a genetic algorithm that learns in\nan artificial market simulation, and investigated whether the artificial\nintelligence discovers market manipulation through learning with an artificial\nmarket simulation despite a builder of artificial intelligence has no intention\nof market manipulation. As a result, the artificial intelligence discovered\nmarket manipulation as an optimal investment strategy. This result suggests\nnecessity of regulation, such as obligating builders of artificial intelligence\nto prevent artificial intelligence from performing market manipulation.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 07:00:31 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Mizuta", "Takanobu", ""]]}, {"id": "2005.12173", "submitter": "Ekaterina Serebryannikova", "authors": "Andrey Leonidov, Ilya Tipunin, Ekaterina Serebryannikova", "title": "On Evaluation of Risky Investment Projects. Investment Certainty\n  Equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of the study is to propose a methodology for evaluation and\nranking of risky investment projects.An investment certainty equivalence\napproach dual to the conventional separation of riskless and risky\ncontributions based on cash flow certainty equivalence is introduced. Proposed\nranking of investment projects is based on gauging them with the Omega measure,\nwhich is defined as the ratio of chances to obtain profit/return greater than\nsome critical (minimal acceptable) profitability over the chances to obtain the\nprofit/return less than the critical one.Detailed consideration of alternative\nriskless investment is presented. Various performance measures characterizing\ninvestment projects with a special focus on the role of reinvestment are\ndiscussed. Relation between the proposed methodology and the conventional\napproach based on utilization of risk-adjusted discount rate (RADR) is\ndiscussed. Findings are supported with an illustrative example.The methodology\nproposed can be used to rank projects of different nature, scale and lifespan.\nIn contrast to the conventional RADR approach for investment project\nevaluation, in the proposed method a risk profile of a specific project is\nexplicitly analyzed in terms of appropriate performance measure distribution.\nNo ad-hoc assumption about suitable risk-premium is made.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 15:42:19 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Leonidov", "Andrey", ""], ["Tipunin", "Ilya", ""], ["Serebryannikova", "Ekaterina", ""]]}, {"id": "2005.12473", "submitter": "Roba Bairakdar", "authors": "Roba Bairakdar, Lu Cao, Melina Mailhot", "title": "Range Value-at-Risk: Multivariate and Extreme Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of univariate Range Value-at-Risk, presented by Cont et al.\n(2010), is extended in the multidimensional setting. Traditional risk measures\nare not well suited when dealing with heavy-tail distributions and infinite\ntail expectations. The multivariate definitions of robust truncated tail\nexpectations are provided to overcome this problem. Robustness and other\nproperties as well as empirical estimators are derived. Closed-form expressions\nand special cases in the extreme value framework are also discussed. Numerical\nand graphical examples are provided to examine the accuracy of the empirical\nestimators.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 01:25:07 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Bairakdar", "Roba", ""], ["Cao", "Lu", ""], ["Mailhot", "Melina", ""]]}, {"id": "2005.12593", "submitter": "Bruno Bouchard", "authors": "Bruno Bouchard (CEREMADE), Adil Reghai, Benjamin Virrion (CEREMADE)", "title": "Computation of Expected Shortfall by fast detection of worst scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-step algorithm for the computation of the historical\nexpected shortfall such as defined by the Basel Minimum Capital Requirements\nfor Market Risk. At each step of the algorithm, we use Monte Carlo simulations\nto reduce the number of historical scenarios that potentially belong to the set\nof worst scenarios. The number of simulations increases as the number of\ncandidate scenarios is reduced and the distance between them diminishes. For\nthe most naive scheme, we show that the L p-error of the estimator of the\nExpected Shortfall is bounded by a linear combination of the probabilities of\ninversion of favorable and unfavorable scenarios at each step, and of the last\nstep Monte Carlo error associated to each scenario. By using concentration\ninequalities, we then show that, for sub-gamma pricing errors, the\nprobabilities of inversion converge at an exponential rate in the number of\nsimulated paths. We then propose an adaptative version in which the algorithm\nimproves step by step its knowledge on the unknown parameters of interest: mean\nand variance of the Monte Carlo estimators of the different scenarios. Both\nschemes can be optimized by using dynamic programming algorithms that can be\nsolved off-line. To our knowledge, these are the first non-asymptotic bounds\nfor such estimators. Our hypotheses are weak enough to allow for the use of\nestimators for the different scenarios and steps based on the same random\nvariables, which, in practice, reduces considerably the computational effort.\nFirst numerical tests are performed.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 09:33:43 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Bouchard", "Bruno", "", "CEREMADE"], ["Reghai", "Adil", "", "CEREMADE"], ["Virrion", "Benjamin", "", "CEREMADE"]]}, {"id": "2005.12619", "submitter": "Riccardo Doyle", "authors": "Riccardo Doyle", "title": "Using Network Interbank Contagion in Bank Default Prediction", "comments": "10 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interbank contagion can theoretically exacerbate losses in a financial system\nand lead to additional cascade defaults during downturn. In this paper we\nproduce default analysis using both regression and neural network models to\nverify whether interbank contagion offers any predictive explanatory power on\ndefault events. We predict defaults of U.S. domiciled commercial banks in the\nfirst quarter of 2010 using data from the preceding four quarters. A number of\nestablished predictors (such as Tier 1 Capital Ratio and Return on Equity) are\nincluded alongside contagion to gauge if the latter adds significance. Based on\nthis methodology, we conclude that interbank contagion is extremely explanatory\nin default prediction, often outperforming more established metrics, in both\nregression and neural network models. These findings have sizeable implications\nfor the future use of interbank contagion as a variable of interest for stress\ntesting, bank issued bond valuation and wider bank default prediction.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 10:37:35 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 22:34:37 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Doyle", "Riccardo", ""]]}, {"id": "2005.13417", "submitter": "Tim Janke", "authors": "Tim Janke and Florian Steinke", "title": "Probabilistic multivariate electricity price forecasting using implicit\n  generative ensemble post-processing", "comments": "To be presented at the 16th International Conference on Probabilistic\n  Methods Applied to Power Systems 2020 (PMAPS 2020)", "journal-ref": null, "doi": "10.1109/PMAPS47429.2020.9183687", "report-no": null, "categories": "stat.AP econ.EM q-fin.RM q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliable estimation of forecast uncertainties is crucial for\nrisk-sensitive optimal decision making. In this paper, we propose implicit\ngenerative ensemble post-processing, a novel framework for multivariate\nprobabilistic electricity price forecasting. We use a likelihood-free implicit\ngenerative model based on an ensemble of point forecasting models to generate\nmultivariate electricity price scenarios with a coherent dependency structure\nas a representation of the joint predictive distribution. Our ensemble\npost-processing method outperforms well-established model combination\nbenchmarks. This is demonstrated on a data set from the German day-ahead\nmarket. As our method works on top of an ensemble of domain-specific expert\nmodels, it can readily be deployed to other forecasting tasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 15:22:10 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Janke", "Tim", ""], ["Steinke", "Florian", ""]]}, {"id": "2005.14670", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "The energy distance for ensemble and scenario reduction", "comments": "Accepted for publication in Philosophical Transactions A", "journal-ref": null, "doi": "10.1098/rsta.2019.0431", "report-no": null, "categories": "stat.ML cs.LG math.OC q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scenario reduction techniques are widely applied for solving sophisticated\ndynamic and stochastic programs, especially in energy and power systems, but\nalso used in probabilistic forecasting, clustering and estimating generative\nadversarial networks (GANs). We propose a new method for ensemble and scenario\nreduction based on the energy distance which is a special case of the maximum\nmean discrepancy (MMD). We discuss the choice of energy distance in detail,\nespecially in comparison to the popular Wasserstein distance which is\ndominating the scenario reduction literature. The energy distance is a metric\nbetween probability measures that allows for powerful tests for equality of\narbitrary multivariate distributions or independence. Thanks to the latter, it\nis a suitable candidate for ensemble and scenario reduction problems. The\ntheoretical properties and considered examples indicate clearly that the\nreduced scenario sets tend to exhibit better statistical properties for the\nenergy distance than a corresponding reduction with respect to the Wasserstein\ndistance. We show applications to a Bernoulli random walk and two real data\nbased examples for electricity demand profiles and day-ahead electricity\nprices.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 16:52:23 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 18:46:10 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ziel", "Florian", ""]]}]