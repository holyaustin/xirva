[{"id": "2009.00062", "submitter": "Daniele Tantari", "authors": "Giovanni Calice, Carlo Sala, Daniele Tantari", "title": "Contingent Convertible Bonds in Financial Networks", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.SI q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of contingent convertible bonds (CoCos) in a network of\ninterconnected banks. We first confirm the phase transitions documented by\nAcemoglu et al. (2015) in absence of CoCos, thus revealing that the structure\nof the interbank network is of fundamental importance for the effectiveness of\nCoCos as a financial stability enhancing mechanism. Furthermore, we show that\nin the presence of a moderate financial shock lightly interconnected financial\nnetworks are more robust than highly interconnected networks, and can possibly\nbe the optimal choice for both CoCos issuers and buyers. Finally our results\nshow that, under some network structures, the presence of CoCos can increase\n(and not reduce) financial fragility, because of the occurring of unneeded\ntriggers and consequential suboptimal conversions that damage CoCos investors.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:00:50 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Calice", "Giovanni", ""], ["Sala", "Carlo", ""], ["Tantari", "Daniele", ""]]}, {"id": "2009.00368", "submitter": "Stephane Crepey", "authors": "Claudio Albanese, Stephane Crepey, Rodney Hoskinson, and Bouazza\n  Saadeddine", "title": "XVA Analysis From the Balance Sheet", "comments": "This is an Accepted Manuscript of an article to be published by\n  Taylor & Francis in Quantitative Finance, which will be available online\n  (once published) at: https://doi.org/10.1080/14697688.2020.1817533", "journal-ref": null, "doi": "10.1080/14697688.2020.1817533", "report-no": null, "categories": "q-fin.RM q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XVAs denote various counterparty risk related valuation adjustments that are\napplied to financial derivatives since the 2007--09 crisis. We root a\ncost-of-capital XVA strategy in a balance sheet perspective which is key in\nidentifying the economic meaning of the XVA terms. Our approach is first\ndetailed in a static setup that is solved explicitly. It is then plugged in the\ndynamic and trade incremental context of a real derivative banking portfolio.\nThe corresponding cost-of-capital XVA strategy ensures to bank shareholders a\nsubmartingale equity process corresponding to a target hurdle rate on their\ncapital at risk, consistently between and throughout deals. Set on a\nforward/backward SDE formulation, this strategy can be solved efficiently using\nGPU computing combined with deep learning regression methods in a whole bank\nbalance sheet context. A numerical case study emphasizes the workability and\nadded value of the ensuing pathwise XVA computations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 11:47:41 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Albanese", "Claudio", ""], ["Crepey", "Stephane", ""], ["Hoskinson", "Rodney", ""], ["Saadeddine", "Bouazza", ""]]}, {"id": "2009.00868", "submitter": "Rusudan Kevkhishvili", "authors": "Masahiko Egami, Rusudan Kevkhishvili", "title": "A New Approach to Estimating Loss-Given-Default Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to estimating the loss-given-default distribution.\nMore precisely, we obtain the default-time distribution of the leverage ratio\n(defined as the ratio of a firm's assets over its debt) by examining its last\npassage time to a certain level. In fact, the use of the last passage time is\nparticularly relevant because it is not a stopping time: this corresponds to\nthe fact that the timing and extent of severe firm-value deterioration, when\ndefault approaching, is neither observed nor easily estimated. We calibrate the\nmodel parameters to the credit market, so that we can illustrate the\nloss-given-default distribution implied in the quoted CDS spreads.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:36:27 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Egami", "Masahiko", ""], ["Kevkhishvili", "Rusudan", ""]]}, {"id": "2009.01644", "submitter": "Stefan Gerhold", "authors": "Stefan Gerhold", "title": "A note on large deviations in life insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large and moderate deviations for a life insurance portfolio,\nwithout assuming identically distributed losses. The crucial assumption is that\nlosses are bounded, and that variances are bounded below. From a standard large\ndeviations upper bound, we get an exponential bound for the probability of the\naverage loss exceeding a threshold. A counterexample shows that a full large\ndeviation principle does not follow from our assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 13:15:04 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Gerhold", "Stefan", ""]]}, {"id": "2009.02904", "submitter": "Bony Josaphat M.Si.", "authors": "Bony Josaphat and Khreshna Syuhada", "title": "Dependent Conditional Value-at-Risk for Aggregate Risk Models", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk measure forecast and model have been developed in order to not only\nprovide better forecast but also preserve its (empirical) property especially\ncoherent property. Whilst the widely used risk measure of Value-at-Risk (VaR)\nhas shown its performance and benefit in many applications, it is in fact not a\ncoherent risk measure. Conditional VaR (CoVaR), defined as mean of losses\nbeyond VaR, is one of alternative risk measures that satisfies coherent\nproperty. There has been several extensions of CoVaR such as Modified CoVaR\n(MCoVaR) and Copula CoVaR (CCoVaR). In this paper, we propose another risk\nmeasure, called Dependent CoVaR (DCoVaR), for a target loss that depends on\nanother random loss, including model parameter treated as random loss. It is\nfound that our DCoVaR outperforms than both MCoVaR and CCoVaR. Numerical\nsimulation is carried out to illustrate the proposed DCoVaR. In addition, we do\nan empirical study of financial returns data to compute the DCoVaR forecast for\nheteroscedastic process.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 06:40:09 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Josaphat", "Bony", ""], ["Syuhada", "Khreshna", ""]]}, {"id": "2009.03653", "submitter": "Sojung Kim", "authors": "Sojung Kim and Stefan Weber", "title": "Simulation Methods for Robust Risk Assessment and the Distorted Mix\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty requires suitable techniques for risk assessment. Combining\nstochastic approximation and stochastic average approximation, we propose an\nefficient algorithm to compute the worst case average value at risk in the face\nof tail uncertainty. Dependence is modelled by the distorted mix method that\nflexibly assigns different copulas to different regions of multivariate\ndistributions. We illustrate the application of our approach in the context of\nfinancial markets and cyber risk.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 11:49:33 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Kim", "Sojung", ""], ["Weber", "Stefan", ""]]}, {"id": "2009.04514", "submitter": "Alberto Elices", "authors": "Alberto Elices", "title": "X-Value adjustments: accounting versus economic management perspectives", "comments": "48 pages, 2 figures, to be submitted to Quantitative Finance", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a mathematical framework based on the principle of\ninvariance to classify institutions in two paradigms according to the way in\nwhich credit, debit and funding adjustments are calculated: accounting and\nmanagement perspectives. This conceptual classification helps to answer\nquestions such as: In which paradigm each institution sits (point of\nsituation)? Where is the market consensus and regulation pointing to (target\npoint)? What are the implications, pros and cons of switching perspective to\nalign with future consensus (design of a transition)? An improved solution of\nthe principle of invariance equations is presented to calculate these metrics\navoiding approximations and irrespective of the discounting curve used in Front\nOffice systems. The perspective is changed by appropriate selection of inputs\nalways using the same calculation engine. A description of balance sheet\nfinancing is presented along with the justification of the funding curves used\nfor both perspectives.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 18:51:16 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Elices", "Alberto", ""]]}, {"id": "2009.04536", "submitter": "Yan Wang", "authors": "Yan Wang, Xuelei Sherry Ni", "title": "Improving Investment Suggestions for Peer-to-Peer (P2P) Lending via\n  Integrating Credit Scoring into Profit Scoring", "comments": null, "journal-ref": null, "doi": "10.1145/3374135.3385272", "report-no": null, "categories": "q-fin.RM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the peer-to-peer (P2P) lending market, lenders lend the money to the\nborrowers through a virtual platform and earn the possible profit generated by\nthe interest rate. From the perspective of lenders, they want to maximize the\nprofit while minimizing the risk. Therefore, many studies have used machine\nlearning algorithms to help the lenders identify the \"best\" loans for making\ninvestments. The studies have mainly focused on two categories to guide the\nlenders' investments: one aims at minimizing the risk of investment (i.e., the\ncredit scoring perspective) while the other aims at maximizing the profit\n(i.e., the profit scoring perspective). However, they have all focused on one\ncategory only and there is seldom research trying to integrate the two\ncategories together. Motivated by this, we propose a two-stage framework that\nincorporates the credit information into a profit scoring modeling. We\nconducted the empirical experiment on a real-world P2P lending data from the US\nP2P market and used the Light Gradient Boosting Machine (lightGBM) algorithm in\nthe two-stage framework. Results show that the proposed two-stage method could\nidentify more profitable loans and thereby provide better investment guidance\nto the investors compared to the existing one-stage profit scoring alone\napproach. Therefore, the proposed framework serves as an innovative perspective\nfor making investment decisions in P2P lending.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 19:41:23 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Wang", "Yan", ""], ["Ni", "Xuelei Sherry", ""]]}, {"id": "2009.05034", "submitter": "Josef Teichmann", "authors": "Thomas Krabichler and Josef Teichmann", "title": "Deep Replication of a Runoff Portfolio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To the best of our knowledge, the application of deep learning in the field\nof quantitative risk management is still a relatively recent phenomenon. This\narticle presents the key notions of Deep Asset Liability Management (Deep~ALM)\nfor a technological transformation in the management of assets and liabilities\nalong a whole term structure. The approach has a profound impact on a wide\nrange of applications such as optimal decision making for treasurers, optimal\nprocurement of commodities or the optimisation of hydroelectric power plants.\nAs a by-product, intriguing aspects of goal-based investing or Asset Liability\nManagement (ALM) in abstract terms concerning urgent challenges of our society\nare expected alongside. We illustrate the potential of the approach in a\nstylised case.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:55:03 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Krabichler", "Thomas", ""], ["Teichmann", "Josef", ""]]}, {"id": "2009.06221", "submitter": "Toma\\v{z} Ko\\v{s}ir", "authors": "Damjana Kokol Bukov\\v{s}ek, Toma\\v{z} Ko\\v{s}ir, Bla\\v{z}\n  Moj\\v{s}kerc, Matja\\v{z} Omladi\\v{c}", "title": "Spearman's footrule and Gini's gamma: Local bounds for bivariate copulas\n  and the exact region with respect to Blomqvist's beta", "comments": "30 pages, 13 figures", "journal-ref": null, "doi": "10.1016/j.cam.2021.113385", "report-no": null, "categories": "math.ST math.PR q-fin.RM q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas are becoming an essential tool in analyzing data thus encouraging\ninterest in related questions. In the early stage of exploratory data analysis,\nsay, it is helpful to know local copula bounds with a fixed value of a given\nmeasure of association. These bounds have been computed for Spearman's rho,\nKendall's tau, and Blomqvist's beta. The importance of another two measures of\nassociation, Spearman's footrule and Gini's gamma, has been reconfirmed\nrecently. It is the main purpose of this paper to fill in the gap and present\nthe mentioned local bounds for these two measures as well. It turns out that\nthis is a quite non-trivial endeavor as the bounds are quasi-copulas that are\nnot copulas for certain values of the two measures. We also give relations\nbetween these two measures of association and Blomqvist's beta.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 06:33:45 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 12:03:04 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Bukov\u0161ek", "Damjana Kokol", ""], ["Ko\u0161ir", "Toma\u017e", ""], ["Moj\u0161kerc", "Bla\u017e", ""], ["Omladi\u010d", "Matja\u017e", ""]]}, {"id": "2009.07341", "submitter": "Timo Dimitriadis", "authors": "Timo Dimitriadis and Xiaochun Liu and Julie Schnaitmann", "title": "Encompassing Tests for Value at Risk and Expected Shortfall Multi-Step\n  Forecasts based on Inference on the Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose forecast encompassing tests for the Expected Shortfall (ES)\njointly with the Value at Risk (VaR) based on flexible link (or combination)\nfunctions. Our setup allows testing encompassing for convex forecast\ncombinations and for link functions which preclude crossings of the combined\nVaR and ES forecasts. As the tests based on these link functions involve\nparameters which are on the boundary of the parameter space under the null\nhypothesis, we derive and base our tests on nonstandard asymptotic theory on\nthe boundary. Our simulation study shows that the encompassing tests based on\nour new link functions outperform tests based on unrestricted linear link\nfunctions for one-step and multi-step forecasts. We further illustrate the\npotential of the proposed tests in a real data analysis for forecasting VaR and\nES of the S&P 500 index.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 20:18:53 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Dimitriadis", "Timo", ""], ["Liu", "Xiaochun", ""], ["Schnaitmann", "Julie", ""]]}, {"id": "2009.08412", "submitter": "Kyle Steinhauer", "authors": "Kyle Steinhauer, Takahisa Fukadai, Sho Yoshida", "title": "Solving the Optimal Trading Trajectory Problem Using Simulated\n  Bifurcation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PM q-fin.RM quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use an optimization procedure based on simulated bifurcation (SB) to solve\nthe integer portfolio and trading trajectory problem with an unprecedented\ncomputational speed. The underlying algorithm is based on a classical\ndescription of quantum adiabatic evolutions of a network of non-linearly\ninteracting oscillators. This formulation has already proven to beat state of\nthe art computation times for other NP-hard problems and is expected to show\nsimilar performance for certain portfolio optimization problems. Inspired by\nsuch we apply the SB approach to the portfolio integer optimization problem\nwith quantity constraints and trading activities. We show first numerical\nresults for portfolios of up to 1000 assets, which already confirm the power of\nthe SB algorithm for its novel use-case as a portfolio and trading trajectory\noptimizer.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 16:42:04 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Steinhauer", "Kyle", ""], ["Fukadai", "Takahisa", ""], ["Yoshida", "Sho", ""]]}, {"id": "2009.08794", "submitter": "Jeremy Turiel", "authors": "Jeremy D. Turiel, Paolo Barucca and Tomaso Aste", "title": "Simplicial persistence of financial markets: filtering, generative\n  processes and portfolio risk", "comments": "8 pages, 5 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:1910.08628", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST physics.soc-ph q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce simplicial persistence, a measure of time evolution of network\nmotifs in subsequent temporal layers. We observe long memory in the evolution\nof structures from correlation filtering, with a two regime power law decay in\nthe number of persistent simplicial complexes. Null models of the underlying\ntime series are tested to investigate properties of the generative process and\nits evolutional constraints. Networks are generated with both TMFG filtering\ntechnique and thresholding showing that embedding-based filtering methods\n(TMFG) are able to identify higher order structures throughout the market\nsample, where thresholding methods fail. The decay exponents of these long\nmemory processes are used to characterise financial markets based on their\nstage of development and liquidity. We find that more liquid markets tend to\nhave a slower persistence decay. This is in contrast with the common\nunderstanding that developed markets are more random. We find that they are\nindeed less predictable for what concerns the dynamics of each single variable\nbut they are more predictable for what concerns the collective evolution of the\nvariables. This could imply higher fragility to systemic shocks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 18:00:21 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Turiel", "Jeremy D.", ""], ["Barucca", "Paolo", ""], ["Aste", "Tomaso", ""]]}, {"id": "2009.09572", "submitter": "Ling Wang", "authors": "Ling Wang (1), Mei Choi Chiu (2), Hoi Ying Wong (1) ((1) The Chinese\n  University of Hong Kong, (2) The Education University of Hong Kong)", "title": "Volterra mortality model: Actuarial valuation and risk management with\n  long-range dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While abundant empirical studies support the long-range dependence (LRD) of\nmortality rates, the corresponding impact on mortality securities are largely\nunknown due to the lack of appropriate tractable models for valuation and risk\nmanagement purposes. We propose a novel class of Volterra mortality models that\nincorporate LRD into the actuarial valuation, retain tractability, and are\nconsistent with the existing continuous-time affine mortality models. We derive\nthe survival probability in closed-form solution by taking into account of the\nhistorical health records. The flexibility and tractability of the models make\nthem useful in valuing mortality-related products such as death benefits,\nannuities, longevity bonds, and many others, as well as offering optimal\nmean-variance mortality hedging rules. Numerical studies are conducted to\nexamine the effect of incorporating LRD into mortality rates on various\ninsurance products and hedging efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 02:04:23 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wang", "Ling", ""], ["Chiu", "Mei Choi", ""], ["Wong", "Hoi Ying", ""]]}, {"id": "2009.10764", "submitter": "Michele Leonardo Bianchi", "authors": "Michele Leonardo Bianchi and Giovanni De Luca and Giorgia Rivieccio", "title": "CoVaR with volatility clustering, heavy tails and non-linear dependence", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we estimate the conditional value-at-risk by fitting different\nmultivariate parametric models capturing some stylized facts about multivariate\nfinancial time series of equity returns: heavy tails, negative skew, asymmetric\ndependence, and volatility clustering. While the volatility clustering effect\nis got by AR-GARCH dynamics of the GJR type, the other stylized facts are\ncaptured through non-Gaussian multivariate models and copula functions. The\nCoVaR$^{\\leq}$ is computed on the basis on the multivariate normal model, the\nmultivariate normal tempered stable (MNTS) model, the multivariate generalized\nhyperbolic model (MGH) and four possible copula functions. These risk measure\nestimates are compared to the CoVaR$^{=}$ based on the multivariate normal\nGARCH model. The comparison is conducted by backtesting the competitor models\nover the time span from January 2007 to March 2020. In the empirical study we\nconsider a sample of listed banks of the euro area belonging to the main or to\nthe additional global systemically important banks (GSIBs) assessment sample.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 19:03:06 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Bianchi", "Michele Leonardo", ""], ["De Luca", "Giovanni", ""], ["Rivieccio", "Giorgia", ""]]}, {"id": "2009.11064", "submitter": "Arno Botha", "authors": "Arno Botha, Conrad Beyers, Pieter de Villiers", "title": "Simulation-based optimisation of the timing of loan recovery across\n  different portfolios", "comments": "Accepted by the journal \"Expert Systems with Applications\". 25 pages\n  (including appendix), 9 figures. arXiv admin note: text overlap with older\n  arXiv:1907.12615", "journal-ref": null, "doi": "10.1016/j.eswa.2021.114878", "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel procedure is presented for the objective comparison and evaluation of\na bank's decision rules in optimising the timing of loan recovery. This\nprocedure is based on finding a delinquency threshold at which the financial\nloss of a loan portfolio (or segment therein) is minimised. Our procedure is an\nexpert system that incorporates the time value of money, costs, and the\nfundamental trade-off between accumulating arrears versus forsaking future\ninterest revenue. Moreover, the procedure can be used with different\ndelinquency measures (other than payments in arrears), thereby allowing an\nindirect comparison of these measures. We demonstrate the system across a range\nof credit risk scenarios and portfolio compositions. The computational results\nshow that threshold optima can exist across all reasonable values of both the\npayment probability (default risk) and the loss rate (loan collateral). In\naddition, the procedure reacts positively to portfolios afflicted by either\nsystematic defaults (such as during an economic downturn) or episodic\ndelinquency (i.e., cycles of curing and re-defaulting). In optimising a\nportfolio's recovery decision, our procedure can better inform the quantitative\naspects of a bank's collection policy than relying on arbitrary discretion\nalone.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 20:23:14 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 06:37:03 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 18:14:31 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Botha", "Arno", ""], ["Beyers", "Conrad", ""], ["de Villiers", "Pieter", ""]]}, {"id": "2009.11367", "submitter": "Cheng Peng", "authors": "Cheng Peng and Young Shin Kim", "title": "Portfolio Optimization on Multivariate Regime Switching GARCH Model with\n  Normal Tempered Stable Innovation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Markov regime switching GARCH model with multivariate normal\ntempered stable innovation to accommodate fat tails and other stylized facts in\nreturns of financial assets. The model is used to simulate sample paths as\ninput for portfolio optimization with risk measures, namely, conditional value\nat risk and conditional drawdown. The motivation is to have a portfolio that\navoids left tail events by combining models that incorporates fat tail with\noptimization that focuses on tail risk. In-sample test is conducted to\ndemonstrate goodness of fit. Out-of-sample test shows that our approach yields\nhigher performance measured by Sharpe-like ratios than the market and equally\nweighted portfolio in recent years which includes some of the most volatile\nperiods in history. We also find that suboptimal portfolios with higher return\nconstraints tend to outperform optimal portfolios.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 20:25:14 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 02:39:48 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Peng", "Cheng", ""], ["Kim", "Young Shin", ""]]}, {"id": "2009.12092", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Meng-Jou Lu, Cathy Yi-Hsuan Chen, Wolfgang Karl H\\\"ardle", "title": "Copula-Based Factor Model for Credit Risk Analysis", "comments": null, "journal-ref": "Review of Quantitative Finance and Accounting, 49, pages 949 to\n  971, 2017", "doi": "10.1007/s11156-016-0613-x", "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A standard quantitative method to access credit risk employs a factor model\nbased on joint multivariate normal distribution properties. By extending a\none-factor Gaussian copula model to make a more accurate default forecast, this\npaper proposes to incorporate a state-dependent recovery rate into the\nconditional factor loading, and model them by sharing a unique common factor.\nThe common factor governs the default rate and recovery rate simultaneously and\ncreates their association implicitly. In accordance with Basel III, this paper\nshows that the tendency of default is more governed by systematic risk rather\nthan idiosyncratic risk during a hectic period. Among the models considered,\nthe one with random factor loading and a state-dependent recovery rate turns\nout to be the most superior on the default prediction.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 08:54:49 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 13:09:29 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Lu", "Meng-Jou", ""], ["Chen", "Cathy Yi-Hsuan", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2009.12121", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Xinwen Ni, Wolfgang Karl H\\\"ardle, Taojun Xie", "title": "A Machine Learning Based Regulatory Risk Index for Cryptocurrencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cryptocurrencies' values often respond aggressively to major policy changes,\nbut none of the existing indices informs on the market risks associated with\nregulatory changes. In this paper, we quantify the risks originating from new\nregulations on FinTech and cryptocurrencies (CCs), and analyse their impact on\nmarket dynamics. Specifically, a Cryptocurrency Regulatory Risk IndeX (CRRIX)\nis constructed based on policy-related news coverage frequency. The unlabeled\nnews data are collected from the top online CC news platforms and further\nclassified using a Latent Dirichlet Allocation model and Hellinger distance.\nOur results show that the machine-learning-based CRRIX successfully captures\nmajor policy-changing moments. The movements for both the VCRIX, a market\nvolatility index, and the CRRIX are synchronous, meaning that the CRRIX could\nbe helpful for all participants in the cryptocurrency market. The algorithms\nand Python code are available for research purposes on www.quantlet.de.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 10:48:41 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 09:35:56 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ni", "Xinwen", ""], ["H\u00e4rdle", "Wolfgang Karl", ""], ["Xie", "Taojun", ""]]}, {"id": "2009.12129", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Shi Chen, Cathy Yi-Hsuan Chen, Wolfgang Karl H\\\"ardle", "title": "A first econometric analysis of the CRIX family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to price contingent claims one needs to first understand the\ndynamics of these indices. Here we provide a first econometric analysis of the\nCRIX family within a time-series framework. The key steps of our analysis\ninclude model selection, estimation and testing. Linear dependence is removed\nby an ARIMA model, the diagnostic checking resulted in an ARIMA(2,0,2) model\nfor the available sample period from Aug 1st, 2014 to April 6th, 2016. The\nmodel residuals showed the well known phenomenon of volatility clustering.\nTherefore a further refinement lead us to an ARIMA(2,0,2)-t-GARCH(1,1) process.\nThis specification conveniently takes care of fat-tail properties that are\ntypical for financial markets. The multivariate GARCH models are implemented on\nthe CRIX index family to explore the interaction.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 11:06:34 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Chen", "Shi", ""], ["Chen", "Cathy Yi-Hsuan", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2009.12274", "submitter": "Manuel Guerra", "authors": "Manuel Guerra and Alexandra B. Moura", "title": "Reinsurance of multiple risks with generic dependence structures", "comments": "A mistake on Proposition 3.2 was corrected. A new section on\n  computation of optimal strategies was added. Several additions to the text.\n  Examples were corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimal reinsurance problem from the point of view of a\ndirect insurer owning several dependent risks, assuming a maximal expected\nutility criterion and independent negotiation of reinsurance for each risk.\nWithout any particular hypothesis on the dependency structure, we show that\noptimal treaties exist in a class of independent randomized contracts. We\nderive optimality conditions and show that under mild assumptions the optimal\ncontracts are of classical (non-randomized) type. A specific for mof the\noptimality conditions applies in that case. We present a numerical scheme to\nsolve the optimality conditions.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 14:43:12 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 21:55:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Guerra", "Manuel", ""], ["Moura", "Alexandra B.", ""]]}, {"id": "2009.12838", "submitter": "Mario Ghossoub", "authors": "Mario Ghossoub and David Saunders", "title": "On the Continuity of the Feasible Set Mapping in Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.TH q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the set of probability measures with given marginal distributions on\nthe product of two complete, separable metric spaces, seen as a correspondence\nwhen the marginal distributions vary. In problems of optimal transport,\ncontinuity of this correspondence from marginal to joint distributions is often\ndesired, in light of Berge's Maximum Theorem, to establish continuity of the\nvalue function in the marginal distributions, as well as stability of the set\nof optimal transport plans. Bergin (1999) established the continuity of this\ncorrespondence, and in this note, we present a novel and considerably shorter\nproof of this important result. We then examine an application to an assignment\ngame (transferable utility matching problem) with unknown type distributions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 13:17:26 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Ghossoub", "Mario", ""], ["Saunders", "David", ""]]}, {"id": "2009.12901", "submitter": "Chris Kenyon", "authors": "Chris Kenyon", "title": "Client engineering of XVA in crisis and normality: Restructuring,\n  Mandatory Breaks and Resets", "comments": "14 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crises challenge client XVA management when continuous collateralization is\nnot possible because a derivative locks in the client credit level and the\nprovider's funding level, on the trade date, for the life of the trade. We\nprice XVA reduction strategies from the client point of view comparing multiple\ntrade strategies using Mandatory Breaks or Restructuring, to modifications of a\nsingle trade using a Reset. We analyse previous crises and recovery of CDS to\ninform our numerical examples. In our numerical examples Resets can be twice as\neffective as Mandatory Break/Restructuring if there is no credit recovery. When\nrecovery is at least 1/3 of the credit shock then Mandatory Break/Restructuring\ncan be more effective.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 17:05:54 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kenyon", "Chris", ""]]}, {"id": "2009.13222", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Lining Yu, Wolfgang Karl H\\\"ardle, Lukas Borke, Thijs Benschop", "title": "An AI approach to measuring financial risk", "comments": null, "journal-ref": "The Singapore Economic Review (2019): pp. 1 to 21", "doi": "10.1142/S0217590819500668", "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AI artificial intelligence brings about new quantitative techniques to assess\nthe state of an economy. Here we describe a new measure for systemic risk: the\nFinancial Risk Meter (FRM). This measure is based on the penalization parameter\n(lambda) of a linear quantile lasso regression. The FRM is calculated by taking\nthe average of the penalization parameters over the 100 largest US publicly\ntraded financial institutions. We demonstrate the suitability of this AI based\nrisk measure by comparing the proposed FRM to other measures for systemic risk,\nsuch as VIX, SRISK and Google Trends. We find that mutual Granger causality\nexists between the FRM and these measures, which indicates the validity of the\nFRM as a systemic risk measure. The implementation of this project is carried\nout using parallel computing, the codes are published on www.quantlet.de with\nkeyword FRM. The R package RiskAnalytics is another tool with the purpose of\nintegrating and facilitating the research, calculation and analysis methods\naround the FRM project. The visualization and the up-to-date FRM can be found\non hu.berlin/frm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 11:29:03 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yu", "Lining", ""], ["H\u00e4rdle", "Wolfgang Karl", ""], ["Borke", "Lukas", ""], ["Benschop", "Thijs", ""]]}, {"id": "2009.14682", "submitter": "Bent Flyvbjerg", "authors": "Bent Flyvbjerg, Alexander Budzier, Daniel Lunn", "title": "Regression to the Tail: Why the Olympics Blow Up", "comments": "arXiv admin note: text overlap with arXiv:1607.04484", "journal-ref": null, "doi": "10.1177/0308518X20958724", "report-no": null, "categories": "q-fin.GN q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Olympic Games are the largest, highest-profile, and most expensive\nmegaevent hosted by cities and nations. Average sports-related costs of hosting\nare $12.0 billion. Non-sports-related costs are typically several times that.\nEvery Olympics since 1960 has run over budget, at an average of 172 percent in\nreal terms, the highest overrun on record for any type of megaproject. The\npaper tests theoretical statistical distributions against empirical data for\nthe costs of the Games, in order to explain the cost risks faced by host cities\nand nations. It is documented, for the first time, that cost and cost overrun\nfor the Games follow a power-law distribution. Olympic costs are subject to\ninfinite mean and variance, with dire consequences for predictability and\nplanning. We name this phenomenon \"regression to the tail\": it is only a matter\nof time until a new extreme event occurs, with an overrun larger than the\nlargest so far, and thus more disruptive and less plannable. The generative\nmechanism for the Olympic power law is identified as strong convexity prompted\nby six causal drivers: irreversibility, fixed deadlines, the Blank Check\nSyndrome, tight coupling, long planning horizons, and an Eternal Beginner\nSyndrome. The power law explains why the Games are so difficult to plan and\nmanage successfully, and why cities and nations should think twice before\nbidding to host. Based on the power law, two heuristics are identified for\nbetter decision making on hosting. Finally, the paper develops measures for\ngood practice in planning and managing the Games, including how to mitigate the\nextreme risks of the Olympic power law.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 20:56:16 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Flyvbjerg", "Bent", ""], ["Budzier", "Alexander", ""], ["Lunn", "Daniel", ""]]}]