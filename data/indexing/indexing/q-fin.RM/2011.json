[{"id": "2011.00552", "submitter": "Vincenzo Candila", "authors": "Vincenzo Candila, Giampiero M. Gallo, Lea Petrella", "title": "Using mixed-frequency and realized measures in quantile regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is an efficient tool when it comes to estimate popular\nmeasures of tail risk such as the conditional quantile Value at Risk. In this\npaper we exploit the availability of data at mixed frequency to build a\nvolatility model for daily returns with low-- (for macro--variables) and\nhigh--frequency (which may include an \\virg{--X} term related to realized\nvolatility measures) components. The quality of the suggested quantile\nregression model, labeled MF--Q--ARCH--X, is assessed in a number of\ndirections: we derive weak stationarity properties, we investigate its finite\nsample properties by means of a Monte Carlo exercise and we apply it on\nfinancial real data. VaR forecast performances are evaluated by backtesting and\nModel Confidence Set inclusion among competitors, showing that the\nMF--Q--ARCH--X has a consistently accurate forecasting capability.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:39:51 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Candila", "Vincenzo", ""], ["Gallo", "Giampiero M.", ""], ["Petrella", "Lea", ""]]}, {"id": "2011.00909", "submitter": "Dietmar Pfeifer Prof. Dr.", "authors": "Dietmar Pfeifer, Olena Ragulina", "title": "Adaptive Bernstein Copulas and Risk Management", "comments": "corrected version; 27 pages, 58 figures, 17 tables", "journal-ref": "Mathematics 2020, 8, 2221", "doi": "10.3390/math8122221", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a constructive approach to Bernstein copulas with an admissible\ndiscrete skeleton in arbitrary dimensions when the underlying marginal grid\nsizes are smaller than the number of observations. This prevents an overfitting\nof the estimated dependence model and reduces the simulation effort for\nBernstein copulas a lot. In a case study, we compare different approaches of\nBernstein and Gaussian copulas w.r.t. the estimation of risk measures in risk\nmanagement.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 11:42:05 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 12:12:20 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 14:19:04 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 17:16:16 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Pfeifer", "Dietmar", ""], ["Ragulina", "Olena", ""]]}, {"id": "2011.02165", "submitter": "Koichi Miyamoto", "authors": "Kazuya Kaneko, Koichi Miyamoto, Naoyuki Takeda, Kazuyoshi Yoshino", "title": "Quantum Speedup of Monte Carlo Integration with respect to the Number of\n  Dimensions and its Application to Finance", "comments": "13 pages, no figure", "journal-ref": "Quantum Inf Process 20, 185 (2021)", "doi": null, "report-no": null, "categories": "quant-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo integration using quantum computers has been widely investigated,\nincluding applications to concrete problems. It is known that quantum\nalgorithms based on quantum amplitude estimation (QAE) can compute an integral\nwith a smaller number of iterative calls of the quantum circuit which\ncalculates the integrand, than classical methods call the integrand subroutine.\nHowever, the issues about the iterative operations in the integrand circuit\nhave not been discussed so much. That is, in the high-dimensional integration,\nmany random numbers are used for calculation of the integrand and in some cases\nsimilar calculations are repeated to obtain one sample value of the integrand.\nIn this paper, we point out that we can reduce the number of such repeated\noperations by a combination of the nested QAE and the use of pseudorandom\nnumbers (PRNs), if the integrand has the separable form with respect to\ncontributions from distinct random numbers. The use of PRNs, which the authors\noriginally proposed in the context of the quantum algorithm for Monte Carlo, is\nthe key factor also in this paper, since it enables parallel computation of the\nseparable terms in the integrand. Furthermore, we pick up one use case of this\nmethod in finance, the credit portfolio risk measurement, and estimate to what\nextent the complexity is reduced.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 07:40:20 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 01:56:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kaneko", "Kazuya", ""], ["Miyamoto", "Koichi", ""], ["Takeda", "Naoyuki", ""], ["Yoshino", "Kazuyoshi", ""]]}, {"id": "2011.02870", "submitter": "Renyuan Xu", "authors": "Anna Ananova, Rama Cont and Renyuan Xu", "title": "Excursion Risk", "comments": "36 pages; 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk and return profiles of a broad class of dynamic trading strategies,\nincluding pairs trading and other statistical arbitrage strategies, may be\ncharacterized in terms of excursions of the market price of a portfolio away\nfrom a reference level. We propose a mathematical framework for the risk\nanalysis of such strategies, based on a description in terms of price\nexcursions, first in a pathwise setting, without probabilistic assumptions,\nthen in a Markovian setting.\n  We introduce the notion of delta-excursion, defined as a path which deviates\nby delta from a reference level before returning to this level. We show that\nevery continuous path has a unique decomposition into delta-excursions, which\nis useful for the scenario analysis of dynamic trading strategies, leading to\nsimple expressions for the number of trades, realized profit, maximum loss and\ndrawdown. As delta is decreased to zero, properties of this decomposition\nrelate to the local time of the path.\n  When the underlying asset follows a Markov process, we combine these results\nwith Ito's excursion theory to obtain a tractable decomposition of the process\nas a concatenation of independent delta-excursions, whose distribution is\ndescribed in terms of Ito's excursion measure. We provide analytical results\nfor linear diffusions and give new examples of stochastic processes for\nflexible and tractable modeling of excursions. Finally, we describe a\nnon-parametric scenario simulation method for generating paths whose excursion\nproperties match those observed in empirical data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 14:45:58 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Ananova", "Anna", ""], ["Cont", "Rama", ""], ["Xu", "Renyuan", ""]]}, {"id": "2011.03314", "submitter": "Alex S.L. Tse", "authors": "John Armstrong, Damiano Brigo, Alex S.L. Tse", "title": "The importance of dynamic risk constraints for limited liability\n  operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous literature shows that prevalent risk measures such as Value at Risk\nor Expected Shortfall are ineffective to curb excessive risk-taking by a\ntail-risk-seeking trader with S-shaped utility function in the context of\nportfolio optimisation. However, these conclusions hold only when the\nconstraints are static in the sense that the risk measure is just applied to\nthe terminal portfolio value. In this paper, we consider a portfolio\noptimisation problem featuring S-shaped utility and a dynamic risk constraint\nwhich is imposed throughout the entire trading horizon. Provided that the risk\ncontrol policy is sufficiently strict relative to the asset performance, the\ntrader's portfolio strategies and the resulting maximal expected utility can be\neffectively constrained by a dynamic risk measure. Finally, we argue that\ndynamic risk constraints might still be ineffective if the trader has access to\na derivatives market.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 12:36:23 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Armstrong", "John", ""], ["Brigo", "Damiano", ""], ["Tse", "Alex S. L.", ""]]}, {"id": "2011.03517", "submitter": "Toma\\v{z} Fleischman", "authors": "Toma\\v{z} Fleischman and Paolo Dini", "title": "Balancing the Payment System", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.RM q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly complex economic and financial environment in which we live\nmakes the management of liquidity in payment systems and the economy in general\na persistent challenge. New technologies are making it possible to address this\nchallenge through alternative solutions that complement and strengthen existing\npayment systems. For example, the interbank balancing method can also be\napplied to private payment systems, complementary currencies, and trade credit\nclearing systems to provide better liquidity and risk management. In this paper\nwe introduce the concept of a balanced payment system and demonstrate the\neffects of balancing on a small example. We show how to construct a balanced\npayment subsystem that can be settled in full and, therefore, that can be\nremoved from the payment system to achieve liquidity-saving and payments\ngridlock resolution. We also briefly introduce a generalization of a payment\nsystem and of the method to balance it in the form of a specific application\n(Tetris Core Technologies), whose wider adoption could contribute to the\nfinancial stability of and better management of liquidity and risk for the\nwhole economy.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:48:07 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Fleischman", "Toma\u017e", ""], ["Dini", "Paolo", ""]]}, {"id": "2011.04544", "submitter": "Mariano Zeron", "authors": "Mariano Zeron, Ignacio Ruiz", "title": "Dynamic sensitivities and Initial Margin via Chebyshev Tensors", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents how to use Chebyshev Tensors to compute dynamic\nsensitivities of financial instruments within a Monte Carlo simulation. Dynamic\nsensitivities are then used to compute Dynamic Initial Margin as defined by\nISDA (SIMM). The technique is benchmarked against the computation of dynamic\nsensitivities obtained by using pricing functions like the ones found in risk\nengines. We obtain high accuracy and computational gains for FX swaps and\nSpread Options.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:38:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zeron", "Mariano", ""], ["Ruiz", "Ignacio", ""]]}, {"id": "2011.04889", "submitter": "Qiuqi Wang", "authors": "Silvana Pesenti, Qiuqi Wang, and Ruodu Wang", "title": "Optimizing distortion riskmetrics with distributional uncertainty", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Optimization of distortion riskmetrics with distributional uncertainty has\nwide applications in finance and operations research. Distortion riskmetrics\ninclude many commonly applied risk measures and deviation measures, which are\nnot necessarily monotone or convex. One of our central findings is a unifying\nresult that allows us to convert an optimization of a non-convex distortion\nriskmetric with distributional uncertainty to a convex one, leading to great\ntractability. The key to the unifying equivalence result is the novel notion of\nclosedness under concentration of sets of distributions. Our results include\nmany special cases that are well studied in the optimization literature,\nincluding but not limited to optimizing probabilities, Value-at-Risk, Expected\nShortfall, and Yaari's dual utility under various forms of distributional\nuncertainty. We illustrate our theoretical results via applications to\nportfolio optimization, optimization under moment constraints, and preference\nrobust optimization.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 04:45:08 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Pesenti", "Silvana", ""], ["Wang", "Qiuqi", ""], ["Wang", "Ruodu", ""]]}, {"id": "2011.05117", "submitter": "Andreas Aigner", "authors": "Andreas A. Aigner and Walter Schrabmair", "title": "Startup & Unicorn Growth Valuation", "comments": "13 pages, 7 Tables, 7 Figures", "journal-ref": null, "doi": "10.13140/RG.2.2.33818.47048", "report-no": null, "categories": "q-fin.TR q-fin.PM q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How do you value companies which have IPOed recently? How do you compare them\namongst their peers? Valuing companies using a linear extrapolation of their\nrevenues and profits leads to an ingenious method to benchmark stocks against\neach other. Here we present such a method, dubbed the growth average U1.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 14:28:00 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Aigner", "Andreas A.", ""], ["Schrabmair", "Walter", ""]]}, {"id": "2011.06281", "submitter": "Dietmar Pfeifer Prof. Dr.", "authors": "Dietmar Pfeifer, Olena Ragulina", "title": "Generating unfavourable VaR scenarios with patchwork copulas", "comments": "26 pages, 14 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The central idea of the paper is to present a general simple patchwork\nconstruction principle for multivariate copulas that create unfavourable VaR\n(i.e. Value at Risk) scenarios while maintaining given marginal distributions.\nThis is of particular interest for the construction of Internal Models in the\ninsurance industry under Solvency II in the European Union. The method is\nexemplified with a 19-dimensional real-life data set of insurance losses.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 09:49:09 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 14:16:58 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 11:59:53 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 18:13:30 GMT"}, {"version": "v5", "created": "Thu, 6 May 2021 13:00:49 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Pfeifer", "Dietmar", ""], ["Ragulina", "Olena", ""]]}, {"id": "2011.07379", "submitter": "Christopher Clack", "authors": "Akber Datoo and Christopher D. Clack", "title": "Smart Close-out Netting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Smart Close-out Netting aims to standardise and automate specific operational\naspects of the legal and regulatory processes of close-out netting for\nprudentially regulated financial institutions. This article provides a review,\nanalysis and perspective of these operational processes, their benefits for\nprudentially regulated trading institutions, their current inefficiencies, and\nthe extent to which they are amenable to standardisation and automation. The\nmain concepts of Smart Close-out Netting are introduced, including the use of a\ncontrolled natural language in legal opinions and the use of a data-driven\nframework during netting determination.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:57:34 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Datoo", "Akber", ""], ["Clack", "Christopher D.", ""]]}, {"id": "2011.07570", "submitter": "Thomas Guhr", "authors": "Thomas Guhr and Andreas Schell", "title": "Exact Multivariate Amplitude Distributions for Non-Stationary Gaussian\n  or Algebraic Fluctuations of Covariances or Correlations", "comments": "39 pages, 6 figures", "journal-ref": null, "doi": "10.1088/1751-8121/abe3c8", "report-no": null, "categories": "cond-mat.dis-nn math-ph math.MP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems are often non-stationary, typical indicators are continuously\nchanging statistical properties of time series. In particular, the correlations\nbetween different time series fluctuate. Models that describe the multivariate\namplitude distributions of such systems are of considerable interest. Extending\nprevious work, we view a set of measured, non-stationary correlation matrices\nas an ensemble for which we set up a random matrix model. We use this ensemble\nto average the stationary multivariate amplitude distributions measured on\nshort time scales and thus obtain for large time scales multivariate amplitude\ndistributions which feature heavy tails. We explicitly work out four cases,\ncombining Gaussian and algebraic distributions. The results are either of\nclosed forms or single integrals. We thus provide, first, explicit multivariate\ndistributions for such non-stationary systems and, second, a tool that\nquantitatively captures the degree of non-stationarity in the correlations.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 16:32:04 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Guhr", "Thomas", ""], ["Schell", "Andreas", ""]]}, {"id": "2011.09248", "submitter": "Davide Biancalana", "authors": "Fabio Baione, Davide Biancalana, Paolo De Angelis", "title": "An application of Zero-One Inflated Beta regression models for\n  predicting health insurance reimbursement", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In actuarial practice the dependency between contract limitations\n(deductibles, copayments) and health care expenditures are measured by the\napplication of the Monte Carlo simulation technique. We propose, for the same\ngoal, an alternative approach based on Generalized Linear Model for Location,\nScale and Shape (GAMLSS). We focus on the estimate of the ratio between the\none-year reimbursement amount (after the effect of limitations) and the one\nyear expenditure (before the effect of limitations). We suggest a regressive\nmodel to investigate the relation between this response variable and a set of\ncovariates, such as limitations and other rating factors related to health\nrisk. In this way a dependency structure between reimbursement and limitations\nis provided. The density function of the ratio is a mixture distribution,\nindeed it can continuously assume values mass at 0 and 1, in addition to the\nprobability density within (0, 1) . This random variable does not belong to the\nexponential family, then an ordinary Generalized Linear Model is not suitable.\nGAMLSS introduces a probability structure compliant with the density of the\nresponse variable, in particular zero-one inflated beta density is assumed. The\nlatter is a mixture between a Bernoulli distribution and a Beta distribution.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:43:22 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Baione", "Fabio", ""], ["Biancalana", "Davide", ""], ["De Angelis", "Paolo", ""]]}, {"id": "2011.09254", "submitter": "Davide Biancalana", "authors": "Fabio Baione, Davide Biancalana, Paolo De Angelis", "title": "A Risk Based approach for the Solvency Capital requirement for Health\n  Plans", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The study deals with the assessment of risk measures for Health Plans in\norder to assess the Solvency Capital Requirement. For the estimation of the\nindividual health care expenditure for several episode types, we suggest an\noriginal approach based on a three-part regression model. We propose three\nGeneralized Linear Models (GLM) to assess claim counts, the allocation of each\nclaim to a specific episode and the severity average expenditures respectively.\nOne of the main practical advantages of our proposal is the reduction of the\nregression models compared to a traditional approach, where several two-part\nmodels for each episode types are requested. As most health plans require\nco-payments or co-insurance, considering at this stage the non-linearity\ncondition of the reimbursement function, we adopt a Montecarlo simulation to\nassess the health plan costs. The simulation approach provides the probability\ndistribution of the Net Asset Value of the Health Plan and the estimate of\nseveral risk measures.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:57:57 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Baione", "Fabio", ""], ["Biancalana", "Davide", ""], ["De Angelis", "Paolo", ""]]}, {"id": "2011.10485", "submitter": "P\\'al Andr\\'as Papp", "authors": "P\\'al Andr\\'as Papp, Roger Wattenhofer", "title": "Sequential Defaulting in Financial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider financial networks, where banks are connected by contracts such\nas debts or credit default swaps. We study the clearing problem in these\nsystems: we want to know which banks end up in a default, and what portion of\ntheir liabilities can these defaulting banks fulfill. We analyze these networks\nin a sequential model where banks announce their default one at a time, and the\nsystem evolves in a step-by-step manner.\n  We first consider the reversible model of these systems, where banks may\nreturn from a default. We show that the stabilization time in this model can\nheavily depend on the ordering of announcements. However, we also show that\nthere are systems where for any choice of ordering, the process lasts for an\nexponential number of steps before an eventual stabilization. We also show that\nfinding the ordering with the smallest (or largest) number of banks ending up\nin default is an NP-hard problem. Furthermore, we prove that defaulting early\ncan be an advantageous strategy for banks in some cases, and in general,\nfinding the best time for a default announcement is NP-hard. Finally, we\ndiscuss how changing some properties of this setting affects the stabilization\ntime of the process, and then use these techniques to devise a monotone model\nof the systems, which ensures that every network stabilizes eventually.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 16:30:44 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Papp", "P\u00e1l Andr\u00e1s", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2011.10826", "submitter": "Viktor Stojkoski MSc", "authors": "Viktor Stojkoski, Petar Jolakoski and Igor Ivanovski", "title": "The short-run impact of COVID-19 on the activity in the insurance\n  industry in the Republic of North Macedonia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the impact of the COVID-19 pandemic on the insurance\nindustry in the Republic of North Macedonia during the first half of 2020. By\nutilizing seasonal autoregressive models and data for 11 insurance classes, we\nfind that the insurance activity shrank by more than 10% compared to what was\nexpected. The total loss in the industry was, however, much less than the\namount of funds made available by the Insurance Supervision Agency. This was\nbecause the pandemic induced changes in the activity structure - the share of\nMotor vehicles class fell at the expense of the property classes.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 17:20:57 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Stojkoski", "Viktor", ""], ["Jolakoski", "Petar", ""], ["Ivanovski", "Igor", ""]]}, {"id": "2011.11394", "submitter": "Alessandra Cornaro Dr", "authors": "Gian Paolo Clemente and Alessandra Cornaro", "title": "Assessing Systemic Risk in the Insurance Sector via Network Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a framework for detecting relevant insurance companies in a\nsystemic risk perspective. Among the alternative methodologies for measuring\nsystemic risk, we propose a complex network approach where insurers are linked\nto form a global interconnected system. We model the reciprocal influence\nbetween insurers calibrating edge weights on the basis of specific risk\nmeasures. Therefore, we provide a suitable network indicator, the Weighted\nEffective Resistance Centrality, able to catch which is the effect of a\nspecific vertex on the network robustness. By means of this indicator, we\nassess the prominence of a company in spreading and receiving risk from the\nothers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:16:32 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Clemente", "Gian Paolo", ""], ["Cornaro", "Alessandra", ""]]}, {"id": "2011.13132", "submitter": "Xing Yan", "authors": "Xiangqian Sun, Xing Yan, Qi Wu", "title": "Generative Learning of Heterogeneous Tail Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multivariate generative model to capture the complex dependence\nstructure often encountered in business and financial data. Our model features\nheterogeneous and asymmetric tail dependence between all pairs of individual\ndimensions while also allowing heterogeneity and asymmetry in the tails of the\nmarginals. A significant merit of our model structure is that it is not prone\nto error propagation in the parameter estimation process, hence very scalable,\nas the dimensions of datasets grow large. However, the likelihood methods are\ninfeasible for parameter estimation in our case due to the lack of a\nclosed-form density function. Instead, we devise a novel moment learning\nalgorithm to learn the parameters. To demonstrate the effectiveness of the\nmodel and its estimator, we test them on simulated as well as real-world\ndatasets. Results show that this framework gives better finite-sample\nperformance compared to the copula-based benchmarks as well as recent similar\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 05:34:31 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Sun", "Xiangqian", ""], ["Yan", "Xing", ""], ["Wu", "Qi", ""]]}, {"id": "2011.13369", "submitter": "Frank Schweitzer", "authors": "Frank Schweitzer, Giona Casiraghi, Mario V. Tomasello, David Garcia", "title": "Fragile, yet resilient: Adaptive decline in a collaboration network of\n  firms", "comments": null, "journal-ref": null, "doi": "10.3389/fams.2021.634006", "report-no": null, "categories": "physics.soc-ph nlin.AO q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of collaboration networks of firms follow a life-cycle of growth\nand decline. That does not imply they also become less resilient. Instead,\ndeclining collaboration networks may still have the ability to mitigate shocks\nfrom firms leaving, and to recover from these losses by adapting to new\npartners. To demonstrate this, we analyze 21.500 R\\&D collaborations of 14.500\nfirms in six different industrial sectors over 25 years. We calculate\ntime-dependent probabilities of firms leaving the network and simulate drop-out\ncascades, to determine the expected dynamics of decline. We then show that\ndeviations from these expectations result from the adaptivity of the network,\nwhich mitigates the decline. These deviations can be used as a measure of\nnetwork resilience.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:22:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Schweitzer", "Frank", ""], ["Casiraghi", "Giona", ""], ["Tomasello", "Mario V.", ""], ["Garcia", "David", ""]]}, {"id": "2011.13637", "submitter": "Jan Rosenzweig", "authors": "Jan Rosenzweig", "title": "Fat Tailed Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard, PCA-based factor analysis suffers from a number of well known\nproblems due to the random nature of pairwise correlations of asset returns. We\nanalyse an alternative based on ICA, where factors are identified based on\ntheir non-Gaussianity, instead of their variance. Generalizations of portfolio\nconstruction to the ICA framework leads to two semi-optimal portfolio\nconstruction methods: a fat-tailed portfolio, which maximises return per unit\nof non-Gaussianity, and the hybrid portfolio, which asymptotically reduces\nvariance and non-Gaussianity in parallel. For fat-tailed portfolios, the\nportfolio weights scale like performance to the power of $1/3$, as opposed to\nlinear scaling of Kelly portfolios; such portfolio construction significantly\nreduces portfolio concentration, and the winner-takes-all problem inherent in\nKelly portfolios. For hybrid portfolios, the variance is diversified at the\nsame rate as Kelly PCA-based portfolios, but excess kurtosis is diversified\nmuch faster than in Kelly, at the rate of $n^{-2}$ compared to Kelly\nportfolios' $n^{-1}$ for increasing number of components $n$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 10:16:44 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 14:46:52 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 09:59:18 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 11:40:53 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Rosenzweig", "Jan", ""]]}]