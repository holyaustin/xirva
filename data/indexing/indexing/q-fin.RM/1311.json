[{"id": "1311.0118", "submitter": "Chris Kenyon", "authors": "Chris Kenyon and Andrew Green", "title": "Regulatory-Compliant Derivatives Pricing is Not Risk-Neutral", "comments": "12 pages; 3 figures, Risk, September 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regulations impose idiosyncratic capital and funding costs for holding\nderivatives. Capital requirements are costly because derivatives desks are\nrisky businesses; funding is costly in part because regulations increase the\nminimum funding tenor. Idiosyncratic costs mean no single measure makes\nderivatives martingales for all market participants. Hence Regulatory-compliant\npricing is not risk-neutral. This has implications for exit prices and\nmark-to-market.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 08:44:06 GMT"}, {"version": "v2", "created": "Tue, 12 Aug 2014 19:59:20 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Kenyon", "Chris", ""], ["Green", "Andrew", ""]]}, {"id": "1311.0270", "submitter": "Marie Kratz", "authors": "Marie Kratz", "title": "There is a VaR beyond usual approximations", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basel II and Solvency 2 both use the Value-at-Risk (VaR) as the risk measure\nto compute the Capital Requirements. In practice, to calibrate the VaR, a\nnormal approximation is often chosen for the unknown distribution of the yearly\nlog returns of financial assets. This is usually justified by the use of the\nCentral Limit Theorem (CLT), when assuming aggregation of independent and\nidentically distributed (iid) observations in the portfolio model. Such a\nchoice of modeling, in particular using light tail distributions, has proven\nduring the crisis of 2008/2009 to be an inadequate approximation when dealing\nwith the presence of extreme returns; as a consequence, it leads to a gross\nunderestimation of the risks. The main objective of our study is to obtain the\nmost accurate evaluations of the aggregated risks distribution and risk\nmeasures when working on financial or insurance data under the presence of\nheavy tail and to provide practical solutions for accurately estimating high\nquantiles of aggregated risks. We explore a new method, called Normex, to\nhandle this problem numerically as well as theoretically, based on properties\nof upper order statistics. Normex provides accurate results, only weakly\ndependent upon the sample size and the tail index. We compare it with existing\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 19:34:07 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Kratz", "Marie", ""]]}, {"id": "1311.0354", "submitter": "Hassan Omidi Firouzi", "authors": "Assa Hirbod, Morales Manuel and Omidi Firouzi Hassan", "title": "On the Capital Allocation Problem for a New Coherent Risk Measure in\n  Collective Risk Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new coherent cumulative risk measure on\n$\\mathcal{R}_L^p$, the space of c\\`adl\\`ag processes having Laplace transform.\nThis new coherent risk measure turns out to be tractable enough within a class\nof models where the aggregate claims is driven by a spectrally positive L\\'evy\nprocess. Moreover, we study the problem of capital allocation in an insurance\ncontext and we show that the capital allocation problem for this risk measure\nhas a unique solution determined by the Euler allocation method. Some examples\nare provided.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 07:41:56 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Hirbod", "Assa", ""], ["Manuel", "Morales", ""], ["Hassan", "Omidi Firouzi", ""]]}, {"id": "1311.0498", "submitter": "Konstantinos Spiliopoulos", "authors": "Konstantinos Spiliopoulos, Richard B. Sowers", "title": "Default Clustering in Large Pools: Large Deviations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large deviations and rare default clustering events in a dynamic\nlarge heterogeneous portfolio of interconnected components. Defaults come as\nPoisson events and the default intensities of the different components in the\nsystem interact through the empirical default rate and via systematic effects\nthat are common to all components. We establish the large deviations principle\nfor the empirical default rate for such an interacting particle system. The\nrate function is derived in an explicit form that is amenable to numerical\ncomputations and derivation of the most likely path to failure for the system\nitself. Numerical studies illustrate the theoretical findings. An understanding\nof the role of the preferred paths to large default rates and the most likely\nways in which contagion and systematic risk combine to lead to large default\nrates would give useful insights into how to optimally safeguard against such\nevents.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 17:56:43 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2015 21:23:51 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Spiliopoulos", "Konstantinos", ""], ["Sowers", "Richard B.", ""]]}, {"id": "1311.1122", "submitter": "Pascal Junod", "authors": "Rodrigue Oeuvray and Pascal Junod", "title": "On time scaling of semivariance in a jump-diffusion process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to examine the time scaling of the semivariance when\nreturns are modeled by various types of jump-diffusion processes, including\nstochastic volatility models with jumps in returns and in volatility. In\nparticular, we derive an exact formula for the semivariance when the volatility\nis kept constant, explaining how it should be scaled when considering a lower\nfrequency. We also provide and justify the use of a generalization of the\nBall-Torous approximation of a jump-diffusion process, this new model appearing\nto deliver a more accurate estimation of the downside risk. We use Markov Chain\nMonte Carlo (MCMC) methods to fit our stochastic volatility model. For the\ntests, we apply our methodology to a highly skewed set of returns based on the\nBarclays US High Yield Index, where we compare different time scalings for the\nsemivariance. Our work shows that the square root of the time horizon seems to\nbe a poor approximation in the context of semivariance and that our methodology\nbased on jump-diffusion processes gives much better results.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 17:05:01 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Oeuvray", "Rodrigue", ""], ["Junod", "Pascal", ""]]}, {"id": "1311.1924", "submitter": "Diego Garlaschelli", "authors": "Mel MacMahon, Diego Garlaschelli", "title": "Community detection for correlation matrices", "comments": "Final version, accepted for publication on PRX", "journal-ref": "Physical Review X 5, 021006 (2015)", "doi": "10.1103/PhysRevX.5.021006", "report-no": null, "categories": "physics.data-an physics.soc-ph q-fin.PM q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenging problem in the study of complex systems is that of resolving,\nwithout prior information, the emergent, mesoscopic organization determined by\ngroups of units whose dynamical activity is more strongly correlated internally\nthan with the rest of the system. The existing techniques to filter\ncorrelations are not explicitly oriented towards identifying such modules and\ncan suffer from an unavoidable information loss. A promising alternative is\nthat of employing community detection techniques developed in network theory.\nUnfortunately, this approach has focused predominantly on replacing network\ndata with correlation matrices, a procedure that tends to be intrinsically\nbiased due to its inconsistency with the null hypotheses underlying the\nexisting algorithms. Here we introduce, via a consistent redefinition of null\nmodels based on random matrix theory, the appropriate correlation-based\ncounterparts of the most popular community detection techniques. Our methods\ncan filter out both unit-specific noise and system-wide dependencies, and the\nresulting communities are internally correlated and mutually anti-correlated.\nWe also implement multiresolution and multifrequency approaches revealing\nhierarchically nested sub-communities with `hard' cores and `soft' peripheries.\nWe apply our techniques to several financial time series and identify\nmesoscopic groups of stocks which are irreducible to a standard, sectorial\ntaxonomy, detect `soft stocks' that alternate between communities, and discuss\nimplications for portfolio optimization and risk management.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 10:29:22 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 12:27:37 GMT"}, {"version": "v3", "created": "Fri, 24 Oct 2014 22:47:02 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["MacMahon", "Mel", ""], ["Garlaschelli", "Diego", ""]]}, {"id": "1311.3764", "submitter": "Abhijnan Rej", "authors": "Abhijnan Rej", "title": "Modeling systemic risks in financial markets", "comments": "9 pages, discussion paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey systemic risks to financial markets and present a high-level\ndescription of an algorithm that measures systemic risk in terms of coupled\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 08:26:52 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Rej", "Abhijnan", ""]]}, {"id": "1311.4266", "submitter": "Sihem Khemakhem", "authors": "Younes Boujelb\\`ene, Sihem Khemakhem", "title": "Pr\\'evision du risque de cr\\'edit : Une \\'etude comparative entre\n  l'Analyse Discriminante et l'Approche Neuronale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Banks are interested in evaluating the risk of the financial distress before\ngiving out a loan. Many researchers proposed the use of models based on the\nNeural Networks in order to help the banker better make a decision. The\nobjective of this paper is to explore a new practical way based on the Neural\nNetworks that would help improve the capacity of the banker to predict the risk\nclass of the companies asking for a loan. This work is motivated by the\ninsufficiency of traditional prevision models. The sample consists of 86\nTunisian firms and 15 financial ratios are calculated, over the period from\n2005 to 2007. The results are compared with those of discriminant analysis.\nThey show that the neural networks technique is the best in term of\npredictability.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 04:47:03 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Boujelb\u00e8ne", "Younes", ""], ["Khemakhem", "Sihem", ""]]}]