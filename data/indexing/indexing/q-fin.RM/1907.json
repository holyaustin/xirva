[{"id": "1907.00293", "submitter": "Tim Leung", "authors": "Tim Leung, Brian Ward", "title": "Tracking VIX with VIX Futures: Portfolio Construction and Performance", "comments": "22 pages, book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a series of static and dynamic portfolios of VIX futures and their\neffectiveness to track the VIX index. We derive each portfolio using\noptimization methods, and evaluate its tracking performance from both empirical\nand theoretical perspectives. Among our results, we show that static portfolios\nof different VIX futures fail to track VIX closely. VIX futures simply do not\nreact quickly enough to movements in the spot VIX. In a discrete-time model, we\ndesign and implement a dynamic trading strategy that adjusts daily to optimally\ntrack VIX. The model is calibrated to historical data and a simulation study is\nperformed to understand the properties exhibited by the strategy. In addition,\ncomparing to the volatility ETN, VXX, we find that our dynamic strategy has a\nsuperior tracking performance.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 23:11:24 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Leung", "Tim", ""], ["Ward", "Brian", ""]]}, {"id": "1907.01306", "submitter": "Tobias Fissler", "authors": "Tobias Fissler, Jana Hlavinov\\'a, Birgit Rudloff", "title": "Elicitability and Identifiability of Systemic Risk Measures", "comments": "42 pages, 3 figures + supplementary material (6 pages, 2 figures)", "journal-ref": "Finance and Stochastics (2021), Volume 25, No. 1, 133-165", "doi": "10.1007/s00780-020-00446-z", "report-no": null, "categories": "math.ST q-fin.MF q-fin.RM q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification and scoring functions are statistical tools to assess the\ncalibration and the relative performance of risk measure estimates, e.g., in\nbacktesting. A risk measures is called identifiable (elicitable) it it admits a\nstrict identification function (strictly consistent scoring function). We\nconsider measures of systemic risk introduced in Feinstein, Rudloff and Weber\n(2017). Since these are set-valued, we work within the theoretical framework of\nFissler, Hlavinov\\'a and Rudloff (2019) for forecast evaluation of set-valued\nfunctionals. We construct oriented selective identification functions, which\ninduce a mixture representation of (strictly) consistent scoring functions.\nTheir applicability is demonstrated with a comprehensive simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 11:52:15 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 12:33:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fissler", "Tobias", ""], ["Hlavinov\u00e1", "Jana", ""], ["Rudloff", "Birgit", ""]]}, {"id": "1907.01800", "submitter": "Jeremy Turiel", "authors": "Jeremy D. Turiel and Tomaso Aste", "title": "P2P Loan acceptance and default prediction with Artificial Intelligence", "comments": "11 pages, 2 figures, 6 tables, presented as case study for the EC\n  HO2020 FinTech project", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic Regression and Support Vector Machine algorithms, together with\nLinear and Non-Linear Deep Neural Networks, are applied to lending data in\norder to replicate lender acceptance of loans and predict the likelihood of\ndefault of issued loans. A two phase model is proposed; the first phase\npredicts loan rejection, while the second one predicts default risk for\napproved loans. Logistic Regression was found to be the best performer for the\nfirst phase, with test set recall macro score of $77.4 \\%$. Deep Neural\nNetworks were applied to the second phase only, were they achieved best\nperformance, with validation set recall score of $72 \\%$, for defaults. This\nshows that AI can improve current credit risk models reducing the default risk\nof issued loans by as much as $70 \\%$. The models were also applied to loans\ntaken for small businesses alone. The first phase of the model performs\nsignificantly better when trained on the whole dataset. Instead, the second\nphase performs significantly better when trained on the small business subset.\nThis suggests a potential discrepancy between how these loans are screened and\nhow they should be analysed in terms of default prediction.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 09:00:00 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Turiel", "Jeremy D.", ""], ["Aste", "Tomaso", ""]]}, {"id": "1907.01828", "submitter": "Jerome Spielmann", "authors": "Yuchao Dong (LASP), J\\'er\\^ome Spielmann (LAREMA, UA)", "title": "Weak Limits of Random Coefficient Autoregressive Processes and their\n  Application in Ruin Theory", "comments": null, "journal-ref": "Insurance: Mathematics and Economics, Elsevier, 2020, 91, pp.1-11", "doi": "10.1016/j.insmatheco.2019.12.001", "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that a large class of discrete-time insurance surplus processes\nconverge weakly to a generalized Ornstein-Uhlenbeck process, under a suitable\nre-normalization and when the time-step goes to 0. Motivated by ruin theory, we\nuse this result to obtain approximations for the moments, the ultimate ruin\nprobability and the discounted penalty function of the discrete-time process.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:10:49 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 09:52:22 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Dong", "Yuchao", "", "LASP"], ["Spielmann", "J\u00e9r\u00f4me", "", "LAREMA, UA"]]}, {"id": "1907.03355", "submitter": "Hung Ba", "authors": "Hung Ba", "title": "Improving Detection of Credit Card Fraudulent Transactions using\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, we employ Generative Adversarial Networks as an oversampling\nmethod to generate artificial data to assist with the classification of credit\ncard fraudulent transactions. GANs is a generative model based on the idea of\ngame theory, in which a generator G and a discriminator D are trying to\noutsmart each other. The objective of the generator is to confuse the\ndiscriminator. The objective of the discriminator is to distinguish the\ninstances coming from the generator and the instances coming from the original\ndataset. By training GANs on a set of credit card fraudulent transactions, we\nare able to improve the discriminatory power of classifiers. The experiment\nresults show that the Wasserstein-GAN is more stable in training and produce\nmore realistic fraudulent transactions than the other GANs. On the other hand,\nthe conditional version of GANs in which labels are set by k-means clustering\ndoes not necessarily improve the non-conditional versions of GANs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 22:20:52 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Ba", "Hung", ""]]}, {"id": "1907.04937", "submitter": "Mohammed Kaicer", "authors": "Mohammed Kaicer and Abdelilah Kaddar", "title": "Mathematical Analysis of Dynamic Risk Default in Microfinance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we will develop a new approach to solve the non repayment\nproblem in microfinance due to the problem of asymmetric information. This\napproach is based on modeling and simulation of ordinary differential systems\nwhere time remains a primordial component, they thus enable microfinance\ninstitutions to manage their risk portfolios by a prediction of numbers of\nsolvent and insolvent borrowers ever a period, in order to define or redefine\nits development strategy, investment and management in an area, where the\npopulation is often poor and in need a mechanism of financial inclusion.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 21:50:36 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Kaicer", "Mohammed", ""], ["Kaddar", "Abdelilah", ""]]}, {"id": "1907.05381", "submitter": "Yuqing Zhang", "authors": "Yuqing Zhang and Neil Walton", "title": "Adaptive Pricing in Insurance: Generalized Linear Models and Gaussian\n  Process Regression Approaches", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.OC q-fin.MF q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the application of dynamic pricing to insurance. We view this as an\nonline revenue management problem where the insurance company looks to set\nprices to optimize the long-run revenue from selling a new insurance product.\nWe develop two pricing models: an adaptive Generalized Linear Model (GLM) and\nan adaptive Gaussian Process (GP) regression model. Both balance between\nexploration, where we choose prices in order to learn the distribution of\ndemands & claims for the insurance product, and exploitation, where we\nmyopically choose the best price from the information gathered so far. The\nperformance of the pricing policies is measured in terms of regret: the\nexpected revenue loss caused by not using the optimal price. As is commonplace\nin insurance, we model demand and claims by GLMs. In our adaptive GLM design,\nwe use the maximum quasi-likelihood estimation (MQLE) to estimate the unknown\nparameters. We show that, if prices are chosen with suitably decreasing\nvariability, the MQLE parameters eventually exist and converge to the correct\nvalues, which in turn implies that the sequence of chosen prices will also\nconverge to the optimal price. In the adaptive GP regression model, we sample\ndemand and claims from Gaussian Processes and then choose selling prices by the\nupper confidence bound rule. We also analyze these GLM and GP pricing\nalgorithms with delayed claims. Although similar results exist in other\ndomains, this is among the first works to consider dynamic pricing problems in\nthe field of insurance. We also believe this is the first work to consider\nGaussian Process regression in the context of insurance pricing. These initial\nfindings suggest that online machine learning algorithms could be a fruitful\narea of future investigation and application in insurance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 08:18:54 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zhang", "Yuqing", ""], ["Walton", "Neil", ""]]}, {"id": "1907.05954", "submitter": "Torsten Heinrich", "authors": "Torsten Heinrich and Juan Sabuco and J. Doyne Farmer", "title": "A simulation of the insurance industry: The problem of risk model\n  homogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an agent-based simulation of the catastrophe insurance and\nreinsurance industry and use it to study the problem of risk model homogeneity.\nThe model simulates the balance sheets of insurance firms, who collect premiums\nfrom clients in return for ensuring them against intermittent, heavy-tailed\nrisks. Firms manage their capital and pay dividends to their investors, and use\neither reinsurance contracts or cat bonds to hedge their tail risk. The model\ngenerates plausible time series of profits and losses and recovers stylized\nfacts, such as the insurance cycle and the emergence of asymmetric, long tailed\nfirm size distributions. We use the model to investigate the problem of risk\nmodel homogeneity. Under Solvency II, insurance companies are required to use\nonly certified risk models. This has led to a situation in which only a few\nfirms provide risk models, creating a systemic fragility to the errors in these\nmodels. We demonstrate that using too few models increases the risk of\nnonpayment and default while lowering profits for the industry as a whole. The\npresence of the reinsurance industry ameliorates the problem but does not\nremove it. Our results suggest that it would be valuable for regulators to\nincentivize model diversity. The framework we develop here provides a first\nstep toward a simulation model of the insurance industry for testing policies\nand strategies for better capital management.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 20:50:08 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 11:00:20 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Heinrich", "Torsten", ""], ["Sabuco", "Juan", ""], ["Farmer", "J. Doyne", ""]]}, {"id": "1907.09990", "submitter": "Mohamed Amine Lkabous", "authors": "Mohamed Amine Lkabous", "title": "Poissonian occupation times of spectrally negative L\\'evy processes with\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the concept of \\emph{Poissonian occupation times}\nbelow level $0$ of spectrally negative L\\'evy processes. In this case,\noccupation time is accumulated only when the process is observed to be negative\nat arrival epochs of an independent Poisson process. Our results extend some\nwell known continuously observed quantities involving occupation times of\nspectrally negative L\\'evy processes. As an application, we establish a link\nbetween Poissonian occupation times and insurance risk models with Parisian\nimplementation delays.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 16:39:30 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Lkabous", "Mohamed Amine", ""]]}, {"id": "1907.09993", "submitter": "Mohamed Amine Lkabous", "authors": "Mohamed Amine Lkabous", "title": "A note on Parisian ruin under a hybrid observation scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the concept of Parisian ruin under the hybrid\nobservation scheme model introduced by Li et al. \\cite{binetal2016}. Under this\nmodel, the process is observed at Poisson arrival times whenever the business\nis financially healthy and it is continuously observed when it goes below $0$.\nThe Parisian ruin is then declared when the process stays below zero for a\nconsecutive period of time greater than a fixed delay. We improve the result\noriginally obtained in \\cite{binetal2016} and we compute other fluctuation\nidentities. All identities are given in terms of second-generation scale\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 16:43:50 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Lkabous", "Mohamed Amine", ""]]}, {"id": "1907.10152", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and Yu Han and Katherine B. Ensor", "title": "Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating\n  Futures Market Realized Volatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Financial markets for Liquified Natural Gas (LNG) are an important and\nrapidly-growing segment of commodities markets. Like other commodities markets,\nthere is an inherent spatial structure to LNG markets, with different price\ndynamics for different points of delivery hubs. Certain hubs support highly\nliquid markets, allowing efficient and robust price discovery, while others are\nhighly illiquid, limiting the effectiveness of standard risk management\ntechniques. We propose a joint modeling strategy, which uses high-frequency\ninformation from thickly-traded hubs to improve volatility estimation and risk\nmanagement at thinly traded hubs. The resulting model has superior in- and\nout-of-sample predictive performance, particularly for several commonly used\nrisk management metrics, demonstrating that joint modeling is indeed possible\nand useful. To improve estimation, a Bayesian estimation strategy is employed\nand data-driven weakly informative priors are suggested. Our model is robust to\nsparse data and can be effectively used in any market with similar irregular\npatterns of data availability.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 21:53:41 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Weylandt", "Michael", ""], ["Han", "Yu", ""], ["Ensor", "Katherine B.", ""]]}, {"id": "1907.11162", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb", "title": "On the Statistical Differences between Binary Forecasts and Real World\n  Payoffs", "comments": "Minor revisions for the accepted version", "journal-ref": "International Journal of Forecasting, April 4, 2020", "doi": "10.1016/j.ijforecast.2019.12.004", "report-no": null, "categories": "q-fin.GN physics.soc-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What do binary (or probabilistic) forecasting abilities have to do with\noverall performance? We map the difference between (univariate) binary\npredictions, bets and \"beliefs\" (expressed as a specific \"event\" will\nhappen/will not happen) and real-world continuous payoffs (numerical benefits\nor harm from an event) and show the effect of their conflation and\nmischaracterization in the decision-science literature. We also examine the\ndifferences under thin and fat tails. The effects are:\n  A- Spuriousness of many psychological results particularly those documenting\nthat humans overestimate tail probabilities and rare events, or that they\noverreact to fears of market crashes, ecological calamities, etc. Many\nperceived \"biases\" are just mischaracterizations by psychologists. There is\nalso a misuse of Hayekian arguments in promoting prediction markets.\n  We quantify such conflations with a metric for \"pseudo-overestimation\".\n  B- Being a \"good forecaster\" in binary space doesn't lead to having a good\nactual performance}, and vice versa, especially under nonlinearities. A binary\nforecasting record is likely to be a reverse indicator under some classes of\ndistributions. Deeper uncertainty or more complicated and realistic probability\ndistribution worsen the conflation .\n  C- Machine Learning: Some nonlinear payoff functions, while not lending\nthemselves to verbalistic expressions and \"forecasts\", are well captured by ML\nor expressed in option contracts.\n  D- Fattailedness: The difference is exacerbated in the power law classes of\nprobability distributions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 11:32:47 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 12:03:30 GMT"}, {"version": "v3", "created": "Sun, 29 Dec 2019 12:35:57 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "1907.11855", "submitter": "Wentao Hu", "authors": "Wentao Hu", "title": "SlideVaR: a risk measure with variable risk attitudes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To find a trade-off between profitability and prudence, financial\npractitioners need to choose appropriate risk measures. Two key points are:\nFirstly, investors' risk attitudes under uncertainty conditions should be an\nimportant reference for risk measures. Secondly, risk attitudes are not\nabsolute. For different market performance, investors have different risk\nattitudes. We proposed a new risk measure named SlideVaR which sufficiently\nreflects the different subjective attitudes of investors and the impact of\nmarket changes on investors' attitudes. We proposed the concept of risk-tail\nregion and risk-tail sub-additivity and proved that SlideVaR satisfies several\nimportant mathematical properties. Moreover, SlideVaR has a simple and\nintuitive form of expression for practical application. Several simulate and\nempirical computations show that SlideVaR has obvious advantages in markets\nwhere the state changes frequently.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 06:44:30 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Hu", "Wentao", ""]]}, {"id": "1907.12179", "submitter": "Fatima Zahra Azayite", "authors": "Fatima Zahra Azayite, Said Achchab", "title": "A hybrid neural network model based on improved PSO and SA for\n  bankruptcy prediction", "comments": "13 pages", "journal-ref": "International Journal of Computer Science Issues, Vol 16, Issue 1,\n  January 2019", "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting firm's failure is one of the most interesting subjects for\ninvestors and decision makers. In this paper, a bankruptcy prediction model is\nproposed based on Artificial Neural networks (ANN). Taking into consideration\nthat the choice of variables to discriminate between bankrupt and non-bankrupt\nfirms influences significantly the model's accuracy and considering the problem\nof local minima, we propose a hybrid ANN based on variables selection\ntechniques. Moreover, we evolve the convergence of Particle Swarm Optimization\n(PSO) by proposing a training algorithm based on an improved PSO and Simulated\nAnnealing. A comparative performance study is reported, and the proposed hybrid\nmodel shows a high performance and convergence in the context of missing data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:07:21 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Azayite", "Fatima Zahra", ""], ["Achchab", "Said", ""]]}, {"id": "1907.12433", "submitter": "Olivier Gu\\'eant", "authors": "Bastien Baldacci, Philippe Bergault, Olivier Gu\\'eant", "title": "Algorithmic market making for options", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we tackle the problem of a market maker in charge of a book\nof options on a single liquid underlying asset. By using an approximation of\nthe portfolio in terms of its vega, we show that the seemingly high-dimensional\nstochastic optimal control problem of an option market maker is in fact\ntractable. More precisely, when volatility is modeled using a classical\nstochastic volatility model -- e.g. the Heston model -- the problem faced by an\noption market maker is characterized by a low-dimensional functional equation\nthat can be solved numerically using a Euler scheme along with interpolation\ntechniques, even for large portfolios. In order to illustrate our findings,\nnumerical examples are provided.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:53:23 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 14:03:40 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 17:49:38 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 15:32:02 GMT"}, {"version": "v5", "created": "Fri, 28 Feb 2020 18:27:38 GMT"}, {"version": "v6", "created": "Mon, 4 May 2020 13:44:20 GMT"}, {"version": "v7", "created": "Thu, 2 Jul 2020 14:46:57 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Baldacci", "Bastien", ""], ["Bergault", "Philippe", ""], ["Gu\u00e9ant", "Olivier", ""]]}, {"id": "1907.12615", "submitter": "Arno Botha", "authors": "Arno Botha, Conrad Beyers, Pieter de Villiers", "title": "A procedure for loss-optimising default definitions across simulated\n  credit risk scenarios", "comments": "This paper is an old and deprecated version of arXiv:2009.11064", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new procedure is presented for the objective comparison and evaluation of\ndefault definitions. This allows the lender to find a default threshold at\nwhich the financial loss of a loan portfolio is minimised, in accordance with\nBasel II. Alternative delinquency measures, other than simply measuring\npayments in arrears, can also be evaluated using this optimisation procedure.\nFurthermore, a simulation study is performed in testing the procedure from\n`first principles' across a wide range of credit risk scenarios. Specifically,\nthree probabilistic techniques are used to generate cash flows, while the\nparameters of each are varied, as part of the simulation study. The results\nshow that loss minima can exist for a select range of credit risk profiles,\nwhich suggests that the loss optimisation of default thresholds can become a\nviable practice. The default decision is therefore framed anew as an\noptimisation problem in choosing a default threshold that is neither too early\nnor too late in loan life. These results also challenges current practices\nwherein default is pragmatically defined as `90 days past due', with little\nobjective evidence for its overall suitability or financial impact, at least\nbeyond flawed roll rate analyses or a regulator's decree.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 19:44:22 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 06:42:27 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Botha", "Arno", ""], ["Beyers", "Conrad", ""], ["de Villiers", "Pieter", ""]]}]