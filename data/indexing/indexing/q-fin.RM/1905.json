[{"id": "1905.00238", "submitter": "Christian P\\\"otz", "authors": "Kathrin Glau, Ricardo Pachon and Christian P\\\"otz", "title": "Fast Calculation of Credit Exposures for Barrier and Bermudan options\n  using Chebyshev interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method to calculate the credit exposure of Bermudan,\ndiscretely monitored barrier and European options. Core of the approach is the\napplication of the dynamic Chebyshev method of Glau et al. (2019). The dynamic\nChebyshev method delivers a closed form approximation of the option prices\nalong the paths together with the options' delta and gamma. Key advantage is\nthe polynomial structure of the approximation, which allows us a highly\nefficient evaluation of the credit exposures, even for a large number of\nsimulated paths. The approach is highly flexible in the model choice, payoff\nprofiles and asset classes. We compute the exposure profiles for Bermudan and\nbarrier options in three different equity models and compare them to the\nprofiles of European options. The analysis reveals potential shortcomings of\ncommon simplifications in the exposure calculation. The proposed method is\nsufficiently simple and efficient to avoid such risk-bearing simplifications.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 09:52:42 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Glau", "Kathrin", ""], ["Pachon", "Ricardo", ""], ["P\u00f6tz", "Christian", ""]]}, {"id": "1905.00486", "submitter": "Fei Sun", "authors": "Fei Sun, Weitao Liu, Xiaozhi Fan", "title": "Set-valued risk statistics with the time value of money", "comments": "arXiv admin note: text overlap with arXiv:1904.11032,\n  arXiv:1904.08829", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time value of money is a critical factor not only in risk analysis, but\nalso in insurance and financial applications. In this paper, we consider a\nspecial class of set-valued risk statistics from the perspective of time value\nof money. This new risk statistic can be uesd for the quantification of\nportfolio risk. By further developing the properties related to these risk\nstatistics, we are able to derive representation results for such risk.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:39:16 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 08:13:24 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 07:30:58 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sun", "Fei", ""], ["Liu", "Weitao", ""], ["Fan", "Xiaozhi", ""]]}, {"id": "1905.04397", "submitter": "Nikolaos Kolliopoulos", "authors": "Ben Hambly, Nikolaos Kolliopoulos", "title": "ERRATUM: Stochastic evolution equations for large portfolios of\n  stochastic volatility models", "comments": "19 pages", "journal-ref": null, "doi": "10.1137/19M1260980", "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the article \"Stochastic evolution equations for large portfolios of\nStochastic Volatility models\" (Arxiv:1701.05640) there is a mistake in the\nproof of Theorem 3.1. In this erratum we establish a weaker version of this\nTheorem and then we redevelop the regularity theory for our problem\naccordingly. This means that most of our regularity results are replaced by\nslightly weaker ones. We also clarify a point in the proof of a correct result.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 23:02:40 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Hambly", "Ben", ""], ["Kolliopoulos", "Nikolaos", ""]]}, {"id": "1905.05023", "submitter": "Adriano Koshiyama", "authors": "Adriano Koshiyama and Nick Firoozye", "title": "Avoiding Backtesting Overfitting by Covariance-Penalties: an empirical\n  investigation of the ordinary and total least squares cases", "comments": "Large portions of this work appeared previously in a replacement of\n  arXiv:1901.01751 (version 2) which was uploaded there by mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic trading strategies are rule-based procedures which choose\nportfolios and allocate assets. In order to attain certain desired return\nprofiles, quantitative strategists must determine a large array of trading\nparameters. Backtesting, the attempt to identify the appropriate parameters\nusing historical data available, has been highly criticized due to the\nabundance of misleading results. Hence, there is an increasing interest in\ndevising procedures for the assessment and comparison of strategies, that is,\ndevising schemes for preventing what is known as backtesting overfitting. So\nfar, many financial researchers have proposed different ways to tackle this\nproblem that can be broadly categorised in three types: Data Snooping,\nOverestimated Performance, and Cross-Validation Evaluation. In this paper, we\npropose a new approach to dealing with financial overfitting, a\nCovariance-Penalty Correction, in which a risk metric is lowered given the\nnumber of parameters and data used to underpins a trading strategy. We outlined\nthe foundation and main results behind the Covariance-Penalty correction for\ntrading strategies. After that, we pursue an empirical investigation, comparing\nits performance with some other approaches in the realm of Covariance-Penalties\nacross more than 1300 assets, using Ordinary and Total Least Squares. Our\nresults suggest that Covariance-Penalties are a suitable procedure to avoid\nBacktesting Overfitting, and Total Least Squares provides superior performance\nwhen compared to Ordinary Least Squares.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 08:39:50 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Koshiyama", "Adriano", ""], ["Firoozye", "Nick", ""]]}, {"id": "1905.05911", "submitter": "Yadong Li", "authors": "Yadong Li, Dimitri Offengenden, Jan Burgy", "title": "Reduced Form Capital Optimization", "comments": "12 pages, 3 figures, 2 tables", "journal-ref": null, "doi": "10.13140/RG.2.2.15535.18080", "report-no": null, "categories": "q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate banks' capital optimization problem as a classic mean variance\noptimization, by leveraging an accurate linear approximation to the Shapely or\nConstrained Aumann-Shapley (CAS) allocation of max or nested max cost\nfunctions. This reduced form formulation admits an analytical solution, to the\noptimal leveraged balance sheet (LBS) and risk weighted assets (RWA) target of\nbanks' business units for achieving the best return on capital.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 01:55:03 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Li", "Yadong", ""], ["Offengenden", "Dimitri", ""], ["Burgy", "Jan", ""]]}, {"id": "1905.05931", "submitter": "Stefan Thurner", "authors": "Christian Diem, Anton Pichler, Stefan Thurner", "title": "What is the Minimal Systemic Risk in Financial Exposure Networks?", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Management of systemic risk in financial markets is traditionally associated\nwith setting (higher) capital requirements for market participants. There are\nindications that while equity ratios have been increased massively since the\nfinancial crisis, systemic risk levels might not have lowered, but even\nincreased. It has been shown that systemic risk is to a large extent related to\nthe underlying network topology of financial exposures. A natural question\narising is how much systemic risk can be eliminated by optimally rearranging\nthese networks and without increasing capital requirements. Overlapping\nportfolios with minimized systemic risk which provide the same market\nfunctionality as empirical ones have been studied by [pichler2018]. Here we\npropose a similar method for direct exposure networks, and apply it to\ncross-sectional interbank loan networks, consisting of 10 quarterly\nobservations of the Austrian interbank market. We show that the suggested\nframework rearranges the network topology, such that systemic risk is reduced\nby a factor of approximately 3.5, and leaves the relevant economic features of\nthe optimized network and its agents unchanged. The presented optimization\nprocedure is not intended to actually re-configure interbank markets, but to\ndemonstrate the huge potential for systemic risk management through rearranging\nexposure networks, in contrast to increasing capital requirements that were\nshown to have only marginal effects on systemic risk [poledna2017]. Ways to\nactually incentivize a self-organized formation toward optimal network\nconfigurations were introduced in [thurner2013] and [poledna2016]. For\nregulatory policies concerning financial market stability the knowledge of\nminimal systemic risk for a given economic environment can serve as a benchmark\nfor monitoring actual systemic risk in markets.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 03:42:06 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Diem", "Christian", ""], ["Pichler", "Anton", ""], ["Thurner", "Stefan", ""]]}, {"id": "1905.07716", "submitter": "Mohammed Berkhouch", "authors": "Mohammed Berkhouch, Ghizlane Lakhnati and Marcelo Brutti Righi", "title": "Spectral risk measures and uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk assessment under different possible scenarios is a source of uncertainty\nthat may lead to concerning financial losses. We address this issue, first, by\nadapting a robust framework to the class of spectral risk measures. Second, we\npropose a Deviation-based approach to quantify uncertainty. Furthermore, the\ntheory is illustrated with a practical case study from NASDAQ index.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 09:59:52 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Berkhouch", "Mohammed", ""], ["Lakhnati", "Ghizlane", ""], ["Righi", "Marcelo Brutti", ""]]}, {"id": "1905.08042", "submitter": "Eric Benhamou", "authors": "Eric Benhamou, David Saltiel, Beatrice Guez, Nicolas Paris", "title": "Testing Sharpe ratio: luck or skill?", "comments": "56 pages, 44 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharpe ratio (sometimes also referred to as information ratio) is widely used\nin asset management to compare and benchmark funds and asset managers. It\ncomputes the ratio of the (excess) net return over the strategy standard\ndeviation. However, the elements to compute the Sharpe ratio, namely, the\nexpected returns and the volatilities are unknown numbers and need to be\nestimated statistically. This means that the Sharpe ratio used by funds is\nlikely to be error prone because of statistical estimation errors. In this\npaper, we provide various tests to measure the quality of the Sharpe ratios. By\nquality, we are aiming at measuring whether a manager was indeed lucky of\nskillful. The test assesses this through the statistical significance of the\nSharpe ratio. We not only look at the traditional Sharpe ratio but also compute\na modified Sharpe insensitive to used Capital. We provide various statistical\ntests that can be used to precisely quantify the fact that the Sharpe is\nstatistically significant. We illustrate in particular the number of trades for\na given Sharpe level that provides statistical significance as well as the\nimpact of auto-correlation by providing reference tables that provides the\nminimum required Sharpe ratio for a given time period and correlation. We also\nprovide for a Sharpe ratio of 0.5, 1.0, 1.5 and 2.0 the skill percentage given\nthe auto-correlation level.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 12:33:48 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 07:15:30 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Benhamou", "Eric", ""], ["Saltiel", "David", ""], ["Guez", "Beatrice", ""], ["Paris", "Nicolas", ""]]}, {"id": "1905.09596", "submitter": "Thorsten Schmidt", "authors": "Laura Ballotta, Ernst Eberlein, Thorsten Schmidt, and Raghid\n  Zeineddine", "title": "Variable annuities in a L\\'evy-based hybrid model with surrender risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a market consistent valuation framework for variable\nannuities with guaranteed minimum accumulation benefit, death benefit and\nsurrender benefit features. The setup is based on a hybrid model for the\nfinancial market and uses time-inhomogeneous L\\'evy processes as risk drivers.\nFurther, we allow for dependence between financial and surrender risks. Our\nmodel leads to explicit analytical formulas for the quantities of interest, and\npractical and efficient numerical procedures for the evaluation of these\nformulas. We illustrate the tractability of this approach by means of a\ndetailed sensitivity analysis of the price of the variable annuity and its\ncomponents with respect to the model parameters. The results highlight the role\nplayed by the surrender behaviour and the importance of its appropriate\nmodelling.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 11:41:09 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Ballotta", "Laura", ""], ["Eberlein", "Ernst", ""], ["Schmidt", "Thorsten", ""], ["Zeineddine", "Raghid", ""]]}, {"id": "1905.09647", "submitter": "Min Shu", "authors": "Min Shu, Wei Zhu", "title": "Real-time Prediction of Bitcoin Bubble Crashes", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.physa.2020.124477", "report-no": null, "categories": "q-fin.ST q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, Bitcoin as an emerging asset class has gained widespread\npublic attention because of their extraordinary returns in phases of extreme\nprice growth and their unpredictable massive crashes. We apply the log-periodic\npower law singularity (LPPLS) confidence indicator as a diagnostic tool for\nidentifying bubbles using the daily data on Bitcoin price in the past two\nyears. We find that the LPPLS confidence indicator based on the daily Bitcoin\nprice data fails to provide effective warnings for detecting the bubbles when\nthe Bitcoin price suffers from a large fluctuation in a short time, especially\nfor positive bubbles. In order to diagnose the existence of bubbles and\naccurately predict the bubble crashes in the cryptocurrency market, this study\nproposes an adaptive multilevel time series detection methodology based on the\nLPPLS model and finer (than daily) timescale for the Bitcoin price data. We\nadopt two levels of time series, 1 hour and 30 minutes, to demonstrate the\nadaptive multilevel time series detection methodology. The results show that\nthe LPPLS confidence indicator based on this new method is an outstanding\ninstrument to effectively detect the bubbles and accurately forecast the bubble\ncrashes, even if a bubble exists in a short time. In addition, we discover that\nthe short-term LPPLS confidence indicator highly sensitive to the extreme\nfluctuations of Bitcoin price can provide some useful insights into the bubble\nstatus on a shorter time scale - on a day to week scale, and the long-term\nLPPLS confidence indicator has a stable performance in terms of effectively\nmonitoring the bubble status on a longer time scale - on a week to month scale.\nThe adaptive multilevel time series detection methodology can provide real-time\ndetection of bubbles and advanced forecast of crashes to warn of the imminent\nrisk.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:38:36 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 05:21:53 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Shu", "Min", ""], ["Zhu", "Wei", ""]]}, {"id": "1905.10164", "submitter": "David Maher", "authors": "David G Maher", "title": "How big should a Stress Shock be?", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stress shocks are often calculated as multiples of the standard deviation of\na history set. This paper investigates how many standard deviations are\nrequired to guarantee that this shock exceeds any observation within the\nhistory set, given the additional constraint of kurtosis. The results of this\nanalysis are then used to validate the shocks produced by some stress test\nmodels, in particular that of Brace-Lauer-Rado. A secondary application of our\nresults is to investigate three known extensions of Chebyshev's Inequality\nwhere the kurtosis is known. It is found that our results give a tighter bound\nthan the well-known inequalities.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 11:47:31 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Maher", "David G", ""]]}, {"id": "1905.12431", "submitter": "Francesca Mariani PhD", "authors": "Lorella Fatone and Francesca Mariani", "title": "An assets-liabilities dynamical model of banking system and systemic\n  risk governance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of governing systemic risk in an assets-liabilities\ndynamical model of banking system. In the model considered each bank is\nrepresented by its assets and its liabilities.The capital reserves of a bank\nare the difference between assets and liabilities of the bank. A bank is\nsolvent when its capital reserves are greater or equal to zero otherwise the\nbank is failed.The banking system dynamics is defined by an initial value\nproblem for a system of stochastic differential equations whose independent\nvariable is time and whose dependent variables are the assets and the\nliabilities of the banks.The banking system model presented generalizes those\ndiscussed in [4],[3] and describes a homogeneous population of banks. The main\nfeatures of the model are a cooperation mechanism among banks and the\npossibility of the (direct) intervention of the monetary authority in the\nbanking system dynamics. We call systemic risk or systemic event in a bounded\ntime interval the fact that in that time interval at least a given fraction of\nthe banks fails. The probability of systemic risk in a bounded time interval is\nevaluated using statistical simulation. The systemic risk governance pursues\nthe goal of keeping the probability of systemic risk in a bounded time interval\nbetween two given thresholds.The monetary authority is responsible for the\nsystemic risk governance.The governance consists in the choice of the assets\nand of the liabilities of a kind of \"ideal bank\" as functions of time and in\nthe choice of the rules that regulate the cooperation mechanism among\nbanks.These rules are obtained solving an optimal control problem for the\npseudo mean field approximation of the banking system model. The governance\ninduces the banks of the system to behave like the \"ideal bank\". Shocks acting\non the assets or on the liabilities of the banks are simulated.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:30:03 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Fatone", "Lorella", ""], ["Mariani", "Francesca", ""]]}, {"id": "1905.13425", "submitter": "Xing Yan", "authors": "Xing Yan, Qi Wu, Wen Zhang", "title": "Cross-sectional Learning of Extremal Dependence among Financial Assets", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, pages\n  3852-3862, 2019", "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel probabilistic model to facilitate the learning of\nmultivariate tail dependence of multiple financial assets. Our method allows\none to construct from known random vectors, e.g., standard normal,\nsophisticated joint heavy-tailed random vectors featuring not only distinct\nmarginal tail heaviness, but also flexible tail dependence structure. The\nnovelty lies in that pairwise tail dependence between any two dimensions is\nmodeled separately from their correlation, and can vary respectively according\nto its own parameter rather than the correlation parameter, which is an\nessential advantage over many commonly used methods such as multivariate $t$ or\nelliptical distribution. It is also intuitive to interpret, easy to track, and\nsimple to sample comparing to the copula approach. We show its flexible tail\ndependence structure through simulation. Coupled with a GARCH model to\neliminate serial dependence of each individual asset return series, we use this\nnovel method to model and forecast multivariate conditional distribution of\nstock returns, and obtain notable performance improvements in multi-dimensional\ncoverage tests. Besides, our empirical finding about the asymmetry of tails of\nthe idiosyncratic component as well as the market component is interesting and\nworth to be well studied in the future.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 05:50:47 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 03:45:47 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 13:24:43 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Yan", "Xing", ""], ["Wu", "Qi", ""], ["Zhang", "Wen", ""]]}, {"id": "1905.13711", "submitter": "Davide Cellai Dr.", "authors": "Davide Cellai and Trevor Fitzpatrick", "title": "The Network Effect in Credit Concentration Risk", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement and management of credit concentration risk is critical for banks\nand relevant for micro-prudential requirements. While several methods exist for\nmeasuring credit concentration risk within institutions, the systemic effect of\ndifferent institutions' exposures to the same counterparties has been less\nexplored so far. In this paper, we propose a measure of the systemic credit\nconcentration risk that arises because of common exposures between different\ninstitutions within a financial system. This approach is based on a network\nmodel that describes the effect of overlapping portfolios. This network metric\nis applied to synthetic and real world data to illustrate that the effect of\ncommon exposures is not fully reflected in single portfolio concentration\nmeasures. It also allows to quantify several aspects of the interplay between\ninterconnectedness and credit risk. Using this network measure, we formulate an\nanalytical approximation for the additional capital requirement corresponding\nto the systemic risk arising from credit concentration interconnectedness. Our\nmethodology also avoids double counting between the granularity adjustment and\nthe common exposure adjustment. Although approximated, this common exposure\nadjustment is able to capture, with only two parameters, an aspect of systemic\nrisk that can extend single portfolios view to a system-wide one.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 16:41:20 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 11:15:03 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Cellai", "Davide", ""], ["Fitzpatrick", "Trevor", ""]]}]