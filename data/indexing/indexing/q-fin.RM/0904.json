[{"id": "0904.0624", "submitter": "Josef Teichmann", "authors": "Juan-Pablo Ortega and Rainer Pullirsch and Josef Teichmann and Julian\n  Wergieluk", "title": "A new approach for scenario generation in Risk management", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new dynamic approach to scenario generation for the purposes of\nrisk management in the banking industry. We connect ideas from conventional\ntechniques -- like historical and Monte Carlo simulation -- and we come up with\na hybrid method that shares the advantages of standard procedures but\neliminates several of their drawbacks. Instead of considering the static\nproblem of constructing one or ten day ahead distributions for vectors of risk\nfactors, we embed the problem into a dynamic framework, where any time horizon\ncan be consistently simulated. Additionally, we use standard models from\nmathematical finance for each risk factor, whence bridging the worlds of\ntrading and risk management.\n  Our approach is based on stochastic differential equations (SDEs), like the\nHJM-equation or the Black-Scholes equation, governing the time evolution of\nrisk factors, on an empirical calibration method to the market for the chosen\nSDEs, and on an Euler scheme (or high-order schemes) for the numerical\nevaluation of the respective SDEs. The empirical calibration procedure\npresented in this paper can be seen as the SDE-counterpart of the so called\nFiltered Historical Simulation method; the behavior of volatility stems in our\ncase out of the assumptions on the underlying SDEs. Furthermore, we are able to\neasily incorporate \"middle-size\" and \"large-size\" events within our framework\nalways making a precise distinction between the information obtained from the\nmarket and the one coming from the necessary a-priori intuition of the risk\nmanager.\n  Results of one concrete implementation are provided.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2009 17:22:04 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2009 16:33:11 GMT"}], "update_date": "2009-08-19", "authors_parsed": [["Ortega", "Juan-Pablo", ""], ["Pullirsch", "Rainer", ""], ["Teichmann", "Josef", ""], ["Wergieluk", "Julian", ""]]}, {"id": "0904.0830", "submitter": "Xiaolin Luo Dr", "authors": "Xiaolin Luo, Pavel V. Shevchenko", "title": "Computing Tails of Compound Distributions Using Direct Numerical\n  Integration", "comments": null, "journal-ref": "Journal of Computational Finance. 13(2), 73-111. (2009)", "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient adaptive direct numerical integration (DNI) algorithm is\ndeveloped for computing high quantiles and conditional Value at Risk (CVaR) of\ncompound distributions using characteristic functions. A key innovation of the\nnumerical scheme is an effective tail integration approximation that reduces\nthe truncation errors significantly with little extra effort. High precision\nresults of the 0.999 quantile and CVaR were obtained for compound losses with\nheavy tails and a very wide range of loss frequencies using the DNI, Fast\nFourier Transform (FFT) and Monte Carlo (MC) methods. These results,\nparticularly relevant to operational risk modelling, can serve as benchmarks\nfor comparing different numerical methods. We found that the adaptive DNI can\nachieve high accuracy with relatively coarse grids. It is much faster than MC\nand competitive with FFT in computing high quantiles and CVaR of compound\ndistributions in the case of moderate to high frequencies and heavy tails.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2009 01:43:48 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2009 04:51:38 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2010 07:04:58 GMT"}], "update_date": "2010-02-04", "authors_parsed": [["Luo", "Xiaolin", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "0904.0870", "submitter": "Sovan Mitra", "authors": "Sovan Mitra", "title": "Risk Measures in Quantitative Finance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.GN q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper was presented and written for two seminars: a national UK\nUniversity Risk Conference and a Risk Management industry workshop. The target\naudience is therefore a cross section of Academics and industry professionals.\n  The current ongoing global credit crunch has highlighted the importance of\nrisk measurement in Finance to companies and regulators alike. Despite risk\nmeasurement's central importance to risk management, few papers exist reviewing\nthem or following their evolution from its foremost beginnings up to the\npresent day risk measures.\n  This paper reviews the most important portfolio risk measures in Financial\nMathematics, from Bernoulli (1738) to Markowitz's Portfolio Theory, to the\npresently preferred risk measures such as CVaR (conditional Value at Risk). We\nprovide a chronological review of the risk measures and survey less commonly\nknown risk measures e.g. Treynor ratio.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2009 09:20:23 GMT"}], "update_date": "2009-04-07", "authors_parsed": [["Mitra", "Sovan", ""]]}, {"id": "0904.1067", "submitter": "Pavel Shevchenko V", "authors": "P. V. Shevchenko and M. V. W\\\"uthrich", "title": "The Structural Modelling of Operational Risk via Bayesian inference:\n  Combining Loss Data with Expert Opinions", "comments": null, "journal-ref": "The Journal of Operational Risk 1(3), pp. 3-26, 2006.\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the Basel II regulatory requirements for the Advanced Measurement\nApproaches, the bank's internal model must include the use of internal data,\nrelevant external data, scenario analysis and factors reflecting the business\nenvironment and internal control systems. Quantification of operational risk\ncannot be based only on historical data but should involve scenario analysis.\nHistorical internal operational risk loss data have limited ability to predict\nfuture behaviour moreover, banks do not have enough internal data to estimate\nlow frequency high impact events adequately. Historical external data are\ndifficult to use due to different volumes and other factors. In addition,\ninternal and external data have a survival bias, since typically one does not\nhave data of all collapsed companies. The idea of scenario analysis is to\nestimate frequency and severity of risk events via expert opinions taking into\naccount bank environment factors with reference to events that have occurred\n(or may have occurred) in other banks. Scenario analysis is forward looking and\ncan reflect changes in the banking environment. It is important to not only\nquantify the operational risk capital but also provide incentives to business\nunits to improve their risk management policies, which can be accomplished\nthrough scenario analysis. By itself, scenario analysis is very subjective but\ncombined with loss data it is a powerful tool to estimate operational risk\nlosses. Bayesian inference is a statistical technique well suited for combining\nexpert opinions and historical data. In this paper, we present examples of the\nBayesian inference methods for operational risk quantification.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2009 07:37:58 GMT"}], "update_date": "2009-04-08", "authors_parsed": [["Shevchenko", "P. V.", ""], ["W\u00fcthrich", "M. V.", ""]]}, {"id": "0904.1131", "submitter": "Sovan Mitra", "authors": "Sovan Mitra", "title": "Optimisation of Stochastic Programming by Hidden Markov Modelling based\n  Scenario Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formed part of a preliminary research report for a risk\nconsultancy and academic research. Stochastic Programming models provide a\npowerful paradigm for decision making under uncertainty. In these models the\nuncertainties are represented by a discrete scenario tree and the quality of\nthe solutions obtained is governed by the quality of the scenarios generated.\nWe propose a new technique to generate scenarios based on Gaussian Mixture\nHidden Markov Modelling. We show that our approach explicitly captures\nimportant time varying dynamics of stochastic processes (such as autoregression\nand jumps) as well as non-Gaussian distribution characteristics (such as\nskewness and kurtosis). Our scenario generation method enables richer\nrobustness and scenario analysis through exploiting the tractable properties of\nMarkov models and Gaussian mixture distributions. We demonstrate the benefits\nof our scenario generation method by conducting numerical experiments on\nFTSE-100 data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2009 13:36:30 GMT"}], "update_date": "2009-04-08", "authors_parsed": [["Mitra", "Sovan", ""]]}, {"id": "0904.1361", "submitter": "Pavel Shevchenko V", "authors": "Dominik D. Lambrigger, Pavel V. Shevchenko and Mario V. W\\\"uthrich", "title": "The Quantification of Operational Risk using Internal Data, Relevant\n  External Data and Expert Opinions", "comments": null, "journal-ref": "The Journal of Operational Risk 2(3), pp.3-27, 2007.\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To quantify an operational risk capital charge under Basel II, many banks\nadopt a Loss Distribution Approach. Under this approach, quantification of the\nfrequency and severity distributions of operational risk involves the bank's\ninternal data, expert opinions and relevant external data. In this paper we\nsuggest a new approach, based on a Bayesian inference method, that allows for a\ncombination of these three sources of information to estimate the parameters of\nthe risk frequency and severity distributions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2009 15:14:42 GMT"}], "update_date": "2009-04-09", "authors_parsed": [["Lambrigger", "Dominik D.", ""], ["Shevchenko", "Pavel V.", ""], ["W\u00fcthrich", "Mario V.", ""]]}, {"id": "0904.1483", "submitter": "Pavel Shevchenko V", "authors": "Gareth W. Peters, Pavel V. Shevchenko and Mario V. W\\\"uthrich", "title": "Model uncertainty in claims reserving within Tweedie's compound Poisson\n  models", "comments": null, "journal-ref": "ASTIN Bulletin 39(1), pp.1-33, 2009", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the claims reserving problem using Tweedie's\ncompound Poisson model. We develop the maximum likelihood and Bayesian Markov\nchain Monte Carlo simulation approaches to fit the model and then compare the\nestimated models under different scenarios. The key point we demonstrate\nrelates to the comparison of reserving quantities with and without model\nuncertainty incorporated into the prediction. We consider both the model\nselection problem and the model averaging solutions for the predicted reserves.\nAs a part of this process we also consider the sub problem of variable\nselection to obtain a parsimonious representation of the model being fitted.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2009 09:23:37 GMT"}], "update_date": "2009-04-10", "authors_parsed": [["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel V.", ""], ["W\u00fcthrich", "Mario V.", ""]]}, {"id": "0904.1653", "submitter": "Didier Rulli\\`ere", "authors": "Didier Rulli\\`ere (SAF), Diana Dorobantu (SAF), Areski Cousin (SAF)", "title": "An extension of Davis and Lo's contagion model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper provides a multi-period contagion model in the credit risk\nfield. Our model is an extension of Davis and Lo's infectious default model. We\nconsider an economy of n firms which may default directly or may be infected by\nother defaulting firms (a domino effect being also possible). The spontaneous\ndefault without external influence and the infections are described by not\nnecessarily independent Bernoulli-type random variables. Moreover, several\ncontaminations could be required to infect another firm. In this paper we\ncompute the probability distribution function of the total number of defaults\nin a dependency context. We also give a simple recursive algorithm to compute\nthis distribution in an exchangeability context. Numerical applications\nillustrate the impact of exchangeability among direct defaults and among\ncontaminations, on different indicators calculated from the law of the total\nnumber of defaults. We then examine the calibration of the model on iTraxx data\nbefore and during the crisis. The dynamic feature together with the contagion\neffect seem to have a significant impact on the model performance, especially\nduring the recent distressed period.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2009 08:00:08 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2010 10:14:31 GMT"}], "update_date": "2010-02-01", "authors_parsed": [["Rulli\u00e8re", "Didier", "", "SAF"], ["Dorobantu", "Diana", "", "SAF"], ["Cousin", "Areski", "", "SAF"]]}, {"id": "0904.1771", "submitter": "Pavel Shevchenko V", "authors": "Pavel V. Shevchenko", "title": "Estimation of Operational Risk Capital Charge under Parameter\n  Uncertainty", "comments": null, "journal-ref": "The Journal of Operational Risk 3(1), pp. 51-63, 2008\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many banks adopt the Loss Distribution Approach to quantify the operational\nrisk capital charge under Basel II requirements. It is common practice to\nestimate the capital charge using the 0.999 quantile of the annual loss\ndistribution, calculated using point estimators of the frequency and severity\ndistribution parameters. The uncertainty of the parameter estimates is\ntypically ignored. One of the unpleasant consequences for the banks accounting\nfor parameter uncertainty is an increase in the capital requirement. This paper\ndemonstrates how the parameter uncertainty can be taken into account using a\nBayesian framework that also allows for incorporation of expert opinions and\nexternal data into the estimation procedure.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2009 02:15:23 GMT"}], "update_date": "2009-04-14", "authors_parsed": [["Shevchenko", "Pavel V.", ""]]}, {"id": "0904.1772", "submitter": "Pavel Shevchenko V", "authors": "Hans B\\\"uhlmann, Pavel V. Shevchenko and Mario V. W\\\"uthrich", "title": "A \"Toy\" Model for Operational Risk Quantification using Credibility\n  Theory", "comments": null, "journal-ref": "The Journal of Operational Risk 2(1), pp. 3-19, 2007.\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the Basel II regulatory requirements for the Advanced Measurement\nApproaches in operational risk, the bank's internal model should make use of\nthe internal data, relevant external data, scenario analysis and factors\nreflecting the business environment and internal control systems. One of the\nunresolved challenges in operational risk is combining of these data sources\nappropriately. In this paper we focus on quantification of the low frequency\nhigh impact losses exceeding some high threshold. We suggest a full credibility\ntheory approach to estimate frequency and severity distributions of these\nlosses by taking into account bank internal data, expert opinions and industry\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2009 02:40:00 GMT"}], "update_date": "2009-04-14", "authors_parsed": [["B\u00fchlmann", "Hans", ""], ["Shevchenko", "Pavel V.", ""], ["W\u00fcthrich", "Mario V.", ""]]}, {"id": "0904.1805", "submitter": "Pavel Shevchenko V", "authors": "Pavel V. Shevchenko", "title": "Implementing Loss Distribution Approach for Operational Risk", "comments": null, "journal-ref": "Applied Stochastic Models in Business and Industry (2010), volume\n  26 issue 3, pages: 277-307", "doi": "10.1002/asmb.812", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To quantify the operational risk capital charge under the current regulatory\nframework for banking supervision, referred to as Basel II, many banks adopt\nthe Loss Distribution Approach. There are many modeling issues that should be\nresolved to use the approach in practice. In this paper we review the\nquantitative methods suggested in literature for implementation of the\napproach. In particular, the use of the Bayesian inference method that allows\nto take expert judgement and parameter uncertainty into account, modeling\ndependence and inclusion of insurance are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2009 14:56:16 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2009 12:20:44 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Shevchenko", "Pavel V.", ""]]}, {"id": "0904.2731", "submitter": "Sovan Mitra", "authors": "Sovan Mitra", "title": "An Introduction to Hedge Funds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report was originally written as an industry white paper on Hedge Funds.\nThis paper gives an overview to Hedge Funds, with a focus on risk management\nissues. We define and explain the general characteristics of Hedge Funds, their\nmain investment strategies and the risk models employed. We address the\nproblems in Hedge Fund modelling, survey current Hedge Funds available on the\nmarket and those that have been withdrawn. Finally, we summarise the supporting\nand opposing arguments for Hedge Fund usage. A unique value of this paper,\ncompared to other Hedge Fund literature freely available on the internet, is\nthat this review is fully sourced from academic references (such as peer\nreviewed journals) and is thus a bona fide study. This paper will be of\ninterest to: Hedge Fund and Mutual Fund Managers, Quantitative Analysts,\n\"Front\" and \"Middle\" office banking functions e.g. Treasury Management,\nRegulators concerned with Hedge Fund Financial Risk Management, Private and\nInstitutional Investors, Academic Researchers in the area of Financial Risk\nManagement and the general Finance community.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2009 15:55:28 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2009 20:48:09 GMT"}], "update_date": "2009-04-20", "authors_parsed": [["Mitra", "Sovan", ""]]}, {"id": "0904.2910", "submitter": "Pavel Shevchenko V", "authors": "Xiaolin Luo, Pavel V. Shevchenko and John B. Donnelly", "title": "Addressing the Impact of Data Truncation and Parameter Uncertainty on\n  Operational Risk Estimates", "comments": null, "journal-ref": "The Journal of Operational Risk 2(4), 3-26, 2007\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, operational risk losses are reported above some threshold. This\npaper studies the impact of ignoring data truncation on the 0.999 quantile of\nthe annual loss distribution for operational risk for a broad range of\ndistribution parameters and truncation levels. Loss frequency and severity are\nmodelled by the Poisson and Lognormal distributions respectively. Two cases of\nignoring data truncation are studied: the \"naive model\" - fitting a Lognormal\ndistribution with support on a positive semi-infinite interval, and \"shifted\nmodel\" - fitting a Lognormal distribution shifted to the truncation level. For\nall practical cases, the \"naive model\" leads to underestimation (that can be\nsevere) of the 0.999 quantile. The \"shifted model\" overestimates the 0.999\nquantile except some cases of small underestimation for large truncation\nlevels. Conservative estimation of capital charge is usually acceptable and the\nuse of the \"shifted model\" can be justified while the \"naive model\" should not\nbe allowed. However, if parameter uncertainty is taken into account (in\npractice it is often ignored), the \"shifted model\" can lead to considerable\nunderestimation of capital charge. This is demonstrated with a practical\nexample.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2009 12:56:50 GMT"}], "update_date": "2009-04-21", "authors_parsed": [["Luo", "Xiaolin", ""], ["Shevchenko", "Pavel V.", ""], ["Donnelly", "John B.", ""]]}, {"id": "0904.4074", "submitter": "Pavel Shevchenko V", "authors": "Gareth W. Peters, Pavel V. Shevchenko and Mario V. W\\\"uthrich", "title": "Dynamic operational risk: modeling dependence and combining different\n  sources of information", "comments": null, "journal-ref": "The Journal of Operational Risk 4(2), pp. 69-104, 2009\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we model dependence between operational risks by allowing risk\nprofiles to evolve stochastically in time and to be dependent. This allows for\na flexible correlation structure where the dependence between frequencies of\ndifferent risk categories and between severities of different risk categories\nas well as within risk categories can be modeled. The model is estimated using\nBayesian inference methodology, allowing for combination of internal data,\nexternal data and expert opinion in the estimation procedure. We use a\nspecialized Markov chain Monte Carlo simulation methodology known as Slice\nsampling to obtain samples from the resulting posterior distribution and\nestimate the model parameters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2009 02:12:24 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2009 07:02:31 GMT"}], "update_date": "2009-07-31", "authors_parsed": [["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel V.", ""], ["W\u00fcthrich", "Mario V.", ""]]}, {"id": "0904.4075", "submitter": "Pavel Shevchenko V", "authors": "Pavel V. Shevchenko and Grigory Temnov", "title": "Modeling operational risk data reported above a time-varying threshold", "comments": null, "journal-ref": "The Journal of Operational Risk 4(2), pp. 19-42, 2009\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, operational risk losses are reported above a threshold. Fitting\ndata reported above a constant threshold is a well known and studied problem.\nHowever, in practice, the losses are scaled for business and other factors\nbefore the fitting and thus the threshold is varying across the scaled data\nsample. A reporting level may also change when a bank changes its reporting\npolicy. We present both the maximum likelihood and Bayesian Markov chain Monte\nCarlo approaches to fitting the frequency and severity loss distributions using\ndata in the case of a time varying threshold. Estimation of the annual loss\ndistribution accounting for parameter uncertainty is also presented.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2009 02:27:43 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2009 01:29:58 GMT"}], "update_date": "2009-07-31", "authors_parsed": [["Shevchenko", "Pavel V.", ""], ["Temnov", "Grigory", ""]]}, {"id": "0904.4099", "submitter": "Marco Bartolozzi Dr", "authors": "M. Bartolozzi and C. Mellen", "title": "Local Risk Decomposition for High-frequency Trading Systems", "comments": "12 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work we address the problem of evaluating the historical\nperformance of a trading strategy or a certain portfolio of assets. Common\nindicators such as the Sharpe ratio and the risk adjusted return have\nsignificant drawbacks. In particular, they are global indices, that is they do\nnot preserve any 'local' information about the performance dynamics either in\ntime or for a particular investment horizon. This information could be\nfundamental for practitioners as the past performance can be affected by the\nnon-stationarity of financial market. In order to highlight this feature, we\nintroduce the 'local risk decomposition' (LRD) formalism, where dynamical\ninformation about a strategy's performance is retained. This framework,\nmotivated by the multi-scaling techniques used in complex system theory, is\nparticularly suitable for high-frequency trading systems and can be applied\ninto problems of strategy optimization.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2009 07:46:12 GMT"}, {"version": "v2", "created": "Wed, 9 Feb 2011 02:47:10 GMT"}], "update_date": "2011-02-10", "authors_parsed": [["Bartolozzi", "M.", ""], ["Mellen", "C.", ""]]}, {"id": "0904.4430", "submitter": "Pawe{\\l} Sieczka", "authors": "Pawe{\\l} Sieczka, Janusz A. Ho{\\l}yst", "title": "Collective firm bankruptcies and phase transition in rating dynamics", "comments": null, "journal-ref": "The European Physical Journal B 71 461-466 (2009)", "doi": "10.1140/epjb/e2009-00322-1", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple model of firm rating evolution. We consider two sources\nof defaults: individual dynamics of economic development and Potts-like\ninteractions between firms. We show that such a defined model leads to phase\ntransition, which results in collective defaults. The existence of the\ncollective phase depends on the mean interaction strength. For small\ninteraction strength parameters, there are many independent bankruptcies of\nindividual companies. For large parameters, there are giant collective defaults\nof firm clusters. In the case when the individual firm dynamics favors dumping\nof rating changes, there is an optimal strength of the firm's interactions from\nthe systemic risk point of view.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2009 15:36:25 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2009 14:21:34 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Sieczka", "Pawe\u0142", ""], ["Ho\u0142yst", "Janusz A.", ""]]}, {"id": "0904.4620", "submitter": "Josep J. Masdemont", "authors": "Josep J. Masdemont, Luis Ortiz-Gracia", "title": "Haar Wavelets-Based Approach for Quantifying Credit Portfolio Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new methodology to compute Value at Risk (VaR) for\nquantifying losses in credit portfolios. We approximate the cumulative\ndistribution of the loss function by a finite combination of Haar wavelets\nbasis functions and calculate the coefficients of the approximation by\ninverting its Laplace transform. In fact, we demonstrate that only a few\ncoefficients of the approximation are needed, so VaR can be reached quickly. To\ntest the methodology we consider the Vasicek one-factor portfolio credit loss\nmodel as our model framework. The Haar wavelets method is fast, accurate and\nrobust to deal with small or concentrated portfolios, when the hypothesis of\nthe Basel II formulas are violated.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2009 14:22:30 GMT"}], "update_date": "2009-04-30", "authors_parsed": [["Masdemont", "Josep J.", ""], ["Ortiz-Gracia", "Luis", ""]]}]