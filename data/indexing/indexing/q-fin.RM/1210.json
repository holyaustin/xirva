[{"id": "1210.0057", "submitter": "Karol Przanowski", "authors": "Karol Przanowski and Jolanta Mamczarz", "title": "Consumer finance data generator - a new approach to Credit Scoring\n  technique comparison", "comments": "21 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to present a general idea of method comparison of Credit\nScoring techniques. Any scorecard can be made in various methods based on\nvariable transformations in the logistic regression model. To make a comparison\nand come up with the proof that one technique is better than another is a big\nchallenge due to the limited availability of data. The same conclusion cannot\nbe guaranteed when using other data from another source. The following research\nchallenge can therefore be formulated: how should the comparison be managed in\norder to get general results that are not biased by particular data? The\nsolution may be in the use of various random data generators. The data\ngenerator uses two approaches: transition matrix and scorings. Here are\npresented both: results of comparison methods and the methodology of these\ncomparison techniques creating. Before building a new model the modeler can\nundertake a comparison exercise that aims at identifying the best method in the\ncase of the particular data. Here are presented various measures of predictive\nmodel like: Gini, Delta Gini, VIF and Max p-value, emphasizing the\nmulti-criteria problem of a \"Good model\". The idea that is being suggested is\nof particular use in the model building process where there are defined complex\ncriteria trying to cover the important problems of model stability over a\nperiod of time, in order to avoid a crisis. Some arguments for choosing Logit\nor WOE approach as the best scorecard technique are presented.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 23:30:00 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Przanowski", "Karol", ""], ["Mamczarz", "Jolanta", ""]]}, {"id": "1210.1848", "submitter": "Tiexin Guo", "authors": "Tiexin Guo, Shien Zhao and Xiaolin Zeng", "title": "On random convex analysis -- the analytic foundation of the module\n  approach to conditional risk measures", "comments": "69 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide a solid analytic foundation for the module approach to conditional\nrisk measures, this paper establishes a complete random convex analysis over\nrandom locally convex modules by simultaneously considering the two kinds of\ntopologies (namely the $(\\varepsilon,\\lambda)$--topology and the locally\n$L^0$-- convex topology). Then, we make use of the advantage of the\n$(\\varepsilon,\\lambda)$--topology and grasp the local property of $L^0$--convex\nconditional risk measures to prove that every $L^{0}$--convex\n$L^{p}$--conditional risk measure ($1\\leq p\\leq+\\infty$) can be uniquely\nextended to an $L^{0}$--convex $L^{p}_{\\mathcal{F}}(\\mathcal{E})$--conditional\nrisk measure and that the dual representation theorem of the former can also be\nregarded as a special case of that of the latter, which shows that the study of\n$L^p$--conditional risk measures can be incorporated into that of\n$L^{p}_{\\mathcal{F}}(\\mathcal{E})$--conditional risk measures. In particular,\nin the process we find that combining the countable concatenation hull of a set\nand the local property of conditional risk measures is a very useful analytic\nskill that may considerably simplify and improve the study of $L^{0}$--convex\nconditional risk measures.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 12:33:35 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2012 10:25:36 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2012 01:05:17 GMT"}, {"version": "v4", "created": "Sun, 4 Nov 2012 13:33:55 GMT"}, {"version": "v5", "created": "Wed, 2 Jan 2013 08:28:31 GMT"}, {"version": "v6", "created": "Thu, 7 Mar 2013 04:52:29 GMT"}], "update_date": "2013-08-03", "authors_parsed": [["Guo", "Tiexin", ""], ["Zhao", "Shien", ""], ["Zeng", "Xiaolin", ""]]}, {"id": "1210.1966", "submitter": "Nassim N. Taleb", "authors": "Nassim N. Taleb", "title": "How We Tend To Overestimate Powerlaw Tail Exponents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of a layer of metaprobabilities (from uncertainty concerning\nthe parameters), the asymptotic tail exponent corresponds to the lowest\npossible tail exponent regardless of its probability. The problem explains\n\"Black Swan\" effects, i.e., why measurements tend to chronically underestimate\ntail contributions, rather than merely deliver imprecise but unbiased\nestimates.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2012 15:02:39 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Taleb", "Nassim N.", ""]]}, {"id": "1210.2021", "submitter": "Abdul Razaque", "authors": "Abdul Razaque, Christian Bach, Nyembo salama, Aziz Alotaibi", "title": "Fostering Project Scheduling and Controlling Risk Management", "comments": "10 pages, 5 figures (International Journal of Business and Social\n  Science)", "journal-ref": "International Journal of Business and Social Science Volume 3(14)\n  Special Issue July 2012", "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deployment of emerging technologies and rapid change in industries has\ncreated a lot of risk for initiating the new projects. Many techniques and\nsuggestions have been introduced but still lack the gap from various\nprospective. This paper proposes a reliable project scheduling approach. The\nobjectives of project scheduling approach are to focus on critical chain\nschedule and risk management. Several risks and reservations exist in projects.\nThese critical reservations may not only foil the projects to be finished\nwithin time limit and budget, but also degrades the quality, and operational\nprocess. In the proposed approach, the potential risks of project are\ncritically analyzed. To overcome these potential risks, fuzzy failure mode and\neffect analysis (FMEA) is introduced. In addition, several affects of each risk\nagainst each activity are evaluated. We use Monte Carlo simulation that helps\nto calculate the total time of project. Our approach helps to control risk\nmitigation that is determined using event tree analysis and fault tree\nanalysis. We also implement distribute critical chain schedule for reliable\nscheduling that makes the project to be implemented within defined plan and\nschedule. Finally, adaptive procedure with density (APD) is deployed to get\nreasonable feeding buffer time and project buffer time.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 05:26:53 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Razaque", "Abdul", ""], ["Bach", "Christian", ""], ["salama", "Nyembo", ""], ["Alotaibi", "Aziz", ""]]}, {"id": "1210.2043", "submitter": "Gregor Wei{\\ss}", "authors": "Gregor Wei{\\ss} and Marcus Scheffer", "title": "Smooth Nonparametric Bernstein Vine Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use nonparametric Bernstein copulas as bivariate pair-copulas\nin high-dimensional vine models. The resulting smooth and nonparametric vine\ncopulas completely obviate the error-prone need for choosing the pair-copulas\nfrom parametric copula families. By means of a simulation study and an\nempirical analysis of financial market data, we show that our proposed smooth\nnonparametric vine copula model is superior to competing parametric vine models\ncalibrated via Akaike's Information Criterion.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 11:35:11 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Wei\u00df", "Gregor", ""], ["Scheffer", "Marcus", ""]]}, {"id": "1210.3814", "submitter": "Andrei Leonidov", "authors": "A.V. Leonidov, E.L. Rumyantsev", "title": "Russian interbank networks: main characteristics and stability with\n  respect to contagion", "comments": "To appear in the Proceedings the International Conference\n  \"Instabilities and Control of Excitable Networks: from macro- to nano-\n  systems\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systemic risks characterizing the Russian overnight interbank market from the\nnetwork point of view are analyzed.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2012 17:09:30 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Leonidov", "A. V.", ""], ["Rumyantsev", "E. L.", ""]]}, {"id": "1210.3849", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Alice X. D. Dong, Robert Kohn", "title": "A Copula Based Bayesian Approach for Paid-Incurred Claims Models for\n  Non-Life Insurance Reserving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article considers the class of recently developed stochastic models that\ncombine claims payments and incurred losses information into a coherent\nreserving methodology. In particular, we develop a family of Heirarchical\nBayesian Paid-Incurred-Claims models, combining the claims reserving models of\nHertig et al. (1985) and Gogol et al. (1993). In the process we extend the\nindependent log-normal model of Merz et al. (2010) by incorporating different\ndependence structures using a Data-Augmented mixture Copula Paid-Incurred\nclaims model.\n  The utility and influence of incorporating both payment and incurred losses\ninto estimating of the full predictive distribution of the outstanding loss\nliabilities and the resulting reserves is demonstrated in the following cases:\n(i) an independent payment (P) data model; (ii) the independent\nPayment-Incurred Claims (PIC) data model of Merz et al. (2010); (iii) a novel\ndependent lag-year telescoping block diagonal Gaussian Copula PIC data model\nincorporating conjugacy via transformation; (iv) a novel data-augmented mixture\nArchimedean copula dependent PIC data model.\n  Inference in such models is developed via a class of adaptive Markov chain\nMonte Carlo sampling algorithms. These incorporate a data-augmentation\nframework utilized to efficiently evaluate the likelihood for the copula based\nPIC model in the loss reserving triangles. The adaptation strategy is based on\nrepresenting a positive definite covariance matrix by the exponential of a\nsymmetric matrix as proposed by Leonard et al. (1992).\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2012 22:12:41 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2012 20:36:38 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2012 23:02:22 GMT"}, {"version": "v4", "created": "Sun, 9 Dec 2012 16:36:55 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Peters", "Gareth W.", ""], ["Dong", "Alice X. D.", ""], ["Kohn", "Robert", ""]]}, {"id": "1210.3851", "submitter": "Gareth Peters Dr", "authors": "P. Del Moral, G. W. Peters, Ch. Verg\\'e", "title": "An introduction to particle integration methods: with applications to\n  risk and insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interacting particle methods are increasingly used to sample from complex and\nhigh-dimensional distributions. These stochastic particle integration\ntechniques can be interpreted as an universal acceptance-rejection sequential\nparticle sampler equipped with adaptive and interacting recycling mechanisms.\nPractically, the particles evolve randomly around the space independently and\nto each particle is associated a positive potential function. Periodically,\nparticles with high potentials duplicate at the expense of low potential\nparticle which die. This natural genetic type selection scheme appears in\nnumerous applications in applied probability, physics, Bayesian statistics,\nsignal processing, biology, and information engineering. It is the intention of\nthis paper to introduce them to risk modeling. From a purely mathematical point\nof view, these stochastic samplers can be interpreted as Feynman-Kac particle\nintegration methods. These functional models are natural mathematical\nextensions of the traditional change of probability measures, commonly used to\ndesign an importance sampling strategy. In this article, we provide a brief\nintroduction to the stochastic modeling and the theoretical analysis of these\nparticle algorithms. Then we conclude with an illustration of a subset of such\nmethods to resolve important risk measure and capital estimation in risk and\ninsurance modelling.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2012 22:34:32 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2012 17:42:43 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Del Moral", "P.", ""], ["Peters", "G. W.", ""], ["Verg\u00e9", "Ch.", ""]]}, {"id": "1210.4713", "submitter": "Brice Hakwa wemaguela", "authors": "Brice Hakwa, Manfred J\\\"ager-Ambro\\.zewicz, Barbara R\\\"udiger", "title": "Measuring and Analysing Marginal Systemic Risk Contribution using CoVaR:\n  A Copula Approach", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the quantification and analysis of marginal risk\ncontribution of a given single financial institution i to the risk of a\nfinancial system s. Our work expands on the CoVaR concept proposed by Adrian\nand Brunnermeier as a tool for the measurement of marginal systemic risk\ncontribution. We first give a mathematical definition of\nCoVaR_{\\alpha}^{s|L^i=l}. Our definition improves the CoVaR concept by\nexpressing CoVaR_{\\alpha}^{s|L^i=l} as a function of a state l and of a given\nprobability level \\alpha relative to i and s respectively. Based on Copula\ntheory we connect CoVaR_{\\alpha}^{s|L^i=l} to the partial derivatives of Copula\nthrough their probabilistic interpretation and definitions (Conditional\nProbability). Using this we provide a closed formula for the calculation of\nCoVaR_{\\alpha}^{s|L^i=l} for a large class of (marginal) distributions and\ndependence structures (linear and non-linear). Our formula allows a better\nanalysis of systemic risk using CoVaR in the sense that it allows to define\nCoVaR_{\\alpha}^{s|L^i=l} depending on the marginal distributions of the losses\nof i and s respectively and the copula between L^i and L^s. We discuss the\nimplications of this in the context of the quantification and analysis of\nsystemic risk contributions. %some mathematical This makes possible the For\nexample we will analyse the marginal effects of L^i, L^s and C of the risk\ncontribution of i.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 12:21:57 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2012 14:35:15 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Hakwa", "Brice", ""], ["J\u00e4ger-Ambro\u017cewicz", "Manfred", ""], ["R\u00fcdiger", "Barbara", ""]]}, {"id": "1210.4973", "submitter": "Xuqing Huang", "authors": "Xuqing Huang, Irena Vodenska, Shlomo Havlin, H. Eugene Stanley", "title": "Cascading Failures in Bi-partite Graphs: Model for Systemic Risk\n  Propagation", "comments": "13 pages, 7 figures", "journal-ref": "Scientific Reports 3, Article number: 1219, 2013", "doi": "10.1038/srep01219", "report-no": null, "categories": "q-fin.GN physics.soc-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As economic entities become increasingly interconnected, a shock in a\nfinancial network can provoke significant cascading failures throughout the\nsystem. To study the systemic risk of financial systems, we create a bi-partite\nbanking network model composed of banks and bank assets and propose a cascading\nfailure model to describe the risk propagation process during crises. We\nempirically test the model with 2007 US commercial banks balance sheet data and\ncompare the model prediction of the failed banks with the real failed banks\nafter 2007. We find that our model efficiently identifies a significant portion\nof the actual failed banks reported by Federal Deposit Insurance Corporation.\nThe results suggest that this model could be useful for systemic risk stress\ntesting for financial systems. The model also identifies that commercial rather\nthan residential real estate assets are major culprits for the failure of over\n350 US commercial banks during 2008-2011.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 21:54:44 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2012 20:08:41 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2013 14:53:00 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Huang", "Xuqing", ""], ["Vodenska", "Irena", ""], ["Havlin", "Shlomo", ""], ["Stanley", "H. Eugene", ""]]}, {"id": "1210.5046", "submitter": "Zorana Grbac", "authors": "St\\'ephane Cr\\'epey, R\\'emi Gerboud, Zorana Grbac and Nathalie Ngor", "title": "Counterparty Risk and Funding: The Four Wings of the TVA", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The credit crisis and the ongoing European sovereign debt crisis have\nhighlighted the native form of credit risk, namely the counterparty risk. The\nrelated Credit Valuation Adjustment, (CVA), Debt Valuation Adjustment (DVA),\nLiquidity Valuation Adjustment (LVA) and Replacement Cost (RC) issues, jointly\nreferred to in this paper as Total Valuation Adjustment (TVA), have been\nthoroughly investigated in the theoretical papers [Cr12a] and [Cr12b]. The\npresent work provides an executive summary and numerical companion to these\npapers, through which the TVA pricing problem can be reduced to Markovian\npre-default TVA BSDEs. The first step consists in the counterparty clean\nvaluation of a portfolio of contracts, which is the valuation in a hypothetical\nsituation where the two parties would be risk-free and funded at a risk-free\nrate. In the second step, the TVA is obtained as the value of an option on the\ncounterparty clean value process called Contingent Credit Default Swap (CCDS).\nNumerical results are presented for interest rate swaps in the Vasicek, as well\nas in the inverse Gaussian Hull-White short rate model, also allowing one to\nassess the related model risk issue.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 08:00:13 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Cr\u00e9pey", "St\u00e9phane", ""], ["Gerboud", "R\u00e9mi", ""], ["Grbac", "Zorana", ""], ["Ngor", "Nathalie", ""]]}, {"id": "1210.5987", "submitter": "Fabio Caccioli", "authors": "Fabio Caccioli, Munik Shrestha, Cristopher Moore, J. Doyne Farmer", "title": "Stability analysis of financial contagion due to overlapping portfolios", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.SI physics.soc-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common asset holdings are widely believed to have been the primary vector of\ncontagion in the recent financial crisis. We develop a network approach to the\namplification of financial contagion due to the combination of overlapping\nportfolios and leverage, and we show how it can be understood in terms of a\ngeneralized branching process. By studying a stylized model we estimate the\ncircumstances under which systemic instabilities are likely to occur as a\nfunction of parameters such as leverage, market crowding, diversification, and\nmarket impact. Although diversification may be good for individual\ninstitutions, it can create dangerous systemic effects, and as a result\nfinancial contagion gets worse with too much diversification. Under our model\nthere is a critical threshold for leverage; below it financial networks are\nalways stable, and above it the unstable region grows as leverage increases.\nThe financial system exhibits \"robust yet fragile\" behavior, with regions of\nthe parameter space where contagion is rare but catastrophic whenever it\noccurs. Our model and methods of analysis can be calibrated to real data and\nprovide simple yet powerful tools for macroprudential stress testing.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 18:13:27 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Caccioli", "Fabio", ""], ["Shrestha", "Munik", ""], ["Moore", "Cristopher", ""], ["Farmer", "J. Doyne", ""]]}, {"id": "1210.6000", "submitter": "Julien Vedani", "authors": "Julien Vedani (SAF), Laurent Devineau (SAF)", "title": "Solvency assessment within the ORSA framework: issues and quantitative\n  methodologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of the Own Risk and Solvency Assessment is a critical\nissue raised by Pillar II of Solvency II framework. In particular the Overall\nSolvency Needs calculation left the Insurance companies to define an optimal\nentity-specific solvency constraint on a multi-year time horizon. In a life\ninsurance society framework, the intuitive approaches to answer this problem\ncan sometimes lead to new implementation issues linked to the highly stochastic\nnature of the methodologies used to project a company Net Asset Value over\nseveral years. One alternative approach can be the use of polynomial proxies to\nreplicate the outcomes of this variable throughout the time horizon. Polynomial\nfunctions are already considered as efficient replication methodologies for the\nNet Asset Value over 1 year. The Curve Fitting and Least Squares Monte-Carlo\nprocedures are the best-known examples of such procedures. In this article we\nintroduce a possibility of adaptation for these methodologies to be used on a\nmulti-year time horizon, in order to assess the Overall Solvency Needs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 19:01:06 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2012 10:44:34 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Vedani", "Julien", "", "SAF"], ["Devineau", "Laurent", "", "SAF"]]}, {"id": "1210.7257", "submitter": "Alois Pichler", "authors": "Alois Pichler, Alexander Shapiro", "title": "Uniqueness of Kusuoka Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses law invariant coherent risk measures and their Kusuoka\nrepresentations. By elaborating the existence of a minimal representation we\nshow that every Kusuoka representation can be reduced to its minimal\nrepresentation. Uniqueness -- in a sense specified in the paper -- of the risk\nmeasure's Kusuoka representation is derived from this initial result.\n  Further, stochastic order relations are employed to identify the minimal\nKusuoka representation. It is shown that measures in the minimal representation\nare extremal with respect to the order relations. The tools are finally\nemployed to provide the minimal representation for important practical\nexamples. Although the Kusuoka representation is usually given only for\nnonatomic probability spaces, this presentation closes the gap to spaces with\natoms.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 21:34:36 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 13:22:31 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2013 07:51:36 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2013 09:02:23 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Pichler", "Alois", ""], ["Shapiro", "Alexander", ""]]}, {"id": "1210.7329", "submitter": "Marco Bianchetti", "authors": "Marco Bianchetti", "title": "The Zeeman Effect in Finance: Libor Spectroscopy and Basis Risk\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR physics.pop-ph q-fin.RM quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Once upon a time there was a classical financial world in which all the\nLibors were equal. Standard textbooks taught that simple relations held, such\nthat, for example, a 6 months Libor Deposit was replicable with a 3 months\nLibor Deposits plus a 3x6 months Forward Rate Agreement (FRA), and that Libor\nwas a good proxy of the risk free rate required as basic building block of\nno-arbitrage pricing theory. Nowadays, in the modern financial world after the\ncredit crunch, some Libors are more equal than others, depending on their rate\ntenor, and classical formulas are history. Banks are not anymore too \"big to\nfail\", Libors are fixed by panels of risky banks, and they are risky rates\nthemselves. These simple empirical facts carry very important consequences in\nderivative's trading and risk management, such as, for example, basis risk,\ncollateralization and regulatory pressure in favour of Central Counterparties.\nSomething that should be carefully considered by anyone managing even a single\nplain vanilla Swap. In this qualitative note we review the problem trying to\nshed some light on this modern animal farm, recurring to an analogy with\nquantum physics, the Zeeman effect.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2012 14:45:43 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Bianchetti", "Marco", ""]]}]