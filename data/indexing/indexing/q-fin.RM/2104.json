[{"id": "2104.00248", "submitter": "Hamed Amini", "authors": "Hamed Amini, Zhongyuan Cao and Agnes Sulem", "title": "Limit Theorems for Default Contagion and Systemic Risk", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.TH math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general tractable model for default contagion and systemic risk\nin a heterogeneous financial network, subject to an exogenous macroeconomic\nshock. We show that, under some regularity assumptions, the default cascade\nmodel could be transferred to a death process problem represented by\nballs-and-bins model. We also reduce the dimension of the problem by\nclassifying banks according to different types, in an appropriate type space.\nThese types may be calibrated to real-world data by using machine learning\ntechniques. We then state various limit theorems regarding the final size of\ndefault cascade over different types. In particular, under suitable assumptions\non the degree and threshold distributions, we show that the final size of\ndefault cascade has asymptotically Gaussian fluctuations. We next state limit\ntheorems for different system-wide wealth aggregation functions and show how\nthe systemic risk measure, in a given stress test scenario, could be related to\nthe structure and heterogeneity of financial networks. We finally show how\nthese results could be used by a social planner to optimally target\ninterventions during a financial crisis, with a budget constraint and under\npartial information of the financial network.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 04:18:44 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Amini", "Hamed", ""], ["Cao", "Zhongyuan", ""], ["Sulem", "Agnes", ""]]}, {"id": "2104.01571", "submitter": "Viktor Stojkoski PhD", "authors": "Viktor Stojkoski, Trifce Sandev, Ljupco Kocarev and Arnab Pal", "title": "Geometric Brownian Motion under Stochastic Resetting: A Stationary yet\n  Non-ergodic Process", "comments": null, "journal-ref": "Phys. Rev. E 104, 014121 (2021)", "doi": "10.1103/PhysRevE.104.014121", "report-no": null, "categories": "q-fin.RM cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effects of stochastic resetting on geometric Brownian motion\n(GBM), a canonical stochastic multiplicative process for non-stationary and\nnon-ergodic dynamics. Resetting is a sudden interruption of a process, which\nconsecutively renews its dynamics. We show that, although resetting renders GBM\nstationary, the resulting process remains non-ergodic. Quite surprisingly, the\neffect of resetting is pivotal in manifesting the non-ergodic behavior. In\nparticular, we observe three different long-time regimes: a quenched state, an\nunstable and a stable annealed state depending on the resetting strength.\nNotably, in the last regime, the system is self-averaging and thus the sample\naverage will always mimic ergodic behavior establishing a stand alone feature\nfor GBM under resetting. Crucially, the above-mentioned regimes are well\nseparated by a self-averaging time period which can be minimized by an optimal\nresetting rate. Our results can be useful to interpret data emanating from\nstock market collapse or reconstitution of investment portfolios.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 09:30:06 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 12:26:44 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Stojkoski", "Viktor", ""], ["Sandev", "Trifce", ""], ["Kocarev", "Ljupco", ""], ["Pal", "Arnab", ""]]}, {"id": "2104.03545", "submitter": "Kevin Kuo", "authors": "Kevin Kuo and Ronald Richman", "title": "Embeddings and Attention in Predictive Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore in depth how categorical data can be processed with embeddings in\nthe context of claim severity modeling. We develop several models that range in\ncomplexity from simple neural networks to state-of-the-art attention based\narchitectures that utilize embeddings. We illustrate the utility of learned\nembeddings from neural networks as pretrained features in generalized linear\nmodels, and discuss methods for visualizing and interpreting embeddings.\nFinally, we explore how attention based models can contextually augment\nembeddings, leading to enhanced predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:54:05 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kuo", "Kevin", ""], ["Richman", "Ronald", ""]]}, {"id": "2104.04918", "submitter": "Chao Wang Dr", "authors": "Giuseppe Storti, Chao Wang", "title": "Modelling uncertainty in financial tail risk: a forecast combination and\n  weighted quantile approach", "comments": "32 pages, 3 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:2005.04868", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel forecast combination and weighted quantile based tail-risk\nforecasting framework is proposed, aiming to reduce the impact of modelling\nuncertainty in tail-risk forecasting. The proposed approach is based on a\ntwo-step estimation procedure. The first step involves the combination of\nValue-at-Risk (VaR) forecasts at a grid of quantile levels. A range of\nparametric and semi-parametric models is selected as the model universe in the\nforecast combination procedure. The quantile forecast combination weights are\nestimated by optimizing the quantile loss. In the second step, the Expected\nShortfall (ES) is computed as a weighted average of combined quantiles. The\nquantiles weighting structure for ES forecasting is determined by minimizing a\nstrictly consistent joint VaR and ES loss function of the Fissler-Ziegel class.\nThe proposed framework is applied to six stock market indices and its\nforecasting performance is compared to each individual model in the universe, a\nsimple average approach and a weighted quantile approach. The forecasting\nresults support the proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 05:01:20 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 06:16:10 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Storti", "Giuseppe", ""], ["Wang", "Chao", ""]]}, {"id": "2104.04960", "submitter": "Sandro Vaienti", "authors": "Fabrizio Lillo, Giulia Livieri, Stefano Marmi, Anton Solomko, Sandro\n  Vaienti", "title": "Analysis of bank leverage via dynamical systems and deep neural networks", "comments": "51 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF math.DS q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a model of a simple financial system consisting of a leveraged\ninvestor that invests in a risky asset and manages risk by using Value-at-Risk\n(VaR). The VaR is estimated by using past data via an adaptive expectation\nscheme. We show that the leverage dynamics can be described by a dynamical\nsystem of slow-fast type associated with a unimodal map on [0,1] with an\nadditive heteroscedastic noise whose variance is related to the portfolio\nrebalancing frequency to target leverage. In absence of noise the model is\npurely deterministic and the parameter space splits in two regions: (i) a\nregion with a globally attracting fixed point or a 2-cycle; (ii) a dynamical\ncore region, where the map could exhibit chaotic behavior. Whenever the model\nis randomly perturbed, we prove the existence of a unique stationary density\nwith bounded variation, the stochastic stability of the process and the almost\ncertain existence and continuity of the Lyapunov exponent for the stationary\nmeasure. We then use deep neural networks to estimate map parameters from a\nshort time series. Using this method, we estimate the model in a large dataset\nof US commercial banks over the period 2001-2014. We find that the parameters\nof a substantial fraction of banks lie in the dynamical core, and their\nleverage time series are consistent with a chaotic behavior. We also present\nevidence that the time series of the leverage of large banks tend to exhibit\nchaoticity more frequently than those of small banks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 08:48:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lillo", "Fabrizio", ""], ["Livieri", "Giulia", ""], ["Marmi", "Stefano", ""], ["Solomko", "Anton", ""], ["Vaienti", "Sandro", ""]]}, {"id": "2104.05831", "submitter": "Gabriel Suarez", "authors": "Gabriel Suarez, Juan Raful, Maria A. Luque, Carlos F. Valencia,\n  Alejandro Correa-Bahnsen", "title": "Enhancing User' s Income Estimation with Super-App Alternative Data", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents the advantages of alternative data from Super-Apps to\nenhance user' s income estimation models. It compares the performance of these\nalternative data sources with the performance of industry-accepted bureau\nincome estimators that takes into account only financial system information;\nsuccessfully showing that the alternative data manage to capture information\nthat bureau income estimators do not. By implementing the TreeSHAP method for\nStochastic Gradient Boosting Interpretation, this paper highlights which of the\ncustomer' s behavioral and transactional patterns within a Super-App have a\nstronger predictive power when estimating user' s income. Ultimately, this\npaper shows the incentive for financial institutions to seek to incorporate\nalternative data into constructing their risk profiles.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:34:44 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Suarez", "Gabriel", ""], ["Raful", "Juan", ""], ["Luque", "Maria A.", ""], ["Valencia", "Carlos F.", ""], ["Correa-Bahnsen", "Alejandro", ""]]}, {"id": "2104.06735", "submitter": "Marcin Chlebus", "authors": "Przemys{\\l}aw Biecek, Marcin Chlebus, Janusz Gajda, Alicja Gosiewska,\n  Anna Kozak, Dominik Ogonowski, Jakub Sztachelski, Piotr Wojewnik", "title": "Enabling Machine Learning Algorithms for Credit Scoring -- Explainable\n  Artificial Intelligence (XAI) methods for clear understanding complex\n  predictive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid development of advanced modelling techniques gives an opportunity to\ndevelop tools that are more and more accurate. However as usually, everything\ncomes with a price and in this case, the price to pay is to loose\ninterpretability of a model while gaining on its accuracy and precision. For\nmanagers to control and effectively manage credit risk and for regulators to be\nconvinced with model quality the price to pay is too high. In this paper, we\nshow how to take credit scoring analytics in to the next level, namely we\npresent comparison of various predictive models (logistic regression, logistic\nregression with weight of evidence transformations and modern artificial\nintelligence algorithms) and show that advanced tree based models give best\nresults in prediction of client default. What is even more important and\nvaluable we also show how to boost advanced models using techniques which allow\nto interpret them and made them more accessible for credit risk practitioners,\nresolving the crucial obstacle in widespread deployment of more complex, 'black\nbox' models like random forests, gradient boosted or extreme gradient boosted\ntrees. All this will be shown on the large dataset obtained from the Polish\nCredit Bureau to which all the banks and most of the lending companies in the\ncountry do report the credit files. In this paper the data from lending\ncompanies were used. The paper then compares state of the art best practices in\ncredit risk modelling with new advanced modern statistical tools boosted by the\nlatest developments in the field of interpretability and explainability of\nartificial intelligence algorithms. We believe that this is a valuable\ncontribution when it comes to presentation of different modelling tools but\nwhat is even more important it is showing which methods might be used to get\ninsight and understanding of AI methods in credit risk context.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:44:04 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Biecek", "Przemys\u0142aw", ""], ["Chlebus", "Marcin", ""], ["Gajda", "Janusz", ""], ["Gosiewska", "Alicja", ""], ["Kozak", "Anna", ""], ["Ogonowski", "Dominik", ""], ["Sztachelski", "Jakub", ""], ["Wojewnik", "Piotr", ""]]}, {"id": "2104.07718", "submitter": "Yuyu Chen", "authors": "Yuyu Chen, Liyuan Lin, Ruodu Wang", "title": "Ordered Risk Aggregation under Dependence Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the aggregation of two risks when the marginal distributions are\nknown and the dependence structure is unknown, under the additional constraint\nthat one risk is no larger than the other. Risk aggregation problems with the\norder constraint are closely related to the recently introduced notion of the\ndirectional lower (DL) coupling. The largest aggregate risk in concave order\n(thus, the smallest aggregate risk in convex order) is attained by the DL\ncoupling. These results are further generalized to calculate the best-case and\nworst-case values of tail risk measures. In particular, we obtain analytical\nformulas for bounds on Value-at-Risk. Our numerical results suggest that the\nnew bounds on risk measures with the extra order constraint can greatly improve\nthose with full dependence uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:58:20 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chen", "Yuyu", ""], ["Lin", "Liyuan", ""], ["Wang", "Ruodu", ""]]}, {"id": "2104.07976", "submitter": "Jan Rosenzweig", "authors": "Jan Rosenzweig", "title": "Power-law Portfolios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Portfolio optimization methods suffer from a catalogue of known problems,\nmainly due to the facts that pair correlations of asset returns are unstable,\nand that extremal risk measures such as maximum drawdown are difficult to\npredict due to the non-Gaussianity of portfolio returns. \\\\ In order to look at\noptimal portfolios for arbitrary risk penalty functions, we construct portfolio\nshapes where the penalty is proportional to a moment of the returns of\narbitrary order $p>2$. \\\\ The resulting component weight in the portfolio\nscales sub-linearly with its return, with the power-law $w \\propto\n\\mu^{1/(p-1)}$. This leads to significantly improved diversification when\ncompared to Kelly portfolios, due to the dilution of the winner-takes-all\neffect.\\\\ In the limit of penalty order $p\\rightarrow\\infty$, we recover the\nsimple trading heuristic whereby assets are allocated a fixed positive weight\nwhen their return exceeds the hurdle rate, and zero otherwise. Infinite order\npower-law portfolios thus fall into the class of perfectly diversified\nportfolios.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:00:32 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 10:40:50 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Rosenzweig", "Jan", ""]]}, {"id": "2104.08605", "submitter": "Sangita Das", "authors": "Sangita Das and Suchandan Kayal", "title": "Ordering results between the largest claims arising from two general\n  heterogeneous portfolios", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is entirely devoted to compare the largest claims from two\nheterogeneous portfolios. It is assumed that the claim amounts in an insurance\nportfolio are nonnegative absolutely continuous random variables and belong to\na general family of distributions. The largest claims have been compared based\non various stochastic orderings. The established sufficient conditions are\nassociated with the matrices and vectors of model parameters. Applications of\nthe results are provided for the purpose of illustration.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:12:08 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Das", "Sangita", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2104.09879", "submitter": "Hibiki Kaibuchi", "authors": "Hibiki Kaibuchi, Yoshinori Kawasaki and Gilles Stupfler", "title": "GARCH-UGH: A bias-reduced approach for dynamic extreme Value-at-Risk\n  estimation in financial time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Value-at-Risk (VaR) is a widely used instrument in financial risk\nmanagement. The question of estimating the VaR of loss return distributions at\nextreme levels is an important question in financial applications, both from\noperational and regulatory perspectives; in particular, the dynamic estimation\nof extreme VaR given the recent past has received substantial attention. We\npropose here a two-step bias-reduced estimation methodology called GARCH-UGH\n(Unbiased Gomes-de Haan), whereby financial returns are first filtered using an\nAR-GARCH model, and then a bias-reduced estimator of extreme quantiles is\napplied to the standardized residuals to estimate one-step ahead dynamic\nextreme VaR. Our results indicate that the GARCH-UGH estimates are more\naccurate than those obtained by combining conventional AR-GARCH filtering and\nextreme value estimates from the perspective of in-sample and out-of-sample\nbacktestings of historical daily returns on several financial time series.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:19:08 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kaibuchi", "Hibiki", ""], ["Kawasaki", "Yoshinori", ""], ["Stupfler", "Gilles", ""]]}, {"id": "2104.10673", "submitter": "Tobias Fissler", "authors": "Tobias Fissler and Yannick Hoga", "title": "Backtesting Systemic Risk Forecasts using Multi-Objective Elicitability", "comments": "61 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Backtesting risk measure forecasts requires identifiability (for model\nvalidation) and elicitability (for model comparison). The systemic risk\nmeasures CoVaR (conditional value-at-risk), CoES (conditional expected\nshortfall) and MES (marginal expected shortfall), measuring the risk of a\nposition $Y$ given that a reference position $X$ is in distress, fail to be\nidentifiable and elicitable. We establish the joint identifiability of CoVaR,\nMES and (CoVaR, CoES) together with the value-at-risk (VaR) of the reference\nposition $X$, but show that an analogue result for elicitability fails. The\nnovel notion of multi-objective elicitability however, relying on multivariate\nscores equipped with an order, leads to a positive result when using the\nlexicographic order on $\\mathbb{R}^2$. We establish comparative backtests of\nDiebold--Mariano type for superior systemic risk forecasts and comparable VaR\nforecasts, accompanied by a traffic-light approach. We demonstrate the\nviability of these backtesting approaches in simulations and in an empirical\napplication to DAX 30 and S&P 500 returns.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:02:06 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 12:12:20 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 11:44:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fissler", "Tobias", ""], ["Hoga", "Yannick", ""]]}, {"id": "2104.11768", "submitter": "Bernhard Hientzsch", "authors": "Narayan Ganesan, Bernhard Hientzsch", "title": "Estimating Future VaR from Value Samples and Applications to Future\n  Initial Margin", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future values at risk (fVaR) is an important problem in finance.\nThey arise in the modelling of future initial margin requirements for\ncounterparty credit risk and future market risk VaR. One is also interested in\nderived quantities such as: i) Dynamic Initial Margin (DIM) and Margin Value\nAdjustment (MVA) in the counterparty risk context; and ii) risk weighted assets\n(RWA) and Capital Value Adjustment (KVA) for market risk. This paper describes\nseveral methods that can be used to predict fVaRs. We begin with the Nested\nMC-empirical quantile method as benchmark, but it is too computationally\nintensive for routine use. We review several known methods and discuss their\nnovel applications to the problem at hand.\n  The techniques considered include computing percentiles from distributions\n(Normal and Johnson) that were matched to parametric moments or percentile\nestimates, quantile regressions methods, and others with more specific\nassumptions or requirements.\n  We also consider how limited inner simulations can be used to improve the\nperformance of these techniques. The paper also provides illustrations,\nresults, and visualizations of intermediate and final results for the various\napproaches and methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:13:39 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ganesan", "Narayan", ""], ["Hientzsch", "Bernhard", ""]]}, {"id": "2104.11863", "submitter": "Zhibin Niu", "authors": "Zhibin Niu, Junqi Wu, Dawei Cheng, and Jiawan Zhang", "title": "Regshock: Interactive Visual Analytics of Systemic Risk in Financial\n  Networks", "comments": "9 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial regulatory agencies are struggling to manage the systemic risks\nattributed to negative economic shocks. Preventive interventions are prominent\nto eliminate the risks and help to build a more resilient financial system.\nAlthough tremendous efforts have been made to measure multi-risk severity\nlevels, understand the contagion behaviors and other risk management problems,\nthere still lacks a theoretical framework revealing what and how regulatory\nintervention measurements can mitigate systemic risk. Here we demonstrate\nregshock, a practical visual analytical approach to support the exploration and\nevaluation of financial regulation measurements. We propose risk-island, an\nunprecedented risk-centered visualization algorithm to help uncover the risk\npatterns while preserving the topology of financial networks. We further\npropose regshock, a novel visual exploration and assessment approach based on\nthe simulation-intervention-evaluation analysis loop, to provide a heuristic\nsurgical intervention capability for systemic risk mitigation. We evaluate our\napproach through extensive case studies and expert reviews. To our knowledge,\nthis is the first practical systemic method for the financial network\nintervention and risk mitigation problem; our validated approach potentially\nimproves the risk management and control capabilities of financial experts.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 02:53:02 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Niu", "Zhibin", ""], ["Wu", "Junqi", ""], ["Cheng", "Dawei", ""], ["Zhang", "Jiawan", ""]]}]