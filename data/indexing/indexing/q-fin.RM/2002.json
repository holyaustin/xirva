[{"id": "2002.01798", "submitter": "Zhengxiao Li", "authors": "Liang Yang, Zhengxiao Li, Shengwang Meng", "title": "Risk Loadings in Classification Ratemaking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk premium of a policy is the sum of the pure premium and the risk\nloading. In the classification ratemaking process, generalized linear models\nare usually used to calculate pure premiums, and various premium principles are\napplied to derive the risk loadings. No matter which premium principle is used,\nsome risk loading parameters should be given in advance subjectively. To\novercome this subjective problem and calculate the risk premium more reasonably\nand objectively, we propose a top-down method to calculate these risk loading\nparameters. First, we implement the bootstrap method to calculate the total\nrisk premium of the portfolio. Then, under the constraint that the portfolio's\ntotal risk premium should equal the sum of the risk premiums of each policy,\nthe risk loading parameters are determined. During this process, besides using\ngeneralized linear models, three kinds of quantile regression models are also\napplied, namely, traditional quantile regression model, fully parametric\nquantile regression model, and quantile regression model with coefficient\nfunctions. The empirical result shows that the risk premiums calculated by the\nmethod proposed in this study can reasonably differentiate the heterogeneity of\ndifferent risk classes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 14:08:32 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Yang", "Liang", ""], ["Li", "Zhengxiao", ""], ["Meng", "Shengwang", ""]]}, {"id": "2002.02229", "submitter": "Fangyuan Zhang", "authors": "An Chen, Mitja Stadje, Fangyuan Zhang", "title": "On the equivalence between Value-at-Risk and Expected Shortfall in\n  non-concave optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies an optimal asset allocation problem for a surplus-driven\nfinancial institution facing a Value-at-Risk (VaR) or an Expected Shortfall\n(ES) constraint corresponding to a non-concave optimization problem under\nconstraints. We obtain the closed-form optimal wealth with the ES constraint as\nwell as with the VaR constraint respectively, and explicitly calculate the\noptimal trading strategy for constant relative risk aversion (CRRA) utility\nfunctions. We find that both VaR and ES-based regulation can effectively reduce\nthe probability of default for a surplus-driven financial institution. However,\nthe liability holders' benefits cannot be fully protected under either VaR- or\nES-based regulation. In addition, we show that the VaR and ES-based regulation\ncan induce the same optimal portfolio choice for a surplus-driven financial\ninstitution. This differs from the conclusion drawn in Basak and Shapiro 2001\nwhere the financial institution aims at maximizing the expected utility of the\ntotal assets, and ES provides better loss protection.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 12:51:22 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 12:38:13 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 12:04:22 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Chen", "An", ""], ["Stadje", "Mitja", ""], ["Zhang", "Fangyuan", ""]]}, {"id": "2002.02675", "submitter": "Thomas Lim", "authors": "Idris Kharroubi (LPSM UMR 8001), Thomas Lim (LaMME, ENSIIE), Xavier\n  Warin (EDF)", "title": "Discretization and Machine Learning Approximation of BSDEs with a\n  Constraint on the Gains-Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximation of backward stochastic differential equations\n(BSDEs for short) with a constraint on the gains process. We first discretize\nthe constraint by applying a so-called facelift operator at times of a grid. We\nshow that this discretely constrained BSDE converges to the continuously\nconstrained one as the mesh grid converges to zero. We then focus on the\napproximation of the discretely constrained BSDE. For that we adopt a machine\nlearning approach. We show that the facelift can be approximated by an\noptimization problem over a class of neural networks under constraints on the\nneural network and its derivative. We then derive an algorithm converging to\nthe discretely constrained BSDE as the number of neurons goes to infinity. We\nend by numerical experiments. Mathematics Subject Classification (2010): 65C30,\n65M75, 60H35, 93E20, 49L25.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 09:11:29 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Kharroubi", "Idris", "", "LPSM UMR 8001"], ["Lim", "Thomas", "", "LaMME, ENSIIE"], ["Warin", "Xavier", "", "EDF"]]}, {"id": "2002.03235", "submitter": "Marco  Vega", "authors": "Roc\\'io Paredes and Marco Vega", "title": "An internal fraud model for operational losses in retail banking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a dynamic internal fraud model for operational losses in\nretail banking. It considers public operational losses arising from internal\nfraud in retail banking within a group of international banks. Additionally,\nthe model takes into account internal factors such as the ethical quality of\nworkers and the risk controls set by bank managers. The model is validated by\nmeasuring the impact of macroeconomic indicators such as GDP growth and the\ncorruption perception upon the severity and frequency of losses implied by the\nmodel. In general,results show that internal fraud losses are pro-cyclical, and\nthat country specific corruption perceptions positively affects internal fraud\nlosses. Namely, when a country is perceived to be more corrupt, retail banking\nin that country will feature more severe internal fraud losses.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 21:39:22 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Paredes", "Roc\u00edo", ""], ["Vega", "Marco", ""]]}, {"id": "2002.03319", "submitter": "Diego Garlaschelli", "authors": "Marc van Kralingen, Diego Garlaschelli, Karolina Scholtus, Iman van\n  Lelyveld", "title": "Crowded trades, market clustering, and price instability", "comments": null, "journal-ref": "Entropy 23(3), 336 (2021)", "doi": "10.3390/e23030336", "report-no": "DNB Working Papers 668, Netherlands Central Bank, Research\n  Department", "categories": "q-fin.ST q-fin.RM q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowded trades by similarly trading peers influence the dynamics of asset\nprices, possibly creating systemic risk. We propose a market clustering measure\nusing granular trading data. For each stock the clustering measure captures the\ndegree of trading overlap among any two investors in that stock. We investigate\nthe effect of crowded trades on stock price stability and show that market\nclustering has a causal effect on the properties of the tails of the stock\nreturn distribution, particularly the positive tail, even after controlling for\ncommonly considered risk drivers. Reduced investor pool diversity could thus\nnegatively affect stock price stability.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 08:53:43 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["van Kralingen", "Marc", ""], ["Garlaschelli", "Diego", ""], ["Scholtus", "Karolina", ""], ["van Lelyveld", "Iman", ""]]}, {"id": "2002.04164", "submitter": "Giuseppe Brandi", "authors": "Giuseppe Brandi and T. Di Matteo", "title": "On the statistics of scaling exponents and the Multiscaling Value at\n  Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling and multiscaling financial time series have been widely studied in\nthe literature. The research on this topic is vast and still flourishing. One\nway to analyze the scaling properties of time series is through the estimation\nof their scaling exponents, that are recognized as being valuable measures to\ndiscriminate between random, persistent, and anti-persistent behaviors in these\ntime series. In the literature, several methods have been proposed to study the\nmultiscaling property. In this paper, we use the generalized Hurst exponent\n(GHE) tool and we propose a novel statistical procedure based on GHE which we\nname Relative Normalized and Standardized Generalized Hurst Exponent (RNSGHE).\nThis method is used to robustly estimate and test the multiscaling property\nand, together with a combination of t-tests and F-tests, serves to discriminate\nbetween real and spurious scaling. Furthermore, we introduce a new tool to\nestimate the optimal aggregation time used in our methodology which we name\nAutocororrelation Segmented Regression. We numerically validate this procedure\non simulated time series by using the Multifractal Random Walk (MRW) and we\nthen apply it to real financial data. We present results for times series with\nand without anomalies and we compute the bias that such anomalies introduce in\nthe measurement of the scaling exponents. We also show how the use of proper\nscaling and multiscaling can ameliorate the estimation of risk measures such as\nValue at Risk (VaR). Finally, we propose a methodology based on Monte Carlo\nsimulation, which we name Multiscaling Value at Risk (MSVaR), that takes into\naccount the statistical properties of multiscaling time series. We show that by\nusing this statistical procedure in combination with the robustly estimated\nmultiscaling exponents, the one year forecasted MSVaR mimics the VaR on the\nannual data for the majority of the stocks analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 01:50:42 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:49:11 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 19:59:07 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Brandi", "Giuseppe", ""], ["Di Matteo", "T.", ""]]}, {"id": "2002.04563", "submitter": "Lucia Cipolina-Kun", "authors": "Lucia Cipolina Kun, Simone Caenazzo, Ksenia Ponomareva", "title": "Mathematical Foundations of Regression Methods for the approximation of\n  the Forward Initial Margin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Abundant literature has been published on approximation methods for the\nforward initial margin. The most popular ones being the family of regression\nmethods. This paper describes the mathematical foundations on which these\nregression approximation methods lie. We introduce mathematical rigor to show\nthat in essence, all the methods propose variations of approximations for the\nconditional expectation function, which is interpreted as an orthogonal\nprojection on Hilbert spaces. We show that each method is simply choosing a\ndifferent functional form to numerically estimate the conditional expectation.\nWe cover in particular the most popular methods in the literature so far,\nPolynomial approximation, Kernel regressions and Neural Networks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 17:38:30 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 02:49:56 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Kun", "Lucia Cipolina", ""], ["Caenazzo", "Simone", ""], ["Ponomareva", "Ksenia", ""]]}, {"id": "2002.04675", "submitter": "Ludovic Mathys", "authors": "Walter Farkas, Ludovic Mathys, Nikola Vasiljevi\\'c", "title": "Intra-Horizon Expected Shortfall and Risk Structure in Models with Jumps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present article deals with intra-horizon risk in models with jumps. Our\ngeneral understanding of intra-horizon risk is along the lines of the approach\ntaken in Boudoukh, Richardson, Stanton and Whitelaw (2004), Rossello (2008),\nBhattacharyya, Misra and Kodase (2009), Bakshi and Panayotov (2010), and\nLeippold and Vasiljevi\\'c (2019). In particular, we believe that quantifying\nmarket risk by strictly relying on point-in-time measures cannot be deemed a\nsatisfactory approach in general. Instead, we argue that complementing this\napproach by studying measures of risk that capture the magnitude of losses\npotentially incurred at any time of a trading horizon is necessary when dealing\nwith (m)any financial position(s). To address this issue, we propose an\nintra-horizon analogue of the expected shortfall for general profit and loss\nprocesses and discuss its key properties. Our intra-horizon expected shortfall\nis well-defined for (m)any popular class(es) of L\\'evy processes encountered\nwhen modeling market dynamics and constitutes a coherent measure of risk, as\nintroduced in Cheridito, Delbaen and Kupper (2004). On the computational side,\nwe provide a simple method to derive the intra-horizon risk inherent to popular\nL\\'evy dynamics. Our general technique relies on results for\nmaturity-randomized first-passage probabilities and allows for a derivation of\ndiffusion and single jump risk contributions. These theoretical results are\ncomplemented with an empirical analysis, where popular L\\'evy dynamics are\ncalibrated to S&P 500 index data and an analysis of the resulting intra-horizon\nrisk is presented.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 20:50:44 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 13:34:39 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Farkas", "Walter", ""], ["Mathys", "Ludovic", ""], ["Vasiljevi\u0107", "Nikola", ""]]}, {"id": "2002.05232", "submitter": "Yongjie Wang", "authors": "Ankush Agarwal, Christian-Oliver Ewald and Yongjie Wang", "title": "Sharing of longevity basis risk in pension schemes with income-drawdown\n  guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies a stochastic optimal control problem for a pension scheme\nwhich provides an income-drawdown policy to its members after their retirement.\nTo manage the scheme efficiently, the manager and members agree to share the\ninvestment risk based on a pre-decided risk-sharing rule. The objective is to\nmaximise both sides' utilities by controlling the manager's investment in risky\nassets and members' benefit withdrawals. We use stochastic affine class models\nto describe the force of mortality of the members' population and consider a\nlongevity bond whose coupon payment is linked to a survival index. In our\nframework, we also investigate the longevity basis risk, which arises when the\nmembers' and the longevity bond's reference populations show different\nmortality behaviours. By applying the dynamic programming principle to solve\nthe corresponding HJB equations, we derive optimal solutions for the single-\nand sub-population cases. Our numerical results show that by sharing the risk,\nboth manager and members increase their utility. Moreover, even in the presence\nof longevity basis risk, we demonstrate that the longevity bond acts as an\neffective hedging instrument.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 20:49:50 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Agarwal", "Ankush", ""], ["Ewald", "Christian-Oliver", ""], ["Wang", "Yongjie", ""]]}, {"id": "2002.07389", "submitter": "Janusz Milek", "authors": "Janusz Milek", "title": "Quantum Implementation of Risk Analysis-relevant Copulas", "comments": "15 pages, 32+10 figures, 4+1 tables. Changes in v.2: updated\n  references p. 2 and 14-15, typo correction p. 6, quantum circuit diagrams\n  improved for gray print", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.CP q-fin.RM quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern quantitative risk management relies on an adequate modeling of the\ntail dependence and a possibly accurate quantification of risk measures, like\nValue at Risk (VaR), at high confidence levels like 1 in 100 or even 1 in 2000.\nQuantum computing makes such a quantification quadratically more efficient than\nthe Monte Carlo method; see (Woerner and Egger, 2018) and, for a broader\nperspective, (Or\\'us et al., 2018). An important element of the risk analysis\ntoolbox is copula, see (Jouanin et al., 2004) regarding financial applications.\nHowever, to the best knowledge of the author, no quantum computing\nimplementation for sampling from a risk modeling-relevant copula in explicit\nform has been published so far. Our focus here is implementation of simple yet\npowerful copula models, capable of a satisfactory capturing the joint tail\nbehaviour of the modelled risk factors. This paper deals with a few simple\ncopula families, including Multivariate B11 (MB11) copula family, presented in\n(Milek, 2014). We will show that this copula family is suitable for the risk\naggregation as it is exceptionally able to reproduce tail dependence\nstructures; see (Embrechts et al., 2016) for a relevant benchmark as well as\nnecessary and sufficient conditions regarding the ultimate feasible bivariate\ntail dependence structures. It turns out that such a discretized copula can be\nexpressed using simple constructs present in the quantum computing: binary\nfraction expansion format, comonotone/independent random variables, controlled\ngates, and convex combinations, and is therefore suitable for a quantum\ncomputer implementation. This paper presents design behind the quantum\nimplementation circuits, numerical and symbolic simulation results, and\nexperimental validation on IBM quantum computer. The paper proposes also a\ngeneric method for quantum implementation of any discretized copula.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 06:05:43 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 06:08:37 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Milek", "Janusz", ""]]}, {"id": "2002.07566", "submitter": "P\\'al Andr\\'as Papp", "authors": "P\\'al Andr\\'as Papp, Roger Wattenhofer", "title": "Network-Aware Strategies in Financial Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the incentives of banks in a financial network, where the network\nconsists of debt contracts and credit default swaps (CDSs) between banks. One\nof the most important questions in such a system is the problem of deciding\nwhich of the banks are in default, and how much of their liabilities these\nbanks can pay. We study the payoff and preferences of the banks in the\ndifferent solutions to this problem. We also introduce a more refined model\nwhich allows assigning priorities to payment obligations; this provides a more\nexpressive and realistic model of real-life financial systems, while it always\nensures the existence of a solution.\n  The main focus of the paper is an analysis of the actions that a single bank\ncan execute in a financial system in order to influence the outcome to its\nadvantage. We show that removing an incoming debt, or donating funds to another\nbank can result in a single new solution that is strictly more favorable to the\nacting bank. We also show that increasing the bank's external funds or\nmodifying the priorities of outgoing payments cannot introduce a more favorable\nnew solution into the system, but may allow the bank to remove some unfavorable\nsolutions, or to increase its recovery rate. Finally, we show how the actions\nof two banks in a simple financial system can result in classical game\ntheoretic situations like the prisoner's dilemma or the dollar auction,\ndemonstrating the wide expressive capability of the financial system model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 14:03:06 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Papp", "P\u00e1l Andr\u00e1s", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2002.08492", "submitter": "Alexandre Carbonneau", "authors": "Alexandre Carbonneau and Fr\\'ed\\'eric Godin", "title": "Equal Risk Pricing of Derivatives with Deep Hedging", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.MF q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a deep reinforcement learning approach to price and\nhedge financial derivatives. This approach extends the work of Guo and Zhu\n(2017) who recently introduced the equal risk pricing framework, where the\nprice of a contingent claim is determined by equating the optimally hedged\nresidual risk exposure associated respectively with the long and short\npositions in the derivative. Modifications to the latter scheme are considered\nto circumvent theoretical pitfalls associated with the original approach.\nDerivative prices obtained through this modified approach are shown to be\narbitrage-free. The current paper also presents a general and tractable\nimplementation for the equal risk pricing framework inspired by the deep\nhedging algorithm of Buehler et al. (2019). An $\\epsilon$-completeness measure\nallowing for the quantification of the residual hedging risk associated with a\nderivative is also proposed. The latter measure generalizes the one presented\nin Bertsimas et al. (2001) based on the quadratic penalty. Monte Carlo\nsimulations are performed under a large variety of market dynamics to\ndemonstrate the practicability of our approach, to perform benchmarking with\nrespect to traditional methods and to conduct sensitivity analyses.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 22:59:59 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 17:30:54 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Carbonneau", "Alexandre", ""], ["Godin", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2002.08531", "submitter": "Wujiang Lou", "authors": "Wujiang Lou", "title": "The Fair Basis: Funding and capital in the reduced form framework", "comments": "23 pages, 4 figures, 3 tables", "journal-ref": "Risk, June 2019", "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.MF q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A negative basis trade enters a long bond position and buys protection on the\nissuer of the bond through credit default swap (CDS), aiming at arbitrage\nprofit due to the bond-CDS basis. To classic reduced form model theorists, the\nexistence of the basis is an abnormality or merely liquidity noise. Such a\nview, however, fails to explain large basis trading losses incurred during the\nfinancial crisis. Employing a bond continuously hedged by CDS under a dynamic\nspread model with bond repo financing, we find that there is unhedged and\nunhedgeable residual jump to default risk that can't be diversified because of\ncredit correlation. An economic capital approach has to apply and a charge on\nthe use of capital follows. Together with the hedge funding cost, it allows us\nto better understand the basis's economics and to predict its fair level.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 02:05:35 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lou", "Wujiang", ""]]}, {"id": "2002.08532", "submitter": "Wujiang Lou", "authors": "Wujiang Lou", "title": "Derivatives Discounting Explained", "comments": "38 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.MF q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Derivative pricing is about cash flow discounting at the riskfree rate. This\nteaching has lost its meaning post the financial crisis, due to the addition of\nextra value adjustments (XVA), which also made derivatives pricing and\nvaluation a very difficult task for investors. This article recovers a properly\ndefined discount rate that corresponds to different collateral and margin\nschemes. A binomial tree model is developed, enabling end-users to price in\ncounterparty default and funding risk. Coherent XVAs, if needed, naturally\nresult from decomposing the discount rate, and can be computed on the same\ntree.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 02:11:19 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lou", "Wujiang", ""]]}, {"id": "2002.09097", "submitter": "Zhi-Qiang Jiang", "authors": "Ying-Ying Shen (ECUST), Zhi-Qiang Jiang (ECUST), Jun-Chao Ma (ECUST),\n  Gang-Jin Wang (HNU), Wei-Xing Zhou (ECUST)", "title": "Sector connectedness in the Chinese stock markets", "comments": "17 pages, 7 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering the risk transmitting path within economic sectors in China is\ncrucial for understanding the stability of the Chinese economic system,\nespecially under the current situation of the China-US trade conflicts. In this\npaper, we try to uncover the risk spreading channels by means of volatility\nspillovers within the Chinese sectors using stock market data. By applying the\ngeneralized variance decomposition framework based on the VAR model and the\nrolling window approach, a set of connectedness matrices is obtained to reveal\nthe overall and dynamic spillovers within sectors. It is found that 17 sectors\n(mechanical equipment, electrical equipment, utilities, and so on) are risk\ntransmitters and 11 sectors (national defence, bank, non-bank finance, and so\non) are risk takers during the whole period. During the periods with the\nextreme risk events (the global financial crisis, the Chinese interbank\nliquidity crisis, the Chinese stock market plunge, and the China-US trade war),\nwe observe that the connectedness measures significantly increase and the\nfinancial sectors play a buffer role in stabilizing the economic system. The\nrobust tests suggest that our results are not sensitive to the changes of model\nparameters. Our results not only uncover the spillover effects within the\nChinese sectors, but also highlight the deep understanding of the risk\ncontagion patterns in the Chinese stock markets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 02:32:13 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Shen", "Ying-Ying", "", "ECUST"], ["Jiang", "Zhi-Qiang", "", "ECUST"], ["Ma", "Jun-Chao", "", "ECUST"], ["Wang", "Gang-Jin", "", "HNU"], ["Zhou", "Wei-Xing", "", "ECUST"]]}, {"id": "2002.10135", "submitter": "Alexander McNeil", "authors": "Alexander J. McNeil", "title": "Modelling volatile time series with v-transforms and copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An approach to the modelling of volatile time series using a class of\nuniformity-preserving transforms for uniform random variables is proposed.\nV-transforms describe the relationship between quantiles of the stationary\ndistribution of the time series and quantiles of the distribution of a\npredictable volatility proxy variable. They can be represented as copulas and\npermit the formulation and estimation of models that combine arbitrary marginal\ndistributions with copula processes for the dynamics of the volatility proxy.\nThe idea is illustrated using a Gaussian ARMA copula process and the resulting\nmodel is shown to replicate many of the stylized facts of financial return\nseries and to facilitate the calculation of marginal and conditional\ncharacteristics of the model including quantile measures of risk. Estimation is\ncarried out by adapting the exact maximum likelihood approach to the estimation\nof ARMA processes and the model is shown to be competitive with standard GARCH\nin an empirical application to Bitcoin return data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 10:00:38 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 16:51:52 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 14:28:00 GMT"}, {"version": "v4", "created": "Tue, 24 Nov 2020 17:08:22 GMT"}, {"version": "v5", "created": "Tue, 12 Jan 2021 18:10:08 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["McNeil", "Alexander J.", ""]]}, {"id": "2002.11705", "submitter": "Aris Anagnostopoulos", "authors": "Tesi Aliaj and Aris Anagnostopoulos and Stefano Piersanti", "title": "Firms Default Prediction with Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academics and practitioners have studied over the years models for predicting\nfirms bankruptcy, using statistical and machine-learning approaches. An earlier\nsign that a company has financial difficulties and may eventually bankrupt is\ngoing in \\emph{default}, which, loosely speaking means that the company has\nbeen having difficulties in repaying its loans towards the banking system.\nFirms default status is not technically a failure but is very relevant for bank\nlending policies and often anticipates the failure of the company. Our study\nuses, for the first time according to our knowledge, a very large database of\ngranular credit data from the Italian Central Credit Register of Bank of Italy\nthat contain information on all Italian companies' past behavior towards the\nentire Italian banking system to predict their default using machine-learning\ntechniques. Furthermore, we combine these data with other information regarding\ncompanies' public balance sheet data. We find that ensemble techniques and\nrandom forest provide the best results, corroborating the findings of Barboza\net al. (Expert Syst. Appl., 2017).\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 10:09:35 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Aliaj", "Tesi", ""], ["Anagnostopoulos", "Aris", ""], ["Piersanti", "Stefano", ""]]}]