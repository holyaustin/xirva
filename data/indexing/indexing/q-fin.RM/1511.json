[{"id": "1511.00140", "submitter": "Jakob Kisiala", "authors": "Jakob Kisiala", "title": "Conditional Value-at-Risk: Theory and Applications", "comments": "62 pages (without bibliography and appendix), 27 figures,\n  Dissertation presented for the degree of MSc in Operational Research,\n  University of Edinburgh", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis presents the Conditional Value-at-Risk concept and combines an\nanalysis that covers its application as a risk measure and as a vector norm.\nFor both areas of application the theory is revised in detail and examples are\ngiven to show how to apply the concept in practice.\n  In the first part, CVaR as a risk measure is introduced and the analysis\ncovers the mathematical definition of CVaR and different methods to calculate\nit. Then, CVaR optimization is analysed in the context of portfolio selection\nand how to apply CVaR optimization for hedging a portfolio consisting of\noptions. The original contributions in this part are an alternative proof of\nAcerbi's Integral Formula in the continuous case and an explicit programme\nformulation for portfolio hedging.\n  The second part first analyses the Scaled and Non-Scaled CVaR norm as new\nfamily of norms in $\\mathbb{R}^n$ and compares this new norm family to the more\nwidely known $L_p$ norms. Then, model (or signal) recovery problems are\ndiscussed and it is described how appropriate norms can be used to recover a\nsignal with less observations than the dimension of the signal. The last\nchapter of this dissertation then shows how the Non-Scaled CVaR norm can be\nused in this model recovery context. The original contributions in this part\nare an alternative proof of the equivalence of two different characterizations\nof the Scaled CVaR norm, a new proposition that the Scaled CVaR norm is\npiecewise convex, and the entire \\autoref{chapter:Recovery_using_CVaR}. Since\nthe CVaR norm is a rather novel concept, its applications in a model recovery\ncontext have not been researched yet. Therefore, the final chapter of this\nthesis might lay the basis for further research in this area.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 15:31:06 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Kisiala", "Jakob", ""]]}, {"id": "1511.01763", "submitter": "Harri Nyrhinen Mr", "authors": "Harri Nyrhinen", "title": "On real growth and run-off companies in insurance ruin theory", "comments": "To appear in Advances in Applied Probability (September 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study solvency of insurers in a comprehensive model where various economic\nfactors affect the capital developments of the companies. The main interest is\nin the impact of real growth to ruin probabilities. The volume of the business\nis allowed to increase or decrease. In the latter case, the study is focused on\nrun-off companies. Our main results give sharp asymptotic estimates for\ninfinite time ruin probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 14:46:41 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Nyrhinen", "Harri", ""]]}, {"id": "1511.02934", "submitter": "Ivan Granito", "authors": "Ivan Granito and Paolo De Angelis", "title": "Capital allocation and risk appetite under Solvency II framework", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.1.1136.8404", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to introduce a method for computing the allocated\nSolvency II Capital Requirement (SCR) of each Risk which the company is exposed\nto, taking in account for the diversification effect among different risks. The\nmethod suggested is based on the Euler principle. We show that it has very\nsuitable properties like coherence in the sense of Denault (2001) and RORAC\ncompatibility, and practical implications for the companies that use the\nstandard formula. Further, we show how this approach can be used to evaluate\nthe underwriting and reinsurance policies and to define a measure of the\nCompany's risk appetite, based on the capital at risk return.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 23:46:54 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Granito", "Ivan", ""], ["De Angelis", "Paolo", ""]]}, {"id": "1511.03876", "submitter": "Victor Blanco", "authors": "V\\'ictor Blanco and Jos\\'e M. P\\'erez-S\\'anchez", "title": "On the aggregation of experts' information in Bonus-Malus systems", "comments": "8 Tables; 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a new premium computation principle based on the use\nof prior information from multiple sources for computing the premium charged to\na policyholder. Under this framework, based on the use of Ordered Weighted\nAveraging (OWA) operators, we propose alternative collective and Bayes premiums\nand describe some approaches to compute them. Several examples illustrates the\nnew framework for premium computation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 12:38:50 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 17:05:15 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Blanco", "V\u00edctor", ""], ["P\u00e9rez-S\u00e1nchez", "Jos\u00e9 M.", ""]]}, {"id": "1511.04764", "submitter": "Zurab Kakushadze", "authors": "Zura Kakushadze", "title": "Shrinkage = Factor Model", "comments": "5 pages; a trivial typo corrected; to appear as an Invited Editorial\n  in Journal of Asset Management", "journal-ref": "Journal of Asset Management 17(2) (2016) 69-72, Invited Editorial", "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrunk sample covariance matrix is a factor model of a special form combining\nsome (typically, style) risk factor(s) and principal components with a\n(block-)diagonal factor covariance matrix. As such, shrinkage, which\nessentially inherits out-of-sample instabilities of the sample covariance\nmatrix, is not an alternative to multifactor risk models but one out of myriad\npossible regularization schemes. We give an example of a scheme designed to be\nless prone to said instabilities. We contextualize this within multifactor\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 20:33:14 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 17:16:16 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Kakushadze", "Zura", ""]]}, {"id": "1511.04935", "submitter": "Jamie Fairbrother", "authors": "Jamie Fairbrother, Amanda Turner, Stein Wallace", "title": "Scenario generation for single-period portfolio selection problems with\n  tail risk measures: coping with high dimensions and integer variables", "comments": null, "journal-ref": "Informs Journal on Computing 30(3):472-491, 2018", "doi": "10.1287/ijoc.2017.0790", "report-no": null, "categories": "q-fin.RM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a problem-driven scenario generation approach to the\nsingle-period portfolio selection problem which use tail risk measures such as\nconditional value-at-risk. Tail risk measures are useful for quantifying\npotential losses in worst cases. However, for scenario-based problems these are\nproblematic: because the value of a tail risk measure only depends on a small\nsubset of the support of the distribution of asset returns, traditional\nscenario based methods, which spread scenarios evenly across the whole support\nof the distribution, yield very unstable solutions unless we use a very large\nnumber of scenarios. The proposed approach works by prioritizing the\nconstruction of scenarios in the areas of a probability distribution which\ncorrespond to the tail losses of feasible portfolios.\n  The proposed approach can be applied to difficult instances of the portfolio\nselection problem characterized by high-dimensions, non-elliptical\ndistributions of asset returns, and the presence of integer variables. It is\nalso observed that the methodology works better as the feasible set of\nportfolios becomes more constrained. Based on this fact, a heuristic algorithm\nbased on the sample average approximation method is proposed. This algorithm\nworks by adding artificial constraints to the problem which are gradually\ntightened, allowing one to telescope onto high quality solutions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 12:43:31 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 19:13:04 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 09:32:54 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Fairbrother", "Jamie", ""], ["Turner", "Amanda", ""], ["Wallace", "Stein", ""]]}, {"id": "1511.05303", "submitter": "Hans Colonius", "authors": "Hans Colonius", "title": "An invitation to coupling and copulas: with applications to multisensory\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an introduction to the stochastic concepts of\n\\emph{coupling} and \\emph{copula}. Coupling means the construction of a joint\ndistribution of two or more random variables that need not be defined on one\nand the same probability space, whereas a copula is a function that joins a\nmultivariate distribution to its one-dimensional margins. Their role in\nstochastic modeling is illustrated by examples from multisensory perception.\nPointers to more advanced and recent treatments are provided.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 08:01:14 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Colonius", "Hans", ""]]}, {"id": "1511.06320", "submitter": "Ilya Molchanov", "authors": "Andreas Haier, Ilya Molchanov and Michael Schmutz", "title": "Intragroup transfers, intragroup diversification and their risk\n  assessment", "comments": "20 pages, 3 figures. Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When assessing group solvency, an important question is to what extent\nintragroup transfers may be considered, as this determines to which extent\ndiversification can be achieved. We suggest a framework to describe the\nfamilies of admissible transfers that range from the free movement of capital\nto excluding any transactions. The constraints on admissible transactions are\ndescribed as random closed sets. The paper focuses on the corresponding\nsolvency tests that amount to the existence of acceptable selections of the\nrandom sets of admissible transactions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 19:33:53 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 07:15:24 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Haier", "Andreas", ""], ["Molchanov", "Ilya", ""], ["Schmutz", "Michael", ""]]}, {"id": "1511.06943", "submitter": "Marcelo Righi", "authors": "Marcelo Brutti Righi", "title": "A composition between risk and deviation measures", "comments": null, "journal-ref": "Annals of Operations Research (2019) 282, 299-313", "doi": "10.1007/s10479-018-2913-0", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intuition of risk is based on two main concepts: loss and variability. In\nthis paper, we present a composition of risk and deviation measures, which\ncontemplate these two concepts. Based on the proposed Limitedness axiom, we\nprove that this resulting composition, based on properties of the two\ncomponents, is a coherent risk measure. Similar results for the cases of convex\nand co-monotone risk measures are exposed. We also provide examples of known\nand new risk measures constructed under this framework in order to highlight\nthe importance of our approach, especially the role of the Limitedness axiom.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 01:00:16 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 12:40:03 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 17:42:40 GMT"}, {"version": "v4", "created": "Mon, 2 Oct 2017 18:24:43 GMT"}, {"version": "v5", "created": "Thu, 24 May 2018 18:30:51 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Righi", "Marcelo Brutti", ""]]}, {"id": "1511.07945", "submitter": "Bill Rea", "authors": "Hannah Cheng Juan Zhan, William Rea, and Alethea Rea", "title": "An Application of Correlation Clustering to Portfolio Diversification", "comments": "33 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel application of a clustering algorithm developed\nfor constructing a phylogenetic network to the correlation matrix for 126\nstocks listed on the Shanghai A Stock Market. We show that by visualizing the\ncorrelation matrix using a Neighbor-Net network and using the circular ordering\nproduced during the construction of the network we can reduce the risk of a\ndiversified portfolio compared with random or industry group based selection\nmethods in times of market increase.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 03:42:33 GMT"}], "update_date": "2015-12-12", "authors_parsed": [["Zhan", "Hannah Cheng Juan", ""], ["Rea", "William", ""], ["Rea", "Alethea", ""]]}, {"id": "1511.08068", "submitter": "Paolo Barucca", "authors": "Paolo Barucca and Fabrizio Lillo", "title": "The organization of the interbank network and how ECB unconventional\n  measures affected the e-MID overnight market", "comments": "33 pages, 5 figures", "journal-ref": "Comput Manag Sci (2017)", "doi": "10.1007/s10287-017-0293-6", "report-no": null, "categories": "q-fin.RM physics.data-an q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topological properties of interbank networks have been discussed widely\nin the literature mainly because of their relevance for systemic risk. Here we\npropose to use the Stochastic Block Model to investigate and perform a model\nselection among several possible two block organizations of the network: these\ninclude bipartite, core-periphery, and modular structures. We apply our method\nto the e-MID interbank market in the period 2010-2014 and we show that in\nnormal conditions the most likely network organization is a bipartite\nstructure. In exceptional conditions, such as after LTRO, one of the most\nimportant unconventional measures by ECB at the beginning of 2012, the most\nlikely structure becomes a random one and only in 2014 the e-MID market went\nback to a normal bipartite organization. By investigating the strategy of\nindividual banks, we explore possible explanations and we show that the\ndisappearance of many lending banks and the strategy switch of a very small set\nof banks from borrower to lender is likely at the origin of this structural\nchange.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 14:15:06 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 12:22:46 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 20:24:06 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Barucca", "Paolo", ""], ["Lillo", "Fabrizio", ""]]}, {"id": "1511.08591", "submitter": "Stefan Rass", "authors": "Stefan Rass", "title": "On Game-Theoretic Risk Management (Part Two) -- Algorithms to Compute\n  Nash-Equilibria in Games with Distributions as Payoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.GT math.ST q-fin.EC q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game-theoretic risk management framework put forth in the precursor work\n\"Towards a Theory of Games with Payoffs that are Probability-Distributions\"\n(arXiv:1506.07368 [q-fin.EC]) is herein extended by algorithmic details on how\nto compute equilibria in games where the payoffs are probability distributions.\nOur approach is \"data driven\" in the sense that we assume empirical data\n(measurements, simulation, etc.) to be available that can be compiled into\ndistribution models, which are suitable for efficient decisions about\npreferences, and setting up and solving games using these as payoffs. While\npreferences among distributions turn out to be quite simple if nonparametric\nmethods (kernel density estimates) are used, computing Nash-equilibria in games\nusing such models is discovered as inefficient (if not impossible). In fact, we\ngive a counterexample in which fictitious play fails to converge for the\n(specifically unfortunate) choice of payoff distributions in the game, and\nintroduce a suitable tail approximation of the payoff densities to tackle the\nissue. The overall procedure is essentially a modified version of fictitious\nplay, and is herein described for standard and multicriteria games, to\niteratively deliver an (approximate) Nash-equilibrium. An exact method using\nlinear programming is also given.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 09:39:06 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 07:18:26 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Rass", "Stefan", ""]]}, {"id": "1511.08666", "submitter": "Tatiana Belkina", "authors": "Tatiana Belkina, Nadezhda Konyukhova, and Sergey Kurochkin", "title": "Singular Problems for Integro-Differential Equations in Dynamic\n  Insurance Models", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-1-4614-7333-6_3", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A second order linear integro-differential equation with Volterra integral\noperator and strong singularities at the endpoints (zero and infinity) is\nconsidered. Under limit conditions at the singular points, and some natural\nassumptions, the problem is a singular initial problem with limit normalizing\nconditions at infinity. An existence and uniqueness theorem is proved and\nasymptotic representations of the solution are given. A numerical algorithm for\nevaluating the solution is proposed, calculations and their interpretation are\ndiscussed. The main singular problem under study describes the survival\n(non-ruin) probability of an insurance company on infinite time interval (as a\nfunction of initial surplus) in the Cramer-Lundberg dynamic insurance model\nwith an exponential claim size distribution and certain company's strategy at\nthe financial market assuming investment of a fixed part of the surplus\n(capital) into risky assets (shares) and the rest of it into a risk free asset\n(bank deposit). Accompanying \"degenerate\" problems are also considered that\nhave an independent meaning in risk theory\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 13:37:02 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Belkina", "Tatiana", ""], ["Konyukhova", "Nadezhda", ""], ["Kurochkin", "Sergey", ""]]}, {"id": "1511.08830", "submitter": "Paolo Barucca", "authors": "Paolo Barucca and Fabrizio Lillo", "title": "Disentangling bipartite and core-periphery structure in financial\n  networks", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.chaos.2016.02.004", "report-no": null, "categories": "q-fin.GN physics.data-an physics.soc-ph q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of systems are represented as networks whose architecture\nconveys significant information and determines many of their properties.\nExamples of network architecture include modular, bipartite, and core-periphery\nstructures. However inferring the network structure is a non trivial task and\ncan depend sometimes on the chosen null model. Here we propose a method for\nclassifying network structures and ranking its nodes in a statistically\nwell-grounded fashion. The method is based on the use of Belief Propagation for\nlearning through Entropy Maximization on both the Stochastic Block Model (SBM)\nand the degree-corrected Stochastic Block Model (dcSBM). As a specific\napplication we show how the combined use of the two ensembles -SBM and dcSBM-\nallows to disentangle the bipartite and the core-periphery structure in the\ncase of the e-MID interbank network. Specifically we find that, taking into\naccount the degree, this interbank network is better described by a bipartite\nstructure, while using the SBM the core-periphery structure emerges only when\ndata are aggregated for more than a week.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 14:25:58 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Barucca", "Paolo", ""], ["Lillo", "Fabrizio", ""]]}]