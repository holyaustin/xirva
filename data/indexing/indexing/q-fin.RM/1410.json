[{"id": "1410.0104", "submitter": "Nima Dehmamy", "authors": "Nima Dehmamy, Sergey V. Buldyrev, Shlomo Havlin, H. Eugene Stanley,\n  Irena Vodenska", "title": "Classical mechanics of economic networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial networks are dynamic. To assess their systemic importance to the\nworld-wide economic network and avert losses we need models that take the time\nvariations of the links and nodes into account. Using the methodology of\nclassical mechanics and Laplacian determinism we develop a model that can\npredict the response of the financial network to a shock. We also propose a way\nof measuring the systemic importance of the banks, which we call BankRank.\nUsing European Bank Authority 2011 stress test exposure data, we apply our\nmodel to the bipartite network connecting the largest institutional debt\nholders of the troubled European countries (Greece, Italy, Portugal, Spain, and\nIreland). From simulating our model we can determine whether a network is in a\n\"stable\" state in which shocks do not cause major losses, or a \"unstable\" state\nin which devastating damages occur. Fitting the parameters of the model, which\nplay the role of physical coupling constants, to Eurozone crisis data shows\nthat before the Eurozone crisis the system was mostly in a \"stable\" regime, and\nthat during the crisis it transitioned into an \"unstable\" regime. The numerical\nsolutions produced by our model match closely the actual time-line of events of\nthe crisis. We also find that, while the largest holders are usually more\nimportant, in the unstable regime smaller holders also exhibit systemic\nimportance. Our model also proves useful for determining the vulnerability of\nbanks and assets to shocks. This suggests that our model may be a useful tool\nfor simulating the response dynamics of shared portfolio networks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 04:10:21 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 19:18:42 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Dehmamy", "Nima", ""], ["Buldyrev", "Sergey V.", ""], ["Havlin", "Shlomo", ""], ["Stanley", "H. Eugene", ""], ["Vodenska", "Irena", ""]]}, {"id": "1410.0125", "submitter": "Andrei Leonidov", "authors": "A.V. Leonidov, E.L. Rumyantsev", "title": "Systemic Interbank Network Risks in Russia", "comments": "Based on the talk given by A. Leonidov at the workshop \"Random Graphs\n  and Their Applications\", Moscow, October 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling of contagion in interbank networks is discussed. A model taking\ninto account bow-tie structure and dissasortativity of interbank networks is\ndeveloped. The model is shown to provide a good quantitative description of the\nRussian interbank market. Detailed arguments favoring the non-percolative\nnature of contagion-related risks in the Russian interbank market are given.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 07:03:35 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Leonidov", "A. V.", ""], ["Rumyantsev", "E. L.", ""]]}, {"id": "1410.0852", "submitter": "Raphael Hauser A", "authors": "Raphael Hauser, Sergey Shahverdyan and Paul Embrechts", "title": "A General Duality Relation with Applications in Quantitative Risk\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in risk management is the robust aggregation of\ndifferent sources of risk in a situation where little or no data are available\nto infer information about their dependencies. A popular approach to solving\nthis problem is to formulate an optimization problem under which one maximizes\na risk measure over all multivariate distributions that are consistent with the\navailable data. In several special cases of such models, there exist dual\nproblems that are easier to solve or approximate, yielding robust bounds on the\naggregated risk. In this chapter we formulate a general optimization problem,\nwhich can be seen as a doubly infinite linear programming problem, and we show\nthat the associated dual generalizes several well known special cases and\nextends to new risk management models we propose.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 14:00:43 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Hauser", "Raphael", ""], ["Shahverdyan", "Sergey", ""], ["Embrechts", "Paul", ""]]}, {"id": "1410.1101", "submitter": "Rodrigo S. Targino", "authors": "Rodrigo S. Targino, Gareth W. Peters, Pavel V. Shevchenko", "title": "Sequential Monte Carlo Samplers for capital allocation under\n  copula-dependent risk models", "comments": null, "journal-ref": "Insurance: Mathematics and Economics 61 (2015) 206-226", "doi": "10.1016/j.insmatheco.2015.01.007", "report-no": null, "categories": "stat.CO q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we assume a multivariate risk model has been developed for a\nportfolio and its capital derived as a homogeneous risk measure. The Euler (or\ngradient) principle, then, states that the capital to be allocated to each\ncomponent of the portfolio has to be calculated as an expectation conditional\nto a rare event, which can be challenging to evaluate in practice. We exploit\nthe copula-dependence within the portfolio risks to design a Sequential Monte\nCarlo Samplers based estimate to the marginal conditional expectations involved\nin the problem, showing its efficiency through a series of computational\nexamples.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 00:10:43 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 18:46:26 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Targino", "Rodrigo S.", ""], ["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "1410.2034", "submitter": "Damiano  Brigo", "authors": "Damiano Brigo, Cyril Durand", "title": "An initial approach to Risk Management of Funding Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we sketch an initial tentative approach to funding costs\nanalysis and management for contracts with bilateral counterparty risk in a\nsimplified setting. We depart from the existing literature by analyzing the\nissue of funding costs and benefits under the assumption that the associated\nrisks cannot be hedged properly. We also model the treasury funding spread by\nmeans of a stochastic Weighted Cost of Funding Spread (WCFS) which helps\ndescribing more realistic financing policies of a financial institution. We\nelaborate on some limitations in replication-based Funding / Credit Valuation\nAdjustments we worked on ourselves in the past, namely CVA, DVA, FVA and\nrelated quantities as generally discussed in the industry. We advocate as a\ndifferent possibility, when replication is not possible, the analysis of the\nfunding profit and loss distribution and explain how long term funding spreads,\nwrong way risk and systemic risk are generally overlooked in most of the\ncurrent literature on risk measurement of funding costs. As a matter of initial\nillustration, we discuss in detail the funding management of interest rate\nswaps with bilateral counterparty risk in the simplified setup of our framework\nthrough numerical examples and via a few simplified assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 09:30:30 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Brigo", "Damiano", ""], ["Durand", "Cyril", ""]]}, {"id": "1410.2570", "submitter": "Zhang Li", "authors": "Zhang Li, Xiaojun Lin, Borja Peleato-Inarrea, and Ilya Pollak", "title": "Optimal Monitoring and Mitigation of Systemic Risk in Financial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of optimally allocating a cash injection into\na financial system in distress. Given a one-period borrower-lender network in\nwhich all debts are due at the same time and have the same seniority, we\naddress the problem of allocating a fixed amount of cash among the nodes to\nminimize the weighted sum of unpaid liabilities. Assuming all the loan amounts\nand asset values are fixed and that there are no bankruptcy costs, we show that\nthis problem is equivalent to a linear program. We develop a duality-based\ndistributed algorithm to solve it which is useful for applications where it is\ndesirable to avoid centralized data gathering and computation. We also consider\nthe problem of minimizing the expectation of the weighted sum of unpaid\nliabilities under the assumption that the net external asset holdings of all\ninstitutions are stochastic. We show that this problem is a two-stage\nstochastic linear program. To solve it, we develop two algorithms based on:\nBenders decomposition algorithm and projected stochastic gradient descent. We\nshow that if the defaulting nodes never pay anything, the deterministic optimal\ncash injection allocation problem is an NP-hard mixed-integer linear program.\nHowever, modern optimization software enables the computation of very accurate\nsolutions to this problem on a personal computer in a few seconds for network\nsizes comparable with the size of the US banking system. In addition, we\naddress the problem of allocating the cash injection amount so as to minimize\nthe number of nodes in default. For this problem, we develop two heuristic\nalgorithms: a reweighted l1 minimization algorithm and a greedy algorithm. We\nillustrate these two algorithms using three synthetic network structures for\nwhich the optimal solution can be calculated exactly. We also compare these two\nalgorithms on three types random networks which are more complex.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 18:49:10 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 22:54:31 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Li", "Zhang", ""], ["Lin", "Xiaojun", ""], ["Peleato-Inarrea", "Borja", ""], ["Pollak", "Ilya", ""]]}, {"id": "1410.4382", "submitter": "Mark Davis", "authors": "Mark H.A. Davis", "title": "Verification of internal risk measure estimates", "comments": "29 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns sequential computation of risk measures for financial\ndata and asks how, given a risk measurement procedure, we can tell whether the\nanswers it produces are `correct'. We draw the distinction between `external'\nand `internal' risk measures and concentrate on the latter, where we observe\ndata in real time, make predictions and observe outcomes. It is argued that\nevaluation of such procedures is best addressed from the point of view of\nprobability forecasting or Dawid's theory of `prequential statistics' [Dawid,\nJRSS(A)1984]. We introduce a concept of `calibration' of a risk measure in a\ndynamic setting, following the precepts of Dawid's weak and strong prequential\nprinciples, and examine its application to quantile forecasting (VaR -- value\nat risk) and to mean estimation (applicable to CVaR -- expected shortfall). The\nrelationship between these ideas and `elicitability' [Gneiting, JASA 2011] is\nexamined. We show in particular that VaR has special properties not shared by\nany other risk measure. Turning to CVaR we argue that its main deficiency is\nthe unquantifiable tail dependence of estimators. In a final section we show\nthat a simple data-driven feedback algorithm can produce VaR estimates on\nfinancial data that easily pass both the consistency test and a further\nnewly-introduced statistical test for independence of a binary sequence.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 11:52:25 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 19:03:56 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Davis", "Mark H. A.", ""]]}, {"id": "1410.4847", "submitter": "Yoshiharu Maeno", "authors": "Yoshiharu Maeno, Kenji Nishiguchi, Satoshi Morinaga, Hirokazu\n  Matsushima", "title": "Impact of shadow banks on financial contagion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An asset network systemic risk (ANWSER) model is presented to investigate the\nimpact of how shadow banks are intermingled in a financial system on the\nseverity of financial contagion. Particularly, the focus of this study is the\nimpact of the following three representative topologies of an interbank loan\nnetwork between shadow banks and regulated banks. (1) Random mixing network:\nshadow banks and regulated banks are intermingled randomly. (2)\nAsset-correlated mixing network: banks having bigger assets are a regulated\nbank and other banks are shadow banks. (3) Layered mixing network: banks in a\nshadow bank layer are connected to banks in a regulated bank layer with some\ninterbank loans.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 08:06:53 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Maeno", "Yoshiharu", ""], ["Nishiguchi", "Kenji", ""], ["Morinaga", "Satoshi", ""], ["Matsushima", "Hirokazu", ""]]}, {"id": "1410.5621", "submitter": "Nicol\\'o Musmeci Mr", "authors": "Nicol\\'o Musmeci and Tomaso Aste and Tiziana Di Matteo", "title": "Risk diversification: a study of persistence with a filtered\n  correlation-network approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution with time of the correlation structure of equity returns is\nstudied by means of a filtered network approach investigating persistences and\nrecurrences and their implications for risk diversification strategies. We\nbuild dynamically Planar Maximally Filtered Graphs from the correlation\nstructure over a rolling window and we study the persistence of the associated\nDirected Bubble Hierarchical Tree (DBHT) clustering structure. We observe that\nthe DBHT clustering structure is quite stable during the early 2000' becoming\ngradually less persistent before the unfolding of the 2007-2008 crisis. The\ncorrelation structure eventually recovers persistence in the aftermath of the\ncrisis settling up a new phase, distinct from the pre-cysts structure, where\nthe market structure is less related to industrial sector activity. Notably, we\nobserve that - presently - the correlation structure is loosing again\npersistence indicating the building-up of another, different, phase. Such\ndynamical changes in persistence and their occurrence at the unfolding of\nfinancial crises rises concerns about the effectiveness of correlation-based\nportfolio management tools for risk diversification.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 11:27:52 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Musmeci", "Nicol\u00f3", ""], ["Aste", "Tomaso", ""], ["Di Matteo", "Tiziana", ""]]}, {"id": "1410.6841", "submitter": "Yuri A. Katz", "authors": "Yuri A. Katz", "title": "qGaussian model of default", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the qGaussian generalization of the Merton framework, which takes\ninto account slow fluctuations of the volatility of the firms market value of\nfinancial assets. The minimal version of the model depends on the Tsallis\nentropic parameter q and the generalized distance to default. The empirical\nfoundation and implications of the model are illustrated by the study of 645\nNorth American industrial firms during the financial crisis, 2006 - 2012. All\ndefaulters in the sample have exceptionally large, corresponding to unusually\nfat-tailed unconditional distributions of log-asset-returns. Using Receiver\nOperating Characteristic curves, we demonstrate the high forecasting power of\nthe model in prediction of 1-year defaults. Our study suggests that the level\nof complexity of the realized time series, quantified by q, should be taken\ninto account to improve valuations of default risk.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 21:28:29 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Katz", "Yuri A.", ""]]}, {"id": "1410.6898", "submitter": "Mauro Bernardi", "authors": "Mauro Bernardi, Leopoldo Catania and Lea Petrella", "title": "Are news important to predict large losses?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the impact of news to predict extreme financial\nreturns using high frequency data. We consider several model specifications\ndiffering for the dynamic property of the underlying stochastic process as well\nas for the innovation process. Since news are essentially qualitative measures,\nthey are firstly transformed into quantitative measures which are subsequently\nintroduced as exogenous regressors into the conditional volatility dynamics.\nThree basic sentiment indexes are constructed starting from three list of words\ndefined by historical market news response and by a discriminant analysis.\nModels are evaluated in terms of their predictive accuracy to forecast\nout-of-sample Value-at-Risk of the STOXX Europe 600 sectors at different\nconfidence levels using several statistic tests and the Model Confidence Set\nprocedure of Hansen et al. (2011). Since the Hansen's procedure usually\ndelivers a set of models having the same VaR predictive ability, we propose a\nnew forecasting combination technique that dynamically weights the VaR\npredictions obtained by the models belonging to the optimal final set. Our\nresults confirms that the inclusion of exogenous information as well as the\nright specification of the returns' conditional distribution significantly\ndecrease the number of actual versus expected VaR violations towards one, as\nthis is especially true for higher confidence levels.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 08:53:09 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 11:15:14 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Bernardi", "Mauro", ""], ["Catania", "Leopoldo", ""], ["Petrella", "Lea", ""]]}, {"id": "1410.7845", "submitter": "Chuancun Yin", "authors": "Ying Zhang and Chuancun Yin", "title": "A new multivariate dependence measure based on comonotonicity", "comments": "18pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new multivariate dependence measure based on\ncomonotonicity by means of product moment which motivated by the recent papers\nof Koch and Schepper (ASTIN Bulletin 41 (2011) 191-213) and Dhaene et al.\n(Journal of Computational and Applied Mathematics 263 (2014) 78-87). Some\ndifferences and relations between the new dependence measure and other\nmultivariate measures are an- alyzed. We also give several characteristics of\nthis measure and estimations based on the definitions and its property are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 00:44:12 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Zhang", "Ying", ""], ["Yin", "Chuancun", ""]]}, {"id": "1410.8671", "submitter": "Oliver Kley", "authors": "Oliver Kley and Claudia Kluppelberg and Gesine Reinert", "title": "Risk in a large claims insurance market with bipartite graph structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the influence of sharing large exogeneous losses to the reinsurance\nmarket by a bipartite graph. Using Pareto-tailed claims and multivariate\nregular variation we obtain asymptotic results for the Value-at-Risk and the\nConditional Tail Expectation. We show that the dependence on the network\nstructure plays a fundamental role in their asymptotic behaviour. As is\nwell-known in a non-network setting, if the Pareto exponent is larger than 1,\nthen for the individual agent (reinsurance company) diversification is\nbeneficial, whereas when it is less than 1, concentration on a few objects is\nthe better strategy. An additional aspect of this paper is the amount of\nuninsured losses which have to be convered by society. In the situation of\nnetworks of agents, in our setting diversification is never detrimental\nconcerning the amount of uninsured losses. If the Pareto-tailed claims have\nfinite mean, diversification turns out to be never detrimental, both for\nsociety and for individual agents. In contrast, if the Pareto-tailed claims\nhave infinite mean, a conflicting situation may arise between the incentives of\nindividual agents and the interest of some regulator to keep risk for society\nsmall. We explain the influence of the network structure on diversification\neffects in different network scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 08:54:24 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 10:43:14 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2015 18:38:59 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Kley", "Oliver", ""], ["Kluppelberg", "Claudia", ""], ["Reinert", "Gesine", ""]]}]