[{"id": "1212.0092", "submitter": "Joris L. van Velsen", "authors": "J. L. van Velsen", "title": "Parameter estimation of a Levy copula of a discretely observed bivariate\n  compound Poisson process with an application to operational risk modelling", "comments": "25 pages including 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is developed to estimate the parameters of a Levy copula of a\ndiscretely observed bivariate compound Poisson process without knowledge of\ncommon shocks. The method is tested in a small sample simulation study. Also,\nthe method is applied to a real data set and a goodness of fit test is\ndeveloped. With the methodology of this work, the Levy copula becomes a\nrealistic tool of the advanced measurement approach of operational risk.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 10:55:05 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["van Velsen", "J. L.", ""]]}, {"id": "1212.2833", "submitter": "Didier Sornette", "authors": "D. Sornette and P. Cauwels", "title": "The Illusion of the Perpetual Money Machine", "comments": "27 pages, 18 figures (Notenstein Academy White Paper Series)", "journal-ref": "Risks 2, 103-131 (2014)", "doi": null, "report-no": null, "categories": "q-fin.GN physics.soc-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the present crisis and stalling economy continuing since 2007\nare rooted in the delusionary belief in policies based on a \"perpetual money\nmachine\" type of thinking. We document strong evidence that, since the early\n1980s, consumption has been increasingly funded by smaller savings, booming\nfinancial profits, wealth extracted from house price appreciation and explosive\ndebt. This is in stark contrast with the productivity-fueled growth that was\nseen in the 1950s and 1960s. This transition, starting in the early 1980s, was\nfurther supported by a climate of deregulation and a massive growth in\nfinancial derivatives designed to spread and diversify the risks globally. The\nresult has been a succession of bubbles and crashes, including the worldwide\nstock market bubble and great crash of October 1987, the savings and loans\ncrisis of the 1980s, the burst in 1991 of the enormous Japanese real estate and\nstock market bubbles, the emerging markets bubbles and crashes in 1994 and\n1997, the LTCM crisis of 1998, the dotcom bubble bursting in 2000, the recent\nhouse price bubbles, the financialization bubble via special investment\nvehicles, the stock market bubble, the commodity and oil bubbles and the debt\nbubbles, all developing jointly and feeding on each other. Rather than still\nhoping that real wealth will come out of money creation, we need fundamentally\nnew ways of thinking. In uncertain times, it is essential, more than ever, to\nthink in scenarios: what can happen in the future, and, what would be the\neffect on your wealth and capital? How can you protect against adverse\nscenarios? We thus end by examining the question \"what can we do?\" from the\nmacro level, discussing the fundamental issue of incentives and of constructing\nand predicting scenarios as well as developing investment insights.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 14:59:44 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Sornette", "D.", ""], ["Cauwels", "P.", ""]]}, {"id": "1212.3195", "submitter": "Raffaello Morales", "authors": "Raffaello Morales, T. Di Matteo and Tomaso Aste", "title": "Non stationary multifractality in stock returns", "comments": "27 pages, 10 figures", "journal-ref": "Physica A, 392, 6470-6483, 2013", "doi": "10.1016/j.physa.2013.08.037", "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform an extensive empirical analysis of scaling properties of equity\nreturns, suggesting that financial data show time varying multifractal\nproperties. This is obtained by comparing empirical observations of the\nweighted generalised Hurst exponent (wGHE) with time series simulated via\nMultifractal Random Walk (MRW) by Bacry \\textit{et al.} [\\textit{E.Bacry,\nJ.Delour and J.Muzy, Phys.Rev.E \\,{\\bf 64} 026103, 2001}]. While dynamical wGHE\ncomputed on synthetic MRW series is consistent with a scenario where\nmultifractality is constant over time, fluctuations in the dynamical wGHE\nobserved in empirical data are not in agreement with a MRW with constant\nintermittency parameter. We test these hypotheses of constant multifractality\nconsidering different specifications of MRW model with fatter tails: in all\ncases considered, although the thickness of the tails accounts for most of\nanomalous fluctuations of multifractality, still cannot fully explain the\nobserved fluctuations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 15:25:38 GMT"}, {"version": "v2", "created": "Thu, 23 May 2013 08:20:53 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Morales", "Raffaello", ""], ["Di Matteo", "T.", ""], ["Aste", "Tomaso", ""]]}, {"id": "1212.3716", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "The art of probability-of-default curve calibration", "comments": "35 pages, 1 figure, 12 tables, minor changes", "journal-ref": "Journal of Credit Risk 9(4), 63-103, 2013", "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PD curve calibration refers to the transformation of a set of rating grade\nlevel probabilities of default (PDs) to another average PD level that is\ndetermined by a change of the underlying portfolio-wide PD. This paper presents\na framework that allows to explore a variety of calibration approaches and the\nconditions under which they are fit for purpose. We test the approaches\ndiscussed by applying them to publicly available datasets of agency rating and\ndefault statistics that can be considered typical for the scope of application\nof the approaches. We show that the popular 'scaled PDs' approach is\ntheoretically questionable and identify an alternative calibration approach\n('scaled likelihood ratio') that is both theoretically sound and performs\nbetter on the test datasets.\n  Keywords: Probability of default, calibration, likelihood ratio, Bayes'\nformula, rating profile, binary classification.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2012 19:08:46 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2013 20:08:42 GMT"}, {"version": "v3", "created": "Sat, 27 Apr 2013 15:30:34 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2013 17:37:38 GMT"}, {"version": "v5", "created": "Sat, 19 Oct 2013 08:51:29 GMT"}, {"version": "v6", "created": "Tue, 26 Nov 2013 18:43:50 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1212.3958", "submitter": "Sara Biagini", "authors": "Sara Biagini and Jocelyne Bion-Nadal", "title": "Dynamic quasi-concave performance measures", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define Conditional quasi concave Performance Measures (CPMs), on random\nvariables bounded from below, to accommodate for additional information. Our\nnotion encompasses a wide variety of cases, from conditional expected utility\nand certainty equivalent to conditional acceptability indexes. We provide the\ncharacterization of a CPM in terms of an induced family of conditional convex\nrisk measures. In the case of indexes these risk measures are coherent. Then,\nDynamic Performance Measures (DPMs) are introduced and the problem of time\nconsistency is addressed. The definition of time consistency chosen here\nensures that the positions which are considered good tomorrow are already\nconsidered good today. We prove the equivalence between time consistency for a\nDPM and weak acceptance consistency for the induced families of risk measures.\nFinally, we extend CPMs and DPMs to dividend processes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 11:13:00 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Biagini", "Sara", ""], ["Bion-Nadal", "Jocelyne", ""]]}, {"id": "1212.4126", "submitter": "Richard Warnung", "authors": "Rainer Haidinger and Richard Warnung", "title": "Risk Measures in a Regime Switching Model Capturing Stylized Facts", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We pick up the regime switching model for asset returns introduced by Rogers\nand Zhang. The calibration involves various markets including implied\nvolatility in order to gain additional predictive power. We focus on the\ncalculation of risk measures by Fourier methods that have successfully been\napplied to option pricing and analyze the accuracy of the results.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 20:31:20 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Haidinger", "Rainer", ""], ["Warnung", "Richard", ""]]}, {"id": "1212.5395", "submitter": "Claudio Fontana", "authors": "Claudio Fontana and Juan Miguel A. Montes", "title": "A unified approach to pricing and risk management of equity and credit\n  risk", "comments": "18 pages, 4 figures. Revised version (remarks and references added)", "journal-ref": "Journal of Computational and Applied Mathematics (2014), vol. 259,\n  pp. 350-261", "doi": "10.1016/j.cam.2013.04.047", "report-no": null, "categories": "q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified framework for equity and credit risk modeling, where the\ndefault time is a doubly stochastic random time with intensity driven by an\nunderlying affine factor process. This approach allows for flexible\ninteractions between the defaultable stock price, its stochastic volatility and\nthe default intensity, while maintaining full analytical tractability. We\ncharacterise all risk-neutral measures which preserve the affine structure of\nthe model and show that risk management as well as pricing problems can be\ndealt with efficiently by shifting to suitable survival measures. As an\nexample, we consider a jump-to-default extension of the Heston stochastic\nvolatility model.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 11:06:58 GMT"}, {"version": "v2", "created": "Sun, 19 May 2013 18:04:50 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Fontana", "Claudio", ""], ["Montes", "Juan Miguel A.", ""]]}, {"id": "1212.5563", "submitter": "Birgit Rudloff", "authors": "Zachary Feinstein, Birgit Rudloff", "title": "Multiportfolio time consistency for set-valued convex and coherent risk\n  measures", "comments": null, "journal-ref": "Finance and Stochastics 19 (1), 67-107 (2015)", "doi": "10.1007/s00780-014-0247-6", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equivalent characterizations of multiportfolio time consistency are deduced\nfor closed convex and coherent set-valued risk measures on $L^p(\\Omega,\\mathcal\nF, P; R^d)$ with image space in the power set of $L^p(\\Omega,\\mathcal\nF_t,P;R^d)$. In the convex case, multiportfolio time consistency is equivalent\nto a cocycle condition on the sum of minimal penalty functions. In the coherent\ncase, multiportfolio time consistency is equivalent to a generalized version of\nstability of the dual variables. As examples, the set-valued entropic risk\nmeasure with constant risk aversion coefficient is shown to satisfy the cocycle\ncondition for its minimal penalty functions, the set of superhedging portfolios\nin markets with proportional transaction costs is shown to have the stability\nproperty and in markets with convex transaction costs is shown to satisfy the\ncomposed cocycle condition, and a multiportfolio time consistent version of the\nset-valued average value at risk, the composed AV@R, is given and its dual\nrepresentation deduced.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 19:10:08 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2013 19:23:55 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2013 08:21:53 GMT"}, {"version": "v4", "created": "Wed, 21 May 2014 14:28:13 GMT"}, {"version": "v5", "created": "Fri, 10 Oct 2014 07:32:37 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Feinstein", "Zachary", ""], ["Rudloff", "Birgit", ""]]}, {"id": "1212.6732", "submitter": "Samuel Drapeau", "authors": "Samuel Drapeau and Michael Kupper and Antonis Papapantoleon", "title": "A Fourier Approach to the Computation of CV@R and Optimized Certainty\n  Equivalents", "comments": null, "journal-ref": "Journal of Risk 16(6), 3-29, 2014", "doi": null, "report-no": null, "categories": "q-fin.RM math.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the class of risk measures associated with optimized certainty\nequivalents. This class includes several popular examples, such as CV@R and\nmonotone mean-variance. Numerical schemes are developed for the computation of\nthese risk measures using Fourier transform methods. This leads, in particular,\nto a very competitive method for the calculation of CV@R which is comparable in\ncomputational time to the calculation of V@R. We also develop methods for the\nefficient computation of risk contributions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2012 15:36:46 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2013 10:47:32 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2013 14:17:56 GMT"}, {"version": "v4", "created": "Fri, 13 Dec 2013 10:42:11 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Drapeau", "Samuel", ""], ["Kupper", "Michael", ""], ["Papapantoleon", "Antonis", ""]]}]