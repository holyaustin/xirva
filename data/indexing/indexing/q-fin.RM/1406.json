[{"id": "1406.0044", "submitter": "Zurab Kakushadze", "authors": "Zura Kakushadze", "title": "Can Turnover Go to Zero?", "comments": "28 pages; minor misprints corrected; to appear in Journal of\n  Derivatives & Hedge Funds", "journal-ref": "Journal of Derivatives & Hedge Funds 20(3) (2014) 157-176", "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internal crossing of trades between multiple alpha streams results in\nportfolio turnover reduction. Turnover reduction can be modeled using the\ncorrelation structure of the alpha streams. As more and more alphas are added,\ngenerally turnover reduces. In this note we use a factor model approach to\naddress the question of whether the turnover goes to zero or a finite limit as\nthe number of alphas N goes to infinity. We argue that the limiting turnover\nvalue is determined by the number of alpha clusters F, not the number of alphas\nN. This limiting value behaves according to the \"power law\" ~ F^(-3/2). So, to\nachieve zero limiting turnover, the number of alpha clusters must go to\ninfinity along with the number of alphas. We further argue on general grounds\nthat, if the number of underlying tradable instruments is finite, then the\nturnover cannot go to zero, which implies that the number of alpha clusters\nalso appears to be finite.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 02:33:20 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 14:56:15 GMT"}, {"version": "v3", "created": "Mon, 11 Aug 2014 23:49:16 GMT"}, {"version": "v4", "created": "Wed, 1 Oct 2014 17:49:57 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Kakushadze", "Zura", ""]]}, {"id": "1406.0389", "submitter": "J.D. Opdyke", "authors": "J.D. Opdyke", "title": "Estimating Operational Risk Capital with Greater Accuracy, Precision,\n  and Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The largest US banks are required by regulatory mandate to estimate the\noperational risk capital they must hold using an Advanced Measurement Approach\n(AMA) as defined by the Basel II/III Accords. Most use the Loss Distribution\nApproach (LDA) which defines the aggregate loss distribution as the convolution\nof a frequency and a severity distribution representing the number and\nmagnitude of losses, respectively. Estimated capital is a Value-at-Risk (99.9th\npercentile) estimate of this annual loss distribution. In practice, the\nseverity distribution drives the capital estimate, which is essentially a very\nhigh quantile of the estimated severity distribution. Unfortunately, because\nthe relevant severities are heavy-tailed AND the quantiles being estimated are\nso high, VaR always appears to be a convex function of the severity parameters,\ncausing all widely-used estimators to generate biased capital estimates\n(apparently) due to Jensen's Inequality. The observed capital inflation is\nsometimes enormous, even at the unit-of-measure (UoM) level (even billions\nUSD). Herein I present an estimator of capital that essentially eliminates this\nupward bias. The Reduced-bias Capital Estimator (RCE) is more consistent with\nthe regulatory intent of the LDA framework than implementations that fail to\nmitigate this bias. RCE also notably increases the precision of the capital\nestimate and consistently increases its robustness to violations of the i.i.d.\ndata presumption (which are endemic to operational risk loss event data). So\nwith greater capital accuracy, precision, and robustness, RCE lowers capital\nrequirements at both the UoM and enterprise levels, increases capital stability\nfrom quarter to quarter, ceteris paribus, and does both while more accurately\nand precisely reflecting regulatory intent. RCE is straightforward to implement\nusing any major statistical software package.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 14:45:44 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 18:44:33 GMT"}, {"version": "v3", "created": "Sat, 5 Jul 2014 17:29:56 GMT"}, {"version": "v4", "created": "Sun, 26 Oct 2014 21:17:14 GMT"}, {"version": "v5", "created": "Sun, 16 Nov 2014 21:30:17 GMT"}, {"version": "v6", "created": "Thu, 27 Nov 2014 13:40:22 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Opdyke", "J. D.", ""]]}, {"id": "1406.1249", "submitter": "Zurab Kakushadze", "authors": "Zura Kakushadze", "title": "Notes on Alpha Stream Optimization", "comments": "42 pages; clarifying remarks added, minor misprints corrected; to\n  appear in The Journal of Investment Strategies", "journal-ref": "The Journal of Investment Strategies 4(3) (2015) 37-81", "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In these notes we discuss investment allocation to multiple alpha streams\ntraded on the same execution platform, including when trades are crossed\ninternally resulting in turnover reduction. We discuss approaches to alpha\nweight optimization where one maximizes P&L subject to bounds on volatility (or\nSharpe ratio). The presence of negative alpha weights, which are allowed when\nalpha streams are traded on the same execution platform, complicates the\noptimization problem. By using factor model approach to alpha covariance\nmatrix, the original optimization problem can be viewed as a 1-dimensional root\nsearching problem plus an optimization problem that requires a finite number of\niterations. We discuss this approach without costs and with linear costs, and\nalso with nonlinear costs in a certain approximation, which makes the\nallocation problem tractable without forgoing nonlinear portfolio capacity\nbound effects.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 00:41:28 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 17:58:09 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Kakushadze", "Zura", ""]]}, {"id": "1406.2950", "submitter": "Hirbod Assa", "authors": "Hirbod Assa", "title": "On Optimal Reinsurance Policy with Distortion Risk Measures and Premiums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of optimal reinsurance design, when\nthe risk is measured by a distortion risk measure and the premium is given by a\ndistortion risk premium. First, we show how the optimal reinsurance design for\nthe ceding company, the reinsurance company and the social planner can be\nformulated in the same way. Second, by introducing the marginal indemnification\nfunctions, we characterize the optimal reinsurance contracts. We show that, for\nan optimal policy, the associated marginal indemnification function only takes\nthe values zero and one. We will see how the roles of the market preferences\nand premiums and that of the total risk are separated.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 16:09:15 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Assa", "Hirbod", ""]]}, {"id": "1406.3396", "submitter": "Zurab Kakushadze", "authors": "Zura Kakushadze", "title": "Factor Models for Alpha Streams", "comments": "27 pages; discussion section and references added; to appear in The\n  Journal of Investment Strategies", "journal-ref": "The Journal of Investment Strategies 4(1) (2014) 83-109", "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for constructing factor models for alpha streams. Our\nmotivation is threefold. 1) When the number of alphas is large, the sample\ncovariance matrix is singular. 2) Its out-of-sample stability is challenging.\n3) Optimization of investment allocation into alpha streams can be tractable\nfor a factor model alpha covariance matrix. We discuss various risk factors for\nalphas such as: style risk factors; cluster risk factors based on alpha\ntaxonomy; principal components; and also using the underlying tradables\n(stocks) as alpha risk factors, for which computing the factor loadings and\nfactor covariance matrices does not involve any correlations with alphas, and\ntheir number is much larger than that of the relevant principal components. We\ndraw insight from stock factor models, but also point out substantial\ndifferences.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 00:52:52 GMT"}, {"version": "v2", "created": "Thu, 2 Oct 2014 11:18:37 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Kakushadze", "Zura", ""]]}, {"id": "1406.4222", "submitter": "Zuo Quan Xu Dr.", "authors": "Zuo Quan Xu", "title": "Investment under Duality Risk Measure", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One index satisfies the duality axiom if one agent, who is uniformly more\nrisk-averse than another, accepts a gamble, the latter accepts any less risky\ngamble under the index. Aumann and Serrano (2008) show that only one index\ndefined for so-called gambles satisfies the duality and positive homogeneity\naxioms. We call it a duality index. This paper extends the definition of\nduality index to all outcomes including all gambles, and considers a portfolio\nselection problem in a complete market, in which the agent's target is to\nminimize the index of the utility of the relative investment outcome. By\nlinking this problem to a series of Merton's optimum consumption-like problems,\nthe optimal solution is explicitly derived. It is shown that if the prior\nbenchmark level is too high (which can be verified), then the investment risk\nwill be beyond any agent's risk tolerance. If the benchmark level is\nreasonable, then the optimal solution will be the same as that of one of the\nMerton's series problems, but with a particular value of absolute risk\naversion, which is given by an explicit algebraic equation as a part of the\noptimal solution. According to our result, it is riskier to achieve the same\nsurplus profit in a stable market than in a less-stable market, which is\nconsistent with the common financial intuition.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 02:56:52 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Xu", "Zuo Quan", ""]]}, {"id": "1406.4322", "submitter": "Matthew Ames", "authors": "Matthew Ames, Gareth W. Peters, Guillaume Bagnarosa and Ioannis\n  Kosmidis", "title": "Upside and Downside Risk Exposures of Currency Carry Trades via Tail\n  Dependence", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.4314", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currency carry trade is the investment strategy that involves selling low\ninterest rate currencies in order to purchase higher interest rate currencies,\nthus profiting from the interest rate differentials. This is a well known\nfinancial puzzle to explain, since assuming foreign exchange risk is\nuninhibited and the markets have rational risk-neutral investors, then one\nwould not expect profits from such strategies. That is, according to uncovered\ninterest rate parity (UIP), changes in the related exchange rates should offset\nthe potential to profit from such interest rate differentials. However, it has\nbeen shown empirically, that investors can earn profits on average by borrowing\nin a country with a lower interest rate, exchanging for foreign currency, and\ninvesting in a foreign country with a higher interest rate, whilst allowing for\nany losses from exchanging back to their domestic currency at maturity. This\npaper explores the financial risk that trading strategies seeking to exploit a\nviolation of the UIP condition are exposed to with respect to multivariate tail\ndependence present in both the funding and investment currency baskets. It will\noutline in what contexts these portfolio risk exposures will benefit\naccumulated portfolio returns and under what conditions such tail exposures\nwill reduce portfolio returns.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 11:22:06 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Ames", "Matthew", ""], ["Peters", "Gareth W.", ""], ["Bagnarosa", "Guillaume", ""], ["Kosmidis", "Ioannis", ""]]}, {"id": "1406.5755", "submitter": "Johan Gunnesson", "authors": "Johan Gunnesson, Alberto Fern\\'andez Mu\\~noz de Morales", "title": "A Bond Consistent Derivative Fair Value", "comments": "Minor changes. Additional comments and reference added", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a rigorously motivated pricing equation for\nderivatives, including general cash collateralization schemes, which is\nconsistent with quoted market bond prices. Traditionally, there have been\ndifferences in how instruments with similar cash flow structures have been\npriced if their definition falls under that of a financial derivative versus if\nthey correspond to bonds, leading to possibilities such as funding through\nderivatives transactions. Furthermore, the problem has not been solved with the\nrecent introduction of Funding Valuation Adjustments in derivatives pricing,\nand in some cases has even been made worse.\n  In contrast, our proposed equation is not only consistent with fixed income\nassets and liabilities, but is also symmetric, implying a well-defined exit\nprice, independent of the entity performing the valuation. Also, we provide\nsome practical proxies, such as first-order approximations or basing\ncalculations of CVA and DVA on bond curves, rather than Credit Default Swaps.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 19:45:16 GMT"}, {"version": "v2", "created": "Fri, 19 Sep 2014 10:36:29 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Gunnesson", "Johan", ""], ["de Morales", "Alberto Fern\u00e1ndez Mu\u00f1oz", ""]]}, {"id": "1406.5817", "submitter": "Vinko Zlati\\'c", "authors": "Vinko Zlati\\'c, Giampaolo Gabbi, Hrvoje Abraham", "title": "Reduction of systemic risk by means of Pigouvian taxation", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0114928", "report-no": null, "categories": "q-fin.RM physics.soc-ph q-fin.GN q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the possibility of reduction of systemic risk in financial markets\nthrough Pigouvian taxation of financial institutions which is used to support\nthe rescue fund. We introduce the concept of the cascade risk with a clear\noperational definition as a subclass and a network related measure of the\nsystemic risk. Using financial networks constructed from real Italian money\nmarket data and using realistic parameters, we show that the cascade risk can\nbe substantially reduced by a small rate of taxation and by means of a simple\nstrategy of the money transfer from the rescue fund to interbanking market\nsubjects. Furthermore, we show that while negative effects on the return on\ninvestment ($ROI$) are direct and certain, an overall positive effect on risk\nadjusted return on investments ($ROI^{RA}$) is visible. Please note that\n\\emph{the taxation} is introduced as a monetary/regulatory, not as a fiscal\nmeasure, as the term could suggest. \\emph{The rescue fund} is implemented in a\nform of a common reserve fund.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 06:40:44 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Zlati\u0107", "Vinko", ""], ["Gabbi", "Giampaolo", ""], ["Abraham", "Hrvoje", ""]]}, {"id": "1406.6038", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Exact fit of simple finite mixture models", "comments": "16 pages, 2 tables, some corrections, section on cost quantification\n  inserted", "journal-ref": "Journal of Risk and Financial Management 7(4), 150-164, 2014", "doi": "10.3390/jrfm7040150", "report-no": null, "categories": "stat.ML q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to forecast next year's portfolio-wide credit default rate based on last\nyear's default observations and the current score distribution? A classical\napproach to this problem consists of fitting a mixture of the conditional score\ndistributions observed last year to the current score distribution. This is a\nspecial (simple) case of a finite mixture model where the mixture components\nare fixed and only the weights of the components are estimated. The optimum\nweights provide a forecast of next year's portfolio-wide default rate. We point\nout that the maximum-likelihood (ML) approach to fitting the mixture\ndistribution not only gives an optimum but even an exact fit if we allow the\nmixture components to vary but keep their density ratio fix. From this\nobservation we can conclude that the standard default rate forecast based on\nlast year's conditional default rates will always be located between last\nyear's portfolio-wide default rate and the ML forecast for next year. As an\napplication example, then cost quantification is discussed. We also discuss how\nthe mixture model based estimation methods can be used to forecast total loss.\nThis involves the reinterpretation of an individual classification problem as a\ncollective quantification problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 19:45:40 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 20:33:26 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1406.6142", "submitter": "Andreas Lager{\\aa}s", "authors": "Andreas Lager{\\aa}s", "title": "How to hedge extrapolated yield curves", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework on how to hedge the interest rate sensitivity of\nliabilities discounted by an extrapolated yield curve. The framework is based\non functional analysis in that we consider the extrapolated yield curve as a\nfunctional of an observed yield curve and use its G\\^ateaux variation to\nunderstand the sensitivity to any possible yield curve shift. We apply the\nframework to analyse the Smith-Wilson method of extrapolation that is proposed\nby the European Insurance and Occupational Pensions Authority (EIOPA) in the\ncoming EU legislation Solvency II, and the method recently introduced, and\ncurrently prescribed, by the Swedish Financial Supervisory Authority.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 05:56:33 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Lager\u00e5s", "Andreas", ""]]}, {"id": "1406.6575", "submitter": "Oliver Kley", "authors": "Oliver Kley and Claudia Kl\\\"uppelberg and Lukas Reichel", "title": "Systemic risk through contagion in a core-periphery structured banking\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute to the understanding of how systemic risk arises in a network\nof credit-interlinked agents. Motivated by empirical studies we formulate a\nnetwork model which, despite its simplicity, depicts the nature of interbank\nmarkets better than a homogeneous model. The components of a vector\nOrnstein-Uhlenbeck process living on the vertices of the network describe the\nfinancial robustnesses of the agents. For this system, we prove a LLN for\ngrowing network size leading to a propagation of chaos result. We state\nproperties, which arise from such a structure, and examine the effect of\ninhomogeneity on several risk management issues and the possibility of\ncontagion.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 14:05:51 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Kley", "Oliver", ""], ["Kl\u00fcppelberg", "Claudia", ""], ["Reichel", "Lukas", ""]]}, {"id": "1406.6952", "submitter": "Hassan Omidi Firouzi", "authors": "Zied Ben-Salah, H\\'el\\`ene Gu\\'erin, Manuel Morales and Hassan Omidi\n  Firouzi", "title": "On the Depletion Problem for an Insurance Risk Process: New Non-ruin\n  Quantities in Collective Risk Theory", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of risk theory has traditionally focused on ruin-related\nquantities. In particular, the socalled Expected Discounted Penalty Function\nhas been the object of a thorough study over the years. Although interesting in\ntheir own right, ruin related quantities do not seem to capture path-dependent\nproperties of the reserve. In this article we aim at presenting the\nprobabilistic properties of drawdowns and the speed at which an insurance\nreserve depletes as a consequence of the risk exposure of the company. These\nnew quantities are not ruin related yet they capture important features of an\ninsurance position and we believe it can lead to the design of a meaningful\nrisk measures. Studying drawdowns and speed of depletion for L\\'evy insurance\nrisk processes represent a novel and challenging concept in insurance\nmathematics. In this paper, all these concepts are formally introduced in an\ninsurance setting. Moreover, using recent results in fluctuation theory for\nL\\'evy processes, we derive expressions for the distribution of several\nquantities related to the depletion problem. Of particular interest are the\ndistribution of drawdowns and the Laplace transform for the speed of depletion.\nThese expressions are given for some examples of L\\'evy insurance risk\nprocesses for which they can be calculated, in particular for the classical\nCramer-Lundberg model.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 17:37:19 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Ben-Salah", "Zied", ""], ["Gu\u00e9rin", "H\u00e9l\u00e8ne", ""], ["Morales", "Manuel", ""], ["Firouzi", "Hassan Omidi", ""]]}, {"id": "1406.7752", "submitter": "Peter Sarlin", "authors": "Samuel R\\\"onnqvist and Peter Sarlin", "title": "Bank Networks from Text: Interrelations, Centrality and Determinants", "comments": "Quantitative Finance, forthcoming in 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the wake of the still ongoing global financial crisis, bank\ninterdependencies have come into focus in trying to assess linkages among banks\nand systemic risk. To date, such analysis has largely been based on numerical\ndata. By contrast, this study attempts to gain further insight into bank\ninterconnections by tapping into financial discourse. We present a\ntext-to-network process, which has its basis in co-occurrences of bank names\nand can be analyzed quantitatively and visualized. To quantify bank importance,\nwe propose an information centrality measure to rank and assess trends of bank\ncentrality in discussion. For qualitative assessment of bank networks, we put\nforward a visual, interactive interface for better illustrating network\nstructures. We illustrate the text-based approach on European Large and Complex\nBanking Groups (LCBGs) during the ongoing financial crisis by quantifying bank\ninterrelations and centrality from discussion in 3M news articles, spanning\n2007Q1 to 2014Q3.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 14:23:40 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2015 03:32:48 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1406.7775", "submitter": "Maria Sousa", "authors": "Maria Rocha Sousa, Jo\\~ao Gama, Manuel J. Silva Gon\\c{c}alves", "title": "A two-stage model for dealing with temporal degradation of credit\n  scoring", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is attached to the BRICS 2013 competition. We propose a two-stage\nmodel for dealing with the temporal degradation of credit scoring models. This\nmethodology produced motivating results in a 1-year horizon. We anticipate that\nit can be extended to other applications of risk assessment with great success.\nFuture extensions should cover predictions in larger time frames and consider\nlagged periods. This methodology can be further improved if more information\nabout the economic cycles is integrated in the forecasting of default.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 15:32:00 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Sousa", "Maria Rocha", ""], ["Gama", "Jo\u00e3o", ""], ["Gon\u00e7alves", "Manuel J. Silva", ""]]}]