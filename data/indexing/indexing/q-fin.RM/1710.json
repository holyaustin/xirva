[{"id": "1710.00859", "submitter": "Jinglun Yao", "authors": "Jinglun Yao, Sabine Laurent, Brice B\\'enaben", "title": "Managing Volatility Risk: An Application of Karhunen-Lo\\`eve\n  Decomposition and Filtered Historical Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Implied volatilities form a well-known structure of smile or surface which\naccommodates the Bachelier model and observed market prices of interest rate\noptions. For the swaptions that we study, three parameters are taken into\naccount for indexing the implied volatilities and form a \"volatility cube\":\nstrike (or moneyness), time to maturity of the option contract, duration of the\nunderlying swap contract. It should be noted that the implied volatility\nstructure changes across time, which makes it important to study its dynamics\nin order to well manage the volatility risk. As volatilities are correlated\nacross the cube, it is preferable to decompose the dynamics on orthogonal\nprincipal components, which is the idea of Karhunen-Lo\\`eve decomposition that\nwe have adopted in the article. The projections on principal components are\ninvestigated by Filtered Historical Simulation in order to predict the Value at\nRisk (VaR), which is then examined by standard tests and non-arbitrage\ncondition to ensure its appropriateness.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 18:37:17 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Yao", "Jinglun", ""], ["Laurent", "Sabine", ""], ["B\u00e9naben", "Brice", ""]]}, {"id": "1710.01503", "submitter": "Chung-Han Hsieh", "authors": "Chung-Han Hsieh and B. Ross Barmish", "title": "On Drawdown-Modulated Feedback Control in Stock Trading", "comments": "Proc. 20th IFAC World Congress (IFAC WC 2017), Toulouse, France, July\n  9-14, 2017 (accepted)", "journal-ref": "IFAC-PapersOnLine Volume 50, Issue 1, 2017", "doi": "10.1016/j.ifacol.2017.08.167", "report-no": null, "categories": "math.OC q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control of drawdown, that is, the control of the drops in wealth over time\nfrom peaks to subsequent lows, is of great concern from a risk management\nperspective. With this motivation in mind, the focal point of this paper is to\naddress the drawdown issue in a stock trading context. Although our analysis\ncan be carried out without reference to control theory, to make the work\naccessible to this community, we use the language of feedback systems. The\ntakeoff point for the results to follow, which we call the Drawdown Modulation\nLemma, characterizes any investment which guarantees that the percentage\ndrawdown is no greater than a prespecified level with probability one. With the\naid of this lemma, we introduce a new scheme which we call the\ndrawdown-modulated feedback control. To illustrate the power of the theory, we\nconsider a drawdown-constrained version of the well-known Kelly Optimization\nProblem which involves maximizing the expected logarithmic growth of the\ntrader's account value. As the drawdown parameter dmax in our new formulation\ntends to one, we recover existing results as a special case. This new theory\nleads to an optimal investment strategy whose application is illustrated via an\nexample with historical stock-price data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 08:24:28 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Hsieh", "Chung-Han", ""], ["Barmish", "B. Ross", ""]]}, {"id": "1710.01578", "submitter": "Steffen Schuldenzucker", "authors": "Steffen Schuldenzucker, Sven Seuken, Stefano Battiston", "title": "The Computational Complexity of Financial Networks with Credit Default\n  Swaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.CC cs.GT q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2008 financial crisis has been attributed to \"excessive complexity\" of\nthe financial system due to financial innovation. We employ computational\ncomplexity theory to make this notion precise. Specifically, we consider the\nproblem of clearing a financial network after a shock. Prior work has shown\nthat when banks can only enter into simple debt contracts with each other, then\nthis problem can be solved in polynomial time. In contrast, if they can also\nenter into credit default swaps (CDSs), i.e., financial derivative contracts\nthat depend on the default of another bank, a solution may not even exist.\n  In this work, we show that deciding if a solution exists is NP-complete if\nCDSs are allowed. This remains true if we relax the problem to\n$\\varepsilon$-approximate solutions, for a constant $\\varepsilon$. We further\nshow that, under sufficient conditions where a solution is guaranteed to exist,\nthe approximate search problem is PPAD-complete for constant $\\varepsilon$. We\nthen try to isolate the \"origin\" of the complexity. It turns out that already\ndetermining which banks default is hard. Further, we show that the complexity\nis not driven by the dependence of counterparties on each other, but rather\nhinges on the presence of so-called naked CDSs. If naked CDSs are not present,\nwe receive a simple polynomial-time algorithm. Our results are of practical\nimportance for regulators' stress tests and regulatory policy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 12:47:49 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 12:54:48 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 12:45:20 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Schuldenzucker", "Steffen", ""], ["Seuken", "Sven", ""], ["Battiston", "Stefano", ""]]}, {"id": "1710.01786", "submitter": "Chung-Han Hsieh", "authors": "Chung-Han Hsieh, B. Ross Barmish, and John A. Gubner", "title": "Kelly Betting Can Be Too Conservative", "comments": "Accepted in 2016 IEEE 55th Conference on Decision and Control (CDC)", "journal-ref": "Proceedings of the IEEE Conference on Decision and Control (CDC),\n  pp .3695-3701, 2016", "doi": "10.1109/CDC.2016.7798825", "report-no": null, "categories": "q-fin.PM math.OC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kelly betting is a prescription for optimal resource allocation among a set\nof gambles which are typically repeated in an independent and identically\ndistributed manner. In this setting, there is a large body of literature which\nincludes arguments that the theory often leads to bets which are \"too\naggressive\" with respect to various risk metrics. To remedy this problem, many\npapers include prescriptions for scaling down the bet size. Such schemes are\nreferred to as Fractional Kelly Betting. In this paper, we take the opposite\ntack. That is, we show that in many cases, the theoretical Kelly-based results\nmay lead to bets which are \"too conservative\" rather than too aggressive. To\nmake this argument, we consider a random vector X with its assumed probability\ndistribution and draw m samples to obtain an empirically-derived counterpart\nXhat. Subsequently, we derive and compare the resulting Kelly bets for both X\nand Xhat with consideration of sample size m as part of the analysis. This\nleads to identification of many cases which have the following salient feature:\nThe resulting bet size using the true theoretical distribution for X is much\nsmaller than that for Xhat. If instead the bet is based on empirical data,\n\"golden\" opportunities are identified which are essentially rejected when the\npurely theoretical model is used. To formalize these ideas, we provide a result\nwhich we call the Restricted Betting Theorem. An extreme case of the theorem is\nobtained when X has unbounded support. In this situation, using X, the Kelly\ntheory can lead to no betting at all.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 20:01:54 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Hsieh", "Chung-Han", ""], ["Barmish", "B. Ross", ""], ["Gubner", "John A.", ""]]}, {"id": "1710.01787", "submitter": "Chung-Han Hsieh", "authors": "Chung-Han Hsieh and B. Ross Barmish", "title": "On Kelly Betting: Some Limitations", "comments": "Accepted in 53rd Annual Allerton Conference on Communication,\n  Control, and Computing, 2015", "journal-ref": "Proceedings of the Annual Allerton Conference on Communication,\n  Control, and Computing, pp.165-172, 2015", "doi": "10.1109/ALLERTON.2015.7447000", "report-no": null, "categories": "math.OC q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focal point of this paper is the so-called Kelly Criterion, a\nprescription for optimal resource allocation among a set of gambles which are\nrepeated over time. The criterion calls for maximization of the expected value\nof the logarithmic growth of wealth. While significant literature exists\nproviding the rationale for such an optimization, this paper concentrates on\nthe limitations of the Kelly-based theory. To this end, we fill a void in\npublished results by providing specific examples quantifying what difficulties\nare encountered when Taylor-style approximations are used and when wealth\ndrawdowns are considered. For the case of drawdown, we describe some research\ndirections which we feel are promising for improvement of the theory.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 20:05:32 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Hsieh", "Chung-Han", ""], ["Barmish", "B. Ross", ""]]}, {"id": "1710.02127", "submitter": "Yang Xu", "authors": "Yang Xu", "title": "Intervention On Default Contagion Under Partial Information", "comments": "63 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the default contagion process in a large heterogeneous financial\nnetwork under the interventions of a regulator (a central bank) with only\npartial information which is a more realistic setting than most current\nliterature. We provide the analytical results for the asymptotic optimal\nintervention policies and the asymptotic magnitude of default contagion in\nterms of the network characteristics. We extend the results of Amini et al.\n(2013) to incorporate interventions and the model of Amini et al. (2015); Amini\net al. (2017) to heterogeneous networks with a given degree sequence and\narbitrary initial equity levels. The insights from the results are that the\noptimal intervention policy is \"monotonic\" in terms of the intervention cost,\nthe closeness to invulnerability and connectivity. Moreover, we should keep\nintervening on a bank once we have intervened on it. Our simulation results\nshow a good agreement with the theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 17:39:47 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Xu", "Yang", ""]]}, {"id": "1710.03161", "submitter": "Chris Kenyon", "authors": "Chris Kenyon and Mourad Berrahoui and Benjamin Poncet", "title": "Counterparty Trading Limits Revisited:CSAs, IM, SwapAgent(r), from PFE\n  to PFL", "comments": "14 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utility of Potential Future Exposure (PFE) for counterparty trading\nlimits is being challenged by new market developments, notably widespread\nregulatory Initial Margin (using 99% 10-day exposure), and netting of trade and\ncollateral flows. However PFE has pre-existing challenges w.r.t.\nportfolios/distributions, collateralization, netting set seniority, and\noverlaps with CVA. We introduce Potential Future Loss (PFL) which combines\nexpected shortfall (ES) and loss given default (LGD) as a replacement for PFE.\nWith two additional variants Adjusted PFL (aPFL) and Protected Adjusted PFL\n(paPFL) these deal with both new and pre-existing challenges. We provide a\ntheoretical background and numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 15:56:22 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 19:57:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kenyon", "Chris", ""], ["Berrahoui", "Mourad", ""], ["Poncet", "Benjamin", ""]]}, {"id": "1710.03252", "submitter": "Valeria Bignozzi", "authors": "Valeria Bignozzi, Claudio Macci, Lea Petrella", "title": "Large deviations for risk measures in finite mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their heterogeneity, insurance risks can be properly described as a\nmixture of different fixed models, where the weights assigned to each model may\nbe estimated empirically from a sample of available data. If a risk measure is\nevaluated on the estimated mixture instead of the (unknown) true one, then it\nis important to investigate the committed error. In this paper we study the\nasymptotic behaviour of estimated risk measures, as the data sample size tends\nto infinity, in the fashion of large deviations. We obtain large deviation\nresults by applying the contraction principle, and the rate functions are given\nby a suitable variational formula; explicit expressions are available for\nmixtures of two models. Finally, our results are applied to the most common\nrisk measures, namely the quantiles, the Expected Shortfall and the shortfall\nrisk measures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 18:20:03 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 08:16:50 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Bignozzi", "Valeria", ""], ["Macci", "Claudio", ""], ["Petrella", "Lea", ""]]}, {"id": "1710.04818", "submitter": "Stanislaus  Maier-Paape", "authors": "Stanislaus Maier-Paape and Qiji Jim Zhu", "title": "A General Framework for Portfolio Theory. Part II: drawdown risk\n  measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to provide several examples of convex risk measures\nnecessary for the application of the general framework for portfolio theory of\nMaier-Paape and Zhu, presented in Part I of this series (arXiv:1710.04579\n[q-fin.PM]). As alternative to classical portfolio risk measures such as the\nstandard deviation we in particular construct risk measures related to the\ncurrent drawdown of the portfolio equity. Combined with the results of Part I\n(arXiv:1710.04579 [q-fin.PM]), this allows us to calculate efficient portfolios\nbased on a drawdown risk measure constraint.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 06:20:49 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Maier-Paape", "Stanislaus", ""], ["Zhu", "Qiji Jim", ""]]}, {"id": "1710.05204", "submitter": "Mike Ludkovski", "authors": "Michael Ludkovski and James Risk", "title": "Sequential Design and Spatial Modeling for Portfolio Tail Risk\n  Measurement", "comments": "40 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider calculation of capital requirements when the underlying economic\nscenarios are determined by simulatable risk factors. In the respective nested\nsimulation framework, the goal is to estimate portfolio tail risk, quantified\nvia VaR or TVaR of a given collection of future economic scenarios representing\nfactor levels at the risk horizon. Traditionally, evaluating portfolio losses\nof an outer scenario is done by computing a conditional expectation via\ninner-level Monte Carlo and is computationally expensive. We introduce several\ninter-related machine learning techniques to speed up this computation, in\nparticular by properly accounting for the simulation noise. Our main workhorse\nis an advanced Gaussian Process (GP) regression approach which uses\nnonparametric spatial modeling to efficiently learn the relationship between\nthe stochastic factors defining scenarios and corresponding portfolio value.\nLeveraging this emulator, we develop sequential algorithms that adaptively\nallocate inner simulation budgets to target the quantile region. The GP\nframework also yields better uncertainty quantification for the resulting\nVaR/TVaR estimators that reduces bias and variance compared to existing\nmethods. We illustrate the proposed strategies with two case-studies in two and\nsix dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 16:12:23 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 13:24:49 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Ludkovski", "Michael", ""], ["Risk", "James", ""]]}, {"id": "1710.10692", "submitter": "Wenhao Li", "authors": "Wenhao Li, Bolong Wang, Tianxiang Shen, Ronghua Zhu, Dehui Wang", "title": "Research on ruin probability of risk model based on AR(1) series", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this text, we establish the risk model based on AR(1) series and propose\nthe basic model which has a dependent structure under intensity of claim\nnumber. Considering some properties of the risk model, we take advantage of\nnewton iteration method to figure out the adjustment coefficient and estimate\nthe exponential upper bound of ruin probability. This is significant to refine\nthe research of ruin theory. As a result, our theory will help develop\ninsurance industry stably.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 21:05:32 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Li", "Wenhao", ""], ["Wang", "Bolong", ""], ["Shen", "Tianxiang", ""], ["Zhu", "Ronghua", ""], ["Wang", "Dehui", ""]]}, {"id": "1710.10980", "submitter": "Giulio Cimini", "authors": "Matteo Serafino, Andrea Gabrielli, Guido Caldarelli, Giulio Cimini", "title": "Statistical validation of financial time series via visibility graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical physics of complex systems exploits network theory not only to\nmodel, but also to effectively extract information from many dynamical\nreal-world systems. A pivotal case of study is given by financial systems:\nmarket prediction represents an unsolved scientific challenge yet with crucial\nimplications for society, as financial crises have devastating effects on real\neconomies. Thus, nowadays the quest for a robust estimator of market efficiency\nis both a scientific and institutional priority. In this work we study the\nvisibility graphs built from the time series of several trade market indices.\nWe propose a validation procedure for each link of these graphs against a null\nhypothesis derived from ARCH-type modeling of such series. Building on this\nframework, we devise a market indicator that turns out to be highly correlated\nand even predictive of financial instability periods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 14:38:04 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Serafino", "Matteo", ""], ["Gabrielli", "Andrea", ""], ["Caldarelli", "Guido", ""], ["Cimini", "Giulio", ""]]}, {"id": "1710.11065", "submitter": "Jose Garrido", "authors": "Zied Ben Salah, Jos\\'e Garrido", "title": "On Fair Reinsurance Premiums; Capital Injections in a Perturbed Risk\n  Model", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a risk model where deficits after ruin are covered by a new type\nof reinsurance contract that provides capital injections. To allow the\ninsurance company's survival after ruin, the reinsurer injects capital only at\nruin times caused by jumps larger than a chosen retention level. Otherwise\ncapital must be raised from the shareholders for small deficits. The problem\nhere is to determine adequate reinsurance premiums. It seems fair to base the\nnet reinsurance premium on the discounted expected value of any future capital\ninjections. Inspired by the results of Huzak et al. (2004) and Ben Salah (2014)\non successive ruin events, we show that an explicit formula for these\nreinsurance premiums exists in a setting where aggregate claims are modeled by\na subordinator and a Brownian perturbation. Here ruin events are due either to\nBrownian oscillations or jumps and reinsurance capital injections only apply in\nthe latter case. The results are illustrated explicitly for two specific risk\nmodels and in some numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:56:46 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 16:54:48 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 09:11:26 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 12:15:21 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Salah", "Zied Ben", ""], ["Garrido", "Jos\u00e9", ""]]}, {"id": "1710.11512", "submitter": "Teruyoshi Kobayashi", "authors": "Fabio Caccioli, Paolo Barucca, and Teruyoshi Kobayashi", "title": "Network models of financial systemic risk: A review", "comments": "33 pages, 6 figures", "journal-ref": "Journal of Computational Social Science (2018) 1: 81-114", "doi": "10.1007/s42001-017-0008-3", "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global financial system can be represented as a large complex network in\nwhich banks, hedge funds and other financial institutions are interconnected to\neach other through visible and invisible financial linkages. Recently, a lot of\nattention has been paid to the understanding of the mechanisms that can lead to\na breakdown of this network. This can happen when the existing financial links\nturn from being a means of risk diversification to channels for the propagation\nof risk across financial institutions. In this review article, we summarize\nrecent developments in the modeling of financial systemic risk. We focus in\nparticular on network approaches, such as models of default cascades due to\nbilateral exposures or to overlapping portfolios, and we also report on recent\nfindings on the empirical structure of interbank networks. The current review\nprovides a landscape of the newly arising interdisciplinary field lying at the\nintersection of several disciplines, such as network science, physics,\nengineering, economics, and ecology.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 14:34:25 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Caccioli", "Fabio", ""], ["Barucca", "Paolo", ""], ["Kobayashi", "Teruyoshi", ""]]}]