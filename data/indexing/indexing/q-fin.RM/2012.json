[{"id": "2012.03200", "submitter": "Wing Fung Chong", "authors": "Xiaowei Chen, Wing Fung Chong, Runhuan Feng, Linfeng Zhang", "title": "Pandemic risk management: resources contingency planning and allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.GN math.OC q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Repeated history of pandemics, such as SARS, H1N1, Ebola, Zika, and COVID-19,\nhas shown that pandemic risk is inevitable. Extraordinary shortages of medical\nresources have been observed in many parts of the world. Some attributing\nfactors include the lack of sufficient stockpiles and the lack of coordinated\nefforts to deploy existing resources to the location of greatest needs. The\npaper investigates contingency planning and resources allocation from a risk\nmanagement perspective, as opposed to the prevailing supply chain perspective.\nThe key idea is that the competition of limited critical resources is not only\npresent in different geographical locations but also at different stages of a\npandemic. This paper draws on an analogy between risk aggregation and capital\nallocation in finance and pandemic resources planning and allocation for\nhealthcare systems. The main contribution is to introduce new strategies for\noptimal stockpiling and allocation balancing spatio-temporal competitions of\nmedical supply and demand.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 07:32:01 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Chen", "Xiaowei", ""], ["Chong", "Wing Fung", ""], ["Feng", "Runhuan", ""], ["Zhang", "Linfeng", ""]]}, {"id": "2012.03744", "submitter": "Bojing Feng", "authors": "Bojing Feng, Wenfang Xue, Bindang Xue, Zeyu Liu", "title": "Every Corporation Owns Its Image: Corporate Credit Ratings via\n  Convolutional Neural Networks", "comments": "6 pages. arXiv admin note: text overlap with arXiv:2012.01933", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit rating is an analysis of the credit risks associated with a\ncorporation, which reflect the level of the riskiness and reliability in\ninvesting. There have emerged many studies that implement machine learning\ntechniques to deal with corporate credit rating. However, the ability of these\nmodels is limited by enormous amounts of data from financial statement reports.\nIn this work, we analyze the performance of traditional machine learning models\nin predicting corporate credit rating. For utilizing the powerful convolutional\nneural networks and enormous financial data, we propose a novel end-to-end\nmethod, Corporate Credit Ratings via Convolutional Neural Networks, CCR-CNN for\nbrevity. In the proposed model, each corporation is transformed into an image.\nBased on this image, CNN can capture complex feature interactions of data,\nwhich are difficult to be revealed by previous machine learning models.\nExtensive experiments conducted on the Chinese public-listed corporate rating\ndataset which we build, prove that CCR-CNN outperforms the state-of-the-art\nmethods consistently.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 01:01:34 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Feng", "Bojing", ""], ["Xue", "Wenfang", ""], ["Xue", "Bindang", ""], ["Liu", "Zeyu", ""]]}, {"id": "2012.03749", "submitter": "Lara Marie Demajo Ms", "authors": "Lara Marie Demajo, Vince Vella and Alexiei Dingli", "title": "Explainable AI for Interpretable Credit Scoring", "comments": "19 pages, David C. Wyld et al. (Eds): ACITY, DPPR, VLSI, WeST, DSA,\n  CNDC, IoTE, AIAA, NLPTA - 2020", "journal-ref": null, "doi": "10.5121/csit.2020.101516", "report-no": null, "categories": "q-fin.RM cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the ever-growing achievements in Artificial Intelligence (AI) and the\nrecent boosted enthusiasm in Financial Technology (FinTech), applications such\nas credit scoring have gained substantial academic interest. Credit scoring\nhelps financial experts make better decisions regarding whether or not to\naccept a loan application, such that loans with a high probability of default\nare not accepted. Apart from the noisy and highly imbalanced data challenges\nfaced by such credit scoring models, recent regulations such as the `right to\nexplanation' introduced by the General Data Protection Regulation (GDPR) and\nthe Equal Credit Opportunity Act (ECOA) have added the need for model\ninterpretability to ensure that algorithmic decisions are understandable and\ncoherent. An interesting concept that has been recently introduced is\neXplainable AI (XAI), which focuses on making black-box models more\ninterpretable. In this work, we present a credit scoring model that is both\naccurate and interpretable. For classification, state-of-the-art performance on\nthe Home Equity Line of Credit (HELOC) and Lending Club (LC) Datasets is\nachieved using the Extreme Gradient Boosting (XGBoost) model. The model is then\nfurther enhanced with a 360-degree explanation framework, which provides\ndifferent explanations (i.e. global, local feature-based and local\ninstance-based) that are required by different people in different situations.\nEvaluation through the use of functionallygrounded, application-grounded and\nhuman-grounded analysis show that the explanations provided are simple,\nconsistent as well as satisfy the six predetermined hypotheses testing for\ncorrectness, effectiveness, easy understanding, detail sufficiency and\ntrustworthiness.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:44:03 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Demajo", "Lara Marie", ""], ["Vella", "Vince", ""], ["Dingli", "Alexiei", ""]]}, {"id": "2012.03798", "submitter": "Bahman Angoshtari", "authors": "Bahman Angoshtari, Virginia R. Young", "title": "Optimal Insurance to Minimize the Probability of Ruin: Inverse Survival\n  Function Formulation", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We find the optimal indemnity to minimize the probability of ruin when\npremium is calculated according to the distortion premium principle with a\nproportional risk load, and admissible indemnities are such that both the\nindemnity and retention are non-decreasing functions of the underlying loss. We\nreformulate the problem with the inverse survival function as the control\nvariable and show that deductible insurance with maximum limit is optimal. Our\nmain contribution is in solving this problem via the inverse survival function.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:39:43 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Angoshtari", "Bahman", ""], ["Young", "Virginia R.", ""]]}, {"id": "2012.04181", "submitter": "Kyungsub Lee", "authors": "Hyun Jin Jang, Kiseop Lee, Kyungsub Lee", "title": "Systemic Risk in Market Microstructure of Crude Oil and Gasoline Futures\n  Prices: A Hawkes Flocking Model Approach", "comments": null, "journal-ref": "Journal of Futures Markets, 40, 2020, 247-275", "doi": "10.1002/fut.22048", "report-no": null, "categories": "q-fin.TR q-fin.RM q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the Hawkes flocking model that assesses systemic risk in\nhigh-frequency processes at the two perspectives -- endogeneity and\ninteractivity. We examine the futures markets of WTI crude oil and gasoline for\nthe past decade, and perform a comparative analysis with conditional\nvalue-at-risk as a benchmark measure. In terms of high-frequency structure, we\nderive the empirical findings. The endogenous systemic risk in WTI was\nsignificantly higher than that in gasoline, and the level at which gasoline\naffects WTI was constantly higher than in the opposite case. Moreover, although\nthe relative influence's degree was asymmetric, its difference has gradually\nreduced.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 02:54:05 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Jang", "Hyun Jin", ""], ["Lee", "Kiseop", ""], ["Lee", "Kyungsub", ""]]}, {"id": "2012.04364", "submitter": "Karim Barigou", "authors": "Karim Barigou (SAF), Valeria Bignozzi, Andreas Tsanakas", "title": "Insurance valuation: A two-step generalised regression approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to fair valuation in insurance often follow a two-step\napproach, combining quadratic hedging with application of a risk measure on the\nresidual liability, to obtain a cost-of-capital margin. In such approaches, the\npreferences represented by the regulatory risk measure are not reflected in the\nhedging process. We address this issue by an alternative two-step hedging\nprocedure, based on generalised regression arguments, which leads to portfolios\nthat are neutral with respect to a risk measure, such as Value-at-Risk or the\nexpectile. First, a portfolio of traded assets aimed at replicating the\nliability is determined by local quadratic hedging. Second, the residual\nliability is hedged using an alternative objective function. The risk margin is\nthen defined as the cost of the capital required to hedge the residual\nliability. In the case quantile regression is used in the second step, yearly\nsolvency constraints are naturally satisfied; furthermore, the portfolio is a\nrisk minimiser among all hedging portfolios that satisfy such constraints. We\npresent a neural network algorithm for the valuation and hedging of insurance\nliabilities based on a backward iterations scheme. The algorithm is fairly\ngeneral and easily applicable, as it only requires simulated paths of risk\ndrivers.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:20:39 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Barigou", "Karim", "", "SAF"], ["Bignozzi", "Valeria", ""], ["Tsanakas", "Andreas", ""]]}, {"id": "2012.04500", "submitter": "Sebastian Jaimungal", "authors": "Silvana Pesenti and Sebastian Jaimungal", "title": "Portfolio Optimisation within a Wasserstein Ball", "comments": "37 pages, 2 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.PM q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of active portfolio management where an investor aims to\noutperform a benchmark strategy's risk profile while not deviating too far from\nit. Specifically, an investor considers alternative strategies whose terminal\nwealth lie within a Wasserstein ball surrounding a benchmark's -- being\ndistributionally close -- and that have a specified dependence/copula -- tying\nstate-by-state outcomes -- to it. The investor then chooses the alternative\nstrategy that minimises a distortion risk measure of terminal wealth. In a\ngeneral (complete) market model, we prove that an optimal dynamic strategy\nexists and provide its characterisation through the notion of isotonic\nprojections.\n  We further propose a simulation approach to calculate the optimal strategy's\nterminal wealth, making our approach applicable to a wide range of market\nmodels. Finally, we illustrate how investors with different copula and risk\npreferences invest and improve upon the benchmark using the Tail Value-at-Risk,\ninverse S-shaped, and lower- and upper-tail distortion risk measures as\nexamples. We find that investors' optimal terminal wealth distribution has\nlarger probability masses in regions that reduce their risk measure relative to\nthe benchmark while preserving the benchmark's structure.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:38:05 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 17:28:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pesenti", "Silvana", ""], ["Jaimungal", "Sebastian", ""]]}, {"id": "2012.04506", "submitter": "Victor Olkhov", "authors": "Victor Olkhov", "title": "Business Cycles as Collective Risk Fluctuations", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We suggest use continuous numerical risk grades [0,1] of R for a single risk\nor the unit cube in Rn for n risks as the economic domain. We consider risk\nratings of economic agents as their coordinates in the economic domain.\nEconomic activity of agents, economic or other factors change agents risk\nratings and that cause motion of agents in the economic domain. Aggregations of\nvariables and transactions of individual agents in small volume of economic\ndomain establish the continuous economic media approximation that describes\ncollective variables, transactions and their flows in the economic domain as\nfunctions of risk coordinates. Any economic variable A(t,x) defines mean risk\nXA(t) as risk weighted by economic variable A(t,x). Collective flows of\neconomic variables in bounded economic domain fluctuate from secure to risky\narea and back. These fluctuations of flows cause time oscillations of\nmacroeconomic variables A(t) and their mean risks XA(t) in economic domain and\nare the origin of any business and credit cycles. We derive equations that\ndescribe evolution of collective variables, transactions and their flows in the\neconomic domain. As illustration we present simple self-consistent equations of\nsupply-demand cycles that describe fluctuations of supply, demand and their\nmean risks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:45:11 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Olkhov", "Victor", ""]]}, {"id": "2012.04521", "submitter": "Alexander Glauner", "authors": "Nicole B\\\"auerle and Alexander Glauner", "title": "Minimizing Spectral Risk Measures Applied to Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimization of a spectral risk measure of the total discounted\ncost generated by a Markov Decision Process (MDP) over a finite or infinite\nplanning horizon. The MDP is assumed to have Borel state and action spaces and\nthe cost function may be unbounded above. The optimization problem is split\ninto two minimization problems using an infimum representation for spectral\nrisk measures. We show that the inner minimization problem can be solved as an\nordinary MDP on an extended state space and give sufficient conditions under\nwhich an optimal policy exists. Regarding the infinite dimensional outer\nminimization problem, we prove the existence of a solution and derive an\nalgorithm for its numerical approximation. Our results include the findings in\nB\\\"auerle and Ott (2011) in the special case that the risk measure is Expected\nShortfall. As an application, we present a dynamic extension of the classical\nstatic optimal reinsurance problem, where an insurance company minimizes its\ncost of capital.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:10:38 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["B\u00e4uerle", "Nicole", ""], ["Glauner", "Alexander", ""]]}, {"id": "2012.05219", "submitter": "Yunran Wei", "authors": "Fabio Bellini, Tolulope Fadina, Ruodu Wang, Yunran Wei", "title": "Parametric measures of variability induced by risk measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study general classes of parametric measures of variability with\napplications in risk management. Particular focus is put on variability\nmeasures induced by three classes of popular risk measures: the Value-at-Risk,\nthe Expected Shortfall, and the expectiles. Properties of these variability\nmeasures are explored in detail, and a characterization result is obtained via\nthe mixture of inter-ES differences. Convergence properties and asymptotic\nnormality of their empirical estimators are established. We provide an\nillustration of the three classes of variability measures applied to financial\ndata and analyze their relative advantages.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:29:53 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Bellini", "Fabio", ""], ["Fadina", "Tolulope", ""], ["Wang", "Ruodu", ""], ["Wei", "Yunran", ""]]}, {"id": "2012.05757", "submitter": "Vincent Tan", "authors": "Vincent W. C. Tan, Stefan Zohren", "title": "Large Non-Stationary Noisy Covariance Matrices: A Cross-Validation\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel covariance estimator that exploits the heteroscedastic\nnature of financial time series by employing exponential weighted moving\naverages and shrinking the in-sample eigenvalues through cross-validation. Our\nestimator is model-agnostic in that we make no assumptions on the distribution\nof the random entries of the matrix or structure of the covariance matrix.\nAdditionally, we show how Random Matrix Theory can provide guidance for\nautomatic tuning of the hyperparameter which characterizes the time scale for\nthe dynamics of the estimator. By attenuating the noise from both the\ncross-sectional and time-series dimensions, we empirically demonstrate the\nsuperiority of our estimator over competing estimators that are based on\nexponentially-weighted and uniformly-weighted covariance matrices.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:41:17 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Tan", "Vincent W. C.", ""], ["Zohren", "Stefan", ""]]}, {"id": "2012.06173", "submitter": "\\c{C}a\\u{g}{\\i}n Ararat", "authors": "\\c{C}a\\u{g}{\\i}n Ararat", "title": "Portfolio optimization with two quasiconvex risk measures", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a static portfolio optimization problem with two risk measures: a\nprinciple risk measure in the objective function and a secondary risk measure\nwhose value is controlled in the constraints. This problem is of interest when\nit is necessary to consider the risk preferences of two parties, such as a\nportfolio manager and a regulator, at the same time. A special case of this\nproblem where the risk measures are assumed to be coherent (positively\nhomogeneous) is studied recently in a joint work of the author. The present\npaper extends the analysis to a more general setting by assuming that the two\nrisk measures are only quasiconvex. First, we study the case where the\nprincipal risk measure is convex. We introduce a dual problem, show that there\nis zero duality gap between the portfolio optimization problem and the dual\nproblem, and finally identify a condition under which the Lagrange multiplier\nassociated to the dual problem at optimality gives an optimal portfolio. Next,\nwe study the general case without the convexity assumption and show that an\napproximately optimal solution with prescribed optimality gap can be achieved\nby using the well-known bisection algorithm combined with a duality result that\nwe prove.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 07:40:36 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Ararat", "\u00c7a\u011f\u0131n", ""]]}, {"id": "2012.06751", "submitter": "Jianming Xia", "authors": "Guangyan Jia, Jianming Xia, Rongjie Zhao", "title": "Monetary Risk Measures", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study general monetary risk measures (without any convexity\nor weak convexity). A monetary (respectively, positively homogeneous) risk\nmeasure can be characterized as the lower envelope of a family of convex\n(respectively, coherent) risk measures. The proof does not depend on but easily\nleads to the classical representation theorems for convex and coherent risk\nmeasures. When the law-invariance and the SSD (second-order stochastic\ndominance)-consistency are involved, it is not the convexity (respectively,\ncoherence) but the comonotonic convexity (respectively, comonotonic coherence)\nof risk measures that can be used for such kind of lower envelope\ncharacterizations in a unified form. The representation of a law-invariant risk\nmeasure in terms of VaR is provided.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 07:43:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jia", "Guangyan", ""], ["Xia", "Jianming", ""], ["Zhao", "Rongjie", ""]]}, {"id": "2012.07440", "submitter": "Mariano Zeron", "authors": "Mariano Zeron, Ignacio Ruiz", "title": "Tensoring volatility calibration", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by a series of remarkable papers in recent years that use Deep\nNeural Nets to substantially speed up the calibration of pricing models, we\ninvestigate the use of Chebyshev Tensors instead of Deep Neural Nets. Given\nthat Chebyshev Tensors can be, under certain circumstances, more efficient than\nDeep Neural Nets at exploring the input space of the function to be\napproximated, due to their exponential convergence, the problem of calibration\nof pricing models seems, a priori, a good case where Chebyshev Tensors can\nexcel.\n  In this piece of research, we built Chebyshev Tensors, either directly or\nwith the help of the Tensor Extension Algorithms, to tackle the computational\nbottleneck associated with the calibration of the rough Bergomi volatility\nmodel. Results are encouraging as the accuracy of model calibration via\nChebyshev Tensors is similar to that when using Deep Neural Nets, but with\nbuilding efforts that range between 5 and 100 times more efficient in the\nexperiments run. Our tests indicate that when using Chebyshev Tensors, the\ncalibration of the rough Bergomi volatility model is around 40,000 times more\nefficient than if calibrated via brute-force (using the pricing function).\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 11:51:37 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 10:47:42 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 18:31:03 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Zeron", "Mariano", ""], ["Ruiz", "Ignacio", ""]]}, {"id": "2012.09041", "submitter": "Ricardo Cris\\'ostomo", "authors": "Ricardo Cris\\'ostomo", "title": "Estimating real-world probabilities: A forward-looking behavioral\n  framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PR q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that disentangling sentiment-induced biases from fundamental\nexpectations significantly improves the accuracy and consistency of\nprobabilistic forecasts. Using data from 1994 to 2017, we analyze 15 stochastic\nmodels and risk-preference combinations and in all possible cases a simple\nbehavioral transformation delivers substantial forecast gains. Our results are\nrobust across different evaluation methods, risk-preference hypotheses and\nsentiment calibrations, demonstrating that behavioral effects can be\neffectively used to forecast asset prices. Further analyses confirm that our\nreal-world densities outperform densities recalibrated to avoid past mistakes\nand improve predictive models where risk aversion is dynamically estimated from\noption prices.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:02:00 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 16:40:19 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cris\u00f3stomo", "Ricardo", ""]]}, {"id": "2012.09448", "submitter": "Yiyan Huang", "authors": "Yiyan Huang, Cheuk Hang Leung, Xing Yan, Qi Wu, Nanbo Peng, Dongdong\n  Wang, Zhixiang Huang", "title": "The Causal Learning of Retail Delinquency", "comments": "This paper was accepted and will be published in the Thirty-Fifth\n  AAAI Conference on Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the expected difference in borrower's repayment when\nthere is a change in the lender's credit decisions. Classical estimators\noverlook the confounding effects and hence the estimation error can be\nmagnificent. As such, we propose another approach to construct the estimators\nsuch that the error can be greatly reduced. The proposed estimators are shown\nto be unbiased, consistent, and robust through a combination of theoretical\nanalysis and numerical testing. Moreover, we compare the power of estimating\nthe causal quantities between the classical estimators and the proposed\nestimators. The comparison is tested across a wide range of models, including\nlinear regression models, tree-based models, and neural network-based models,\nunder different simulated datasets that exhibit different levels of causality,\ndifferent degrees of nonlinearity, and different distributional properties.\nMost importantly, we apply our approaches to a large observational dataset\nprovided by a global technology firm that operates in both the e-commerce and\nthe lending business. We find that the relative reduction of estimation error\nis strikingly substantial if the causal effects are accounted for correctly.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:46:01 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Huang", "Yiyan", ""], ["Leung", "Cheuk Hang", ""], ["Yan", "Xing", ""], ["Wu", "Qi", ""], ["Peng", "Nanbo", ""], ["Wang", "Dongdong", ""], ["Huang", "Zhixiang", ""]]}, {"id": "2012.09648", "submitter": "Alexander Glauner", "authors": "Alexander Glauner", "title": "Dynamic Reinsurance in Discrete Time Minimizing the Insurer's Cost of\n  Capital", "comments": "arXiv admin note: text overlap with arXiv:2010.07220", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical static optimal reinsurance problem, the cost of capital for\nthe insurer's risk exposure determined by a monetary risk measure is minimized\nover the class of reinsurance treaties represented by increasing Lipschitz\nretained loss functions. In this paper, we consider a dynamic extension of this\nreinsurance problem in discrete time which can be viewed as a risk-sensitive\nMarkov Decision Process. The model allows for both insurance claims and premium\nincome to be stochastic and operates with general risk measures and premium\nprinciples. We derive the Bellman equation and show the existence of a\nMarkovian optimal reinsurance policy. Under an infinite planning horizon, the\nmodel is shown to be contractive and the optimal reinsurance policy to be\nstationary. The results are illustrated with examples where the optimal policy\ncan be determined explicitly.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 14:55:27 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Glauner", "Alexander", ""]]}, {"id": "2012.12154", "submitter": "Pedro Cadenas Dr.", "authors": "Pedro Cadenas (1), Henryk Gzyl (2), Hyun Woong Park (1) ((1) Denison\n  University, (2) IESA)", "title": "How dark is the dark side of diversification?", "comments": "The manuscript is currently under revision by the Journal of Risk\n  Finance", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Against the widely held belief that diversification at banking institutions\ncontributes to the stability of the financial system, Wagner (2010) found that\ndiversification actually makes systemic crisis more likely. While it is true,\nas Wagner asserts, that the probability of joint default of the diversified\nportfolios is larger; we contend that, as common practice, the effect of\ndiversification is examined with respect to a risk measure like VaR. We find\nthat when banks use VaR, diversification does reduce individual and systemic\nrisk. This, in turn, generates a different set of incentives for banks and\nregulators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:42:59 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Cadenas", "Pedro", ""], ["Gzyl", "Henryk", ""], ["Park", "Hyun Woong", ""]]}, {"id": "2012.12702", "submitter": "Matthew O. Jackson", "authors": "Matthew O. Jackson and Agathe Pernoud", "title": "Systemic Risk in Financial Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an overview of the relationship between financial networks and\nsystemic risk. We present a taxonomy of different types of systemic risk,\ndifferentiating between direct externalities between financial organizations\n(e.g., defaults, correlated portfolios and firesales), and perceptions and\nfeedback effects (e.g., bank runs, credit freezes). We also discuss optimal\nregulation and bailouts, measurements of systemic risk and financial\ncentrality, choices by banks' regarding their portfolios and partnerships, and\nthe changing nature of financial networks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 05:48:59 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Jackson", "Matthew O.", ""], ["Pernoud", "Agathe", ""]]}, {"id": "2012.13830", "submitter": "Alexei Kitaev", "authors": "Stanislav Shalunov, Alexei Kitaev, Yakov Shalunov, Arseniy Akopyan", "title": "Calculated Boldness: Optimizing Financial Decisions with Illiquid Assets", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM econ.TH physics.bio-ph q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider games of chance played by someone with external capital that\ncannot be applied to the game and determine how this affects risk-adjusted\noptimal betting. Specifically, we focus on Kelly optimization as a metric,\noptimizing the expected logarithm of total capital including both capital in\nplay and the external capital. For games with multiple rounds, we determine the\noptimal strategy through dynamic programming and construct a close\napproximation through the WKB method. The strategy can be described in terms of\nshort-term utility functions, with risk aversion depending on the ratio of the\namount in the game to the external money. Thus, a rational player's behavior\nvaries between conservative play that approaches Kelly strategy as they are\nable to invest a larger fraction of total wealth and extremely aggressive play\nthat maximizes linear expectation when a larger portion of their capital is\nlocked away. Because you always have expected future productivity to account\nfor as external resources, this goes counter to the conventional wisdom that\nsuper-Kelly betting is a ruinous proposition.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 23:33:33 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Shalunov", "Stanislav", ""], ["Kitaev", "Alexei", ""], ["Shalunov", "Yakov", ""], ["Akopyan", "Arseniy", ""]]}, {"id": "2012.15330", "submitter": "Jillian Clements", "authors": "Jillian M. Clements, Di Xu, Nooshin Yousefi, Dmitry Efimov", "title": "Sequential Deep Learning for Credit Risk Monitoring with Tabular\n  Financial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning plays an essential role in preventing financial losses in\nthe banking industry. Perhaps the most pertinent prediction task that can\nresult in billions of dollars in losses each year is the assessment of credit\nrisk (i.e., the risk of default on debt). Today, much of the gains from machine\nlearning to predict credit risk are driven by gradient boosted decision tree\nmodels. However, these gains begin to plateau without the addition of expensive\nnew data sources or highly engineered features. In this paper, we present our\nattempts to create a novel approach to assessing credit risk using deep\nlearning that does not rely on new model inputs. We propose a new credit card\ntransaction sampling technique to use with deep recurrent and causal\nconvolution-based neural networks that exploits long historical sequences of\nfinancial data without costly resource requirements. We show that our\nsequential deep learning approach using a temporal convolutional network\noutperformed the benchmark non-sequential tree-based model, achieving\nsignificant financial savings and earlier detection of credit risk. We also\ndemonstrate the potential for our approach to be used in a production\nenvironment, where our sampling technique allows for sequences to be stored\nefficiently in memory and used for fast online learning and inference.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 21:29:48 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Clements", "Jillian M.", ""], ["Xu", "Di", ""], ["Yousefi", "Nooshin", ""], ["Efimov", "Dmitry", ""]]}, {"id": "2012.15541", "submitter": "David Ba\\~nos", "authors": "David R. Ba\\~nos", "title": "Life insurance policies with cash flows subject to random interest rate\n  changes", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main purpose of this work is to derive a partial differential equation\nfor the reserves of life insurance liabilities subject to stochastic interest\nrates where the benefits and premiums depend directly on changes in the\ninterest rate curve. In particular, we allow the payment streams to depend on\nthe performance of an overnight technical interest rate, making them stochastic\nas well. This opens up for considering new types of contracts based on the\nperformance of the insurer's returns on their own investments. We provide\nexplicit solutions for the reserves when the premiums and benefits vary\naccording to interest rate levels or averages under the Vasicek model and\nconduct some simulations computing reserve surfaces numerically. We also give\nan example of a reinsurance treaty taking over pension payments when the\ninsurer's average returns fall under some specified threshold.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 10:44:19 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ba\u00f1os", "David R.", ""]]}]