[{"id": "1411.0426", "submitter": "Valeria Bignozzi Dr.", "authors": "Freddy Delbaen, Fabio Bellini, Valeria Bignozzi and Johanna F. Ziegel", "title": "Risk measures with the CxLS property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present contribution we characterize law determined convex risk\nmeasures that have convex level sets at the level of distributions. By relaxing\nthe assumptions in Weber (2006), we show that these risk measures can be\nidentified with a class of generalized shortfall risk measures. As a direct\nconsequence, we are able to extend the results in Ziegel (2014) and Bellini and\nBignozzi (2014) on convex elicitable risk measures and confirm that expectiles\nare the only elicitable coherent risk measures. Further, we provide a simple\ncharacterization of robustness for convex risk measures in terms of a weak\nnotion of mixture continuity.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 11:12:51 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Delbaen", "Freddy", ""], ["Bellini", "Fabio", ""], ["Bignozzi", "Valeria", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1411.1348", "submitter": "Raffaella Calabrese", "authors": "Raffaella Calabrese and Silvia Osmetti", "title": "Modelling cross-border systemic risk in the European banking sector: a\n  copula approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new methodology based on the Marshall-Olkin (MO) copula to model\ncross-border systemic risk. The proposed framework estimates the impact of the\nsystematic and idiosyncratic components on systemic risk. Initially, we propose\na maximum-likelihood method to estimate the parameter of the MO copula. In\norder to use the data on non-distressed banks for these estimates, we consider\ntimes to bank failures as censored samples. Hence, we propose an estimation\nprocedure for the MO copula on censored data. The empirical evidence from\nEuropean banks shows that the proposed censored model avoid possible\nunderestimation of the contagion risk.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 18:19:30 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Calabrese", "Raffaella", ""], ["Osmetti", "Silvia", ""]]}, {"id": "1411.1356", "submitter": "Yoshiharu Maeno", "authors": "Yoshiharu Maeno, Kenji Nishiguchi, Satoshi Morinaga, Hirokazu\n  Matsushima", "title": "Impact of credit default swaps on financial contagion", "comments": "presented at the IEEE Computational Intelligence for Financial\n  Engineering and Economics, London, March 2014", "journal-ref": null, "doi": "10.1109/CIFEr.2014.6924067", "report-no": null, "categories": "q-fin.RM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It had been believed in the conventional practice that the risk of a bank\ngoing bankrupt is lessened in a straightforward manner by transferring the risk\nof loan defaults. But the failure of American International Group in 2008 posed\na more complex aspect of financial contagion. This study presents an extension\nof the asset network systemic risk model (ANWSER) to investigate whether credit\ndefault swaps mitigate or intensify the severity of financial contagion. A\nprotection buyer bank transfers the risk of every possible debtor bank default\nto protection seller banks. The empirical distribution of the number of bank\nbankruptcies is obtained with the extended model. Systemic capital buffer ratio\nis calculated from the distribution. The ratio quantifies the effective loss\nabsorbency capability of the entire financial system to force back financial\ncontagion. The key finding is that the leverage ratio is a good estimate of a\nsystemic capital buffer ratio as the backstop of a financial system. The risk\ntransfer from small and medium banks to big banks in an interbank network does\nnot mitigate the severity of financial contagion.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 02:37:10 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Maeno", "Yoshiharu", ""], ["Nishiguchi", "Kenji", ""], ["Morinaga", "Satoshi", ""], ["Matsushima", "Hirokazu", ""]]}, {"id": "1411.1609", "submitter": "Halim Zeghdoudi", "authors": "Halim Zeghdoudi, Meriem Bouhadjar and Mohamed Riad Remita", "title": "On Stochastic Orders and its applications : Policy limits and\n  Deductibles", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  errors in typing and mathematical formulas", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on stochastic orders and its applications : policy limits\nand deductibles. Further, many applications and some examples are given :\ncomparison of two families of copulas, individual and collective risk model,\nreinsurance contracts and dependent portfolios increase risk. More precisely,\nwe propose a new model for insurance risks while we give some properties. To\nthis end, we obtain the ordering of the optimal allocation of policy limits and\ndeductibles for this model.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 13:42:53 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 22:04:00 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Zeghdoudi", "Halim", ""], ["Bouhadjar", "Meriem", ""], ["Remita", "Mohamed Riad", ""]]}, {"id": "1411.3977", "submitter": "Chiara Sabelli", "authors": "Chiara Sabelli, Michele Pioppi, Luca Sitzia and Giacomo Bormetti", "title": "Multi-curve HJM modelling for risk management", "comments": "63 pages, 18 figures, 15 tables; new calibration procedure to\n  estimate market risk-premia; new forecasting methodology; added references;\n  minor revisions to the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a HJM approach to the projection of multiple yield curves\ndeveloped to capture the volatility content of historical term structures for\nrisk management purposes. Since we observe the empirical data at daily\nfrequency and only for a finite number of time-to-maturity buckets, we propose\na modelling framework which is inherently discrete. In particular, we show how\nto approximate the HJM continuous time description of the multi-curve dynamics\nby a Vector Autoregressive process of order one. The resulting dynamics lends\nitself to a feasible estimation of the model volatility-correlation structure\nand market risk-premia. Then, resorting to the Principal Component Analysis we\nfurther simplify the dynamics reducing the number of covariance components.\nApplying the constant volatility version of our model on a sample of curves\nfrom the Euro area, we demonstrate its forecasting ability through an\nout-of-sample test.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 17:07:10 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2015 17:22:42 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2015 09:11:44 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Sabelli", "Chiara", ""], ["Pioppi", "Michele", ""], ["Sitzia", "Luca", ""], ["Bormetti", "Giacomo", ""]]}, {"id": "1411.4265", "submitter": "Wolfgang Reitgruber Dr.", "authors": "Wolfgang Reitgruber", "title": "Methodological thoughts on expected loss estimates for IFRS 9\n  impairment: hidden reserves, cyclical loss predictions and LGD backtesting", "comments": "31 pages. Forthcoming in Credit Technology by Serasa Experian, No 92,\n  Sept 2015 (in English and Portuguese with local market additions by Carlos\n  Antonio Campos Nogueira)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the release of the final accounting standards for impairment in July\n2014 by the IASB, banks will face the next significant methodological challenge\nafter Basel 2. In this paper, first methodological thoughts are presented, and\nways how to approach underlying questions are proposed. It starts with a\ndetailed discussion of the structural conservatism in the final standard. The\nexposure value iACV(c) (idealized Amortized Cost Value), as originally\nintroduced in the Exposure Draft 2009 (ED 2009), will be interpreted as\neconomic value under amortized cost accounting and provides the valuation\nbenchmark under IFRS 9. Consequently, iACV(c) can be used to quantify\nconservatism (ie potential hidden reserves) in the actual implementation of the\nfinal standard and to separate operational side-effects caused by the local\nimplementation from actual credit risk impacts. The second part continues with\na quantification of expected credit losses based on Impact of Risk(c) instead\nof traditional cost of risk measures. An objective framework is suggested which\nallows for improved testing of forward looking credit risk estimates during\ncredit cycles. This framework will prove useful to mitigate overly pro-cyclical\nprovisioning and to reduce earnings volatility. Finally, an LGD monitoring and\nbacktesting approach, applicable under regulatory requirements and accounting\nstandards as well, is proposed. On basis of the NPL Dashboard, part of the\nImpact of Risk(c) framework, specific key risk indicators are introduced that\nallow for a detailed assessment of collections performance versus LGD in in NPL\nportfolio (bucket 3).\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 14:47:44 GMT"}, {"version": "v2", "created": "Sun, 4 Jan 2015 11:33:58 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2015 11:28:09 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Reitgruber", "Wolfgang", ""]]}, {"id": "1411.4441", "submitter": "Kerem U\\u{g}urlu", "authors": "Kerem Ugurlu", "title": "On the Coherent Risk Measure Representations in the Discrete Probability\n  Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a complete characterization of both comonotone and not comonotone\ncoherent risk measures in the discrete finite probability space, where each\noutcome is equally likely. To the best of our knowledge, this is the first work\nthat characterizes \\textit{and} distinguishes comonotone and not comonotone\ncoherent risk measures via a simplified AVaR representation in this probability\nspace, which is crucial in applications and simulations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 11:44:12 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 10:32:28 GMT"}, {"version": "v3", "created": "Tue, 2 Dec 2014 10:46:56 GMT"}, {"version": "v4", "created": "Wed, 24 Dec 2014 09:48:12 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Ugurlu", "Kerem", ""]]}, {"id": "1411.4756", "submitter": "Zoltan Neda", "authors": "Gabriell Mate and Zoltan Neda", "title": "Diversification versus specialization -- lessons from a noise driven\n  linear dynamical system", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph nlin.AO physics.pop-ph q-bio.QM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialization and diversification are two major strategies that complex\nsystems might exploit. Given a fixed amount of resources, the question is\nwhether to invest this in elements that respond in a correlated manner to\nexternal perturbations, or to build a diversified system with groups of\nelements that respond in a not necessarily correlated manner. This general\ndilemma is investigated here using a high dimensional discrete dynamical system\nsubject to an external noise, analyzing the statistical properties of an order\nparameter that quantifies growth. Our analytical solution suggests that\ndiversification is a good strategy once the system has a fair amount of\nresources. For systems with small or extremely large supplies, we argue that\nspecialization might be a more successful strategy. We discuss the results also\nfrom the perspective of economic and biologic systems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 08:08:24 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Mate", "Gabriell", ""], ["Neda", "Zoltan", ""]]}, {"id": "1411.5625", "submitter": "Erika  Gomes Goncalves", "authors": "Erika Gomes-Gon\\c{c}alves (UC3M), Henryk Gzyl (IESA) and Silvia\n  Mayoral (UC3M)", "title": "Two maxentropic approaches to determine the probability density of\n  compound risk losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present an application of two maxentropic procedures to determine the\nprobability density distribution of compound sums of random variables, using\nonly a finite number of empirically determined fractional moments. The two\nmethods are the Standard method of Maximum Entropy (SME), and the method of\nMaximum Entropy in the Mean (MEM). We shall verify that the reconstructions\nobtained satisfy a variety of statistical quality criteria, and provide good\nestimations of VaR and TVaR, which are important measures for risk management\npurposes. We analyze the performance and robustness of these two procedures in\nseveral numerical examples, in which the frequency of losses is Poisson and the\nindividual losses are lognormal random variables. As side product of the work,\nwe obtain a rather accurate description of the density of the compound random\nvariable. This is an extension of a previous application based on the Standard\nMaximum Entropy approach (SME) where the analytic form of the Laplace transform\nwas available to a case in which only observed or simulated data is used. These\napproaches are also used to develop a procedure to determine the distribution\nof the individual losses through the knowledge of the total loss. Then, in the\ncase of having only historical total losses, it is possible to decompound or\ndisaggregate the random sums in its frequency/severity distributions, through a\nprobabilistic inverse problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 18:04:26 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 14:07:30 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Gomes-Gon\u00e7alves", "Erika", "", "UC3M"], ["Gzyl", "Henryk", "", "IESA"], ["Mayoral", "Silvia", "", "UC3M"]]}, {"id": "1411.6256", "submitter": "Jos\\'e Miguel Zapata", "authors": "Jose Miguel Zapata", "title": "Randomized versions of Mazur lemma and Krein-Smulian theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend to the framework of locally $L^0$-convex modules some results from\nclassical convex analysis. Namely, randomized versions of Mazur lemma and\nKrein-Smulian theorem under mild stability properties are provided.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 16:06:49 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 19:50:36 GMT"}, {"version": "v3", "created": "Sun, 15 Nov 2015 16:11:18 GMT"}, {"version": "v4", "created": "Sun, 24 Jan 2016 10:52:45 GMT"}, {"version": "v5", "created": "Thu, 22 Dec 2016 20:04:01 GMT"}, {"version": "v6", "created": "Mon, 26 Dec 2016 17:37:03 GMT"}, {"version": "v7", "created": "Wed, 10 May 2017 18:50:11 GMT"}, {"version": "v8", "created": "Mon, 19 Jun 2017 14:11:24 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Zapata", "Jose Miguel", ""]]}, {"id": "1411.7805", "submitter": "Gregor Chliamovitch", "authors": "Gregor Chliamovitch, Alexandre Dupuis, Bastien Chopard, Anton Golub", "title": "Improving predictability of time series using maximum entropy methods", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": "10.1209/0295-5075/110/10003", "report-no": null, "categories": "q-fin.RM nlin.CD physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss how maximum entropy methods may be applied to the reconstruction\nof Markov processes underlying empirical time series and compare this approach\nto usual frequency sampling. It is shown that, at least in low dimension, there\nexists a subset of the space of stochastic matrices for which the MaxEnt method\nis more efficient than sampling, in the sense that shorter historical samples\nhave to be considered to reach the same accuracy. Considering short samples is\nof particular interest when modelling smoothly non-stationary processes, for\nthen it provides, under some conditions, a powerful forecasting tool. The\nmethod is illustrated for a discretized empirical series of exchange rates.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 10:37:32 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Chliamovitch", "Gregor", ""], ["Dupuis", "Alexandre", ""], ["Chopard", "Bastien", ""], ["Golub", "Anton", ""]]}]