[{"id": "1908.00225", "submitter": "Nathaniel Tomasetti", "authors": "Nathaniel Tomasetti and Catherine S. Forbes and Anastasios\n  Panagiotelis", "title": "Updating Variational Bayes: Fast sequential posterior inference", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayesian (VB) methods produce posterior inference in a time frame\nconsiderably smaller than traditional Markov Chain Monte Carlo approaches.\nAlthough the VB posterior is an approximation, it has been shown to produce\ngood parameter estimates and predicted values when a rich classes of\napproximating distributions are considered. In this paper we propose Updating\nVB (UVB), a recursive algorithm used to update a sequence of VB posterior\napproximations in an online setting, with the computation of each posterior\nupdate requiring only the data observed since the previous update. An extension\nto the proposed algorithm, named UVB-IS, allows the user to trade accuracy for\na substantial increase in computational speed through the use of importance\nsampling. The two methods and their properties are detailed in two separate\nsimulation studies. Two empirical illustrations of the proposed UVB methods are\nprovided, including one where a Dirichlet Process Mixture model with a novel\nposterior dependence structure is repeatedly updated in the context of\npredicting the future behaviour of vehicles on a stretch of the US Highway 101.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 06:12:00 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Tomasetti", "Nathaniel", ""], ["Forbes", "Catherine S.", ""], ["Panagiotelis", "Anastasios", ""]]}, {"id": "1908.00618", "submitter": "Michael Kane", "authors": "Michael J. Kane and Nan Chen and Alexander M. Kaizer and Xun Jiang and\n  H. Amy Xia and Brian P. Hobbs", "title": "Analyzing Basket Trials under Multisource Exchangeability Assumptions", "comments": "18 pages, 4 figures, 3 tables, submitted to the Journal of Open\n  Source Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basket designs are prospective clinical trials that are devised with the\nhypothesis that the presence of selected molecular features determine a\npatient's subsequent response to a particular \"targeted\" treatment strategy.\nBasket trials are designed to enroll multiple clinical subpopulations to which\nit is assumed that the therapy in question offers beneficial efficacy in the\npresence of the targeted molecular profile. The treatment, however, may not\noffer acceptable efficacy to all subpopulations enrolled. Moreover, for rare\ndisease settings, such as oncology wherein these trials have become popular,\nmarginal measures of statistical evidence are difficult to interpret for\nsparsely enrolled subpopulations. Consequently, basket trials pose challenges\nto the traditional paradigm for trial design, which assumes inter-patient\nexchangeability. The R-package \\pkg{basket} facilitates the analysis of basket\ntrials by implementing multi-source exchangeability models. By evaluating all\npossible pairwise exchangeability relationships, this hierarchical modeling\nframework facilitates Bayesian posterior shrinkage among a collection of\ndiscrete and pre-specified subpopulations. Analysis functions are provided to\nimplement posterior inference of the response rates and all possible\nexchangeability relationships between subpopulations. In addition, the package\ncan identify \"poolable\" subsets of and report their response characteristics.\nThe functionality of the package is demonstrated using data from an oncology\nstudy with subpopulations defined by tumor histology.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 20:45:56 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Kane", "Michael J.", ""], ["Chen", "Nan", ""], ["Kaizer", "Alexander M.", ""], ["Jiang", "Xun", ""], ["Xia", "H. Amy", ""], ["Hobbs", "Brian P.", ""]]}, {"id": "1908.00850", "submitter": "Ahmad AlAmmouri", "authors": "Ahmad AlAmmouri, Jianhua Mo, Boon Loong Ng, Jianzhong Charlie Zhang,\n  Jeffrey G. Andrews", "title": "Grip-Aware Analog mmWave Beam Codebook Adaptation for 5G Mobile Handsets", "comments": "GLOBECOM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the effect of the user hand grip on the design of\nbeamforming codebooks for 5G millimeter-wave (mmWave) mobile handsets. The\nhigh-frequency structure simulator (HFSS) is used to characterize the radiation\nfields for fourteen possible handgrip profiles based on experiments we\nconducted. The loss from hand blockage on the antenna gains can be up to 20-25\ndB, which implies that the possible hand grip profiles need to be taken into\naccount while designing beam codebooks. Specifically, we consider three\ndifferent codebook adaption schemes: a grip-aware scheme, where perfect\nknowledge of the hand grip is available; a semi-aware scheme, where just the\napplication (voice call, messaging, etc.) and the orientation of the mobile\nhandset is known; and a grip-agnostic scheme, where the codebook ignores hand\nblockage. Our results show that the ideal grip-aware scheme can provide more\nthan 50% gain in terms of the spherical coverage over the agnostic scheme,\ndepending on the grip and orientation. Encouragingly, the more practical\nsemi-aware scheme we propose provides performance approaching the fully\ngrip-aware scheme. Overall, we demonstrate that 5G mmWave handsets are\ndifferent from pre-5G handsets: the user grip needs to be explicitly factored\ninto the codebook design.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 13:37:44 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["AlAmmouri", "Ahmad", ""], ["Mo", "Jianhua", ""], ["Ng", "Boon Loong", ""], ["Zhang", "Jianzhong Charlie", ""], ["Andrews", "Jeffrey G.", ""]]}, {"id": "1908.00947", "submitter": "Kevin Vanslette", "authors": "Kevin Vanslette, Abdullatif Al Alsheikh, and Kamal Youcef-Toumi", "title": "Why Simple Quadrature is just as good as Monte Carlo", "comments": null, "journal-ref": null, "doi": "10.1515/mcma-2020-2055", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motive and calculate Newton--Cotes quadrature integration variance and\ncompare it directly with Monte Carlo (MC) integration variance. We find an\nequivalence between deterministic quadrature sampling and random MC sampling by\nnoting that MC random sampling is statistically indistinguishable from a method\nthat uses deterministic sampling on a randomly shuffled (permuted) function. We\nuse this statistical equivalence to regularize the form of permissible Bayesian\nquadrature integration priors such that they are guaranteed to be objectively\ncomparable with MC. This leads to the proof that simple quadrature methods have\nexpected variances that are less than or equal to their corresponding\ntheoretical MC integration variances. Separately, using Bayesian probability\ntheory, we find that the theoretical standard deviations of the unbiased errors\nof simple Newton--Cotes composite quadrature integrations improve over their\nworst case errors by an extra dimension independent factor $\\propto N^{-1/2}$.\nThis dimension independent factor is validated in our simulations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 16:38:39 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 15:02:26 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 14:16:55 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Vanslette", "Kevin", ""], ["Alsheikh", "Abdullatif Al", ""], ["Youcef-Toumi", "Kamal", ""]]}, {"id": "1908.01034", "submitter": "Vasilis Kontonis", "authors": "Vasilis Kontonis, Christos Tzamos, Manolis Zampetakis", "title": "Efficient Truncated Statistics with Unknown Truncation", "comments": "to appear at 60th Annual IEEE Symposium on Foundations of Computer\n  Science (FOCS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the parameters of a Gaussian distribution\nwhen samples are only shown if they fall in some (unknown) subset $S \\subseteq\n\\R^d$. This core problem in truncated statistics has long history going back to\nGalton, Lee, Pearson and Fisher. Recent work by Daskalakis et al. (FOCS'18),\nprovides the first efficient algorithm that works for arbitrary sets in high\ndimension when the set is known, but leaves as an open problem the more\nchallenging and relevant case of unknown truncation set.\n  Our main result is a computationally and sample efficient algorithm for\nestimating the parameters of the Gaussian under arbitrary unknown truncation\nsets whose performance decays with a natural measure of complexity of the set,\nnamely its Gaussian surface area. Notably, this algorithm works for large\nfamilies of sets including intersections of halfspaces, polynomial threshold\nfunctions and general convex sets. We show that our algorithm closely captures\nthe tradeoff between the complexity of the set and the number of samples needed\nto learn the parameters by exhibiting a set with small Gaussian surface area\nfor which it is information theoretically impossible to learn the true Gaussian\nwith few samples.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 20:05:52 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1908.01757", "submitter": "Raphael Saavedra", "authors": "Raphael Saavedra, Guilherme Bodin, Mario Souto", "title": "StateSpaceModels.jl: a Julia Package for Time-Series Analysis in a\n  State-Space Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  StateSpaceModels.jl is an open-source Julia package for modeling, forecasting\nand simulating time series in a state-space framework. The package represents a\nstraightforward tool that can be useful for a wide range of applications that\ndeal with time series. In addition, it contains features that are not present\nin related commercial software, such as Monte Carlo simulation and the\npossibility of setting any user-defined linear model.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 17:55:20 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 23:08:12 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Saavedra", "Raphael", ""], ["Bodin", "Guilherme", ""], ["Souto", "Mario", ""]]}, {"id": "1908.02062", "submitter": "Jonathan Law", "authors": "Jonathan Law and Darren Wilkinson", "title": "Functional probabilistic programming for scalable Bayesian modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian inference involves the specification of a statistical model by a\nstatistician or practitioner, with careful thought about what each parameter\nrepresents. This results in particularly interpretable models which can be used\nto explain relationships present in the observed data. Bayesian models are\nuseful when an experiment has only a small number of observations and in\napplications where transparency of data driven decisions is important.\nTraditionally, parameter inference in Bayesian statistics has involved\nconstructing bespoke MCMC (Markov chain Monte Carlo) schemes for each newly\nproposed statistical model. This results in plausible models not being\nconsidered since efficient inference schemes are challenging to develop or\nimplement. Probabilistic programming aims to reduce the barrier to performing\nBayesian inference by developing a domain specific language (DSL) for model\nspecification which is decoupled from the parameter inference algorithms. This\npaper introduces functional programming principles which can be used to develop\nan embedded probabilistic programming language. Model inference can be carried\nout using any generic inference algorithm. In this paper Hamiltonian Monte\nCarlo (HMC) is used, an efficient MCMC method requiring the gradient of the\nun-normalised log-posterior, calculated using automatic differentiation. The\nconcepts are illustrated using the Scala programming language.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 10:31:34 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Law", "Jonathan", ""], ["Wilkinson", "Darren", ""]]}, {"id": "1908.02246", "submitter": "Ping Li", "authors": "Xiao-Tong Yuan and Ping Li", "title": "On Convergence of Distributed Approximate Newton Methods: Globalization,\n  Sharper Bounds and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DANE algorithm is an approximate Newton method popularly used for\ncommunication-efficient distributed machine learning. Reasons for the interest\nin DANE include scalability and versatility. Convergence of DANE, however, can\nbe tricky; its appealing convergence rate is only rigorous for quadratic\nobjective, and for more general convex functions the known results are no\nstronger than those of the classic first-order methods. To remedy these\ndrawbacks, we propose in this paper some new alternatives of DANE which are\nmore suitable for analysis. We first introduce a simple variant of DANE\nequipped with backtracking line search, for which global asymptotic convergence\nand sharper local non-asymptotic convergence rate guarantees can be proved for\nboth quadratic and non-quadratic strongly convex functions. Then we propose a\nheavy-ball method to accelerate the convergence of DANE, showing that nearly\ntight local rate of convergence can be established for strongly convex\nfunctions, and with proper modification of algorithm the same result applies\nglobally to linear prediction models. Numerical evidence is provided to confirm\nthe theoretical and practical advantages of our methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 16:36:30 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yuan", "Xiao-Tong", ""], ["Li", "Ping", ""]]}, {"id": "1908.02370", "submitter": "Teng Zhang", "authors": "Feng Yu, Yi Yang, Teng Zhang", "title": "An Algorithm for Graph-Fused Lasso Based on Graph Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new algorithm for solving the graph-fused lasso (GFL), a\nmethod for parameter estimation that operates under the assumption that the\nsignal tends to be locally constant over a predefined graph structure. The\nproposed method applies the alternating direction method of multipliers (ADMM)\nalgorithm and is based on the decomposition of the objective function into two\ncomponents. While ADMM has been widely used in this problem, existing works\nsuch as network lasso decompose the objective function into the loss function\ncomponent and the total variation penalty component. In comparison, this work\nproposes to decompose the objective function into two components, where one\ncomponent is the loss function plus part of the total variation penalty, and\nthe other component is the remaining total variation penalty. Compared with the\nnetwork lasso algorithm, this method has a smaller computational cost per\niteration and converges faster in most simulations numerically.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 21:14:42 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Yu", "Feng", ""], ["Yang", "Yi", ""], ["Zhang", "Teng", ""]]}, {"id": "1908.02891", "submitter": "Feng Li", "authors": "Xiaoqian Wang, Yanfei Kang, Fotios Petropoulos, Feng Li", "title": "The uncertainty estimation of feature-based forecast combinations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting is an indispensable element of operational research (OR) and an\nimportant aid to planning. The accurate estimation of the forecast uncertainty\nfacilitates several operations management activities, predominantly in\nsupporting decisions in inventory and supply chain management and effectively\nsetting safety stocks. In this paper, we introduce a feature-based framework,\nwhich links the relationship between time series features and the interval\nforecasting performance into providing reliable interval forecasts. We propose\nan optimal threshold ratio searching algorithm and a new weight determination\nmechanism for selecting an appropriate subset of models and assigning\ncombination weights for each time series tailored to the observed features. We\nevaluate our approach using a large set of time series from the M4 competition.\nOur experiments show that our approach significantly outperforms a wide range\nof benchmark models, both in terms of point forecasts as well as prediction\nintervals.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 00:52:55 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 13:11:39 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 02:17:30 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Xiaoqian", ""], ["Kang", "Yanfei", ""], ["Petropoulos", "Fotios", ""], ["Li", "Feng", ""]]}, {"id": "1908.02964", "submitter": "Francois-Xavier Briol", "authors": "Francois-Xavier Briol, Francisco A. Diaz De la O, Peter O. Hristov", "title": "Contributed Discussion of \"A Bayesian Conjugate Gradient Method\"", "comments": "Paper in press at \"Bayesian Analysis\", and will be published\n  alongside \"A Bayesian Conjugate Gradient Method\" by J. Cockayne, C. Oates, I.\n  Ipsen and M. Girolami (doi:10.1214/19-BA1145, arXiv:1801.05242)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We would like to congratulate the authors of \"A Bayesian Conjugate Gradient\nMethod\" on their insightful paper, and welcome this publication which we firmly\nbelieve will become a fundamental contribution to the growing field of\nprobabilistic numerical methods and in particular the sub-field of Bayesian\nnumerical methods. In this short piece, which will be published as a comment\nalongside the main paper, we first initiate a discussion on the choice of\npriors for solving linear systems, then propose an extension of the Bayesian\nconjugate gradient (BayesCG) algorithm for solving several related linear\nsystems simultaneously.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 08:13:13 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Briol", "Francois-Xavier", ""], ["De la O", "Francisco A. Diaz", ""], ["Hristov", "Peter O.", ""]]}, {"id": "1908.04655", "submitter": "Xi Chen", "authors": "Xi Chen, Farhan Feroz, Michael Hobson", "title": "Bayesian posterior repartitioning for nested sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priors in Bayesian analyses often encode informative domain knowledge that\ncan be useful in making the inference process more efficient. Occasionally,\nhowever, priors may be unrepresentative of the parameter values for a given\ndataset, which can result in inefficient parameter space exploration, or even\nincorrect inferences, particularly for nested sampling (NS) algorithms. Simply\nbroadening the prior in such cases may be inappropriate or impossible in some\napplications. Hence our previous solution to this problem, known as posterior\nrepartitioning (PR), redefines the prior and likelihood while keeping their\nproduct fixed, so that the posterior inferences and evidence estimates remain\nunchanged, but the efficiency of the NS process is significantly increased. In\nits most practical form, PR raises the prior to some power $\\beta$, which is\nintroduced as an auxiliary variable that must be determined on a case-by-case\nbasis, usually by lowering $\\beta$ from unity according to some pre-defined\n`annealing schedule' until the resulting inferences converge to a consistent\nsolution. Here we present a very simple yet powerful alternative Bayesian\napproach, in which $\\beta$ is instead treated as a hyperparameter that is\ninferred from the data alongside the original parameters of the problem, and\nthen marginalised over to obtain the final inference. We show through numerical\nexamples that this Bayesian PR (BPR) method provides a very robust,\nself-adapting and computationally efficient `hands-off' solution to the problem\nof unrepresentative priors in Bayesian inference using NS. Moreover, unlike the\noriginal PR method, we show that even for representative priors BPR has a\nnegligible computational overhead relative to standard nesting sampling, which\nsuggests that it should be used as the default in all NS analyses.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 14:19:41 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 17:39:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Xi", ""], ["Feroz", "Farhan", ""], ["Hobson", "Michael", ""]]}, {"id": "1908.04904", "submitter": "Feng Li", "authors": "Xuening Zhu, Feng Li, Hansheng Wang", "title": "Least Squares Approximation for a Distributed System", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 2021", "doi": "10.1080/10618600.2021.1923517", "report-no": null, "categories": "stat.ME cs.DC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a distributed least squares approximation (DLSA)\nmethod that is able to solve a large family of regression problems (e.g.,\nlinear regression, logistic regression, and Cox's model) on a distributed\nsystem. By approximating the local objective function using a local quadratic\nform, we are able to obtain a combined estimator by taking a weighted average\nof local estimators. The resulting estimator is proved to be statistically as\nefficient as the global estimator. Moreover, it requires only one round of\ncommunication. We further conduct a shrinkage estimation based on the DLSA\nestimation using an adaptive Lasso approach. The solution can be easily\nobtained by using the LARS algorithm on the master node. It is theoretically\nshown that the resulting estimator possesses the oracle property and is\nselection consistent by using a newly designed distributed Bayesian information\ncriterion (DBIC). The finite sample performance and computational efficiency\nare further illustrated by an extensive numerical study and an airline dataset.\nThe airline dataset is 52 GB in size. The entire methodology has been\nimplemented in Python for a {\\it de-facto} standard Spark system. The proposed\nDLSA algorithm on the Spark system takes 26 minutes to obtain a logistic\nregression estimator, which is more efficient and memory friendly than\nconventional methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:05:21 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 01:46:47 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 07:11:52 GMT"}, {"version": "v4", "created": "Tue, 13 Apr 2021 09:53:50 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhu", "Xuening", ""], ["Li", "Feng", ""], ["Wang", "Hansheng", ""]]}, {"id": "1908.05386", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "Robust estimation of the mean with bounded relative standard deviation", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many randomized approximation algorithms operate by giving a procedure for\nsimulating a random variable $X$ which has mean $\\mu$ equal to the target\nanswer, and a relative standard deviation bounded above by a known constant\n$c$. Examples of this type of algorithm includes methods for approximating the\nnumber of satisfying assignments to 2-SAT or DNF, the volume of a convex body,\nand the partition function of a Gibbs distribution. Because the answer is\nusually exponentially large in the problem input size, it is typical to require\nan estimate $\\hat \\mu$ satisfy $\\mathbb{P}(|\\hat \\mu/\\mu - 1| > \\epsilon) \\leq\n\\delta$, where $\\epsilon$ and $\\delta$ are user specified nonnegative\nparameters. The current best algorithm uses $\\lceil\n2c^2\\epsilon^{-2}(1+\\epsilon)^2 \\ln(2/\\delta) \\rceil$ samples to achieve such\nan estimate. By modifying the algorithm in order to balance the tails, it is\npossible to improve this result to $\\lceil 2(c^2\\epsilon^{-2} +\n1)/(1-\\epsilon^2)\\ln(2/\\delta) \\rceil$ samples. Aside from the theoretical\nimprovement, we also consider how to best implement this algorithm in practice.\nNumerical experiments show the behavior of the estimator on distributions where\nthe relative standard deviation is unknown or infinite.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 01:13:18 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "1908.05745", "submitter": "Guanyu Hu", "authors": "Jieying Jiao, Guanyu Hu, Jun Yan", "title": "A Bayesian marked spatial point processes model for basketball shot\n  chart", "comments": null, "journal-ref": "Journal of Quantitative Analysis in Sports (2020)", "doi": "10.1515/jqas-2019-0106", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success rate of a basketball shot may be higher at locations where a\nplayer makes more shots. For a marked spatial point process, this means that\nthe mark and the intensity are associated. We propose a Bayesian joint model\nfor the mark and the intensity of marked point processes, where the intensity\nis incorporated in the mark model as a covariate. Inferences are done with a\nMarkov chain Monte Carlo algorithm. Two Bayesian model comparison criteria, the\nDeviance Information Criterion and the Logarithm of the Pseudo-Marginal\nLikelihood, were used to assess the model. The performances of the proposed\nmethods were examined in extensive simulation studies. The proposed methods\nwere applied to the shot charts of four players (Curry, Harden, Durant, and\nJames) in the 2017--2018 regular season of the National Basketball Association\nto analyze their shot intensity in the field and the field goal percentage in\ndetail. Application to the top 50 most frequent shooters in the season suggests\nthat the field goal percentage and the shot intensity are positively associated\nfor a majority of the players. The fitted parameters were used as inputs in a\nsecondary analysis to cluster the players into different groups.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 20:16:49 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 17:21:23 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 20:00:44 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jiao", "Jieying", ""], ["Hu", "Guanyu", ""], ["Yan", "Jun", ""]]}, {"id": "1908.06335", "submitter": "Jurriaan Parie", "authors": "Frank Phillipson, Jurriaan Parie, Ron Weikamp", "title": "Prune Sampling: a MCMC inference technique for discrete and\n  deterministic Bayesian networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and characterise the performance of the Markov chain Monte Carlo\n(MCMC) inference method Prune Sampling for discrete and deterministic Bayesian\nnetworks (BNs). We developed a procedure to obtain the performance of a MCMC\nsampling method in the limit of infinite simulation time, extrapolated from\nrelatively short simulations. This approach was used to conduct a study to\ncompare the accuracy, rate of convergence and the time consumption of Prune\nSampling with two conventional MCMC sampling methods: Gibbs- and Metropolis\nsampling. We show that Markov chains created by Prune Sampling always converge\nto the desired posterior distribution, also for networks where conventional\nGibbs sampling fails. Beside this, we demonstrate that pruning outperforms\nGibbs sampling, at least for a certain class of BNs. Though, this tempting\nfeature comes at a price. In the first version of Prune Sampling, for large BNs\nthe procedure to choose the next iteration step uniformly is rather time\nintensive. Our conclusion is that Prune Sampling is a competitive method for\nall types of small and medium sized BNs, but (for now) standard methods still\nperform better for all types of large BNs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 20:05:23 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Phillipson", "Frank", ""], ["Parie", "Jurriaan", ""], ["Weikamp", "Ron", ""]]}, {"id": "1908.06438", "submitter": "Angelo Mele", "authors": "Angelo Mele and Lingxin Hao and Joshua Cape and Carey E. Priebe", "title": "Spectral inference for large Stochastic Blockmodels with nodal\n  covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of network analysis, it is important to distinguish\nbetween observed and unobserved factors affecting network structure. To this\nend, we develop spectral estimators for both unobserved blocks and the effect\nof covariates in stochastic blockmodels. On the theoretical side, we establish\nasymptotic normality of our estimators for the subsequent purpose of performing\ninference. On the applied side, we show that computing our estimator is much\nfaster than standard variational expectation--maximization algorithms and\nscales well for large networks. Monte Carlo experiments suggest that the\nestimator performs well under different data generating processes. Our\napplication to Facebook data shows evidence of homophily in gender, role and\ncampus-residence, while allowing us to discover unobserved communities. The\nresults in this paper provide a foundation for spectral estimation of the\neffect of observed covariates as well as unobserved latent community structure\non the probability of link formation in networks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 13:03:13 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 11:26:08 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Mele", "Angelo", ""], ["Hao", "Lingxin", ""], ["Cape", "Joshua", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1908.06514", "submitter": "Felipe Medina-Aguayo", "authors": "Felipe J Medina-Aguayo and Richard G Everitt", "title": "Revisiting the balance heuristic for estimating normalising constants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple importance sampling estimators are widely used for computing\nintractable constants due to its reliability and robustness. The celebrated\nbalance heuristic estimator belongs to this class of methods and has proved\nvery successful in computer graphics. The basic ingredients for computing the\nestimator are: a set of proposal distributions, indexed by some discrete label,\nand a predetermined number of draws from each of these proposals. However, if\nthe number of available proposals is much larger than the number of permitted\nimportance points, one needs to select, possibly at random, which of these\ndistributions will be used. The focus of this work lies within the previous\ncontext, exploring some improvements and variations of the balance heuristic\nvia a novel extended-space representation of the estimator, leading to\nstraightforward annealing schemes for variance reduction purposes. In addition,\nwe also look at the intractable scenario where the proposal density is only\navailable as a joint function with the discrete label, as may be encountered in\nproblems where an ordering is imposed. For this case, we look at combinations\nof correlated unbiased estimators which also fit into the extended-space\nrepresentation and, in turn, will provide other interesting solutions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 21:26:50 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 23:10:39 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Medina-Aguayo", "Felipe J", ""], ["Everitt", "Richard G", ""]]}, {"id": "1908.06515", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder, Stephen Wright, Andrew Zheng", "title": "Computing Estimators of Dantzig Selector type via Column and Constraint\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of linear-programming based estimators in reconstructing\na sparse signal from linear measurements. Specific formulations of the\nreconstruction problem considered here include Dantzig selector, basis pursuit\n(for the case in which the measurements contain no errors), and the fused\nDantzig selector (for the case in which the underlying signal is piecewise\nconstant). In spite of being estimators central to sparse signal processing and\nmachine learning, solving these linear programming problems for large scale\ninstances remains a challenging task, thereby limiting their usage in practice.\nWe show that classic constraint- and column-generation techniques from large\nscale linear programming, when used in conjunction with a commercial\nimplementation of the simplex method, and initialized with the solution from a\nclosely-related Lasso formulation, yields solutions with high efficiency in\nmany settings.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 21:31:52 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Mazumder", "Rahul", ""], ["Wright", "Stephen", ""], ["Zheng", "Andrew", ""]]}, {"id": "1908.06687", "submitter": "Lucie Biard", "authors": "Lucie Biard, Anne Bergeron, Sylvie Chevret", "title": "Bayesian models for survival data of clinical trials: Comparison of\n  implementations using R software", "comments": "21 pages, 8 figures (5 as supplementary material), 5 tables (4 as\n  supplementary material); Corrected typos in arXiv abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To provide guidance for the use of the main functions available in\nR for performing post hoc Bayesian analysis of a randomized clinical trial with\na survival endpoint using proportional hazard models. Study Design and Setting:\nData derived from the ALLOZITHRO trial, conducted with 465 patients after\nallograft to prevent pulmonary complications and allocated between azithromycin\nand placebo; airflow decline-free survival at 2 years after randomization was\nthe main endpoint. Results: Despite heterogeneity in modeling assumptions, in\nparticular for the baseline hazard (parametric or nonparametric), and in\nestimation methods, Bayesian posterior mean hazard ratio (HR) estimates of\nazithromycin effect were close to those obtained by the maximum likelihood\napproach. Conclusion: Bayesian models can be implemented using various R\npackages, providing results in close agreement with the maximum likelihood\nestimates. These models provide probabilistic statements that could not be\nobtained otherwise.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 10:43:54 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 07:52:19 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 16:48:34 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Biard", "Lucie", ""], ["Bergeron", "Anne", ""], ["Chevret", "Sylvie", ""]]}, {"id": "1908.06835", "submitter": "Fabrizio Laurini", "authors": "Fabrizio Laurini, Paul Fearnhead, Jonathan A. Tawn", "title": "Evaluation of extremal properties of GARCH(p,q) processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized autoregressive conditionally heteroskedastic (GARCH) processes\nare widely used for modelling features commonly found in observed financial\nreturns. The extremal properties of these processes are of considerable\ninterest for market risk management. For the simplest GARCH(p,q) process, with\nmax(p,q) = 1, all extremal features have been fully characterised. Although the\nmarginal features of extreme values of the process have been theoretically\ncharacterised when max(p, q) >= 2, much remains to be found about both marginal\nand dependence structure during extreme excursions. Specifically, a reliable\nmethod is required for evaluating the tail index, which regulates the marginal\ntail behaviour and there is a need for methods and algorithms for determining\nclustering. In particular, for the latter, the mean number of extreme values in\na short-term cluster, i.e., the reciprocal of the extremal index, has only been\ncharacterised in special cases which exclude all GARCH(p,q) processes that are\nused in practice. Although recent research has identified the multivariate\nregular variation property of stationary GARCH(p,q) processes, currently there\nare no reliable methods for numerically evaluating key components of these\ncharacterisations. We overcome these issues and are able to generate the\nforward tail chain of the process to derive the extremal index and a range of\nother cluster functionals for all GARCH(p, q) processes including integrated\nGARCH processes and processes with unbounded and asymmetric innovations. The\nnew theory and methods we present extend to assessing the strict stationarity\nand extremal properties for a much broader class of stochastic recurrence\nequations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 14:38:23 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Laurini", "Fabrizio", ""], ["Fearnhead", "Paul", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "1908.06901", "submitter": "Roi Naveiro", "authors": "Roi Naveiro and David R\\'ios Insua", "title": "Gradient Methods for Solving Stackelberg Games", "comments": "Accepted in ADT Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stackelberg Games are gaining importance in the last years due to the raise\nof Adversarial Machine Learning (AML). Within this context, a new paradigm must\nbe faced: in classical game theory, intervening agents were humans whose\ndecisions are generally discrete and low dimensional. In AML, decisions are\nmade by algorithms and are usually continuous and high dimensional, e.g.\nchoosing the weights of a neural network. As closed form solutions for\nStackelberg games generally do not exist, it is mandatory to have efficient\nalgorithms to search for numerical solutions. We study two different procedures\nfor solving this type of games using gradient methods. We study time and space\nscalability of both approaches and discuss in which situation it is more\nappropriate to use each of them. Finally, we illustrate their use in an\nadversarial prediction problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:00:57 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 13:48:16 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 19:54:12 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Naveiro", "Roi", ""], ["Insua", "David R\u00edos", ""]]}, {"id": "1908.06936", "submitter": "Sameh Abdulah", "authors": "Sameh Abdulah, Yuxiao Li, Jian Cao, Hatem Ltaief, David E. Keyes, Marc\n  G. Genton, Ying Sun", "title": "ExaGeoStatR: A Package for Large-Scale Geostatistics in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel computing in Gaussian process calculation becomes a necessity for\navoiding computational and memory restrictions associated with Geostatistics\napplications. The evaluation of the Gaussian log-likelihood function requires\nO(n^2) storage and O(n^3) operations where n is the number of geographical\nlocations. In this paper, we present ExaGeoStatR, a package for large-scale\nGeostatistics in R that supports parallel computation of the maximum likelihood\nfunction on shared memory, GPU, and distributed systems. The parallelization\ndepends on breaking down the numerical linear algebra operations into a set of\ntasks and rendering them for a task-based programming model. ExaGeoStatR\nsupports several maximum likelihood computation variants such as exact,\nDiagonal Super Tile (DST), and Tile Low-Rank (TLR) approximation besides\nproviding a tool to generate large-scale synthetic datasets which can be used\nto test and compare different approximations methods. The package can be used\ndirectly through the R environment without any C, CUDA, or MPIknowledge. Here,\nwe demonstrate the ExaGeoStatR package by illustrating its implementation\ndetails, analyzing its performance on various parallel architectures, and\nassessing its accuracy using both synthetic datasets and a sea surface\ntemperature dataset. The performance evaluation involves spatial datasets with\nup to 250K observations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 18:28:17 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Abdulah", "Sameh", ""], ["Li", "Yuxiao", ""], ["Cao", "Jian", ""], ["Ltaief", "Hatem", ""], ["Keyes", "David E.", ""], ["Genton", "Marc G.", ""], ["Sun", "Ying", ""]]}, {"id": "1908.07072", "submitter": "Sean McGrath", "authors": "Victoria Lin, Sean McGrath, Zilu Zhang, Lucia C. Petito, Roger W.\n  Logan, Miguel A. Hern\\'an, Jessica G. Young", "title": "gfoRmula: An R package for estimating effects of general time-varying\n  treatment interventions via the parametric g-formula", "comments": "V. Lin and S. McGrath made equal contributions. M.A. Hernan and J.G.\n  Young made equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are often interested in using longitudinal data to estimate the\ncausal effects of hypothetical time-varying treatment interventions on the mean\nor risk of a future outcome. Standard regression/conditioning methods for\nconfounding control generally fail to recover causal effects when time-varying\nconfounders are themselves affected by past treatment. In such settings,\nestimators derived from Robins's g-formula may recover time-varying treatment\neffects provided sufficient covariates are measured to control confounding by\nunmeasured risk factors. The package gfoRmula implements in R one such\nestimator: the parametric g-formula. This estimator easily adapts to binary or\ncontinuous time-varying treatments as well as contrasts defined by static or\ndynamic, deterministic or random treatment interventions, as well as\ninterventions that depend on the natural value of treatment. The package\naccommodates survival outcomes as well as binary or continuous end of follow-up\noutcomes. For survival outcomes, the package has different options for handling\ncompeting events. This paper describes the gfoRmula package, along with\nmotivating background, features, and examples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 21:09:38 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 21:23:25 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Lin", "Victoria", ""], ["McGrath", "Sean", ""], ["Zhang", "Zilu", ""], ["Petito", "Lucia C.", ""], ["Logan", "Roger W.", ""], ["Hern\u00e1n", "Miguel A.", ""], ["Young", "Jessica G.", ""]]}, {"id": "1908.07204", "submitter": "Gael Martin Prof", "authors": "Patrick Leung, Catherine S. Forbes, Gael M. Martin and Brendan McCabe", "title": "Forecasting observables with particle filters: Any filter will do!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the impact of filter choice on forecast accuracy in state\nspace models. The filters are used both to estimate the posterior distribution\nof the parameters, via a particle marginal Metropolis-Hastings (PMMH)\nalgorithm, and to produce draws from the filtered distribution of the final\nstate. Multiple filters are entertained, including two new data-driven methods.\nSimulation exercises are used to document the performance of each PMMH\nalgorithm, in terms of computation time and the efficiency of the chain. We\nthen produce the forecast distributions for the one-step-ahead value of the\nobserved variable, using a fixed number of particles and Markov chain draws.\nDespite distinct differences in efficiency, the filters yield virtually\nidentical forecasting accuracy, with this result holding under both correct and\nincorrect specification of the model. This invariance of forecast performance\nto the specification of the filter also characterizes an empirical analysis of\nS&P500 daily returns.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 07:51:57 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Leung", "Patrick", ""], ["Forbes", "Catherine S.", ""], ["Martin", "Gael M.", ""], ["McCabe", "Brendan", ""]]}, {"id": "1908.07232", "submitter": "Zhijian He", "authors": "Zhijian He", "title": "Sensitivity estimation of conditional value at risk using randomized\n  quasi-Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional value at risk (CVaR) is a popular measure for quantifying\nportfolio risk. Sensitivity analysis of CVaR is very useful in risk management\nand gradient-based optimization algorithms. In this paper, we study the\ninfinitesimal perturbation analysis estimator for CVaR sensitivity using\nrandomized quasi-Monte Carlo (RQMC) simulation. We first prove that the\nRQMC-based estimator is strongly consistent under very mild conditions. Under\nsome technical conditions, RQMC that uses $d$-dimensional points in CVaR\nsensitivity estimation yields a mean error rate of\n$O(n^{-1/2-1/(4d-2)+\\epsilon})$ for arbitrarily small $\\epsilon>0$. The\nnumerical results show that the RQMC method performs better than the Monte\nCarlo method for all cases. The gain of plain RQMC deteriorates as the\ndimension $d$ increases, as predicted by the established theoretical error\nrate.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 09:16:01 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 07:12:16 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["He", "Zhijian", ""]]}, {"id": "1908.07254", "submitter": "Sylvain Le Corff", "authors": "Pierre Gloaguen (MIA-Paris), Sylvain Le Corff (IP Paris, CITI,\n  TIPIC-SAMOVAR), Jimmy Olsson (KTH Royal Institute of Technology)", "title": "A pseudo-marginal sequential Monte Carlo online smoothing algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online computation of expectations of additive state functionals\nunder general path probability measures proportional to products of\nunnormalised transition densities. These transition densities are assumed to be\nintractable but possible to estimate, with or without bias. Using\npseudo-marginalisation techniques we are able to extend the particle-based,\nrapid incremental smoother (PaRIS) algorithm proposed in [J.Olsson and\nJ.Westerborn. Efficient particle-based online smoothing in general hidden\nMarkov models: The PaRIS algorithm. Bernoulli, 23(3):1951--1996, 2017] to this\nsetting. The resulting algorithm, which has a linear complexity in the number\nof particles and constant memory requirements, applies to a wide range of\nchallenging path-space Monte Carlo problems, including smoothing in partially\nobserved diffusion processes and models with intractable likelihood. The\nalgorithm is furnished with several theoretical results, including a central\nlimit theorem, establishing its convergence and numerical stability. Moreover,\nunder strong mixing assumptions we establish a novel $O(n \\varepsilon)$ bound\non the asymptotic bias of the algorithm, where $n$ is the path length and\n$\\varepsilon$ controls the bias of the density estimators.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 09:53:17 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:28:46 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gloaguen", "Pierre", "", "MIA-Paris"], ["Corff", "Sylvain Le", "", "IP Paris, CITI,\n  TIPIC-SAMOVAR"], ["Olsson", "Jimmy", "", "KTH Royal Institute of Technology"]]}, {"id": "1908.07607", "submitter": "Tomer Lancewicki Ph.D.", "authors": "Tomer Lancewicki, Selcuk Kopru", "title": "Automatic and Simultaneous Adjustment of Learning Rate and Momentum for\n  Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) methods are prominent for training machine\nlearning and deep learning models. The performance of these techniques depends\non their hyperparameter tuning over time and varies for different models and\nproblems. Manual adjustment of hyperparameters is very costly and\ntime-consuming, and even if done correctly, it lacks theoretical justification\nwhich inevitably leads to \"rule of thumb\" settings. In this paper, we propose a\ngeneric approach that utilizes the statistics of an unbiased gradient estimator\nto automatically and simultaneously adjust two paramount hyperparameters: the\nlearning rate and momentum. We deploy the proposed general technique for\nvarious SGD methods to train Convolutional Neural Networks (CNN's). The results\nmatch the performance of the best settings obtained through an exhaustive\nsearch and therefore, removes the need for a tedious manual tuning.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 20:56:41 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lancewicki", "Tomer", ""], ["Kopru", "Selcuk", ""]]}, {"id": "1908.07798", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe, Roman Liesenfeld, Guilherme Valle Moura and Atle\n  Oglend", "title": "Analyzing Commodity Futures Using Factor State-Space Models with Wishart\n  Stochastic Volatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a factor state-space approach with stochastic volatility to model\nand forecast the term structure of future contracts on commodities. Our\napproach builds upon the dynamic 3-factor Nelson-Siegel model and its 4-factor\nSvensson extension and assumes for the latent level, slope and curvature\nfactors a Gaussian vector autoregression with a multivariate Wishart stochastic\nvolatility process. Exploiting the conjugacy of the Wishart and the Gaussian\ndistribution, we develop a computationally fast and easy to implement MCMC\nalgorithm for the Bayesian posterior analysis. An empirical application to\ndaily prices for contracts on crude oil with stipulated delivery dates ranging\nfrom one to 24 months ahead show that the estimated 4-factor Svensson model\nwith two curvature factors provides a good parsimonious representation of the\nserial correlation in the individual prices and their volatility. It also shows\nthat this model has a good out-of-sample forecast performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 11:15:28 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Kleppe", "Tore Selland", ""], ["Liesenfeld", "Roman", ""], ["Moura", "Guilherme Valle", ""], ["Oglend", "Atle", ""]]}, {"id": "1908.08320", "submitter": "Philipp Otto", "authors": "Philipp Otto and Wolfgang Schmid", "title": "Spatial and Spatiotemporal GARCH Models -- A Unified Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time-series analyses, particularly for finance, generalized autoregressive\nconditional heteroscedasticity (GARCH) models are widely applied statistical\ntools for modelling volatility clusters (i.e., periods of increased or\ndecreased risk). In contrast, it has not been considered to be of critical\nimportance until now to model spatial dependence in the conditional second\nmoments. Only a few models have been proposed for modelling local clusters of\nincreased risks. In this paper, we introduce novel spatial GARCH and\nexponential GARCH processes in a unified spatial and spatiotemporal GARCH-type\nmodel, which also covers all previously proposed spatial ARCH models as well as\ntime-series GARCH models. For this common modelling framework, estimators are\nderived based on nonlinear least squares and on the maximum-likelihood\napproach. In addition to the theoretical contributions of this paper, we\nsuggest a model selection strategy that is verified by a series of Monte Carlo\nsimulation studies. Eventually, the use of the unified model is demonstrated by\nan empirical example that focuses on real estate prices from 1995 to 2014\nacross the ZIP-Code areas of Berlin. A spatial autoregressive model is applied\nto the data to illustrate how locally varying model uncertainties can be\ncaptured by the spatial GARCH-type models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 11:29:03 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 08:33:41 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Otto", "Philipp", ""], ["Schmid", "Wolfgang", ""]]}, {"id": "1908.08845", "submitter": "Luis Vargas Mieles", "authors": "Luis Vargas, Marcelo Pereyra and Konstantinos C. Zygalakis", "title": "Accelerating proximal Markov chain Monte Carlo by using an explicit\n  stabilised method", "comments": "28 pages, 13 figures. Accepted for publication in SIAM Journal on\n  Imaging Sciences (SIIMS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a highly efficient proximal Markov chain Monte Carlo methodology\nto perform Bayesian computation in imaging problems. Similarly to previous\nproximal Monte Carlo approaches, the proposed method is derived from an\napproximation of the Langevin diffusion. However, instead of the conventional\nEuler-Maruyama approximation that underpins existing proximal Monte Carlo\nmethods, here we use a state-of-the-art orthogonal Runge-Kutta-Chebyshev\nstochastic approximation that combines several gradient evaluations to\nsignificantly accelerate its convergence speed, similarly to accelerated\ngradient optimisation methods. The proposed methodology is demonstrated via a\nrange of numerical experiments, including non-blind image deconvolution,\nhyperspectral unmixing, and tomographic reconstruction, with total-variation\nand $\\ell_1$-type priors. Comparisons with Euler-type proximal Monte Carlo\nmethods confirm that the Markov chains generated with our method exhibit\nsignificantly faster convergence speeds, achieve larger effective sample sizes,\nand produce lower mean square estimation errors at equal computational budget.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:44:28 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 15:52:49 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 13:55:48 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Vargas", "Luis", ""], ["Pereyra", "Marcelo", ""], ["Zygalakis", "Konstantinos C.", ""]]}, {"id": "1908.09363", "submitter": "Matthias Sachs", "authors": "Benedict Leimkuhler, Matthias Sachs and Gabriel Stoltz", "title": "Hypocoercivity properties of adaptive Langevin dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.FA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Langevin dynamics is a method for sampling the Boltzmann-Gibbs\ndistribution at prescribed temperature in cases where the potential gradient is\nsubject to stochastic perturbation of unknown magnitude. The method replaces\nthe friction in underdamped Langevin dynamics with a dynamical variable,\nupdated according to a negative feedback loop control law as in the\nNos\\'e-Hoover thermostat. Using a hypocoercivity analysis we show that the law\nof Adaptive Langevin dynamics converges exponentially rapidly to the stationary\ndistribution, with a rate that can be quantified in terms of the key parameters\nof the dynamics. This allows us in particular to obtain a central limit theorem\nwith respect to the time averages computed along a stochastic path. Our\ntheoretical findings are illustrated by numerical simulations involving\nclassification of the MNIST data set of handwritten digits using Bayesian\nlogistic regression.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 17:14:00 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 17:16:38 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Leimkuhler", "Benedict", ""], ["Sachs", "Matthias", ""], ["Stoltz", "Gabriel", ""]]}, {"id": "1908.09429", "submitter": "Xin Tong Thomson", "authors": "X. T. Tong and M. Morzfeld and Y. M. Marzouk", "title": "MALA-within-Gibbs samplers for high-dimensional distributions with\n  sparse conditional structure", "comments": "38 ages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) samplers are numerical methods for drawing\nsamples from a given target probability distribution. We discuss one particular\nMCMC sampler, the MALA-within-Gibbs sampler, from the theoretical and practical\nperspectives. We first show that the acceptance ratio and step size of this\nsampler are independent of the overall problem dimension when (i) the target\ndistribution has sparse conditional structure, and (ii) this structure is\nreflected in the partial updating strategy of MALA-within-Gibbs. If, in\naddition, the target density is block-wise log-concave, then the sampler's\nconvergence rate is independent of dimension. From a practical perspective, we\nexpect that MALA-within-Gibbs is useful for solving high-dimensional Bayesian\ninference problems where the posterior exhibits sparse conditional structure at\nleast approximately. In this context, a partitioning of the state that\ncorrectly reflects the sparse conditional structure must be found, and we\nillustrate this process in two numerical examples. We also discuss trade-offs\nbetween the block size used for partial updating and computational requirements\nthat may increase with the number of blocks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 01:39:32 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 08:36:22 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Tong", "X. T.", ""], ["Morzfeld", "M.", ""], ["Marzouk", "Y. M.", ""]]}, {"id": "1908.09482", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein, David J. Nott and Michael Stanley Smith", "title": "Marginally-calibrated deep distributional regression", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics (2020)", "doi": "10.1080/10618600.2020.1807996", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) regression models are widely used in applications\nrequiring state-of-the-art predictive accuracy. However, until recently there\nhas been little work on accurate uncertainty quantification for predictions\nfrom such models. We add to this literature by outlining an approach to\nconstructing predictive distributions that are `marginally calibrated'. This is\nwhere the long run average of the predictive distributions of the response\nvariable matches the observed empirical margin. Our approach considers a DNN\nregression with a conditionally Gaussian prior for the final layer weights,\nfrom which an implicit copula process on the feature space is extracted. This\ncopula process is combined with a non-parametrically estimated marginal\ndistribution for the response. The end result is a scalable distributional DNN\nregression method with marginally calibrated predictions, and our work\ncomplements existing methods for probability calibration. The approach is first\nillustrated using two applications of dense layer feed-forward neural networks.\nHowever, our main motivating applications are in likelihood-free inference,\nwhere distributional deep regression is used to estimate marginal posterior\ndistributions. In two complex ecological time series examples we employ the\nimplicit copulas of convolutional networks, and show that marginal calibration\nresults in improved uncertainty quantification. Our approach also avoids the\nneed for manual specification of summary statistics, a requirement that is\nburdensome for users and typical of competing likelihood-free inference\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:47:34 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 09:29:21 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 19:33:06 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Klein", "Nadja", ""], ["Nott", "David J.", ""], ["Smith", "Michael Stanley", ""]]}, {"id": "1908.10859", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Yi-An Ma, Martin J. Wainwright, Peter L. Bartlett,\n  Michael I. Jordan", "title": "High-Order Langevin Diffusion Yields an Accelerated MCMC Algorithm", "comments": "Changes from v1: improved algorithm with $O (d^{1/4} /\n  \\varepsilon^{1/2})$ mixing time", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Markov chain Monte Carlo (MCMC) algorithm based on third-order\nLangevin dynamics for sampling from distributions with log-concave and smooth\ndensities. The higher-order dynamics allow for more flexible discretization\nschemes, and we develop a specific method that combines splitting with more\naccurate integration. For a broad class of $d$-dimensional distributions\narising from generalized linear models, we prove that the resulting third-order\nalgorithm produces samples from a distribution that is at most $\\varepsilon >\n0$ in Wasserstein distance from the target distribution in\n$O\\left(\\frac{d^{1/4}}{ \\varepsilon^{1/2}} \\right)$ steps. This result requires\nonly Lipschitz conditions on the gradient. For general strongly convex\npotentials with $\\alpha$-th order smoothness, we prove that the mixing time\nscales as $O \\left(\\frac{d^{1/4}}{\\varepsilon^{1/2}} +\n\\frac{d^{1/2}}{\\varepsilon^{1/(\\alpha - 1)}} \\right)$.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 17:59:29 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 15:10:59 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Mou", "Wenlong", ""], ["Ma", "Yi-An", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1908.11048", "submitter": "Hyowon An", "authors": "Hyowon An, Kai Zhang, Hannu Oja and J. S. Marron", "title": "Variable screening based on Gaussian Centered L-moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in big data is identification of important variables.\nIn this paper, we propose methods of discovering variables with non-standard\nunivariate marginal distributions. The conventional moments-based summary\nstatistics can be well-adopted for that purpose, but their sensitivity to\noutliers can lead to selection based on a few outliers rather than\ndistributional shape such as bimodality. To address this type of\nnon-robustness, we consider the L-moments. Using these in practice, however,\nhas a limitation because they do not take zero values at the Gaussian\ndistributions to which the shape of a marginal distribution is most naturally\ncompared. As a remedy, we propose Gaussian Centered L-moments which share\nadvantages of the L-moments but have zeros at the Gaussian distributions. The\nstrength of Gaussian Centered L-moments over other conventional moments is\nshown in theoretical and practical aspects such as their performances in\nscreening important genes in cancer genetics data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 04:50:43 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["An", "Hyowon", ""], ["Zhang", "Kai", ""], ["Oja", "Hannu", ""], ["Marron", "J. S.", ""]]}, {"id": "1908.11246", "submitter": "Kevin Vanslette", "authors": "Kevin Vanslette, Arwa Alanqari, Zeyad Al-awwad, Kamal Youcef-Toumi", "title": "Vectorized Uncertainty Propagation and Input Probability Sensitivity\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we construct a theoretical and computational process for\nassessing Input Probability Sensitivity Analysis (IPSA) using a Graphics\nProcessing Unit (GPU) enabled technique called Vectorized Uncertainty\nPropagation (VUP). VUP propagates probability distributions through a\nparametric computational model in a way that's computational time complexity\ngrows sublinearly in the number of distinct propagated input probability\ndistributions. VUP can therefore be used to efficiently implement IPSA, which\nestimates a model's probabilistic sensitivity to measurement and parametric\nuncertainty over each relevant measurement location. Theory and simulation\nillustrate the effectiveness of these methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 14:13:31 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Vanslette", "Kevin", ""], ["Alanqari", "Arwa", ""], ["Al-awwad", "Zeyad", ""], ["Youcef-Toumi", "Kamal", ""]]}, {"id": "1908.11251", "submitter": "Kevin Vanslette", "authors": "Kevin Vanslette, Tony Tohme, Kamal Youcef-Toumi", "title": "A General Model Validation and Testing Tool", "comments": "A few more examples and figures were added", "journal-ref": "Reliability Engineering & System Safety, February 2019, Volume\n  195, 106684", "doi": "10.1016/j.ress.2019.106684", "report-no": null, "categories": "stat.ME physics.data-an stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We construct and propose the \"Bayesian Validation Metric\" (BVM) as a general\nmodel validation and testing tool. We find the BVM to be capable of\nrepresenting all of the standard validation metrics (square error, reliability,\nprobability of agreement, frequentist, area, probability density comparison,\nstatistical hypothesis testing, and Bayesian model testing) as special cases\nand find that it can be used to improve, generalize, or further quantify their\nuncertainties. Thus, the BVM allows us to assess the similarities and\ndifferences between existing validation metrics in a new light.\n  The BVM has the capacity to allow users to invent and select models according\nto novel validation requirements. We formulate and test a few novel compound\nvalidation metrics that improve upon other validation metrics in the\nliterature. Further, we construct the BVM Ratio for the purpose of quantifying\nmodel selection under user defined definitions of agreement in the presence or\nabsence of uncertainty. This construction generalizes the Bayesian model\ntesting framework.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 14:18:50 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 15:32:39 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Vanslette", "Kevin", ""], ["Tohme", "Tony", ""], ["Youcef-Toumi", "Kamal", ""]]}, {"id": "1908.11523", "submitter": "Niccol\\`o Dalmasso", "authors": "Niccol\\`o Dalmasso and Taylor Pospisil and Ann B. Lee and Rafael\n  Izbicki and Peter E. Freeman and Alex I. Malz", "title": "Conditional Density Estimation Tools in Python and R with Applications\n  to Photometric Redshifts and Likelihood-Free Cosmological Inference", "comments": "27 pages, 7 figures, 4 tables", "journal-ref": null, "doi": "10.1016/j.ascom.2019.100362", "report-no": null, "categories": "astro-ph.IM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known in astronomy that propagating non-Gaussian prediction\nuncertainty in photometric redshift estimates is key to reducing bias in\ndownstream cosmological analyses. Similarly, likelihood-free inference\napproaches, which are beginning to emerge as a tool for cosmological analysis,\nrequire a characterization of the full uncertainty landscape of the parameters\nof interest given observed data. However, most machine learning (ML) or\ntraining-based methods with open-source software target point prediction or\nclassification, and hence fall short in quantifying uncertainty in complex\nregression and parameter inference settings. As an alternative to methods that\nfocus on predicting the response (or parameters) $\\mathbf{y}$ from features\n$\\mathbf{x}$, we provide nonparametric conditional density estimation (CDE)\ntools for approximating and validating the entire probability density function\n(PDF) $\\mathrm{p}(\\mathbf{y}|\\mathbf{x})$ of $\\mathbf{y}$ given (i.e.,\nconditional on) $\\mathbf{x}$. As there is no one-size-fits-all CDE method, the\ngoal of this work is to provide a comprehensive range of statistical tools and\nopen-source software for nonparametric CDE and method assessment which can\naccommodate different types of settings and be easily fit to the problem at\nhand. Specifically, we introduce four CDE software packages in\n$\\texttt{Python}$ and $\\texttt{R}$ based on ML prediction methods adapted and\noptimized for CDE: $\\texttt{NNKCDE}$, $\\texttt{RFCDE}$, $\\texttt{FlexCode}$,\nand $\\texttt{DeepCDE}$. Furthermore, we present the $\\texttt{cdetools}$\npackage, which includes functions for computing a CDE loss function for tuning\nand assessing the quality of individual PDFs, along with diagnostic functions.\nWe provide sample code in $\\texttt{Python}$ and $\\texttt{R}$ as well as\nexamples of applications to photometric redshift estimation and likelihood-free\ncosmological inference via CDE.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 03:56:17 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 02:49:21 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Dalmasso", "Niccol\u00f2", ""], ["Pospisil", "Taylor", ""], ["Lee", "Ann B.", ""], ["Izbicki", "Rafael", ""], ["Freeman", "Peter E.", ""], ["Malz", "Alex I.", ""]]}, {"id": "1908.11548", "submitter": "Thomas Whitaker Mr", "authors": "Thomas Whitaker, Boris Beranger and Scott A. Sisson", "title": "Composite likelihood methods for histogram-valued random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic data analysis has been proposed as a technique for summarising large\nand complex datasets into a much smaller and tractable number of distributions\n-- such as random rectangles or histograms -- each describing a portion of the\nlarger dataset. Recent work has developed likelihood-based methods that permit\nfitting models for the underlying data while only observing the distributional\nsummaries. However, while powerful, when working with random histograms this\napproach rapidly becomes computationally intractable as the dimension of the\nunderlying data increases. We introduce a composite-likelihood variation of\nthis likelihood-based approach for the analysis of random histograms in $K$\ndimensions, through the construction of lower-dimensional marginal histograms.\nThe performance of this approach is examined through simulated and real data\nanalysis of max-stable models for spatial extremes using millions of observed\ndatapoints in more than $K=100$ dimensions. Large computational savings are\navailable compared to existing model fitting approaches.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 06:20:49 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 05:39:10 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Whitaker", "Thomas", ""], ["Beranger", "Boris", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1908.11812", "submitter": "Giacomo Zanella", "authors": "Samuel Livingstone and Giacomo Zanella", "title": "The Barker proposal: combining robustness and efficiency in\n  gradient-based MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a tension between robustness and efficiency when designing Markov\nchain Monte Carlo (MCMC) sampling algorithms. Here we focus on robustness with\nrespect to tuning parameters, showing that more sophisticated algorithms tend\nto be more sensitive to the choice of step-size parameter and less robust to\nheterogeneity of the distribution of interest. We characterise this phenomenon\nby studying the behaviour of spectral gaps as an increasingly poor step-size is\nchosen for the algorithm. Motivated by these considerations, we propose a novel\nand simple gradient-based MCMC algorithm, inspired by the classical Barker\naccept-reject rule, with improved robustness properties. Extensive theoretical\nresults, dealing with robustness to tuning, geometric ergodicity and scaling\nwith dimension, suggest that the novel scheme combines the robustness of simple\nschemes with the efficiency of gradient-based ones. We show numerically that\nthis type of robustness is particularly beneficial in the context of adaptive\nMCMC, giving examples where our proposed scheme significantly outperforms\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 16:09:22 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 14:40:25 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Livingstone", "Samuel", ""], ["Zanella", "Giacomo", ""]]}]