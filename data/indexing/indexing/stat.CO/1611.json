[{"id": "1611.00040", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Lasso, fractional norm and structured sparse estimation using a Hadamard\n  product parametrization", "comments": "This revision includes a comparison to cyclic coordinate descent and\n  a new algorithm for sparse high-dimensional settings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a multiplicative reparametrization, I show that a subclass of $L_q$\npenalties with $q\\leq 1$ can be expressed as sums of $L_2$ penalties. It\nfollows that the lasso and other norm-penalized regression estimates may be\nobtained using a very simple and intuitive alternating ridge regression\nalgorithm. As compared to a similarly intuitive EM algorithm for $L_q$\noptimization, the proposed algorithm avoids some numerical instability issues\nand is also competitive in terms of speed. Furthermore, the proposed algorithm\ncan be extended to accommodate sparse high-dimensional scenarios, generalized\nlinear models, and can be used to create structured sparsity via penalties\nderived from covariance models for the parameters. Such model-based penalties\nmay be useful for sparse estimation of spatially or temporally structured\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 20:52:27 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 00:48:33 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1611.00328", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, David M.\n  Blei", "title": "Variational Inference via $\\chi$-Upper Bound Minimization", "comments": "Neural Information Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference (VI) is widely used as an efficient alternative to\nMarkov chain Monte Carlo. It posits a family of approximating distributions $q$\nand finds the closest member to the exact posterior $p$. Closeness is usually\nmeasured via a divergence $D(q || p)$ from $q$ to $p$. While successful, this\napproach also has problems. Notably, it typically leads to underestimation of\nthe posterior variance. In this paper we propose CHIVI, a black-box variational\ninference algorithm that minimizes $D_{\\chi}(p || q)$, the $\\chi$-divergence\nfrom $p$ to $q$. CHIVI minimizes an upper bound of the model evidence, which we\nterm the $\\chi$ upper bound (CUBO). Minimizing the CUBO leads to improved\nposterior uncertainty, and it can also be used with the classical VI lower\nbound (ELBO) to provide a sandwich estimate of the model evidence. We study\nCHIVI on three models: probit regression, Gaussian process classification, and\na Cox process model of basketball plays. When compared to expectation\npropagation and classical VI, CHIVI produces better error rates and more\naccurate estimates of posterior variance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 18:40:23 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:00:03 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 00:29:21 GMT"}, {"version": "v4", "created": "Sun, 12 Nov 2017 19:00:57 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Dieng", "Adji B.", ""], ["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Paisley", "John", ""], ["Blei", "David M.", ""]]}, {"id": "1611.01069", "submitter": "Leonardo Egidi", "authors": "Leonardo Egidi, Roberta Pappad\\`a, Francesco Pauli and Nicola Torelli", "title": "Maxima Units Search (MUS) algorithm: methodology and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for extracting identity submatrices of small rank and pivotal\nunits from large and sparse matrices is proposed. The procedure has already\nbeen satisfactorily applied for solving the label switching problem in Bayesian\nmixture models. Here we introduce it on its own and explore possible\napplications in different contexts.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 15:45:06 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Egidi", "Leonardo", ""], ["Pappad\u00e0", "Roberta", ""], ["Pauli", "Francesco", ""], ["Torelli", "Nicola", ""]]}, {"id": "1611.01213", "submitter": "Nilabja Guha", "authors": "Keren Yang, Nilabja Guha, Yalchin Efendiev, Bani K. Mallick", "title": "Bayesian and Variational Bayesian approaches for flows in heterogenous\n  random media", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.04.034", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study porous media flows in heterogeneous stochastic media.\nWe propose an efficient forward simulation technique that is tailored for\nvariational Bayesian inversion. As a starting point, the proposed forward\nsimulation technique decomposes the solution into the sum of separable\nfunctions (with respect to randomness and the space), where each term is\ncalculated based on a variational approach. This is similar to Proper\nGeneralized Decomposition (PGD). Next, we apply a multiscale technique to solve\nfor each term and, further, decompose the random function into 1D fields. As a\nresult, our proposed method provides an approximation hierarchy for the\nsolution as we increase the number of terms in the expansion and, also,\nincrease the spatial resolution of each term. We use the hierarchical solution\ndistributions in a variational Bayesian approximation to perform uncertainty\nquantification in the inverse problem. We conduct a detailed numerical study to\nexplore the performance of the proposed uncertainty quantification technique\nand show the theoretical posterior concentration.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 22:31:05 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 01:25:27 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 02:53:41 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Yang", "Keren", ""], ["Guha", "Nilabja", ""], ["Efendiev", "Yalchin", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1611.01310", "submitter": "Angela Bitto", "authors": "Angela Bitto, Sylvia Fr\\\"uhwirth-Schnatter", "title": "Achieving Shrinkage in a Time-Varying Parameter Model Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage for time-varying parameter (TVP) models is investigated within a\nBayesian framework, with the aim to automatically reduce time-varying\nparameters to static ones, if the model is overfitting. This is achieved\nthrough placing the double gamma shrinkage prior on the process variances. An\nefficient Markov chain Monte Carlo scheme is developed, exploiting boosting\nbased on the ancillarity-sufficiency interweaving strategy. The method is\napplicable both to TVP models for univariate as well as multivariate time\nseries. Applications include a TVP generalized Phillips curve for EU area\ninflation modelling and a multivariate TVP Cholesky stochastic volatility model\nfor joint modelling of the returns from the DAX-30 index.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 10:21:55 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 13:48:20 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Bitto", "Angela", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "1611.01353", "submitter": "Alessandro Achille", "authors": "Alessandro Achille, Stefano Soatto", "title": "Information Dropout: Learning Optimal Representations Through Noisy\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-entropy loss commonly used in deep learning is closely related to\nthe defining properties of optimal representations, but does not enforce some\nof the key properties. We show that this can be solved by adding a\nregularization term, which is in turn related to injecting multiplicative noise\nin the activations of a Deep Neural Network, a special case of which is the\ncommon practice of dropout. We show that our regularized loss function can be\nefficiently minimized using Information Dropout, a generalization of dropout\nrooted in information theoretic principles that automatically adapts to the\ndata and can better exploit architectures of limited capacity. When the task is\nthe reconstruction of the input, we show that our loss function yields a\nVariational Autoencoder as a special case, thus providing a link between\nrepresentation learning, information theory and variational inference. Finally,\nwe prove that we can promote the creation of disentangled representations\nsimply by enforcing a factorized prior, a fact that has been observed\nempirically in recent work. Our experiments validate the theoretical intuitions\nbehind our method, and we find that information dropout achieves a comparable\nor better generalization performance than binary dropout, especially on smaller\nmodels, since it can automatically adapt the noise to the structure of the\nnetwork, as well as to the test sample.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 12:46:37 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 00:40:19 GMT"}, {"version": "v3", "created": "Sun, 12 Feb 2017 09:26:25 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Achille", "Alessandro", ""], ["Soatto", "Stefano", ""]]}, {"id": "1611.01450", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin and Geir Storvik", "title": "Estimating the marginal likelihood with Integrated nested Laplace\n  approximation (INLA)", "comments": "12 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The marginal likelihood is a well established model selection criterion in\nBayesian statistics. It also allows to efficiently calculate the marginal\nposterior model probabilities that can be used for Bayesian model averaging of\nquantities of interest. For many complex models, including latent modeling\napproaches, marginal likelihoods are however difficult to compute. One recent\npromising approach for approximating the marginal likelihood is Integrated\nNested Laplace Approximation (INLA), design for models with latent Gaussian\nstructures. In this study we compare the approximations obtained with INLA to\nsome alternative approaches on a number of examples of different complexity. In\nparticular we address a simple linear latent model, a Bayesian linear\nregression model, logistic Bayesian regression models with probit and logit\nlinks, and a Poisson longitudinal generalized linear mixed model.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 16:41:50 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""]]}, {"id": "1611.01511", "submitter": "Brian Gaines", "authors": "Brian R. Gaines and Hua Zhou", "title": "Algorithms for Fitting the Constrained Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare alternative computing strategies for solving the constrained lasso\nproblem. As its name suggests, the constrained lasso extends the widely-used\nlasso to handle linear constraints, which allow the user to incorporate prior\ninformation into the model. In addition to quadratic programming, we employ the\nalternating direction method of multipliers (ADMM) and also derive an efficient\nsolution path algorithm. Through both simulations and real data examples, we\ncompare the different algorithms and provide practical recommendations in terms\nof efficiency and accuracy for various sizes of data. We also show that, for an\narbitrary penalty matrix, the generalized lasso can be transformed to a\nconstrained lasso, while the converse is not true. Thus, our methods can also\nbe used for estimating a generalized lasso, which has wide-ranging\napplications. Code for implementing the algorithms is freely available in the\nMatlab toolbox SparseReg.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 19:06:55 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Gaines", "Brian R.", ""], ["Zhou", "Hua", ""]]}, {"id": "1611.02213", "submitter": "Hillary Fairbanks", "authors": "Hillary Fairbanks, Alireza Doostan, Christian Ketelsen, Gianluca\n  Iaccarino", "title": "A Low-rank Control Variate for Multilevel Monte Carlo Simulation of\n  High-dimensional Uncertain Systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.03.060", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel Monte Carlo (MLMC) is a recently proposed variation of Monte Carlo\n(MC) simulation that achieves variance reduction by simulating the governing\nequations on a series of spatial (or temporal) grids with increasing\nresolution. Instead of directly employing the fine grid solutions, MLMC\nestimates the expectation of the quantity of interest from the coarsest grid\nsolutions as well as differences between each two consecutive grid solutions.\nWhen the differences corresponding to finer grids become smaller, hence less\nvariable, fewer MC realizations of finer grid solutions are needed to compute\nthe difference expectations, thus leading to a reduction in the overall work.\nThis paper presents an extension of MLMC, referred to as multilevel control\nvariates (MLCV), where a low-rank approximation to the solution on each grid,\nobtained primarily based on coarser grid solutions, is used as a control\nvariate for estimating the expectations involved in MLMC. Cost estimates as\nwell as numerical examples are presented to demonstrate the advantage of this\nnew MLCV approach over the standard MLMC when the solution of interest admits a\nlow-rank approximation and the cost of simulating finer grids grows fast.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 00:59:46 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Fairbanks", "Hillary", ""], ["Doostan", "Alireza", ""], ["Ketelsen", "Christian", ""], ["Iaccarino", "Gianluca", ""]]}, {"id": "1611.02256", "submitter": "Zheng Zhang", "authors": "Zheng Zhang and Tsui-Wei Weng and Luca Daniel", "title": "A Big-Data Approach to Handle Many Process Variations: Tensor Recovery\n  and Applications", "comments": "8 figures", "journal-ref": "IEEE Transactions on Component, Packaging and Manufacturing\n  Technology, 2017", "doi": null, "report-no": null, "categories": "cs.CE math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fabrication process variations are a major source of yield degradation in the\nnano-scale design of integrated circuits (IC), microelectromechanical systems\n(MEMS) and photonic circuits. Stochastic spectral methods are a promising\ntechnique to quantify the uncertainties caused by process variations. Despite\ntheir superior efficiency over Monte Carlo for many design cases, these\nalgorithms suffer from the curse of dimensionality; i.e., their computational\ncost grows very fast as the number of random parameters increases. In order to\nsolve this challenging problem, this paper presents a high-dimensional\nuncertainty quantification algorithm from a big-data perspective. Specifically,\nwe show that the huge number of (e.g., $1.5 \\times 10^{27}$) simulation samples\nin standard stochastic collocation can be reduced to a very small one (e.g.,\n$500$) by exploiting some hidden structures of a high-dimensional data array.\nThis idea is formulated as a tensor recovery problem with sparse and low-rank\nconstraints; and it is solved with an alternating minimization approach.\nNumerical results show that our approach can simulate efficiently some ICs, as\nwell as MEMS and photonic problems with over 50 independent random parameters,\nwhereas the traditional algorithm can only handle several random parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:35:35 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Zhang", "Zheng", ""], ["Weng", "Tsui-Wei", ""], ["Daniel", "Luca", ""]]}, {"id": "1611.02456", "submitter": "Tianyu Wu", "authors": "Yat Tin Chow, Tianyu Wu, Wotao Yin", "title": "Cyclic Coordinate Update Algorithms for Fixed-Point Problems: Analysis\n  and Applications", "comments": "Minor revisions throughout the paper for clarification", "journal-ref": null, "doi": null, "report-no": "UCLA CAM Report 16-78", "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems reduce to the fixed-point problem of solving $x=T(x)$. To this\nproblem, we apply the coordinate-update algorithms, which update only one or a\nfew components of $x$ at each step. When each update is cheap, these algorithms\nare faster than the full fixed-point iteration (which updates all the\ncomponents).\n  In this paper, we focus on the coordinate-update algorithms based on the\ncyclic selection rules, where the ordering of coordinates in each cycle is\narbitrary. These algorithms are fast, but their convergence is unknown in the\nfixed-point setting.\n  When $T$ is a nonexpansive operator and has a fixed point, we show that the\nsequence of coordinate-update iterates converges to a fixed point under proper\nstep sizes. This result applies to the primal-dual coordinate-update\nalgorithms, which have applications to optimization problems with nonseparable\nnonsmooth objectives, as well as global linear constraints.\n  Numerically, we apply coordinate-update algorithms with the cyclic, shuffled\ncyclic, and random selection rules to $\\ell_1$ robust least squares, a CT image\nreconstruction problem, as well as nonnegative matrix factorization. They\nconverge much faster than the standard fixed-point iteration. Among the three\nrules, cyclic and shuffled cyclic rules are overall faster than the random\nrule.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 09:55:00 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 23:20:39 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Chow", "Yat Tin", ""], ["Wu", "Tianyu", ""], ["Yin", "Wotao", ""]]}, {"id": "1611.02492", "submitter": "Dennis Prangle", "authors": "Dennis Prangle and Richard G. Everitt and Theodore Kypraios", "title": "A rare event approach to high dimensional Approximate Bayesian\n  computation", "comments": "Supplementary material at end of pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods permit approximate inference\nfor intractable likelihoods when it is possible to simulate from the model.\nHowever they perform poorly for high dimensional data, and in practice must\nusually be used in conjunction with dimension reduction methods, resulting in a\nloss of accuracy which is hard to quantify or control. We propose a new ABC\nmethod for high dimensional data based on rare event methods which we refer to\nas RE-ABC. This uses a latent variable representation of the model. For a given\nparameter value, we estimate the probability of the rare event that the latent\nvariables correspond to data roughly consistent with the observations. This is\nperformed using sequential Monte Carlo and slice sampling to systematically\nsearch the space of latent variables. In contrast standard ABC can be viewed as\nusing a more naive Monte Carlo estimate. We use our rare event probability\nestimator as a likelihood estimate within the pseudo-marginal\nMetropolis-Hastings algorithm for parameter inference.\n  We provide asymptotics showing that RE-ABC has a lower computational cost for\nhigh dimensional data than standard ABC methods. We also illustrate our\napproach empirically, on a Gaussian distribution and an application in\ninfectious disease modelling.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 12:03:19 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 13:43:33 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Prangle", "Dennis", ""], ["Everitt", "Richard G.", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1611.02754", "submitter": "Panagiotis Tsilifis", "authors": "Panagiotis A. Tsilifis", "title": "Gradient-informed basis adaptation for Legendre Chaos expansions", "comments": "Accepted article in ASME JVVUQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced basis adaptation method for Homogeneous (Wiener)\nChaos expansions is explored in a new context where the rotation/projection\nmatrices are computed by discovering the active subspace where the random input\nexhibits most of its variability. In the case where a 1-dimensional active\nsubspace exists, the methodology can be applicable to generalized Polynomial\nChaos expansions, thus enabling the projection of a high dimensional input to a\nsingle input variable and the efficient estimation of a univariate chaos\nexpansion. Attractive features of this approach, such as the significant\ncomputational savings and the high accuracy in computing statistics of interest\nare investigated.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 22:40:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 08:30:08 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Tsilifis", "Panagiotis A.", ""]]}, {"id": "1611.03112", "submitter": "Simon Grund", "authors": "Simon Grund, Oliver L\\\"udtke, Alexander Robitzsch", "title": "Multiple imputation of multilevel missing data: An introduction to the R\n  package pan", "comments": null, "journal-ref": "SAGE Open, 6(4), 1-17 (2016)", "doi": "10.1177/2158244016668220", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment of missing data can be difficult in multilevel research because\nstate-of-the-art procedures such as multiple imputation (MI) may require\nadvanced statistical knowledge or a high degree of familiarity with certain\nstatistical software. In the missing data literature, pan has been recommended\nfor MI of multilevel data. In this article, we provide an introduction to MI of\nmultilevel missing data using the R package pan, and we discuss its\npossibilities and limitations in accommodating typical questions in multilevel\nresearch. In order to make pan more accessible to applied researchers, we make\nuse of the mitml package, which provides a user-friendly interface to the pan\npackage and several tools for managing and analyzing multiply imputed data\nsets. We illustrate the use of pan and mitml with two empirical examples that\nrepresent common applications of multilevel models, and we discuss how these\nprocedures may be used in conjunction with other software.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 15:41:02 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Grund", "Simon", ""], ["L\u00fcdtke", "Oliver", ""], ["Robitzsch", "Alexander", ""]]}, {"id": "1611.03177", "submitter": "Ajay Jasra", "authors": "Pierre Del Moral, Ajay Jasra", "title": "A Note on Random Walks with Absorbing barriers and Sequential Monte\n  Carlo Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider importance sampling (IS) and sequential Monte\nCarlo (SMC) methods in the context of 1-dimensional random walks with absorbing\nbarriers. In particular, we develop a very precise variance analysis for\nseveral IS and SMC procedures. We take advantage of some explicit spectral\nformulae available for these models to derive sharp and explicit estimates;\nthis provides stability properties of the associated normalized Feynman-Kac\nsemigroups. Our analysis allows one to compare the variance of SMC and IS\ntechniques for these models. The work in this article, is one of the few to\nconsider an in-depth analysis of an SMC method for a particular model-type as\nwell as variance comparison of SMC algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 03:45:34 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Del Moral", "Pierre", ""], ["Jasra", "Ajay", ""]]}, {"id": "1611.03835", "submitter": "Matthias Katzfuss", "authors": "Jonathan R. Stroud, Matthias Katzfuss, and Christopher K. Wikle", "title": "A Bayesian adaptive ensemble Kalman filter for sequential state and\n  parameter estimation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new methodology for sequential state and parameter\nestimation within the ensemble Kalman filter. The method is fully Bayesian and\npropagates the joint posterior density of states and parameters over time. In\norder to implement the method we consider two representations of the marginal\nposterior distribution of the parameters: a grid-based approach and a Gaussian\napproximation. Contrary to existing algorithms, the new method explicitly\naccounts for parameter uncertainty and provides a formal way to combine\ninformation about the parameters from data at different time periods. The\nmethod is illustrated and compared to existing approaches using simulated and\nreal data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 19:59:26 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Stroud", "Jonathan R.", ""], ["Katzfuss", "Matthias", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1611.03969", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen", "title": "An Introduction to MM Algorithms for Machine Learning and Statistical", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 08:18:38 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Nguyen", "Hien D.", ""]]}, {"id": "1611.04167", "submitter": "Richard Henwood Mr", "authors": "R. Henwood, N. W. Watkins, S. C. Chapman, R. McLay", "title": "A parallel workload has extreme variability", "comments": "7 pages, 2 figures, 1 code listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both high-performance computing (HPC) environments and the public cloud,\nthe duration of time to retrieve or save your results is simultaneously\nunpredictable and important to your over all resource budget. It is generally\naccepted (\"Google: Taming the Long Latency Tail - When More Machines Equals\nWorse Results\", Todd Hoff, highscalability.com 2012), but without a robust\nexplanation, that identical parallel tasks do take different durations to\ncomplete -- a phenomena known as variability. This paper advances understanding\nof this topic. We carefully choose a model from which system-level complexity\nemerges that can be studied directly. We find that a generalized extreme value\n(GEV) model for variability naturally emerges. Using the public cloud, we find\nreal-world observations have excellent agreement with our model. Since the GEV\ndistribution is a limit distribution this suggests a universal property of\nparallel systems gated by the slowest communication element of some sort.\nHence, this model is applicable to a variety of processing and IO tasks in\nparallel environments. These findings have important implications, ranging from\ncharacterizing ideal performance for parallel codes to detecting degraded\nbehaviour at extreme scales.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 18:18:36 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 15:37:14 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Henwood", "R.", ""], ["Watkins", "N. W.", ""], ["Chapman", "S. C.", ""], ["McLay", "R.", ""]]}, {"id": "1611.04416", "submitter": "Alexis Roche", "authors": "Alexis Roche", "title": "On numerical approximation schemes for expectation propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several numerical approximation strategies for the expectation-propagation\nalgorithm are studied in the context of large-scale learning: the Laplace\nmethod, a faster variant of it, Gaussian quadrature, and a deterministic\nversion of variational sampling (i.e., combining quadrature with variational\napproximation). Experiments in training linear binary classifiers show that the\nexpectation-propagation algorithm converges best using variational sampling,\nwhile it also converges well using Laplace-style methods with smooth factors\nbut tends to be unstable with non-differentiable ones. Gaussian quadrature\nyields unstable behavior or convergence to a sub-optimal solution in most\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 15:21:23 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Roche", "Alexis", ""]]}, {"id": "1611.04538", "submitter": "Li Ma", "authors": "Li Ma", "title": "Recursive partitioning and multi-scale modeling on conditional densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a nonparametric prior on the conditional distribution of a\n(univariate or multivariate) response given a set of predictors. The prior is\nconstructed in the form of a two-stage generative procedure, which in the first\nstage recursively partitions the predictor space, and then in the second stage\ngenerates the conditional distribution by a multi-scale nonparametric density\nmodel on each predictor partition block generated in the first stage. This\ndesign allows adaptive smoothing on both the predictor space and the response\nspace, and it results in the full posterior conjugacy of the model, allowing\nexact Bayesian inference to be completed analytically through a\nforward-backward recursive algorithm without the need of MCMC, and thus\nenjoying high computational efficiency (scaling linearly with the sample size).\nWe show that this prior enjoys desirable theoretical properties such as full\n$L_1$ support and posterior consistency. We illustrate how to apply the model\nto a variety of inference problems such as conditional density estimation as\nwell as hypothesis testing and model selection in a manner similar to applying\na parametric conjugate prior, while attaining full nonparametricity. Also\nprovided is a comparison to two other state-of-the-art Bayesian nonparametric\nmodels for conditional densities in both model fit and computational time. A\nreal data example from flow cytometry containing 455,472 observations is given\nto illustrate the substantial computational efficiency of our method and its\napplication to multivariate problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:28:25 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 23:08:54 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 18:38:41 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Ma", "Li", ""]]}, {"id": "1611.05487", "submitter": "Ilya Safro", "authors": "Ehsan Sadrfaridpour, Sandeep Jeereddy, Ken Kennedy, Andre Luckow,\n  Talayeh Razzaghi, Ilya Safro", "title": "Algebraic multigrid support vector machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine is a flexible optimization-based technique widely\nused for classification problems. In practice, its training part becomes\ncomputationally expensive on large-scale data sets because of such reasons as\nthe complexity and number of iterations in parameter fitting methods,\nunderlying optimization solvers, and nonlinearity of kernels. We introduce a\nfast multilevel framework for solving support vector machine models that is\ninspired by the algebraic multigrid. Significant improvement in the running has\nbeen achieved without any loss in the quality. The proposed technique is highly\nbeneficial on imbalanced sets. We demonstrate computational results on publicly\navailable and industrial data sets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 22:32:50 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 01:10:21 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Sadrfaridpour", "Ehsan", ""], ["Jeereddy", "Sandeep", ""], ["Kennedy", "Ken", ""], ["Luckow", "Andre", ""], ["Razzaghi", "Talayeh", ""], ["Safro", "Ilya", ""]]}, {"id": "1611.05780", "submitter": "Eugene Ndiaye", "authors": "Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort and Joseph Salmon", "title": "Gap Safe screening rules for sparsity enforcing penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional regression settings, sparsity enforcing penalties have\nproved useful to regularize the data-fitting term. A recently introduced\ntechnique called screening rules propose to ignore some variables in the\noptimization leveraging the expected sparsity of the solutions and consequently\nleading to faster solvers. When the procedure is guaranteed not to discard\nvariables wrongly the rules are said to be safe. In this work, we propose a\nunifying framework for generalized linear models regularized with standard\nsparsity enforcing penalties such as $\\ell_1$ or $\\ell_1/\\ell_2$ norms. Our\ntechnique allows to discard safely more variables than previously considered\nsafe rules, particularly for low regularization parameters. Our proposed Gap\nSafe rules (so called because they rely on duality gap computation) can cope\nwith any iterative solver but are particularly well suited to (block)\ncoordinate descent methods. Applied to many standard learning tasks, Lasso,\nSparse-Group Lasso, multi-task Lasso, binary and multinomial logistic\nregression, etc., we report significant speed-ups compared to previously\nproposed safe rules on all tested data sets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:55:12 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 16:38:35 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 08:05:19 GMT"}, {"version": "v4", "created": "Wed, 27 Dec 2017 17:26:38 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1611.05902", "submitter": "Mickael Binois", "authors": "Mickael Binois, Robert B. Gramacy, Michael Ludkovski", "title": "Practical heteroskedastic Gaussian process modeling for large simulation\n  experiments", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": "10.1080/10618600.2018.1458625", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified view of likelihood based Gaussian progress regression\nfor simulation experiments exhibiting input-dependent noise. Replication plays\nan important role in that context, however previous methods leveraging\nreplicates have either ignored the computational savings that come from such\ndesign, or have short-cut full likelihood-based inference to remain tractable.\nStarting with homoskedastic processes, we show how multiple applications of a\nwell-known Woodbury identity facilitate inference for all parameters under the\nlikelihood (without approximation), bypassing the typical full-data sized\ncalculations. We then borrow a latent-variable idea from machine learning to\naddress heteroskedasticity, adapting it to work within the same thrifty\ninferential framework, thereby simultaneously leveraging the computational and\nstatistical efficiency of designs with replication. The result is an\ninferential scheme that can be characterized as single objective function,\ncomplete with closed form derivatives, for rapid library-based optimization.\nIllustrations are provided, including real-world simulation experiments from\nmanufacturing and the management of epidemics.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 21:21:52 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 22:28:26 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Binois", "Mickael", ""], ["Gramacy", "Robert B.", ""], ["Ludkovski", "Michael", ""]]}, {"id": "1611.06649", "submitter": "Enes Makalic", "authors": "Enes Makalic and Daniel F. Schmidt", "title": "High-Dimensional Bayesian Regularised Regression with the BayesReg\n  Package", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian penalized regression techniques, such as the Bayesian lasso and the\nBayesian horseshoe estimator, have recently received a significant amount of\nattention in the statistics literature. However, software implementing\nstate-of-the-art Bayesian penalized regression, outside of general purpose\nMarkov chain Monte Carlo platforms such as STAN, is relatively rare. This paper\nintroduces bayesreg, a new toolbox for fitting Bayesian penalized regression\nmodels with continuous shrinkage prior densities. The toolbox features Bayesian\nlinear regression with Gaussian or heavy-tailed error models and Bayesian\nlogistic regression with ridge, lasso, horseshoe and horseshoe$+$ estimators.\nThe toolbox is free, open-source and available for use with the MATLAB and R\nnumerical platforms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 05:02:08 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 06:33:50 GMT"}, {"version": "v3", "created": "Tue, 20 Dec 2016 01:01:25 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Makalic", "Enes", ""], ["Schmidt", "Daniel F.", ""]]}, {"id": "1611.06686", "submitter": "Murat A. Erdogdu", "authors": "Murat A. Erdogdu, Mohsen Bayati, Lee H. Dicker", "title": "Scalable Approximations for Generalized Linear Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stochastic optimization, the population risk is generally approximated by\nthe empirical risk. However, in the large-scale setting, minimization of the\nempirical risk may be computationally restrictive. In this paper, we design an\nefficient algorithm to approximate the population risk minimizer in generalized\nlinear problems such as binary classification with surrogate losses and\ngeneralized linear regression models. We focus on large-scale problems, where\nthe iterative minimization of the empirical risk is computationally\nintractable, i.e., the number of observations $n$ is much larger than the\ndimension of the parameter $p$, i.e. $n \\gg p \\gg 1$. We show that under random\nsub-Gaussian design, the true minimizer of the population risk is approximately\nproportional to the corresponding ordinary least squares (OLS) estimator. Using\nthis relation, we design an algorithm that achieves the same accuracy as the\nempirical risk minimizer through iterations that attain up to a cubic\nconvergence rate, and that are cheaper than any batch optimization algorithm by\nat least a factor of $\\mathcal{O}(p)$. We provide theoretical guarantees for\nour algorithm, and analyze the convergence behavior in terms of data\ndimensions. Finally, we demonstrate the performance of our algorithm on\nwell-known classification and regression problems, through extensive numerical\nstudies on large-scale datasets, and show that it achieves the highest\nperformance compared to several other widely used and specialized optimization\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 09:10:05 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Erdogdu", "Murat A.", ""], ["Bayati", "Mohsen", ""], ["Dicker", "Lee H.", ""]]}, {"id": "1611.06874", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Fl\\'avio Eler de Melo, Simon Maskell", "title": "Langevin Incremental Mixture Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel method through which local information about the\ntarget density can be used to construct an efficient importance sampler. The\nbackbone of the proposed method is the Incremental Mixture Importance Sampling\n(IMIS) algorithm of Raftery and Bao (2010), which builds a mixture importance\ndistribution incrementally, by positioning new mixture components where the\nimportance density lacks mass, relative to the target. The key innovation\nproposed here is that the mixture components used by IMIS are local\napproximations to the target density. In particular, their mean vectors and\ncovariance matrices are constructed by numerically solving certain differential\nequations, whose solution depends on the gradient field of the target\nlog-density. The new sampler has a number of advantages: a) it provides an\nextremely parsimonious parametrization of the mixture importance density, whose\nconfiguration effectively depends only on the shape of the target and on a\nsingle free parameter representing pseudo-time; b) it scales well with the\ndimensionality of the target; c) it can deal with targets that are not log-\nconcave. The performance of the proposed approach is demonstrated on a\nsynthetic non-Gaussian multimodal density, defined on up to eighty dimensions,\nand on a Bayesian logistic regression model, using the Sonar data-set. The\nJulia code implementing the importance sampler proposed here can be found at\nhttps:/github.com/mfasiolo/LIMIS.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 16:14:30 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Fasiolo", "Matteo", ""], ["de Melo", "Fl\u00e1vio Eler", ""], ["Maskell", "Simon", ""]]}, {"id": "1611.07056", "submitter": "Luca Martino", "authors": "Luca Martino, Victor Elvira, Gustau Camps-Valls", "title": "The Recycling Gibbs Sampler for Efficient Learning", "comments": "The MATLAB code of the numerical examples is provided at\n  http://isp.uv.es/code/RG.zip", "journal-ref": "Digital Signal Processing, Volume 74, 2018", "doi": "10.1016/j.dsp.2017.11.012", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods are essential tools for Bayesian inference. Gibbs\nsampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively\nused in signal processing, machine learning, and statistics, employed to draw\nsamples from complicated high-dimensional posterior distributions. The key\npoint for the successful application of the Gibbs sampler is the ability to\ndraw efficiently samples from the full-conditional probability density\nfunctions. Since in the general case this is not possible, in order to speed up\nthe convergence of the chain, it is required to generate auxiliary samples\nwhose information is eventually disregarded. In this work, we show that these\nauxiliary samples can be recycled within the Gibbs estimators, improving their\nefficiency with no extra cost. This novel scheme arises naturally after\npointing out the relationship between the standard Gibbs sampler and the chain\nrule used for sampling purposes. Numerical simulations involving simple and\nreal inference problems confirm the excellent performance of the proposed\nscheme in terms of accuracy and computational efficiency. In particular we give\nempirical evidence of performance in a toy example, inference of Gaussian\nprocesses hyperparameters, and learning dependence graphs through regression.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 21:13:00 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 14:59:08 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Martino", "Luca", ""], ["Elvira", "Victor", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1611.07417", "submitter": "Yili Hong", "authors": "Zhongnan Jin, Yimeng Xie, Yili Hong, and Jennifer H. Van Mullekom", "title": "ADDT: An R Package for Analysis of Accelerated Destructive Degradation\n  Test Data", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated destructive degradation tests (ADDT) are often used to collect\nnecessary data for assessing the long-term properties of polymeric materials.\nBased on the data, a thermal index (TI) is estimated. The TI can be useful for\nmaterial rating and comparisons. The R package ADDT provides the\nfunctionalities of performing the traditional method based on the least-squares\nmethod, the parametric method based on maximum likelihood estimation, and the\nsemiparametric method based on spline methods for analyzing ADDT data, and then\nestimating the TI for polymeric materials. In this chapter, we provide a\ndetailed introduction to the ADDT package. We provide a step-by-step\nillustration for the use of functions in the package. Publicly available\ndatasets are used for illustrations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 17:00:19 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Jin", "Zhongnan", ""], ["Xie", "Yimeng", ""], ["Hong", "Yili", ""], ["Van Mullekom", "Jennifer H.", ""]]}, {"id": "1611.07505", "submitter": "Matthew Friedlander D", "authors": "Matthew Friedlander", "title": "Fitting log-linear models in sparse contingency tables using the\n  eMLEloglin R package", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-linear modeling is a popular method for the analysis of contingency table\ndata. When the table is sparse, and the data falls on a proper face $F$ of the\nconvex support, there are consequences on model inference and model selection.\nKnowledge of the cells determining $F$ is crucial to mitigating these effects.\nWe introduce the R package (R Core Team (2016)) eMLEloglin for determining $F$\nand passing that information on to the glm package to fit the model properly.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 20:37:58 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 14:59:14 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Friedlander", "Matthew", ""]]}, {"id": "1611.07521", "submitter": "Manav Vohra", "authors": "Kemelli C. Estacio-Hiroms and Ernesto E. Prudencio and Nicholas P.\n  Malaya and Manav Vohra and Damon McDougall", "title": "The QUESO Library, User's Manual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QUESO stands for Quantification of Uncertainty for Estimation, Simulation and\nOptimization and consists of algorithms and C++ classes intended for research\nin uncertainty quantification, including the solution of statistical inverse\nproblem and statistical forward problems, the validation of mathematical models\nunder uncertainty, and the prediction of quantities of interest from such\nmodels along with quantification of their uncertainties. QUESO is designed for\nflexibility, portability, ease of use and ease of extension. Its software\ndesign follows an object oriented approach and its code is written on C++ and\nover MPI. It can run over uniprocessor and multiprocessor environments. QUESO\ncontains two forms of documentation: a user's manual available in PDF format\nand a lower level code documentation available in web based/HTML format. The\npresent document is a user's manual which provides an overview of the\ncapabilities of QUESO, procedures for software execution, and includes example\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 04:05:26 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Estacio-Hiroms", "Kemelli C.", ""], ["Prudencio", "Ernesto E.", ""], ["Malaya", "Nicholas P.", ""], ["Vohra", "Manav", ""], ["McDougall", "Damon", ""]]}, {"id": "1611.07537", "submitter": "Matthew Friedlander D", "authors": "Matthew Friedlander and Adrian Dobra and Helene Massam and Laurent\n  Briollais", "title": "Analyzing Genome-wide Association Study Data with the R Package genMOSS", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package (R Core Team (2016)) genMOSS is specifically designed for the\nBayesian analysis of genome-wide association study data. The package implements\nthe mode oriented stochastic search (MOSS) procedure as well as a simple moving\nwindow approach to identify combinations of single nucleotide polymorphisms\nassociated with a response. The prior used in Bayesian computations is the\ngeneralized hyper Dirichlet.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 21:08:28 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Friedlander", "Matthew", ""], ["Dobra", "Adrian", ""], ["Massam", "Helene", ""], ["Briollais", "Laurent", ""]]}, {"id": "1611.07554", "submitter": "Sebastian Matera", "authors": "Max J. Hoffmann, Felix Engelmann, Sebastian Matera", "title": "A practical approach to the sensitivity analysis for kinetic Monte Carlo\n  simulation of heterogeneous catalysis", "comments": null, "journal-ref": null, "doi": "10.1063/1.4974261", "report-no": null, "categories": "physics.comp-ph physics.chem-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lattice kinetic Monte Carlo simulations have become a vital tool for\npredictive quality atomistic understanding of complex surface chemical reaction\nkinetics over a wide range of reaction conditions. In order to expand their\npractical value in terms of giving guidelines for atomic level design of\ncatalytic systems, it is very desirable to readily evaluate a sensitivity\nanalysis for a given model. The result of such a sensitivity analysis\nquantitatively expresses the dependency of the turnover frequency, being the\nmain output variable, on the rate constants entering the model. In the past the\napplication of sensitivity analysis, such as Degree of Rate Control, has been\nhampered by its exuberant computational effort required to accurately sample\nnumerical derivatives of a property that is obtained from a stochastic\nsimulation method. In this study we present an efficient and robust three stage\napproach that is capable of reliably evaluating the sensitivity measures for\nstiff microkinetic models as we demonstrate using CO oxidation on RuO2(110) as\na prototypical reaction. In a first step, we utilize the Fisher Information\nMatrix for filtering out elementary processes which only yield negligible\nsensitivity. Then we employ an estimator based on linear response theory for\ncalculating the sensitivity measure for non-critical conditions which covers\nthe majority of cases. Finally we adopt a method for sampling coupled finite\ndifferences for evaluating the sensitivity measure of lattice based models.\nThis allows efficient evaluation even in critical regions near a second order\nphase transition that are hitherto difficult to control. The combined approach\nleads to significant computational savings over straightforward numerical\nderivatives and should aid in accelerating the nano scale design of\nheterogeneous catalysts.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 22:16:23 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Hoffmann", "Max J.", ""], ["Engelmann", "Felix", ""], ["Matera", "Sebastian", ""]]}, {"id": "1611.07873", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead, Joris Bierkens, Murray Pollock and Gareth O Roberts", "title": "Piecewise Deterministic Markov Processes for Continuous-Time Monte Carlo", "comments": null, "journal-ref": "Statist. Sci., Volume 33, Number 3 (2018), 386-412", "doi": "10.1214/18-STS648", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there have been exciting developments in Monte Carlo methods, with\nthe development of new MCMC and sequential Monte Carlo (SMC) algorithms which\nare based on continuous-time, rather than discrete-time, Markov processes. This\nhas led to some fundamentally new Monte Carlo algorithms which can be used to\nsample from, say, a posterior distribution. Interestingly, continuous-time\nalgorithms seem particularly well suited to Bayesian analysis in big-data\nsettings as they need only access a small sub-set of data points at each\niteration, and yet are still guaranteed to target the true posterior\ndistribution. Whilst continuous-time MCMC and SMC methods have been developed\nindependently we show here that they are related by the fact that both involve\nsimulating a piecewise deterministic Markov process. Furthermore we show that\nthe methods developed to date are just specific cases of a potentially much\nwider class of continuous-time Monte Carlo algorithms. We give an informal\nintroduction to piecewise deterministic Markov processes, covering the aspects\nrelevant to these new Monte Carlo algorithms, with a view to making the\ndevelopment of new continuous-time Monte Carlo more accessible. We focus on how\nand why sub-sampling ideas can be used with these algorithms, and aim to give\ninsight into how these new algorithms can be implemented, and what are some of\nthe issues that affect their efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 16:42:29 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Fearnhead", "Paul", ""], ["Bierkens", "Joris", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O", ""]]}, {"id": "1611.09252", "submitter": "Yasin Abbasi-Yadkori", "authors": "Yasin Abbasi-Yadkori", "title": "Fast Mixing Random Walks and Regularity of Incompressible Vector Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show sufficient conditions under which the \\textsc{BallWalk} algorithm\nmixes fast in a bounded connected subset of $\\Real^n$. In particular, we show\nfast mixing if the space is the transformation of a convex space under a smooth\nincompressible flow. Construction of such smooth flows is in turn reduced to\nthe study of the regularity of the solution of the Dirichlet problem for\nLaplace's equation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 15:35:40 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""]]}, {"id": "1611.09790", "submitter": "Xu Chen", "authors": "Xu Chen, Shaan Qamar and Surya T. Tokdar", "title": "Paired-move multiple-try stochastic search for Bayesian variable\n  selection", "comments": "28 pages; 5 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is a key issue when analyzing high-dimensional data. The\nexplosion of data with large sample sizes and dimensionality brings new\nchallenges to this problem in both inference accuracy and computational\ncomplexity. To alleviate these problems, we propose a new scalable Markov chain\nMonte Carlo (MCMC) sampling algorithm for \"large $p$ small $n$\" scenarios by\ngeneralizing multiple-try Metropolis to discrete model spaces and further\nincorporating neighborhood-based stochastic search. The proof of reversibility\nof the proposed MCMC algorithm is provided. Extensive simulation studies are\nperformed to examine the efficiency of the new algorithm compared with existing\nmethods. A real data example is provided to illustrate the prediction\nperformances of the new algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:01:06 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Chen", "Xu", ""], ["Qamar", "Shaan", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1611.10171", "submitter": "Janek Thomas", "authors": "Janek Thomas, Andreas Mayr, Bernd Bischl, Matthias Schmid, Adam Smith,\n  Benjamin Hofner", "title": "Stability selection for component-wise gradient boosting in multiple\n  dimensions", "comments": "16 pages", "journal-ref": null, "doi": "10.1007/s11222-017-9754-6", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for boosting generalized additive models for\nlocation, scale and shape (GAMLSS) that allows to incorporate stability\nselection, an increasingly popular way to obtain stable sets of covariates\nwhile controlling the per-family error rate (PFER). The model is fitted\nrepeatedly to subsampled data and variables with high selection frequencies are\nextracted. To apply stability selection to boosted GAMLSS, we develop a new\n\"noncyclical\" fitting algorithm that incorporates an additional selection step\nof the best-fitting distribution parameter in each iteration. This new\nalgorithms has the additional advantage that optimizing the tuning parameters\nof boosting is reduced from a multi-dimensional to a one-dimensional problem\nwith vastly decreased complexity. The performance of the novel algorithm is\nevaluated in an extensive simulation study. We apply this new algorithm to a\nstudy to estimate abundance of common eider in Massachusetts, USA, featuring\nexcess zeros, overdispersion, non-linearity and spatio-temporal structures.\nEider abundance is estimated via boosted GAMLSS, allowing both mean and\noverdispersion to be regressed on covariates. Stability selection is used to\nobtain a sparse set of stable predictors.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 14:28:09 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Thomas", "Janek", ""], ["Mayr", "Andreas", ""], ["Bischl", "Bernd", ""], ["Schmid", "Matthias", ""], ["Smith", "Adam", ""], ["Hofner", "Benjamin", ""]]}, {"id": "1611.10242", "submitter": "Owen Thomas", "authors": "Owen Thomas, Ritabrata Dutta, Jukka Corander, Samuel Kaski, Michael U.\n  Gutmann", "title": "Likelihood-free inference by ratio estimation", "comments": "Accepted to Bayesian Analysis (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of parametric statistical inference when likelihood\ncomputations are prohibitively expensive but sampling from the model is\npossible. Several so-called likelihood-free methods have been developed to\nperform inference in the absence of a likelihood function. The popular\nsynthetic likelihood approach infers the parameters by modelling summary\nstatistics of the data by a Gaussian probability distribution. In another\npopular approach called approximate Bayesian computation, the inference is\nperformed by identifying parameter values for which the summary statistics of\nthe simulated data are close to those of the observed data. Synthetic\nlikelihood is easier to use as no measure of `closeness' is required but the\nGaussianity assumption is often limiting. Moreover, both approaches require\njudiciously chosen summary statistics. We here present an alternative inference\napproach that is as easy to use as synthetic likelihood but not as restricted\nin its assumptions, and that, in a natural way, enables automatic selection of\nrelevant summary statistic from a large set of candidates. The basic idea is to\nframe the problem of estimating the posterior as a problem of estimating the\nratio between the data generating distribution and the marginal distribution.\nThis problem can be solved by logistic regression, and including regularising\npenalty terms enables automatic selection of the summary statistics relevant to\nthe inference task. We illustrate the general theory on canonical examples and\nemploy it to perform inference for challenging stochastic nonlinear dynamical\nsystems and high-dimensional summary statistics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 16:03:07 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 19:38:59 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 15:44:21 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 10:14:43 GMT"}, {"version": "v5", "created": "Wed, 16 Oct 2019 15:09:31 GMT"}, {"version": "v6", "created": "Fri, 11 Sep 2020 09:28:01 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Thomas", "Owen", ""], ["Dutta", "Ritabrata", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1611.10359", "submitter": "Shinichiro Shirota Mr", "authors": "Shinichiro Shirota and Alan E. Gelfand", "title": "Inference for log Gaussian Cox processes using an approximate marginal\n  posterior", "comments": "previous version is \"Approximate Marginal Posterior for Log Gaussian\n  Cox Processes\". arXiv admin note: text overlap with arXiv:1606.07984", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log Gaussian Cox process is a flexible class of point pattern models for\ncapturing spatial and spatio-temporal dependence for point patterns. Model\nfitting requires approximation of stochastic integrals which is implemented\nthrough discretization of the domain of interest. With fine scale\ndiscretization, inference based on Markov chain Monte Carlo is computationally\nheavy because of the cost of repeated iteration or inversion or Cholesky\ndecomposition (cubic order) of high dimensional covariance matrices associated\nwith latent Gaussian variables. Furthermore, hyperparameters for latent\nGaussian variables have strong dependence with sampled latent Gaussian\nvariables. Altogether, standard Markov chain Monte Carlo strategies are\ninefficient and not well behaved.\n  In this paper, we propose an efficient computational strategy for fitting and\ninferring with spatial log Gaussian Cox processes. The proposed algorithm is\nbased on a pseudo-marginal Markov chain Monte Carlo approach. We estimate an\napproximate marginal posterior for parameters of log Gaussian Cox processes and\npropose comprehensive model inference strategy. We provide details for all of\nthe above along with some simulation investigation for the univariate and\nmultivariate settings. As an example, we present an analysis of a point pattern\nof locations of three tree species, exhibiting positive and negative\ninteraction between different species.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:59:10 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Gelfand", "Alan E.", ""]]}]