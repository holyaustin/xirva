[{"id": "1205.0070", "submitter": "Radford M. Neal", "authors": "Radford M. Neal", "title": "How to view an MCMC simulation as a permutation, with applications to\n  parallel simulation and improved importance sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": "Tech. Rep. No. 1201, Dept. of Statistics, University of Toronto", "categories": "stat.CO physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a Markov chain defined on a finite state space, X, that leaves\ninvariant the uniform distribution on X, and whose transition probabilities are\ninteger multiples of 1/Q, for some integer Q. I show how a simulation of n\ntransitions of this chain starting at x_0 can be viewed as applying a random\npermutation on the space XxU, where U={0,1,...,Q-1}, to the start state\n(x_0,u_0), with u_0 drawn uniformly from U. This result can be applied to a\nnon-uniform distribution with probabilities that are integer multiples of 1/P,\nfor some integer P, by representing it as the marginal distribution for X from\nthe uniform distribution on a suitably-defined subset of XxY, where\nY={0,1,...,P-1}. By letting Q, P, and the cardinality of X go to infinity, this\nresult can be generalized to non-rational probabilities and to continuous state\nspaces, with permutations on a finite space replaced by volume-preserving\none-to-one maps from a continuous space to itself. These constructions can be\nefficiently implemented for chains commonly used in Markov chain Monte Carlo\n(MCMC) simulations. I present two applications in this context - simulation of\nK realizations of a chain from K initial states, but with transitions defined\nby a single stream of random numbers, as may be efficient with a vector\nprocessor or multiple processors, and use of MCMC to improve an importance\nsampling distribution that already has substantial overlap with the\ndistribution of interest. I also discuss the implications of this \"permutation\nMCMC\" method regarding the role of randomness in MCMC simulation, and the\npotential use of non-random and quasi-random numbers.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2012 01:47:04 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Neal", "Radford M.", ""]]}, {"id": "1205.0310", "submitter": "James Scott", "authors": "Nicholas G. Polson, James G. Scott, Jesse Windle", "title": "Bayesian inference for logistic models using Polya-Gamma latent\n  variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new data-augmentation strategy for fully Bayesian inference in\nmodels with binomial likelihoods. The approach appeals to a new class of\nPolya-Gamma distributions, which are constructed in detail. A variety of\nexamples are presented to show the versatility of the method, including\nlogistic regression, negative binomial regression, nonlinear mixed-effects\nmodels, and spatial models for count data. In each case, our data-augmentation\nstrategy leads to simple, effective methods for posterior inference that: (1)\ncircumvent the need for analytic approximations, numerical integration, or\nMetropolis-Hastings; and (2) outperform other known data-augmentation\nstrategies, both in ease of use and in computational efficiency. All methods,\nincluding an efficient sampler for the Polya-Gamma distribution, are\nimplemented in the R package BayesLogit.\n  In the technical supplement appended to the end of the paper, we provide\nfurther details regarding the generation of Polya-Gamma random variables; the\nempirical benchmarks reported in the main manuscript; and the extension of the\nbasic data-augmentation framework to contingency tables and multinomial\noutcomes.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 02:52:37 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2013 20:20:15 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2013 16:19:08 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""], ["Windle", "Jesse", ""]]}, {"id": "1205.0482", "submitter": "Luca Martino", "authors": "Luca Martino, David Luengo, Joaqu\\'in M\\'iguez", "title": "On the Generalized Ratio of Uniforms as a Combination of Transformed\n  Rejection and Extended Inverse of Density Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the relationship among three classical sampling\ntechniques: the inverse of density (Khintchine's theorem), the transformed\nrejection (TR) and the generalized ratio of uniforms (GRoU). Given a monotonic\nprobability density function (PDF), we show that the transformed area obtained\nusing the generalized ratio of uniforms method can be found equivalently by\napplying the transformed rejection sampling approach to the inverse function of\nthe target density. Then we provide an extension of the classical inverse of\ndensity idea, showing that it is completely equivalent to the GRoU method for\nmonotonic densities. Although we concentrate on monotonic probability density\nfunctions (PDFs), we also discuss how the results presented here can be\nextended to any non-monotonic PDF that can be decomposed into a collection of\nintervals where it is monotonically increasing or decreasing. In this general\ncase, we show the connections with transformations of certain random variables\nand the generalized inverse PDF with the GRoU technique. Finally, we also\nintroduce a GRoU technique to handle unbounded target densities.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 16:20:31 GMT"}, {"version": "v2", "created": "Mon, 7 May 2012 20:02:00 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2012 13:52:34 GMT"}, {"version": "v4", "created": "Sun, 22 Jul 2012 18:32:26 GMT"}, {"version": "v5", "created": "Sat, 4 Aug 2012 14:57:00 GMT"}, {"version": "v6", "created": "Sat, 11 Aug 2012 16:42:07 GMT"}, {"version": "v7", "created": "Tue, 16 Jul 2013 15:48:18 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Martino", "Luca", ""], ["Luengo", "David", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1205.0499", "submitter": "Murali Haran", "authors": "Murali Haran and Luke Tierney", "title": "On automating Markov chain Monte Carlo for a class of spatial models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms provide a very general recipe for\nestimating properties of complicated distributions. While their use has become\ncommonplace and there is a large literature on MCMC theory and practice, MCMC\nusers still have to contend with several challenges with each implementation of\nthe algorithm. These challenges include determining how to construct an\nefficient algorithm, finding reasonable starting values, deciding whether the\nsample-based estimates are accurate, and determining an appropriate length\n(stopping rule) for the Markov chain. We describe an approach for resolving\nthese issues in a theoretically sound fashion in the context of spatial\ngeneralized linear models, an important class of models that result in\nchallenging posterior distributions. Our approach combines analytical\napproximations for constructing provably fast mixing MCMC algorithms, and takes\nadvantage of recent developments in MCMC theory. We apply our methods to real\ndata examples, and find that our MCMC algorithm is automated and efficient.\nFurthermore, since starting values, rigorous error estimates and theoretically\njustified stopping rules for the sampling algorithm are all easily obtained for\nour examples, our MCMC-based estimation is practically as easy to perform as\nMonte Carlo estimation based on independent and identically distributed draws.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 17:43:00 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Haran", "Murali", ""], ["Tierney", "Luke", ""]]}, {"id": "1205.0831", "submitter": "Andino Maseleno", "authors": "Andino Maseleno, Md. Mahmud Hasan", "title": "African Trypanosomiasis Detection using Dempster-Shafer Theory", "comments": null, "journal-ref": "International Journal of Emerging Trends in Computing and\n  Information Sciences, Vol. 3, No. 4, 2012, pp. 480 - 487", "doi": null, "report-no": null, "categories": "cs.AI stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  World Health Organization reports that African Trypanosomiasis affects mostly\npoor populations living in remote rural areas of Africa that can be fatal if\nproperly not treated. This paper presents Dempster-Shafer Theory for the\ndetection of African trypanosomiasis. Sustainable elimination of African\ntrypanosomiasis as a public-health problem is feasible and requires continuous\nefforts and innovative approaches. In this research, we implement\nDempster-Shafer theory for detecting African trypanosomiasis and displaying the\nresult of detection process. We describe eleven symptoms as major symptoms\nwhich include fever, red urine, skin rash, paralysis, headache, bleeding around\nthe bite, joint the paint, swollen lymph nodes, sleep disturbances, meningitis\nand arthritis. Dempster-Shafer theory to quantify the degree of belief, our\napproach uses Dempster-Shafer theory to combine beliefs under conditions of\nuncertainty and ignorance, and allows quantitative measurement of the belief\nand plausibility in our identification result.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2012 22:33:34 GMT"}], "update_date": "2012-05-07", "authors_parsed": [["Maseleno", "Andino", ""], ["Hasan", "Md. Mahmud", ""]]}, {"id": "1205.1076", "submitter": "Matti Vihola", "authors": "Blazej Miasojedow, Eric Moulines and Matti Vihola", "title": "Adaptive parallel tempering algorithm", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel tempering is a generic Markov chain Monte Carlo sampling method\nwhich allows good mixing with multimodal target distributions, where\nconventional Metropolis-Hastings algorithms often fail. The mixing properties\nof the sampler depend strongly on the choice of tuning parameters, such as the\ntemperature schedule and the proposal distribution used for local exploration.\nWe propose an adaptive algorithm which tunes both the temperature schedule and\nthe parameters of the random-walk Metropolis kernel automatically. We prove the\nconvergence of the adaptation and a strong law of large numbers for the\nalgorithm. We illustrate the performance of our method with examples. Our\nempirical findings indicate that the algorithm can cope well with different\nkind of scenarios without prior tuning.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2012 22:01:04 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Miasojedow", "Blazej", ""], ["Moulines", "Eric", ""], ["Vihola", "Matti", ""]]}, {"id": "1205.1245", "submitter": "Martin Vincent", "authors": "Martin Vincent, Niels Richard Hansen", "title": "Sparse group lasso and high dimensional multinomial classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse group lasso optimization problem is solved using a coordinate\ngradient descent algorithm. The algorithm is applicable to a broad class of\nconvex loss functions. Convergence of the algorithm is established, and the\nalgorithm is used to investigate the performance of the multinomial sparse\ngroup lasso classifier. On three different real data examples the multinomial\ngroup lasso clearly outperforms multinomial lasso in terms of achieved\nclassification error rate and in terms of including fewer features for the\nclassification. The run-time of our sparse group lasso implementation is of the\nsame order of magnitude as the multinomial lasso algorithm implemented in the R\npackage glmnet. Our implementation scales well with the problem size. One of\nthe high dimensional examples considered is a 50 class classification problem\nwith 10k features, which amounts to estimating 500k parameters. The\nimplementation is available as the R package msgl.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2012 20:18:13 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2013 09:36:02 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Vincent", "Martin", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "1205.1774", "submitter": "Art Owen", "authors": "Art B. Owen", "title": "Variance components and generalized Sobol' indices", "comments": "24 pages, 0 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces generalized Sobol' indices, compares strategies for\ntheir estimation, and makes a systematic search for efficient estimators. Of\nparticular interest are contrasts, sums of squares and indices of bilinear form\nwhich allow a reduced number of function evaluations compared to alternatives.\nThe bilinear framework includes some efficient estimators from Saltelli (2002)\nand Mauntz (2002) as well as some new estimators for specific variance\ncomponents and mean dimensions. This paper also provides a bias corrected\nversion of the estimator of Janon et al.\\,(2012) and extends the bias\ncorrection to generalized Sobol' indices. Some numerical comparisons are given.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 19:04:23 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Owen", "Art B.", ""]]}, {"id": "1205.1880", "submitter": "Paolo D'Alberto", "authors": "Paolo D'Alberto and Chris Drome and Ali Dasdan", "title": "Non-Parametric Methods Applied to the N-Sample Series Comparison", "comments": "65 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly and similarity detection in multidimensional series have a long\nhistory and have found practical usage in many different fields such as\nmedicine, networks, and finance. Anomaly detection is of great appeal for many\ndifferent disciplines; for example, mathematicians searching for a unified\nmathematical formulation based on probability, statisticians searching for\nerror bound estimates, and computer scientists who are trying to design fast\nalgorithms, to name just a few. In summary, we have two contributions: First,\nwe present a self-contained survey of the most promising methods being used in\nthe fields of machine learning, statistics, and bio-informatics today. Included\nwe present discussions about conformal prediction, kernels in the Hilbert\nspace, Kolmogorov's information measure, and non-parametric cumulative\ndistribution function comparison methods (NCDF). Second, building upon this\nfoundation, we provide a powerful NCDF method for series with small\ndimensionality. Through a combination of data organization and statistical\ntests, we describe extensions that scale well with increased dimensionality.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 06:35:04 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["D'Alberto", "Paolo", ""], ["Drome", "Chris", ""], ["Dasdan", "Ali", ""]]}, {"id": "1205.1997", "submitter": "Aaron Francis McDaid", "authors": "Aaron F. McDaid, Brendan Thomas Murphy, Nial Friel, Neil J. Hurley", "title": "Model-based clustering in networks with Stochastic Community Finding", "comments": "Presented at COMPSTAT 2012 http://www.compstat2012.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In the model-based clustering of networks, blockmodelling may be used to\nidentify roles in the network. We identify a special case of the Stochastic\nBlock Model (SBM) where we constrain the cluster-cluster interactions such that\nthe density inside the clusters of nodes is expected to be greater than the\ndensity between clusters. This corresponds to the intuition behind\ncommunity-finding methods, where nodes tend to clustered together if they link\nto each other. We call this model Stochastic Community Finding (SCF) and\npresent an efficient MCMC algorithm which can cluster the nodes, given the\nnetwork. The algorithm is evaluated on synthetic data and is applied to a\nsocial network of interactions at a karate club and at a monastery,\ndemonstrating how the SCF finds the 'ground truth' clustering where sometimes\nthe SBM does not. The SCF is only one possible form of constraint or\nspecialization that may be applied to the SBM. In a more supervised context, it\nmay be appropriate to use other specializations to guide the SBM.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 14:33:29 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2012 23:20:56 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["McDaid", "Aaron F.", ""], ["Murphy", "Brendan Thomas", ""], ["Friel", "Nial", ""], ["Hurley", "Neil J.", ""]]}, {"id": "1205.2106", "submitter": "Lingsong Zhang Lingsong Zhang", "authors": "Lingsong Zhang and Zhengyuan Zhu", "title": "Spatial Multiresolution Cluster Detection Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel multi-resolution cluster detection (MCD) method is proposed to\nidentify irregularly shaped clusters in space. Multi-scale test statistic on a\nsingle cell is derived based on likelihood ratio statistic for Bernoulli\nsequence, Poisson sequence and Normal sequence. A neighborhood variability\nmeasure is defined to select the optimal test threshold. The MCD method is\ncompared with single scale testing methods controlling for false discovery rate\nand the spatial scan statistics using simulation and f-MRI data. The MCD method\nis shown to be more effective for discovering irregularly shaped clusters, and\nthe implementation of this method does not require heavy computation, making it\nsuitable for cluster detection for large spatial data.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 21:15:08 GMT"}], "update_date": "2012-05-11", "authors_parsed": [["Zhang", "Lingsong", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1205.2334", "submitter": "Yong Zhang", "authors": "Zhaosong Lu and Yong Zhang", "title": "Sparse Approximation via Penalty Decomposition Methods", "comments": "31 pages, 3 figures and 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:1008.5372", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider sparse approximation problems, that is, general\n$l_0$ minimization problems with the $l_0$-\"norm\" of a vector being a part of\nconstraints or objective function. In particular, we first study the\nfirst-order optimality conditions for these problems. We then propose penalty\ndecomposition (PD) methods for solving them in which a sequence of penalty\nsubproblems are solved by a block coordinate descent (BCD) method. Under some\nsuitable assumptions, we establish that any accumulation point of the sequence\ngenerated by the PD methods satisfies the first-order optimality conditions of\nthe problems. Furthermore, for the problems in which the $l_0$ part is the only\nnonconvex part, we show that such an accumulation point is a local minimizer of\nthe problems. In addition, we show that any accumulation point of the sequence\ngenerated by the BCD method is a saddle point of the penalty subproblem.\nMoreover, for the problems in which the $l_0$ part is the only nonconvex part,\nwe establish that such an accumulation point is a local minimizer of the\npenalty subproblem. Finally, we test the performance of our PD methods by\napplying them to sparse logistic regression, sparse inverse covariance\nselection, and compressed sensing problems. The computational results\ndemonstrate that our methods generally outperform the existing methods in terms\nof solution quality and/or speed.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2012 18:25:06 GMT"}, {"version": "v2", "created": "Wed, 30 May 2012 00:49:30 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "1205.2643", "submitter": "Matthias Hoffman", "authors": "Matthias Hoffman, Hendrik Kueck, Nando de Freitas, Arnaud Doucet", "title": "New inference strategies for solving Markov Decision Processes using\n  reversible jump MCMC", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-223-231", "categories": "cs.LG cs.SY math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we build on previous work which uses inferences techniques, in\nparticular Markov Chain Monte Carlo (MCMC) methods, to solve parameterized\ncontrol problems. We propose a number of modifications in order to make this\napproach more practical in general, higher-dimensional spaces. We first\nintroduce a new target distribution which is able to incorporate more reward\ninformation from sampled trajectories. We also show how to break strong\ncorrelations between the policy parameters and sampled trajectories in order to\nsample more freely. Finally, we show how to incorporate these techniques in a\nprincipled manner to obtain estimates of the optimal policy.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 15:26:47 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Hoffman", "Matthias", ""], ["Kueck", "Hendrik", ""], ["de Freitas", "Nando", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1205.2746", "submitter": "Alex Lenkoski", "authors": "Yuan Cheng, Alex Lenkoski", "title": "A Multivariate Graphical Stochastic Volatility Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian Graphical Model (GGM) is a popular tool for incorporating\nsparsity into joint multivariate distributions. The G-Wishart distribution, a\nconjugate prior for precision matrices satisfying general GGM constraints, has\nnow been in existence for over a decade. However, due to the lack of a direct\nsampler, its use has been limited in hierarchical Bayesian contexts, relegating\nmixing over the class of GGMs mostly to situations involving standard Gaussian\nlikelihoods. Recent work, however, has developed methods that couple model and\nparameter moves, first through reversible jump methods and later by direct\nevaluation of conditional Bayes factors and subsequent resampling. Further,\nmethods for avoiding prior normalizing constant calculations--a serious\nbottleneck and source of numerical instability--have been proposed. We review\nand clarify these developments and then propose a new methodology for GGM\ncomparison that blends many recent themes. Theoretical developments and\ncomputational timing experiments reveal an algorithm that has limited\ncomputational demands and dramatically improves on computing times of existing\nmethods. We conclude by developing a parsimonious multivariate stochastic\nvolatility model that embeds GGM uncertainty in a larger hierarchical\nframework. The method is shown to be capable of adapting to the extreme swings\nin market volatility experienced in 2008 after the collapse of Lehman Brothers,\noffering considerable improvement in posterior predictive distribution\ncalibration.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2012 07:50:17 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Cheng", "Yuan", ""], ["Lenkoski", "Alex", ""]]}, {"id": "1205.2911", "submitter": "Antonino Abbruzzo AA", "authors": "E. C. Wit, A. Abbruzzo", "title": "Factorial graphical lasso for dynamic networks", "comments": "30 pp, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks models describe a growing number of important scientific\nprocesses, from cell biology and epidemiology to sociology and finance. There\nare many aspects of dynamical networks that require statistical considerations.\nIn this paper we focus on determining network structure. Estimating dynamic\nnetworks is a difficult task since the number of components involved in the\nsystem is very large. As a result, the number of parameters to be estimated is\nbigger than the number of observations. However, a characteristic of many\nnetworks is that they are sparse. For example, the molecular structure of genes\nmake interactions with other components a highly-structured and therefore\nsparse process.\n  Penalized Gaussian graphical models have been used to estimate sparse\nnetworks. However, the literature has focussed on static networks, which lack\nspecific temporal constraints. We propose a structured Gaussian dynamical\ngraphical model, where structures can consist of specific time dynamics, known\npresence or absence of links and block equality constraints on the parameters.\nThus, the number of parameters to be estimated is reduced and accuracy of the\nestimates, including the identification of the network, can be tuned up. Here,\nwe show that the constrained optimization problem can be solved by taking\nadvantage of an efficient solver, logdetPPA, developed in convex optimization.\nMoreover, model selection methods for checking the sensitivity of the inferred\nnetworks are described. Finally, synthetic and real data illustrate the\nproposed methodologies.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2012 22:02:28 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Wit", "E. C.", ""], ["Abbruzzo", "A.", ""]]}, {"id": "1205.3246", "submitter": "Shigeki Nakagome", "authors": "Shigeki Nakagome, Kenji Fukumizu and Shuhei Mano", "title": "Kernel Approximate Bayesian Computation for Population Genetic\n  Inferences", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a likelihood-free approach for\nBayesian inferences based on a rejection algorithm method that applies a\ntolerance of dissimilarity between summary statistics from observed and\nsimulated data. Although several improvements to the algorithm have been\nproposed, none of these improvements avoid the following two sources of\napproximation: 1) lack of sufficient statistics: sampling is not from the true\nposterior density given data but from an approximate posterior density given\nsummary statistics; and 2) non-zero tolerance: sampling from the posterior\ndensity given summary statistics is achieved only in the limit of zero\ntolerance. The first source of approximation can be improved by adding a\nsummary statistic, but an increase in the number of summary statistics could\nintroduce additional variance caused by the low acceptance rate. Consequently,\nmany researchers have attempted to develop techniques to choose informative\nsummary statistics. The present study evaluated the utility of a kernel-based\nABC method (Fukumizu et al. 2010, arXiv:1009.5736 and 2011, NIPS 24: 1549-1557)\nfor complex problems that demand many summary statistics. Specifically, kernel\nABC was applied to population genetic inference. We demonstrate that, in\ncontrast to conventional ABCs, kernel ABC can incorporate a large number of\nsummary statistics while maintaining high performance of the inference.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2012 03:20:04 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 07:23:56 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2013 03:25:11 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Nakagome", "Shigeki", ""], ["Fukumizu", "Kenji", ""], ["Mano", "Shuhei", ""]]}, {"id": "1205.3347", "submitter": "Eric Frichot", "authors": "Eric Frichot (1), Sean Schoville (1), Guillaume Bouchard (2) and\n  Olivier Fran\\c{c}ois (1) ((1) UJF, CNRS, TIMC-IMAG, FRANCE, (2) Xerox\n  Research Center Europe, France)", "title": "Testing for Associations between Loci and Environmental Gradients Using\n  Latent Factor Mixed Models", "comments": "29 pages with 8 pages of Supplementary Material (V2 revised\n  presentation and results part)", "journal-ref": "Mol Biol Evol (2013) 30 (7): 1687-1699", "doi": "10.1093/molbev/mst063", "report-no": null, "categories": "q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptation to local environments often occurs through natural selection\nacting on a large number of loci, each having a weak phenotypic effect. One way\nto detect these loci is to identify genetic polymorphisms that exhibit high\ncorrelation with environmental variables used as proxies for ecological\npressures. Here, we propose new algorithms based on population genetics,\necological modeling, and statistical learning techniques to screen genomes for\nsignatures of local adaptation. Implemented in the computer program \"latent\nfactor mixed model\" (LFMM), these algorithms employ an approach in which\npopulation structure is introduced using unobserved variables. These fast and\ncomputationally efficient algorithms detect correlations between environmental\nand genetic variation while simultaneously inferring background levels of\npopulation structure. Comparing these new algorithms with related methods\nprovides evidence that LFMM can efficiently estimate random effects due to\npopulation history and isolation-by-distance patterns when computing\ngene-environment correlations, and decrease the number of false-positive\nassociations in genome scans. We then apply these models to plant and human\ngenetic data, identifying several genes with functions related to development\nthat exhibit strong correlations with climatic gradients.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2012 12:46:34 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2012 12:34:56 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2013 15:42:19 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Frichot", "Eric", ""], ["Schoville", "Sean", ""], ["Bouchard", "Guillaume", ""], ["Fran\u00e7ois", "Olivier", ""]]}, {"id": "1205.3845", "submitter": "Luke Bornn", "authors": "Luke Bornn, Marian Anghel, Ingo Steinwart", "title": "Forecasting with Historical Data or Process Knowledge under\n  Misspecification: A Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with the task of forecasting a dynamic system, practitioners often\nhave available historical data, knowledge of the system, or a combination of\nboth. While intuition dictates that perfect knowledge of the system should in\ntheory yield perfect forecasting, often knowledge of the system is only\npartially known, known up to parameters, or known incorrectly. In contrast,\nforecasting using previous data without any process knowledge might result in\naccurate prediction for simple systems, but will fail for highly nonlinear and\nchaotic systems. In this paper, the authors demonstrate how even in chaotic\nsystems, forecasting with historical data is preferable to using process\nknowledge if this knowledge exhibits certain forms of misspecification. Through\nan extensive simulation study, a range of misspecification and forecasting\nscenarios are examined with the goal of gaining an improved understanding of\nthe circumstances under which forecasting from historical data is to be\npreferred over using process knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2012 04:06:29 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Bornn", "Luke", ""], ["Anghel", "Marian", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1205.3906", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan, David J. Nott", "title": "Variational Inference for Generalized Linear Mixed Models Using\n  Partially Noncentered Parametrizations", "comments": "Published in at http://dx.doi.org/10.1214/13-STS418 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 168-188", "doi": "10.1214/13-STS418", "report-no": "IMS-STS-STS418", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effects of different parametrizations on the convergence of Bayesian\ncomputational algorithms for hierarchical models are well explored. Techniques\nsuch as centering, noncentering and partial noncentering can be used to\naccelerate convergence in MCMC and EM algorithms but are still not well studied\nfor variational Bayes (VB) methods. As a fast deterministic approach to\nposterior approximation, VB is attracting increasing interest due to its\nsuitability for large high-dimensional data. Use of different parametrizations\nfor VB has not only computational but also statistical implications, as\ndifferent parametrizations are associated with different factorized posterior\napproximations. We examine the use of partially noncentered parametrizations in\nVB for generalized linear mixed models (GLMMs). Our paper makes four\ncontributions. First, we show how to implement an algorithm called nonconjugate\nvariational message passing for GLMMs. Second, we show that the partially\nnoncentered parametrization can adapt to the quantity of information in the\ndata and determine a parametrization close to optimal. Third, we show that\npartial noncentering can accelerate convergence and produce more accurate\nposterior approximations than centering or noncentering. Finally, we\ndemonstrate how the variational lower bound, produced as part of the\ncomputation, can be useful for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2012 11:17:08 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 02:28:54 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2013 08:09:12 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Tan", "Linda S. L.", ""], ["Nott", "David J.", ""]]}, {"id": "1205.4062", "submitter": "Andres Christen", "authors": "J. Andr\\'es Christen, Colin Fox, Diego Andr\\'es P\\'erez-Ruiz and Mario\n  Santana-Cibrian", "title": "On optimal direction gibbs sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Gibbs kernels are those that may take any direction not\nnecessarily bounded to each axis along the parameters of the objective\nfunction. We study how to optimally choose such directions in a Directional,\nrandom scan, Gibbs sampler setting. The optimal direction is chosen by\nminimizing to the mutual information (Kullback-Leibler divergence) of two steps\nof the MCMC for a truncated Normal objective function. The result is\ngeneralized to be used when a Multivariate Normal (local) approximation is\navailable for the objective function. Three Gibbs direction distributions are\ntested in highly skewed non-normal objective functions.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 00:12:53 GMT"}], "update_date": "2012-05-21", "authors_parsed": [["Christen", "J. Andr\u00e9s", ""], ["Fox", "Colin", ""], ["P\u00e9rez-Ruiz", "Diego Andr\u00e9s", ""], ["Santana-Cibrian", "Mario", ""]]}, {"id": "1205.4120", "submitter": "Hao Wang", "authors": "Hao Wang", "title": "Two New Algorithms for Solving Covariance Graphical Lasso Based on\n  Coordinate Descent and ECM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance graphical lasso applies a lasso penalty on the elements of the\ncovariance matrix. This method is useful because it not only produces sparse\nestimation of covariance matrix but also discovers marginal independence\nstructures by generating zeros in the covariance matrix. We propose and explore\ntwo new algorithms for solving the covariance graphical lasso problem. Our new\nalgorithms are based on coordinate descent and ECM. We show that these two\nalgorithms are more attractive than the only existing competing algorithm of\nBien and Tibshirani (2011) in terms of simplicity, speed and stability. We also\ndiscuss convergence properties of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 09:33:29 GMT"}], "update_date": "2012-05-21", "authors_parsed": [["Wang", "Hao", ""]]}, {"id": "1205.4423", "submitter": "Richard Brent", "authors": "Juan Arias de Reyna, Richard P. Brent and Jan van de Lune", "title": "On the sign of the real part of the Riemann zeta-function", "comments": "22 pages, 3 tables. To appear in Proceedings of the International\n  Number Theory Conference in Memory of Alf van der Poorten (Newcastle,\n  Australia, 2011)", "journal-ref": "Springer Proceedings in Mathematics and Statistics, vol. 43, 2013,\n  75-97", "doi": "10.1007/978-1-4614-6642-0_3", "report-no": null, "categories": "math.NT cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distribution of $\\arg\\zeta(\\sigma+it)$ on fixed lines $\\sigma\n> \\frac12$, and in particular the density \\[d(\\sigma) = \\lim_{T \\rightarrow\n+\\infty}\n  \\frac{1}{2T}\n  |\\{t \\in [-T,+T]: |\\arg\\zeta(\\sigma+it)| > \\pi/2\\}|\\,,\\] and the closely\nrelated density \\[d_{-}(\\sigma) = \\lim_{T \\rightarrow +\\infty}\n  \\frac{1}{2T}\n  |\\{t \\in [-T,+T]: \\Re\\zeta(\\sigma+it) < 0\\}|\\,.\\] Using classical results of\nBohr and Jessen, we obtain an explicit expression for the characteristic\nfunction $\\psi_\\sigma(x)$ associated with $\\arg\\zeta(\\sigma+it)$. We give\nexplicit expressions for $d(\\sigma)$ and $d_{-}(\\sigma)$ in terms of\n$\\psi_\\sigma(x)$. Finally, we give a practical algorithm for evaluating these\nexpressions to obtain accurate numerical values of $d(\\sigma)$ and\n$d_{-}(\\sigma)$.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2012 15:13:02 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2012 03:20:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["de Reyna", "Juan Arias", ""], ["Brent", "Richard P.", ""], ["van de Lune", "Jan", ""]]}, {"id": "1205.4481", "submitter": "Hua Ouyang", "authors": "Hua Ouyang, Alexander Gray", "title": "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by\n  Exploiting Structure", "comments": "Full length version of ICML'12 with all proofs. In this version, a\n  bug in proving Theorem 6 is fixed. We'd like to thank Dr. Francesco Orabona\n  for pointing it out", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the stochastic minimization of nonsmooth convex loss\nfunctions, a central problem in machine learning. We propose a novel algorithm\ncalled Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which\nexploits the structure of common nonsmooth loss functions to achieve optimal\nconvergence rates for a class of problems including SVMs. It is the first\nstochastic algorithm that can achieve the optimal O(1/t) rate for minimizing\nnonsmooth loss functions (with strong convexity). The fast rates are confirmed\nby empirical comparisons, in which ANSGD significantly outperforms previous\nsubgradient descent algorithms including SGD.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 03:29:17 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2012 14:53:38 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2012 15:15:42 GMT"}, {"version": "v4", "created": "Mon, 1 Oct 2012 16:55:06 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Ouyang", "Hua", ""], ["Gray", "Alexander", ""]]}, {"id": "1205.4503", "submitter": "Jessica Leigh", "authors": "Jessica W. Leigh and David Bryant", "title": "Parameter Exploration in Simulation Experiments: A Bayesian Framework", "comments": "29 pages incl. supplementary information. 4 figures, 2 supp. figures,\n  2 supp. tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulations often involve the use of model parameters which are unknown or\nuncertain. For this reason, simulation experiments are often repeated for\nmultiple combinations of parameter values, often iterating through parameter\nvalues lying on a fixed grid. However, the use of a discrete grid places limits\non the dimension of the parameter space and creates the potential to miss\nimportant parameter combinations which fall in the gaps between grid points.\nHere we draw parallels with strategies for numerical integration and describe a\nMarkov chain Monte-Carlo strategy for exploring parameter values. We illustrate\nthe approach using examples from phylogenetics, archaeology, and epidemiology.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 07:32:50 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Leigh", "Jessica W.", ""], ["Bryant", "David", ""]]}, {"id": "1205.4841", "submitter": "Jakob Stoeber", "authors": "Jakob St\\\"ober and Ulf Schepsmeier", "title": "Is there significant time-variation in multivariate copulas?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how the uncertainty of parameter point estimates can be\nassessed in a maximum likelihood framework in order to prevent overfitting and\nerroneous detection of time-inhomogeneity. The class of models we consider are\nregular vine (R-vine) copula models, for which we describe a new algorithm for\nthe exact computation of the score function and observed information. R-vine\ncopulas constitute a flexible class of dependence models which are constructed\nhierarchically from bivariate copulas as building blocks only, and our\nalgorithm exploits the hierarchical nature for subsequent computation of\nlog-likelihood derivatives. Results obtained using the proposed methods are\ndiscussed in the context of the asymptotic efficiency of different estimation\nmethods for R-vine based models. In a substantial application to a dataset of\nexchange rates, we obtain clear indications for time-inhomogeneous dependence\nbetween some currency pairs.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 08:46:36 GMT"}], "update_date": "2012-05-23", "authors_parsed": [["St\u00f6ber", "Jakob", ""], ["Schepsmeier", "Ulf", ""]]}, {"id": "1205.5082", "submitter": "Dominic Lee", "authors": "Dominic S. Lee and Carey E. Priebe", "title": "Bayesian Vertex Nomination", "comments": "25 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an attributed graph whose vertices are colored green or red, but\nonly a few are observed to be red. The color of the other vertices is\nunobserved. Typically, the unknown total number of red vertices is small. The\nvertex nomination problem is to nominate one of the unobserved vertices as\nbeing red. The edge set of the graph is a subset of the set of unordered pairs\nof vertices. Suppose that each edge is also colored green or red and this is\nobserved for all edges. The context statistic of a vertex is defined as the\nnumber of observed red vertices connected to it, and its content statistic is\nthe number of red edges incident to it. Assuming that these statistics are\nindependent between vertices and that red edges are more likely between red\nvertices, Coppersmith and Priebe (2012) proposed a likelihood model based on\nthese statistics. Here, we formulate a Bayesian model using the proposed\nlikelihood together with prior distributions chosen for the unknown parameters\nand unobserved vertex colors. From the resulting posterior distribution, the\nnominated vertex is the one with the highest posterior probability of being\nred. Inference is conducted using a Metropolis-within-Gibbs algorithm, and\nperformance is illustrated by a simulation study. Results show that (i) the\nBayesian model performs significantly better than chance; (ii) the probability\nof correct nomination increases with increasing posterior probability that the\nnominated vertex is red; and (iii) the Bayesian model either matches or\nperforms better than the method in Coppersmith and Priebe. An application\nexample is provided using the Enron email corpus, where vertices represent\nEnron employees and their associates, observed red vertices are known\nfraudsters, red edges represent email communications perceived as fraudulent,\nand we wish to identify one of the latent vertices as most likely to be a\nfraudster.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2012 01:19:37 GMT"}], "update_date": "2012-05-24", "authors_parsed": [["Lee", "Dominic S.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1205.5494", "submitter": "Luca Martino", "authors": "Luca Martino, Jesse Read, David Luengo", "title": "Improved Adaptive Rejection Metropolis Sampling Algorithms", "comments": "Matlab code provided in http://a2rms.sourceforge.net/", "journal-ref": "Independent Doubly Adaptive Rejection Metropolis Sampling Within\n  Gibbs Sampling, IEEE Transactions on Signal Processing, Volume 63, Issue 12,\n  Pages 3123-3138, 2015", "doi": "10.1109/TSP.2015.2420537", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods, such as the Metropolis-Hastings (MH)\nalgorithm, are widely used for Bayesian inference. One of the most important\nissues for any MCMC method is the convergence of the Markov chain, which\ndepends crucially on a suitable choice of the proposal density. Adaptive\nRejection Metropolis Sampling (ARMS) is a well-known MH scheme that generates\nsamples from one-dimensional target densities making use of adaptive piecewise\nproposals constructed using support points taken from rejected samples. In this\nwork we pinpoint a crucial drawback in the adaptive procedure in ARMS: support\npoints might never be added inside regions where the proposal is below the\ntarget. When this happens in many regions it leads to a poor performance of\nARMS, with the proposal never converging to the target. In order to overcome\nthis limitation we propose two improved adaptive schemes for constructing the\nproposal. The first one is a direct modification of the ARMS procedure that\nincorporates support points inside regions where the proposal is below the\ntarget, while satisfying the diminishing adaptation property, one of the\nrequired conditions to assure the convergence of the Markov chain. The second\none is an adaptive independent MH algorithm with the ability to learn from all\nprevious samples except for the current state of the chain, thus also\nguaranteeing the convergence to the invariant density. These two new schemes\nimprove the adaptive strategy of ARMS, thus simplifying the complexity in the\nconstruction of the proposals. Numerical results show that the new techniques\nprovide better performance w.r.t. the standard ARMS.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2012 16:21:50 GMT"}, {"version": "v2", "created": "Sun, 27 May 2012 00:39:12 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2012 10:39:51 GMT"}, {"version": "v4", "created": "Mon, 8 Oct 2012 17:05:05 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Martino", "Luca", ""], ["Read", "Jesse", ""], ["Luengo", "David", ""]]}, {"id": "1205.5658", "submitter": "Christian P. Robert", "authors": "K. L. Mengersen (QUT, Brisbane), P. Pudlo (Universite Montpellier 2),\n  and C. P. Robert (Universite Paris-Dauphine)", "title": "Bayesian computation via empirical likelihood", "comments": "21 pages, 12 figures, revised version of the previous version with a\n  new title", "journal-ref": null, "doi": "10.1073/pnas.1208827110", "report-no": null, "categories": "stat.CO q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) has become an essential tool for the\nanalysis of complex stochastic models when the likelihood function is\nnumerically unavailable. However, the well-established statistical method of\nempirical likelihood provides another route to such settings that bypasses\nsimulations from the model and the choices of the ABC parameters (summary\nstatistics, distance, tolerance), while being convergent in the number of\nobservations. Furthermore, bypassing model simulations may lead to significant\ntime savings in complex models, for instance those found in population\ngenetics. The BCel algorithm we develop in this paper also provides an\nevaluation of its own performance through an associated effective sample size.\nThe method is illustrated using several examples, including estimation of\nstandard distributions, time series, and population genetics models.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2012 10:42:35 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2012 14:46:58 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2012 21:27:42 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Mengersen", "K. L.", "", "QUT, Brisbane"], ["Pudlo", "P.", "", "Universite Montpellier 2"], ["Robert", "C. P.", "", "Universite Paris-Dauphine"]]}, {"id": "1205.5723", "submitter": "Peter McCullagh", "authors": "Peter McCullagh", "title": "An asymptotic approximation for the permanent of a doubly stochastic\n  matrix", "comments": "One figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A determinantal approximation is obtained for the permanent of a doubly\nstochastic matrix. For moderate-deviation matrix sequences, the asymptotic\nrelative error is of order $O(n^{-1})$.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2012 15:20:02 GMT"}], "update_date": "2012-05-28", "authors_parsed": [["McCullagh", "Peter", ""]]}, {"id": "1205.5868", "submitter": "Kei Hirose", "authors": "Kei Hirose, Michio Yamamoto", "title": "Sparse estimation via nonconcave penalized likelihood in a factor\n  analysis model", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse estimation in a factor analysis model. A\ntraditional estimation procedure in use is the following two-step approach: the\nmodel is estimated by maximum likelihood method and then a rotation technique\nis utilized to find sparse factor loadings. However, the maximum likelihood\nestimates cannot be obtained when the number of variables is much larger than\nthe number of observations. Furthermore, even if the maximum likelihood\nestimates are available, the rotation technique does not often produce a\nsufficiently sparse solution. In order to handle these problems, this paper\nintroduces a penalized likelihood procedure that imposes a nonconvex penalty on\nthe factor loadings. We show that the penalized likelihood procedure can be\nviewed as a generalization of the traditional two-step approach, and the\nproposed methodology can produce sparser solutions than the rotation technique.\nA new algorithm via the EM algorithm along with coordinate descent is\nintroduced to compute the entire solution path, which permits the application\nto a wide variety of convex and nonconvex penalties. Monte Carlo simulations\nare conducted to investigate the performance of our modeling strategy. A real\ndata example is also given to illustrate our procedure.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 10:04:27 GMT"}, {"version": "v2", "created": "Tue, 29 May 2012 14:28:46 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2013 11:41:15 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Hirose", "Kei", ""], ["Yamamoto", "Michio", ""]]}, {"id": "1205.6326", "submitter": "Iain Murray", "authors": "Krzysztof Chalupka, Christopher K. I. Williams and Iain Murray", "title": "A Framework for Evaluating Approximation Methods for Gaussian Process\n  Regression", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) predictors are an important component of many Bayesian\napproaches to machine learning. However, even a straightforward implementation\nof Gaussian process regression (GPR) requires O(n^2) space and O(n^3) time for\na dataset of n examples. Several approximation methods have been proposed, but\nthere is a lack of understanding of the relative merits of the different\napproximations, and in what situations they are most useful. We recommend\nassessing the quality of the predictions obtained as a function of the compute\ntime taken, and comparing to standard baselines (e.g., Subset of Data and\nFITC). We empirically investigate four different approximation algorithms on\nfour different prediction problems, and make our code available to encourage\nfuture comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 10:59:30 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2012 17:39:32 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Williams", "Christopher K. I.", ""], ["Murray", "Iain", ""]]}, {"id": "1205.6439", "submitter": "Kevin Dayaratna", "authors": "Kevin D. Dayaratna and P. K. Kannan", "title": "A Mathematical Reformulation of the Reference Price", "comments": "22 pages, 2 tables. Forthcoming in Marketing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference prices have long been studied in applied economics and business\nresearch. One of the classic formulations of the reference price is in terms of\nan iterative function of past prices. There are a number of limitations of such\na formulation, however. Such limitations include burdensome computational time\nto estimate parameters, an inability to truly account for customer\nheterogeneity, and an estimation procedure that implies a misspecified model.\nManagerial recommendations based on inferences from such a model can be quite\nmisleading. We mathematically reformulate the reference price by developing a\nclosed-form expansion that addresses the aforementioned issues, enabling one to\nelicit truly meaningful managerial advice from the model. We estimate our model\non a real world data set to illustrate the efficacy of our approach. Our work\nis not only useful from a modeling perspective, but also has important\nbehavioral and managerial implications, which modelers and non-modelers alike\nwould find useful.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 18:05:14 GMT"}], "update_date": "2012-05-30", "authors_parsed": [["Dayaratna", "Kevin D.", ""], ["Kannan", "P. K.", ""]]}, {"id": "1205.6458", "submitter": "Burnecki Krzysztof", "authors": "Krzysztof Burnecki, Agnieszka Wy{\\l}oma\\'nska, Aleksei Beletskii,\n  Vsevolod Gonchar, and Aleksei Chechkin", "title": "Recognition of stable distribution with Levy index alpha close to 2", "comments": "Accepted to PRE", "journal-ref": "PRE 85, 056711 (2012)", "doi": "10.1103/PhysRevE.85.056711", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of recognizing alpha-stable Levy distribution with\nLevy index close to 2 from experimental data. We are interested in the case\nwhen the sample size of available data is not large, thus the power law\nasymptotics of the distribution is not clearly detectable, and the shape of\nempirical probability density function is close to a Gaussian. We propose a\ntesting procedure combining a simple visual test based on empirical fourth\nmoment with the Anderson-Darling and Jarque-Bera statistical tests and we check\nthe efficiency of the method on simulated data. Furthermore, we apply our\nmethod to the analysis of turbulent plasma density and potential fluctuations\nmeasured in the stellarator type fusion device and demonstrate that the\nphenomenon of L-H transition occurring in this device is accompanied by the\ntransition from Levy to Gaussian fluctuation statistics.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 19:33:04 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Burnecki", "Krzysztof", ""], ["Wy\u0142oma\u0144ska", "Agnieszka", ""], ["Beletskii", "Aleksei", ""], ["Gonchar", "Vsevolod", ""], ["Chechkin", "Aleksei", ""]]}, {"id": "1205.6857", "submitter": "Geoff Nicholls", "authors": "Geoff K. Nicholls, Colin Fox and Alexis Muir Watt", "title": "Coupled MCMC with a randomized acceptance probability", "comments": "20 pages, 5 graphs in 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Metropolis Hastings MCMC in cases where the log of the ratio of\ntarget distributions is replaced by an estimator. The estimator is based on m\nsamples from an independent online Monte Carlo simulation. Under some\nconditions on the distribution of the estimator the process resembles\nMetropolis Hastings MCMC with a randomized transition kernel. When this is the\ncase there is a correction to the estimated acceptance probability which\nensures that the target distribution remains the equilibrium distribution. The\nsimplest versions of the Penalty Method of Ceperley and Dewing (1999), the\nUniversal Algorithm of Ball et al. (2003) and the Single Variable Exchange\nalgorithm of Murray et al. (2006) are special cases. In many applications of\ninterest the correction terms cannot be computed. We consider approximate\nversions of the algorithms. We show that on average O(m) of the samples\nrealized by a simulation approximating a randomized chain of length n are\nexactly the same as those of a coupled (exact) randomized chain. Approximation\nbiases Monte Carlo estimates with terms O(1/m) or smaller. This should be\ncompared to the Monte Carlo error which is O(1/sqrt(n)).\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 23:45:14 GMT"}], "update_date": "2012-06-01", "authors_parsed": [["Nicholls", "Geoff K.", ""], ["Fox", "Colin", ""], ["Watt", "Alexis Muir", ""]]}]