[{"id": "1401.0087", "submitter": "Iuliana Teodorescu", "authors": "Iuliana Teodorescu and Chris Tsokos", "title": "Contributors of carbon dioxide in the atmosphere in Europe: the surface\n  response analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1312.7827", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a continuation of the statistical modeling of the nonlinear\nrelationship between atmospheric CO2 and attributable variables that can\naccount for emissions, based on data from EU countries, in order to compare the\nrelevant findings to those obtained in the case of US data, in [1, 2]. The\ncurrent study was initiated in [3], leading to the optimal second-order model,\nbased on three linear terms and five second-order terms. We conclude this study\nin the present work, by finding the canonical decomposition of the nonlinear\nmodel, and by computing the specific two-dimensional confidence regions that it\nleads to. We then use the model in order to quantify the net effect of various\nrisk factors, and compare to the results obtained in the US case.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 04:10:40 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Teodorescu", "Iuliana", ""], ["Tsokos", "Chris", ""]]}, {"id": "1401.0100", "submitter": "Feng Li", "authors": "Feng Li and Yanfei Kang", "title": "Improving forecasting performance using covariate-dependent copula\n  models", "comments": null, "journal-ref": "International Journal of Forecasting (2018), 34(3), pp. 456-476", "doi": "10.1016/j.ijforecast.2018.01.007", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas provide an attractive approach for constructing multivariate\ndistributions with flexible marginal distributions and different forms of\ndependences. Of particular importance in many areas is the possibility of\nexplicitly forecasting the tail-dependences. Most of the available approaches\nare only able to estimate tail-dependences and correlations via nuisance\nparameters, but can neither be used for interpretation, nor for forecasting.\nAiming to improve copula forecasting performance, we propose a general Bayesian\napproach for modeling and forecasting tail-dependences and correlations as\nexplicit functions of covariates. The proposed covariate-dependent copula model\nalso allows for Bayesian variable selection among covariates from the marginal\nmodels as well as the copula density. The copulas we study include Joe-Clayton\ncopula, Clayton copula, Gumbel copula and Student's \\emph{t}-copula. Posterior\ninference is carried out using an efficient MCMC simulation method. Our\napproach is applied to both simulated data and the S\\&P 100 and S\\&P 600 stock\nindices. The forecasting performance of the proposed approach is compared with\nother modeling strategies based on log predictive scores. Value-at-Risk\nevaluation is also preformed for model comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 06:01:53 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 03:01:23 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 14:03:32 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Li", "Feng", ""], ["Kang", "Yanfei", ""]]}, {"id": "1401.0118", "submitter": "Rajesh Ranganath", "authors": "Rajesh Ranganath and Sean Gerrish and David M. Blei", "title": "Black Box Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference has become a widely used method to approximate\nposteriors in complex latent variables models. However, deriving a variational\ninference algorithm generally requires significant model-specific analysis, and\nthese efforts can hinder and deter us from quickly developing and exploring a\nvariety of models for a problem at hand. In this paper, we present a \"black\nbox\" variational inference algorithm, one that can be quickly applied to many\nmodels with little additional derivation. Our method is based on a stochastic\noptimization of the variational objective where the noisy gradient is computed\nfrom Monte Carlo samples from the variational distribution. We develop a number\nof methods to reduce the variance of the gradient, always maintaining the\ncriterion that we want to avoid difficult model-based derivations. We evaluate\nour method against the corresponding black box sampling based methods. We find\nthat our method reaches better predictive likelihoods much faster than sampling\nmethods. Finally, we demonstrate that Black Box Variational Inference lets us\neasily explore a wide space of models by quickly constructing and evaluating\nseveral models of longitudinal healthcare data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 09:32:43 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Gerrish", "Sean", ""], ["Blei", "David M.", ""]]}, {"id": "1401.0265", "submitter": "Ajay Jasra", "authors": "Ajay Jasra", "title": "Approximate Bayesian Computation for a Class of Time Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following article we consider approximate Bayesian computation (ABC)\nfor certain classes of time series models. In particular, we focus upon\nscenarios where the likelihoods of the observations and parameter are\nintractable, by which we mean that one cannot evaluate the likelihood even\nup-to a positive unbiased estimate. This paper reviews and develops a class of\napproximation procedures based upon the idea of ABC, but, specifically\nmaintains the probabilistic structure of the original statistical model. This\nidea is useful, in that it can facilitate an analysis of the bias of the\napproximation and the adaptation of established computational methods for\nparameter inference. Several existing results in the literature are surveyed\nand novel developments with regards to computation are given.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 08:59:39 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Jasra", "Ajay", ""]]}, {"id": "1401.0604", "submitter": "Fredrik Lindsten", "authors": "Fredrik Lindsten and Michael I. Jordan and Thomas B. Sch\\\"on", "title": "Particle Gibbs with Ancestor Sampling", "comments": null, "journal-ref": "Journal of Machine Learning Research, 15 (2014) 2145-2184", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining\nthe two main tools used for Monte Carlo statistical inference: sequential Monte\nCarlo (SMC) and Markov chain Monte Carlo (MCMC). We present a novel PMCMC\nalgorithm that we refer to as particle Gibbs with ancestor sampling (PGAS).\nPGAS provides the data analyst with an off-the-shelf class of Markov kernels\nthat can be used to simulate the typically high-dimensional and highly\nautocorrelated state trajectory in a state-space model. The ancestor sampling\nprocedure enables fast mixing of the PGAS kernel even when using seemingly few\nparticles in the underlying SMC sampler. This is important as it can\nsignificantly reduce the computational burden that is typically associated with\nusing SMC. PGAS is conceptually similar to the existing PG with backward\nsimulation (PGBS) procedure. Instead of using separate forward and backward\nsweeps as in PGBS, however, we achieve the same effect in a single forward\nsweep. This makes PGAS well suited for addressing inference problems not only\nin state-space models, but also in models with more complex dependencies, such\nas non-Markovian, Bayesian nonparametric, and general probabilistic graphical\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 08:17:51 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Lindsten", "Fredrik", ""], ["Jordan", "Michael I.", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1401.0711", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay and Hod Lipson", "title": "Computing Entropy Rate Of Symbol Sources & A Distribution-free Limit\n  Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy rate of sequential data-streams naturally quantifies the complexity\nof the generative process. Thus entropy rate fluctuations could be used as a\ntool to recognize dynamical perturbations in signal sources, and could\npotentially be carried out without explicit background noise characterization.\nHowever, state of the art algorithms to estimate the entropy rate have markedly\nslow convergence; making such entropic approaches non-viable in practice. We\npresent here a fundamentally new approach to estimate entropy rates, which is\ndemonstrated to converge significantly faster in terms of input data lengths,\nand is shown to be effective in diverse applications ranging from the\nestimation of the entropy rate of English texts to the estimation of complexity\nof chaotic dynamical systems. Additionally, the convergence rate of entropy\nestimates do not follow from any standard limit theorem, and reported\nalgorithms fail to provide any confidence bounds on the computed values.\nExploiting a connection to the theory of probabilistic automata, we establish a\nconvergence rate of $O(\\log \\vert s \\vert/\\sqrt[3]{\\vert s \\vert})$ as a\nfunction of the input length $\\vert s \\vert$, which then yields explicit\nuncertainty estimates, as well as required data lengths to satisfy\npre-specified confidence bounds.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 20:30:01 GMT"}, {"version": "v2", "created": "Fri, 21 Mar 2014 07:29:34 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Chattopadhyay", "Ishanu", ""], ["Lipson", "Hod", ""]]}, {"id": "1401.0794", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama, Lars Borin", "title": "Properties of phoneme N -grams across the world's language families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate the properties of phoneme N-grams across half\nof the world's languages. We investigate if the sizes of three different N-gram\ndistributions of the world's language families obey a power law. Further, the\nN-gram distributions of language families parallel the sizes of the families,\nwhich seem to obey a power law distribution. The correlation between N-gram\ndistributions and language family sizes improves with increasing values of N.\nWe applied statistical tests, originally given by physicists, to test the\nhypothesis of power law fit to twelve different datasets. The study also raises\nsome new questions about the use of N-gram distributions in linguistic\nresearch, which we answer by running a statistical test.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 09:50:55 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Rama", "Taraka", ""], ["Borin", "Lars", ""]]}, {"id": "1401.0869", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Yong Zhang", "title": "Schatten-$p$ Quasi-Norm Regularized Matrix Optimization via Iterative\n  Reweighted Singular Value Minimization", "comments": "This paper has been withdrawn by the author due to major revision and\n  corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study general Schatten-$p$ quasi-norm (SPQN) regularized\nmatrix minimization problems. In particular, we first introduce a class of\nfirst-order stationary points for them, and show that the first-order\nstationary points introduced in [11] for an SPQN regularized $vector$\nminimization problem are equivalent to those of an SPQN regularized $matrix$\nminimization reformulation. We also show that any local minimizer of the SPQN\nregularized matrix minimization problems must be a first-order stationary\npoint. Moreover, we derive lower bounds for nonzero singular values of the\nfirst-order stationary points and hence also of the local minimizers of the\nSPQN regularized matrix minimization problems. The iterative reweighted\nsingular value minimization (IRSVM) methods are then proposed to solve these\nproblems, whose subproblems are shown to have a closed-form solution. In\ncontrast to the analogous methods for the SPQN regularized $vector$\nminimization problems, the convergence analysis of these methods is\nsignificantly more challenging. We develop a novel approach to establishing the\nconvergence of these methods, which makes use of the expression of a specific\nsolution of their subproblems and avoids the intricate issue of finding the\nexplicit expression for the Clarke subdifferential of the objective of their\nsubproblems. In particular, we show that any accumulation point of the sequence\ngenerated by the IRSVM methods is a first-order stationary point of the\nproblems. Our computational results demonstrate that the IRSVM methods\ngenerally outperform some recently developed state-of-the-art methods in terms\nof solution quality and/or speed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 06:37:50 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 18:41:44 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 17:30:58 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "1401.1022", "submitter": "Tim Salimans", "authors": "Tim Salimans and David A. Knowles", "title": "On Using Control Variates with Stochastic Approximation for Variational\n  Bayes and its Connection to Stochastic Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we and several other authors have written about the possibilities\nof using stochastic approximation techniques for fitting variational\napproximations to intractable Bayesian posterior distributions. Naive\nimplementations of stochastic approximation suffer from high variance in this\nsetting. Several authors have therefore suggested using control variates to\nreduce this variance, while we have taken a different but analogous approach to\nreducing the variance which we call stochastic linear regression. In this note\nwe take the former perspective and derive the ideal set of control variates for\nstochastic approximation variational Bayes under a certain set of assumptions.\nWe then show that using these control variates is closely related to using the\nstochastic linear regression approximation technique we proposed earlier. A\nsimple example shows that our method for constructing control variates leads to\nstochastic estimators with much lower variance compared to other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 09:51:15 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2014 17:59:53 GMT"}, {"version": "v3", "created": "Sun, 12 Jan 2014 09:52:17 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Salimans", "Tim", ""], ["Knowles", "David A.", ""]]}, {"id": "1401.1269", "submitter": "Peng Ding", "authors": "Peng Ding", "title": "Bayesian Robust Inference of Sample Selection Using Selection-t Models", "comments": "Journal of Multivariate Analysis (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heckman selection model is the most popular econometric model in analysis of\ndata with sample selection. However, selection models with Normal errors cannot\naccommodate heavy tails in the error distribution. Recently, Marchenko and\nGenton proposed a selection-t model to perform frequentist' robust analysis of\nsample selection. Instead of using their maximum likelihood estimates, our\npaper develops new Bayesian procedures for the selection-t models with either\ncontinuous or binary outcomes. By exploiting the Normal mixture representation\nof the t distribution, we can use data augmentation to impute the missing data,\nand use parameter expansion to sample the restricted covariance matrices. The\nBayesian procedures only involve simple steps, without calculating analytical\nor numerical derivatives of the complicated log likelihood functions.\nSimulation studies show the vulnerability of the selection models with Normal\nerrors, as well as the robustness of the selection models with t errors.\nInterestingly, we find evidence of heavy-tailedness in three real examples\nanalyzed by previous studies, and the conclusions about the existence of\nselection effect are very sensitive to the distributional assumptions of the\nerror terms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 04:32:47 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Ding", "Peng", ""]]}, {"id": "1401.1436", "submitter": "Richard Wilkinson", "authors": "Richard D Wilkinson", "title": "Accelerating ABC methods using Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods are used to approximate\nposterior distributions using simulation rather than likelihood calculations.\nWe introduce Gaussian process (GP) accelerated ABC, which we show can\nsignificantly reduce the number of simulations required. As computational\nresource is usually the main determinant of accuracy in ABC, GP-accelerated\nmethods can thus enable more accurate inference in some models. GP models of\nthe unknown log-likelihood function are used to exploit continuity and\nsmoothness, reducing the required computation. We use a sequence of models that\nincrease in accuracy, using intermediate models to rule out regions of the\nparameter space as implausible. The methods will not be suitable for all\nproblems, but when they can be used, can result in significant computational\nsavings. For the Ricker model, we are able to achieve accurate approximations\nto the posterior distribution using a factor of 100 fewer simulator evaluations\nthan comparable Monte Carlo approaches, and for a population genetics model we\nare able to approximate the exact posterior for the first time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 16:38:14 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2014 20:29:36 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Wilkinson", "Richard D", ""]]}, {"id": "1401.1667", "submitter": "David Gunawan", "authors": "Eduardo F. Mendes, Christopher K. Carter, David Gunawan, Robert Kohn", "title": "A flexible Particle Markov chain Monte Carlo method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Markov Chain Monte Carlo methods are used to carry out inference in\nnon-linear and non-Gaussian state space models, where the posterior density of\nthe states is approximated using particles. Current approaches usually perform\nBayesian inference using either a particle Marginal Metropolis-Hastings (PMMH)\nalgorithm or a particle Gibbs (PG) sampler. This paper shows how the two ways\nof generating variables mentioned above can be combined in a flexible manner to\ngive sampling schemes that converge to a desired target distribution. The\nadvantage of our approach is that the sampling scheme can be tailored to obtain\ngood results for different applications. For example, when some parameters and\nthe states are highly correlated, such parameters can be generated using PMMH,\nwhile all other parameters are generated using PG because it is easier to\nobtain good proposals for the parameters within the PG framework. We derive\nsome convergence properties of our sampling scheme and also investigate its\nperformance empirically by applying it to univariate and multivariate\nstochastic volatility models and comparing it to other PMCMC methods proposed\nin the literature.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 11:25:24 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 07:00:23 GMT"}, {"version": "v3", "created": "Mon, 19 Sep 2016 09:35:28 GMT"}, {"version": "v4", "created": "Tue, 23 Jan 2018 09:15:51 GMT"}, {"version": "v5", "created": "Fri, 12 Oct 2018 03:21:02 GMT"}, {"version": "v6", "created": "Fri, 27 Sep 2019 04:43:49 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Mendes", "Eduardo F.", ""], ["Carter", "Christopher K.", ""], ["Gunawan", "David", ""], ["Kohn", "Robert", ""]]}, {"id": "1401.2135", "submitter": "Tim Salimans", "authors": "Tim Salimans", "title": "Implementing and Automating Fixed-Form Variational Posterior\n  Approximation through Stochastic Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently proposed a general algorithm for approximating nonstandard\nBayesian posterior distributions by minimization of their Kullback-Leibler\ndivergence with respect to a more convenient approximating distribution. In\nthis note we offer details on how to efficiently implement this algorithm in\npractice. We also suggest default choices for the form of the posterior\napproximation, the number of iterations, the step size, and other user choices.\nBy using these defaults it becomes possible to construct good posterior\napproximations for hierarchical models completely automatically.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 19:44:36 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Salimans", "Tim", ""]]}, {"id": "1401.2163", "submitter": "Ying Lu", "authors": "Xia Cui, Ying Lu, Heng Peng", "title": "Estimation of Partially Linear Regression Model under Partial\n  Consistency Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, utilizing recent theoretical results in high dimensional\nstatistical modeling, we propose a model-free yet computationally simple\napproach to estimate the partially linear model $Y=X\\beta+g(Z)+\\varepsilon$.\nMotivated by the partial consistency phenomena, we propose to model $g(Z)$ via\nincidental parameters. Based on partitioning the support of $Z$, a simple local\naverage is used to estimate the response surface. The proposed method seeks to\nstrike a balance between computation burden and efficiency of the estimators\nwhile minimizing model bias. Computationally this approach only involves least\nsquares. We show that given the inconsistent estimator of $g(Z)$, a root $n$\nconsistent estimator of parametric component $\\beta$ of the partially linear\nmodel can be obtained with little cost in efficiency. Moreover, conditional on\nthe $\\beta$ estimates, an optimal estimator of $g(Z)$ can then be obtained\nusing classic nonparametric methods. The statistical inference problem\nregarding $\\beta$ and a two-population nonparametric testing problem regarding\n$g(Z)$ are considered. Our results show that the behavior of test statistics\nare satisfactory. To assess the performance of our method in comparison with\nother methods, three simulation studies are conducted and a real dataset about\nrisk factors of birth weights is analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 21:05:41 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Cui", "Xia", ""], ["Lu", "Ying", ""], ["Peng", "Heng", ""]]}, {"id": "1401.2395", "submitter": "Zhigang Yao", "authors": "Zhigang Yao, William F. Eddy", "title": "A statistical approach to the inverse problem in magnetoencephalography", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS716 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 1119-1144", "doi": "10.1214/14-AOAS716", "report-no": "IMS-AOAS-AOAS716", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetoencephalography (MEG) is an imaging technique used to measure the\nmagnetic field outside the human head produced by the electrical activity\ninside the brain. The MEG inverse problem, identifying the location of the\nelectrical sources from the magnetic signal measurements, is ill-posed, that\nis, there are an infinite number of mathematically correct solutions. Common\nsource localization methods assume the source does not vary with time and do\nnot provide estimates of the variability of the fitted model. Here, we\nreformulate the MEG inverse problem by considering time-varying locations for\nthe sources and their electrical moments and we model their time evolution\nusing a state space model. Based on our predictive model, we investigate the\ninverse problem by finding the posterior source distribution given the multiple\nchannels of observations at each time rather than fitting fixed source\nparameters. Our new model is more realistic than common models and allows us to\nestimate the variation of the strength, orientation and position. We propose\ntwo new Monte Carlo methods based on sequential importance sampling. Unlike the\nusual MCMC sampling scheme, our new methods work in this situation without\nneeding to tune a high-dimensional transition kernel which has a very high\ncost. The dimensionality of the unknown parameters is extremely large and the\nsize of the data is even larger. We use Parallel Virtual Machine (PVM) to speed\nup the computation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 16:34:37 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 13:41:49 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Yao", "Zhigang", ""], ["Eddy", "William F.", ""]]}, {"id": "1401.2490", "submitter": "Sinan Yildirim", "authors": "Sinan Yildirim, A. Taylan Cemgil, Sumeetpal S. Singh", "title": "An Online Expectation-Maximisation Algorithm for Nonnegative Matrix\n  Factorisation Models", "comments": "6 pages, 3 figures", "journal-ref": "16th IFAC Symposium on System Identification, 2012, Volume 16,\n  Part 1,", "doi": "10.3182/20120711-3-BE-2027.00312", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formulate the nonnegative matrix factorisation (NMF) problem\nas a maximum likelihood estimation problem for hidden Markov models and propose\nonline expectation-maximisation (EM) algorithms to estimate the NMF and the\nother unknown static parameters. We also propose a sequential Monte Carlo\napproximation of our online EM algorithm. We show the performance of the\nproposed method with two numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 00:54:27 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Yildirim", "Sinan", ""], ["Cemgil", "A. Taylan", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1401.2791", "submitter": "Pete Bunch", "authors": "Pete Bunch, Simon Godsill", "title": "The Progressive Proposal Particle Filter: Better Approximations to the\n  Optimal Importance Density", "comments": "Superseded by arXiv:1406.3183 Changes include a substantially revised\n  presentation and several errors corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crucial step in designing a particle filter for a particular application\nis the choice of importance density. The optimal scheme is to use the\nconditional posterior density of the state, but this cannot be sampled or\ncalculated analytically in most case. In practice, approximations of this\ndensity are used, particularly Gaussian densities based on linearisation or the\nunscented transform. For many highly nonlinear or non-Gaussian models, these\napproximations can be poor, leading to degeneracy of the particle approximation\nor even the filter \"losing track\" completely. In this paper, we develop a new\nmechanism for approximating the optimal importance density, which we call the\nprogressive proposal method. This works by introducing the observation\nprogressively and performing a series of state updates, each using a local\nGaussian approximation to the optimal importance density. A number of\nrefinements and extensions to the basic algorithm are also introduced.\nSimulations are used to demonstrate an improvement in performance over simpler\nparticle filters on a number of applications.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 11:26:57 GMT"}, {"version": "v2", "created": "Thu, 14 Aug 2014 07:37:38 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Bunch", "Pete", ""], ["Godsill", "Simon", ""]]}, {"id": "1401.2822", "submitter": "Alexandru  Am\\u{a}rioarei", "authors": "Alexandru Amarioarei and Cristian Preda", "title": "Approximations for two-dimensional discrete scan statistics in some\n  block-factor type dependent models", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the two-dimensional discrete scan statistic generated by a\nblock-factor type model obtained from i.i.d. sequence. We present an\napproximation for the distribution of the scan statistics and the corresponding\nerror bounds. A simulation study illustrates our methodology.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 12:57:57 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Amarioarei", "Alexandru", ""], ["Preda", "Cristian", ""]]}, {"id": "1401.2894", "submitter": "Theodore  Kypraios", "authors": "Christopher J. Fallaize and Theodore Kypraios", "title": "Exact Bayesian Inference for the Bingham Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with making Bayesian inference from data that are\nassumed to be drawn from a Bingham distribution. A barrier to the Bayesian\napproach is the parameter-dependent normalising constant of the Bingham\ndistribution, which, even when it can be evaluated or accurately approximated,\nwould have to be calculated at each iteration of an MCMC scheme, thereby\ngreatly increasing the computational burden. We propose a method which enables\nexact (in Monte Carlo sense) Bayesian inference for the unknown parameters of\nthe Bingham distribution by completely avoiding the need to evaluate this\nconstant. We apply the method to simulated and real data, and illustrate that\nit is simpler to implement, faster, and performs better than an alternative\nalgorithm that has recently been proposed in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 16:10:07 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Fallaize", "Christopher J.", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1401.2957", "submitter": "Wagner Bonat", "authors": "Wagner Hugo Bonat and Paulo Justiniano Ribeiro Jr and Silvia emiko\n  Shimakura", "title": "Bayesian analysis for a class of beta mixed models", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear mixed models (GLMM) encompass large class of statistical\nmodels, with a vast range of applications areas. GLMM extends the linear mixed\nmodels allowing for different types of response variable. Three most common\ndata types are continuous, counts and binary and standard distributions for\nthese types of response variables are Gaussian, Poisson and Binomial,\nrespectively. Despite that flexibility, there are situations where the response\nvariable is continuous, but bounded, such as rates, percentages, indexes and\nproportions. In such situations the usual GLMM's are not adequate because\nbounds are ignored and the beta distribution can be used. Likelihood and\nBayesian inference for beta mixed models are not straightforward demanding a\ncomputational overhead. Recently, a new algorithm for Bayesian inference called\nINLA (Integrated Nested Laplace Approximation) was proposed.INLA allows\ncomputation of many Bayesian GLMMs in a reasonable amount time allowing\nextensive comparison among models. We explore Bayesian inference for beta mixed\nmodels by INLA. We discuss the choice of prior distributions, sensitivity\nanalysis and model selection measures through a real data set. The results\nobtained from INLA are compared with those obtained by an MCMC algorithm and\nlikelihood analysis. We analyze data from an study on a life quality index of\nindustry workers collected according to a hierarchical sampling scheme. Results\nshow that the INLA approach is suitable and faster to fit the proposed beta\nmixed models producing results similar to alternative algorithms and with\neasier handling of modeling alternatives. Sensitivity analysis, measures of\ngoodness of fit and model choice are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 19:06:40 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 12:47:47 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bonat", "Wagner Hugo", ""], ["Ribeiro", "Paulo Justiniano", "Jr"], ["Shimakura", "Silvia emiko", ""]]}, {"id": "1401.3105", "submitter": "Pepa Ram\\'irez-Cobo", "authors": "Emilio Carrizosa and Pepa Ram\\'irez-Cobo", "title": "Maximum likelihood estimation in the two-state Markovian arrival process", "comments": "20 PAGES, 2 FIGURES", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markovian arrival process (MAP) has proven a versatile model for fitting\ndependent and non-exponential interarrival times, with a number of applications\nto queueing, teletraffic, reliability or finance. Despite theoretical\nproperties of MAPs and models involving MAPs are well studied, their estimation\nremains less explored. This paper examines maximum likelihood estimation of the\nsecond-order MAP using a recently obtained parameterization of the two-state\nMAPs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 09:12:47 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Carrizosa", "Emilio", ""], ["Ram\u00edrez-Cobo", "Pepa", ""]]}, {"id": "1401.3269", "submitter": "Nicholas Horton", "authors": "Nicholas J Horton and Benjamin S Baumer and Hadley Wickham", "title": "Teaching precursors to data science in introductory and second courses\n  in statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CY stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics students need to develop the capacity to make sense of the\nstaggering amount of information collected in our increasingly data-centered\nworld. Data science is an important part of modern statistics, but our\nintroductory and second statistics courses often neglect this fact. This paper\ndiscusses ways to provide a practical foundation for students to learn to\n\"compute with data\" as defined by Nolan and Temple Lang (2010), as well as\ndevelop \"data habits of mind\" (Finzer, 2013). We describe how introductory and\nsecond courses can integrate two key precursors to data science: the use of\nreproducible analysis tools and access to large databases. By introducing\nstudents to commonplace tools for data management, visualization, and\nreproducible analysis in data science and applying these to real-world\nscenarios, we prepare them to think statistically in the era of big data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 17:43:11 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Horton", "Nicholas J", ""], ["Baumer", "Benjamin S", ""], ["Wickham", "Hadley", ""]]}, {"id": "1401.3632", "submitter": "Shaan Qamar", "authors": "Shaan Qamar, Rajarshi Guhaniyogi, David B. Dunson", "title": "Bayesian Conditional Density Filtering", "comments": "41 pages, 7 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Conditional Density Filtering (C-DF) algorithm for efficient\nonline Bayesian inference. C-DF adapts MCMC sampling to the online setting,\nsampling from approximations to conditional posterior distributions obtained by\npropagating surrogate conditional sufficient statistics (a function of data and\nparameter estimates) as new data arrive. These quantities eliminate the need to\nstore or process the entire dataset simultaneously and offer a number of\ndesirable features. Often, these include a reduction in memory requirements and\nruntime and improved mixing, along with state-of-the-art parameter inference\nand prediction. These improvements are demonstrated through several\nillustrative examples including an application to high dimensional compressed\nregression. Finally, we show that C-DF samples converge to the target posterior\ndistribution asymptotically as sampling proceeds and more data arrives.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 15:40:40 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 21:47:00 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2015 07:41:00 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Qamar", "Shaan", ""], ["Guhaniyogi", "Rajarshi", ""], ["Dunson", "David B.", ""]]}, {"id": "1401.3812", "submitter": "Ozgur Asar", "authors": "Ozgur Asar, Ozlem Ilk, Osman Dag", "title": "Estimating Box-Cox power transformation parameter via goodness of fit\n  tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Box-Cox power transformation is a commonly used methodology to transform the\ndistribution of a non-normal data into a normal one. Estimation of the\ntransformation parameter is crucial in this methodology. In this study, the\nestimation process is hold via a searching algorithm and is integrated into\nwell-known seven goodness of fit tests for normal distribution. An artificial\ncovariate method is also included for comparative purposes. Simulation studies\nare implemented to compare the effectiveness of the proposed methods. The\nmethods are also illustrated on two different real life data applications.\nMoreover, an R package AID is proposed for implementation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 02:21:05 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Asar", "Ozgur", ""], ["Ilk", "Ozlem", ""], ["Dag", "Osman", ""]]}, {"id": "1401.4082", "submitter": "Shakir Mohamed", "authors": "Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative\n  Models", "comments": "Appears In Proceedings of the 31st International Conference on\n  Machine Learning (ICML), JMLR: W\\&CP volume 32, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We marry ideas from deep neural networks and approximate Bayesian inference\nto derive a generalised class of deep, directed generative models, endowed with\na new algorithm for scalable inference and learning. Our algorithm introduces a\nrecognition model to represent approximate posterior distributions, and that\nacts as a stochastic encoder of the data. We develop stochastic\nback-propagation -- rules for back-propagation through stochastic variables --\nand use this to develop an algorithm that allows for joint optimisation of the\nparameters of both the generative and recognition model. We demonstrate on\nseveral real-world data sets that the model generates realistic samples,\nprovides accurate imputations of missing data and is a useful tool for\nhigh-dimensional data visualisation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 16:33:23 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 12:53:17 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 10:00:36 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""], ["Wierstra", "Daan", ""]]}, {"id": "1401.4369", "submitter": "Andrew Golightly", "authors": "Andrew Golightly, Daniel A. Henderson and Chris Sherlock", "title": "Delayed acceptance particle MCMC for exact inference in stochastic\n  kinetic models", "comments": "Statistics and Computing (to appear)", "journal-ref": null, "doi": "10.1007/s11222-014-9469-x", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently-proposed particle MCMC methods provide a flexible way of performing\nBayesian inference for parameters governing stochastic kinetic models defined\nas Markov (jump) processes (MJPs). Each iteration of the scheme requires an\nestimate of the marginal likelihood calculated from the output of a sequential\nMonte Carlo scheme (also known as a particle filter). Consequently, the method\ncan be extremely computationally intensive. We therefore aim to avoid most\ninstances of the expensive likelihood calculation through use of a fast\napproximation. We consider two approximations: the chemical Langevin equation\ndiffusion approximation (CLE) and the linear noise approximation (LNA). Either\nan estimate of the marginal likelihood under the CLE, or the tractable marginal\nlikelihood under the LNA can be used to calculate a first step acceptance\nprobability. Only if a proposal is accepted under the approximation do we then\nrun a sequential Monte Carlo scheme to compute an estimate of the marginal\nlikelihood under the true MJP and construct a second stage acceptance\nprobability that permits exact (simulation based) inference for the MJP. We\ntherefore avoid expensive calculations for proposals that are likely to be\nrejected. We illustrate the method by considering inference for parameters\ngoverning a Lotka-Volterra system, a model of gene expression and a simple\nepidemic process.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 14:37:00 GMT"}, {"version": "v2", "created": "Fri, 16 May 2014 12:39:50 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Golightly", "Andrew", ""], ["Henderson", "Daniel A.", ""], ["Sherlock", "Chris", ""]]}, {"id": "1401.4896", "submitter": "Sonja Petrovic", "authors": "Elizabeth Gross, Sonja Petrovi\\'c, Despina Stasi", "title": "Goodness-of-fit for log-linear network models: Dynamic Markov bases\n  using hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks and other large sparse data sets pose significant challenges\nfor statistical inference, as many standard statistical methods for testing\nmodel fit are not applicable in such settings. Algebraic statistics offers a\ntheoretically justified approach to goodness-of-fit testing that relies on the\ntheory of Markov bases and is intimately connected with the geometry of the\nmodel as described by its fibers.\n  Most current practices require the computation of the entire basis, which is\ninfeasible in many practical settings. We present a dynamic approach to explore\nthe fiber of a model, which bypasses this issue, and is based on the\ncombinatorics of hypergraphs arising from the toric algebra structure of\nlog-linear models.\n  We demonstrate the approach on the Holland-Leinhardt $p_1$ model for random\ndirected graphs that allows for reciprocated edges.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 13:38:10 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Gross", "Elizabeth", ""], ["Petrovi\u0107", "Sonja", ""], ["Stasi", "Despina", ""]]}, {"id": "1401.4912", "submitter": "Mehdi Molkaraie", "authors": "Mehdi Molkaraie", "title": "An Importance Sampling Scheme on Dual Factor Graphs. I. Models in a\n  Strong External Field", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cond-mat.stat-mech cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an importance sampling scheme to estimate the partition function\nof the two-dimensional ferromagnetic Ising model and the two-dimensional\nferromagnetic $q$-state Potts model, both in the presence of an external\nmagnetic field. The proposed scheme operates in the dual Forney factor graph\nand is capable of efficiently computing an estimate of the partition function\nunder a wide range of model parameters. In particular, we consider models that\nare in a strong external magnetic field.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 14:19:09 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2014 18:48:59 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 01:44:43 GMT"}, {"version": "v4", "created": "Wed, 21 Jan 2015 10:12:39 GMT"}, {"version": "v5", "created": "Thu, 5 Feb 2015 15:20:25 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Molkaraie", "Mehdi", ""]]}, {"id": "1401.5548", "submitter": "Radford M. Neal", "authors": "Alexander Y. Shestopaloff and Radford M. Neal", "title": "On Bayesian inference for the M/G/1 queue with efficient MCMC sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an efficient MCMC sampling scheme to perform Bayesian inference\nin the M/G/1 queueing model given only observations of interdeparture times.\nOur MCMC scheme uses a combination of Gibbs sampling and simple Metropolis\nupdates together with three novel \"shift\" and \"scale\" updates. We show that our\nnovel updates improve the speed of sampling considerably, by factors of about\n60 to about 180 on a variety of simulated data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 03:33:45 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Shestopaloff", "Alexander Y.", ""], ["Neal", "Radford M.", ""]]}, {"id": "1401.5617", "submitter": "Paolo Paruolo", "authors": "William Becker, Paolo Paruolo, Andrea Saltelli", "title": "Exploring Hoover and Perez's experimental designs using global\n  sensitivity analysis", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates variable-selection procedures in regression that make\nuse of global sensitivity analysis. The approach is combined with existing\nalgorithms and it is applied to the time series regression designs proposed by\nHoover and Perez. A comparison of an algorithm employing global sensitivity\nanalysis and the (optimized) algorithm of Hoover and Perez shows that the\nformer significantly improves the recovery rates of original specifications.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 10:47:46 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Becker", "William", ""], ["Paruolo", "Paolo", ""], ["Saltelli", "Andrea", ""]]}, {"id": "1401.5684", "submitter": "Matthieu Marbac", "authors": "Matthieu Marbac, Christophe Biernacki, Vincent Vandewalle", "title": "Model-based clustering for conditionally correlated categorical data", "comments": null, "journal-ref": "Journal of Classification, 2015, Volume 32, Issue 2 , pp 145-175", "doi": "10.1007/s00357-015-9180-4", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extension of the latent class model is presented for clustering\ncategorical data by relaxing the classical \"class conditional independence\nassumption\" of variables. This model consists in grouping the variables into\ninter-independent and intra-dependent blocks, in order to consider the main\nintra-class correlations. The dependency between variables grouped inside the\nsame block of a class is taken into account by mixing two extreme\ndistributions, which are respectively the independence and the maximum\ndependency. When the variables are dependent given the class, this approach is\nexpected to reduce the biases of the latent class model. Indeed, it produces a\nmeaningful dependency model with only a few additional parameters. The\nparameters are estimated, by maximum likelihood, by means of an EM algorithm.\nMoreover, a Gibbs sampler is used for model selection in order to overcome the\ncomputational intractability of the combinatorial problems involved by the\nblock structure search. Two applications on medical and biological data sets\nshow the relevance of this new model. The results strengthen the view that this\nmodel is meaningful and that it reduces the biases induced by the conditional\nindependence assumption of the latent class model.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 14:39:22 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2014 15:22:02 GMT"}, {"version": "v3", "created": "Thu, 10 Jul 2014 14:12:55 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Marbac", "Matthieu", ""], ["Biernacki", "Christophe", ""], ["Vandewalle", "Vincent", ""]]}, {"id": "1401.6389", "submitter": "Terence Sloan", "authors": "T. M. Sloan, M. Piotrowski, T. Forster, P. Ghazal", "title": "Parallel Optimisation of Bootstrapping in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrapping is a popular and computationally demanding resampling method\nused for measuring the accuracy of sample estimates and assisting with\nstatistical inference. R is a freely available language and environment for\nstatistical computing popular with biostatisticians for genomic data analyses.\nA survey of such R users highlighted its implementation of bootstrapping as a\nprime candidate for parallelization to overcome computational bottlenecks. The\nSimple Parallel R Interface (SPRINT) is a package that allows R users to\nexploit high performance computing in multi-core desktops and supercomputers\nwithout expert knowledge of such systems. This paper describes the\nparallelization of bootstrapping for inclusion in the SPRINT R package.\nDepending on the complexity of the bootstrap statistic and the number of\nresamples, this implementation has close to optimal speed up on up to 16 nodes\nof a supercomputer and close to 100 on 512 nodes. This performance in a\nmulti-node setting compares favourably with an existing parallelization option\nin the native R implementation of bootstrapping.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 16:17:55 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Sloan", "T. M.", ""], ["Piotrowski", "M.", ""], ["Forster", "T.", ""], ["Ghazal", "P.", ""]]}, {"id": "1401.7372", "submitter": "Jeroen Ooms", "authors": "Dirk Eddelbuettel, Murray Stokely, Jeroen Ooms", "title": "RProtoBuf: Efficient Cross-Language Data Serialization in R", "comments": null, "journal-ref": null, "doi": "10.18637/jss.v071.i02", "report-no": null, "categories": "stat.CO cs.MS cs.SE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Modern data collection and analysis pipelines often involve a sophisticated\nmix of applications written in general purpose and specialized programming\nlanguages. Many formats commonly used to import and export data between\ndifferent programs or systems, such as CSV or JSON, are verbose, inefficient,\nnot type-safe, or tied to a specific programming language. Protocol Buffers are\na popular method of serializing structured data between applications - while\nremaining independent of programming languages or operating systems. They offer\na unique combination of features, performance, and maturity that seems\nparticularly well suited for data-driven applications and numerical computing.\nThe RProtoBuf package provides a complete interface to Protocol Buffers from\nthe R environment for statistical computing. This paper outlines the general\nclass of data serialization requirements for statistical computing, describes\nthe implementation of the RProtoBuf package, and illustrates its use with\nexample applications in large-scale data collection pipelines and web services.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 23:57:02 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Eddelbuettel", "Dirk", ""], ["Stokely", "Murray", ""], ["Ooms", "Jeroen", ""]]}, {"id": "1401.8236", "submitter": "Michael Braun", "authors": "Michael Braun and Paul Damien", "title": "Scalable Rejection Sampling for Bayesian Hierarchical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical modeling is a popular approach to capturing unobserved\nheterogeneity across individual units. However, standard estimation methods\nsuch as Markov chain Monte Carlo (MCMC) can be impracticable for modeling\noutcomes from a large number of units. We develop a new method to sample from\nposterior distributions of Bayesian models, without using MCMC. Samples are\nindependent, so they can be collected in parallel, and we do not need to be\nconcerned with issues like chain convergence and autocorrelation. The algorithm\nis scalable under the weak assumption that individual units are conditionally\nindependent, making it applicable for large datasets. It can also be used to\ncompute marginal likelihoods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 17:35:09 GMT"}, {"version": "v2", "created": "Wed, 6 Aug 2014 21:36:14 GMT"}, {"version": "v3", "created": "Sun, 2 Nov 2014 01:00:14 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Braun", "Michael", ""], ["Damien", "Paul", ""]]}]