[{"id": "1703.00368", "submitter": "Gabriel Terejanu", "authors": "Xiao Lin, Asif Chowdhury, Xiaofan Wang, Gabriel Terejanu", "title": "Approximate Computational Approaches for Bayesian Sensor Placement in\n  High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the cost of installing and maintaining sensors is usually high, sensor\nlocations are always strategically selected. For those aiming at inferring\ncertain quantities of interest (QoI), it is desirable to explore the dependency\nbetween sensor measurements and QoI. One of the most popular metric for the\ndependency is mutual information which naturally measures how much information\nabout one variable can be obtained given the other. However, computing mutual\ninformation is always challenging, and the result is unreliable in high\ndimension. In this paper, we propose an approach to find an approximate lower\nbound of mutual information and compute it in a lower dimension. Then, sensors\nare placed where highest mutual information (lower bound) is achieved and QoI\nis inferred via Bayes rule given sensor measurements. In addition, Bayesian\noptimization is introduced to provide a continuous mutual information surface\nover the domain and thus reduce the number of evaluations. A chemical release\naccident is simulated where multiple sensors are placed to locate the source of\nthe release. The result shows that the proposed approach is both effective and\nefficient in inferring QoI.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 16:29:12 GMT"}, {"version": "v2", "created": "Sun, 19 Mar 2017 16:03:04 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Lin", "Xiao", ""], ["Chowdhury", "Asif", ""], ["Wang", "Xiaofan", ""], ["Terejanu", "Gabriel", ""]]}, {"id": "1703.00864", "submitter": "Mark Rowland", "authors": "Krzysztof Choromanski, Mark Rowland, Adrian Weller", "title": "The Unreasonable Effectiveness of Structured Random Orthogonal\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a class of embeddings based on structured random matrices with\northogonal rows which can be applied in many machine learning applications\nincluding dimensionality reduction and kernel approximation. For both the\nJohnson-Lindenstrauss transform and the angular kernel, we show that we can\nselect matrices yielding guaranteed improved performance in accuracy and/or\nspeed compared to earlier methods. We introduce matrices with complex entries\nwhich give significant further accuracy improvement. We provide geometric and\nMarkov chain-based perspectives to help understand the benefits, and empirical\nresults which suggest that the approach is helpful in a wider range of\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 17:33:58 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 21:25:36 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 18:28:37 GMT"}, {"version": "v4", "created": "Tue, 2 Jan 2018 19:57:05 GMT"}, {"version": "v5", "created": "Mon, 3 Sep 2018 08:55:55 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Rowland", "Mark", ""], ["Weller", "Adrian", ""]]}, {"id": "1703.01106", "submitter": "Mikko Heikkil\\\"a", "authors": "Mikko Heikkil\\\"a and Eemil Lagerspetz and Samuel Kaski and Kana\n  Shimizu and Sasu Tarkoma and Antti Honkela", "title": "Differentially Private Bayesian Learning on Distributed Data", "comments": "13 pages, 7 figures. Modified text, changed algorithm used, included\n  tests on additional dataset, fixed several errors, added proof of asymptotic\n  efficiency to supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of machine learning, for example in health care, would\nbenefit from methods that can guarantee privacy of data subjects. Differential\nprivacy (DP) has become established as a standard for protecting learning\nresults. The standard DP algorithms require a single trusted party to have\naccess to the entire data, which is a clear weakness. We consider DP Bayesian\nlearning in a distributed setting, where each party only holds a single sample\nor a few samples of the data. We propose a learning strategy based on a secure\nmulti-party sum function for aggregating summaries from data holders and the\nGaussian mechanism for DP. Our method builds on an asymptotically optimal and\npractically efficient DP Bayesian inference with rapidly diminishing extra\ncost.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 10:44:47 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 15:11:26 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Heikkil\u00e4", "Mikko", ""], ["Lagerspetz", "Eemil", ""], ["Kaski", "Samuel", ""], ["Shimizu", "Kana", ""], ["Tarkoma", "Sasu", ""], ["Honkela", "Antti", ""]]}, {"id": "1703.01234", "submitter": "Ian Vernon Dr", "authors": "Ian Vernon and John Paul Gosling", "title": "A Bayesian computer model analysis of Robust Bayesian analyses", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We harness the power of Bayesian emulation techniques, designed to aid the\nanalysis of complex computer models, to examine the structure of complex\nBayesian analyses themselves. These techniques facilitate robust Bayesian\nanalyses and/or sensitivity analyses of complex problems, and hence allow\nglobal exploration of the impacts of choices made in both the likelihood and\nprior specification. We show how previously intractable problems in robustness\nstudies can be overcome using emulation techniques, and how these methods allow\nother scientists to quickly extract approximations to posterior results\ncorresponding to their own particular subjective specification. The utility and\nflexibility of our method is demonstrated on a reanalysis of a real application\nwhere Bayesian methods were employed to capture beliefs about river flow. We\ndiscuss the obvious extensions and directions of future research that such an\napproach opens up.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 16:29:21 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Vernon", "Ian", ""], ["Gosling", "John Paul", ""]]}, {"id": "1703.01273", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio Gomez-Rubio, Roger S. Bivand, H{\\aa}vard Rue", "title": "Estimating Spatial Econometrics Models with Integrated Nested Laplace\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Nested Laplace Approximation provides a fast and effective method\nfor marginal inference on Bayesian hierarchical models. This methodology has\nbeen implemented in the R-INLA package which permits INLA to be used from\nwithin R statistical software. Although INLA is implemented as a general\nmethodology, its use in practice is limited to the models implemented in the\nR-INLA package.\n  Spatial autoregressive models are widely used in spatial econometrics but\nhave until now been missing from the R-INLA package. In this paper, we describe\nthe implementation and application of a new class of latent models in INLA made\navailable through R-INLA. This new latent class implements a standard spatial\nlag model, which is widely used and that can be used to build more complex\nmodels in spatial econometrics.\n  The implementation of this latent model in R-INLA also means that all the\nother features of INLA can be used for model fitting, model selection and\ninference in spatial econometrics, as will be shown in this paper. Finally, we\nwill illustrate the use of this new latent model and its applications with two\ndatasets based on Gaussian and binary outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 18:07:15 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 09:50:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gomez-Rubio", "Virgilio", ""], ["Bivand", "Roger S.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1703.01421", "submitter": "Zhou Fan", "authors": "Zhou Fan and Leying Guan", "title": "Approximate $l_0$-penalized estimation of piecewise-constant signals on\n  graphs", "comments": "v2: Title change, renumbering of sections and theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study recovery of piecewise-constant signals on graphs by the estimator\nminimizing an $l_0$-edge-penalized objective. Although exact minimization of\nthis objective may be computationally intractable, we show that the same\nstatistical risk guarantees are achieved by the $\\alpha$-expansion algorithm\nwhich computes an approximate minimizer in polynomial time. We establish that\nfor graphs with small average vertex degree, these guarantees are minimax\nrate-optimal over classes of edge-sparse signals. For spatially inhomogeneous\ngraphs, we propose minimization of an edge-weighted objective where each edge\nis weighted by its effective resistance or another measure of its contribution\nto the graph's connectivity. We establish minimax optimality of the resulting\nestimators over corresponding edge-weighted sparsity classes. We show\ntheoretically that these risk guarantees are not always achieved by the\nestimator minimizing the $l_1$/total-variation relaxation, and empirically that\nthe $l_0$-based estimates are more accurate in high signal-to-noise settings.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 09:12:42 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 17:16:01 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Fan", "Zhou", ""], ["Guan", "Leying", ""]]}, {"id": "1703.01526", "submitter": "Prashanth R", "authors": "R. Prashanth, Sumantra Dutta Roy, Pravat K. Mandal, Shantanu Ghosh", "title": "High Accuracy Classification of Parkinson's Disease through Shape\n  Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging", "comments": "9 pages, 5 figures, Accepted in the IEEE Journal of Biomedical and\n  Health Informatics, Additional supplementary documents available at\n  http://ieeexplore.ieee.org/document/7442754/", "journal-ref": null, "doi": "10.1109/JBHI.2016.2547901", "report-no": null, "categories": "stat.AP cs.CV physics.data-an stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early and accurate identification of parkinsonian syndromes (PS) involving\npresynaptic degeneration from non-degenerative variants such as Scans Without\nEvidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for\neffective patient management as the course, therapy and prognosis differ\nsubstantially between the two groups. In this study, we use Single Photon\nEmission Computed Tomography (SPECT) images from healthy normal, early PD and\nSWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative\n(PPMI) database, and process them to compute shape- and surface fitting-based\nfeatures for the three groups. We use these features to develop and compare\nvarious classification models that can discriminate between scans showing\ndopaminergic deficit, as in PD, from scans without the deficit, as in healthy\nnormal or SWEDD. Along with it, we also compare these features with Striatal\nBinding Ratio (SBR)-based features, which are well-established and clinically\nused, by computing a feature importance score using Random forests technique.\nWe observe that the Support Vector Machine (SVM) classifier gave the best\nperformance with an accuracy of 97.29%. These features also showed higher\nimportance than the SBR-based features. We infer from the study that shape\nanalysis and surface fitting are useful and promising methods for extracting\ndiscriminatory features that can be used to develop diagnostic models that\nmight have the potential to help clinicians in the diagnostic process.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 21:50:25 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Prashanth", "R.", ""], ["Roy", "Sumantra Dutta", ""], ["Mandal", "Pravat K.", ""], ["Ghosh", "Shantanu", ""]]}, {"id": "1703.02081", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Trevor Hastie, Daryl Pregibon", "title": "Estimation and prediction in sparse and unbalanced tables", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem where we have a multi-way table of means, indexed by\nseveral factors, where each factor can have a large number of levels. The entry\nin each cell is the mean of some response, averaged over the observations\nfalling into that cell. Some cells may be very sparsely populated, and in\nextreme cases, not populated at all. We might still like to estimate an\nexpected response in such cells. We propose here a novel hierarchical ANOVA\n(HANOVA) representation for such data. Sparse cells will lean more on the\nlower-order interaction model for the data. These in turn could have components\nthat are poorly represented in the data, in which case they rely on yet\nlower-order models. Our approach leads to a simple hierarchical algorithm,\nrequiring repeated calculations of sub-table means of modified counts. The\nalgorithm has shown superiority over the unshrinked methods in both simulations\nand real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 19:44:45 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Hastie", "Trevor", ""], ["Pregibon", "Daryl", ""]]}, {"id": "1703.02151", "submitter": "Anthony Ebert", "authors": "Anthony Ebert, Paul Wu, Kerrie Mengersen, Fabrizio Ruggeri", "title": "Computationally Efficient Simulation of Queues: The R Package\n  queuecomputer", "comments": "Updated for queuecomputer_0.8.3", "journal-ref": "Journal of Statistical Software 95.1 (2020): 1-29", "doi": "10.18637/jss.v095.i05", "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large networks of queueing systems model important real-world systems such as\nMapReduce clusters, web-servers, hospitals, call centers and airport passenger\nterminals. To model such systems accurately, we must infer queueing parameters\nfrom data. Unfortunately, for many queueing networks there is no clear way to\nproceed with parameter inference from data. Approximate Bayesian computation\ncould offer a straightforward way to infer parameters for such networks if we\ncould simulate data quickly enough.\n  We present a computationally efficient method for simulating from a very\ngeneral set of queueing networks with the R package queuecomputer. Remarkable\nspeedups of more than 2 orders of magnitude are observed relative to the\npopular DES packages simmer and simpy. We replicate output from these packages\nto validate the package.\n  The package is modular and integrates well with the popular R package dplyr.\nComplex queueing networks with tandem, parallel and fork/join topologies can\neasily be built with these two packages together. We show how to use this\npackage with two examples: a call center and an airport terminal.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 23:48:50 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 08:22:25 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 05:56:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ebert", "Anthony", ""], ["Wu", "Paul", ""], ["Mengersen", "Kerrie", ""], ["Ruggeri", "Fabrizio", ""]]}, {"id": "1703.02177", "submitter": "Paul McNicholas", "authors": "Yuhong Wei, Yang Tang and Paul D. McNicholas", "title": "Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t\n  Distributions for Model-Based Clustering with Incomplete Data", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2018.08.016", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust clustering from incomplete data is an important topic because, in many\npractical situations, real data sets are heavy-tailed, asymmetric, and/or have\narbitrary patterns of missing observations. Flexible methods and algorithms for\nmodel-based clustering are presented via mixture of the generalized hyperbolic\ndistributions and its limiting case, the mixture of multivariate skew-t\ndistributions. An analytically feasible EM algorithm is formulated for\nparameter estimation and imputation of missing values for mixture models\nemploying missing at random mechanisms. The proposed methodologies are\ninvestigated through a simulation study with varying proportions of synthetic\nmissing values and illustrated using a real dataset. Comparisons are made with\nthose obtained from the traditional mixture of generalized hyperbolic\ndistribution counterparts by filling in the missing data using the mean\nimputation method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 02:14:38 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 17:47:33 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 18:37:31 GMT"}, {"version": "v4", "created": "Fri, 27 Apr 2018 18:06:09 GMT"}, {"version": "v5", "created": "Sun, 19 Aug 2018 21:50:15 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wei", "Yuhong", ""], ["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1703.02237", "submitter": "Cheng Ju", "authors": "Cheng Ju, Susan Gruber, Samuel D. Lendle, Antoine Chambaz, Jessica M.\n  Franklin, Richard Wyss, Sebastian Schneeweiss, Mark J. van der Laan", "title": "Scalable Collaborative Targeted Learning for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust inference of a low-dimensional parameter in a large semi-parametric\nmodel relies on external estimators of infinite-dimensional features of the\ndistribution of the data. Typically, only one of the latter is optimized for\nthe sake of constructing a well behaved estimator of the low-dimensional\nparameter of interest. Optimizing more than one of them for the sake of\nachieving a better bias-variance trade-off in the estimation of the parameter\nof interest is the core idea driving the general template of the collaborative\ntargeted minimum loss-based estimation (C-TMLE) procedure. The original\nimplementation/instantiation of the C-TMLE template can be presented as a\ngreedy forward stepwise C-TMLE algorithm. It does not scale well when the\nnumber $p$ of covariates increases drastically. This motivates the introduction\nof a novel instantiation of the C-TMLE template where the covariates are\npre-ordered. Its time complexity is $\\mathcal{O}(p)$ as opposed to the original\n$\\mathcal{O}(p^2)$, a remarkable gain. We propose two pre-ordering strategies\nand suggest a rule of thumb to develop other meaningful strategies. Because it\nis usually unclear a priori which pre-ordering strategy to choose, we also\nintroduce another implementation/instantiation called SL-C-TMLE algorithm that\nenables the data-driven choice of the better pre-ordering strategy given the\nproblem at hand. Its time complexity is $\\mathcal{O}(p)$ as well. The\ncomputational burden and relative performance of these algorithms were compared\nin simulation studies involving fully synthetic data or partially synthetic\ndata based on a real world large electronic health database; and in analyses of\nthree real, large electronic health databases. In all analyses involving\nelectronic health databases, the greedy C-TMLE algorithm is unacceptably slow.\nSimulation studies indicate our scalable C-TMLE and SL-C-TMLE algorithms work\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 06:40:44 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Ju", "Cheng", ""], ["Gruber", "Susan", ""], ["Lendle", "Samuel D.", ""], ["Chambaz", "Antoine", ""], ["Franklin", "Jessica M.", ""], ["Wyss", "Richard", ""], ["Schneeweiss", "Sebastian", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1703.02251", "submitter": "Martin Helmer", "authors": "Carlos Am\\'endola, Nathan Bliss, Isaac Burke, Courtney R. Gibbons,\n  Martin Helmer, Serkan Ho\\c{s}ten, Evan D. Nash, Jose Israel Rodriguez, Daniel\n  Smolkin", "title": "The Maximum Likelihood Degree of Toric Varieties", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.jsc.2018.04.016", "report-no": null, "categories": "math.AG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum likelihood degree (ML degree) of toric varieties, known\nas discrete exponential models in statistics. By introducing scaling\ncoefficients to the monomial parameterization of the toric variety, one can\nchange the ML degree. We show that the ML degree is equal to the degree of the\ntoric variety for generic scalings, while it drops if and only if the scaling\nvector is in the locus of the principal $A$-determinant. We also illustrate how\nto compute the ML estimate of a toric variety numerically via homotopy\ncontinuation from a scaled toric variety with low ML degree. Throughout, we\ninclude examples motivated by algebraic geometry and statistics. We compute the\nML degree of rational normal scrolls and a large class of Veronese-type\nvarieties. In addition, we investigate the ML degree of scaled Segre varieties,\nhierarchical loglinear models, and graphical models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 07:44:52 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 20:36:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Bliss", "Nathan", ""], ["Burke", "Isaac", ""], ["Gibbons", "Courtney R.", ""], ["Helmer", "Martin", ""], ["Ho\u015ften", "Serkan", ""], ["Nash", "Evan D.", ""], ["Rodriguez", "Jose Israel", ""], ["Smolkin", "Daniel", ""]]}, {"id": "1703.02293", "submitter": "Mohammed Sedki", "authors": "Matthieu Marbac, Mohammed Sedki", "title": "Variable selection for mixed data clustering: a model-based approach", "comments": "submitted to biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two approaches for selecting variables in latent class analysis\n(i.e.,mixture model assuming within component independence), which is the\ncommon model-based clustering method for mixed data. The first approach\nconsists in optimizing the BIC with a modified version of the EM algorithm.\nThis approach simultaneously performs both model selection and parameter\ninference. The second approach consists in maximizing the MICL, which considers\nthe clustering task, with an algorithm of alternate optimization. This approach\nperforms model selection without requiring the maximum likelihood estimates for\nmodel comparison, then parameter inference is done for the unique selected\nmodel. Thus, the benefits of both approaches is to avoid the computation of the\nmaximum likelihood estimates for each model comparison. Moreover, they also\navoid the use of the standard algorithms for variable selection which are often\nsuboptimal (e.g. stepwise method) and computationally expensive. The case of\ndata with missing values is also discussed. The interest of both proposed\ncriteria is shown on simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 09:33:21 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Marbac", "Matthieu", ""], ["Sedki", "Mohammed", ""]]}, {"id": "1703.02337", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen", "title": "A Note on the Convergence of the Gaussian Mean Shift Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean shift (MS) algorithms are popular methods for mode finding in pattern\nanalysis. Each MS algorithm can be phrased as a fixed-point iteration scheme,\nwhich operates on a kernel density estimate (KDE) based on some data. The\nability of an MS algorithm to obtain the modes of its KDE depends on whether or\nnot the fixed-point scheme converges. The convergence of MS algorithms have\nrecently been proved under some general conditions via first principle\narguments. We complement the recent proofs by demonstrating that the MS\nalgorithm operating on a Gaussian KDE can be viewed as an MM\n(minorization-maximization) algorithm, and thus permits the application of\nconvergence techniques for such constructions. For the Gaussian case, we extend\nupon the previously results by showing that the fixed-points of the MS\nalgorithm are all stationary points of the KDE in cases where the stationary\npoints may not necessarily be isolated.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 11:35:29 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 11:11:48 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Nguyen", "Hien D", ""]]}, {"id": "1703.02341", "submitter": "Jonathan Harrison", "authors": "Jonathan U Harrison and Ruth E Baker", "title": "An automatic adaptive method to combine summary statistics in\n  approximate Bayesian computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To infer the parameters of mechanistic models with intractable likelihoods,\ntechniques such as approximate Bayesian computation (ABC) are increasingly\nbeing adopted. One of the main disadvantages of ABC in practical situations,\nhowever, is that parameter inference must generally rely on summary statistics\nof the data. This is particularly the case for problems involving\nhigh-dimensional data, such as biological imaging experiments. However, some\nsummary statistics contain more information about parameters of interest than\nothers, and it is not always clear how to weight their contributions within the\nABC framework. We address this problem by developing an automatic, adaptive\nalgorithm that chooses weights for each summary statistic. Our algorithm aims\nto maximize the distance between the prior and the approximate posterior by\nautomatically adapting the weights within the ABC distance function.\nComputationally, we use a nearest neighbour estimator of the distance between\ndistributions. We justify the algorithm theoretically based on properties of\nthe nearest neighbour distance estimator. To demonstrate the effectiveness of\nour algorithm, we apply it to a variety of test problems, including several\nstochastic models of biochemical reaction networks, and a spatial model of\ndiffusion, and compare our results with existing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 11:48:45 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 10:52:37 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Harrison", "Jonathan U", ""], ["Baker", "Ruth E", ""]]}, {"id": "1703.02419", "submitter": "Andreas Svensson", "authors": "Thomas B. Sch\\\"on, Andreas Svensson, Lawrence Murray, Fredrik Lindsten", "title": "Probabilistic learning of nonlinear dynamical systems using sequential\n  Monte Carlo", "comments": "Thomas B. Sch\\\"on, Andreas Svensson, Lawrence Murray and Fredrik\n  Lindsten, 2018. Probabilistic learning of nonlinear dynamical systems using\n  sequential Monte Carlo. In Mechanical Systems and Signal Processing, Volume\n  104, pp. 866-883", "journal-ref": null, "doi": "10.1016/j.ymssp.2017.10.033", "report-no": null, "categories": "stat.CO cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling provides the capability to represent and manipulate\nuncertainty in data, models, predictions and decisions. We are concerned with\nthe problem of learning probabilistic models of dynamical systems from measured\ndata. Specifically, we consider learning of probabilistic nonlinear state-space\nmodels. There is no closed-form solution available for this problem, implying\nthat we are forced to use approximations. In this tutorial we will provide a\nself-contained introduction to one of the state-of-the-art methods---the\nparticle Metropolis--Hastings algorithm---which has proven to offer a practical\napproximation. This is a Monte Carlo based method, where the particle filter is\nused to guide a Markov chain Monte Carlo method through the parameter space.\nOne of the key merits of the particle Metropolis--Hastings algorithm is that it\nis guaranteed to converge to the \"true solution\" under mild assumptions,\ndespite being based on a particle filter with only a finite number of\nparticles. We will also provide a motivating numerical example illustrating the\nmethod using a modeling language tailored for sequential Monte Carlo methods.\nThe intention of modeling languages of this kind is to open up the power of\nsophisticated Monte Carlo methods---including particle\nMetropolis--Hastings---to a large group of users without requiring them to know\nall the underlying mathematical details.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:01:51 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 08:52:57 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Sch\u00f6n", "Thomas B.", ""], ["Svensson", "Andreas", ""], ["Murray", "Lawrence", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1703.02428", "submitter": "Michael Roth", "authors": "Michael Roth, Tohid Ardeshiri, Emre \\\"Ozkan, Fredrik Gustafsson", "title": "Robust Bayesian Filtering and Smoothing Using Student's t Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State estimation in heavy-tailed process and measurement noise is an\nimportant challenge that must be addressed in, e.g., tracking scenarios with\nagile targets and outlier-corrupted measurements. The performance of the Kalman\nfilter (KF) can deteriorate in such applications because of the close relation\nto the Gaussian distribution. Therefore, this paper describes the use of\nStudent's t distribution to develop robust, scalable, and simple filtering and\nsmoothing algorithms.\n  After a discussion of Student's t distribution, exact filtering in linear\nstate-space models with t noise is analyzed. Intermediate approximation steps\nare used to arrive at filtering and smoothing algorithms that closely resemble\nthe KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear\nmeasurement-dependent matrix update. The required approximations are discussed\nand an undesirable behavior of moment matching for t densities is revealed. A\nfavorable approximation based on minimization of the Kullback-Leibler\ndivergence is presented. Because of its relation to the KF, some properties and\nalgorithmic extensions are inherited by the t filter. Instructive simulation\nexamples demonstrate the performance and robustness of the novel algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:13:08 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Roth", "Michael", ""], ["Ardeshiri", "Tohid", ""], ["\u00d6zkan", "Emre", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1703.02518", "submitter": "Martin Jaggi", "authors": "Dmytro Perekrestenko, Volkan Cevher, Martin Jaggi", "title": "Faster Coordinate Descent via Adaptive Importance Sampling", "comments": "appearing at AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coordinate descent methods employ random partial updates of decision\nvariables in order to solve huge-scale convex optimization problems. In this\nwork, we introduce new adaptive rules for the random selection of their\nupdates. By adaptive, we mean that our selection rules are based on the dual\nresidual or the primal-dual gap estimates and can change at each iteration. We\ntheoretically characterize the performance of our selection rules and\ndemonstrate improvements over the state-of-the-art, and extend our theory and\nalgorithms to general convex objectives. Numerical evidence with hinge-loss\nsupport vector machines and Lasso confirm that the practice follows the theory.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:36:55 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Perekrestenko", "Dmytro", ""], ["Cevher", "Volkan", ""], ["Jaggi", "Martin", ""]]}, {"id": "1703.02998", "submitter": "Karl Rohe", "authors": "Karl Rohe, Jun Tao, Xintian Han, Norbert Binkiewicz", "title": "A note on quickly sampling a sparse matrix with low rank expectation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given matrices $X,Y \\in R^{n \\times K}$ and $S \\in R^{K \\times K}$ with\npositive elements, this paper proposes an algorithm fastRG to sample a sparse\nmatrix $A$ with low rank expectation $E(A) = XSY^T$ and independent Poisson\nelements. This allows for quickly sampling from a broad class of stochastic\nblockmodel graphs (degree-corrected, mixed membership, overlapping) all of\nwhich are specific parameterizations of the generalized random product graph\nmodel defined in Section 2.2. The basic idea of fastRG is to first sample the\nnumber of edges $m$ and then sample each edge. The key insight is that because\nof the the low rank expectation, it is easy to sample individual edges. The\nnaive \"element-wise\" algorithm requires $O(n^2)$ operations to generate the\n$n\\times n$ adjacency matrix $A$. In sparse graphs, where $m = O(n)$, ignoring\nlog terms, fastRG runs in time $O(n)$. An implementation in fastRG is available\non github. A computational experiment in Section 2.4 simulates graphs up to\n$n=10,000,000$ nodes with $m = 100,000,000$ edges. For example, on a graph with\n$n=500,000$ and $m = 5,000,000$, fastRG runs in less than one second on a 3.5\nGHz Intel i5.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 19:22:11 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Rohe", "Karl", ""], ["Tao", "Jun", ""], ["Han", "Xintian", ""], ["Binkiewicz", "Norbert", ""]]}, {"id": "1703.03004", "submitter": "Yakoub Boularouk", "authors": "Yakoub Boularouk, Nasr-eddine Hamri", "title": "New approximation for GARCH parameters estimate", "comments": "10 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for the optimization of GARCH parameters\nestimation. Firstly, we propose a method for the localization of the maximum.\nThereafter, using the methods of least squares, we make a local approximation\nfor the projection of the likelihood function curve on two dimensional planes\nby a polynomial of order two which will be used to calculate an estimation of\nthe maximum.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 19:36:24 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 21:00:26 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Boularouk", "Yakoub", ""], ["Hamri", "Nasr-eddine", ""]]}, {"id": "1703.03352", "submitter": "Toby Hocking", "authors": "Toby Dylan Hocking, Guillem Rigaill, Paul Fearnhead, Guillaume Bourque", "title": "A log-linear time algorithm for constrained changepoint detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.GN stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Changepoint detection is a central problem in time series and genomic data.\nFor some applications, it is natural to impose constraints on the directions of\nchanges. One example is ChIP-seq data, for which adding an up-down constraint\nimproves peak detection accuracy, but makes the optimization problem more\ncomplicated. We show how a recently proposed functional pruning technique can\nbe adapted to solve such constrained changepoint detection problems. This leads\nto a new algorithm which can solve problems with arbitrary affine constraints\non adjacent segment means, and which has empirical time complexity that is\nlog-linear in the amount of data. This algorithm achieves state-of-the-art\naccuracy in a benchmark of several genomic data sets, and is orders of\nmagnitude faster than existing algorithms that have similar accuracy. Our\nimplementation is available as the PeakSegPDPA function in the coseg R package,\nhttps://github.com/tdhock/coseg\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 17:17:39 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Hocking", "Toby Dylan", ""], ["Rigaill", "Guillem", ""], ["Fearnhead", "Paul", ""], ["Bourque", "Guillaume", ""]]}, {"id": "1703.03475", "submitter": "Theodore  Kypraios", "authors": "Iker Perez, David Hodge, Theodore Kypraios", "title": "Auxiliary Variables for Bayesian Inference in Multi-Class Queueing\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Queue networks describe complex stochastic systems of both theoretical and\npractical interest. They provide the means to assess alterations, diagnose poor\nperformance and evaluate robustness across sets of interconnected resources. In\nthe present paper, we focus on the underlying continuous-time Markov chains\ninduced by these networks, and we present a flexible method for drawing\nparameter inference in multi-class Markovian cases with switching and different\nservice disciplines. The approach is directed towards the inferential problem\nwith missing data and introduces a slice sampling technique with mappings to\nthe measurable space of task transitions between service stations. The method\ndeals with time and tractability issues, can handle prior system knowledge and\novercomes common restrictions on service rates across existing inferential\nframeworks. Finally, the proposed algorithm is validated on synthetic data and\napplied to a real data set, obtained from a service delivery tasking tool\nimplemented in two university hospitals.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 21:57:00 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 15:00:44 GMT"}, {"version": "v3", "created": "Wed, 1 Nov 2017 13:59:36 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Perez", "Iker", ""], ["Hodge", "David", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1703.03680", "submitter": "Han Cheng Lie", "authors": "H. C. Lie and A. M. Stuart and T. J. Sullivan", "title": "Strong convergence rates of probabilistic integrators for ordinary\n  differential equations", "comments": "25 pages", "journal-ref": "Statistics and Computing (2019)", "doi": "10.1007/s11222-019-09898-6", "report-no": null, "categories": "math.NA cs.NA math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic integration of a continuous dynamical system is a way of\nsystematically introducing model error, at scales no larger than errors\nintroduced by standard numerical discretisation, in order to enable thorough\nexploration of possible responses of the system to inputs. It is thus a\npotentially useful approach in a number of applications such as forward\nuncertainty quantification, inverse problems, and data assimilation. We extend\nthe convergence analysis of probabilistic integrators for deterministic\nordinary differential equations, as proposed by Conrad et al.\\ (\\textit{Stat.\\\nComput.}, 2017), to establish mean-square convergence in the uniform norm on\ndiscrete- or continuous-time solutions under relaxed regularity assumptions on\nthe driving vector fields and their induced flows. Specifically, we show that\nrandomised high-order integrators for globally Lipschitz flows and randomised\nEuler integrators for dissipative vector fields with polynomially-bounded local\nLipschitz constants all have the same mean-square convergence rate as their\ndeterministic counterparts, provided that the variance of the integration noise\nis not of higher order than the corresponding deterministic integrator. These\nand similar results are proven for probabilistic integrators where the random\nperturbations may be state-dependent, non-Gaussian, or non-centred random\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 13:43:06 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 09:21:47 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 15:23:43 GMT"}, {"version": "v4", "created": "Thu, 27 Sep 2018 13:50:16 GMT"}, {"version": "v5", "created": "Tue, 8 Jan 2019 21:19:24 GMT"}, {"version": "v6", "created": "Sat, 26 Oct 2019 18:01:13 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lie", "H. C.", ""], ["Stuart", "A. M.", ""], ["Sullivan", "T. J.", ""]]}, {"id": "1703.04025", "submitter": "Bryon Aragam", "authors": "Bryon Aragam, Jiaying Gu, Qing Zhou", "title": "Learning Large-Scale Bayesian Networks with the sparsebn Package", "comments": "To appear in the Journal of Statistical Software, 39 pages, 7 figures", "journal-ref": "Journal of Statistical Software, 91(11), 1-38, 2019", "doi": "10.18637/jss.v091.i11", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning graphical models from data is an important problem with wide\napplications, ranging from genomics to the social sciences. Nowadays datasets\noften have upwards of thousands---sometimes tens or hundreds of thousands---of\nvariables and far fewer samples. To meet this challenge, we have developed a\nnew R package called sparsebn for learning the structure of large, sparse\ngraphical models with a focus on Bayesian networks. While there are many\nexisting software packages for this task, this package focuses on the unique\nsetting of learning large networks from high-dimensional data, possibly with\ninterventions. As such, the methods provided place a premium on scalability and\nconsistency in a high-dimensional setting. Furthermore, in the presence of\ninterventions, the methods implemented here achieve the goal of learning a\ncausal network from data. Additionally, the sparsebn package is fully\ncompatible with existing software packages for network analysis.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 20:07:06 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 23:22:10 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Aragam", "Bryon", ""], ["Gu", "Jiaying", ""], ["Zhou", "Qing", ""]]}, {"id": "1703.04334", "submitter": "Fani Tsapeli", "authors": "Fani Tsapeli, Peter Tino, Mirco Musolesi", "title": "Probabilistic Matching: Causal Inference under Measurement Errors", "comments": "In Proceedings of International Joint Conference Of Neural Networks\n  (IJCNN) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of data produced daily from large variety of sources has\nboosted the need of novel approaches on causal inference analysis from\nobservational data. Observational data often contain noisy or missing entries.\nMoreover, causal inference studies may require unobserved high-level\ninformation which needs to be inferred from other observed attributes. In such\ncases, inaccuracies of the applied inference methods will result in noisy\noutputs. In this study, we propose a novel approach for causal inference when\none or more key variables are noisy. Our method utilizes the knowledge about\nthe uncertainty of the real values of key variables in order to reduce the bias\ninduced by noisy measurements. We evaluate our approach in comparison with\nexisting methods both on simulated and real scenarios and we demonstrate that\nour method reduces the bias and avoids false causal inference conclusions in\nmost cases.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 11:19:28 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Tsapeli", "Fani", ""], ["Tino", "Peter", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1703.04467", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami", "title": "Spatial regression modeling using the spmoran package: Boston housing\n  price data examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study demonstrates how to use \"spmoran\", an R package estimating spatial\nadditive mixed models and other spatial regression models for Gaussian and\nnon-Gaussian data. Moran eigenvectors are used to an approximate Gaussian\nprocess modeling which is interpretable in terms of the Moran coefficient. The\nGP is used for modeling the spatial processes in residuals and regression\ncoefficients. All these models are estimated computationally efficiently. For\nthe sample code used in this paper, see https://github.com/dmuraka/spmoran.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 16:19:01 GMT"}, {"version": "v10", "created": "Mon, 11 Jan 2021 01:13:46 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 09:09:43 GMT"}, {"version": "v3", "created": "Fri, 23 Jun 2017 13:31:23 GMT"}, {"version": "v4", "created": "Fri, 15 Sep 2017 15:30:29 GMT"}, {"version": "v5", "created": "Fri, 6 Apr 2018 02:41:25 GMT"}, {"version": "v6", "created": "Sat, 6 Oct 2018 04:42:22 GMT"}, {"version": "v7", "created": "Tue, 23 Jul 2019 12:31:56 GMT"}, {"version": "v8", "created": "Wed, 20 May 2020 12:10:35 GMT"}, {"version": "v9", "created": "Sun, 31 May 2020 09:02:19 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Murakami", "Daisuke", ""]]}, {"id": "1703.04866", "submitter": "Ajay Jasra", "authors": "Alexandros Beskos, Ajay Jasra, Kody Law, Youssef Marzouk, Yan Zhou", "title": "Multilevel Sequential Monte Carlo with Dimension-Independent\n  Likelihood-Informed Proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we develop a new sequential Monte Carlo (SMC) method for\nmultilevel (ML) Monte Carlo estimation. In particular, the method can be used\nto estimate expectations with respect to a target probability distribution over\nan infinite-dimensional and non-compact space as given, for example, by a\nBayesian inverse problem with Gaussian random field prior. Under suitable\nassumptions the MLSMC method has the optimal $O(\\epsilon^{-2})$ bound on the\ncost to obtain a mean-square error of $O(\\epsilon^2)$. The algorithm is\naccelerated by dimension-independent likelihood-informed (DILI) proposals\ndesigned for Gaussian priors, leveraging a novel variation which uses empirical\nsample covariance information in lieu of Hessian information, hence eliminating\nthe requirement for gradient evaluations. The efficiency of the algorithm is\nillustrated on two examples: inversion of noisy pressure measurements in a PDE\nmodel of Darcy flow to recover the posterior distribution of the permeability\nfield, and inversion of noisy measurements of the solution of an SDE to recover\nthe posterior path measure.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 01:18:57 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Beskos", "Alexandros", ""], ["Jasra", "Ajay", ""], ["Law", "Kody", ""], ["Marzouk", "Youssef", ""], ["Zhou", "Yan", ""]]}, {"id": "1703.05060", "submitter": "Dave Zachariah", "authors": "Dave Zachariah and Petre Stoica and Thomas B. Sch\\\"on", "title": "Online Learning for Distribution-Free Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an online learning method for prediction, which is important in\nproblems with large and/or streaming data sets. We formulate the learning\napproach using a covariance-fitting methodology, and show that the resulting\npredictor has desirable computational and distribution-free properties: It is\nimplemented online with a runtime that scales linearly in the number of\nsamples; has a constant memory requirement; avoids local minima problems; and\nprunes away redundant feature dimensions without relying on restrictive\nassumptions on the data distribution. In conjunction with the split conformal\napproach, it also produces distribution-free prediction confidence intervals in\na computationally efficient manner. The method is demonstrated on both real and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 10:20:32 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Zachariah", "Dave", ""], ["Stoica", "Petre", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1703.05144", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Nial Friel", "title": "Bergm: Bayesian exponential random graph models in R", "comments": "To appear in the ISBA Bulletin", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bergm package provides a comprehensive framework for Bayesian inference\nusing Markov chain Monte Carlo (MCMC) algorithms. It can also supply graphical\nBayesian goodness-of-fit procedures that address the issue of model adequacy.\nThe package is simple to use and represents an attractive way of analysing\nnetwork data as it offers the advantage of a complete probabilistic treatment\nof uncertainty. Bergm is based on the ergm package and therefore it makes use\nof the same model set-up and network simulation algorithms. The Bergm package\nhas been continually improved in terms of speed performance over the last years\nand now offers the end-user a feasible option for carrying out Bayesian\ninference for networks with several thousands of nodes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 13:31:23 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 15:00:32 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Caimo", "Alberto", ""], ["Friel", "Nial", ""]]}, {"id": "1703.05471", "submitter": "Patricio Maturana", "authors": "Patricio Maturana, Brendon J. Brewer, Steffen Klaere, Remco Bouckaert", "title": "Model selection and parameter inference in phylogenetics using Nested\n  Sampling", "comments": "23 pages, 12 figures, 3 tables", "journal-ref": null, "doi": "10.1093/sysbio/syy050", "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian inference methods rely on numerical algorithms for both model\nselection and parameter inference. In general, these algorithms require a high\ncomputational effort to yield reliable estimates. One of the major challenges\nin phylogenetics is the estimation of the marginal likelihood. This quantity is\ncommonly used for comparing different evolutionary models, but its calculation,\neven for simple models, incurs high computational cost. Another interesting\nchallenge relates to the estimation of the posterior distribution. Often, long\nMarkov chains are required to get sufficient samples to carry out parameter\ninference, especially for tree distributions. In general, these problems are\naddressed separately by using different procedures. Nested sampling (NS) is a\nBayesian computation algorithm which provides the means to estimate marginal\nlikelihoods together with their uncertainties, and to sample from the posterior\ndistribution at no extra cost. The methods currently used in phylogenetics for\nmarginal likelihood estimation lack in practicality due to their dependence on\nmany tuning parameters and the inability of most implementations to provide a\ndirect way to calculate the uncertainties associated with the estimates. To\naddress these issues, we introduce NS to phylogenetics. Its performance is\nassessed under different scenarios and compared to established methods. We\nconclude that NS is a competitive and attractive algorithm for phylogenetic\ninference. An implementation is available as a package for BEAST 2 under the\nLGPL licence, accessible at https://github.com/BEAST2-Dev/nested-sampling.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 05:00:37 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 05:30:12 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 05:22:48 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Maturana", "Patricio", ""], ["Brewer", "Brendon J.", ""], ["Klaere", "Steffen", ""], ["Bouckaert", "Remco", ""]]}, {"id": "1703.05511", "submitter": "David Price", "authors": "David J. Price, Nigel G. Bean, Joshua V. Ross, Jonathan Tuke", "title": "An Induced Natural Selection Heuristic for Finding Optimal Bayesian\n  Experimental Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimal experimental design has immense potential to inform the\ncollection of data so as to subsequently enhance our understanding of a variety\nof processes. However, a major impediment is the difficulty in evaluating\noptimal designs for problems with large, or high-dimensional, design spaces. We\npropose an efficient search heuristic suitable for general optimisation\nproblems, with a particular focus on optimal Bayesian experimental design\nproblems. The heuristic evaluates the objective (utility) function at an\ninitial, randomly generated set of input values. At each generation of the\nalgorithm, input values are \"accepted\" if their corresponding objective\n(utility) function satisfies some acceptance criteria, and new inputs are\nsampled about these accepted points. We demonstrate the new algorithm by\nevaluating the optimal Bayesian experimental designs for the previously\nconsidered death, pharmacokinetic and logistic regression models. Comparisons\nto the current \"gold-standard\" method are given to demonstrate the proposed\nalgorithm as a computationally-efficient alternative for moderately-large\ndesign problems (i.e., up to approximately 40-dimensions).\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 09:01:22 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 06:18:28 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Price", "David J.", ""], ["Bean", "Nigel G.", ""], ["Ross", "Joshua V.", ""], ["Tuke", "Jonathan", ""]]}, {"id": "1703.05984", "submitter": "Quentin Frederik Gronau", "authors": "Quentin F. Gronau, Alexandra Sarafoglou, Dora Matzke, Alexander Ly,\n  Udo Boehm, Maarten Marsman, David S. Leslie, Jonathan J. Forster, Eric-Jan\n  Wagenmakers, Helen Steingroever", "title": "A Tutorial on Bridge Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The marginal likelihood plays an important role in many areas of Bayesian\nstatistics such as parameter estimation, model comparison, and model averaging.\nIn most applications, however, the marginal likelihood is not analytically\ntractable and must be approximated using numerical methods. Here we provide a\ntutorial on bridge sampling (Bennett, 1976; Meng & Wong, 1996), a reliable and\nrelatively straightforward sampling method that allows researchers to obtain\nthe marginal likelihood for models of varying complexity. First, we introduce\nbridge sampling and three related sampling methods using the beta-binomial\nmodel as a running example. We then apply bridge sampling to estimate the\nmarginal likelihood for the Expectancy Valence (EV) model---a popular model for\nreinforcement learning. Our results indicate that bridge sampling provides\naccurate estimates for both a single participant and a hierarchical version of\nthe EV model. We conclude that bridge sampling is an attractive method for\nmathematical psychologists who typically aim to approximate the marginal\nlikelihood for a limited set of possibly high-dimensional models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 12:11:34 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 10:47:21 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Gronau", "Quentin F.", ""], ["Sarafoglou", "Alexandra", ""], ["Matzke", "Dora", ""], ["Ly", "Alexander", ""], ["Boehm", "Udo", ""], ["Marsman", "Maarten", ""], ["Leslie", "David S.", ""], ["Forster", "Jonathan J.", ""], ["Wagenmakers", "Eric-Jan", ""], ["Steingroever", "Helen", ""]]}, {"id": "1703.06098", "submitter": "Giacomo Zanella", "authors": "Giacomo Zanella, Gareth Roberts", "title": "Multilevel linear models, Gibbs samplers and multigrid decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence properties of the Gibbs Sampler in the context of\nposterior distributions arising from Bayesian analysis of conditionally\nGaussian hierarchical models. We develop a multigrid approach to derive\nanalytic expressions for the convergence rates of the algorithm for various\nwidely used model structures, including nested and crossed random effects. Our\nresults apply to multilevel models with an arbitrary number of layers in the\nhierarchy, while most previous work was limited to the two-level nested case.\nThe theoretical results provide explicit and easy-to-implement guidelines to\noptimize practical implementations of the Gibbs Sampler, such as indications on\nwhich parametrization to choose (e.g. centred and non-centred), which\nconstraint to impose to guarantee statistical identifiability, and which\nparameters to monitor in the diagnostic process. Simulations suggest that the\nresults are informative also in the context of non-Gaussian distributions and\nmore general MCMC schemes, such as gradient-based ones.implementation of Gibbs\nsamplers on conditionally Gaussian hierarchical models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:00:53 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 10:40:26 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Zanella", "Giacomo", ""], ["Roberts", "Gareth", ""]]}, {"id": "1703.06131", "submitter": "Alessio Spantini", "authors": "Alessio Spantini, Daniele Bigoni, Youssef Marzouk", "title": "Inference via low-dimensional couplings", "comments": "78 pages, 25 figures", "journal-ref": "Journal of Machine Learning Research, volume 19 (66): 1-71, 2018", "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the low-dimensional structure of deterministic transformations\nbetween random variables, i.e., transport maps between probability measures. In\nthe context of statistics and machine learning, these transformations can be\nused to couple a tractable \"reference\" measure (e.g., a standard Gaussian) with\na target measure of interest. Direct simulation from the desired measure can\nthen be achieved by pushing forward reference samples through the map. Yet\ncharacterizing such a map---e.g., representing and evaluating it---grows\nchallenging in high dimensions. The central contribution of this paper is to\nestablish a link between the Markov properties of the target measure and the\nexistence of low-dimensional couplings, induced by transport maps that are\nsparse and/or decomposable. Our analysis not only facilitates the construction\nof transformations in high-dimensional settings, but also suggests new\ninference methodologies for continuous non-Gaussian graphical models. For\ninstance, in the context of nonlinear state-space models, we describe new\nvariational algorithms for filtering, smoothing, and sequential parameter\ninference. These algorithms can be understood as the natural\ngeneralization---to the non-Gaussian case---of the square-root\nRauch-Tung-Striebel Gaussian smoother.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:50:44 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 13:07:43 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 15:41:07 GMT"}, {"version": "v4", "created": "Sun, 1 Jul 2018 23:28:16 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Spantini", "Alessio", ""], ["Bigoni", "Daniele", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1703.06206", "submitter": "Perry de Valpine", "authors": "Nicholas Michaud, Perry de Valpine, Daniel Turek, Christopher J.\n  Paciorek, Dao Nguyen", "title": "Sequential Monte Carlo Methods in the nimble R Package", "comments": "32 pages. 5 figures. To be published in Journal of Statistical\n  Software. The nimble R package is available on CRAN at\n  https://CRAN.R-project.org/package=nimble", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  nimble is an R package for constructing algorithms and conducting inference\non hierarchical models. The nimble package provides a unique combination of\nflexible model specification and the ability to program model-generic\nalgorithms. Specifically, the package allows users to code models in the BUGS\nlanguage, and it allows users to write algorithms that can be applied to any\nappropriate model. In this paper, we introduce nimble's capabilities for\nstate-space model analysis using sequential Monte Carlo (SMC) techniques. We\nfirst provide an overview of state-space models and commonly-used SMC\nalgorithms. We then describe how to build a state-space model and conduct\ninference using existing SMC algorithms within nimble. SMC algorithms within\nnimble currently include the bootstrap filter, auxiliary particle filter,\nensemble Kalman filter, IF2 method of iterated filtering, and a particle MCMC\nsampler. These algorithms can be run in R or compiled into C++ for more\nefficient execution. Examples of applying SMC algorithms to linear\nautoregressive models and a stochastic volatility model are provided. Finally,\nwe give an overview of how model-generic algorithms are coded within nimble by\nproviding code for a simple SMC algorithm. This illustrates how users can\neasily extend nimble's SMC methods in high-level code.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 21:49:28 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 17:57:37 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 16:55:26 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Michaud", "Nicholas", ""], ["de Valpine", "Perry", ""], ["Turek", "Daniel", ""], ["Paciorek", "Christopher J.", ""], ["Nguyen", "Dao", ""]]}, {"id": "1703.06359", "submitter": "Toni Karvonen", "authors": "Toni Karvonen and Simo S\\\"arkk\\\"a", "title": "Fully symmetric kernel quadrature", "comments": "Accepted for publication in SIAM Journal on Scientific Computing.\n  Minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel quadratures and other kernel-based approximation methods typically\nsuffer from prohibitive cubic time and quadratic space complexity in the number\nof function evaluations. The problem arises because a system of linear\nequations needs to be solved. In this article we show that the weights of a\nkernel quadrature rule can be computed efficiently and exactly for up to tens\nof millions of nodes if the kernel, integration domain, and measure are fully\nsymmetric and the node set is a union of fully symmetric sets. This is based on\nthe observations that in such a setting there are only as many distinct weights\nas there are fully symmetric sets and that these weights can be solved from a\nlinear system of equations constructed out of row sums of certain submatrices\nof the full kernel matrix. We present several numerical examples that show\nfeasibility, both for a large number of nodes and in high dimensions, of the\ndeveloped fully symmetric kernel quadrature rules. Most prominent of the fully\nsymmetric kernel quadrature rules we propose are those that use sparse grids.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 22:12:08 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 13:23:32 GMT"}, {"version": "v3", "created": "Sat, 6 Jan 2018 12:17:21 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Karvonen", "Toni", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1703.06419", "submitter": "Wenlin Dai", "authors": "Wenlin Dai and Marc G. Genton", "title": "Multivariate Functional Data Visualization and Outlier Detection", "comments": "30 pages, 10 figures, 1 table, Journal of Computational and Graphical\n  Statistics, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a new graphical tool, the magnitude-shape (MS) plot,\nfor visualizing both the magnitude and shape outlyingness of multivariate\nfunctional data. The proposed tool builds on the recent notion of functional\ndirectional outlyingness, which measures the centrality of functional data by\nsimultaneously considering the level and the direction of their deviation from\nthe central region. The MS-plot intuitively presents not only levels but also\ndirections of magnitude outlyingness on the horizontal axis or plane, and\ndemonstrates shape outlyingness on the vertical axis. A dividing curve or\nsurface is provided to separate non-outlying data from the outliers. Both the\nsimulated data and the practical examples confirm that the MS-plot is superior\nto existing tools for visualizing centrality and detecting outliers for\nfunctional data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 10:51:41 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 14:00:31 GMT"}, {"version": "v3", "created": "Sun, 22 Apr 2018 06:39:31 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Dai", "Wenlin", ""], ["Genton", "Marc G.", ""]]}, {"id": "1703.06826", "submitter": "Alicja Wolny-Dominiak Alicja Wolny-Dominiak", "authors": "Waldemar W. Koczkodaj, Alicja Wolny-Dominiak", "title": "RatingScaleReduction package: stepwise rating scale item reduction\n  without predictability loss", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents an innovative method for reducing the number of rating\nscale items without predictability loss. The \"area under the re- ceiver\noperator curve method\" (AUC ROC) is used to implement in the\nRatingScaleReduction package posted on CRAN. Several cases have been used to\nillustrate how the stepwise method has reduced the number of rating scale items\n(variables).\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 14:09:50 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Koczkodaj", "Waldemar W.", ""], ["Wolny-Dominiak", "Alicja", ""]]}, {"id": "1703.07039", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen", "title": "A Simple Online Parameter Estimation Technique with Asymptotic\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern settings, data are acquired iteratively over time, rather than\nall at once. Such settings are known as online, as opposed to offline or batch.\nWe introduce a simple technique for online parameter estimation, which can\noperate in low memory settings, settings where data are correlated, and only\nrequires a single inspection of the available data at each time period. We show\nthat the estimators---constructed via the technique---are asymptotically normal\nunder generous assumptions, and present a technique for the online computation\nof the covariance matrices for such estimators. A set of numerical studies\ndemonstrates that our estimators can be as efficient as their offline\ncounterparts, and that our technique generates estimates and confidence\nintervals that match their offline counterparts in various parameter estimation\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 03:27:22 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Nguyen", "Hien D", ""]]}, {"id": "1703.07285", "submitter": "Mathurin Massias", "authors": "Mathurin Massias and Alexandre Gramfort and Joseph Salmon", "title": "From safe screening rules to working sets for faster Lasso-type solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex sparsity-promoting regularizations are ubiquitous in modern\nstatistical learning. By construction, they yield solutions with few non-zero\ncoefficients, which correspond to saturated constraints in the dual\noptimization formulation. Working set (WS) strategies are generic optimization\ntechniques that consist in solving simpler problems that only consider a subset\nof constraints, whose indices form the WS. Working set methods therefore\ninvolve two nested iterations: the outer loop corresponds to the definition of\nthe WS and the inner loop calls a solver for the subproblems. For the Lasso\nestimator a WS is a set of features, while for a Group Lasso it refers to a set\nof groups. In practice, WS are generally small in this context so the\nassociated feature Gram matrix can fit in memory. Here we show that the\nGauss-Southwell rule (a greedy strategy for block coordinate descent\ntechniques) leads to fast solvers in this case. Combined with a working set\nstrategy based on an aggressive use of so-called Gap Safe screening rules, we\npropose a solver achieving state-of-the-art performance on sparse learning\nproblems. Results are presented on Lasso and multi-task Lasso estimators.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:42:38 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 10:30:25 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Massias", "Mathurin", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1703.08520", "submitter": "Kaspar M\\\"artens", "authors": "Kaspar M\\\"artens, Michalis K Titsias, Christopher Yau", "title": "Augmented Ensemble MCMC sampling in Factorial Hidden Markov Models", "comments": null, "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2019, Naha,Okinawa, Japan. PMLR: Volume\n  89", "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for factorial hidden Markov models is challenging due to\nthe exponentially sized latent variable space. Standard Monte Carlo samplers\ncan have difficulties effectively exploring the posterior landscape and are\noften restricted to exploration around localised regions that depend on\ninitialisation. We introduce a general purpose ensemble Markov Chain Monte\nCarlo (MCMC) technique to improve on existing poorly mixing samplers. This is\nachieved by combining parallel tempering and an auxiliary variable scheme to\nexchange information between the chains in an efficient way. The latter\nexploits a genetic algorithm within an augmented Gibbs sampler. We compare our\ntechnique with various existing samplers in a simulation study as well as in a\ncancer genomics application, demonstrating the improvements obtained by our\naugmented ensemble approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 17:17:45 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 20:46:32 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["M\u00e4rtens", "Kaspar", ""], ["Titsias", "Michalis K", ""], ["Yau", "Christopher", ""]]}, {"id": "1703.08627", "submitter": "Stephen DeSalvo", "authors": "Stephen DeSalvo", "title": "Random sampling of Latin squares via binary contingency tables and\n  probabilistic divide-and-conquer", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a novel approach for the random sampling of Latin squares of\norder~$n$ via probabilistic divide-and-conquer. The algorithm divides the\nentries of the table modulo powers of $2$, and samples a corresponding binary\ncontingency table at each level. The sampling distribution is based on the\nBoltzmann sampling heuristic, along with probabilistic divide-and-conquer.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 23:49:33 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["DeSalvo", "Stephen", ""]]}, {"id": "1703.08676", "submitter": "Manuel Rizzo", "authors": "Manuel Rizzo (1), Francesco Battaglia (1) ((1) Department of\n  Statistical Sciences, Sapienza University of Rome, Italy)", "title": "Statistical and Computational Tradeoff in Genetic Algorithm-Based\n  Estimation", "comments": "17 pages, 5 figures", "journal-ref": "Journal of Statistical Computation and Simulation (2018), 88(16),\n  3081-3097", "doi": "10.1080/00949655.2018.1499740", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a Genetic Algorithm (GA), or a stochastic algorithm in general, is\nemployed in a statistical problem, the obtained result is affected by both\nvariability due to sampling, that refers to the fact that only a sample is\nobserved, and variability due to the stochastic elements of the algorithm. This\ntopic can be easily set in a framework of statistical and computational\ntradeoff question, crucial in recent problems, for which statisticians must\ncarefully set statistical and computational part of the analysis, taking\naccount of some resource or time constraints. In the present work we analyze\nestimation problems tackled by GAs, for which variability of estimates can be\ndecomposed in the two sources of variability, considering some constraints in\nthe form of cost functions, related to both data acquisition and runtime of the\nalgorithm. Simulation studies will be presented to discuss the statistical and\ncomputational tradeoff question.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 11:20:21 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Rizzo", "Manuel", ""], ["Battaglia", "Francesco", ""]]}, {"id": "1703.08723", "submitter": "Paul McNicholas", "authors": "Yuhong Wei, Yang Tang, Emilie Shireman, Paul D. McNicholas and Douglas\n  L. Steinley", "title": "Extending Growth Mixture Models Using Continuous Non-Elliptical\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growth mixture models (GMMs) incorporate both conventional random effects\ngrowth modeling and latent trajectory classes as in finite mixture modeling;\ntherefore, they offer a way to handle the unobserved heterogeneity between\nsubjects in their development. GMMs with Gaussian random effects dominate the\nliterature. When the data are asymmetric and/or have heavier tails, more than\none latent class is required to capture the observed variable distribution.\nTherefore, a GMM with continuous non-elliptical distributions is proposed to\ncapture skewness and heavier tails in the data set. Specifically, multivariate\nskew-t distributions and generalized hyperbolic distributions are introduced to\nextend GMMs. When extending GMMs, four statistical models are considered with\ndiffering distributions of measurement errors and random effects. The\nmathematical development of GMMs with non-elliptical distributions relies on\ntheir expression as normal variance-mean mixtures and the resultant\nrelationship with the generalized inverse Gaussian distribution. Parameter\nestimation is outlined within the expectation-maximization framework before the\nperformance of our GMMs with non-elliptical distributions is illustrated on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 17:57:31 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 00:14:28 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wei", "Yuhong", ""], ["Tang", "Yang", ""], ["Shireman", "Emilie", ""], ["McNicholas", "Paul D.", ""], ["Steinley", "Douglas L.", ""]]}, {"id": "1703.08882", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Finite Mixtures of Skewed Matrix Variate Distributions", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.143", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is the process of finding underlying group structures in data.\nAlthough mixture model-based clustering is firmly established in the\nmultivariate case, there is a relative paucity of work on matrix variate\ndistributions and none for clustering with mixtures of skewed matrix variate\ndistributions. Four finite mixtures of skewed matrix variate distributions are\nconsidered. Parameter estimation is carried out using an\nexpectation-conditional maximization algorithm, and both simulated and real\ndata are used for illustration.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 22:49:31 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 19:44:17 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 18:28:33 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1703.09062", "submitter": "Stella Hadjiantoni", "authors": "Stella Hadjiantoni and Erricos J. Kontoghiorghes", "title": "A numerical method for the estimation of time-varying parameter models\n  in large dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel numerical method for the estimation of large time-varying parameter\n(TVP) models is proposed. The updating and smoothing estimates of the TVP model\nare derived within the context of generalised linear least squares and through\nnumerically stable orthogonal transformations. The method developed is based on\ncomputationally efficient strategies. The computational cost is reduced by\nexploiting the special sparse structure of the TVP model and by utilising\nprevious computations. The proposed method is also extended to the rolling\nwindow estimation of the TVP model. Experimental results show the effectiveness\nof the new updating, window and smoothing strategies in high dimensions when a\nlarge number of covariates and regressions are included in the TVP model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 13:33:39 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 18:09:20 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Hadjiantoni", "Stella", ""], ["Kontoghiorghes", "Erricos J.", ""]]}, {"id": "1703.09074", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson and Krithika Manohar and Steven L. Brunton and J.\n  Nathan Kutz", "title": "Randomized CP Tensor Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular\ndimensionality-reduction method for multiway data. Dimensionality reduction is\noften sought after since many high-dimensional tensors have low intrinsic rank\nrelative to the dimension of the ambient measurement space. However, the\nemergence of `big data' poses significant computational challenges for\ncomputing this fundamental tensor decomposition. By leveraging modern\nrandomized algorithms, we demonstrate that coherent structures can be learned\nfrom a smaller representation of the tensor in a fraction of the time. Thus,\nthis simple but powerful algorithm enables one to compute the approximate CP\ndecomposition even for massive tensors. The approximation error can thereby be\ncontrolled via oversampling and the computation of power iterations. In\naddition to theoretical results, several empirical results demonstrate the\nperformance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 13:41:00 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 06:27:21 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Manohar", "Krithika", ""], ["Brunton", "Steven L.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1703.09163", "submitter": "Kshitij Khare", "authors": "Bala Rajaratnam, Doug Sparks, Kshitij Khare and Liyuan Zhang", "title": "Scalable Bayesian shrinkage and uncertainty quantification in\n  high-dimensional regression", "comments": "This paper is already available at arXiv with identifier\n  arXiv:1509.03697, and was submitted again due to some confusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian shrinkage methods have generated a lot of recent interest as tools\nfor high-dimensional regression and model selection. These methods naturally\nfacilitate tractable uncertainty quantification and incorporation of prior\ninformation. A common feature of these models, including the Bayesian lasso,\nglobal-local shrinkage priors, and spike-and-slab priors is that the\ncorresponding priors on the regression coefficients can be expressed as scale\nmixture of normals. While the three-step Gibbs sampler used to sample from the\noften intractable associated posterior density has been shown to be\ngeometrically ergodic for several of these models (Khare and Hobert, 2013; Pal\nand Khare, 2014), it has been demonstrated recently that convergence of this\nsampler can still be quite slow in modern high-dimensional settings despite\nthis apparent theoretical safeguard. We propose a new method to draw from the\nsame posterior via a tractable two-step blocked Gibbs sampler. We demonstrate\nthat our proposed two-step blocked sampler exhibits vastly superior convergence\nbehavior compared to the original three- step sampler in high-dimensional\nregimes on both real and simulated data. We also provide a detailed theoretical\nunderpinning to the new method in the context of the Bayesian lasso. First, we\nderive explicit upper bounds for the (geometric) rate of convergence.\nFurthermore, we demonstrate theoretically that while the original Bayesian\nlasso chain is not Hilbert-Schmidt, the proposed chain is trace class (and\nhence Hilbert-Schmidt). The trace class property has useful theoretical and\npractical implications. It implies that the corresponding Markov operator is\ncompact, and its eigenvalues are summable. It also facilitates a rigorous\ncomparison of the two-step blocked chain with \"sandwich\" algorithms which aim\nto improve performance of the two-step chain by inserting an inexpensive extra\nstep.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 16:14:18 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 14:38:01 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Rajaratnam", "Bala", ""], ["Sparks", "Doug", ""], ["Khare", "Kshitij", ""], ["Zhang", "Liyuan", ""]]}, {"id": "1703.09301", "submitter": "Michael Schweinberger", "authors": "Sergii Babkin, Jonathan Stewart, Xiaochen Long, Michael Schweinberger", "title": "Large-scale estimation of random graph models with local dependence", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2020.107029", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of random graph models is considered, combining features of\nexponential-family models and latent structure models, with the goal of\nretaining the strengths of both of them while reducing the weaknesses of each\nof them. An open problem is how to estimate such models from large networks. A\nnovel approach to large-scale estimation is proposed, taking advantage of the\nlocal structure of such models for the purpose of local computing. The main\nidea is that random graphs with local dependence can be decomposed into\nsubgraphs, which enables parallel computing on subgraphs and suggests a\ntwo-step estimation approach. The first step estimates the local structure\nunderlying random graphs. The second step estimates parameters given the\nestimated local structure of random graphs. Both steps can be implemented in\nparallel, which enables large-scale estimation. The advantages of the two-step\nestimation approach are demonstrated by simulation studies with up to 10,000\nnodes and an application to a large Amazon product recommendation network with\nmore than 10,000 products.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 20:37:04 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 00:54:54 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 19:59:29 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Babkin", "Sergii", ""], ["Stewart", "Jonathan", ""], ["Long", "Xiaochen", ""], ["Schweinberger", "Michael", ""]]}, {"id": "1703.09382", "submitter": "Le-Yu Chen", "authors": "Le-Yu Chen and Sokbae Lee", "title": "Exact computation of GMM estimators for instrumental variable quantile\n  regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the generalized method of moments (GMM) estimation problem in\ninstrumental variable quantile regression (IVQR) models can be equivalently\nformulated as a mixed integer quadratic programming problem. This enables exact\ncomputation of the GMM estimators for the IVQR models. We illustrate the\nusefulness of our algorithm via Monte Carlo experiments and an application to\ndemand for fish.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 03:08:35 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Chen", "Le-Yu", ""], ["Lee", "Sokbae", ""]]}, {"id": "1703.09570", "submitter": "Taylor Arnold", "authors": "Taylor Arnold", "title": "A Tidy Data Model for Natural Language Processing using cleanNLP", "comments": "20 pages; 4 figures", "journal-ref": "The R Journal, 9.2, 248-267 (2017)", "doi": null, "report-no": null, "categories": "cs.CL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The package cleanNLP provides a set of fast tools for converting a textual\ncorpus into a set of normalized tables. The underlying natural language\nprocessing pipeline utilizes Stanford's CoreNLP library, exposing a number of\nannotation tasks for text written in English, French, German, and Spanish.\nAnnotators include tokenization, part of speech tagging, named entity\nrecognition, entity linking, sentiment analysis, dependency parsing,\ncoreference resolution, and information extraction.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 02:18:36 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 16:45:10 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Arnold", "Taylor", ""]]}, {"id": "1703.09658", "submitter": "Amirhossein Sobhani", "authors": "Rahman Farnoosh, Amirhossein Sobhani, Hamidreza Rezazadeh", "title": "An orthogonal basis expansion method for solving path-independent\n  stochastic differential equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present an orthogonal basis expansion method for solving\nstochastic differential equations with a path-independent solution of the form\n$X_{t}=\\phi(t,W_{t})$. For this purpose, we define a Hilbert space and\nconstruct an orthogonal basis for this inner product space with the aid of\n2D-Hermite polynomials. With considering $X_{t}$ as orthogonal basis expansion,\nthis method is implemented and the expansion coefficients are obtained by\nsolving a system of nonlinear integro-differential equations. The strength of\nsuch a method is that expectation and variance of the solution is computed by\nthese coefficients directly. Eventually, numerical results demonstrate its\nvalidity and efficiency in comparison with other numerical methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 16:24:42 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 23:41:47 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 04:16:09 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Farnoosh", "Rahman", ""], ["Sobhani", "Amirhossein", ""], ["Rezazadeh", "Hamidreza", ""]]}, {"id": "1703.09930", "submitter": "JInglai Li", "authors": "Hongqiao Wang, Jinglai Li", "title": "Adaptive Gaussian process approximation for Bayesian inference with\n  expensive likelihood functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference problems with computationally intensive\nlikelihood functions. We propose a Gaussian process (GP) based method to\napproximate the joint distribution of the unknown parameters and the data. In\nparticular, we write the joint density approximately as a product of an\napproximate posterior density and an exponentiated GP surrogate. We then\nprovide an adaptive algorithm to construct such an approximation, where an\nactive learning method is used to choose the design points. With numerical\nexamples, we illustrate that the proposed method has competitive performance\nagainst existing approaches for Bayesian computation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 08:32:21 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 06:09:34 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 02:09:04 GMT"}, {"version": "v4", "created": "Wed, 14 Mar 2018 09:16:53 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wang", "Hongqiao", ""], ["Li", "Jinglai", ""]]}, {"id": "1703.10245", "submitter": "Helga Wagner Dr.", "authors": "Daniela Pauger and Helga Wagner", "title": "Bayesian Effect Fusion for Categorical Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Bayesian approach to obtain a sparse\nrepresentation of the effect of a categorical predictor in regression type\nmodels. As the effect of a categorical predictor is captured by a group of\nlevel effects, sparsity cannot only be achieved by excluding single irrelevant\nlevel effects but also by excluding the whole group of effects associated to a\npredictor or by fusing levels which have essentially the same effect on the\nresponse. To achieve this goal, we propose a prior which allows for almost\nperfect as well as almost zero dependence between level effects a priori. We\nshow how this prior can be obtained by specifying spike and slab prior\ndistributions on all effect differences associated to one categorical predictor\nand how restricted fusion can be implemented. An efficient MCMC method for\nposterior computation is developed. The performance of the proposed method is\ninvestigated on simulated data. Finally, we illustrate its application on real\ndata from EU-SILC.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 20:56:31 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 04:49:16 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Pauger", "Daniela", ""], ["Wagner", "Helga", ""]]}, {"id": "1703.10364", "submitter": "Daniel W. Heck", "authors": "Daniel W. Heck, Antony M. Overstall, Quentin F. Gronau, Eric-Jan\n  Wagenmakers", "title": "Quantifying Uncertainty in Transdimensional Markov Chain Monte Carlo\n  Using Discrete Markov Models", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-018-9828-0", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian analysis often concerns an evaluation of models with different\ndimensionality as is necessary in, for example, model selection or mixture\nmodels. To facilitate this evaluation, transdimensional Markov chain Monte\nCarlo (MCMC) relies on sampling a discrete indexing variable to estimate the\nposterior model probabilities. However, little attention has been paid to the\nprecision of these estimates. If only few switches occur between the models in\nthe transdimensional MCMC output, precision may be low and assessment based on\nthe assumption of independent samples misleading. Here, we propose a new method\nto estimate the precision based on the observed transition matrix of the\nmodel-indexing variable. Assuming a first order Markov model, the method\nsamples from the posterior of the stationary distribution. This allows\nassessment of the uncertainty in the estimated posterior model probabilities,\nmodel ranks, and Bayes factors. Moreover, the method provides an estimate for\nthe effective sample size of the MCMC output. In two model-selection examples,\nwe show that the proposed approach provides a good assessment of the\nuncertainty associated with the estimated posterior model probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 08:54:34 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 10:37:50 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 14:00:02 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Heck", "Daniel W.", ""], ["Overstall", "Antony M.", ""], ["Gronau", "Quentin F.", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1703.10423", "submitter": "Olli-Pekka Koistinen", "authors": "Olli-Pekka Koistinen, Emile Maras, Aki Vehtari, Hannes J\\'onsson", "title": "Minimum energy path calculations with Gaussian process regression", "comments": null, "journal-ref": "Nanosystems: Physics, Chemisty, Mathematics, 2016, 7 (6), p.\n  925-935", "doi": "10.17586/2220-8054-2016-7-6-925-935", "report-no": null, "categories": "physics.chem-ph physics.atm-clus physics.comp-ph stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The calculation of minimum energy paths for transitions such as atomic and/or\nspin re-arrangements is an important task in many contexts and can often be\nused to determine the mechanism and rate of transitions. An important challenge\nis to reduce the computational effort in such calculations, especially when ab\ninitio or electron density functional calculations are used to evaluate the\nenergy since they can require large computational effort. Gaussian process\nregression is used here to reduce significantly the number of energy\nevaluations needed to find minimum energy paths of atomic rearrangements. By\nusing results of previous calculations to construct an approximate energy\nsurface and then converge to the minimum energy path on that surface in each\nGaussian process iteration, the number of energy evaluations is reduced\nsignificantly as compared with regular nudged elastic band calculations. For a\ntest problem involving rearrangements of a heptamer island on a crystal\nsurface, the number of energy evaluations is reduced to less than a fifth. The\nscaling of the computational effort with the number of degrees of freedom as\nwell as various possible further improvements to this approach are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 11:49:27 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Koistinen", "Olli-Pekka", ""], ["Maras", "Emile", ""], ["Vehtari", "Aki", ""], ["J\u00f3nsson", "Hannes", ""]]}]