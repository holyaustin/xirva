[{"id": "1209.0021", "submitter": "Mike Tsionas", "authors": "Efthymios G. Tsionas", "title": "Simple techniques for likelihood analysis of univariate and multivariate\n  stable distributions: with extensions to multivariate stochastic volatility\n  and dynamic factor models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a variety of procedures for numerical statistical\ninference in the family of univariate and multivariate stable distributions. In\nconnection with univariate distributions (i) we provide approximations by\nfinite location-scale mixtures and (ii) versions of approximate Bayesian\ncomputation (ABC) using the characteristic function and the asymptotic form of\nthe likelihood function. In the context of multivariate stable distributions we\npropose several ways to perform statistical inference and obtain the spectral\nmeasure associated with the distributions, a quantity that has been a major\nimpediment in using them in applied work. We extend the techniques to handle\nunivariate and multivariate stochastic volatility models, static and dynamic\nfactor models with disturbances and factors from general stable distributions,\na novel way to model multivariate stochastic volatility through time-varying\nspectral measures and a novel way to multivariate stable distributions through\ncopulae. The new techniques are applied to artificial as well as real data (ten\nmajor currencies, SP100 and individual returns). In connection with ABC special\nattention is paid to crafting well-performing proposal distributions for MCMC\nand extensive numerical experiments are conducted to provide critical values of\nthe \"closeness\" parameter that can be useful for further applied econometric\nwork.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 21:32:30 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Tsionas", "Efthymios G.", ""]]}, {"id": "1209.0185", "submitter": "Adam Persing", "authors": "Adam Persing and Ajay Jasra", "title": "Marginal Likelihood Computation for Hidden Markov Models via Generalized\n  Two-Filter Smoothing", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we introduce an estimate for the marginal likelihood associated\nto hidden Markov models (HMMs) using sequential Monte Carlo (SMC)\napproximations of the generalized two-filter smoothing decomposition (Briers,\n2010). This estimate is shown to be unbiased and a central limit theorem (CLT)\nis established. This latter CLT also allows one to prove a CLT associated to\nestimates of expectations w.r.t. a marginal of the joint smoothing\ndistribution; these form some of the first theoretical results associated to\nthe SMC approximation of the generalized two-filter smoothing decomposition.\nThe new estimate and its application is investigated from a numerical\nperspective.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2012 15:52:09 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Persing", "Adam", ""], ["Jasra", "Ajay", ""]]}, {"id": "1209.0542", "submitter": "Piet Groeneboom", "authors": "Piet Groeneboom", "title": "The bivariate current status model", "comments": "18 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the univariate current status and, more generally, the interval censoring\nmodel, distribution theory has been developed for the maximum likelihood\nestimator (MLE) and smoothed maximum likelihood estimator (SMLE) of the unknown\ndistribution function, see, e.g., [12], [7], [4], [5], [6], [10], [11] and [8].\nFor the bivariate current status and interval censoring models distribution\ntheory of this type is still absent and even the rate at which we can expect\nreasonable estimators to converge is unknown. We define a purely discrete\nplug-in estimator of the distribution function which locally converges at rate\nn^{1/3} and derive its (normal) limit distribution. Unlike the MLE or SMLE,\nthis estimator is not a proper distribution function. Since the estimator is\npurely discrete, it demonstrates that the n^{1/3} convergence rate is in\nprinciple possible for the MLE, but whether this actually holds for the MLE is\nstill an open problem. If the cube root n rate holds for the MLE, this would\nmean that the local 1-dimensional rate of the MLE continues to hold in\ndimension 2, a (perhaps) somewhat surprising result. The simulation results do\nnot seem to be in contradiction with this assumption, however. We compare the\nbehavior of the plug-in estimator with the behavior of the MLE on a sieve and\nthe SMLE in a simulation study. This indicates that the plug-in estimator and\nthe SMLE have a smaller variance but a larger bias than the sieved MLE. The\nSMLE is conjectured to have a n^{1/3}-rate of convergence if we use bandwidths\nof order n^{-1/6}. We derive its (normal) limit distribution, using this\nassumption. Finally, we demonstrate the behavior of the MLE and SMLE for the\nbivariate interval censored data of [1], which have been discussed by many\nauthors, see e.g., [18], [3], [2] and [15].\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 07:11:30 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2012 14:19:50 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2012 09:22:19 GMT"}, {"version": "v4", "created": "Sat, 15 Jun 2013 09:38:15 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Groeneboom", "Piet", ""]]}, {"id": "1209.0876", "submitter": "Antonio Forcina", "authors": "Antonio Forcina and Salvatore Modica", "title": "Fitting directed acyclic graphs with latent nodes as finite mixtures\n  models, with application to education transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an efficient EM algorithm for maximum likelihood\nestimation of a system of nonlinear structural equations corresponding to a\ndirected acyclic graph model that can contain an arbitrary number of latent\nvariables. The endogenous variables in the model must be categorical, while the\nexogenous variables may be arbitrary. The models discussed in this paper are an\nextended version of finite mixture models suitable for causal inference. An\napplication to the problem of education transmission is presented as an\nillustration.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 07:10:45 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2012 09:17:13 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2015 09:07:27 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2015 10:20:13 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Forcina", "Antonio", ""], ["Modica", "Salvatore", ""]]}, {"id": "1209.1740", "submitter": "Kinjal Basu", "authors": "Kinjal Basu and Debapriya Sengupta", "title": "Spline Smoothing for Estimation of Circular Probability Distributions\n  via Spectral Isomorphism and its Spatial Adaptation", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem when $X_1,X_2,..., X_n$ are distributed on a circle\nfollowing an unknown distribution $F$ on $S^1$. In this article we have\nconsider the absolute general set-up where the density can have local features\nsuch as discontinuities and edges. Furthermore, there can be outlying data\nwhich can follow some discrete distributions. The traditional Kernel Density\nEstimation methods fail to identify such local features in the data. Here we\ndevice a non-parametric density estimate on $S^1$, by the use of a novel\ntechnique which we term as Fourier Spline. We have also tried to identify and\nincorporate local features such as support, discontinuity or edges in the final\ndensity estimate. Several new results are proved in this regard. Simulation\nstudies have also been performed to see how our methodology works. Finally a\nreal life example is also shown.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 18:39:48 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Basu", "Kinjal", ""], ["Sengupta", "Debapriya", ""]]}, {"id": "1209.1988", "submitter": "Paul Marriott", "authors": "Karim Anaya-Izquierdo and Frank Critchley and Paul Marriott and Paul\n  W. Vos", "title": "Computational information geometry: theory and practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper lays the foundations for a unified framework for numerically and\ncomputationally applying methods drawn from a range of currently distinct\ngeometrical approaches to statistical modelling. In so doing, it extends\ninformation geometry from a manifold based approach to one where the simplex is\nthe fundamental geometrical object, thereby allowing applications to models\nwhich do not have a fixed dimension or support. Finally, it starts to build a\ncomputational framework which will act as a proxy for the 'space of all\ndistributions' that can be used, in particular, to investigate model selection\nand model uncertainty. A varied set of substantive running examples is used to\nillustrate theoretical and practical aspects of the discussion. Further\ndevelopments are briefly indicated.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 13:47:12 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Anaya-Izquierdo", "Karim", ""], ["Critchley", "Frank", ""], ["Marriott", "Paul", ""], ["Vos", "Paul W.", ""]]}, {"id": "1209.1994", "submitter": "Heng Peng", "authors": "Heng Peng", "title": "Nonconcave Penalized Spline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Regression spline is a useful tool in nonparametric regression. However,\nfinding the optimal knot locations is a known difficult problem. In this\narticle, we introduce the Non-concave Penalized Regression Spline. This\nproposal method not only produces smoothing spline with optimal convergence\nrate, but also can adaptively select optimal knots simultaneously. It is\ninsensitive to the number of origin knots. The method's performance in a\nsimulation has been studied to compare the other methods. The problem of how to\nchoose smoothing parameters, i.e. penalty parameters in the non-concave\nregression spline is addressed.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 13:57:26 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Peng", "Heng", ""]]}, {"id": "1209.2013", "submitter": "Daniel Simpson", "authors": "Yu Ryan Yue, Daniel Simpson, Finn Lindgren and H{\\aa}vard Rue", "title": "Bayesian Adaptive Smoothing Spline using Stochastic Differential\n  Equations", "comments": "26 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": "NTNU Statistics Technical Report number 8/2012", "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smoothing spline is one of the most popular curve-fitting methods, partly\nbecause of empirical evidence supporting its effectiveness and partly because\nof its elegant mathematical formulation. However, there are two obstacles that\nrestrict the use of smoothing spline in practical statistical work. Firstly, it\nbecomes computationally prohibitive for large data sets because the number of\nbasis functions roughly equals the sample size. Secondly, its global smoothing\nparameter can only provide constant amount of smoothing, which often results in\npoor performances when estimating inhomogeneous functions. In this work, we\nintroduce a class of adaptive smoothing spline models that is derived by\nsolving certain stochastic differential equations with finite element methods.\nThe solution extends the smoothing parameter to a continuous data-driven\nfunction, which is able to capture the change of the smoothness of underlying\nprocess. The new model is Markovian, which makes Bayesian computation fast. A\nsimulation study and real data example are presented to demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 14:46:56 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Yue", "Yu Ryan", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1209.2160", "submitter": "Patrick Breheny", "authors": "Patrick Breheny, Jian Huang", "title": "Group descent algorithms for nonconvex penalized linear and logistic\n  regression models with grouped predictors", "comments": null, "journal-ref": "Statistics and Computing, 25: 173-187 (2015)", "doi": "10.1007/s11222-013-9424-2", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression is an attractive framework for variable selection\nproblems. Often, variables possess a grouping structure, and the relevant\nselection problem is that of selecting groups, not individual variables. The\ngroup lasso has been proposed as a way of extending the ideas of the lasso to\nthe problem of group selection. Nonconvex penalties such as SCAD and MCP have\nbeen proposed and shown to have several advantages over the lasso; these\npenalties may also be extended to the group selection problem, giving rise to\ngroup SCAD and group MCP methods. Here, we describe algorithms for fitting\nthese models stably and efficiently. In addition, we present simulation results\nand real data examples comparing and contrasting the statistical properties of\nthese methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 21:13:29 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 15:34:02 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Breheny", "Patrick", ""], ["Huang", "Jian", ""]]}, {"id": "1209.2978", "submitter": "Robin Evans", "authors": "Robin J. Evans", "title": "Graphical methods for inequality constraints in marginalized DAGs", "comments": "A final version will appear in the proceedings of the 22nd Workshop\n  on Machine Learning and Signal Processing, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a graphical approach to deriving inequality constraints for\ndirected acyclic graph (DAG) models, where some variables are unobserved. In\nparticular we show that the observed distribution of a discrete model is always\nrestricted if any two observed variables are neither adjacent in the graph, nor\nshare a latent parent; this generalizes the well known instrumental inequality.\nThe method also provides inequalities on interventional distributions, which\ncan be used to bound causal effects. All these constraints are characterized in\nterms of a new graphical separation criterion, providing an easy and intuitive\nmethod for their derivation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 18:05:26 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Evans", "Robin J.", ""]]}, {"id": "1209.3198", "submitter": "Merrilee Hurn Dr", "authors": "Nial Friel, Merrilee Hurn, Jason Wyse", "title": "Improving power posterior estimation of statistical evidence", "comments": "Revised version (to appear in Statistics and Computing). This version\n  corrects the typo in Equation (17), with thanks to Sabine Hug for pointing\n  this out", "journal-ref": null, "doi": "10.1007/s11222-013-9397-1", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical evidence (or marginal likelihood) is a key quantity in\nBayesian statistics, allowing one to assess the probability of the data given\nthe model under investigation. This paper focuses on refining the power\nposterior approach to improve estimation of the evidence. The power posterior\nmethod involves transitioning from the prior to the posterior by powering the\nlikelihood by an inverse temperature. In common with other tempering\nalgorithms, the power posterior involves some degree of tuning. The main\ncontributions of this article are twofold -- we present a result from the\nnumerical analysis literature which can reduce the bias in the estimate of the\nevidence by addressing the error arising from numerically integrating across\nthe inverse temperatures. We also tackle the selection of the inverse\ntemperature ladder, applying this approach additionally to the Stepping Stone\nsampler estimation of evidence.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 14:00:51 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2013 16:24:43 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2013 16:36:21 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Friel", "Nial", ""], ["Hurn", "Merrilee", ""], ["Wyse", "Jason", ""]]}, {"id": "1209.3766", "submitter": "Nikolai Gagunashvili", "authors": "N. D. Gagunashvili and M. Schmelling", "title": "Kernel based unfolding of data obtained from detectors with finite\n  resolution and limited acceptance", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex nucl-ex stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A kernel based procedure for correcting experimental data for distortions due\nto the finite resolution and limited detector acceptance is presented. The\nunfolding problem is known to be an ill-posed problem that can not be solved\nwithout some a priori information about solution such as, for example,\nsmoothness or positivity. In the approach presented here the true distribution\nis estimated by a weighted sum of kernels, with the width of the kernels acting\nas a regularization parameter responsible for the smoothness of the result.\nCross-validation is used to determine an optimal value for this parameter. A\nnumerical example with a simulation study of systematical and statistical\nerrors is presented to illustrate the procedure.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 14:42:12 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Gagunashvili", "N. D.", ""], ["Schmelling", "M.", ""]]}, {"id": "1209.4129", "submitter": "John Duchi", "authors": "Yuchen Zhang and John C. Duchi and Martin Wainwright", "title": "Comunication-Efficient Algorithms for Statistical Optimization", "comments": "44 pages, to appear in Journal of Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze two communication-efficient algorithms for distributed statistical\noptimization on large-scale data sets. The first algorithm is a standard\naveraging method that distributes the $N$ data samples evenly to $\\nummac$\nmachines, performs separate minimization on each subset, and then averages the\nestimates. We provide a sharp analysis of this average mixture algorithm,\nshowing that under a reasonable set of conditions, the combined parameter\nachieves mean-squared error that decays as $\\order(N^{-1}+(N/m)^{-2})$.\nWhenever $m \\le \\sqrt{N}$, this guarantee matches the best possible rate\nachievable by a centralized algorithm having access to all $\\totalnumobs$\nsamples. The second algorithm is a novel method, based on an appropriate form\nof bootstrap subsampling. Requiring only a single round of communication, it\nhas mean-squared error that decays as $\\order(N^{-1} + (N/m)^{-3})$, and so is\nmore robust to the amount of parallelization. In addition, we show that a\nstochastic gradient-based method attains mean-squared error decaying as\n$O(N^{-1} + (N/ m)^{-3/2})$, easing computation at the expense of penalties in\nthe rate of convergence. We also provide experimental evaluation of our\nmethods, investigating their performance both on simulated data and on a\nlarge-scale regression problem from the internet search domain. In particular,\nwe show that our methods can be used to efficiently solve an advertisement\nprediction problem from the Chinese SoSo Search Engine, which involves logistic\nregression with $N \\approx 2.4 \\times 10^8$ samples and $d \\approx 740,000$\ncovariates.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 01:27:40 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 03:12:01 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 19:23:38 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Zhang", "Yuchen", ""], ["Duchi", "John C.", ""], ["Wainwright", "Martin", ""]]}, {"id": "1209.4678", "submitter": "Taras Lazariv", "authors": "Taras Lazariv, Wolfgang Schmid, and Svitlana Zabolotska", "title": "On Control Charts for Monitoring the Variance of a Time Series", "comments": "30 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive control charts for the variance of a Gaussian process\nusing the likelihood ratio approach, the generalized likelihood ratio approach,\nthe sequential probability ratio method and a generalized sequential\nprobability ratio procedure, the Shiryaev-Roberts procedure and a generalized\nShiryaev-Roberts ap- proach. Recursive presentations for the calculation of the\ncontrol statistics are given for autoregressive processes of order 1. In an\nextensive simulation study these schemes are compared with existing control\ncharts for the variance. In order to asses the performance of the schemes both\nthe average run length and the average delay are used.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 22:34:57 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Lazariv", "Taras", ""], ["Schmid", "Wolfgang", ""], ["Zabolotska", "Svitlana", ""]]}, {"id": "1209.5359", "submitter": "Luai Al Labadi", "authors": "Luai Al Labadi and Mahmoud Zarepour", "title": "On Simulations from the Two-Parameter Poisson-Dirichlet Process and the\n  Normalized Inverse-Gaussian Process", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop simple, yet efficient, procedures for sampling\napproximations of the two-Parameter Poisson-Dirichlet Process and the\nnormalized inverse-Gaussian process. We compare the efficiency of the new\napproximations to the corresponding stick-breaking approximations, in which we\ndemonstrate a substantial improvement.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 18:35:58 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Labadi", "Luai Al", ""], ["Zarepour", "Mahmoud", ""]]}, {"id": "1209.6241", "submitter": "Krista Gile", "authors": "Mark S. Handcock and Krista J. Gile and Corinne M. Mar", "title": "Estimating Hidden Population Size using Respondent-Driven Sampling Data", "comments": "36 pages, 7 figures, including color figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-Driven Sampling (RDS) is an approach to sampling design and\ninference in hard-to-reach human populations. Typically, a sampling frame is\nnot available, and population members are difficult to identify or recruit from\nbroader sampling frames. Common examples include injecting drug users, men who\nhave sex with men, and female sex workers. Most analysis of RDS data has\nfocused on estimating aggregate characteristics, such as disease prevalence.\nHowever, RDS is often conducted in settings where the population size is\nunknown and of great independent interest. This paper presents an approach to\nestimating the size of a target population based on data collected through RDS.\n  The proposed approach uses a successive sampling approximation to RDS to\nleverage information in the ordered sequence of observed personal network\nsizes. The inference uses the Bayesian framework, allowing for the\nincorporation of prior knowledge. A flexible class of priors for the population\nsize is proposed that aids elicitation. An extensive simulation study provides\ninsight into the performance of the method for estimating population size under\na broad range of conditions. A further study shows the approach also improves\nestimation of aggregate characteristics. A particular choice of the prior\nproduces interval estimates with good frequentist properties. Finally, the\nmethod demonstrates sensible results when used to estimate the numbers of\nsub-populations most at risk for HIV in two cities in El Salvador.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 14:17:41 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Handcock", "Mark S.", ""], ["Gile", "Krista J.", ""], ["Mar", "Corinne M.", ""]]}, {"id": "1209.6283", "submitter": "Alicia Johnson", "authors": "Alicia A. Johnson, Owen Burbank", "title": "Geometric Ergodicity & Scanning Strategies For Two-Component Gibbs\n  Samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In any Markov chain Monte Carlo analysis, rapid convergence of the chain to\nits target probability distribution is of practical and theoretical importance.\nA chain that converges at a geometric rate is geometrically ergodic. In this\npaper, we explore geometric ergodicity for two-component Gibbs samplers which,\nunder a chosen scanning strategy, evolve by combining one-at-a-time updates of\nthe two components. We compare convergence behaviors between and within three\nsuch strategies: composition, random sequence scan, and random scan. Our main\nresults are twofold. First, we establish that if the Gibbs sampler is\ngeometrically ergodic under any one of these strategies, so too are the others.\nFurther, we establish a simple and verifiable set of sufficient conditions for\nthe geometric ergodicity of the Gibbs samplers. Our results are illustrated\nusing two examples.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 16:52:03 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2012 02:54:30 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Johnson", "Alicia A.", ""], ["Burbank", "Owen", ""]]}, {"id": "1209.6463", "submitter": "Antonio Punzo", "authors": "Sanjeena Subedi and Antonio Punzo and Salvatore Ingrassia and Paul D.\n  McNicholas", "title": "Clustering and Classification via Cluster-Weighted Factor Analyzers", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s11634-013-0124-8", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model-based clustering and classification, the cluster-weighted model\nconstitutes a convenient approach when the random vector of interest\nconstitutes a response variable Y and a set p of explanatory variables X.\nHowever, its applicability may be limited when p is high. To overcome this\nproblem, this paper assumes a latent factor structure for X in each mixture\ncomponent. This leads to the cluster-weighted factor analyzers (CWFA) model. By\nimposing constraints on the variance of Y and the covariance matrix of X, a\nnovel family of sixteen CWFA models is introduced for model-based clustering\nand classification. The alternating expectation-conditional maximization\nalgorithm, for maximum likelihood estimation of the parameters of all the\nmodels in the family, is described; to initialize the algorithm, a 5-step\nhierarchical procedure is proposed, which uses the nested structures of the\nmodels within the family and thus guarantees the natural ranking among the\nsixteen likelihoods. Artificial and real data show that these models have very\ngood clustering and classification performance and that the algorithm is able\nto recover the parameters very well.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 09:50:35 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Subedi", "Sanjeena", ""], ["Punzo", "Antonio", ""], ["Ingrassia", "Salvatore", ""], ["McNicholas", "Paul D.", ""]]}]