[{"id": "1904.00176", "submitter": "Zhipeng Wang", "authors": "Zhipeng Wang and David W. Scott", "title": "Nonparametric Density Estimation for High-Dimensional Data - Algorithms\n  and Applications", "comments": null, "journal-ref": "Wiley Interdisciplinary Reviews: Computational Statistics, 2019", "doi": "10.1002/wics.1461", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density Estimation is one of the central areas of statistics whose purpose is\nto estimate the probability density function underlying the observed data. It\nserves as a building block for many tasks in statistical inference,\nvisualization, and machine learning. Density Estimation is widely adopted in\nthe domain of unsupervised learning especially for the application of\nclustering. As big data become pervasive in almost every area of data sciences,\nanalyzing high-dimensional data that have many features and variables appears\nto be a major focus in both academia and industry. High-dimensional data pose\nchallenges not only from the theoretical aspects of statistical inference, but\nalso from the algorithmic/computational considerations of machine learning and\ndata analytics. This paper reviews a collection of selected nonparametric\ndensity estimation algorithms for high-dimensional data, some of them are\nrecently published and provide interesting mathematical insights. The important\napplication domain of nonparametric density estimation, such as { modal\nclustering}, are also included in this paper. Several research directions\nrelated to density estimation and high-dimensional data analysis are suggested\nby the authors.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 09:08:45 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wang", "Zhipeng", ""], ["Scott", "David W.", ""]]}, {"id": "1904.00670", "submitter": "Borislav Ikonomov", "authors": "Borislav Ikonomov, Michael U. Gutmann", "title": "Robust Optimisation Monte Carlo", "comments": "8 pages + 6 page appendix; v2: made clarifications, added a second\n  possible algorithm implementation and its results; v3: small clarifications,\n  to be published in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on Bayesian inference for parametric statistical models that\nare defined by a stochastic simulator which specifies how data is generated.\nExact sampling is then possible but evaluating the likelihood function is\ntypically prohibitively expensive. Approximate Bayesian Computation (ABC) is a\nframework to perform approximate inference in such situations. While basic ABC\nalgorithms are widely applicable, they are notoriously slow and much research\nhas focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has\nrecently been proposed as an efficient and embarrassingly parallel method that\nleverages optimisation to accelerate the inference. In this paper, we\ndemonstrate an important previously unrecognised failure mode of OMC: It\ngenerates strongly overconfident approximations by collapsing regions of\nsimilar or near-constant likelihood into a single point. We propose an\nefficient, robust generalisation of OMC that corrects this. It makes fewer\nassumptions, retains the main benefits of OMC, and can be performed either as\npost-processing to OMC or as a stand-alone computation. We demonstrate the\neffectiveness of the proposed Robust OMC on toy examples and tasks in\ninverse-graphics where we perform Bayesian inference with a complex image\nrenderer.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:50:41 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 15:54:56 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 13:45:56 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Ikonomov", "Borislav", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1904.00749", "submitter": "Roel Ceballos", "authors": "Novy Ann M. Etac and Roel F. Ceballos", "title": "Forecasting the Volatilities of Philippine Stock Exchange Composite\n  Index Using the Generalized Autoregressive Conditional Heteroskedasticity\n  Modeling", "comments": null, "journal-ref": "International Journal of Statistics and Economics, 19(3), 2018", "doi": null, "report-no": null, "categories": "q-fin.ST stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study was conducted to find an appropriate statistical model to forecast\nthe volatilities of PSEi using the model Generalized Autoregressive Conditional\nHeteroskedasticity (GARCH). Using the R software, the log returns of PSEi is\nmodeled using various ARIMA models and with the presence of heteroskedasticity,\nthe log returns was modeled using GARCH. Based on the analysis, GARCH models\nare the most appropriate to use for the log returns of PSEi. Among the selected\nGARCH models, GARCH (1,2) has the lowest AIC value and also has the highest LL\nvalue implying that GARCH (1,2) is the best model for the log returns of PSEi.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 10:42:27 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Etac", "Novy Ann M.", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "1904.02054", "submitter": "Etienne Roquain", "authors": "Guillermo Durand, Florian Junge, Sebastian D\\\"ohler, Etienne Roquain", "title": "DiscreteFDR: An R package for controlling the false discovery rate for\n  discrete test statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simultaneous analysis of many statistical tests is ubiquitous in\napplications. Perhaps the most popular error rate used for avoiding type one\nerror inflation is the false discovery rate (FDR). However, most theoretical\nand software development for FDR control has focused on the case of continuous\ntest statistics. For discrete data, methods that provide proven FDR control and\ngood performance have been proposed only recently. The R package DiscreteFDR\nprovides an implementation of these methods. For particular commonly used\ndiscrete tests such as Fisher's exact test, it can be applied as an\noff-the-shelf tool by taking only the raw data as input. It can also be used\nfor any arbitrary discrete test statistics by using some additional information\non the distribution of these statistics. The paper reviews the statistical\nmethods in a non-technical way, provides a detailed description of the\nimplementation in DiscreteFDR and presents some sample code and analyses.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:22:18 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Durand", "Guillermo", ""], ["Junge", "Florian", ""], ["D\u00f6hler", "Sebastian", ""], ["Roquain", "Etienne", ""]]}, {"id": "1904.02101", "submitter": "Mateusz Staniak", "authors": "Mateusz Staniak and Przemyslaw Biecek", "title": "The Landscape of R Packages for Automated Exploratory Data Analysis", "comments": null, "journal-ref": null, "doi": "10.32614/RJ-2019-033", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing availability of large but noisy data sets with a large number\nof heterogeneous variables leads to the increasing interest in the automation\nof common tasks for data analysis. The most time-consuming part of this process\nis the Exploratory Data Analysis, crucial for better domain understanding, data\ncleaning, data validation, and feature engineering.\n  There is a growing number of libraries that attempt to automate some of the\ntypical Exploratory Data Analysis tasks to make the search for new insights\neasier and faster. In this paper, we present a systematic review of existing\ntools for Automated Exploratory Data Analysis (autoEDA). We explore the\nfeatures of twelve popular R packages to identify the parts of analysis that\ncan be effectively automated with the current tools and to point out new\ndirections for further autoEDA development.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 09:49:33 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 19:38:14 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 08:27:29 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Staniak", "Mateusz", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1904.02180", "submitter": "Joshua Speagle", "authors": "Joshua S Speagle", "title": "dynesty: A Dynamic Nested Sampling Package for Estimating Bayesian\n  Posteriors and Evidences", "comments": "28 pages, 12 figures, submitted to MNRAS; code available at\n  https://github.com/joshspeagle/dynesty", "journal-ref": null, "doi": "10.1093/mnras/staa278", "report-no": null, "categories": "astro-ph.IM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present dynesty, a public, open-source, Python package to estimate\nBayesian posteriors and evidences (marginal likelihoods) using Dynamic Nested\nSampling. By adaptively allocating samples based on posterior structure,\nDynamic Nested Sampling has the benefits of Markov Chain Monte Carlo algorithms\nthat focus exclusively on posterior estimation while retaining Nested\nSampling's ability to estimate evidences and sample from complex, multi-modal\ndistributions. We provide an overview of Nested Sampling, its extension to\nDynamic Nested Sampling, the algorithmic challenges involved, and the various\napproaches taken to solve them. We then examine dynesty's performance on a\nvariety of toy problems along with several astronomical applications. We find\nin particular problems dynesty can provide substantial improvements in sampling\nefficiency compared to popular MCMC approaches in the astronomical literature.\nMore detailed statistical results related to Nested Sampling are also included\nin the Appendix.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 18:04:57 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Speagle", "Joshua S", ""]]}, {"id": "1904.02597", "submitter": "Joshua Lukemire", "authors": "Shrabanti Chowdhury, Joshua Lukemire, Abhyuday Mandal", "title": "A-ComVar: A Flexible Extension of Common Variance Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonregular fractions of factorial experiments for a class of\nlinear models. These models have a common general mean and main effects,\nhowever they may have different 2-factor interactions. Here we assume for\nsimplicity that 3-factor and higher order interactions are negligible. In the\nabsence of a priori knowledge about which interactions are important, it is\nreasonable to prefer a design that results in equal variance for the estimates\nof all interaction effects to aid in model discrimination. Such designs are\ncalled common variance designs and can be quite challenging to identify without\nperforming an exhaustive search of possible designs. In this work, we introduce\nan extension of common variance designs called approximate common variance, or\nA-ComVar designs. We develop a numerical approach to finding A-ComVar designs\nthat is much more efficient than an exhaustive search. We present the types of\nA-ComVar designs that can be found for different number of factors, runs, and\ninteractions. We further demonstrate the competitive performance of both common\nvariance and A-ComVar designs using several comparisons to other popular\ndesigns in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:14:51 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 18:07:14 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Chowdhury", "Shrabanti", ""], ["Lukemire", "Joshua", ""], ["Mandal", "Abhyuday", ""]]}, {"id": "1904.02961", "submitter": "Aleksey Polunchenko", "authors": "Kexuan Li and Aleksey S. Polunchenko and Andrey Pepelyshev", "title": "Analytic Evaluation of the Fractional Moments for the Quasi-Stationary\n  Distribution of the Shiryaev Martingale on an Interval", "comments": "Accepted for publication in Communications in Statistics - Simulation\n  and Computation. arXiv admin note: text overlap with arXiv:1805.07580", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the quasi-stationary distribution of the classical Shiryaev\ndiffusion restricted to the interval $[0,A]$ with absorption at a fixed $A>0$.\nWe derive analytically a closed-form formula for the distribution's fractional\nmoment of an {\\em arbitrary} given order $s\\in\\mathbb{R}$; the formula is\nconsistent with that previously found by Polunchenko and Pepelyshev (2018) for\nthe case of $s\\in\\mathbb{N}$. We also show by virtue of the formula that, if\n$s<1$, then the $s$-th fractional moment of the quasi-stationary distribution\nbecomes that of the exponential distribution (with mean $1/2$) in the limit as\n$A\\to+\\infty$; the limiting exponential distribution is the stationary\ndistribution of the reciprocal of the Shiryaev diffusion.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 09:46:54 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Li", "Kexuan", ""], ["Polunchenko", "Aleksey S.", ""], ["Pepelyshev", "Andrey", ""]]}, {"id": "1904.02966", "submitter": "Krzysztof Bisewski", "authors": "Krzysztof Bisewski, Daan Crommelin, and Michel Mandjes", "title": "Rare Event Simulation for Steady-State Probabilities via Recurrency\n  Cycles", "comments": "30 pages, 6 figures", "journal-ref": "Chaos: An Interdisciplinary Journal of Nonlinear Science,\n  29(3):033131 (2019)", "doi": "10.1063/1.5080296", "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new algorithm for the estimation of rare event probabilities\nassociated with the steady-state of a Markov stochastic process with continuous\nstate space $\\mathbb R^d$ and discrete time steps (i.e. a discrete-time\n$\\mathbb R^d$-valued Markov chain). The algorithm, which we coin Recurrent\nMultilevel Splitting (RMS), relies on the Markov chain's underlying recurrent\nstructure, in combination with the Multilevel Splitting method. Extensive\nsimulation experiments are performed, including experiments with a nonlinear\nstochastic model that has some characteristics of complex climate models. The\nnumerical experiments show that RMS can boost the computational efficiency by\nseveral orders of magnitude compared to the Monte Carlo method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 09:50:02 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Bisewski", "Krzysztof", ""], ["Crommelin", "Daan", ""], ["Mandjes", "Michel", ""]]}, {"id": "1904.03469", "submitter": "Yuqing Pan", "authors": "Yuqing Pan, Qing Mai, Xin Zhang", "title": "TULIP: A Toolbox for Linear Discriminant Analysis with Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear discriminant analysis (LDA) is a powerful tool in building classifiers\nwith easy computation and interpretation. Recent advancements in science\ntechnology have led to the popularity of datasets with high dimensions, high\norders and complicated structure. Such datasetes motivate the generalization of\nLDA in various research directions. The R package TULIP integrates several\npopular high-dimensional LDA-based methods and provides a comprehensive and\nuser-friendly toolbox for linear, semi-parametric and tensor-variate\nclassification. Functions are included for model fitting, cross validation and\nprediction. In addition, motivated by datasets with diverse sources of\npredictors, we further include functions for covariate adjustment. Our package\nis carefully tailored for low storage and high computation efficiency.\nMoreover, our package is the first R package for many of these methods,\nproviding great convenience to researchers in this area.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 15:26:15 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Pan", "Yuqing", ""], ["Mai", "Qing", ""], ["Zhang", "Xin", ""]]}, {"id": "1904.03836", "submitter": "Guanyang Wang", "authors": "Guanyang Wang", "title": "A Fast MCMC for the Uniform Sampling of Binary Matrices with Fixed\n  Margins", "comments": null, "journal-ref": "Electronic Journal of Statistics 2020", "doi": "10.1214/20-EJS1702", "report-no": null, "categories": "stat.CO cs.DS math.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform sampling of binary matrix with fixed margins is an important and\ndifficult problem in statistics, computer science, ecology and so on. The\nwell-known swap algorithm would be inefficient when the size of the matrix\nbecomes large or when the matrix is too sparse/dense.\n  Here we propose the Rectangle Loop algorithm, a Markov chain Monte Carlo\nalgorithm to sample binary matrices with fixed margins uniformly. Theoretically\nthe Rectangle Loop algorithm is better than the swap algorithm in Peskun's\norder. Empirically studies also demonstrates the Rectangle Loop algorithm is\nremarkablely more efficient than the swap algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 04:41:15 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 02:22:32 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Wang", "Guanyang", ""]]}, {"id": "1904.03920", "submitter": "Pierre Alquier", "authors": "Badr-Eddine Ch\\'erief-Abdellatif, Pierre Alquier, Mohammad Emtiyaz\n  Khan", "title": "A Generalization Bound for Online Variational Inference", "comments": "Published in the proceedings of ACML 2019", "journal-ref": "Proceedings in Machine Learning Research, 2019, vol. 101, pp.\n  662-677", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference provides an attractive online-learning framework to\nanalyze sequential data, and offers generalization guarantees which hold even\nwith model mismatch and adversaries. Unfortunately, exact Bayesian inference is\nrarely feasible in practice and approximation methods are usually employed, but\ndo such methods preserve the generalization properties of Bayesian inference ?\nIn this paper, we show that this is indeed the case for some variational\ninference (VI) algorithms. We consider a few existing online, tempered VI\nalgorithms, as well as a new algorithm, and derive their generalization bounds.\nOur theoretical result relies on the convexity of the variational objective,\nbut we argue that the result should hold more generally and present empirical\nevidence in support of this. Our work in this paper presents theoretical\njustifications in favor of online algorithms relying on approximate Bayesian\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:53:25 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 07:32:41 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""], ["Khan", "Mohammad Emtiyaz", ""]]}, {"id": "1904.04020", "submitter": "Xin Huang", "authors": "Xin Huang, Yulia R. Gel", "title": "CRAD: Clustering with Robust Autocuts and Depth", "comments": "9 pages, 6 figures", "journal-ref": "2017 IEEE International Conference on Data Mining (ICDM),\n  925--930} (2017)", "doi": "10.1109/ICDM.2017.116", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new density-based clustering algorithm named CRAD which is based\non a new neighbor searching function with a robust data depth as the\ndissimilarity measure. Our experiments prove that the new CRAD is highly\ncompetitive at detecting clusters with varying densities, compared with the\nexisting algorithms such as DBSCAN, OPTICS and DBCA. Furthermore, a new\neffective parameter selection procedure is developed to select the optimal\nunderlying parameter in the real-world clustering, when the ground truth is\nunknown. Lastly, we suggest a new clustering framework that extends CRAD from\nspatial data clustering to time series clustering without a-priori knowledge of\nthe true number of clusters. The performance of CRAD is evaluated through\nextensive experimental studies.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:49:45 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Huang", "Xin", ""], ["Gel", "Yulia R.", ""]]}, {"id": "1904.04488", "submitter": "Arnald Puy", "authors": "Arnald Puy, Samuele Lo Piano, Andrea Saltelli", "title": "A sensitivity analysis of the PAWN sensitivity index", "comments": "The paper has been accepted for publication in Environmental Modeling\n  and Software (use the journal version to cite the paper)", "journal-ref": "Environmental Modeling & Software 127, 104679 (2020)", "doi": "10.1016/j.envsoft.2020.104679", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The PAWN index is gaining traction among the modelling community as a\nsensitivity measure. However, the robustness to its design parameters has not\nyet been scrutinized: the size ($N$) and sampling ($\\varepsilon$) of the model\noutput, the number of conditioning intervals ($n$) or the summary statistic\n($\\theta$). Here we fill this gap by running a sensitivity analysis of a\nPAWN-based sensitivity analysis. We compare the results with the design\nuncertainties of the Sobol' total-order index ($S_{Ti}^*$). Unlike in\n$S_{Ti}^*$, the design uncertainties in PAWN create non-negligible chances of\nproducing biased results when ranking or screening inputs. The dependence of\nPAWN upon ($N,n,\\varepsilon, \\theta$) is difficult to tame, as these parameters\ninteract with one another. Even in an ideal setting in which the optimum choice\nfor ($N,n,\\varepsilon, \\theta$) is known in advance, PAWN might not allow to\ndistinguish an influential, non-additive model input from a truly\nnon-influential model input.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 06:50:40 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 09:27:08 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 17:29:08 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Puy", "Arnald", ""], ["Piano", "Samuele Lo", ""], ["Saltelli", "Andrea", ""]]}, {"id": "1904.04551", "submitter": "David Frazier", "authors": "David T. Frazier and Christopher Drovandi", "title": "Robust Approximate Bayesian Inference with Synthetic Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bayesian synthetic likelihood (BSL) is now an established method for\nconducting approximate Bayesian inference in models where, due to the\nintractability of the likelihood function, exact Bayesian approaches are either\ninfeasible or computationally too demanding. Implicit in the application of BSL\nis the assumption that the data generating process (DGP) can produce simulated\nsummary statistics that capture the behaviour of the observed summary\nstatistics. We demonstrate that if this compatibility between the actual and\nassumed DGP is not satisfied, i.e., if the model is misspecified, BSL can yield\nunreliable parameter inference. To circumvent this issue, we propose a new BSL\napproach that can detect the presence of model misspecification, and\nsimultaneously deliver useful inferences even under significant model\nmisspecification. Two simulated and two real data examples demonstrate the\nperformance of this new approach to BSL, and document its superior accuracy\nover standard BSL when the assumed model is misspecified.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:16:22 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 05:53:40 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 07:23:53 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Frazier", "David T.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1904.04650", "submitter": "Davood Hajinezhad", "authors": "Davood Hajinezhad and Michael Zavlanos", "title": "Gradient-Free Multi-Agent Nonconvex Nonsmooth Optimization", "comments": "Long version of CDC paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of minimizing the sum of nonconvex and\npossibly nonsmooth functions over a connected multi-agent network, where the\nagents have partial knowledge about the global cost function and can only\naccess the zeroth-order information (i.e., the functional values) of their\nlocal cost functions. We propose and analyze a distributed primal-dual\ngradient-free algorithm for this challenging problem. We show that by\nappropriately choosing the parameters, the proposed algorithm converges to the\nset of first order stationary solutions with a provable global sublinear\nconvergence rate. Numerical experiments demonstrate the effectiveness of our\nproposed method for optimizing nonconvex and nonsmooth problems over a network.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 13:24:44 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hajinezhad", "Davood", ""], ["Zavlanos", "Michael", ""]]}, {"id": "1904.05310", "submitter": "Fei Lu", "authors": "Fei Lu, Nils Weitzel, Adam H. Monahan", "title": "Joint state-parameter estimation of a nonlinear stochastic energy\n  balance model from sparse noisy data", "comments": null, "journal-ref": "Nonlin. Processes Geophys. 26, 2019", "doi": "10.5194/npg-26-227-2019", "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While nonlinear stochastic partial differential equations arise naturally in\nspatiotemporal modeling, inference for such systems often faces two major\nchallenges: sparse noisy data and ill-posedness of the inverse problem of\nparameter estimation. To overcome the challenges, we introduce a strongly\nregularized posterior by normalizing the likelihood and by imposing physical\nconstraints through priors of the parameters and states. We investigate joint\nparameter-state estimation by the regularized posterior in a physically\nmotivated nonlinear stochastic energy balance model (SEBM) for paleoclimate\nreconstruction. The high-dimensional posterior is sampled by a particle Gibbs\nsampler that combines MCMC with an optimal particle filter exploiting the\nstructure of the SEBM. In tests using either Gaussian or uniform priors based\non the physical range of parameters, the regularized posteriors overcome the\nill-posedness and lead to samples within physical ranges, quantifying the\nuncertainty in estimation. Due to the ill-posedness and the regularization, the\nposterior of parameters presents a relatively large uncertainty, and\nconsequently, the maximum of the posterior, which is the minimizer in a\nvariational approach, can have a large variation. In contrast, the posterior of\nstates generally concentrates near the truth, substantially filtering out\nobservation noise and reducing uncertainty in the unconstrained SEBM.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:19:06 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lu", "Fei", ""], ["Weitzel", "Nils", ""], ["Monahan", "Adam H.", ""]]}, {"id": "1904.05312", "submitter": "Angelos Alexopoulos Dr", "authors": "Angelos Alexopoulos, Petros Dellaportas, Omiros Papaspiliopoulos", "title": "Bayesian prediction of jumps in large panels of time series data", "comments": "49 pages, 27 figures, 4 tables", "journal-ref": null, "doi": "10.1214/21-BA1268", "report-no": null, "categories": "q-fin.ST stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a new look at the problem of disentangling the volatility and jumps\nprocesses of daily stock returns. We first provide a computational framework\nfor the univariate stochastic volatility model with Poisson-driven jumps that\noffers a competitive inference alternative to the existing tools. This\nmethodology is then extended to a large set of stocks for which we assume that\ntheir unobserved jump intensities co-evolve in time through a dynamic factor\nmodel. To evaluate the proposed modelling approach we conduct out-of-sample\nforecasts and we compare the posterior predictive distributions obtained from\nthe different models. We provide evidence that joint modelling of jumps\nimproves the predictive ability of the stochastic volatility models.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 22:59:32 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 07:57:45 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 15:24:56 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 11:29:49 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Alexopoulos", "Angelos", ""], ["Dellaportas", "Petros", ""], ["Papaspiliopoulos", "Omiros", ""]]}, {"id": "1904.05703", "submitter": "Dennis Prangle", "authors": "Dennis Prangle, Sophie Harbisher, Colin S Gillespie", "title": "Bayesian experimental design without posterior calculations: an\n  adversarial approach", "comments": "V3 is a major revision adding an adversarial approach to address\n  problems with the designs produced in previous versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most computational approaches to Bayesian experimental design require making\nposterior calculations, such evidence estimates, repeatedly for a large number\nof potential designs and/or simulated datasets. This can be expensive and\nprohibit scaling up these methods to models with many parameters, or designs\nwith many unknowns to select. We introduce an efficient alternative approach\nwithout posterior calculations, based on optimising the expected trace of the\nFisher information, as discussed by Walker (2016). We illustrate drawbacks of\nthis approach, including lack of invariance to reparameterisation and\nencouraging designs which are informative about one parameter combination but\nnot any others. We show these can be avoided by using an adversarial approach:\nthe experimenter must select their design while an adversary attempts to select\nthe least favourable parameterisation. We present theoretical properties of\nthis approach and show it can be used with gradient based optimisation methods\nto find designs efficiently in practice.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:05:15 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 12:41:20 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2020 10:42:49 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Prangle", "Dennis", ""], ["Harbisher", "Sophie", ""], ["Gillespie", "Colin S", ""]]}, {"id": "1904.05886", "submitter": "Jordan Franks", "authors": "Jordan Franks", "title": "Markov chain Monte Carlo importance samplers for Bayesian models with\n  intractable likelihoods", "comments": "41 pages. Introductory part of doctoral dissertation. Involves\n  previous joint works with Matti Vihola, Jouni Helske, Ajay Jasra, and Kody\n  J.H. Law. PhD thesis, University of Jyv\\\"askyl\\\"a, 2019. Available online at\n  http://urn.fi/URN:ISBN:978-951-39-7738-2", "journal-ref": null, "doi": null, "report-no": "JYU Dissertations 79", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the efficient use of an approximation within Markov chain Monte\nCarlo (MCMC), with subsequent importance sampling (IS) correction of the Markov\nchain inexact output, leading to asymptotically exact inference. We detail\nconvergence and central limit theorems for the resulting MCMC-IS estimators. We\nalso consider the case where the approximate Markov chain is pseudo-marginal,\nrequiring unbiased estimators for its approximate marginal target. Convergence\nresults with asymptotic variance formulae are shown for this case, and for the\ncase where the IS weights based on unbiased estimators are only calculated for\ndistinct output samples of the so-called `jump' chain, which, with a suitable\nreweighting, allows for improved efficiency. As the IS type weights may assume\nnegative values, extended classes of unbiased estimators may be used for the IS\ntype correction, such as those obtained from randomised multilevel Monte Carlo.\nUsing Euler approximations and coupling of particle filters, we apply the\nresulting estimator using randomised weights to the problem of parameter\ninference for partially observed It\\^{o} diffusions. Convergence of the\nestimator is verified to hold under regularity assumptions which do not require\nthat the diffusion can be simulated exactly. In the context of approximate\nBayesian computation (ABC), we suggest an adaptive MCMC approach to deal with\nthe selection of a suitably large tolerance, with IS correction possible to\nfiner tolerance, and with provided approximate confidence intervals. A\nprominent question is the efficiency of MCMC-IS compared to standard direct\nMCMC, such as pseudo-marginal, delayed acceptance, and ABC-MCMC. We provide a\ncomparison criterion which generalises the covariance ordering to the IS\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 13:30:03 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Franks", "Jordan", ""]]}, {"id": "1904.06819", "submitter": "Robert Foster", "authors": "Robert C. Foster, Brian Weaver, James Gattiker", "title": "Applications of Quantum Annealing in Statistics", "comments": "Corrected a few typos, added a few references, clarified some points,\n  added information about potential polynomial speedup of quantum annealing and\n  effect of difference between ground and excited energy states", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computation offers exciting new possibilities for statistics. This\npaper explores the use of the D-Wave machine, a specialized type of quantum\ncomputer, which performs quantum annealing. A general description of quantum\nannealing through the use of the D-Wave is given, along with technical issues\nto be encountered. Quantum annealing is used to perform maximum likelihood\nestimation, generate an experimental design, and perform matrix inversion.\nThough the results show that quantum computing is still at an early stage which\nis not yet superior to classical computation, there is promise for quantum\ncomputation in the future.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 02:59:13 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 17:49:21 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Foster", "Robert C.", ""], ["Weaver", "Brian", ""], ["Gattiker", "James", ""]]}, {"id": "1904.07270", "submitter": "Joshua Hewitt", "authors": "Joshua Hewitt, Jennifer A. Hoeting", "title": "Approximate Bayesian Inference via Sparse grid Quadrature Evaluation for\n  Hierarchical Models", "comments": "35 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine conditioning techniques with sparse grid quadrature rules to\ndevelop a computationally efficient method to approximate marginal, but not\nnecessarily univariate, posterior quantities, yielding approximate Bayesian\ninference via Sparse grid Quadrature Evaluation (BISQuE) for hierarchical\nmodels. BISQuE reformulates posterior quantities as weighted integrals of\nconditional quantities, such as densities and expectations. Sparse grid\nquadrature rules allow computationally efficient approximation of high\ndimensional integrals, which appear in hierarchical models with many\nhyperparameters. BISQuE reduces computational effort relative to standard,\nMarkov chain Monte Carlo methods by at least two orders of magnitude on several\napplied and illustrative models. We also briefly discuss using BISQuE to apply\nIntegrated Nested Laplace Approximations (INLA) to models with more\nhyperparameters than is currently practical.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 18:07:31 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Hewitt", "Joshua", ""], ["Hoeting", "Jennifer A.", ""]]}, {"id": "1904.07462", "submitter": "Nhat Ho", "authors": "Nhat Ho and Tianyi Lin and Michael I. Jordan", "title": "On Structured Filtering-Clustering: Global Error Bound and Optimal\n  First-Order Algorithms", "comments": "The first two authors contributed equally to this work. This version\n  greatly improves and expands the results in the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the filtering-clustering problems have been a central topic\nin statistics and machine learning, especially the $\\ell_1$-trend filtering and\n$\\ell_2$-convex clustering problems. In practice, such structured problems are\ntypically solved by first-order algorithms despite the extremely\nill-conditioned structures of difference operator matrices. Inspired by the\ndesire to analyze the convergence rates of these algorithms, we show that for a\nlarge class of filtering-clustering problems, a \\textit{global error bound}\ncondition is satisfied for the dual filtering-clustering problems when a\ncertain regularization is chosen. Based on this result, we show that many\nfirst-order algorithms attain the \\textit{optimal rate of convergence} in\ndifferent settings. In particular, we establish a generalized dual gradient\nascent (GDGA) algorithmic framework with several subroutines. In deterministic\nsetting when the subroutine is accelerated gradient descent (AGD), the\nresulting algorithm attains the linear convergence. This linear convergence\nalso holds for the finite-sum setting in which the subroutine is the Katyusha\nalgorithm. We also demonstrate that the GDGA with stochastic gradient descent\n(SGD) subroutine attains the optimal rate of convergence up to the logarithmic\nfactor, shedding the light to the possibility of solving the\nfiltering-clustering problems efficiently in online setting. Experiments\nconducted on $\\ell_1$-trend filtering problems illustrate the favorable\nperformance of our algorithms over other competing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:54:47 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 00:13:20 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Ho", "Nhat", ""], ["Lin", "Tianyi", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1904.07495", "submitter": "David  Nott", "authors": "Michael Stanley Smith, Ruben Loaiza-Maya and David J. Nott", "title": "High-dimensional copula variational approximation through transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational methods are attractive for computing Bayesian inference for\nhighly parametrized models and large datasets where exact inference is\nimpractical. They approximate a target distribution - either the posterior or\nan augmented posterior - using a simpler distribution that is selected to\nbalance accuracy with computational feasibility. Here we approximate an\nelement-wise parametric transformation of the target distribution as\nmultivariate Gaussian or skew-normal. Approximations of this kind are implicit\ncopula models for the original parameters, with a Gaussian or skew-normal\ncopula function and flexible parametric margins. A key observation is that\ntheir adoption can improve the accuracy of variational inference in high\ndimensions at limited or no additional computational cost. We consider the\nYeo-Johnson and G&H transformations, along with sparse factor structures for\nthe scale matrix of the Gaussian or skew-normal. We also show how to implement\nefficient reparametrization gradient methods for these copula-based\napproximations. The efficacy of the approach is illustrated by computing\nposterior inference for three different models using six real datasets. In each\ncase, we show that our proposed copula model distributions are more accurate\nvariational approximations than Gaussian or skew-normal distributions, but at\nonly a minor or no increase in computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 07:06:44 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 07:29:35 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Smith", "Michael Stanley", ""], ["Loaiza-Maya", "Ruben", ""], ["Nott", "David J.", ""]]}, {"id": "1904.08064", "submitter": "Feng Li", "authors": "Xixi Li, Yanfei Kang, Feng Li", "title": "Forecasting with time series imaging", "comments": null, "journal-ref": "Expert Systems with Applications (2020)", "doi": "10.1016/j.eswa.2020.113680", "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based time series representations have attracted substantial\nattention in a wide range of time series analysis methods. Recently, the use of\ntime series features for forecast model averaging has been an emerging research\nfocus in the forecasting community. Nonetheless, most of the existing\napproaches depend on the manual choice of an appropriate set of features.\nExploiting machine learning methods to extract features from time series\nautomatically becomes crucial in state-of-the-art time series analysis. In this\npaper, we introduce an automated approach to extract time series features based\non time series imaging. We first transform time series into recurrence plots,\nfrom which local features can be extracted using computer vision algorithms.\nThe extracted features are used for forecast model averaging. Our experiments\nshow that forecasting based on automatically extracted features, with less\nhuman intervention and a more comprehensive view of the raw time series data,\nyields highly comparable performances with the best methods in the largest\nforecasting competition dataset (M4) and outperforms the top methods in the\nTourism forecasting competition dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:18:45 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 03:33:57 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 03:13:12 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Xixi", ""], ["Kang", "Yanfei", ""], ["Li", "Feng", ""]]}, {"id": "1904.08356", "submitter": "Iker Perez", "authors": "Iker Perez and Theodore Kypraios", "title": "Scalable Bayesian Inference for Population Markov Jump Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for Markov jump processes (MJPs) where available\nobservations relate to either system states or jumps typically relies on\ndata-augmentation Markov Chain Monte Carlo. State-of-the-art developments\ninvolve representing MJP paths with auxiliary candidate jump times that are\nlater thinned. However, these algorithms are i) unfeasible in situations\ninvolving large or infinite capacity systems and ii) not amenable for all\nobservation types. In this paper we establish and present a general\ndata-augmentation framework for population MJPs based on uniformized\nrepresentations of the underlying non-stationary jump processes. This leads to\nmultiple novel MCMC samplers which enable exact (in the Monte Carlo sense)\ninference tasks for model parameters. We show that proposed samplers outperform\nexisting popular approaches, and offer substantial efficiency gains in\napplications to partially observed stochastic epidemics, immigration processes\nand predator-prey dynamical systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 16:45:05 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Perez", "Iker", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1904.08870", "submitter": "Antonia Gieschen", "authors": "Antonia Gieschen, Jake Ansell, Raffaella Calabrese, Belen\n  Martin-Barragan", "title": "Modelling antimicrobial prescriptions in Scotland: A spatio-temporal\n  clustering approach", "comments": "27 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2016 the British government acknowledged the importance of reducing\nantimicrobial prescriptions in order to avoid the long-term harmful effects of\nover-prescription. Prescription needs are highly dependent on factors that have\na spatio-temporal component, such as the presence of a bacterial outbreak and\nthe population density. In this context, density-based clustering algorithms\nare flexible tools to analyse data by searching for group structures. The case\nof Scotland presents an additional challenge due to the diversity of population\ndensities under the area of study. We present here a spatio-temporal clustering\napproach for highlighting the behaviour of general practitioners (GPs) in\nScotland. Particularly, we consider the density-based spatial clustering of\napplications with noise algorithm (DBSCAN) due to its ability to include both\nspatial and temporal data, as well as its flexibility to be extended with\nfurther variables. We extend this approach into two directions. For the\ntemporal analysis, we use dynamic time warping to measure the dissimilarity\nbetween warped and shifted time series. For the spatial component, we introduce\na new way of weighting spatial distances with continuous weights derived from a\nKDE-based process. This makes our approach suitable for cases involving spatial\nclusters with differing densities, which is a well-known issue for the original\nDBSCAN. We show an improved performance compared to both the latter and the\npopular k-means algorithm on simulated, as well as empirical data, presenting\nevidence for the ability to cluster more elements correctly and deliver\nactionable insights.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 16:26:33 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Gieschen", "Antonia", ""], ["Ansell", "Jake", ""], ["Calabrese", "Raffaella", ""], ["Martin-Barragan", "Belen", ""]]}, {"id": "1904.09339", "submitter": "Reza Mohammadi", "authors": "Reza Mohammadi, Matthew Pratola, Maurits Kaptein", "title": "Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models", "comments": "Published at http://jmlr.org/papers/v21/19-307 in the Journal of\n  Machine Learning Research (https://www.jmlr.org)", "journal-ref": "Journal of Machine Learning Research 2020, Vol. 21, No. 201, 1-26", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees are flexible models that are well suited for many statistical\nregression problems. In a Bayesian framework for regression trees, Markov Chain\nMonte Carlo (MCMC) search algorithms are required to generate samples of tree\nmodels according to their posterior probabilities. The critical component of\nsuch an MCMC algorithm is to construct good Metropolis-Hastings steps for\nupdating the tree topology. However, such algorithms frequently suffering from\nlocal mode stickiness and poor mixing. As a result, the algorithms are slow to\nconverge. Hitherto, authors have primarily used discrete-time birth/death\nmechanisms for Bayesian (sums of) regression tree models to explore the model\nspace. These algorithms are efficient only if the acceptance rate is high which\nis not always the case. Here we overcome this issue by developing a new search\nalgorithm which is based on a continuous-time birth-death Markov process. This\nsearch algorithm explores the model space by jumping between parameter spaces\ncorresponding to different tree structures. In the proposed algorithm, the\nmoves between models are always accepted which can dramatically improve the\nconvergence and mixing properties of the MCMC algorithm. We provide theoretical\nsupport of the algorithm for Bayesian regression tree models and demonstrate\nits performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 20:37:05 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 11:55:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mohammadi", "Reza", ""], ["Pratola", "Matthew", ""], ["Kaptein", "Maurits", ""]]}, {"id": "1904.09591", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan and Aishwarya Bhaskaran and David J. Nott", "title": "Conditionally structured variational Gaussian approximation with\n  importance weights", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop flexible methods of deriving variational inference for models with\ncomplex latent variable structure. By splitting the variables in these models\ninto \"global\" parameters and \"local\" latent variables, we define a class of\nvariational approximations that exploit this partitioning and go beyond\nGaussian variational approximation. This approximation is motivated by the fact\nthat in many hierarchical models, there are global variance parameters which\ndetermine the scale of local latent variables in their posterior conditional on\nthe global parameters. We also consider parsimonious parametrizations by using\nconditional independence structure, and improved estimation of the log marginal\nlikelihood and variational density using importance weights. These methods are\nshown to improve significantly on Gaussian variational approximation methods\nfor a similar computational cost. Application of the methodology is illustrated\nusing generalized linear mixed models and state space models.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 12:48:40 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Tan", "Linda S. L.", ""], ["Bhaskaran", "Aishwarya", ""], ["Nott", "David J.", ""]]}, {"id": "1904.09623", "submitter": "Deborshee Sen", "authors": "Deborshee Sen and Alexandre H Thiery", "title": "Particle filter efficiency under limited communication", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods are typically not straightforward to\nimplement on parallel architectures. This is because standard resampling\nschemes involve communication between all particles in the system. In this\narticle, we consider the $\\alpha$-SMC algorithm, a generalised particle filter\nalgorithm with limited communications between particles. The communication\nbetween different particles is controlled through the so-called\n$\\alpha$-matrices. We study the influence of the communication structure on the\nconvergence and stability properties of the resulting algorithms. We prove that\nunder standard assumptions, it is possible to use randomised communication\nstructures where each particle only communicates with a small number of\nneighbouring particles while still having good mixing properties, and this\nensures that the resulting algorithms are stable in time and converge at the\nusual Monte Carlo rate. A particularly simple approach to implement these ideas\nconsists of choosing the $\\alpha$-matrices as the Markov transition matrices of\nrandom walks on Ramanujan graphs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 16:49:02 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sen", "Deborshee", ""], ["Thiery", "Alexandre H", ""]]}, {"id": "1904.09668", "submitter": "Alexander Litvinenko", "authors": "Sergey Dolgov, Alexander Litvinenko, Dishi Liu", "title": "Kriging in Tensor Train data format", "comments": "19 pages,4 figures, 1 table, UNCECOMP 2019 3rd International\n  Conference on Uncertainty Quantification in Computational Sciences and\n  Engineering 24-26 June 2019, Crete, Greece https://2019.uncecomp.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combination of low-tensor rank techniques and the Fast Fourier transform\n(FFT) based methods had turned out to be prominent in accelerating various\nstatistical operations such as Kriging, computing conditional covariance,\ngeostatistical optimal design, and others. However, the approximation of a full\ntensor by its low-rank format can be computationally formidable. In this work,\nwe incorporate the robust Tensor Train (TT) approximation of covariance\nmatrices and the efficient TT-Cross algorithm into the FFT-based Kriging. It is\nshown that here the computational complexity of Kriging is reduced to\n$\\mathcal{O}(d r^3 n)$, where $n$ is the mode size of the estimation grid, $d$\nis the number of variables (the dimension), and $r$ is the rank of the TT\napproximation of the covariance matrix. For many popular covariance functions\nthe TT rank $r$ remains stable for increasing $n$ and $d$. The advantages of\nthis approach against those using plain FFT are demonstrated in synthetic and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 22:01:01 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Dolgov", "Sergey", ""], ["Litvinenko", "Alexander", ""], ["Liu", "Dishi", ""]]}, {"id": "1904.09733", "submitter": "Raffaele Argiento", "authors": "Raffaele Argiento and Maria De Iorio", "title": "Is infinity that far? A Bayesian nonparametric perspective of finite\n  mixture models", "comments": "46 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mixture models are one of the most widely used statistical tools when dealing\nwith data from heterogeneous populations. This paper considers the\nlong-standing debate over finite mixture and infinite mixtures and brings the\ntwo modelling strategies together, by showing that a finite mixture is simply a\nrealization of a point process. Following a Bayesian nonparametric perspective,\nwe introduce a new class of prior: the Normalized Independent Point Processes.\nWe investigate the probabilistic properties of this new class. Moreover, we\ndesign a conditional algorithm for finite mixture models with a random number\nof components overcoming the challenges associated with the Reversible Jump\nscheme and the recently proposed marginal algorithms. We illustrate our model\non real data and discuss an important application in population genetics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:59:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Argiento", "Raffaele", ""], ["De Iorio", "Maria", ""]]}, {"id": "1904.09734", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i, Livio Finos, Gianmarco Alto\\`e, and Massimiliano\n  Pastore", "title": "A Maximum Entropy Procedure to Solve Likelihood Equations", "comments": "14 pages, 2 figure, research article", "journal-ref": "Entropy, MDPI, 2019, 21(6), 596", "doi": "10.3390/e21060596", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we provide initial findings regarding the problem of solving\nlikelihood equations by means of a maximum entropy approach. Unlike standard\nprocedures that require equating at zero the score function of the\nmaximum-likelihood problem, we propose an alternative strategy where the score\nis instead used as external informative constraint to the maximization of the\nconvex Shannon's entropy function. The problem involves the re-parameterization\nof the score parameters as expected values of discrete probability\ndistributions where probabilities need to be estimated. This leads to a simpler\nsituation where parameters are searched in smaller (hyper) simplex space. We\nassessed our proposal by means of empirical case studies and a simulation\nstudy, this latter involving the most critical case of logistic regression\nunder data separation. The results suggested that the maximum entropy\nre-formulation of the score problem solves the likelihood equation problem.\nSimilarly, when maximum-likelihood estimation is difficult, as for the case of\nlogistic regression under separation, the maximum entropy proposal achieved\nresults (numerically) comparable to those obtained by the Firth's\nBias-corrected approach. Overall, these first findings reveal that a maximum\nentropy solution can be considered as an alternative technique to solve the\nlikelihood equation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:59:52 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 12:26:44 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 10:41:49 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""], ["Finos", "Livio", ""], ["Alto\u00e8", "Gianmarco", ""], ["Pastore", "Massimiliano", ""]]}, {"id": "1904.09808", "submitter": "Valentin De Bortoli", "authors": "Valentin De Bortoli, Alain Durmus", "title": "Convergence of diffusions and their discretizations: from continuous to\n  discrete processes and back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish new quantitative convergence bounds for a class\nof functional autoregressive models in weighted total variation metrics. To\nderive our results, we show that under mild assumptions, explicit minorization\nand Foster-Lyapunov drift conditions hold. The main applications and\nconsequences of the bounds we obtain concern the geometric convergence of\nEuler-Maruyama discretizations of diffusions with identity covariance matrix.\nSecond, as a corollary, we provide a new approach to establish quantitative\nconvergence of these diffusion processes by applying our conclusions in the\ndiscrete-time setting to a well-suited sequence of discretizations whose\nassociated stepsizes decrease towards zero.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 12:05:46 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 15:23:16 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 22:22:29 GMT"}, {"version": "v4", "created": "Fri, 1 May 2020 19:26:55 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["De Bortoli", "Valentin", ""], ["Durmus", "Alain", ""]]}, {"id": "1904.09929", "submitter": "Yanan Pei", "authors": "Jose H. Blanchet, Peter W. Glynn, Yanan Pei", "title": "Unbiased Multilevel Monte Carlo: Stochastic Optimization, Steady-state\n  Simulation, Quantiles, and Other Applications", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present general principles for the design and analysis of unbiased Monte\nCarlo estimators in a wide range of settings. Our estimators posses finite\nwork-normalized variance under mild regularity conditions. We apply our\nestimators to various settings of interest, including unbiased optimization in\nSample Average Approximations, unbiased steady-state simulation of regenerative\nprocesses, quantile estimation and nested simulation problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 15:44:14 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Blanchet", "Jose H.", ""], ["Glynn", "Peter W.", ""], ["Pei", "Yanan", ""]]}, {"id": "1904.10172", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i, Massimiliano Pastore, and Gianmarco Alto\\`e", "title": "ssMousetrack: Analysing computerized tracking data via Bayesian\n  state-space models in {R}", "comments": null, "journal-ref": "Math. Comput. Appl. 2020, 25(3), 41", "doi": "10.3390/mca25030041", "report-no": null, "categories": "stat.CO stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological advances have provided new settings to enhance\nindividual-based data collection and computerized-tracking data have became\ncommon in many behavioral and social research. By adopting instantaneous\ntracking devices such as computer-mouse, wii, and joysticks, such data provide\nnew insights for analysing the dynamic unfolding of response process.\nssMousetrack is a R package for modeling and analysing computerized-tracking\ndata by means of a Bayesian state-space approach. The package provides a set of\nfunctions to prepare data, fit the model, and assess results via simple\ndiagnostic checks. This paper describes the package and illustrates how it can\nbe used to model and analyse computerized-tracking data. A case study is also\nincluded to show the use of the package in empirical case studies.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 06:30:24 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""], ["Pastore", "Massimiliano", ""], ["Alto\u00e8", "Gianmarco", ""]]}, {"id": "1904.10499", "submitter": "Mar\\'ia Juliana Gambini", "authors": "Alejandro C. Frery, Juliana Gambini", "title": "Comparing Samples from the $\\mathcal{G}^0$ Distribution using a Geodesic\n  Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\mathcal{G}^0$ distribution is widely used for monopolarized SAR image\nmodeling because it can characterize regions with different degree of texture\naccurately. It is indexed by three parameters: the number of looks (which can\nbe estimated for the whole image), a scale parameter and a texture parameter.\nThis paper presents a new proposal for comparing samples from the\n$\\mathcal{G}^0$ distribution using a Geodesic Distance (GD) as a measure of\ndissimilarity between models. The objective is quantifying the difference\nbetween pairs of samples from SAR data using both local parameters (scale and\ntexture) of the $\\mathcal{G}^0$ distribution. We propose three tests based on\nthe GD which combine the tests presented in~\\cite{GeodesicDistanceGI0JSTARS},\nand we estimate their probability distributions using permutation methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 19:13:29 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Frery", "Alejandro C.", ""], ["Gambini", "Juliana", ""]]}, {"id": "1904.10582", "submitter": "Eric Chi", "authors": "Halley L. Brantley and Joseph Guinness and Eric C. Chi", "title": "Baseline Drift Estimation for Air Quality Data Using Quantile Trend\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating smoothly varying baseline trends in time\nseries data. This problem arises in a wide range of fields, including\nchemistry, macroeconomics, and medicine; however, our study is motivated by the\nanalysis of data from low cost air quality sensors. Our methods extend the\nquantile trend filtering framework to enable the estimation of multiple\nquantile trends simultaneously while ensuring that the quantiles do not cross.\nTo handle the computational challenge posed by very long time series, we\npropose a parallelizable alternating direction method of moments (ADMM)\nalgorithm. The ADMM algorthim enables the estimation of trends in a piecewise\nmanner, both reducing the computation time and extending the limits of the\nmethod to larger data sizes. We also address smoothing parameter selection and\npropose a modified criterion based on the extended Bayesian Information\nCriterion. Through simulation studies and our motivating application to low\ncost air quality sensor data, we demonstrate that our model provides better\nquantile trend estimates than existing methods and improves signal\nclassification of low-cost air quality sensor output.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 00:23:00 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Brantley", "Halley L.", ""], ["Guinness", "Joseph", ""], ["Chi", "Eric C.", ""]]}, {"id": "1904.10886", "submitter": "Behram Wali", "authors": "Behram Wali, Asad Khattak, David Greene, Jun Liu", "title": "Fuel Economy Gaps Within & Across Garages: A Bivariate Random Parameters\n  Seemingly Unrelated Regression Approach", "comments": "Fuel economy gap, two-vehicles, garage, My MPG, On-road & test cycle\n  estimates, Random parameters, Seemingly unrelated regression estimation", "journal-ref": "International Journal of Sustainable Transportation, 1-16 (2018)", "doi": "10.1080/15568318.2018.1466222", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key objective of this study is to investigate the interrelationship\nbetween fuel economy gaps and to quantify the differential effects of several\nfactors on fuel economy gaps of vehicles operated by the same garage. By using\na unique fuel economy database (fueleconomy.gov), users self-reported fuel\neconomy estimates and government fuel economy ratings are analyzed for more\nthan 7000 garages across the U.S. The empirical analysis, nonetheless, is\ncomplicated owing to the presence of important methodological concerns\nincluding potential interrelationship between vehicles within the same garage\nand unobserved heterogeneity. To address these concerns, bivariate seemingly\nunrelated fixed and random parameter models are presented. With government test\ncycle ratings tending to over-estimate the actual on-road fuel economy, a\nsignificant variation is observed in the fuel economy gaps for the two vehicles\nacross garages. A wide variety of factors such as driving style, fuel economy\ncalculation method, and several vehicle specific characteristics are\nconsidered. Drivers who drive for maximum gas mileage or drives with the\ntraffic flow have greater on-road fuel economy relative to the government\nofficial ratings. Contrarily, volatile drivers have smaller on-road fuel\neconomy relative to the official ratings. Compared to the previous findings,\nour analysis suggests that the relationship between fuel type and fuel economy\ngaps is complex and not unidirectional. Regarding several vehicle and\nmanufacturer related variables, the effects do not just significantly vary in\nmagnitude but also in the direction, underscoring the importance of accounting\nfor within-garage correlation and unobserved heterogeneity for making reliable\ninferences.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 20:30:14 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Wali", "Behram", ""], ["Khattak", "Asad", ""], ["Greene", "David", ""], ["Liu", "Jun", ""]]}, {"id": "1904.11403", "submitter": "Anna Nikishova", "authors": "Anna Nikishova, Giovanni E. Comi, Alfons G. Hoekstra", "title": "Sensitivity analysis based dimension reduction of multiscale models", "comments": null, "journal-ref": null, "doi": "10.1016/j.matcom.2019.10.013", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the sensitivity analysis of a single scale model is employed\nin order to reduce the input dimensionality of the related multiscale model, in\nthis way, improving the efficiency of its uncertainty estimation. The approach\nis illustrated with two examples: a reaction model and the standard\nOrnstein-Uhlenbeck process. Additionally, a counterexample shows that an\nuncertain input should not be excluded from uncertainty quantification without\nestimating the response sensitivity to this parameter. In particular, an\nanalysis of the function defining the relation between single scale components\nis required to understand whether single scale sensitivity analysis can be used\nto reduce the dimensionality of the overall multiscale model input space.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 15:21:50 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 13:19:03 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Nikishova", "Anna", ""], ["Comi", "Giovanni E.", ""], ["Hoekstra", "Alfons G.", ""]]}, {"id": "1904.11665", "submitter": "William Leeb", "authors": "William Leeb", "title": "Rapid evaluation of the spectral signal detection threshold and\n  Stieltjes transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of signal components is a frequently-encountered challenge\nin statistical applications with low signal-to-noise ratio. This problem is\nparticularly challenging in settings with heteroscedastic noise. In certain\nsignal-plus-noise models of data, such as the classical spiked covariance model\nand its variants, there are closed formulas for the spectral signal detection\nthreshold (the largest sample eigenvalue attributable solely to noise) in the\nisotropic noise setting. However, existing methods for numerically evaluating\nthe threshold for more general noise models remain unsatisfactory.\n  In this work, we introduce a rapid algorithm for evaluating the spectral\nsignal detection threshold. We consider noise matrices with a separable\nvariance profile, as these arise often in applications. The solution is based\non nested applications of Newton's method. We also devise a new algorithm for\nevaluating the Stieltjes transform of the spectral distribution at real values\nexceeding the threshold. The Stieltjes transform on this domain is known to be\na key quantity in parameter estimation for spectral denoising methods. The\ncorrectness of both algorithms is proven from a detailed analysis of the master\nequations characterizing the Stieltjes transform, and their performance is\ndemonstrated in numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 04:14:41 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:44:33 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Leeb", "William", ""]]}, {"id": "1904.11912", "submitter": "James M. Flegal", "authors": "Nathan Robertson, James M. Flegal, Dootika Vats, and Galin L. Jones", "title": "Assessing and Visualizing Simultaneous Simulation Error", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo experiments produce samples in order to estimate features of a\ngiven distribution. However, simultaneous estimation of means and quantiles has\nreceived little attention, despite being common practice. In this setting we\nestablish a multivariate central limit theorem for any finite combination of\nsample means and quantiles under the assumption of a strongly mixing process,\nwhich includes the standard Monte Carlo and Markov chain Monte Carlo settings.\nWe build on this to provide a fast algorithm for constructing hyperrectangular\nconfidence regions having the desired simultaneous coverage probability and a\nconvenient marginal interpretation. The methods are incorporated into standard\nways of visualizing the results of Monte Carlo experiments enabling the\npractitioner to more easily assess the reliability of the results. We\ndemonstrate the utility of this approach in various Monte Carlo settings\nincluding simulation studies based on independent and identically distributed\nsamples and Bayesian analyses using Markov chain Monte Carlo sampling.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 16:00:35 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 19:30:23 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Robertson", "Nathan", ""], ["Flegal", "James M.", ""], ["Vats", "Dootika", ""], ["Jones", "Galin L.", ""]]}, {"id": "1904.12064", "submitter": "Adam Sykulski Dr", "authors": "Jeffrey J. Early and Adam M. Sykulski", "title": "Smoothing and Interpolating Noisy GPS Data with Smoothing Splines", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": "10.1175/JTECH-D-19-0087.1", "report-no": null, "categories": "stat.ME physics.data-an stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive methodology is provided for smoothing noisy, irregularly\nsampled data with non-Gaussian noise using smoothing splines. We demonstrate\nhow the spline order and tension parameter can be chosen a priori from physical\nreasoning. We also show how to allow for non-Gaussian noise and outliers which\nare typical in GPS signals. We demonstrate the effectiveness of our methods on\nGPS trajectory data obtained from oceanographic floating instruments known as\ndrifters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 22:28:58 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 23:14:02 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Early", "Jeffrey J.", ""], ["Sykulski", "Adam M.", ""]]}, {"id": "1904.12083", "submitter": "Bo Dai", "authors": "Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, Dale\n  Schuurmans", "title": "Exponential Family Estimation via Adversarial Dynamics Embedding", "comments": "Appearing in NeurIPS 2019 Vancouver, Canada; a preliminary version\n  published in NeurIPS2018 Bayesian Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for maximum likelihood estimation (MLE) of\nexponential family models, with a general parametrization of the energy\nfunction that includes neural networks. We exploit the primal-dual view of the\nMLE with a kinetics augmented model to obtain an estimate associated with an\nadversarial dual sampler. To represent this sampler, we introduce a novel\nneural architecture, dynamics embedding, that generalizes Hamiltonian\nMonte-Carlo (HMC). The proposed approach inherits the flexibility of HMC while\nenabling tractable entropy estimation for the augmented model. By learning both\na dual sampler and the primal model simultaneously, and sharing parameters\nbetween them, we obviate the requirement to design a separate sampling\nprocedure once the model has been trained, leading to more effective learning.\nWe show that many existing estimators, such as contrastive divergence,\npseudo/composite-likelihood, score matching, minimum Stein discrepancy\nestimator, non-local contrastive objectives, noise-contrastive estimation, and\nminimum probability flow, are special cases of the proposed approach, each\nexpressed by a different (fixed) dual sampler. An empirical investigation shows\nthat adapting the sampler during MLE can significantly improve on\nstate-of-the-art estimators.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 01:20:21 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 06:36:27 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 20:20:43 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Dai", "Bo", ""], ["Liu", "Zhen", ""], ["Dai", "Hanjun", ""], ["He", "Niao", ""], ["Gretton", "Arthur", ""], ["Song", "Le", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1904.12092", "submitter": "Andrew Raim", "authors": "Andrew M. Raim, Scott H. Holan, Jonathan R. Bradley, and Christopher\n  K. Wikle", "title": "Spatio-Temporal Change of Support Modeling with R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal change of support methods are designed for statistical\nanalysis on spatial and temporal domains which can differ from those of the\nobserved data. Previous work introduced a parsimonious class of Bayesian\nhierarchical spatio-temporal models, which we refer to as STCOS, for the case\nof Gaussian outcomes. Application of STCOS methodology from this literature\nrequires a level of proficiency with spatio-temporal methods and statistical\ncomputing which may be a hurdle for potential users. The present work seeks to\nbridge this gap by guiding readers through STCOS computations. We focus on the\nR computing environment because of its popularity, free availability, and high\nquality contributed packages. The stcos package is introduced to facilitate\ncomputations for the STCOS model. A motivating application is the American\nCommunity Survey (ACS), an ongoing survey administered by the U.S. Census\nBureau that measures key socioeconomic and demographic variables for various\npopulations in the United States. The STCOS methodology offers a principled\napproach to compute model-based estimates and associated measures of\nuncertainty for ACS variables on customized geographies and/or time periods. We\npresent a detailed case study with ACS data as a guide for change of support\nanalysis in R, and as a foundation which can be customized to other\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 02:34:45 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 20:24:25 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 12:01:43 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Raim", "Andrew M.", ""], ["Holan", "Scott H.", ""], ["Bradley", "Jonathan R.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1904.12157", "submitter": "Jun Yang", "authors": "Jun Yang, Gareth O. Roberts, Jeffrey S. Rosenthal", "title": "Optimal Scaling of Random-Walk Metropolis Algorithms on General Target\n  Distributions", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One main limitation of the existing optimal scaling results for\nMetropolis--Hastings algorithms is that the assumptions on the target\ndistribution are unrealistic. In this paper, we consider optimal scaling of\nrandom-walk Metropolis algorithms on general target distributions in high\ndimensions arising from practical MCMC models from Bayesian statistics. For\noptimal scaling by maximizing expected squared jumping distance (ESJD), we show\nthe asymptotically optimal acceptance rate $0.234$ can be obtained under\ngeneral realistic sufficient conditions on the target distribution. The new\nsufficient conditions are easy to be verified and may hold for some general\nclasses of MCMC models arising from Bayesian statistics applications, which\nsubstantially generalize the product i.i.d. condition required in most existing\nliterature of optimal scaling. Furthermore, we show one-dimensional diffusion\nlimits can be obtained under slightly stronger conditions, which still allow\ndependent coordinates of the target distribution. We also connect the new\ndiffusion limit results to complexity bounds of Metropolis algorithms in high\ndimensions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 13:16:39 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 16:08:46 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 16:13:01 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yang", "Jun", ""], ["Roberts", "Gareth O.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1904.12204", "submitter": "Ionut-Gabriel Farcas", "authors": "Ionut-Gabriel Farcas, Jonas Latz, Elisabeth Ullmann, Tobias Neckel and\n  Hans-Joachim Bungartz", "title": "Multilevel adaptive sparse Leja approximations for Bayesian inverse\n  problems", "comments": "24 pages, 9 figures", "journal-ref": "SIAM J. Sci. Comput. 42(1), pp. A424-A451, 2020", "doi": "10.1137/19M1260293", "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic interpolation and quadrature methods are often unsuitable to\naddress Bayesian inverse problems depending on computationally expensive\nforward mathematical models. While interpolation may give precise posterior\napproximations, deterministic quadrature is usually unable to efficiently\ninvestigate an informative and thus concentrated likelihood. This leads to a\nlarge number of required expensive evaluations of the mathematical model. To\novercome these challenges, we formulate and test a multilevel adaptive sparse\nLeja algorithm. At each level, adaptive sparse grid interpolation and\nquadrature are used to approximate the posterior and perform all quadrature\noperations, respectively. Specifically, our algorithm uses coarse\ndiscretizations of the underlying mathematical model to investigate the\nparameter space and to identify areas of high posterior probability. Adaptive\nsparse grid algorithms are then used to place points in these areas, and ignore\nother areas of small posterior probability. The points are weighted Leja\npoints. As the model discretization is coarse, the construction of the sparse\ngrid is computationally efficient. On this sparse grid, the posterior measure\ncan be approximated accurately with few expensive, fine model discretizations.\nThe efficiency of the algorithm can be enhanced further by exploiting more than\ntwo discretization levels. We apply the proposed multilevel adaptive sparse\nLeja algorithm in numerical experiments involving elliptic inverse problems in\n2D and 3D space, in which we compare it with Markov chain Monte Carlo sampling\nand a standard multilevel approximation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:23:50 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 14:04:54 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Farcas", "Ionut-Gabriel", ""], ["Latz", "Jonas", ""], ["Ullmann", "Elisabeth", ""], ["Neckel", "Tobias", ""], ["Bungartz", "Hans-Joachim", ""]]}, {"id": "1904.13021", "submitter": "Anshui Li Mr", "authors": "Linghui Li, Anshui Li, Huizeng Zhang", "title": "On the parameter estimation of ARMA(p,q) model by approximate Bayesian\n  computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the parameter estimation of ARMA(p,q) model is given by\napproximate Bayesian computation algorithm. In order to improve the sampling\nefficiency of the algorithm, approximate Bayesian computation should select as\nmany statistics as possible with parameter information in low dimension.\nFirstly, we use the autocorrelation coefficient of the first p+q order sample\nas the statistic and obtain an approximate Bayesian estimation of the AR\ncoefficient, transforming the ARMA(p,q) model into the MA(q) model. Considering\nthe first q order sample autocorrelation functions and sample variance as the\nstatistics, the approximate Bayesian estimation of MA coefficient and white\nnoise variances can be given. The method mentioned above is more accurate and\npowerful than the maximum likelihood estimation, which is verified by the\nnumerical simulations and experiment study.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 02:33:03 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Li", "Linghui", ""], ["Li", "Anshui", ""], ["Zhang", "Huizeng", ""]]}]