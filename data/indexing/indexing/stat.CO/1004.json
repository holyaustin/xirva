[{"id": "1004.0524", "submitter": "Yunxiao He", "authors": "Yunxiao He and Chuanhai Liu", "title": "The Dynamic ECME Algorithm", "comments": "24 pages, 9 figures, and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ECME algorithm has proven to be an effective way of accelerating the EM\nalgorithm for many problems. Recognising the limitation of using prefixed\nacceleration subspace in ECME, we propose the new Dynamic ECME (DECME)\nalgorithm which allows the acceleration subspace to be chosen dynamically. Our\ninvestigation of an inefficient special case of DECME, the classical Successive\nOverrelaxation (SOR) method, leads to an efficient, simple, and widely\napplicable DECME implementation, called DECME_v1. The fast convergence of\nDECME_v1 is established by the theoretical result that, in a small\nneighbourhood of the maximum likelihood estimate (MLE), DECME_v1 is equivalent\nto a conjugate direction method. Numerical results show that DECME_v1 and its\ntwo variants are very stable and often converge faster than EM by a factor of\none hundred in terms of number of iterations and a factor of thirty in terms of\nCPU time when EM is very slow.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2010 18:21:57 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["He", "Yunxiao", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1004.0784", "submitter": "Jean-Michel Marin", "authors": "Yves Auffray and Pierre Barbillon and Jean-Michel Marin", "title": "Maximin design on non hypercube domain and kernel interpolation", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paradigm of computer experiments, the choice of an experimental design\nis an important issue. When no information is available about the black-box\nfunction to be approximated, an exploratory design have to be used. In this\ncontext, two dispersion criteria are usually considered: the minimax and the\nmaximin ones. In the case of a hypercube domain, a standard strategy consists\nof taking the maximin design within the class of Latin hypercube designs.\nHowever, in a non hypercube context, it does not make sense to use the Latin\nhypercube strategy. Moreover, whatever the design is, the black-box function is\ntypically approximated thanks to kernel interpolation. Here, we first provide a\ntheoretical justification to the maximin criterion with respect to kernel\ninterpolations. Then, we propose simulated annealing algorithms to determine\nmaximin designs in any bounded connected domain. We prove the convergence of\nthe different schemes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 07:03:34 GMT"}, {"version": "v2", "created": "Wed, 5 Jan 2011 01:26:50 GMT"}, {"version": "v3", "created": "Tue, 24 May 2011 16:51:52 GMT"}], "update_date": "2011-05-25", "authors_parsed": [["Auffray", "Yves", ""], ["Barbillon", "Pierre", ""], ["Marin", "Jean-Michel", ""]]}, {"id": "1004.0887", "submitter": "Guillem Rigaill", "authors": "Guillem Rigaill", "title": "A pruned dynamic programming algorithm to recover the best segmentations\n  with $1$ to $K_{max}$ change-points", "comments": "31 pages, An extended version of the pre-print", "journal-ref": "J-Sfds Vol. 156, No 4 2015 pgs. 180-205", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common computational problem in multiple change-point models is to recover\nthe segmentations with $1$ to $K_{max}$ change-points of minimal cost with\nrespect to some loss function. Here we present an algorithm to prune the set of\ncandidate change-points which is based on a functional representation of the\ncost of segmentations. We study the worst case complexity of the algorithm when\nthere is a unidimensional parameter per segment and demonstrate that it is at\nworst equivalent to the complexity of the segment neighbourhood algorithm:\n$\\mathcal{O}(K_{max} n^2)$. For a particular loss function we demonstrate that\npruning is on average efficient even if there are no change-points in the\nsignal. Finally, we empirically study the performance of the algorithm in the\ncase of the quadratic loss and show that it is faster than the segment\nneighbourhood algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 16:37:45 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 14:11:06 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Rigaill", "Guillem", ""]]}, {"id": "1004.1112", "submitter": "Dennis Prangle", "authors": "Paul Fearnhead and Dennis Prangle", "title": "Constructing Summary Statistics for Approximate Bayesian Computation:\n  Semi-automatic ABC", "comments": "v2: Revised in response to reviewer comments, adding more examples\n  and a method for inference from multiple data sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern statistical applications involve inference for complex stochastic\nmodels, where it is easy to simulate from the models, but impossible to\ncalculate likelihoods. Approximate Bayesian computation (ABC) is a method of\ninference for such models. It replaces calculation of the likelihood by a step\nwhich involves simulating artificial data for different parameter values, and\ncomparing summary statistics of the simulated data to summary statistics of the\nobserved data. Here we show how to construct appropriate summary statistics for\nABC in a semi-automatic manner. We aim for summary statistics which will enable\ninference about certain parameters of interest to be as accurate as possible.\nTheoretical results show that optimal summary statistics are the posterior\nmeans of the parameters. While these cannot be calculated analytically, we use\nan extra stage of simulation to estimate how the posterior means vary as a\nfunction of the data; and then use these estimates of our summary statistics\nwithin ABC. Empirical results show that our approach is a robust method for\nchoosing summary statistics, that can result in substantially more accurate ABC\nanalyses than the ad-hoc choices of summary statistics proposed in the\nliterature. We also demonstrate advantages over two alternative methods of\nsimulation-based inference.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 15:32:49 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2011 19:34:21 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Fearnhead", "Paul", ""], ["Prangle", "Dennis", ""]]}, {"id": "1004.1876", "submitter": "Satoshi Aoki", "authors": "Satoshi Aoki and Akimichi Takemura", "title": "Design and analysis of fractional factorial experiments from the\n  viewpoint of computational algebraic statistics", "comments": "16 pages", "journal-ref": "Journal of Statistical Theory and Practice, Vol. 6 (2012), No. 1,\n  147--161,", "doi": "10.1080/15598608.2012.647556", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an expository review of applications of computational algebraic\nstatistics to design and analysis of fractional factorial experiments based on\nour recent works. For the purpose of design, the techniques of Gr\\\"obner bases\nand indicator functions allow us to treat fractional factorial designs without\ndistinction between regular designs and non-regular designs. For the purpose of\nanalysis of data from fractional factorial designs, the techniques of Markov\nbases allow us to handle discrete observations. Thus the approach of\ncomputational algebraic statistics greatly enlarges the scope of fractional\nfactorial designs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 05:50:08 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Aoki", "Satoshi", ""], ["Takemura", "Akimichi", ""]]}, {"id": "1004.2548", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Mario V. W\\\"uthrich, Pavel V. Shevchenko", "title": "Chain ladder method: Bayesian bootstrap versus classical bootstrap", "comments": null, "journal-ref": "Insurance: Mathematics and Economics (2010)", "doi": "10.1016/j.insmatheco.2010.03.007", "report-no": null, "categories": "q-fin.CP q-fin.RM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intention of this paper is to estimate a Bayesian distribution-free chain\nladder (DFCL) model using approximate Bayesian computation (ABC) methodology.\nWe demonstrate how to estimate quantities of interest in claims reserving and\ncompare the estimates to those obtained from classical and credibility\napproaches. In this context, a novel numerical procedure utilising Markov chain\nMonte Carlo (MCMC), ABC and a Bayesian bootstrap procedure was developed in a\ntruly distribution-free setting. The ABC methodology arises because we work in\na distribution-free setting in which we make no parametric assumptions, meaning\nwe can not evaluate the likelihood point-wise or in this case simulate directly\nfrom the likelihood model. The use of a bootstrap procedure allows us to\ngenerate samples from the intractable likelihood without the requirement of\ndistributional assumptions, this is crucial to the ABC framework. The developed\nmethodology is used to obtain the empirical distribution of the DFCL model\nparameters and the predictive distribution of the outstanding loss liabilities\nconditional on the observed claims. We then estimate predictive Bayesian\ncapital estimates, the Value at Risk (VaR) and the mean square error of\nprediction (MSEP). The latter is compared with the classical bootstrap and\ncredibility methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2010 04:48:39 GMT"}], "update_date": "2010-04-16", "authors_parsed": [["Peters", "Gareth W.", ""], ["W\u00fcthrich", "Mario V.", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "1004.2840", "submitter": "Firas Hamze", "authors": "Firas Hamze, Neil Dickson, Kamran Karimi", "title": "Robust Parameter Selection for Parallel Tempering", "comments": "Accepted in International Journal of Modern Physics C 2010,\n  http://www.worldscinet.com/ijmpc", "journal-ref": null, "doi": "10.1142/S0129183110015361", "report-no": null, "categories": "cond-mat.other physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an algorithm for selecting parameter values (e.g.\ntemperature values) at which to measure equilibrium properties with Parallel\nTempering Monte Carlo simulation. Simple approaches to choosing parameter\nvalues can lead to poor equilibration of the simulation, especially for Ising\nspin systems that undergo $1^st$-order phase transitions. However, starting\nfrom an initial set of parameter values, the careful, iterative respacing of\nthese values based on results with the previous set of values greatly improves\nequilibration. Example spin systems presented here appear in the context of\nQuantum Monte Carlo.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2010 18:17:13 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Hamze", "Firas", ""], ["Dickson", "Neil", ""], ["Karimi", "Kamran", ""]]}, {"id": "1004.2910", "submitter": "Matthew Harrison", "authors": "Matthew T. Harrison", "title": "Conservative Hypothesis Tests and Confidence Intervals using Importance\n  Sampling", "comments": "26 pages, 3 figures, 3 tables [significant rewrite of version 1,\n  including additional examples, title change]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is a common technique for Monte Carlo approximation,\nincluding Monte Carlo approximation of p-values. Here it is shown that a simple\ncorrection of the usual importance sampling p-values creates valid p-values,\nmeaning that a hypothesis test created by rejecting the null when the p-value\nis <= alpha will also have a type I error rate <= alpha. This correction uses\nthe importance weight of the original observation, which gives valuable\ndiagnostic information under the null hypothesis. Using the corrected p-values\ncan be crucial for multiple testing and also in problems where evaluating the\naccuracy of importance sampling approximations is difficult. Inverting the\ncorrected p-values provides a useful way to create Monte Carlo confidence\nintervals that maintain the nominal significance level and use only a single\nMonte Carlo sample. Several applications are described, including accelerated\nmultiple testing for a large neurophysiological dataset and exact conditional\ninference for a logistic regression model with nuisance parameters.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2010 19:27:10 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2011 21:01:11 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Harrison", "Matthew T.", ""]]}, {"id": "1004.3105", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Fast normal random number generators on vector processors", "comments": "An old Technical Report, not published elsewhere. 6 pages. For\n  details see http://wwwmaths.anu.edu.au/~brent/pub/pub141.html", "journal-ref": null, "doi": null, "report-no": "Technical Report TR-CS-93-04, Computer Sciences Laboratory,\n  Australian National University, March 1993.", "categories": "cs.DS math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider pseudo-random number generators suitable for vector processors.\nIn particular, we describe vectorised implementations of the Box-Muller and\nPolar methods, and show that they give good performance on the Fujitsu VP2200.\nWe also consider some other popular methods, e.g. the Ratio method of Kinderman\nand Monahan (1977) (as improved by Leva (1992)), and the method of Von Neumann\nand Forsythe, and show why they are unlikely to be competitive with the Polar\nmethod on vector processors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 06:45:43 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2010 01:03:00 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3114", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "A fast vectorised implementation of Wallace's normal random number\n  generator", "comments": "An old Technical Report, not published elsewhere. 9 pages. For\n  further details see http://wwwmaths.anu.edu.au/~brent/pub/pub170.html", "journal-ref": null, "doi": null, "report-no": "Technical Report TR-CS-97-07, Computer Sciences Laboratory,\n  Australian National University, April 1997", "categories": "cs.DS math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wallace has proposed a new class of pseudo-random generators for normal\nvariates. These generators do not require a stream of uniform pseudo-random\nnumbers, except for initialisation. The inner loops are essentially\nmatrix-vector multiplications and are very suitable for implementation on\nvector processors or vector/parallel processors such as the Fujitsu VPP300. In\nthis report we outline Wallace's idea, consider some variations on it, and\ndescribe a vectorised implementation RANN4 which is more than three times\nfaster than its best competitors (the Polar and Box-Muller methods) on the\nFujitsu VP2200 and VPP300.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 07:37:44 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3115", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Some long-period random number generators using shifts and xors", "comments": "11 pages", "journal-ref": "ANZIAM Journal 48 (CTAC2006), C188-C202, 2007", "doi": null, "report-no": null, "categories": "cs.DS math.NT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marsaglia recently introduced a class of xorshift random number generators\n(RNGs) with periods 2n-1 for n = 32, 64, etc. Here we give a generalisation of\nMarsaglia's xorshift generators in order to obtain fast and high-quality RNGs\nwith extremely long periods. RNGs based on primitive trinomials may be\nunsatisfactory because a trinomial has very small weight. In contrast, our\ngenerators can be chosen so that their minimal polynomials have large weight\n(number of nonzero terms). A computer search using Magma has found good\ngenerators for n a power of two up to 4096. These have been implemented in a\nfree software package xorgens.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 07:52:08 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3616", "submitter": "Christian Meyer", "authors": "Christian Meyer", "title": "Recursive Numerical Evaluation of the Cumulative Bivariate Normal\n  Distribution", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": "10.18637/jss.v052.i10", "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for evaluation of the cumulative bivariate normal\ndistribution, building upon Marsaglia's ideas for evaluation of the cumulative\nunivariate normal distribution. The algorithm is mathematically transparent,\ndelivers competitive performance and can easily be extended to arbitrary\nprecision.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 04:39:45 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Meyer", "Christian", ""]]}, {"id": "1004.3830", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters and Balakrishnan Kannan and Ben Lasscock and Chris\n  Mellen", "title": "Model Selection and Adaptive Markov chain Monte Carlo for Bayesian\n  Cointegrated VAR model", "comments": "to appear journal Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PM q-fin.ST stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a matrix-variate adaptive Markov chain Monte Carlo (MCMC)\nmethodology for Bayesian Cointegrated Vector Auto Regressions (CVAR). We\nreplace the popular approach to sampling Bayesian CVAR models, involving griddy\nGibbs, with an automated efficient alternative, based on the Adaptive\nMetropolis algorithm of Roberts and Rosenthal, (2009). Developing the adaptive\nMCMC framework for Bayesian CVAR models allows for efficient estimation of\nposterior parameters in significantly higher dimensional CVAR series than\npreviously possible with existing griddy Gibbs samplers. For a n-dimensional\nCVAR series, the matrix-variate posterior is in dimension $3n^2 + n$, with\nsignificant correlation present between the blocks of matrix random variables.\nWe also treat the rank of the CVAR model as a random variable and perform joint\ninference on the rank and model parameters. This is achieved with a Bayesian\nposterior distribution defined over both the rank and the CVAR model\nparameters, and inference is made via Bayes Factor analysis of rank.\nPractically the adaptive sampler also aids in the development of automated\nBayesian cointegration models for algorithmic trading systems considering\ninstruments made up of several assets, such as currency baskets. Previously the\nliterature on financial applications of CVAR trading models typically only\nconsiders pairs trading (n=2) due to the computational cost of the griddy\nGibbs. We are able to extend under our adaptive framework to $n >> 2$ and\ndemonstrate an example with n = 10, resulting in a posterior distribution with\nparameters up to dimension 310. By also considering the rank as a random\nquantity we can ensure our resulting trading models are able to adjust to\npotentially time varying market conditions in a coherent statistical framework.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 02:26:17 GMT"}], "update_date": "2010-04-23", "authors_parsed": [["Peters", "Gareth W.", ""], ["Kannan", "Balakrishnan", ""], ["Lasscock", "Ben", ""], ["Mellen", "Chris", ""]]}, {"id": "1004.3871", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Susanne Ditlevsen", "title": "Practical Estimation of High Dimensional Stochastic Differential\n  Mixed-Effects Models", "comments": "Forthcoming in \"Computational Statistics & Data Analysis\"", "journal-ref": "Computational Statistics & Data Analysis, 2011, Volume 55, Issue\n  3, pages 1426-1444", "doi": "10.1016/j.csda.2010.10.003", "report-no": null, "categories": "stat.CO math.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) are established tools to model\nphysical phenomena whose dynamics are affected by random noise. By estimating\nparameters of an SDE intrinsic randomness of a system around its drift can be\nidentified and separated from the drift itself. When it is of interest to model\ndynamics within a given population, i.e. to model simultaneously the\nperformance of several experiments or subjects, mixed-effects modelling allows\nfor the distinction of between and within experiment variability. A framework\nto model dynamics within a population using SDEs is proposed, representing\nsimultaneously several sources of variation: variability between experiments\nusing a mixed-effects approach and stochasticity in the individual dynamics\nusing SDEs. These \"stochastic differential mixed-effects models\" have\napplications in e.g. pharmacokinetics/pharmacodynamics and biomedical\nmodelling. A parameter estimation method is proposed and computational\nguidelines for an efficient implementation are given. Finally the method is\nevaluated using simulations from standard models like the two-dimensional\nOrnstein-Uhlenbeck (OU) and the square root models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 09:23:58 GMT"}, {"version": "v2", "created": "Sun, 3 Oct 2010 15:17:39 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Picchini", "Umberto", ""], ["Ditlevsen", "Susanne", ""]]}, {"id": "1004.3895", "submitter": "Peter Ruckdeschel", "authors": "Peter Ruckdeschel", "title": "Optimally Robust Kalman Filtering at Work: AO-, IO-, and Simultaneously\n  IO- and AO- Robust Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take up optimality results for robust Kalman filtering from\nRuckdeschel[2001,2010] where robustness is understood in a distributional\nsense, i.e.; we enlarge the distribution assumptions made in the ideal model by\nsuitable neighborhoods, allowing for outliers which in our context may be\nsystem-endogenous/propagating or -exogenous/non-propagating, inducing the\nsomewhat conflicting goals of tracking and attenuation. Correspondingly, the\ncited references provide optimally-robust procedures to deal with each type of\noutliers separately, but in case of IO-robustness does not say much about the\nimplementation. We discuss this in more detail in this paper. Most importantly,\nwe define a hybrid filter combining AO- and IO-optimal ones, which is able to\ntreat both types of outliers simultaneously, albeit with a certain delay. We\ncheck our filters at a reference state space model, and compare the results\nwith those obtained by the ACM filter Martin and Masreliez[1977], Martin[1979]\nand non-parametric, repeated-median based filters Fried et al.[2006,2007].\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 12:06:56 GMT"}], "update_date": "2010-04-23", "authors_parsed": [["Ruckdeschel", "Peter", ""]]}, {"id": "1004.3925", "submitter": "Nial Friel", "authors": "Nial Friel and Anthony N. Pettitt", "title": "Classification using distance nearest neighbours", "comments": "12 pages, 2 figures. To appear in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-010-9179-y", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new probabilistic classification algorithm using a\nMarkov random field approach. The joint distribution of class labels is\nexplicitly modelled using the distances between feature vectors. Intuitively, a\nclass label should depend more on class labels which are closer in the feature\nspace, than those which are further away. Our approach builds on previous work\nby Holmes and Adams (2002, 2003) and Cucala et al. (2008). Our work shares many\nof the advantages of these approaches in providing a probabilistic basis for\nthe statistical inference. In comparison to previous work, we present a more\nefficient computational algorithm to overcome the intractability of the Markov\nrandom field model. The results of our algorithm are encouraging in comparison\nto the k-nearest neighbour algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 14:09:08 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2010 09:55:51 GMT"}], "update_date": "2010-06-02", "authors_parsed": [["Friel", "Nial", ""], ["Pettitt", "Anthony N.", ""]]}, {"id": "1004.4041", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori, Hiroaki Uehara, Masakazu Jimbo", "title": "Pooling Design and Bias Correction in DNA Library Screening", "comments": "18 pages, 1 figure, 8 tables, submitted.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the group test for DNA library screening based on probabilistic\napproach. Group test is a method of detecting a few positive items from among a\nlarge number of items, and has wide range of applications. In DNA library\nscreening, positive item corresponds to the clone having a specified DNA\nsegment, and it is necessary to identify and isolate the positive clones for\ncompiling the libraries. In the group test, a group of items, called pool, is\nassayed in a lump in order to save the cost of testing, and positive items are\ndetected based on the observation from each pool. It is known that the design\nof grouping, that is, pooling design is important to %reduce the estimation\nbias and achieve accurate detection. In the probabilistic approach, positive\nclones are picked up based on the posterior probability. Naive methods of\ncomputing the posterior, however, involves exponentially many sums, and thus we\nneed a device. Loopy belief propagation (loopy BP) algorithm is one of popular\nmethods to obtain approximate posterior probability efficiently. There are some\nworks investigating the relation between the accuracy of the loopy BP and the\npooling design. Based on these works, we develop pooling design with small\nestimation bias of posterior probability, and we show that the balanced\nincomplete block design (BIBD) has nice property for our purpose. Some\nnumerical experiments show that the bias correction under the BIBD is useful to\nimprove the estimation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 02:15:41 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2010 00:21:28 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Uehara", "Hiroaki", ""], ["Jimbo", "Masakazu", ""]]}, {"id": "1004.4116", "submitter": "Simon Tavar\\'e", "authors": "A. D. Barbour and Simon Tavar\\'e", "title": "Assessing molecular variability in cancer genomes", "comments": "22 pages, 1 figure. Chapter 4 of \"Probability and Mathematical\n  Genetics: Papers in Honour of Sir John Kingman\" (Editors N.H. Bingham and\n  C.M. Goldie), Cambridge University Press, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of tumour evolution are not well understood. In this paper we\nprovide a statistical framework for evaluating the molecular variation observed\nin different parts of a colorectal tumour. A multi-sample version of the Ewens\nSampling Formula forms the basis for our modelling of the data, and we provide\na simulation procedure for use in obtaining reference distributions for the\nstatistics of interest. We also describe the large-sample asymptotics of the\njoint distributions of the variation observed in different parts of the tumour.\nWhile actual data should be evaluated with reference to the simulation\nprocedure, the asymptotics serve to provide theoretical guidelines, for\ninstance with reference to the choice of possible statistics.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 22:07:53 GMT"}], "update_date": "2010-04-26", "authors_parsed": [["Barbour", "A. D.", ""], ["Tavar\u00e9", "Simon", ""]]}, {"id": "1004.4347", "submitter": "Guillem Rigaill", "authors": "Guillem Rigaill, Emilie Lebarbier, St\\'ephane Robin", "title": "Exact posterior distributions over the segmentation space and model\n  selection for multiple change-point detection problems", "comments": "15 pages", "journal-ref": "Statistics and Computing, Volume 22, Issue 4, pp 917-929 (2012)", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In segmentation problems, inference on change-point position and model\nselection are two difficult issues due to the discrete nature of change-points.\nIn a Bayesian context, we derive exact, non-asymptotic, explicit and tractable\nformulae for the posterior distribution of variables such as the number of\nchange-points or their positions. We also derive a new selection criterion that\naccounts for the reliability of the results. All these results are based on an\nefficient strategy to explore the whole segmentation space, which is very\nlarge. We illustrate our methodology on both simulated data and a comparative\ngenomic hybridisation profile.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2010 12:25:14 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Rigaill", "Guillem", ""], ["Lebarbier", "Emilie", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1004.5442", "submitter": "Aiguo Xu Dr.", "authors": "Feng Chen, Aiguo Xu, Guangcai Zhang, Yingjun Li and Sauro Succi", "title": "Multiple-Relaxation-Time Lattice Boltzmann Approach to Compressible\n  Flows with Flexible Specific-Heat Ratio and Prandtl Number", "comments": "Accepted for publication in EPL", "journal-ref": "EPL (Europhysics Letters) 90, 54003 (2010)", "doi": "10.1209/0295-5075/90/54003", "report-no": null, "categories": "cond-mat.soft cs.CE nlin.CG physics.comp-ph physics.flu-dyn stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new multiple-relaxation-time lattice Boltzmann scheme for compressible\nflows with arbitrary specific heat ratio and Prandtl number is presented. In\nthe new scheme, which is based on a two-dimensional 16-discrete-velocity model,\nthe moment space and the corresponding transformation matrix are constructed\naccording to the seven-moment relations associated with the local equilibrium\ndistribution function. In the continuum limit, the model recovers the\ncompressible Navier-Stokes equations with flexible specific-heat ratio and\nPrandtl number. Numerical experiments show that compressible flows with strong\nshocks can be simulated by the present model up to Mach numbers $Ma \\sim 5$.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 03:09:46 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2010 01:42:58 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Chen", "Feng", ""], ["Xu", "Aiguo", ""], ["Zhang", "Guangcai", ""], ["Li", "Yingjun", ""], ["Succi", "Sauro", ""]]}, {"id": "1004.5538", "submitter": "Francois Orieux", "authors": "Francois Orieux, Jean-Francois Giovannelli, Thomas Rodet", "title": "Bayesian estimation of regularization and PSF parameters for Wiener-Hunt\n  deconvolution", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.27.001593", "report-no": null, "categories": "stat.CO cs.CV physics.data-an stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper tackles the problem of image deconvolution with joint estimation\nof PSF parameters and hyperparameters. Within a Bayesian framework, the\nsolution is inferred via a global a posteriori law for unknown parameters and\nobject. The estimate is chosen as the posterior mean, numerically calculated by\nmeans of a Monte-Carlo Markov chain algorithm. The estimates are efficiently\ncomputed in the Fourier domain and the effectiveness of the method is shown on\nsimulated examples. Results show precise estimates for PSF parameters and\nhyperparameters as well as precise image estimates including restoration of\nhigh-frequencies and spatial details, within a global and coherent approach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 14:23:46 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Orieux", "Francois", ""], ["Giovannelli", "Jean-Francois", ""], ["Rodet", "Thomas", ""]]}]