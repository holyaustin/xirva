[{"id": "1610.00345", "submitter": "Daniel W. Meyer", "authors": "Daniel W. Meyer", "title": "Density Estimation with Distribution Element Trees", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-017-9751-9", "report-no": null, "categories": "stat.ME cs.MS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of probability densities based on available data is a central\ntask in many statistical applications. Especially in the case of large\nensembles with many samples or high-dimensional sample spaces, computationally\nefficient methods are needed. We propose a new method that is based on a\ndecomposition of the unknown distribution in terms of so-called distribution\nelements (DEs). These elements enable an adaptive and hierarchical\ndiscretization of the sample space with small or large elements in regions with\nsmoothly or highly variable densities, respectively. The novel refinement\nstrategy that we propose is based on statistical goodness-of-fit and pair-wise\n(as an approximation to mutual) independence tests that evaluate the local\napproximation of the distribution in terms of DEs. The capabilities of our new\nmethod are inspected based on several examples of different dimensionality and\nsuccessfully compared with other state-of-the-art density estimators.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 20:07:28 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 07:08:16 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Meyer", "Daniel W.", ""]]}, {"id": "1610.00773", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Bootstrap methods for stationary functional time series", "comments": "3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap methods for estimating the long-run covariance of stationary\nfunctional time series are considered. We introduce a versatile bootstrap\nmethod that relies on functional principal component analysis, where principal\ncomponent scores can be bootstrapped by maximum entropy. Two other bootstrap\nmethods resample error functions, after the dependence structure being modeled\nlinearly by a sieve method or nonlinearly by a functional kernel regression.\nThrough a series of Monte-Carlo simulation, we evaluate and compare the\nfinite-sample performances of these three bootstrap methods for estimating the\nlong-run covariance in a functional time series. Using the intraday particulate\nmatter (PM10) data set in Graz, the proposed bootstrap methods provide a way of\nconstructing the distribution of estimated long-run covariance for functional\ntime series.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 22:26:31 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1610.00781", "submitter": "Richard Norton", "authors": "Richard A. Norton and Colin Fox", "title": "Tuning of MCMC with Langevin, Hamiltonian, and other stochastic\n  autoregressive proposals", "comments": "44 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:1605.05441", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proposals for Metropolis-Hastings MCMC derived by discretizing Langevin\ndiffusion or Hamiltonian dynamics are examples of stochastic autoregressive\nproposals that form a natural wider class of proposals with equivalent\ncomputability. We analyze Metropolis-Hastings MCMC with stochastic\nautoregressive proposals applied to target distributions that are absolutely\ncontinuous with respect to some Gaussian distribution to derive expressions for\nexpected acceptance probability and expected jump size, as well as measures of\ncomputational cost, in the limit of high dimension. Thus, we are able to unify\nexisting analyzes for these classes of proposals, and to extend the theoretical\nresults that provide useful guidelines for tuning the proposals for optimal\ncomputational efficiency. For the simplified Langevin algorithm we find that it\nis optimal to take at least three steps of the proposal before the\nMetropolis-Hastings accept-reject step, and for Hamiltonian/hybrid Monte Carlo\nwe provide new guidelines for the optimal number of integration steps and\ncriteria for choosing the optimal mass matrix.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 22:53:26 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Norton", "Richard A.", ""], ["Fox", "Colin", ""]]}, {"id": "1610.00984", "submitter": "Amelia McNamara", "authors": "Amelia McNamara", "title": "On the State of Computing in Statistics Education: Tools for Learning\n  and for Doing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper lays out the current landscape of tools used in statistics\neducation. In particular, it considers graphing calculators, spreadsheets,\napplets and microworlds, standalone educational software, statistical\nprogramming tools, tools for reproducible research and bespoke tools. The\nstrengths and weaknesses of the tools are considered, particularly in the\ncontext of McNamara (2016)'s list of attributes for a statistical computing\ntool. Best practices for computing in introductory statistics are suggested.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 01:08:50 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["McNamara", "Amelia", ""]]}, {"id": "1610.00985", "submitter": "Amelia McNamara", "authors": "Amelia McNamara", "title": "Key attributes of a modern statistical computing tool", "comments": null, "journal-ref": null, "doi": "10.1080/00031305.2018.1482784", "report-no": null, "categories": "stat.CO cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 1990s, statisticians began thinking in a principled way about how\ncomputation could better support the learning and doing of statistics. Since\nthen, the pace of software development has accelerated, advancements in\ncomputing and data science have moved the goalposts, and it is time to\nreassess. Software continues to be developed to help do and learn statistics,\nbut there is little critical evaluation of the resulting tools, and no accepted\nframework with which to critique them. This paper presents a set of attributes\nnecessary for a modern statistical computing tool. The framework was designed\nto be broadly applicable to both novice and expert users, with a particular\nfocus on making more supportive statistical computing environments. A modern\nstatistical computing tool should be accessible, provide easy entry, privilege\ndata as a first-order object, support exploratory and confirmatory analysis,\nallow for flexible plot creation, support randomization, be interactive,\ninclude inherent documentation, support narrative, publishing, and\nreproducibility, and be flexible to extensions. Ideally, all these attributes\ncould be incorporated into one tool, supporting users at all levels, but a more\nreasonable goal is for tools designed for novices and professionals to `reach\nacross the gap,' taking inspiration from each others' strengths.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 00:56:16 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 21:04:53 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["McNamara", "Amelia", ""]]}, {"id": "1610.02106", "submitter": "Richard Norton", "authors": "Richard A. Norton, Colin Fox, Malcolm E. Morrison", "title": "Numerical approximation of the Frobenius-Perron operator using the\n  finite volume method", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a finite-dimensional approximation of the Frobenius-Perron\noperator using the finite volume method applied to the continuity equation for\nthe evolution of probability. A Courant-Friedrichs-Lewy condition ensures that\nthe approximation satisfies the Markov property, while existing convergence\ntheory for the finite volume method guarantees convergence of the discrete\noperator to the continuous operator as mesh size tends to zero. Properties of\nthe approximation are demonstrated in a computed example of sequential\ninference for the state of a low-dimensional mechanical system when\nobservations give rise to multi-modal distributions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 00:39:57 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Norton", "Richard A.", ""], ["Fox", "Colin", ""], ["Morrison", "Malcolm E.", ""]]}, {"id": "1610.02588", "submitter": "Yiyuan She", "authors": "Yiyuan She, Shao Tang", "title": "Iterative proportional scaling revisited: a modern optimization\n  perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the classic iterative proportional scaling (IPS) from a\nmodern optimization perspective. In contrast to the criticisms made in the\nliterature, we show that based on a coordinate descent characterization, IPS\ncan be slightly modified to deliver coefficient estimates, and from a\nmajorization-minimization standpoint, IPS can be extended to handle log-affine\nmodels with features not necessarily binary-valued or nonnegative. Furthermore,\nsome state-of-the-art optimization techniques such as block-wise computation,\nrandomization and momentum-based acceleration can be employed to provide more\nscalable IPS algorithms, as well as some regularized variants of IPS for\nconcurrent feature selection.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 22:06:58 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 19:30:01 GMT"}, {"version": "v3", "created": "Sat, 5 May 2018 00:47:30 GMT"}, {"version": "v4", "created": "Mon, 2 Jul 2018 23:46:08 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["She", "Yiyuan", ""], ["Tang", "Shao", ""]]}, {"id": "1610.03182", "submitter": "Rui Sun", "authors": "Rui Sun, Billy Chang, Benny Chung-Ying Zee and Maggie Haitian Wang", "title": "wtest: an R Package for Testing Main and Interaction Effect in Genotype\n  Data with Binary Traits", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This R package evaluates main and pair-wise interaction effect of single\nnucleotide polymorphisms (SNPs) via the W-test, scalable to whole genome-wide\ndata sets. The package provides fast and accurate p-value estimation of genetic\nmarkers, as well as diagnostic checking on the probability distributions. It\nallows flexible stage-wise or exhaustive association testing in a user-friendly\ninterface. Availability: The package is available in CRAN, or from website:\nhttp://www2.ccrb.cuhk.edu.hk/wtest\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 04:34:13 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Sun", "Rui", ""], ["Chang", "Billy", ""], ["Zee", "Benny Chung-Ying", ""], ["Wang", "Maggie Haitian", ""]]}, {"id": "1610.03434", "submitter": "Mathias Drton", "authors": "Mathias Drton, Christopher Fox and Y. Samuel Wang", "title": "Computation of maximum likelihood estimates in cyclic structural\n  equation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software for computation of maximum likelihood estimates in linear structural\nequation models typically employs general techniques from non-linear\noptimization, such as quasi-Newton methods. In practice, careful tuning of\ninitial values is often required to avoid convergence issues. As an alternative\napproach, we propose a block-coordinate descent method that cycles through the\nconsidered variables, updating only the parameters related to a given variable\nin each step. We show that the resulting block update problems can be solved in\nclosed form even when the structural equation model comprises feedback cycles.\nFurthermore, we give a characterization of the models for which the\nblock-coordinate descent algorithm is well-defined, meaning that for generic\ndata and starting values all block optimization problems admit a unique\nsolution. For the characterization, we represent each model by its mixed graph\n(also known as path diagram), which leads to criteria that can be checked in\ntime that is polynomial in the number of considered variables.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 17:31:06 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Drton", "Mathias", ""], ["Fox", "Christopher", ""], ["Wang", "Y. Samuel", ""]]}, {"id": "1610.03483", "submitter": "Balaji Lakshminarayanan", "authors": "Shakir Mohamed and Balaji Lakshminarayanan", "title": "Learning in Implicit Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) provide an algorithmic framework for\nconstructing generative models with several appealing properties: they do not\nrequire a likelihood function to be specified, only a generating procedure;\nthey provide samples that are sharp and compelling; and they allow us to\nharness our knowledge of building highly accurate neural network classifiers.\nHere, we develop our understanding of GANs with the aim of forming a rich view\nof this growing area of machine learning---to build connections to the diverse\nset of statistical thinking on this topic, of which much can be gained by a\nmutual exchange of ideas. We frame GANs within the wider landscape of\nalgorithms for learning in implicit generative models--models that only specify\na stochastic procedure with which to generate data--and relate these ideas to\nmodelling problems in related fields, such as econometrics and approximate\nBayesian computation. We develop likelihood-free inference methods and\nhighlight hypothesis testing as a principle for learning in implicit generative\nmodels, using which we are able to derive the objective function used by GANs,\nand many other related objectives. The testing viewpoint directs our focus to\nthe general problem of density ratio estimation. There are four approaches for\ndensity ratio estimation, one of which is a solution using classifiers to\ndistinguish real from generated data. Other approaches such as divergence\nminimisation and moment matching have also been explored in the GAN literature,\nand we synthesise these views to form an understanding in terms of the\nrelationships between them and the wider literature, highlighting avenues for\nfuture exploration and cross-pollination.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 19:59:39 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 15:11:20 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 17:44:52 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 05:47:30 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Mohamed", "Shakir", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "1610.03701", "submitter": "Sylvain Robert", "authors": "Sylvain Robert and Hans R. K\\\"unsch", "title": "Localization in High-Dimensional Monte Carlo Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO nlin.CD physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high dimensionality and computational constraints associated with\nfiltering problems in large-scale geophysical applications are particularly\nchallenging for the Particle Filter (PF). Approximate but efficient methods\nsuch as the Ensemble Kalman Filter (EnKF) are therefore usually preferred. A\nkey element of these approximate methods is localization, which is in principle\na general technique to avoid the curse of dimensionality and consists in\nlimiting the influence of observations to neighboring sites. However, while it\nworks effectively with the EnKF, localization introduces harmful\ndiscontinuities in the estimated physical fields when applied blindly to the\nPF. In the present paper, we explore two possible local algorithms based on the\nEnKPF, a hybrid method combining the EnKF and the PF. A simulation study in a\nconjugate normal setup allows to highlight the trade-offs involved when\napplying localization to PF type of algorithms in the high-dimensional setting.\nExperiments with the Lorenz96 model demonstrate the ability of the local EnKPF\nalgorithms to perform well even with a small number of particles compared to\nthe problem size.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 13:18:41 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 16:18:07 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Robert", "Sylvain", ""], ["K\u00fcnsch", "Hans R.", ""]]}, {"id": "1610.03939", "submitter": "Andrew J. Dolgert", "authors": "Andrew J. Dolgert", "title": "Continuous-Time, Discrete-Event Simulation from Counting Processes", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a method for discrete event simulation specified by survival\nanalysis. It presents a sequence of steps. First, hazard rates from survival\nanalysis specify the rates of a set of counting processes. Second, those\ncounting processes define a transition kernel. Third, there are four different\nways to sample that transition kernel, including a first-principles derivation\nof exact stochastic simulation algorithms (SSA) in continuous time. This\nsimulation allows time-dependent intensities which include both continuous and\natomic components. Separating the steps involved makes a clear correspondence\nbetween mathematical formulation and algorithmic implementation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 04:58:38 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Dolgert", "Andrew J.", ""]]}, {"id": "1610.04272", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Kim Batselier, Haotian Liu, Luca Daniel, Ngai Wong", "title": "Tensor Computation: A New Framework for High-Dimensional Problems in EDA", "comments": "14 figures. Accepted by IEEE Trans. CAD of Integrated Circuits and\n  Systems", "journal-ref": null, "doi": "10.1109/TCAD.2016.2618879", "report-no": null, "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many critical EDA problems suffer from the curse of dimensionality, i.e. the\nvery fast-scaling computational burden produced by large number of parameters\nand/or unknown variables. This phenomenon may be caused by multiple spatial or\ntemporal factors (e.g. 3-D field solvers discretizations and multi-rate circuit\nsimulation), nonlinearity of devices and circuits, large number of design or\noptimization parameters (e.g. full-chip routing/placement and circuit sizing),\nor extensive process variations (e.g. variability/reliability analysis and\ndesign for manufacturability). The computational challenges generated by such\nhigh dimensional problems are generally hard to handle efficiently with\ntraditional EDA core algorithms that are based on matrix and vector\ncomputation. This paper presents \"tensor computation\" as an alternative general\nframework for the development of efficient EDA algorithms and tools. A tensor\nis a high-dimensional generalization of a matrix and a vector, and is a natural\nchoice for both storing and solving efficiently high-dimensional EDA problems.\nThis paper gives a basic tutorial on tensors, demonstrates some recent examples\nof EDA applications (e.g., nonlinear circuit modeling and high-dimensional\nuncertainty quantification), and suggests further open EDA problems where the\nuse of tensor computation could be of advantage.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 21:56:14 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Zheng", ""], ["Batselier", "Kim", ""], ["Liu", "Haotian", ""], ["Daniel", "Luca", ""], ["Wong", "Ngai", ""]]}, {"id": "1610.04386", "submitter": "Kurt Cutajar", "authors": "Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi, Maurizio Filippone", "title": "Random Feature Expansions for Deep Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The composition of multiple Gaussian Processes as a Deep Gaussian Process\n(DGP) enables a deep probabilistic nonparametric approach to flexibly tackle\ncomplex machine learning problems with sound quantification of uncertainty.\nExisting inference approaches for DGP models have limited scalability and are\nnotoriously cumbersome to construct. In this work, we introduce a novel\nformulation of DGPs based on random feature expansions that we train using\nstochastic variational inference. This yields a practical learning framework\nwhich significantly advances the state-of-the-art in inference for DGPs, and\nenables accurate quantification of uncertainty. We extensively showcase the\nscalability and performance of our proposal on several datasets with up to 8\nmillion observations, and various DGP architectures with up to 30 hidden\nlayers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 09:56:17 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 14:06:21 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Cutajar", "Kurt", ""], ["Bonilla", "Edwin V.", ""], ["Michiardi", "Pietro", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1610.04397", "submitter": "Robert Pich\\'e", "authors": "Robert Piche", "title": "Automatic numerical differentiation by maximum likelihood estimation of\n  state-space model", "comments": "submitted to Automatica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A linear Gaussian state-space smoothing algorithm is presented for estimation\nof derivatives from a sequence of noisy measurements. The algorithm uses\nnumerically stable square-root formulas, can handle simultaneous independent\nmeasurements and non-equally spaced abscissas, and can compute state estimates\nat points between the data abscissas. The state space model's parameters,\nincluding driving noise intensity, measurement variance, and initial state, are\ndetermined from the given data sequence using maximum likelihood estimation\ncomputed using a expectation maximisation iteration. In tests with synthetic\nbiomechanics data, the algorithm has equivalent or better accuracy compared to\nother automatic numerical differentiation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 10:28:12 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Piche", "Robert", ""]]}, {"id": "1610.05108", "submitter": "Gian-Andrea Thanei", "authors": "Gian-Andrea Thanei, Nicolai Meinshausen, Rajen D. Shah", "title": "The xyz algorithm for fast interaction search in high-dimensional data", "comments": null, "journal-ref": "JMLR. Journal of Machine Learning Research. The xyz algorithm for\n  fast interaction search in high-dimensional data. Gian-Andrea Thanei, Nicolai\n  Meinshausen, Rajen D. Shah. 19.37.1.42. 2018", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing regression on a dataset with $p$ variables, it is often of\ninterest to go beyond using main linear effects and include interactions as\nproducts between individual variables. For small-scale problems, these\ninteractions can be computed explicitly but this leads to a computational\ncomplexity of at least $\\mathcal{O}(p^2)$ if done naively. This cost can be\nprohibitive if $p$ is very large. We introduce a new randomised algorithm that\nis able to discover interactions with high probability and under mild\nconditions has a runtime that is subquadratic in $p$. We show that strong\ninteractions can be discovered in almost linear time, whilst finding weaker\ninteractions requires $\\mathcal{O}(p^\\alpha)$ operations for $1 < \\alpha < 2$\ndepending on their strength. The underlying idea is to transform interaction\nsearch into a closestpair problem which can be solved efficiently in\nsubquadratic time. The algorithm is called $\\mathit{xyz}$ and is implemented in\nthe language R. We demonstrate its efficiency for application to genome-wide\nassociation studies, where more than $10^{11}$ interactions can be screened in\nunder $280$ seconds with a single-core $1.2$ GHz CPU.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 13:42:22 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 06:29:04 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 23:02:36 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 20:25:04 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Thanei", "Gian-Andrea", ""], ["Meinshausen", "Nicolai", ""], ["Shah", "Rajen D.", ""]]}, {"id": "1610.05246", "submitter": "Kai Zhang", "authors": "Kai Zhang", "title": "BET on Independence", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1537921", "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of nonparametric dependence detection. Many existing\nmethods may suffer severe power loss due to non-uniform consistency, which we\nillustrate with a paradox. To avoid such power loss, we approach the\nnonparametric test of independence through the new framework of binary\nexpansion statistics (BEStat) and binary expansion testing (BET), which examine\ndependence through a novel binary expansion filtration approximation of the\ncopula. Through a Hadamard transform, we find that the symmetry statistics in\nthe filtration are complete sufficient statistics for dependence. These\nstatistics are also uncorrelated under the null. By utilizing symmetry\nstatistics, the BET avoids the problem of non-uniform consistency and improves\nupon a wide class of commonly used methods (a) by achieving the minimax rate in\nsample size requirement for reliable power and (b) by providing clear\ninterpretations of global relationships upon rejection of independence. The\nbinary expansion approach also connects the symmetry statistics with the\ncurrent computing system to facilitate efficient bitwise implementation. We\nillustrate the BET with a study of the distribution of stars in the night sky\nand with an exploratory data analysis of the TCGA breast cancer data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 18:19:49 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 03:26:00 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2017 07:09:37 GMT"}, {"version": "v4", "created": "Sun, 23 Apr 2017 02:08:08 GMT"}, {"version": "v5", "created": "Mon, 20 Nov 2017 15:57:14 GMT"}, {"version": "v6", "created": "Sun, 13 May 2018 02:25:46 GMT"}, {"version": "v7", "created": "Mon, 15 Apr 2019 20:39:38 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Kai", ""]]}, {"id": "1610.05400", "submitter": "Arvind Saibaba", "authors": "Eric Chi, Liuiyi Hu, Arvind K. Saibaba, Arvind U. K. Rao", "title": "Going off the Grid: Iterative Model Selection for Biclustered Matrix\n  Completion", "comments": "42 pages, 7 figures. Supplementary material\n  (https://github.com/echi/IMS/blob/master/BMC_Supplement_JCGS.pdf) and codes\n  available (https://github.com/echi/IMS)", "journal-ref": "Journal of Computational and Graphical Statistics, 28(1):36--47,\n  2019", "doi": "10.1080/10618600.2018.1482763", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of performing matrix completion with side information\non row-by-row and column-by-column similarities. We build upon recent proposals\nfor matrix estimation with smoothness constraints with respect to row and\ncolumn graphs. We present a novel iterative procedure for directly minimizing\nan information criterion in order to select an appropriate amount row and\ncolumn smoothing, namely perform model selection. We also discuss how to\nexploit the special structure of the problem to scale up the estimation and\nmodel selection procedure via the Hutchinson estimator. We present simulation\nresults and an application to predicting associations in imaging-genomics\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 01:50:52 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 13:57:56 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chi", "Eric", ""], ["Hu", "Liuiyi", ""], ["Saibaba", "Arvind K.", ""], ["Rao", "Arvind U. K.", ""]]}, {"id": "1610.05660", "submitter": "Arampatzis Georgios", "authors": "Georgios Arampatzis, Daniel W\\\"alchli, Panagiotis Angelikopoulos,\n  Stephen Wu, Panagiotis Hadjidoukas and Petros Koumoutsakos", "title": "Langevin Diffusion for Population Based Sampling with an Application in\n  Bayesian Inference for Pharmacodynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for the efficient and robust sampling of the\nposterior probability distribution in Bayesian inference problems. The\nalgorithm combines the local search capabilities of the Manifold Metropolis\nAdjusted Langevin transition kernels with the advantages of global exploration\nby a population based sampling algorithm, the Transitional Markov Chain Monte\nCarlo (TMCMC). The Langevin diffusion process is determined by either the\nHessian or the Fisher Information of the target distribution with appropriate\nmodifications for non positive definiteness. The present methods is shown to be\nsuperior over other population based algorithms, in sampling probability\ndistributions for which gradients are available and is shown to handle\notherwise unidentifiable models. We demonstrate the capabilities and advantages\nof the method in computing the posterior distribution of the parameters in a\nPharmacodynamics model, for glioma growth and its drug induced inhibition,\nusing clinical data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 15:00:27 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 13:05:29 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Arampatzis", "Georgios", ""], ["W\u00e4lchli", "Daniel", ""], ["Angelikopoulos", "Panagiotis", ""], ["Wu", "Stephen", ""], ["Hadjidoukas", "Panagiotis", ""], ["Koumoutsakos", "Petros", ""]]}, {"id": "1610.05817", "submitter": "Julia Palacios Julia Palacios", "authors": "Michael D. Karcher and Julia A. Palacios and Shiwei Lan and Vladimir\n  N. Minin", "title": "phylodyn: an R package for phylodynamic simulation and inference", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce phylodyn, an R package for phylodynamic analysis based on gene\ngenealogies. The package main functionality is Bayesian nonparametric\nestimation of effective population size fluctuations over time. Our\nimplementation includes several Markov chain Monte Carlo-based methods and an\nintegrated nested Laplace approximation-based approach for phylodynamic\ninference that have been developed in recent years. Genealogical data describe\nthe timed ancestral relationships of individuals sampled from a population of\ninterest. Here, individuals are assumed to be sampled at the same point in time\n(isochronous sampling) or at different points in time (heterochronous\nsampling); in addition, sampling events can be modeled with preferential\nsampling, which means that the intensity of sampling events is allowed to\ndepend on the effective population size trajectory. We assume the coalescent\nand the sequentially Markov coalescent processes as generative models of\ngenealogies. We include several coalescent simulation functions that are useful\nfor testing our phylodynamics methods via simulation studies. We compare the\nperformance and outputs of various methods implemented in phylodyn and outline\ntheir strengths and weaknesses. R package phylodyn is available at\nhttps://github.com/mdkarcher/phylodyn.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 22:25:58 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Karcher", "Michael D.", ""], ["Palacios", "Julia A.", ""], ["Lan", "Shiwei", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1610.06632", "submitter": "Richard Norton", "authors": "Richard A. Norton, J. Andres Christen, Colin Fox", "title": "Sampling hyperparameters in hierarchical models: improving on Gibbs for\n  high-dimensional latent fields and large data sets", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider posterior sampling in the very common Bayesian hierarchical model\nin which observed data depends on high-dimensional latent variables that, in\nturn, depend on relatively few hyperparameters. When the full conditional over\nthe latent variables has a known form, the marginal posterior distribution over\nhyperparameters is accessible and can be sampled using a Markov chain Monte\nCarlo (MCMC) method on a low-dimensional parameter space. This may improve\ncomputational efficiency over standard Gibbs sampling since computation is not\nover the high-dimensional space of latent variables and correlations between\nhyperparameters and latent variables become irrelevant. When the marginal\nposterior over hyperparameters depends on a fixed-dimensional sufficient\nstatistic, precomputation of the sufficient statistic renders the cost of the\nlow-dimensional MCMC independent of data size. Then, when the hyperparameters\nare the primary variables of interest, inference may be performed in big-data\nsettings at modest cost. Moreover, since the form of the full conditional for\nthe latent variables does not depend on the form of the hyperprior\ndistribution, the method imposes no restriction on the hyperprior, unlike Gibbs\nsampling that typically requires conjugate distributions. We demonstrate these\nefficiency gains in four computed examples.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 00:21:30 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Norton", "Richard A.", ""], ["Christen", "J. Andres", ""], ["Fox", "Colin", ""]]}, {"id": "1610.06752", "submitter": "Francois-Xavier Briol", "authors": "Francois-Xavier Briol, Jon Cockayne and Onur Teymur", "title": "Comments on \"Bayesian Solution Uncertainty Quantification for\n  Differential Equations\" by Chkrebtii, Campbell, Calderhead & Girolami", "comments": null, "journal-ref": "Bayesian Analysis, Vol 11, Num 4, pp1285-1293, 2016", "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We commend the authors for an exciting paper which provides a strong\ncontribution to the emerging field of probabilistic numerics (PN). Below, we\ndiscuss aspects of prior modelling which need to be considered thoroughly in\nfuture work.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 12:07:47 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Briol", "Francois-Xavier", ""], ["Cockayne", "Jon", ""], ["Teymur", "Onur", ""]]}, {"id": "1610.07310", "submitter": "Rodrigo Canales", "authors": "Rodrigo Canales, Elmar Peise, Paolo Bientinesi", "title": "Large Scale Parallel Computations in R through Elemental", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though in recent years the scale of statistical analysis problems has\nincreased tremendously, many statistical software tools are still limited to\nsingle-node computations. However, statistical analyses are largely based on\ndense linear algebra operations, which have been deeply studied, optimized and\nparallelized in the high-performance-computing community. To make\nhigh-performance distributed computations available for statistical analysis,\nand thus enable large scale statistical computations, we introduce RElem, an\nopen source package that integrates the distributed dense linear algebra\nlibrary Elemental into R. While on the one hand, RElem provides direct wrappers\nof Elemental's routines, on the other hand, it overloads various operators and\nfunctions to provide an entirely native R experience for distributed\ncomputations. We showcase how simple it is to port existing R programs to Relem\nand demonstrate that Relem indeed allows to scale beyond the single-node\nlimitation of R with the full performance of Elemental without any overhead.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 07:30:27 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Canales", "Rodrigo", ""], ["Peise", "Elmar", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1610.07894", "submitter": "Ivan Fernandez-Val", "authors": "Mingli Chen, Victor Chernozhukov, Iv\\'an Fern\\'andez-Val, and Blaise\n  Melly", "title": "Counterfactual: An R Package for Counterfactual Analysis", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Counterfactual package implements the estimation and inference methods of\nChernozhukov, Fern\\'andez-Val and Melly (2013) for counterfactual analysis. The\ncounterfactual distributions considered are the result of changing either the\nmarginal distribution of covariates related to the outcome variable of\ninterest, or the conditional distribution of the outcome given the covariates.\nThey can be applied to estimate quantile treatment effects and wage\ndecompositions. This paper serves as an introduction to the package and\ndisplays basic functionality of the commands contained within.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 14:32:40 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Chen", "Mingli", ""], ["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Melly", "Blaise", ""]]}, {"id": "1610.08088", "submitter": "Art Owen", "authors": "K. Gao and A. B. Owen", "title": "Estimation and Inference for Very Large Linear Mixed Effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed models with large imbalanced crossed random effects structures\npose severe computational problems for maximum likelihood estimation and for\nBayesian analysis. The costs can grow as fast as $N^{3/2}$ when there are N\nobservations. Such problems arise in any setting where the underlying factors\nsatisfy a many to many relationship (instead of a nested one) and in electronic\ncommerce applications, the N can be quite large. Methods that do not account\nfor the correlation structure can greatly underestimate uncertainty. We propose\na method of moments approach that takes account of the correlation structure\nand that can be computed at O(N) cost. The method of moments is very amenable\nto parallel computation and it does not require parametric distributional\nassumptions, tuning parameters or convergence diagnostics. For the regression\ncoefficients, we give conditions for consistency and asymptotic normality as\nwell as a consistent variance estimate. For the variance components, we give\nconditions for consistency and we use consistent estimates of a mildly\nconservative variance estimate. All of these computations can be done in O(N)\nwork. We illustrate the algorithm with some data from Stitch Fix where the\ncrossed random effects correspond to clients and items.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 20:38:24 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 23:28:55 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Gao", "K.", ""], ["Owen", "A. B.", ""]]}, {"id": "1610.08329", "submitter": "Ivan Fernandez-Val", "authors": "Michael Lipsitz, Alexandre Belloni, Victor Chernozhukov, and Iv\\'an\n  Fern\\'andez-Val", "title": "quantreg.nonpar: An R Package for Performing Nonparametric Series\n  Quantile Regression", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package quantreg.nonpar implements nonparametric quantile regression\nmethods to estimate and make inference on partially linear quantile models.\nquantreg.nonpar obtains point estimates of the conditional quantile function\nand its derivatives based on series approximations to the nonparametric part of\nthe model. It also provides pointwise and uniform confidence intervals over a\nregion of covariate values and/or quantile indices for the same functions using\nanalytical and resampling methods. This paper serves as an introduction to the\npackage and displays basic functionality of the functions contained within.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 13:48:39 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Lipsitz", "Michael", ""], ["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""]]}, {"id": "1610.08363", "submitter": "Jon Cockayne", "authors": "Jon Cockayne", "title": "Comments on \"Bayesian Solution Uncertainty Quantification for\n  Differential Equations\" by Chkrebtii, Campbell, Calderhead & Girolami", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I would like to thank the authors for their interesting and very clearly\npresented paper discussing probabilistic solvers for ODEs and PDEs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 11:48:39 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Cockayne", "Jon", ""]]}, {"id": "1610.08417", "submitter": "Onur Teymur", "authors": "Onur Teymur, Konstantinos Zygalakis, Ben Calderhead", "title": "Probabilistic Linear Multistep Methods", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": "Advances in Neural Information Processing Systems 29 (2016) pp.\n  4321-4328", "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a derivation and theoretical investigation of the Adams-Bashforth\nand Adams-Moulton family of linear multistep methods for solving ordinary\ndifferential equations, starting from a Gaussian process (GP) framework. In the\nlimit, this formulation coincides with the classical deterministic methods,\nwhich have been used as higher-order initial value problem solvers for over a\ncentury. Furthermore, the natural probabilistic framework provided by the GP\nformulation allows us to derive probabilistic versions of these methods, in the\nspirit of a number of other probabilistic ODE solvers presented in the recent\nliterature. In contrast to higher-order Runge-Kutta methods, which require\nmultiple intermediate function evaluations per step, Adams family methods make\nuse of previous function evaluations, so that increased accuracy arising from a\nhigher-order multistep approach comes at very little additional computational\ncost. We show that through a careful choice of covariance function for the GP,\nthe posterior mean and standard deviation over the numerical solution can be\nmade to exactly coincide with the value given by the deterministic method and\nits local truncation error respectively. We provide a rigorous proof of the\nconvergence of these new methods, as well as an empirical investigation (up to\nfifth order) demonstrating their convergence rates in practice.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:53:32 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Teymur", "Onur", ""], ["Zygalakis", "Konstantinos", ""], ["Calderhead", "Ben", ""]]}, {"id": "1610.08865", "submitter": "Yasin Abbasi-Yadkori", "authors": "Yasin Abbasi-Yadkori and Peter L. Bartlett and Victor Gabillon and\n  Alan Malek", "title": "Hit-and-Run for Sampling and Planning in Non-Convex Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Hit-and-Run algorithm for planning and sampling problems in\nnon-convex spaces. For sampling, we show the first analysis of the Hit-and-Run\nalgorithm in non-convex spaces and show that it mixes fast as long as certain\nsmoothness conditions are satisfied. In particular, our analysis reveals an\nintriguing connection between fast mixing and the existence of smooth\nmeasure-preserving mappings from a convex space to the non-convex space. For\nplanning, we show advantages of Hit-and-Run compared to state-of-the-art\nplanning methods such as Rapidly-Exploring Random Trees.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 04:39:53 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""], ["Bartlett", "Peter L.", ""], ["Gabillon", "Victor", ""], ["Malek", "Alan", ""]]}, {"id": "1610.08962", "submitter": "Axel Finke", "authors": "Axel Finke, Arnaud Doucet, Adam M. Johansen", "title": "On embedded hidden Markov models and particle Markov chain Monte Carlo\n  methods", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The embedded hidden Markov model (EHMM) sampling method is a Markov chain\nMonte Carlo (MCMC) technique for state inference in non-linear non-Gaussian\nstate-space models which was proposed in Neal (2003); Neal et al. (2004) and\nextended in Shestopaloff and Neal (2016). An extension to Bayesian parameter\ninference was presented in Shestopaloff and Neal (2013). An alternative class\nof MCMC schemes addressing similar inference problems is provided by particle\nMCMC (PMCMC) methods (Andrieu et al. 2009; 2010). All these methods rely on the\nintroduction of artificial extended target distributions for multiple state\nsequences which, by construction, are such that one randomly indexed sequence\nis distributed according to the posterior of interest. By adapting the\nMetropolis-Hastings algorithms developed in the framework of PMCMC methods to\nthe EHMM framework, we obtain novel particle filter (PF)-type algorithms for\nstate inference and novel MCMC schemes for parameter and state inference. In\naddition, we show that most of these algorithms can be viewed as particular\ncases of a general PF and PMCMC framework. We compare the empirical performance\nof the various algorithms on low- to high-dimensional state-space models. We\ndemonstrate that a properly tuned conditional PF with \"local\" MCMC moves\nproposed in Shestopaloff and Neal (2016) can outperform the standard\nconditional PF significantly when applied to high-dimensional state-space\nmodels while the novel PF-type algorithm could prove to be an interesting\nalternative to standard PFs for likelihood estimation in some lower-dimensional\nscenarios.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 19:54:12 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Finke", "Axel", ""], ["Doucet", "Arnaud", ""], ["Johansen", "Adam M.", ""]]}, {"id": "1610.09005", "submitter": "Vincent Brault", "authors": "Vincent Brault and Antoine Channarond", "title": "Fast and Consistent Algorithm for the Latent Block Model", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the algorithm $Largest$ $Gaps$ is introduced, for\nsimultaneously clustering both rows and columns of a matrix to form homogeneous\nblocks. The definition of clustering is model-based: clusters and data are\ngenerated under the Latent Block Model. In comparison with algorithms designed\nfor this model, the major advantage of the $Largest$ $Gaps$ algorithm is to\ncluster using only some marginals of the matrix, the size of which is much\nsmaller than the whole matrix. The procedure is linear with respect to the\nnumber of entries and thus much faster than the classical algorithms. It\nsimultaneously selects the number of classes as well, and the estimation of the\nparameters is then made very easily once the classification is obtained.\nMoreover, the paper proves the procedure to be consistent under the LBM, and it\nillustrates the statistical performance with some numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 20:36:58 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Brault", "Vincent", ""], ["Channarond", "Antoine", ""]]}, {"id": "1610.09033", "submitter": "Jaan Altosaar", "authors": "Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei", "title": "Operator Variational Inference", "comments": "Appears in Neural Information Processing Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is an umbrella term for algorithms which cast Bayesian\ninference as optimization. Classically, variational inference uses the\nKullback-Leibler divergence to define the optimization. Though this divergence\nhas been widely used, the resultant posterior approximation can suffer from\nundesirable statistical properties. To address this, we reexamine variational\ninference from its roots as an optimization problem. We use operators, or\nfunctions of functions, to design variational objectives. As one example, we\ndesign a variational objective with a Langevin-Stein operator. We develop a\nblack box algorithm, operator variational inference (OPVI), for optimizing any\noperator objective. Importantly, operators enable us to make explicit the\nstatistical and computational tradeoffs for variational inference. We can\ncharacterize different properties of variational objectives, such as objectives\nthat admit data subsampling---allowing inference to scale to massive data---as\nwell as objectives that admit variational programs---a rich class of posterior\napproximations that does not require a tractable density. We illustrate the\nbenefits of OPVI on a mixture model and a generative model of images.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:32:25 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 23:58:43 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 01:08:06 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Altosaar", "Jaan", ""], ["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1610.09400", "submitter": "Yongjia Song", "authors": "Qiong Zhang and Yongjia Song", "title": "Moment Matching Based Conjugacy Approximation for Bayesian Ranking and\n  Selection", "comments": null, "journal-ref": null, "doi": "10.1145/3149013", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the conjugacy approximation method in the context of Bayesian\nranking and selection with unknown correlations. Under the assumption of\nnormal-inverse-Wishart prior distribution, the posterior distribution remains a\nnormal-inverse-Wishart distribution thanks to the conjugacy property when all\nalternatives are sampled at each step. However, this conjugacy property no\nlonger holds if only one alternative is sampled at a time, an appropriate\nsetting when there is a limited budget on the number of samples. We propose two\nnew conjugacy approximation methods based on the idea of moment matching. Both\nof them yield closed-form Bayesian prior updating formulas. This updating\nformula can then be combined with the knowledge gradient algorithm under the\n\"value of information\" framework. We conduct computational experiments to show\nthe superiority of the proposed conjugacy approximation methods, including\napplications in wind farm placement and computer model calibration.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 20:57:36 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Zhang", "Qiong", ""], ["Song", "Yongjia", ""]]}, {"id": "1610.09572", "submitter": "Harish S. Bhat", "authors": "Harish S. Bhat and R. W. M. A. Madushani", "title": "Density Tracking by Quadrature for Stochastic Differential Equations", "comments": "42 pages, 2 figures, minor revisions made to previous version,\n  comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze a method, density tracking by quadrature (DTQ), to\ncompute the probability density function of the solution of a stochastic\ndifferential equation. The derivation of the method begins with the\ndiscretization in time of the stochastic differential equation, resulting in a\ndiscrete-time Markov chain with continuous state space. At each time step, DTQ\napplies quadrature to solve the Chapman-Kolmogorov equation for this Markov\nchain. In this paper, we focus on a particular case of the DTQ method that\narises from applying the Euler-Maruyama method in time and the trapezoidal\nquadrature rule in space. Our main result establishes that the density computed\nby DTQ converges in $L^1$ to both the exact density of the Markov chain (with\nexponential convergence rate), and to the exact density of the stochastic\ndifferential equation (with first-order convergence rate). We establish a\nChernoff bound that implies convergence of a domain-truncated version of DTQ.\nWe carry out numerical tests to show that the empirical performance of DTQ\nmatches theoretical results, and also to demonstrate that DTQ can compute\ndensities several times faster than a Fokker-Planck solver, for the same level\nof error.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 21:43:03 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 18:31:46 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 21:58:39 GMT"}, {"version": "v4", "created": "Sat, 3 Mar 2018 03:24:41 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Bhat", "Harish S.", ""], ["Madushani", "R. W. M. A.", ""]]}, {"id": "1610.09641", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias and Omiros Papaspiliopoulos", "title": "Auxiliary gradient-based sampling algorithms", "comments": "41 pages, 8 figures, 11 tables, To appear in Journal of the Royal\n  Statistical Society: Series B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of MCMC samplers that combine auxiliary variables,\nGibbs sampling and Taylor expansions of the target density. Our approach\npermits the marginalisation over the auxiliary variables yielding marginal\nsamplers, or the augmentation of the auxiliary variables, yielding auxiliary\nsamplers. The well-known Metropolis-adjusted Langevin algorithm (MALA) and\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm are shown to be special\ncases. We prove that marginal samplers are superior in terms of asymptotic\nvariance and demonstrate cases where they are slower in computing time compared\nto auxiliary samplers. In the context of latent Gaussian models we propose new\nauxiliary and marginal samplers whose implementation requires a single tuning\nparameter, which can be found automatically during the transient phase.\nExtensive experimentation shows that the increase in efficiency (measured as\neffective sample size per unit of computing time) relative to (optimised\nimplementations of) pCNL, elliptical slice sampling and MALA ranges from\n10-fold in binary classification problems to 25-fold in log-Gaussian Cox\nprocesses to 100-fold in Gaussian process regression, and it is on par with\nRiemann manifold Hamiltonian Monte Carlo in an example where the latter has the\nsame complexity as the aforementioned algorithms. We explain this remarkable\nimprovement in terms of the way alternative samplers try to approximate the\neigenvalues of the target. We introduce a novel MCMC sampling scheme for\nhyperparameter learning that builds upon the auxiliary samplers. The MATLAB\ncode for reproducing the experiments in the article is publicly available and a\nSupplement to this article contains additional experiments and implementation\ndetails.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 12:13:15 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 16:03:52 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 16:38:21 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Papaspiliopoulos", "Omiros", ""]]}, {"id": "1610.09724", "submitter": "Sandipan Roy", "authors": "Sandipan Roy, Yves Atchad\\'e and George Michailidis", "title": "Likelihood Inference for Large Scale Stochastic Blockmodels with\n  Covariates based on a Divide-and-Conquer Parallelizable Algorithm with\n  Communication", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic blockmodel equipped with node covariate information,\nthat is helpful in analyzing social network data. The key objective is to\nobtain maximum likelihood estimates of the model parameters. For this task, we\ndevise a fast, scalable Monte Carlo EM type algorithm based on case-control\napproximation of the log-likelihood coupled with a subsampling approach. A key\nfeature of the proposed algorithm is its parallelizability, by processing\nportions of the data on several cores, while leveraging communication of key\nstatistics across the cores during each iteration of the algorithm. The\nperformance of the algorithm is evaluated on synthetic data sets and compared\nwith competing methods for blockmodel parameter estimation. We also illustrate\nthe model on data from a Facebook derived social network enhanced with node\ncovariate information.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 22:57:43 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 14:28:06 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 16:01:01 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Roy", "Sandipan", ""], ["Atchad\u00e9", "Yves", ""], ["Michailidis", "George", ""]]}, {"id": "1610.09787", "submitter": "Dustin Tran", "authors": "Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja Rudolph, Dawen Liang,\n  David M. Blei", "title": "Edward: A library for probabilistic modeling, inference, and criticism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.PL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling is a powerful approach for analyzing empirical\ninformation. We describe Edward, a library for probabilistic modeling. Edward's\ndesign reflects an iterative process pioneered by George Box: build a model of\na phenomenon, make inferences about the model given data, and criticize the\nmodel's fit to the data. Edward supports a broad class of probabilistic models,\nefficient algorithms for inference, and many techniques for model criticism.\nThe library builds on top of TensorFlow to support distributed training and\nhardware such as GPUs. Edward enables the development of complex probabilistic\nmodels and their algorithms at a massive scale.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 04:56:13 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 06:47:13 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 01:37:04 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Tran", "Dustin", ""], ["Kucukelbir", "Alp", ""], ["Dieng", "Adji B.", ""], ["Rudolph", "Maja", ""], ["Liang", "Dawen", ""], ["Blei", "David M.", ""]]}, {"id": "1610.09788", "submitter": "Alexandre Thiery", "authors": "Chris Sherlock, Alexandre Thiery and Anthony Lee", "title": "Pseudo-marginal Metropolis--Hastings using averages of unbiased\n  estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a pseudo-marginal Metropolis--Hastings kernel $P_m$ that is\nconstructed using an average of $m$ exchangeable random variables, as well as\nan analogous kernel $P_s$ that averages $s<m$ of these same random variables.\nUsing an embedding technique to facilitate comparisons, we show that the\nasymptotic variances of ergodic averages associated with $P_m$ are lower\nbounded in terms of those associated with $P_s$. We show that the bound\nprovided is tight and disprove a conjecture that when the random variables to\nbe averaged are independent, the asymptotic variance under $P_m$ is never less\nthan $s/m$ times the variance under $P_s$. The conjecture does, however, hold\nwhen considering continuous-time Markov chains. These results imply that if the\ncomputational cost of the algorithm is proportional to $m$, it is often better\nto set $m=1$. We provide intuition as to why these findings differ so markedly\nfrom recent results for pseudo-marginal kernels employing particle filter\napproximations. Our results are exemplified through two simulation studies; in\nthe first the computational cost is effectively proportional to $m$ and in the\nsecond there is a considerable start-up cost at each iteration.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 05:01:09 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Sherlock", "Chris", ""], ["Thiery", "Alexandre", ""], ["Lee", "Anthony", ""]]}]