[{"id": "0708.0711", "submitter": "C. P. Robert", "authors": "R. Douc, A. Guillin, J.-M. Marin, C. P. Robert", "title": "Convergence of adaptive mixtures of importance sampling schemes", "comments": "Published at http://dx.doi.org/10.1214/009053606000001154 in the\n  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics (2007), Vol. 35, No. 1, 420-448", "doi": "10.1214/009053606000001154", "report-no": "IMS-AOS-AOS0211", "categories": "math.ST stat.CO stat.TH", "license": null, "abstract": "  In the design of efficient simulation algorithms, one is often beset with a\npoor choice of proposal distributions. Although the performance of a given\nsimulation kernel can clarify a posteriori how adequate this kernel is for the\nproblem at hand, a permanent on-line modification of kernels causes concerns\nabout the validity of the resulting algorithm. While the issue is most often\nintractable for MCMC algorithms, the equivalent version for importance sampling\nalgorithms can be validated quite precisely. We derive sufficient convergence\nconditions for adaptive mixtures of population Monte Carlo algorithms and show\nthat Rao--Blackwellized versions asymptotically achieve an optimum in terms of\na Kullback divergence criterion, while more rudimentary versions do not benefit\nfrom repeated updating.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2007 07:39:35 GMT"}], "update_date": "2010-10-11", "authors_parsed": [["Douc", "R.", ""], ["Guillin", "A.", ""], ["Marin", "J. -M.", ""], ["Robert", "C. P.", ""]]}, {"id": "0708.1051", "submitter": "Colin Mallows", "authors": "Colin Mallows", "title": "Deconvolution by simulation", "comments": "Published at http://dx.doi.org/10.1214/074921707000000021 in the IMS\n  Lecture Notes Monograph Series\n  (http://www.imstat.org/publications/lecnotes.htm) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "IMS Lecture Notes Monograph Series 2007, Vol. 54, 1-11", "doi": "10.1214/074921707000000021", "report-no": "IMS-LNMS54-LNMS5401", "categories": "stat.CO", "license": null, "abstract": "  Given samples (x_1,...,x_m) and (z_1,...,z_n) which we believe are\nindependent realizations of random variables X and Z respectively, where we\nfurther believe that Z=X+Y with Y independent of X, the problem is to estimate\nthe distribution of Y. We present a new method for doing this, involving\nsimulation. Experiments suggest that the method provides useful estimates.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2007 07:15:05 GMT"}], "update_date": "2007-08-22", "authors_parsed": [["Mallows", "Colin", ""]]}, {"id": "0708.1485", "submitter": "Rob Tibshirani", "authors": "Jerome Friedman, Trevor Hastie, Holger H\\\"ofling, Robert Tibshirani", "title": "Pathwise coordinate optimization", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS131 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2007, Vol. 1, No. 2, 302-332", "doi": "10.1214/07-AOAS131", "report-no": "IMS-AOAS-AOAS131", "categories": "stat.CO math.OC", "license": null, "abstract": "  We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class\nof convex optimization problems. An algorithm of this kind has been proposed\nfor the $L_1$-penalized regression (lasso) in the literature, but it seems to\nhave been largely ignored. Indeed, it seems that coordinate-wise algorithms are\nnot often used in convex optimization. We show that this algorithm is very\ncompetitive with the well-known LARS (or homotopy) procedure in large lasso\nproblems, and that it can be applied to related methods such as the garotte and\nelastic net. It turns out that coordinate-wise descent does not work in the\n``fused lasso,'' however, so we derive a generalized algorithm that yields the\nsolution in much less time that a standard convex optimizer. Finally, we\ngeneralize the procedure to the two-dimensional fused lasso, and demonstrate\nits performance on some image smoothing problems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2007 16:54:30 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2007 07:44:22 GMT"}], "update_date": "2007-12-18", "authors_parsed": [["Friedman", "Jerome", ""], ["Hastie", "Trevor", ""], ["H\u00f6fling", "Holger", ""], ["Tibshirani", "Robert", ""]]}, {"id": "0708.1593", "submitter": "Adom Giffin", "authors": "Adom Giffin and Ariel Caticha", "title": "Updating Probabilities with Data and Moments", "comments": "Presented at the 27th International Workshop on Bayesian Inference\n  and Maximum Entropy Methods in Science and Engineering, Saratoga Springs, NY,\n  July 8-13, 2007. 10 pages, 1 figure V2 has a small typo in the end of the\n  appendix that was fixed. aj=mj+1 is now aj=m(k-j)+1", "journal-ref": null, "doi": "10.1063/1.2821302", "report-no": null, "categories": "physics.data-an cs.IT math.IT math.ST physics.comp-ph physics.pop-ph stat.AP stat.CO stat.ME stat.TH", "license": null, "abstract": "  We use the method of Maximum (relative) Entropy to process information in the\nform of observed data and moment constraints. The generic \"canonical\" form of\nthe posterior distribution for the problem of simultaneous updating with data\nand moments is obtained. We discuss the general problem of non-commuting\nconstraints, when they should be processed sequentially and when\nsimultaneously. As an illustration, the multinomial example of die tosses is\nsolved in detail for two superficially similar but actually very different\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2007 19:25:41 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2007 20:23:25 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Giffin", "Adom", ""], ["Caticha", "Ariel", ""]]}, {"id": "0708.4152", "submitter": "Cheng-Der Fuh", "authors": "Cheng-Der Fuh, Inchi Hu", "title": "Estimation in hidden Markov models via efficient importance sampling", "comments": "Published at http://dx.doi.org/10.3150/07--BEJ5163 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2007, Vol. 13, No. 2, 492-513", "doi": "10.3150/07--BEJ5163", "report-no": "IMS-BEJ-BEJ5163", "categories": "stat.CO math.ST stat.TH", "license": null, "abstract": "  Given a sequence of observations from a discrete-time, finite-state hidden\nMarkov model, we would like to estimate the sampling distribution of a\nstatistic. The bootstrap method is employed to approximate the confidence\nregions of a multi-dimensional parameter. We propose an importance sampling\nformula for efficient simulation in this context. Our approach consists of\nconstructing a locally asymptotically normal (LAN) family of probability\ndistributions around the default resampling rule and then minimizing the\nasymptotic variance within the LAN family. The solution of this minimization\nproblem characterizes the asymptotically optimal resampling scheme, which is\ngiven by a tilting formula. The implementation of the tilting formula is\nfacilitated by solving a Poisson equation. A few numerical examples are given\nto demonstrate the efficiency of the proposed importance sampling scheme.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2007 13:06:33 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Fuh", "Cheng-Der", ""], ["Hu", "Inchi", ""]]}]