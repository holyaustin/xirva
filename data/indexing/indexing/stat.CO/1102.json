[{"id": "1102.0927", "submitter": "Fabio Rapallo", "authors": "Fabio Rapallo", "title": "Outliers and patterns of outliers in contingency tables with Algebraic\n  Statistics", "comments": "24 pages, several examples and comments added in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a definition of pattern of outliers in contingency\ntables within a model-based framework. In particular, we make use of log-linear\nmodels and exact goodness-of-fit tests to specify the notions of outlier and\npattern of outliers. The language and some techniques from Algebraic Statistics\nare essential tools to make the definition clear and easily applicable. Some\nnumerical examples show how to use our definitions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 14:38:50 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2011 07:40:41 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Rapallo", "Fabio", ""]]}, {"id": "1102.0996", "submitter": "Charles Keeton", "authors": "Charles R. Keeton", "title": "On statistical uncertainty in nested sampling", "comments": "9 pages, 4 figures; accepted to MNRAS", "journal-ref": null, "doi": "10.1111/j.1365-2966.2011.18474.x", "report-no": null, "categories": "astro-ph.IM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested sampling has emerged as a valuable tool for Bayesian analysis, in\nparticular for determining the Bayesian evidence. The method is based on a\nspecific type of random sampling of the likelihood function and prior volume of\nthe parameter space. I study the statistical uncertainty in the evidence\ncomputed with nested sampling. I examine the uncertainty estimator from\nSkilling (2004, 2006) and introduce a new estimator based on a detailed\nanalysis of the statistical properties of nested sampling. Both perform well in\ntest cases and make it possible to obtain the statistical uncertainty in the\nevidence with no additional computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 19:07:57 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Keeton", "Charles R.", ""]]}, {"id": "1102.1796", "submitter": "Alexandre Lung-Yut-Fong", "authors": "Alexandre Lung-Yut-Fong (LTCI), C\\'eline L\\'evy-Leduc (LTCI), Olivier\n  Capp\\'e (LTCI)", "title": "Robust Retrospective Multiple Change-point Estimation for Multivariate\n  Data", "comments": "submitted to IEEE Workshop on Statistical Signal Processing 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric statistical procedure for detecting multiple\nchange-points in multidimensional signals. The method is based on a test\nstatistic that generalizes the well-known Kruskal-Wallis procedure to the\nmultivariate setting. The proposed approach does not require any knowledge\nabout the distribution of the observations and is parameter-free. It is\ncomputationally efficient thanks to the use of dynamic programming and can also\nbe applied when the number of change-points is unknown. The method is shown\nthrough simulations to be more robust than alternatives, particularly when\nfaced with atypical distributions (e.g., with outliers), high noise levels\nand/or high-dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Feb 2011 07:38:55 GMT"}, {"version": "v2", "created": "Thu, 10 Feb 2011 20:07:46 GMT"}], "update_date": "2011-02-11", "authors_parsed": [["Lung-Yut-Fong", "Alexandre", "", "LTCI"], ["L\u00e9vy-Leduc", "C\u00e9line", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"]]}, {"id": "1102.2171", "submitter": "Krzysztof Latuszynski", "authors": "Krzysztof Latuszynski and Gareth O. Roberts", "title": "CLTs and asymptotic variance of time-sampled Markov chains", "comments": "A small simulation illustrating theoretical results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a Markov transition kernel $P$ and a probability distribution $ \\mu$ on\nnonnegative integers, a time-sampled Markov chain evolves according to the\ntransition kernel $P_{\\mu} = \\sum_k \\mu(k)P^k.$ In this note we obtain CLT\nconditions for time-sampled Markov chains and derive a spectral formula for the\nasymptotic variance. Using these results we compare efficiency of Barker's and\nMetropolis algorithms in terms of asymptotic variance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Feb 2011 16:58:37 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2011 20:55:28 GMT"}], "update_date": "2011-06-07", "authors_parsed": [["Latuszynski", "Krzysztof", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1102.2280", "submitter": "Constantinos Daskalakis", "authors": "Constantinos Daskalakis and Christos H. Papadimitriou", "title": "On Oblivious PTAS's for Nash Equilibrium", "comments": "extended version of paper of the same title that appeared in STOC\n  2009", "journal-ref": "STOC 2009", "doi": null, "report-no": null, "categories": "cs.GT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a game has a Nash equilibrium with probability values that are either zero\nor Omega(1) then this equilibrium can be found exhaustively in polynomial time.\nSomewhat surprisingly, we show that there is a PTAS for the games whose\nequilibria are guaranteed to have small-O(1/n)-values, and therefore\nlarge-Omega(n)-supports. We also point out that there is a PTAS for games with\nsparse payoff matrices, which are known to be PPAD-complete to solve exactly.\nBoth algorithms are of a special kind that we call oblivious: The algorithm\njust samples a fixed distribution on pairs of mixed strategies, and the game is\nonly used to determine whether the sampled strategies comprise an eps-Nash\nequilibrium; the answer is yes with inverse polynomial probability. These\nresults bring about the question: Is there an oblivious PTAS for Nash\nequilibrium in general games? We answer this question in the negative; our\nlower bound comes close to the quasi-polynomial upper bound of [Lipton,\nMarkakis, Mehta 2003].\n  Another recent PTAS for anonymous games is also oblivious in a weaker sense\nappropriate for this class of games (it samples from a fixed distribution on\nunordered collections of mixed strategies), but its runtime is exponential in\n1/eps. We prove that any oblivious PTAS for anonymous games with two strategies\nand three player types must have 1/eps^c in the exponent of the running time\nfor some c>1/3, rendering the algorithm in [Daskalakis 2008] essentially\noptimal within oblivious algorithms. In contrast, we devise a poly(n)\n(1/eps)^O(log^2(1/eps)) non-oblivious PTAS for anonymous games with 2\nstrategies and any bounded number of player types.\n  Our algorithm is based on the construction of a sparse (and efficiently\ncomputable) eps-cover of the set of all possible sums of n independent\nindicators, under the total variation distance. The size of the cover is\npoly(n) (1/ eps^{O(log^2 (1/eps))}.\n", "versions": [{"version": "v1", "created": "Fri, 11 Feb 2011 04:42:36 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Papadimitriou", "Christos H.", ""]]}, {"id": "1102.2878", "submitter": "Dongryeol Lee", "authors": "Dongryeol Lee, Alexander G. Gray, and Andrew W. Moore", "title": "Dual-Tree Fast Gauss Transforms", "comments": "Extended version of a conference paper. Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel density estimation (KDE) is a popular statistical technique for\nestimating the underlying density distribution with minimal assumptions.\nAlthough they can be shown to achieve asymptotic estimation optimality for any\ninput distribution, cross-validating for an optimal parameter requires\nsignificant computation dominated by kernel summations. In this paper we\npresent an improvement to the dual-tree algorithm, the first practical kernel\nsummation algorithm for general dimension. Our extension is based on the\nseries-expansion for the Gaussian kernel used by fast Gauss transform. First,\nwe derive two additional analytical machinery for extending the original\nalgorithm to utilize a hierarchical data structure, demonstrating the first\ntruly hierarchical fast Gauss transform. Second, we show how to integrate the\nseries-expansion approximation within the dual-tree approach to compute kernel\nsummations with a user-controllable relative error bound. We evaluate our\nalgorithm on real-world datasets in the context of optimal bandwidth selection\nin kernel density estimation. Our results demonstrate that our new algorithm is\nthe only one that guarantees a hard relative error bound and offers fast\nperformance across a wide range of bandwidths evaluated in cross validation\nprocedures.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 20:24:01 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Lee", "Dongryeol", ""], ["Gray", "Alexander G.", ""], ["Moore", "Andrew W.", ""]]}, {"id": "1102.3582", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Pavel Shevchenko, Mark Young, Wendy Yip", "title": "Analytic Loss Distributional Approach Model for Operational Risk from\n  the alpha-Stable Doubly Stochastic Compound Processes and Implications for\n  Capital Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the Basel II standards, the Operational Risk (OpRisk) advanced\nmeasurement approach is not prescriptive regarding the class of statistical\nmodel utilised to undertake capital estimation. It has however become well\naccepted to utlise a Loss Distributional Approach (LDA) paradigm to model the\nindividual OpRisk loss process corresponding to the Basel II Business\nline/event type. In this paper we derive a novel class of doubly stochastic\nalpha-stable family LDA models. These models provide the ability to capture the\nheavy tailed loss process typical of OpRisk whilst also providing analytic\nexpressions for the compound process annual loss density and distributions as\nwell as the aggregated compound process annual loss models. In particular we\ndevelop models of the annual loss process in two scenarios. The first scenario\nconsiders the loss process with a stochastic intensity parameter, resulting in\nan inhomogeneous compound Poisson processes annually. The resulting arrival\nprocess of losses under such a model will have independent counts over\nincrements within the year. The second scenario considers discretization of the\nannual loss process into monthly increments with dependent time increments as\ncaptured by a Binomial process with a stochastic probability of success\nchanging annually. Each of these models will be coupled under an LDA framework\nwith heavy-tailed severity models comprised of $\\alpha$-stable severities for\nthe loss amounts per loss event. In this paper we will derive analytic results\nfor the annual loss distribution density and distribution under each of these\nmodels and study their properties.\n", "versions": [{"version": "v1", "created": "Thu, 17 Feb 2011 13:38:56 GMT"}], "update_date": "2011-02-18", "authors_parsed": [["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel", ""], ["Young", "Mark", ""], ["Yip", "Wendy", ""]]}, {"id": "1102.4152", "submitter": "Sabyasachi Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay and Sourabh Bhattacharya (Bayesian and\n  Interdisciplinary Research Unit, Indian Statistical Institute)", "title": "Perfect Simulation for Mixtures with Known and Unknown Number of\n  components", "comments": "This updated version is accepted for publication in Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and develop a novel and effective perfect sampling methodology for\nsimulating from posteriors corresponding to mixtures with either known (fixed)\nor unknown number of components. For the latter we consider the Dirichlet\nprocess-based mixture model developed by these authors, and show that our ideas\nare applicable to conjugate, and importantly, to non-conjugate cases. As to be\nexpected, and, as we show, perfect sampling for mixtures with known number of\ncomponents can be achieved with much less effort with a simplified version of\nour general methodology, whether or not conjugate or non-conjugate priors are\nused. While no special assumption is necessary in the conjugate set-up for our\ntheory to work, we require the assumption of bounded parameter space in the\nnon-conjugate set-up. However, we argue, with appropriate analytical,\nsimulation, and real data studies as support, that such boundedness assumption\nis not unrealistic and is not an impediment in practice. Not only do we\nvalidate our ideas theoretically and with simulation studies, but we also\nconsider application of our proposal to three real data sets used by several\nauthors in the past in connection with mixture models. The results we achieved\nin each of our experiments with either simulation study or real data\napplication, are quite encouraging.\n", "versions": [{"version": "v1", "created": "Mon, 21 Feb 2011 07:47:53 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2012 15:02:40 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", "", "Bayesian and\n  Interdisciplinary Research Unit, Indian Statistical Institute"], ["Bhattacharya", "Sourabh", "", "Bayesian and\n  Interdisciplinary Research Unit, Indian Statistical Institute"]]}, {"id": "1102.4432", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (University Paris-Dauphine), Jean-Marie Cornuet\n  (INRA, Montpellier), Jean-Michel Marin (I3M, Montpellier) and Natesh Pillai\n  (Harvard University)", "title": "Lack of confidence in ABC model choice", "comments": "9 pages, 7 figures, 1 table, second revision, submitted to the\n  Proceedings of the National Academy of Sciences, extension of arXiv:1101.5091", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) have become a essential tool for the\nanalysis of complex stochastic models. Earlier, Grelaud et al. (2009) advocated\nthe use of ABC for Bayesian model choice in the specific case of Gibbs random\nfields, relying on a inter-model sufficiency property to show that the\napproximation was legitimate. Having implemented ABC-based model choice in a\nwide range of phylogenetic models in the DIY-ABC software (Cornuet et al.,\n2008), we now present theoretical background as to why a generic use of ABC for\nmodel choice is ungrounded, since it depends on an unknown amount of\ninformation loss induced by the use of insufficient summary statistics. The\napproximation error of the posterior probabilities of the models under\ncomparison may thus be unrelated with the computational effort spent in running\nan ABC algorithm. We then conclude that additional empirical verifications of\nthe performances of the ABC procedure as those available in DIYABC are\nnecessary to conduct model choice.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 08:58:59 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2011 13:25:31 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2011 08:16:05 GMT"}, {"version": "v4", "created": "Mon, 20 Jun 2011 21:12:27 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Robert", "Christian P.", "", "University Paris-Dauphine"], ["Cornuet", "Jean-Marie", "", "INRA, Montpellier"], ["Marin", "Jean-Michel", "", "I3M, Montpellier"], ["Pillai", "Natesh", "", "Harvard University"]]}, {"id": "1102.4668", "submitter": "Alexandre Janon", "authors": "Alexandre Janon (INRIA Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann,\n  - M\\'ethodes d'Analyse Stochastique des Codes et Traitements Num\\'eriques,\n  UJF), Ma\\\"elle Nodet (INRIA Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann),\n  Cl\\'ementine Prieur (INRIA Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann, -\n  M\\'ethodes d'Analyse Stochastique des Codes et Traitements Num\\'eriques, UJF)", "title": "Confidence intervals for sensitivity indices using reduced-basis\n  metamodels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis is often impracticable for complex and time\ndemanding numerical models, as it requires a large number of runs. The\nreduced-basis approach provides a way to replace the original model by a much\nfaster to run code. In this paper, we are interested in the information loss\ninduced by the approximation on the estimation of sensitivity indices. We\npresent a method to provide a robust error assessment, hence enabling\nsignificant time savings without sacrifice on precision and rigourousness. We\nillustrate our method with an experiment where computation time is divided by a\nfactor of nearly 6. We also give directions on tuning some of the parameters\nused in our estimation algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 06:19:57 GMT"}], "update_date": "2011-02-25", "authors_parsed": [["Janon", "Alexandre", "", "INRIA Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann,\n  - M\u00e9thodes d'Analyse Stochastique des Codes et Traitements Num\u00e9riques,\n  UJF"], ["Nodet", "Ma\u00eblle", "", "INRIA Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann"], ["Prieur", "Cl\u00e9mentine", "", "INRIA Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann, -\n  M\u00e9thodes d'Analyse Stochastique des Codes et Traitements Num\u00e9riques, UJF"]]}, {"id": "1102.4816", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Computationally efficient algorithms for statistical image processing.\n  Implementation in R", "comments": "This paper initially appeared in 2010 as EURANDOM Report 2010-053.\n  Link to EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/053-report.pdf Link to the abstract\n  at EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/053-abstract.pdf", "journal-ref": null, "doi": null, "report-no": "EURANDOM Report 2010-053", "categories": "stat.CO cs.CV stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the series of our earlier papers on the subject, we proposed a novel\nstatistical hypothesis testing method for detection of objects in noisy images.\nThe method uses results from percolation theory and random graph theory. We\ndeveloped algorithms that allowed to detect objects of unknown shapes in the\npresence of nonparametric noise of unknown level and of unknown distribution.\nNo boundary shape constraints were imposed on the objects, only a weak bulk\ncondition for the object's interior was required. Our algorithms have linear\ncomplexity and exponential accuracy. In the present paper, we describe an\nimplementation of our nonparametric hypothesis testing method. We provide a\nprogram that can be used for statistical experiments in image processing. This\nprogram is written in the statistical programming language R.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 18:46:56 GMT"}], "update_date": "2011-02-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}]