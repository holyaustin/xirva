[{"id": "1706.00098", "submitter": "Lei Sun", "authors": "Nicholas G. Polson and Lei Sun", "title": "Bayesian $l_0$-regularized Least Squares", "comments": "22 pages, 6 figures, 1 table", "journal-ref": null, "doi": "10.1002/asmb.2381", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian $l_0$-regularized least squares is a variable selection technique\nfor high dimensional predictors. The challenge is optimizing a non-convex\nobjective function via search over model space consisting of all possible\npredictor combinations. Spike-and-slab (a.k.a. Bernoulli-Gaussian) priors are\nthe gold standard for Bayesian variable selection, with a caveat of\ncomputational speed and scalability. Single Best Replacement (SBR) provides a\nfast scalable alternative. We provide a link between Bayesian regularization\nand proximal updating, which provides an equivalence between finding a\nposterior mode and a posterior mean with a different regularization prior. This\nallows us to use SBR to find the spike-and-slab estimator. To illustrate our\nmethodology, we provide simulation evidence and a real data example on the\nstatistical properties and computational efficiency of SBR versus direct\nposterior sampling using spike-and-slab priors. Finally, we conclude with\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 21:29:40 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 17:10:53 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Sun", "Lei", ""]]}, {"id": "1706.00348", "submitter": "David Schnoerr", "authors": "David Schnoerr, Botond Cseke, Ramon Grima and Guido Sanguinetti", "title": "Efficient Low-Order Approximation of First-Passage Time Distributions", "comments": "5 pages, 3 figures", "journal-ref": "Phys. Rev. Lett. 119, 210601 (2017)", "doi": "10.1103/PhysRevLett.119.210601", "report-no": null, "categories": "physics.comp-ph physics.bio-ph q-bio.QM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing first-passage time distributions for\nreaction processes modelled by master equations. We show that this generally\nintractable class of problems is equivalent to a sequential Bayesian inference\nproblem for an auxiliary observation process. The solution can be approximated\nefficiently by solving a closed set of coupled ordinary differential equations\n(for the low-order moments of the process) whose size scales with the number of\nspecies. We apply it to an epidemic model and a trimerisation process, and show\ngood agreement with stochastic simulations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 15:26:29 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 16:07:34 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Schnoerr", "David", ""], ["Cseke", "Botond", ""], ["Grima", "Ramon", ""], ["Sanguinetti", "Guido", ""]]}, {"id": "1706.00689", "submitter": "Philip Maybank", "authors": "Philip Maybank, Ingo Bojak, Richard G. Everitt", "title": "Fast approximate Bayesian inference for stable differential equation\n  models", "comments": "39 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for mechanistic models is challenging because of nonlinear\ninteractions between model parameters and a lack of identifiability. Here we\nfocus on a specific class of mechanistic models, which we term stable\ndifferential equations. The dynamics in these models are approximately linear\naround a stable fixed point of the system. We exploit this property to develop\nfast approximate methods for posterior inference. We illustrate our approach\nusing simulated data on a mechanistic neuroscience model with EEG data. More\ngenerally, stable differential equation models and the corresponding inference\nmethods are useful for analysis of stationary time-series data. Compared to the\nexisting state-of-the art, our methods are several orders of magnitude faster,\nand are particularly suited to analysis of long time-series (>10,000\ntime-points) and models of moderate dimension (10-50 state variables and 10-50\nparameters.)\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 14:00:04 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 10:47:59 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Maybank", "Philip", ""], ["Bojak", "Ingo", ""], ["Everitt", "Richard G.", ""]]}, {"id": "1706.00824", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko and Vasanthan Raghavan", "title": "Comparative Performance Analysis of the Cumulative Sum Chart and the\n  Shiryaev-Roberts Procedure for Detecting Changes in Autocorrelated Data", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of quickest change-point detection where the\nobservations form a first-order autoregressive (AR) process driven by\ntemporally independent standard Gaussian noise. Subject to possible change are\nboth the drift of the AR(1) process ($\\mu$) as well as its correlation\ncoefficient ($\\lambda$), both known. The change is abrupt and persistent, and\nis of known magnitude, with $\\vert\\lambda\\vert<1$ throughout. For this\nscenario, we carry out a comparative performance analysis of the popular\nCumulative Sum (CUSUM) chart and its less well-known but worthy competitor --\nthe Shiryaev-Roberts (SR) procedure. Specifically, the performance is measured\nthrough Pollak's Supremum (conditional) Average Delay to Detection (SADD)\nconstrained to a pre-specified level of the Average Run Length (ARL) to false\nalarm. Particular attention is drawn to the sensitivity of each procedure's\nSADD and ARL with respect to the value of $\\lambda$ before and after the\nchange. The performance is studied through the solution of the respective\nintegral renewal equations obtained via Monte Carlo simulations. The\nsimulations are designed to estimate the sought performance metrics in an\nunbiased and asymptotically strongly consistent manner, and to within a\nprescribed proportional closeness (also asymptotically). Our extensive\nnumerical studies suggest that both the CUSUM chart and the SR procedure are\nasymptotically second-order optimal, even though the CUSUM chart is found to be\nslightly better than the SR procedure, irrespective of the model parameters.\nMoreover, the existence of a worst-case post-change correlation parameter\ncorresponding to the poorest detectability of the change for a given ARL to\nfalse alarm is established as well. To the best of our knowledge, this is the\nfirst time the performance of the SR procedure is studied for autocorrelated\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 19:26:24 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Raghavan", "Vasanthan", ""]]}, {"id": "1706.01260", "submitter": "Raphael Clifford", "authors": "Peter Clifford and Rapha\\\"el Clifford", "title": "The Classical Complexity of Boson Sampling", "comments": "15 pages. To appear in SODA '18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical complexity of the exact Boson Sampling problem where\nthe objective is to produce provably correct random samples from a particular\nquantum mechanical distribution. The computational framework was proposed by\nAaronson and Arkhipov in 2011 as an attainable demonstration of `quantum\nsupremacy', that is a practical quantum computing experiment able to produce\noutput at a speed beyond the reach of classical (that is non-quantum) computer\nhardware. Since its introduction Boson Sampling has been the subject of intense\ninternational research in the world of quantum computing. On the face of it,\nthe problem is challenging for classical computation. Aaronson and Arkhipov\nshow that exact Boson Sampling is not efficiently solvable by a classical\ncomputer unless $P^{\\#P} = BPP^{NP}$ and the polynomial hierarchy collapses to\nthe third level.\n  The fastest known exact classical algorithm for the standard Boson Sampling\nproblem takes $O({m + n -1 \\choose n} n 2^n )$ time to produce samples for a\nsystem with input size $n$ and $m$ output modes, making it infeasible for\nanything but the smallest values of $n$ and $m$. We give an algorithm that is\nmuch faster, running in $O(n 2^n + \\operatorname{poly}(m,n))$ time and $O(m)$\nadditional space. The algorithm is simple to implement and has low constant\nfactor overheads. As a consequence our classical algorithm is able to solve the\nexact Boson Sampling problem for system sizes far beyond current photonic\nquantum computing experimentation, thereby significantly reducing the\nlikelihood of achieving near-term quantum supremacy in the context of Boson\nSampling.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 10:31:00 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 20:12:57 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Clifford", "Peter", ""], ["Clifford", "Rapha\u00ebl", ""]]}, {"id": "1706.01435", "submitter": "Marco Broccardo", "authors": "Ziqi Wang, Marco Broccardo, Junho Song", "title": "Hamiltonian Monte Carlo Methods for Subset Simulation in Reliability\n  Analysis", "comments": "35 pages, 14 figures, submitted to Structural Safety", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a non-random-walk Markov Chain Monte Carlo method, namely\nthe Hamiltonian Monte Carlo (HMC) method in the context of Subset Simulation\nused for structural reliability analysis. The HMC method relies on a\ndeterministic mechanism inspired by Hamiltonian dynamics to propose samples\nfollowing a target probability distribution. The method alleviates the random\nwalk behavior to achieve a more effective and consistent exploration of the\nprobability space compared to standard Gibbs or Metropolis-Hastings techniques.\nAfter a brief review of the basic concepts of the HMC method and its\ncomputational details, two algorithms are proposed to facilitate the\napplication of the HMC method to Subset Simulation in structural reliability\nanalysis. Next, the behavior of the two HMC algorithms is illustrated using\nsimple probability distribution models. Finally, the accuracy and efficiency of\nSubset Simulation employing the two HMC algorithms are tested using various\nreliability examples. The supporting source code and data are available for\ndownload at (the URL that will become available once the paper is accepted).\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:40:38 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 15:44:48 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Wang", "Ziqi", ""], ["Broccardo", "Marco", ""], ["Song", "Junho", ""]]}, {"id": "1706.01478", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "An optimal $(\\epsilon,\\delta)$-approximation scheme for the mean of\n  random variables with bounded relative variance", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized approximation algorithms for many #P-complete problems (such as\nthe partition function of a Gibbs distribution, the volume of a convex body,\nthe permanent of a $\\{0,1\\}$-matrix, and many others) reduce to creating random\nvariables $X_1,X_2,\\ldots$ with finite mean $\\mu$ and standard\ndeviation$\\sigma$ such that $\\mu$ is the solution for the problem input, and\nthe relative standard deviation $|\\sigma/\\mu| \\leq c$ for known $c$. Under\nthese circumstances, it is known that the number of samples from the $\\{X_i\\}$\nneeded to form an $(\\epsilon,\\delta)$-approximation $\\hat \\mu$ that satisfies\n$\\mathbb{P}(|\\hat \\mu - \\mu| > \\epsilon \\mu) \\leq \\delta$ is at least\n$(2-o(1))\\epsilon^{-2} c^2\\ln(1/\\delta)$. We present here an easy to implement\n$(\\epsilon,\\delta)$-approximation $\\hat \\mu$ that uses\n$(2+o(1))c^2\\epsilon^{-2}\\ln(1/\\delta)$ samples. This achieves the same optimal\nrunning time as other estimators, but without the need for extra conditions\nsuch as bounds on third or fourth moments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 18:09:36 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 17:43:13 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "1706.01629", "submitter": "Sanjib Sharma", "authors": "Sanjib Sharma", "title": "Markov Chain Monte Carlo Methods for Bayesian Data Analysis in Astronomy", "comments": "49 pages, draft version, to appear in Annual Review of Astronomy and\n  Astrophysics", "journal-ref": null, "doi": "10.1146/annurev-astro-082214-122339", "report-no": null, "categories": "astro-ph.IM physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo based Bayesian data analysis has now become the\nmethod of choice for analyzing and interpreting data in almost all disciplines\nof science. In astronomy, over the last decade, we have also seen a steady\nincrease in the number of papers that employ Monte Carlo based Bayesian\nanalysis. New, efficient Monte Carlo based methods are continuously being\ndeveloped and explored. In this review, we first explain the basics of Bayesian\ntheory and discuss how to set up data analysis problems within this framework.\nNext, we provide an overview of various Monte Carlo based methods for\nperforming Bayesian data analysis. Finally, we discuss advanced ideas that\nenable us to tackle complex problems and thus hold great promise for the\nfuture. We also distribute downloadable computer software (available at\nhttps://github.com/sanjibs/bmcmc/ ) that implements some of the algorithms and\nexamples discussed here.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 07:09:25 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Sharma", "Sanjib", ""]]}, {"id": "1706.01724", "submitter": "Mingyuan Zhou", "authors": "Yulai Cong, Bo Chen, Hongwei Liu, Mingyuan Zhou", "title": "Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic\n  Gradient Riemannian MCMC", "comments": "Appearing in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to develop stochastic gradient based scalable inference for\ndeep discrete latent variable models (LVMs), due to the difficulties in not\nonly computing the gradients, but also adapting the step sizes to different\nlatent factors and hidden layers. For the Poisson gamma belief network (PGBN),\na recently proposed deep discrete LVM, we derive an alternative representation\nthat is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data\naugmentation and marginalization techniques, we derive a block-diagonal Fisher\ninformation matrix and its inverse for the simplex-constrained global model\nparameters of DLDA. Exploiting that Fisher information matrix with stochastic\ngradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian\n(TLASGR) MCMC that jointly learns simplex-constrained global parameters across\nall layers and topics, with topic and layer specific learning rates.\nState-of-the-art results are demonstrated on big data sets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 12:15:42 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Cong", "Yulai", ""], ["Chen", "Bo", ""], ["Liu", "Hongwei", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1706.02289", "submitter": "Evgeny Burnaev", "authors": "Smolyakov Dmitry, Alexander Korotin, Pavel Erofeev, Artem Papanov,\n  Evgeny Burnaev", "title": "Meta-Learning for Resampling Recommendation Systems", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One possible approach to tackle the class imbalance in classification tasks\nis to resample a training dataset, i.e., to drop some of its elements or to\nsynthesize new ones. There exist several widely-used resampling methods. Recent\nresearch showed that the choice of resampling method significantly affects the\nquality of classification, which raises resampling selection problem.\nExhaustive search for optimal resampling is time-consuming and hence it is of\nlimited use. In this paper, we describe an alternative approach to the\nresampling selection. We follow the meta-learning concept to build resampling\nrecommendation systems, i.e., algorithms recommending resampling for datasets\non the basis of their properties.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 22:02:27 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 21:19:48 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 08:08:13 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 07:50:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Dmitry", "Smolyakov", ""], ["Korotin", "Alexander", ""], ["Erofeev", "Pavel", ""], ["Papanov", "Artem", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.02353", "submitter": "Dengdeng Yu", "authors": "Dengdeng Yu, Li Zhang, Ivan Mizera, Bei Jiang, and Linglong Kong", "title": "Sparse Wavelet Estimation in Quantile Regression with Multiple\n  Functional Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we study quantile regression in partial functional linear\nmodel where response is scalar and predictors include both scalars and multiple\nfunctions. Wavelet basis are adopted to better approximate functional slopes\nwhile effectively detect local features. The sparse group lasso penalty is\nimposed to select important functional predictors while capture shared\ninformation among them. The estimation problem can be reformulated into a\nstandard second-order cone program and then solved by an interior point method.\nWe also give a novel algorithm by using alternating direction method of\nmultipliers (ADMM) which was recently employed by many researchers in solving\npenalized quantile regression problems. The asymptotic properties such as the\nconvergence rate and prediction error bound have been established. Simulations\nand a real data from ADHD-200 fMRI data are investigated to show the\nsuperiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:28:53 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 23:34:51 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Yu", "Dengdeng", ""], ["Zhang", "Li", ""], ["Mizera", "Ivan", ""], ["Jiang", "Bei", ""], ["Kong", "Linglong", ""]]}, {"id": "1706.02380", "submitter": "Anru Zhang", "authors": "Yuanpei Cao and Anru Zhang and Hongzhe Li", "title": "Multi-sample Estimation of Bacterial Composition Matrix in Metagenomics\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Metagenomics sequencing is routinely applied to quantify bacterial abundances\nin microbiome studies, where the bacterial composition is estimated based on\nthe sequencing read counts. Due to limited sequencing depth and DNA dropouts,\nmany rare bacterial taxa might not be captured in the final sequencing reads,\nwhich results in many zero counts. Naive composition estimation using count\nnormalization leads to many zero proportions, which tend to result in\ninaccurate estimates of bacterial abundance and diversity. This paper takes a\nmulti-sample approach to the estimation of bacterial abundances in order to\nborrow information across samples and across species. Empirical results from\nreal data sets suggest that the composition matrix over multiple samples is\napproximately low rank, which motivates a regularized maximum likelihood\nestimation with a nuclear norm penalty. An efficient optimization algorithm\nusing the generalized accelerated proximal gradient and Euclidean projection\nonto simplex space is developed. The theoretical upper bounds and the minimax\nlower bounds of the estimation errors, measured by the Kullback-Leibler\ndivergence and the Frobenius norm, are established. Simulation studies\ndemonstrate that the proposed estimator outperforms the naive estimators. The\nmethod is applied to an analysis of a human gut microbiome dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 21:05:26 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 05:50:09 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 01:44:19 GMT"}, {"version": "v4", "created": "Wed, 24 Apr 2019 18:41:04 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Cao", "Yuanpei", ""], ["Zhang", "Anru", ""], ["Li", "Hongzhe", ""]]}, {"id": "1706.02649", "submitter": "Samuel Livingstone", "authors": "Samuel Livingstone, Michael F. Faulkner and Gareth O. Roberts", "title": "Kinetic energy choice in Hamiltonian/hybrid Monte Carlo", "comments": "15 pages (+7 page supplement, included here as an appendix), 2\n  figures (+1 in supplement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider how different choices of kinetic energy in Hamiltonian Monte\nCarlo affect algorithm performance. To this end, we introduce two quantities\nwhich can be easily evaluated, the composite gradient and the implicit noise.\nResults are established on integrator stability and geometric convergence, and\nwe show that choices of kinetic energy that result in heavy-tailed momentum\ndistributions can exhibit an undesirable negligible moves property, which we\ndefine. A general efficiency-robustness trade off is outlined, and\nimplementations which rely on approximate gradients are also discussed. Two\nnumerical studies illustrate our theoretical findings, showing that the\nstandard choice which results in a Gaussian momentum distribution is not always\noptimal in terms of either robustness or efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 15:44:01 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 15:36:18 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 12:32:08 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Livingstone", "Samuel", ""], ["Faulkner", "Michael F.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1706.02808", "submitter": "Art Owen", "authors": "Art B. Owen", "title": "A randomized Halton algorithm in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized quasi-Monte Carlo (RQMC) sampling can bring orders of magnitude\nreduction in variance compared to plain Monte Carlo (MC) sampling. The extent\nof the efficiency gain varies from problem to problem and can be hard to\npredict. This article presents an R function rhalton that produces scrambled\nversions of Halton sequences. On some problems it brings efficiency gains of\nseveral thousand fold. On other problems, the efficiency gain is minor. The\ncode is designed to make it easy to determine whether a given integrand will\nbenefit from RQMC sampling. An RQMC sample of n points in $[0,1]^d$ can be\nextended later to a larger n and/or d.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 01:44:03 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 22:04:44 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Owen", "Art B.", ""]]}, {"id": "1706.02940", "submitter": "Theodore  Kypraios", "authors": "Theodore Kypraios and Philip D. O'Neill", "title": "Bayesian nonparametrics for stochastic epidemic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of models for the spread of communicable diseases are\nparametric in nature and involve underlying assumptions about how the disease\nspreads through a population. In this article we consider the use of Bayesian\nnonparametric approaches to analysing data from disease outbreaks. Specifically\nwe focus on methods for estimating the infection process in simple models under\nthe assumption that this process has an explicit time-dependence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 13:12:27 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Kypraios", "Theodore", ""], ["O'Neill", "Philip D.", ""]]}, {"id": "1706.02952", "submitter": "Amit Dhurandhar", "authors": "Amit Dhurandhar, Vijay Iyengar, Ronny Luss and Karthikeyan Shanmugam", "title": "TIP: Typifying the Interpretability of Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel notion of what it means to be interpretable, looking past\nthe usual association with human understanding. Our key insight is that\ninterpretability is not an absolute concept and so we define it relative to a\ntarget model, which may or may not be a human. We define a framework that\nallows for comparing interpretable procedures by linking them to important\npractical aspects such as accuracy and robustness. We characterize many of the\ncurrent state-of-the-art interpretable methods in our framework portraying its\ngeneral applicability. Finally, principled interpretable strategies are\nproposed and empirically evaluated on synthetic data, as well as on the largest\npublic olfaction dataset that was made recently available \\cite{olfs}. We also\nexperiment on MNIST with a simple target model and different oracle models of\nvarying complexity. This leads to the insight that the improvement in the\ntarget model is not only a function of the oracle model's performance, but also\nits relative complexity with respect to the target model. Further experiments\non CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit\nof our methods over Knowledge Distillation when the target models are simple\nand the complex model is a neural network.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 13:55:18 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 16:12:02 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 15:49:37 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Iyengar", "Vijay", ""], ["Luss", "Ronny", ""], ["Shanmugam", "Karthikeyan", ""]]}, {"id": "1706.03369", "submitter": "Francois-Xavier Briol", "authors": "Francois-Xavier Briol and Chris J. Oates and Jon Cockayne and Wilson\n  Ye Chen and Mark Girolami", "title": "On the Sampling Problem for Kernel Quadrature", "comments": "To appear at Thirty-fourth International Conference on Machine\n  Learning (ICML 2017)", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:586-595, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard Kernel Quadrature method for numerical integration with random\npoint sets (also called Bayesian Monte Carlo) is known to converge in root mean\nsquare error at a rate determined by the ratio $s/d$, where $s$ and $d$ encode\nthe smoothness and dimension of the integrand. However, an empirical\ninvestigation reveals that the rate constant $C$ is highly sensitive to the\ndistribution of the random points. In contrast to standard Monte Carlo\nintegration, for which optimal importance sampling is well-understood, the\nsampling distribution that minimises $C$ for Kernel Quadrature does not admit a\nclosed form. This paper argues that the practical choice of sampling\ndistribution is an important open problem. One solution is considered; a novel\nautomatic approach based on adaptive tempering and sequential Monte Carlo.\nEmpirical results demonstrate a dramatic reduction in integration error of up\nto 4 orders of magnitude can be achieved with the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 16:08:17 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Briol", "Francois-Xavier", ""], ["Oates", "Chris J.", ""], ["Cockayne", "Jon", ""], ["Chen", "Wilson Ye", ""], ["Girolami", "Mark", ""]]}, {"id": "1706.03409", "submitter": "Yujing Jiang", "authors": "Yujing Jiang, Xin He, Mei-Ling Ting Lee, Bernard Rosner, Jun Yan", "title": "Wilcoxon Rank-Based Tests for Clustered Data with R Package clusrank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wilcoxon Rank-based tests are distribution-free alternatives to the popular\ntwo-sample and paired t-tests. For independent data, they are available in\nseveral R packages such as stats and coin. For clustered data, in spite of the\nrecent methodological developments, there did not exist an R package that makes\nthem available at one place. We present a package clusrank where the latest\ndevelopments are implemented and wrapped under a unified user-friendly\ninterface. With different methods dispatched based on the inputs, this package\noffers great flexibility in rank-based tests for various clustered data. Exact\ntests based on permutations are also provided for some methods. Details of the\nmajor schools of different methods are briefly reviewed. Usages of the package\nclusrank are illustrated with simulated data as well as a real dataset from an\nophthalmological study. The package also enables convenient comparison between\nselected methods under settings that have not been studied before and the\nresults are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 21:29:02 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Jiang", "Yujing", ""], ["He", "Xin", ""], ["Lee", "Mei-Ling Ting", ""], ["Rosner", "Bernard", ""], ["Yan", "Jun", ""]]}, {"id": "1706.03412", "submitter": "Evgeny Burnaev", "authors": "Vladislav Ishimtsev, Ivan Nazarov, Alexander Bernstein and Evgeny\n  Burnaev", "title": "Conformal k-NN Anomaly Detector for Univariate Data Streams", "comments": "15 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies in time-series data give essential and often actionable information\nin many applications. In this paper we consider a model-free anomaly detection\nmethod for univariate time-series which adapts to non-stationarity in the data\nstream and provides probabilistic abnormality scores based on the conformal\nprediction paradigm. Despite its simplicity the method performs on par with\ncomplex prediction-based models on the Numenta Anomaly Detection benchmark and\nthe Yahoo! S5 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 21:45:24 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Ishimtsev", "Vladislav", ""], ["Nazarov", "Ivan", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.03415", "submitter": "Evgeny Burnaev", "authors": "Denis Volkhonskiy, Ilia Nouretdinov, Alexander Gammerman, Vladimir\n  Vovk, Evgeny Burnaev", "title": "Inductive Conformal Martingales for Change-Point Detection", "comments": "22 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of quickest change-point detection in data streams.\nClassical change-point detection procedures, such as CUSUM, Shiryaev-Roberts\nand Posterior Probability statistics, are optimal only if the change-point\nmodel is known, which is an unrealistic assumption in typical applied problems.\nInstead we propose a new method for change-point detection based on Inductive\nConformal Martingales, which requires only the independence and identical\ndistribution of observations. We compare the proposed approach to standard\nmethods, as well as to change-point detection oracles, which model a typical\npractical situation when we have only imprecise (albeit parametric) information\nabout pre- and post-change data distributions. Results of comparison provide\nevidence that change-point detection based on Inductive Conformal Martingales\nis an efficient tool, capable to work under quite general conditions unlike\ntraditional approaches.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 21:49:19 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Volkhonskiy", "Denis", ""], ["Nouretdinov", "Ilia", ""], ["Gammerman", "Alexander", ""], ["Vovk", "Vladimir", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.03649", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Umut \\c{S}im\\c{s}ekli", "title": "Fractional Langevin Monte Carlo: Exploring L\\'{e}vy Driven Stochastic\n  Differential Equations for Markov Chain Monte Carlo", "comments": "Published in the International Conference on Machine Learning (ICML\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the recent advances in scalable Markov Chain Monte Carlo methods,\nsampling techniques that are based on Langevin diffusions have started\nreceiving increasing attention. These so called Langevin Monte Carlo (LMC)\nmethods are based on diffusions driven by a Brownian motion, which gives rise\nto Gaussian proposal distributions in the resulting algorithms. Even though\nthese approaches have proven successful in many applications, their performance\ncan be limited by the light-tailed nature of the Gaussian proposals. In this\nstudy, we extend classical LMC and develop a novel Fractional LMC (FLMC)\nframework that is based on a family of heavy-tailed distributions, called\n$\\alpha$-stable L\\'{e}vy distributions. As opposed to classical approaches, the\nproposed approach can possess large jumps while targeting the correct\ndistribution, which would be beneficial for efficient exploration of the state\nspace. We develop novel computational methods that can scale up to large-scale\nproblems and we provide formal convergence analysis of the proposed scheme. Our\nexperiments support our theory: FLMC can provide superior performance in\nmulti-modal settings, improved convergence rates, and robustness to algorithm\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 14:07:00 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["\u015eim\u015fekli", "Umut", ""]]}, {"id": "1706.03665", "submitter": "Daniel Ahfock", "authors": "Daniel Ahfock, William J. Astle and Sylvia Richardson", "title": "Statistical properties of sketching algorithms", "comments": "added central limit theorem under weaker conditions, unconditional\n  results, corrected expression for variance of partial sketch", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching is a probabilistic data compression technique that has been largely\ndeveloped in the computer science community. Numerical operations on big\ndatasets can be intolerably slow; sketching algorithms address this issue by\ngenerating a smaller surrogate dataset. Typically, inference proceeds on the\ncompressed dataset. Sketching algorithms generally use random projections to\ncompress the original dataset and this stochastic generation process makes them\namenable to statistical analysis. We argue that the sketched data can be\nmodelled as a random sample, thus placing this family of data compression\nmethods firmly within an inferential framework. In particular, we focus on the\nGaussian, Hadamard and Clarkson-Woodruff sketches, and their use in single pass\nsketching algorithms for linear regression with huge $n$. We explore the\nstatistical properties of sketched regression algorithms and derive new\ndistributional results for a large class of sketched estimators. A key result\nis a conditional central limit theorem for data oblivious sketches. An\nimportant finding is that the best choice of sketching algorithm in terms of\nmean square error is related to the signal to noise ratio in the source\ndataset. Finally, we demonstrate the theory and the limits of its applicability\non two real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 14:44:47 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 23:51:39 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ahfock", "Daniel", ""], ["Astle", "William J.", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1706.03883", "submitter": "Nhat Ho", "authors": "Nhat Ho, XuanLong Nguyen, Mikhail Yurochkin, Hung Hai Bui, Viet Huynh,\n  Dinh Phung", "title": "Multilevel Clustering via Wasserstein Means", "comments": "Proceedings of the ICML, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to the problem of multilevel clustering, which\naims to simultaneously partition data in each group and discover grouping\npatterns among groups in a potentially large hierarchically structured corpus\nof data. Our method involves a joint optimization formulation over several\nspaces of discrete probability measures, which are endowed with Wasserstein\ndistance metrics. We propose a number of variants of this problem, which admit\nfast optimization algorithms, by exploiting the connection to the problem of\nfinding Wasserstein barycenters. Consistency properties are established for the\nestimates of both local and global clusters. Finally, experiment results with\nboth synthetic and real data are presented to demonstrate the flexibility and\nscalability of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 01:15:04 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Ho", "Nhat", ""], ["Nguyen", "XuanLong", ""], ["Yurochkin", "Mikhail", ""], ["Bui", "Hung Hai", ""], ["Huynh", "Viet", ""], ["Phung", "Dinh", ""]]}, {"id": "1706.04032", "submitter": "Tijana Radivojevic", "authors": "Tijana Radivojevi\\'c and Elena Akhmatskaya", "title": "Modified Hamiltonian Monte Carlo for Bayesian inference", "comments": "30 pages, 17 figures", "journal-ref": "Stat Comput (2019). https://doi.org/10.1007/s11222-019-09885-x", "doi": "10.1007/s11222-019-09885-x", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hamiltonian Monte Carlo (HMC) method has been recognized as a powerful\nsampling tool in computational statistics. We show that performance of HMC can\nbe significantly improved by incorporating importance sampling and an\nirreversible part of the dynamics into a chain. This is achieved by replacing\nHamiltonians in the Metropolis test with modified Hamiltonians, and a complete\nmomentum update with a partial momentum refreshment. We call the resulting\ngeneralized HMC importance sampler---Mix & Match Hamiltonian Monte Carlo\n(MMHMC). The method is irreversible by construction and further benefits from\n(i) the efficient algorithms for computation of modified Hamiltonians; (ii) the\nimplicit momentum update procedure and (iii) the multi-stage splitting\nintegrators specially derived for the methods sampling with modified\nHamiltonians. MMHMC has been implemented, tested on the popular statistical\nmodels and compared in sampling efficiency with HMC, Riemann Manifold\nHamiltonian Monte Carlo, Generalized Hybrid Monte Carlo, Generalized Shadow\nHybrid Monte Carlo, Metropolis Adjusted Langevin Algorithm and Random Walk\nMetropolis-Hastings. To make a fair comparison, we propose a metric that\naccounts for correlations among samples and weights, and can be readily used\nfor all methods which generate such samples. The experiments reveal the\nsuperiority of MMHMC over popular sampling techniques, especially in solving\nhigh dimensional problems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 12:58:41 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 04:18:05 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Radivojevi\u0107", "Tijana", ""], ["Akhmatskaya", "Elena", ""]]}, {"id": "1706.04059", "submitter": "Yohann De Castro", "authors": "Yohann De Castro, Fabrice Gamboa, Didier Henrion, Roxana Hess,\n  Jean-Bernard Lasserre", "title": "Approximate Optimal Designs for Multivariate Polynomial Regression", "comments": "30 Pages, 8 Figures. arXiv admin note: substantial text overlap with\n  arXiv:1703.01777", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.NA stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach aiming at computing approximate optimal designs\nfor multivariate polynomial regressions on compact (semi-algebraic) design\nspaces. We use the moment-sum-of-squares hierarchy of semidefinite programming\nproblems to solve numerically the approximate optimal design problem. The\ngeometry of the design is recovered via semidefinite programming duality\ntheory. This article shows that the hierarchy converges to the approximate\noptimal design as the order of the hierarchy increases. Furthermore, we provide\na dual certificate ensuring finite convergence of the hierarchy and showing\nthat the approximate optimal design can be computed numerically with our\nmethod. As a byproduct, we revisit the equivalence theorem of the experimental\ndesign theory: it is linked to the Christoffel polynomial and it characterizes\nfinite convergence of the moment-sum-of-square hierarchies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 19:19:36 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 13:03:59 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 12:50:56 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["De Castro", "Yohann", ""], ["Gamboa", "Fabrice", ""], ["Henrion", "Didier", ""], ["Hess", "Roxana", ""], ["Lasserre", "Jean-Bernard", ""]]}, {"id": "1706.04440", "submitter": "Gabriel Becker", "authors": "Gabriel Becker, Sara E. Moore, Michael Lawrence", "title": "trackr: A Framework for Enhancing Discoverability and Reproducibility of\n  Data Visualizations and Other Artifacts in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research is an incremental, iterative process, with new results relying and\nbuilding upon previous ones. Scientists need to find, retrieve, understand, and\nverify results in order to confidently extend them, even when the results are\ntheir own. We present the trackr framework for organizing, automatically\nannotating, discovering, and retrieving results. We identify sources of\nautomatically extractable metadata for computational results, and we define an\nextensible system for organizing, annotating, and searching for results based\non these and other metadata. We present an open-source implementation of these\nconcepts for plots, computational artifacts, and woven dynamic reports\ngenerated in the R statistical computing language.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:12:08 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Becker", "Gabriel", ""], ["Moore", "Sara E.", ""], ["Lawrence", "Michael", ""]]}, {"id": "1706.04606", "submitter": "Olli-Pekka Koistinen", "authors": "Olli-Pekka Koistinen, Freyja B. Dagbjartsd\\'ottir, Vilhj\\'almur\n  \\'Asgeirsson, Aki Vehtari, Hannes J\\'onsson", "title": "Nudged elastic band calculations accelerated with Gaussian process\n  regression", "comments": null, "journal-ref": "The Journal of Chemical Physics 147, 152720 (2017)", "doi": "10.1063/1.4986787", "report-no": null, "categories": "physics.chem-ph physics.atm-clus physics.comp-ph stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum energy paths for transitions such as atomic and/or spin\nrearrangements in thermalized systems are the transition paths of largest\nstatistical weight. Such paths are frequently calculated using the nudged\nelastic band method, where an initial path is iteratively shifted to the\nnearest minimum energy path. The computational effort can be large, especially\nwhen ab initio or electron density functional calculations are used to evaluate\nthe energy and atomic forces. Here, we show how the number of such evaluations\ncan be reduced by an order of magnitude using a Gaussian process regression\napproach where an approximate energy surface is generated and refined in each\niteration. When the goal is to evaluate the transition rate within harmonic\ntransition state theory, the evaluation of the Hessian matrix at the initial\nand final state minima can be carried out beforehand and used as input in the\nminimum energy path calculation, thereby improving stability and reducing the\nnumber of iterations needed for convergence. A Gaussian process model also\nprovides an uncertainty estimate for the approximate energy surface, and this\ncan be used to focus the calculations on the lesser-known part of the path,\nthereby reducing the number of needed energy and force evaluations to a half in\nthe present calculations. The methodology is illustrated using the\ntwo-dimensional M\\\"uller-Brown potential surface and performance assessed on an\nestablished benchmark involving 13 rearrangement transitions of a heptamer\nisland on a solid surface.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 17:48:49 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 15:03:41 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Koistinen", "Olli-Pekka", ""], ["Dagbjartsd\u00f3ttir", "Freyja B.", ""], ["\u00c1sgeirsson", "Vilhj\u00e1lmur", ""], ["Vehtari", "Aki", ""], ["J\u00f3nsson", "Hannes", ""]]}, {"id": "1706.04780", "submitter": "Changye Wu", "authors": "Changye Wu, Christian P. Robert", "title": "Average of Recentered Parallel MCMC for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big data context, traditional MCMC methods, such as Metropolis-Hastings\nalgorithms and hybrid Monte Carlo, scale poorly because of their need to\nevaluate the likelihood over the whole data set at each iteration. In order to\nresurrect MCMC methods, numerous approaches belonging to two categories:\ndivide-and-conquer and subsampling, are proposed. In this article, we study the\nparallel MCMC and propose a new combination method in the divide-and-conquer\nframework. Compared with some parallel MCMC methods, such as consensus Monte\nCarlo, Weierstrass Sampler, instead of sampling from subposteriors, our method\nruns MCMC on rescaled subposteriors, but share the same computation cost in the\nparallel stage. We also give the mathematical justification of our method and\nshow its performance in several models. Besides, even though our new methods is\nproposed in parametric framework, it can been applied to non-parametric cases\nwithout difficulty.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 09:09:11 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 21:30:28 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Wu", "Changye", ""], ["Robert", "Christian P.", ""]]}, {"id": "1706.04781", "submitter": "Changye Wu", "authors": "Changye Wu, Christian P. Robert", "title": "Generalized Bouncy Particle Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a special example of piecewise deterministic Markov process, bouncy\nparticle sampler is a rejection-free, irreversible Markov chain Monte Carlo\nalgorithm and can draw samples from target distribution efficiently. We\ngeneralize bouncy particle sampler in terms of its transition dynamics. In BPS,\nthe transition dynamic at event time is deterministic, but in GBPS, it is\nrandom. With the help of this randomness, GBPS can overcome the reducibility\nproblem in BPS without refreshement.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 09:09:19 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 21:26:21 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Wu", "Changye", ""], ["Robert", "Christian P.", ""]]}, {"id": "1706.05003", "submitter": "Michael Wurm", "authors": "Michael J. Wurm, Paul J. Rathouz, and Bret M. Hanlon", "title": "Regularized Ordinal Regression and the ordinalNet R Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization techniques such as the lasso (Tibshirani 1996) and elastic net\n(Zou and Hastie 2005) can be used to improve regression model coefficient\nestimation and prediction accuracy, as well as to perform variable selection.\nOrdinal regression models are widely used in applications where the use of\nregularization could be beneficial; however, these models are not included in\nmany popular software packages for regularized regression. We propose a\ncoordinate descent algorithm to fit a broad class of ordinal regression models\nwith an elastic net penalty. Furthermore, we demonstrate that each model in\nthis class generalizes to a more flexible form, for instance to accommodate\nunordered categorical data. We introduce an elastic net penalty class that\napplies to both model forms. Additionally, this penalty can be used to shrink a\nnon-ordinal model toward its ordinal counterpart. Finally, we introduce the R\npackage ordinalNet, which implements the algorithm for this model class.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 16:46:31 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Wurm", "Michael J.", ""], ["Rathouz", "Paul J.", ""], ["Hanlon", "Bret M.", ""]]}, {"id": "1706.05280", "submitter": "Gregor Kastner", "authors": "Gregor Kastner, Sylvia Fr\\\"uhwirth-Schnatter", "title": "Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC\n  Estimation of Stochastic Volatility Models", "comments": null, "journal-ref": "Computational Statistics & Data Analysis 76, 408-423 (2014)", "doi": "10.1016/j.csda.2013.01.002", "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for stochastic volatility models using MCMC methods highly\ndepends on actual parameter values in terms of sampling efficiency. While draws\nfrom the posterior utilizing the standard centered parameterization break down\nwhen the volatility of volatility parameter in the latent state equation is\nsmall, non-centered versions of the model show deficiencies for highly\npersistent latent variable series. The novel approach of\nancillarity-sufficiency interweaving has recently been shown to aid in\novercoming these issues for a broad class of multilevel models. In this paper,\nwe demonstrate how such an interweaving strategy can be applied to stochastic\nvolatility models in order to greatly improve sampling efficiency for all\nparameters and throughout the entire parameter range. Moreover, this method of\n\"combining best of different worlds\" allows for inference for parameter\nconstellations that have previously been infeasible to estimate without the\nneed to select a particular parameterization beforehand.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 14:11:43 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Kastner", "Gregor", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "1706.05305", "submitter": "Nicolas Chopin", "authors": "Nicolas Chopin and Mathieu Gerber", "title": "Sequential quasi-Monte Carlo: Introduction for Non-Experts, Dimension\n  Reduction, Application to Partly Observed Diffusion Processes", "comments": "To be published in the proceedings of MCMQMC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SMC (Sequential Monte Carlo) is a class of Monte Carlo algorithms for\nfiltering and related sequential problems. Gerber and Chopin (2015) introduced\nSQMC (Sequential quasi-Monte Carlo), a QMC version of SMC. This paper has two\nobjectives: (a) to introduce Sequential Monte Carlo to the QMC community, whose\nmembers are usually less familiar with state-space models and particle\nfiltering; (b) to extend SQMC to the filtering of continuous-time state-space\nmodels, where the latent process is a diffusion. A recurring point in the paper\nwill be the notion of dimension reduction, that is how to implement SQMC in\nsuch a way that it provides good performance despite the high dimension of the\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 15:01:00 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Chopin", "Nicolas", ""], ["Gerber", "Mathieu", ""]]}, {"id": "1706.05439", "submitter": "Jack Baker", "authors": "Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth", "title": "Control Variates for Stochastic Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Markov chain Monte Carlo (MCMC) methods scale poorly\nwith dataset size. A popular class of methods for solving this issue is\nstochastic gradient MCMC. These methods use a noisy estimate of the gradient of\nthe log posterior, which reduces the per iteration computational cost of the\nalgorithm. Despite this, there are a number of results suggesting that\nstochastic gradient Langevin dynamics (SGLD), probably the most popular of\nthese methods, still has computational cost proportional to the dataset size.\nWe suggest an alternative log posterior gradient estimate for stochastic\ngradient MCMC, which uses control variates to reduce the variance. We analyse\nSGLD using this gradient estimate, and show that, under log-concavity\nassumptions on the target distribution, the computational cost required for a\ngiven level of accuracy is independent of the dataset size. Next we show that a\ndifferent control variate technique, known as zero variance control variates\ncan be applied to SGMCMC algorithms for free. This post-processing step\nimproves the inference of the algorithm by reducing the variance of the MCMC\noutput. Zero variance control variates rely on the gradient of the log\nposterior; we explore how the variance reduction is affected by replacing this\nwith the noisy gradient estimate calculated by SGMCMC.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 22:00:54 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 09:53:14 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Baker", "Jack", ""], ["Fearnhead", "Paul", ""], ["Fox", "Emily B.", ""], ["Nemeth", "Christopher", ""]]}, {"id": "1706.06185", "submitter": "Paul McNicholas", "authors": "Yuhong Wei, Yang Tang and Paul D. McNicholas", "title": "Flexible High-Dimensional Unsupervised Learning with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture of factor analyzers (MFA) model is a famous mixture model-based\napproach for unsupervised learning with high-dimensional data. It can be\nuseful, inter alia, in situations where the data dimensionality far exceeds the\nnumber of observations. In recent years, the MFA model has been extended to\nnon-Gaussian mixtures to account for clusters with heavier tail weight and/or\nasymmetry. The generalized hyperbolic factor analyzers (MGHFA) model is one\nsuch extension, which leads to a flexible modelling paradigm that accounts for\nboth heavier tail weight and cluster asymmetry. In many practical applications,\nthe occurrence of missing values often complicates data analyses. A\ngeneralization of the MGHFA is presented to accommodate missing values. Under a\nmissing-at-random mechanism, we develop a computationally efficient alternating\nexpectation conditional maximization algorithm for parameter estimation of the\nMGHFA model with different patterns of missing values. The imputation of\nmissing values under an incomplete-data structure of MGHFA is also\ninvestigated. The performance of our proposed methodology is illustrated\nthrough the analysis of simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 21:37:56 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 20:30:16 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wei", "Yuhong", ""], ["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1706.06344", "submitter": "Lampros Bouranis", "authors": "Lampros Bouranis, Nial Friel, Florian Maire", "title": "Bayesian model selection for exponential random graph models via\n  adjusted pseudolikelihoods", "comments": "Supplementary material attached. To view attachments, please download\n  and extract the gzzipped source file listed under \"Other formats\"", "journal-ref": "Journal of Computational and Graphical Statistics 27:3 (2018)\n  516-528", "doi": "10.1080/10618600.2018.1448832", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models with intractable likelihood functions arise in areas including network\nanalysis and spatial statistics, especially those involving Gibbs random\nfields. Posterior parameter es timation in these settings is termed a\ndoubly-intractable problem because both the likelihood function and the\nposterior distribution are intractable. The comparison of Bayesian models is\noften based on the statistical evidence, the integral of the un-normalised\nposterior distribution over the model parameters which is rarely available in\nclosed form. For doubly-intractable models, estimating the evidence adds\nanother layer of difficulty. Consequently, the selection of the model that best\ndescribes an observed network among a collection of exponential random graph\nmodels for network analysis is a daunting task. Pseudolikelihoods offer a\ntractable approximation to the likelihood but should be treated with caution\nbecause they can lead to an unreasonable inference. This paper specifies a\nmethod to adjust pseudolikelihoods in order to obtain a reasonable, yet\ntractable, approximation to the likelihood. This allows implementation of\nwidely used computational methods for evidence estimation and pursuit of\nBayesian model selection of exponential random graph models for the analysis of\nsocial networks. Empirical comparisons to existing methods show that our\nprocedure yields similar evidence estimates, but at a lower computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 09:52:59 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 15:38:28 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Bouranis", "Lampros", ""], ["Friel", "Nial", ""], ["Maire", "Florian", ""]]}, {"id": "1706.06664", "submitter": "Anshumali Shrivastava", "authors": "Chen Luo, Anshumali Shrivastava", "title": "Arrays of (locality-sensitive) Count Estimators (ACE): High-Speed\n  Anomaly Detection via Cache Lookups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is one of the frequent and important subroutines deployed\nin large-scale data processing systems. Even being a well-studied topic,\nexisting techniques for unsupervised anomaly detection require storing\nsignificant amounts of data, which is prohibitive from memory and latency\nperspective. In the big-data world existing methods fail to address the new set\nof memory and latency constraints. In this paper, we propose ACE (Arrays of\n(locality-sensitive) Count Estimators) algorithm that can be 60x faster than\nthe ELKI package~\\cite{DBLP:conf/ssd/AchtertBKSZ09}, which has the fastest\nimplementation of the unsupervised anomaly detection algorithms. ACE algorithm\nrequires less than $4MB$ memory, to dynamically compress the full data\ninformation into a set of count arrays. These tiny $4MB$ arrays of counts are\nsufficient for unsupervised anomaly detection. At the core of the ACE\nalgorithm, there is a novel statistical estimator which is derived from the\nsampling view of Locality Sensitive Hashing(LSH). This view is significantly\ndifferent and efficient than the widely popular view of LSH for near-neighbor\nsearch. We show the superiority of ACE algorithm over 11 popular baselines on 3\nbenchmark datasets, including the KDD-Cup99 data which is the largest available\nbenchmark comprising of more than half a million entries with ground truth\nanomaly labels.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 21:09:22 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Luo", "Chen", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1706.06889", "submitter": "Dennis Prangle", "authors": "Dennis Prangle", "title": "gk: An R Package for the g-and-k and generalised g-and-h Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The g-and-k and (generalised) g-and-h distributions are flexible univariate\ndistributions which can model highly skewed or heavy tailed data through only\nfour parameters: location and scale, and two shape parameters influencing the\nskewness and kurtosis. These distributions have the unusual property that they\nare defined through their quantile function (inverse cumulative distribution\nfunction) and their density is unavailable in closed form, which makes\nparameter inference complicated. This paper presents the gk R package to work\nwith these distributions. It provides the usual distribution functions and\nseveral algorithms for inference of independent identically distributed data,\nincluding the finite difference stochastic approximation method, which has not\nbeen used before for this problem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 13:23:30 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Prangle", "Dennis", ""]]}, {"id": "1706.07564", "submitter": "Alireza Doostan", "authors": "Mohammad Hadigol and Alireza Doostan", "title": "Least Squares Polynomial Chaos Expansion: A Review of Sampling\n  Strategies", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2017.12.019", "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As non-institutive polynomial chaos expansion (PCE) techniques have gained\ngrowing popularity among researchers, we here provide a comprehensive review of\nmajor sampling strategies for the least squares based PCE. Traditional sampling\nmethods, such as Monte Carlo, Latin hypercube, quasi-Monte Carlo, optimal\ndesign of experiments (ODE), Gaussian quadratures, as well as more recent\ntechniques, such as coherence-optimal and randomized quadratures are discussed.\nWe also propose a hybrid sampling method, dubbed alphabetic-coherence-optimal,\nthat employs the so-called alphabetic optimality criteria used in the context\nof ODE in conjunction with coherence-optimal samples. A comparison between the\nempirical performance of the selected sampling methods applied to three\nnumerical examples, including high-order PCE's, high-dimensional problems, and\nlow oversampling ratios, is presented to provide a road map for practitioners\nseeking the most suitable sampling technique for a problem at hand. We observed\nthat the alphabetic-coherence-optimal technique outperforms other sampling\nmethods, specially when high-order ODE are employed and/or the oversampling\nratio is low.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 04:49:01 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Hadigol", "Mohammad", ""], ["Doostan", "Alireza", ""]]}, {"id": "1706.07650", "submitter": "Dominic Schuhmacher", "authors": "Valentin Hartmann and Dominic Schuhmacher", "title": "Semi-discrete optimal transport - the case p=1", "comments": "28 pages, 8 figures; new application added and more thorough\n  performance evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding an optimal transport plan between an\nabsolutely continuous measure $\\mu$ on $\\mathcal{X} \\subset \\mathbb{R}^d$ and a\nfinitely supported measure $\\nu$ on $\\mathbb{R}^d$ when the transport cost is\nthe Euclidean distance. We may think of this problem as closest distance\nallocation of some ressource continuously distributed over space to a finite\nnumber of processing sites with capacity constraints.\n  This article gives a detailed discussion of the problem, including a\ncomparison with the much better studied case of squared Euclidean cost (\"the\ncase $p=2$\"). We present an algorithm for computing the optimal transport plan,\nwhich is similar to the approach for $p=2$ by Aurenhammer, Hoffmann and Aronov\n[Algorithmica 20, 61-76, 1998] and M\\'erigot [Computer Graphics Forum 30,\n1583--1592, 2011]. We show the necessary results to make the approach work for\nthe Euclidean cost, evaluate its performance on a set of test cases, and give a\nnumber of applications. The later include goodness-of-fit partitions, a novel\nvisual tool for assessing whether a finite sample is consistent with a posited\nprobability density.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 11:57:37 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 17:03:36 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Hartmann", "Valentin", ""], ["Schuhmacher", "Dominic", ""]]}, {"id": "1706.07712", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead", "title": "Asymptotics of ABC", "comments": "This document is due to appear as a chapter of the forthcoming\n  Handbook of Approximate Bayesian Computation (ABC) edited by S. Sisson, Y.\n  Fan, and M. Beaumont", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an informal review of recent work on the asymptotics of\nApproximate Bayesian Computation (ABC). In particular we focus on how does the\nABC posterior, or point estimates obtained by ABC, behave in the limit as we\nhave more data? The results we review show that ABC can perform well in terms\nof point estimation, but standard implementations will over-estimate the\nuncertainty about the parameters. If we use the regression correction of\nBeaumont et al. then ABC can also accurately quantify this uncertainty. The\ntheoretical results also have practical implications for how to implement ABC.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 14:01:34 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Fearnhead", "Paul", ""]]}, {"id": "1706.07797", "submitter": "Christopher O'Neill", "authors": "David Kahle, Christopher O'Neill, Jeff Sommars", "title": "A computer algebra system for R: Macaulay2 and the m2r package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic methods have a long history in statistics. The most prominent\nmanifestation of modern algebra in statistics can be seen in the field of\nalgebraic statistics, which brings tools from commutative algebra and algebraic\ngeometry to bear on statistical problems. Now over two decades old, algebraic\nstatistics has applications in a wide range of theoretical and applied\nstatistical domains. Nevertheless, algebraic statistical methods are still not\nmainstream, mostly due to a lack of easy off-the-shelf implementations. In this\narticle we debut m2r, an R package that connects R to Macaulay2 through a\npersistent back-end socket connection running locally or on a cloud server.\nTopics range from basic use of m2r to applications and design philosophy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 14:48:49 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Kahle", "David", ""], ["O'Neill", "Christopher", ""], ["Sommars", "Jeff", ""]]}, {"id": "1706.08209", "submitter": "Xu Li", "authors": "Xu Li, Chunlin Gong, Liangxian Gu, Wenkun Gao, Zhao Jing, Hua Su", "title": "A sequential surrogate method for reliability analysis based on radial\n  basis function", "comments": "24 pages,9 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A radial basis function (RBF) based sequential surrogate reliability method\n(SSRM) is proposed, in which a special optimization problem is solved to update\nthe surrogate model of the limit state function (LSF) iteratively. The\nobjective of the optimization problem is to find a new point to maximize the\nprobability density function (PDF), subject to the constraints that the new\npoint is on the approximated LSF and the minimum distance to the existing\npoints is greater than or equal to the given distance. By updating the\nsurrogate model with the new points, the surrogate model of the LSF becomes\nmore and more accurate in the important region with a high failure probability\nand on the LSF boundary. Moreover, the accuracy of the unimportant region is\nalso improved within the iteration due to the minimum distance constraint. SSRM\ntakes advantage of the information of PDF and LSF to capture the failure\nfeatures, which decreases the number of the expensive LSF evaluations. Six\nnumerical examples show that SSRM improves the accuracy of the surrogate model\nin the important region around the failure boundary with small number of\nsamples and has better adaptability to the nonlinear LSF, hence increases the\naccuracy and efficiency of the reliability analysis.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 02:35:59 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Li", "Xu", ""], ["Gong", "Chunlin", ""], ["Gu", "Liangxian", ""], ["Gao", "Wenkun", ""], ["Jing", "Zhao", ""], ["Su", "Hua", ""]]}, {"id": "1706.08327", "submitter": "Florian Maire", "authors": "Florian Maire, Nial Friel, Pierre Alquier", "title": "Informed Sub-Sampling MCMC: Approximate Bayesian Inference for Large\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a framework for speeding up Bayesian inference\nconducted in presence of large datasets. We design a Markov chain whose\ntransition kernel uses an (unknown) fraction of (fixed size) of the available\ndata that is randomly refreshed throughout the algorithm. Inspired by the\nApproximate Bayesian Computation (ABC) literature, the subsampling process is\nguided by the fidelity to the observed data, as measured by summary statistics.\nThe resulting algorithm, Informed Sub-Sampling MCMC (ISS-MCMC), is a generic\nand flexible approach which, contrary to existing scalable methodologies,\npreserves the simplicity of the Metropolis-Hastings algorithm. Even though\nexactness is lost, i.e. the chain distribution approximates the posterior, we\nstudy and quantify theoretically this bias and show on a diverse set of\nexamples that it yields excellent performances when the computational budget is\nlimited. If available and cheap to compute, we show that setting the summary\nstatistics as the maximum likelihood estimator is supported by theoretical\narguments.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 11:24:51 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 00:43:36 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 09:20:50 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Maire", "Florian", ""], ["Friel", "Nial", ""], ["Alquier", "Pierre", ""]]}, {"id": "1706.08822", "submitter": "Przemyslaw Biecek", "authors": "Przemyslaw Biecek, Marcin Kosinski", "title": "archivist: An R Package for Managing, Recording and Restoring Data\n  Analysis Results", "comments": "Submitted to JSS in 2015, conditionally accepted", "journal-ref": "Journal of Statistical Software (2017) vol 82 (11)", "doi": "10.18637/jss.v082.i11", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everything that exists in R is an object [Chambers2016]. This article\nexamines what would be possible if we kept copies of all R objects that have\never been created. Not only objects but also their properties, meta-data,\nrelations with other objects and information about context in which they were\ncreated.\n  We introduce archivist, an R package designed to improve the management of\nresults of data analysis. Key functionalities of this package include: (i)\nmanagement of local and remote repositories which contain R objects and their\nmeta-data (objects' properties and relations between them); (ii) archiving R\nobjects to repositories; (iii) sharing and retrieving objects (and it's\npedigree) by their unique hooks; (iv) searching for objects with specific\nproperties or relations to other objects; (v) verification of object's identity\nand context of it's creation.\n  The presented archivist package extends, in a combination with packages such\nas knitr and Sweave, the reproducible research paradigm by creating new ways to\nretrieve and validate previously calculated objects. These new features give a\nvariety of opportunities such as: sharing R objects within reports or articles;\nadding hooks to R objects in table or figure captions; interactive exploration\nof object repositories; caching function calls with their results; retrieving\nobject's pedigree (information about how the object was created); automated\ntracking of the performance of considered models, restoring R libraries to the\nstate in which object was archived.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 12:44:39 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Biecek", "Przemyslaw", ""], ["Kosinski", "Marcin", ""]]}, {"id": "1706.09693", "submitter": "Elizabeth Newman", "authors": "Elizabeth Newman, Misha Kilmer, and Lior Horesh", "title": "Image classification using local tensor singular value decompositions", "comments": "Submitted to IEEE CAMSAP 2017 Conference, 5 pages, 9 figures and\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From linear classifiers to neural networks, image classification has been a\nwidely explored topic in mathematics, and many algorithms have proven to be\neffective classifiers. However, the most accurate classifiers typically have\nsignificantly high storage costs, or require complicated procedures that may be\ncomputationally expensive. We present a novel (nonlinear) classification\napproach using truncation of local tensor singular value decompositions (tSVD)\nthat robustly offers accurate results, while maintaining manageable storage\ncosts. Our approach takes advantage of the optimality of the representation\nunder the tensor algebra described to determine to which class an image\nbelongs. We extend our approach to a method that can determine specific\npairwise match scores, which could be useful in, for example, object\nrecognition problems where pose/position are different. We demonstrate the\npromise of our new techniques on the MNIST data set.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 11:45:54 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Newman", "Elizabeth", ""], ["Kilmer", "Misha", ""], ["Horesh", "Lior", ""]]}, {"id": "1706.09873", "submitter": "Jordan Franks", "authors": "Jordan Franks and Matti Vihola", "title": "Importance sampling correction versus standard averages of reversible\n  MCMCs in terms of the asymptotic variance", "comments": "27 pages", "journal-ref": "Stochastic Processes and their Applications, 2020", "doi": "10.1016/j.spa.2020.05.006", "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish an ordering criterion for the asymptotic variances of two\nconsistent Markov chain Monte Carlo (MCMC) estimators: an importance sampling\n(IS) estimator, based on an approximate reversible chain and subsequent IS\nweighting, and a standard MCMC estimator, based on an exact reversible chain.\nEssentially, we relax the criterion of the Peskun type covariance ordering by\nconsidering two different invariant probabilities, and obtain, in place of a\nstrict ordering of asymptotic variances, a bound of the asymptotic variance of\nIS by that of the direct MCMC. Simple examples show that IS can have\narbitrarily better or worse asymptotic variance than Metropolis-Hastings and\ndelayed-acceptance (DA) MCMC. Our ordering implies that IS is guaranteed to be\ncompetitive up to a factor depending on the supremum of the (marginal) IS\nweight. We elaborate upon the criterion in case of unbiased estimators as part\nof an auxiliary variable framework. We show how the criterion implies\nasymptotic variance guarantees for IS in terms of pseudo-marginal (PM) and DA\ncorrections, essentially if the ratio of exact and approximate likelihoods is\nbounded. We also show that convergence of the IS chain can be less affected by\nunbounded high-variance unbiased estimators than PM and DA chains.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:37:23 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 17:42:31 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 16:57:22 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 12:01:56 GMT"}, {"version": "v5", "created": "Mon, 1 Jul 2019 10:38:37 GMT"}, {"version": "v6", "created": "Tue, 24 Mar 2020 16:07:03 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Franks", "Jordan", ""], ["Vihola", "Matti", ""]]}, {"id": "1706.09888", "submitter": "Quan Zhou", "authors": "Quan Zhou and Yongtao Guan", "title": "Fast model-fitting of Bayesian variable selection regression using the\n  iterative complex factorization algorithm", "comments": "Accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian variable selection regression (BVSR) is able to jointly analyze\ngenome-wide genetic datasets, but the slow computation via Markov chain Monte\nCarlo (MCMC) hampered its wide-spread usage. Here we present a novel iterative\nmethod to solve a special class of linear systems, which can increase the speed\nof the BVSR model-fitting tenfold. The iterative method hinges on the complex\nfactorization of the sum of two matrices and the solution path resides in the\ncomplex domain (instead of the real domain). Compared to the Gauss-Seidel\nmethod, the complex factorization converges almost instantaneously and its\nerror is several magnitude smaller than that of the Gauss-Seidel method. More\nimportantly, the error is always within the pre-specified precision while the\nGauss-Seidel method is not. For large problems with thousands of covariates,\nthe complex factorization is 10 -- 100 times faster than either the\nGauss-Seidel method or the direct method via the Cholesky decomposition. In\nBVSR, one needs to repetitively solve large penalized regression systems whose\ndesign matrices only change slightly between adjacent MCMC steps. This slight\nchange in design matrix enables the adaptation of the iterative complex\nfactorization method. The computational innovation will facilitate the\nwide-spread use of BVSR in reanalyzing genome-wide association datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:58:49 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 18:58:05 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 17:46:11 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 05:35:12 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Zhou", "Quan", ""], ["Guan", "Yongtao", ""]]}, {"id": "1706.10029", "submitter": "Cheng Ju", "authors": "Cheng Ju and Richard Wyss and Jessica M. Franklin and Sebastian\n  Schneeweiss and Jenny H\\\"aggstr\\\"om and Mark J. van der Laan", "title": "Collaborative-controlled LASSO for Constructing Propensity Score-based\n  Estimators in High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score (PS) based estimators are increasingly used for causal\ninference in observational studies. However, model selection for PS estimation\nin high-dimensional data has received little attention. In these settings, PS\nmodels have traditionally been selected based on the goodness-of-fit for the\ntreatment mechanism itself, without consideration of the causal parameter of\ninterest. Collaborative minimum loss-based estimation (C-TMLE) is a novel\nmethodology for causal inference that takes into account information on the\ncausal parameter of interest when selecting a PS model. This \"collaborative\nlearning\" considers variable associations with both treatment and outcome when\nselecting a PS model in order to minimize a bias-variance trade off in the\nestimated treatment effect. In this study, we introduce a novel approach for\ncollaborative model selection when using the LASSO estimator for PS estimation\nin high-dimensional covariate settings. To demonstrate the importance of\nselecting the PS model collaboratively, we designed quasi-experiments based on\na real electronic healthcare database, where only the potential outcomes were\nmanually generated, and the treatment and baseline covariates remained\nunchanged. Results showed that the C-TMLE algorithm outperformed other\ncompeting estimators for both point estimation and confidence interval\ncoverage. In addition, the PS model selected by C-TMLE could be applied to\nother PS-based estimators, which also resulted in substantive improvement for\nboth point estimation and confidence interval coverage. We illustrate the\ndiscussed concepts through an empirical example comparing the effects of\nnon-selective nonsteroidal anti-inflammatory drugs with selective COX-2\ninhibitors on gastrointestinal complications in a population of Medicare\nbeneficiaries.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 05:44:00 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Ju", "Cheng", ""], ["Wyss", "Richard", ""], ["Franklin", "Jessica M.", ""], ["Schneeweiss", "Sebastian", ""], ["H\u00e4ggstr\u00f6m", "Jenny", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1706.10096", "submitter": "Julien Stoehr", "authors": "Julien Stoehr, Alan Benson and Nial Friel", "title": "Noisy Hamiltonian Monte Carlo for doubly-intractable distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) has been progressively incorporated within the\nstatistician's toolbox as an alternative sampling method in settings when\nstandard Metropolis-Hastings is inefficient. HMC generates a Markov chain on an\naugmented state space with transitions based on a deterministic differential\nflow derived from Hamiltonian mechanics. In practice, the evolution of\nHamiltonian systems cannot be solved analytically, requiring numerical\nintegration schemes. Under numerical integration, the resulting approximate\nsolution no longer preserves the measure of the target distribution, therefore\nan accept-reject step is used to correct the bias. For doubly-intractable\ndistributions -- such as posterior distributions based on Gibbs random fields\n-- HMC suffers from some computational difficulties: computation of gradients\nin the differential flow and computation of the accept-reject proposals poses\ndifficulty. In this paper, we study the behaviour of HMC when these quantities\nare replaced by Monte Carlo estimates.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 10:04:46 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 18:20:49 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Stoehr", "Julien", ""], ["Benson", "Alan", ""], ["Friel", "Nial", ""]]}]