[{"id": "1405.0058", "submitter": "Filippo Radicchi", "authors": "Filippo Radicchi", "title": "Underestimating extreme events in power-law behavior due to\n  machine-dependent cutoffs", "comments": "8 pages, 4 figures", "journal-ref": "Phys. Rev. E 90, 050801 (2014)", "doi": "10.1103/PhysRevE.90.050801", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power-law distributions are typical macroscopic features occurring in almost\nall complex systems observable in nature. As a result, researchers in\nquantitative analyses must often generate random synthetic variates obeying\npower-law distributions. The task is usually performed through standard methods\nthat map uniform random variates into the desired probability space. Whereas\nall these algorithms are theoretically solid, in this paper we show that they\nare subject to severe machine-dependent limitations. As a result, two dramatic\nconsequences arise: (i) the sampling in the tail of the distribution is not\nrandom but deterministic; (ii) the moments of the sample distribution, which\nare theoretically expected to diverge as functions of the sample sizes,\nconverge instead to finite values. We provide quantitative indications for the\nrange of distribution parameters that can be safely handled by standard\nlibraries used in computational analyses. Whereas our findings indicate\npossible reinterpretations of numerical results obtained through flawed\nsampling methodologies, they also pave the way for the search for a concrete\nsolution to this central issue shared by all quantitative sciences dealing with\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 23:40:52 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 22:33:19 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Radicchi", "Filippo", ""]]}, {"id": "1405.0102", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Capacity estimation of two-dimensional channels using Sequential Monte\n  Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a new Sequential-Monte-Carlo-based algorithm to estimate the\ncapacity of two-dimensional channel models. The focus is on computing the\nnoiseless capacity of the 2-D one-infinity run-length limited constrained\nchannel, but the underlying idea is generally applicable. The proposed\nalgorithm is profiled against a state-of-the-art method, yielding more than an\norder of magnitude improvement in estimation accuracy for a given computation\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 06:06:45 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 10:36:37 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1405.0182", "submitter": "Natesh Pillai", "authors": "Natesh S. Pillai and Aaron Smith", "title": "Ergodicity of Approximate MCMC Chains with Applications to Large Data\n  Sets", "comments": "Substantially revised and shortened from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern applications, difficulty in evaluating the posterior density\nmakes performing even a single MCMC step slow. This difficulty can be caused by\nintractable likelihood functions, but also appears for routine problems with\nlarge data sets. Many researchers have responded by running approximate\nversions of MCMC algorithms. In this note, we develop quantitative bounds for\nshowing the ergodicity of these approximate samplers. We then use these bounds\nto study the bias-variance trade-off of approximate MCMC algorithms. We apply\nour results to simple versions of recently proposed algorithms, including a\nvariant of the \"austerity\" framework of Korratikara et al.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 15:13:51 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 18:36:27 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1405.0223", "submitter": "Sivaram Ambikasaran", "authors": "Sivaram Ambikasaran, Michael O'Neil, Karan Raj Singh", "title": "Fast symmetric factorization of hierarchical matrices with applications", "comments": "18 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.flu-dyn stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast direct algorithm for computing symmetric factorizations,\ni.e. $A = WW^T$, of symmetric positive-definite hierarchical matrices with\nweak-admissibility conditions. The computational cost for the symmetric\nfactorization scales as $\\mathcal{O}(n \\log^2 n)$ for hierarchically\noff-diagonal low-rank matrices. Once this factorization is obtained, the cost\nfor inversion, application, and determinant computation scales as\n$\\mathcal{O}(n \\log n)$. In particular, this allows for the near optimal\ngeneration of correlated random variates in the case where $A$ is a covariance\nmatrix. This symmetric factorization algorithm depends on two key ingredients.\nFirst, we present a novel symmetric factorization formula for low-rank updates\nto the identity of the form $I+UKU^T$. This factorization can be computed in\n$\\mathcal{O}(n)$ time if the rank of the perturbation is sufficiently small.\nSecond, combining this formula with a recursive divide-and-conquer strategy,\nnear linear complexity symmetric factorizations for hierarchically structured\nmatrices can be obtained. We present numerical results for matrices relevant to\nproblems in probability \\& statistics (Gaussian processes), interpolation\n(Radial basis functions), and Brownian dynamics calculations in fluid mechanics\n(the Rotne-Prager-Yamakawa tensor).\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 17:18:17 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 08:11:13 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Ambikasaran", "Sivaram", ""], ["O'Neil", "Michael", ""], ["Singh", "Karan Raj", ""]]}, {"id": "1405.0294", "submitter": "Julio C\\'esar Hern\\'andez S\\'anchez", "authors": "Jos\\'e Luis Vicente-Villard\\'on, Julio C\\'esar Hern\\'andez S\\'anchez", "title": "Logistic Biplots for Ordinal Data with an Application to Job\n  Satisfaction of Doctorate Degree Holders in Spain", "comments": "26 pages, 11 figures,2 tables. arXiv admin note: text overlap with\n  arXiv:1309.5486", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biplot Methods allow for the simultaneous representation of individuals and\nvariables of a data matrix. For Binary or Nominal data, Logistic biplots have\nbeen recently developed to extend the classical linear representations for\ncontinuous data. When data are ordinal, linear, binary or nominal logistic\nbiplots are not adequate and techniques as Categorical Principal Component\nAnalysis (CATPCA) or Item Response Theory (IRT) for ordinal items should be\nused instead.\n  In this paper we extend the Biplot to ordinal data. The resulting method is\ntermed Ordinal Logistic Biplot (OLB). Row scores are computed to have ordinal\nlogistic responses along the dimensions and column parameters produce logistic\nresponse surfaces that, projected onto the space spanned by the row scores,\ndefine a linear biplot. A proportional odds model is used, obtaining a\nmultidimensional model known as graded response model in the Item Response\nTheory literature. We study the geometry of such a representation and construct\ncomputational algorithms for the estimation of parameters and the calculation\nof prediction directions. Ordinal Logistic Biplots extend both CATPCA and IRT\nin the sense that gives a graphical representation for IRT similar to the\nbiplot for CATPCA.\n  The main theoretical results are applied to the study of job satisfaction of\ndoctorate (PhD) holders in Spain. Holders of doctorate degrees or other\nresearch qualifications are crucial to the creation, commercialization and\ndissemination of knowledge and to innovation. The proposed methods are used to\nextract useful information from the Spanish data from the international 'Survey\non the careers of doctorate holders (CDH)', jointly carried out Eurostat, the\nOrganisation for Economic Co-operation and Development (OECD) and UNESCO's\nInstitute for Statistics (UIS).\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 20:06:17 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Vicente-Villard\u00f3n", "Jos\u00e9 Luis", ""], ["S\u00e1nchez", "Julio C\u00e9sar Hern\u00e1ndez", ""]]}, {"id": "1405.0362", "submitter": "Yves Rozenholc", "authors": "Nelo Magalh\\~aes and Yves Rozenholc", "title": "An efficient algorithm for T-estimation", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an efficient and exact algorithm, together with a faster but\napproximate version, which implements with a sub-quadratic complexity the\nhold-out derived from T-estimation. We study empirically the performance of\nthis hold-out in the context of density estimation considering well-known\ncompetitors (hold-out derived from least-squares or Kullback-Leibler\ndivergence, model selection procedures, etc.) and classical problems including\nhistogram or bandwidth selection. Our algorithms are integrated in a companion\nR-package called {\\it Density.T.HoldOut} available on the CRAN:\n{\\url{http://cran.r-project.org/web/packages/Density.T.HoldOut/index.html}}.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 09:00:01 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 21:12:12 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Magalh\u00e3es", "Nelo", ""], ["Rozenholc", "Yves", ""]]}, {"id": "1405.0377", "submitter": "Antonio Punzo", "authors": "Antonio Punzo, Ryan P. Browne, Paul D. McNicholas", "title": "Hypothesis Testing for Parsimonious Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixture models with eigen-decomposed covariance structures make up\nthe most popular family of mixture models for clustering and classification,\ni.e., the Gaussian parsimonious clustering models (GPCM). Although the GPCM\nfamily has been used for almost 20 years, selecting the best member of the\nfamily in a given situation remains a troublesome problem. Likelihood ratio\ntests are developed to tackle this problems. These likelihood ratio tests use\nthe heteroscedastic model under the alternative hypothesis but provide much\nmore flexibility and real-world applicability than previous approaches that\ncompare the homoscedastic Gaussian mixture versus the heteroscedastic one.\nAlong the way, a novel maximum likelihood estimation procedure is developed for\ntwo members of the GPCM family. Simulations show that the $\\chi^2$ reference\ndistribution gives reasonable approximation for the LR statistics only when the\nsample size is considerable and when the mixture components are well separated;\naccordingly, following Lo (2008), a parametric bootstrap is adopted.\nFurthermore, by generalizing the idea of Greselin and Punzo (2013) to the\nclustering context, a closed testing procedure, having the defined likelihood\nratio tests as local tests, is introduced to assess a unique model in the\ngeneral family. The advantages of this likelihood ratio testing procedure are\nillustrated via an application to the well-known Iris data set.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 10:33:49 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Punzo", "Antonio", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1405.0506", "submitter": "Jesse Windle", "authors": "Jesse Windle and Nicholas G. Polson and James G. Scott", "title": "Sampling Polya-Gamma random variates: alternate and approximate\n  techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently sampling from the P\\'olya-Gamma distribution, ${PG}(b,z)$, is an\nessential element of P\\'olya-Gamma data augmentation. Polson et. al (2013) show\nhow to efficiently sample from the ${PG}(1,z)$ distribution. We build two new\nsamplers that offer improved performance when sampling from the ${PG}(b,z)$\ndistribution and $b$ is not unity.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 20:37:59 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Windle", "Jesse", ""], ["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1405.0605", "submitter": "Enkelejd Hashorva", "authors": "D. Kortschak and E. Hashorva", "title": "Second order asymptotics of aggregated log-elliptical risk", "comments": "in press in Methodology and Computing in Applied Probability", "journal-ref": null, "doi": "10.1007/s11009-013-9356-5", "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we establish the error rate of first order asymptotic\napproximation for the tail probability of sums of log-elliptical risks. Our\napproach is motivated by extreme value theory which allows us to impose only\nsome weak asymptotic conditions satisfied in particular by log-normal risks.\nGiven the wide range of applications of the log-normal model in finance and\ninsurance our result is of interest for both rare-event simulations and\nnumerical calculations. We present numerical examples which illustrate that the\nsecond order approximation derived in this paper significantly improves over\nthe first order approximation.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 16:57:10 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Kortschak", "D.", ""], ["Hashorva", "E.", ""]]}, {"id": "1405.0913", "submitter": "Kushal  Dey", "authors": "Kushal Kr Dey, Sourabh Bhattacharya", "title": "A Brief Review of Optimal Scaling of the Main MCMC Approaches and\n  Optimal Scaling of Additive TMCMC Under Non-Regular Cases", "comments": "38 pages, this version will appear in Brazilian Journal of\n  Probability and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very recently, Transformation based Markov Chain Monte Carlo (TMCMC) was\nproposed by Dutta and Bhattcharya (2013) as a much efficient alternative to the\nMetropolis-Hastings algorithm, Random Walk Metropolis (RWM) algorithm,\nespecially in high dimensions. The main advantage of this algorithm is that it\nsimultaneously updates all components of a high dimensional parameter by some\nappropriate deterministic transformation of a single random variable, thereby\nreducing time complexity and enhancing the acceptance rate. The optimal scaling\nof the additive TMCMC approach has already been studied for the Gaussian\nproposal density by Dey and Bhattacharya(2013). In this paper, we discuss\ndiffusion-based optimal scaling behavior for non-Gaussian proposal densities -\nin particular, uniform, Student's t and Cauchy proposals. We also consider\ndiffusion based optimal scaling for non-Gaussian proposals when the target\ndensity is discontinuous. In the case of the Random Walk metropolis (RWM)\nalgorithm these non-regular situations have been studied by Neal and Roberts\n(2011) in terms of expected squared jumping distance (ESJD), but the diffusion\nbased approach has not been considered. Although we could not formally prove\nour diffusion result for the Cauchy proposal, simulation based results led us\nto a conjecture that the diffusion result still holds for the Cauchy case. We\ncompare our diffusion based TMCMC approach with that of ESJD based RWM approach\nfor the very challenging Cauchy proposal case, showing that our former approach\nclearly outperforms the latter.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 14:54:39 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 05:41:42 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2015 18:51:51 GMT"}, {"version": "v4", "created": "Mon, 23 Jan 2017 14:42:45 GMT"}, {"version": "v5", "created": "Tue, 25 Jul 2017 03:11:59 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Dey", "Kushal Kr", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1405.0922", "submitter": "Aaron Fisher", "authors": "Aaron Fisher, Brian Caffo, Brian Schwartz and Vadim Zipunnikov", "title": "Fast, Exact Bootstrap Principal Component Analysis for p>1 million", "comments": "25 pages, including 9 figures and link to R package. 2014-05-14\n  update: final formatting edits for journal submission, condensed figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many have suggested a bootstrap procedure for estimating the sampling\nvariability of principal component analysis (PCA) results. However, when the\nnumber of measurements per subject ($p$) is much larger than the number of\nsubjects ($n$), the challenge of calculating and storing the leading principal\ncomponents from each bootstrap sample can be computationally infeasible. To\naddress this, we outline methods for fast, exact calculation of bootstrap\nprincipal components, eigenvalues, and scores. Our methods leverage the fact\nthat all bootstrap samples occupy the same $n$-dimensional subspace as the\noriginal sample. As a result, all bootstrap principal components are limited to\nthe same $n$-dimensional subspace and can be efficiently represented by their\nlow dimensional coordinates in that subspace. Several uncertainty metrics can\nbe computed solely based on the bootstrap distribution of these low dimensional\ncoordinates, without calculating or storing the $p$-dimensional bootstrap\ncomponents. Fast bootstrap PCA is applied to a dataset of sleep\nelectroencephalogram (EEG) recordings ($p=900$, $n=392$), and to a dataset of\nbrain magnetic resonance images (MRIs) ($p\\approx$ 3 million, $n=352$). For the\nbrain MRI dataset, our method allows for standard errors for the first 3\nprincipal components based on 1000 bootstrap samples to be calculated on a\nstandard laptop in 47 minutes, as opposed to approximately 4 days with standard\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 15:19:42 GMT"}, {"version": "v2", "created": "Tue, 6 May 2014 22:04:00 GMT"}, {"version": "v3", "created": "Wed, 14 May 2014 14:12:12 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Fisher", "Aaron", ""], ["Caffo", "Brian", ""], ["Schwartz", "Brian", ""], ["Zipunnikov", "Vadim", ""]]}, {"id": "1405.1250", "submitter": "Wilhelmiina H\\\"am\\\"al\\\"ainen", "authors": "Wilhelmiina H\\\"am\\\"al\\\"ainen", "title": "New tight approximations for Fisher's exact test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's exact test is often a preferred method to estimate the significance\nof statistical dependence. However, in large data sets the test is usually too\nworksome to be applied, especially in an exhaustive search (data mining). The\ntraditional solution is to approximate the significance with the\n$\\chi^2$-measure, but the accuracy is often unacceptable. As a solution, we\nintroduce a family of upper bounds, which are fast to calculate and approximate\nFisher's $p$-value accurately. In addition, the new approximations are not\nsensitive to the data size, distribution, or smallest expected counts like the\n$\\chi^2$-based approximation. According to both theoretical and experimental\nanalysis, the new approximations produce accurate results for all sufficiently\nstrong dependencies. The basic form of the approximation can fail with weak\ndependencies, but the general form of the upper bounds can be adjusted to be\narbitrarily accurate.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 12:58:39 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Wilhelmiina", ""]]}, {"id": "1405.1383", "submitter": "Art Owen", "authors": "Jessica L. Larson and Art B. Owen", "title": "Moment based gene set tests", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\bf Motivation:} Permutation-based gene set tests are standard approaches\nfor testing relationshi ps between collections of related genes and an outcome\nof interest in high throughput expression analyses. Using $M$ random\npermutations, one can attain $p$-values as small as $1/(M+1)$. When many gene\nsets are tested, we need smaller $p$-values, hence larger $M$, to achieve\nsignificance while accounting for the n umber of simultaneous tests being made.\nAs a result, the number of permutations to be done rises along with the cost\nper permutation. To reduce this cost, we seek parametric approximations to the\npermutation distributions for gene set tes ts.\n  {\\bf Results:} We focus on two gene set methods related to sums and sums of\nsquared $t$ statistics. Our approach calculates exact relevant moments of a\nweighted sum of (squared) test statistics under permutation. We find\nmoment-based gene set enrichment $p$-values that closely approximate the\npermutation method $p$-values. The computational cost of our algorithm for\nlinear statistics is on the order of doing $|G|$ permutations, where $|G|$ is\nthe number of genes in set $G$. For the quadratic statistics, the cost is on\nthe order of $|G|^2$ permutations which is orders of magnitude faster than\nnaive permutation. We applied the permutation approximation method to three\npublic Parkinson's Disease expression datasets and discovered enriched gene\nsets not previously discussed. In the analysis of these experiments with our\nmethod, we are able to remove the granularity effects of permutation analyses\nand have a substantial computational speedup with little cost to accura cy.\n  {\\bf Availability:} Methods available as a Bioconductor package, npGSEA\n(www.bioconductor.org).\n  {\\bf Contact:} {larson.jessica@gene.com} \\end{abstract}\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 17:59:37 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Larson", "Jessica L.", ""], ["Owen", "Art B.", ""]]}, {"id": "1405.1491", "submitter": "Xumeng Cao", "authors": "Xumeng Cao", "title": "Demonstration of Enhanced Monte Carlo Computation of the Fisher\n  Information for Complex Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher information matrix summarizes the amount of information in a set\nof data relative to the quantities of interest. There are many applications of\nthe information matrix in statistical modeling, system identification and\nparameter estimation. This short paper reviews a feedback-based method and an\nindependent perturbation approach for computing the information matrix for\ncomplex problems, where a closed form of the information matrix is not\nachievable. We show through numerical examples how these methods improve the\naccuracy of the estimate of the information matrix compared to the basic\nresampling-based approach. Some relevant theory is summarized.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 02:42:55 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Cao", "Xumeng", ""]]}, {"id": "1405.1792", "submitter": "Ping Li", "authors": "Radhendushka Srivastava, Ping Li, David Ruppert", "title": "RAPTT: An Exact Two-Sample Test in High Dimensions Using Random\n  Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensions, the classical Hotelling's $T^2$ test tends to have low\npower or becomes undefined due to singularity of the sample covariance matrix.\nIn this paper, this problem is overcome by projecting the data matrix onto\nlower dimensional subspaces through multiplication by random matrices. We\npropose RAPTT (RAndom Projection T-Test), an exact test for equality of means\nof two normal populations based on projected lower dimensional data. RAPTT does\nnot require any constraints on the dimension of the data or the sample size. A\nsimulation study indicates that in high dimensions the power of this test is\noften greater than that of competing tests. The advantage of RAPTT is\nillustrated on high-dimensional gene expression data involving the\ndiscrimination of tumor and normal colon tissues.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 02:09:51 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Srivastava", "Radhendushka", ""], ["Li", "Ping", ""], ["Ruppert", "David", ""]]}, {"id": "1405.1796", "submitter": "Shifeng Xiong Doc", "authors": "Ke Zhang, Fan Yin, Shifeng Xiong", "title": "Comparisons of penalized least squares methods by simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized least squares methods are commonly used for simultaneous estimation\nand variable selection in high-dimensional linear models. In this paper we\ncompare several prevailing methods including the lasso, nonnegative garrote,\nand SCAD in this area through Monte Carlo simulations. Criterion for evaluating\nthese methods in terms of variable selection and estimation are presented. This\npaper focuses on the traditional n > p cases. For larger p, our results are\nstill helpful to practitioners after the dimensionality is reduced by a\nscreening method. K\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 03:18:27 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Zhang", "Ke", ""], ["Yin", "Fan", ""], ["Xiong", "Shifeng", ""]]}, {"id": "1405.2673", "submitter": "Pierre Minvielle", "authors": "P. Minvielle, A. Todeschini, F. Caron, P. Del Moral", "title": "Particle MCMC for Bayesian Microwave Control", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/542/1/012007", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of local radioelectric property estimation from\nglobal electromagnetic scattering measurements. This challenging ill-posed high\ndimensional inverse problem can be explored by intensive computations of a\nparallel Maxwell solver on a petaflopic supercomputer. Then, it is shown how\nBayesian inference can be perfomed with a Particle Marginal Metropolis-Hastings\n(PMMH) approach, which includes a Rao-Blackwellised Sequential Monte Carlo\nalgorithm with interacting Kalman filters. Material properties, including a\nmultiple components \"Debye relaxation\"/\"Lorenzian resonant\" material model, are\nestimated; it is illustrated on synthetic data. Eventually, we propose\ndifferent ways to deal with higher dimensional problems, from parallelization\nto the original introduction of efficient sequential data assimilation\ntechniques, widely used in weather forecasting, oceanography, geophysics, etc.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 08:43:56 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Minvielle", "P.", ""], ["Todeschini", "A.", ""], ["Caron", "F.", ""], ["Del Moral", "P.", ""]]}, {"id": "1405.2800", "submitter": "Cl\\'ement Walter", "authors": "Cl\\'ement Walter", "title": "Moving Particles: a parallel optimal Multilevel Splitting method with\n  application in quantiles estimation and meta-model based algorithms", "comments": "26 pages (included 6 pages of appendix), 20 figures", "journal-ref": null, "doi": "10.1016/j.strusafe.2015.02.002", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the issue of estimating small probabilities p, ie. measuring a\nrare domain F = {x | g(x) > q} with respect to the distribution of a random\nvector X, Multilevel Splitting strategies (also called Subset Simulation) aim\nat writing F as an intersection of less rare events (nested subsets) such that\ntheir measures are conditionally easily computable. However the definition of\nan appropriate sequence of nested subsets remains an open issue.\n  We introduce here a new approach to Multilevel Splitting methods in terms of\na move of particles in the input space. This allows us to derive two main\nresults: (1) the number of samples required to get a realisation of X in F is\ndrastically reduced, following a Poisson law with parameter log 1/p (to be\ncompared with 1/p for naive Monte-Carlo); and (2) we get a parallel optimal\nMultilevel Splitting algorithm where there is indeed no subset to define any\nmore.\n  We also apply result (1) in quantile estimation producing a new parallel\nalgorithm and derive a new strategy for the construction of first Design Of\nExperiments in meta-model based algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 15:24:53 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Walter", "Cl\u00e9ment", ""]]}, {"id": "1405.3034", "submitter": "Onkar Dalal", "authors": "Onkar Dalal and Bala Rajaratnam", "title": "G-AMA: Sparse Gaussian graphical model estimation via alternating\n  minimization", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been recently proposed for estimating sparse Gaussian\ngraphical models using $\\ell_{1}$ regularization on the inverse covariance\nmatrix. Despite recent advances, contemporary applications require methods that\nare even faster in order to handle ill-conditioned high dimensional modern day\ndatasets. In this paper, we propose a new method, G-AMA, to solve the sparse\ninverse covariance estimation problem using Alternating Minimization Algorithm\n(AMA), that effectively works as a proximal gradient algorithm on the dual\nproblem. Our approach has several novel advantages over existing methods.\nFirst, we demonstrate that G-AMA is faster than the previous best algorithms by\nmany orders of magnitude and is thus an ideal approach for modern high\nthroughput applications. Second, global linear convergence of G-AMA is\ndemonstrated rigorously, underscoring its good theoretical properties. Third,\nthe dual algorithm operates on the covariance matrix, and thus easily\nfacilitates incorporating additional constraints on pairwise/marginal\nrelationships between feature pairs based on domain specific knowledge. Over\nand above estimating a sparse inverse covariance matrix, we also illustrate how\nto (1) incorporate constraints on the (bivariate) correlations and, (2)\nincorporate equality (equisparsity) or linear constraints between individual\ninverse covariance elements. Fourth, we also show that G-AMA is better adept at\nhandling extremely ill-conditioned problems, as is often the case with real\ndata. The methodology is demonstrated on both simulated and real datasets to\nillustrate its superior performance over recently proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 04:43:50 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 06:05:06 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Dalal", "Onkar", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1405.3222", "submitter": "Taylor Arnold", "authors": "Taylor Arnold and Ryan Tibshirani", "title": "Efficient Implementations of the Generalized Lasso Dual Path Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider efficient implementations of the generalized lasso dual path\nalgorithm of Tibshirani and Taylor (2011). We first describe a generic approach\nthat covers any penalty matrix D and any (full column rank) matrix X of\npredictor variables. We then describe fast implementations for the special\ncases of trend filtering problems, fused lasso problems, and sparse fused lasso\nproblems, both with X=I and a general matrix X. These specialized\nimplementations offer a considerable improvement over the generic\nimplementation, both in terms of numerical stability and efficiency of the\nsolution path computation. These algorithms are all available for use in the\ngenlasso R package, which can be found in the CRAN repository.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:42:45 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 16:44:50 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Arnold", "Taylor", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1405.3319", "submitter": "Longhai Li", "authors": "Longhai Li and Weixin Yao", "title": "Fully Bayesian Logistic Regression with Hyper-Lasso Priors for\n  High-dimensional Feature Selection", "comments": "33 pages. arXiv admin note: substantial text overlap with\n  arXiv:1308.4690", "journal-ref": "Journal of Statistical Computation and Simulation, 2018, 88:14,\n  2827-2851", "doi": "10.1080/00949655.2018.1490418", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional feature selection arises in many areas of modern science.\nFor example, in genomic research we want to find the genes that can be used to\nseparate tissues of different classes (e.g. cancer and normal) from tens of\nthousands of genes that are active (expressed) in certain tissue cells. To this\nend, we wish to fit regression and classification models with a large number of\nfeatures (also called variables, predictors). In the past decade, penalized\nlikelihood methods for fitting regression models based on hyper-LASSO\npenalization have received increasing attention in the literature. However,\nfully Bayesian methods that use Markov chain Monte Carlo (MCMC) are still in\nlack of development in the literature. In this paper we introduce an MCMC\n(fully Bayesian) method for learning severely multi-modal posteriors of\nlogistic regression models based on hyper-LASSO priors (non-convex penalties).\nOur MCMC algorithm uses Hamiltonian Monte Carlo in a restricted Gibbs sampling\nframework; we call our method Bayesian logistic regression with hyper-LASSO\n(BLRHL) priors. We have used simulation studies and real data analysis to\ndemonstrate the superior performance of hyper-LASSO priors, and to investigate\nthe issues of choosing heaviness and scale of hyper-LASSO priors.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 22:31:09 GMT"}, {"version": "v2", "created": "Mon, 19 May 2014 19:03:36 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 16:50:44 GMT"}, {"version": "v4", "created": "Sat, 12 May 2018 03:10:10 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Li", "Longhai", ""], ["Yao", "Weixin", ""]]}, {"id": "1405.3718", "submitter": "Fabio M. Bayer Ph.D", "authors": "F\\'abio M. Bayer and Francisco Cribari-Neto", "title": "Model selection criteria in beta regression with varying dispersion", "comments": "22 pages, 3 figures, 7 tables", "journal-ref": "Communications in Statistics - Simulation and Computation, 2017,\n  Vol 46, Issue 1", "doi": "10.1016/j.cnsns.2016.10.016", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of model selection in beta regressions with varying\ndispersion. The model consists of two submodels, namely: for the mean and for\nthe dispersion. Our focus is on the selection of the covariates for each\nsubmodel. Our Monte Carlo evidence reveals that the joint selection of\ncovariates for the two submodels is not accurate in finite samples. We\nintroduce two new model selection criteria that explicitly account for varying\ndispersion and propose a fast two step model selection scheme which is\nconsiderably more accurate and is computationally less costly than usual joint\nmodel selection. Monte Carlo evidence is presented and discussed. We also\npresent the results of an empirical application.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 00:08:14 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 19:59:30 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Bayer", "F\u00e1bio M.", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "1405.4081", "submitter": "Lawrence Murray", "authors": "Pierre Del Moral and Lawrence M. Murray", "title": "Sequential Monte Carlo with Highly Informative Observations", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose sequential Monte Carlo (SMC) methods for sampling the posterior\ndistribution of state-space models under highly informative observation\nregimes, a situation in which standard SMC methods can perform poorly. A\nspecial case is simulating bridges between given initial and final values. The\nbasic idea is to introduce a schedule of intermediate weighting and resampling\ntimes between observation times, which guide particles towards the final state.\nThis can always be done for continuous-time models, and may be done for\ndiscrete-time models under sparse observation regimes; our main focus is on\ncontinuous-time diffusion processes. The methods are broadly applicable in that\nthey support multivariate models with partial observation, do not require\nsimulation of the backward transition (which is often unavailable), and, where\npossible, avoid pointwise evaluation of the forward transition. When simulating\nbridges, the last cannot be avoided entirely without concessions, and we\nsuggest an epsilon-ball approach (reminiscent of Approximate Bayesian\nComputation) as a workaround. Compared to the bootstrap particle filter, the\nnew methods deliver substantially reduced mean squared error in normalising\nconstant estimates, even after accounting for execution time. The methods are\ndemonstrated for state estimation with two toy examples, and for parameter\nestimation (within a particle marginal Metropolis--Hastings sampler) with three\napplied examples in econometrics, epidemiology and marine biogeochemistry.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 07:53:07 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 15:44:16 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Del Moral", "Pierre", ""], ["Murray", "Lawrence M.", ""]]}, {"id": "1405.4141", "submitter": "Alexander Matthews BA MSci MA (Cantab)", "authors": "Alexander G. de. G Matthews and Zoubin Ghahramani", "title": "Classification using log Gaussian Cox processes", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  McCullagh and Yang (2006) suggest a family of classification algorithms based\non Cox processes. We further investigate the log Gaussian variant which has a\nnumber of appealing properties. Conditioned on the covariates, the distribution\nover labels is given by a type of conditional Markov random field. In the\nsupervised case, computation of the predictive probability of a single test\npoint scales linearly with the number of training points and the multiclass\ngeneralization is straightforward. We show new links between the supervised\nmethod and classical nonparametric methods. We give a detailed analysis of the\npairwise graph representable Markov random field, which we use to extend the\nmodel to semi-supervised learning problems, and propose an inference method\nbased on graph min-cuts. We give the first experimental analysis on supervised\nand semi-supervised datasets and show good empirical performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 12:10:12 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 13:02:49 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Matthews", "Alexander G. de. G", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1405.4323", "submitter": "Emilian Vankov", "authors": "Emilian Vankov, Katherine B. Ensor", "title": "Stochastic Volatility Filtering with Intractable Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with particle filtering for $\\alpha$-stable\nstochastic volatility models. The $\\alpha$-stable distribution provides a\nflexible framework for modeling asymmetry and heavy tails, which is useful when\nmodeling financial returns. An issue with this distributional assumption is the\nlack of a closed form for the probability density function. To estimate the\nvolatility of financial returns in this setting, we develop a novel auxiliary\nparticle filter. The algorithm we develop can be easily applied to any hidden\nMarkov model for which the likelihood function is intractable or\ncomputationally expensive. The approximate target distribution of our auxiliary\nfilter is based on the idea of approximate Bayesian computation (ABC). ABC\nmethods allow for inference on posterior quantities in situations when the\nlikelihood of the underlying model is not available in closed form, but\nsimulating samples from it is possible. The ABC auxiliary particle filter\n(ABC-APF) that we propose provides not only a good alternative to state\nestimation in stochastic volatility models, but it also improves on the\nexisting ABC literature. It allows for more flexibility in state estimation\nwhile improving on the accuracy through better proposal distributions in cases\nwhen the optimal importance density of the filter is unavailable in closed\nform. We assess the performance of the ABC-APF on a simulated dataset from the\n$\\alpha$-stable stochastic volatility model and compare it to other currently\nexisting ABC filters.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 22:19:01 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Vankov", "Emilian", ""], ["Ensor", "Katherine B.", ""]]}, {"id": "1405.4525", "submitter": "Fabio M. Bayer Ph.D", "authors": "F\\'abio M. Bayer and Francisco Cribari-Neto", "title": "Bootstrap-based model selection criteria for beta regressions", "comments": "27 pages, 1 figure, 7 tables", "journal-ref": "TEST, Volume 24, Issue 4, pp 776-795 (2015)", "doi": "10.1007/s11749-015-0434-6", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Akaike information criterion (AIC) is a model selection criterion widely\nused in practical applications. The AIC is an estimator of the log-likelihood\nexpected value, and measures the discrepancy between the true model and the\nestimated model. In small samples the AIC is biased and tends to select\noverparameterized models. To circumvent that problem, we propose two new\nselection criteria, namely: the bootstrapped likelihood quasi-CV (BQCV) and its\n632QCV variant. We use Monte Carlo simulation to compare the finite sample\nperformances of the two proposed criteria to those of the AIC and its\nvariations that use the bootstrapped log-likelihood in the class of varying\ndispersion beta regressions. The numerical evidence shows that the proposed\nmodel selection criteria perform well in small samples. We also present and\ndiscuss and empirical application.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 16:47:49 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Bayer", "F\u00e1bio M.", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "1405.5576", "submitter": "Sam Davanloo", "authors": "Sam Davanloo Tajbakhsh, Necdet Serhat Aybat, Enrique Del Castillo", "title": "On the Theoretical Guarantees for Parameter Estimation of Gaussian\n  Random Field Models: A Sparse Precision Matrix Approach", "comments": "Two new sections 4.2.1, 5.1 and four new figures 5-8 are added. The\n  title, abstract, and concluding remarks are revised", "journal-ref": "Journal of Machine Learning Research. 21 (2020) 1-41", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative methods for fitting a Gaussian Random Field (GRF) model via maximum\nlikelihood (ML) estimation requires solving a nonconvex optimization problem.\nThe problem is aggravated for anisotropic GRFs where the number of covariance\nfunction parameters increases with the dimension. Even evaluation of the\nlikelihood function requires $O(n^3)$ floating point operations, where $n$\ndenotes the number of data locations. In this paper, we propose a new two-stage\nprocedure to estimate the parameters of second-order stationary GRFs. First, a\nconvex likelihood problem regularized with a weighted $\\ell_1$-norm, utilizing\nthe available distance information between observation locations, is solved to\nfit a sparse precision (inverse covariance) matrix to the observed data.\nSecond, the parameters of the covariance function are estimated by solving a\nleast squares problem. Theoretical error bounds for the solutions of stage I\nand II problems are provided, and their tightness are investigated.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 23:54:14 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 19:47:11 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2015 04:09:39 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 03:52:45 GMT"}, {"version": "v5", "created": "Thu, 6 Feb 2020 22:52:34 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Tajbakhsh", "Sam Davanloo", ""], ["Aybat", "Necdet Serhat", ""], ["Del Castillo", "Enrique", ""]]}, {"id": "1405.5740", "submitter": "Bruno Sudret", "authors": "Bruno Sudret and Chu Van Mai", "title": "Computing derivative-based global sensitivity measures using polynomial\n  chaos expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of computer experiments sensitivity analysis aims at quantifying\nthe relative importance of each input parameter (or combinations thereof) of a\ncomputational model with respect to the model output uncertainty. Variance\ndecomposition methods leading to the well-known Sobol' indices are recognized\nas accurate techniques, at a rather high computational cost though. The use of\npolynomial chaos expansions (PCE) to compute Sobol' indices has allowed to\nalleviate the computational burden though. However, when dealing with large\ndimensional input vectors, it is good practice to first use screening methods\nin order to discard unimportant variables. The {\\em derivative-based global\nsensitivity measures} (DGSM) have been developed recently in this respect. In\nthis paper we show how polynomial chaos expansions may be used to compute\nanalytically DGSMs as a mere post-processing. This requires the analytical\nderivation of derivatives of the orthonormal polynomials which enter PC\nexpansions. The efficiency of the approach is illustrated on two well-known\nbenchmark problems in sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 13:09:34 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Sudret", "Bruno", ""], ["Van Mai", "Chu", ""]]}, {"id": "1405.5841", "submitter": "Chanchal Kundu", "authors": "Asok K. Nanda, Sudhansu S. Maiti, Chanchal Kundu and Amarjit Kundu", "title": "Parameter Estimates of General Failure Rate Model: A Bayesian Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The failure rate function plays an important role in studying the lifetime\ndistributions in reliability theory and life testing models. A study of the\ngeneral failure rate model $r(t)=a+bt^{\\theta-1}$, under squared error loss\nfunction taking $a$ and $b$ independent exponential random variables has been\nanalyzed in the literature. In this article, we consider $a$ and $b$ not\nnecessarily independent. The estimates of the parameters $a$ and $b$ under\nsquared error loss, linex loss and entropy loss functions are obtained here.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 17:58:37 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Nanda", "Asok K.", ""], ["Maiti", "Sudhansu S.", ""], ["Kundu", "Chanchal", ""], ["Kundu", "Amarjit", ""]]}, {"id": "1405.6210", "submitter": "Jacob Bien", "authors": "Jacob Bien, Florentina Bunea, Luo Xiao", "title": "Convex Banding of the Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new sparse estimator of the covariance matrix for\nhigh-dimensional models in which the variables have a known ordering. Our\nestimator, which is the solution to a convex optimization problem, is\nequivalently expressed as an estimator which tapers the sample covariance\nmatrix by a Toeplitz, sparsely-banded, data-adaptive matrix. As a result of\nthis adaptivity, the convex banding estimator enjoys theoretical optimality\nproperties not attained by previous banding or tapered estimators. In\nparticular, our convex banding estimator is minimax rate adaptive in Frobenius\nand operator norms, up to log factors, over commonly-studied classes of\ncovariance matrices, and over more general classes. Furthermore, it correctly\nrecovers the bandwidth when the true covariance is exactly banded. Our convex\nformulation admits a simple and efficient algorithm. Empirical studies\ndemonstrate its practical effectiveness and illustrate that our exactly-banded\nestimator works well even when the true covariance matrix is only close to a\nbanded matrix, confirming our theoretical results. Our method compares\nfavorably with all existing methods, in terms of accuracy and speed. We\nillustrate the practical merits of the convex banding estimator by showing that\nit can be used to improve the performance of discriminant analysis for\nclassifying sound recordings.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 20:00:52 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Bien", "Jacob", ""], ["Bunea", "Florentina", ""], ["Xiao", "Luo", ""]]}, {"id": "1405.6397", "submitter": "Gerhard Kurz", "authors": "Gerhard Kurz and Igor Gilitschenski and Uwe D. Hanebeck", "title": "Efficient Evaluation of the Probability Density Function of a Wrapped\n  Normal Distribution", "comments": null, "journal-ref": null, "doi": "10.1109/SDF.2014.6954713", "report-no": null, "categories": "stat.CO cs.SY math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wrapped normal distribution arises when a the density of a\none-dimensional normal distribution is wrapped around the circle infinitely\nmany times. At first look, evaluation of its probability density function\nappears tedious as an infinite series is involved. In this paper, we\ninvestigate the evaluation of two truncated series representations. As one\nrepresentation performs well for small uncertainties whereas the other performs\nwell for large uncertainties, we show that in all cases a small number of\nsummands is sufficient to achieve high accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 25 May 2014 15:37:52 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Kurz", "Gerhard", ""], ["Gilitschenski", "Igor", ""], ["Hanebeck", "Uwe D.", ""]]}, {"id": "1405.6460", "submitter": "Branko Ristic", "authors": "Branko Ristic, Ajith Gunatilaka, Ralph Gailis, Alex Skvortsov", "title": "Bayesian likelihood-free localisation of a biochemical source using\n  multiple dispersion models", "comments": "23 pages, 7 figures", "journal-ref": "Signal Processing 108 (2015): 13-24", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localisation of a source of a toxic release of biochemical aerosols in the\natmosphere is a problem of great importance for public safety. Two main\npractical difficulties are encountered in this problem: the lack of knowledge\nof the likelihood function of measurements collected by biochemical sensors,\nand the plethora of candidate dispersion models, developed under various\nassumptions (e.g. meteorological conditions, terrain). Aiming to overcome these\ntwo difficulties, the paper proposes a likelihood-free approximate Bayesian\ncomputation method, which simultaneously uses a set of candidate dispersion\nmodels, to localise the source. This estimation framework is implemented via\nthe Monte Carlo method and tested using two experimental datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 05:05:54 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Ristic", "Branko", ""], ["Gunatilaka", "Ajith", ""], ["Gailis", "Ralph", ""], ["Skvortsov", "Alex", ""]]}, {"id": "1405.6469", "submitter": "Karthyek Rajhaa Annaswamy Murthy", "authors": "Jose Blanchet and Karthyek R. A. Murthy", "title": "Exact Simulation of Multidimensional Reflected Brownian Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first exact simulation method for multidimensional reflected\nBrownian motion (RBM). Exact simulation in this setting is challenging because\nof the presence of correlated local-time-like terms in the definition of RBM.\nWe apply recently developed so-called $\\varepsilon-$strong simulation\ntechniques (also known as Tolerance-Enforced Simulation) which allow us to\nprovide a piece-wise linear approximation to RBM with $\\varepsilon $\n(deterministic) error in uniform norm. A novel conditional acceptance/rejection\nstep is then used to eliminate the error. In particular, we condition on a\nsuitably designed information structure so that a feasible proposal\ndistribution can be applied.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 06:17:36 GMT"}, {"version": "v2", "created": "Sat, 27 Sep 2014 06:04:41 GMT"}, {"version": "v3", "created": "Wed, 30 Aug 2017 16:12:33 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Blanchet", "Jose", ""], ["Murthy", "Karthyek R. A.", ""]]}, {"id": "1405.6947", "submitter": "\\'Oli Geirsson", "authors": "\\'Oli P\\'all Geirsson, Birgir Hrafnkelsson and Daniel Simpson", "title": "Computationally efficient spatial modeling of annual maximum 24 hour\n  precipitation. An application to data from Iceland", "comments": "32 pages, 16 figures, submitted to Environmetrics", "journal-ref": null, "doi": "10.1002/env.2343", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient statistical method to obtain\ndistributional properties of annual maximum 24 hour precipitation on a 1 km by\n1 km regular grid over Iceland. A latent Gaussian model is built which takes\ninto account observations, spatial variations and outputs from a local\nmeteorological model. A covariate based on the meteorological model is\nconstructed at each observational site and each grid point in order to\nassimilate available scientific knowledge about precipitation into the\nstatistical model. The model is applied to two data sets on extreme\nprecipitation, one uncorrected data set and one data set that is corrected for\nphase and wind. The observations are assumed to follow the generalized extreme\nvalue distribution. At the latent level, we implement SPDE spatial models for\nboth the location and scale parameters of the likelihood. An efficient MCMC\nsampler which exploits the model structure is constructed, which yields fast\ncontinuous spatial predictions for spatially varying model parameters and\nquantiles.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 15:18:01 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Geirsson", "\u00d3li P\u00e1ll", ""], ["Hrafnkelsson", "Birgir", ""], ["Simpson", "Daniel", ""]]}, {"id": "1405.7091", "submitter": "Jonathan Heydari", "authors": "Jonathan Heydari", "title": "Bayesian hierarchical modelling for inferring genetic interactions in\n  yeast", "comments": "All images rasterized. Contact author for non-rasterized and\n  searchable Figures. 158 pages, PhD thesis, Newcastle University (2014),\n  Institute for Cell & Molecular Biosciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.CO stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying genetic interactions for a given microorganism such as yeast is\ndifficult. Quantitative Fitness Analysis (QFA) is a high-throughput\nexperimental and computational methodology for quantifying the fitness of\nmicrobial cultures. QFA can be used to compare between fitness observations for\ndifferent genotypes and thereby infer genetic interaction strengths. Current\n\"naive\" frequentist statistical approaches used in QFA do not model\nbetween-genotype variation or difference in genotype variation under different\nconditions. In this thesis, a Bayesian approach is introduced to evaluate\nhierarchical models that better reflect the structure or design of QFA\nexperiments. First, a two-stage approach is presented: a hierarchical logistic\nmodel is fitted to microbial culture growth curves and then a hierarchical\ninteraction model is fitted to fitness summaries inferred for each genotype.\nNext, a one-stage Bayesian approach is presented: a joint hierarchical model\nwhich does not require a univariate summary of fitness, used to pass\ninformation between models. The new hierarchical approaches are then compared\nusing a dataset examining the effect of telomere defects on yeast. By better\ndescribing the experimental structure, new evidence is found for genes and\ncomplexes which interact with the telomere cap. Various extensions of these\nmodels, including models for data transformation, batch effects, and\nintrinsically stochastic growth models are also considered.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 23:49:48 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Heydari", "Jonathan", ""]]}, {"id": "1405.7569", "submitter": "Ngoc-Cuong Nguyen Dr.", "authors": "Ngoc-Cuong Nguyen and Jaime Peraire", "title": "Functional Gaussian processes for regression with linear PDE models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new statistical approach to the problem of\nincorporating experimental observations into a mathematical model described by\nlinear partial differential equations (PDEs) to improve the prediction of the\nstate of a physical system. We augment the linear PDE with a functional that\naccounts for the uncertainty in the mathematical model and is modeled as a {\\em\nGaussian process}. This gives rise to a stochastic PDE which is characterized\nby the Gaussian functional. We develop a {\\em functional Gaussian process\nregression} method to determine the posterior mean and covariance of the\nGaussian functional, thereby solving the stochastic PDE to obtain the posterior\ndistribution for our prediction of the physical state. Our method has the\nfollowing features which distinguish itself from other regression methods.\nFirst, it incorporates both the mathematical model and the observations into\nthe regression procedure. Second, it can handle the observations given in the\nform of linear functionals of the field variable. Third, the method is\nnon-parametric in the sense that it provides a systematic way to optimally\ndetermine the prior covariance operator of the Gaussian functional based on the\nobservations. Fourth, it provides the posterior distribution quantifying the\nmagnitude of uncertainty in our prediction of the physical state. We present\nnumerical results to illustrate these features of the method and compare its\nperformance to that of the standard Gaussian process regression.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 14:40:11 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Nguyen", "Ngoc-Cuong", ""], ["Peraire", "Jaime", ""]]}, {"id": "1405.7867", "submitter": "Dennis Prangle", "authors": "Dennis Prangle", "title": "Lazy ABC", "comments": "Pre-publication version. Revised to fix typos and update bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) performs statistical inference for\notherwise intractable probability models by accepting parameter proposals when\ncorresponding simulated datasets are sufficiently close to the observations.\nProducing the large quantity of simulations needed requires considerable\ncomputing time. However, it is often clear before a simulation ends that it is\nunpromising: it is likely to produce a poor match or require excessive time.\nThis paper proposes lazy ABC, an ABC importance sampling algorithm which saves\ntime by sometimes abandoning such simulations. This makes ABC more scalable to\napplications where simulation is expensive. By using a random stopping rule and\nappropriate reweighting step, the target distribution is unchanged from that of\nstandard ABC. Theory and practical methods to tune lazy ABC are presented and\nillustrated on a simple epidemic model example. They are also demonstrated on\nthe computationally demanding spatial extremes application of Erhardt and Smith\n(2012), producing efficiency gains, in terms of effective sample size per unit\nCPU time, of roughly 3 times for a 20 location dataset, and 8 times for 35\nlocations.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 14:04:51 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 16:09:25 GMT"}, {"version": "v3", "created": "Thu, 4 Dec 2014 09:39:42 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Prangle", "Dennis", ""]]}]