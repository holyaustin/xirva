[{"id": "2009.00003", "submitter": "Andrew Allmon", "authors": "Andrew G. Allmon, J.S. Marron, and Michael G. Hudgens", "title": "diproperm: An R Package for the DiProPerm Test", "comments": "Package located at https://github.com/allmondrew/diproperm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional low sample size (HDLSS) data sets emerge frequently in many\nbiomedical applications. A common task for analyzing HDLSS data is to assign\ndata to the correct class using a classifier. Classifiers which use two labels\nand a linear combination of features are known as binary linear classifiers.\nThe direction-projection-permutation (DiProPerm) test was developed for testing\nthe difference of two high-dimensional distributions induced by a binary linear\nclassifier. This paper discusses the key components of the DiProPerm test,\nintroduces the diproperm R package, and demonstrates the package on a\nreal-world data set.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 20:14:26 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Allmon", "Andrew G.", ""], ["Marron", "J. S.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "2009.00337", "submitter": "Florian Puchhammer", "authors": "Florian Puchhammer and Amal Ben Abdellah and Pierre L'Ecuyer", "title": "Variance Reduction with Array-RQMC for Tau-Leaping Simulation of\n  Stochastic Biological and Chemical Reaction Networks", "comments": "27 pages, 3 figures, 6 tables We want to thank the anonymous referees\n  who raised very relevant questions that helped us to improve the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of Array-RQMC, a randomized quasi-Monte Carlo method\ndesigned for the simulation of Markov chains, to reduce the variance when\nsimulating stochastic biological or chemical reaction networks with\n$\\tau$-leaping. The task is to estimate the expectation of a function of\nmolecule copy numbers at a given future time $T$ by the sample average over $n$\nsample paths, and the goal is to reduce the variance of this sample-average\nestimator. We find that when the method is properly applied, variance\nreductions by factors in the thousands can be obtained. These factors are much\nlarger than those observed previously by other authors who tried RQMC methods\nfor the same examples. Array-RQMC simulates an array of realizations of the\nMarkov chain and requires a sorting function to reorder these chains according\nto their states, after each step. The choice of sorting function is a key\ningredient for the efficiency of the method, although in our experiments,\nArray-RQMC was never worse than ordinary Monte Carlo, regardless of the sorting\nmethod. The expected number of reactions of each type per step also has an\nimpact on the efficiency gain.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 10:41:40 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 20:06:15 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 19:53:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Puchhammer", "Florian", ""], ["Abdellah", "Amal Ben", ""], ["L'Ecuyer", "Pierre", ""]]}, {"id": "2009.00354", "submitter": "Dongwei Ye", "authors": "Dongwei Ye (1), Anna Nikishova (1), Lourens Veen (2), Pavel Zun (1,3\n  and 4), Alfons G. Hoekstra (1) ((1) Computational Science Lab, Informatics\n  Institute, University of Amsterdam, (2) Netherlands eScience Center, (3) ITMO\n  University, (4) Erasmus University Medical Center)", "title": "Non-intrusive and semi-intrusive uncertainty quantification of a\n  multiscale in-stent restenosis model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimations are presented of the response of a multiscale\nin-stent restenosis model, as obtained by both non-intrusive and semi-intrusive\nuncertainty quantification. The in-stent restenosis model is a fully coupled\nmultiscale simulation of post-stenting tissue growth, in which the most costly\nsubmodel is the blood flow simulation. Surrogate modeling for non-intrusive\nuncertainty quantification takes the whole model as a black-box and maps\ndirectly from the three uncertain inputs to the quantity of interest, the\nneointimal area. The corresponding uncertain estimates matched the results from\nquasi-Monte Carlo simulations well. In the semi-intrusive uncertainty\nquantification, the most expensive submodel is replaced with a surrogate model.\nWe developed a surrogate model for the blood flow simulation by using a\nconvolutional neural network. The semi-intrusive method with the new surrogate\nmodel offered efficient estimates of uncertainty and sensitivity while keeping\nrelatively high accuracy. It outperformed the result obtained with earlier\nsurrogate models. It also achieved the estimates comparable to the\nnon-intrusive method with similar efficiency. Presented results on uncertainty\npropagation with non-intrusive and semi-intrusive metamodeling methods allow us\nto draw some conclusions on the advantages and limitations of these methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 11:19:41 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 07:39:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ye", "Dongwei", "", "1,3\n  and 4"], ["Nikishova", "Anna", "", "1,3\n  and 4"], ["Veen", "Lourens", "", "1,3\n  and 4"], ["Zun", "Pavel", "", "1,3\n  and 4"], ["Hoekstra", "Alfons G.", ""]]}, {"id": "2009.00471", "submitter": "Yuling Yao", "authors": "Yuling Yao, Collin Cademartori, Aki Vehtari, Andrew Gelman", "title": "Adaptive Path Sampling in Metastable Posterior Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalizing constant plays an important role in Bayesian computation, and\nthere is a large literature on methods for computing or approximating\nnormalizing constants that cannot be evaluated in closed form. When the\nnormalizing constant varies by orders of magnitude, methods based on importance\nsampling can require many rounds of tuning. We present an improved approach\nusing adaptive path sampling, iteratively reducing gaps between the base and\ntarget. Using this adaptive strategy, we develop two metastable sampling\nschemes. They are automated in Stan and require little tuning. For a multimodal\nposterior density, we equip simulated tempering with a continuous temperature.\nFor a funnel-shaped entropic barrier, we adaptively increase mass in bottleneck\nregions to form an implicit divide-and-conquer. Both approaches empirically\nperform better than existing methods for sampling from metastable\ndistributions, including higher accuracy and computation efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 14:28:31 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Yao", "Yuling", ""], ["Cademartori", "Collin", ""], ["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""]]}, {"id": "2009.00761", "submitter": "Drew Schmidt", "authors": "Drew Schmidt", "title": "A Survey of Singular Value Decomposition Methods for Distributed\n  Tall/Skinny Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Singular Value Decomposition (SVD) is one of the most important matrix\nfactorizations, enjoying a wide variety of applications across numerous\napplication domains. In statistics and data analysis, the common applications\nof SVD such as Principal Components Analysis (PCA) and linear regression.\nUsually these applications arise on data that has far more rows than columns,\nso-called \"tall/skinny\" matrices. In the big data analytics context, this may\ntake the form of hundreds of millions to billions of rows with only a few\nhundred columns. There is a need, therefore, for fast, accurate, and scalable\ntall/skinny SVD implementations which can fully utilize modern computing\nresources. To that end, we present a survey of three different algorithms for\ncomputing the SVD for these kinds of tall/skinny data layouts using MPI for\ncommunication. We contextualize these with common big data analytics\ntechniques, principally PCA. Finally, we present both CPU and GPU timing\nresults from the Summit supercomputer, and discuss possible alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 00:34:54 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Schmidt", "Drew", ""]]}, {"id": "2009.00874", "submitter": "Takashi Goda", "authors": "Takashi Goda", "title": "A simple algorithm for global sensitivity analysis with Shapley effects", "comments": null, "journal-ref": "Reliability Engineering and System Safety, Volume 213, Article\n  107702, 2021", "doi": "10.1016/j.ress.2021.107702", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Global sensitivity analysis aims at measuring the relative importance of\ndifferent variables or groups of variables for the variability of a quantity of\ninterest. Among several sensitivity indices, so-called Shapley effects have\nrecently gained popularity mainly because the Shapley effects for all the\nindividual variables are summed up to the overall variance, which gives a\nbetter interpretability than the classical sensitivity indices called main\neffects and total effects. In this paper, assuming that all the input variables\nare independent, we introduce a quite simple Monte Carlo algorithm to estimate\nthe Shapley effects for all the individual variables simultaneously, which\ndrastically simplifies the existing algorithms proposed in the literature. We\npresent a short Matlab implementation of our algorithm and show some numerical\nresults. A possible extension to the case where the input variables are\ndependent is also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:50:55 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 02:05:28 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 01:04:08 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 01:14:30 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Goda", "Takashi", ""]]}, {"id": "2009.01202", "submitter": "Christian Schmid", "authors": "Christian S. Schmid and David R. Hunter", "title": "Improving ERGM Starting Values Using Simulated Annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the theory of estimation for exponential family models, which include\nexponential-family random graph models (ERGMs) as a special case, is\nwell-established and maximum likelihood estimates in particular enjoy many\ndesirable properties. However, in the case of many ERGMs, direct calculation of\nMLEs is impossible and therefore methods for approximating MLEs and/or\nalternative estimation methods must be employed. Many MLE approximation methods\nrequire alternative estimates as starting points. We discuss one class of such\nalternatives here. The MLE satisfies the so-called \"likelihood principle,\"\nunlike the MPLE. This means that different networks may have different MPLEs\neven if they have the same sufficient statistics. We exploit this fact here to\nsearch for improved starting values for approximation-based MLE methods. The\nmethod we propose has shown its merit in producing an MLE for a network dataset\nand model that had defied estimation using all other known methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:20:48 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 15:56:33 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Schmid", "Christian S.", ""], ["Hunter", "David R.", ""]]}, {"id": "2009.01471", "submitter": "Jian Cao Dr.", "authors": "Jian Cao, Daniele Durante, and Marc G. Genton", "title": "Scalable computation of predictive probabilities in probit models with\n  Gaussian process priors", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models for binary data are fundamental in various fields, and the\ngrowing complexity of modern applications has motivated several flexible\nspecifications for modeling the relationship between the observed predictors\nand the binary responses. A widely-implemented solution is to express the\nprobability parameter via a probit mapping of a Gaussian process indexed by\npredictors. However, unlike for continuous settings, there is a lack of\nclosed-form results for predictive distributions in binary models with Gaussian\nprocess priors. Markov chain Monte Carlo methods and approximation strategies\nprovide common solutions to this problem, but state-of-the-art algorithms are\neither computationally intractable or inaccurate in moderate-to-high\ndimensions. In this article, we aim to cover this gap by deriving closed-form\nexpressions for the predictive probabilities in probit Gaussian processes that\nrely either on cumulative distribution functions of multivariate Gaussians or\non functionals of multivariate truncated normals. To evaluate these quantities\nwe develop novel scalable solutions based on tile-low-rank Monte Carlo methods\nfor computing multivariate Gaussian probabilities, and on mean-field\nvariational approximations of multivariate truncated normals. Closed-form\nexpressions for the marginal likelihood and for the posterior distribution of\nthe Gaussian process are also discussed. As shown in simulated and real-world\nempirical studies, the proposed methods scale to dimensions where\nstate-of-the-art solutions are impractical.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 06:24:53 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 14:24:30 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 00:48:46 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cao", "Jian", ""], ["Durante", "Daniele", ""], ["Genton", "Marc G.", ""]]}, {"id": "2009.01799", "submitter": "Dootika Vats", "authors": "Medha Agarwal and Dootika Vats", "title": "Globally-centered autocovariances in MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autocovariances are a fundamental quantity of interest in Markov chain Monte\nCarlo (MCMC) simulations with autocorrelation function (ACF) plots being an\nintegral visualization tool for performance assessment. Unfortunately, for slow\nmixing Markov chains, the empirical autocovariance can highly underestimate the\ntruth. For multiple-chain MCMC sampling, we propose a globally-centered\nestimator of the autocovariance function (G-ACvF) that exhibits significant\ntheoretical and empirical improvements. We show that the bias of the G-ACvF\nestimator is smaller than the bias of the current state-of-the-art. The impact\nof this improved estimator is evident in three critical output analysis\napplications: (1) ACF plots, (2) estimates of the Monte Carlo asymptotic\ncovariance matrix, and (3) estimates of the effective sample size. Under weak\nconditions, we establish strong consistency of our improved asymptotic\ncovariance estimator, and obtain its large-sample bias and variance. The\nperformance of the new estimators is demonstrated through various examples.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 16:59:47 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Agarwal", "Medha", ""], ["Vats", "Dootika", ""]]}, {"id": "2009.02414", "submitter": "Sela Fried", "authors": "S. Fried", "title": "On the restrictiveness of the hazard rate order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every element $\\theta=(\\theta_1,\\ldots,\\theta_n)$ of the probability\n$n$-simplex induces a probability distribution $P_\\theta$ of a random variable\n$X$ that can assume only a finite number of real values $x_1 < \\cdots < x_n$ by\ndefining $P_\\theta(X=x_i) = \\theta_i, 1\\leq i \\leq n$. We show that if $\\Theta$\nand $\\Theta'$ are two random vectors uniformly distributed on $\\Delta^n$, then\n$P(P_\\Theta\\leq_{\\rm hr} P_{\\Theta'})=\\frac{1}{2^{n-1}}$ where $\\leq_{\\rm hr}$\ndenotes the hazard rate order.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 22:35:42 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Fried", "S.", ""]]}, {"id": "2009.02517", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau and Jiajie Zeng and Ajay Jasra", "title": "Uncertainty modelling and computational aspects of data association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel solution to the smoothing problem for multi-object dynamical systems\nis proposed and evaluated. The systems of interest contain an unknown and\nvarying number of dynamical objects that are partially observed under noisy and\ncorrupted observations. An alternative representation of uncertainty is\nconsidered in order to account for the lack of information about the different\naspects of this type of complex system. The corresponding statistical model can\nbe formulated as a hierarchical model consisting of conditionally-independent\nhidden Markov models. This particular structure is leveraged to propose an\nefficient method in the context of Markov chain Monte Carlo (MCMC) by relying\non an approximate solution to the corresponding filtering problem, in a similar\nfashion to particle MCMC. This approach is shown to outperform existing\nalgorithms in a range of scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 11:27:51 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Zeng", "Jiajie", ""], ["Jasra", "Ajay", ""]]}, {"id": "2009.02709", "submitter": "Eugene Ndiaye", "authors": "Eugene Ndiaye and Olivier Fercoq and Joseph Salmon", "title": "Screening Rules and its Complexity for Active Set Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening rules were recently introduced as a technique for explicitly\nidentifying active structures such as sparsity, in optimization problem arising\nin machine learning. This has led to new methods of acceleration based on a\nsubstantial dimension reduction. We show that screening rules stem from a\ncombination of natural properties of subdifferential sets and optimality\nconditions, and can hence be understood in a unified way. Under mild\nassumptions, we analyze the number of iterations needed to identify the optimal\nactive set for any converging algorithm. We show that it only depends on its\nconvergence rate.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 11:10:34 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Salmon", "Joseph", ""]]}, {"id": "2009.02993", "submitter": "Raphael Sonabend", "authors": "Raphael Sonabend and Franz Kiraly", "title": "distr6: R6 Object-Oriented Probability Distributions Interface in R", "comments": "Accepted in The R Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.MS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  distr6 is an object-oriented (OO) probability distributions interface\nleveraging the extensibility and scalability of R6, and the speed and\nefficiency of Rcpp. Over 50 probability distributions are currently implemented\nin the package with `core' methods including density, distribution, and\ngenerating functions, and more `exotic' ones including hazards and distribution\nfunction anti-derivatives. In addition to simple distributions, distr6 supports\ncompositions such as truncation, mixtures, and product distributions. This\npaper presents the core functionality of the package and demonstrates examples\nfor key use-cases. In addition this paper provides a critical review of the\nobject-oriented programming paradigms in R and describes some novel\nimplementations for design patterns and core object-oriented features\nintroduced by the package for supporting distr6 components.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:20:00 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 12:19:34 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 11:04:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sonabend", "Raphael", ""], ["Kiraly", "Franz", ""]]}, {"id": "2009.03699", "submitter": "Joshua Bon", "authors": "Joshua J Bon, Anthony Lee, Christopher Drovandi", "title": "Accelerating sequential Monte Carlo with surrogate likelihoods", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delayed-acceptance is a technique for reducing computational effort for\nBayesian models with expensive likelihoods. Using a delayed-acceptance kernel\nfor Markov chain Monte Carlo can reduce the number of expensive likelihoods\nevaluations required to approximate a posterior expectation. Delayed-acceptance\nuses a surrogate, or approximate, likelihood to avoid evaluation of the\nexpensive likelihood when possible. Within the sequential Monte Carlo\nframework, we utilise the history of the sampler to adaptively tune the\nsurrogate likelihood to yield better approximations of the expensive\nlikelihood, and use a surrogate first annealing schedule to further increase\ncomputational efficiency. Moreover, we propose a framework for optimising\ncomputation time whilst avoiding particle degeneracy, which encapsulates\nexisting strategies in the literature. Overall, we develop a novel algorithm\nfor computationally efficient SMC with expensive likelihood functions. The\nmethod is applied to static Bayesian models, which we demonstrate on toy and\nreal examples, code for which is available at\nhttps://github.com/bonStats/smcdar.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 12:41:43 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 23:49:21 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Bon", "Joshua J", ""], ["Lee", "Anthony", ""], ["Drovandi", "Christopher", ""]]}, {"id": "2009.03798", "submitter": "Georg Heiler", "authors": "Georg Heiler and Allan Hanbury and Peter Filzmoser", "title": "The impact of COVID-19 on relative changes in aggregated mobility using\n  mobile-phone data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating relative changes leads to additional insights which would remain\nhidden when only evaluating absolute changes. We analyze a dataset describing\nmobility of mobile phones in Austria before, during COVID-19 lock-down measures\nuntil recent. By applying compositional data analysis we show that formerly\nhidden information becomes available: we see that the elderly population groups\nincrease relative mobility and that the younger groups especially on weekends\nalso do not decrease their mobility as much as the others.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 14:53:12 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Heiler", "Georg", ""], ["Hanbury", "Allan", ""], ["Filzmoser", "Peter", ""]]}, {"id": "2009.03840", "submitter": "Viviane Philipps", "authors": "Viviane Philipps (1 and 2), Boris P Hejblum (1, 2 and 3), M\\'elanie\n  Prague (1, 2 and 3), Daniel Commenges (1, 2 and 3), C\\'ecile Proust-Lima (1\n  and 2) ((1) Inserm Bordeaux Population Health Research Center, (2) University\n  of Bordeaux, (3) Inria BSO)", "title": "Robust and Efficient Optimization Using a Marquardt-Levenberg Algorithm\n  with R Package marqLevAlg", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementations in R of classical general-purpose algorithms generally have\ntwo major limitations which make them unusable in complex problems: too loose\nconvergence criteria and too long calculation time. By relying on a\nMarquardt-Levenberg algorithm (MLA), a Newton-like method particularly robust\nfor solving local optimization problems, we provide with marqLevAlg package an\nefficient and general-purpose local optimizer which (i) prevents convergence to\nsaddle points by using a stringent convergence criterion based on the relative\ndistance to minimum/maximum in addition to the stability of the parameters and\nof the objective function; and (ii) reduces the computation time in complex\nsettings by allowing parallel calculations at each iteration. We demonstrate\nthrough a variety of cases from the literature that our implementation reliably\nand consistently reaches the optimum (even when other optimizers fail), and\nalso largely reduces computational time in complex settings through the example\nof maximum likelihood estimation of different sophisticated statistical models.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 16:19:40 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 15:36:46 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Philipps", "Viviane", "", "1 and 2"], ["Hejblum", "Boris P", "", "1, 2 and 3"], ["Prague", "M\u00e9lanie", "", "1, 2 and 3"], ["Commenges", "Daniel", "", "1, 2 and 3"], ["Proust-Lima", "C\u00e9cile", "", "1\n  and 2"]]}, {"id": "2009.04137", "submitter": "Rowland Seymour", "authors": "R. G. Seymour, T. Kypraios, P. D. O'Neill, T. J. Hagenaars", "title": "A Bayesian Nonparametric Analysis of the 2003 Outbreak of Highly\n  Pathogenic Avian Influenza in the Netherlands", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases on farms pose both public and animal health risks, so\nunderstanding how they spread between farms is crucial for developing disease\ncontrol strategies to prevent future outbreaks. We develop novel Bayesian\nnonparametric methodology to fit spatial stochastic transmission models in\nwhich the infection rate between any two farms is a function that depends on\nthe distance between them, but without assuming a specified parametric form.\nMaking nonparametric inference in this context is challenging since the\nlikelihood function of the observed data is intractable because the underlying\ntransmission process is unobserved. We adopt a fully Bayesian approach by\nassigning a transformed Gaussian Process prior distribution to the infection\nrate function, and then develop an efficient data augmentation Markov Chain\nMonte Carlo algorithm to perform Bayesian inference. We use the posterior\npredictive distribution to simulate the effect of different disease control\nmethods and their economic impact. We analyse a large outbreak of Avian\nInfluenza in the Netherlands and infer the between-farm infection rate, as well\nas the unknown infection status of farms which were pre-emptively culled. We\nuse our results to analyse ring-culling strategies, and conclude that although\neffective, ring-culling has limited impact in high density areas.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 07:19:49 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Seymour", "R. G.", ""], ["Kypraios", "T.", ""], ["O'Neill", "P. D.", ""], ["Hagenaars", "T. J.", ""]]}, {"id": "2009.04239", "submitter": "Jon Cockayne", "authors": "Jon Cockayne and Andrew B. Duncan", "title": "Probabilistic Gradients for Fast Calibration of Differential Equation\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of large-scale differential equation models to observational or\nexperimental data is a widespread challenge throughout applied sciences and\nengineering. A crucial bottleneck in state-of-the art calibration methods is\nthe calculation of local sensitivities, i.e. derivatives of the loss function\nwith respect to the estimated parameters, which often necessitates several\nnumerical solves of the underlying system of partial or ordinary differential\nequations. In this paper we present a new probabilistic approach to computing\nlocal sensitivities. The proposed method has several advantages over classical\nmethods. Firstly, it operates within a constrained computational budget and\nprovides a probabilistic quantification of uncertainty incurred in the\nsensitivities from this constraint. Secondly, information from previous\nsensitivity estimates can be recycled in subsequent computations, reducing the\noverall computational effort for iterative gradient-based calibration methods.\nThe methodology presented is applied to two challenging test problems and\ncompared against classical methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 10:35:09 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 08:08:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Cockayne", "Jon", ""], ["Duncan", "Andrew B.", ""]]}, {"id": "2009.04551", "submitter": "Yousef El-Laham", "authors": "Yousef El-Laham, Liu Yang, Petar M. Djuric, Monica F. Bugallo", "title": "Particle Filtering Under General Regime Switching", "comments": "Accepted to EUSIPCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a new framework for particle filtering under model\nuncertainty that operates beyond the scope of Markovian switching systems.\nSpecifically, we develop a novel particle filtering algorithm that applies to\ngeneral regime switching systems, where the model index is augmented as an\nunknown time-varying parameter in the system. The proposed approach does not\nrequire the use of multiple filters and can maintain a diverse set of particles\nfor each considered model through appropriate choice of the particle filtering\nproposal distribution. The flexibility of the proposed approach allows for\nlong-term dependencies between the models, which enables its use to a wider\nvariety of real-world applications. We validate the method on a synthetic data\nexperiment and show that it outperforms state-of-the-art multiple model\nparticle filtering approaches that require the use of multiple filters.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 20:20:28 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["El-Laham", "Yousef", ""], ["Yang", "Liu", ""], ["Djuric", "Petar M.", ""], ["Bugallo", "Monica F.", ""]]}, {"id": "2009.04800", "submitter": "Nora L\\\"uthen", "authors": "Nora L\\\"uthen, Stefano Marelli, Bruno Sudret", "title": "Automatic selection of basis-adaptive sparse polynomial chaos expansions\n  for engineering applications", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2020-011C", "categories": "stat.CO cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse polynomial chaos expansions (PCE) are an efficient and widely used\nsurrogate modeling method in uncertainty quantification for engineering\nproblems with computationally expensive models. To make use of the available\ninformation in the most efficient way, several approaches for so-called\nbasis-adaptive sparse PCE have been proposed to determine the set of polynomial\nregressors (\"basis\") for PCE adaptively.\n  The goal of this paper is to help practitioners identify the most suitable\nmethods for constructing a surrogate PCE for their model. We describe three\nstate-of-the-art basis-adaptive approaches from the recent sparse PCE\nliterature and conduct an extensive benchmark in terms of global approximation\naccuracy on a large set of computational models. Investigating the synergies\nbetween sparse regression solvers and basis adaptivity schemes, we find that\nthe choice of the proper solver and basis-adaptive scheme is very important, as\nit can result in more than one order of magnitude difference in performance. No\nsingle method significantly outperforms the others, but dividing the analysis\ninto classes (regarding input dimension and experimental design size), we are\nable to identify specific sparse solver and basis adaptivity combinations for\neach class that show comparatively good performance.\n  To further improve on these findings, we introduce a novel solver and basis\nadaptivity selection scheme guided by cross-validation error. We demonstrate\nthat this automatic selection procedure provides close-to-optimal results in\nterms of accuracy, and significantly more robust solutions, while being more\ngeneral than the case-by-case recommendations obtained by the benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 12:13:57 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 11:25:17 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 16:44:02 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["L\u00fcthen", "Nora", ""], ["Marelli", "Stefano", ""], ["Sudret", "Bruno", ""]]}, {"id": "2009.04954", "submitter": "Amit Moscovich", "authors": "Amit Moscovich", "title": "Fast calculation of p-values for one-sided Kolmogorov-Smirnov type\n  statistics", "comments": "22 pages, 3 figures. Supplementary code is included under the\n  crossprob and benchmarks directories", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for computing exact p-values for a large family of\none-sided continuous goodness-of-fit statistics. This includes the higher\ncriticism statistic, one-sided weighted Kolmogorov-Smirnov statistics, and the\none-sided Berk-Jones statistics. For a sample size of 10,000, our method takes\nmerely 0.15 seconds to run and it scales to sample sizes in the hundreds of\nthousands. This allows practitioners working on genome-wide association studies\nand other high-dimensional analyses to use exact finite-sample computations\ninstead of statistic-specific approximation schemes.\n  Our work has other applications in statistics, including power analysis,\nfinding alpha-level thresholds for goodness-of-fit tests, and the construction\nof confidence bands for the empirical distribution function. The algorithm is\nbased on a reduction to the boundary-crossing probability of a pure jump\nprocess and is also applicable to fields outside of statistics, for example in\nfinancial risk modeling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:51:41 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Moscovich", "Amit", ""]]}, {"id": "2009.05098", "submitter": "Sanjeena Subedi", "authors": "Wangshu Tu and Sanjeena Subedi", "title": "A Family of Mixture Models for Biclustering", "comments": "45", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering is used for simultaneous clustering of the observations and\nvariables when there is no group structure known \\textit{a priori}. It is being\nincreasingly used in bioinformatics, text analytics, etc. Previously,\nbiclustering has been introduced in a model-based clustering framework by\nutilizing a structure similar to a mixture of factor analyzers. In such models,\nobserved variables $\\mathbf{X}$ are modelled using a latent variable\n$\\mathbf{U}$ that is assumed to be from $N(\\mathbf{0}, \\mathbf{I})$. Clustering\nof variables is introduced by imposing constraints on the entries of the factor\nloading matrix to be 0 and 1 that results in a block diagonal covariance\nmatrices. However, this approach is overly restrictive as off-diagonal elements\nin the blocks of the covariance matrices can only be 1 which can lead to\nunsatisfactory model fit on complex data. Here, the latent variable\n$\\mathbf{U}$ is assumed to be from a $N(\\mathbf{0}, \\mathbf{T})$ where\n$\\mathbf{T}$ is a diagonal matrix. This ensures that the off-diagonal terms in\nthe block matrices within the covariance matrices are non-zero and not\nrestricted to be 1. This leads to a superior model fit on complex data. A\nfamily of models are developed by imposing constraints on the components of the\ncovariance matrix. For parameter estimation, an alternating expectation\nconditional maximization (AECM) algorithm is used. Finally, the proposed method\nis illustrated using simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 19:06:35 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Tu", "Wangshu", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "2009.05298", "submitter": "Sven Wang", "authors": "Richard Nickl and Sven Wang", "title": "On polynomial-time computation of high-dimensional posterior measures by\n  Langevin-type algorithms", "comments": "68 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.AP math.NA math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of generating random samples of high-dimensional posterior\ndistributions is considered. The main results consist of non-asymptotic\ncomputational guarantees for Langevin-type MCMC algorithms which scale\npolynomially in key quantities such as the dimension of the model, the desired\nprecision level, and the number of available statistical measurements. As a\ndirect consequence, it is shown that posterior mean vectors as well as\noptimisation based maximum a posteriori (MAP) estimates are computable in\npolynomial time, with high probability under the distribution of the data.\nThese results are complemented by statistical guarantees for recovery of the\nground truth parameter generating the data.\n  Our results are derived in a general high-dimensional non-linear regression\nsetting (with Gaussian process priors) where posterior measures are not\nnecessarily log-concave, employing a set of local `geometric' assumptions on\nthe parameter space, and assuming that a good initialiser of the algorithm is\navailable. The theory is applied to a representative non-linear example from\nPDEs involving a steady-state Schr\\\"odinger equation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 09:00:54 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Nickl", "Richard", ""], ["Wang", "Sven", ""]]}, {"id": "2009.05318", "submitter": "Andrew Golightly", "authors": "Andrew Golightly and Chris Sherlock", "title": "Augmented pseudo-marginal Metropolis-Hastings for partially observed\n  diffusion processes", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inference for nonlinear, multivariate diffusion\nprocesses, satisfying It\\^o stochastic differential equations (SDEs), using\ndata at discrete times that may be incomplete and subject to measurement error.\nOur starting point is a state-of-the-art correlated pseudo-marginal\nMetropolis-Hastings scheme, that uses correlated particle filters to induce\nstrong and positive correlation between successive marginal likelihood\nestimates. However, unless the measurement error or the dimension of the SDE is\nsmall, correlation can be eroded by the resampling steps in the particle\nfilter. We therefore propose a novel augmentation scheme, that allows for\nconditioning on values of the latent process at the observation times,\ncompletely avoiding the need for resampling steps. We integrate over the\nuncertainty at the observation times with an additional Gibbs step. Connections\nbetween the resulting pseudo-marginal scheme and existing inference schemes for\ndiffusion processes are made. The methodology is applied in three examples of\nincreasing complexity. We find that our approach offers substantial increases\nin overall efficiency, compared to competing methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 10:02:51 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Golightly", "Andrew", ""], ["Sherlock", "Chris", ""]]}, {"id": "2009.05446", "submitter": "Jonathan Baxter", "authors": "Jonathan Baxter", "title": "Bayesian Beta-Binomial Prevalence Estimation Using an Imperfect Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following [Diggle 2011, Greenland 1995], we give a simple formula for the\nBayesian posterior density of a prevalence parameter based on unreliable\ntesting of a population. This problem is of particular importance when the\nfalse positive test rate is close to the prevalence in the population being\ntested. An efficient Monte Carlo algorithm for approximating the posterior\ndensity is presented, and applied to estimating the Covid-19 infection rate in\nSanta Clara county, CA using the data reported in [Bendavid 2020]. We show that\nthe true Bayesian posterior places considerably more mass near zero, resulting\nin a prevalence estimate of 5,000--70,000 infections (median: 42,000) (2.17%\n(95CI 0.27%--3.63%)), compared to the estimate of 48,000--81,000 infections\nderived in [Bendavid 2020] using the delta method.\n  A demonstration, with code and additional examples, is available at\ntestprev.com.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:49:33 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Baxter", "Jonathan", ""]]}, {"id": "2009.05482", "submitter": "Vartan Choulakian", "authors": "J. Allard, S. Champigny, V. Choulakian, S. Mahdi", "title": "TCA and TLRA: A comparison on contingency tables and compositional data", "comments": "22 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two popular general approaches for the analysis and visualization\nof a contingency table and a compositional data set: Correspondence analysis\n(CA) and log ratio analysis (LRA). LRA includes two independently well\ndeveloped methods: association models and compositional data analysis. The\napplication of either CA or LRA to a contingency table or to compositional data\nset includes a preprocessing centering step. In CA the centering step is\nmultiplicative, while in LRA it is log bi-additive. A preprocessed matrix is\ndouble-centered, so it is a residuel matrix; which implies that it affects the\nfinal results of the analysis. This paper introduces a novel index named the\nintrinsic measure of the quality of the signs of the residuals (QSR) for the\nchoice of the preprocessing, and consequently of the method. The criterion is\nbased on taxicab singular value decomposition (TSVD) on which the package\nTaxicabCA in R is developed. We present a minimal R script that can be executed\nto obtain the numerical results and the maps in this paper. Three relatively\nsmall sized data sets available freely on the web are used as examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 15:00:51 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Allard", "J.", ""], ["Champigny", "S.", ""], ["Choulakian", "V.", ""], ["Mahdi", "S.", ""]]}, {"id": "2009.06078", "submitter": "Andreas Groll", "authors": "Tobias Markus Krabel, Thi Ngoc Tien Tran, Andreas Groll, Daniel Horn,\n  Carsten Jentsch", "title": "Random boosting and random^2 forests -- A random tree depth injection\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The induction of additional randomness in parallel and sequential ensemble\nmethods has proven to be worthwhile in many aspects. In this manuscript, we\npropose and examine a novel random tree depth injection approach suitable for\nsequential and parallel tree-based approaches including Boosting and Random\nForests. The resulting methods are called \\emph{Random Boost} and\n\\emph{Random$^2$ Forest}. Both approaches serve as valuable extensions to the\nexisting literature on the gradient boosting framework and random forests. A\nMonte Carlo simulation, in which tree-shaped data sets with different numbers\nof final partitions are built, suggests that there are several scenarios where\n\\emph{Random Boost} and \\emph{Random$^2$ Forest} can improve the prediction\nperformance of conventional hierarchical boosting and random forest approaches.\nThe new algorithms appear to be especially successful in cases where there are\nmerely a few high-order interactions in the generated data. In addition, our\nsimulations suggest that our random tree depth injection approach can improve\ncomputation time by up to 40%, while at the same time the performance losses in\nterms of prediction accuracy turn out to be minor or even negligible in most\ncases.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 20:14:50 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Krabel", "Tobias Markus", ""], ["Tran", "Thi Ngoc Tien", ""], ["Groll", "Andreas", ""], ["Horn", "Daniel", ""], ["Jentsch", "Carsten", ""]]}, {"id": "2009.06670", "submitter": "Lawrence Bardwell", "authors": "Alexander T. M. Fisch, Lawrence Bardwell and Idris A. Eckley", "title": "Real Time Anomaly Detection And Categorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to quickly and accurately detect anomalous structure within data\nsequences is an inference challenge of growing importance. This work extends\nrecently proposed post-hoc (offline) anomaly detection methodology to the\nsequential setting. The resultant procedure is capable of real-time analysis\nand categorisation between baseline and two forms of anomalous structure: point\nand collective anomalies. Various theoretical properties of the procedure are\nderived. These, together with an extensive simulation study, highlight that the\naverage run length to false alarm and the average detection delay of the\nproposed online algorithm are very close to that of the offline version.\nExperiments on simulated and real data are provided to demonstrate the benefits\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 18:11:19 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Bardwell", "Lawrence", ""], ["Eckley", "Idris A.", ""]]}, {"id": "2009.07063", "submitter": "Jouni Helske", "authors": "Jouni Helske", "title": "Efficient Bayesian generalized linear models with time-varying\n  coefficients: The walker package in R", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The R package walker extends standard Bayesian general linear models to the\ncase where the effects of the explanatory variables can vary in time. This\nallows, for example, to model the effects of interventions such as changes in\ntax policy which gradually increases their effect over time. The Markov chain\nMonte Carlo algorithms powering the Bayesian inference are based on Hamiltonian\nMonte Carlo provided by Stan software, using a state space representation of\nthe model to marginalise over the regression coefficients for efficient\nlow-dimensional sampling.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:14:37 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Helske", "Jouni", ""]]}, {"id": "2009.07385", "submitter": "Siavash Ameli", "authors": "Siavash Ameli, Shawn C. Shadden", "title": "Interpolating the Trace of the Inverse of Matrix $\\mathbf{A} + t\n  \\mathbf{B}$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop heuristic interpolation methods for the function $t \\mapsto\n\\operatorname{trace}\\left( (\\mathbf{A} + t \\mathbf{B})^{-1} \\right)$, where the\nmatrices $\\mathbf{A}$ and $\\mathbf{B}$ are symmetric and positive definite and\n$t$ is a real variable. This function is featured in many applications in\nstatistics, machine learning, and computational physics. The presented\ninterpolation functions are based on the modification of a sharp upper bound\nthat we derive for this function, which is a new trace inequality for matrices.\nWe demonstrate the accuracy and performance of the proposed method with\nnumerical examples, namely, the marginal maximum likelihood estimation for\nlinear Gaussian process regression and the estimation of the regularization\nparameter of ridge regression with the generalized cross-validation method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 23:11:17 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ameli", "Siavash", ""], ["Shadden", "Shawn C.", ""]]}, {"id": "2009.07594", "submitter": "Andrew Golightly", "authors": "Holly F. Fisher and Richard J. Boys and Colin S. Gillespie and Carole\n  J. Proctor and Andrew Golightly", "title": "Parameter inference for a stochastic kinetic model of expanded\n  polyglutamine proteins", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of protein aggregates in cells is a known feature of many human\nage-related diseases, such as Huntington's disease. Simulations using fixed\nparameter values in a model of the dynamic evolution of expanded polyglutamine\n(PolyQ) proteins in cells have been used to gain a better understanding of the\nbiological system, how to focus drug development and how to construct more\nefficient designs of future laboratory-based in vitro experiments. However,\nthere is considerable uncertainty about the values of some of the parameters\ngoverning the system. Currently, appropriate values are chosen by ad hoc\nattempts to tune the parameters so that the model output matches experimental\ndata. The problem is further complicated by the fact that the data only offer a\npartial insight into the underlying biological process: the data consist only\nof the proportions of cell death and of cells with inclusion bodies at a few\ntime points, corrupted by measurement error.\n  Developing inference procedures to estimate the model parameters in this\nscenario is a significant task. The model probabilities corresponding to the\nobserved proportions cannot be evaluated exactly and so they are estimated\nwithin the inference algorithm by repeatedly simulating realisations from the\nmodel. In general such an approach is computationally very expensive and we\ntherefore construct Gaussian process emulators for the key quantities and\nreformulate our algorithm around these fast stochastic approximations. We\nconclude by examining the fit of our model and highlight appropriate values of\nthe model parameters leading to new insights into the underlying biological\nprocesses such as the kinetics of aggregation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 10:41:19 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Fisher", "Holly F.", ""], ["Boys", "Richard J.", ""], ["Gillespie", "Colin S.", ""], ["Proctor", "Carole J.", ""], ["Golightly", "Andrew", ""]]}, {"id": "2009.08011", "submitter": "Xiang Lyu", "authors": "Xiang Lyu, Jian Kang, Lexin Li", "title": "Statistical Inference for High-Dimensional Vector Autoregression with\n  Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional vector autoregression with measurement error is frequently\nencountered in a large variety of scientific and business applications. In this\narticle, we study statistical inference of the transition matrix under this\nmodel. While there has been a large body of literature studying sparse\nestimation of the transition matrix, there is a paucity of inference solutions,\nespecially in the high-dimensional scenario. We develop inferential procedures\nfor both the global and simultaneous testing of the transition matrix. We first\ndevelop a new sparse expectation-maximization algorithm to estimate the model\nparameters, and carefully characterize their estimation precisions. We then\nconstruct a Gaussian matrix, after proper bias and variance corrections, from\nwhich we derive the test statistics. Finally, we develop the testing procedures\nand establish their asymptotic guarantees. We study the finite-sample\nperformance of our tests through intensive simulations, and illustrate with a\nbrain connectivity analysis example.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:57:45 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lyu", "Xiang", ""], ["Kang", "Jian", ""], ["Li", "Lexin", ""]]}, {"id": "2009.08077", "submitter": "Tuhin Sahai", "authors": "Tuhin Sahai", "title": "Stochastic Optimization using Polynomial Chaos Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos based methods enable the efficient computation of output\nvariability in the presence of input uncertainty in complex models.\nConsequently, they have been used extensively for propagating uncertainty\nthrough a wide variety of physical systems. These methods have also been\nemployed to build surrogate models for accelerating inverse uncertainty\nquantification (infer model parameters from data) and construct transport maps.\nIn this work, we explore the use of polynomial chaos based approaches for\noptimizing functions in the presence of uncertainty. These methods enable the\nfast propagation of uncertainty through smooth systems. If the dimensionality\nof the random parameters is low, these methods provide orders of magnitude\nacceleration over Monte Carlo sampling. We construct a generalized polynomial\nchaos based methodology for optimizing smooth functions in the presence of\nrandom parameters that are drawn from \\emph{known} distributions. By expanding\nthe optimization variables using orthogonal polynomials, the stochastic\noptimization problem reduces to a deterministic one that provides estimates for\nall moments of the output distribution. Thus, this approach enables one to\navoid computationally expensive random sampling based approaches such as Monte\nCarlo and Quasi-Monte Carlo. In this work, we develop the overall framework,\nderive error bounds, construct the framework for the inclusion of constraints,\nanalyze various properties of the approach, and demonstrate the proposed\ntechnique on illustrative examples.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 05:27:45 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Sahai", "Tuhin", ""]]}, {"id": "2009.08693", "submitter": "Louis Sharrock", "authors": "Louis Sharrock, Nikolas Kantas", "title": "Joint Online Parameter Estimation and Optimal Sensor Placement for the\n  Partially Observed Stochastic Advection-Diffusion Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of jointly performing online parameter\nestimation and optimal sensor placement for a partially observed infinite\ndimensional linear diffusion process. We present a novel solution to this\nproblem in the form of a continuous-time, two-timescale stochastic gradient\ndescent algorithm, which recursively seeks to maximise the log-likelihood with\nrespect to the unknown model parameters, and to minimise the expected mean\nsquared error of the hidden state estimate with respect to the sensor\nlocations. We also provide extensive numerical results illustrating the\nperformance of the proposed approach in the case that the hidden signal is\ngoverned by the two-dimensional stochastic advection-diffusion equation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:00:16 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 13:17:04 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Sharrock", "Louis", ""], ["Kantas", "Nikolas", ""]]}, {"id": "2009.08782", "submitter": "Colin Fox", "authors": "Colin Fox and Tiangang Cui and Markus Neumayer", "title": "Randomized Reduced Forward Models for Efficient Metropolis--Hastings\n  MCMC, with Application to Subsurface Fluid Flow and Capacitance Tomography", "comments": "This is a pre-print of an article submitted to GEM - International\n  Journal on Geomathematics. arXiv admin note: text overlap with\n  arXiv:1809.03176", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian modelling and computational inference by Markov chain Monte Carlo\n(MCMC) is a principled framework for large-scale uncertainty quantification,\nthough is limited in practice by computational cost when implemented in the\nsimplest form that requires simulating an accurate computer model at each\niteration of the MCMC. The delayed acceptance Metropolis--Hastings MCMC\nleverages a reduced model for the forward map to lower the compute cost per\niteration, though necessarily reduces statistical efficiency that can, without\ncare, lead to no reduction in the computational cost of computing estimates to\na desired accuracy. Randomizing the reduced model for the forward map can\ndramatically improve computational efficiency, by maintaining the low cost per\niteration but also avoiding appreciable loss of statistical efficiency.\nRandomized maps are constructed by a posteriori adaptive tuning of a randomized\nand locally-corrected deterministic reduced model. Equivalently, the\napproximated posterior distribution may be viewed as induced by a modified\nlikelihood function for use with the reduced map, with parameters tuned to\noptimize the quality of the approximation to the correct posterior\ndistribution. Conditions for adaptive MCMC algorithms allow practical\napproximations and algorithms that have guaranteed ergodicity for the target\ndistribution. Good statistical and computational efficiencies are demonstrated\nin examples of calibration of large-scale numerical models of geothermal\nreservoirs and electrical capacitance tomography.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 05:08:41 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Fox", "Colin", ""], ["Cui", "Tiangang", ""], ["Neumayer", "Markus", ""]]}, {"id": "2009.09111", "submitter": "Barinder Thind", "authors": "Barinder Thind, Sidi Wu, Richard Groenewald, Jiguo Cao", "title": "FuncNN: An R Package to Fit Deep Neural Networks Using Generalized Input\n  Spaces", "comments": "23 pages, 5 figures, submitted to JSS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have excelled at regression and classification problems when\nthe input space consists of scalar variables. As a result of this proficiency,\nseveral popular packages have been developed that allow users to easily fit\nthese kinds of models. However, the methodology has excluded the use of\nfunctional covariates and to date, there exists no software that allows users\nto build deep learning models with this generalized input space. To the best of\nour knowledge, the functional neural network (FuncNN) library is the first such\npackage in any programming language; the library has been developed for R and\nis built on top of the keras architecture. Throughout this paper, several\nfunctions are introduced that provide users an avenue to easily build models,\ngenerate predictions, and run cross-validations. A summary of the underlying\nmethodology is also presented. The ultimate contribution is a package that\nprovides a set of general modelling and diagnostic tools for data problems in\nwhich there exist both functional and scalar covariates.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 22:32:29 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 04:41:36 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Thind", "Barinder", ""], ["Wu", "Sidi", ""], ["Groenewald", "Richard", ""], ["Cao", "Jiguo", ""]]}, {"id": "2009.09217", "submitter": "Luca Martino", "authors": "Luca Martino, Jesse Read", "title": "A Joint introduction to Gaussian Processes and Relevance Vector Machines\n  with Connections to Kalman filtering and other Kernel Smoothers", "comments": null, "journal-ref": "Information Fusion, Volume 74, Pages 17-38, 2021", "doi": "10.1016/j.inffus.2021.03.002", "report-no": null, "categories": "cs.LG cs.AI cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expressive power of Bayesian kernel-based methods has led them to become\nan important tool across many different facets of artificial intelligence, and\nuseful to a plethora of modern application domains, providing both power and\ninterpretability via uncertainty analysis. This article introduces and\ndiscusses two methods which straddle the areas of probabilistic Bayesian\nschemes and kernel methods for regression: Gaussian Processes and Relevance\nVector Machines. Our focus is on developing a common framework with which to\nview these methods, via intermediate methods a probabilistic version of the\nwell-known kernel ridge regression, and drawing connections among them, via\ndual formulations, and discussion of their application in the context of major\ntasks: regression, smoothing, interpolation, and filtering. Overall, we provide\nunderstanding of the mathematical concepts behind these models, and we\nsummarize and discuss in depth different interpretations and highlight the\nrelationship to other methods, such as linear kernel smoothers, Kalman\nfiltering and Fourier approximations. Throughout, we provide numerous figures\nto promote understanding, and we make numerous recommendations to\npractitioners. Benefits and drawbacks of the different techniques are\nhighlighted. To our knowledge, this is the most in-depth study of its kind to\ndate focused on these two methods, and will be relevant to theoretical\nunderstanding and practitioners throughout the domains of data-science, signal\nprocessing, machine learning, and artificial intelligence in general.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 12:22:41 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 20:03:15 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 17:02:30 GMT"}, {"version": "v4", "created": "Sun, 11 Jul 2021 19:28:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Martino", "Luca", ""], ["Read", "Jesse", ""]]}, {"id": "2009.09974", "submitter": "Francesca Romana Crucinio", "authors": "Francesca R Crucinio, Arnaud Doucet, Adam M Johansen", "title": "A Particle Method for Solving Fredholm Equations of the First Kind", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fredholm integral equations of the first kind are the prototypical example of\nill-posed linear inverse problems. They model, among other things,\nreconstruction of distorted noisy observations and indirect density estimation\nand also appear in instrumental variable regression. However, their numerical\nsolution remains a challenging problem. Many techniques currently available\nrequire a preliminary discretization of the domain of the solution and make\nstrong assumptions about its regularity. For example, the popular expectation\nmaximization smoothing (EMS) scheme requires the assumption of piecewise\nconstant solutions which is inappropriate for most applications. We propose\nhere a novel particle method that circumvents these two issues. This algorithm\ncan be thought of as a Monte Carlo approximation of the EMS scheme which not\nonly performs an adaptive stochastic discretization of the domain but also\nresults in smooth approximate solutions. We analyze the theoretical properties\nof the EMS iteration and of the corresponding particle algorithm. Compared to\nstandard EMS, we show experimentally that our novel particle method provides\nstate-of-the-art performance for realistic systems, including motion deblurring\nand reconstruction of cross-section images of the brain from positron emission\ntomography.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:57:16 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 13:34:55 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Crucinio", "Francesca R", ""], ["Doucet", "Arnaud", ""], ["Johansen", "Adam M", ""]]}, {"id": "2009.10303", "submitter": "Ricardo Baptista", "authors": "Ricardo Baptista, Olivier Zahm, Youssef Marzouk", "title": "An adaptive transport framework for joint and conditional density\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework to robustly characterize joint and conditional\nprobability distributions via transport maps. Transport maps or \"flows\"\ndeterministically couple two distributions via an expressive monotone\ntransformation. Yet, learning the parameters of such transformations in high\ndimensions is challenging given few samples from the unknown target\ndistribution, and structural choices for these transformations can have a\nsignificant impact on performance. Here we formulate a systematic framework for\nrepresenting and learning monotone maps, via invertible transformations of\nsmooth functions, and demonstrate that the associated minimization problem has\na unique global optimum. Given a hierarchical basis for the appropriate\nfunction space, we propose a sample-efficient adaptive algorithm that estimates\na sparse approximation for the map. We demonstrate how this framework can learn\ndensities with stable generalization performance across a wide range of sample\nsizes on real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 03:41:45 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Baptista", "Ricardo", ""], ["Zahm", "Olivier", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2009.10316", "submitter": "Renate Meyer Dr", "authors": "Renate Meyer, Matthew C. Edwards, Patricio Maturana-Russel, Nelson\n  Christensen", "title": "Computational Techniques for Parameter Estimation of Gravitational Wave\n  Signals", "comments": "45 pages, 5 figures, to be published in Wiley Interdisciplinary\n  Reviews -- Computational Statistics", "journal-ref": null, "doi": "10.1002/wics.1532", "report-no": null, "categories": "gr-qc stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the very first detection of gravitational waves from the coalescence of\ntwo black holes in 2015, Bayesian statistical methods have been routinely\napplied by LIGO and Virgo to extract the signal out of noisy interferometric\nmeasurements, obtain point estimates of the physical parameters responsible for\nproducing the signal, and rigorously quantify their uncertainties. Different\ncomputational techniques have been devised depending on the source of the\ngravitational radiation and the gravitational waveform model used. Prominent\nsources of gravitational waves are binary black hole or neutron star mergers,\nthe only objects that have been observed by detectors to date. But also\ngravitational waves from core collapse supernovae, rapidly rotating neutron\nstars, and the stochastic gravitational wave background are in the sensitivity\nband of the ground-based interferometers and expected to be observable in\nfuture observation runs. As nonlinearities of the complex waveforms and the\nhigh-dimensional parameter spaces preclude analytic evaluation of the posterior\ndistribution, posterior inference for all these sources relies on\ncomputer-intensive simulation techniques such as Markov chain Monte Carlo\nmethods. A review of state-of-the-art Bayesian statistical parameter estimation\nmethods will be given for researchers in this cross-disciplinary area of\ngravitational wave data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 04:52:12 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Meyer", "Renate", ""], ["Edwards", "Matthew C.", ""], ["Maturana-Russel", "Patricio", ""], ["Christensen", "Nelson", ""]]}, {"id": "2009.10440", "submitter": "Marcin Mider", "authors": "Marcin Mider, Paul A. Jenkins, Murray Pollock and Gareth O. Roberts", "title": "The computational cost of blocking for sampling discretely observed\n  diffusions", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches for conducting Bayesian inference on discretely observed\ndiffusions involve imputing diffusion bridges between observations. This can be\ncomputationally challenging in settings in which the temporal horizon between\nsubsequent observations is large, due to the poor scaling of algorithms for\nsimulating bridges as observation distance increases. It is common in practical\nsettings to use a blocking scheme, in which the path is split into a\n(user-specified) number of overlapping segments and a Gibbs sampler is employed\nto update segments in turn. Substituting the independent simulation of\ndiffusion bridges for one obtained using blocking introduces an inherent\ntrade-off: we are now imputing shorter bridges at the cost of introducing a\ndependency between subsequent iterations of the bridge sampler. This is further\ncomplicated by the fact that there are a number of possible ways to implement\nthe blocking scheme, each of which introduces a different dependency structure\nbetween iterations. Although blocking schemes have had considerable empirical\nsuccess in practice, there has been no analysis of this trade-off nor guidance\nto practitioners on the particular specifications that should be used to obtain\na computationally efficient implementation. In this article we conduct this\nanalysis (under the simplifying assumption that the underlying diffusion is a\nGaussian process), and demonstrate that the expected computational cost of a\nblocked path-space rejection sampler scales asymptotically at an almost cubic\nrate with respect to the observation distance. Numerical experiments suggest\napplicability of both the results of our paper and the guidance we provide\nbeyond the class of linear diffusions considered.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 10:35:27 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Mider", "Marcin", ""], ["Jenkins", "Paul A.", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2009.10481", "submitter": "Izhar Asael Alonzo Matamoros", "authors": "Izhar Asael Alonzo Matamoros and Alicia Nieto-Reyes", "title": "An R package for Normality in Stationary Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normality is the main assumption for analyzing dependent data in several time\nseries models, and tests of normality have been widely studied in the\nliterature, however, the implementations of these tests are limited. The\n\\textbf{nortsTest} package performs the tests of \\textit{Lobato and Velasco,\nEpps, Psaradakis and Vavra} and \\textit{random projection} for normality of\nstationary processes. In addition, the package offers visual diagnostics for\nchecking stationarity and normality assumptions for the most used time series\nmodels in several \\R packages. The aim of this work is to show the\nfunctionality of the package, presenting each test performance with simulated\nexamples, and the package utility for model diagnostic in time series analysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 12:00:40 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Matamoros", "Izhar Asael Alonzo", ""], ["Nieto-Reyes", "Alicia", ""]]}, {"id": "2009.10532", "submitter": "Robert Miller", "authors": "Robert Miller and Phil Maguire", "title": "A rapidly updating stratified mix-adjusted median property price index\n  model", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homeowners, first-time buyers, banks, governments and construction companies\nare highly interested in following the state of the property market. Currently,\nproperty price indexes are published several months out of date and hence do\nnot offer the up-to-date information which housing market stakeholders need in\norder to make informed decisions. In this article, we present an updated\nversion of a central-price tendency based property price index which uses\ngeospatial property data and stratification in order to compare similar houses.\nThe expansion of the algorithm to include additional parameters owing to a new\ndata structure implementation and a richer dataset allows for the construction\nof a far smoother and more robust index than the original algorithm produced.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 13:23:03 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Miller", "Robert", ""], ["Maguire", "Phil", ""]]}, {"id": "2009.10548", "submitter": "Vishist Srivastava", "authors": "Vishist Srivastava, Prashant Yadav, Ajuni Singh", "title": "Football and externalities: Using mathematical modelling to predict the\n  changing fortunes of Newcastle United", "comments": "13 Pages, 4 figures, code snippets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Public Investment Fund (PIF), is Saudi Arabia's sovereign wealth fund. It\nis one of the world's largest sovereign wealth funds, with an estimated net\ncapital of $382 billion. It was established to invest funds on behalf of the\nGovernment of Saudi Arabia. Saudi Arabia is aiming to transfer the PIF from a\nmere local authority to the world's largest sovereign fund. Thus, PIF is\nworking to manage $400 billion worth of assets by 2020. It was with this Public\nInvestment Fund that Saudi Arabia decided to buy out the football club-\nNewcastle United FC- a mid-table club of the premier league. In this paper, we\naim to forecast the investment levels and the subsequent improve in the league\nposition of Newcastle United FC using the model of another premier league club-\nManchester City as the base. We employ the DiD approach of logistical\nregression through Python.\n  Keywords: Regression, Investment, Football, Forecasting\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 13:40:16 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Srivastava", "Vishist", ""], ["Yadav", "Prashant", ""], ["Singh", "Ajuni", ""]]}, {"id": "2009.10629", "submitter": "Kai Yang", "authors": "Kai Yang, Masoud Asgharian, Sahir Bhatnagar", "title": "Improving Convergence for Nonconvex Composite Programming", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional nonconvex problems are popular in today's machine learning\nand statistical genetics research. Recently, Ghadimi and Lan [1] proposed an\nalgorithm to optimize nonconvex high-dimensional problems. There are several\nparameters in their algorithm that are to be set before running the algorithm.\nIt is not trivial how to choose these parameters nor there is, to the best of\nour knowledge, an explicit rule on how to select the parameters to make the\nalgorithm converges faster. We analyze Ghadimi and Lan's algorithm to gain an\ninterpretation based on the inequality constraints for convergence and the\nupper bound for the norm of the gradient analogue. Our interpretation of their\nalgorithm suggests this to be a damped Nesterov's acceleration scheme. Based on\nthis, we propose an approach on how to select the parameters to improve\nconvergence of the algorithm. Our numerical studies using high-dimensional\nnonconvex sparse learning problems, motivated by image denoising and\nstatistical genetics applications, show that convergence can be made, on\naverage, considerably faster than that of the conventional ISTA algorithm for\nsuch optimization problems with over $10000$ variables should the parameters be\nchosen using our proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:37:09 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 14:56:25 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yang", "Kai", ""], ["Asgharian", "Masoud", ""], ["Bhatnagar", "Sahir", ""]]}, {"id": "2009.10641", "submitter": "Jes\\'us Daniel Arroyo Reli\\'on", "authors": "Jes\\'us Arroyo, Elizaveta Levina", "title": "Overlapping community detection in networks via sparse spectral\n  decomposition", "comments": null, "journal-ref": null, "doi": "10.1007/s13171-021-00245-4", "report-no": null, "categories": "cs.SI cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating overlapping community memberships in a\nnetwork, where each node can belong to multiple communities. More than a few\ncommunities per node are difficult to both estimate and interpret, so we focus\non sparse node membership vectors. Our algorithm is based on sparse principal\nsubspace estimation with iterative thresholding. The method is computationally\nefficient, with a computational cost equivalent to estimating the leading\neigenvectors of the adjacency matrix, and does not require an additional\nclustering step, unlike spectral clustering methods. We show that a fixed point\nof the algorithm corresponds to correct node memberships under a version of the\nstochastic block model. The methods are evaluated empirically on simulated and\nreal-world networks, showing good statistical performance and computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 07:31:09 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 06:43:18 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Arroyo", "Jes\u00fas", ""], ["Levina", "Elizaveta", ""]]}, {"id": "2009.10831", "submitter": "Zijian Wang", "authors": "Daniel Sanz-Alonso, Zijian Wang", "title": "Bayesian Update with Importance Sampling: Required Sample Size", "comments": null, "journal-ref": null, "doi": "10.3390/e23010022", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is used to approximate Bayes' rule in many computational\napproaches to Bayesian inverse problems, data assimilation and machine\nlearning. This paper reviews and further investigates the required sample size\nfor importance sampling in terms of the $\\chi^2$-divergence between target and\nproposal. We develop general abstract theory and illustrate through numerous\nexamples the roles that dimension, noise-level and other model parameters play\nin approximating the Bayesian update with importance sampling. Our examples\nalso facilitate a new direct comparison of standard and optimal proposals for\nparticle filtering.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 21:34:21 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Sanz-Alonso", "Daniel", ""], ["Wang", "Zijian", ""]]}, {"id": "2009.10979", "submitter": "Ursula Laa", "authors": "Ursula Laa, Dianne Cook, Stuart Lee", "title": "Burning sage: Reversing the curse of dimensionality in the visualization\n  of high-dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data analysis the curse of dimensionality reasons that\npoints tend to be far away from the center of the distribution and on the edge\nof high-dimensional space. Contrary to this, is that projected data tends to\nclump at the center. This gives a sense that any structure near the center of\nthe projection is obscured, whether this is true or not. A transformation to\nreverse the curse, is defined in this paper, which uses radial transformations\non the projected data. It is integrated seamlessly into the grand tour\nalgorithm, and we have called it a burning sage tour, to indicate that it\nreverses the curse. The work is implemented into the tourr package in R.\nSeveral case studies are included that show how the sage visualizations enhance\nexploratory clustering and classification problems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:42:39 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Laa", "Ursula", ""], ["Cook", "Dianne", ""], ["Lee", "Stuart", ""]]}, {"id": "2009.11581", "submitter": "Karsten W\\\"ullems", "authors": "Karsten W\\\"ullems, Daniel G\\\"obel, Annika Zurowietz, Hanna Bednarz,\n  Karsten Niehaus, Tim W. Nattkemper", "title": "COBI-GRINE: A Tool for Visualization and Advanced Evaluation of\n  Communities in Mass Channel Similarity Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of groups of molecules that co-localize with histopathological\npatterns or sub-structures is an important step to combine the rich\nhigh-dimensional content of mass spectrometry imaging (MSI) with classic\nhistopathological staining. Here we present the evolution of GRINE to\nCOBI-GRINE, an interactive web tool that maps MSI data onto a graph structure\nto detect communities of laterally similar distributed molecules and\nco-visualizes the communities with Hematoxylin and Eosin (HE) stained images.\nThereby the tool enables biologists and pathologists to examine the MSI image\ngraph in a target-oriented manner and links molecular co-localization to\npathology. Another feature is the manual optimization of cluster results with\nthe assist of graph statistics in order to improve the community results. As\nthe graphs can become very complex, those statistics provide good heuristics to\nsupport and accelerate the detection of sub-clusters and misclusterings. This\nkind of edited cluster optimization allows the integration of expert background\nknowledge into the clustering result and a more precise analysis of links\nbetween molecular co-localization and pathology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 10:13:16 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["W\u00fcllems", "Karsten", ""], ["G\u00f6bel", "Daniel", ""], ["Zurowietz", "Annika", ""], ["Bednarz", "Hanna", ""], ["Niehaus", "Karsten", ""], ["Nattkemper", "Tim W.", ""]]}, {"id": "2009.12132", "submitter": "Joris Tavernier", "authors": "Joris Tavernier, Jaak Simm, Adam Arany, Karl Meerbergen, Yves Moreau", "title": "Multilevel Gibbs Sampling for Bayesian Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian regression remains a simple but effective tool based on Bayesian\ninference techniques. For large-scale applications, with complicated posterior\ndistributions, Markov Chain Monte Carlo methods are applied. To improve the\nwell-known computational burden of Markov Chain Monte Carlo approach for\nBayesian regression, we developed a multilevel Gibbs sampler for Bayesian\nregression of linear mixed models. The level hierarchy of data matrices is\ncreated by clustering the features and/or samples of data matrices.\nAdditionally, the use of correlated samples is investigated for variance\nreduction to improve the convergence of the Markov Chain. Testing on a diverse\nset of data sets, speed-up is achieved for almost all of them without\nsignificant loss in predictive performance.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 11:18:17 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Tavernier", "Joris", ""], ["Simm", "Jaak", ""], ["Arany", "Adam", ""], ["Meerbergen", "Karl", ""], ["Moreau", "Yves", ""]]}, {"id": "2009.12424", "submitter": "Jeffrey Rosenthal", "authors": "Gareth O. Roberts, Jeffrey S. Rosenthal, and Nicholas G. Tawn", "title": "Skew Brownian Motion and Complexity of the ALPS Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulated tempering is a popular method of allowing MCMC algorithms to move\nbetween modes of a multimodal target density {\\pi}. The paper [24] introduced\nthe Annealed Leap-Point Sampler (ALPS) to allow for rapid movement between\nmodes. In this paper, we prove that, under appropriate assumptions, a suitably\nscaled version of the ALPS algorithm converges weakly to skew Brownian motion.\nOur results show that under appropriate assumptions, the ALPS algorithm mixes\nin time O(d[log(d)]^2 ) or O(d), depending on which version is used.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 20:29:26 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 16:40:38 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Roberts", "Gareth O.", ""], ["Rosenthal", "Jeffrey S.", ""], ["Tawn", "Nicholas G.", ""]]}, {"id": "2009.12933", "submitter": "Yawei Ge", "authors": "Yawei Ge, Heike Hofmann", "title": "A grammar of graphics framework for generalized parallel coordinate\n  plots", "comments": "26 pages, 14 figures. For the implementation in R, see:\n  https://github.com/yaweige/ggpcp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel coordinate plots (PCP) are a useful tool in exploratory data\nanalysis of high-dimensional numerical data. The use of PCPs is limited when\nworking with categorical variables or a mix of categorical and continuous\nvariables. In this paper, we propose generalized parallel coordinate plots\n(GPCP) to extend the ability of PCPs from just numeric variables to dealing\nseamlessly with a mix of categorical and numeric variables in a single plot. In\nthis process we find that existing solutions for categorical values only, such\nas hammock plots or parsets become edge cases in the new framework. By focusing\non individual observation rather a marginal frequency we gain additional\nflexibility. The resulting approach is implemented in the R package ggpcp.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 19:55:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Ge", "Yawei", ""], ["Hofmann", "Heike", ""]]}, {"id": "2009.13636", "submitter": "Paul Parker", "authors": "Paul A. Parker, Scott H. Holan, and Skye A. Wills", "title": "A General Bayesian Model for Heteroskedastic Data with Fully Conjugate\n  Full-Conditional Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for heteroskedastic data are relevant in a wide variety of\napplications ranging from financial time series to environmental statistics.\nHowever, the topic of modeling the variance function conditionally has not seen\nnear as much attention as modeling the mean. Volatility models have been used\nin specific applications, but these models can be difficult to fit in a\nBayesian setting due to posterior distributions that are challenging to sample\nfrom efficiently. In this work, we introduce a general model for\nheteroskedastic data. This approach models the conditional variance in a mixed\nmodel approach as a function of any desired covariates or random effects. We\nrely on new distribution theory in order to construct priors that yield fully\nconjugate full conditional distributions. Thus, our approach can easily be fit\nvia Gibbs sampling. Furthermore, we extend the model to a deep learning\napproach that can provide highly accurate estimates for time dependent data. We\nalso provide an extension for heavy-tailed data. We illustrate our methodology\nvia three applications. The first application utilizes a high dimensional soil\ndataset with inherent spatial dependence. The second application involves\nmodeling of asset volatility. The third application focuses on clinical trial\ndata for creatinine.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:25:44 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Parker", "Paul A.", ""], ["Holan", "Scott H.", ""], ["Wills", "Skye A.", ""]]}, {"id": "2009.13958", "submitter": "Xiancheng Li", "authors": "Xiancheng Li, Luca Verginer, Massimo Riccaboni and Pietro Panzarasa", "title": "A network approach to expertise retrieval based on path similarity and\n  credit allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing availability of online scholarly databases, publication\nrecords can be easily extracted and analysed. Researchers can promptly keep\nabreast of others' scientific production and, in principle, can select new\ncollaborators and build new research teams. A critical factor one should\nconsider when contemplating new potential collaborations is the possibility of\nunambiguously defining the expertise of other researchers. While some\norganisations have established database systems to enable their members to\nmanually produce a profile, maintaining such systems is time-consuming and\ncostly. Therefore, there has been a growing interest in retrieving expertise\nthrough automated approaches. Indeed, the identification of researchers'\nexpertise is of great value in many applications, such as identifying qualified\nexperts to supervise new researchers, assigning manuscripts to reviewers, and\nforming a qualified team. Here, we propose a network-based approach to the\nconstruction of authors' expertise profiles. Using the MEDLINE corpus as an\nexample, we show that our method can be applied to a number of widely used data\nsets and outperforms other methods traditionally used for expertise\nidentification.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 12:19:22 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Li", "Xiancheng", ""], ["Verginer", "Luca", ""], ["Riccaboni", "Massimo", ""], ["Panzarasa", "Pietro", ""]]}, {"id": "2009.14131", "submitter": "Hedibert Lopes", "authors": "Paloma W. Uribe and Hedibert F. Lopes", "title": "Dynamic sparsity on dynamic regression models", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work, we consider variable selection and shrinkage for the\nGaussian dynamic linear regression within a Bayesian framework. In particular,\nwe propose a novel method that allows for time-varying sparsity, based on an\nextension of spike-and-slab priors for dynamic models. This is done by\nassigning appropriate Markov switching priors for the time-varying\ncoefficients' variances, extending the previous work of Ishwaran and Rao\n(2005). Furthermore, we investigate different priors, including the common\nInverted gamma prior for the process variances, and other mixture prior\ndistributions such as Gamma priors for both the spike and the slab, which leads\nto a mixture of Normal-Gammas priors (Griffin ad Brown, 2010) for the\ncoefficients. In this sense, our prior can be view as a dynamic variable\nselection prior which induces either smoothness (through the slab) or shrinkage\ntowards zero (through the spike) at each time point. The MCMC method used for\nposterior computation uses Markov latent variables that can assume binary\nregimes at each time point to generate the coefficients' variances. In that\nway, our model is a dynamic mixture model, thus, we could use the algorithm of\nGerlach et al (2000) to generate the latent processes without conditioning on\nthe states. Finally, our approach is exemplified through simulated examples and\na real data application.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:26:08 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Uribe", "Paloma W.", ""], ["Lopes", "Hedibert F.", ""]]}, {"id": "2009.14296", "submitter": "Hedibert Lopes", "authors": "Bruno Fava and Hedibert F. Lopes", "title": "The Illusion of the Illusion of Sparsity: An exercise in prior\n  sensitivity", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Big Data raises the question of how to model economic\nrelations when there is a large number of possible explanatory variables. We\nrevisit the issue by comparing the possibility of using dense or sparse models\nin a Bayesian approach, allowing for variable selection and shrinkage. More\nspecifically, we discuss the results reached by Giannone, Lenza, and Primiceri\n(2020) through a \"Spike-and-Slab\" prior, which suggest an \"illusion of\nsparsity\" in economic data, as no clear patterns of sparsity could be detected.\nWe make a further revision of the posterior distributions of the model, and\npropose three experiments to evaluate the robustness of the adopted prior\ndistribution. We find that the pattern of sparsity is sensitive to the prior\ndistribution of the regression coefficients, and present evidence that the\nmodel indirectly induces variable selection and shrinkage, which suggests that\nthe \"illusion of sparsity\" could be, itself, an illusion. Code is available on\ngithub.com/bfava/IllusionOfIllusion.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 20:39:13 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Fava", "Bruno", ""], ["Lopes", "Hedibert F.", ""]]}, {"id": "2009.14615", "submitter": "Haoyang Cheng", "authors": "Haoyang Cheng, Wenquan Cui, Xu Jianjun", "title": "Online Sparse Sliced Inverse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the demand for tackling the problem of streaming data with high\ndimensional covariates, we propose an online sparse sliced inverse regression\n(OSSIR) method for online sufficient dimension reduction. The existing online\nsufficient dimension reduction methods focus on the case when the dimension $p$\nis small. In this article, we show that our method can achieve better\nstatistical accuracy and computation speed when the dimension $p$ is large.\nThere are two important steps in our method, one is to extend the online\nprincipal component analysis to iteratively obtain the eigenvalues and\neigenvectors of the kernel matrix, the other is to use the truncated gradient\nto achieve online $L_{1}$ regularization. We also analyze the convergence of\nthe extended Candid covariance-free incremental PCA(CCIPCA) and our method. By\ncomparing several existing methods in the simulations and real data\napplications, we demonstrate the effectiveness and efficiency of our method.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 12:41:52 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 08:32:18 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Cheng", "Haoyang", ""], ["Cui", "Wenquan", ""], ["Jianjun", "Xu", ""]]}, {"id": "2009.14677", "submitter": "Karsten W\\\"ullems", "authors": "Karsten W\\\"ullems, Tim W. Nattkemper", "title": "SoRC -- Evaluation of Computational Molecular Co-Localization Analysis\n  in Mass Spectrometry Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational analysis of Mass Spectrometry Imaging (MSI) data aims at\nthe identification of interesting mass co-localizations and the visualization\nof their lateral distribution in the sample, usually a tissue cross section.\nBut as the morphological structure of tissues and the different kinds of mass\nco-localization naturally show a huge diversity, the selection and tuning of\nthe computational method is a time-consuming effort. In this work we address\nthe special problem of computationally grouping mass channel images according\nto their similarities in their lateral distribution patterns. Such an analysis\nis driven by the idea, that groups of molecules that feature a similar\ndistribution pattern may have a functional relation. But the selection of the\nsimilarity function and other parameters is often done by a time-consuming and\nunsatsifactory trial and error. We propose a new flexible workflow scheme\ncalled SoRC (sum of ranked cluster indices) for automating this tuning step and\nmaking it much more efficient. We test SoRC using three different data sets\nacquired from the lab for three different kinds of samples (barley seed, mouse\nbladder tissue, human PXE skin). We show, that SORC can be applied to score and\nvisualize the results obtained with the applied methods in short time without\ntoo much effort. In our application example, the SoRC results for the three\ndata sets reveal that a) some well-known similarity functions are suited to\nachieve good results for all three data sets and b) for the MSI data featuring\na higher degree of irregularity improved results can be achieved by applying\nnon-standard similarity functions. The SoRC scores computed with our approach\nindicate that an automated testing and scoring of different methods for mass\nchannel image grouping can improve the final outcome of a study by finally\nselecting the methods of the highest scores.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 10:24:41 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["W\u00fcllems", "Karsten", ""], ["Nattkemper", "Tim W.", ""]]}]