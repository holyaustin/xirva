[{"id": "0709.0935", "submitter": "Serkan Hosten", "authors": "Serkan Hosten and Seth Sullivant", "title": "The Algebraic Complexity of Maximum Likelihood Estimation for Bivariate\n  Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.CO stat.TH", "license": null, "abstract": "  We study the problem of maximum likelihood estimation for general patterns of\nbivariate missing data for normal and multinomial random variables, under the\nassumption that the data is missing at random (MAR). For normal data, the score\nequations have nine complex solutions, at least one of which is real and\nstatistically significant. Our computations suggest that the number of real\nsolutions is related to whether or not the MAR assumption is satisfied. In the\nmultinomial case, all solutions to the score equations are real and the number\nof real solutions grows exponentially in the number of states of the underlying\nrandom variables, though there is always precisely one statistically\nsignificant local maxima.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2007 19:34:06 GMT"}], "update_date": "2007-09-07", "authors_parsed": [["Hosten", "Serkan", ""], ["Sullivant", "Seth", ""]]}, {"id": "0709.0957", "submitter": "Serkan Hosten", "authors": "Max-Louis G. Buot, Serkan Hosten, and Donald St. P. Richards", "title": "Counting and Locating the Solutions of Polynomial Systems of Maximum\n  Likelihood Equations, II: The Behrens-Fisher Problem", "comments": "To appear in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.CO stat.TH", "license": null, "abstract": "  Let $\\mu$ be a $p$-dimensional vector, and let $\\Sigma_1$ and $\\Sigma_2$ be\n$p \\times p$ positive definite covariance matrices. On being given random\nsamples of sizes $N_1$ and $N_2$ from independent multivariate normal\npopulations $N_p(\\mu,\\Sigma_1)$ and $N_p(\\mu,\\Sigma_2)$, respectively, the\nBehrens-Fisher problem is to solve the likelihood equations for estimating the\nunknown parameters $\\mu$, $\\Sigma_1$, and $\\Sigma_2$. We shall prove that for\n$N_1, N_2 > p$ there are, almost surely, exactly $2p+1$ complex solutions of\nthe likelihood equations. For the case in which $p = 2$, we utilize Monte Carlo\nsimulation to estimate the relative frequency with which a typical\nBehrens-Fisher problem has multiple real solutions; we find that multiple real\nsolutions occur infrequently.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2007 21:03:00 GMT"}], "update_date": "2007-09-10", "authors_parsed": [["Buot", "Max-Louis G.", ""], ["Hosten", "Serkan", ""], ["Richards", "Donald St. P.", ""]]}, {"id": "0709.1309", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Bayes and empirical Bayes changepoint problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": null, "abstract": "  We generalize the approach of Liu and Lawrence (1999) for multiple\nchangepoint problems where the number of changepoints is unknown. The approach\nis based on dynamic programming recursion for efficient calculation of the\nmarginal probability of the data with the hidden parameters integrated out. For\nthe estimation of the hyperparameters, we propose to use Monte Carlo EM when\ntraining data are available. We argue that there is some advantages of using\nsamples from the posterior which takes into account the uncertainty of the\nchangepoints, compared to the traditional MAP estimator, which is also more\nexpensive to compute in this context. The samples from the posterior obtained\nby our algorithm are independent, getting rid of the convergence issue\nassociated with the MCMC approach. We illustrate our approach on limited\nsimulations and some real data set.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2007 04:31:05 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "0709.1721", "submitter": "Jonathan Weare", "authors": "Jonathan Weare", "title": "Parallel marginalization Monte Carlo with applications to conditional\n  path sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": null, "abstract": "  Monte Carlo sampling methods often suffer from long correlation times.\nConsequently, these methods must be run for many steps to generate an\nindependent sample. In this paper a method is proposed to overcome this\ndifficulty. The method utilizes information from rapidly equilibrating coarse\nMarkov chains that sample marginal distributions of the full system. This is\naccomplished through exchanges between the full chain and the auxiliary coarse\nchains. Results of numerical tests on the bridge sampling and\nfiltering/smoothing problems for a stochastic differential equation are\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2007 20:43:15 GMT"}], "update_date": "2007-09-13", "authors_parsed": [["Weare", "Jonathan", ""]]}, {"id": "0709.2317", "submitter": "Alexey Koloydenko", "authors": "J. Lember, A. Koloydenko", "title": "Adjusted Viterbi training for hidden Markov models", "comments": "45 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "07-01", "categories": "math.ST math.PR stat.CO stat.TH", "license": null, "abstract": "  To estimate the emission parameters in hidden Markov models one commonly uses\nthe EM algorithm or its variation. Our primary motivation, however, is the\nPhilips speech recognition system wherein the EM algorithm is replaced by the\nViterbi training algorithm. Viterbi training is faster and computationally less\ninvolved than EM, but it is also biased and need not even be consistent. We\npropose an alternative to the Viterbi training -- adjusted Viterbi training --\nthat has the same order of computational complexity as Viterbi training but\ngives more accurate estimators. Elsewhere, we studied the adjusted Viterbi\ntraining for a special case of mixtures, supporting the theory by simulations.\nThis paper proves the adjusted Viterbi training to be also possible for more\ngeneral hidden Markov models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2007 15:00:52 GMT"}], "update_date": "2007-09-17", "authors_parsed": [["Lember", "J.", ""], ["Koloydenko", "A.", ""]]}, {"id": "0709.2776", "submitter": "Hacene Belbachir", "authors": "Abdelhakim Aknouche Hac\\`ene Belbachir Fay\\c{c}al Hamdi", "title": "A note on calculating autocovariances of periodic ARMA models", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.CO stat.CO", "license": null, "abstract": "  An analytically simple and tractable formula for the start-up autocovariances\nof periodic ARMA (PARMA) models is provided.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2007 09:10:25 GMT"}], "update_date": "2007-09-19", "authors_parsed": [["Hamdi", "Abdelhakim Aknouche Hac\u00e8ne Belbachir Fay\u00e7al", ""]]}, {"id": "0709.2997", "submitter": "Eva Riccomagno", "authors": "Roberto Notari, Eva Riccomagno, Maria-Piera Rogantin", "title": "Two polynomial representations of experimental design", "comments": "13 pages", "journal-ref": "Journal of Statistical Theory and Practice, 1:3-4, 329-346 (2008)", "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  In the context of algebraic statistics an experimental design is described by\na set of polynomials called the design ideal. This, in turn, is generated by\nfinite sets of polynomials. Two types of generating sets are mostly used in the\nliterature: Groebner bases and indicator functions. We briefly describe them\nboth, how they are used in the analysis and planning of a design and how to\nswitch between them. Examples include fractions of full factorial designs and\ndesigns for mixture experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2007 11:50:48 GMT"}], "update_date": "2008-09-10", "authors_parsed": [["Notari", "Roberto", ""], ["Riccomagno", "Eva", ""], ["Rogantin", "Maria-Piera", ""]]}, {"id": "0709.3560", "submitter": "Yeong-Shyeong Tsai", "authors": "Yeong-Shyeong Tsai, Ying-Lin Hsu and Mung-Chung Shung", "title": "On The Density Estimation by Super-Parametric Method", "comments": "In this paper, new aspproaches od density estimation are studied. The\n  B-spline estimatos and the Berzier spline are stdudied carfully. The\n  consistency of the Bezier spline estimator is studied as well. There are 17\n  pages including 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The super-parametric density estimators and its related algorism were\nsuggested by Y. -S. Tsai et al [7]. The number of parameters is unlimited in\nthe super- parametric estimators and it is a general theory in sense of\nunifying or connecting nonparametric and parametric estimators. Before applying\nto numerical examples, we can not give any comment of the estimators. In this\npaper, we will focus on the implementation, the computer programming, of the\nalgorism and strategies of choosing window functions. B-splines, Bezier splines\nand covering windows are studied as well. According to the criterion of the\nconvergence conditions for Parzen window, the number of the window functions\nshall be, roughly, proportional to the number of samples and so is the number\nof the variables. Since the algorism is designed for solving the optimization\nof likelihood function, there will be a set of nonlinear equations with a large\nnumber of variables. The results show that algorism suggested by Y. -S. Tsai is\nvery powerful and effective in the sense of mathematics, that is, the iteration\nprocedures converge and the rates of convergence are very fast. Also, the\nnumerical results of different window functions show that the approach of\nsuper-parametric density estimators has ushered a new era of statistics.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2007 04:55:59 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2007 10:08:56 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2007 03:40:43 GMT"}, {"version": "v4", "created": "Fri, 5 Oct 2007 04:50:31 GMT"}, {"version": "v5", "created": "Mon, 15 Oct 2007 14:04:20 GMT"}, {"version": "v6", "created": "Tue, 16 Oct 2007 03:42:34 GMT"}, {"version": "v7", "created": "Fri, 7 Nov 2008 18:57:05 GMT"}], "update_date": "2008-11-07", "authors_parsed": [["Tsai", "Yeong-Shyeong", ""], ["Hsu", "Ying-Lin", ""], ["Shung", "Mung-Chung", ""]]}, {"id": "0709.3906", "submitter": "Simon Wood", "authors": "Simon N. Wood", "title": "Fast stable direct fitting and smoothness selection for Generalized\n  Additive Models", "comments": null, "journal-ref": null, "doi": "10.1111/j.1467-9868.2007.00646.x", "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  Existing computationally efficient methods for penalized likelihood GAM\nfitting employ iterative smoothness selection on working linear models (or\nworking mixed models). Such schemes fail to converge for a non-negligible\nproportion of models, with failure being particularly frequent in the presence\nof concurvity. If smoothness selection is performed by optimizing `whole model'\ncriteria these problems disappear, but until now attempts to do this have\nemployed finite difference based optimization schemes which are computationally\ninefficient, and can suffer from false convergence. This paper develops the\nfirst computationally efficient method for direct GAM smoothness selection. It\nis highly stable, but by careful structuring achieves a computational\nefficiency that leads, in simulations, to lower mean computation times than the\nschemes based on working-model smoothness selection. The method also offers a\nreliable way of fitting generalized additive mixed models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2007 10:31:42 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Wood", "Simon N.", ""]]}]