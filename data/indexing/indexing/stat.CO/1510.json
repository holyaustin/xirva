[{"id": "1510.00012", "submitter": "Jianbo Ye", "authors": "Jianbo Ye, Panruo Wu, James Z. Wang and Jia Li", "title": "Fast Discrete Distribution Clustering Using Wasserstein Barycenter with\n  Sparse Support", "comments": "double-column, 17 pages, 3 figures, 5 tables. English usage improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of research areas, the weighted bag of vectors and the histogram\nare widely used descriptors for complex objects. Both can be expressed as\ndiscrete distributions. D2-clustering pursues the minimum total within-cluster\nvariation for a set of discrete distributions subject to the\nKantorovich-Wasserstein metric. D2-clustering has a severe scalability issue,\nthe bottleneck being the computation of a centroid distribution, called\nWasserstein barycenter, that minimizes its sum of squared distances to the\ncluster members. In this paper, we develop a modified Bregman ADMM approach for\ncomputing the approximate discrete Wasserstein barycenter of large clusters. In\nthe case when the support points of the barycenters are unknown and have low\ncardinality, our method achieves high accuracy empirically at a much reduced\ncomputational cost. The strengths and weaknesses of our method and its\nalternatives are examined through experiments, and we recommend scenarios for\ntheir respective usage. Moreover, we develop both serial and parallelized\nversions of the algorithm. By experimenting with large-scale data, we\ndemonstrate the computational efficiency of the new methods and investigate\ntheir convergence properties and numerical stability. The clustering results\nobtained on several datasets in different domains are highly competitive in\ncomparison with some widely used methods in the corresponding areas.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 20:10:59 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 22:40:26 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 23:41:22 GMT"}, {"version": "v4", "created": "Mon, 9 Jan 2017 18:14:20 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Ye", "Jianbo", ""], ["Wu", "Panruo", ""], ["Wang", "James Z.", ""], ["Li", "Jia", ""]]}, {"id": "1510.00024", "submitter": "Paul Constantine", "authors": "Paul G. Constantine and Carson Kent and Tan Bui-Thanh", "title": "Accelerating MCMC with active subspaces", "comments": null, "journal-ref": null, "doi": "10.1137/15M1042127", "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markov chain Monte Carlo (MCMC) method is the computational workhorse for\nBayesian inverse problems. However, MCMC struggles in high-dimensional\nparameter spaces, since its iterates must sequentially explore the\nhigh-dimensional space. This struggle is compounded in physical applications\nwhen the nonlinear forward model is computationally expensive. One approach to\naccelerate MCMC is to reduce the dimension of the state space. Active subspaces\nare part of an emerging set of tools for subspace-based dimension reduction. An\nactive subspace in a given inverse problem indicates a separation between a\nlow-dimensional subspace that is informed by the data and its orthogonal\ncomplement that is constrained by the prior. With this information, one can run\nthe sequential MCMC on the active variables while sampling independently\naccording to the prior on the inactive variables. However, this approach to\nincrease efficiency may introduce bias. We provide a bound on the Hellinger\ndistance between the true posterior and its active subspace- exploiting\napproximation. And we demonstrate the active subspace-accelerated MCMC on two\ncomputational examples: (i) a two-dimensional parameter space with a quadratic\nforward model and one-dimensional active subspace and (ii) a 100-dimensional\nparameter space with a PDE-based forward model and a two-dimensional active\nsubspace.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 20:21:19 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 20:32:11 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Constantine", "Paul G.", ""], ["Kent", "Carson", ""], ["Bui-Thanh", "Tan", ""]]}, {"id": "1510.00041", "submitter": "Michael Kane", "authors": "Taylor Arnold, Michael Kane, and Simon Urbanek", "title": "iotools: High-Performance I/O Tools for R", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iotools package provides a set of tools for Input/Output (I/O) intensive\ndatasets processing in R (R Core Team, 2014). Efficent parsing methods are\nincluded which minimize copying and avoid the use of intermediate string\nrepresentations whenever possible. Functions for applying chunk-wise operations\nallow for computing on streaming input as well as arbitrarily large files. We\npresent a set of example use cases for iotools, as well as extensive benchmarks\ncomparing comparable functions provided in both core-R as well as other\ncontributed packages.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 21:31:42 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 17:49:16 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Arnold", "Taylor", ""], ["Kane", "Michael", ""], ["Urbanek", "Simon", ""]]}, {"id": "1510.00084", "submitter": "Xiangyu Wang", "authors": "Binyan Jiang, Xiangyu Wang, and Chenlei Leng", "title": "A Direct Approach for Sparse Quadratic Discriminant Analysis", "comments": "Updated to the JMLR format", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic discriminant analysis (QDA) is a standard tool for classification\ndue to its simplicity and flexibility. Because the number of its parameters\nscales quadratically with the number of the variables, QDA is not practical,\nhowever, when the dimensionality is relatively large. To address this, we\npropose a novel procedure named DA-QDA for QDA in analyzing high-dimensional\ndata. Formulated in a simple and coherent framework, DA-QDA aims to directly\nestimate the key quantities in the Bayes discriminant function including\nquadratic interactions and a linear index of the variables for classification.\nUnder appropriate sparsity assumptions, we establish consistency results for\nestimating the interactions and the linear index, and further demonstrate that\nthe misclassification rate of our procedure converges to the optimal Bayes\nrisk, even when the dimensionality is exponentially high with respect to the\nsample size. An efficient algorithm based on the alternating direction method\nof multipliers (ADMM) is developed for finding interactions, which is much\nfaster than its competitor in the literature. The promising performance of\nDA-QDA is illustrated via extensive simulation studies and the analysis of four\nreal datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 01:24:20 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 05:57:40 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 05:00:10 GMT"}, {"version": "v4", "created": "Wed, 5 Sep 2018 08:36:22 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Jiang", "Binyan", ""], ["Wang", "Xiangyu", ""], ["Leng", "Chenlei", ""]]}, {"id": "1510.00292", "submitter": "Sergey Kovalchuk", "authors": "Sergey V. Kovalchuk, Aleksey V. Krikunov, Konstantin V. Knyazkov,\n  Sergey S. Kosukhin, Alexander V. Boukhanovsky", "title": "On Classification Issues within Ensemble-Based Complex System Simulation\n  Tasks", "comments": "To be presented at CCS'15 (http://www.ccs2015.org/)", "journal-ref": null, "doi": "10.1007/s00477-016-1324-5", "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary tasks of complex system simulation are often related to the\nissue of uncertainty management. It comes from the lack of information or\nknowledge about the simulated system as well as from restrictions of the model\nset being used. One of the powerful tools for the uncertainty management is\nensemble-based simulation, which uses variation in input or output data, model\nparameters, or available versions of models to improve the simulation\nperformance. Furthermore the system of models for complex system simulation\n(especially in case of hiring ensemble-based approach) can be considered as a\ncomplex system. As a result, the identification of the complex model's\nstructure and parameters provide additional sources of uncertainty to be\nmanaged. Within the presented work we are developing a conceptual and\ntechnological approach to manage the ensemble-based simulation taking into\naccount changing states of both simulated system and system of models within\nthe ensemble-based approach. The states of these systems are considered as a\nsubject of classification with consequent inference of better strategies for\nensemble evolution over the simulation time and ensemble aggregation. Here the\nensemble evolution enables implementation of dynamic reactive solutions which\ncan automatically conform to the changing states of both systems. The ensemble\naggregation can be considered within a scope of averaging (regression way) or\nselection (classification way, which complement the classification mentioned\nearlier) approach. The technological basis for such approach includes\nensemble-based simulation techniques using domain-specific software combined\nwithin a composite application; data science approaches for analysis of\navailable datasets (simulation data, observations, situation assessment etc.);\nand machine learning algorithms for classes identification, ensemble management\nand knowledge acquisition.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 15:57:16 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 14:01:16 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Kovalchuk", "Sergey V.", ""], ["Krikunov", "Aleksey V.", ""], ["Knyazkov", "Konstantin V.", ""], ["Kosukhin", "Sergey S.", ""], ["Boukhanovsky", "Alexander V.", ""]]}, {"id": "1510.00503", "submitter": "Julien Bect", "authors": "Paul Feliot (L2S, GdR MASCOT-NUM), Julien Bect (L2S, GdR MASCOT-NUM),\n  Emmanuel Vazquez (L2S, GdR MASCOT-NUM)", "title": "A Bayesian approach to constrained single- and multi-objective\n  optimization", "comments": null, "journal-ref": "Journal of Global Optimization, 67(1):97-133 (2017)", "doi": "10.1007/s10898-016-0427-3", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the problem of derivative-free (single- or\nmulti-objective) optimization subject to multiple inequality constraints. Both\nthe objective and constraint functions are assumed to be smooth, non-linear and\nexpensive to evaluate. As a consequence, the number of evaluations that can be\nused to carry out the optimization is very limited, as in complex industrial\ndesign optimization problems. The method we propose to overcome this difficulty\nhas its roots in both the Bayesian and the multi-objective optimization\nliteratures. More specifically, an extended domination rule is used to handle\nobjectives and constraints in a unified way, and a corresponding expected\nhyper-volume improvement sampling criterion is proposed. This new criterion is\nnaturally adapted to the search of a feasible point when none is available, and\nreduces to existing Bayesian sampling criteria---the classical Expected\nImprovement (EI) criterion and some of its constrained/multi-objective\nextensions---as soon as at least one feasible point is available. The\ncalculation and optimization of the criterion are performed using Sequential\nMonte Carlo techniques. In particular, an algorithm similar to the subset\nsimulation method, which is well known in the field of structural reliability,\nis used to estimate the criterion. The method, which we call BMOO (for Bayesian\nMulti-Objective Optimization), is compared to state-of-the-art algorithms for\nsingle- and multi-objective constrained optimization.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 06:41:49 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 06:18:14 GMT"}, {"version": "v3", "created": "Mon, 9 May 2016 07:34:53 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Feliot", "Paul", "", "L2S, GdR MASCOT-NUM"], ["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "1510.00551", "submitter": "Adrian O Hagan Dr", "authors": "Adrian O'Hagan, Thomas Brendan Murphy, Luca Scrucca and Isobel Claire\n  Gormley", "title": "Investigation of Parameter Uncertainty in Clustering Using a Gaussian\n  Mixture Model Via Jackknife, Bootstrap and Weighted Likelihood Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are a popular tool in model-based clustering. Such a model is\noften fitted by a procedure that maximizes the likelihood, such as the EM\nalgorithm. At convergence, the maximum likelihood parameter estimates are\ntypically reported, but in most cases little emphasis is placed on the\nvariability associated with these estimates. In part this may be due to the\nfact that standard errors are not directly calculated in the model-fitting\nalgorithm, either because they are not required to fit the model, or because\nthey are difficult to compute. The examination of standard errors in\nmodel-based clustering is therefore typically neglected. The widely used R\npackage mclust has recently introduced bootstrap and weighted likelihood\nbootstrap methods to facilitate standard error estimation. This paper provides\nan empirical comparison of these methods (along with the jackknife method) for\nproducing standard errors and confidence intervals for mixture parameters.\nThese methods are illustrated and contrasted in both a simulation study and in\nthe traditional Old Faithful data set and Thyroid data set.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 10:26:57 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 11:18:50 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 16:21:19 GMT"}, {"version": "v4", "created": "Wed, 28 Feb 2018 15:53:16 GMT"}, {"version": "v5", "created": "Mon, 22 Jul 2019 13:23:43 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["O'Hagan", "Adrian", ""], ["Murphy", "Thomas Brendan", ""], ["Scrucca", "Luca", ""], ["Gormley", "Isobel Claire", ""]]}, {"id": "1510.00563", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Thomas B. Sch\\\"on, Arno Solin and Simo S\\\"arkk\\\"a", "title": "Nonlinear State Space Model Identification Using a Regularized Basis\n  Function Expansion", "comments": "Accepted to the 6th IEEE international workshop on computational\n  advances in multi-sensor adaptive processing (CAMSAP), Cancun, Mexico,\n  December 2015", "journal-ref": null, "doi": "10.1109/CAMSAP.2015.7383841", "report-no": null, "categories": "stat.CO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with black-box identification of nonlinear state\nspace models. By using a basis function expansion within the state space model,\nwe obtain a flexible structure. The model is identified using an expectation\nmaximization approach, where the states and the parameters are updated\niteratively in such a way that a maximum likelihood estimate is obtained. We\nuse recent particle methods with sound theoretical properties to infer the\nstates, whereas the model parameters can be updated using closed-form\nexpressions by exploiting the fact that our model is linear in the parameters.\nNot to over-fit the flexible model to the data, we also propose a\nregularization scheme without increasing the computational burden. Importantly,\nthis opens up for systematic use of regularization in nonlinear state space\nmodels. We conclude by evaluating our proposed approach on one simulation\nexample and two real-data problems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 11:29:58 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Svensson", "Andreas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Solin", "Arno", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1510.00755", "submitter": "Taylor Arnold", "authors": "Taylor Arnold", "title": "Sparse Density Representations for Simultaneous Inference on Large\n  Spatial Datasets", "comments": "9 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large spatial datasets often represent a number of spatial point processes\ngenerated by distinct entities or classes of events. When crossed with\ncovariates, such as discrete time buckets, this can quickly result in a data\nset with millions of individual density estimates. Applications that require\nsimultaneous access to a substantial subset of these estimates become resource\nconstrained when densities are stored in complex and incompatible formats. We\npresent a method for representing spatial densities along the nodes of sparsely\npopulated trees. Fast algorithms are provided for performing set operations and\nqueries on the resulting compact tree structures. The speed and simplicity of\nthe approach is demonstrated on both real and simulated spatial data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 23:05:48 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Arnold", "Taylor", ""]]}, {"id": "1510.00861", "submitter": "Babak Shahbaba", "authors": "Tian Chen, Jeffrey Streets, Babak Shahbaba", "title": "A Geometric View of Posterior Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Bayesian methods are robust and principled, their application in\npractice could be limited since they typically rely on computationally\nintensive Markov Chain Monte Carlo algorithms for their implementation. One\npossible solution is to find a fast approximation of posterior distribution and\nuse it for statistical inference. For commonly used approximation methods, such\nas Laplace and variational free energy, the objective is mainly defined in\nterms of computational convenience as opposed to a true distance measure\nbetween the target and approximating distributions. In this paper, we provide a\ngeometric view of posterior approximation based on a valid distance measure\nderived from ambient Fisher geometry. Our proposed framework is easily\ngeneralizable and can inspire a new class of methods for approximate Bayesian\ninference.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 20:05:58 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Chen", "Tian", ""], ["Streets", "Jeffrey", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1510.00934", "submitter": "Lampros Bouranis", "authors": "Lampros Bouranis, Nial Friel, Florian Maire", "title": "Efficient Bayesian inference for exponential random graph models by\n  correcting the pseudo-posterior distribution", "comments": "Title change from previous version, addtional example section added", "journal-ref": "Soc. Networks 50 (2017) 98-108", "doi": "10.1016/j.socnet.2017.03.013", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential random graph models are an important tool in the statistical\nanalysis of data. However, Bayesian parameter estimation for these models is\nextremely challenging, since evaluation of the posterior distribution typically\ninvolves the calculation of an intractable normalizing constant. This barrier\nmotivates the consideration of tractable approximations to the likelihood\nfunction, such as the pseudolikelihood function, which offers an approach to\nconstructing such an approximation. Naive implementation of what we term a\npseudo-posterior resulting from replacing the likelihood function in the\nposterior distribution by the pseudolikelihood is likely to give misleading\ninferences. We provide practical guidelines to correct a sample from such a\npseudo-posterior distribution so that it is approximately distributed from the\ntarget posterior distribution and discuss the computational and statistical\nefficiency that result from this approach. We illustrate our methodology\nthrough the analysis of real-world graphs. Comparisons against the approximate\nexchange algorithm of Caimo and Friel (2011) are provided, followed by\nconcluding remarks.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 12:53:12 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2016 12:04:58 GMT"}, {"version": "v3", "created": "Thu, 4 May 2017 09:13:28 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Bouranis", "Lampros", ""], ["Friel", "Nial", ""], ["Maire", "Florian", ""]]}, {"id": "1510.00967", "submitter": "Thibaut Horel", "authors": "Panos Toulis, Thibaut Horel, Edoardo M. Airoldi", "title": "The Proximal Robbins-Monro Method", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for parameter estimation with massive datasets has reinvigorated\ninterest in stochastic optimization and iterative estimation procedures.\nStochastic approximations are at the forefront of this recent development as\nthey yield procedures that are simple, general, and fast. However, standard\nstochastic approximations are often numerically unstable. Deterministic\noptimization, on the other hand, increasingly uses proximal updates to achieve\nnumerical stability in a principled manner. A theoretical gap has thus emerged.\nWhile standard stochastic approximations are subsumed by the framework of\nRobbins and Monro (1951), there is no such framework for stochastic\napproximations with proximal updates. In this paper, we conceptualize a\nproximal version of the classical Robbins-Monro procedure. Our theoretical\nanalysis demonstrates that the proposed procedure has important stability\nbenefits over the classical Robbins-Monro procedure, while it retains the best\nknown convergence rates. Exact implementations of the proximal Robbins-Monro\nprocedure are challenging, but we show that approximate implementations lead to\nprocedures that are easy to implement, and still dominate classical procedures\nby achieving numerical stability, practically without tradeoffs. Moreover,\napproximate proximal Robbins-Monro procedures can be applied even when the\nobjective cannot be calculated analytically, and so they generalize stochastic\nproximal procedures currently in use.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 19:07:41 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 00:37:35 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 03:01:41 GMT"}, {"version": "v4", "created": "Sat, 1 Feb 2020 17:50:22 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Toulis", "Panos", ""], ["Horel", "Thibaut", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1510.01188", "submitter": "Alexander Ly", "authors": "Alexander Ly, Maarten Marsman, and Eric-Jan Wagenmakers", "title": "Analytic Posteriors for Pearson's Correlation Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pearson's correlation is one of the most common measures of linear\ndependence. Recently, Bernardo (2015) introduced a flexible class of priors to\nstudy this measure in a Bayesian setting. For this large class of priors we\nshow that the (marginal) posterior for Pearson's correlation coefficient and\nall of the posterior moments are analytic. Our results are available in the\nopen-source software package JASP.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 15:36:58 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 14:05:30 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Ly", "Alexander", ""], ["Marsman", "Maarten", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1510.02175", "submitter": "Bai Jiang", "authors": "Bai Jiang, Tung-yu Wu, Charles Zheng, Wing H. Wong", "title": "Learning Summary Statistic for Approximate Bayesian Computation via Deep\n  Neural Network", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": "10.5705/ss.202015.0340", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) methods are used to approximate\nposterior distributions in models with unknown or computationally intractable\nlikelihoods. Both the accuracy and computational efficiency of ABC depend on\nthe choice of summary statistic, but outside of special cases where the optimal\nsummary statistics are known, it is unclear which guiding principles can be\nused to construct effective summary statistics. In this paper we explore the\npossibility of automating the process of constructing summary statistics by\ntraining deep neural networks to predict the parameters from artificially\ngenerated data: the resulting summary statistics are approximately posterior\nmeans of the parameters. With minimal model-specific tuning, our method\nconstructs summary statistics for the Ising model and the moving-average model,\nwhich match or exceed theoretically-motivated summary statistics in terms of\nthe accuracies of the resulting posteriors.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 00:33:51 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 21:20:53 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 04:47:20 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Jiang", "Bai", ""], ["Wu", "Tung-yu", ""], ["Zheng", "Charles", ""], ["Wong", "Wing H.", ""]]}, {"id": "1510.02425", "submitter": "Vahed Maroufy", "authors": "Vahed Maroufy and Paul Marriott", "title": "Generalizing the Frailty Assumptions in Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies Cox's regression hazard model with an unobservable random\nfrailty where no specific distribution is postulated for the frailty variable,\nand the marginal lifetime distribution allows both parametric and\nnon-parametric models. Laplace's approximation method and gradient search on\nsmooth manifolds embedded in Euclidean space are applied, and a non-iterative\nprofile likelihood optimization method is proposed for estimating the\nregression coefficients. The proposed method is compared with the\nExpected-Maximization method developed based on a gamma frailty assumption, and\nalso in the case when the frailty model is misspecified.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 17:58:51 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Maroufy", "Vahed", ""], ["Marriott", "Paul", ""]]}, {"id": "1510.02577", "submitter": "Alexandre Thiery", "authors": "Alexandros Beskos, Gareth Roberts, Alexandre Thiery and Natesh Pillai", "title": "Asymptotic Analysis of the Random-Walk Metropolis Algorithm on Ridged\n  Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the asymptotic behavior of the Random-Walk Metropolis\nalgorithm on probability densities with two different `scales', where most of\nthe probability mass is distributed along certain key directions with the\n`orthogonal' directions containing relatively less mass. Such class of\nprobability measures arise in various applied contexts including Bayesian\ninverse problems where the posterior measure concentrates on a sub-manifold\nwhen the noise variance goes to zero. When the target measure concentrates on a\nlinear sub-manifold, we derive analytically a diffusion limit for the\nRandom-Walk Metropolis Markov chain as the scale parameter goes to zero. In\ncontrast to the existing works on scaling limits, our limiting Stochastic\nDifferential Equation does not in general have a constant diffusion\ncoefficient. Our results show that in some cases, the usual practice of\nadapting the step-size to control the acceptance probability might be\nsub-optimal as the optimal acceptance probability is zero (in the limit).\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 06:55:25 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Beskos", "Alexandros", ""], ["Roberts", "Gareth", ""], ["Thiery", "Alexandre", ""], ["Pillai", "Natesh", ""]]}, {"id": "1510.02604", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth, Paul Fearnhead, Lyudmila Mihaylova", "title": "Sequential Monte Carlo Methods for State and Parameter Estimation in\n  Abruptly Changing Environments", "comments": "26 pages, 5 figures", "journal-ref": "IEEE Transactions on Signal Processing (2014), Volume:62, Issue: 5", "doi": "10.1109/TSP.2013.2296278", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a novel sequential Monte Carlo (SMC) approach for joint\nstate and parameter estimation that can deal efficiently with abruptly changing\nparameters which is a common case when tracking maneuvering targets. The\napproach combines Bayesian methods for dealing with changepoints with methods\nfor estimating static parameters within the SMC framework. The result is an\napproach which adaptively estimates the model parameters in accordance with\nchanges to the target's trajectory. The developed approach is compared against\nthe Interacting Multiple Model (IMM) filter for tracking a maneuvering target\nover a complex maneuvering scenario with nonlinear observations. In the IMM\nfilter a large combination of models is required to account for unknown\nparameters. In contrast, the proposed approach circumvents the combinatorial\ncomplexity of applying multiple models in the IMM filter through Bayesian\nparameter estimation techniques. The developed approach is validated over\ncomplex maneuvering scenarios where both the system parameters and measurement\nnoise parameters are unknown. Accurate estimation results are presented.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 09:46:38 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Nemeth", "Christopher", ""], ["Fearnhead", "Paul", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1510.02934", "submitter": "Cheng Koay", "authors": "Cheng Guan Koay, Ping-Hong Yeh, John M. Ollinger, M. Okan\n  \\.Irfano\\u{g}lu, Carlo Pierpaoli, Peter J. Basser, Terrence R. Oakes, Gerard\n  Riedy", "title": "Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A\n  framework for single-subject analysis in diffusion tensor imaging", "comments": "49 pages, 6 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.neuroimage.2015.11.046", "report-no": null, "categories": "physics.med-ph cs.CV stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is to develop a framework for single-subject\nanalysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI)\nis capable of testing whether an individual tract as represented by the major\neigenvector of the diffusion tensor and its corresponding angular dispersion\nare significantly different from a group of tracts on a voxel-by-voxel basis.\nThis work develops two complementary statistical tests based on the elliptical\ncone of uncertainty (COU), which is a model of uncertainty or dispersion of the\nmajor eigenvector of the diffusion tensor. The orientation deviation test\nexamines whether the major eigenvector from a single subject is within the\naverage elliptical COU formed by a collection of elliptical COUs. The shape\ndeviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample test\nbetween the normalized shape measures (area and circumference) of the\nelliptical cones of uncertainty of the single subject against a group of\ncontrols. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR)\nwere incorporated in the orientation deviation test. The shape deviation test\nuses FDR only. TOADDI was found to be numerically accurate and statistically\neffective. Clinical data from two Traumatic Brain Injury (TBI) patients and one\nnon-TBI subject were tested against the data obtained from a group of 45\nnon-TBI controls to illustrate the application of the proposed framework in\nsingle-subject analysis. The frontal portion of the superior longitudinal\nfasciculus seemed to be implicated in both tests as significantly different\nfrom that of the control group. The TBI patients and the single non-TBI subject\nwere well separated under the shape deviation test at the chosen FDR level of\n0.0005. TOADDI is a simple but novel geometrically based statistical framework\nfor analyzing DTI data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 13:54:07 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 01:35:14 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Koay", "Cheng Guan", ""], ["Yeh", "Ping-Hong", ""], ["Ollinger", "John M.", ""], ["\u0130rfano\u011flu", "M. Okan", ""], ["Pierpaoli", "Carlo", ""], ["Basser", "Peter J.", ""], ["Oakes", "Terrence R.", ""], ["Riedy", "Gerard", ""]]}, {"id": "1510.02955", "submitter": "Cesar Manchein", "authors": "Rafael M. da Silva, Marcus W. Beims, Cesar Manchein", "title": "Recurrence-time statistics in non-Hamiltonian volume preserving maps and\n  flows", "comments": "10 pages and 8 figures", "journal-ref": "Phys. Rev. E 92, 022921 (2015)", "doi": "10.1103/PhysRevE.92.022921", "report-no": null, "categories": "nlin.CD physics.flu-dyn stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the recurrence-time statistics (RTS) in three-dimensional\nnon-Hamiltonian volume preserving systems (VPS): an extended standard map, and\na fluid model. The extended map is a standard map weakly coupled to an\nextra-dimension which contains a deterministic regular, mixed (regular and\nchaotic) or chaotic motion. The extra-dimension strongly enhances the trapping\ntimes inducing plateaus and distinct algebraic and exponential decays in the\nRTS plots. The combined analysis of the RTS with the classification of ordered\nand chaotic regimes and scaling properties, allows us to describe the intricate\nway trajectories penetrate the before impenetrable regular islands from the\nuncoupled case. Essentially the plateaus found in the RTS are related to\ntrajectories that stay long times inside trapping tubes, not allowing\nrecurrences, and then penetrates diffusively the islands (from the uncoupled\ncase) by a diffusive motion along such tubes in the extra-dimension. All\nasymptotic exponential decays for the RTS are related to an ordered regime\n(quasi-regular motion) and a mixing dynamics is conjectured for the model.\nThese results are compared to the RTS of the standard map with dissipation or\nnoise, showing the peculiarities obtained by using three-dimensional VPS. We\nalso analyze the RTS for a fluid model and show remarkable similarities to the\nRTS in the extended standard map problem.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 16:10:40 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["da Silva", "Rafael M.", ""], ["Beims", "Marcus W.", ""], ["Manchein", "Cesar", ""]]}, {"id": "1510.02958", "submitter": "Iain Murray", "authors": "Iain Murray and Matthew M. Graham", "title": "Pseudo-Marginal Slice Sampling", "comments": "9 pages, 6 figures, 1 table. Version 2 includes citations to\n  closely-related work released on arXiv since version 1", "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics, JMLR W&CP, Volume 51, pp. 911-919, 2016", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods asymptotically sample from complex\nprobability distributions. The pseudo-marginal MCMC framework only requires an\nunbiased estimator of the unnormalized probability distribution function to\nconstruct a Markov chain. However, the resulting chains are harder to tune to a\ntarget distribution than conventional MCMC, and the types of updates available\nare limited. We describe a general way to clamp and update the random numbers\nused in a pseudo-marginal method's unbiased estimator. In this framework we can\nuse slice sampling and other adaptive methods. We obtain more robust Markov\nchains, which often mix more quickly.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 16:26:36 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 16:40:53 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Murray", "Iain", ""], ["Graham", "Matthew M.", ""]]}, {"id": "1510.03105", "submitter": "Ingmar Schuster", "authors": "Ingmar Schuster, Heiko Strathmann, Brooks Paige and Dino Sejdinovic", "title": "Kernel Sequential Monte Carlo", "comments": "ECML-PKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose kernel sequential Monte Carlo (KSMC), a framework for sampling\nfrom static target densities. KSMC is a family of sequential Monte Carlo\nalgorithms that are based on building emulator models of the current particle\nsystem in a reproducing kernel Hilbert space. We here focus on modelling\nnonlinear covariance structure and gradients of the target. The emulator's\ngeometry is adaptively updated and subsequently used to inform local proposals.\nUnlike in adaptive Markov chain Monte Carlo, continuous adaptation does not\ncompromise convergence of the sampler. KSMC combines the strengths of sequental\nMonte Carlo and kernel methods: superior performance for multimodal targets and\nthe ability to estimate model evidence as compared to Markov chain Monte Carlo,\nand the emulator's ability to represent targets that exhibit high degrees of\nnonlinearity. As KSMC does not require access to target gradients, it is\nparticularly applicable on targets whose gradients are unknown or prohibitively\nexpensive. We describe necessary tuning details and demonstrate the benefits of\nthe the proposed methodology on a series of challenging synthetic and\nreal-world examples.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 21:28:48 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 20:22:25 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2016 20:07:52 GMT"}, {"version": "v4", "created": "Tue, 25 Jul 2017 12:37:43 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Schuster", "Ingmar", ""], ["Strathmann", "Heiko", ""], ["Paige", "Brooks", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1510.03298", "submitter": "Adam Lund", "authors": "Adam Lund, Martin Vincent, Niels Richard Hansen", "title": "Penalized estimation in large-scale generalized linear array models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale generalized linear array models (GLAMs) can be challenging to\nfit. Computation and storage of its tensor product design matrix can be\nimpossible due to time and memory constraints, and previously considered design\nmatrix free algorithms do not scale well with the dimension of the parameter\nvector. A new design matrix free algorithm is proposed for computing the\npenalized maximum likelihood estimate for GLAMs, which, in particular, handles\nnondifferentiable penalty functions. The proposed algorithm is implemented and\navailable via the R package \\verb+glamlasso+. It combines several ideas --\npreviously considered separately -- to obtain sparse estimates while at the\nsame time efficiently exploiting the GLAM structure. In this paper the\nconvergence of the algorithm is treated and the performance of its\nimplementation is investigated and compared to that of \\verb+glmnet+ on\nsimulated as well as real data. It is shown that the computation time for\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 14:26:36 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 09:54:30 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 11:19:08 GMT"}, {"version": "v4", "created": "Fri, 2 Sep 2016 06:08:40 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Lund", "Adam", ""], ["Vincent", "Martin", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "1510.04163", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Chong Wang, Eric Xing", "title": "Embarrassingly Parallel Variational Inference in Nonconjugate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a parallel variational inference (VI) procedure for use in\ndata-distributed settings, where each machine only has access to a subset of\ndata and runs VI independently, without communicating with other machines. This\ntype of \"embarrassingly parallel\" procedure has recently been developed for\nMCMC inference algorithms; however, in many cases it is not possible to\ndirectly extend this procedure to VI methods without requiring certain\nrestrictive exponential family conditions on the form of the model.\nFurthermore, most existing (nonparallel) VI methods are restricted to use on\nconditionally conjugate models, which limits their applicability. To combat\nthese issues, we make use of the recently proposed nonparametric VI to\nfacilitate an embarrassingly parallel VI procedure that can be applied to a\nwider scope of models, including to nonconjugate models. We derive our\nembarrassingly parallel VI algorithm, analyze our method theoretically, and\ndemonstrate our method empirically on a few nonconjugate models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 15:48:19 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wang", "Chong", ""], ["Xing", "Eric", ""]]}, {"id": "1510.04923", "submitter": "Xiangrui Meng", "authors": "Xiangrui Meng", "title": "Simpler Online Updates for Arbitrary-Order Central Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical moments are widely used in descriptive statistics. Therefore\nefficient and numerically stable implementations are important in practice.\nPebay [1] derives online update formulas for arbitrary-order central moments.\nWe present a simpler version that is also easier to implement.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 15:42:25 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Meng", "Xiangrui", ""]]}, {"id": "1510.04977", "submitter": "Kody Law", "authors": "Ajay Jasra, Kengo Kamatani, Kody J. H. Law, Yan Zhou", "title": "Multilevel particle filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the filtering of partially observed diffusions, with\ndiscrete-time observations, is considered. It is assumed that only biased\napproximations of the diffusion can be obtained, for choice of an accuracy\nparameter indexed by $l$. A multilevel estimator is proposed, consisting of a\ntelescopic sum of increment estimators associated to the successive levels. The\nwork associated to $\\mathcal{O}(\\varepsilon^2)$ mean-square error between the\nmultilevel estimator and average with respect to the filtering distribution is\nshown to scale optimally, for example as $\\mathcal{O}(\\varepsilon^{-2})$ for\noptimal rates of convergence of the underlying diffusion approximation. The\nmethod is illustrated on some toy examples as well as estimation of interest\nrate based on real S&P 500 stock price data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 18:48:10 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Jasra", "Ajay", ""], ["Kamatani", "Kengo", ""], ["Law", "Kody J. H.", ""], ["Zhou", "Yan", ""]]}, {"id": "1510.05532", "submitter": "Michael Beard", "authors": "Michael Beard, Ba-Tuong Vo, Ba-Ngu Vo, Sanjeev Arulampalam", "title": "Void Probabilities and Cauchy-Schwarz Divergence for Generalized Labeled\n  Multi-Bernoulli Models", "comments": "13 pages, 11 figures, submitted to IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized labeled multi-Bernoulli (GLMB) is a family of tractable\nmodels that alleviates the limitations of the Poisson family in dynamic\nBayesian inference of point processes. In this paper, we derive closed form\nexpressions for the void probability functional and the Cauchy-Schwarz\ndivergence for GLMBs. The proposed analytic void probability functional is a\nnecessary and sufficient statistic that uniquely characterizes a GLMB, while\nthe proposed analytic Cauchy-Schwarz divergence provides a tractable measure of\nsimilarity between GLMBs. We demonstrate the use of both results on a partially\nobserved Markov decision process for GLMBs, with Cauchy-Schwarz divergence\nbased reward, and void probability constraint.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 15:31:51 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Beard", "Michael", ""], ["Vo", "Ba-Tuong", ""], ["Vo", "Ba-Ngu", ""], ["Arulampalam", "Sanjeev", ""]]}, {"id": "1510.06053", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, Youssef M. Marzouk, Karen E. Willcox", "title": "Scalable posterior approximations for large-scale Bayesian inverse\n  problems via likelihood-informed parameter and state reduction", "comments": "35 pages, 12 figures", "journal-ref": "Journal of Computational Physics, Volume 315, 15 June 2016, Pages\n  363-387", "doi": "10.1016/j.jcp.2016.03.055", "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two major bottlenecks to the solution of large-scale Bayesian inverse\nproblems are the scaling of posterior sampling algorithms to high-dimensional\nparameter spaces and the computational cost of forward model evaluations. Yet\nincomplete or noisy data, the state variation and parameter dependence of the\nforward model, and correlations in the prior collectively provide useful\nstructure that can be exploited for dimension reduction in this setting--both\nin the parameter space of the inverse problem and in the state space of the\nforward model. To this end, we show how to jointly construct low-dimensional\nsubspaces of the parameter space and the state space in order to accelerate the\nBayesian solution of the inverse problem. As a byproduct of state dimension\nreduction, we also show how to identify low-dimensional subspaces of the data\nin problems with high-dimensional observations. These subspaces enable\napproximation of the posterior as a product of two factors: (i) a projection of\nthe posterior onto a low-dimensional parameter subspace, wherein the original\nlikelihood is replaced by an approximation involving a reduced model; and (ii)\nthe marginal prior distribution on the high-dimensional complement of the\nparameter subspace. We present and compare several strategies for constructing\nthese subspaces using only a limited number of forward and adjoint model\nsimulations. The resulting posterior approximations can rapidly be\ncharacterized using standard sampling techniques, e.g., Markov chain Monte\nCarlo. Two numerical examples demonstrate the accuracy and efficiency of our\napproach: inversion of an integral equation in atmospheric remote sensing,\nwhere the data dimension is very high; and the inference of a heterogeneous\ntransmissivity field in a groundwater system, which involves a partial\ndifferential equation forward model with high dimensional state and parameters.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 20:45:41 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 01:42:29 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cui", "Tiangang", ""], ["Marzouk", "Youssef M.", ""], ["Willcox", "Karen E.", ""]]}, {"id": "1510.06772", "submitter": "John Nolan", "authors": "John P Nolan", "title": "Models for generalized spherical and related distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A flexible model is developed for multivariate generalized spherical\ndistributions, i.e. ones with level sets that are star shaped. To work in\ndimension above 2 requires tools from computational geometry and multivariate\nnumerical integration. In order to simulate from these star shaped contours, an\nalgorithm to simulate from general tessellations has been developed that has\napplications in other situations. These techniques are implemented in an R\npackage gensphere.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 21:27:38 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Nolan", "John P", ""]]}, {"id": "1510.06989", "submitter": "Alfredo Garbuno-Inigo", "authors": "F.A. DiazDelaO, A. Garbuno-Inigo, S.K. Au, I. Yoshida", "title": "Bayesian updating and model class selection with Subset Simulation", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2017.01.006", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the parameters of a model and rating competitive models based on\nmeasured data has been among the most important but challenging topics in\nmodern science and engineering, with great potential of application in\nstructural system identification, updating and development of high fidelity\nmodels. These problems in principle can be tackled using a Bayesian\nprobabilistic approach, where the parameters to be identified are treated as\nuncertain and their inference information are given in terms of their posterior\n(i.e., given data) probability distribution. For complex models encountered in\napplications, efficient computational tools robust to the number of uncertain\nparameters in the problem are required for computing the posterior statistics,\nwhich can generally be formulated as a multi-dimensional integral over the\nspace of the uncertain parameters. Subset Simulation (SuS) has been developed\nfor solving reliability problems involving complex systems and it is found to\nbe robust to the number of uncertain parameters. An analogy has been recently\nestablished between a Bayesian updating problem and a reliability problem,\nwhich opens up the possibility of efficient solution by SuS. The formulation,\ncalled BUS (Bayesian Updating with Structural reliability methods), is based on\nconventional rejection principle. Its theoretical correctness and efficiency\nrequires the prudent choice of a multiplier, which has remained an open\nquestion. Motivated by the choice of the multiplier and its philosophical role,\nthis paper presents a study of BUS. The work leads to a revised formulation\nthat resolves the issues regarding the multiplier so that SuS can be\nimplemented without knowing the multiplier. Examples are presented to\nillustrate the theory and applications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 16:52:52 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 12:24:02 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 14:29:10 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["DiazDelaO", "F. A.", ""], ["Garbuno-Inigo", "A.", ""], ["Au", "S. K.", ""], ["Yoshida", "I.", ""]]}, {"id": "1510.07727", "submitter": "Art Owen", "authors": "Art B. Owen", "title": "Statistically efficient thinning of a Markov chain sampler", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to subsample Markov chain output to reduce the storage burden.\nGeyer (1992) shows that discarding $k-1$ out of every $k$ observations will not\nimprove statistical efficiency, as quantified through variance in a given\ncomputational budget. That observation is often taken to mean that thinning\nMCMC output cannot improve statistical efficiency. Here we suppose that it\ncosts one unit of time to advance a Markov chain and then $\\theta>0$ units of\ntime to compute a sampled quantity of interest. For a thinned process, that\ncost $\\theta$ is incurred less often, so it can be advanced through more\nstages. Here we provide examples to show that thinning will improve statistical\nefficiency if $\\theta$ is large and the sample autocorrelations decay slowly\nenough. If the lag $\\ell\\ge1$ autocorrelations of a scalar measurement satisfy\n$\\rho_\\ell\\ge\\rho_{\\ell+1}\\ge0$, then there is always a $\\theta<\\infty$ at\nwhich thinning becomes more efficient for averages of that scalar. Many sample\nautocorrelation functions resemble first order AR(1) processes with $\\rho_\\ell\n=\\rho^{|\\ell|}$ for some $-1<\\rho<1$. For an AR(1) process it is possible to\ncompute the most efficient subsampling frequency $k$. The optimal $k$ grows\nrapidly as $\\rho$ increases towards $1$. The resulting efficiency gain depends\nprimarily on $\\theta$, not $\\rho$. Taking $k=1$ (no thinning) is optimal when\n$\\rho\\le0$. For $\\rho>0$ it is optimal if and only if $\\theta \\le\n(1-\\rho)^2/(2\\rho)$. This efficiency gain never exceeds $1+\\theta$. This paper\nalso gives efficiency bounds for autocorrelations bounded between those of two\nAR(1) processes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 00:00:54 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 21:55:29 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2015 22:06:58 GMT"}, {"version": "v4", "created": "Sun, 31 Jan 2016 23:47:55 GMT"}, {"version": "v5", "created": "Wed, 21 Sep 2016 01:03:26 GMT"}, {"version": "v6", "created": "Tue, 14 Mar 2017 22:51:02 GMT"}, {"version": "v7", "created": "Tue, 11 Apr 2017 02:23:00 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Owen", "Art B.", ""]]}]