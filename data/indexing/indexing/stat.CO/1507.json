[{"id": "1507.00181", "submitter": "Belinda Hernandez", "authors": "Belinda Hern\\'andez, Adrian E. Raftery, Stephen R. Pennington, Andrew\n  C. Parnell", "title": "Bayesian Additive Regression Trees using Bayesian Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Additive Regression Trees (BART) is a statistical sum of trees\nmodel. It can be considered a Bayesian version of machine learning tree\nensemble methods where the individual trees are the base learners. However for\ndata sets where the number of variables $p$ is large (e.g. $p>5,000$) the\nalgorithm can become prohibitively expensive, computationally.\n  Another method which is popular for high dimensional data is random forests,\na machine learning algorithm which grows trees using a greedy search for the\nbest split points. However, as it is not a statistical model, it cannot produce\nprobabilistic estimates or predictions.\n  We propose an alternative algorithm for BART called BART-BMA, which uses\nBayesian Model Averaging and a greedy search algorithm to produce a model which\nis much more efficient than BART for datasets with large $p$. BART-BMA\nincorporates elements of both BART and random forests to offer a model-based\nalgorithm which can deal with high-dimensional data.\n  We have found that BART-BMA can be run in a reasonable time on a standard\nlaptop for the \"small $n$ large $p$\" scenario which is common in many areas of\nbioinformatics. We showcase this method using simulated data and data from two\nreal proteomic experiments; one to distinguish between patients with\ncardiovascular disease and controls and another to classify agressive from\nnon-agressive prostate cancer. We compare our results to their main\ncompetitors.\n  Open source code written in R and Rcpp to run BART-BMA can be found at:\nhttps://github.com/BelindaHernandez/BART-BMA.git\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 10:58:46 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 14:08:27 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Hern\u00e1ndez", "Belinda", ""], ["Raftery", "Adrian E.", ""], ["Pennington", "Stephen R.", ""], ["Parnell", "Andrew C.", ""]]}, {"id": "1507.00398", "submitter": "Damon McDougall", "authors": "Damon McDougall, Nicholas Malaya, Robert D. Moser", "title": "The Parallel C++ Statistical Library for Bayesian Inference: QUESO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Parallel C++ Statistical Library for the Quantification of Uncertainty\nfor Estimation, Simulation and Optimization, Queso, is a collection of\nstatistical algorithms and programming constructs supporting research into the\nquantification of uncertainty of models and their predictions. Queso is\nprimarily focused on solving statistical inverse problems using Bayes's\ntheorem, which expresses a distribution of possible values for a set of\nuncertain parameters (the posterior distribution) in terms of the existing\nknowledge of the system (the prior) and noisy observations of a physical\nprocess, represented by a likelihood distribution. The posterior distribution\nis not often known analytically, and so requires computational methods. It is\ntypical to compute probabilities and moments from the posterior distribution,\nbut this is often a high-dimensional object and standard Reimann-type methods\nfor quadrature become prohibitively expensive. The approach Queso takes in this\nregard is to rely on Markov chain Monte Carlo (MCMC) methods which are well\nsuited to evaluating quantities such as probabilities and moments of\nhigh-dimensional probability distributions. Queso's intended use is as tool to\nassist and facilitate coupling uncertainty quantification to a specific\napplication called a forward problem. While many libraries presently exist that\nsolve Bayesian inference problems, Queso is a specialized piece of software\nprimarily designed to solve such problems by utilizing parallel environments\ndemanded by large-scale forward problems. Queso is written in C++, uses MPI,\nand utilizes libraries already available to the scientific community.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 00:19:57 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["McDougall", "Damon", ""], ["Malaya", "Nicholas", ""], ["Moser", "Robert D.", ""]]}, {"id": "1507.00843", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "Optimal linear Bernoulli factories for small mean problems", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose a coin with unknown probability $p$ of heads can be flipped as often\nas desired. A Bernoulli factory for a function $f$ is an algorithm that uses\nflips of the coin together with auxiliary randomness to flip a single coin with\nprobability $f(p)$ of heads. Applications include near perfect sampling from\nthe stationary distribution of regenerative processes. When $f$ is analytic,\nthe problem can be reduced to a Bernoulli factory of the form $f(p) = Cp$ for\nconstant $C$. Presented here is a new algorithm where for small values of $Cp$,\nrequires roughly only $C$ coin flips to generate a $Cp$ coin. From information\ntheory considerations, this is also conjectured to be (to first order) the\nminimum number of flips needed by any such algorithm.\n  For $Cp$ large, the new algorithm can also be used to build a new Bernoulli\nfactory that uses only 80\\% of the expected coin flips of the older method, and\napplies to the more general problem of a multivariate Bernoulli factory, where\nthere are $k$ coins, the $k$th coin has unknown probability $p_k$ of heads, and\nthe goal is to simulate a coin flip with probability $C_1 p_1 + \\cdots + C_k\np_k$ of heads.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 07:56:55 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 20:56:56 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "1507.00874", "submitter": "Dennis Prangle", "authors": "Dennis Prangle", "title": "Adapting the ABC distance function", "comments": "Revised based on referee reports, including addition of a new method\n  (Algorithm 4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation performs approximate inference for models\nwhere likelihood computations are expensive or impossible. Instead simulations\nfrom the model are performed for various parameter values and accepted if they\nare close enough to the observations. There has been much progress on deciding\nwhich summary statistics of the data should be used to judge closeness, but\nless work on how to weight them. Typically weights are chosen at the start of\nthe algorithm which normalise the summary statistics to vary on similar scales.\nHowever these may not be appropriate in iterative ABC algorithms, where the\ndistribution from which the parameters are proposed is updated. This can\nsubstantially alter the resulting distribution of summary statistics, so that\ndifferent weights are needed for normalisation. This paper presents two\niterative ABC algorithms which adaptively update their weights and demonstrates\nimproved results on test applications.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 11:11:39 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 08:26:59 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 11:01:08 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Prangle", "Dennis", ""]]}, {"id": "1507.00919", "submitter": "Cl\\'ement Walter", "authors": "Cl\\'ement Walter", "title": "Rare Event Simulation and Splitting for Discontinuous Random Variables", "comments": "16 pages (12 + Appendix 4 pages), 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel Splitting methods, also called Sequential Monte-Carlo or\n\\emph{Subset Simulation}, are widely used methods for estimating extreme\nprobabilities of the form $P[S(\\mathbf{U}) > q]$ where $S$ is a deterministic\nreal-valued function and $\\mathbf{U}$ can be a random finite- or\ninfinite-dimensional vector. Very often, $X := S(\\mathbf{U})$ is supposed to be\na continuous random variable and a lot of theoretical results on the\nstatistical behaviour of the estimator are now derived with this hypothesis.\nHowever, as soon as some threshold effect appears in $S$ and/or $\\mathbf{U}$ is\ndiscrete or mixed discrete/continuous this assumption does not hold any more\nand the estimator is not consistent.\n  In this paper, we study the impact of discontinuities in the \\emph{cdf} of\n$X$ and present three unbiased \\emph{corrected} estimators to handle them.\nThese estimators do not require to know in advance if $X$ is actually\ndiscontinuous or not and become all equal if $X$ is continuous. Especially, one\nof them has the same statistical properties in any case. Efficiency is shown on\na 2-D diffusive process as well as on the \\emph{Boolean SATisfiability problem}\n(SAT).\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 14:19:31 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Walter", "Cl\u00e9ment", ""]]}, {"id": "1507.00964", "submitter": "Omri Har Shemesh Mr.", "authors": "Omri Har Shemesh, Rick Quax, Borja Mi\\~nano, Alfons G. Hoekstra, Peter\n  M. A. Sloot", "title": "Non-parametric estimation of Fisher information from real data", "comments": "6 pages, 5 figures", "journal-ref": "Phys. Rev. E 93, 023301 (2016)", "doi": "10.1103/PhysRevE.93.023301", "report-no": null, "categories": "stat.CO physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher Information matrix is a widely used measure for applications\nranging from statistical inference, information geometry, experiment design, to\nthe study of criticality in biological systems. Yet there is no commonly\naccepted non-parametric algorithm to estimate it from real data. In this rapid\ncommunication we show how to accurately estimate the Fisher information in a\nnonparametric way. We also develop a numerical procedure to minimize the errors\nby choosing the interval of the finite difference scheme necessary to compute\nthe derivatives in the definition of the Fisher information. Our method uses\nthe recently published \"Density Estimation using Field Theory\" algorithm to\ncompute the probability density functions for continuous densities. We use the\nFisher information of the normal distribution to validate our method and as an\nexample we compute the temperature component of the Fisher Information Matrix\nin the two dimensional Ising model and show that it obeys the expected relation\nto the heat capacity and therefore peaks at the phase transition at the correct\ncritical temperature.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 16:29:17 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 14:33:09 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Shemesh", "Omri Har", ""], ["Quax", "Rick", ""], ["Mi\u00f1ano", "Borja", ""], ["Hoekstra", "Alfons G.", ""], ["Sloot", "Peter M. A.", ""]]}, {"id": "1507.01044", "submitter": "Pasquale Erto", "authors": "Pasquale Erto", "title": "A first look at the performances of a Bayesian chart to monitor the\n  ratio of two Weibull percentiles", "comments": "9 pages, 3 figures, 3 tables. Invited talk at the 4th International\n  Symposium on Statistical Process Monitoring (http://isspm2015.stat.unipd.it),\n  July 7-9, 2015, Padua, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the present work is to investigate the performances of a specific\nBayesian control chart used to compare two processes. The chart monitors the\nratio of the percentiles of a key characteristic associated with the processes.\nThe variability of such a characteristic is modeled via the Weibull\ndistribution and a practical Bayesian approach to deal with Weibull data is\nadopted. The percentiles of the two monitored processes are assumed to be\nindependent random variables. The Weibull distributions of the key\ncharacteristic of both processes are assumed to have the same and stable shape\nparameter. This is usually experienced in practice because the Weibull shape\nparameter is related to the main involved factor of variability. However, if a\nchange of the shape parameters of the processes is suspected, the involved\ndistributions can be used to monitor their stability. We first tested the\neffects of the number of the training data on the responsiveness of the chart.\nThen we tested the robustness of the chart in spite of very poor prior\ninformation. To this end, the prior values were changed to reflect a 50% shift\nin both directions from the original values of the shape parameter and the\npercentiles of the two monitored processes. Finally, various combinations of\nshifts were considered for the sampling distributions after the Phase I, with\nthe purpose of estimating the diagnostic ability of the charts to signal an\nout-of-control state. The traditional approach based on the Average Run Length,\nempirically computed via a Monte Carlo simulation, was adopted.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 22:28:36 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Erto", "Pasquale", ""]]}, {"id": "1507.01154", "submitter": "R\\'emi Bardenet", "authors": "R\\'emi Bardenet, Michalis K. Titsias", "title": "Inference for determinantal point processes without spectral knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are point process models that naturally\nencode diversity between the points of a given realization, through a positive\ndefinite kernel $K$. DPPs possess desirable properties, such as exact sampling\nor analyticity of the moments, but learning the parameters of kernel $K$\nthrough likelihood-based inference is not straightforward. First, the kernel\nthat appears in the likelihood is not $K$, but another kernel $L$ related to\n$K$ through an often intractable spectral decomposition. This issue is\ntypically bypassed in machine learning by directly parametrizing the kernel\n$L$, at the price of some interpretability of the model parameters. We follow\nthis approach here. Second, the likelihood has an intractable normalizing\nconstant, which takes the form of a large determinant in the case of a DPP over\na finite set of objects, and the form of a Fredholm determinant in the case of\na DPP over a continuous domain. Our main contribution is to derive bounds on\nthe likelihood of a DPP, both for finite and continuous domains. Unlike\nprevious work, our bounds are cheap to evaluate since they do not rely on\napproximating the spectrum of a large matrix or an operator. Through usual\narguments, these bounds thus yield cheap variational inference and moderately\nexpensive exact Markov chain Monte Carlo inference methods for DPPs.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 23:26:27 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Titsias", "Michalis K.", ""]]}, {"id": "1507.01173", "submitter": "Klaus Holst K", "authors": "Klaus K. Holst", "title": "Model Diagnostics Based on Cumulative Residuals: The R-package gof", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized linear model is widely used in all areas of applied\nstatistics and while correct asymptotic inference can be achieved under\nmisspecification of the distributional assumptions, a correctly specified mean\nstructure is crucial to obtain interpretable results. Usually the linearity and\nfunctional form of predictors are checked by inspecting various scatter plots\nof the residuals, however, the subjective task of judging these can be\nchallenging. In this paper we present an implementation of model diagnostics\nfor the generalized linear model as well as structural equation models, based\non aggregates of the residuals where the asymptotic behavior under the null is\nimitated by simulations. A procedure for checking the proportional hazard\nassumption in the Cox regression is also implemented.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 07:14:16 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Holst", "Klaus K.", ""]]}, {"id": "1507.01614", "submitter": "Colin Fox", "authors": "Colin Fox and Richard A. Norton", "title": "Fast sampling in a linear-Gaussian inverse problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the inverse problem of deblurring a pixelized image of Jupiter using\nregularized deconvolution and by sample-based Bayesian inference. By\nefficiently sampling the marginal posterior distribution for hyperparameters,\nthen the full conditional for the deblurred image, we find that we can evaluate\nthe posterior mean faster than regularized inversion, when selection of the\nregularizing parameter is considered. To our knowledge, this is the first\ndemonstration of sampling and inference that takes less compute time than\nregularized inversion in an inverse problems. Comparison to random-walk\nMetropolis-Hastings and block Gibbs MCMC shows that marginal then conditional\nsampling also outperforms these more common sampling algorithms, having better\nscaling with problem size. When problem-specific computations are feasible the\nasymptotic cost of an independent sample is one linear solve, implying that\nsample-based Bayesian inference may be performed directly over function spaces,\nwhen that limit exists.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 20:29:42 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 02:50:34 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Fox", "Colin", ""], ["Norton", "Richard A.", ""]]}, {"id": "1507.02226", "submitter": "Quentin Stout", "authors": "Quentin F. Stout", "title": "L infinity Isotonic Regression for Linear, Multidimensional, and Tree\n  Orders", "comments": "updated references, minor modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms are given for determining $L_\\infty$ isotonic regression of\nweighted data. For a linear order, grid in multidimensional space, or tree, of\n$n$ vertices, optimal algorithms are given, taking $\\Theta(n)$ time. These\nimprove upon previous algorithms by a factor of $\\Omega(\\log n)$. For vertices\nat arbitrary positions in $d$-dimensional space a $\\Theta(n \\log^{d-1} n)$\nalgorithm employs iterative sorting to yield the functionality of a\nmultidimensional structure while using only $\\Theta(n)$ space. The algorithms\nutilize a new non-constructive feasibility test on a rendezvous graph, with\nbounded error envelopes at each vertex.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 17:16:28 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 22:06:37 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Stout", "Quentin F.", ""]]}, {"id": "1507.02646", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, Jonah Gabry", "title": "Pareto Smoothed Importance Sampling", "comments": "Minor revision: fixed some typos and updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance weighting is a general way to adjust Monte Carlo integration to\naccount for draws from the wrong distribution, but the resulting estimate can\nbe noisy when the importance ratios have a heavy right tail. This routinely\noccurs when there are aspects of the target distribution that are not well\ncaptured by the approximating distribution, in which case more stable estimates\ncan be obtained by modifying extreme importance ratios. We present a new method\nfor stabilizing importance weights using a generalized Pareto distribution fit\nto the upper tail of the distribution of the simulated importance ratios. The\nmethod, which empirically performs better than existing methods for stabilizing\nimportance sampling estimates, includes stabilized effective sample size\nestimates, Monte Carlo error estimates and convergence diagnostics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 18:43:28 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 18:37:17 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 11:30:37 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 08:34:49 GMT"}, {"version": "v5", "created": "Sat, 21 Oct 2017 08:37:46 GMT"}, {"version": "v6", "created": "Tue, 2 Jul 2019 13:56:16 GMT"}, {"version": "v7", "created": "Tue, 23 Feb 2021 10:07:05 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Vehtari", "Aki", ""], ["Simpson", "Daniel", ""], ["Gelman", "Andrew", ""], ["Yao", "Yuling", ""], ["Gabry", "Jonah", ""]]}, {"id": "1507.02971", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Mattias Villani and Robert Kohn", "title": "Scalable MCMC for Large Data Problems using Data Subsampling and the\n  Difference Estimator", "comments": "The content in this paper is now in arXiv:1404.4178, as a result of a\n  major revision of that paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic Markov Chain Monte Carlo (MCMC) algorithm to speed up\ncomputations for datasets with many observations. A key feature of our approach\nis the use of the highly efficient difference estimator from the survey\nsampling literature to estimate the log-likelihood accurately using only a\nsmall fraction of the data. Our algorithm improves on the $O(n)$ complexity of\nregular MCMC by operating over local data clusters instead of the full sample\nwhen computing the likelihood. The likelihood estimate is used in a\nPseudo-marginal framework to sample from a perturbed posterior which is within\n$O(m^{-1/2})$ of the true posterior, where $m$ is the subsample size. The\nmethod is applied to a logistic regression model to predict firm bankruptcy for\na large data set. We document a significant speed up in comparison to the\nstandard MCMC on the full dataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 17:10:33 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 11:49:19 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 00:33:01 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Quiroz", "Matias", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""]]}, {"id": "1507.03047", "submitter": "Chris Oates", "authors": "Chris. J. Oates", "title": "Accelerated Nonparametrics for Cascades of Poisson Processes", "comments": "To appear in Stat", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascades of Poisson processes are probabilistic models for spatio-temporal\nphenomena in which (i) previous events may trigger subsequent events, and (ii)\nboth the background and triggering processes are conditionally Poisson. Such\nphenomena are typically \"data rich but knowledge poor\", in the sense that large\ndatasets are available yet a mechanistic understanding of the background and\ntriggering processes which generate the data are unavailable. In these settings\nnonparametric estimation plays a central role. However existing nonparametric\nestimators have computational and storage complexity $\\mathcal{O}(N^2)$,\nprecluding their application on large datasets. Here, by assuming the\ntriggering process acts only locally, we derive nonparametric estimators with\ncomputational complexity $\\mathcal{O}(N\\log N)$ and storage complexity\n$\\mathcal{O}(N)$. Our approach automatically learns the domain of the\ntriggering process from data and is essentially free from hyperparameters. The\nmethodology is applied to a large seismic dataset where estimation under\nexisting algorithms would be infeasible.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 23:34:44 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Oates", "Chris. J.", ""]]}, {"id": "1507.03130", "submitter": "Yun Yang", "authors": "Yun Yang and Surya Tokdar", "title": "Joint estimation of quantile planes over arbitrary predictor spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the recent surge of interest in quantile regression, joint\nestimation of linear quantile planes remains a great challenge in statistics\nand econometrics. We propose a novel parametrization that characterizes any\ncollection of non-crossing quantile planes over arbitrarily shaped convex\npredictor domains in any dimension by means of unconstrained scalar, vector and\nfunction valued parameters. Statistical models based on this parametrization\ninherit a fast computation of the likelihood function, enabling penalized\nlikelihood or Bayesian approaches to model fitting. We introduce a complete\nBayesian methodology by using Gaussian process prior distributions on the\nfunction valued parameters and develop a robust and efficient Markov chain\nMonte Carlo parameter estimation. The resulting method is shown to offer\nposterior consistency under mild tail and regularity conditions. We present\nseveral illustrative examples where the new method is compared against existing\napproaches and is found to offer better accuracy, coverage and model fit.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 17:30:04 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Yang", "Yun", ""], ["Tokdar", "Surya", ""]]}, {"id": "1507.03133", "submitter": "Rahul Mazumder", "authors": "Dimitris Bertsimas, Angela King and Rahul Mazumder", "title": "Best Subset Selection via a Modern Optimization Lens", "comments": "This is a revised version (May, 2015) of the first submission in June\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last twenty-five years (1990-2014), algorithmic advances in integer\noptimization combined with hardware improvements have resulted in an\nastonishing 200 billion factor speedup in solving Mixed Integer Optimization\n(MIO) problems. We present a MIO approach for solving the classical best subset\nselection problem of choosing $k$ out of $p$ features in linear regression\ngiven $n$ observations. We develop a discrete extension of modern first order\ncontinuous optimization methods to find high quality feasible solutions that we\nuse as warm starts to a MIO solver that finds provably optimal solutions. The\nresulting algorithm (a) provides a solution with a guarantee on its\nsuboptimality even if we terminate the algorithm early, (b) can accommodate\nside constraints on the coefficients of the linear regression and (c) extends\nto finding best subset solutions for the least absolute deviation loss\nfunction. Using a wide variety of synthetic and real datasets, we demonstrate\nthat our approach solves problems with $n$ in the 1000s and $p$ in the 100s in\nminutes to provable optimality, and finds near optimal solutions for $n$ in the\n100s and $p$ in the 1000s in minutes. We also establish via numerical\nexperiments that the MIO approach performs better than {\\texttt {Lasso}} and\nother popularly used sparse learning procedures, in terms of achieving sparse\nsolutions with good predictive power.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 18:19:27 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["King", "Angela", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1507.03293", "submitter": "Clementine Mottet", "authors": "Henry Lam and Clementine Mottet", "title": "Tail Analysis without Parametric Models: A Worst-case Perspective", "comments": null, "journal-ref": "Operations Research (2017)", "doi": "10.1287/opre.2017.1643", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common bottleneck in evaluating extremal performance measures is that, due\nto their very nature, tail data are often very limited. The conventional\napproach selects the best probability distribution from tail data using\nparametric fitting, but the validity of the parametric choice can be difficult\nto verify. This paper describes an alternative based on the computation of\nworst-case bounds under the geometric premise of tail convexity, a feature\nshared by all common parametric tail distributions. We characterize the\noptimality structure of the resulting optimization problem, and demonstrate\nthat the worst-case convex tail behavior is in a sense either extremely\nlight-tailed or extremely heavy-tailed. We develop low-dimensional nonlinear\nprograms that distinguish between the two cases and compute the worst-case\nbound. We numerically illustrate how the proposed approach can give more\nreliable performances than conventional parametric methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 23:41:27 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 01:30:06 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 17:49:18 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Lam", "Henry", ""], ["Mottet", "Clementine", ""]]}, {"id": "1507.03833", "submitter": "Shih-Kang Chao", "authors": "Shih-Kang Chao, Wolfgang Karl H\\\"ardle, Ming Yuan", "title": "Factorisable Multitask Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multivariate quantile regression model with a factor structure is proposed\nto study data with many responses of interest. The factor structure is allowed\nto vary with the quantile levels, which makes our framework more flexible than\nthe classical factor models. The model is estimated with the nuclear norm\nregularization in order to accommodate the high dimensionality of data, but the\nincurred optimization problem can only be efficiently solved in an approximate\nmanner by off-the-shelf optimization methods. Such a scenario is often seen\nwhen the empirical risk is non-smooth or the numerical procedure involves\nexpensive subroutines such as singular value decomposition. To ensure that the\napproximate estimator accurately estimates the model, non-asymptotic bounds on\nerror of the the approximate estimator is established. For implementation, a\nnumerical procedure that provably marginalizes the approximate error is\nproposed. The merits of our model and the proposed numerical procedures are\ndemonstrated through Monte Carlo experiments and an application to finance\ninvolving a large pool of asset returns.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 12:59:33 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 21:41:25 GMT"}, {"version": "v3", "created": "Sat, 18 Jan 2020 17:25:44 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chao", "Shih-Kang", ""], ["H\u00e4rdle", "Wolfgang Karl", ""], ["Yuan", "Ming", ""]]}, {"id": "1507.03887", "submitter": "Muhammad Farooq", "authors": "Muhammad Farooq, Ingo Steinwart", "title": "An SVM-like Approach for Expectile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectile regression is a nice tool for investigating conditional\ndistributions beyond the conditional mean. It is well-known that expectiles can\nbe described with the help of the asymmetric least square loss function, and\nthis link makes it possible to estimate expectiles in a non-parametric\nframework by a support vector machine like approach. In this work we develop an\nefficient sequential-minimal-optimization-based solver for the underlying\noptimization problem. The behavior of the solver is investigated by conducting\nvarious experiments and the results are compared with the recent R-package\nER-Boost.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 15:24:52 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Farooq", "Muhammad", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1507.04528", "submitter": "Ilaria Bianchini", "authors": "Raffaele Argiento, Ilaria Bianchini and Alessandra Guglielmi", "title": "A priori truncation method for posterior sampling from homogeneous\n  normalized completely random measure mixture models", "comments": "32 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adopts a Bayesian nonparametric mixture model where the mixing\ndistribution belongs to the wide class of normalized homogeneous completely\nrandom measures. We propose a truncation method for the mixing distribution by\ndiscarding the weights of the unnormalized measure smaller than a threshold. We\nprove convergence in law of our approximation, provide some theoretical\nproperties and characterize its posterior distribution so that a blocked Gibbs\nsampler is devised. The versatility of the approximation is illustrated by two\ndifferent applications. In the first the normalized Bessel random measure,\nencompassing the Dirichlet process, is introduced; goodness of fit indexes show\nits good performances as mixing measure for density estimation. The second\ndescribes how to incorporate covariates in the support of the normalized\nmeasure, leading to a linear dependent model for regression and clustering.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 11:18:01 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Argiento", "Raffaele", ""], ["Bianchini", "Ilaria", ""], ["Guglielmi", "Alessandra", ""]]}, {"id": "1507.04544", "submitter": "Aki Vehtari", "authors": "Aki Vehtari and Andrew Gelman and Jonah Gabry", "title": "Practical Bayesian model evaluation using leave-one-out cross-validation\n  and WAIC", "comments": null, "journal-ref": "Statistics and Computing, 2017, Volume 27, Issue 5, pp 1413-1432", "doi": "10.1007/s11222-016-9696-4", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leave-one-out cross-validation (LOO) and the widely applicable information\ncriterion (WAIC) are methods for estimating pointwise out-of-sample prediction\naccuracy from a fitted Bayesian model using the log-likelihood evaluated at the\nposterior simulations of the parameter values. LOO and WAIC have various\nadvantages over simpler estimates of predictive error such as AIC and DIC but\nare less used in practice because they involve additional computational steps.\nHere we lay out fast and stable computations for LOO and WAIC that can be\nperformed using existing simulation draws. We introduce an efficient\ncomputation of LOO using Pareto-smoothed importance sampling (PSIS), a new\nprocedure for regularizing importance weights. Although WAIC is asymptotically\nequal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case\nwith weak priors or influential observations. As a byproduct of our\ncalculations, we also obtain approximate standard errors for estimated\npredictive errors and for comparing of predictive errors between two models. We\nimplement the computations in an R package called 'loo' and demonstrate using\nmodels fit with the Bayesian inference package Stan.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 12:25:54 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 10:28:11 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 17:39:03 GMT"}, {"version": "v4", "created": "Mon, 11 Jul 2016 14:58:15 GMT"}, {"version": "v5", "created": "Mon, 12 Sep 2016 18:34:18 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""], ["Gabry", "Jonah", ""]]}, {"id": "1507.04553", "submitter": "Johanna Bertl", "authors": "Johanna Bertl, Gregory Ewing, Carolin Kosiol, Andreas Futschik", "title": "Approximate Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, methods of approximate parameter estimation have attracted\nconsiderable interest in complex problems where exact likelihoods are hard to\nobtain. In their most basic form, Bayesian methods such as Approximate Bayesian\nComputation (ABC) involve sampling from the parameter space and keeping those\nparameters that produce data that fit sufficiently well to the actually\nobserved data. Exploring the whole parameter space, however, makes this\napproach inefficient in high dimensional problems. This led to the proposal of\nmore sophisticated iterative methods of inference such as particle filters.\n  Here, we propose an alternative approach that is based on stochastic gradient\nmethods and applicable both in a frequentist and a Bayesian setting. By moving\nalong a simulated gradient, the algorithm produces a sequence of estimates that\nwill eventually converge either to the maximum likelihood estimate or to the\nmaximum of the posterior distribution, in each case under a set of observed\nsummary statistics. To avoid reaching only a local maximum, we propose to run\nthe algorithm from a set of random starting values.\n  As good tuning of the algorithm is important, we explored several tuning\nstrategies, and propose a set of guidelines that worked best in our\nsimulations. We investigate the performance of our approach in simulation\nstudies, and also apply the algorithm to two models with intractable likelihood\nfunctions. First, we present an application to inference in the context of\nqueuing systems. We also re-analyze population genetic data and estimate\nparameters describing the demographic history of Sumatran and Bornean\norang-utan populations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 12:54:56 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Bertl", "Johanna", ""], ["Ewing", "Gregory", ""], ["Kosiol", "Carolin", ""], ["Futschik", "Andreas", ""]]}, {"id": "1507.04727", "submitter": "Behtash Babadi", "authors": "Alireza Sheikhattar, Jonathan B. Fritz, Shihab A. Shamma, and Behtash\n  Babadi", "title": "Recursive Sparse Point Process Regression with Application to\n  Spectrotemporal Receptive Field Plasticity Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2512560", "report-no": null, "categories": "cs.NE cs.SY math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the sparse time-varying parameter\nvectors of a point process model in an online fashion, where the observations\nand inputs respectively consist of binary and continuous time series. We\nconstruct a novel objective function by incorporating a forgetting factor\nmechanism into the point process log-likelihood to enforce adaptivity and\nemploy $\\ell_1$-regularization to capture the sparsity. We provide a rigorous\nanalysis of the maximizers of the objective function, which extends the\nguarantees of compressed sensing to our setting. We construct two recursive\nfilters for online estimation of the parameter vectors based on proximal\noptimization techniques, as well as a novel filter for recursive computation of\nstatistical confidence regions. Simulation studies reveal that our algorithms\noutperform several existing point process filters in terms of trackability,\ngoodness-of-fit and mean square error. We finally apply our filtering\nalgorithms to experimentally recorded spiking data from the ferret primary\nauditory cortex during attentive behavior in a click rate discrimination task.\nOur analysis provides new insights into the time-course of the spectrotemporal\nreceptive field plasticity of the auditory neurons.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 19:43:54 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Sheikhattar", "Alireza", ""], ["Fritz", "Jonathan B.", ""], ["Shamma", "Shihab A.", ""], ["Babadi", "Behtash", ""]]}, {"id": "1507.04789", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss", "title": "A multi-resolution approximation for massive spatial datasets", "comments": "23 pages; to be published in Journal of the American Statistical\n  Association", "journal-ref": null, "doi": "10.1080/01621459.2015.1123632", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated sensing instruments on satellites and aircraft have enabled the\ncollection of massive amounts of high-resolution observations of spatial fields\nover large spatial regions. If these datasets can be efficiently exploited,\nthey can provide new insights on a wide variety of issues. However, traditional\nspatial-statistical techniques such as kriging are not computationally feasible\nfor big datasets. We propose a multi-resolution approximation (M-RA) of\nGaussian processes observed at irregular locations in space. The M-RA process\nis specified as a linear combination of basis functions at multiple levels of\nspatial resolution, which can capture spatial structure from very fine to very\nlarge scales. The basis functions are automatically chosen to approximate a\ngiven covariance function, which can be nonstationary. All computations\ninvolving the M-RA, including parameter inference and prediction, are highly\nscalable for massive datasets. Crucially, the inference algorithms can also be\nparallelized to take full advantage of large distributed-memory computing\nenvironments. In comparisons using simulated data and a large satellite\ndataset, the M-RA outperforms a related state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 22:44:56 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 19:04:25 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Katzfuss", "Matthias", ""]]}, {"id": "1507.05021", "submitter": "Alain Durmus", "authors": "Alain Durmus (LTCI), Eric Moulines (CMAP)", "title": "Non-asymptotic convergence analysis for the Unadjusted Langevin\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a method to sample from a target distribution $\\pi$\nover $\\mathbb{R}^d$ having a positive density with respect to the Lebesgue\nmeasure, known up to a normalisation factor. This method is based on the Euler\ndiscretization of the overdamped Langevin stochastic differential equation\nassociated with $\\pi$. For both constant and decreasing step sizes in the Euler\ndiscretization, we obtain non-asymptotic bounds for the convergence to the\ntarget distribution $\\pi$ in total variation distance. A particular attention\nis paid to the dependency on the dimension $d$, to demonstrate the\napplicability of this method in the high dimensional setting. These bounds\nimprove and extend the results of (Dalalyan 2014).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 16:23:23 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 17:07:11 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 14:27:42 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Durmus", "Alain", "", "LTCI"], ["Moulines", "Eric", "", "CMAP"]]}, {"id": "1507.05073", "submitter": "Michael Stephanou", "authors": "Michael Stephanou, Melvin Varughese and Iain Macdonald", "title": "Sequential Quantiles via Hermite Series Density Estimation", "comments": "43 pages, 9 figures. Improved version incorporating referee comments,\n  as appears in Electronic Journal of Statistics", "journal-ref": "Electron. J. Statist. 11 (2017), no. 1, 570--607", "doi": "10.1214/17-EJS1245", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential quantile estimation refers to incorporating observations into\nquantile estimates in an incremental fashion thus furnishing an online estimate\nof one or more quantiles at any given point in time. Sequential quantile\nestimation is also known as online quantile estimation. This area is relevant\nto the analysis of data streams and to the one-pass analysis of massive data\nsets. Applications include network traffic and latency analysis, real time\nfraud detection and high frequency trading. We introduce new techniques for\nonline quantile estimation based on Hermite series estimators in the settings\nof static quantile estimation and dynamic quantile estimation. In the static\nquantile estimation setting we apply the existing Gauss-Hermite expansion in a\nnovel manner. In particular, we exploit the fact that Gauss-Hermite\ncoefficients can be updated in a sequential manner. To treat dynamic quantile\nestimation we introduce a novel expansion with an exponentially weighted\nestimator for the Gauss-Hermite coefficients which we term the Exponentially\nWeighted Gauss-Hermite (EWGH) expansion. These algorithms go beyond existing\nsequential quantile estimation algorithms in that they allow arbitrary\nquantiles (as opposed to pre-specified quantiles) to be estimated at any point\nin time. In doing so we provide a solution to online distribution function and\nonline quantile function estimation on data streams. In particular we derive an\nanalytical expression for the CDF and prove consistency results for the CDF\nunder certain conditions. In addition we analyse the associated quantile\nestimator. Simulation studies and tests on real data reveal the Gauss-Hermite\nbased algorithms to be competitive with a leading existing algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 19:10:03 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 11:30:39 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Stephanou", "Michael", ""], ["Varughese", "Melvin", ""], ["Macdonald", "Iain", ""]]}, {"id": "1507.05079", "submitter": "Ricardo Ehlers", "authors": "Mauricio Zevallos, Loretta Gasco, Ricardo Ehlers", "title": "Riemann Manifold Langevin Methods on Stochastic Volatility Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we perform Bayesian estimation of stochastic volatility models\nwith heavy tail distributions using Metropolis adjusted Langevin (MALA) and\nRiemman manifold Langevin (MMALA) methods. We provide analytical expressions\nfor the application of these methods, assess the performance of these\nmethodologies in simulated data and illustrate their use on two financial time\nseries data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 19:22:41 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Zevallos", "Mauricio", ""], ["Gasco", "Loretta", ""], ["Ehlers", "Ricardo", ""]]}, {"id": "1507.05780", "submitter": "Samuel Livingstone", "authors": "Samuel Livingstone", "title": "Geometric ergodicity of the Random Walk Metropolis with\n  position-dependent proposal covariance", "comments": "13 pages including appendices, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Metropolis--Hastings method with proposal $\\mathcal{N}(x,\nhG(x)^{-1})$, where $x$ is the current state, and study its ergodicity\nproperties. We show that suitable choices of $G(x)$ can change these compared\nto the Random Walk Metropolis case $\\mathcal{N}(x, h\\Sigma)$, either for better\nor worse. We find that if the proposal variance is allowed to grow unboundedly\nin the tails of the distribution then geometric ergodicity can be established\nwhen the target distribution for the algorithm has tails that are heavier than\nexponential, but that the growth rate must be carefully controlled to prevent\nthe rejection rate approaching unity. We also illustrate that a judicious\nchoice of $G(x)$ can result in a geometrically ergodic chain when probability\nconcentrates on an ever narrower ridge in the tails, something that is not true\nfor the Random Walk Metropolis.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 10:49:33 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 10:09:40 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 11:32:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Livingstone", "Samuel", ""]]}, {"id": "1507.05990", "submitter": "Frank Noe", "authors": "Benjamin Trendelkamp-Schroer, Hao Wu, Fabian Paul and Frank No\\'e", "title": "Estimation and uncertainty of reversible Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph math-ph math.MP physics.comp-ph physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversibility is a key concept in Markov models and Master-equation models of\nmolecular kinetics. The analysis and interpretation of the transition matrix\nencoding the kinetic properties of the model relies heavily on the\nreversibility property. The estimation of a reversible transition matrix from\nsimulation data is therefore crucial to the successful application of the\npreviously developed theory. In this work we discuss methods for the maximum\nlikelihood estimation of transition matrices from finite simulation data and\npresent a new algorithm for the estimation if reversibility with respect to a\ngiven stationary vector is desired. We also develop new methods for the\nBayesian posterior inference of reversible transition matrices with and without\ngiven stationary vector taking into account the need for a suitable prior\ndistribution preserving the meta- stable features of the observed process\nduring posterior inference. All algorithms here are implemented in the PyEMMA\nsoftware - http://pyemma.org - as of version 2.0.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 21:38:45 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 03:13:42 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Trendelkamp-Schroer", "Benjamin", ""], ["Wu", "Hao", ""], ["Paul", "Fabian", ""], ["No\u00e9", "Frank", ""]]}, {"id": "1507.06055", "submitter": "Giri Gopalan", "authors": "Giri Gopalan and Luke Bornn", "title": "FastGP: An R Package for Gaussian Processes", "comments": "Paper submitted to JSS and R package submitted to CRAN. Temporarily\n  available at: https://github.com/ggopalan/FastGP", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their promise and ubiquity, Gaussian processes (GPs) can be difficult\nto use in practice due to the computational impediments of fitting and sampling\nfrom them. Here we discuss a short R package for efficient multivariate normal\nfunctions which uses the Rcpp and RcppEigen packages at its core. GPs have\nproperties that allow standard functions to be sped up; as an example we\ninclude functionality for Toeplitz matrices whose inverse can be computed in\nO(n^2) time with methods due to Trench and Durbin (Golub & Van Loan 1996),\nwhich is particularly apt when time points (or spatial locations) of a Gaussian\nprocess are evenly spaced, since the associated covariance matrix is Toeplitz\nin this case. Additionally, we include functionality to sample from a latent\nvariable Gaussian process model with elliptical slice sampling (Murray, Adams,\n& MacKay 2010).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 04:42:58 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Gopalan", "Giri", ""], ["Bornn", "Luke", ""]]}, {"id": "1507.06110", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Minh-Ngoc Tran, Mattias Villani, Robert Kohn", "title": "Speeding Up MCMC by Delayed Acceptance and Data Subsampling", "comments": "Accepted for publication in Journal of Computational and Graphical\n  Statistics", "journal-ref": null, "doi": "10.1080/10618600.2017.1307117", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of the Metropolis-Hastings (MH) algorithm arises from the\nrequirement of a likelihood evaluation for the full data set in each iteration.\nPayne and Mallick (2015) propose to speed up the algorithm by a delayed\nacceptance approach where the acceptance decision proceeds in two stages. In\nthe first stage, an estimate of the likelihood based on a random subsample\ndetermines if it is likely that the draw will be accepted and, if so, the\nsecond stage uses the full data likelihood to decide upon final acceptance.\nEvaluating the full data likelihood is thus avoided for draws that are unlikely\nto be accepted. We propose a more precise likelihood estimator which\nincorporates auxiliary information about the full data likelihood while only\noperating on a sparse set of the data. We prove that the resulting delayed\nacceptance MH is more efficient compared to that of Payne and Mallick (2015).\nThe caveat of this approach is that the full data set needs to be evaluated in\nthe second stage. We therefore propose to substitute this evaluation by an\nestimate and construct a state-dependent approximation thereof to use in the\nfirst stage. This results in an algorithm that (i) can use a smaller subsample\nm by leveraging on recent advances in Pseudo-Marginal MH (PMMH) and (ii) is\nprovably within $O(m^{-2})$ of the true posterior.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 09:25:21 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 11:52:10 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 23:26:05 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Quiroz", "Matias", ""], ["Tran", "Minh-Ngoc", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""]]}, {"id": "1507.06244", "submitter": "Shiwei Lan", "authors": "Shiwei Lan, Tan Bui-Thanh, Mike Christie and Mark Girolami", "title": "Emulation of Higher-Order Tensors in Manifold Monte Carlo Methods for\n  Bayesian Inverse Problems", "comments": "46 pages, 11 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2015.12.032", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian approach to Inverse Problems relies predominantly on Markov\nChain Monte Carlo methods for posterior inference. The typical nonlinear\nconcentration of posterior measure observed in many such Inverse Problems\npresents severe challenges to existing simulation based inference methods.\nMotivated by these challenges the exploitation of local geometric information\nin the form of covariant gradients, metric tensors, Levi-Civita connections,\nand local geodesic flows, have been introduced to more effectively locally\nexplore the configuration space of the posterior measure. However, obtaining\nsuch geometric quantities usually requires extensive computational effort and\ndespite their effectiveness affect the applicability of these\ngeometrically-based Monte Carlo methods. In this paper we explore one way to\naddress this issue by the construction of an emulator of the model from which\nall geometric objects can be obtained in a much more computationally feasible\nmanner. The main concept is to approximate the geometric quantities using a\nGaussian Process emulator which is conditioned on a carefully chosen design set\nof configuration points, which also determines the quality of the emulator. To\nthis end we propose the use of statistical experiment design methods to refine\na potentially arbitrarily initialized design online without destroying the\nconvergence of the resulting Markov chain to the desired invariant measure. The\npractical examples considered in this paper provide a demonstration of the\nsignificant improvement possible in terms of computational loading suggesting\nthis is a promising avenue of further development.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 16:32:36 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 09:06:04 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Lan", "Shiwei", ""], ["Bui-Thanh", "Tan", ""], ["Christie", "Mike", ""], ["Girolami", "Mark", ""]]}, {"id": "1507.06336", "submitter": "Thomas House", "authors": "Thomas House", "title": "Hessian corrections to the Metropolis Adjusted Langevin Algorithm", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural method for the introduction of second-order derivatives of the log\nlikelihood into MCMC algorithms is introduced, based on Taylor expansion of the\nLangevin equation followed by exact solution of the truncated system.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 20:40:55 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["House", "Thomas", ""]]}, {"id": "1507.06370", "submitter": "Tengyu Ma", "authors": "Tengyu Ma, Avi Wigderson", "title": "Sum-of-Squares Lower Bounds for Sparse PCA", "comments": "to appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a statistical versus computational trade-off for\nsolving a basic high-dimensional machine learning problem via a basic convex\nrelaxation method. Specifically, we consider the {\\em Sparse Principal\nComponent Analysis} (Sparse PCA) problem, and the family of {\\em\nSum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well\nknown that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em\nin principle} detected using only $n \\approx k\\log p$ (Gaussian or Bernoulli)\nsamples, but all {\\em efficient} (polynomial time) algorithms known require $n\n\\approx k^2$ samples. It was also known that this quadratic gap cannot be\nimproved by the the most basic {\\em semi-definite} (SDP, aka spectral)\nrelaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also\ndegree-4 SoS algorithms cannot improve this quadratic gap. This average-case\nlower bound adds to the small collection of hardness results in machine\nlearning for this powerful family of convex relaxation algorithms. Moreover,\nour design of moments (or \"pseudo-expectations\") for this lower bound is quite\ndifferent than previous lower bounds. Establishing lower bounds for higher\ndegree SoS algorithms for remains a challenging problem.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 01:50:43 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 05:50:16 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ma", "Tengyu", ""], ["Wigderson", "Avi", ""]]}, {"id": "1507.06496", "submitter": "Mariella Dimiccoli", "authors": "Mariella Dimiccoli", "title": "Fundamentals of cone regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cone regression is a particular case of quadratic programming that minimizes\na weighted sum of squared residuals under a set of linear inequality\nconstraints. Several important statistical problems such as isotonic, concave\nregression or ANOVA under partial orderings, just to name a few, can be\nconsidered as particular instances of the cone regression problem. Given its\nrelevance in Statistics, this paper aims to address the fundamentals of cone\nregression from a theoretical and practical point of view. Several formulations\nof the cone regression problem are considered and, focusing on the particular\ncase of concave regression as example, several algorithms are analyzed and\ncompared both qualitatively and quantitatively through numerical simulations.\nSeveral improvements to enhance numerical stability and bound the computational\ncost are proposed. For each analyzed algorithm, the pseudo-code and its\ncorresponding code in Scilab are provided. The results from this study\ndemonstrate that the choice of the optimization approach strongly impacts the\nnumerical performances. It is also shown that methods are not currently\navailable to solve efficiently cone regression problems with large dimension\n(more than many thousands of points). We suggest further research to fill this\ngap by exploiting and adapting classical multi-scale strategy to compute an\napproximate solution.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 14:00:49 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 10:24:03 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2015 01:30:31 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2015 11:32:17 GMT"}, {"version": "v5", "created": "Mon, 11 Apr 2016 10:21:53 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Dimiccoli", "Mariella", ""]]}, {"id": "1507.06716", "submitter": "Michael Shields", "authors": "Michael D. Shields and Jiaxin Zhang", "title": "The generalization of Latin hypercube sampling", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": "10.1016/j.ress.2015.12.002", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latin hypercube sampling (LHS) is generalized in terms of a spectrum of\nstratified sampling (SS) designs referred to as partially stratified sample\n(PSS) designs. True SS and LHS are shown to represent the extremes of the PSS\nspectrum. The variance of PSS estimates is derived along with some asymptotic\nproperties. PSS designs are shown to reduce variance associated with variable\ninteractions, whereas LHS reduces variance associated with main effects.\nChallenges associated with the use of PSS designs and their limitations are\ndiscussed. To overcome these challenges, the PSS method is coupled with a new\nmethod called Latinized stratified sampling (LSS) that produces sample sets\nthat are simultaneously SS and LHS. The LSS method is equivalent to an\nOrthogonal Array based LHS under certain conditions but is easier to obtain.\nUtilizing an LSS on the subspaces of a PSS provides a sampling strategy that\nreduces variance associated with both main effects and variable interactions\nand can be designed specially to minimize variance for a given problem. Several\nhigh-dimensional numerical examples highlight the strengths and limitations of\nthe method. The Latinized partially stratified sampling method is then applied\nto identify the best sample strategy for uncertainty quantification on a plate\nbuckling problem.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 01:50:28 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Shields", "Michael D.", ""], ["Zhang", "Jiaxin", ""]]}, {"id": "1507.06736", "submitter": "Bubacarr Bah", "authors": "Bubacarr Bah, and Rachel Ward", "title": "The sample complexity of weighted sparse approximation", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": "10.1109/TSP.2016.2543211", "report-no": null, "categories": "math.NA cs.CC cs.IT math.FA math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Gaussian sampling matrices, we provide bounds on the minimal number of\nmeasurements $m$ required to achieve robust weighted sparse recovery guarantees\nin terms of how well a given prior model for the sparsity support aligns with\nthe true underlying support. Our main contribution is that for a sparse vector\n${\\bf x} \\in \\mathbb{R}^N$ supported on an unknown set $\\mathcal{S} \\subset\n\\{1, \\dots, N\\}$ with $|\\mathcal{S}|\\leq k$, if $\\mathcal{S}$ has\n\\emph{weighted cardinality} $\\omega(\\mathcal{S}) := \\sum_{j \\in \\mathcal{S}}\n\\omega_j^2$, and if the weights on $\\mathcal{S}^c$ exhibit mild growth,\n$\\omega_j^2 \\geq \\gamma \\log(j/\\omega(\\mathcal{S}))$ for $j\\in\\mathcal{S}^c$\nand $\\gamma > 0$, then the sample complexity for sparse recovery via weighted\n$\\ell_1$-minimization using weights $\\omega_j$ is linear in the weighted\nsparsity level, and $m = \\mathcal{O}(\\omega(\\mathcal{S})/\\gamma)$. This main\nresult is a generalization of special cases including a) the standard sparse\nrecovery setting where all weights $\\omega_j \\equiv 1$, and $m =\n\\mathcal{O}\\left(k\\log\\left(N/k\\right)\\right)$; b) the setting where the\nsupport is known a priori, and $m = \\mathcal{O}(k)$; and c) the setting of\nsparse recovery with prior information, and $m$ depends on how well the weights\nare aligned with the support set $\\mathcal{S}$. We further extend the results\nin case c) to the setting of additive noise. Our results are {\\em nonuniform}\nthat is they apply for a fixed support, unknown a priori, and the weights on\n$\\mathcal{S}$ do not all have to be smaller than the weights on $\\mathcal{S}^c$\nfor our recovery results to hold.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 04:22:05 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 17:54:49 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2015 04:39:13 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2016 05:09:14 GMT"}, {"version": "v5", "created": "Sat, 9 Apr 2016 13:03:19 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Bah", "Bubacarr", ""], ["Ward", "Rachel", ""]]}, {"id": "1507.06759", "submitter": "Phaedon-Stelios Koutsourelakis", "authors": "Phaedon-Stelios Koutsourelakis", "title": "Variational Bayesian strategies for high-dimensional, stochastic design\n  problems", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2015.12.031", "report-no": null, "categories": "stat.CO math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a lesser-studied problem in the context of\nmodel-based, uncertainty quantification (UQ), that of\noptimization/design/control under uncertainty. The solution of such problems is\nhindered not only by the usual difficulties encountered in UQ tasks (e.g. the\nhigh computational cost of each forward simulation, the large number of random\nvariables) but also by the need to solve a nonlinear optimization problem\ninvolving large numbers of design variables and potentially constraints. We\npropose a framework that is suitable for a large class of such problems and is\nbased on the idea of recasting them as probabilistic inference tasks. To that\nend, we propose a Variational Bayesian (VB) formulation and an iterative\nVB-Expectation-Maximization scheme that is also capable of identifying a\nlow-dimensional set of directions in the design space, along which, the\nobjective exhibits the largest sensitivity.\n  We demonstrate the validity of the proposed approach in the context of two\nnumerical examples involving $\\mathcal{O}(10^3)$ random and design variables.\nIn all cases considered the cost of the computations in terms of calls to the\nforward model was of the order $\\mathcal{O}(10^2)$. The accuracy of the\napproximations provided is assessed by appropriate information-theoretic\nmetrics.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 07:00:32 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 10:56:46 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "1507.06780", "submitter": "Jia Liu", "authors": "Jia Liu", "title": "An improved EM algorithm for solving MLE in constrained diffusion\n  kurtosis imaging of human brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The displacement distribution of a water molecular is characterized\nmathematically as Gaussianity without considering potential diffusion barriers\nand compartments. However, this is not true in real scenario: most biological\ntissues are comprised of cell membranes, various intracellular and\nextracellular spaces, and of other compartments, where the water diffusion is\nreferred to have a non-Gaussian distribution. Diffusion kurtosis imaging (DKI),\nrecently considered to be one sensitive biomarker, is an extension of diffusion\ntensor imaging, which quantifies the degree of non-Gaussianity of the\ndiffusion. This work proposes an efficient scheme of maximum likelihood\nestimation (MLE) in DKI: we start from the Rician noise model of the signal\nintensities. By augmenting a Von-Mises distributed latent phase variable, the\nRician likelihood is transformed to a tractable joint density without loss of\ngenerality. A fast computational method, an expectation-maximization (EM)\nalgorithm for MLE is proposed in DKI. To guarantee the physical relevance of\nthe diffusion kurtosis we apply the ternary quartic (TQ) parametrization to\nutilize its positivity, which imposes the upper bound to the kurtosis. A\nFisher-scoring method is used for achieving fast convergence of the individual\ndiffusion compartments. In addition, we use the barrier method to constrain the\nlower bound to the kurtosis. The proposed estimation scheme is conducted on\nboth synthetic and real data with an objective of healthy human brain. We\ncompared the method with the other popular ones with promising performance\nshown in the results.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 08:43:23 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Liu", "Jia", ""]]}, {"id": "1507.06807", "submitter": "Andrew Golightly", "authors": "Gavin A. Whitaker, Andrew Golightly, Richard J. Boys and Chris\n  Sherlock", "title": "Bayesian inference for diffusion driven mixed-effects models", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) provide a natural framework for\nmodelling intrinsic stochasticity inherent in many continuous-time physical\nprocesses. When such processes are observed in multiple individuals or\nexperimental units, SDE driven mixed-effects models allow the quantification of\nbetween (as well as within) individual variation. Performing Bayesian inference\nfor such models, using discrete time data that may be incomplete and subject to\nmeasurement error is a challenging problem and is the focus of this paper. We\nextend a recently proposed MCMC scheme to include the SDE driven mixed-effects\nframework. Fundamental to our approach is the development of a novel construct\nthat allows for efficient sampling of conditioned SDEs that may exhibit\nnonlinear dynamics between observation times. We apply the resulting scheme to\nsynthetic data generated from a simple SDE model of orange tree growth, and\nreal data consisting of observations on aphid numbers recorded under a variety\nof different treatment regimes. In addition, we provide a systematic comparison\nof our approach with an inference scheme based on a tractable approximation of\nthe SDE, that is, the linear noise approximation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 10:52:44 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 13:52:01 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Whitaker", "Gavin A.", ""], ["Golightly", "Andrew", ""], ["Boys", "Richard J.", ""], ["Sherlock", "Chris", ""]]}, {"id": "1507.07024", "submitter": "Matthew Parno", "authors": "Matthew Parno, Tarek Moselhy, and Youssef Marzouk", "title": "A multiscale strategy for Bayesian inference using transport maps", "comments": null, "journal-ref": null, "doi": "10.1137/15M1032478", "report-no": null, "categories": "stat.CO math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many inverse problems, model parameters cannot be precisely determined\nfrom observational data. Bayesian inference provides a mechanism for capturing\nthe resulting parameter uncertainty, but typically at a high computational\ncost. This work introduces a multiscale decomposition that exploits conditional\nindependence across scales, when present in certain classes of inverse\nproblems, to decouple Bayesian inference into two stages: (1) a computationally\ntractable coarse-scale inference problem; and (2) a mapping of the\nlow-dimensional coarse-scale posterior distribution into the original\nhigh-dimensional parameter space. This decomposition relies on a\ncharacterization of the non-Gaussian joint distribution of coarse- and\nfine-scale quantities via optimal transport maps. We demonstrate our approach\non a sequence of inverse problems arising in subsurface flow, using the\nmultiscale finite element method to discretize the steady state pressure\nequation. We compare the multiscale strategy with full-dimensional Markov chain\nMonte Carlo on a problem of moderate dimension (100 parameters) and then use it\nto infer a conductivity field described by over 10,000 parameters.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 21:40:44 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 00:04:24 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Parno", "Matthew", ""], ["Moselhy", "Tarek", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1507.07070", "submitter": "Baidurya Bhattacharya", "authors": "Baidurya Bhattacharya", "title": "The Extremal Index and the Maximum of a Dependent Stationary Pulse Load\n  Process Observed above a High Threshold", "comments": null, "journal-ref": "Structural Safety, Elsevier, vol. 30, no. 1, pp. 34 - 48, 2008", "doi": "10.1016/j.strusafe.2006.05.001", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing a load process above high thresholds, modeling it as a pulse\nprocess with random occurrence times and magnitudes, and extrapolating\nlife-time maximum or design loads from the data is a common task in structural\nreliability analyses. In this paper, we consider a stationary live load\nsequence that arrive according to a dependent point process and allow for a\nweakened mixing-type dependence in the load pulse magnitudes that\nasymptotically decreases to zero with increasing separation in the sequence.\nInclusion of dependence in the model eliminates the unnecessary conservatism\nintroduced by the i.i.d. (independent and identically distributed) assumption\noften made in determining maximum live load distribution. The scale of\nfluctuation of the loading process is used to identify clusters of exceedances\nabove high thresholds which in turn is used to estimate the extremal index of\nthe process. A Bayesian updating of the empirical distribution function,\nderived from the distribution of order statistics in a dependent stationary\nseries, is performed. The pulse arrival instants are modeled as a Cox process\ngoverened by a stationary lognormal intensity. An illustrative example utilizes\nin-service peak strain data from ambient traffic collected on a high volume\nhighway bridge, and analyzes the asymptotic behavior of the maximum load.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 05:42:01 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Bhattacharya", "Baidurya", ""]]}, {"id": "1507.07106", "submitter": "Minsuk Shin", "authors": "Minsuk Shin, Anirban Bhattacharya, and Valen E. Johnson", "title": "Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in\n  Ultrahigh-Dimensional Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model selection procedures based on nonlocal alternative prior\ndensities are extended to ultrahigh dimensional settings and compared to other\nvariable selection procedures using precision-recall curves. Variable selection\nprocedures included in these comparisons include methods based on $g$-priors,\nreciprocal lasso, adaptive lasso, scad, and minimax concave penalty criteria.\nThe use of precision-recall curves eliminates the sensitivity of our\nconclusions to the choice of tuning parameters. We find that Bayesian variable\nselection procedures based on nonlocal priors are competitive to all other\nprocedures in a range of simulation scenarios, and we subsequently explain this\nfavorable performance through a theoretical examination of their consistency\nproperties. When certain regularity conditions apply, we demonstrate that the\nnonlocal procedures are consistent for linear models even when the number of\ncovariates $p$ increases sub-exponentially with the sample size $n$. A model\nselection procedure based on Zellner's $g$-prior is also found to be\ncompetitive with penalized likelihood methods in identifying the true model,\nbut the posterior distribution on the model space induced by this method is\nmuch more dispersed than the posterior distribution induced on the model space\nby the nonlocal prior methods. We investigate the asymptotic form of the\nmarginal likelihood based on the nonlocal priors and show that it attains a\nunique term that cannot be derived from the other Bayesian model selection\nprocedures. We also propose a scalable and efficient algorithm called\nSimplified Shotgun Stochastic Search with Screening (S5) to explore the\nenormous model space, and we show that S5 dramatically reduces the computing\ntime without losing the capacity to search the interesting region in the model\nspace. The S5 algorithm is available in an \\verb R ~package {\\it BayesS5} on\n\\texttt{CRAN}.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 15:03:45 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 21:14:52 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 20:57:31 GMT"}, {"version": "v4", "created": "Wed, 18 Jan 2017 05:14:06 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Shin", "Minsuk", ""], ["Bhattacharya", "Anirban", ""], ["Johnson", "Valen E.", ""]]}, {"id": "1507.07392", "submitter": "Michael Beard", "authors": "Michael Beard, Stephan Reuter, Karl Granstr\\\"om, Ba-Tuong Vo, Ba-Ngu\n  Vo, Alexander Scheel", "title": "Multiple Extended Target Tracking with Labelled Random Finite Sets", "comments": "13 pages, 10 figures, submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2505683", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targets that generate multiple measurements at a given instant in time are\ncommonly known as extended targets. These present a challenge for many tracking\nalgorithms, as they violate one of the key assumptions of the standard\nmeasurement model. In this paper, a new algorithm is proposed for tracking\nmultiple extended targets in clutter, that is capable of estimating the number\nof targets, as well the trajectories of their states, comprising the\nkinematics, measurement rates and extents. The proposed technique is based on\nmodelling the multi-target state as a generalised labelled multi-Bernoulli\n(GLMB) random finite set (RFS), within which the extended targets are modelled\nusing gamma Gaussian inverse Wishart (GGIW) distributions. A cheaper variant of\nthe algorithm is also proposed, based on the labelled multi-Bernoulli (LMB)\nfilter. The proposed GLMB/LMB-based algorithms are compared with an extended\ntarget version of the cardinalised probability hypothesis density (CPHD)\nfilter, and simulation results show that the (G)LMB has improved estimation and\ntracking performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 13:05:53 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Beard", "Michael", ""], ["Reuter", "Stephan", ""], ["Granstr\u00f6m", "Karl", ""], ["Vo", "Ba-Tuong", ""], ["Vo", "Ba-Ngu", ""], ["Scheel", "Alexander", ""]]}, {"id": "1507.07445", "submitter": "Sadegh Movahed", "authors": "Z. Koohi Lai, S. M. S. Movahed and G. R. Jafari", "title": "Assessment of petrophysical quantities inspired by joint multifractal\n  approach", "comments": "13 pages, 5 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph physics.geo-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper joint multifractal random walk approach is carried out to\nanalyze some petrophysical quantities for characterizing the petroleum\nreservoir. These quantities include Gamma emission (GR), sonic transient time\n(DT) and Neutron porosity (NPHI) which are collected from four wells of a\nreservoir. To quantify mutual interaction of petrophysical quantities, joint\nmultifractal random walk is implemented. This approach is based on the mutual\nmultiplicative cascade notion in the multifractal formalism and in this\napproach $L_0$ represents a benchmark to describe the nature of\ncross-correlation between two series. The analysis of the petrophysical\nquantities revealed that GR for all wells has strongly multifractal nature due\nto the considerable abundance of large fluctuations in various scales. The\nvariance of probability distribution function, $\\lambda_{\\ell}^2$, at scale\n$\\ell$ and its intercept determine the multifractal properties of the data sets\nsourced by probability density function. The value of $\\lambda_0 ^2$ for NPHI\ndata set is less than GR's, however, DT shows a nearly monofractal behavior,\nnamely $\\lambda_0 ^2\\rightarrow 0$, so we find that $\\lambda_0^2({\\rm\nGR})>\\lambda_0^2({\\rm NPHI})\\gg\\lambda_0^2({\\rm DT})$. While, the value of\nHurst exponents can not discriminate between series GR, NPHI and DT. Joint\nanalysis of the petrophysical quantities for considered wells demonstrates that\n$L_0$ has negative value for GR-NPHI confirming that finding shaly layers is in\ncompetition with finding porous medium while it takes positive value for GR-DT\ndetermining that continuum medium can be detectable by evaluating the\nstatistical properties of GR and its cross-correlation to DT signal.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 15:19:00 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Lai", "Z. Koohi", ""], ["Movahed", "S. M. S.", ""], ["Jafari", "G. R.", ""]]}, {"id": "1507.08050", "submitter": "Thomas Wiecki", "authors": "John Salvatier, Thomas Wiecki, Christopher Fonnesbeck", "title": "Probabilistic Programming in Python using PyMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic programming (PP) allows flexible specification of Bayesian\nstatistical models in code. PyMC3 is a new, open-source PP framework with an\nintutive and readable, yet powerful, syntax that is close to the natural syntax\nstatisticians use to describe models. It features next-generation Markov chain\nMonte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS;\nHoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane,\n1987). Probabilistic programming in Python confers a number of advantages\nincluding multi-platform compatibility, an expressive yet clean and readable\nsyntax, easy integration with other scientific libraries, and extensibility via\nC, C++, Fortran or Cython. These features make it relatively straightforward to\nwrite and use custom statistical distributions, samplers and transformation\nfunctions, as required by Bayesian analysis.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 08:20:22 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Salvatier", "John", ""], ["Wiecki", "Thomas", ""], ["Fonnesbeck", "Christopher", ""]]}, {"id": "1507.08504", "submitter": "Roeland M.H. Merks", "authors": "Sonja E.M. Boas, Maria I. Navarro Jimenez, Roeland M.H. Merks, Joke G.\n  Blom", "title": "A global sensitivity analysis approach for morphogenesis models", "comments": "31 pages, 7 figures", "journal-ref": "Boas, S. E. M., Navarro Jimenez, M. I., Merks, R. M. H., & Blom,\n  J. G. (2015). A global sensitivity analysis approach for morphogenesis\n  models. Bmc Systems Biology, 9(1), 85", "doi": "10.1186/s12918-015-0222-7", "report-no": null, "categories": "q-bio.CB math.NA q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphogenesis is a developmental process in which cells organize into shapes\nand patterns. Complex, multi-factorial models are commonly used to study\nmorphogenesis. It is difficult to understand the relation between the\nuncertainty in the input and the output of such `black-box' models, giving rise\nto the need for sensitivity analysis tools. In this paper, we introduce a\nworkflow for a global sensitivity analysis approach to study the impact of\nsingle parameters and the interactions between them on the output of\nmorphogenesis models. To demonstrate the workflow, we used a published,\nwell-studied model of vascular morphogenesis. The parameters of the model\nrepresent cell properties and behaviors that drive the mechanisms of angiogenic\nsprouting. The global sensitivity analysis correctly identified the dominant\nparameters in the model, consistent with previous studies. Additionally, the\nanalysis provides information on the relative impact of single parameters and\nof interactions between them. The uncertainty in the output of the model was\nlargely caused by single parameters. The parameter interactions, although of\nlow impact, provided new insights in the mechanisms of \\emph{in silico}\nsprouting. Finally, the analysis indicated that the model could be reduced by\none parameter. We propose global sensitivity analysis as an alternative\napproach to study and validate the mechanisms of morphogenesis. Comparison of\nthe ranking of the impact of the model parameters to knowledge derived from\nexperimental data and validation of manipulation experiments can help to\nfalsify models and to find the operand mechanisms in morphogenesis. The\nworkflow is applicable to all `black-box' models, including high-throughput\n\\emph{in vitro} models in which an output measure is affected by a set of\nexperimental perturbations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 13:53:35 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 12:57:21 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Boas", "Sonja E. M.", ""], ["Jimenez", "Maria I. Navarro", ""], ["Merks", "Roeland M. H.", ""], ["Blom", "Joke G.", ""]]}, {"id": "1507.08526", "submitter": "Allan De Freitas", "authors": "Allan De Freitas and Fran\\c{c}ois Septier and Lyudmila Mihaylova and\n  Simon Godsill", "title": "How Can Subsampling Reduce Complexity in Sequential MCMC Methods and\n  Deal with Big Data in Target Tracking?", "comments": "International Conference on Information Fusion, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target tracking faces the challenge in coping with large volumes of data\nwhich requires efficient methods for real time applications. The complexity\nconsidered in this paper is when there is a large number of measurements which\nare required to be processed at each time step. Sequential Markov chain Monte\nCarlo (MCMC) has been shown to be a promising approach to target tracking in\ncomplex environments, especially when dealing with clutter. However, a large\nnumber of measurements usually results in large processing requirements. This\npaper goes beyond the current state-of-the-art and presents a novel Sequential\nMCMC approach that can overcome this challenge through adaptively subsampling\nthe set of measurements. Instead of using the whole large volume of available\ndata, the proposed algorithm performs a trade off between the number of\nmeasurements to be used and the desired accuracy of the estimates to be\nobtained in the presence of clutter. We show results with large improvements in\nprocessing time, more than 40% with a negligible loss in tracking performance,\ncompared with the solution without subsampling.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 14:54:41 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["De Freitas", "Allan", ""], ["Septier", "Fran\u00e7ois", ""], ["Mihaylova", "Lyudmila", ""], ["Godsill", "Simon", ""]]}, {"id": "1507.08554", "submitter": "Aaron Smith", "authors": "Natesh S. Pillai and Aaron Smith", "title": "Kac's Walk on $n$-sphere mixes in $n\\log n$ steps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the mixing time of Kac's random walk on the sphere\n$\\mathrm{S}^{n-1}$ is a long-standing open problem. We show that the total\nvariation mixing time of Kac's walk on $\\mathrm{S}^{n-1}$ is between\n$\\frac{1}{2} \\, n \\log(n)$ and $200 \\,n \\log(n)$. Our bound is thus optimal up\nto a constant factor, improving on the best-known upper bound of $O(n^{5}\n\\log(n)^{2})$ due to Jiang. Our main tool is a `non-Markovian' coupling\nrecently introduced by the second author for obtaining the convergence rates of\ncertain high dimensional Gibbs samplers in continuous state spaces.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 15:47:20 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 16:22:53 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2015 21:12:28 GMT"}, {"version": "v4", "created": "Thu, 31 Mar 2016 15:06:31 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1507.08563", "submitter": "Dean Oliver", "authors": "Dean S. Oliver", "title": "Metropolized Randomized Maximum Likelihood for sampling from multimodal\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a method for using optimization to derive efficient\nindependent transition functions for Markov chain Monte Carlo simulations. Our\ninterest is in sampling from a posterior density $\\pi(x)$ for problems in which\nthe dimension of the model space is large, $\\pi(x)$ is multimodal with regions\nof low probability separating the modes, and evaluation of the likelihood is\nexpensive. We restrict our attention to the special case for which the target\ndensity is the product of a multivariate Gaussian prior and a likelihood\nfunction for which the errors in observations are additive and Gaussian.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 16:24:56 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 10:36:18 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Oliver", "Dean S.", ""]]}, {"id": "1507.08577", "submitter": "Luca Martino", "authors": "L. Martino, V. Elvira, D. Luengo, J. Corander, F. Louzada", "title": "Orthogonal parallel MCMC methods for sampling and optimization", "comments": null, "journal-ref": "Digital Signal Processing Volume 58, Pages: 64-84, 2016", "doi": "10.1016/j.dsp.2016.07.013", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) methods are widely used for Bayesian inference and\noptimization in statistics, signal processing and machine learning. A\nwell-known class of MC methods are Markov Chain Monte Carlo (MCMC) algorithms.\nIn order to foster better exploration of the state space, specially in\nhigh-dimensional applications, several schemes employing multiple parallel MCMC\nchains have been recently introduced. In this work, we describe a novel\nparallel interacting MCMC scheme, called {\\it orthogonal MCMC} (O-MCMC), where\na set of \"vertical\" parallel MCMC chains share information using some\n\"horizontal\" MCMC techniques working on the entire population of current\nstates. More specifically, the vertical chains are led by random-walk\nproposals, whereas the horizontal MCMC techniques employ independent proposals,\nthus allowing an efficient combination of global exploration and local\napproximation. The interaction is contained in these horizontal iterations.\nWithin the analysis of different implementations of O-MCMC, novel schemes in\norder to reduce the overall computational cost of parallel multiple try\nMetropolis (MTM) chains are also presented. Furthermore, a modified version of\nO-MCMC for optimization is provided by considering parallel simulated annealing\n(SA) algorithms. Numerical results show the advantages of the proposed sampling\nscheme in terms of efficiency in the estimation, as well as robustness in terms\nof independence with respect to initial values and the choice of the\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 16:47:33 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 11:37:28 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Martino", "L.", ""], ["Elvira", "V.", ""], ["Luengo", "D.", ""], ["Corander", "J.", ""], ["Louzada", "F.", ""]]}, {"id": "1507.08613", "submitter": "Mark Risser", "authors": "Mark D. Risser and Catherine A. Calder", "title": "Local likelihood estimation for covariance functions with\n  spatially-varying parameters: the convoSPAT package for R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the interest in and appeal of convolution-based approaches for\nnonstationary spatial modeling, off-the-shelf software for model fitting does\nnot as of yet exist. Convolution-based models are highly flexible yet\nnotoriously difficult to fit, even with relatively small data sets. The general\nlack of pre-packaged options for model fitting makes it difficult to compare\nnew methodology in nonstationary modeling with other existing methods, and as a\nresult most new models are simply compared to stationary models. Using a\nconvolution-based approach, we present a new nonstationary covariance function\nfor spatial Gaussian process models that allows for efficient computing in two\nways: first, by representing the spatially-varying parameters via a discrete\nmixture or \"mixture component\" model, and second, by estimating the mixture\ncomponent parameters through a local likelihood approach. In order to make\ncomputation for a convolution-based nonstationary spatial model readily\navailable, this paper also presents and describes the convoSPAT package for R.\nThe nonstationary model is fit to both a synthetic data set and a real data\napplication involving annual precipitation to demonstrate the capabilities of\nthe package.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 18:24:39 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 23:45:32 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2016 22:06:28 GMT"}, {"version": "v4", "created": "Fri, 3 Feb 2017 19:16:24 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Risser", "Mark D.", ""], ["Calder", "Catherine A.", ""]]}, {"id": "1507.08645", "submitter": "Reza Solgi", "authors": "Luke Bornn, Neil Shephard and Reza Solgi", "title": "Moment conditions and Bayesian nonparametrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models phrased though moment conditions are central to much of modern\ninference. Here these moment conditions are embedded within a nonparametric\nBayesian setup. Handling such a model is not probabilistically straightforward\nas the posterior has support on a manifold. We solve the relevant issues,\nbuilding new probability and computational tools using Hausdorff measures to\nanalyze them on real and simulated data. These new methods which involve\nsimulating on a manifold can be applied widely, including providing Bayesian\nanalysis of quasi-likelihoods, linear and nonlinear regression, missing data\nand hierarchical models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 19:45:52 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 18:58:42 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Bornn", "Luke", ""], ["Shephard", "Neil", ""], ["Solgi", "Reza", ""]]}]