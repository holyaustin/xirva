[{"id": "1702.00204", "submitter": "Caitr\\'iona Ryan", "authors": "Caitriona Ryan, Jason Wyse, Nial Friel", "title": "Bayesian model selection for the latent position cluster model for\n  Social Networks", "comments": "21 pages. arXiv admin note: substantial text overlap with\n  arXiv:1308.4871", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent position cluster model is a popular model for the statistical\nanalysis of network data. This model assumes that there is an underlying latent\nspace in which the actors follow a finite mixture distribution. Moreover,\nactors which are close in this latent space are more likely to be tied by an\nedge. This is an appealing approach since it allows the model to cluster actors\nwhich consequently provides the practitioner with useful qualitative\ninformation. However, exploring the uncertainty in the number of underlying\nlatent components in the mixture distribution is a complex task. The current\nstate-of-the-art is to use an approximate form of BIC for this purpose, where\nan approximation of the log-likelihood is used instead of the true\nlog-likelihood which is unavailable. The main contribution of this paper is to\nshow that through the use of conjugate prior distributions it is possible to\nanalytically integrate out almost all of the model parameters, leaving a\nposterior distribution which depends on the allocation vector of the mixture\nmodel. This enables posterior inference over the number of components in the\nlatent mixture distribution without using trans- dimensional MCMC algorithms\nsuch as reversible jump MCMC. Our approach is compared with the\nstate-of-the-art latentnet (Krivitsky & Handcock 2015) and VBLPCM\n(Salter-Townshend & Murphy 2013) packages.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 10:55:12 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Ryan", "Caitriona", ""], ["Wyse", "Jason", ""], ["Friel", "Nial", ""]]}, {"id": "1702.00317", "submitter": "Vivak Patel", "authors": "Vivak Patel", "title": "On SGD's Failure in Practice: Characterizing and Overcoming Stalling", "comments": "17 pages, 4 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is widely used in machine learning problems\nto efficiently perform empirical risk minimization, yet, in practice, SGD is\nknown to stall before reaching the actual minimizer of the empirical risk. SGD\nstalling has often been attributed to its sensitivity to the conditioning of\nthe problem; however, as we demonstrate, SGD will stall even when applied to a\nsimple linear regression problem with unity condition number for standard\nlearning rates. Thus, in this work, we numerically demonstrate and\nmathematically argue that stalling is a crippling and generic limitation of SGD\nand its variants in practice. Once we have established the problem of stalling,\nwe generalize an existing framework for hedging against its effects, which (1)\ndeters SGD and its variants from stalling, (2) still provides convergence\nguarantees, and (3) makes SGD and its variants more practical methods for\nminimization.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 15:33:01 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 22:13:25 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Patel", "Vivak", ""]]}, {"id": "1702.00428", "submitter": "Zhipeng Liu", "authors": "Jose Blanchet and Zhipeng Liu", "title": "Malliavin-based Multilevel Monte Carlo Estimators for Densities of\n  Max-stable Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of unbiased Monte Carlo estimators for the multivariate\ndensity of max-stable fields generated by Gaussian processes. Our estimators\ntake advantage of recent results on exact simulation of max-stable fields\ncombined with identities studied in the Malliavin calculus literature and ideas\ndeveloped in the multilevel Monte Carlo literature. Our approach allows\nestimating multivariate densities of max-stable fields with precision\n$\\varepsilon $ at a computational cost of order $O\\left( \\varepsilon ^{-2}\\log\n\\log \\log \\left( 1/\\varepsilon \\right) \\right) $.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 19:31:38 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 17:17:15 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Blanchet", "Jose", ""], ["Liu", "Zhipeng", ""]]}, {"id": "1702.00434", "submitter": "Andrew Finley Dr.", "authors": "Andrew O. Finley, Abhirup Datta, Bruce C. Cook, Douglas C. Morton,\n  Hans E. Andersen, Sudipto Banerjee", "title": "Efficient algorithms for Bayesian Nearest Neighbor Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider alternate formulations of recently proposed hierarchical Nearest\nNeighbor Gaussian Process (NNGP) models (Datta et al., 2016a) for improved\nconvergence, faster computing time, and more robust and reproducible Bayesian\ninference. Algorithms are defined that improve CPU memory management and\nexploit existing high-performance numerical linear algebra libraries.\nComputational and inferential benefits are assessed for alternate NNGP\nspecifications using simulated datasets and remotely sensed light detection and\nranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit\n(TIU) in a remote portion of Interior Alaska. The resulting data product is the\nfirst statistically robust map of forest canopy for the TIU.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 19:44:14 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 19:25:08 GMT"}, {"version": "v3", "created": "Sat, 3 Mar 2018 02:24:33 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Finley", "Andrew O.", ""], ["Datta", "Abhirup", ""], ["Cook", "Bruce C.", ""], ["Morton", "Douglas C.", ""], ["Andersen", "Hans E.", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1702.00817", "submitter": "Renato J Cintra", "authors": "F. M. Bayer, R. J. Cintra", "title": "DCT-like Transform for Image Compression Requires 14 Additions Only", "comments": "6 pages, 3 figures, 1 table", "journal-ref": "Electronics Letters, Volume 48, Issue 15, pp. 919-921 (2012)", "doi": "10.1049/el.2012.1148", "report-no": null, "categories": "cs.MM cs.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-complexity 8-point orthogonal approximate DCT is introduced. The\nproposed transform requires no multiplications or bit-shift operations. The\nderived fast algorithm requires only 14 additions, less than any existing DCT\napproximation. Moreover, in several image compression scenarios, the proposed\ntransform could outperform the well-known signed DCT, as well as\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 20:08:42 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1702.01166", "submitter": "HaiYing Wang", "authors": "HaiYing Wang, Rong Zhu, Ping Ma", "title": "Optimal Subsampling for Large Sample Logistic Regression", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2017.1292914", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For massive data, the family of subsampling algorithms is popular to downsize\nthe data volume and reduce computational burden. Existing studies focus on\napproximating the ordinary least squares estimate in linear regression, where\nstatistical leverage scores are often used to define subsampling probabilities.\nIn this paper, we propose fast subsampling algorithms to efficiently\napproximate the maximum likelihood estimate in logistic regression. We first\nestablish consistency and asymptotic normality of the estimator from a general\nsubsampling algorithm, and then derive optimal subsampling probabilities that\nminimize the asymptotic mean squared error of the resultant estimator. An\nalternative minimization criterion is also proposed to further reduce the\ncomputational cost. The optimal subsampling probabilities depend on the full\ndata estimate, so we develop a two-step algorithm to approximate the optimal\nsubsampling procedure. This algorithm is computationally efficient and has a\nsignificant reduction in computing time compared to the full data approach.\nConsistency and asymptotic normality of the estimator from a two-step algorithm\nare also established. Synthetic and real data sets are used to evaluate the\npractical performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:23:46 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 17:01:40 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wang", "HaiYing", ""], ["Zhu", "Rong", ""], ["Ma", "Ping", ""]]}, {"id": "1702.01185", "submitter": "Jerrad Hampton", "authors": "Jerrad Hampton, Alireza Doostan", "title": "Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC)", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2018.03.035", "report-no": null, "categories": "stat.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a large class of orthogonal basis functions, there has been a recent\nidentification of expansion methods for computing accurate, stable\napproximations of a quantity of interest. This paper presents, within the\ncontext of uncertainty quantification, a practical implementation using basis\nadaptation, and coherence motivated sampling, which under assumptions has\nsatisfying guarantees. This implementation is referred to as Basis Adaptive\nSample Efficient Polynomial Chaos (BASE-PC). A key component of this is the use\nof anisotropic polynomial order which admits evolving global bases for\napproximation in an efficient manner, leading to consistently stable\napproximation for a practical class of smooth functionals. This fully adaptive,\nnon-intrusive method, requires no a priori information of the solution, and has\nsatisfying theoretical guarantees of recovery. A key contribution to stability\nis the use of a presented correction sampling for coherence-optimal sampling in\norder to improve stability and accuracy within the adaptive basis scheme.\nTheoretically, the method may dramatically reduce the impact of dimensionality\nin function approximation, and numerically the method is demonstrated to\nperform well on problems with dimension up to 1000.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 22:07:37 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Hampton", "Jerrad", ""], ["Doostan", "Alireza", ""]]}, {"id": "1702.01326", "submitter": "Yili Hong", "authors": "Man Zhang, Yili Hong and Narayanaswamy Balakrishnan", "title": "An Algorithm for Computing the Distribution Function of the Generalized\n  Poisson-Binomial Distribution", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Poisson-binomial distribution is useful in many applied problems in\nengineering, actuarial science, and data mining. The Poisson-binomial\ndistribution models the distribution of the sum of independent but not\nidentically distributed Bernoulli random variables whose success probabilities\nvary. In this paper, we extend the Poisson-binomial distribution to the\ngeneralized Poisson-binomial (GPB) distribution. The GPB distribution is\ndefined in cases where the Bernoulli variables can take any two arbitrary\nvalues instead of 0 and~1. The GPB distribution is useful in many areas such as\nvoting theory, actuarial science, warranty prediction, and probability theory.\nWith few previous works studying the GPB distribution, we derive the\nprobability distribution via the discrete Fourier transform of the\ncharacteristic function of the distribution. We develop an efficient algorithm\nfor computing the distribution function, which uses the fast Fourier transform.\nWe test the accuracy of the developed algorithm upon comparing it with\nenumeration-based exact method and the results from the binomial distribution.\nWe also study the computational time of the algorithm in various parameter\nsettings. Finally, we discus the factors affecting the computational efficiency\nof this algorithm, and illustrate the use of the software package.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 18:58:02 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Zhang", "Man", ""], ["Hong", "Yili", ""], ["Balakrishnan", "Narayanaswamy", ""]]}, {"id": "1702.01373", "submitter": "Jun Song", "authors": "Chenchao Zhao and Jun S. Song", "title": "Exact heat kernel on a hypersphere and its applications in kernel SVM", "comments": null, "journal-ref": null, "doi": "10.3389/fams.2018.00001", "report-no": null, "categories": "stat.ML q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many contemporary statistical learning methods assume a Euclidean feature\nspace. This paper presents a method for defining similarity based on\nhyperspherical geometry and shows that it often improves the performance of\nsupport vector machine compared to other competing similarity measures.\nSpecifically, the idea of using heat diffusion on a hypersphere to measure\nsimilarity has been previously proposed, demonstrating promising results based\non a heuristic heat kernel obtained from the zeroth order parametrix expansion;\nhowever, how well this heuristic kernel agrees with the exact hyperspherical\nheat kernel remains unknown. This paper presents a higher order parametrix\nexpansion of the heat kernel on a unit hypersphere and discusses several\nproblems associated with this expansion method. We then compare the heuristic\nkernel with an exact form of the heat kernel expressed in terms of a uniformly\nand absolutely convergent series in high-dimensional angular momentum\neigenmodes. Being a natural measure of similarity between sample points\ndwelling on a hypersphere, the exact kernel often shows superior performance in\nkernel SVM classifications applied to text mining, tumor somatic mutation\nimputation, and stock market analysis.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 04:55:14 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 04:39:59 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zhao", "Chenchao", ""], ["Song", "Jun S.", ""]]}, {"id": "1702.01418", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli, Pierre Latouche, Nial Friel", "title": "Choosing the number of groups in a latent stochastic block model for\n  dynamic networks", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent stochastic block models are flexible statistical models that are\nwidely used in social network analysis. In recent years, efforts have been made\nto extend these models to temporal dynamic networks, whereby the connections\nbetween nodes are observed at a number of different times. In this paper we\nextend the original stochastic block model by using a Markovian property to\ndescribe the evolution of nodes' cluster memberships over time. We recast the\nproblem of clustering the nodes of the network into a model-based context, and\nshow that the integrated completed likelihood can be evaluated analytically for\na number of likelihood models. Then, we propose a scalable greedy algorithm to\nmaximise this quantity, thereby estimating both the optimal partition and the\nideal number of groups in a single inferential framework. Finally we propose\napplications of our methodology to both real and artificial datasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 15:43:17 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 14:35:39 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Latouche", "Pierre", ""], ["Friel", "Nial", ""]]}, {"id": "1702.01618", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Thomas B. Sch\\\"on and Fredrik Lindsten", "title": "Learning of state-space models with highly informative observations: a\n  tempered Sequential Monte Carlo solution", "comments": null, "journal-ref": "Mechanical Systems and Signal Processing, Volume 104 (May 2018),\n  Pages 915-928", "doi": "10.1016/j.ymssp.2017.09.016", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic (or Bayesian) modeling and learning offers interesting\npossibilities for systematic representation of uncertainty using probability\ntheory. However, probabilistic learning often leads to computationally\nchallenging problems. Some problems of this type that were previously\nintractable can now be solved on standard personal computers thanks to recent\nadvances in Monte Carlo methods. In particular, for learning of unknown\nparameters in nonlinear state-space models, methods based on the particle\nfilter (a Monte Carlo method) have proven very useful. A notoriously\nchallenging problem, however, still occurs when the observations in the\nstate-space model are highly informative, i.e. when there is very little or no\nmeasurement noise present, relative to the amount of process noise. The\nparticle filter will then struggle in estimating one of the basic components\nfor probabilistic learning, namely the likelihood $p($data$|$parameters$)$. To\nthis end we suggest an algorithm which initially assumes that there is\nsubstantial amount of artificial measurement noise present. The variance of\nthis noise is sequentially decreased in an adaptive fashion such that we, in\nthe end, recover the original problem or possibly a very close approximation of\nit. The main component in our algorithm is a sequential Monte Carlo (SMC)\nsampler, which gives our proposed method a clear resemblance to the SMC^2\nmethod. Another natural link is also made to the ideas underlying the\napproximate Bayesian computation (ABC). We illustrate it with numerical\nexamples, and in particular show promising results for a challenging\nWiener-Hammerstein benchmark problem.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 14:01:20 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 10:24:40 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Svensson", "Andreas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1702.02658", "submitter": "Patrick Perry", "authors": "Wei Fu and Patrick O. Perry", "title": "Estimating the number of clusters using cross-validation", "comments": "44 pages; includes supplementary appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering methods, including k-means, require the user to specify the\nnumber of clusters as an input parameter. A variety of methods have been\ndevised to choose the number of clusters automatically, but they often rely on\nstrong modeling assumptions. This paper proposes a data-driven approach to\nestimate the number of clusters based on a novel form of cross-validation. The\nproposed method differs from ordinary cross-validation, because clustering is\nfundamentally an unsupervised learning problem. Simulation and real data\nanalysis results show that the proposed method outperforms existing methods,\nespecially in high-dimensional settings with heterogeneous or heavy-tailed\nnoise. In a yeast cell cycle dataset, the proposed method finds a parsimonious\nclustering with interpretable gene groupings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 00:11:27 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Fu", "Wei", ""], ["Perry", "Patrick O.", ""]]}, {"id": "1702.02707", "submitter": "Jiwoong Kim", "authors": "Jiwoong Kim", "title": "A Fast Algorithm for the Coordinate-wise Minimum Distance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of the minimum distance method to the linear regression model for\nestimating regression parameters is a difficult and time-consuming process due\nto the complexity of its distance function, and hence, it is computationally\nexpensive. To deal with the computational cost, this paper proposes a fast\nalgorithm which mainly uses technique of coordinate-wise minimization in order\nto estimate the regression parameters. R package based on the proposed\nalgorithm and written in Rcpp is available online.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 05:09:34 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 04:22:00 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Kim", "Jiwoong", ""]]}, {"id": "1702.03057", "submitter": "Jeremie Houssineau", "authors": "Dan Crisan and Pierre Del Moral and Jeremie Houssineau and Ajay Jasra", "title": "Unbiased Multi-index Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of Monte Carlo based approximations of expectations\nof random variables such that their laws are only available via certain\ndiscretizations. Sampling from the discretized versions of these laws can\ntypically introduce a bias. In this paper, we show how to remove that bias, by\nintroducing a new version of multi-index Monte Carlo (MIMC) that has the added\nadvantage of reducing the computational effort, relative to i.i.d. sampling\nfrom the most precise discretization, for a given level of error. We cover\nextensions of results regarding variance and optimality criteria for the new\napproach. We apply the methodology to the problem of computing an unbiased\nmollified version of the solution of a partial differential equation with\nrandom coefficients. A second application concerns the Bayesian inference (the\nsmoothing problem) of an infinite dimensional signal modelled by the solution\nof a stochastic partial differential equation that is observed on a discrete\nspace grid and at discrete times. Both applications are complemented by\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 03:25:14 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 10:14:43 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 01:36:32 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Crisan", "Dan", ""], ["Del Moral", "Pierre", ""], ["Houssineau", "Jeremie", ""], ["Jasra", "Ajay", ""]]}, {"id": "1702.03126", "submitter": "David Warne", "authors": "David J. Warne (1) and Ruth E. Baker (2) and Matthew J. Simpson (1)\n  ((1) Queensland University of Technology, (2) University of Oxford)", "title": "Multilevel rejection sampling for approximate Bayesian computation", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2018.02.009", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free methods, such as approximate Bayesian computation, are\npowerful tools for practical inference problems with intractable likelihood\nfunctions. Markov chain Monte Carlo and sequential Monte Carlo variants of\napproximate Bayesian computation can be effective techniques for sampling\nposterior distributions in an approximate Bayesian computation setting.\nHowever, without careful consideration of convergence criteria and selection of\nproposal kernels, such methods can lead to very biased inference or\ncomputationally inefficient sampling. In contrast, rejection sampling for\napproximate Bayesian computation, despite being computationally intensive,\nresults in independent, identically distributed samples from the approximated\nposterior. An alternative method is proposed for the acceleration of\nlikelihood-free Bayesian inference that applies multilevel Monte Carlo variance\nreduction techniques directly to rejection sampling. The resulting method\nretains the accuracy advantages of rejection sampling while significantly\nimproving the computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 10:44:23 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 01:36:46 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 04:27:36 GMT"}, {"version": "v4", "created": "Wed, 28 Feb 2018 13:15:21 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Warne", "David J.", "", "Queensland University of Technology"], ["Baker", "Ruth E.", "", "University of Oxford"], ["Simpson", "Matthew J.", "", "Queensland University of Technology"]]}, {"id": "1702.03146", "submitter": "Joaqu\\'in M\\'iguez", "authors": "Joaquin Miguez, Ines P. Mari\\~no, Manuel A. Vazquez", "title": "Analysis of a nonlinear importance sampling scheme for Bayesian\n  parameter estimation in state-space models", "comments": "arXiv admin note: text overlap with arXiv:1512.03976", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian estimation of the unknown parameters of state-space (dynamical)\nsystems has received considerable attention over the past decade, with a\nhandful of powerful algorithms being introduced. In this paper we tackle the\ntheoretical analysis of the recently proposed {\\it nonlinear} population Monte\nCarlo (NPMC). This is an iterative importance sampling scheme whose key\nfeatures, compared to conventional importance samplers, are (i) the approximate\ncomputation of the importance weights (IWs) assigned to the Monte Carlo samples\nand (ii) the nonlinear transformation of these IWs in order to prevent the\ndegeneracy problem that flaws the performance of conventional importance\nsamplers. The contribution of the present paper is a rigorous proof of\nconvergence of the nonlinear IS (NIS) scheme as the number of Monte Carlo\nsamples, $M$, increases. Our analysis reveals that the NIS approximation errors\nconverge to 0 almost surely and with the optimal Monte Carlo rate of\n$M^{-\\frac{1}{2}}$. Moreover, we prove that this is achieved even when the mean\nestimation error of the IWs remains constant, a property that has been termed\n{\\it exact approximation} in the Markov chain Monte Carlo literature. We\nillustrate these theoretical results by means of a computer simulation example\ninvolving the estimation of the parameters of a state-space model typically\nused for target tracking.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 12:26:52 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Miguez", "Joaquin", ""], ["Mari\u00f1o", "Ines P.", ""], ["Vazquez", "Manuel A.", ""]]}, {"id": "1702.03673", "submitter": "Jon Cockayne", "authors": "Jon Cockayne, Chris Oates, Tim Sullivan, Mark Girolami", "title": "Bayesian Probabilistic Numerical Methods", "comments": null, "journal-ref": "SIAM Review 61(4):756--789, 2019", "doi": "10.1137/17M1139357", "report-no": null, "categories": "stat.ME cs.NA math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergent field of probabilistic numerics has thus far lacked clear\nstatistical principals. This paper establishes Bayesian probabilistic numerical\nmethods as those which can be cast as solutions to certain inverse problems\nwithin the Bayesian framework. This allows us to establish general conditions\nunder which Bayesian probabilistic numerical methods are well-defined,\nencompassing both non-linear and non-Gaussian models. For general computation,\na numerical approximation scheme is proposed and its asymptotic convergence\nestablished. The theoretical development is then extended to pipelines of\ncomputation, wherein probabilistic numerical methods are composed to solve more\nchallenging numerical tasks. The contribution highlights an important research\nfrontier at the interface of numerical analysis and uncertainty quantification,\nwith a challenging industrial application presented.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 08:52:58 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 13:58:53 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Cockayne", "Jon", ""], ["Oates", "Chris", ""], ["Sullivan", "Tim", ""], ["Girolami", "Mark", ""]]}, {"id": "1702.03891", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio G\\'omez-Rubio and Francisco Palm\\'i-Perales", "title": "Spatial Models with the Integrated Nested Laplace Approximation within\n  Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Integrated Nested Laplace Approximation (INLA) is a convenient way to\nobtain approximations to the posterior marginals for parameters in Bayesian\nhierarchical models when the latent effects can be expressed as a Gaussian\nMarkov Random Field (GMRF). In addition, its implementation in the R-INLA\npackage for the R statistical software provides an easy way to fit models using\nINLA in practice. R-INLA implements a number of widely used latent models,\nincluding several spatial models. In addition, R-INLA can fit models in a\nfraction of the time than other computer intensive methods (e.g. Markov Chain\nMonte Carlo) take to fit the same model.\n  Although INLA provides a fast approximation to the marginals of the model\nparameters, it is difficult to use it with models not implemented in R-INLA. It\nis also difficult to make multivariate posterior inference on the parameters of\nthe model as INLA focuses on the posterior marginals and not the joint\nposterior distribution.\n  In this paper we describe how to use INLA within the Metropolis-Hastings\nalgorithm to fit spatial models and estimate the joint posterior distribution\nof a reduced number of parameters. We will illustrate the benefits of this new\nmethod with two examples on spatial econometrics and disease mapping where\ncomplex spatial models with several spatial structures need to be fitted.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 17:23:02 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["G\u00f3mez-Rubio", "Virgilio", ""], ["Palm\u00ed-Perales", "Francisco", ""]]}, {"id": "1702.04391", "submitter": "Fabio M. Bayer Ph.D", "authors": "Bruna Gregory Palm and F\\'abio M. Bayer", "title": "Bootstrap-based inferential improvements in beta autoregressive moving\n  average model", "comments": "Accepted paper", "journal-ref": "Communications in Statistics - Simulation and Computation, 2017", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the issue of performing accurate small sample inference in beta\nautoregressive moving average model, which is useful for modeling and\nforecasting continuous variables that assumes values in the interval $(0,1)$.\nThe inferences based on conditional maximum likelihood estimation have good\nasymptotic properties, but their performances in small samples may be poor.\nThis way, we propose bootstrap bias corrections of the point estimators and\ndifferent bootstrap strategies for confidence interval improvements. Our Monte\nCarlo simulations show that finite sample inference based on bootstrap\ncorrections is much more reliable than the usual inferences. We also presented\nan empirical application.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 21:20:23 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Palm", "Bruna Gregory", ""], ["Bayer", "F\u00e1bio M.", ""]]}, {"id": "1702.04561", "submitter": "Janek Thomas", "authors": "Janek Thomas and Tobias Hepp and Andreas Mayr and Bernd Bischl", "title": "Probing for sparse and fast variable selection with model-based boosting", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new variable selection method based on model-based gradient\nboosting and randomly permuted variables. Model-based boosting is a tool to fit\na statistical model while performing variable selection at the same time. A\ndrawback of the fitting lies in the need of multiple model fits on slightly\naltered data (e.g. cross-validation or bootstrap) to find the optimal number of\nboosting iterations and prevent overfitting. In our proposed approach, we\naugment the data set with randomly permuted versions of the true variables, so\ncalled shadow variables, and stop the step-wise fitting as soon as such a\nvariable would be added to the model. This allows variable selection in a\nsingle fit of the model without requiring further parameter tuning. We show\nthat our probing approach can compete with state-of-the-art selection methods\nlike stability selection in a high-dimensional classification benchmark and\napply it on gene expression data for the estimation of riboflavin production of\nBacillus subtilis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 11:52:14 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Thomas", "Janek", ""], ["Hepp", "Tobias", ""], ["Mayr", "Andreas", ""], ["Bischl", "Bernd", ""]]}, {"id": "1702.05462", "submitter": "Fabrizio Leisen", "authors": "Laurentiu Hinoveanu, Fabrizio Leisen and Cristiano Villa", "title": "Objective Bayesian Analysis for Change Point Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a loss-based approach to change point analysis. In\nparticular, we look at the problem from two perspectives. The first focuses on\nthe definition of a prior when the number of change points is known a priori.\nThe second contribution aims to estimate the number of change points by using a\nloss-based approach recently introduced in the literature. The latter considers\nchange point estimation as a model selection exercise. We show the performance\nof the proposed approach on simulated data and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 18:06:27 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 13:58:48 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hinoveanu", "Laurentiu", ""], ["Leisen", "Fabrizio", ""], ["Villa", "Cristiano", ""]]}, {"id": "1702.05518", "submitter": "Andrew Brown", "authors": "D. Andrew Brown, Christopher S. McMahan, and Stella Watson Self", "title": "Sampling strategies for fast updating of Gaussian Markov random fields", "comments": "Revised introduction and expanded numerical examples to include Rcpp\n  and parallel implementation. Supplementary material available from the\n  authors. 38 pages, 8 figures", "journal-ref": "The American Statistician, 2019", "doi": "10.1080/00031305.2019.1595144", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Markov random fields (GMRFs) are popular for modeling dependence in\nlarge areal datasets due to their ease of interpretation and computational\nconvenience afforded by the sparse precision matrices needed for random\nvariable generation. Typically in Bayesian computation, GMRFs are updated\njointly in a block Gibbs sampler or componentwise in a single-site sampler via\nthe full conditional distributions. The former approach can speed convergence\nby updating correlated variables all at once, while the latter avoids solving\nlarge matrices. We consider a sampling approach in which the underlying graph\ncan be cut so that conditionally independent sites are updated simultaneously.\nThis algorithm allows a practitioner to parallelize updates of subsets of\nlocations or to take advantage of `vectorized' calculations in a high-level\nlanguage such as R. Through both simulated and real data, we demonstrate\ncomputational savings that can be achieved versus both single-site and block\nupdating, regardless of whether the data are on a regular or an irregular\nlattice. The approach provides a good compromise between statistical and\ncomputational efficiency and is accessible to statisticians without expertise\nin numerical analysis or advanced computing.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 21:16:40 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 23:40:45 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 15:22:39 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Brown", "D. Andrew", ""], ["McMahan", "Christopher S.", ""], ["Self", "Stella Watson", ""]]}, {"id": "1702.05546", "submitter": "Bin Liu", "authors": "Bin Liu, Giuseppe Vinci, Adam C. Snyder, Robert E. Kass", "title": "A Sequential Scheme for Large Scale Bayesian Multiple Testing", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of large scale multiple testing arises in many contexts,\nincluding testing for pairwise interaction among large numbers of neurons. With\nadvances in technologies, it has become common to record from hundreds of\nneurons simultaneously, and this number is growing quickly, so that the number\nof pairwise tests can be very large. It is important to control the rate at\nwhich false positives occur. In addition, there is sometimes information that\naffects the probability of a positive result for any given pair. In the case of\nneurons, they are more likely to have correlated activity when they are close\ntogether, and when they respond similarly to various stimuli. Recently a method\nwas developed to control false positives when covariate information, such as\ndistances between pairs of neurons, is available. This method, however, relies\non computationally-intensive Markov Chain Monte Carlo (MCMC). Here we develop\nan alternative, based on Sequential Monte Carlo, which scales well with the\nsize of the dataset. This scheme considers data items sequentially, with\nrelevant probabilities being updated at each step. Simulation experiments\ndemonstrate that the proposed algorithm delivers results as accurately as the\nprevious MCMC method with only a single pass through the data. We illustrate\nthe method by using it to analyze neural recordings from extrastriate cortex in\na macaque monkey.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 00:04:25 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 05:04:00 GMT"}, {"version": "v3", "created": "Sat, 28 Oct 2017 15:59:16 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 00:23:36 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Liu", "Bin", ""], ["Vinci", "Giuseppe", ""], ["Snyder", "Adam C.", ""], ["Kass", "Robert E.", ""]]}, {"id": "1702.05698", "submitter": "Wei Xiao", "authors": "Wei Xiao, Xiaolin Huang, Jorge Silva, Saba Emrani and Arin Chaudhuri", "title": "Online Robust Principal Component Analysis with Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust PCA methods are typically batch algorithms which requires loading all\nobservations into memory before processing. This makes them inefficient to\nprocess big data. In this paper, we develop an efficient online robust\nprincipal component methods, namely online moving window robust principal\ncomponent analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can\nsuccessfully track not only slowly changing subspace but also abruptly changed\nsubspace. By embedding hypothesis testing into the algorithm, OMWRPCA can\ndetect change points of the underlying subspaces. Extensive simulation studies\ndemonstrate the superior performance of OMWRPCA compared with other\nstate-of-art approaches. We also apply the algorithm for real-time background\nsubtraction of surveillance video.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 04:08:18 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 19:49:02 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Xiao", "Wei", ""], ["Huang", "Xiaolin", ""], ["Silva", "Jorge", ""], ["Emrani", "Saba", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1702.06407", "submitter": "John V Monaco", "authors": "John V. Monaco, Malka Gorfine, Li Hsu", "title": "General Semiparametric Shared Frailty Model Estimation and Simulation\n  with frailtySurv", "comments": null, "journal-ref": "Monaco, J., Gorfine, M., & Hsu, L. (2018). General Semiparametric\n  Shared Frailty Model: Estimation and Simulation with frailtySurv. Journal of\n  Statistical Software, 86(4), 1 - 42.\n  doi:http://dx.doi.org/10.18637/jss.v086.i04", "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R package frailtySurv for simulating and fitting semi-parametric shared\nfrailty models is introduced. Package frailtySurv implements semi-parametric\nconsistent estimators for a variety of frailty distributions, including gamma,\nlog-normal, inverse Gaussian and power variance function, and provides\nconsistent estimators of the standard errors of the parameters' estimators. The\nparameters' estimators are asymptotically normally distributed, and therefore\nstatistical inference based on the results of this package, such as hypothesis\ntesting and confidence intervals, can be performed using the normal\ndistribution. Extensive simulations demonstrate the flexibility and correct\nimplementation of the estimator. Two case studies performed with publicly\navailable datasets demonstrate applicability of the package. In the Diabetic\nRetinopathy Study, the onset of blindness is clustered by patient, and in a\nlarge hard drive failure dataset, failure times are thought to be clustered by\nthe hard drive manufacturer and model.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 14:40:29 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 14:07:34 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 22:01:05 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Monaco", "John V.", ""], ["Gorfine", "Malka", ""], ["Hsu", "Li", ""]]}, {"id": "1702.06488", "submitter": "Kaizheng Wang", "authors": "Jianqing Fan, Dong Wang, Kaizheng Wang, Ziwei Zhu", "title": "Distributed Estimation of Principal Eigenspaces", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is fundamental to statistical machine\nlearning. It extracts latent principal factors that contribute to the most\nvariation of the data. When data are stored across multiple machines, however,\ncommunication cost can prohibit the computation of PCA in a central location\nand distributed algorithms for PCA are thus needed. This paper proposes and\nstudies a distributed PCA algorithm: each node machine computes the top $K$\neigenvectors and transmits them to the central server; the central server then\naggregates the information from all the node machines and conducts a PCA based\non the aggregated information. We investigate the bias and variance for the\nresulting distributed estimator of the top $K$ eigenvectors. In particular, we\nshow that for distributions with symmetric innovation, the empirical top\neigenspaces are unbiased and hence the distributed PCA is \"unbiased\". We derive\nthe rate of convergence for distributed PCA estimators, which depends\nexplicitly on the effective rank of covariance, eigen-gap, and the number of\nmachines. We show that when the number of machines is not unreasonably large,\nthe distributed PCA performs as well as the whole sample PCA, even without full\naccess of whole data. The theoretical results are verified by an extensive\nsimulation study. We also extend our analysis to the heterogeneous case where\nthe population covariance matrices are different across local machines but\nshare similar top eigen-structures.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 17:38:26 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 19:01:53 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 18:24:30 GMT"}, {"version": "v4", "created": "Wed, 10 Jan 2018 14:53:30 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Fan", "Jianqing", ""], ["Wang", "Dong", ""], ["Wang", "Kaizheng", ""], ["Zhu", "Ziwei", ""]]}, {"id": "1702.06632", "submitter": "John Lombard", "authors": "John Lombard", "title": "Novel Algorithms for Sampling Abstract Simplicial Complexes", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide dual algorithms for sampling the space of abstract simplicial\ncomplexes on a fixed number of vertices. We develop a generative and\ndescriptive sampler designed with heuristics to help balance the combinatorial\nmultiplicities of the states and more widely sample across the space of\nnonisomorphic complexes. We provide a formula for the exact probabilities with\nwhich this algorithm will produce a requested labeled state, and compare with\nan existing benchmark. We also design a highly conductive local ergodic random\nwalk with known transition probabilities. We characterize the autocorrelation\nof the walk, and numerically test it against our sampler to illustrate its\nefficacy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 00:43:01 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2018 01:13:35 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Lombard", "John", ""]]}, {"id": "1702.07094", "submitter": "William Nicholson", "authors": "William Nicholson, David Matteson, Jacob Bien", "title": "BigVAR: Tools for Modeling Sparse High-Dimensional Multivariate Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package BigVAR allows for the simultaneous estimation of\nhigh-dimensional time series by applying structured penalties to the\nconventional vector autoregression (VAR) and vector autoregression with\nexogenous variables (VARX) frameworks. Our methods can be utilized in many\nforecasting applications that make use of time-dependent data such as\nmacroeconomics, finance, and internet traffic. Our package extends solution\nalgorithms from the machine learning and signal processing literatures to a\ntime dependent setting: selecting the regularization parameter by sequential\ncross validation and provides substantial improvements in forecasting\nperformance over conventional methods. We offer a user-friendly interface that\nutilizes R's s4 object class structure which makes our methodology easily\naccessible to practicioners.\n  In this paper, we present an overview of our notation, the models that\ncomprise BigVAR, and the functionality of our package with a detailed example\nusing publicly available macroeconomic data. In addition, we present a\nsimulation study comparing the performance of several procedures that refit the\nsupport selected by a BigVAR procedure according to several variants of least\nsquares and conclude that refitting generally degrades forecast performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 04:40:41 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Nicholson", "William", ""], ["Matteson", "David", ""], ["Bien", "Jacob", ""]]}, {"id": "1702.07400", "submitter": "Anindya Bhadra", "authors": "Anindya Bhadra, Jyotishka Datta, Nicholas G. Polson, Brandon Willard", "title": "Horseshoe Regularization for Feature Subset Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature subset selection arises in many high-dimensional applications of\nstatistics, such as compressed sensing and genomics. The $\\ell_0$ penalty is\nideal for this task, the caveat being it requires the NP-hard combinatorial\nevaluation of all models. A recent area of considerable interest is to develop\nefficient algorithms to fit models with a non-convex $\\ell_\\gamma$ penalty for\n$\\gamma\\in (0,1)$, which results in sparser models than the convex $\\ell_1$ or\nlasso penalty, but is harder to fit. We propose an alternative, termed the\nhorseshoe regularization penalty for feature subset selection, and demonstrate\nits theoretical and computational advantages. The distinguishing feature from\nexisting non-convex optimization approaches is a full probabilistic\nrepresentation of the penalty as the negative of the logarithm of a suitable\nprior, which in turn enables efficient expectation-maximization and local\nlinear approximation algorithms for optimization and MCMC for uncertainty\nquantification. In synthetic and real data, the resulting algorithms provide\nbetter statistical performance, and the computation requires a fraction of time\nof state-of-the-art non-convex solvers.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 21:37:06 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 18:35:34 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Bhadra", "Anindya", ""], ["Datta", "Jyotishka", ""], ["Polson", "Nicholas G.", ""], ["Willard", "Brandon", ""]]}, {"id": "1702.07662", "submitter": "Clement Lee", "authors": "Clement Lee and Andrew Garbett and Darren J. Wilkinson", "title": "A Network Epidemic Model for Online Community Commissioning Data", "comments": "28 pages, 9 figures, 2 tables", "journal-ref": null, "doi": "10.1007/s11222-017-9770-6", "report-no": null, "categories": "stat.CO cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical model assuming a preferential attachment network, which is\ngenerated by adding nodes sequentially according to a few simple rules, usually\ndescribes real-life networks better than a model assuming, for example, a\nBernoulli random graph, in which any two nodes have the same probability of\nbeing connected, does. Therefore, to study the propogation of \"infection\"\nacross a social network, we propose a network epidemic model by combining a\nstochastic epidemic model and a preferential attachment model. A simulation\nstudy based on the subsequent Markov Chain Monte Carlo algorithm reveals an\nidentifiability issue with the model parameters. Finally, the network epidemic\nmodel is applied to a set of online commissioning data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:01:59 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 11:03:01 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Lee", "Clement", ""], ["Garbett", "Andrew", ""], ["Wilkinson", "Darren J.", ""]]}, {"id": "1702.07685", "submitter": "Rebecka J\\\"ornsten", "authors": "Jonatan Kallus, Jose Sanchez, Alexandra Jauhiainen, Sven Nelander,\n  Rebecka J\\\"ornsten", "title": "ROPE: high-dimensional network modeling with robust control of edge FDR", "comments": "Main paper contains 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network modeling has become increasingly popular for analyzing genomic data,\nto aid in the interpretation and discovery of possible mechanistic components\nand therapeutic targets. However, genomic-scale networks are high-dimensional\nmodels and are usually estimated from a relatively small number of samples.\nTherefore, their usefulness is hampered by estimation instability. In addition,\nthe complexity of the models is controlled by one or more penalization (tuning)\nparameters where small changes to these can lead to vastly different networks,\nthus making interpretation of models difficult. This necessitates the\ndevelopment of techniques to produce robust network models accompanied by\nestimation quality assessments.\n  We introduce Resampling of Penalized Estimates (ROPE): a novel statistical\nmethod for robust network modeling. The method utilizes resampling-based\nnetwork estimation and integrates results from several levels of penalization\nthrough a constrained, over-dispersed beta-binomial mixture model. ROPE\nprovides robust False Discovery Rate (FDR) control of network estimates and\neach edge is assigned a measure of validity, the q-value, corresponding to the\nFDR-level for which the edge would be included in the network model. We apply\nROPE to several simulated data sets as well as genomic data from The Cancer\nGenome Atlas. We show that ROPE outperforms state-of-the-art methods in terms\nof FDR control and robust performance across data sets. We illustrate how to\nuse ROPE to make a principled model selection for which genomic associations to\nstudy further. ROPE is available as an R package on CRAN.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:54:23 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Kallus", "Jonatan", ""], ["Sanchez", "Jose", ""], ["Jauhiainen", "Alexandra", ""], ["Nelander", "Sven", ""], ["J\u00f6rnsten", "Rebecka", ""]]}, {"id": "1702.07830", "submitter": "Hadi Meidani", "authors": "Negin Alemazkoor, Hadi Meidani", "title": "A Near-Optimal Sampling Strategy for Sparse Recovery of Polynomial Chaos\n  Expansions", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2018.05.025", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sampling has become a widely used approach to construct\npolynomial chaos surrogates when the number of available simulation samples is\nlimited. Originally, these expensive simulation samples would be obtained at\nrandom locations in the parameter space. It was later shown that the choice of\nsample locations could significantly impact the accuracy of resulting\nsurrogates. This motivated new sampling strategies or design-of-experiment\napproaches, such as coherence-optimal sampling, which aim at improving the\ncoherence property. In this paper, we propose a sampling strategy that can\nidentify near-optimal sample locations that lead to improvement in\nlocal-coherence property and also enhancement of cross-correlation properties\nof measurement matrices. We provide theoretical motivations for the proposed\nsampling strategy along with several numerical examples that show that our\nnear-optimal sampling strategy produces substantially more accurate results,\ncompared to other sampling strategies.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 03:46:20 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 21:32:21 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Alemazkoor", "Negin", ""], ["Meidani", "Hadi", ""]]}, {"id": "1702.07930", "submitter": "Renliang Gu", "authors": "Renliang Gu and Aleksandar Dogand\\v{z}i\\'c", "title": "Upper-Bounding the Regularization Constant for Convex Sparse Signal\n  Reconstruction", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider reconstructing a signal $x$ by minimizing a weighted sum of a convex\ndifferentiable negative log-likelihood (NLL) (data-fidelity) term and a convex\nregularization term that imposes a convex-set constraint on $x$ and enforces\nits sparsity using $\\ell_1$-norm analysis regularization. We compute upper\nbounds on the regularization tuning constant beyond which the regularization\nterm overwhelmingly dominates the NLL term so that the set of minimum points of\nthe objective function does not change. Necessary and sufficient conditions for\nirrelevance of sparse signal regularization and a condition for the existence\nof finite upper bounds are established. We formulate an optimization problem\nfor finding these bounds when the regularization term can be globally minimized\nby a feasible $x$ and also develop an alternating direction method of\nmultipliers (ADMM) type method for their computation. Simulation examples show\nthat the derived and empirical bounds match.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 17:47:33 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Gu", "Renliang", ""], ["Dogand\u017ei\u0107", "Aleksandar", ""]]}, {"id": "1702.08061", "submitter": "Michael Roth", "authors": "Michael Roth, Gustaf Hendeby, Carsten Fritsche, Fredrik Gustafsson", "title": "The Ensemble Kalman Filter: A Signal Processing Perspective", "comments": null, "journal-ref": "EURASIP J. Adv. Signal Process. (2017) 2017: 56", "doi": "10.1186/s13634-017-0492-x", "report-no": null, "categories": "stat.ME cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ensemble Kalman filter (EnKF) is a Monte Carlo based implementation of\nthe Kalman filter (KF) for extremely high-dimensional, possibly nonlinear and\nnon-Gaussian state estimation problems. Its ability to handle state dimensions\nin the order of millions has made the EnKF a popular algorithm in different\ngeoscientific disciplines. Despite a similarly vital need for scalable\nalgorithms in signal processing, e.g., to make sense of the ever increasing\namount of sensor data, the EnKF is hardly discussed in our field.\n  This self-contained review paper is aimed at signal processing researchers\nand provides all the knowledge to get started with the EnKF. The algorithm is\nderived in a KF framework, without the often encountered geoscientific\nterminology. Algorithmic challenges and required extensions of the EnKF are\nprovided, as well as relations to sigma-point KF and particle filters. The\nrelevant EnKF literature is summarized in an extensive survey and unique\nsimulation examples, including popular benchmark problems, complement the\ntheory with practical insights. The signal processing perspective highlights\nnew directions of research and facilitates the exchange of potentially\nbeneficial ideas, both for the EnKF and high-dimensional nonlinear and\nnon-Gaussian filtering in general.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 17:50:28 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 09:59:57 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Roth", "Michael", ""], ["Hendeby", "Gustaf", ""], ["Fritsche", "Carsten", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1702.08140", "submitter": "Adrien Ickowicz", "authors": "Adrien Ickowicz and Jessica H. Ford and Keith R. Hayes", "title": "A mixture model approach to infer land-use influence on point referenced\n  water quality", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of water quality across space and time is of considerable\ninterest for both agricultural and public health reasons. The standard method\nto assess the water quality of a catchment, or a group of catchments, usually\ninvolves collecting point measurements of water quality and other additional\ninformation such as the date and time of measurements, rainfall amounts, the\nland-use and soil-type of the catchment and the elevation. Some of this\nauxiliary information will be point data, measured at the exact location,\nwhereas other such as land-use will be areal data often in a compositional\nformat. Two problems arise if analysts try to incorporate this information into\na statistical model in order to predict (for example) the influence of land-use\non water quality. First is the spatial change of support problem that arises\nwhen using areal data to predict outcomes at point locations. Secondly, the\nphysical process driving water quality is not compositional, rather it is the\nobservation process that provides compositional data. In this paper we present\nan approach that accounts for these two issues by using a latent variable to\nidentify the land-use that most likely influences water quality. This latent\nvariable is used in a spatial mixture model to help estimate the influence of\nland-use on water quality. We demonstrate the potential of this approach with\ndata from a water quality research study in the Mount Lofty range, in South\nAustralia.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 04:17:36 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Ickowicz", "Adrien", ""], ["Ford", "Jessica H.", ""], ["Hayes", "Keith R.", ""]]}, {"id": "1702.08185", "submitter": "Andreas Mayr", "authors": "Andreas Mayr, Benjamin Hofner, Elisabeth Waldmann, Tobias Hepp, Olaf\n  Gefeller, Matthias Schmid", "title": "An update on statistical boosting in biomedicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical boosting algorithms have triggered a lot of research during the\nlast decade. They combine a powerful machine-learning approach with classical\nstatistical modelling, offering various practical advantages like automated\nvariable selection and implicit regularization of effect estimates. They are\nextremely flexible, as the underlying base-learners (regression functions\ndefining the type of effect for the explanatory variables) can be combined with\nany kind of loss function (target function to be optimized, defining the type\nof regression setting). In this review article, we highlight the most recent\nmethodological developments on statistical boosting regarding variable\nselection, functional regression and advanced time-to-event modelling.\nAdditionally, we provide a short overview on relevant applications of\nstatistical boosting in biomedicine.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 08:33:26 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Mayr", "Andreas", ""], ["Hofner", "Benjamin", ""], ["Waldmann", "Elisabeth", ""], ["Hepp", "Tobias", ""], ["Gefeller", "Olaf", ""], ["Schmid", "Matthias", ""]]}, {"id": "1702.08188", "submitter": "Florian Gerber", "authors": "Florian Gerber, Kaspar M\\\"osinger and Reinhard Furrer", "title": "dotCall64: An Efficient Interface to Compiled C/C++ and Fortran Code\n  Supporting Long Vectors", "comments": "17 pages", "journal-ref": "SoftwareX, 7, 217-221, 2018", "doi": "10.1016/j.softx.2018.06.002", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R functions .C() and .Fortran() can be used to call compiled C/C++ and\nFortran code from R. This so-called foreign function interface is convenient,\nsince it does not require any interactions with the C API of R. However, it\ndoes not support long vectors (i.e., vectors of more than 2^31 elements). To\novercome this limitation, the R package dotCall64 provides .C64(), which can be\nused to call compiled C/C++ and Fortran functions. It transparently supports\nlong vectors and does the necessary castings to pass numeric R vectors to\n64-bit integer arguments of the compiled code. Moreover, .C64() features a\nmechanism to avoid unnecessary copies of function arguments, making it\nefficient in terms of speed and memory usage.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 08:42:49 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Gerber", "Florian", ""], ["M\u00f6singer", "Kaspar", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1702.08248", "submitter": "Olivier Bachem", "authors": "Olivier Bachem, Mario Lucic, Andreas Krause", "title": "Scalable k-Means Clustering via Lightweight Coresets", "comments": "To appear in the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining (KDD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coresets are compact representations of data sets such that models trained on\na coreset are provably competitive with models trained on the full data set. As\nsuch, they have been successfully used to scale up clustering models to massive\ndata sets. While existing approaches generally only allow for multiplicative\napproximation errors, we propose a novel notion of lightweight coresets that\nallows for both multiplicative and additive errors. We provide a single\nalgorithm to construct lightweight coresets for k-means clustering as well as\nsoft and hard Bregman clustering. The algorithm is substantially faster than\nexisting constructions, embarrassingly parallel, and the resulting coresets are\nsmaller. We further show that the proposed approach naturally generalizes to\nstatistical k-means clustering and that, compared to existing results, it can\nbe used to compute smaller summaries for empirical risk minimization. In\nextensive experiments, we demonstrate that the proposed algorithm outperforms\nexisting data summarization strategies in practice.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 12:03:01 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 21:49:52 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Bachem", "Olivier", ""], ["Lucic", "Mario", ""], ["Krause", "Andreas", ""]]}, {"id": "1702.08251", "submitter": "Thomas House", "authors": "Thomas House", "title": "Hessian corrections to Hybrid Monte Carlo", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for the introduction of second-order derivatives of the log\nlikelihood into HMC algorithms is introduced, which does not require the\nHessian to be evaluated at each leapfrog step but only at the start and end of\ntrajectories.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 12:12:46 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["House", "Thomas", ""]]}, {"id": "1702.08397", "submitter": "Manon Michel", "authors": "Manon Michel, Alain Durmus and St\\'ephane S\\'en\\'ecal", "title": "Forward Event-Chain Monte Carlo: Fast sampling by randomness control in\n  irreversible Markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irreversible and rejection-free Monte Carlo methods, recently developed in\nPhysics under the name Event-Chain and known in Statistics as Piecewise\nDeterministic Monte Carlo (PDMC), have proven to produce clear acceleration\nover standard Monte Carlo methods, thanks to the reduction of their random-walk\nbehavior. However, while applying such schemes to standard statistical models,\none generally needs to introduce an additional randomization for sake of\ncorrectness. We propose here a new class of Event-Chain Monte Carlo methods\nthat reduces this extra-randomization to a bare minimum. We compare the\nefficiency of this new methodology to standard PDMC and Monte Carlo methods.\nAccelerations up to several magnitudes and reduced dimensional scalings are\nexhibited.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:43:59 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 14:41:11 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 19:07:53 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 16:41:43 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Michel", "Manon", ""], ["Durmus", "Alain", ""], ["S\u00e9n\u00e9cal", "St\u00e9phane", ""]]}, {"id": "1702.08446", "submitter": "Emilio Zappa", "authors": "Emilio Zappa, Miranda Holmes-Cerfon, Jonathan Goodman", "title": "Monte Carlo on manifolds: sampling densities and integrating functions", "comments": "New version. 32 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze some Monte Carlo methods for manifolds in Euclidean\nspace defined by equality and inequality constraints. First, we give an MCMC\nsampler for probability distributions defined by un-normalized densities on\nsuch manifolds. The sampler uses a specific orthogonal projection to the\nsurface that requires only information about the tangent space to the manifold,\nobtainable from first derivatives of the constraint functions, hence avoiding\nthe need for curvature information or second derivatives. Second, we use the\nsampler to develop a multi-stage algorithm to compute integrals over such\nmanifolds. We provide single-run error estimates that avoid the need for\nmultiple independent runs. Computational experiments on various test problems\nshow that the algorithms and error estimates work in practice. The method is\napplied to compute the entropies of different sticky hard sphere systems. These\npredict the temperature or interaction energy at which loops of hard sticky\nspheres become preferable to chains.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 17:50:02 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 11:56:47 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Zappa", "Emilio", ""], ["Holmes-Cerfon", "Miranda", ""], ["Goodman", "Jonathan", ""]]}, {"id": "1702.08572", "submitter": "Richard Minkah", "authors": "Richard Minkah and Tertius de Wet", "title": "Comparison of Confidence Interval Estimators: an Index Approach", "comments": "26 pages, 1 figure and 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical problems, several estimators are usually available for\ninterval estimation of a parameter of interest, and hence, the selection of an\nappropriate estimator is important. The criterion for a good estimator is to\nhave a high coverage probability close to the nominal level and a shorter\ninterval length. However, these two concepts are in opposition to each other:\nhigh and low coverages are associated with longer and shorter interval lengths\nrespectively. Some methods, such as bootstrap calibration, modify the nominal\nlevel to improve the coverage and thereby allow the selection of intervals\nbased on interval lengths only. Nonetheless, these methods are computationally\nexpensive. In this paper, we propose an index which offers an easy to compute\napproach of comparing confidence interval estimators based on a compromise\nbetween the coverage probability and the confidence interval length. We\nillustrate that the confidence interval index has range of values within the\nneighborhood of the range of the coverage probability, [0,1]. In addition, a\ngood confidence interval estimator has an index value approaching 1; and a bad\nconfidence interval has an index value approaching 0. A simulation study was\nconducted to assess the finite sample performance of the index. The proposed\nindex is illustrated with a practical example from the literature\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 22:52:29 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 11:51:50 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 14:54:13 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Minkah", "Richard", ""], ["de Wet", "Tertius", ""]]}, {"id": "1702.08738", "submitter": "Nabil Kahale", "authors": "Nabil Kahale", "title": "Efficient simulation of high dimensional Gaussian vectors", "comments": "21 pages. An earlier version has been posted on SSRN on July 14, 2016", "journal-ref": "Mathematics of Operations Research 2019 44:1, 58-73", "doi": "10.1287/moor.2017.0914", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a Markov chain Monte Carlo method to approximately simulate a\ncentered d-dimensional Gaussian vector X with given covariance matrix. The\nstandard Monte Carlo method is based on the Cholesky decomposition, which takes\ncubic time and has quadratic storage cost in d. In contrast, the storage cost\nof our algorithm is linear in d. We give a bound on the quadractic Wasserstein\ndistance between the distribution of our sample and the target distribution.\nOur method can be used to estimate the expectation of h(X), where h is a\nreal-valued function of d variables. Under certain conditions, we show that the\nmean square error of our method is inversely proportional to its running time.\nWe also prove that, under suitable conditions, our method is faster than the\nstandard Monte Carlo method by a factor nearly proportional to d. A numerical\nexample is given.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 11:02:04 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kahale", "Nabil", ""]]}, {"id": "1702.08781", "submitter": "Maria Lomeli Dr", "authors": "Maria Lomeli", "title": "General Bayesian inference schemes in infinite mixture models", "comments": "Doctoral dissertation, University College London", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian statistical models allow us to formalise our knowledge about the\nworld and reason about our uncertainty, but there is a need for better\nprocedures to accurately encode its complexity. One way to do so is through\ncompositional models, which are formed by combining blocks consisting of\nsimpler models. One can increase the complexity of the compositional model by\neither stacking more blocks or by using a not-so-simple model as a building\nblock. This thesis is an example of the latter. One first aim is to expand the\nchoice of Bayesian nonparametric (BNP) blocks for constructing tractable\ncompositional models. So far, most of the models that have a Bayesian\nnonparametric component use a Dirichlet Process or a Pitman-Yor process because\nof the availability of tractable and compact representations. This thesis shows\nhow to overcome certain intractabilities in order to obtain analogous compact\nrepresentations for the class of Poisson-Kingman priors which includes the\nDirichlet and Pitman-Yor processes.\n  A major impediment to the widespread use of Bayesian nonparametric building\nblocks is that inference is often costly, intractable or difficult to carry\nout. This is an active research area since dealing with the model's infinite\ndimensional component forbids the direct use of standard simulation-based\nmethods. The main contribution of this thesis is a variety of inference schemes\nthat tackle this problem: Markov chain Monte Carlo and Sequential Monte Carlo\nmethods, which are exact inference schemes since they target the true\nposterior. The contributions of this thesis, in a larger context, provide\ngeneral purpose exact inference schemes in the flavour or probabilistic\nprogramming: the user is able to choose from a variety of models, focusing only\non the modelling part. Indeed, if the wide enough class of Poisson-Kingman\npriors is used as one of our blocks, this objective is achieved.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 13:34:02 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Lomeli", "Maria", ""]]}, {"id": "1702.08849", "submitter": "Ba Tuong Vo Prof", "authors": "Ba Ngu Vo and Ba Tuong Vo", "title": "Multi-Sensor Multi-object Tracking with the Generalized Labeled\n  Multi-Bernoulli Filter", "comments": "arXiv admin note: substantial text overlap with arXiv:1606.08350", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient implementation of the multi-sensor\ngeneralized labeled multi-Bernoulli (GLMB) filter. The solution exploits the\nGLMB joint prediction and update together with a new technique for truncating\nthe GLMB filtering density based on Gibbs sampling. The resulting algorithm has\nquadratic complexity in the number of hypothesized object and linear in the\nnumber of measurements of each individual sensors.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 16:31:21 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Vo", "Ba Ngu", ""], ["Vo", "Ba Tuong", ""]]}, {"id": "1702.08896", "submitter": "Dustin Tran", "authors": "Dustin Tran, Rajesh Ranganath, David M. Blei", "title": "Hierarchical Implicit Models and Likelihood-Free Variational Inference", "comments": "Appears in Neural Information Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit probabilistic models are a flexible class of models defined by a\nsimulation process for data. They form the basis for theories which encompass\nour understanding of the physical world. Despite this fundamental nature, the\nuse of implicit models remains limited due to challenges in specifying complex\nlatent structure in them, and in performing inferences in such models with\nlarge data sets. In this paper, we first introduce hierarchical implicit models\n(HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian\nmodeling, thereby defining models via simulators of data with rich hidden\nstructure. Next, we develop likelihood-free variational inference (LFVI), a\nscalable variational inference algorithm for HIMs. Key to LFVI is specifying a\nvariational family that is also implicit. This matches the model's flexibility\nand allows for accurate approximation of the posterior. We demonstrate diverse\napplications: a large-scale physical simulator for predator-prey populations in\necology; a Bayesian generative adversarial network for discrete data; and a\ndeep implicit model for text generation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:33:32 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 17:28:04 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 01:52:45 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}]