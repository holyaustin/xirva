[{"id": "2004.00041", "submitter": "Zhou Fan", "authors": "Zhou Fan, Yi Sun, Tianhao Wang, and Yihong Wu", "title": "Likelihood landscape and maximum likelihood estimation for the discrete\n  orbit recovery model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the non-convex optimization landscape for maximum likelihood\nestimation in the discrete orbit recovery model with Gaussian noise. This model\nis motivated by applications in molecular microscopy and image processing,\nwhere each measurement of an unknown object is subject to an independent random\nrotation from a rotational group. Equivalently, it is a Gaussian mixture model\nwhere the mixture centers belong to a group orbit.\n  We show that fundamental properties of the likelihood landscape depend on the\nsignal-to-noise ratio and the group structure. At low noise, this landscape is\n\"benign\" for any discrete group, possessing no spurious local optima and only\nstrict saddle points. At high noise, this landscape may develop spurious local\noptima, depending on the specific group. We discuss several positive and\nnegative examples, and provide a general condition that ensures a globally\nbenign landscape. For cyclic permutations of coordinates on $\\mathbb{R}^d$\n(multi-reference alignment), there may be spurious local optima when $d \\geq\n6$, and we establish a correspondence between these local optima and those of a\nsurrogate function of the phase variables in the Fourier domain.\n  We show that the Fisher information matrix transitions from resembling that\nof a single Gaussian in low noise to having a graded eigenvalue structure in\nhigh noise, which is determined by the graded algebra of invariant polynomials\nunder the group action. In a local neighborhood of the true object, the\nlikelihood landscape is strongly convex in a reparametrized system of variables\ngiven by a transcendence basis of this polynomial algebra. We discuss\nimplications for optimization algorithms, including slow convergence of\nexpectation-maximization, and possible advantages of momentum-based\nacceleration and variable reparametrization for first- and second-order descent\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:13:18 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 01:11:32 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 20:37:00 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 00:30:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fan", "Zhou", ""], ["Sun", "Yi", ""], ["Wang", "Tianhao", ""], ["Wu", "Yihong", ""]]}, {"id": "2004.00160", "submitter": "Chaoran Hu", "authors": "Chaoran Hu, Mark Elbroch, Thomas Meyer, Vladimir Pozdnyakov, Jun Yan", "title": "Moving-Resting Process with Measurement Error in Animal Movement\n  Modeling", "comments": "26 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling of animal movement is of critical importance. The\ncontinuous trajectory of an animal's movements is only observed at discrete,\noften irregularly spaced time points. Most existing models cannot handle the\nunequal sampling interval naturally and/or do not allow inactivity periods such\nas resting or sleeping. The recently proposed moving-resting (MR) model is a\nBrownian motion governed by a telegraph process, which allows periods of\ninactivity in one state of the telegraph process. The MR model shows promise in\nmodeling the movements of predators with long inactive periods such as many\nfelids, but the lack of accommodation of measurement errors seriously prohibits\nits application in practice. Here we incorporate measurement errors in the MR\nmodel and derive basic properties of the model. Inferences are based on a\ncomposite likelihood using the Markov property of the chain composed by every\nother observed increments. The performance of the method is validated in finite\nsample simulation studies. Application to the movement data of a mountain lion\nin Wyoming illustrates the utility of the method.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 23:22:27 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 16:59:29 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 01:38:55 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Hu", "Chaoran", ""], ["Elbroch", "Mark", ""], ["Meyer", "Thomas", ""], ["Pozdnyakov", "Vladimir", ""], ["Yan", "Jun", ""]]}, {"id": "2004.00231", "submitter": "Samuel Fischer", "authors": "Samuel M. Fischer and Mark A. Lewis", "title": "A robust and efficient algorithm to find profile likelihood confidence\n  intervals", "comments": "Keywords: computer algorithm; constrained optimization; parameter\n  estimation; estimability; identifiability", "journal-ref": "Stat Comput 31, 38 (2021)", "doi": "10.1007/s11222-021-10012-y", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Profile likelihood confidence intervals are a robust alternative to Wald's\nmethod if the asymptotic properties of the maximum likelihood estimator are not\nmet. However, the constrained optimization problem defining profile likelihood\nconfidence intervals can be difficult to solve in these situations, because the\nlikelihood function may exhibit unfavorable properties. As a result, existing\nmethods may be inefficient and yield misleading results. In this paper, we\naddress this problem by computing profile likelihood confidence intervals via a\ntrust-region approach, where steps computed based on local approximations are\nconstrained to regions where these approximations are sufficiently precise. As\nour algorithm also accounts for numerical issues arising if the likelihood\nfunction is strongly non-linear or parameters are not estimable, the method is\napplicable in many scenarios where earlier approaches are shown to be\nunreliable. To demonstrate its potential in applications, we apply our\nalgorithm to benchmark problems and compare it with 6 existing approaches to\ncompute profile likelihood confidence intervals. Our algorithm consistently\nachieved higher success rates than any competitor while also being among the\nquickest methods. As our algorithm can be applied to compute both confidence\nintervals of parameters and model predictions, it is useful in a wide range of\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 04:42:34 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Fischer", "Samuel M.", ""], ["Lewis", "Mark A.", ""]]}, {"id": "2004.00715", "submitter": "JInglai Li", "authors": "Ziqiao Ao and Jinglai Li", "title": "An approximate KLD based experimental design for models with intractable\n  likelihoods", "comments": "To appear in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collection is a critical step in statistical inference and data science,\nand the goal of statistical experimental design (ED) is to find the data\ncollection setup that can provide most information for the inference. In this\nwork we consider a special type of ED problems where the likelihoods are not\navailable in a closed form. In this case, the popular information-theoretic\nKullback-Leibler divergence (KLD) based design criterion can not be used\ndirectly, as it requires to evaluate the likelihood function. To address the\nissue, we derive a new utility function, which is a lower bound of the original\nKLD utility. This lower bound is expressed in terms of the summation of two or\nmore entropies in the data space, and thus can be evaluated efficiently via\nentropy estimation methods. We provide several numerical examples to\ndemonstrate the performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 21:18:28 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 11:48:28 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Ao", "Ziqiao", ""], ["Li", "Jinglai", ""]]}, {"id": "2004.00792", "submitter": "HaiYing Wang", "authors": "Luc Pronzato and HaiYing Wang", "title": "Sequential online subsampling for thinning experimental designs", "comments": "35 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a design problem where experimental conditions (design points\n$X_i$) are presented in the form of a sequence of i.i.d.\\ random variables,\ngenerated with an unknown probability measure $\\mu$, and only a given\nproportion $\\alpha\\in(0,1)$ can be selected. The objective is to select good\ncandidates $X_i$ on the fly and maximize a concave function $\\Phi$ of the\ncorresponding information matrix. The optimal solution corresponds to the\nconstruction of an optimal bounded design measure $\\xi_\\alpha^*\\leq\n\\mu/\\alpha$, with the difficulty that $\\mu$ is unknown and $\\xi_\\alpha^*$ must\nbe constructed online. The construction proposed relies on the definition of a\nthreshold $\\tau$ on the directional derivative of $\\Phi$ at the current\ninformation matrix, the value of $\\tau$ being fixed by a certain quantile of\nthe distribution of this directional derivative. Combination with recursive\nquantile estimation yields a nonlinear two-time-scale stochastic approximation\nmethod. It can be applied to very long design sequences since only the current\ninformation matrix and estimated quantile need to be stored. Convergence to an\noptimum design is proved. Various illustrative examples are presented.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 03:19:11 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 12:58:58 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Pronzato", "Luc", ""], ["Wang", "HaiYing", ""]]}, {"id": "2004.01341", "submitter": "Bledar Konomi", "authors": "Si Cheng, Bledar A. Konomi, Jessica L. Matthews, Georgios Karagiannis,\n  Emily L. Kang", "title": "Hierarchical Bayesian Nearest Neighbor Co-Kriging Gaussian Process\n  Models; An Application to Intersatellite Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in remote sensing technology and the increasing size of\nsatellite constellations allows massive geophysical information to be gathered\ndaily on a global scale by numerous platforms of different fidelity. The\nauto-regressive co-kriging model is a suitable framework to analyse such data\nsets because it accounts for cross-dependencies among different fidelity\nsatellite outputs. However, its implementation in multifidelity large spatial\ndata-sets is practically infeasible because its computational complexity\nincreases cubically with the total number of observations. In this paper, we\npropose a nearest neighbour co-kriging Gaussian process that couples the\nauto-regressive model and nearest neighbour GP by using augmentation ideas;\nreducing the computational complexity to be linear with the total number of\nspatial observed locations. The latent process of the nearest neighbour GP is\naugmented in a manner which allows the specification of semi-conjugate priors.\nThis facilitates the design of an efficient MCMC sampler involving mostly\ndirect sampling updates which can be implemented in parallel computational\nenvironments. The good predictive performance of the proposed method is\ndemonstrated in a simulation study. We use the proposed method to analyze\nHigh-resolution Infrared Radiation Sounder data gathered from two NOAA polar\norbiting satellites.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 02:26:38 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 06:27:39 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 21:38:24 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cheng", "Si", ""], ["Konomi", "Bledar A.", ""], ["Matthews", "Jessica L.", ""], ["Karagiannis", "Georgios", ""], ["Kang", "Emily L.", ""]]}, {"id": "2004.01571", "submitter": "Antoine Baker", "authors": "Antoine Baker, Benjamin Aubin, Florent Krzakala, Lenka Zdeborov\\'a", "title": "TRAMP: Compositional Inference with TRee Approximate Message Passing", "comments": "Source code available at https://github.com/sphinxteam/tramp. For\n  some examples, see https://github.com/benjaminaubin/tramp_examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG eess.SP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce tramp, standing for TRee Approximate Message Passing, a python\npackage for compositional inference in high-dimensional tree-structured models.\nThe package provides an unifying framework to study several approximate message\npassing algorithms previously derived for a variety of machine learning tasks\nsuch as generalized linear models, inference in multi-layer networks, matrix\nfactorization, and reconstruction using non-separable penalties. For some\nmodels, the asymptotic performance of the algorithm can be theoretically\npredicted by the state evolution, and the measurements entropy estimated by the\nfree entropy formalism. The implementation is modular by design: each module,\nwhich implements a factor, can be composed at will with other modules to solve\ncomplex inference tasks. The user only needs to declare the factor graph of the\nmodel: the inference algorithm, state evolution and entropy estimation are\nfully automated.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:51:10 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 11:27:59 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Baker", "Antoine", ""], ["Aubin", "Benjamin", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2004.01975", "submitter": "Wenshuo Wang", "authors": "Yichao Li, Wenshuo Wang, Ke Deng and Jun S Liu", "title": "Stratification and Optimal Resampling for Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC), also known as particle filters, has been widely\naccepted as a powerful computational tool for making inference with dynamical\nsystems. A key step in SMC is resampling, which plays the role of steering the\nalgorithm towards the future dynamics. Several strategies have been proposed\nand used in practice, including multinomial resampling, residual resampling\n(Liu and Chen 1998), optimal resampling (Fearnhead and Clifford 2003),\nstratified resampling (Kitagawa 1996), and optimal transport resampling (Reich\n2013). We show that, in the one dimensional case, optimal transport resampling\nis equivalent to stratified resampling on the sorted particles, and they both\nminimize the resampling variance as well as the expected squared energy\ndistance between the original and resampled empirical distributions; in the\nmultidimensional case, the variance of stratified resampling after sorting\nparticles using Hilbert curve (Gerber et al. 2019) in $\\mathbb{R}^d$ is\n$O(m^{-(1+2/d)})$, an improved rate compared to the original $O(m^{-(1+1/d)})$,\nwhere $m$ is the number of resampled particles. This improved rate is the\nlowest for ordered stratified resampling schemes, as conjectured in Gerber et\nal. (2019). We also present an almost sure bound on the Wasserstein distance\nbetween the original and Hilbert-curve-resampled empirical distributions. In\nlight of these theoretical results, we propose the stratified\nmultiple-descendant growth (SMG) algorithm, which allows us to explore the\nsample space more efficiently compared to the standard i.i.d.\nmultiple-descendant sampling-resampling approach as measured by the Wasserstein\nmetric. Numerical evidence is provided to demonstrate the effectiveness of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 17:12:09 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 08:41:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Li", "Yichao", ""], ["Wang", "Wenshuo", ""], ["Deng", "Ke", ""], ["Liu", "Jun S", ""]]}, {"id": "2004.02065", "submitter": "Deukwoo Kwon", "authors": "Roopesh Reddy Sadashiva Reddy, Isildinha M. Reis, and Deukwoo Kwon", "title": "ABCMETAapp: R Shiny Application for Simulation-based Estimation of Mean\n  and Standard Deviation for Meta-analysis via Approximate Bayesian Computation\n  (ABC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Background and Objective: In meta-analysis based on continuous outcome,\nestimated means and corresponding standard deviations from the selected studies\nare key inputs to obtain a pooled estimate of the mean and its confidence\ninterval. We often encounter the situation that these quantities are not\ndirectly reported in the literatures. Instead, other summary statistics are\nreported such as median, minimum, maximum, quartiles, and study sample size.\nBased on available summary statistics, we need to estimate estimates of mean\nand standard deviation for meta-analysis. Methods: We developed a R Shiny code\nbased on approximate Bayesian computation (ABC), ABCMETA, to deal with this\nsituation. Results: In this article, we present an interactive and\nuser-friendly R Shiny application for implementing the proposed method (named\nABCMETAapp). In ABCMETAapp, users can choose an underlying outcome distribution\nother than the normal distribution when the distribution of the outcome\nvariable is skewed or heavy tailed. We show how to run ABCMETAapp with\nexamples. Conclusions: ABCMETAapp provides a R Shiny implementation. This\nmethod is more flexible than the existing analytical methods since estimation\ncan be based on five different distribution (Normal, Lognormal, Exponential,\nWeibull, and Beta) for the outcome variable.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 01:29:57 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Reddy", "Roopesh Reddy Sadashiva", ""], ["Reis", "Isildinha M.", ""], ["Kwon", "Deukwoo", ""]]}, {"id": "2004.02278", "submitter": "George Yuan", "authors": "George Xianzhi Yuan, Lan Di, Yudi Gu, Guoqi Qian, and Xiaosong Qian", "title": "The Framework for the Prediction of the Critical Turning Period for\n  Outbreak of COVID-19 Spread in China based on the iSEIR Model", "comments": "24 paages, 9 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this study is to establish a general framework for predicting the\nso-called critical Turning Period in an infectious disease epidemic such as the\nCOVID-19 outbreak in China early this year. This framework enabled a timely\nprediction of the turning period when applied to Wuhan COVID-19 epidemic and\ninformed the relevant authority for taking appropriate and timely actions to\ncontrol the epidemic. It is expected to provide insightful information on\nturning period for the world's current battle against the COVID-19 pandemic.\nThe underlying mathematical model in our framework is the individual\nSusceptible-Exposed- Infective-Removed (iSEIR) model, which is a set of\ndifferential equations extending the classic SEIR model. We used the observed\ndaily cases of COVID-19 in Wuhan from February 6 to 10, 2020 as the input to\nthe iSEIR model and were able to generate the trajectory of COVID-19 cases\ndynamics for the following days at midnight of February 10 based on the updated\nmodel, from which we predicted that the turning period of CIVID-19 outbreak in\nWuhan would arrive within one week after February 14. This prediction turned to\nbe timely and accurate, providing adequate time for the government, hospitals,\nessential industry sectors and services to meet peak demands and to prepare\naftermath planning. Our study also supports the observed effectiveness on\nflatting the epidemic curve by decisively imposing the Lockdown and Isolation\nControl Program in Wuhan since January 23, 2020. The Wuhan experience provides\nan exemplary lesson for the whole world to learn in combating COVID-19.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 18:43:48 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yuan", "George Xianzhi", ""], ["Di", "Lan", ""], ["Gu", "Yudi", ""], ["Qian", "Guoqi", ""], ["Qian", "Xiaosong", ""]]}, {"id": "2004.02324", "submitter": "Tim Lucas", "authors": "Tim Lucas, Andre Python and David Redding", "title": "Graphical outputs and Spatial Cross-validation for the R-INLA package\n  using INLAutils", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analyses proceed by an iterative process of model fitting and\nchecking. The R-INLA package facilitates this iteration by fitting many\nBayesian models much faster than alternative MCMC approaches. As the\ninterpretation of results and model objects from Bayesian analyses can be\ncomplex, the R package INLAutils provides users with easily accessible, clear\nand customisable graphical summaries of model outputs from R- INLA.\nFurthermore, it offers a function for performing and visualizing the results of\na spatial leave-one-out cross-validation (SLOOCV) approach that can be applied\nto compare the predictive performance of multiple spatial models. In this\npaper, we describe and illustrate the use of (1) graphical summary plotting\nfunctions and (2) the SLOOCV approach. We conclude the paper by identifying the\nlimits of our approach and discuss future potential improvements.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 21:50:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lucas", "Tim", ""], ["Python", "Andre", ""], ["Redding", "David", ""]]}, {"id": "2004.02425", "submitter": "Kirankumar Shiragur", "authors": "Nima Anari, Moses Charikar, Kirankumar Shiragur, Aaron Sidford", "title": "The Bethe and Sinkhorn Permanents of Low Rank Matrices and Implications\n  for Profile Maximum Likelihood", "comments": "59 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of computing the likelihood of the\nprofile of a discrete distribution, i.e., the probability of observing the\nmultiset of element frequencies, and computing a profile maximum likelihood\n(PML) distribution, i.e., a distribution with the maximum profile likelihood.\nFor each problem we provide polynomial time algorithms that given $n$ i.i.d.\\\nsamples from a discrete distribution, achieve an approximation factor of\n$\\exp\\left(-O(\\sqrt{n} \\log n) \\right)$, improving upon the previous best-known\nbound achievable in polynomial time of $\\exp(-O(n^{2/3} \\log n))$ (Charikar,\nShiragur and Sidford, 2019). Through the work of Acharya, Das, Orlitsky and\nSuresh (2016), this implies a polynomial time universal estimator for symmetric\nproperties of discrete distributions in a broader range of error parameter.\n  We achieve these results by providing new bounds on the quality of\napproximation of the Bethe and Sinkhorn permanents (Vontobel, 2012 and 2014).\nWe show that each of these are $\\exp(O(k \\log(N/k)))$ approximations to the\npermanent of $N \\times N$ matrices with non-negative rank at most $k$,\nimproving upon the previous known bounds of $\\exp(O(N))$. To obtain our results\non PML, we exploit the fact that the PML objective is proportional to the\npermanent of a certain Vandermonde matrix with $\\sqrt{n}$ distinct columns,\ni.e. with non-negative rank at most $\\sqrt{n}$. As a by-product of our work we\nestablish a surprising connection between the convex relaxation in prior work\n(CSS19) and the well-studied Bethe and Sinkhorn approximations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 06:40:03 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Anari", "Nima", ""], ["Charikar", "Moses", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "2004.02653", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist", "title": "Gaussian Process Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel way to combine boosting with Gaussian process and mixed\neffects models. This allows for relaxing, first, the linearity assumption for\nthe mean function in Gaussian process and grouped random effects models in a\nflexible non-parametric way and, second, the independence assumption made in\nmost boosting algorithms. The former is advantageous for predictive accuracy\nand for avoiding model misspecifications. The latter is important for more\nefficient learning of the mean function and for obtaining probabilistic\npredictions. In addition, we present an extension that scales to large data\nusing a Vecchia approximation for the Gaussian process model relying on novel\nresults for covariance parameter inference. We obtain increased predictive\naccuracy compared to existing approaches on several simulated and real-world\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:19:54 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 20:00:36 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 12:06:59 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Sigrist", "Fabio", ""]]}, {"id": "2004.03165", "submitter": "Christian Bongiorno", "authors": "Christian Bongiorno", "title": "Bootstraps Regularize Singular Correlation Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.SP q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I show analytically that the average of $k$ bootstrapped correlation matrices\nrapidly becomes positive-definite as $k$ increases, which provides a simple\napproach to regularize singular Pearson correlation matrices. If $n$ is the\nnumber of objects and $t$ the number of features, the averaged correlation\nmatrix is almost surely positive-definite if $k> \\frac{e}{e-1}\\frac{n}{t}\\simeq\n1.58\\frac{n}{t}$ in the limit of large $t$ and $n$. The probability of\nobtaining a positive-definite correlation matrix with $k$ bootstraps is also\nderived for finite $n$ and $t$. Finally, I demonstrate that the number of\nrequired bootstraps is always smaller than $n$. This method is particularly\nrelevant in fields where $n$ is orders of magnitude larger than the size of\ndata points $t$, e.g., in finance, genetics, social science, or image\nprocessing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 07:26:35 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bongiorno", "Christian", ""]]}, {"id": "2004.04177", "submitter": "Alireza Vafaei Sadr", "authors": "Alireza Vafaei Sadr, Farida Farsian", "title": "Inpainting via Generative Adversarial Networks for CMB data analysis", "comments": "19 pages, 21 figures. Prepared for submission to JCAP. All codes will\n  be published after acceptance", "journal-ref": null, "doi": "10.1088/1475-7516/2021/03/012", "report-no": null, "categories": "astro-ph.CO cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new method to inpaint the CMB signal in regions\nmasked out following a point source extraction process. We adopt a modified\nGenerative Adversarial Network (GAN) and compare different combinations of\ninternal (hyper-)parameters and training strategies. We study the performance\nusing a suitable $\\mathcal{C}_r$ variable in order to estimate the performance\nregarding the CMB power spectrum recovery. We consider a test set where one\npoint source is masked out in each sky patch with a 1.83 $\\times$ 1.83 squared\ndegree extension, which, in our gridding, corresponds to 64 $\\times$ 64 pixels.\nThe GAN is optimized for estimating performance on Planck 2018 total intensity\nsimulations. The training makes the GAN effective in reconstructing a masking\ncorresponding to about 1500 pixels with $1\\%$ error down to angular scales\ncorresponding to about 5 arcminutes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 18:00:10 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 07:38:15 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sadr", "Alireza Vafaei", ""], ["Farsian", "Farida", ""]]}, {"id": "2004.04254", "submitter": "Matthias Sachs", "authors": "Matthias Sachs, Deborshee Sen, Jianfeng Lu, and David Dunson", "title": "Posterior computation with the Gibbs zig-zag sampler", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intriguing new class of piecewise deterministic Markov processes (PDMPs)\nhas recently been proposed as an alternative to Markov chain Monte Carlo\n(MCMC). In order to facilitate the application to a larger class of problems,\nwe propose a new class of PDMPs termed Gibbs zig-zag samplers, which allow\nparameters to be updated in blocks with a zig-zag sampler applied to certain\nparameters and traditional MCMC-style updates to others. We demonstrate the\nflexibility of this framework on posterior sampling for logistic models with\nshrinkage priors for high-dimensional regression and random effects and provide\nconditions for geometric ergodicity and the validity of a central limit\ntheorem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 21:05:10 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 04:59:44 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sachs", "Matthias", ""], ["Sen", "Deborshee", ""], ["Lu", "Jianfeng", ""], ["Dunson", "David", ""]]}, {"id": "2004.04480", "submitter": "Bruno Sudret", "authors": "S. Marelli, P.-R. Wagner, C. Lataniotis and B. Sudret", "title": "Stochastic spectral embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2020-003", "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing approximations that can accurately mimic the behavior of complex\nmodels at reduced computational costs is an important aspect of uncertainty\nquantification. Despite their flexibility and efficiency, classical surrogate\nmodels such as Kriging or polynomial chaos expansions tend to struggle with\nhighly non-linear, localized or non-stationary computational models. We hereby\npropose a novel sequential adaptive surrogate modeling method based on\nrecursively embedding locally spectral expansions. It is achieved by means of\ndisjoint recursive partitioning of the input domain, which consists in\nsequentially splitting the latter into smaller subdomains, and constructing a\nsimpler local spectral expansions in each, exploiting the trade-off complexity\nvs. locality. The resulting expansion, which we refer to as \"stochastic\nspectral embedding\" (SSE), is a piece-wise continuous approximation of the\nmodel response that shows promising approximation capabilities, and good\nscaling with both the problem dimension and the size of the training set. We\nfinally show how the method compares favorably against state-of-the-art sparse\npolynomial chaos expansions on a set of models with different complexity and\ninput dimension.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 11:00:07 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 08:02:44 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Marelli", "S.", ""], ["Wagner", "P. -R.", ""], ["Lataniotis", "C.", ""], ["Sudret", "B.", ""]]}, {"id": "2004.04620", "submitter": "Matthew Moores", "authors": "Matthew T. Moores, Anthony N. Pettitt and Kerrie Mengersen", "title": "Bayesian Computation with Intractable Likelihoods", "comments": "arXiv admin note: text overlap with arXiv:1503.08066", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article surveys computational methods for posterior inference with\nintractable likelihoods, that is where the likelihood function is unavailable\nin closed form, or where evaluation of the likelihood is infeasible. We review\nrecent developments in pseudo-marginal methods, approximate Bayesian\ncomputation (ABC), the exchange algorithm, thermodynamic integration, and\ncomposite likelihood, paying particular attention to advancements in\nscalability for large datasets. We also mention R and MATLAB source code for\nimplementations of these algorithms, where they are available.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 02:16:40 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Moores", "Matthew T.", ""], ["Pettitt", "Anthony N.", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2004.05102", "submitter": "Toshihiro Hirano", "authors": "Toshihiro Hirano", "title": "A multi-resolution approximation via linear projection for large spatial\n  datasets", "comments": "44 pages, 3 figure, 7 tables", "journal-ref": "Japanese Journal of Statistics and Data Science (2021), Vol. 4,\n  215-256", "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technical advances in collecting spatial data have been increasing the\ndemand for methods to analyze large spatial datasets. The statistical analysis\nfor these types of datasets can provide useful knowledge in various fields.\nHowever, conventional spatial statistical methods, such as maximum likelihood\nestimation and kriging, are impractically time-consuming for large spatial\ndatasets due to the necessary matrix inversions. To cope with this problem, we\npropose a multi-resolution approximation via linear projection ($M$-RA-lp). The\n$M$-RA-lp conducts a linear projection approach on each subregion whenever a\nspatial domain is subdivided, which leads to an approximated covariance\nfunction capturing both the large- and small-scale spatial variations.\nMoreover, we elicit the algorithms for fast computation of the log-likelihood\nfunction and predictive distribution with the approximated covariance function\nobtained by the $M$-RA-lp. Simulation studies and a real data analysis for air\ndose rates demonstrate that our proposed $M$-RA-lp works well relative to the\nrelated existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:40:14 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 17:53:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hirano", "Toshihiro", ""]]}, {"id": "2004.05105", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis and Ioannis Ntzoufras", "title": "On the identifiability of Bayesian factor analytic models", "comments": "36 pages 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well known identifiability issue in factor analytic models is the\ninvariance with respect to orthogonal transformations. This problem burdens the\ninference under a Bayesian setup, where Markov chain Monte Carlo (MCMC) methods\nare used to generate samples from the posterior distribution. We introduce a\npost-processing scheme in order to deal with rotation, sign and permutation\ninvariance of the MCMC sample. The exact version of the contributed algorithm\nrequires to solve $2^q$ assignment problems per (retained) MCMC iteration,\nwhere $q$ denotes the number of factors of the fitted model. For large numbers\nof factors two approximate schemes based on simulated annealing are also\ndiscussed. We demonstrate that the proposed method leads to interpretable\nposterior distributions using synthetic and publicly available data from\ntypical factor analytic models as well as mixtures of factor analyzers. An R\npackage is available online at CRAN web-page.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:43:15 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Papastamoulis", "Panagiotis", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "2004.05308", "submitter": "Tanmay Sen", "authors": "Tanmay Sen, Ritwik Bhattacharya, Biswabrata Pradhan and Yogesh Mani\n  Tripathi", "title": "Statistical inference and Bayesian optimal life-testing plans under\n  Type-II unified hybrid censoring scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the inferential procedures and Bayesian optimal\nlife-testing issues under Type-II unified hybrid censoring scheme. First, the\nexplicit expressions of expected number of failures, expected duration of\ntesting and Fisher information matrix for the unknown parameters of the\nunderlying lifetime model are derived. Then, using these quantities, the\nBayesian optimal life-testing plans are computed in subsequent section. A cost\nconstraint D-optimal optimization problem has been formulated and the\ncorresponding solution algorithm is provided to obtain optimal plans.\nComputational procedures are illustrated through numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 05:26:47 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Sen", "Tanmay", ""], ["Bhattacharya", "Ritwik", ""], ["Pradhan", "Biswabrata", ""], ["Tripathi", "Yogesh Mani", ""]]}, {"id": "2004.05426", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues", "title": "Scaling Bayesian inference of mixed multinomial logit models to very\n  large datasets", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference methods have been shown to lead to significant\nimprovements in the computational efficiency of approximate Bayesian inference\nin mixed multinomial logit models when compared to standard Markov-chain Monte\nCarlo (MCMC) methods without compromising accuracy. However, despite their\ndemonstrated efficiency gains, existing methods still suffer from important\nlimitations that prevent them to scale to very large datasets, while providing\nthe flexibility to allow for rich prior distributions and to capture complex\nposterior distributions. In this paper, we propose an Amortized Variational\nInference approach that leverages stochastic backpropagation, automatic\ndifferentiation and GPU-accelerated computation, for effectively scaling\nBayesian inference in Mixed Multinomial Logit models to very large datasets.\nMoreover, we show how normalizing flows can be used to increase the flexibility\nof the variational posterior approximations. Through an extensive simulation\nstudy, we empirically show that the proposed approach is able to achieve\ncomputational speedups of multiple orders of magnitude over traditional MSLE\nand MCMC approaches for large datasets without compromising estimation\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 15:30:47 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Rodrigues", "Filipe", ""]]}, {"id": "2004.05684", "submitter": "Kisor Sahu Dr.", "authors": "Raj Kishore, Prashant Kumar Jha, Shreeja Das, Dheeresh Agarwal, Tanmay\n  Maloo, Hansraj Pegu, Devadatta Sahoo, Ankita Singhal, Kisor K. Sahu", "title": "A kinetic model for qualitative understanding and analysis of the effect\n  of complete lockdown imposed by India for controlling the COVID-19 disease\n  spread by the SARS-CoV-2 virus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present ongoing global pandemic caused by SARS-CoV-2 virus is creating\nhavoc across the world. The absence of any vaccine as well as any definitive\ndrug to cure, has made the situation very grave. Therefore only few effective\ntools are available to contain the rapid pace of spread of this disease, named\nas COVID-19. On 24th March, 2020, the the Union Government of India made an\nannouncement of unprecedented complete lockdown of the entire country effective\nfrom the next day. No exercise of similar scale and magnitude has been ever\nundertaken anywhere on the globe in the history of entire mankind. This study\naims to scientifically analyze the implications of this decision using a\nkinetic model covering more than 96% of Indian territory. This model was\nfurther constrained by large sets of realistic parameters pertinent to India in\norder to capture the ground realities prevailing in India, such as: (i) true\nstate wise population density distribution, (ii) accurate state wise infection\ndistribution for the zeroth day of simulation (20th March, 2020), (iii)\nrealistic movements of average clusters, (iv) rich diversity in movements\npatterns across different states, (v) migration patterns across different\ngeographies, (vi) different migration patterns for pre- and post-COVID-19\noutbreak, (vii) Indian demographic data based on the 2011 census, (viii) World\nHealth Organization (WHO) report on demography wise infection rate and (ix)\nincubation period as per WHO report. This model does not attempt to make a\nlong-term prediction about the disease spread on a standalone basis; but to\ncompare between two different scenarios (complete lockdown vs. no lockdown). In\nthe framework of model assumptions, our model conclusively shows significant\nsuccess of the lockdown in containing the disease within a tiny fraction of the\npopulation and in the absence of it, it would have led to a very grave\nsituation.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 19:34:12 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Kishore", "Raj", ""], ["Jha", "Prashant Kumar", ""], ["Das", "Shreeja", ""], ["Agarwal", "Dheeresh", ""], ["Maloo", "Tanmay", ""], ["Pegu", "Hansraj", ""], ["Sahoo", "Devadatta", ""], ["Singhal", "Ankita", ""], ["Sahu", "Kisor K.", ""]]}, {"id": "2004.05730", "submitter": "Hyokyoung G. Hong Dr.", "authors": "Hyokyoung G. Hong and Yi Li", "title": "Estimation of time-varying reproduction numbers underlying\n  epidemiological processes: a new statistical tool for the COVID-19 pandemic", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0236464", "report-no": null, "categories": "q-bio.PE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus pandemic has rapidly evolved into an unprecedented crisis.\nThe susceptible-infectious-removed (SIR) model and its variants have been used\nfor modeling the pandemic. However, time-independent parameters in the\nclassical models may not capture the dynamic transmission and removal\nprocesses, governed by virus containment strategies taken at various phases of\nthe epidemic. Moreover, very few models account for possible inaccuracies of\nthe reported cases. We propose a Poisson model with time-dependent transmission\nand removal rates to account for possible random errors in reporting and\nestimate a time-dependent disease reproduction number, which may be used to\nassess the effectiveness of virus control strategies. We apply our method to\nstudy the pandemic in several severely impacted countries, and analyze and\nforecast the evolving spread of the coronavirus. We have developed an\ninteractive web application to facilitate readers' use of our method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 00:14:15 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 19:22:36 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 03:24:55 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Hong", "Hyokyoung G.", ""], ["Li", "Yi", ""]]}, {"id": "2004.05771", "submitter": "Mert Korkali", "authors": "Yijun Xu, Kiran Karra, Lamine Mili, Mert Korkali, Xiao Chen, Zhixiong\n  Hu", "title": "Probabilistic Load-Margin Assessment using Vine Copula and Gaussian\n  Process Emulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing penetration of renewable energy along with the variations of\nthe loads bring large uncertainties in the power system states that are\nthreatening the security of power system planning and operation. Facing these\nchallenges, this paper proposes a cost-effective, nonparametric method to\nquantify the impact of uncertain power injections on the load margins. First,\nwe propose to generate system uncertain inputs via a novel vine copula due to\nits capability in simulating complex multivariate highly dependent model\ninputs. Furthermore, to reduce the prohibitive computational time required in\nthe traditional Monte-Carlo method, we propose to use a nonparametric,\nGaussian-process-emulator-based reduced-order model to replace the original\ncomplicated continuation power-flow model. This emulator allows us to execute\nthe time-consuming continuation power-flow solver at the sampled values with a\nnegligible computational cost. The simulations conducted on the IEEE 57-bus\nsystem, to which correlated renewable generation are attached, reveal the\nexcellent performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 05:07:39 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Xu", "Yijun", ""], ["Karra", "Kiran", ""], ["Mili", "Lamine", ""], ["Korkali", "Mert", ""], ["Chen", "Xiao", ""], ["Hu", "Zhixiong", ""]]}, {"id": "2004.05796", "submitter": "Hongqiao Wang", "authors": "Hongqiao Wang and Xiang Zhou", "title": "Explicit Estimation of Derivatives from Data and Differential Equations\n  by Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we employ the Bayesian inference framework to solve the problem\nof estimating the solution and particularly, its derivatives, which satisfy a\nknown differential equation, from the given noisy and scarce observations of\nthe solution data only. To address the key issue of accuracy and robustness of\nderivative estimation, we use the Gaussian processes to jointly model the\nsolution, the derivatives, and the differential equation. By regarding the\nlinear differential equation as a linear constraint, a Gaussian process\nregression with constraint method (GPRC) is developed to improve the accuracy\nof prediction of derivatives. For nonlinear differential equations, we propose\na Picard-iteration-like approximation of linearization around the Gaussian\nprocess obtained only from data so that our GPRC can be still iteratively\napplicable. Besides, a product of experts method is applied to ensure the\ninitial or boundary condition is considered to further enhance the prediction\naccuracy of the derivatives. We present several numerical results to illustrate\nthe advantages of our new method in comparison to the standard data-driven\nGaussian process regression.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 07:08:02 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 04:36:28 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Wang", "Hongqiao", ""], ["Zhou", "Xiang", ""]]}, {"id": "2004.06092", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej", "title": "mFLICA: An R package for Inferring Leadership of Coordination From Time\n  Series", "comments": "The latest version of R package can be found at\n  https://github.com/DarkEyes/mFLICA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI econ.EM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leadership is a process that leaders influence followers to achieve\ncollective goals. One of special cases of leadership is the coordinated pattern\ninitiation. In this context, leaders are initiators who initiate coordinated\npatterns that everyone follows. Given a set of individual-multivariate time\nseries of real numbers, the mFLICA package provides a framework for R users to\ninfer coordination events within time series, initiators and followers of these\ncoordination events, as well as dynamics of group merging and splitting. The\nmFLICA package also has a visualization function to make results of leadership\ninference more understandable. The package is available on Comprehensive R\nArchive Network (CRAN) at https://CRAN.R-project.org/package=mFLICA.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:14:06 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""]]}, {"id": "2004.06152", "submitter": "Hussein Hazimeh", "authors": "Hussein Hazimeh, Rahul Mazumder, Ali Saab", "title": "Sparse Regression at Scale: Branch-and-Bound rooted in First-Order\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the least squares regression problem, penalized with a\ncombination of the $\\ell_{0}$ and squared $\\ell_{2}$ penalty functions (a.k.a.\n$\\ell_0 \\ell_2$ regularization). Recent work shows that the resulting\nestimators are of key importance in many high-dimensional statistical settings.\nHowever, exact computation of these estimators remains a major challenge.\nIndeed, modern exact methods, based on mixed integer programming (MIP), face\ndifficulties when the number of features $p \\sim 10^4$. In this work, we\npresent a new exact MIP framework for $\\ell_0\\ell_2$-regularized regression\nthat can scale to $p \\sim 10^7$, achieving speedups of at least $5000$x,\ncompared to state-of-the-art exact methods. Unlike recent work, which relies on\nmodern commercial MIP solvers, we design a specialized nonlinear\nbranch-and-bound (BnB) framework, by critically exploiting the problem\nstructure. A key distinguishing component in our framework lies in efficiently\nsolving the node relaxations using a specialized first-order method, based on\ncoordinate descent (CD). Our CD-based method effectively leverages information\nacross the BnB nodes, through using warm starts, active sets, and gradient\nscreening. In addition, we design a novel method for obtaining dual bounds from\nprimal CD solutions, which certifiably works in high dimensions. Experiments on\nsynthetic and real high-dimensional datasets demonstrate that our framework is\nnot only significantly faster than the state of the art, but can also deliver\ncertifiably optimal solutions to statistically challenging instances that\ncannot be handled with existing methods. We open source the implementation\nthrough our toolkit L0BnB.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 18:45:29 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 20:18:21 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hazimeh", "Hussein", ""], ["Mazumder", "Rahul", ""], ["Saab", "Ali", ""]]}, {"id": "2004.06363", "submitter": "Shijia Wang", "authors": "Shijia Wang and Liangliang Wang", "title": "Particle Gibbs Sampling for Bayesian Phylogenetic inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combinatorial sequential Monte Carlo (CSMC) has been demonstrated to be\nan efficient complementary method to the standard Markov chain Monte Carlo\n(MCMC) for Bayesian phylogenetic tree inference using biological sequences. It\nis appealing to combine the CSMC and MCMC in the framework of the particle\nGibbs (PG) sampler to jointly estimate the phylogenetic trees and evolutionary\nparameters. However, the Markov chain of the particle Gibbs may mix poorly if\nthe underlying SMC suffers from the path degeneracy issue. Some remedies,\nincluding the particle Gibbs with ancestor sampling and the interacting\nparticle MCMC, have been proposed to improve the PG. But they either cannot be\napplied to or remain inefficient for the combinatorial tree space. We introduce\na novel CSMC method by proposing a more efficient proposal distribution. It\nalso can be combined into the particle Gibbs sampler framework to infer\nparameters in the evolutionary model. The new algorithm can be easily\nparallelized by allocating samples over different computing cores. We validate\nthat the developed CSMC can sample trees more efficiently in various particle\nGibbs samplers via numerical experiments. Our implementation is available at\nhttps://github.com/liangliangwangsfu/phyloPMCMC\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 08:59:10 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 08:49:41 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Wang", "Shijia", ""], ["Wang", "Liangliang", ""]]}, {"id": "2004.06425", "submitter": "Gael Martin Prof", "authors": "Gael M. Martin (Monash University), David T. Frazier (Monash\n  University), and Christian P. Robert (Universit\\'e Paris Dauphine, University\n  of Warwick, and CREST)", "title": "Computing Bayes: Bayesian Computation from 1763 to the 21st Century", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Bayesian statistical paradigm uses the language of probability to express\nuncertainty about the phenomena that generate observed data. Probability\ndistributions thus characterize Bayesian analysis, with the rules of\nprobability used to transform prior probability distributions for all unknowns\n- parameters, latent variables, models - into posterior distributions,\nsubsequent to the observation of data. Conducting Bayesian analysis requires\nthe evaluation of integrals in which these probability distributions appear.\nBayesian computation is all about evaluating such integrals in the typical case\nwhere no analytical solution exists. This paper takes the reader on a\nchronological tour of Bayesian computation over the past two and a half\ncenturies. Beginning with the one-dimensional integral first confronted by\nBayes in 1763, through to recent problems in which the unknowns number in the\nmillions, we place all computational problems into a common framework, and\ndescribe all computational methods using a common notation. The aim is to help\nnew researchers in particular - and more generally those interested in adopting\na Bayesian approach to empirical work - make sense of the plethora of\ncomputational techniques that are now on offer; understand when and why\ndifferent methods are useful; and see the links that do exist, between them\nall.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 11:24:27 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 21:41:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Martin", "Gael M.", "", "Monash University"], ["Frazier", "David T.", "", "Monash\n  University"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris Dauphine, University\n  of Warwick, and CREST"]]}, {"id": "2004.06445", "submitter": "Amir Abdollahi", "authors": "Maryam Rahbaralam, Amir Abdollahi, Daniel Fern\\`andez-Garcia, Xavier\n  Sanchez-Vila", "title": "Stochastic modeling of non-linear adsorption with Gaussian kernel\n  density estimators", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adsorption is a relevant process in many fields, such as product\nmanufacturing or pollution remediation in porous materials. Adsorption takes\nplace at the molecular scale, amenable to be modeled by Lagrangian numerical\nmethods. We have proposed a chemical diffusion-reaction model for the\nsimulation of adsorption, based on the combination of a random walk particle\ntracking method involving the use of Gaussian Kernel Density Estimators. The\nmain feature of the proposed model is that it can effectively reproduce the\nnonlinear behavior characteristic of the Langmuir and Freundlich isotherms. In\nthe former, it is enough to add a finite number of sorption sites of\nhomogeneous sorption properties, and to set the process as the combination of\nthe forward and the backward reactions, each one of them with a prespecified\nreaction rate. To model the Freundlich isotherm instead, typical of low to\nintermediate range of solute concentrations, there is a need to assign a\ndifferent equilibrium constant to each specific sorption site, provided they\nare all drawn from a truncated power-law distribution. Both nonlinear models\ncan be combined in a single framework to obtain a typical observed behavior for\na wide range of concentration values.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 12:21:33 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Rahbaralam", "Maryam", ""], ["Abdollahi", "Amir", ""], ["Fern\u00e0ndez-Garcia", "Daniel", ""], ["Sanchez-Vila", "Xavier", ""]]}, {"id": "2004.06459", "submitter": "Gherardo Varando", "authors": "Federico Carli, Manuele Leonelli, Eva Riccomagno, Gherardo Varando", "title": "The R Package stagedtrees for Structural Learning of Stratified Staged\n  Trees", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  stagedtrees is an R package which includes several algorithms for learning\nthe structure of staged trees and chain event graphs from data. Score-based and\nclustering-based algorithms are implemented, as well as various functionalities\nto plot the models and perform inference. The capabilities of stagedtrees are\nillustrated using mainly two datasets both included in the package or bundled\nin R.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:02:59 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 17:11:54 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Carli", "Federico", ""], ["Leonelli", "Manuele", ""], ["Riccomagno", "Eva", ""], ["Varando", "Gherardo", ""]]}, {"id": "2004.06477", "submitter": "Babagana Modu", "authors": "Babagana Modu, Nereida Polovina, Savas Konur", "title": "Agent-Based Modelling of Malaria Transmission Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent statistics of malaria shows that over 200 million cases and estimated\ndeaths of nearly half a million occur globally. Africa alone accounts for\nalmost 90% of the cases. Several studies have been conducted to understand the\ndisease transmission dynamics. In particular, mathematical methods have been\nfrequently used to model and understand the disease dynamics and outbreak\npatterns. Although, mathematical methods have provided good results for\nhomogeneous populations, these methods impose significant limitations for\nstudying malaria dynamics in heterogeneous populations, a result of various\nfactors, e.g. spatial and temporal fluctuations, social networks, human\nmovements pattern etc. This paper proposes an agent-based modelling approach\nthat permits modelling and analysing malaria dynamics for heterogenous\npopulations. Our approach is illustrated using the climate and demographic data\nfor the Tripura, Limpopo and Benin cities. Our agent-based simulation has been\nvalidated against the reported cases of malaria collected in the cities\nmentioned. Furthermore, the efficiency of the proposed model has been compared\nwith the mathematical model used as benchmark. A statistical test confirms the\nproposed model is robust and has potential for predicting the peak seasons of\nmalaria. This potentially makes our methods a useful tool as an intervention\nmechanism, which will have impact on hospitals, healthcare providers, health\norganisations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 08:04:29 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Modu", "Babagana", ""], ["Polovina", "Nereida", ""], ["Konur", "Savas", ""]]}, {"id": "2004.06483", "submitter": "Sina Taheri", "authors": "Sina Taheri, Vassilis Kekatos, Harsha Veeramachaneni", "title": "Strategic Investment in Energy Markets: A Multiparametric Programming\n  Approach", "comments": "Submitted to IEEE Transaction Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An investor has to carefully select the location and size of new generation\nunits it intends to build, since adding capacity in a market affects the profit\nfrom units this investor may already own. To capture this closed-loop\ncharacteristic, strategic investment (SI) can be posed as a bilevel\noptimization. By analytically studying a small market, we first show that its\nobjective function can be non-convex and discontinuous. Realizing that existing\nmixed-integer problem formulations become impractical for larger markets and\nincreasing number of scenarios, this work put forth two SI solvers: a grid\nsearch to handle setups where the candidate investment locations are few, and a\nstochastic gradient descent approach for otherwise. Both solvers leverage the\npowerful toolbox of multiparametric programming (MPP), each in a unique way.\nThe grid search entails finding the primal/dual solutions for a large number of\noptimal power flow (OPF) problems, which nonetheless can be efficiently\ncomputed several at once thanks to the properties of MPP. The same properties\nfacilitate the rapid calculation of gradients in a mini-batch fashion, thus\naccelerating the implementation of a stochastic gradient descent search. Tests\non the IEEE 118-bus system using real-world data corroborate the advantages of\nthe novel MPP-aided solvers.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:27:28 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 00:19:02 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Taheri", "Sina", ""], ["Kekatos", "Vassilis", ""], ["Veeramachaneni", "Harsha", ""]]}, {"id": "2004.06568", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Rita SahaRay, Sayan Chakrabarty, Sayan Bhadra", "title": "Robust Generalised Quadratic Discriminant Analysis", "comments": "Pre-print. Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic discriminant analysis (QDA) is a widely used statistical tool to\nclassify observations from different multivariate Normal populations. The\ngeneralized quadratic discriminant analysis (GQDA) classification\nrule/classifier, which generalizes the QDA and the minimum Mahalanobis distance\n(MMD) classifiers to discriminate between populations with underlying\nelliptically symmetric distributions competes quite favorably with the QDA\nclassifier when it is optimal and performs much better when QDA fails under\nnon-Normal underlying distributions, e.g. Cauchy distribution. However, the\nclassification rule in GQDA is based on the sample mean vector and the sample\ndispersion matrix of a training sample, which are extremely non-robust under\ndata contamination. In real world, since it is quite common to face data highly\nvulnerable to outliers, the lack of robustness of the classical estimators of\nthe mean vector and the dispersion matrix reduces the efficiency of the GQDA\nclassifier significantly, increasing the misclassification errors. The present\npaper investigates the performance of the GQDA classifier when the classical\nestimators of the mean vector and the dispersion matrix used therein are\nreplaced by various robust counterparts. Applications to various real data sets\nas well as simulation studies reveal far better performance of the proposed\nrobust versions of the GQDA classifier. A Comparative study has been made to\nadvocate the appropriate choice of the robust estimators to be used in a\nspecific situation of the degree of contamination of the data sets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 18:21:06 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ghosh", "Abhik", ""], ["SahaRay", "Rita", ""], ["Chakrabarty", "Sayan", ""], ["Bhadra", "Sayan", ""]]}, {"id": "2004.06586", "submitter": "Xiaochun Meng", "authors": "Carol Alexander, Xiaochun Meng, Wei Wei", "title": "Extensions of Random Orthogonal Matrix Simulation for Targetting Kollo\n  Skewness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST q-fin.CP q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling multivariate systems is important for many applications in\nengineering and operational research. The multivariate distributions under\nscrutiny usually have no analytic or closed form. Therefore their modelling\nemploys a numerical technique, typically multivariate simulations, which can\nhave very high dimensions. Random Orthogonal Matrix (ROM) simulation is a\nmethod that has gained some popularity because of the absence of certain\nsimulation errors. Specifically, it exactly matches a target mean, covariance\nmatrix and certain higher moments with every simulation. This paper extends the\nROM simulation algorithm presented by Hanke et al. (2017), hereafter referred\nto as HPSW, which matches the target mean, covariance matrix and Kollo skewness\nvector exactly. Our first contribution is to establish necessary and sufficient\nconditions for the HPSW algorithm to work. Our second contribution is to\ndevelop a general approach for constructing admissible values in the HPSW. Our\nthird theoretical contribution is to analyse the effect of multivariate sample\nconcatenation on the target Kollo skewness. Finally, we illustrate the\nextensions we develop here using a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:18:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 16:51:37 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Alexander", "Carol", ""], ["Meng", "Xiaochun", ""], ["Wei", "Wei", ""]]}, {"id": "2004.06857", "submitter": "Sanjeena Subedi", "authors": "Sanjeena Subedi and Ryan Browne", "title": "A parsimonious family of multivariate Poisson-lognormal distributions\n  for clustering multivariate count data", "comments": "31 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate count data are commonly encountered through high-throughput\nsequencing technologies in bioinformatics, text mining, or in sports analytics.\nAlthough the Poisson distribution seems a natural fit to these count data, its\nmultivariate extension is computationally expensive.In most cases mutual\nindependence among the variables is assumed, however this fails to take into\naccount the correlation among the variables usually observed in the data.\nRecently, mixtures of multivariate Poisson-lognormal (MPLN) models have been\nused to analyze such multivariate count measurements with a dependence\nstructure. In the MPLN model, each count is modeled using an independent\nPoisson distribution conditional on a latent multivariate Gaussian variable.\nDue to this hierarchical structure, the MPLN model can account for\nover-dispersion as opposed to the traditional Poisson distribution and allows\nfor correlation between the variables. Rather than relying on a Monte\nCarlo-based estimation framework which is computationally inefficient, a fast\nvariational-EM based framework is used here for parameter estimation. Further,\na parsimonious family of mixtures of Poisson-lognormal distributions are\nproposed by decomposing the covariance matrix and imposing constraints on these\ndecompositions. Utility of such models is shown using simulated and benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 02:09:56 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Subedi", "Sanjeena", ""], ["Browne", "Ryan", ""]]}, {"id": "2004.07083", "submitter": "Izhar Asael Alonzo Matamoros", "authors": "Izhar Asael Alonzo Matamoros", "title": "An introduction to computational complexity in Markov Chain Monte Carlo\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to give an introduction to the theoretical background\nand computational complexity of Markov chain Monte Carlo methods. Most of the\nmathematical results related to the convergence are not found in most of the\nstatistical references, and computational complexity is still an open question\nfor most of the MCMC methods. In this work, we provide a general overview,\nreferences, and discussion about all these theoretical subjects.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:13:47 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Matamoros", "Izhar Asael Alonzo", ""]]}, {"id": "2004.07177", "submitter": "Jonas Latz", "authors": "Jonas Latz", "title": "Analysis of Stochastic Gradient Descent in Continuous Time", "comments": null, "journal-ref": "Statistics and Computing 31, 39, 2021", "doi": "10.1007/s11222-021-10016-8", "report-no": null, "categories": "math.PR cs.LG cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent is an optimisation method that combines classical\ngradient descent with random subsampling within the target functional. In this\nwork, we introduce the stochastic gradient process as a continuous-time\nrepresentation of stochastic gradient descent. The stochastic gradient process\nis a dynamical system that is coupled with a continuous-time Markov process\nliving on a finite state space. The dynamical system -- a gradient flow --\nrepresents the gradient descent part, the process on the finite state space\nrepresents the random subsampling. Processes of this type are, for instance,\nused to model clonal populations in fluctuating environments. After introducing\nit, we study theoretical properties of the stochastic gradient process: We show\nthat it converges weakly to the gradient flow with respect to the full target\nfunction, as the learning rate approaches zero. We give conditions under which\nthe stochastic gradient process with constant learning rate is exponentially\nergodic in the Wasserstein sense. Then we study the case, where the learning\nrate goes to zero sufficiently slowly and the single target functions are\nstrongly convex. In this case, the process converges weakly to the point mass\nconcentrated in the global minimum of the full target function; indicating\nconsistency of the method. We conclude after a discussion of discretisation\nstrategies for the stochastic gradient process and numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 16:04:41 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 22:49:08 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 14:07:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Latz", "Jonas", ""]]}, {"id": "2004.07403", "submitter": "Nisheeth Vishnoi", "authors": "Jonathan Leake and Nisheeth K. Vishnoi", "title": "On the computability of continuous maximum entropy distributions with\n  applications", "comments": "50 pages, STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate a study of the following problem: Given a continuous domain\n$\\Omega$ along with its convex hull $\\mathcal{K}$, a point $A \\in \\mathcal{K}$\nand a prior measure $\\mu$ on $\\Omega$, find the probability density over\n$\\Omega$ whose marginal is $A$ and that minimizes the KL-divergence to $\\mu$.\nThis framework gives rise to several extremal distributions that arise in\nmathematics, quantum mechanics, statistics, and theoretical computer science.\nOur technical contributions include a polynomial bound on the norm of the\noptimizer of the dual problem that holds in a very general setting and relies\non a \"balance\" property of the measure $\\mu$ on $\\Omega$, and exact algorithms\nfor evaluating the dual and its gradient for several interesting settings of\n$\\Omega$ and $\\mu$. Together, along with the ellipsoid method, these results\nimply polynomial-time algorithms to compute such KL-divergence minimizing\ndistributions in several cases. Applications of our results include: 1) an\noptimization characterization of the Goemans-Williamson measure that is used to\nround a positive semidefinite matrix to a vector, 2) the computability of the\nentropic barrier for polytopes studied by Bubeck and Eldan, and 3) a\npolynomial-time algorithm to compute the barycentric quantum entropy of a\ndensity matrix that was proposed as an alternative to von Neumann entropy in\nthe 1970s: this corresponds to the case when $\\Omega$ is the set of rank one\nprojections matrices and $\\mu$ corresponds to the Haar measure on the unit\nsphere. Our techniques generalize to the setting of Hermitian rank $k$\nprojections using the Harish-Chandra-Itzykson-Zuber formula, and are applicable\neven beyond, to adjoint orbits of compact Lie groups.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 00:41:40 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Leake", "Jonathan", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2004.07429", "submitter": "Uri Keich", "authors": "Noah Peres, Andrew Lee, Uri Keich", "title": "Exactly computing the tail of the Poisson-Binomial Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer ShiftConvolvePoibin, a fast exact method to compute the tail of a\nPoisson-Binomial distribution (PBD). Our method employs an exponential shift to\nretain its accuracy when computing a tail probability, and in practice we find\nthat it is immune to the significant relative errors that other methods, exact\nor approximate, can suffer from when computing very small tail probabilities of\nthe PBD. The accompanying R package is also competitive with the fastest\nimplementations for computing the entire PBD.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:04:27 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Peres", "Noah", ""], ["Lee", "Andrew", ""], ["Keich", "Uri", ""]]}, {"id": "2004.07471", "submitter": "Dootika Vats", "authors": "Dootika Vats, Fl\\'avio Gon\\c{c}alves, Krzysztof {\\L}atuszy\\'nski,\n  Gareth O. Roberts", "title": "Efficient Bernoulli factory MCMC for intractable posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accept-reject based Markov chain Monte Carlo (MCMC) algorithms have\ntraditionally utilised acceptance probabilities that can be explicitly written\nas a function of the ratio of the target density at the two contested points.\nThis feature is rendered almost useless in Bayesian posteriors with unknown\nfunctional forms. We introduce a new family of MCMC acceptance probabilities\nthat has the distinguishing feature of not being a function of the ratio of the\ntarget density at the two points. We present two stable Bernoulli factories\nthat generate events within this class of acceptance probabilities. The\nefficiency of our methods rely on obtaining reasonable local upper or lower\nbounds on the target density and we present two classes of problems where such\nbounds are viable: Bayesian inference for diffusions and MCMC on constrained\nspaces. The resulting portkey Barker's algorithms are exact and computationally\nmore efficient that the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 06:07:04 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 18:47:42 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 09:26:19 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Vats", "Dootika", ""], ["Gon\u00e7alves", "Fl\u00e1vio", ""], ["\u0141atuszy\u0144ski", "Krzysztof", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2004.07579", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen and Siliang Zhang", "title": "Estimation Methods for Item Factor Analysis: An Overview", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item factor analysis (IFA) refers to the factor models and statistical\ninference procedures for analyzing multivariate categorical data. IFA\ntechniques are commonly used in social and behavioral sciences for analyzing\nitem-level response data. Such models summarize and interpret the dependence\nstructure among a set of categorical variables by a small number of latent\nfactors. In this chapter, we review the IFA modeling technique and commonly\nused IFA models. Then we discuss estimation methods for IFA models and their\ncomputation, with a focus on the situation where the sample size, the number of\nitems, and the number of factors are all large. Existing statistical softwares\nfor IFA are surveyed. This chapter is concluded with suggestions for practical\napplications of IFA methods and discussions of future directions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:39:35 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Chen", "Yunxiao", ""], ["Zhang", "Siliang", ""]]}, {"id": "2004.07859", "submitter": "Hiteshi Tandon", "authors": "Hiteshi Tandon, Prabhat Ranjan, Tanmoy Chakraborty and Vandana Suhag", "title": "Coronavirus (COVID-19): ARIMA based time-series analysis to forecast\n  near future", "comments": "Pages: 11, Tables: 4, Figures: 6; Author Contributions: H.T. and T.C.\n  conceptualized the project. H.T. designed the study, performed the\n  computations and investigations, contributed to data analysis and wrote the\n  manuscript. P.R. provided the resources. T.C. and V.S. supervised the study\n  and reviewed the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19, a novel coronavirus, is currently a major worldwide threat. It has\ninfected more than a million people globally leading to hundred-thousands of\ndeaths. In such grave circumstances, it is very important to predict the future\ninfected cases to support prevention of the disease and aid in the healthcare\nservice preparation. Following that notion, we have developed a model and then\nemployed it for forecasting future COVID-19 cases in India. The study indicates\nan ascending trend for the cases in the coming days. A time series analysis\nalso presents an exponential increase in the number of cases. It is supposed\nthat the present prediction models will assist the government and medical\npersonnel to be prepared for the upcoming conditions and have more readiness in\nhealthcare systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 18:12:08 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Tandon", "Hiteshi", ""], ["Ranjan", "Prabhat", ""], ["Chakraborty", "Tanmoy", ""], ["Suhag", "Vandana", ""]]}, {"id": "2004.07878", "submitter": "Alfredo Garbuno-Inigo", "authors": "Alfredo Garbuno-Inigo, F. Alejandro DiazDelaO, Konstantin M. Zuev", "title": "History matching with probabilistic emulators and active learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific understanding of real-world processes has dramatically\nimproved over the years through computer simulations. Such simulators represent\ncomplex mathematical models that are implemented as computer codes which are\noften expensive. The validity of using a particular simulator to draw accurate\nconclusions relies on the assumption that the computer code is correctly\ncalibrated. This calibration procedure is often pursued under extensive\nexperimentation and comparison with data from a real-world process. The problem\nis that the data collection may be so expensive that only a handful of\nexperiments are feasible. History matching is a calibration technique that,\ngiven a simulator, it iteratively discards regions of the input space using an\nimplausibility measure. When the simulator is computationally expensive, an\nemulator is used to explore the input space. In this paper, a Gaussian process\nprovides a complete probabilistic output that is incorporated into the\nimplausibility measure. The identification of regions of interest is\naccomplished with recently developed annealing sampling techniques. Active\nlearning functions are incorporated into the history matching procedure to\nrefocus on the input space and improve the emulator. The efficiency of the\nproposed framework is tested in well-known examples from the history matching\nliterature, as well as in a proposed testbed of functions of higher dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 18:57:53 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Garbuno-Inigo", "Alfredo", ""], ["DiazDelaO", "F. Alejandro", ""], ["Zuev", "Konstantin M.", ""]]}, {"id": "2004.08000", "submitter": "Ruiyi Yang", "authors": "Daniel Sanz-Alonso and Ruiyi Yang", "title": "The SPDE Approach to Mat\\'ern Fields: Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates Gaussian Markov random field approximations to\nnonstationary Gaussian fields using graph representations of stochastic partial\ndifferential equations. We establish approximation error guarantees building on\nthe theory of spectral convergence of graph Laplacians. The proposed graph\nrepresentations provide a generalization of the Mat\\'ern model to unstructured\npoint clouds, and facilitate inference and sampling using linear algebra\nmethods for sparse matrices. In addition, they bridge and unify several models\nin Bayesian inverse problems, spatial statistics and graph-based machine\nlearning. We demonstrate through examples in these three disciplines that the\nunity revealed by graph representations facilitates the exchange of ideas\nacross them.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 23:58:36 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 19:44:58 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 02:28:41 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Sanz-Alonso", "Daniel", ""], ["Yang", "Ruiyi", ""]]}, {"id": "2004.08064", "submitter": "Fan Yin", "authors": "Fan Yin, Carter T. Butts", "title": "Kernel-based Approximate Bayesian Inference for Exponential Family\n  Random Graph Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for exponential family random graph models (ERGMs) is a\ndoubly-intractable problem because of the intractability of both the likelihood\nand posterior normalizing factor. Auxiliary variable based Markov Chain Monte\nCarlo (MCMC) methods for this problem are asymptotically exact but\ncomputationally demanding, and are difficult to extend to modified ERGM\nfamilies. In this work, we propose a kernel-based approximate Bayesian\ncomputation algorithm for fitting ERGMs. By employing an adaptive importance\nsampling technique, we greatly improve the efficiency of the sampling step.\nThough approximate, our easily parallelizable approach is yields comparable\naccuracy to state-of-the-art methods with substantial improvements in compute\ntime on multi-core hardware. Our approach also flexibly accommodates both\nalgorithmic enhancements (including improved learning algorithms for estimating\nconditional expectations) and extensions to non-standard cases such as\ninference from non-sufficient statistics. We demonstrate the performance of\nthis approach on two well-known network data sets, comparing its accuracy and\nefficiency with results obtained using the approximate exchange algorithm. Our\ntests show a wallclock time advantage of up to 50% with five cores, and the\nability to fit models in 1/5th the time at 30 cores; further speed enhancements\nare possible when more cores are available.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 05:12:03 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 03:56:17 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Yin", "Fan", ""], ["Butts", "Carter T.", ""]]}, {"id": "2004.08115", "submitter": "Meixia Lin", "authors": "Meixia Lin, Defeng Sun, Kim-Chuan Toh, Chengjing Wang", "title": "Estimation of sparse Gaussian graphical models with hidden clustering\n  structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of Gaussian graphical models is important in natural science when\nmodeling the statistical relationships between variables in the form of a\ngraph. The sparsity and clustering structure of the concentration matrix is\nenforced to reduce model complexity and describe inherent regularities. We\npropose a model to estimate the sparse Gaussian graphical models with hidden\nclustering structure, which also allows additional linear constraints to be\nimposed on the concentration matrix. We design an efficient two-phase algorithm\nfor solving the proposed model. We develop a symmetric Gauss-Seidel based\nalternating direction method of the multipliers (sGS-ADMM) to generate an\ninitial point to warm-start the second phase algorithm, which is a proximal\naugmented Lagrangian method (pALM), to get a solution with high accuracy.\nNumerical experiments on both synthetic data and real data demonstrate the good\nperformance of our model, as well as the efficiency and robustness of our\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:43:31 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Lin", "Meixia", ""], ["Sun", "Defeng", ""], ["Toh", "Kim-Chuan", ""], ["Wang", "Chengjing", ""]]}, {"id": "2004.08336", "submitter": "J\\\"uri Lember", "authors": "Alexey Koloydenko, Kristi Kuljus, J\\\"uri Lember", "title": "MAP segmentation in Bayesian hidden Markov models: a case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the maximum posterior probability (MAP)\nstate sequence for a finite state and finite emission alphabet hidden Markov\nmodel (HMM) in the Bayesian setup, where both emission and transition matrices\nhave Dirichlet priors. We study a training set consisting of thousands of\nprotein alignment pairs. The training data is used to set the prior\nhyperparameters for Bayesian MAP segmentation. Since the Viterbi algorithm is\nnot applicable any more, there is no simple procedure to find the MAP path, and\nseveral iterative algorithms are considered and compared. The main goal of the\npaper is to test the Bayesian setup against the frequentist one, where the\nparameters of HMM are estimated using the training data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 16:42:18 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Koloydenko", "Alexey", ""], ["Kuljus", "Kristi", ""], ["Lember", "J\u00fcri", ""]]}, {"id": "2004.08376", "submitter": "Jinlong Wu", "authors": "Tapio Schneider, Andrew M. Stuart, Jin-Long Wu", "title": "Learning Stochastic Closures Using Ensemble Kalman Inversion", "comments": "35 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the governing equations of many systems, when derived from first\nprinciples, may be viewed as known, it is often too expensive to numerically\nsimulate all the interactions they describe. Therefore researchers often seek\nsimpler descriptions that describe complex phenomena without numerically\nresolving all the interacting components. Stochastic differential equations\n(SDEs) arise naturally as models in this context. The growth in data\nacquisition, both through experiment and through simulations, provides an\nopportunity for the systematic derivation of SDE models in many disciplines.\nHowever, inconsistencies between SDEs and real data at short time scales often\ncause problems, when standard statistical methodology is applied to parameter\nestimation. The incompatibility between SDEs and real data can be addressed by\nderiving sufficient statistics from the time-series data and learning\nparameters of SDEs based on these. Following this approach, we formulate the\nfitting of SDEs to sufficient statistics from real data as an inverse problem\nand demonstrate that this inverse problem can be solved by using ensemble\nKalman inversion (EKI). Furthermore, we create a framework for non-parametric\nlearning of drift and diffusion terms by introducing hierarchical, refinable\nparameterizations of unknown functions, using Gaussian process regression. We\ndemonstrate the proposed methodology for the fitting of SDE models, first in a\nsimulation study with a noisy Lorenz '63 model, and then in other applications,\nincluding dimension reduction in deterministic chaotic systems arising in the\natmospheric sciences, large-scale pattern modeling in climate dynamics, and\nsimplified models for key observables arising in molecular dynamics. The\nresults confirm that the proposed methodology provides a robust and systematic\napproach to fitting SDE models to real data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:55:56 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 05:07:12 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Schneider", "Tapio", ""], ["Stuart", "Andrew M.", ""], ["Wu", "Jin-Long", ""]]}, {"id": "2004.08492", "submitter": "Zhishi Wang", "authors": "Edwin Ng, Zhishi Wang, Huigang Chen, Steve Yang, Slawek Smyl", "title": "Orbit: Probabilistic Forecast with Exponential Smoothing", "comments": "arXiv admin note: text overlap with arXiv:1909.13316 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is an active research topic in academia as well as\nindustry. Although we see an increasing amount of adoptions of machine learning\nmethods in solving some of those forecasting challenges, statistical methods\nremain powerful while dealing with low granularity data. This paper introduces\na refined Bayesian exponential smoothing model with the help of probabilistic\nprogramming languages including Stan. Our model refinements include additional\nglobal trend, transformation for multiplicative form, noise distribution and\nchoice of priors. A benchmark study is conducted on a rich set of time-series\ndata sets for our models along with other well-known time series models.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 00:21:53 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 20:24:22 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 00:47:46 GMT"}, {"version": "v4", "created": "Fri, 22 Jan 2021 22:05:11 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ng", "Edwin", ""], ["Wang", "Zhishi", ""], ["Chen", "Huigang", ""], ["Yang", "Steve", ""], ["Smyl", "Slawek", ""]]}, {"id": "2004.08533", "submitter": "Tanmay Sen", "authors": "Tanmay Sen, Ritwik Bhattacharya, Biswabrata Pradhan and Yogesh Mani\n  Tripathi", "title": "Determination of Bayesian optimal warranty length under Type-II unified\n  hybrid censoring scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determination of an appropriate warranty length for the lifetime of the\nproduct is an important issue to the manufacturer. In this article, optimal\nwarranty length of the product for the combined free replacement and the\npro-rata warranty policy is computed based on the Type-II unified hybrid\ncensored data. A non-linear pro-rata warranty policy is proposed in this\ncontext. The optimal warranty length is obtained by maximizing an expected\nutility function. The expectation is taken with respect to the posterior\npredictive model for the time-to-failure data. It is observed that the\nnon-linear pro-rata warranty policy gives a larger warranty length with maximum\nprofit as compared to linear warranty policy. Finally, a real-data set is\nanalyzed in order to illustrate the advantage of using non-linear pro-rata\nwarranty policy.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 05:54:28 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sen", "Tanmay", ""], ["Bhattacharya", "Ritwik", ""], ["Pradhan", "Biswabrata", ""], ["Tripathi", "Yogesh Mani", ""]]}, {"id": "2004.08705", "submitter": "Victor Gallego", "authors": "Victor Gallego, Roi Naveiro, Alberto Redondo, David Rios Insua,\n  Fabrizio Ruggeri", "title": "Protecting Classifiers From Attacks. A Bayesian Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification problems in security settings are usually modeled as\nconfrontations in which an adversary tries to fool a classifier manipulating\nthe covariates of instances to obtain a benefit. Most approaches to such\nproblems have focused on game-theoretic ideas with strong underlying common\nknowledge assumptions, which are not realistic in the security realm. We\nprovide an alternative Bayesian framework that accounts for the lack of precise\nknowledge about the attacker's behavior using adversarial risk analysis. A key\ningredient required by our framework is the ability to sample from the\ndistribution of originating instances given the possibly attacked observed one.\nWe propose a sampling procedure based on approximate Bayesian computation, in\nwhich we simulate the attacker's problem taking into account our uncertainty\nabout his elements. For large scale problems, we propose an alternative,\nscalable approach that could be used when dealing with differentiable\nclassifiers. Within it, we move the computational load to the training phase,\nsimulating attacks from an adversary, adapting the framework to obtain a\nclassifier robustified against attacks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 21:21:56 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Gallego", "Victor", ""], ["Naveiro", "Roi", ""], ["Redondo", "Alberto", ""], ["Insua", "David Rios", ""], ["Ruggeri", "Fabrizio", ""]]}, {"id": "2004.08807", "submitter": "Jere Koskela", "authors": "Jere Koskela", "title": "Zig-zag sampling for discrete structures and non-reversible phylogenetic\n  MCMC", "comments": "21 pages, 10 figures, 3 tables. This is a major revision which\n  introduces a generic zig-zag process for a hybrid target with both discrete\n  and continuous parameters. Applications to the coalescent from earlier\n  versions are retained as examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST q-bio.PE stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a zig-zag process targeting a posterior distribution defined on\na hybrid state space consisting of both discrete and continuous variables. The\nconstruction does not require any assumptions on the structure among discrete\nvariables. We demonstrate our method on two examples in genetics based on the\nKingman coalescent, showing that the zig-zag process can lead to efficiency\ngains of up to several orders of magnitude over classical Metropolis--Hastings\nalgorithms, and that it is well suited to parallel computation. Our\nconstruction resembles existing techniques for Hamiltonian Monte Carlo on a\nhybrid state space, which suffers from implementationally and analytically\ncomplex boundary crossings when applied to the coalescent. We demonstrate that\nthe continuous-time zig-zag process avoids these complications.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 10:30:34 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 14:19:59 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 15:14:32 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Koskela", "Jere", ""]]}, {"id": "2004.09623", "submitter": "Bryan W. Ting", "authors": "Bryan W. Ting, Fred A. Wright, Yi-Hui Zhou", "title": "Fast Multivariate Probit Estimation via a Two-Stage Composite Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate probit is popular for modeling correlated binary data, with\nan attractive balance of flexibility and simplicity. However, considerable\nchallenges remain in computation and in devising a clear statistical framework.\nInterest in the multivariate probit has increased in recent years. Current\napplications include genomics and precision medicine, where simultaneous\nmodeling of multiple traits may be of interest, and computational efficiency is\nan important consideration. We propose a fast method for multivariate probit\nestimation via a two-stage composite likelihood. We explore computational and\nstatistical efficiency, and note that the approach sets the stage for\nextensions beyond the purely binary setting.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:37:16 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Ting", "Bryan W.", ""], ["Wright", "Fred A.", ""], ["Zhou", "Yi-Hui", ""]]}, {"id": "2004.09887", "submitter": "Yiou Li", "authors": "Yiou Li, Lulu Kang, Fred J. Hickernell", "title": "Is a Transformed Low Discrepancy Design Also Low Discrepancy?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental designs intended to match arbitrary target distributions are\ntypically constructed via a variable transformation of a uniform experimental\ndesign. The inverse distribution function is one such transformation. The\ndiscrepancy is a measure of how well the empirical distribution of any design\nmatches its target distribution. This chapter addresses the question of whether\na variable transformation of a low discrepancy uniform design yields a low\ndiscrepancy design for the desired target distribution. The answer depends on\nthe two kernel functions used to define the respective discrepancies. If these\nkernels satisfy certain conditions, then the answer is yes. However, these\nconditions may be undesirable for practical reasons. In such a case, the\ntransformation of a low discrepancy uniform design may yield a design with a\nlarge discrepancy. We illustrate how this may occur. We also suggest some\nremedies. One remedy is to ensure that the original uniform design has optimal\none-dimensional projection, but this remedy works best if the design is dense,\nor in other words, the ratio of sample size divided by the dimension of the\nrandom variable is relatively large. Another remedy is to use the transformed\ndesign as the input to a coordinate-exchange algorithm that optimizes the\ndesired discrepancy, and this works for both dense or sparse designs. The\neffectiveness of these two remedies is illustrated via simulation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 10:30:13 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Yiou", ""], ["Kang", "Lulu", ""], ["Hickernell", "Fred J.", ""]]}, {"id": "2004.10092", "submitter": "Oskar Gustafsson", "authors": "Oskar Gustafsson, Mattias Villani and P\\\"ar Stockhammar", "title": "Bayesian Optimization of Hyperparameters when the Marginal Likelihood is\n  Estimated by MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models often involve a small set of hyperparameters determined by\nmaximizing the marginal likelihood. Bayesian optimization is a popular\niterative method where a Gaussian process posterior of the underlying function\nis sequentially updated by new function evaluations. An acquisition strategy\nuses this posterior distribution to decide where to place the next function\nevaluation. We propose a novel Bayesian optimization framework for situations\nwhere the user controls the computational effort, and therefore the precision\nof the function evaluations. This is a common situation in econometrics where\nthe marginal likelihood is often computed by Markov Chain Monte Carlo (MCMC)\nmethods, with the precision of the marginal likelihood estimate determined by\nthe number of MCMC draws. The proposed acquisition strategy gives the optimizer\nthe option to explore the function with cheap noisy evaluations and therefore\nfinds the optimum faster. Prior hyperparameter estimation in the steady-state\nBayesian vector autoregressive (BVAR) model on US macroeconomic time series\ndata is used for illustration. The proposed method is shown to find the optimum\nmuch quicker than traditional Bayesian optimization or grid search.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:11:19 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Gustafsson", "Oskar", ""], ["Villani", "Mattias", ""], ["Stockhammar", "P\u00e4r", ""]]}, {"id": "2004.10101", "submitter": "Luc Villandr\\'e", "authors": "Luc Villandr\\'e, Jean-Fran\\c{c}ois Plante, Thierry Duchesne, Patrick\n  Brown", "title": "Fully Bayesian inference for spatiotemporal data with the\n  multi-resolution approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large spatiotemporal datasets are a challenge for conventional Bayesian\nmodels because of the cubic computational complexity of the algorithms for\nobtaining the Cholesky decomposition of the covariance matrix in the\nmultivariate normal density. Moreover, standard numerical algorithms for\nposterior estimation, such as Markov Chain Monte Carlo (MCMC), are intractable\nin this context, as they require thousands, if not millions, of costly\nlikelihood evaluations. To overcome those limitations, we propose IS-MRA\n(Importance sampling - Multi-Resolution Approximation), which takes advantage\nof the sparse inverse covariance structure produced by the Multi-Resolution\nApproximation (MRA) approach. IS-MRA is fully Bayesian and facilitates the\napproximation of the hyperparameter marginal posterior distributions. We apply\nIS-MRA to large MODIS Level 3 Land Surface Temperature (LST) datasets, sampled\nbetween May 18 and May 31, 2012 in the western part of the state of\nMaharashtra, India. We find that IS-MRA can produce realistic prediction\nsurfaces over regions where concentrated missingness, caused by sizable cloud\ncover, is observed. Through a validation analysis and simulation study, we also\nfind that predictions tend to be very accurate.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:21:31 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 21:47:10 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 22:26:54 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Villandr\u00e9", "Luc", ""], ["Plante", "Jean-Fran\u00e7ois", ""], ["Duchesne", "Thierry", ""], ["Brown", "Patrick", ""]]}, {"id": "2004.10678", "submitter": "Jean-Baptiste Delisle", "authors": "J.-B. Delisle, N. Hara, and D. S\\'egransan", "title": "Efficient modeling of correlated noise II. A flexible noise model with\n  fast and scalable methods", "comments": "Accepted in A&A", "journal-ref": "A&A 638, A95 (2020)", "doi": "10.1051/0004-6361/201936906", "report-no": null, "categories": "astro-ph.IM astro-ph.EP astro-ph.SR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated noise affects most astronomical datasets and to neglect accounting\nfor it can lead to spurious signal detections, especially in low\nsignal-to-noise conditions, which is often the context in which new discoveries\nare pursued. For instance, in the realm of exoplanet detection with radial\nvelocity time series, stellar variability can induce false detections. However,\na white noise approximation is often used because accounting for correlated\nnoise when analyzing data implies a more complex analysis. Moreover, the\ncomputational cost can be prohibitive as it typically scales as the cube of the\ndataset size.\n  For some restricted classes of correlated noise models, there are specific\nalgorithms that can be used to help bring down the computational cost. This\nimprovement in speed is particularly useful in the context of Gaussian process\nregression, however, it comes at the expense of the generality of the noise\nmodel.\n  Here, we present the S+LEAF noise model, which allows us to account for a\nlarge class of correlated noises with a linear scaling of the computational\ncost with respect to the size of the dataset. The S+LEAF model includes, in\nparticular, mixtures of quasiperiodic kernels and calibration noise. This\nefficient modeling is made possible by a sparse representation of the\ncovariance matrix of the noise and the use of dedicated algorithms for matrix\ninversion, solving, determinant computation, etc.\n  We applied the S+LEAF model to reanalyze the HARPS radial velocity time\nseries of HD 136352. We illustrate the flexibility of the S+LEAF model in\nhandling various sources of noise. We demonstrate the importance of taking\ncorrelated noise into account, and especially calibration noise, to correctly\nassess the significance of detected signals.\n  We provide an open-source implementation of the S+LEAF model, available at\nhttps://gitlab.unige.ch/jean-baptiste.delisle/spleaf.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 16:24:25 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Delisle", "J. -B.", ""], ["Hara", "N.", ""], ["S\u00e9gransan", "D.", ""]]}, {"id": "2004.10701", "submitter": "John Ram\\'irez", "authors": "John Ram\\'irez-Figueroa, Carlos Mart\\'in-Barreiro, Ana B.\n  Nieto-Librero, Victor Leiva-S\\'anchez, Purificaci\\'on Galindo-Villard\\'on", "title": "Disjoint principal component analysis by constrained binary particle\n  swarm optimization", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an alternative method to the disjoint principal\ncomponent analysis. The method consists of a principal component analysis with\nconstraints, which allows us to determine disjoint components that are linear\ncombinations of disjoint subsets of the original variables. The proposed method\nis named constrained binary optimization by particle swarm disjoint principal\ncomponent analysis, since it is based on the particle swarm optimization. The\nmethod uses stochastic optimization to find solutions in cases of high\ncomputational complexity. The algorithm associated with the method starts\ngenerating randomly a particle population which iteratively evolves until\nattaining a global optimum which is function of the disjoint components.\nNumerical results are provided to confirm the quality of the solutions attained\nby the proposed method. Illustrative examples with real data are conducted to\nshow the potential applications of the method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:07:19 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Ram\u00edrez-Figueroa", "John", ""], ["Mart\u00edn-Barreiro", "Carlos", ""], ["Nieto-Librero", "Ana B.", ""], ["Leiva-S\u00e1nchez", "Victor", ""], ["Galindo-Villard\u00f3n", "Purificaci\u00f3n", ""]]}, {"id": "2004.11193", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Pietro Spitali, Roula Tsonaka", "title": "Poisson-Tweedie mixed-effects model: a flexible approach for the\n  analysis of longitudinal RNA-seq data", "comments": "The final (published) version of the article can be downloaded for\n  free (Open Access) from the editor's website (click on the DOI link below).\n  Link to the R package ptmixed:\n  https://cran.r-project.org/web/packages/ptmixed/index.html", "journal-ref": "Statistical Modelling (2020)", "doi": "10.1177/1471082X20936017", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new modelling approach for longitudinal count data that is\nmotivated by the increasing availability of longitudinal RNA-sequencing\nexperiments. The distribution of RNA-seq counts typically exhibits\noverdispersion, zero-inflation and heavy tails; moreover, in longitudinal\ndesigns repeated measurements from the same subject are typically (positively)\ncorrelated. We propose a generalized linear mixed model based on the\nPoisson-Tweedie distribution that can flexibly handle each of the\naforementioned features of longitudinal overdispersed counts. We develop a\ncomputational approach to accurately evaluate the likelihood of the proposed\nmodel and to perform maximum likelihood estimation. Our approach is implemented\nin the R package ptmixed, which can be freely downloaded from CRAN. We assess\nthe performance of ptmixed on simulated data and we present an application to a\ndataset with longitudinal RNA-sequencing measurements from healthy and\ndystrophic mice. The applicability of the Poisson-Tweedie mixed-effects model\nis not restricted to longitudinal RNA-seq data, but it extends to any scenario\nwhere non-independent measurements of a discrete overdispersed response\nvariable are available.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:38:02 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 08:13:01 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Signorelli", "Mirko", ""], ["Spitali", "Pietro", ""], ["Tsonaka", "Roula", ""]]}, {"id": "2004.11408", "submitter": "Paul-Christian B\\\"urkner", "authors": "Gabriel Riutort-Mayol, Paul-Christian B\\\"urkner, Michael R. Andersen,\n  Arno Solin, Aki Vehtari", "title": "Practical Hilbert space approximate Bayesian Gaussian processes for\n  probabilistic programming", "comments": "33 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are powerful non-parametric probabilistic models for\nstochastic functions. However they entail a complexity that is computationally\nintractable when the number of observations is large, especially when estimated\nwith fully Bayesian methods such as Markov chain Monte Carlo. In this paper, we\nfocus on a novel approach for low-rank approximate Bayesian Gaussian processes,\nbased on a basis function approximation via Laplace eigenfunctions for\nstationary covariance functions. The main contribution of this paper is a\ndetailed analysis of the performance and practical implementation of the method\nin relation to key factors such as the number of basis functions, domain of the\nprediction space, and smoothness of the latent function. We provide intuitive\nvisualizations and recommendations for choosing the values of these factors,\nwhich make it easier for users to improve approximation accuracy and\ncomputational performance. We also propose diagnostics for checking that the\nnumber of basis functions and the domain of the prediction space are adequate\ngiven the data. The proposed approach is simple and exhibits an attractive\ncomputational complexity due to its linear structure, and it is easy to\nimplement in probabilistic programming frameworks. Several illustrative\nexamples of the performance and applicability of the method in the\nprobabilistic programming language Stan are presented together with the\nunderlying Stan model code.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 18:07:24 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Riutort-Mayol", "Gabriel", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Andersen", "Michael R.", ""], ["Solin", "Arno", ""], ["Vehtari", "Aki", ""]]}, {"id": "2004.11486", "submitter": "Dimitris Korobilis Prof", "authors": "Dimitris Korobilis and Davide Pettenuzzo", "title": "Machine Learning Econometrics: Bayesian algorithms and methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the amount of economic and other data generated worldwide increases\nvastly, a challenge for future generations of econometricians will be to master\nefficient algorithms for inference in empirical models with large information\nsets. This Chapter provides a review of popular estimation algorithms for\nBayesian inference in econometrics and surveys alternative algorithms developed\nin machine learning and computing science that allow for efficient computation\nin high-dimensional settings. The focus is on scalability and parallelizability\nof each algorithm, as well as their ability to be adopted in various empirical\nsettings in economics and finance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 23:15:33 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Korobilis", "Dimitris", ""], ["Pettenuzzo", "Davide", ""]]}, {"id": "2004.11698", "submitter": "Kai Zhou", "authors": "Kai Zhou and Jiong Tang", "title": "Structural Model Updating Using Adaptive Multi-Response Gaussian Process\n  Meta-modeling", "comments": null, "journal-ref": null, "doi": "10.1016/j.ymssp.2020.107121", "report-no": null, "categories": "cs.CE eess.SP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite element model updating utilizing frequency response functions as\ninputs is an important procedure in structural analysis, design and control.\nThis paper presents a highly efficient framework that is built upon Gaussian\nprocess emulation to inversely identify model parameters through sampling. In\nparticular, a multi-response Gaussian process (MRGP) meta-modeling approach is\nformulated that can accurately construct the error response surface, i.e., the\ndiscrepancies between the frequency response predictions and actual\nmeasurement. In order to reduce the computational cost of repeated finite\nelement simulations, an adaptive sampling strategy is established, where the\nsearch of unknown parameters is guided by the response surface features.\nMeanwhile, the information of previously sampled model parameters and the\ncorresponding errors is utilized as additional training data to refine the MRGP\nmeta-model. Two stochastic optimization techniques, i.e., particle swarm and\nsimulated annealing, are employed to train the MRGP meta-model for comparison.\nSystematic case studies are conducted to examine the accuracy and robustness of\nthe new framework of model updating.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:26:33 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Zhou", "Kai", ""], ["Tang", "Jiong", ""]]}, {"id": "2004.12012", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Shariq Mohammed, Arvind Rao, Veerabhadran\n  Baladandayuthapani", "title": "Integrative Bayesian models using Post-selective Inference: a case study\n  in Radiogenomics", "comments": "44 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying direct links between genomic pathways and clinical endpoints for\nhighly fatal diseases such as cancer is a formidable task. By selecting\nstatistically relevant associations between a wealth of intermediary variables\nsuch as imaging and genomic measurements, integrative analyses can potentially\nresult in sharper clinical models with interpretable parameters, in terms of\ntheir mechanisms. Estimates of uncertainty in the resulting models are however\nunreliable unless inference accounts for the preceding steps of selection. In\nthis article, we develop selection-aware Bayesian methods which are: (i)\namenable to a flexible class of integrative Bayesian models post a selection of\npromising variables via $\\ell_1$-regularized algorithms; (ii) enjoy\ncomputational efficiency due to a focus on sharp models with meaning; (iii)\nstrike a crucial tradeoff between the quality of model selection and\ninferential power. Central to our selection-aware workflow, a conditional\nlikelihood constructed with a reparameterization map is deployed for obtaining\nuncertainty estimates in integrative models. Investigating the potential of our\nmethods in a radiogenomic analysis, we successfully recover several important\ngene pathways and calibrate uncertainties for their associations with patient\nsurvival times.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 22:39:26 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 16:19:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["Mohammed", "Shariq", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "2004.12102", "submitter": "Antoni Musolas", "authors": "Antoni Musolas, Estelle Massart, Julien M. Hendrickx, P.-A. Absil and\n  Youssef Marzouk", "title": "Low-rank multi-parametric covariance identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a differential geometric construction for families of low-rank\ncovariance matrices, via interpolation on low-rank matrix manifolds. In\ncontrast with standard parametric covariance classes, these families offer\nsignificant flexibility for problem-specific tailoring via the choice of\n\"anchor\" matrices for the interpolation. Moreover, their low-rank facilitates\ncomputational tractability in high dimensions and with limited data. We employ\nthese covariance families for both interpolation and identification, where the\nlatter problem comprises selecting the most representative member of the\ncovariance family given a data set. In this setting, standard procedures such\nas maximum likelihood estimation are nontrivial because the covariance family\nis rank-deficient; we resolve this issue by casting the identification problem\nas distance minimization. We demonstrate the power of these differential\ngeometric families for interpolation and identification in a practical\napplication: wind field covariance approximation for unmanned aerial vehicle\nnavigation.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 10:17:36 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Musolas", "Antoni", ""], ["Massart", "Estelle", ""], ["Hendrickx", "Julien M.", ""], ["Absil", "P. -A.", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2004.12140", "submitter": "Fuad Noman", "authors": "Fuad Noman, Ammar Al-Kahtani, Vassilios Agelidis and Sieh Kiong Tiong", "title": "Wind Data Analysis for Assessing the Potential of Off-Grid Direct EV\n  Charging Stations", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of large-scale wind farms and large-scale charging stations\nfor electric vehicles with the electricity grids necessitate energy storage\nsupport for both technologies. Matching the energy variability of the wind\nfarms with the demand variability of the electric vehicles (EVs) off-grid could\npotentially eliminate the need for expensive energy storage technologies\nrequired to stabilize the grid. The objective of this paper is to investigate\nthe feasibility of using wind generation as direct energy source to power the\nEV charging stations. An interval-based approach corresponding to the time slot\nof EV charging is introduced for wind energy conversion and analyzed using\ndifferent constrains and criteria including, wind speed averaging time\ninterval, various turbines manufacturers, and standard high-resolution wind\nspeed data sets. We performed a piecewise recursive of wind turbines' output\nenergy to measure the EV charging efficiency. Wind averaging results show that\nthe three minutes intervals have increased the total number of EVs by more than\n80% compared to one-- and two -minute intervals. The potential cost reduction\ndue to decoupling of both technologies from the utility grid, energy storage\nsystems, and the associated energy conversion power electronics has merit and\nresearch in this direction is worth pursuing\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 13:22:15 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Noman", "Fuad", ""], ["Al-Kahtani", "Ammar", ""], ["Agelidis", "Vassilios", ""], ["Tiong", "Sieh Kiong", ""]]}, {"id": "2004.12211", "submitter": "Kamran Javid Mr", "authors": "Kamran Javid, Will Handley, Mike Hobson, Anthony Lasenby", "title": "Compromise-free Bayesian neural networks", "comments": "https://github.com/PolyChord/PolyChordLite;\n  https://github.com/SuperKam91/bnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a thorough analysis of the relationship between the out-of-sample\nperformance and the Bayesian evidence (marginal likelihood) of Bayesian neural\nnetworks (BNNs), as well as looking at the performance of ensembles of BNNs,\nboth using the Boston housing dataset. Using the state-of-the-art in nested\nsampling, we numerically sample the full (non-Gaussian and multimodal) network\nposterior and obtain numerical estimates of the Bayesian evidence, considering\nnetwork models with up to 156 trainable parameters. The networks have between\nzero and four hidden layers, either $\\tanh$ or $ReLU$ activation functions, and\nwith and without hierarchical priors. The ensembles of BNNs are obtained by\ndetermining the posterior distribution over networks, from the posterior\nsamples of individual BNNs re-weighted by the associated Bayesian evidence\nvalues. There is good correlation between out-of-sample performance and\nevidence, as well as a remarkable symmetry between the evidence versus model\nsize and out-of-sample performance versus model size planes. Networks with\n$ReLU$ activation functions have consistently higher evidences than those with\n$\\tanh$ functions, and this is reflected in their out-of-sample performance.\nEnsembling over architectures acts to further improve performance relative to\nthe individual BNNs.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:12:56 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:23:29 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 12:03:28 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Javid", "Kamran", ""], ["Handley", "Will", ""], ["Hobson", "Mike", ""], ["Lasenby", "Anthony", ""]]}, {"id": "2004.12250", "submitter": "Anuj Srivastava", "authors": "Anuj Srivastava", "title": "Agent-Level Pandemic Simulation (ALPS) for Analyzing Effects of Lockdown\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an agent-level simulation model, termed ALPS, for\nsimulating the spread of an infectious disease in a confined community. The\nmechanism of transmission is agent-to-agent contact, using parameters reported\nfor Corona COVID-19 pandemic. The main goal of the ALPS simulation is analyze\neffects of preventive measures -- imposition and lifting of lockdown norms --\non the rates of infections, fatalities and recoveries. The model assumptions\nand choices represent a balance between competing demands of being realistic\nand being efficient for real-time inferences. The model provides quantification\nof gains in reducing casualties by imposition and maintenance of restrictive\nmeasures in place.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 23:14:16 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Srivastava", "Anuj", ""]]}, {"id": "2004.12359", "submitter": "Vinicius Mayrink", "authors": "Vin\\'icius D. Mayrink and Jo\\~ao Daniel N. Duarte and F\\'abio N.\n  Demarqui", "title": "pexm: a JAGS module for applications involving the piecewise exponential\n  distribution", "comments": "19 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a new module built for users interested in a\nprogramming language similar to BUGS to fit a Bayesian model based on the\npiecewise exponential (PE) distribution. The module is an extension to the\nopen-source program JAGS by which a Gibbs sampler can be applied without\nrequiring the derivation of complete conditionals and the subsequent\nimplementation of strategies to draw samples from unknown distributions. The PE\ndistribution is widely used in the fields of survival analysis and reliability.\nCurrently, it can only be implemented in JAGS through methods to indirectly\nspecify the likelihood based on the Poisson or Bernoulli probabilities. Our\nmodule provides a more straightforward implementation and is thus more\nattractive to the researchers aiming to spend more time exploring the results\nfrom the Bayesian inference rather than implementing the Markov Chain Monte\nCarlo (MCMC) algorithm. For those interested in extending JAGS, this work can\nbe seen as a tutorial including important information not well investigated or\norganized in other materials. Here, we describe how to use the module taking\nadvantage of the interface between R and JAGS. A short simulation study is\ndeveloped to ensure that the module behaves well and a real illustration,\ninvolving two PE models, exhibits a context where the module can be used in\npractice.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 12:05:48 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mayrink", "Vin\u00edcius D.", ""], ["Duarte", "Jo\u00e3o Daniel N.", ""], ["Demarqui", "F\u00e1bio N.", ""]]}, {"id": "2004.12550", "submitter": "Charles Margossian", "authors": "Charles C. Margossian, Aki Vehtari, Daniel Simpson and Raj Agrawal", "title": "Hamiltonian Monte Carlo using an adjoint-differentiated Laplace\n  approximation: Bayesian inference for latent Gaussian models and beyond", "comments": "18 pages, 5 figures", "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian latent variable models are a key class of Bayesian hierarchical\nmodels with applications in many fields. Performing Bayesian inference on such\nmodels can be challenging as Markov chain Monte Carlo algorithms struggle with\nthe geometry of the resulting posterior distribution and can be prohibitively\nslow. An alternative is to use a Laplace approximation to marginalize out the\nlatent Gaussian variables and then integrate out the remaining hyperparameters\nusing dynamic Hamiltonian Monte Carlo, a gradient-based Markov chain Monte\nCarlo sampler. To implement this scheme efficiently, we derive a novel adjoint\nmethod that propagates the minimal information needed to construct the gradient\nof the approximate marginal likelihood. This strategy yields a scalable\ndifferentiation method that is orders of magnitude faster than state of the art\ndifferentiation techniques when the hyperparameters are high dimensional. We\nprototype the method in the probabilistic programming framework Stan and test\nthe utility of the embedded Laplace approximation on several models, including\none where the dimension of the hyperparameter is $\\sim$6,000. Depending on the\ncases, the benefits can include an alleviation of the geometric pathologies\nthat frustrate Hamiltonian Monte Carlo and a dramatic speed-up.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 02:31:19 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 19:26:20 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 17:22:50 GMT"}, {"version": "v4", "created": "Sun, 25 Oct 2020 22:30:00 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Margossian", "Charles C.", ""], ["Vehtari", "Aki", ""], ["Simpson", "Daniel", ""], ["Agrawal", "Raj", ""]]}, {"id": "2004.12588", "submitter": "Hugo Lewi Hammer Dr.", "authors": "Hugo L. Hammer, Anis Yazidi, Michael A. Riegler and H{\\aa}vard Rue", "title": "Efficient Quantile Tracking Using an Oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For incremental quantile estimators the step size and possibly other tuning\nparameters must be carefully set. However, little attention has been given on\nhow to set these values in an online manner. In this article we suggest two\nnovel procedures that address this issue.\n  The core part of the procedures is to estimate the current tracking mean\nsquared error (MSE). The MSE is decomposed in tracking variance and bias and\nnovel and efficient procedures to estimate these quantities are presented. It\nis shown that estimation bias can be tracked by associating it with the portion\nof observations below the quantile estimates.\n  The first procedure runs an ensemble of $L$ quantile estimators for wide\nrange of values of the tuning parameters and typically around $L = 100$. In\neach iteration an oracle selects the best estimate by the guidance of the\nestimated MSEs. The second method only runs an ensemble of $L = 3$ estimators\nand thus the values of the tuning parameters need from time to time to be\nadjusted for the running estimators. The procedures have a low memory foot\nprint of $8L$ and a computational complexity of $8L$ per iteration.\n  The experiments show that the procedures are highly efficient and track\nquantiles with an error close to the theoretical optimum. The Oracle approach\nperforms best, but comes with higher computational cost. The procedures were\nfurther applied to a massive real-life data stream of tweets and proofed real\nworld applicability of them.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 05:49:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hammer", "Hugo L.", ""], ["Yazidi", "Anis", ""], ["Riegler", "Michael A.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "2004.12838", "submitter": "Peter Green", "authors": "Peter L Green, Robert E Moore, Ryan J Jackson, Jinglai Li, Simon\n  Maskell", "title": "Increasing the efficiency of Sequential Monte Carlo samplers through the\n  use of approximately optimal L-kernels", "comments": "29 pages, 14 figures", "journal-ref": null, "doi": "10.1016/j.ymssp.2021.108028", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By facilitating the generation of samples from arbitrary probability\ndistributions, Markov Chain Monte Carlo (MCMC) is, arguably, \\emph{the} tool\nfor the evaluation of Bayesian inference problems that yield non-standard\nposterior distributions. In recent years, however, it has become apparent that\nSequential Monte Carlo (SMC) samplers have the potential to outperform MCMC in\na number of ways. SMC samplers are better suited to highly parallel computing\narchitectures and also feature various tuning parameters that are not available\nto MCMC. One such parameter - the `L-kernel' - is a user-defined probability\ndistribution that can be used to influence the efficiency of the sampler. In\nthe current paper, the authors explain how to derive an expression for the\nL-kernel that minimises the variance of the estimates realised by an SMC\nsampler. Various approximation methods are then proposed to aid implementation\nof the proposed L-kernel. The improved performance of the resulting algorithm\nis demonstrated in multiple scenarios. For the examples shown in the current\npaper, the use of an approximately optimum L-kernel has reduced the variance of\nthe SMC estimates by up to 99 % while also reducing the number of times that\nresampling was required by between 65 % and 70 %. Python code and code tests\naccompanying this manuscript are available through the Github repository\n\\url{https://github.com/plgreenLIRU/SMC_approx_optL}.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 16:25:00 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Green", "Peter L", ""], ["Moore", "Robert E", ""], ["Jackson", "Ryan J", ""], ["Li", "Jinglai", ""], ["Maskell", "Simon", ""]]}, {"id": "2004.13118", "submitter": "Federico Pavone", "authors": "Federico Pavone, Juho Piironen, Paul-Christian B\\\"urkner and Aki\n  Vehtari", "title": "Using reference models in variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection, or more generally, model reduction is an important aspect\nof the statistical workflow aiming to provide insights from data. In this\npaper, we discuss and demonstrate the benefits of using a reference model in\nvariable selection. A reference model acts as a noise-filter on the target\nvariable by modeling its data generating mechanism. As a result, using the\nreference model predictions in the model selection procedure reduces the\nvariability and improves stability leading to improved model selection\nperformance. Assuming that a Bayesian reference model describes the true\ndistribution of future data well, the theoretically preferred usage of the\nreference model is to project its predictive distribution to a reduced model\nleading to projection predictive variable selection approach. Alternatively,\nreference models may also be used in an ad-hoc manner in combination with\ncommon variable selection methods. In several numerical experiments, we\ninvestigate the performance of the projective prediction approach as well as\nalternative variable selection methods with and without reference models. Our\nresults indicate that the use of reference models generally translates into\nbetter and more stable variable selection. Additionally, we demonstrate that\nthe projection predictive approach shows superior performance as compared to\nalternative variable selection methods independently of whether or not they use\nreference models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 19:31:20 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Pavone", "Federico", ""], ["Piironen", "Juho", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "2004.13235", "submitter": "Rodrigo S. Targino", "authors": "Takaaki Koike and Yuri F. Saporito and Rodrigo S. Targino", "title": "Avoiding zero probability events when computing Value at Risk\n  contributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the process of risk allocation for a generic\nmultivariate model when the risk measure is chosen as the Value-at-Risk (VaR).\nWe recast the traditional Euler contributions from an expectation conditional\non an event of zero probability to a ratio involving conditional expectations\nwhose conditioning events have stricktly positive probability. We derive an\nanalytical form of the proposed representation of VaR contributions for various\nparametric models. Our numerical experiments show that the estimator using this\nnovel representation outperforms the standard Monte Carlo estimator in terms of\nbias and variance. Moreover, unlike the existing estimators, the proposed\nestimator is free from hyperparameters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 01:23:44 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 16:51:08 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Koike", "Takaaki", ""], ["Saporito", "Yuri F.", ""], ["Targino", "Rodrigo S.", ""]]}, {"id": "2004.13314", "submitter": "Hamed Majidifard", "authors": "Hamed Majidifard, Yaw Adu-Gyamfi, William G. Buttlar", "title": "Deep Machine Learning Approach to Develop a New Asphalt Pavement\n  Condition Index", "comments": null, "journal-ref": null, "doi": "10.1016/j.conbuildmat.2020.118513", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated pavement distress detection via road images is still a challenging\nissue among pavement researchers and computer-vision community. In recent\nyears, advancement in deep learning has enabled researchers to develop robust\ntools for analyzing pavement images at unprecedented accuracies. Nevertheless,\ndeep learning models necessitate a big ground truth dataset, which is often not\nreadily accessible for pavement field. In this study, we reviewed our previous\nstudy, which a labeled pavement dataset was presented as the first step towards\na more robust, easy-to-deploy pavement condition assessment system. In total,\n7237 google street-view images were extracted, manually annotated for\nclassification (nine categories of distress classes). Afterward, YOLO (you look\nonly once) deep learning framework was implemented to train the model using the\nlabeled dataset. In the current study, a U-net based model is developed to\nquantify the severity of the distresses, and finally, a hybrid model is\ndeveloped by integrating the YOLO and U-net model to classify the distresses\nand quantify their severity simultaneously. Various pavement condition indices\nare developed by implementing various machine learning algorithms using the\nYOLO deep learning framework for distress classification and U-net for\nsegmentation and distress densification. The output of the distress\nclassification and segmentation models are used to develop a comprehensive\npavement condition tool which rates each pavement image according to the type\nand severity of distress extracted.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 05:57:43 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Majidifard", "Hamed", ""], ["Adu-Gyamfi", "Yaw", ""], ["Buttlar", "William G.", ""]]}, {"id": "2004.13327", "submitter": "Ursula Laa", "authors": "Ursula Laa, Dianne Cook, Andreas Buja, German Valencia", "title": "Hole or grain? A Section Pursuit Index for Finding Hidden Structure in\n  Multiple Dimensions", "comments": "21 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate data is often visualized using linear projections, produced by\ntechniques such as principal component analysis, linear discriminant analysis,\nand projection pursuit. A problem with projections is that they obscure low and\nhigh density regions near the center of the distribution. Sections, or slices,\ncan help to reveal them. This paper develops a section pursuit method, building\non the extensive work in projection pursuit, to search for interesting slices\nof the data. Linear projections are used to define sections of the parameter\nspace, and to calculate interestingness by comparing the distribution of\nobservations, inside and outside a section. By optimizing this index, it is\npossible to reveal features such as holes (low density) or grains (high\ndensity). The optimization is incorporated into a guided tour so that the\nsearch for structure can be dynamic. The approach can be useful for problems\nwhen data distributions depart from uniform or normal, as in visually exploring\nnonlinear manifolds, and functions in multivariate space. Two applications of\nsection pursuit are shown: exploring decision boundaries from classification\nmodels, and exploring subspaces induced by complex inequality conditions from\nmultiple parameter model. The new methods are available in R, in the tourr\npackage.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 06:41:18 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Laa", "Ursula", ""], ["Cook", "Dianne", ""], ["Buja", "Andreas", ""], ["Valencia", "German", ""]]}, {"id": "2004.13538", "submitter": "Neeraj Poonia", "authors": "Neeraj Poonia, Sarita Azad", "title": "Short-term forecasts of COVID-19 spread across Indian states until 1 May\n  2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The very first case of corona-virus illness was recorded on 30 January 2020,\nin India and the number of infected cases, including the death toll, continues\nto rise. In this paper, we present short-term forecasts of COVID-19 for 28\nIndian states and five union territories using real-time data from 30 January\nto 21 April 2020. Applying Holt's second-order exponential smoothing method and\nautoregressive integrated moving average (ARIMA) model, we generate 10-day\nahead forecasts of the likely number of infected cases and deaths in India for\n22 April to 1 May 2020. Our results show that the number of cumulative cases in\nIndia will rise to 36335.63 [PI 95% (30884.56, 42918.87)], concurrently the\nnumber of deaths may increase to 1099.38 [PI 95% (959.77, 1553.76)] by 1 May\n2020. Further, we have divided the country into severity zones based on the\ncumulative cases. According to this analysis, Maharashtra is likely to be the\nmost affected states with around 9787.24 [PI 95% (6949.81, 13757.06)]\ncumulative cases by 1 May 2020. However, Kerala and Karnataka are likely to\nshift from the red zone (i.e. highly affected) to the lesser affected region.\nOn the other hand, Gujarat and Madhya Pradesh will move to the red zone. These\nresults mark the states where lockdown by 3 May 2020, can be loosened.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:14:01 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 17:43:40 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Poonia", "Neeraj", ""], ["Azad", "Sarita", ""]]}, {"id": "2004.14455", "submitter": "Florian Sch\\\"afer", "authors": "Florian Sch\\\"afer, Matthias Katzfuss, and Houman Owhadi", "title": "Sparse Cholesky factorization by Kullback-Leibler minimization", "comments": "The code used to run the numerical experiments can be found under\n  https://github.com/f-t-s/cholesky_by_KL_minimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to compute a sparse approximate inverse Cholesky factor $L$ of a\ndense covariance matrix $\\Theta$ by minimizing the Kullback-Leibler divergence\nbetween the Gaussian distributions $\\mathcal{N}(0, \\Theta)$ and $\\mathcal{N}(0,\nL^{-\\top} L^{-1})$, subject to a sparsity constraint. Surprisingly, this\nproblem has a closed-form solution that can be computed efficiently, recovering\nthe popular Vecchia approximation in spatial statistics. Based on recent\nresults on the approximate sparsity of inverse Cholesky factors of $\\Theta$\nobtained from pairwise evaluation of Green's functions of elliptic\nboundary-value problems at points $\\{x_{i}\\}_{1 \\leq i \\leq N} \\subset\n\\mathbb{R}^{d}$, we propose an elimination ordering and sparsity pattern that\nallows us to compute $\\epsilon$-approximate inverse Cholesky factors of such\n$\\Theta$ in computational complexity $\\mathcal{O}(N \\log(N/\\epsilon)^d)$ in\nspace and $\\mathcal{O}(N \\log(N/\\epsilon)^{2d})$ in time. To the best of our\nknowledge, this is the best asymptotic complexity for this class of problems.\nFurthermore, our method is embarrassingly parallel, automatically exploits\nlow-dimensional structure in the data, and can perform Gaussian-process\nregression in linear (in $N$) space complexity. Motivated by the optimality\nproperties of our methods, we propose methods for applying it to the joint\ncovariance of training and prediction points in Gaussian-process regression,\ngreatly improving stability and computational cost. Finally, we show how to\napply our method to the important setting of Gaussian processes with additive\nnoise, sacrificing neither accuracy nor computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 20:04:53 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 18:55:32 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sch\u00e4fer", "Florian", ""], ["Katzfuss", "Matthias", ""], ["Owhadi", "Houman", ""]]}, {"id": "2004.14521", "submitter": "Robert Bassett", "authors": "Robert Bassett and Julio Deride", "title": "One-Step Estimation With Scaled Proximal Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical estimators computed using iterative optimization methods\nthat are not run until completion. Classical results on maximum likelihood\nestimators (MLEs) assert that a one-step estimator (OSE), in which a single\nNewton-Raphson iteration is performed from a starting point with certain\nproperties, is asymptotically equivalent to the MLE. We further develop these\nearly-stopping results by deriving properties of one-step estimators defined by\na single iteration of scaled proximal methods. Our main results show the\nasymptotic equivalence of the likelihood-based estimator and various one-step\nestimators defined by scaled proximal methods. By interpreting OSEs as the last\nof a sequence of iterates, our results provide insight on scaling numerical\ntolerance with sample size. Our setting contains scaled proximal gradient\ndescent applied to certain composite models as a special case, making our\nresults applicable to many problems of practical interest. Additionally, our\nresults provide support for the utility of the scaled Moreau envelope as a\nstatistical smoother by interpreting scaled proximal descent as a quasi-Newton\nmethod applied to the scaled Moreau envelope.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:06:54 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 17:33:53 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 19:44:17 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Bassett", "Robert", ""], ["Deride", "Julio", ""]]}, {"id": "2004.14660", "submitter": "Yici Chen", "authors": "Yici Chen, Kenichiro Tanaka", "title": "Maximum likelihood estimation of the Fisher-Bingham distribution via\n  efficient calculation of its normalizing constant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient numerical integration formula to compute the\nnormalizing constant of Fisher--Bingham distributions. This formula uses a\nnumerical integration formula with the continuous Euler transform to a\nFourier-type integral representation of the normalizing constant. As this\nmethod is fast and accurate, it can be applied to the calculation of the\nnormalizing constant of high-dimensional Fisher--Bingham distributions. More\nprecisely, the error decays exponentially with an increase in the integration\npoints, and the computation cost increases linearly with the dimensions. In\naddition, this formula is useful for calculating the gradient and Hessian\nmatrix of the normalizing constant. Therefore, we apply this formula to\nefficiently calculate the maximum likelihood estimation (MLE) of\nhigh-dimensional data. Finally, we apply the MLE to the hyperspherical\nvariational auto-encoder (S-VAE), a deep-learning-based generative model that\nrestricts the latent space to a unit hypersphere. We use the S-VAE trained with\nimages of handwritten numbers to estimate the distributions of each label. This\napplication is useful for adding new labels to the models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 09:47:10 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Chen", "Yici", ""], ["Tanaka", "Kenichiro", ""]]}]