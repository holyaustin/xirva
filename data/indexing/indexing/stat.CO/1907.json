[{"id": "1907.00057", "submitter": "Hillary Koch", "authors": "Hillary Koch and Gregory P. Bopp", "title": "Fast and Exact Simulation of Multivariate Normal and Wishart Random\n  Variables with Box Constraints", "comments": "There is an error in the the technical proofs for the proposed\n  algorithms' validity. While the proposed algorithms can in many cases produce\n  approximate draws from the described target distributions, they do not\n  produce exact draws", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Models which include domain constraints occur in myriad contexts such as\neconometrics, genomics, and environmetrics, though simulating from constrained\ndistributions can be computationally expensive. In particular, repeated\nsampling from constrained distributions is a common task in Bayesian\ninferential methods, where coping with these constraints can cause troublesome\ncomputational burden. Here, we introduce computationally efficient methods to\nmake exact and independent draws from both the multivariate normal and Wishart\ndistributions with box constraints. In both cases, these variables are sampled\nusing a direct algorithm. By substantially reducing computing time, these new\nalgorithms improve the feasibility of Monte Carlo-based inference for\nbox-constrained, multivariate normal and Wishart distributions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 19:56:31 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 20:14:58 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Koch", "Hillary", ""], ["Bopp", "Gregory P.", ""]]}, {"id": "1907.00161", "submitter": "Kristian Brock", "authors": "Kristian Brock", "title": "trialr: Bayesian Clinical Trial Designs in R and Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript introduces an \\proglang{R} package called \\pkg{trialr} that\nimplements a collection of clinical trial methods in \\proglang{Stan} and\n\\proglang{R}. In this article, we explore three methods in detail. The first is\nthe continual reassessment method for conducting phase I dose-finding trials\nthat seek a maximum tolerable dose. The second is EffTox, a dose-finding design\nthat scrutinises doses by joint efficacy and toxicity outcomes. The third is\nthe augmented binary method for modelling the probability of treatment success\nin phase II oncology trials with reference to repeated measures of continuous\ntumour size and binary indicators of treatment failure. We emphasise in this\narticle the benefits that stem from having access to posterior samples,\nincluding flexible inference and powerful visualisation. We hope that this\npackage encourages the use of Bayesian methods in clinical trials.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 07:33:20 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Brock", "Kristian", ""]]}, {"id": "1907.00249", "submitter": "Jude Kong", "authors": "Christina P. Tadiri, Jude D. Kong, Gregor F. Fussmann, Marilyn E.\n  Scott, and Hao Wang", "title": "A Data-Validated Host-Parasite Model for Infectious Disease Outbreaks", "comments": "* Equally contributing first authors", "journal-ref": null, "doi": "10.3389/fevo.2019.00307", "report-no": null, "categories": "q-bio.PE math.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of model experimental systems and mathematical models is important to\nfurther understanding of infectious disease dynamics and strategize disease\nmitigation. Gyrodactylids are helminth ectoparasites of teleost fish which have\nmany dynamical characteristics of microparasites but offer the advantage that\nthey can be quantified and tracked over time, allowing further insight into\nwithin-host and epidemic dynamics. In this paper, we design a model to describe\nhost-parasite dynamics of the well-studied guppy-Gyrodactylus turnbulli system,\nusing experimental data to estimate parameters and validate it. We estimate the\nbasic reproduction number (R_0), for this system. Sensitivity analysis reveals\nthat parasite growth rate, and the rate at which the guppy mounts an immune\nresponse have the greatest impact on outbreak peak and timing both for initial\noutbreaks and on longer time scales. These findings highlight guppy population\nresistance and parasite virulence as key factors in disease control, and future\nwork should focus on incorporating heterogeneity in host resistance into\ndisease models and extrapolating to other host-parasite systems.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 18:21:46 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 17:12:46 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Tadiri", "Christina P.", ""], ["Kong", "Jude D.", ""], ["Fussmann", "Gregor F.", ""], ["Scott", "Marilyn E.", ""], ["Wang", "Hao", ""]]}, {"id": "1907.00389", "submitter": "Alessio Spantini", "authors": "Alessio Spantini, Ricardo Baptista, Youssef Marzouk", "title": "Coupling techniques for nonlinear ensemble filtering", "comments": "43 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider filtering in high-dimensional non-Gaussian state-space models\nwith intractable transition kernels, nonlinear and possibly chaotic dynamics,\nand sparse observations in space and time. We propose a novel filtering\nmethodology that harnesses transportation of measures, convex optimization, and\nideas from probabilistic graphical models to yield robust ensemble\napproximations of the filtering distribution in high dimensions. Our approach\ncan be understood as the natural generalization of the ensemble Kalman filter\n(EnKF) to nonlinear updates, using stochastic or deterministic couplings. The\nuse of nonlinear updates can reduce the intrinsic bias of the EnKF at a\nmarginal increase in computational cost. We avoid any form of importance\nsampling and introduce non-Gaussian localization approaches for dimension\nscalability. Our framework achieves state-of-the-art tracking performance on\nchallenging configurations of the Lorenz-96 model in the chaotic regime.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 14:51:05 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Spantini", "Alessio", ""], ["Baptista", "Ricardo", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1907.00914", "submitter": "Peter DeWitt", "authors": "Peter E. DeWitt and Tellen D. Bennett", "title": "ensr: R Package for Simultaneous Selection of Elastic Net Tuning\n  Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Elastic net regression is a form of penalized regression that\nlies between ridge and least absolute shrinkage and selection operator (LASSO)\nregression. The elastic net penalty is a powerful tool controlling the impact\nof correlated predictors and the overall complexity of generalized linear\nregression models. The elastic net penalty has two tuning parameters:\n${\\lambda}$ for the complexity and ${\\alpha}$ for the compromise between LASSO\nand ridge. The R package glmnet provides efficient tools for fitting elastic\nnet models and selecting ${\\lambda}$ for a given ${\\alpha}.$ However, glmnet\ndoes not simultaneously search the ${\\lambda} - {\\alpha}$ space for the\noptional elastic net model.\n  Results: We built the R package ensr, elastic net searcher. enser extends the\nfunctionality of glment to search the ${\\lambda} - {\\alpha}$ space and identify\nan optimal ${\\lambda} - {\\alpha}$ pair.\n  Availability: ensr is available from the Comprehensive R Archive Network at\nhttps://cran.r-project.org/package=ensr\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 16:33:42 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["DeWitt", "Peter E.", ""], ["Bennett", "Tellen D.", ""]]}, {"id": "1907.01063", "submitter": "Erik \\v{S}trumbelj", "authors": "Rok \\v{C}e\\v{s}novar, Steve Bronder, Davor Sluga, Jure Dem\\v{s}ar,\n  Tadej Ciglari\\v{c}, Sean Talts, Erik \\v{S}trumbelj", "title": "GPU-based Parallel Computation Support for Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper details an extensible OpenCL framework that allows Stan to utilize\nheterogeneous compute devices. It includes GPU-optimized routines for the\nCholesky decomposition, its derivative, other matrix algebra primitives and\nsome commonly used likelihoods, with more additions planned for the near\nfuture. Stan users can now benefit from large speedups offered by GPUs with\nlittle effort and without changes to their existing Stan code. We demonstrate\nthe practical utility of our work with two examples - logistic regression and\nGaussian Process regression.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 20:36:16 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 23:23:03 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["\u010ce\u0161novar", "Rok", ""], ["Bronder", "Steve", ""], ["Sluga", "Davor", ""], ["Dem\u0161ar", "Jure", ""], ["Ciglari\u010d", "Tadej", ""], ["Talts", "Sean", ""], ["\u0160trumbelj", "Erik", ""]]}, {"id": "1907.01170", "submitter": "Anwesha Bhattacharyya", "authors": "Anwesha Bhattacharyya and Yves Atchade", "title": "Bayesian Analysis of High-dimensional Discrete Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a Bayesian methodology for fitting large discrete\ngraphical models with spike-and-slab priors to encode sparsity. We consider a\nquasi-likelihood approach that enables node-wise parallel computation resulting\nin reduced computational complexity. We introduce a scalable Langevin MCMC\nalgorithm for sampling from the quasi-posterior distribution which enables\nvariable selection and estimation simultaneously. We present extensive\nsimulation results to demonstrate scalability and accuracy of the method. We\nalso analyze the 16 Personality Factors (PF) dataset to illustrate performance\nof the method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 05:10:15 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 20:41:42 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Bhattacharyya", "Anwesha", ""], ["Atchade", "Yves", ""]]}, {"id": "1907.01248", "submitter": "Andrea Riebler", "authors": "Sara Martino and Andrea Riebler", "title": "Integrated Nested Laplace Approximations (INLA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a short description and basic introduction to the Integrated nested\nLaplace approximations (INLA) approach. INLA is a deterministic paradigm for\nBayesian inference in latent Gaussian models (LGMs) introduced in Rue et al.\n(2009). INLA relies on a combination of analytical approximations and efficient\nnumerical integration schemes to achieve highly accurate deterministic\napproximations to posterior quantities of interest. The main benefit of using\nINLA instead of Markov chain Monte Carlo (MCMC) techniques for LGMs is\ncomputational; INLA is fast even for large, complex models. Moreover, being a\ndeterministic algorithm, INLA does not suffer from slow convergence and poor\nmixing. INLA is implemented in the R package R-INLA, which represents a\nuser-friendly and versatile tool for doing Bayesian inference. R-INLA returns\nposterior marginals for all model parameters and the corresponding posterior\nsummary information. Model choice criteria as well as predictive diagnostics\nare directly available. Here, we outline the theory behind INLA, present the\nR-INLA package and describe new developments of combining INLA with MCMC for\nmodels that are not possible to fit with R-INLA.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 09:18:42 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Martino", "Sara", ""], ["Riebler", "Andrea", ""]]}, {"id": "1907.01505", "submitter": "Umberto Simola Mr.", "authors": "Umberto Simola, Jessica Cisewski-Kehe, Michael U. Gutmann, Jukka\n  Corander", "title": "Adaptive Approximate Bayesian Computation Tolerance Selection", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) methods are increasingly used for\ninference in situations in which the likelihood function is either\ncomputationally costly or intractable to evaluate. Extensions of the basic ABC\nrejection algorithm have improved the computational efficiency of the procedure\nand broadened its applicability. The ABC-Population Monte Carlo (ABC-PMC)\napproach of Beaumont et al. (2009) has become a popular choice for approximate\nsampling from the posterior. ABC-PMC is a sequential sampler with an\niteratively decreasing value of the tolerance, which specifies how close the\nsimulated data need to be to the real data for acceptance. We propose a method\nfor adaptively selecting a sequence of tolerances that improves the\ncomputational efficiency of the algorithm over other common techniques. In\naddition we define a stopping rule as a by-product of the adaptation procedure,\nwhich assists in automating termination of sampling. The proposed automatic\nABC-PMC algorithm can be easily implemented and we present several examples\ndemonstrating its benefits in terms of computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 16:02:58 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 07:47:45 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Simola", "Umberto", ""], ["Cisewski-Kehe", "Jessica", ""], ["Gutmann", "Michael U.", ""], ["Corander", "Jukka", ""]]}, {"id": "1907.01551", "submitter": "Wilkins Aquino", "authors": "Zilong Zou, Sayan Mukherjee, Harbir Antil, and Wilkins Aquino", "title": "Adaptive particle-based approximations of the Gibbs posterior for\n  inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we adopt a general framework based on the Gibbs posterior to\nupdate belief distributions for inverse problems governed by partial\ndifferential equations (PDEs). The Gibbs posterior formulation is a\ngeneralization of standard Bayesian inference that only relies on a loss\nfunction connecting the unknown parameters to the data. It is particularly\nuseful when the true data generating mechanism (or noise distribution) is\nunknown or difficult to specify. The Gibbs posterior coincides with Bayesian\nupdating when a true likelihood function is known and the loss function\ncorresponds to the negative log-likelihood, yet provides subjective inference\nin more general settings.\n  We employ a sequential Monte Carlo (SMC) approach to approximate the Gibbs\nposterior using particles. To manage the computational cost of propagating\nincreasing numbers of particles through the loss function, we employ a recently\ndeveloped local reduced basis method to build an efficient surrogate loss\nfunction that is used in the Gibbs update formula in place of the true loss. We\nderive error bounds for our approximation and propose an adaptive approach to\nconstruct the surrogate model in an efficient manner. We demonstrate the\nefficiency of our approach through several numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 01:29:12 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zou", "Zilong", ""], ["Mukherjee", "Sayan", ""], ["Antil", "Harbir", ""], ["Aquino", "Wilkins", ""]]}, {"id": "1907.01736", "submitter": "Luai Al-Labadi Dr.", "authors": "Luai Al-Labadi, Forough Fazeli Asl and Zahra Saberi", "title": "A Bayesian Semiparametric Gaussian Copula Approach to a Multivariate\n  Normality Test", "comments": "30 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Bayesian semiparametric copula approach is used to model the\nunderlying multivariate distribution $F_{true}$. First, the Dirichlet process\nis constructed on the unknown marginal distributions of $F_{true}$. Then a\nGaussian copula model is utilized to capture the dependence structure of\n$F_{true}$. As a result, a Bayesian multivariate normality test is developed by\ncombining the relative belief ratio and the Energy distance. Several\ninteresting theoretical results of the approach are derived. Finally, through\nseveral simulated examples and a real data set, the proposed approach reveals\nexcellent performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 04:45:54 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 04:50:09 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Asl", "Forough Fazeli", ""], ["Saberi", "Zahra", ""]]}, {"id": "1907.01938", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang and Michael P. B. Gallaugher and Ryan P. Browne and\n  Paul D. McNicholas", "title": "Model-based clustering and classification using mixtures of multivariate\n  skewed power exponential distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Families of mixtures of multivariate power exponential (MPE) distributions\nhave been previously introduced and shown to be competitive for cluster\nanalysis in comparison to other elliptical mixtures including mixtures of\nGaussian distributions. Herein, we propose a family of mixtures of multivariate\nskewed power exponential distributions to combine the flexibility of the MPE\ndistribution with the ability to model skewness. These mixtures are more robust\nto variations from normality and can account for skewness, varying tail weight,\nand peakedness of data. A generalized expectation-maximization approach\ncombining minorization-maximization and optimization based on accelerated line\nsearch algorithms on the Stiefel manifold is used for parameter estimation.\nThese mixtures are implemented both in the model-based clustering and\nclassification frameworks. Both simulated and benchmark data are used for\nillustration and comparison to other mixture families.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:38:47 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:34:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["Gallaugher", "Michael P. B.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1907.01942", "submitter": "Art Owen", "authors": "Christopher R. Hoyt and Art B. Owen", "title": "Mean Dimension of Ridge Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the mean dimension of some ridge functions of spherical Gaussian\nrandom vectors of dimension $d$. If the ridge function is Lipschitz continuous,\nthen the mean dimension remains bounded as $d\\to\\infty$. If instead, the ridge\nfunction is discontinuous, then the mean dimension depends on a measure of the\nridge function's sparsity, and absent sparsity the mean dimension can grow\nproportionally to $\\sqrt{d}$. Preintegrating a ridge function yields a new,\npotentially much smoother ridge function. We include an example where, if one\nof the ridge coefficients is bounded away from zero as $d\\to\\infty$, then\npreintegration can reduce the mean dimension from $O(\\sqrt{d})$ to $O(1)$.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 19:01:56 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Hoyt", "Christopher R.", ""], ["Owen", "Art B.", ""]]}, {"id": "1907.01954", "submitter": "Sokbae Lee", "authors": "Sokbae Lee, Serena Ng", "title": "An Econometric Perspective on Algorithmic Subsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets that are terabytes in size are increasingly common, but computer\nbottlenecks often frustrate a complete analysis of the data. While more data\nare better than less, diminishing returns suggest that we may not need\nterabytes of data to estimate a parameter or test a hypothesis. But which rows\nof data should we analyze, and might an arbitrary subset of rows preserve the\nfeatures of the original data? This paper reviews a line of work that is\ngrounded in theoretical computer science and numerical linear algebra, and\nwhich finds that an algorithmically desirable sketch, which is a randomly\nchosen subset of the data, must preserve the eigenstructure of the data, a\nproperty known as a subspace embedding. Building on this work, we study how\nprediction and inference can be affected by data sketching within a linear\nregression setup. We show that the sketching error is small compared to the\nsample size effect which a researcher can control. As a sketch size that is\nalgorithmically optimal may not be suitable for prediction and inference, we\nuse statistical arguments to provide 'inference conscious' guides to the sketch\nsize. When appropriately implemented, an estimator that pools over different\nsketches can be nearly as efficient as the infeasible one using the full\nsample.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 14:04:12 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 16:33:53 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 16:16:56 GMT"}, {"version": "v4", "created": "Thu, 30 Apr 2020 16:44:12 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Lee", "Sokbae", ""], ["Ng", "Serena", ""]]}, {"id": "1907.02088", "submitter": "Sambit Panda", "authors": "Sambit Panda, Satish Palaniappan, Junhao Xiong, Eric W. Bridgeford,\n  Ronak Mehta, Cencheng Shen, Joshua T. Vogelstein", "title": "hyppo: A Multivariate Hypothesis Testing Python Package", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce hyppo, a unified library for performing multivariate hypothesis\ntesting, including independence, two-sample, and k-sample testing. While many\nmultivariate independence tests have R packages available, the interfaces are\ninconsistent and most are not available in Python. hyppo includes many state of\nthe art multivariate testing procedures. The package is easy-to-use and is\nflexible enough to enable future extensions. The documentation and all releases\nare available at https://hyppo.neurodata.io.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:05:25 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 19:20:43 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 18:29:49 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2020 15:21:36 GMT"}, {"version": "v5", "created": "Thu, 20 Aug 2020 12:28:44 GMT"}, {"version": "v6", "created": "Thu, 1 Apr 2021 15:13:13 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Panda", "Sambit", ""], ["Palaniappan", "Satish", ""], ["Xiong", "Junhao", ""], ["Bridgeford", "Eric W.", ""], ["Mehta", "Ronak", ""], ["Shen", "Cencheng", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1907.02179", "submitter": "Hayden Moffat", "authors": "Hayden Moffat, Markus Hainy, Nikos E. Papanikolaou and Christopher\n  Drovandi", "title": "Sequential Experimental Design for Predator-Prey Functional Response\n  Experiments", "comments": "Main Text: 23 pages, 7 Figures - Supplementary Text: 11 pages, 5\n  Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding functional response within a predator-prey dynamic is a\ncornerstone for many quantitative ecological studies. Over the past 60 years,\nthe methodology for modelling functional response has gradually transitioned\nfrom the classic mechanistic models to more statistically oriented models. To\nobtain inferences on these statistical models, a substantial number of\nexperiments need to be conducted. The obvious disadvantages of collecting this\nvolume of data include cost, time and the sacrificing of animals. Therefore,\noptimally designed experiments are useful as they may reduce the total number\nof experimental runs required to attain the same statistical results. In this\npaper, we develop the first sequential experimental design method for\npredator-prey functional response experiments. To make inferences on the\nparameters in each of the statistical models we consider, we use sequential\nMonte Carlo, which is computationally efficient and facilitates convenient\nestimation of important utility functions. It provides coverage of experimental\ngoals including parameter estimation, model discrimination as well as a\ncombination of these. The results of our simulation study illustrate that for\npredator-prey functional response experiments sequential design outperforms\nstatic design for our experimental goals. R code for implementing the\nmethodology is available via\nhttps://github.com/haydenmoffat/sequential_design_for_predator_prey_experiments.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 01:18:20 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 13:03:22 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Moffat", "Hayden", ""], ["Hainy", "Markus", ""], ["Papanikolaou", "Nikos E.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1907.02212", "submitter": "Yishu Xue", "authors": "Zhihua Ma, Yishu Xue, Guanyu Hu", "title": "Heterogeneous Regression Models for Clusters of Spatial Dependent Data", "comments": null, "journal-ref": null, "doi": "10.1080/17421772.2020.1784989", "report-no": null, "categories": "econ.EM stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In economic development, there are often regions that share similar economic\ncharacteristics, and economic models on such regions tend to have similar\ncovariate effects. In this paper, we propose a Bayesian clustered regression\nfor spatially dependent data in order to detect clusters in the covariate\neffects. Our proposed method is based on the Dirichlet process which provides a\nprobabilistic framework for simultaneous inference of the number of clusters\nand the clustering configurations. The usage of our method is illustrated both\nin simulation studies and an application to a housing cost dataset of Georgia.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 04:13:36 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 00:42:12 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 04:35:55 GMT"}, {"version": "v4", "created": "Wed, 8 Apr 2020 23:25:27 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ma", "Zhihua", ""], ["Xue", "Yishu", ""], ["Hu", "Guanyu", ""]]}, {"id": "1907.02437", "submitter": "Liang Guo", "authors": "Liang Guo, Jianya Liu, Ruodan Lu", "title": "Subsampling Bias and The Best-Discrepancy Systematic Cross Validation", "comments": "SCIENCE China Mathematics. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine learning models should be evaluated and validated before\nputting to work. Conventional k-fold Monte Carlo Cross-Validation (MCCV)\nprocedure uses a pseudo-random sequence to partition instances into k subsets,\nwhich usually causes subsampling bias, inflates generalization errors and\njeopardizes the reliability and effectiveness of cross-validation. Based on\nordered systematic sampling theory in statistics and low-discrepancy sequence\ntheory in number theory, we propose a new k-fold cross-validation procedure by\nreplacing a pseudo-random sequence with a best-discrepancy sequence, which\nensures low subsampling bias and leads to more precise\nExpected-Prediction-Error estimates. Experiments with 156 benchmark datasets\nand three classifiers (logistic regression, decision tree and naive bayes) show\nthat in general, our cross-validation procedure can extrude subsampling bias in\nthe MCCV by lowering the EPE around 7.18% and the variances around 26.73%. In\ncomparison, the stratified MCCV can reduce the EPE and variances of the MCCV\naround 1.58% and 11.85% respectively. The Leave-One-Out (LOO) can lower the EPE\naround 2.50% but its variances are much higher than the any other CV procedure.\nThe computational time of our cross-validation procedure is just 8.64% of the\nMCCV, 8.67% of the stratified MCCV and 16.72% of the LOO. Experiments also show\nthat our approach is more beneficial for datasets characterized by relatively\nsmall size and large aspect ratio. This makes our approach particularly\npertinent when solving bioscience classification problems. Our proposed\nsystematic subsampling technique could be generalized to other machine learning\nalgorithms that involve random subsampling mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 14:55:02 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Guo", "Liang", ""], ["Liu", "Jianya", ""], ["Lu", "Ruodan", ""]]}, {"id": "1907.02447", "submitter": "Arthur Guillaumin", "authors": "Arthur P. Guillaumin, Adam M. Sykulski, Sofia C. Olhede, Frederik J.\n  Simons", "title": "The Debiased Spatial Whittle Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a computationally and statistically efficient method for\nestimating the parameters of a stochastic Gaussian model observed on a regular\nspatial grid in any number of dimensions. Our proposed method, which we call\nthe debiased spatial Whittle likelihood, makes important corrections to the\nwell-known Whittle likelihood to account for large sources of bias caused by\nboundary effects and aliasing. We generalise the approach to flexibly allow for\nsignificant volumes of missing data, for the usage of irregular sampling\nschemes including those with lower-dimensional substructure, and for irregular\nsampling boundaries. We build a theoretical framework under relatively weak\nassumptions which ensures consistency and asymptotic normality in numerous\npractical settings. We provide detailed implementation guidelines which ensure\nthe estimation procedure can still be conducted in $\\mathcal{O}(n\\log n)$\noperations, where $n$ is the number of points of the encapsulating rectangular\ngrid, thus keeping the computational scalability of Fourier and Whittle-based\nmethods for large data sets. We validate our procedure over a range of\nsimulated and real world settings, and compare with state-of-the-art\nalternatives, demonstrating the enduring significant practical appeal of\nFourier-based methods, provided they are corrected by the constructive\nprocedures developed in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 15:11:36 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 14:33:14 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 21:17:15 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Guillaumin", "Arthur P.", ""], ["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Simons", "Frederik J.", ""]]}, {"id": "1907.02676", "submitter": "Aleksey Polunchenko", "authors": "Kexuan Li and Aleksey S. Polunchenko", "title": "On the Convergence Rate of the Quasi- to Stationary Distribution for the\n  Shiryaev-Roberts Diffusion", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the classical Shiryaev--Roberts martingale diffusion considered on the\ninterval $[0,A]$, where $A>0$ is a given absorbing boundary, it is shown that\nthe rate of convergence of the diffusion's quasi-stationary cumulative\ndistribution function (cdf), $Q_{A}(x)$, to its stationary cdf, $H(x)$, as\n$A\\to+\\infty$, is no worse than $O(\\log(A)/A)$, uniformly in $x\\ge0$. The\nresult is established explicitly, by constructing new tight lower- and\nupper-bounds for $Q_{A}(x)$ using certain latest monotonicity properties of the\nmodified Bessel $K$ function involved in the exact closed-form formula for\n$Q_{A}(x)$ recently obtained by Polunchenko (2017).\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 04:49:21 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 05:44:47 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Li", "Kexuan", ""], ["Polunchenko", "Aleksey S.", ""]]}, {"id": "1907.02706", "submitter": "Unn Dahlen", "authors": "Unn Dahlen and Johan Linstr\\\"om and Marko Scholze", "title": "Spatio-Temporal Reconstructions of Global CO2-Fluxes using Gaussian\n  Markov Random Fields", "comments": "Article: 37 pages, 11 figures, including references and appendix.\n  Supplemental material: 11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atmospheric inverse modelling is a method for reconstructing historical\nfluxes of green-house gas between land and atmosphere, using observed\natmospheric concentrations and an atmospheric tracer transport model. The small\nnumber of observed atmospheric concentrations in relation to the number of\nunknown flux components makes the inverse problem ill-conditioned, and\nassumptions on the fluxes are needed to constrain the solution. A common\npractise is to model the fluxes using latent Gaussian fields with a mean\nstructure based on estimated fluxes from combinations of process modelling\n(natural fluxes) and statistical bookkeeping (anthropogenic emissions). Here,\nwe reconstruct global \\CO flux fields by modelling fluxes using Gaussian Markov\nRandom Fields (GMRF), resulting in a flexible and computational beneficial\nmodel with a Mat\\'ern-like spatial covariance, and a temporal covariance\ndefined through an auto-regressive model with seasonal dependence.\n  In contrast to previous inversions, the flux is defined on a spatially\ncontinuous domain, and the traditionally discrete flux representation is\nreplaced by integrated fluxes at the resolution specified by the transport\nmodel. This formulation removes aggregation errors in the flux covariance, due\nto the traditional representation of area integrals by fluxes at discrete\npoints, and provides a model closer resembling real-life space-time continuous\nfluxes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 07:31:30 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Dahlen", "Unn", ""], ["Linstr\u00f6m", "Johan", ""], ["Scholze", "Marko", ""]]}, {"id": "1907.03206", "submitter": "Ben Moews", "authors": "Ben Moews, Jaime R. Argueta Jr., Antonia Gieschen", "title": "Filaments of crime: Informing policing via thresholded ridge estimation", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.dss.2021.113518", "report-no": null, "categories": "stat.AP cs.CY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: We introduce a new method for reducing crime in hot spots and\nacross cities through ridge estimation. In doing so, our goal is to explore the\napplication of density ridges to hot spots and patrol optimization, and to\ncontribute to the policing literature in police patrolling and crime reduction\nstrategies.\n  Methods: We make use of the subspace-constrained mean shift algorithm, a\nrecently introduced approach for ridge estimation further developed in\ncosmology, which we modify and extend for geospatial datasets and hot spot\nanalysis. Our experiments extract density ridges of Part I crime incidents from\nthe City of Chicago during the year 2018 and early 2019 to demonstrate the\napplication to current data.\n  Results: Our results demonstrate nonlinear mode-following ridges in agreement\nwith broader kernel density estimates. Using early 2019 incidents with\npredictive ridges extracted from 2018 data, we create multi-run confidence\nintervals and show that our patrol templates cover around 94% of incidents for\n0.1-mile envelopes around ridges, quickly rising to near-complete coverage. We\nalso develop and provide researchers, as well as practitioners, with a\nuser-friendly and open-source software for fast geospatial density ridge\nestimation.\n  Conclusions: We show that ridges following crime report densities can be used\nto enhance patrolling capabilities. Our empirical tests show the stability of\nridges based on past data, offering an accessible way of identifying routes\nwithin hot spots instead of patrolling epicenters. We suggest further research\ninto the application and efficacy of density ridges for patrolling.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 23:59:22 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Moews", "Ben", ""], ["Argueta", "Jaime R.", "Jr."], ["Gieschen", "Antonia", ""]]}, {"id": "1907.03427", "submitter": "Clemens Kreutz", "authors": "Clemens Kreutz", "title": "Guidelines for benchmarking of optimization approaches for fitting\n  mathematical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Insufficient performance of optimization approaches for fitting of\nmathematical models is still a major bottleneck in systems biology. In this\nmanuscript, the reasons and methodological challenges are summarized as well as\ntheir impact in benchmark studies. Important aspects for increasing evidence of\noutcomes of benchmark analyses are discussed. Based on general guidelines for\nbenchmarking in computational biology, a collection of tailored guidelines is\npresented for performing informative and unbiased benchmarking of\noptimization-based fitting approaches. Comprehensive benchmark studies based on\nthese recommendations are urgently required for establishing of a robust and\nreliable methodology for the systems biology community.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 07:16:36 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Kreutz", "Clemens", ""]]}, {"id": "1907.04008", "submitter": "Nisar Ahmed", "authors": "Nisar R. Ahmed", "title": "Decentralized Gaussian Mixture Fusion through Unified Quotient\n  Approximations", "comments": "submitted for journal review to Information Fusion; conference\n  version published in IEEE MFI 2015 conference: N. Ahmed, \"What's One Mixture\n  Divided by Another? A unified approach to high-fidelity distributed data\n  fusion with mixture models\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.RO cs.SY eess.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the problem of using finite Gaussian mixtures (GM)\nprobability density functions in recursive Bayesian peer-to-peer decentralized\ndata fusion (DDF). It is shown that algorithms for both exact and approximate\nGM DDF lead to the same problem of finding a suitable GM approximation to a\nposterior fusion pdf resulting from the division of a `naive Bayes' fusion GM\n(representing direct combination of possibly dependent information sources) by\nanother non-Gaussian pdf (representing removal of either the actual or\nestimated `common information' between the information sources). The resulting\nquotient pdf for general GM fusion is naturally a mixture pdf, although the\nfused mixands are non-Gaussian and are not analytically tractable for recursive\nBayesian updates. Parallelizable importance sampling algorithms for both direct\nlocal approximation and indirect global approximation of the quotient mixture\nare developed to find tractable GM approximations to the non-Gaussian `sum of\nquotients' mixtures. Practical application examples for multi-platform static\ntarget search and maneuverable range-based target tracking demonstrate the\nhigher fidelity of the resulting approximations compared to existing GM DDF\ntechniques, as well as their favorable computational features.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:36:34 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Ahmed", "Nisar R.", ""]]}, {"id": "1907.04059", "submitter": "Joaqu\\'in Mart\\'inez-Minaya", "authors": "Joaqu\\'in Mart\\'inez-Minaya, Finn Lindgren, Antonio L\\'opez-Qu\\'ilez,\n  Daniel Simpson and David Conesa", "title": "The Integrated nested Laplace approximation for fitting models with\n  multivariate response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Laplace approximation to Bayesian inference in\nregression models for multivariate response variables. We focus on Dirichlet\nregression models, which can be used to analyze a set of variables on a simplex\nexhibiting skewness and heteroscedasticity, without having to transform the\ndata. These data, which mainly consist of proportions or percentages of\ndisjoint categories, are widely known as compositional data and are common in\nareas such as ecology, geology, and psychology. We provide both the theoretical\nfoundations and a description of how this Laplace approximation can be\nimplemented in the case of Dirichlet regression. The paper also introduces the\npackage dirinla in the R-language that extends the INLA package, which can not\ndeal directly with multivariate likelihoods like the Dirichlet likelihood.\nSimulation studies are presented to validate the good behaviour of the proposed\nmethod, while a real data case-study is used to show how this approach can be\napplied.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 09:50:07 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 08:50:04 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Mart\u00ednez-Minaya", "Joaqu\u00edn", ""], ["Lindgren", "Finn", ""], ["L\u00f3pez-Qu\u00edlez", "Antonio", ""], ["Simpson", "Daniel", ""], ["Conesa", "David", ""]]}, {"id": "1907.04615", "submitter": "Jan Kudlicka", "authors": "Jan Kudlicka and Lawrence M. Murray and Fredrik Ronquist and Thomas B.\n  Sch\\\"on", "title": "Probabilistic programming for birth-death models of evolution using an\n  alive particle filter with delayed sampling", "comments": null, "journal-ref": "Conference on Uncertainty in Artificial Intelligence (UAI) 2019", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider probabilistic programming for birth-death models of evolution and\nintroduce a new widely-applicable inference method that combines an extension\nof the alive particle filter (APF) with automatic Rao-Blackwellization via\ndelayed sampling. Birth-death models of evolution are an important family of\nphylogenetic models of the diversification processes that lead to evolutionary\ntrees. Probabilistic programming languages (PPLs) give phylogeneticists a new\nand exciting tool: their models can be implemented as probabilistic programs\nwith just a basic knowledge of programming. The general inference methods in\nPPLs reduce the need for external experts, allow quick prototyping and testing,\nand accelerate the development and deployment of new models. We show how these\nbirth-death models can be implemented as simple programs in existing PPLs, and\ndemonstrate the usefulness of the proposed inference method for such models.\nFor the popular BiSSE model the method yields an increase of the effective\nsample size and the conditional acceptance rate by a factor of 30 in comparison\nwith a standard bootstrap particle filter. Although concentrating on\nphylogenetics, the extended APF is a general inference method that shows its\nstrength in situations where particles are often assigned zero weight. In the\ncase when the weights are always positive, the extra cost of using the APF\nrather than the bootstrap particle filter is negligible, making our method a\nsuitable drop-in replacement for the bootstrap particle filter in probabilistic\nprogramming inference.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:09:00 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 10:01:29 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2021 17:23:33 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kudlicka", "Jan", ""], ["Murray", "Lawrence M.", ""], ["Ronquist", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1907.04620", "submitter": "Nick Koning", "authors": "Nick Koning and Paul Bekker", "title": "Sparse Unit-Sum Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers sparsity in linear regression under the restriction that\nthe regression weights sum to one. We propose an approach that combines\n$\\ell_0$- and $\\ell_1$-regularization. We compute its solution by adapting a\nrecent methodological innovation made by Bertsimas et al. (2016) for\n$\\ell_0$-regularization in standard linear regression. In a simulation\nexperiment we compare our approach to $\\ell_0$-regularization and\n$\\ell_1$-regularization and find that it performs favorably in terms of\npredictive performance and sparsity. In an application to index tracking we\nshow that our approach can obtain substantially sparser portfolios compared to\n$\\ell_1$-regularization while maintaining a similar tracking performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:16:05 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Koning", "Nick", ""], ["Bekker", "Paul", ""]]}, {"id": "1907.04670", "submitter": "Larkin Liu", "authors": "Larkin Liu, Yu-Chung Lin, Joshua Reid", "title": "Improving the Performance of the LSTM and HMM Model via Hybridization", "comments": "Working Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.CO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Language models based on deep neural networks and traditional stochastic\nmodelling have become both highly functional and effective in recent times. In\nthis work, a general survey into the two types of language modelling is\nconducted. We investigate the effectiveness of the Hidden Markov Model (HMM),\nand the Long Short-Term Memory Model (LSTM). We analyze the hidden state\nstructures common to both models, and present an analysis on structural\nsimilarity of the hidden states, common to both HMM's and LSTM's. We compare\nthe LSTM's predictive accuracy and hidden state output with respect to the HMM\nfor a varying number of hidden states. In this work, we justify that the less\ncomplex HMM can serve as an appropriate approximation of the LSTM model.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 15:12:51 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 21:05:24 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 10:56:06 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 13:16:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Larkin", ""], ["Lin", "Yu-Chung", ""], ["Reid", "Joshua", ""]]}, {"id": "1907.05077", "submitter": "Nick Koning", "authors": "Nick Koning", "title": "Directing Power Towards Conic Parameter Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a high-dimensional parameter of interest, tests based on quadratic\nstatistics are known to have low power against subsets of the parameter space\n(henceforth, parameter subspaces). In addition, they typically involve an\ninverse covariance matrix which is difficult to estimate in high-dimensional\nsettings. I simultaneously address these two issues by proposing a novel test\nstatistic that is large in a conic parameter subspace of interest. This test\nstatistic generalizes the Wald statistic and nests many well-known test\nstatistics. For a given parameter subspace, the statistic is free of tuning\nparameters and suitable for high-dimensional settings if the subspace is\nsufficiently small. It can be computed using regularized linear regression,\nwhere the type of regularization and the regularization parameters are\ncompletely determined by the parameter subspace of interest. I illustrate the\nstatistic on subspaces that consist of sparse or nearly-sparse vectors, for\nwhich the computation corresponds to $\\ell_0$- and $\\ell_1$-regularized\nregression, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 09:56:23 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 15:37:18 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 15:38:53 GMT"}, {"version": "v4", "created": "Mon, 9 Sep 2019 14:26:55 GMT"}, {"version": "v5", "created": "Fri, 8 Nov 2019 23:15:58 GMT"}, {"version": "v6", "created": "Tue, 19 Nov 2019 16:11:03 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Koning", "Nick", ""]]}, {"id": "1907.05648", "submitter": "Andriy Olenko", "authors": "Daniel Fryer, Ming Li, Andriy Olenko", "title": "rcosmo: R Package for Analysis of Spherical, HEALPix and Cosmological\n  Data", "comments": "20 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of spatial observations on a sphere is important in areas such\nas geosciences, physics and embryo research, just to name a few. The purpose of\nthe package rcosmo is to conduct efficient information processing,\nvisualisation, manipulation and spatial statistical analysis of Cosmic\nMicrowave Background (CMB) radiation and other spherical data. The package was\ndeveloped for spherical data stored in the Hierarchical Equal Area isoLatitude\nPixelation (Healpix) representation. rcosmo has more than 100 different\nfunctions. Most of them initially were developed for CMB, but also can be used\nfor other spherical data as rcosmo contains tools for transforming spherical\ndata in cartesian and geographic coordinates into the HEALPix representation.\nWe give a general description of the package and illustrate some important\nfunctionalities and benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 09:51:50 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Fryer", "Daniel", ""], ["Li", "Ming", ""], ["Olenko", "Andriy", ""]]}, {"id": "1907.06249", "submitter": "Feras Saad", "authors": "Feras A. Saad, Marco F. Cusumano-Towner, Ulrich Schaechtle, Martin C.\n  Rinard, Vikash K. Mansinghka", "title": "Bayesian Synthesis of Probabilistic Programs for Automatic Data Modeling", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 3, POPL, Article 37 (January 2019)", "doi": "10.1145/3290350", "report-no": null, "categories": "cs.PL cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new techniques for automatically constructing probabilistic\nprograms for data analysis, interpretation, and prediction. These techniques\nwork with probabilistic domain-specific data modeling languages that capture\nkey properties of a broad class of data generating processes, using Bayesian\ninference to synthesize probabilistic programs in these modeling languages\ngiven observed data. We provide a precise formulation of Bayesian synthesis for\nautomatic data modeling that identifies sufficient conditions for the resulting\nsynthesis procedure to be sound. We also derive a general class of synthesis\nalgorithms for domain-specific languages specified by probabilistic\ncontext-free grammars and establish the soundness of our approach for these\nlanguages. We apply the techniques to automatically synthesize probabilistic\nprograms for time series data and multivariate tabular data. We show how to\nanalyze the structure of the synthesized programs to compute, for key\nqualitative properties of interest, the probability that the underlying data\ngenerating process exhibits each of these properties. Second, we translate\nprobabilistic programs in the domain-specific language into probabilistic\nprograms in Venture, a general-purpose probabilistic programming system. The\ntranslated Venture programs are then executed to obtain predictions of new time\nseries data and new multivariate data records. Experimental results show that\nour techniques can accurately infer qualitative structure in multiple\nreal-world data sets and outperform standard data analysis methods in\nforecasting and predicting new data.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 17:12:55 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Saad", "Feras A.", ""], ["Cusumano-Towner", "Marco F.", ""], ["Schaechtle", "Ulrich", ""], ["Rinard", "Martin C.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1907.06544", "submitter": "Joonha Park", "authors": "Joonha Park, Yves F. Atchad\\'e", "title": "Markov chain Monte Carlo algorithms with sequential proposals", "comments": "corrects Figure 7 (previously it showed the same plot as Figure 6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a general framework in Markov chain Monte Carlo (MCMC) sampling\nwhere sequential proposals are tried as a candidate for the next state of the\nMarkov chain. This sequential-proposal framework can be applied to various\nexisting MCMC methods, including Metropolis-Hastings algorithms using random\nproposals and methods that use deterministic proposals such as Hamiltonian\nMonte Carlo (HMC) or the bouncy particle sampler. Sequential-proposal MCMC\nmethods construct the same Markov chains as those constructed by the delayed\nrejection method under certain circumstances. In the context of HMC, the\nsequential-proposal approach has been proposed as extra chance generalized\nhybrid Monte Carlo (XCGHMC). We develop two novel methods in which the\ntrajectories leading to proposals in HMC are automatically tuned to avoid\ndoubling back, as in the No-U-Turn sampler (NUTS). The numerical efficiency of\nthese new methods compare favorably to the NUTS. We additionally show that the\nsequential-proposal bouncy particle sampler enables the constructed Markov\nchain to pass through regions of low target density and thus facilitates better\nmixing of the chain when the target density is multimodal.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 15:16:58 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 17:20:40 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 02:57:06 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Park", "Joonha", ""], ["Atchad\u00e9", "Yves F.", ""]]}, {"id": "1907.06748", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "Designing Perfect Simulation Algorithms using Local Correctness", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a randomized algorithm that draws samples exactly from a\ndistribution using recursion. Such an algorithm is called a perfect simulation,\nand here a variety of methods for building this type of algorithm are shown to\nderive from the same result: the Fundamental Theorem of Perfect Simulation\n(FTPS). The FTPS gives two necessary and sufficient conditions for the output\nof a recursive probabilistic algorithm to come exactly from the desired\ndistribution. First, the algorithm must terminate with probability 1. Second,\nthe algorithm must be locally correct, which means that if the recursive calls\nin the original algorithm are replaced by oracles that draw from the desired\ndistribution, then this new algorithm can be proven to be correct. While it is\nusually straightforward to verify these conditions, they are surprisingly\npowerful, giving the correctness of Acceptance/Rejection, Coupling from the\nPast, the Randomness Recycler, Read-once CFTP, Partial Rejection Sampling,\nPartially Recursive Acceptance Rejection, and various Bernoulli Factories. We\nillustrate the use of this algorithm by building a new Bernoulli Factory for\nlinear functions that is 41\\% faster than the previous method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:46:43 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "1907.06986", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth, Paul Fearnhead", "title": "Stochastic gradient Markov chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms are generally regarded as the gold\nstandard technique for Bayesian inference. They are theoretically\nwell-understood and conceptually simple to apply in practice. The drawback of\nMCMC is that in general performing exact inference requires all of the data to\nbe processed at each iteration of the algorithm. For large data sets, the\ncomputational cost of MCMC can be prohibitive, which has led to recent\ndevelopments in scalable Monte Carlo algorithms that have a significantly lower\ncomputational cost than standard MCMC. In this paper, we focus on a particular\nclass of scalable Monte Carlo algorithms, stochastic gradient Markov chain\nMonte Carlo (SGMCMC) which utilises data subsampling techniques to reduce the\nper-iteration cost of MCMC. We provide an introduction to some popular SGMCMC\nalgorithms and review the supporting theoretical results, as well as comparing\nthe efficiency of SGMCMC algorithms against MCMC on benchmark examples. The\nsupporting R code is available online.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:34:44 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Nemeth", "Christopher", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1907.07065", "submitter": "Peter Knaus", "authors": "Peter Knaus, Angela Bitto-Nemling, Annalisa Cadonna, Sylvia\n  Fr\\\"uhwirth-Schnatter", "title": "Shrinkage in the Time-Varying Parameter Model Framework Using the R\n  Package shrinkTVP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-varying parameter (TVP) models are widely used in time series analysis\nto flexibly deal with processes which gradually change over time. However, the\nrisk of overfitting in TVP models is well known. This issue can be dealt with\nusing appropriate global-local shrinkage priors, which pull time-varying\nparameters towards static ones. In this paper, we introduce the R package\nshrinkTVP (Knaus, Bitto-Nemling, Cadonna, and Fr\\\"uhwirth-Schnatter 2019),\nwhich provides a fully Bayesian implementation of shrinkage priors for TVP\nmodels, taking advantage of recent developments in the literature, in\nparticular that of Bitto and Fr\\\"uhwirth-Schnatter (2019). The package\nshrinkTVP allows for posterior simulation of the parameters through an\nefficient Markov Chain Monte Carlo (MCMC) scheme. Moreover, summary and\nvisualization methods, as well as the possibility of assessing predictive\nperformance through log predictive density scores (LPDSs), are provided. The\ncomputationally intensive tasks have been implemented in C++ and interfaced\nwith R. The paper includes a brief overview of the models and shrinkage priors\nimplemented in the package. Furthermore, core functionalities are illustrated,\nboth with simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:24:42 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 12:32:35 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 15:13:32 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Knaus", "Peter", ""], ["Bitto-Nemling", "Angela", ""], ["Cadonna", "Annalisa", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "1907.07309", "submitter": "Tao Zhang", "authors": "Tao Zhang, Yang Ning and David Ruppert", "title": "Optimal Sampling for Generalized Linear Models under Measurement\n  Constraints", "comments": "52 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under \"measurement constraints,\" responses are expensive to measure and\ninitially unavailable on most of records in the dataset, but the covariates are\navailable for the entire dataset. Our goal is to sample a relatively small\nportion of the dataset where the expensive responses will be measured and the\nresultant sampling estimator is statistically efficient. Measurement\nconstraints require the sampling probabilities can only depend on a very small\nset of the responses. A sampling procedure that uses responses at most only on\na small pilot sample will be called \"response-free.\" We propose a response-free\nsampling procedure \\mbox{(OSUMC)} for generalized linear models (GLMs). Using\nthe A-optimality criterion, i.e., the trace of the asymptotic variance, the\nresultant estimator is statistically efficient within a class of sampling\nestimators. We establish the unconditional asymptotic distribution of a general\nclass of response-free sampling estimators. This result is novel compared with\nthe existing conditional results obtained by conditioning on both covariates\nand responses. Under our unconditional framework, the subsamples are no longer\nindependent and new martingale techniques are developed for our asymptotic\ntheory. We further derive the A-optimal response-free sampling distribution.\nSince this distribution depends on population level quantities, we propose the\nOptimal Sampling Under Measurement Constraints (OSUMC) algorithm to approximate\nthe theoretical optimal sampling. Finally, we conduct an intensive empirical\nstudy to demonstrate the advantages of OSUMC algorithm over existing methods in\nboth statistical and computational perspectives.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 02:46:36 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 04:05:22 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 02:31:47 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhang", "Tao", ""], ["Ning", "Yang", ""], ["Ruppert", "David", ""]]}, {"id": "1907.07552", "submitter": "Themistoklis Sapsis", "authors": "Themistoklis P. Sapsis", "title": "Output-weighted optimal sampling for Bayesian regression and rare event\n  statistics using few samples", "comments": "34 pages; 13 figures", "journal-ref": null, "doi": "10.1098/rspa.2019.0834", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many important problems the quantity of interest is an unknown function\nof the parameters, which is a random vector with known statistics. Since the\ndependence of the output on this random vector is unknown, the challenge is to\nidentify its statistics, using the minimum number of function evaluations. This\nproblem can been seen in the context of active learning or optimal experimental\ndesign. We employ Bayesian regression to represent the derived model\nuncertainty due to finite and small number of input-output pairs. In this\ncontext we evaluate existing methods for optimal sample selection, such as\nmodel error minimization and mutual information maximization. We show that for\nthe case of known output variance, the commonly employed criteria in the\nliterature do not take into account the output values of the existing\ninput-output pairs, while for the case of unknown output variance this\ndependence can be very weak. We introduce a criterion that takes into account\nthe values of the output for the existing samples and adaptively selects inputs\nfrom regions of the parameter space which have important contribution to the\noutput. The new method allows for application to high-dimensional inputs,\npaving the way for optimal experimental design in high-dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 14:51:11 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 16:14:43 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Sapsis", "Themistoklis P.", ""]]}, {"id": "1907.07813", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion and Jonathan Rougier", "title": "Multi-Scale Process Modelling and Distributed Computation for Spatial\n  Data", "comments": "33 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a huge development in spatial modelling and prediction\nmethodology, driven by the increased availability of remote-sensing data and\nthe reduced cost of distributed-processing technology. It is well known that\nmodelling and prediction using infinite-dimensional process models is not\npossible with large data sets, and that both approximate models and, often,\napproximate-inference methods, are needed. The problem of fitting simple global\nspatial models to large data sets has been solved through the likes of\nmulti-resolution approximations and nearest-neighbour techniques. Here we\ntackle the next challenge, that of fitting complex, nonstationary, multi-scale\nmodels to large data sets. We propose doing this through the use of\nsuperpositions of spatial processes with increasing spatial scale and\nincreasing degrees of nonstationarity. Computation is facilitated through the\nuse of Gaussian Markov random fields and parallel Markov chain Monte Carlo\nbased on graph colouring. The resulting model allows for both distributed\ncomputing and distributed data. Importantly, it provides opportunities for\ngenuine model and data scaleability and yet is still able to borrow strength\nacross large spatial scales. We illustrate a two-scale version on a data set of\nsea-surface temperature containing on the order of one million observations,\nand compare our approach to state-of-the-art spatial modelling and prediction\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 23:43:05 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 22:09:28 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Rougier", "Jonathan", ""]]}, {"id": "1907.08082", "submitter": "Adam Golinski", "authors": "Adam Goli\\'nski, Frank Wood, Tom Rainforth", "title": "Amortized Monte Carlo Integration", "comments": "Awarded Best Paper Honourable Mention at International Conference on\n  Machine Learning (ICML) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to amortizing Bayesian inference focus solely on\napproximating the posterior distribution. Typically, this approximation is, in\nturn, used to calculate expectations for one or more target functions - a\ncomputational pipeline which is inefficient when the target function(s) are\nknown upfront. In this paper, we address this inefficiency by introducing AMCI,\na method for amortizing Monte Carlo integration directly. AMCI operates\nsimilarly to amortized inference but produces three distinct amortized\nproposals, each tailored to a different component of the overall expectation\ncalculation. At runtime, samples are produced separately from each amortized\nproposal, before being combined to an overall estimate of the expectation. We\nshow that while existing approaches are fundamentally limited in the level of\naccuracy they can achieve, AMCI can theoretically produce arbitrarily small\nerrors for any integrable target function using only a single sample from each\nproposal at runtime. We further show that it is able to empirically outperform\nthe theoretically optimal self-normalized importance sampler on a number of\nexample problems. Furthermore, AMCI allows not only for amortizing over\ndatasets but also amortizing over target functions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:36:48 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Goli\u0144ski", "Adam", ""], ["Wood", "Frank", ""], ["Rainforth", "Tom", ""]]}, {"id": "1907.08245", "submitter": "Leonardo Bottolo", "authors": "Angelos Alexopoulos and Leonardo Bottolo", "title": "Bayesian Variable Selection for Gaussian copula regression models", "comments": "39 pages main paper and 21 pages Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a novel Bayesian method to select important predictors in\nregression models with multiple responses of diverse types. A sparse Gaussian\ncopula regression model is used to account for the multivariate dependencies\nbetween any combination of discrete and/or continuous responses and their\nassociation with a set of predictors. We utilize the parameter expansion for\ndata augmentation strategy to construct a Markov chain Monte Carlo algorithm\nfor the estimation of the parameters and the latent variables of the model.\nBased on a centered parametrization of the Gaussian latent variables, we design\na fixed-dimensional proposal distribution to update jointly the latent binary\nvectors of important predictors and the corresponding non-zero regression\ncoefficients. For Gaussian responses and for outcomes that can be modeled as a\ndependent version of a Gaussian response, this proposal leads to a\nMetropolis-Hastings step that allows an efficient exploration of the\npredictors' model space. The proposed strategy is tested on simulated data and\napplied to real data sets in which the responses consist of low-intensity\ncounts, binary, ordinal and continuous variables.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:55:21 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 17:23:00 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Alexopoulos", "Angelos", ""], ["Bottolo", "Leonardo", ""]]}, {"id": "1907.08260", "submitter": "Caroline Moosm\\\"uller", "authors": "Caroline Moosm\\\"uller, Felix Dietrich, Ioannis G. Kevrekidis", "title": "A geometric approach to the transport of discontinuous densities", "comments": "26 pages, 15 figures, updated funding information", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different observations of a relation between inputs (\"sources\") and outputs\n(\"targets\") are often reported in terms of histograms (discretizations of the\nsource and the target densities). Transporting these densities to each other\nprovides insight regarding the underlying relation. In (forward) uncertainty\nquantification, one typically studies how the distribution of inputs to a\nsystem affects the distribution of the system responses. Here, we focus on the\nidentification of the system (the transport map) itself, once the input and\noutput distributions are determined, and suggest a modification of current\npractice by including data from what we call \"an observation process\". We\nhypothesize that there exists a smooth manifold underlying the relation; the\nsources and the targets are then partial observations (possibly projections) of\nthis manifold. Knowledge of such a manifold implies knowledge of the relation,\nand thus of \"the right\" transport between source and target observations. When\nthe source-target observations are not bijective (when the manifold is not the\ngraph of a function over both observation spaces, either because folds over\nthem give rise to density singularities, or because it marginalizes over\nseveral observables), recovery of the manifold is obscured. Using ideas from\nattractor reconstruction in dynamical systems, we demonstrate how additional\ninformation in the form of short histories of an observation process can help\nus recover the underlying manifold. The types of additional information\nemployed and the relation to optimal transport based solely on density\nobservations is illustrated and discussed, along with limitations in the\nrecovery of the true underlying relation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 19:33:25 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 16:55:30 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Moosm\u00fcller", "Caroline", ""], ["Dietrich", "Felix", ""], ["Kevrekidis", "Ioannis G.", ""]]}, {"id": "1907.08306", "submitter": "Brian Axelrod", "authors": "Brian Axelrod, Ilias Diakonikolas, Anastasios Sidiropoulos, Alistair\n  Stewart and Gregory Valiant", "title": "A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via\n  Locally Exponential Families", "comments": "The present paper is a merger of two independent works\n  arXiv:1811.03204 and arXiv:1812.05524, proposing essentially the same\n  algorithm to compute the log-concave MLE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the maximum likelihood multivariate\nlog-concave distribution for a set of points. Specifically, we present an\nalgorithm which, given $n$ points in $\\mathbb{R}^d$ and an accuracy parameter\n$\\epsilon>0$, runs in time $poly(n,d,1/\\epsilon),$ and returns a log-concave\ndistribution which, with high probability, has the property that the likelihood\nof the $n$ points under the returned distribution is at most an additive\n$\\epsilon$ less than the maximum likelihood that could be achieved via any\nlog-concave distribution. This is the first computationally efficient\n(polynomial time) algorithm for this fundamental and practically important\ntask. Our algorithm rests on a novel connection with exponential families: the\nmaximum likelihood log-concave distribution belongs to a class of structured\ndistributions which, while not an exponential family, \"locally\" possesses key\nproperties of exponential families. This connection then allows the problem of\ncomputing the log-concave maximum likelihood distribution to be formulated as a\nconvex optimization problem, and solved via an approximate first-order method.\nEfficiently approximating the (sub) gradients of the objective function of this\noptimization problem is quite delicate, and is the main technical challenge in\nthis work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 22:04:21 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Axelrod", "Brian", ""], ["Diakonikolas", "Ilias", ""], ["Sidiropoulos", "Anastasios", ""], ["Stewart", "Alistair", ""], ["Valiant", "Gregory", ""]]}, {"id": "1907.08372", "submitter": "David Stoffer", "authors": "Chen Gong and David S. Stoffer", "title": "An Approach to Efficient Fitting of Univariate and Multivariate\n  Stochastic Volatility Models", "comments": "Code here: https://github.com/nickpoison/Stochastic-Volatility-Models", "journal-ref": null, "doi": "10.13140/RG.2.2.29926.37440", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The stochastic volatility model is a popular tool for modeling the volatility\nof assets. The model is a nonlinear and non-Gaussian state space model, and\nconsequently is difficult to fit. Many approaches, both classical and Bayesian,\nhave been developed that rely on numerically intensive techniques such as\nquasi-maximum likelihood estimation and Markov chain Monte Carlo (MCMC).\nConvergence and mixing problems still plague MCMC algorithms when drawing\nsamples sequentially from the posterior distributions. While particle Gibbs\nmethods have been successful when applied to nonlinear or non-Gaussian state\nspace models in general, slow convergence still haunts the technique when\napplied specifically to stochastic volatility models. We present an approach\nthat couples particle Gibbs with ancestral sampling and joint parameter\nsampling that ameliorates the slow convergence and mixing problems when fitting\nboth univariate and multivariate stochastic volatility models. We demonstrate\nthe enhanced method on various numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 04:58:10 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Gong", "Chen", ""], ["Stoffer", "David S.", ""]]}, {"id": "1907.08387", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i, Luigi Lombardi, Marco D'Alessandro", "title": "A state space approach to dynamic modeling of mouse-tracking data", "comments": "The manuscript consists of 29 pages, 10 figures, and 5 tables. It\n  also contains Supplementary Materials (10 pages, 8 figures, and 3 tables)\n  providing extended results along with a simulation study", "journal-ref": "Frontiers in Psychology - Quantitative Psychology and Measurement,\n  2019, 10:2716", "doi": "10.3389/fpsyg.2019.02716", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mouse-tracking recording techniques are becoming very attractive in\nexperimental psychology. They provide an effective means of enhancing the\nmeasurement of some real-time cognitive processes involved in categorization,\ndecision-making, and lexical decision tasks. Mouse-tracking data are commonly\nanalysed using a two-step procedure which first summarizes individuals' hand\ntrajectories with independent measures, and then applies standard statistical\nmodels on them. However, this approach can be problematic in many cases. In\nparticular, it does not provide a direct way to capitalize the richness of hand\nmovement variability within a consistent and unified representation. In this\narticle we present a novel, unified framework for mouse-tracking data. Unlike\nstandard approaches to mouse-tracking, our proposal uses stochastic state-space\nmodeling to represent the observed trajectories in terms of both individual\nmovement dynamics and experimental variables. The model is estimated via a\nMetropolis-Hastings algorithm coupled with a non-linear recursive filter. The\ncharacteristics and potentials of the proposed approach are illustrated using a\nlexical decision case study. The results highlighted how dynamic modeling of\nmouse-tracking data can considerably improve the analysis of mouse-tracking\ntasks and the conclusions researchers can draw from them.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 06:50:00 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 06:01:28 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""], ["Lombardi", "Luigi", ""], ["D'Alessandro", "Marco", ""]]}, {"id": "1907.08414", "submitter": "Guo Yu", "authors": "Guo Yu, Jacob Bien, Ryan Tibshirani", "title": "Reluctant Interaction Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Including pairwise interactions between the predictors of a regression model\ncan produce better predicting models. However, to fit such interaction models\non typical data sets in biology and other fields can often require solving\nenormous variable selection problems with billions of interactions. The scale\nof such problems demands methods that are computationally cheap (both in time\nand memory) yet still have sound statistical properties. Motivated by these\nlarge-scale problem sizes, we adopt a very simple guiding principle: One should\nprefer a main effect over an interaction if all else is equal. This\n\"reluctance\" to interactions, while reminiscent of the hierarchy principle for\ninteractions, is much less restrictive. We design a computationally efficient\nmethod built upon this principle and provide theoretical results indicating\nfavorable statistical properties. Empirical results show dramatic computational\nimprovement without sacrificing statistical properties. For example, the\nproposed method can solve a problem with 10 billion interactions with 5-fold\ncross-validation in under 7 hours on a single CPU.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 08:56:07 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Yu", "Guo", ""], ["Bien", "Jacob", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1907.08611", "submitter": "Mathieu Besan\\c{c}on", "authors": "Mathieu Besan\\c{c}on, Theodore Papamarkou, David Anthoff, Alex Arslan,\n  Simon Byrne, Dahua Lin, John Pearson", "title": "Distributions.jl: Definition and Modeling of Probability Distributions\n  in the JuliaStats Ecosystem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random variables and their distributions are a central part in many areas of\nstatistical methods. The Distributions.jl package provides Julia users and\ndevelopers tools for working with probability distributions, leveraging Julia\nfeatures for their intuitive and flexible manipulation, while remaining highly\nefficient through zero-cost abstractions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 17:59:56 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 07:18:54 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 09:30:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Besan\u00e7on", "Mathieu", ""], ["Papamarkou", "Theodore", ""], ["Anthoff", "David", ""], ["Arslan", "Alex", ""], ["Byrne", "Simon", ""], ["Lin", "Dahua", ""], ["Pearson", "John", ""]]}, {"id": "1907.08733", "submitter": "Wenjie Zhao", "authors": "Wenjie Zhao, Raquel Prado", "title": "Efficient Bayesian PARCOR Approaches for Dynamic Modeling of\n  Multivariate Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian lattice filtering and smoothing approach is proposed for fast and\naccurate modeling and inference in multivariate non-stationary time series.\nThis approach offers computational feasibility and interpretable time-frequency\nanalysis in the multivariate context. The proposed framework allows us to\nobtain posterior estimates of the time-varying spectral densities of individual\ntime series components, as well as posterior measurements of the time-frequency\nrelationships across multiple components, such as time-varying coherence and\npartial coherence.\n  The proposed formulation considers multivariate dynamic linear models (MDLMs)\non the forward and backward time-varying partial autocorrelation coefficients\n(TV-VPARCOR). Computationally expensive schemes for posterior inference on the\nmultivariate dynamic PARCOR model are avoided using approximations in the MDLM\ncontext. Approximate inference on the corresponding time-varying vector\nautoregressive (TV-VAR) coefficients is obtained via Whittle's algorithm. A key\naspect of the proposed TV-VPARCOR representations is that they are of lower\ndimension, and therefore more efficient, than TV-VAR representations. The\nperformance of the TV-VPARCOR models is illustrated in simulation studies and\nin the analysis of multivariate non-stationary temporal data arising in\nneuroscience and environmental applications. Model performance is evaluated\nusing goodness-of-fit measurements in the time-frequency domain and also by\nassessing the quality of short-term forecasting.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 01:20:19 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zhao", "Wenjie", ""], ["Prado", "Raquel", ""]]}, {"id": "1907.09013", "submitter": "Tom LaGatta", "authors": "Brian d'Alessandro, Cathy O'Neil, Tom LaGatta", "title": "Conscientious Classification: A Data Scientist's Guide to\n  Discrimination-Aware Classification", "comments": "30 pages, 3 figures", "journal-ref": "Big Data, 5(2), 120-134 (2017)", "doi": "10.1089/big.2016.0048", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has helped to cultivate growing awareness that machine\nlearning systems fueled by big data can create or exacerbate troubling\ndisparities in society. Much of this research comes from outside of the\npracticing data science community, leaving its members with little concrete\nguidance to proactively address these concerns. This article introduces issues\nof discrimination to the data science community on its own terms. In it, we\ntour the familiar data mining process while providing a taxonomy of common\npractices that have the potential to produce unintended discrimination. We also\nsurvey how discrimination is commonly measured, and suggest how familiar\ndevelopment processes can be augmented to mitigate systems' discriminatory\npotential. We advocate that data scientists should be intentional about\nmodeling and reducing discriminatory outcomes. Without doing so, their efforts\nwill result in perpetuating any systemic discrimination that may exist, but\nunder a misleading veil of data-driven objectivity.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 17:57:45 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["d'Alessandro", "Brian", ""], ["O'Neil", "Cathy", ""], ["LaGatta", "Tom", ""]]}, {"id": "1907.09162", "submitter": "Martin Magris", "authors": "Martin Magris", "title": "On the simulation of the Hawkes process via Lambert-W functions", "comments": "A short discussion paper. 7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.PR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been developed for the simulation of the Hawkes process.\nThe oldest approach is the inverse sampling transform (ITS) suggested in\n\\citep{ozaki1979maximum}, but rapidly abandoned in favor of more efficient\nalternatives. This manuscript shows that the ITS approach can be conveniently\ndiscussed in terms of Lambert-W functions. An optimized and efficient\nimplementation suggests that this approach is computationally more performing\nthan more recent alternatives available for the simulation of the Hawkes\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 07:28:22 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Magris", "Martin", ""]]}, {"id": "1907.09164", "submitter": "Catherine Matias", "authors": "Tabea Rebafka (LPSM (UMR\\_8001)), Estelle Kuhn (MaIAGE), Catherine\n  Matias (LPSM (UMR\\_8001))", "title": "Properties of the Stochastic Approximation EM Algorithm with Mini-batch\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To deal with very large datasets a mini-batch version of the Monte Carlo\nMarkov Chain Stochastic Approximation Expectation-Maximization algorithm for\ngeneral latent variable models is proposed. For exponential models the\nalgorithm is shown to be convergent under classicalconditions as the number of\niterations increases. Numerical experiments illustrate the performance of the\nmini-batch algorithm in various models.In particular, we highlight that\nmini-batch sampling results in an important speed-up of the convergence of the\nsequence of estimators generated by the algorithm. Moreover, insights on the\neffect of the mini-batch size on the limit distribution are presented. Finally,\nwe illustrate how to use mini-batch sampling in practice to improve results\nwhen a constraint on the computing time is given.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 07:29:55 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 13:43:00 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 16:26:19 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Rebafka", "Tabea", "", "LPSM"], ["Kuhn", "Estelle", "", "MaIAGE"], ["Matias", "Catherine", "", "LPSM"]]}, {"id": "1907.09333", "submitter": "Lee Richardson", "authors": "Lee F. Richardson", "title": "Continuously Updated Data Analysis Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When doing data science, it's important to know what you're building. This\npaper describes an idealized final product of a data science project, called a\nContinuously Updated Data-Analysis System (CUDAS). The CUDAS concept\nsynthesizes ideas from a range of successful data science projects, such as\nNate Silver's FiveThirtyEight. A CUDAS can be built for any context, such as\nthe state of the economy, the state of the climate, and so on. To demonstrate,\nwe build two CUDAS systems. The first provides continuously-updated ratings for\nsoccer players, based on the newly developed Augmented Adjusted Plus-Minus\nstatistic. The second creates a large dataset of synthetic ecosystems, which is\nused for agent-based modeling of infectious diseases.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 12:26:14 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Richardson", "Lee F.", ""]]}, {"id": "1907.09565", "submitter": "Ranjan Maitra", "authors": "Geoffrey Z. Thompson and Ranjan Maitra and William Q. Meeker and\n  Ashraf Bastawros", "title": "Classification with the matrix-variate-$t$ distribution", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1080/10618600.2019.1696208", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-variate distributions can intuitively model the dependence structure\nof matrix-valued observations that arise in applications with multivariate time\nseries, spatio-temporal or repeated measures. This paper develops an\nExpectation-Maximization algorithm for discriminant analysis and classification\nwith matrix-variate $t$-distributions. The methodology shows promise on\nsimulated datasets or when applied to the forensic matching of fractured\nsurfaces or the classification of functional Magnetic Resonance, satellite or\nhand gestures images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:44:34 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 22:07:55 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Thompson", "Geoffrey Z.", ""], ["Maitra", "Ranjan", ""], ["Meeker", "William Q.", ""], ["Bastawros", "Ashraf", ""]]}, {"id": "1907.09808", "submitter": "Haiyan Liu", "authors": "Haiyan Liu, Georgios Aivaliotis, Jeanine Houwing-Duistermaat", "title": "On estimation of the effect lag of predictors and prediction in\n  functional linear model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a functional linear model to predict a response using multiple\nfunctional and longitudinal predictors and to estimate the effect lags of\npredictors. The coefficient functions are written as the expansion of a basis\nsystem (e.g. functional principal components, splines), and the coefficients of\nthe fixed basis functions are estimated via optimizing a penalization\ncriterion. Then time lags are determined by simultaneously searching on a prior\ngrid mesh based on minimization of prediction error criterion. Moreover,\nmathematical properties of the estimated parameters and predicted responses are\nstudied and performance of the method is evaluated by extensive simulations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 10:42:57 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Liu", "Haiyan", ""], ["Aivaliotis", "Georgios", ""], ["Houwing-Duistermaat", "Jeanine", ""]]}, {"id": "1907.09851", "submitter": "Samuel Wiqvist", "authors": "Samuel Wiqvist, Andrew Golightly, Ashleigh T. McLean, Umberto Picchini", "title": "Efficient inference for stochastic differential equation mixed-effects\n  models using correlated particle pseudo-marginal algorithms", "comments": "Accepted manuscript. DOI: https://doi.org/10.1016/j.csda.2020.107151.\n  31 pages, 15 Figures and . 31 pages, 15 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Stochastic differential equation mixed-effects models (SDEMEMs) are flexible\nhierarchical models that are able to account for random variability inherent in\nthe underlying time-dynamics, as well as the variability between experimental\nunits and, optionally, account for measurement error. Fully Bayesian inference\nfor state-space SDEMEMs is performed, using data at discrete times that may be\nincomplete and subject to measurement error. However, the inference problem is\ncomplicated by the typical intractability of the observed data likelihood which\nmotivates the use of sampling-based approaches such as Markov chain Monte\nCarlo. A Gibbs sampler is proposed to target the marginal posterior of all\nparameter values of interest. The algorithm is made computationally efficient\nthrough careful use of blocking strategies and correlated pseudo-marginal\nMetropolis-Hastings steps within the Gibbs scheme. The resulting methodology is\nflexible and is able to deal with a large class of SDEMEMs. The methodology is\ndemonstrated on three case studies, including tumor growth dynamics and\nneuronal data. The gains in terms of increased computational efficiency are\nmodel and data dependent, but unless bespoke sampling strategies requiring\nanalytical derivations are possible for a given model, we generally observe an\nefficiency increase of one order of magnitude when using correlated particle\nmethods together with our blocked-Gibbs strategy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 13:02:22 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 11:20:22 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 10:58:16 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 14:44:26 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Wiqvist", "Samuel", ""], ["Golightly", "Andrew", ""], ["McLean", "Ashleigh T.", ""], ["Picchini", "Umberto", ""]]}, {"id": "1907.10109", "submitter": "Andrew Finley Dr.", "authors": "Shinichiro Shirota, Andrew O. Finley, Bruce D. Cook, Sudipto Banerjee", "title": "Conjugate Nearest Neighbor Gaussian Process Models for Efficient\n  Statistical Interpolation of Large Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in spatial statistics is the analysis for massive\nspatially-referenced data sets. Such analyses often proceed from Gaussian\nprocess specifications that can produce rich and robust inference, but involve\ndense covariance matrices that lack computationally exploitable structures. The\nmatrix computations required for fitting such models involve floating point\noperations in cubic order of the number of spatial locations and dynamic memory\nstorage in quadratic order. Recent developments in spatial statistics offer a\nvariety of massively scalable approaches. Bayesian inference and hierarchical\nmodels, in particular, have gained popularity due to their richness and\nflexibility in accommodating spatial processes. Our current contribution is to\nprovide computationally efficient exact algorithms for spatial interpolation of\nmassive data sets using scalable spatial processes. We combine low-rank\nGaussian processes with efficient sparse approximations. Following recent work\nby [1], we model the low-rank process using a Gaussian predictive process (GPP)\nand the residual process as a sparsity-inducing nearest-neighbor Gaussian\nprocess (NNGP). A key contribution here is to implement these models using\nexact conjugate Bayesian modeling to avoid expensive iterative algorithms.\nThrough the simulation studies, we evaluate performance of the proposed\napproach and the robustness of our models, especially for long range\nprediction. We implement our approaches for remotely sensed light detection and\nranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit\n(TIU) in a remote portion of Interior Alaska.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 19:36:27 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Finley", "Andrew O.", ""], ["Cook", "Bruce D.", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1907.10117", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek", "title": "Simulating an infinite mean waiting time", "comments": null, "journal-ref": "Mathematica Applicanda (Matematyka Stosowana) 47(1): 93-102, 2019", "doi": "10.14708/ma.v47i1.6476", "report-no": null, "categories": "stat.ME q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a hybrid method to simulate the return time to the initial state\nin a critical-case birth--death process. The expected value of this return time\nis infinite, but its distribution asymptotically follows a power-law. Hence,\nthe simulation approach is to directly simulate the process, unless the\nsimulated time exceeds some threshold and if it does, draw the return time from\nthe tail of the power law.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 20:04:11 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 06:55:49 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Bartoszek", "Krzysztof", ""]]}, {"id": "1907.10397", "submitter": "Adelchi Azzalini", "authors": "Adelchi Azzalini and Mahdi Salehi", "title": "Some computational aspects of maximum likelihood estimation of the\n  skew-$t$ distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction, the skew-$t$ distribution has received much attention\nin the literature both for the study of theoretical properties and as a model\nfor data fitting in empirical work. A major motivation for this interest is the\nhigh degree of flexibility of the distribution as the parameters span their\nadmissible range, with ample variation of the associated measures of skewness\nand kurtosis. While this high flexibility allows to adapt a member of the\nparametric family to a wide range of data patterns, it also implies that\nparameter estimation is a more delicate operation with respect to less flexible\nparametric families, given that a small variation of the parameters can have a\nsubstantial effect on the selected distribution. In this context, the aim of\nthe present contribution is to deal with some computational aspects of maximum\nlikelihood estimation. A problem of interest is the possible presence of\nmultiple local maxima of the log-likelihood function. Another one, to which\nmost of our attention is dedicated, is the development of a quick and reliable\ninitialization method for the subsequent numerical maximization of the\nlog-likelihood function, both in the univariate and the multivariate context.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 12:40:12 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Azzalini", "Adelchi", ""], ["Salehi", "Mahdi", ""]]}, {"id": "1907.10426", "submitter": "Janet van Niekerk Dr", "authors": "Janet van Niekerk, Haakon Bakka, Haavard Rue and Olaf Schenk", "title": "New frontiers in Bayesian modeling using the INLA package in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The INLA package provides a tool for computationally efficient Bayesian\nmodeling and inference for various widely used models, more formally the class\nof latent Gaussian models. It is a non-sampling based framework which provides\napproximate results for Bayesian inference, using sparse matrices. The swift\nuptake of this framework for Bayesian modeling is rooted in the computational\nefficiency of the approach and catalyzed by the demand presented by the big\ndata era. In this paper, we present new developments within the INLA package\nwith the aim to provide a computationally efficient mechanism for the Bayesian\ninference of relevant challenging situations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:11:06 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 05:52:01 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["van Niekerk", "Janet", ""], ["Bakka", "Haakon", ""], ["Rue", "Haavard", ""], ["Schenk", "Olaf", ""]]}, {"id": "1907.10448", "submitter": "Leo Duan", "authors": "Leo L. Duan", "title": "Transport Monte Carlo: High-Accuracy Posterior Approximation via Random\n  Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian applications, there is a huge interest in rapid and accurate\nestimation of the posterior distribution, particularly for high dimensional or\nhierarchical models. In this article, we propose to use optimization to solve\nfor a joint distribution (random transport plan) between two random variables,\n$\\theta$ from the posterior distribution and $\\beta$ from the simple\nmultivariate uniform. Specifically, we obtain an approximate estimate of the\nconditional distribution $\\Pi(\\beta\\mid \\theta)$ as an infinite mixture of\nsimple location-scale changes; applying the Bayes' theorem,\n$\\Pi(\\theta\\mid\\beta)$ can be sampled as one of the reversed transforms from\nthe uniform, with the weight proportional to the posterior density/mass\nfunction. This produces independent random samples with high approximation\naccuracy, as well as nice theoretic guarantees. Our method shows compelling\nadvantages in performance and accuracy, compared to the state-of-the-art Markov\nchain Monte Carlo and approximations such as variational Bayes and normalizing\nflow. We illustrate this approach via several challenging applications, such as\nsampling from multi-modal distribution, estimating sparse signals in high\ndimension, and soft-thresholding of a graph with a prior on the degrees.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:47:41 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 04:09:32 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 01:28:07 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 18:08:19 GMT"}, {"version": "v5", "created": "Thu, 30 Jul 2020 18:07:48 GMT"}, {"version": "v6", "created": "Wed, 10 Mar 2021 19:51:28 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Duan", "Leo L.", ""]]}, {"id": "1907.10477", "submitter": "Axel Finke", "authors": "Axel Finke, Alexandre H. Thiery", "title": "On importance-weighted autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance weighted autoencoder (IWAE) (Burda et al., 2016) is a popular\nvariational-inference method which achieves a tighter evidence bound (and hence\na lower bias) than standard variational autoencoders by optimising a\nmulti-sample objective, i.e. an objective that is expressible as an integral\nover $K > 1$ Monte Carlo samples. Unfortunately, IWAE crucially relies on the\navailability of reparametrisations and even if these exist, the multi-sample\nobjective leads to inference-network gradients which break down as $K$ is\nincreased (Rainforth et al., 2018). This breakdown can only be circumvented by\nremoving high-variance score-function terms, either by heuristically ignoring\nthem (which yields the 'sticking-the-landing' IWAE (IWAE-STL) gradient from\nRoeder et al. (2017)) or through an identity from Tucker et al. (2019) (which\nyields the 'doubly-reparametrised' IWAE (IWAE-DREG) gradient). In this work, we\nargue that directly optimising the proposal distribution in importance sampling\nas in the reweighted wake-sleep (RWS) algorithm from Bornschein & Bengio (2015)\nis preferable to optimising IWAE-type multi-sample objectives. To formalise\nthis argument, we introduce an adaptive-importance sampling framework termed\nadaptive importance sampling for learning (AISLE) which slightly generalises\nthe RWS algorithm. We then show that AISLE admits IWAE-STL and IWAE-DREG (i.e.\nthe IWAE-gradients which avoid breakdown) as special cases.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 14:52:12 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 09:46:02 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Finke", "Axel", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "1907.10524", "submitter": "Raju Rimal Mr", "authors": "Raju Rimal, Trygve Alm{\\o}y and Solve S{\\ae}b{\\o}", "title": "Comparison of Multi-response Estimation Methods", "comments": "24 Pages", "journal-ref": null, "doi": "10.1016/j.chemolab.2020.104093", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prediction performance does not always reflect the estimation behaviour of a\nmethod. High error in estimation may necessarily not result in high prediction\nerror, but can lead to an unreliable prediction if test data lie in a slightly\ndifferent subspace than the training data. In addition, high estimation error\noften leads to unstable estimates, and consequently, the estimated effect of\npredictors on the response can not have a valid interpretation. Many research\nfields show more interest in the effect of predictor variables than actual\nprediction performance. This study compares some newly-developed (envelope) and\nwell-established (PCR, PLS) estimation methods using simulated data with\nspecifically designed properties such as Multicollinearity in the predictor\nvariables, the correlation between multiple responses and the position of\nprincipal components corresponding to predictors that are relevant for the\nresponse. This study aims to give some insights into these methods and help the\nresearchers to understand and use them for further study. Here we have, not\nsurprisingly, found that no single method is superior to others, but each has\nits strength for some specific nature of data. In addition, the newly developed\nenvelope method has shown impressive results in finding relevant information\nfrom data using significantly fewer components than the other methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:37:30 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 08:31:00 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Rimal", "Raju", ""], ["Alm\u00f8y", "Trygve", ""], ["S\u00e6b\u00f8", "Solve", ""]]}, {"id": "1907.10867", "submitter": "Nicole Erler", "authors": "Nicole S. Erler, Dimitris Rizopoulos and Emmanuel M. E. H. Lesaffre", "title": "JointAI: Joint Analysis and Imputation of Incomplete Data in R", "comments": "imputation, Bayesian, missing covariates, non-linear, interaction,\n  multi-level, survival, joint model R, JAGS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data occur in many types of studies and typically complicate the\nanalysis. Multiple imputation, either using joint modelling or the more\nflexible fully conditional specification approach, are popular and work well in\nstandard settings. In settings involving non-linear associations or\ninteractions, however, incompatibility of the imputation model with the\nanalysis model is an issue often resulting in bias. Similarly, complex outcomes\nsuch as longitudinal or survival outcomes cannot be adequately handled by\nstandard implementations. In this paper, we introduce the R package JointAI,\nwhich utilizes the Bayesian framework to perform simultaneous analysis and\nimputation in regression models with incomplete covariates. Using a fully\nBayesian joint modelling approach it overcomes the issue of uncongeniality\nwhile retaining the attractive flexibility of fully conditional specification\nmultiple imputation by specifying the joint distribution of analysis and\nimputation models as a sequence of univariate models that can be adapted to the\ntype of variable. JointAI provides functions for Bayesian inference with\ngeneralized linear and generalized linear mixed models and extensions thereof\nas well as survival models and joint models for longitudinal and survival data,\nthat take arguments analogous to corresponding well known functions for the\nanalysis of complete data from base R and other packages. Usage and features of\nJointAI are described and illustrated using various examples and the\ntheoretical background is outlined.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 07:19:34 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 10:19:14 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 08:08:46 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Erler", "Nicole S.", ""], ["Rizopoulos", "Dimitris", ""], ["Lesaffre", "Emmanuel M. E. H.", ""]]}, {"id": "1907.10940", "submitter": "Ziwen An", "authors": "Ziwen An, Leah F South and Christopher Drovandi", "title": "BSL: An R Package for Efficient Parameter Estimation for\n  Simulation-Based Models via Bayesian Synthetic Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian synthetic likelihood (BSL) is a popular method for estimating the\nparameter posterior distribution for complex statistical models and stochastic\nprocesses that possess a computationally intractable likelihood function.\nInstead of evaluating the likelihood, BSL approximates the likelihood of a\njudiciously chosen summary statistic of the data via model simulation and\ndensity estimation. Compared to alternative methods such as approximate\nBayesian computation (ABC), BSL requires little tuning and requires less model\nsimulations than ABC when the chosen summary statistic is high-dimensional. The\noriginal synthetic likelihood relies on a multivariate normal approximation of\nthe intractable likelihood, where the mean and covariance are estimated by\nsimulation. An extension of BSL considers replacing the sample covariance with\na penalised covariance estimator to reduce the number of required model\nsimulations. Further, a semi-parametric approach has been developed to relax\nthe normality assumption. In this paper, we present an R package called BSL\nthat amalgamates the aforementioned methods and more into a single, easy-to-use\nand coherent piece of software. The R package also includes several examples to\nillustrate how to use the package and demonstrate the utility of the methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 10:06:08 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["An", "Ziwen", ""], ["South", "Leah F", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1907.11017", "submitter": "Imke Botha", "authors": "Imke Botha, Robert Kohn, Christopher Drovandi", "title": "Particle Methods for Stochastic Differential Equation Mixed Effects\n  Models", "comments": "Minor revisions throughout, added a simple running example, some\n  updates to example and results (Sections 5-7), link to code on GitHub is\n  provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter inference for stochastic differential equation mixed effects models\n(SDEMEMs) is a challenging problem. Analytical solutions for these models are\nrarely available, which means that the likelihood is also intractable. In this\ncase, exact inference is possible using the pseudo-marginal method, where the\nintractable likelihood is replaced by its nonnegative unbiased estimate. A\nuseful application of this idea is particle MCMC, which uses a particle filter\nestimate of the likelihood. While the exact posterior is targeted by these\nmethods, a naive implementation for SDEMEMs can be highly inefficient. We\ndevelop three extensions to the naive approach which exploits specific aspects\nof SDEMEMs and other advances such as correlated pseudo-marginal methods. We\ncompare these methods on real and simulated data from a tumour xenography study\non mice.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:02:45 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 01:36:45 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Botha", "Imke", ""], ["Kohn", "Robert", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1907.11077", "submitter": "Adam Walder", "authors": "Adam Walder and Ephraim M. Hanks", "title": "Bayesian Analysis of Spatial Generalized Linear Mixed Models with\n  Laplace Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random field (GRF) models are widely used in spatial statistics to\ncapture spatially correlated error. We investigate the results of replacing\nGaussian processes with Laplace moving averages (LMAs) in spatial generalized\nlinear mixed models (SGLMMs). We demonstrate that LMAs offer improved\npredictive power when the data exhibits localized spikes in the response.\nSGLMMs with LMAs are shown to maintain analogous parameter inference and\nsimilar computing to Gaussian SGLMMs. We propose a novel discrete space LMA\nmodel for irregular lattices and construct conjugate samplers for LMAs with\ngeoreferenced and areal support. We provide a Bayesian analysis of SGLMMs with\nLMAs and GRFs over multiple data support and response types.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:13:44 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Walder", "Adam", ""], ["Hanks", "Ephraim M.", ""]]}, {"id": "1907.11331", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Nicolas Flammarion, Martin J. Wainwright, Peter L.\n  Bartlett", "title": "Improved Bounds for Discretization of Langevin Diffusions: Near-Optimal\n  Rates without Convexity", "comments": "Changes from v1: corrections in the proof of Lemma 6 and Lemma 10;\n  fixed some minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved analysis of the Euler-Maruyama discretization of the\nLangevin diffusion. Our analysis does not require global contractivity, and\nyields polynomial dependence on the time horizon. Compared to existing\napproaches, we make an additional smoothness assumption, and improve the\nexisting rate from $O(\\eta)$ to $O(\\eta^2)$ in terms of the KL divergence. This\nresult matches the correct order for numerical SDEs, without suffering from\nexponential time dependence. When applied to algorithms for sampling and\nlearning, this result simultaneously improves all those methods based on\nDalayan's approach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 22:54:45 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 17:57:36 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mou", "Wenlong", ""], ["Flammarion", "Nicolas", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1907.11541", "submitter": "Mucyo Karemera", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "Phase Transition Unbiased Estimation in High Dimensional Settings", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.04443", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important challenge in statistical analysis concerns the control of the\nfinite sample bias of estimators. For example, the maximum likelihood estimator\nhas a bias that can result in a significant inferential loss. This problem is\ntypically magnified in high-dimensional settings where the number of variables\n$p$ is allowed to diverge with the sample size $n$. However, it is generally\ndifficult to establish whether an estimator is unbiased and therefore its\nasymptotic order is a common approach used (in low-dimensional settings) to\nquantify the magnitude of the bias. As an alternative, we introduce a new and\nstronger property, possibly for high-dimensional settings, called phase\ntransition unbiasedness. An estimator satisfying this property is unbiased for\nall $n$ greater than a finite sample size $n^\\ast$. Moreover, we propose a\nphase transition unbiased estimator built upon the idea of matching an initial\nestimator computed on the sample and on simulated data. It is not required for\nthis initial estimator to be consistent and thus it can be chosen for its\ncomputational efficiency and/or for other desirable properties such as\nrobustness. This estimator can be computed using a suitable simulation based\nalgorithm, namely the iterative bootstrap, which is shown to converge\nexponentially fast. In addition, we demonstrate the consistency and the\nlimiting distribution of this estimator in high-dimensional settings. Finally,\nas an illustration, we use our approach to develop new estimators for the\nlogistic regression model, with and without random effects, that also enjoy\nother properties such as robustness to data contamination and are also not\naffected by the problem of separability. In a simulation exercise, the\ntheoretical results are confirmed in settings where the sample size is\nrelatively small compared to the model dimension.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:58:07 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 15:27:00 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 15:57:08 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1907.11559", "submitter": "Guilherme Pombo", "authors": "Guilherme Pombo, Robert Gray, Tom Varsavsky, John Ashburner, Parashkev\n  Nachev", "title": "Bayesian Volumetric Autoregressive generative models for better\n  semisupervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are rapidly gaining traction in medical imaging.\nNonetheless, most generative architectures struggle to capture the underlying\nprobability distributions of volumetric data, exhibit convergence problems, and\noffer no robust indices of model uncertainty. By comparison, the autoregressive\ngenerative model PixelCNN can be extended to volumetric data with relative\nease, it readily attempts to learn the true underlying probability distribution\nand it still admits a Bayesian reformulation that provides a principled\nframework for reasoning about model uncertainty. Our contributions in this\npaper are two fold: first, we extend PixelCNN to work with volumetric brain\nmagnetic resonance imaging data. Second, we show that reformulating this model\nto approximate a deep Gaussian process yields a measure of uncertainty that\nimproves the performance of semi-supervised learning, in particular\nclassification performance in settings where the proportion of labelled data is\nlow. We quantify this improvement across classification, regression, and\nsemantic segmentation tasks, training and testing on clinical magnetic\nresonance brain imaging data comprising T1-weighted and diffusion-weighted\nsequences.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 13:08:36 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Pombo", "Guilherme", ""], ["Gray", "Robert", ""], ["Varsavsky", "Tom", ""], ["Ashburner", "John", ""], ["Nachev", "Parashkev", ""]]}, {"id": "1907.11572", "submitter": "Nathan Wycoff", "authors": "Nathan Wycoff and Mickael Binois and Stefan M. Wild", "title": "Sequential Learning of Active Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, active subspace methods (ASMs) have become a popular means\nof performing subspace sensitivity analysis on black-box functions. Naively\napplied, however, ASMs require gradient evaluations of the target function. In\nthe event of noisy, expensive, or stochastic simulators, evaluating gradients\nvia finite differencing may be infeasible. In such cases, often a surrogate\nmodel is employed, on which finite differencing is performed. When the\nsurrogate model is a Gaussian process, we show that the ASM estimator is\navailable in closed form, rendering the finite-difference approximation\nunnecessary. We use our closed-form solution to develop acquisition functions\nfocused on sequential learning tailored to sensitivity analysis on top of ASMs.\nWe also show that the traditional ASM estimator may be viewed as a method of\nmoments estimator for a certain class of Gaussian processes. We demonstrate how\nuncertainty on Gaussian process hyperparameters may be propagated to\nuncertainty on the sensitivity analysis, allowing model-based confidence\nintervals on the active subspace. Our methodological developments are\nillustrated on several examples.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 13:39:23 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 16:14:39 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wycoff", "Nathan", ""], ["Binois", "Mickael", ""], ["Wild", "Stefan M.", ""]]}, {"id": "1907.11680", "submitter": "Dootika Vats", "authors": "Dootika Vats, Nathan Robertson, James M Flegal, Galin L Jones", "title": "Analyzing MCMC Output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is a sampling-based method for estimating\nfeatures of probability distributions. MCMC methods produce a serially\ncorrelated, yet representative, sample from the desired distribution. As such\nit can be difficult to know when the MCMC method is producing reliable results.\nWe introduce some fundamental methods for ensuring a trustworthy simulation\nexperiment. In particular, we present a workflow for output analysis in MCMC\nproviding estimators, approximate sampling distributions, stopping rules, and\nvisualization tools.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 17:17:02 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 16:34:12 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Vats", "Dootika", ""], ["Robertson", "Nathan", ""], ["Flegal", "James M", ""], ["Jones", "Galin L", ""]]}, {"id": "1907.11826", "submitter": "Kush Bhatia", "authors": "Kush Bhatia, Yi-An Ma, Anca D. Dragan, Peter L. Bartlett, Michael I.\n  Jordan", "title": "Bayesian Robustness: A Nonasymptotic Viewpoint", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of robustly estimating the posterior distribution for\nthe setting where observed data can be contaminated with potentially\nadversarial outliers. We propose Rob-ULA, a robust variant of the Unadjusted\nLangevin Algorithm (ULA), and provide a finite-sample analysis of its sampling\ndistribution. In particular, we show that after $T=\n\\tilde{\\mathcal{O}}(d/\\varepsilon_{\\textsf{acc}})$ iterations, we can sample\nfrom $p_T$ such that $\\text{dist}(p_T, p^*) \\leq \\varepsilon_{\\textsf{acc}} +\n\\tilde{\\mathcal{O}}(\\epsilon)$, where $\\epsilon$ is the fraction of\ncorruptions. We corroborate our theoretical analysis with experiments on both\nsynthetic and real-world data sets for mean estimation, regression and binary\nclassification.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 01:42:29 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bhatia", "Kush", ""], ["Ma", "Yi-An", ""], ["Dragan", "Anca D.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1907.11969", "submitter": "Birgir Hrafnkelsson", "authors": "Birgir Hrafnkelsson, Stefan Siegert, Rapha\\\"el Huser, Haakon Bakka,\n  \\'Arni V. J\\'ohannesson", "title": "Max-and-Smooth: a two-step approach for approximate Bayesian inference\n  in latent Gaussian models", "comments": "This is version 2 of the Max-and-Smooth paper. Minor changes have\n  been made to this version with respect to version 1 of the Max-and-Smooth\n  paper. The order of the authors was changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With modern high-dimensional data, complex statistical models are necessary,\nrequiring computationally feasible inference schemes. We introduce\nMax-and-Smooth, an approximate Bayesian inference scheme for a flexible class\nof latent Gaussian models (LGMs) where one or more of the likelihood parameters\nare modeled by latent additive Gaussian processes. Max-and-Smooth consists of\ntwo-steps. In the first step (Max), the likelihood function is approximated by\na Gaussian density with mean and covariance equal to either (a) the maximum\nlikelihood estimate and the inverse observed information, respectively, or (b)\nthe mean and covariance of the normalized likelihood function. In the second\nstep (Smooth), the latent parameters and hyperparameters are inferred and\nsmoothed with the approximated likelihood function. The proposed method ensures\nthat the uncertainty from the first step is correctly propagated to the second\nstep. Since the approximated likelihood function is Gaussian, the approximate\nposterior density of the latent parameters of the LGM (conditional on the\nhyperparameters) is also Gaussian, thus facilitating efficient posterior\ninference in high dimensions. Furthermore, the approximate marginal posterior\ndistribution of the hyperparameters is tractable, and as a result, the\nhyperparameters can be sampled independently of the latent parameters. In the\ncase of a large number of independent data replicates, sparse precision\nmatrices, and high-dimensional latent vectors, the speedup is substantial in\ncomparison to an MCMC scheme that infers the posterior density from the exact\nlikelihood function. The proposed inference scheme is demonstrated on one\nspatially referenced real dataset and on simulated data mimicking spatial,\ntemporal, and spatio-temporal inference problems. Our results show that\nMax-and-Smooth is accurate and fast.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 19:59:47 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 08:30:45 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Hrafnkelsson", "Birgir", ""], ["Siegert", "Stefan", ""], ["Huser", "Rapha\u00ebl", ""], ["Bakka", "Haakon", ""], ["J\u00f3hannesson", "\u00c1rni V.", ""]]}, {"id": "1907.11970", "submitter": "Ranjan Maitra", "authors": "Fan Dai, Somak Dutta and Ranjan Maitra", "title": "A Matrix--free Likelihood Method for Exploratory Factor Analysis of\n  High-dimensional Gaussian Data", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": "10.1080/10618600.2019.1704296", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel profile likelihood method for estimating the\ncovariance parameters in exploratory factor analysis of high-dimensional\nGaussian datasets with fewer observations than number of variables. An\nimplicitly restarted Lanczos algorithm and a limited-memory quasi-Newton method\nare implemented to develop a matrix-free framework for likelihood maximization.\nSimulation results show that our method is substantially faster than the\nexpectation-maximization solution without sacrificing accuracy. Our method is\napplied to fit factor models on data from suicide attempters, suicide ideators\nand a control group.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 20:04:55 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 19:25:43 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Dai", "Fan", ""], ["Dutta", "Somak", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1907.11985", "submitter": "Chenguang Dai", "authors": "Chenguang Dai and Jun S. Liu", "title": "The Wang-Landau Algorithm as Stochastic Optimization and Its\n  Acceleration", "comments": "10 pages, 3 figures", "journal-ref": "Phys. Rev. E 101, 033301 (2020)", "doi": "10.1103/PhysRevE.101.033301", "report-no": null, "categories": "stat.CO cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Wang-Landau algorithm can be formulated as a stochastic\ngradient descent algorithm minimizing a smooth and convex objective function,\nof which the gradient is estimated using Markov chain Monte Carlo iterations.\nThe optimization formulation provides us a new way to establish the convergence\nrate of the Wang-Landau algorithm, by exploiting the fact that almost surely,\nthe density estimates (on the logarithmic scale) remain in a compact set, upon\nwhich the objective function is strongly convex. The optimization viewpoint\nmotivates us to improve the efficiency of the Wang-Landau algorithm using\npopular tools including the momentum method and the adaptive learning rate\nmethod. We demonstrate the accelerated Wang-Landau algorithm on a\ntwo-dimensional Ising model and a two-dimensional ten-state Potts model.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 22:31:08 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 20:45:57 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Dai", "Chenguang", ""], ["Liu", "Jun S.", ""]]}, {"id": "1907.12012", "submitter": "Michael Weylandt", "authors": "Michael Weylandt", "title": "Multi-Rank Sparse and Functional PCA: Manifold Optimization and\n  Iterative Deflation Techniques", "comments": "To appear in IEEE CAMSAP 2019", "journal-ref": "2019 IEEE 8th International Workshop on Computational Advances in\n  Multi-Sensor Adaptive Processing (CAMSAP), pp.500-504", "doi": "10.1109/CAMSAP45676.2019.9022486", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of estimating multiple principal components using the\nrecently-proposed Sparse and Functional Principal Components Analysis (SFPCA)\nestimator. We first propose an extension of SFPCA which estimates several\nprincipal components simultaneously using manifold optimization techniques to\nenforce orthogonality constraints. While effective, this approach is\ncomputationally burdensome so we also consider iterative deflation approaches\nwhich take advantage of existing fast algorithms for rank-one SFPCA. We show\nthat alternative deflation schemes can more efficiently extract signal from the\ndata, in turn improving estimation of subsequent components. Finally, we\ncompare the performance of our manifold optimization and deflation techniques\nin a scenario where orthogonality does not hold and find that they still lead\nto significantly improved performance.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 04:43:54 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 20:57:59 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Weylandt", "Michael", ""]]}, {"id": "1907.12160", "submitter": "Soumya Mohanty", "authors": "Soumya D. Mohanty, Ethan Fahnestock", "title": "Adaptive spline fitting with particle swarm optimization", "comments": "Accepted version; Typo corrected in equation 3; Minor changes to text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fitting data with a spline, finding the optimal placement of knots can\nsignificantly improve the quality of the fit. However, the challenging\nhigh-dimensional and non-convex optimization problem associated with completely\nfree knot placement has been a major roadblock in using this approach. We\npresent a method that uses particle swarm optimization (PSO) combined with\nmodel selection to address this challenge. The problem of overfitting due to\nknot clustering that accompanies free knot placement is mitigated in this\nmethod by explicit regularization, resulting in a significantly improved\nperformance on highly noisy data. The principal design choices available in the\nmethod are delineated and a statistically rigorous study of their effect on\nperformance is carried out using simulated data and a wide variety of benchmark\nfunctions. Our results demonstrate that PSO-based free knot placement leads to\na viable and flexible adaptive spline fitting approach that allows the fitting\nof both smooth and non-smooth functions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 23:30:15 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 02:29:55 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 23:52:23 GMT"}, {"version": "v4", "created": "Sun, 28 Jun 2020 17:37:24 GMT"}, {"version": "v5", "created": "Sun, 26 Jul 2020 21:43:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mohanty", "Soumya D.", ""], ["Fahnestock", "Ethan", ""]]}, {"id": "1907.13554", "submitter": "Won Chang", "authors": "Won Chang, Bledar A. Konomi, Georgios Karagiannis, Yawen Guan, Murali\n  Haran", "title": "Ice Model Calibration Using Semi-continuous Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid changes in Earth's cryosphere caused by human activity can lead to\nsignificant environmental impacts. Computer models provide a useful tool for\nunderstanding the behavior and projecting the future of Arctic and Antarctic\nice sheets. However, these models are typically subject to large parametric\nuncertainties due to poorly constrained model input parameters that govern the\nbehavior of simulated ice sheets. Computer model calibration provides a formal\nstatistical framework to infer parameters using observational data, and to\nquantify the uncertainty in projections due to the uncertainty in these\nparameters. Calibration of ice sheet models is often challenging because the\nrelevant model output and observational data take the form of semi-continuous\nspatial data, with a point mass at zero and a right-skewed continuous\ndistribution for positive values. Current calibration approaches cannot handle\nsuch data. Here we introduce a hierarchical latent variable model that handles\nbinary spatial patterns and positive continuous spatial patterns as separate\ncomponents. To overcome challenges due to high-dimensionality we use\nlikelihood-based generalized principal component analysis to impose\nlow-dimensional structures on the latent variables for spatial dependence. We\napply our methodology to calibrate a physical model for the Antarctic ice sheet\nand demonstrate that we can overcome the aforementioned modeling and\ncomputational challenges. As a result of our calibration, we obtain improved\nfuture ice-volume change projections.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:30:36 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chang", "Won", ""], ["Konomi", "Bledar A.", ""], ["Karagiannis", "Georgios", ""], ["Guan", "Yawen", ""], ["Haran", "Murali", ""]]}, {"id": "1907.13563", "submitter": "Francisco Javier Rubio", "authors": "David Rossell and Francisco Javier Rubio", "title": "Additive Bayesian variable selection under censoring and\n  misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the role of misspecification and censoring on Bayesian model\nselection in the contexts of right-censored survival and concave log-likelihood\nregression. Misspecification includes wrongly assuming the censoring mechanism\nto be non-informative. Emphasis is placed on additive accelerated failure time,\nCox proportional hazards and probit models. We offer a theoretical treatment\nthat includes local and non-local priors, and a general non-linear effect\ndecomposition to improve power-sparsity trade-offs. We discuss a fundamental\nquestion: what solution can one hope to obtain when (inevitably) models are\nmisspecified, and how to interpret it? Asymptotically, covariates that do not\nhave predictive power for neither the outcome nor (for survival data) censoring\ntimes, in the sense of reducing a likelihood-associated loss, are discarded.\nMisspecification and censoring have an asymptotically negligible effect on\nfalse positives, but their impact on power is exponential. We show that it can\nbe advantageous to consider simple models that are computationally practical\nyet attain good power to detect potentially complex effects, including the use\nof finite-dimensional basis to detect truly non-parametric effects. We also\ndiscuss algorithms to capitalize on sufficient statistics and fast likelihood\napproximations for Gaussian-based survival and binary models.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:43:40 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 22:19:24 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 18:39:21 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 15:18:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Rossell", "David", ""], ["Rubio", "Francisco Javier", ""]]}, {"id": "1907.13570", "submitter": "Chris Sherlock Dr.", "authors": "Matthew Ludkin and Chris Sherlock", "title": "Hug and Hop: a discrete-time, non-reversible Markov chain Monte-Carlo\n  algorithm", "comments": "7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduced the Hug and Hop Markov chain Monte Carlo algorithm for\nestimating expectations with respect to an intractable distribution. The\nalgorithm alternates between two kernels: Hug and Hop. Hug is a non-reversible\nkernel that repeatedly applies the bounce mechanism from the recently proposed\nBouncy Particle Sampler to produce a proposal point far from the current\nposition, yet on almost the same contour of the target density, leading to a\nhigh acceptance probability. Hug is complemented by Hop, which deliberately\nproposes jumps between contours and has an efficiency that degrades very slowly\nwith increasing dimension. There are many parallels between Hug and Hamiltonian\nMonte Carlo (HMC) using a leapfrog integrator, including the order of the\nintegration scheme, however Hug is also able to make use of local Hessian\ninformation without requiring implicit numerical integration steps, and its\nperformance is not terminally affected by unbounded gradients of the\nlog-posterior. We test Hug and Hop empirically on a variety of toy targets and\nreal statistical models and find that it can, and often does, outperform HMC.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 12:57:41 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 14:36:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ludkin", "Matthew", ""], ["Sherlock", "Chris", ""]]}]