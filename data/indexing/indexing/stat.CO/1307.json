[{"id": "1307.0065", "submitter": "Jose Miguel Pasini", "authors": "Jos\\'e Miguel Pasini and Tuhin Sahai", "title": "Polynomial chaos based uncertainty quantification in Hamiltonian,\n  multi-time scale, and chaotic systems", "comments": "26 pages, 11 figures, accepted for publication in the Journal of\n  Computational Dynamics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos is a powerful technique for propagating uncertainty through\nordinary and partial differential equations. Random variables are expanded in\nterms of orthogonal polynomials and differential equations are derived for the\nexpansion coefficients. Here we study the structure and dynamics of these\ndifferential equations when the original system has Hamiltonian structure, has\nmultiple time scales, or displays chaotic dynamics. In particular, we prove\nthat the differential equations for the expansion coefficients in generalized\npolynomial chaos expansions of Hamiltonian systems retain the Hamiltonian\nstructure relative to the ensemble average Hamiltonian. We connect this with\nthe volume-preserving property of Hamiltonian flows to show that, for an\noscillator with uncertain frequency, a finite expansion must fail at long\ntimes, regardless of the order of the expansion. Also, using a two-time scale\nforced nonlinear oscillator, we show that a polynomial chaos expansion of the\ntime-averaged equations captures uncertainty in the slow evolution of the\nPoincar\\'e section of the system and that, as the time scale separation\nincreases, the computational advantage of this procedure increases. Finally,\nusing the forced Duffing oscillator as an example, we demonstrate that when the\noriginal dynamical system displays chaotic dynamics, the resulting dynamical\nsystem from polynomial chaos also displays chaotic dynamics, limiting its\napplicability.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 03:35:04 GMT"}, {"version": "v2", "created": "Tue, 17 Jun 2014 19:07:09 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Pasini", "Jos\u00e9 Miguel", ""], ["Sahai", "Tuhin", ""]]}, {"id": "1307.0680", "submitter": "Konstantinos Themelis", "authors": "Konstantinos D. Koutroumbas, Konstantinos E. Themelis, Athanasios A.\n  Rontogiannis", "title": "Approximating the mean of a truncated normal distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non trivial problem that arises in several applications is the estimation\nof the mean of a truncated normal distribution. In this paper, an iterative\ndeterministic scheme for approximating this mean is proposed. It has been\ninspired from an iterative Markov chain Monte Carlo (MCMC) scheme that\naddresses this problem and it can be viewed as a generalization of a recently\nproposed relevant model. Conditions are provided under which it is proved that\nthe scheme converges to a unique fixed point. Finally, the theoretical results\nare also supported by computer simulations, which also show the rapid\nconvergence of the method to a solution vector that is very close to the mean\nof the truncated normal distribution under study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 11:00:21 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 12:17:56 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Koutroumbas", "Konstantinos D.", ""], ["Themelis", "Konstantinos E.", ""], ["Rontogiannis", "Athanasios A.", ""]]}, {"id": "1307.1368", "submitter": "Xiangping Hu", "authors": "Xiangping Hu, Daniel Simpson and H{\\aa}vard Rue", "title": "Specifying Gaussian Markov Random Fields with Incomplete Orthogonal\n  Factorization using Givens Rotations", "comments": "34 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an approach for finding a sparse incomplete Cholesky factor\nthrough an incomplete orthogonal factorization with Givens rotations is\ndiscussed and applied to Gaussian Markov random fields (GMRFs). The incomplete\nCholesky factor obtained from the incomplete orthogonal factorization is\nusually sparser than the commonly used Cholesky factor obtained through the\nstandard Cholesky factorization. On the computational side, this approach can\nprovide a sparser Cholesky factor, which gives a computationally more efficient\nrepresentation of GMRFs. On the theoretical side, this approach is stable and\nrobust and always returns a sparse Cholesky factor. Since this approach applies\nboth to square matrices and to rectangle matrices, it works well not only on\nprecision matrices for GMRFs but also when the GMRFs are conditioned on a\nsubset of the variables or on observed data. Some common structures for\nprecision matrices are tested in order to illustrate the usefulness of the\napproach. One drawback to this approach is that the incomplete orthogonal\nfactorization is usually slower than the standard Cholesky factorization\nimplemented in standard libraries and currently it can be slower to build the\nsparse Cholesky factor.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 15:15:30 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Hu", "Xiangping", ""], ["Simpson", "Daniel", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1307.1799", "submitter": "Krzysztof Latuszynski", "authors": "Krzysztof Latuszynski and Jeffrey S. Rosenthal", "title": "The Containment Condition and AdapFail algorithms", "comments": "slight revision and with referees comments incorporated", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note investigates convergence of adaptive MCMC algorithms, i.e.\\\nalgorithms which modify the Markov chain update probabilities on the fly. We\nfocus on the Containment condition introduced in \\cite{roberts2007coupling}. We\nshow that if the Containment condition is \\emph{not} satisfied, then the\nalgorithm will perform very poorly. Specifically, with positive probability,\nthe adaptive algorithm will be asymptotically less efficient then \\emph{any}\nnonadaptive ergodic MCMC algorithm. We call such algorithms \\texttt{AdapFail},\nand conclude that they should not be used.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 16:18:48 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2013 10:50:34 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Latuszynski", "Krzysztof", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1307.2435", "submitter": "Ioannis Ntzoufras", "authors": "Dimitris Fouskakis and Ioannis Ntzoufras", "title": "Limiting behavior of the Jeffreys Power-Expected-Posterior Bayes Factor\n  in Gaussian Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expected-posterior priors (EPP) have been proved to be extremely useful for\ntesting hypothesis on the regression coefficients of normal linear models. One\nof the advantages of using EPPs is that impropriety of baseline priors causes\nno indeterminacy. However, in regression problems, they based on one or more\n\\textit{training samples}, that could influence the resulting posterior\ndistribution. The power-expected-posterior priors are minimally-informative\npriors that diminishing the effect of training samples on the EPP approach, by\ncombining ideas from the power-prior and unit-information-prior methodologies.\nIn this paper we show the consistency of the Bayes factors when using the\npower-expected-posterior priors, with the independence Jeffreys (or reference)\nprior as a baseline, for normal linear models under very mild conditions on the\ndesign matrix.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 13:03:00 GMT"}, {"version": "v2", "created": "Sun, 30 Nov 2014 15:58:40 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Fouskakis", "Dimitris", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "1307.2442", "submitter": "Dimitris Fouskakis", "authors": "Dimitris Fouskakis, Ioannis Ntzoufras and David Draper", "title": "Power-Expected-Posterior Priors for Variable Selection in Gaussian\n  Linear Models", "comments": null, "journal-ref": "Bayesian Anal. Volume 10, Number 1 (2015), 75-107", "doi": "10.1214/14-BA887", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the expected-posterior prior (EPP) approach to Bayesian\nvariable selection in linear models, we combine ideas from power-prior and\nunit-information-prior methodologies to simultaneously produce a\nminimally-informative prior and diminish the effect of training samples. The\nresult is that in practice our power-expected-posterior (PEP) methodology is\nsufficiently insensitive to the size n* of the training sample, due to PEP's\nunit-information construction, that one may take n* equal to the full-data\nsample size n and dispense with training samples altogether. In this paper we\nfocus on Gaussian linear models and develop our method under two different\nbaseline prior choices: the independence Jeffreys (or reference) prior,\nyielding the J-PEP posterior, and the Zellner g-prior, leading to Z-PEP. We\nfind that, under the reference baseline prior, the asymptotics of PEP Bayes\nfactors are equivalent to those of Schwartz's BIC criterion, ensuring\nconsistency of the PEP approach to model selection. We compare the performance\nof our method, in simulation studies and a real example involving prediction of\nair-pollutant concentrations from meteorological covariates, with that of a\nvariety of previously-defined variants on Bayes factors for objective variable\nselection. Our prior, due to its unit-information structure, leads to a\nvariable-selection procedure that (1) is systematically more parsimonious than\nthe basic EPP with minimal training sample, while sacrificing no desirable\nperformance characteristics to achieve this parsimony; (2) is robust to the\nsize of the training sample, thus enjoying the advantages described above\narising from the avoidance of training samples altogether; and (3) identifies\nmaximum-a-posteriori models that achieve good out-of-sample predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 13:15:13 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 10:05:39 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Fouskakis", "Dimitris", ""], ["Ntzoufras", "Ioannis", ""], ["Draper", "David", ""]]}, {"id": "1307.2449", "submitter": "Dimitris Fouskakis", "authors": "Dimitris Fouskakis and Ioannis Ntzoufras", "title": "Power-Conditional-Expected Priors: Using g-priors with Random Imaginary\n  Data for Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Zellner's g-prior and its recent hierarchical extensions are the most\npopular default prior choices in the Bayesian variable selection context. These\nprior set-ups can be expressed power-priors with fixed set of imaginary data.\nIn this paper, we borrow ideas from the power-expected-posterior (PEP) priors\nin order to introduce, under the g-prior approach, an extra hierarchical level\nthat accounts for the imaginary data uncertainty. For normal regression\nvariable selection problems, the resulting power-conditional-expected-posterior\n(PCEP) prior is a conjugate normal-inverse gamma prior which provides a\nconsistent variable selection procedure and gives support to more parsimonious\nmodels than the ones supported using the g-prior and the hyper-g prior for\nfinite samples. Detailed illustrations and comparisons of the variable\nselection procedures using the proposed method, the g-prior and the hyper-g\nprior are provided using both simulated and real data examples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 13:32:16 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Fouskakis", "Dimitris", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "1307.2668", "submitter": "Heng Lian", "authors": "Yuao Hu and Kaifeng Zhao and Heng Lian", "title": "Bayesian Quantile Regression for Partially Linear Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we develop a semiparametric Bayesian estimation and model\nselection approach for partially linear additive models in conditional quantile\nregression. The asymmetric Laplace distribution provides a mechanism for\nBayesian inferences of quantile regression models based on the check loss. The\nadvantage of this new method is that nonlinear, linear and zero function\ncomponents can be separated automatically and simultaneously during model\nfitting without the need of pre-specification or parameter tuning. This is\nachieved by spike-and-slab priors using two sets of indicator variables. For\nposterior inferences, we design an effective partially collapsed Gibbs sampler.\nSimulation studies are used to illustrate our algorithm. The proposed approach\nis further illustrated by applications to two real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 04:31:06 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Hu", "Yuao", ""], ["Zhao", "Kaifeng", ""], ["Lian", "Heng", ""]]}, {"id": "1307.3180", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob (National University of Singapore) and Lawrence Murray\n  (CSIRO Mathematics, Informatics & Statistics) and Sylvain Rubenthaler (Univ.\n  Nice Sophia Antipolis)", "title": "Path storage in the particle filter", "comments": "9 pages, 5 figures. To appear in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-013-9445-x", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the problem of storing the paths generated by a\nparticle filter and more generally by a sequential Monte Carlo algorithm. It\nprovides a theoretical result bounding the expected memory cost by $T + C N\n\\log N$ where $T$ is the time horizon, $N$ is the number of particles and $C$\nis a constant, as well as an efficient algorithm to realise this. The\ntheoretical result and the algorithm are illustrated with numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 16:45:47 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2014 10:37:15 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Jacob", "Pierre E.", "", "National University of Singapore"], ["Murray", "Lawrence", "", "CSIRO Mathematics, Informatics & Statistics"], ["Rubenthaler", "Sylvain", "", "Univ.\n  Nice Sophia Antipolis"]]}, {"id": "1307.3214", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko and Grigory Sokolov and Wenyu Du", "title": "An Accurate Method for Determining the Pre-Change Run-Length\n  Distribution of the Generalized Shiryaev--Roberts Detection Procedure", "comments": "24 pages, 5 figures, 5 tables, accepted for publication in Sequential\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change-of-measure is a powerful technique used across statistics, probability\nand analysis. Particularly known as Wald's likelihood ratio identity, the\ntechnique enabled the proof of a number of exact and asymptotic optimality\nresults pertaining to the problem of quickest change-point detection. Within\nthe latter problem's context we apply the technique to develop a numerical\nmethod to compute the Generalized Shiryaev--Roberts (GSR) detection procedure's\npre-change Run-Length distribution. Specifically, the method is based on the\nintegral-equations approach and uses the collocation framework with the basis\nfunctions chosen so as to exploit a certain change-of-measure identity and a\nspecific martingale property of the GSR procedure's detection statistic. As a\nresult, the method's accuracy and robustness improve substantially, even though\nthe method's theoretical rate of convergence is shown to be merely quadratic. A\ntight upper bound on the method's error is supplied as well. The method is not\nrestricted to a particular data distribution or to a specific value of the GSR\ndetection statistic's \"headstart\". To conclude, we offer a case study to\ndemonstrate the proposed method at work, drawing particular attention to the\nmethod's accuracy and its robustness with respect to three factors: (a)\npartition size, (b) change magnitude, and (c) Average Run Length (ARL) to false\nalarm level. Specifically, assuming independent standard Gaussian observations\nundergoing a surge in the mean, we employ the method to study the GSR\nprocedure's Run-Length's pre-change distribution, its average (i.e., the usual\nARL to false alarm) and standard deviation. As expected from the theoretical\nanalysis, the method's high accuracy and robustness with respect to the\nforegoing three factors are confirmed experimentally. We also comment on\nextending the method to handle other performance measures and other procedures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 18:58:45 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 22:08:10 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""], ["Du", "Wenyu", ""]]}, {"id": "1307.3282", "submitter": "Anna Klimova", "authors": "Anna Klimova and Tamas Rudas", "title": "Iterative Scaling in Curved Exponential Families", "comments": "The paper has one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a generalized iterative proportional fitting procedure\nwhich can be used for maximum likelihood estimation in a special class of the\ngeneral log-linear model. The models in this class, called relational, apply to\nmultivariate discrete sample spaces which do not necessarily have a Cartesian\nproduct structure and may not contain an overall effect. When applied to the\ncell probabilities, the models without the overall effect are curved\nexponential families and the values of the sufficient statistics are reproduced\nby the MLE only up to a constant of proportionality. The paper shows that\nIterative Proportional Fitting, Generalized Iterative Scaling and Improved\nIterative Scaling, fail to work for such models. The algorithm proposed here is\nbased on iterated Bregman projections. As a by-product, estimates of the\nmultiplicative parameters are also obtained.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 22:07:17 GMT"}, {"version": "v2", "created": "Sat, 29 Mar 2014 16:39:34 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Klimova", "Anna", ""], ["Rudas", "Tamas", ""]]}, {"id": "1307.3283", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni and J. Fraser Forbes", "title": "A particle filter approach to approximate posterior Cram\\'er-Rao lower\n  bound", "comments": "A condensed version of this article has been published in IEEE\n  Transactions on Aerospace and Electronic Systems and is in press", "journal-ref": "IEEE Transactions on Aerospace and Electronic Systems, Vol. 49,\n  no. 4, 2013", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The posterior Cram\\'er-Rao lower bound (PCRLB) derived in Tichavsk\\'y et al.,\n1998, provides a bound on the mean square error (MSE) obtained with any\nnon-linear state filter. Computing the PCRLB involves solving complex,\nmulti-dimensional expectations, which do not lend themselves to an easy\nanalytical solution. Furthermore, any attempt to approximate it using numerical\nor simulation based approaches require a priori access to the true states,\nwhich may not be available, except in simulations or in carefully designed\nexperiments. To allow recursive approximation of the PCRLB when the states are\nhidden or unmeasured, a new approach based on sequential Monte-Carlo (SMC) or\nparticle filters (PF) is proposed. The approach uses SMC methods to estimate\nthe hidden states using a sequence of the available sensor measurements. The\ndeveloped method is general and can be used to approximate the PCRLB in\nnon-linear systems with non-Gaussian state and sensor noise. The efficacy of\nthe developed method is illustrated on two simulation examples, including a\npractical problem of ballistic target tracking at re-entry phase.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 22:13:00 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.3490", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni and J. Fraser Forbes", "title": "On-line Bayesian parameter estimation in general non-linear state-space\n  models: A tutorial and new results", "comments": "A condensed version of this article has been published in: Tulsyan,\n  A., Huang, B., Gopaluni, R.B., Forbes, J.F. \"On simultaneous on-line state\n  and parameter estimation in non-linear state-space models\". Journal of\n  Process Control, vol 23, no. 4, 2013", "journal-ref": "Journal of Process Control, vol 23, no. 4, 2013", "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-line estimation plays an important role in process control and monitoring.\nObtaining a theoretical solution to the simultaneous state-parameter estimation\nproblem for non-linear stochastic systems involves solving complex\nmulti-dimensional integrals that are not amenable to analytical solution. While\nbasic sequential Monte-Carlo (SMC) or particle filtering (PF) algorithms for\nsimultaneous estimation exist, it is well recognized that there is a need for\nmaking these on-line algorithms non-degenerate, fast and applicable to\nprocesses with missing measurements. To overcome the deficiencies in\ntraditional algorithms, this work proposes a Bayesian approach to on-line state\nand parameter estimation. Its extension to handle missing data in real-time is\nalso provided. The simultaneous estimation is performed by filtering an\nextended vector of states and parameters using an adaptive\nsequential-importance-resampling (SIR) filter with a kernel density estimation\nmethod. The approach uses an on-line optimization algorithm based on\nKullback-Leibler (KL) divergence to allow adaptation of the SIR filter for\ncombined state-parameter estimation. An optimal tuning rule to control the\nwidth of the kernel and the variance of the artificial noise added to the\nparameters is also proposed. The approach is illustrated through numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 15:30:38 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.3598", "submitter": "Paul McNicholas", "authors": "Irene Vrbik and Paul D. McNicholas", "title": "Fractionally-Supervised Classification", "comments": null, "journal-ref": null, "doi": "10.1007/s00357-015-9188-9", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, there are three species of classification: unsupervised,\nsupervised, and semi-supervised. Supervised and semi-supervised classification\ndiffer by whether or not weight is given to unlabelled observations in the\nclassification procedure. In unsupervised classification, or clustering, all\nobservations are unlabeled and hence full weight is given to unlabelled\nobservations. When some observations are unlabelled, it can be very difficult\nto \\textit{a~priori} choose the optimal level of supervision, and the\nconsequences of a sub-optimal choice can be non-trivial. A flexible\nfractionally-supervised approach to classification is introduced, where any\nlevel of supervision --- ranging from unsupervised to supervised --- can be\nattained. Our approach uses a weighted likelihood, wherein weights control the\nrelative role that labelled and unlabelled data have in building a classifier.\nA comparison between our approach and the traditional species is presented\nusing simulated and real data. Gaussian mixture models are used as a vehicle to\nillustrate our fractionally-supervised classification approach; however, it is\nbroadly applicable and variations on the postulated model can be easily made.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 00:41:37 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2013 16:15:30 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 05:29:13 GMT"}, {"version": "v4", "created": "Sun, 13 Sep 2015 18:04:19 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2015 18:16:39 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Vrbik", "Irene", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1307.3645", "submitter": "Mehdi Molkaraie", "authors": "Mehdi Molkaraie and Hans-Andrea Loeliger", "title": "Partition Function of the Ising Model via Factor Graph Duality", "comments": "Proc. IEEE Int. Symp. on Information Theory (ISIT), Istanbul, Turkey,\n  July 7-12, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cond-mat.stat-mech math.IT physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partition function of a factor graph and the partition function of the\ndual factor graph are related to each other by the normal factor graph duality\ntheorem. We apply this result to the classical problem of computing the\npartition function of the Ising model. In the one-dimensional case, we thus\nobtain an alternative derivation of the (well-known) analytical solution. In\nthe two-dimensional case, we find that Monte Carlo methods are much more\nefficient on the dual graph than on the original graph, especially at low\ntemperature.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 13:20:52 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Molkaraie", "Mehdi", ""], ["Loeliger", "Hans-Andrea", ""]]}, {"id": "1307.3719", "submitter": "Florian Maire", "authors": "Florian Maire, Randal Douc, Jimmy Olsson", "title": "Comparison of asymptotic variances of inhomogeneous Markov chains with\n  application to Markov chain Monte Carlo methods", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1209 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 4, 1483-1510", "doi": "10.1214/14-AOS1209", "report-no": "IMS-AOS-AOS1209", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the asymptotic variance of sample path averages for\ninhomogeneous Markov chains that evolve alternatingly according to two\ndifferent $\\pi$-reversible Markov transition kernels $P$ and $Q$. More\nspecifically, our main result allows us to compare directly the asymptotic\nvariances of two inhomogeneous Markov chains associated with different kernels\n$P_i$ and $Q_i$, $i\\in\\{0,1\\}$, as soon as the kernels of each pair $(P_0,P_1)$\nand $(Q_0,Q_1)$ can be ordered in the sense of lag-one autocovariance. As an\nimportant application, we use this result for comparing different\ndata-augmentation-type Metropolis-Hastings algorithms. In particular, we\ncompare some pseudo-marginal algorithms and propose a novel exact algorithm,\nreferred to as the random refreshment algorithm, which is more efficient, in\nterms of asymptotic variance, than the Grouped Independence Metropolis-Hastings\nalgorithm and has a computational complexity that does not exceed that of the\nMonte Carlo Within Metropolis algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 10:22:35 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 00:46:58 GMT"}, {"version": "v3", "created": "Fri, 21 Mar 2014 17:22:59 GMT"}, {"version": "v4", "created": "Thu, 14 Aug 2014 06:54:15 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Maire", "Florian", ""], ["Douc", "Randal", ""], ["Olsson", "Jimmy", ""]]}, {"id": "1307.5381", "submitter": "Sang-Yun Oh", "authors": "Kshitij Khare, Sang-Yun Oh, Bala Rajaratnam", "title": "A convex pseudo-likelihood framework for high dimensional partial\n  correlation estimation with convergence guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse high dimensional graphical model selection is a topic of much interest\nin modern day statistics. A popular approach is to apply l1-penalties to either\n(1) parametric likelihoods, or, (2) regularized regression/pseudo-likelihoods,\nwith the latter having the distinct advantage that they do not explicitly\nassume Gaussianity. As none of the popular methods proposed for solving\npseudo-likelihood based objective functions have provable convergence\nguarantees, it is not clear if corresponding estimators exist or are even\ncomputable, or if they actually yield correct partial correlation graphs. This\npaper proposes a new pseudo-likelihood based graphical model selection method\nthat aims to overcome some of the shortcomings of current methods, but at the\nsame time retain all their respective strengths. In particular, we introduce a\nnovel framework that leads to a convex formulation of the partial covariance\nregression graph problem, resulting in an objective function comprised of\nquadratic forms. The objective is then optimized via a coordinate-wise\napproach. The specific functional form of the objective function facilitates\nrigorous convergence analysis leading to convergence guarantees; an important\nproperty that cannot be established using standard results, when the dimension\nis larger than the sample size, as is often the case in high dimensional\napplications. These convergence guarantees ensure that estimators are\nwell-defined under very general conditions, and are always computable. In\naddition, the approach yields estimators that have good large sample properties\nand also respect symmetry. Furthermore, application to simulated/real data,\ntiming comparisons and numerical convergence is demonstrated. We also present a\nnovel unifying framework that places all graphical pseudo-likelihood methods as\nspecial cases of a more general formulation, leading to important insights.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 07:01:20 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2014 05:27:44 GMT"}, {"version": "v3", "created": "Thu, 14 Aug 2014 21:15:28 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Khare", "Kshitij", ""], ["Oh", "Sang-Yun", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1307.5558", "submitter": "Paul McNicholas", "authors": "Paula M. Murray, Paul D. McNicholas and Ryan P. Browne", "title": "Mixtures of Common Skew-t Factor Analyzers", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.43", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of common skew-t factor analyzers model is introduced for\nmodel-based clustering of high-dimensional data. By assuming common component\nfactor loadings, this model allows clustering to be performed in the presence\nof a large number of mixture components or when the number of dimensions is too\nlarge to be well-modelled by the mixtures of factor analyzers model or a\nvariant thereof. Furthermore, assuming that the component densities follow a\nskew-t distribution allows robust clustering of skewed data. The alternating\nexpectation-conditional maximization algorithm is employed for parameter\nestimation. We demonstrate excellent clustering performance when our model is\napplied to real and simulated data.This paper marks the first time that skewed\ncommon factors have been used.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 19:18:39 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2013 15:56:34 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2013 21:28:57 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Murray", "Paula M.", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1307.5626", "submitter": "Joseph Dureau", "authors": "Joseph Dureau, S\\'ebastien Ballesteros and Tiffany Bogich", "title": "SSM: Inference for time series analysis with State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main motivation behind the open source library SSM is to reduce the\ntechnical friction that prevents modellers from sharing their work, quickly\niterating in crisis situations, and making their work directly usable by public\nauthorities to serve decision-making.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 09:15:33 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 22:10:36 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2013 09:41:38 GMT"}, {"version": "v4", "created": "Sun, 16 Feb 2014 12:09:00 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Dureau", "Joseph", ""], ["Ballesteros", "S\u00e9bastien", ""], ["Bogich", "Tiffany", ""]]}, {"id": "1307.6127", "submitter": "Nikolas Kantas", "authors": "Nikolas Kantas, Alexandros Beskos and Ajay Jasra", "title": "Sequential Monte Carlo Methods for High-Dimensional Inverse Problems: A\n  case study for the Navier-Stokes equations", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the inverse problem of estimating the initial condition of a\npartial differential equation, which is only observed through noisy\nmeasurements at discrete time intervals. In particular, we focus on the case\nwhere Eulerian measurements are obtained from the time and space evolving\nvector field, whose evolution obeys the two-dimensional Navier-Stokes equations\ndefined on a torus. This context is particularly relevant to the area of\nnumerical weather forecasting and data assimilation. We will adopt a Bayesian\nformulation resulting from a particular regularization that ensures the problem\nis well posed. In the context of Monte Carlo based inference, it is a\nchallenging task to obtain samples from the resulting high dimensional\nposterior on the initial condition. In real data assimilation applications it\nis common for computational methods to invoke the use of heuristics and\nGaussian approximations. The resulting inferences are biased and not\nwell-justified in the presence of non-linear dynamics and observations. On the\nother hand, Monte Carlo methods can be used to assimilate data in a principled\nmanner, but are often perceived as inefficient in this context due to the\nhigh-dimensionality of the problem. In this work we will propose a generic\nSequential Monte Carlo (SMC) sampling approach for high dimensional inverse\nproblems that overcomes these difficulties. The method builds upon Markov chain\nMonte Carlo (MCMC) techniques, which are currently considered as benchmarks for\nevaluating data assimilation algorithms used in practice. In our numerical\nexamples, the proposed SMC approach achieves the same accuracy as MCMC but in a\nmuch more efficient manner.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 15:39:43 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Kantas", "Nikolas", ""], ["Beskos", "Alexandros", ""], ["Jasra", "Ajay", ""]]}, {"id": "1307.6254", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni and J. Fraser Forbes", "title": "Error analysis in Bayesian identification of non-linear state-space\n  models", "comments": "This article has been published in: Tulsyan, A, B. Huang, R.B.\n  Gopaluni and J.F. Forbes (2013). Bayesian identification of non-linear\n  state-space models: Part II- Error Analysis. In: Proceedings of the 10th IFAC\n  International Symposium on Dynamics and Control of Process Systems. Mumbai,\n  India", "journal-ref": "Proceedings of the 10th IFAC International Symposium on Dynamics\n  and Control of Process Systems. Mumbai, India, 2013", "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, several methods based on sequential Monte Carlo\n(SMC) and Markov chain Monte Carlo (MCMC) have been proposed for Bayesian\nidentification of stochastic non-linear state-space models (SSMs). It is well\nknown that the performance of these simulation based identification methods\ndepends on the numerical approximations used in their design. We propose the\nuse of posterior Cram\\'er-Rao lower bound (PCRLB) as a mean square error (MSE)\nbound. Using PCRLB, a systematic procedure is developed to analyse the\nestimates delivered by Bayesian identification methods in terms of bias, MSE,\nand efficiency. The efficacy and utility of the proposed approach is\nillustrated through a numerical example.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 21:53:53 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.6258", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Swanand R. Khare, Biao Huang, R. Bhushan Gopaluni and\n  J. Fraser Forbes", "title": "Input design for Bayesian identification of non-linear state-space\n  models", "comments": "This article has been published in: Tulsyan, A, S.R. Khare, B. Huang,\n  R.B. Gopaluni and J.F. Forbes (2013). Bayesian identification of non-linear\n  state-space models: Part I- Input design. In: Proceedings of the 10th IFAC\n  International Symposium on Dynamics and Control of Process Systems. Mumbai,\n  India", "journal-ref": "Proceedings of the 10th IFAC International Symposium on Dynamics\n  and Control of Process Systems. Mumbai, India, 2013", "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for designing optimal inputs for on-line Bayesian\nidentification of stochastic non-linear state-space models. The proposed method\nrelies on minimization of the posterior Cram\\'er Rao lower bound derived for\nthe model parameters, with respect to the input sequence. To render the\noptimization problem computationally tractable, the inputs are parametrized as\na multi-dimensional Markov chain in the input space. The proposed approach is\nillustrated through a simulation example.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 22:09:04 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Khare", "Swanand R.", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.6701", "submitter": "Fabian Dunker", "authors": "Fabian Dunker, Jean-Pierre Florens, Thorsten Hohage, Jan Johannes,\n  Enno Mammen", "title": "Iterative Estimation of Solutions to Noisy Nonlinear Operator Equations\n  in Nonparametric Instrumental Regression", "comments": null, "journal-ref": "Journal of Econometrics , 2014, 178, 444-455", "doi": "10.1016/j.jeconom.2013.06.001", "report-no": null, "categories": "math.NA math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the solution of nonlinear integral equations with noisy\nintegral kernels as they appear in nonparametric instrumental regression. We\npropose a regularized Newton-type iteration and establish convergence and\nconvergence rate results. A particular emphasis is on instrumental regression\nmodels where the usual conditional mean assumption is replaced by a stronger\nindependence assumption. We demonstrate for the case of a binary instrument\nthat our approach allows the correct estimation of regression functions which\nare not identifiable with the standard model. This is illustrated in computed\nexamples with simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 11:25:11 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Dunker", "Fabian", ""], ["Florens", "Jean-Pierre", ""], ["Hohage", "Thorsten", ""], ["Johannes", "Jan", ""], ["Mammen", "Enno", ""]]}, {"id": "1307.7126", "submitter": "Grigory Sokolov", "authors": "Aleksey S. Polunchenko, Grigory Sokolov and Alexander G. Tartakovsky", "title": "Optimal Design and Analysis of the Exponentially Weighted Moving Average\n  Chart for Exponential Data", "comments": "28 pages, 5 figures, accepted for publication in the Sri Lankan\n  Journal of Applied Statistics. arXiv admin note: text overlap with\n  arXiv:1202.2849. text overlap with arXiv:1202.2849", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimal design of the Exponentially Weighted Moving Average (EWMA)\nchart by a proper choice of the smoothing factor and the initial value\n(headstart) of the decision statistic. The particular problem addressed is that\nof quickest detection of an abrupt change in the parameter of a discrete-time\nexponential model. Both pre- and post-change parameter values are assumed\nknown, but the change-point is not known. For this change-point detection\nscenario, we examine the performance of the conventional one-sided EWMA chart\nwith respect to two optimality criteria: Pollak's minimax criterion associated\nwith the maximal conditional expected delay to detection and Shiryaev's\nmulti-cyclic setup associated with the stationary expected delay to detection.\nUsing the integral-equations approach, we derive the exact closed-form formulae\nfor all of the required performance measures. Based on these formulae we find\nthe optimal smoothing factor and headstart by solving the corresponding two\nbivariate constraint optimization problems. Finally, the performance of the\noptimized EWMA chart is compared against that of the Shiryaev--Roberts--$r$\nprocedure in the minimax setting, and against that of the original\nShiryaev--Roberts procedure in the multi-cyclic setting. The main conclusion is\nthat the EWMA chart, when fully optimized, turns out to be a very competitive\nprocedure, with performance nearly indistinguishable from that of the\nknown-to-be-best Shiryaev--Roberts--$r$ and Shiryaev--Roberts procedures.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 19:11:35 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 20:11:11 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 20:30:44 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""], ["Tartakovsky", "Alexander G.", ""]]}, {"id": "1307.7948", "submitter": "Kristi Kuljus", "authors": "Kristi Kuljus and J\\\"uri Lember", "title": "On the accuracy of the Viterbi alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a hidden Markov model, the underlying Markov chain is usually hidden.\nOften, the maximum likelihood alignment (Viterbi alignment) is used as its\nestimate. Although having the biggest likelihood, the Viterbi alignment can\nbehave very untypically by passing states that are at most unexpected. To avoid\nsuch situations, the Viterbi alignment can be modified by forcing it not to\npass these states. In this article, an iterative procedure for improving the\nViterbi alignment is proposed and studied. The iterative approach is compared\nwith a simple bunch approach where a number of states with low probability are\nall replaced at the same time. It can be seen that the iterative way of\nadjusting the Viterbi alignment is more efficient and it has several advantages\nover the bunch approach. The same iterative algorithm for improving the Viterbi\nalignment can be used in the case of peeping, that is when it is possible to\nreveal hidden states. In addition, lower bounds for classification\nprobabilities of the Viterbi alignment under different conditions on the model\nparameters are studied.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 12:40:16 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Kuljus", "Kristi", ""], ["Lember", "J\u00fcri", ""]]}, {"id": "1307.8270", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "Applying least absolute deviation regression to regression-type\n  estimation of the index of a stable distribution using the characteristic\n  function", "comments": null, "journal-ref": "Communications in Statistics: Simulation and Computation, 2015,\n  44(9), pp. 2442-2462", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least absolute deviation regression is applied using a fixed number of points\nfor all values of the index to estimate the index and scale parameter of the\nstable distribution using regression methods based on the empirical\ncharacteristic function. The recognized fixed number of points estimation\nprocedure uses ten points in the interval zero to one, and least squares\nestimation. It is shown that using the more robust least absolute regression\nbased on iteratively re-weighted least squares outperforms the least squares\nprocedure with respect to bias and also mean square error in smaller samples.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 10:21:39 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["van Zyl", "J. Martin", ""]]}]