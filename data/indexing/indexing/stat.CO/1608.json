[{"id": "1608.00053", "submitter": "Yang Shi", "authors": "Yang Shi, Huining Kang, Ji-Hyun Lee and Hui Jiang", "title": "Efficiently estimating small p-values in permutation tests using\n  importance sampling and cross-entropy method", "comments": "21 pages, 3 tables, 1 supplemental file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation tests are commonly used for estimating p-values from statistical\nhypothesis testing when the sampling distribution of the test statistic under\nthe null hypothesis is not available or unreliable for finite sample sizes. One\ncritical challenge for permutation tests in genomic studies is that an enormous\nnumber of permutations is needed for obtaining reliable estimations of small\np-values, which requires intensive computational efforts. In this paper, we\ndevelop a computationally efficient algorithm for evaluating small p-values\nfrom permutation tests based on an adaptive importance sampling approach, which\nuses the cross-entropy method for finding the optimal proposal density.\nSimulation studies and analysis of a real microarray dataset demonstrate that\nour approach achieves considerable gains in computational efficiency comparing\nwith existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 00:20:26 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Shi", "Yang", ""], ["Kang", "Huining", ""], ["Lee", "Ji-Hyun", ""], ["Jiang", "Hui", ""]]}, {"id": "1608.00236", "submitter": "Hui Jiang", "authors": "Tzu-Ying Liu, Hui Jiang", "title": "Minimizing Sum of Truncated Convex Functions and Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a class of problems where the sum of truncated convex\nfunctions is minimized. In statistical applications, they are commonly\nencountered when $\\ell_0$-penalized models are fitted and usually lead to\nNP-Hard non-convex optimization problems. In this paper, we propose a general\nalgorithm for the global minimizer in low-dimensional settings. We also extend\nthe algorithm to high-dimensional settings, where an approximate solution can\nbe found efficiently. We introduce several applications where the sum of\ntruncated convex functions is used, compare our proposed algorithm with other\nexisting algorithms in simulation studies, and show its utility in\nedge-preserving image restoration on real data.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 16:14:13 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 01:59:24 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 03:05:49 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Liu", "Tzu-Ying", ""], ["Jiang", "Hui", ""]]}, {"id": "1608.00451", "submitter": "Avanti Athreya", "authors": "Avanti Athreya, Michael Kane, Bryan Lewis, Zachary Lubberts, Vince\n  Lyzinski, Youngser Park, Carey E. Priebe, and Minh Tang", "title": "Numerical tolerance for spectral decompositions of random matrices", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We precisely quantify the impact of statistical error in the quality of a\nnumerical approximation to a random matrix eigendecomposition, and under mild\nconditions, we use this to introduce an optimal numerical tolerance for\nresidual error in spectral decompositions of random matrices. We demonstrate\nthat terminating an eigendecomposition algorithm when the numerical error and\nstatistical error are of the same order results in computational savings with\nno loss of accuracy. We also repair a flaw in a ubiquitous termination\ncondition, one in wide employ in several computational linear algebra\nimplementations. We illustrate the practical consequences of our stopping\ncriterion with an analysis of simulated and real networks. Our theoretical\nresults and real-data examples establish that the tradeoff between statistical\nand numerical error is of significant import for data science.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 14:46:02 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 16:37:56 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 00:09:09 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Athreya", "Avanti", ""], ["Kane", "Michael", ""], ["Lewis", "Bryan", ""], ["Lubberts", "Zachary", ""], ["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""], ["Tang", "Minh", ""]]}, {"id": "1608.00481", "submitter": "Chang-Yun Lin", "authors": "Chang-Yun Lin", "title": "Robust split-plot designs for model misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing methods for constructing optimal split-plot designs, such as\nD-optimal designs, only focus on minimizing the variances and covariances of\nthe estimation for the fitted model. However, the underlying true model is\nusually complicated and unknown and the fitted model is often misspecified. If\nthere exist significant effects that are not included in the model, then the\nestimation could be highly biased. Therefore, a good split-plot designs should\nbe able to simultaneously control the variances/covariances and the bias of the\nestimation. In this paper, we propose a new method for constructing optimal\nsplit-plot designs that are robust for model misspecification. We provide a\ngeneral form of the loss function used for the D-optimal minimax criterion and\napply it to searching for robust split-plot designs. To more efficiently\nconstruct designs, we develop an algorithm which combines the anneal algorithm\nand point-exchange algorithm. We modify the update formulas for calculating the\ndeterminant and inverse of the updated matrix and apply them to increasing the\ncomputing speed for our developed program.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 16:12:19 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Lin", "Chang-Yun", ""]]}, {"id": "1608.00874", "submitter": "Fabrizio Leisen", "authors": "Jim Griffin and Fabrizio Leisen", "title": "Modelling and computation using NCoRM mixtures for density regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized compound random measures are flexible nonparametric priors for\nrelated distributions. We consider building general nonparametric regression\nmodels using normalized compound random measure mixture models. Posterior\ninference is made using a novel pseudo-marginal Metropolis-Hastings sampler for\nnormalized compound random measure mixture models. The algorithm makes use of a\nnew general approach to the unbiased estimation of Laplace functionals of\ncompound random measures (which includes completely random measures as a\nspecial case). The approach is illustrated on problems of density regression.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:47:42 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 10:41:56 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 10:21:58 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Griffin", "Jim", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1608.00945", "submitter": "Xin Zhang", "authors": "Xin Zhang and Scott A. Sisson", "title": "Blocking Collapsed Gibbs Sampler for Latent Dirichlet Allocation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent Dirichlet allocation (LDA) model is a widely-used latent variable\nmodel in machine learning for text analysis. Inference for this model typically\ninvolves a single-site collapsed Gibbs sampling step for latent variables\nassociated with observations. The efficiency of the sampling is critical to the\nsuccess of the model in practical large scale applications. In this article, we\nintroduce a blocking scheme to the collapsed Gibbs sampler for the LDA model\nwhich can, with a theoretical guarantee, improve chain mixing efficiency. We\ndevelop two procedures, an O(K)-step backward simulation and an O(log K)-step\nnested simulation, to directly sample the latent variables within each block.\nWe demonstrate that the blocking scheme achieves substantial improvements in\nchain mixing compared to the state of the art single-site collapsed Gibbs\nsampler. We also show that when the number of topics is over hundreds, the\nnested-simulation blocking scheme can achieve a significant reduction in\ncomputation time compared to the single-site sampler.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 19:24:50 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Zhang", "Xin", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1608.00990", "submitter": "Sebastian Liem", "authors": "Sebastian Liem", "title": "Barrett: out-of-core processing of MultiNest output", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Barrett is a Python package for processing and visualising statistical\ninferences made using the nested sampling algorithm MultiNest. The main\ndifferential feature from competitors are full out-of-core processing allowing\nbarrett to handle arbitrarily large datasets. This is achieved by using the\nHDF5 data format.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 20:22:25 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Liem", "Sebastian", ""]]}, {"id": "1608.01201", "submitter": "Fabio Rapallo", "authors": "Flavio Mignone, Fabio Rapallo", "title": "Detection of outlying proportions", "comments": "15 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new method for detecting outliers in a set of\nproportions. It is based on the construction of a suitable two-way contingency\ntable and on the application of an algorithm for the detection of outlying\ncells in such table. We exploit the special structure of the relevant\ncontingency table to increase the efficiency of the method. The main properties\nof our algorithm, together with a guide for the choice of the parameters, are\ninvestigated through simulations, and in simple cases some theoretical\njustifications are provided. Several examples on synthetic data and an example\nbased on pseudo-real data from biological experiments demonstrate the good\nperformances of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 14:23:07 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Mignone", "Flavio", ""], ["Rapallo", "Fabio", ""]]}, {"id": "1608.01274", "submitter": "Daniel Kessler", "authors": "Daniel Kessler, Michael Angstadt, Chandra Sripada", "title": "Which Findings from the Functional Neuromaging Literature Can We Trust?", "comments": "All authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their recent \"Cluster Failure\" paper, Eklund and colleagues cast doubt on\nthe accuracy of a widely used statistical test in functional neuroimaging.\nHere, we leverage nonparametric methods that control the false discovery rate\nto offer more nuanced, quantitative guidance about which findings in the\nexisting literature can be trusted. We show that, in the task studies examined\nby Eklund et al., most clusters originally reported to be significant are\nindeed trustworthy by the false discovery rate benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 18:18:23 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Kessler", "Daniel", ""], ["Angstadt", "Michael", ""], ["Sripada", "Chandra", ""]]}, {"id": "1608.01455", "submitter": "Majid K. Vakilzadeh", "authors": "Majid K. Vakilzadeh, James L. Beck, Thomas Abrahamsson", "title": "Using Approximate Bayesian Computation by Subset Simulation for\n  Efficient Posterior Assessment of Dynamic State-Space Model Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) methods have gained in their\npopularity over the last decade because they expand the horizon of Bayesian\nparameter inference methods to the range of models for which only forward\nsimulation is available. The majority of the ABC methods rely on the choice of\na set of summary statistics to reduce the dimension of the data. However, as\nhas been noted in the ABC literature, the lack of convergence guarantees that\nis induced by the absence of a vector of sufficient summary statistics that\nassures inter-model sufficiency over the set of competing models, hinders the\nuse of the usual ABC methods when applied to Bayesian model selection or\nassessment. In this paper, we present a novel ABC model selection procedure for\ndynamical systems based on a newly appeared multi-level Markov chain Monte\nCarlo method, self-regulating ABC-SubSim, and a hierarchical state-space\nformulation of dynamic models. We show that this formulation makes it possible\nto independently approximate the model evidence required for assessing the\nposterior probability of each of the competing models. We also show that\nABC-SubSim not only provides an estimate of the model evidence as a simple\nby-product but also it gives the posterior probability of each model as a\nfunction of the tolerance level, which allows the ABC model choices made in\nprevious studies to be understood. We illustrate the performance of the\nproposed framework for ABC model updating and model class selection by applying\nit to two problems in Bayesian system identification: a single\ndegree-of-freedom bilinear hysteretic oscillator and a three-story shear\nbuilding with Masing hysteresis, both of which are subject to a seismic\nexcitation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 08:11:55 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Vakilzadeh", "Majid K.", ""], ["Beck", "James L.", ""], ["Abrahamsson", "Thomas", ""]]}, {"id": "1608.01734", "submitter": "Lingyao Meng", "authors": "Lingyao Meng", "title": "Method for Computation of the Fisher Information Matrix in the\n  Expectation-Maximization Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm is an iterative computational\nmethod to calculate the maximum likelihood estimators (MLEs) from the sample\ndata. It converts a complicated one-time calculation for the MLE of the\nincomplete data to a series of relatively simple calculations for the MLEs of\nthe complete data. When the MLE is available, we naturally want the Fisher\ninformation matrix (FIM) of unknown parameters. The FIM is, in fact, a good\nmeasure of the amount of information a sample of data provides and can be used\nto determine the lower bound of the variance and the asymptotic variance of the\nestimators. However, one of the limitations of the EM is that the FIM is not an\nautomatic by-product of the algorithm. In this paper, we review some basic\nideas of the EM and the FIM. Then we construct a simple Monte Carlo-based\nmethod requiring only the gradient values of the function we obtain from the E\nstep and basic operations. Finally, we conduct theoretical analysis and\nnumerical examples to show the efficiency of our method. The key part of our\nmethod is to utilize the simultaneous perturbation stochastic approximation\nmethod to approximate the Hessian matrix from the gradient of the conditional\nexpectation of the complete-data log-likelihood function. Key words: Fisher\ninformation matrix, EM algorithm, Monte Carlo, Simultaneous perturbation\nstochastic approximation\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 01:16:04 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Meng", "Lingyao", ""]]}, {"id": "1608.01958", "submitter": "Matthias Morzfeld", "authors": "Matthias Morzfeld, Marcus S. Day, Ray W. Grout, George Shu Heng Pau,\n  Stefan A. Finsterle, John B. Bell", "title": "Iterative importance sampling algorithms for parameter estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parameter estimation problems one computes a posterior distribution over\nuncertain parameters defined jointly by a prior distribution, a model, and\nnoisy data. Markov Chain Monte Carlo (MCMC) is often used for the numerical\nsolution of such problems. An alternative to MCMC is importance sampling, which\ncan exhibit near perfect scaling with the number of cores on high performance\ncomputing systems because samples are drawn independently. However, finding a\nsuitable proposal distribution is a challenging task. Several sampling\nalgorithms have been proposed over the past years that take an iterative\napproach to constructing a proposal distribution. We investigate the\napplicability of such algorithms by applying them to two realistic and\nchallenging test problems, one in subsurface flow, and one in combustion\nmodeling. More specifically, we implement importance sampling algorithms that\niterate over the mean and covariance matrix of Gaussian or multivariate\nt-proposal distributions. Our implementation leverages massively parallel\ncomputers, and we present strategies to initialize the iterations using\n\"coarse\" MCMC runs or Gaussian mixture models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 17:59:17 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 17:46:22 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Morzfeld", "Matthias", ""], ["Day", "Marcus S.", ""], ["Grout", "Ray W.", ""], ["Pau", "George Shu Heng", ""], ["Finsterle", "Stefan A.", ""], ["Bell", "John B.", ""]]}, {"id": "1608.02148", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson, Sergey Voronin, Steven L. Brunton, J. Nathan\n  Kutz", "title": "Randomized Matrix Decompositions using R", "comments": null, "journal-ref": "Journal of Statistical Software. May 2019, Volume 89, Issue 11", "doi": "10.18637/jss.v089.i11", "report-no": null, "categories": "stat.CO cs.MS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix decompositions are fundamental tools in the area of applied\nmathematics, statistical computing, and machine learning. In particular,\nlow-rank matrix decompositions are vital, and widely used for data analysis,\ndimensionality reduction, and data compression. Massive datasets, however, pose\na computational challenge for traditional algorithms, placing significant\nconstraints on both memory and processing power. Recently, the powerful concept\nof randomness has been introduced as a strategy to ease the computational load.\nThe essential idea of probabilistic algorithms is to employ some amount of\nrandomness in order to derive a smaller matrix from a high-dimensional data\nmatrix. The smaller matrix is then used to compute the desired low-rank\napproximation. Such algorithms are shown to be computationally efficient for\napproximating matrices with low-rank structure. We present the \\proglang{R}\npackage rsvd, and provide a tutorial introduction to randomized matrix\ndecompositions. Specifically, randomized routines for the singular value\ndecomposition, (robust) principal component analysis, interpolative\ndecomposition, and CUR decomposition are discussed. Several examples\ndemonstrate the routines, and show the computational advantage over other\nmethods implemented in R.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 19:47:48 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 20:03:57 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 23:01:44 GMT"}, {"version": "v4", "created": "Sun, 1 Apr 2018 21:26:59 GMT"}, {"version": "v5", "created": "Tue, 26 Nov 2019 23:33:14 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Voronin", "Sergey", ""], ["Brunton", "Steven L.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1608.02199", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Biplab Paul and Debasis Kundu", "title": "An EM algorithm for absolutely continuous Marshall-Olkin bivariate\n  Pareto distribution with location and scale", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently \\cite{AsimitFurmanVernic:2016} used EM algorithm to estimate\nsingular Marshall-Olkin bivariate Pareto distribution. We describe\n\\textbf{absolutely continuous} version of this distribution. We study\nestimation of the parameters by EM algorithm both in presence and without\npresence of location and scale parameters. Some innovative solutions are\nprovided for different problems arised during implementation of EM algorithm. A\nreal-life data analysis is also shown for illustrative purpose.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 09:39:13 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 19:17:40 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 13:05:11 GMT"}, {"version": "v4", "created": "Sun, 18 Mar 2018 03:51:30 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Paul", "Biplab", ""], ["Kundu", "Debasis", ""]]}, {"id": "1608.02797", "submitter": "Sharon Lee", "authors": "Sharon X Lee, Kaleb L Leemaqz, Geoffrey J McLachlan", "title": "A block EM algorithm for multivariate skew normal and skew t-mixture\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of skew distributions provide a flexible tool for modelling\nheterogeneous data with asymmetric distributional features. However, parameter\nestimation via the Expectation-Maximization (EM) algorithm can become very\ntime-consuming due to the complicated expressions involved in the E-step that\nare numerically expensive to evaluate. A more time-efficient implementation of\nthe EM algorithm was recently proposed which allows each component of the\nmixture model to be evaluated in parallel. In this paper, we develop a block\nimplementation of the EM algorithm that facilitates the calculations in the E-\nand M-steps to be spread across a larger number of threads. We focus on the\nfitting of finite mixtures of multivariate skew normal and skew\nt-distributions, and show that both the E- and M-steps in the EM algorithm can\nbe modified to allow the data to be split into blocks. The approach can be\neasily implemented for use by multicore and multi-processor machines. It can\nalso be applied concurrently with the recently proposed multithreaded EM\nalgorithm to achieve further reduction in computation time. The improvement in\ntime performance is illustrated on some real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 13:28:38 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Lee", "Sharon X", ""], ["Leemaqz", "Kaleb L", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1608.03302", "submitter": "Arthur White PhD", "authors": "Arthur White and Thomas Brendan Murphy", "title": "Exponential Family Mixed Membership Models for Soft~Clustering of\n  Multivariate Data", "comments": null, "journal-ref": "White, A. & Murphy, T.B. Adv Data Anal Classif (2016) 10: 521", "doi": "10.1007/s11634-016-0267-5", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several years, model-based clustering methods have successfully tackled\nmany of the challenges presented by data-analysts. However, as the scope of\ndata analysis has evolved, some problems may be beyond the standard mixture\nmodel framework. One such problem is when observations in a dataset come from\noverlapping clusters, whereby different clusters will possess similar\nparameters for multiple variables. In this setting, mixed membership models, a\nsoft clustering approach whereby observations are not restricted to single\ncluster membership, have proved to be an effective tool. In this paper, a\nmethod for fitting mixed membership models to data generated by a member of an\nexponential family is outlined. The method is applied to count data obtained\nfrom an ultra running competition, and compared with a standard mixture model\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 21:12:01 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["White", "Arthur", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1608.03352", "submitter": "Deborshee Sen", "authors": "Deborshee Sen, Ajay Jasra and Yan Zhou", "title": "Some Contributions to Sequential Monte Carlo Methods for Option Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.CP q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pricing options is an important problem in financial engineering. In many\nscenarios of practical interest, financial option prices associated to an\nunderlying asset reduces to computing an expectation w.r.t.~a diffusion\nprocess. In general, these expectations cannot be calculated analytically, and\none way to approximate these quantities is via the Monte Carlo method; Monte\nCarlo methods have been used to price options since at least the 1970's. It has\nbeen seen in Del Moral, P. \\& Shevchenko, P.V. (2014) `Valuation of barrier\noptions using Sequential Monte Carlo' and Jasra, A. \\& Del Moral, P. (2011)\n`Sequential Monte Carlo for option pricing' that Sequential Monte Carlo (SMC)\nmethods are a natural tool to apply in this context and can vastly improve over\nstandard Monte Carlo. In this article, in a similar spirit to Del Moral, P. \\&\nShevchenko, P.V. (2014) `Valuation of barrier options using sequential Monte\nCarlo' and Jasra, A. \\& Del Moral, P. (2011) `Sequential Monte Carlo for option\npricing' we show that one can achieve significant gains by using SMC methods by\nconstructing a sequence of artificial target densities over time. In\nparticular, we approximate the optimal importance sampling distribution in the\nSMC algorithm by using a sequence of weighting functions. This is demonstrated\non two examples, barrier options and target accrual redemption notes (TARN's).\nWe also provide a proof of unbiasedness of our SMC estimate.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 02:54:31 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Sen", "Deborshee", ""], ["Jasra", "Ajay", ""], ["Zhou", "Yan", ""]]}, {"id": "1608.03393", "submitter": "Abolfazl Keshvari", "authors": "Abolfazl Keshvari", "title": "A penalized method for multivariate concave least squares with\n  application to productivity analysis", "comments": null, "journal-ref": null, "doi": "10.1016/j.ejor.2016.08.026", "report-no": null, "categories": "stat.CO math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a penalized method for the least squares estimator of a\nmultivariate concave regression function. This estimator is formulated as a\nquadratic programming (QP) problem with $O(n^2)$ constraints, where n is the\nnumber of observations. Computing such an estimator is a very time-consuming\ntask, and the computational burden rises dramatically as the number of\nobservations increases. By introducing a quadratic penalty function, we\nreformulate the concave least squares estimator as a QP with only\nnon-negativity constraints. This reformulation can be adapted for estimating\nvariants of shape restricted least squares, i.e. the monotonic-concave/convex\nleast squares. The experimental results and an empirical study show that the\nreformulated problem and its dual are solved significantly faster than the\noriginal problem. The Matlab and R codes for implementing the penalized\nproblems are provided in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 07:55:05 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Keshvari", "Abolfazl", ""]]}, {"id": "1608.03859", "submitter": "Jianbo Ye", "authors": "Jianbo Ye and James Z. Wang and Jia Li", "title": "A Simulated Annealing based Inexact Oracle for Wasserstein Loss\n  Minimization", "comments": "accepted by ICML 2017. 13 pages (supplement included), double column,\n  5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization\n(WLM), is an emerging research topic for gaining insights from a large set of\nstructured objects. Despite being conceptually simple, WLM problems are\ncomputationally challenging because they involve minimizing over functions of\nquantities (i.e. Wasserstein distances) that themselves require numerical\nalgorithms to compute. In this paper, we introduce a stochastic approach based\non simulated annealing for solving WLMs. Particularly, we have developed a\nGibbs sampler to approximate effectively and efficiently the partial gradients\nof a sequence of Wasserstein losses. Our new approach has the advantages of\nnumerical stability and readiness for warm starts. These characteristics are\nvaluable for WLM problems that often require multiple levels of iterations in\nwhich the oracle for computing the value and gradient of a loss function is\nembedded. We applied the method to optimal transport with Coulomb cost and the\nWasserstein non-negative matrix factorization problem, and made comparisons\nwith the existing method of entropy regularization.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 17:49:10 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 19:30:55 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 17:24:51 GMT"}, {"version": "v4", "created": "Tue, 6 Jun 2017 16:17:56 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Ye", "Jianbo", ""], ["Wang", "James Z.", ""], ["Li", "Jia", ""]]}, {"id": "1608.04109", "submitter": "Pavlo Mozharovskyi", "authors": "Oleksii Pokotylo, Pavlo Mozharovskyi, Rainer Dyckerhoff", "title": "Depth and depth-based classification with R-package ddalpha", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the seminal idea of Tukey, data depth is a function that measures\nhow close an arbitrary point of the space is located to an implicitly defined\ncenter of a data cloud. Having undergone theoretical and computational\ndevelopments, it is now employed in numerous applications with classification\nbeing the most popular one. The R-package ddalpha is a software directed to\nfuse experience of the applicant with recent achievements in the area of data\ndepth and depth-based classification.\n  ddalpha provides an implementation for exact and approximate computation of\nmost reasonable and widely applied notions of data depth. These can be further\nused in the depth-based multivariate and functional classifiers implemented in\nthe package, where the $DD\\alpha$-procedure is in the main focus. The package\nis expandable with user-defined custom depth methods and separators. The\nimplemented functions for depth visualization and the built-in benchmark\nprocedures may also serve to provide insights into the geometry of the data and\nthe quality of pattern recognition.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 14:44:17 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Pokotylo", "Oleksii", ""], ["Mozharovskyi", "Pavlo", ""], ["Dyckerhoff", "Rainer", ""]]}, {"id": "1608.04123", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters, Mark A. van de Wiel, Wessel N. van Wieringen", "title": "The Spectral Condition Number Plot for Regularization Parameter\n  Determination", "comments": "41 pages, 7 figures, includes supplementary material", "journal-ref": "Computational Statistics, 35(2):629-646, 2020", "doi": "10.1007/s00180-019-00912-z", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern statistical applications ask for the estimation of a covariance\n(or precision) matrix in settings where the number of variables is larger than\nthe number of observations. There exists a broad class of ridge-type estimators\nthat employs regularization to cope with the subsequent singularity of the\nsample covariance matrix. These estimators depend on a penalty parameter and\nchoosing its value can be hard, in terms of being computationally unfeasible or\ntenable only for a restricted set of ridge-type estimators. Here we introduce a\nsimple graphical tool, the spectral condition number plot, for informed\nheuristic penalty parameter selection. The proposed tool is computationally\nfriendly and can be employed for the full class of ridge-type covariance\n(precision) estimators.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 18:50:32 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Peeters", "Carel F. W.", ""], ["van de Wiel", "Mark A.", ""], ["van Wieringen", "Wessel N.", ""]]}, {"id": "1608.04329", "submitter": "Alexander Terenin", "authors": "Alexander Terenin, Shawfeng Dong, David Draper", "title": "GPU-accelerated Gibbs sampling: a case study of the Horseshoe Probit\n  model", "comments": null, "journal-ref": "Statistics and Computing 29(2):301-310, 2019", "doi": "10.1007/s11222-018-9809-3", "report-no": null, "categories": "stat.CO cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gibbs sampling is a widely used Markov chain Monte Carlo (MCMC) method for\nnumerically approximating integrals of interest in Bayesian statistics and\nother mathematical sciences. Many implementations of MCMC methods do not extend\neasily to parallel computing environments, as their inherently sequential\nnature incurs a large synchronization cost. In the case study illustrated by\nthis paper, we show how to do Gibbs sampling in a fully data-parallel manner on\na graphics processing unit, for a large class of exchangeable models that admit\nlatent variable representations. Our approach takes a systems perspective, with\nemphasis placed on efficient use of compute hardware. We demonstrate our method\non a Horseshoe Probit regression model and find that our implementation scales\neffectively to thousands of predictors and millions of data points\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:02:41 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 21:02:46 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 07:04:13 GMT"}, {"version": "v4", "created": "Sat, 29 Jul 2017 16:47:51 GMT"}, {"version": "v5", "created": "Wed, 21 Mar 2018 23:48:33 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Terenin", "Alexander", ""], ["Dong", "Shawfeng", ""], ["Draper", "David", ""]]}, {"id": "1608.04636", "submitter": "Julie Nutini", "authors": "Hamed Karimi, Julie Nutini and Mark Schmidt", "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the\n  Polyak-\\L{}ojasiewicz Condition", "comments": "[v4]: Fixes a constant factor in the PL-->QG proof, which also\n  simplifies the PL-->EB proof and implies that PL implies EB and QB with the\n  same constant", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1963, Polyak proposed a simple condition that is sufficient to show a\nglobal linear convergence rate for gradient descent. This condition is a\nspecial case of the \\L{}ojasiewicz inequality proposed in the same year, and it\ndoes not require strong convexity (or even convexity). In this work, we show\nthat this much-older Polyak-\\L{}ojasiewicz (PL) inequality is actually weaker\nthan the main conditions that have been explored to show linear convergence\nrates without strong convexity over the last 25 years. We also use the PL\ninequality to give new analyses of randomized and greedy coordinate descent\nmethods, sign-based gradient descent methods, and stochastic gradient methods\nin the classic setting (with decreasing or constant step-sizes) as well as the\nvariance-reduced setting. We further propose a generalization that applies to\nproximal-gradient methods for non-smooth optimization, leading to simple proofs\nof linear convergence of these methods. Along the way, we give simple\nconvergence results for a wide variety of problems in machine learning: least\nsquares, logistic regression, boosting, resilient backpropagation,\nL1-regularization, support vector machines, stochastic dual coordinate ascent,\nand stochastic variance-reduced gradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 15:28:24 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 04:08:30 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 22:47:30 GMT"}, {"version": "v4", "created": "Sat, 12 Sep 2020 23:03:52 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Karimi", "Hamed", ""], ["Nutini", "Julie", ""], ["Schmidt", "Mark", ""]]}, {"id": "1608.05173", "submitter": "Xiaolong Zhong", "authors": "Xiaolong Zhong and Malay Ghosh", "title": "Approximate Bayesian Computation via Sufficient Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) has gained popularity in recent years\nowing to its easy implementation, nice interpretation and good performance. Its\nadvantages are more visible when one encounters complex models where maximum\nlikelihood estimation as well as Bayesian analysis via Markov chain Monte Carlo\ndemand prohibitively large amount of time. This paper examines properties of\nABC both from a theoretical as well as from a computational point of view.We\nconsolidate the ABC theory by proving theorems related to its limiting\nbehaviour. In particular, we consider partial posteriors, which serve as the\nfirst step towards approximating the full posteriors. Also, a new\nsemi-automatic algorithm of ABC is proposed using sufficient dimension\nreduction (SDR) method. SDR has primarily surfaced in the frequentist\nliterature. But we have demonstrated in this paper that it has connections with\nABC as well.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 04:46:00 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Zhong", "Xiaolong", ""], ["Ghosh", "Malay", ""]]}, {"id": "1608.05292", "submitter": "Paul Birrell", "authors": "Paul J Birrell, Lorenz Wernisch, Brian D M Tom, Leonhard Held, Gareth\n  O Roberts, Richard G Pebody and Daniela De Angelis", "title": "Efficient real-time monitoring of an emerging influenza epidemic: how\n  feasible?", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prompt public health response to a new epidemic relies on the ability to\nmonitor and predict its evolution in real time as data accumulate. The 2009\nA/H1N1 outbreak in the UK revealed pandemic data as noisy, contaminated,\npotentially biased, and originating from multiple sources. This seriously\nchallenges the capacity for real-time monitoring. Here we assess the\nfeasibility of real-time inference based on such data by constructing an\nanalytic tool combining an age-stratified SEIR transmission model with various\nobservation models describing the data generation mechanisms. As batches of\ndata become available, a sequential Monte Carlo (SMC) algorithm is developed to\nsynthesise multiple imperfect data streams, iterate epidemic inferences and\nassess model adequacy amidst a rapidly evolving epidemic environment,\nsubstantially reducing computation time in comparison to standard MCMC, to\nensure timely delivery of real-time epidemic assessments. In application to\nsimulated data designed to mimic the 2009 A/H1N1 epidemic, SMC is shown to have\nadditional benefits in terms of assessing predictive performance and coping\nwith parameter non-identifiability.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 15:18:12 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 04:16:52 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 13:34:06 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Birrell", "Paul J", ""], ["Wernisch", "Lorenz", ""], ["Tom", "Brian D M", ""], ["Held", "Leonhard", ""], ["Roberts", "Gareth O", ""], ["Pebody", "Richard G", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1608.05338", "submitter": "Qingshan Chen", "authors": "Qingsha Chen, Ju Ming", "title": "The multi-level Monte Carlo method for simulations of turbulent flows", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the application of the multi-level Monte Carlo (MLMC) method on\nnumerical simulations of turbulent flows with uncertain parameters is\ninvestigated. Several strategies for setting up the MLMC method are presented,\nand the advantages and disadvantages of each strategy are also discussed. A\nnumerical experiment is carried out using the Antarctic Circumpolar Current\n(ACC) with uncertain, small-scale bottom topographic features. It is\ndemonstrated that, unlike the pointwise solutions, the averaged volume\ntransports are correlated across grid resolutions, and the MLMC method could\nincrease simulation efficiency without losing accuracy in uncertainty\nassessment.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 17:19:49 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Chen", "Qingsha", ""], ["Ming", "Ju", ""]]}, {"id": "1608.05481", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen, Geoffrey J McLachlan, Jeremy F P Ullmann, Andrew L\n  Janke", "title": "Faster Functional Clustering via Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis (FDA) is an important modern paradigm for handling\ninfinite-dimensional data. An important task in FDA is model-based clustering,\nwhich organizes functional populations into groups via subpopulation\nstructures. The most common approach for model-based clustering of functional\ndata is via mixtures of linear mixed-effects models. The mixture of linear\nmixed-effects models (MLMM) approach requires a computationally intensive\nalgorithm for estimation. We provide a novel Gaussian mixture model (GMM)\ncharacterization of the model-based clustering problem. We demonstrate that\nthis GMM-based characterization allows for improved computational speeds over\nthe MLMM approach when applied via available functions in the R programming\nenvironment. Theoretical considerations for the GMM approach are discussed. An\nexample application to a dataset based upon calcium imaging in the larval\nzebrafish brain is provided as a demonstration of the effectiveness of the\nsimpler GMM approach.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 03:04:51 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 05:39:56 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 02:57:20 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Nguyen", "Hien D", ""], ["McLachlan", "Geoffrey J", ""], ["Ullmann", "Jeremy F P", ""], ["Janke", "Andrew L", ""]]}, {"id": "1608.06143", "submitter": "Abderrahim Halimi", "authors": "Abderrahim Halimi and Aurora Maccarone and Aongus McCarthy and Steve\n  McLaughlin and Gerald S. Buller", "title": "Object Depth Profile and Reflectivity Restoration from Sparse\n  Single-Photon Data Acquired in Underwater Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two new algorithms for the joint restoration of depth and\nreflectivity (DR) images constructed from time-correlated single-photon\ncounting (TCSPC) measurements. Two extreme cases are considered: (i) a reduced\nacquisition time that leads to very low photon counts and (ii) a highly\nattenuating environment (such as a turbid medium) which makes the reflectivity\nestimation more difficult at increasing range. Adopting a Bayesian approach,\nthe Poisson distributed observations are combined with prior distributions\nabout the parameters of interest, to build the joint posterior distribution.\nMore precisely, two Markov random field (MRF) priors enforcing spatial\ncorrelations are assigned to the DR images. Under some justified assumptions,\nthe restoration problem (regularized likelihood) reduces to a convex\nformulation with respect to each of the parameters of interest. This problem is\nfirst solved using an adaptive Markov chain Monte Carlo (MCMC) algorithm that\napproximates the minimum mean square parameter estimators. This algorithm is\nfully automatic since it adjusts the parameters of the MRFs by maximum marginal\nlikelihood estimation. However, the MCMC-based algorithm exhibits a relatively\nlong computational time. The second algorithm deals with this issue and is\nbased on a coordinate descent algorithm. Results on single-photon depth data\nfrom laboratory based underwater measurements demonstrate the benefit of the\nproposed strategy that improves the quality of the estimated DR images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 12:23:40 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Halimi", "Abderrahim", ""], ["Maccarone", "Aurora", ""], ["McCarthy", "Aongus", ""], ["McLaughlin", "Steve", ""], ["Buller", "Gerald S.", ""]]}, {"id": "1608.06189", "submitter": "Wang Shaoxin", "authors": "Shaoxin Wang and Hu Yang and Chaoli Yao", "title": "On the penalized maximum likelihood estimation of high-dimensional\n  approximate factor model", "comments": "27 pages,8 tables, accepted by Computational Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we mainly focus on the penalized maximum likelihood estimation\n(MLE) of the high-dimensional approximate factor model. Since the current\nestimation procedure can not guarantee the positive definiteness of the error\ncovariance matrix, by reformulating the estimation of error covariance matrix\nand based on the lagrangian duality, we propose an accelerated proximal\ngradient (APG) algorithm to give a positive definite estimate of the error\ncovariance matrix. Combined the APG algorithm with EM method, a new estimation\nprocedure is proposed to estimate the high-dimensional approximate factor\nmodel. The new method not only gives positive definite estimate of error\ncovariance matrix but also improves the efficiency of estimation for the\nhigh-dimensional approximate factor model. Although the proposed algorithm can\nnot guarantee a global unique solution, it enjoys a desirable non-increasing\nproperty. The efficiency of the new algorithm on estimation and forecasting is\nalso investigated via simulation and real data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 15:04:04 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 06:00:49 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Wang", "Shaoxin", ""], ["Yang", "Hu", ""], ["Yao", "Chaoli", ""]]}, {"id": "1608.06769", "submitter": "Lam Ho", "authors": "Lam Si Tung Ho and Forrest W. Crawford and Marc A. Suchard", "title": "Direct likelihood-based inference for discretely observed stochastic\n  compartmental models of infectious disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic compartmental models are important tools for understanding the\ncourse of infectious diseases epidemics in populations and in prospective\nevaluation of intervention policies. However, calculating the likelihood for\ndiscretely observed data from even simple models -- such as the ubiquitous\nsusceptible-infectious-removed (SIR) model -- has been considered\ncomputationally intractable, since its formulation almost a century ago.\nRecently researchers have proposed methods to circumvent this limitation\nthrough data augmentation or approximation, but these approaches often suffer\nfrom high computational cost or loss of accuracy. We develop the mathematical\nfoundation and an efficient algorithm to compute the likelihood for discretely\nobserved data from a broad class of stochastic compartmental models. We also\ngive expressions for the derivatives of the transition probabilities using the\nsame technique, making possible inference via Hamiltonian Monte Carlo (HMC). We\nuse the 17th century plague in Eyam, a classic example of the SIR model, to\ncompare our recursion method to sequential Monte Carlo, analyze using HMC, and\nassess the model assumptions. We also apply our direct likelihood evaluation to\nperform Bayesian inference for the 2014-2015 Ebola outbreak in Guinea. The\nresults suggest that the epidemic infectious rates have decreased since October\n2014 in the Southeast region of Guinea, while rates remain the same in other\nregions, facilitating understanding of the outbreak and the effectiveness of\nEbola control interventions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 10:16:58 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 13:10:13 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Ho", "Lam Si Tung", ""], ["Crawford", "Forrest W.", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1608.07029", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Functional time series forecasting with dynamic updating: An application\n  to intraday particulate matter concentration", "comments": "31 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental data often take the form of a collection of curves observed\nsequentially over time. An example of this includes daily pollution measurement\ncurves describing the concentration of a particulate matter in ambient air.\nThese curves can be viewed as a time series of functions observed at equally\nspaced intervals over a dense grid. The nature of high-dimensional data poses\nchallenges from a statistical aspect, due to the so-called `curse of\ndimensionality', but it also poses opportunities to analyze a rich source of\ninformation to better understand dynamic changes at short time intervals.\nStatistical methods are introduced and compared for forecasting one-day-ahead\nintraday concentrations of particulate matter; as new data are sequentially\nobserved, dynamic updating methods are proposed to update point and interval\nforecasts to achieve better accuracy. These forecasting methods are validated\nthrough an empirical study of half-hourly concentrations of airborne\nparticulate matter in Graz, Austria.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 06:52:31 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1608.07048", "submitter": "Tore Selland Kleppe", "authors": "Janne Mannseth, Tore Selland Kleppe, Hans J. Skaug", "title": "On the application of higher order symplectic integrators in Hamiltonian\n  Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the construction of new symplectic numerical integration schemes\nto be used in Hamiltonian Monte Carlo and study their efficiency. Two\nintegration schemes from Blanes et al. (2014), and a new scheme based on\noptimal acceptance probability, are considered as candidates to the commonly\nused leapfrog method. All integration schemes are tested within the framework\nof the No-U-Turn sampler (NUTS), both for a logistic regression model and a\nstudent $t$-model. The results show that the leapfrog method is inferior to all\nthe new methods both in terms of asymptotic expected acceptance probability for\na model problem and the and efficient sample size per computing time for the\nrealistic models.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 08:23:04 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Mannseth", "Janne", ""], ["Kleppe", "Tore Selland", ""], ["Skaug", "Hans J.", ""]]}, {"id": "1608.07207", "submitter": "Shengxin Zhu", "authors": "Shengxin Zhu", "title": "Computing log-likelihood and its derivatives for restricted maximum\n  likelihood methods", "comments": "11", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent large scale genome wide association analysis involves large scale\nlinear mixed models. Quantifying (co)-variance parameters in the mixed models\nwith a restricted maximum likelihood method results in a score function which\nis the first derivative of a log-likelihood. To obtain a statistically\nefficient estimate of the variance parameters, one needs to find the root of\nthe score function via the Newton method. Most elements of the Jacobian matrix\nof the score involve a trace term of four parametric matrix-matrix\nmultiplications. It is computationally prohibitively for large scale data sets.\nBy a serial matrix transforms and an averaged information splitting technique,\nan approximate Jacobian matrix can be obtained by splitting the average of the\nJacobian matrix and its expected value. In the approximated Jacobian, its\nelements only involve Four matrix vector multiplications which can be\nefficiently evaluated by solving sparse linear systems with the multi-frontal\nfactorization method.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 16:20:33 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Zhu", "Shengxin", ""]]}, {"id": "1608.07435", "submitter": "Henri Nurminen M.Sc.", "authors": "Henri Nurminen, Tohid Ardeshiri, Robert Pich\\'e, and Fredrik\n  Gustafsson", "title": "Skew-t Filter and Smoother with Improved Covariance Matrix Approximation", "comments": "14 pages, 15 figures", "journal-ref": "H. Nurminen, T. Ardeshiri, R. Pich\\'e, and F. Gustafsson, \"Skew-t\n  Filter and Smoother with Improved Covariance Matrix Approximation\", IEEE\n  Transactions on Signal Processing, vol. 66, no. 21, pp. 5618-5633, 2018", "doi": "10.1109/TSP.2018.2865434", "report-no": null, "categories": "cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering and smoothing algorithms for linear discrete-time state-space\nmodels with skew-t-distributed measurement noise are proposed. The algorithms\nuse a variational Bayes based posterior approximation with coupled location and\nskewness variables to reduce the error caused by the variational approximation.\nAlthough the variational update is done suboptimally using an expectation\npropagation algorithm, our simulations show that the proposed method gives a\nmore accurate approximation of the posterior covariance matrix than an earlier\nproposed variational algorithm. Consequently, the novel filter and smoother\noutperform the earlier proposed robust filter and smoother and other existing\nlow-complexity alternatives in accuracy and speed. We present both simulations\nand tests based on real-world navigation data, in particular GPS data in an\nurban area, to demonstrate the performance of the novel methods. Moreover, the\nextension of the proposed algorithms to cover the case where the distribution\nof the measurement noise is multivariate skew-$t$ is outlined. Finally, the\npaper presents a study of theoretical performance bounds for the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 12:27:39 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 18:23:43 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Nurminen", "Henri", ""], ["Ardeshiri", "Tohid", ""], ["Pich\u00e9", "Robert", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1608.07630", "submitter": "Daniel Hsu", "authors": "Ji Xu, Daniel Hsu, Arian Maleki", "title": "Global analysis of Expectation Maximization for mixtures of two\n  Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation Maximization (EM) is among the most popular algorithms for\nestimating parameters of statistical models. However, EM, which is an iterative\nalgorithm based on the maximum likelihood principle, is generally only\nguaranteed to find stationary points of the likelihood objective, and these\npoints may be far from any maximizer. This article addresses this disconnect\nbetween the statistical principles behind EM and its algorithmic properties.\nSpecifically, it provides a global analysis of EM for specific models in which\nthe observations comprise an i.i.d. sample from a mixture of two Gaussians.\nThis is achieved by (i) studying the sequence of parameters from idealized\nexecution of EM in the infinite sample limit, and fully characterizing the\nlimit points of the sequence in terms of the initial parameters; and then (ii)\nbased on this convergence analysis, establishing statistical consistency (or\nlack thereof) for the actual sequence of parameters produced by EM.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 23:53:43 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Xu", "Ji", ""], ["Hsu", "Daniel", ""], ["Maleki", "Arian", ""]]}, {"id": "1608.08056", "submitter": "Matteo Ruggiero", "authors": "Antonio Canale and Matteo Ruggiero", "title": "Bayesian nonparametric forecasting of monotonic functional time series", "comments": "To appear on the Electronic Journal of Statistics", "journal-ref": "Electron. J. Statist. Volume 10, Number 2 (2016), 3265-3286", "doi": "10.1214/16-EJS1190", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric approach to modelling and predicting a\nclass of functional time series with application to energy markets, based on\nfully observed, noise-free functional data. Traders in such contexts conceive\nprofitable strategies if they can anticipate the impact of their bidding\nactions on the aggregate demand and supply curves, which in turn need to be\npredicted reliably. Here we propose a simple Bayesian nonparametric method for\npredicting such curves, which take the form of monotonic bounded step\nfunctions. We borrow ideas from population genetics by defining a class of\ninteracting particle systems to model the functional trajectory, and develop an\nimplementation strategy which uses ideas from Markov chain Monte Carlo and\napproximate Bayesian computation techniques and allows to circumvent the\nintractability of the likelihood. Our approach shows great adaptation to the\ndegree of smoothness of the curves and the volatility of the functional series,\nproves to be robust to an increase of the forecast horizon and yields an\nuncertainty quantification for the functional forecasts. We illustrate the\nmodel and discuss its performance with simulated datasets and on real data\nrelative to the Italian natural gas market.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 14:08:54 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Canale", "Antonio", ""], ["Ruggiero", "Matteo", ""]]}, {"id": "1608.08126", "submitter": "Esa Ollila", "authors": "Esa Ollila, Ilya Soloveychik, David E. Tyler and Ami Wiesel", "title": "Simultaneous penalized M-estimation of covariance matrices using\n  geodesically convex optimization", "comments": "Submitted to Journal of Multivariate Analysis (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common assumption when sampling $p$-dimensional observations from $K$\ndistinct group is the equality of the covariance matrices. In this paper, we\npropose two penalized $M$-estimation approaches for the estimation of the\ncovariance or scatter matrices under the broader assumption that they may\nsimply be close to each other, and hence roughly deviate from some positive\ndefinite \"center\". The first approach begins by generating a pooled\n$M$-estimator of scatter based on all the data, followed by a penalised\n$M$-estimator of scatter for each group, with the penalty term chosen so that\nthe individual scatter matrices are shrunk towards the pooled scatter matrix.\nIn the second approach, we minimize the sum of the individual group\n$M$-estimation cost functions together with an additive joint penalty term\nwhich enforces some similarity between the individual scatter estimators, i.e.\nshrinkage towards a mutual center. In both approaches, we utilize the concept\nof geodesic convexity to prove the existence and uniqueness of the penalized\nsolution under general conditions. We consider three specific penalty functions\nbased on the Euclidean, the Riemannian, and the Kullback-Leibler distances. In\nthe second approach, the distance based penalties are shown to lead to\nestimators of the mutual center that are related to the arithmetic, the\nRiemannian and the harmonic means of positive definite matrices, respectively.\nA penalty based on an ellipticity measure is also considered which is\nparticularly useful for shape matrix estimators. Fixed point equations are\nderived for each penalty function and the benefits of the estimators are\nillustrated in regularized discriminant analysis problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 16:11:08 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Ollila", "Esa", ""], ["Soloveychik", "Ilya", ""], ["Tyler", "David E.", ""], ["Wiesel", "Ami", ""]]}, {"id": "1608.08291", "submitter": "Mike Ludkovski", "authors": "Mike Ludkovski, Jimmy Risk, Howard Zail", "title": "Gaussian Process Models for Mortality Rates and Improvement Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Gaussian process (\"GP\") framework for modeling mortality rates\nand mortality improvement factors. GP regression is a nonparametric,\ndata-driven approach for determining the spatial dependence in mortality rates\nand jointly smoothing raw rates across dimensions, such as calendar year and\nage. The GP model quantifies uncertainty associated with smoothed historical\nexperience and generates full stochastic trajectories for out-of-sample\nforecasts. Our framework is well suited for updating projections when newly\navailable data arrives, and for dealing with \"edge\" issues where credibility is\nlower. We present a detailed analysis of Gaussian process model performance for\nUS mortality experience based on the CDC datasets. We investigate the\ninteraction between mean and residual modeling, Bayesian and non-Bayesian GP\nmethodologies, accuracy of in-sample and out-of-sample forecasting, and\nstability of model parameters. We also document the general decline, along with\nstrong age-dependency, in mortality improvement factors over the past few\nyears, contrasting our findings with the Society of Actuaries (\"SOA\") MP-2014\nand -2015 models that do not fully reflect these recent trends.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 00:55:11 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 06:55:35 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 22:48:30 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ludkovski", "Mike", ""], ["Risk", "Jimmy", ""], ["Zail", "Howard", ""]]}, {"id": "1608.08468", "submitter": "Gregor Kastner", "authors": "Gregor Kastner", "title": "Sparse Bayesian time-varying covariance estimation in many dimensions", "comments": null, "journal-ref": "Journal of Econometrics 210(1), 98-115 (2019)", "doi": "10.1016/j.jeconom.2018.11.007", "report-no": null, "categories": "stat.ME econ.EM q-fin.PM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the curse of dimensionality in dynamic covariance estimation by\nmodeling the underlying co-volatility dynamics of a time series vector through\nlatent time-varying stochastic factors. The use of a global-local shrinkage\nprior for the elements of the factor loadings matrix pulls loadings on\nsuperfluous factors towards zero. To demonstrate the merits of the proposed\nframework, the model is applied to simulated data as well as to daily\nlog-returns of 300 S&P 500 members. Our approach yields precise correlation\nestimates, strong implied minimum variance portfolio performance and superior\nforecasting accuracy in terms of log predictive scores when compared to typical\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 14:16:54 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 13:43:38 GMT"}, {"version": "v3", "created": "Sat, 11 Nov 2017 11:28:19 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Kastner", "Gregor", ""]]}, {"id": "1608.08666", "submitter": "Rui Vieira", "authors": "Rui Vieira, Darren J. Wilkinson", "title": "Online state and parameter estimation in Dynamic Generalised Linear\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for streaming time-series is tightly coupled with the problem of\nBayesian on-line state and parameter inference. In this paper we will introduce\nDynamic Generalised Linear Models, the class of models often chosen to model\ncontinuous and discrete time-series data. We will look at three different\napproaches which allow on-line estimation and analyse the results when applied\nto different real world datasets related to inference for streaming data.\nSufficient statistics based methods delay known problems, such as particle\nimpoverishment, especially when applied to long running time-series, while\nproviding reasonable parameter estimations when compared to exact methods, such\nas Particle Marginal Metropolis-Hastings. State and observation forecasts will\nalso be analysed as a performance metric. By benchmarking against a \"gold\nstandard\" (off-line) method, we can better understand the performance of\non-line methods in challenging real-world scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 22:00:19 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Vieira", "Rui", ""], ["Wilkinson", "Darren J.", ""]]}, {"id": "1608.08814", "submitter": "Daniel  Sanz-Alonso", "authors": "Daniel Sanz-Alonso", "title": "Importance Sampling and Necessary Sample Size: an Information Theory\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling approximates expectations with respect to a target\nmeasure by using samples from a proposal measure. The performance of the method\nover large classes of test functions depends heavily on the closeness between\nboth measures. We derive a general bound that needs to hold for importance\nsampling to be successful, and relates the $f$-divergence between the target\nand the proposal to the sample size. The bound is deduced from a new and simple\ninformation theory paradigm for the study of importance sampling. As examples\nof the general theory we give necessary conditions on the sample size in terms\nof the Kullback-Leibler and $\\chi^2$ divergences, and the total variation and\nHellinger distances. Our approach is non-asymptotic, and its generality allows\nto tell apart the relative merits of these metrics. Unsurprisingly, the\nnon-symmetric divergences give sharper bounds than total variation or\nHellinger. Our results extend existing necessary conditions -and complement\nsufficient ones- on the sample size required for importance sampling.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 12:06:27 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Sanz-Alonso", "Daniel", ""]]}]