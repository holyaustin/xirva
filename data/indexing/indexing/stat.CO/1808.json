[{"id": "1808.00212", "submitter": "Daniel W. Heck", "authors": "Daniel W. Heck, Morten Moshagen, Edgar Erdfelder", "title": "Model selection by minimum description length: Lower-bound sample sizes\n  for the Fisher information approximation", "comments": null, "journal-ref": "Journal of Mathematical Psychology (2014) 60, 29-34", "doi": "10.1016/j.jmp.2014.06.002", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher information approximation (FIA) is an implementation of the\nminimum description length principle for model selection. Unlike information\ncriteria such as AIC or BIC, it has the advantage of taking the functional form\nof a model into account. Unfortunately, FIA can be misleading in finite\nsamples, resulting in an inversion of the correct rank order of complexity\nterms for competing models in the worst case. As a remedy, we propose a\nlower-bound $N'$ for the sample size that suffices to preclude such errors. We\nillustrate the approach using three examples from the family of multinomial\nprocessing tree models.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 08:00:33 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Heck", "Daniel W.", ""], ["Moshagen", "Morten", ""], ["Erdfelder", "Edgar", ""]]}, {"id": "1808.00685", "submitter": "Thomas Bocklitz", "authors": "Robert Geitner and Robby Fritzsch and J\\\"urgen Popp and Thomas W.\n  Bocklitz", "title": "corr2D - Implementation of Two-Dimensional Correlation Analysis in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the package corr2D two-dimensional correlation analysis is implemented in\nR. This paper describes how two-dimensional correlation analysis is done in the\npackage and how the mathematical equations are translated into R code. The\npaper features a simple tutorial with executable code for beginners, insight\ninto at the calculations done before the correlation analysis, a detailed look\nat the parallelization of the fast Fourier transformation based correlation\nanalysis and a speed test of the calculation. The package corr2D offers the\npossibility to preprocess, correlate and postprocess spectroscopic data using\nexclusively the R language. Thus, corr2D is a welcome addition to the toolbox\nof spectroscopists and makes two-dimensional correlation analysis more\naccessible and transparent.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 06:36:51 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Geitner", "Robert", ""], ["Fritzsch", "Robby", ""], ["Popp", "J\u00fcrgen", ""], ["Bocklitz", "Thomas W.", ""]]}, {"id": "1808.01052", "submitter": "Brandon Jones", "authors": "Brandon A. Jones and Marc Balducci", "title": "Stochastic Expansions Including Random Inputs on the Unit Circle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic expansion-based methods of uncertainty quantification, such as\npolynomial chaos and separated representations, require basis functions\northogonal with respect to the density of random inputs. Many modern\nengineering problems employ stochastic circular quantities, which are defined\non the unit circle in the complex plane and characterized by probability\ndensity functions on this periodic domain. Hence, stochastic expansions with\ncircular data require corresponding orthogonal polynomials on the unit circle\nto allow for their use in uncertainty quantification. Rogers-Szego polynomials\nenable uncertainty quantification for random inputs described by the wrapped\nnormal density. For the general case, this paper presents a framework for\nnumerically generating orthogonal polynomials as a function of the\ndistribution's characteristic function and demonstrates their use with the von\nMises density. The resulting stochastic expansions allow for estimating\nstatistics describing the posterior density using the expansion coefficients.\nResults demonstrate the exponential convergence of these stochastic expansions\nand apply the proposed methods to propagating orbit-state uncertainty with\nequinoctial elements. The astrodynamics application of the theory improves\nrobustness and accuracy when compared to approximating angular quantities as\nvariables on the real line.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 00:33:07 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Jones", "Brandon A.", ""], ["Balducci", "Marc", ""]]}, {"id": "1808.01126", "submitter": "Gilles Kratzer", "authors": "Gilles Kratzer and Reinhard Furrer", "title": "Information-Theoretic Scoring Rules to Learn Additive Bayesian Network\n  Applied to Epidemiology", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network modelling is a well adapted approach to study messy and\nhighly correlated datasets which are very common in, e.g., systems\nepidemiology. A popular approach to learn a Bayesian network from an\nobservational datasets is to identify the maximum a posteriori network in a\nsearch-and-score approach. Many scores have been proposed both Bayesian or\nfrequentist based. In an applied perspective, a suitable approach would allow\nmultiple distributions for the data and is robust enough to run autonomously. A\npromising framework to compute scores are generalized linear models. Indeed,\nthere exists fast algorithms for estimation and many tailored solutions to\ncommon epidemiological issues. The purpose of this paper is to present an R\npackage abn that has an implementation of multiple frequentist scores and some\nrealistic simulations that show its usability and performance. It includes\nfeatures to deal efficiently with data separation and adjustment which are very\ncommon in systems epidemiology.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 09:22:46 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Kratzer", "Gilles", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1808.01770", "submitter": "Vivien Goepp", "authors": "Vivien Goepp (MAP5 - UMR 8145), Olivier Bouaziz (MAP5 - UMR 8145),\n  Gr\\'egory Nuel (LPSM UMR 8001)", "title": "Spline Regression with Automatic Knot Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new method for automatically selecting knots in\nspline regression. The approach consists in setting a large number of initial\nknots and fitting the spline regression through a penalized likelihood\nprocedure called adaptive ridge. The proposed method is similar to penalized\nspline regression methods (e.g. P-splines), with the noticeable difference that\nthe output is a sparse spline regression with a small number of knots. We show\nthat our method called A-spline, for adaptive splines yields sparse regression\nmodels with high interpretability, while having similar predictive performance\nsimilar to penalized spline regression methods. A-spline is applied both to\nsimulated and real dataset. A fast and publicly available implementation in R\nis provided along with this paper.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 08:33:57 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Goepp", "Vivien", "", "MAP5 - UMR 8145"], ["Bouaziz", "Olivier", "", "MAP5 - UMR 8145"], ["Nuel", "Gr\u00e9gory", "", "LPSM UMR 8001"]]}, {"id": "1808.01932", "submitter": "Mathieu Carmassi", "authors": "Mathieu Carmassi, Pierre Barbillon, Matthieu Chiodetti, Merlin Keller,\n  Eric Parent", "title": "CaliCo: a R package for Bayesian calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a recently released R package for Bayesian\ncalibration. Many industrial fields are facing unfeasible or costly field\nexperiments. These experiments are replaced with numerical/computer experiments\nwhich are realized by running a numerical code. Bayesian calibration intends to\nestimate, through a posterior distribution, input parameters of the code in\norder to make the code outputs close to the available experimental data. The\ncode can be time consuming while the Bayesian calibration implies a lot of code\ncalls which makes studies too burdensome. A discrepancy might also appear\nbetween the numerical code and the physical system when facing incompatibility\nbetween experimental data and numerical code outputs. The package CaliCo deals\nwith these issues through four statistical models which deal with a time\nconsuming code or not and with discrepancy or not. A guideline for users is\nprovided in order to illustrate the main functions and their arguments.\nEventually, a toy example is detailed using CaliCo. This example (based on a\nreal physical system) is in five dimensions and uses simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 07:11:39 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 08:05:13 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Carmassi", "Mathieu", ""], ["Barbillon", "Pierre", ""], ["Chiodetti", "Matthieu", ""], ["Keller", "Merlin", ""], ["Parent", "Eric", ""]]}, {"id": "1808.02083", "submitter": "Victor Minden", "authors": "Andrea Giovannucci and Victor Minden and Cengiz Pehlevan and Dmitri B.\n  Chklovskii", "title": "Efficient Principal Subspace Projection of Streaming Data Through Fast\n  Similarity Matching", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data problems frequently require processing datasets in a streaming\nfashion, either because all data are available at once but collectively are\nlarger than available memory or because the data intrinsically arrive one data\npoint at a time and must be processed online. Here, we introduce a\ncomputationally efficient version of similarity matching, a framework for\nonline dimensionality reduction that incrementally estimates the top\nK-dimensional principal subspace of streamed data while keeping in memory only\nthe last sample and the current iterate. To assess the performance of our\napproach, we construct and make public a test suite containing both a synthetic\ndata generator and the infrastructure to test online dimensionality reduction\nalgorithms on real datasets, as well as performant implementations of our\nalgorithm and competing algorithms with similar aims. Among the algorithms\nconsidered we find our approach to be competitive, performing among the best on\nboth synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 19:37:56 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Giovannucci", "Andrea", ""], ["Minden", "Victor", ""], ["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1808.02387", "submitter": "Jessica Young PhD", "authors": "Qoua L. Her, Yury Vilk, Jessica Young, Zilu Zhang, Jessica M.\n  Malenfant, Sarah Malek, Sengwee Toh", "title": "A distributed regression analysis application based on SAS software.\n  Part I: Linear and logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has demonstrated the feasibility and value of conducting\ndistributed regression analysis (DRA), a privacy-protecting analytic method\nthat performs multivariable-adjusted regression analysis with only\nsummary-level information from participating sites. To our knowledge, there are\nno DRA applications in SAS, the statistical software used by several large\nnational distributed data networks (DDNs), including the Sentinel System and\nPCORnet. SAS/IML is available to perform the required matrix computations for\nDRA in the SAS system. However, not all data partners in these large DDNs have\naccess to SAS/IML, which is licensed separately. In this first article of a\ntwo-paper series, we describe a DRA application developed for use in Base SAS\nand SAS/STAT modules for linear and logistic DRA within horizontally\npartitioned DDNs and its successful tests.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 14:12:46 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Her", "Qoua L.", ""], ["Vilk", "Yury", ""], ["Young", "Jessica", ""], ["Zhang", "Zilu", ""], ["Malenfant", "Jessica M.", ""], ["Malek", "Sarah", ""], ["Toh", "Sengwee", ""]]}, {"id": "1808.02392", "submitter": "Jessica Young PhD", "authors": "Yury Vilk, Zilu Zhang, Jessica Young, Qoua L. Her, Jessica M.\n  Malenfant, Sarah Malek, Sengwee Toh", "title": "A distributed regression analysis application based on SAS software Part\n  II: Cox proportional hazards regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has demonstrated the feasibility and value of conducting\ndistributed regression analysis (DRA), a privacy-protecting analytic method\nthat performs multivariable-adjusted regression analysis with only\nsummary-level information from participating sites. To our knowledge, there are\nno DRA applications in SAS, the statistical software used by several large\nnational distributed data networks (DDNs), including the Sentinel System and\nPCORnet. SAS/IML is available to perform the required matrix computations for\nDRA in the SAS system. However, not all data partners in these large DDNs have\naccess to SAS/IML, which is licensed separately. In this second article of a\ntwo-paper series, we describe a DRA application developed using Base SAS and\nSAS/STAT modules for distributed Cox proportional hazards regression within\nhorizontally partitioned DDNs and its successful tests.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 14:17:55 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Vilk", "Yury", ""], ["Zhang", "Zilu", ""], ["Young", "Jessica", ""], ["Her", "Qoua L.", ""], ["Malenfant", "Jessica M.", ""], ["Malek", "Sarah", ""], ["Toh", "Sengwee", ""]]}, {"id": "1808.02932", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga and Chris H. Wiggins", "title": "Nonparametric Gaussian Mixture Models for the Multi-Armed Contextual\n  Bandit", "comments": "The software used for this study is publicly available at\n  https://github.com/iurteaga/bandits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here adopt Bayesian nonparametric mixture models to extend multi-armed\nbandits in general, and Thompson sampling in particular, to scenarios where\nthere is reward model uncertainty. In the stochastic multi-armed bandit, where\nan agent must learn a policy that maximizes long term payoff, the reward for\nthe selected action is generated from an unknown distribution. Thompson\nsampling is a generative and interpretable multi-armed bandit algorithm that\nhas been shown both to perform well in practice, and to enjoy optimality\nproperties for certain reward functions. Nevertheless, Thompson sampling\nrequires knowledge of the true reward model, for calculation of expected\nrewards and sampling from its parameter posterior. In this work, we extend\nThompson sampling to complex scenarios where there is model uncertainty, by\nadopting a very flexible set of reward distributions: Bayesian nonparametric\nGaussian mixture models. The generative process of Bayesian nonparametric\nmixtures naturally aligns with the Bayesian modeling of multi-armed bandits:\nthe nonparametric model autonomously determines its complexity as new rewards\nare observed for the played arms. By characterizing each arm's reward\ndistribution with independent nonparametric mixture models, the proposed method\nsequentially learns the model that best approximates the true underlying reward\ndistribution, achieving successful performance in complex -- not in the\nexponential family -- bandits. Our contribution is valuable for practical\nscenarios, as it avoids stringent case-by-case model specifications and\nhyperparameter tuning, yet attains reduced regret in diverse bandit settings.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 20:40:15 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 18:13:54 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 22:02:51 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1808.02933", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga and Chris H. Wiggins", "title": "(Sequential) Importance Sampling Bandits", "comments": "The software used for this study is publicly available at\n  https://github.com/iurteaga/bandits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work extends existing multi-armed bandit (MAB) algorithms beyond their\noriginal settings by leveraging advances in sequential Monte Carlo (SMC)\nmethods from the approximate inference community. We leverage Monte Carlo\nestimation and, in particular, the flexibility of (sequential) importance\nsampling to allow for accurate estimation of the statistics of interest within\nthe MAB problem. The MAB is a sequential allocation task where the goal is to\nlearn a policy that maximizes long term payoff, where only the reward of the\nexecuted action is observed; i.e., sequential optimal decisions are made, while\nsimultaneously learning how the world operates. In the stochastic setting, the\nreward for each action is generated from an unknown distribution. To decide the\nnext optimal action to take, one must compute sufficient statistics of this\nunknown reward distribution, e.g., upper-confidence bounds (UCB), or\nexpectations in Thompson sampling. Closed-form expressions for these statistics\nof interest are analytically intractable except for simple cases. By combining\nSMC methods --- which estimate posterior densities and expectations in\nprobabilistic models that are analytically intractable --- with Bayesian\nstate-of-the-art MAB algorithms, we extend their applicability to complex\nmodels: those for which sampling may be performed even if analytic computation\nof summary statistics is infeasible --- nonlinear reward functions and dynamic\nbandits. We combine SMC both for Thompson sampling and upper confident\nbound-based (Bayes-UCB) policies, and study different bandit models: classic\nBernoulli and Gaussian distributed cases, as well as dynamic and context\ndependent linear-Gaussian, logistic and categorical-softmax rewards.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 20:40:42 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 18:52:44 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 20:31:32 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1808.02950", "submitter": "Renato J Cintra", "authors": "R. S. Oliveira, R. J. Cintra, F. M. Bayer, T. L. T. da Silveira, A.\n  Madanayake, A. Leite", "title": "Low-complexity 8-point DCT Approximation Based on Angle Similarity for\n  Image and Video Coding", "comments": "16 pages, 12 figures, 10 tables", "journal-ref": "Multidimensional Systems and Signal Processing, 1-32, 2018", "doi": "10.1007/s11045-018-0601-5", "report-no": null, "categories": "eess.IV cs.MM eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principal component analysis (PCA) is widely used for data decorrelation\nand dimensionality reduction. However, the use of PCA may be impractical in\nreal-time applications, or in situations were energy and computing constraints\nare severe. In this context, the discrete cosine transform (DCT) becomes a\nlow-cost alternative to data decorrelation. This paper presents a method to\nderive computationally efficient approximations to the DCT. The proposed method\naims at the minimization of the angle between the rows of the exact DCT matrix\nand the rows of the approximated transformation matrix. The resulting\ntransformations matrices are orthogonal and have extremely low arithmetic\ncomplexity. Considering popular performance measures, one of the proposed\ntransformation matrices outperforms the best competitors in both matrix error\nand coding capabilities. Practical applications in image and video coding\ndemonstrate the relevance of the proposed transformation. In fact, we show that\nthe proposed approximate DCT can outperform the exact DCT for image encoding\nunder certain compression ratios. The proposed transform and its direct\ncompetitors are also physically realized as digital prototype circuits using\nFPGA technology.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 21:56:30 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Oliveira", "R. S.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["da Silveira", "T. L. T.", ""], ["Madanayake", "A.", ""], ["Leite", "A.", ""]]}, {"id": "1808.03076", "submitter": "Matthias Troffaes", "authors": "Nawapon Nakharutai, Matthias C. M. Troffaes, Camila C. S. Caiado", "title": "Improved linear programming methods for checking avoiding sure loss", "comments": "23 pages, 6 figures", "journal-ref": "International Journal of Approximate Reasoning 101 (2018) 293-310", "doi": "10.1016/j.ijar.2018.07.013", "report-no": null, "categories": "math.OC math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the simplex method and two interior-point methods (the affine\nscaling and the primal-dual) for solving linear programming problems for\nchecking avoiding sure loss, and propose novel improvements. We exploit the\nstructure of these problems to reduce their size. We also present an extra\nstopping criterion, and direct ways to calculate feasible starting points in\nalmost all cases. For benchmarking, we present algorithms for generating random\nsets of desirable gambles that either avoid or do not avoid sure loss. We test\nour improvements on these linear programming methods by measuring the\ncomputational time on these generated sets. We assess the relative performance\nof the three methods as a function of the number of desirable gambles and the\nnumber of outcomes. Overall, the affine scaling and primal-dual methods benefit\nfrom the improvements, and they both outperform the simplex method in most\nscenarios. We conclude that the simplex method is not a good choice for\nchecking avoiding sure loss. If problems are small, then there is no tangible\ndifference in performance between all methods. For large problems, our improved\nprimal-dual method performs at least three times faster than any of the other\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 10:37:01 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Nakharutai", "Nawapon", ""], ["Troffaes", "Matthias C. M.", ""], ["Caiado", "Camila C. S.", ""]]}, {"id": "1808.03215", "submitter": "Christopher Geoga", "authors": "Christopher J. Geoga, Mihai Anitescu, Michael L. Stein", "title": "Scalable Gaussian Process Computations Using Hierarchical Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a kernel-independent method that applies hierarchical matrices to\nthe problem of maximum likelihood estimation for Gaussian processes. The\nproposed approximation provides natural and scalable stochastic estimators for\nits gradient and Hessian, as well as the expected Fisher information matrix,\nthat are computable in quasilinear $O(n \\log^2 n)$ complexity for a large range\nof models. To accomplish this, we (i) choose a specific hierarchical\napproximation for covariance matrices that enables the computation of their\nexact derivatives and (ii) use a stabilized form of the Hutchinson stochastic\ntrace estimator. Since both the observed and expected information matrices can\nbe computed in quasilinear complexity, covariance matrices for MLEs can also be\nestimated efficiently. After discussing the associated mathematics, we\ndemonstrate the scalability of the method, discuss details of its\nimplementation, and validate that the resulting MLEs and confidence intervals\nbased on the inverse Fisher information matrix faithfully approach those\nobtained by the exact likelihood.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 16:04:25 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 19:50:06 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Geoga", "Christopher J.", ""], ["Anitescu", "Mihai", ""], ["Stein", "Michael L.", ""]]}, {"id": "1808.03216", "submitter": "Bruno Sudret", "authors": "E. Torre, S. Marelli, P. Embrechts, B. Sudret", "title": "Data-driven polynomial chaos expansion for machine learning regression", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.03.039", "report-no": "RSUQ-2018-005B", "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a regression technique for data-driven problems based on\npolynomial chaos expansion (PCE). PCE is a popular technique in the field of\nuncertainty quantification (UQ), where it is typically used to replace a\nrunnable but expensive computational model subject to random inputs with an\ninexpensive-to-evaluate polynomial function. The metamodel obtained enables a\nreliable estimation of the statistics of the output, provided that a suitable\nprobabilistic model of the input is available. Machine learning (ML) regression\nis a research field that focuses on providing purely data-driven input-output\nmaps, with the focus on pointwise prediction accuracy. We show that a PCE\nmetamodel purely trained on data can yield pointwise predictions whose accuracy\nis comparable to that of other ML regression models, such as neural networks\nand support vector machines. The comparisons are performed on benchmark\ndatasets available from the literature. The methodology also enables the\nquantification of the output uncertainties, and is robust to noise.\nFurthermore, it enjoys additional desirable properties, such as good\nperformance for small training sets and simplicity of construction, with only\nlittle parameter tuning required.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 16:11:31 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 15:22:47 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Torre", "E.", ""], ["Marelli", "S.", ""], ["Embrechts", "P.", ""], ["Sudret", "B.", ""]]}, {"id": "1808.03230", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi, Natesh S. Pillai, and Aaron Smith", "title": "Does Hamiltonian Monte Carlo mix faster than a random walk on multimodal\n  densities?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of\nMarkov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity\nof HMC algorithms is their excellent performance as the dimension $d$ of the\ntarget becomes large: under conditions that are satisfied for many common\nstatistical models, optimally-tuned HMC algorithms have a running time that\nscales like $d^{0.25}$. In stark contrast, the running time of the usual\nRandom-Walk Metropolis (RWM) algorithm, optimally tuned, scales like $d$. This\nsuperior scaling of the HMC algorithm with dimension is attributed to the fact\nthat it, unlike RWM, incorporates the gradient information in the proposal\ndistribution. In this paper, we investigate a different scaling question: does\nHMC beat RWM for highly $\\textit{multimodal}$ targets? We find that the answer\nis often $\\textit{no}$. We compute the spectral gaps for both the algorithms\nfor a specific class of multimodal target densities, and show that they are\nidentical. The key reason is that, within one mode, the gradient is effectively\nignorant about other modes, thus negating the advantage the HMC algorithm\nenjoys in unimodal targets. We also give heuristic arguments suggesting that\nthe above observation may hold quite generally. Our main tool for answering\nthis question is a novel simple formula for the conductance of HMC using\nLiouville's theorem. This result allows us to compute the spectral gap of HMC\nalgorithms, for both the classical HMC with isotropic momentum and the recent\nRiemannian HMC, for multimodal targets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 16:46:51 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 16:20:07 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Mangoubi", "Oren", ""], ["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1808.03239", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi, Natesh S. Pillai, and Aaron Smith", "title": "Simple Conditions for Metastability of Continuous Markov Chains", "comments": "arXiv admin note: text overlap with arXiv:1808.03230", "journal-ref": "J. Appl. Probab. 58 (2021) 83-105", "doi": "10.1017/jpr.2020.83", "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family $\\{Q_{\\beta}\\}_{\\beta \\geq 0}$ of Markov chains is said to exhibit\n$\\textit{metastable mixing}$ with $\\textit{modes}$\n$S_{\\beta}^{(1)},\\ldots,S_{\\beta}^{(k)}$ if its spectral gap (or some other\nmixing property) is very close to the worst conductance\n$\\min(\\Phi_{\\beta}(S_{\\beta}^{(1)}), \\ldots, \\Phi_{\\beta}(S_{\\beta}^{(k)}))$ of\nits modes. We give simple sufficient conditions for a family of Markov chains\nto exhibit metastability in this sense, and verify that these conditions hold\nfor a prototypical Metropolis-Hastings chain targeting a mixture distribution.\nOur work differs from existing work on metastability in that, for the class of\nexamples we are interested in, it gives an asymptotically exact formula for the\nspectral gap (rather than a bound that can be very far from sharp) while at the\nsame time giving technical conditions that are easier to verify for many\nstatistical examples. Our bounds from this paper are used in a companion paper\nto compare the mixing times of the Hamiltonian Monte Carlo algorithm and a\nrandom walk algorithm for multimodal target distributions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 17:15:48 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 16:28:11 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Mangoubi", "Oren", ""], ["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1808.03916", "submitter": "Jiahao Chen", "authors": "Jiahao Chen", "title": "Linguistic Relativity and Programming Languages", "comments": "10 pages, repo at\n  https://github.com/jiahao/statistical-computing-linguistics, Published in\n  Proceedings of the 2016 Joint Statistical Meetings, Chicago, IL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of programming languages can wax and wane across the decades. We\nexamine the split-apply- combine pattern that is common in statistical\ncomputing, and consider how its invocation or implementation in languages like\nMATLAB and APL differ from R/dplyr. The differences in spelling illustrate how\nthe concept of linguistic relativity applies to programming languages in ways\nthat are analogous to human languages. Finally, we discuss how Julia, by being\na high performance yet general purpose dynamic language, allows its users to\nexpress different abstractions to suit individual preferences.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 09:38:34 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Chen", "Jiahao", ""]]}, {"id": "1808.03947", "submitter": "Timo Bechger", "authors": "Timo Bechger, Gunter Maris, Maarten Marsman", "title": "An Asymptotically Efficient Metropolis-Hastings Sampler for Bayesian\n  Inference in Large-Scale Educational Measuremen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper discusses a Metropolis-Hastings algorithm developed by\n\\citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is\nproven that the algorithm becomes more efficient with more data and meets the\ngrowing demands of large scale educational measurement.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 14:08:44 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Bechger", "Timo", ""], ["Maris", "Gunter", ""], ["Marsman", "Maarten", ""]]}, {"id": "1808.04299", "submitter": "George Deligiannidis", "authors": "George Deligiannidis, Daniel Paulin, Alexandre Bouchard-C\\^ot\\'e and\n  Arnaud Doucet", "title": "Randomized Hamiltonian Monte Carlo as Scaling Limit of the Bouncy\n  Particle Sampler and Dimension-Free Convergence Rates", "comments": "55 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bouncy Particle Sampler is a Markov chain Monte Carlo method based on a\nnonreversible piecewise deterministic Markov process. In this scheme, a\nparticle explores the state space of interest by evolving according to a linear\ndynamics which is altered by bouncing on the hyperplane tangent to the gradient\nof the negative log-target density at the arrival times of an inhomogeneous\nPoisson Process (PP) and by randomly perturbing its velocity at the arrival\ntimes of an homogeneous PP. Under regularity conditions, we show here that the\nprocess corresponding to the first component of the particle and its\ncorresponding velocity converges weakly towards a Randomized Hamiltonian Monte\nCarlo (RHMC) process as the dimension of the ambient space goes to infinity.\nRHMC is another piecewise deterministic non-reversible Markov process where a\nHamiltonian dynamics is altered at the arrival times of a homogeneous PP by\nrandomly perturbing the momentum component. We then establish dimension-free\nconvergence rates for RHMC for strongly log-concave targets with bounded\nHessians using coupling ideas and hypocoercivity techniques.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:48:15 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 15:28:14 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 08:11:02 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 09:29:37 GMT"}, {"version": "v5", "created": "Wed, 23 Dec 2020 10:59:50 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Deligiannidis", "George", ""], ["Paulin", "Daniel", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1808.04739", "submitter": "Andee Kaplan", "authors": "Andee Kaplan and Mark S. Kaiser and Soumendra N. Lahiri and Daniel J.\n  Nordman", "title": "Simulating Markov random fields with a conclique-based Gibbs sampler", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": "10.1080/10618600.2019.1668800", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For spatial and network data, we consider models formed from a Markov random\nfield (MRF) structure and the specification of a conditional distribution for\neach observation. Fast simulation from such MRF models is often an important\nconsideration, particularly when repeated generation of large numbers of data\nsets is required. However, a standard Gibbs strategy for simulating from MRF\nmodels involves single-site updates, performed with the conditional univariate\ndistribution of each observation in a sequential manner, whereby a complete\nGibbs iteration may become computationally involved even for moderate samples.\nAs an alternative, we describe a general way to simulate from MRF models using\nGibbs sampling with \"concliques\" (i.e., groups of non-neighboring\nobservations). Compared to standard Gibbs sampling, this simulation scheme can\nbe much faster by reducing Gibbs steps and independently updating all\nobservations per conclique at once. The speed improvement depends on the number\nof concliques relative to the sample size for simulation, and\norder-of-magnitude speed increases are possible with many MRF models (e.g.,\nhaving appropriately bounded neighborhoods). We detail the simulation method,\nestablish its validity, and assess its computational performance through\nnumerical studies, where speed advantages are shown for several spatial and\nnetwork examples.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:19:03 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 20:09:14 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Kaplan", "Andee", ""], ["Kaiser", "Mark S.", ""], ["Lahiri", "Soumendra N.", ""], ["Nordman", "Daniel J.", ""]]}, {"id": "1808.04782", "submitter": "Nicholas Tawn", "authors": "Nicholas G. Tawn, Gareth O. Roberts and Jeffrey S. Rosenthal", "title": "Weight-Preserving Simulated Tempering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulated tempering is popular method of allowing MCMC algorithms to move\nbetween modes of a multimodal target density {\\pi}. One problem with simulated\ntempering for multimodal targets is that the weights of the various modes\nchange for different inverse-temperature values, sometimes dramatically so. In\nthis paper, we provide a fix to overcome this problem, by adjusting the mode\nweights to be preserved (i.e., constant) over different inverse-temperature\nsettings. We then apply simulated tempering algorithms to multimodal targets\nusing our mode weight correction. We present simulations in which our\nweight-preserving algorithm mixes between modes much more successfully than\ntraditional tempering algorithms. We also prove a diffusion limit for an\nversion of our algorithm, which shows that under appropriate assumptions, our\nalgorithm mixes in time O(d [log d]^2).\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 16:20:47 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 12:23:00 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 15:22:56 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Tawn", "Nicholas G.", ""], ["Roberts", "Gareth O.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1808.05126", "submitter": "Satya Singh P", "authors": "Satya Prakash Singh and Pradeep Yadav", "title": "Optimal allocation of subjects in a cluster randomized trial with fixed\n  number of clusters when the ICCs or costs are heterogeneous over clusters", "comments": "There are some technical flaws in the proofs of theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intra-cluster correlation coefficient (ICC) plays an important role while\ndesigning the cluster randomized trials (CRTs). Often optimal CRTs are designed\nassuming that the magnitude of the ICC is constant across the clusters.\nHowever, this assumption is hardly satisfied. In some applications, the precise\ninformation about the cluster specific correlation is known in advance. In this\narticle, we propose an optimal design with non-constant ICC across the\nclusters. Also in many situations, the cost of sampling of an observation from\na particular cluster may differ from that of some other cluster. An optimal\ndesign in those scenarios is also obtained assuming unequal costs of sampling\nfrom different clusters. The theoretical findings are supplemented by thorough\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 15:20:25 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 17:25:00 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Singh", "Satya Prakash", ""], ["Yadav", "Pradeep", ""]]}, {"id": "1808.05414", "submitter": "Wenlin Dai", "authors": "Wenlin Dai, Tomas Mrkvicka, Ying Sun, and Marc G. Genton", "title": "Functional Outlier Detection and Taxonomy by Sequential Transformations", "comments": "32 pages, 9 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis can be seriously impaired by abnormal observations,\nwhich can be classified as either magnitude or shape outliers based on their\nway of deviating from the bulk of data. Identifying magnitude outliers is\nrelatively easy, while detecting shape outliers is much more challenging. We\npropose turning the shape outliers into magnitude outliers through data\ntransformation and detecting them using the functional boxplot. Besides easing\nthe detection procedure, applying several transformations sequentially provides\na reasonable taxonomy for the flagged outliers. A joint functional ranking,\nwhich consists of several transformations, is also defined here. Simulation\nstudies are carried out to evaluate the performance of the proposed method\nusing different functional depth notions. Interesting results are obtained in\nseveral practical applications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 11:04:35 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 16:14:13 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Dai", "Wenlin", ""], ["Mrkvicka", "Tomas", ""], ["Sun", "Ying", ""], ["Genton", "Marc G.", ""]]}, {"id": "1808.05480", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey and Himanshu Jhamb", "title": "A novel Empirical Bayes with Reversible Jump Markov Chain in User-Movie\n  Recommendation system", "comments": "arXiv admin note: text overlap with arXiv:1707.02294", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we select the unknown dimension of the feature by re-\nversible jump MCMC inside a simulated annealing in bayesian set up of\ncollaborative filter. We implement the same in MovieLens small dataset. We also\ntune the hyper parameter by using a modified empirical bayes. It can also be\nused to guess an initial choice for hyper-parameters in grid search procedure\neven for the datasets where MCMC oscillates around the true value or takes long\ntime to converge.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 12:59:14 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Jhamb", "Himanshu", ""]]}, {"id": "1808.05868", "submitter": "Han Bossier", "authors": "Han Bossier, Gustavo Amorim, Jan De Neve, Olivier Thas", "title": "Fitting Probabilistic Index Models on Large Datasets", "comments": "Master dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Thas et al. (2012) introduced a new statistical model for the\nprobability index. This index is defined as $P(Y \\leq Y^*|X, X^*)$ where Y and\nY* are independent random response variables associated with covariates X and\nX* [...] Crucially to estimate the parameters of the model, a set of\npseudo-observations is constructed. For a sample size n, a total of $n(n-1)/2$\npairwise comparisons between observations is considered. Consequently for large\nsample sizes, it becomes computationally infeasible or even impossible to fit\nthe model as the set of pseudo-observations increases nearly quadratically. In\nthis dissertation, we provide two solutions to fit a probabilistic index model.\nThe first algorithm consists of splitting the entire data set into unique\npartitions. On each of these, we fit the model and then aggregate the\nestimates. A second algorithm is a subsampling scheme in which we select $K <<\nn$ observations without replacement and after B iterations aggregate the\nestimates. In Monte Carlo simulations, we show how the partitioning algorithm\noutperforms the latter [...] We illustrate the partitioning algorithm and the\ninterpretation of the probabilistic index model on a real data set (Przybylski\nand Weinstein, 2017) of n = 116,630 where we compare it against the ordinary\nleast squares method. By modelling the probabilistic index, we give an\nintuitive and meaningful quantification of the effect of the time adolescents\nspend using digital devices such as smartphones on self-reported mental\nwell-being. We show how moderate usage is associated with an increased\nprobability of reporting a higher mental well-being compared to random\nadolescents who do not use a smartphone. On the other hand, adolescents who\nexcessively use their smartphone are associated with a higher probability of\nreporting a lower mental well-being than randomly chosen peers who do not use a\nsmartphone.[...]\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 13:48:25 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Bossier", "Han", ""], ["Amorim", "Gustavo", ""], ["De Neve", "Jan", ""], ["Thas", "Olivier", ""]]}, {"id": "1808.05884", "submitter": "Fabio Massimo Zennaro", "authors": "Fabio Massimo Zennaro and Magdalena Ivanovska and Audun J{\\o}sang", "title": "An Empirical Evaluation of the Approximation of Subjective Logic\n  Operators Using Monte Carlo Simulations", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the use of subjective logic as a framework for\nperforming approximate transformations over probability distribution functions.\nAs for any approximation, we evaluate subjective logic in terms of\ncomputational efficiency and bias. However, while the computational cost may be\neasily estimated, the bias of subjective logic operators have not yet been\ninvestigated. In order to evaluate this bias, we propose an experimental\nprotocol that exploits Monte Carlo simulations and their properties to assess\nthe distance between the result produced by subjective logic operators and the\ntrue result of the corresponding transformation over probability distributions.\nThis protocol allows a modeler to get an estimate of the degree of\napproximation she must be ready to accept as a trade-off for the computational\nefficiency and the interpretability of the subjective logic framework.\nConcretely, we apply our method to the relevant case study of the subjective\nlogic operator for binomial multiplication and fusion, and we study empirically\ntheir degree of approximation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 14:38:25 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 18:44:08 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zennaro", "Fabio Massimo", ""], ["Ivanovska", "Magdalena", ""], ["J\u00f8sang", "Audun", ""]]}, {"id": "1808.05889", "submitter": "Andreas Lindholm", "authors": "Andreas Svensson, Dave Zachariah, Petre Stoica, and Thomas B. Sch\\\"on", "title": "Data Consistency Approach to Model Validation", "comments": null, "journal-ref": "IEEE Access, 7(1):59788-59796, 2019", "doi": null, "report-no": null, "categories": "stat.ME eess.SP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scientific inference problems, the underlying statistical modeling\nassumptions have a crucial impact on the end results. There exist, however,\nonly a few automatic means for validating these fundamental modelling\nassumptions. The contribution in this paper is a general criterion to evaluate\nthe consistency of a set of statistical models with respect to observed data.\nThis is achieved by automatically gauging the models' ability to generate data\nthat is similar to the observed data. Importantly, the criterion follows from\nthe model class itself and is therefore directly applicable to a broad range of\ninference problems with varying data types, ranging from independent univariate\ndata to high-dimensional time-series. The proposed data consistency criterion\nis illustrated, evaluated and compared to several well-established methods\nusing three synthetic and two real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 14:51:09 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 07:07:35 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Svensson", "Andreas", ""], ["Zachariah", "Dave", ""], ["Stoica", "Petre", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1808.06943", "submitter": "Aijun Zhang", "authors": "Zebin Yang, Dennis K.J. Lin and Aijun Zhang", "title": "Interval-valued Data Prediction via Regularized Artificial Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regularized artificial neural network (RANN) is proposed for\ninterval-valued data prediction. The ANN model is selected due to its powerful\ncapability in fitting linear and nonlinear functions. To meet mathematical\ncoherence requirement for an interval (i.e., the predicted lower bounds should\nnot cross over their upper bounds), a soft non-crossing regularizer is\nintroduced to the interval-valued ANN model. We conduct extensive experiments\nbased on both simulation datasets and real-life datasets, and compare the\nproposed RANN method with multiple traditional models, including the linear\nconstrained center and range method (CCRM), the least absolute shrinkage and\nselection operator-based interval-valued regression method (Lasso-IR), the\nnonlinear interval kernel regression (IKR), the interval multi-layer perceptron\n(iMLP) and the multi-output support vector regression (MSVR). Experimental\nresults show that the proposed RANN model is an effective tool for\ninterval-valued prediction tasks with high prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 14:54:21 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Yang", "Zebin", ""], ["Lin", "Dennis K. J.", ""], ["Zhang", "Aijun", ""]]}, {"id": "1808.07121", "submitter": "Quan Long", "authors": "Nai-Yuan Chiang, Yinqing Lin, Quan Long", "title": "Efficient Propagation of Uncertainties in Manufacturing Supply Chains:\n  Time Buckets, L-leap and Multilevel Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty propagation of large scale discrete supply chains can be\nprohibitive when a large number of events occur during the simulated period and\ndiscrete event simulations (DES) are costly. We present a time bucket method to\napproximate and accelerate the DES of supply chains. Its stochastic version,\nwhich we call the L(logistic)-leap method, can be viewed as an extension of the\nleap methods, e.g., tau-leap, D-leap, developed in the chemical engineering\ncommunity for the acceleration of stochastic DES of chemical reactions. The\nL-leap method instantaneously updates the system state vector at discrete time\npoints and the production rates and policies of a supply chain are assumed to\nbe stationary during each time bucket. We propose to use Multilevel Monte Carlo\n(MLMC) to efficiently propagate the uncertainties in a supply chain network,\nwhere the levels are naturally defined by the sizes of the time buckets of the\nsimulations. We demonstrate the efficiency and accuracy of our methods using\nfour numerical examples derived from a real world manufacturing material flow.\nIn these examples, our multilevel L-leap approach can be faster than the\nstandard Monte Carlo (MC) method by one or two orders of magnitudes without\ncompromising the accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 20:39:37 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 03:58:40 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Chiang", "Nai-Yuan", ""], ["Lin", "Yinqing", ""], ["Long", "Quan", ""]]}, {"id": "1808.07140", "submitter": "Daniel W. Heck", "authors": "Daniel W. Heck and Clintin P. Davis-Stober", "title": "Multinomial Models with Linear Inequality Constraints: Overview and\n  Improvements of Computational Methods for Bayesian Inference", "comments": null, "journal-ref": "Journal of Mathematical Psychology (2019) 91, 70-87", "doi": "10.1016/j.jmp.2019.03.004", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many psychological theories can be operationalized as linear inequality\nconstraints on the parameters of multinomial distributions (e.g., discrete\nchoice analysis). These constraints can be described in two equivalent ways:\nEither as the solution set to a system of linear inequalities or as the convex\nhull of a set of extremal points (vertices). For both representations, we\ndescribe a general Gibbs sampler for drawing posterior samples in order to\ncarry out Bayesian analyses. We also summarize alternative sampling methods for\nestimating Bayes factors for these model representations using the encompassing\nBayes factor method. We introduce the R package multinomineq, which provides an\neasily-accessible interface to a computationally efficient implementation of\nthese techniques.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 21:35:36 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 08:16:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Heck", "Daniel W.", ""], ["Davis-Stober", "Clintin P.", ""]]}, {"id": "1808.07730", "submitter": "Alexander Buchholz", "authors": "Alexander Buchholz and Nicolas Chopin and Pierre E. Jacob", "title": "Adaptive Tuning Of Hamiltonian Monte Carlo Within Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) samplers form an attractive alternative to MCMC\nfor Bayesian computation. However, their performance depends strongly on the\nMarkov kernels used to rejuvenate particles. We discuss how to calibrate\nautomatically (using the current particles) Hamiltonian Monte Carlo kernels\nwithin SMC. To do so, we build upon the adaptive SMC approach of Fearnhead and\nTaylor (2013), and we also suggest alternative methods. We illustrate the\nadvantages of using HMC kernels within an SMC sampler via an extensive\nnumerical study.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 12:53:55 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 17:34:16 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Buchholz", "Alexander", ""], ["Chopin", "Nicolas", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "1808.08592", "submitter": "Alain Durmus", "authors": "Christophe Andrieu, Alain Durmus, Nikolas N\\\"usken, Julien Roussel", "title": "Hypocoercivity of Piecewise Deterministic Markov Process-Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we establish $\\mathrm{L}^2$-exponential convergence for a broad\nclass of Piecewise Deterministic Markov Processes recently proposed in the\ncontext of Markov Process Monte Carlo methods and covering in particular the\nRandomized Hamiltonian Monte Carlo, the Zig-Zag process and the Bouncy Particle\nSampler. The kernel of the symmetric part of the generator of such processes is\nnon-trivial, and we follow the ideas recently introduced by (Dolbeault et al.,\n2009, 2015) to develop a rigorous framework for hypocoercivity in a fairly\ngeneral and unifying set-up, while deriving tractable estimates of the\nconstants involved in terms of the parameters of the dynamics. As a by-product\nwe characterize the scaling properties of these algorithms with respect to the\ndimension of classes of problems, therefore providing some theoretical evidence\nto support their practical relevance.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 16:55:00 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 05:05:58 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Andrieu", "Christophe", ""], ["Durmus", "Alain", ""], ["N\u00fcsken", "Nikolas", ""], ["Roussel", "Julien", ""]]}, {"id": "1808.08618", "submitter": "Vadim Sokolov", "authors": "Nicholas Polson and Vadim Sokolov", "title": "Deep Learning: Computational Aspects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we review computational aspects of Deep Learning (DL). Deep\nlearning uses network architectures consisting of hierarchical layers of latent\nvariables to construct predictors for high-dimensional input-output models.\nTraining a deep learning architecture is computationally intensive, and\nefficient linear algebra libraries is the key for training and inference.\nStochastic gradient descent (SGD) optimization and batch sampling are used to\nlearn from massive data sets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 20:26:11 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 21:54:41 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Polson", "Nicholas", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1808.08624", "submitter": "Alexander Kreuzer", "authors": "Alexander Kreuzer, Claudia Czado", "title": "Bayesian inference for a single factor copula stochastic volatility\n  model using Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For modeling multivariate financial time series we propose a single factor\ncopula model together with stochastic volatility margins. This model\ngeneralizes single factor models relying on the multivariate normal\ndistribution and allows for symmetric and asymmetric tail dependence. We\ndevelop joint Bayesian inference using Hamiltonian Monte Carlo (HMC) within\nGibbs sampling. Thus we avoid information loss caused by the two-step approach\nfor margins and dependence in copula models as followed by Schamberger et\nal(2017). Further, the Bayesian approach allows for high dimensional parameter\nspaces as they are present here in addition to uncertainty quantification\nthrough credible intervals. By allowing for indicators for different copula\nfamilies the copula families are selected automatically in the Bayesian\nframework. In a first simulation study the performance of HMC is compared to\nthe Markov Chain Monte Carlo (MCMC) approach developed by Schamberger et\nal(2017) for the copula part. It is shown that HMC considerably outperforms\nthis approach in terms of effective sample size, MSE and observed coverage\nprobabilities. In a second simulation study satisfactory performance is seen\nfor the full HMC within Gibbs procedure. The approach is illustrated for a\nportfolio of financial assets with respect to one-day ahead value at risk\nforecasts. We provide comparison to a two-step estimation procedure of the\nproposed model and to relevant benchmark models: a model with dynamic linear\nmodels for the margins and a single factor copula for the dependence proposed\nby Schamberger et al(2017) and a multivariate factor stochastic volatility\nmodel proposed by Kastner et al(2017). Our proposed approach shows superior\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 21:24:33 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 13:06:34 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Kreuzer", "Alexander", ""], ["Czado", "Claudia", ""]]}, {"id": "1808.09262", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli", "title": "The Sparse Latent Position Model for nonnegative weighted networks", "comments": "40 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new methodology to analyse bipartite and unipartite\nnetworks with nonnegative edge values. The proposed approach combines and\nadapts a number of ideas from the literature on latent variable network models.\nThe resulting framework is a new type of latent position model which exhibits\ngreat flexibility, and is able to capture important features that are generally\nexhibited by observed networks, such as sparsity and heavy tailed degree\ndistributions. A crucial advantage of the proposed method is that the number of\nlatent dimensions is automatically deduced from the data in one single\nalgorithmic framework. In addition, the model attaches a weight to each of the\nlatent dimensions, hence providing a measure of their relative importance. A\nfast variational Bayesian algorithm is proposed to estimate the parameters of\nthe model. Finally, applications of the proposed methodology are illustrated on\nboth artificial and real datasets, and comparisons with other existing\nprocedures are provided.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 12:58:35 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Rastelli", "Riccardo", ""]]}, {"id": "1808.09340", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen and Alexandre Moesching and Christof Straehl", "title": "Active set algorithms for estimating shape-constrained density ratios", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2021.107300", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many instances, imposing a constraint on the shape of a density is a\nreasonable and flexible assumption. It offers an alternative to parametric\nmodels which can be too rigid and to other nonparametric methods requiring the\nchoice of tuning parameters. This paper treats the nonparametric estimation of\nlog-concave or log-convex density ratios by means of active set algorithms in a\nunified framework. In the setting of log-concave densities, the new algorithm\nis similar to but substantially faster than previously considered active set\nmethods. Log-convexity is a less common shape constraint which is described by\nsome authors as \"tail inflation\". The active set method proposed here is novel\nin this context. As a by-product, new goodness-of-fit tests of single\nhypotheses are formulated and are shown to be more powerful than higher\ncriticism tests in a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 14:52:31 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 09:28:05 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 15:28:35 GMT"}, {"version": "v4", "created": "Tue, 8 Dec 2020 10:49:03 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Duembgen", "Lutz", ""], ["Moesching", "Alexandre", ""], ["Straehl", "Christof", ""]]}, {"id": "1808.09489", "submitter": "Jiangning Chen", "authors": "Jiangning Chen", "title": "Convergence Rate of Krasulina Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is one of the most commonly used\nstatistical procedures with a wide range of applications. Consider the points\n$X_1, X_2,..., X_n$ are vectors drawn i.i.d. from a distribution with mean zero\nand covariance $\\Sigma$, where $\\Sigma$ is unknown. Let $A_n = X_nX_n^T$, then\n$E[A_n] = \\Sigma$. This paper consider the problem of finding the least\neigenvalue and eigenvector of matrix $\\Sigma$. A classical such estimator are\ndue to Krasulina\\cite{krasulina_method_1969}. We are going to state the\nconvergence proof of Krasulina for the least eigenvalue and corresponding\neigenvector, and then find their convergence rate.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 18:47:20 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 22:12:53 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 18:09:15 GMT"}, {"version": "v4", "created": "Tue, 23 Jul 2019 04:05:08 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chen", "Jiangning", ""]]}, {"id": "1808.09552", "submitter": "Camille Chapdelaine", "authors": "Camille Chapdelaine", "title": "Variational Bayesian Approach and Gauss-Markov-Potts prior model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many inverse problems such as 3D X-ray Computed Tomography (CT), the\nestimation of an unknown quantity, such as a volume or an image, can be greatly\nenhanced, compared to maximum-likelihood techniques, by incorporating a prior\nmodel on the quantity to reconstruct. A complex prior can be designed for\nmulti-channel estimation such as reconstruction and segmentation thanks to\nGauss-Markov-Potts prior model. For very large inverse problems such as 3D\nX-ray CT, maximization a posteriori (MAP) techniques are often used due to the\nhuge size of the data and the unknown. Nevertheless, MAP estimation does not\nenable to have quantify uncertainties on the retrieved reconstruction, which\ncan be useful for post-reconstruction processes for instance in industry and\nmedicine. A way to tackle the problem of uncertainties estimation is to compute\nposterior mean (PM) for which the uncertainties are the variances of the\nposterior distribution. Because MCMC methods are not affordable for very large\n3D problems, this paper presents an algorithm to jointly estimate the\nreconstruction and the uncertainties by computing PM thanks to variational\nBayesian approach (VBA). The prior model we consider for the unknowns is a\nGauss-Markov-Potts prior which has been shown to give good results in many\ninverse problems. After having detailed the used prior models, the algorithm\nbased on VBA is detailed : it corresponds to an iterative computation\nofapproximate distributions through the iterative updates of their parameters.\nThe updating formulae are given in the last section. We also provide a method\nfor initialization of the algorithm, as a method to fix each parameter.\nPerspectives are applications of this algorithm to large 3D problems such as 3D\nX-ray CT.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 21:46:10 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 08:17:36 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Chapdelaine", "Camille", ""]]}]