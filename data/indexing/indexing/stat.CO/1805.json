[{"id": "1805.00318", "submitter": "Karl Oskar Ekvall", "authors": "Karl Oskar Ekvall and Brian R. Gray", "title": "Separable correlation and maximum likelihood", "comments": "14 pages, 2 figures, 1 supplemental file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of the covariance matrix of a multivariate normal\ndistribution when the correlation matrix is separable in the sense that it\nfactors as a Kronecker product of two smaller matrices. A computationally\nconvenient coordinate descent-type algorithm is developed for maximum\nlikelihood estimation. Simulations indicate our method often gives smaller\nestimation error than some common alternatives when correlation is separable,\nand that correctly sized tests for correlation separability can be obtained\nusing a parametric bootstrap. Using dissolved oxygen data from the Upper\nMississippi River, we illustrate how our model can lead to interesting\nscientific findings that may be missed when using competing models.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 13:20:14 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 15:10:18 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Ekvall", "Karl Oskar", ""], ["Gray", "Brian R.", ""]]}, {"id": "1805.00452", "submitter": "Nawaf Bou-Rabee", "authors": "Nawaf Bou-Rabee, Andreas Eberle, Raphael Zimmer", "title": "Coupling and Convergence for Hamiltonian Monte Carlo", "comments": "50 pages, 8 figures, extended the coupling approach to include\n  corresponding results under a Foster-Lyapunov condition", "journal-ref": "Ann. Appl. Probab., Volume 30, Number 3 (2020), 1209-1250", "doi": "10.1214/19-AAP1528", "report-no": null, "categories": "math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a new coupling approach, we prove that the transition step of the\nHamiltonian Monte Carlo algorithm is contractive w.r.t. a carefully designed\nKantorovich (L1 Wasserstein) distance. The lower bound for the contraction rate\nis explicit. Global convexity of the potential is not required, and thus\nmultimodal target distributions are included. Explicit quantitative bounds for\nthe number of steps required to approximate the stationary distribution up to a\ngiven error are a direct consequence of contractivity. These bounds show that\nHMC can overcome diffusive behaviour if the duration of the Hamiltonian\ndynamics is adjusted appropriately.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 17:38:40 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 06:30:21 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Bou-Rabee", "Nawaf", ""], ["Eberle", "Andreas", ""], ["Zimmer", "Raphael", ""]]}, {"id": "1805.00541", "submitter": "Giacomo Zanella", "authors": "Giacomo Zanella and Gareth Roberts", "title": "Scalable Importance Tempering and Bayesian Variable Selection", "comments": "Online supplement not included", "journal-ref": "J. R. Statist. Soc. B (2019) 81, Part 3, pp. 489-517", "doi": "10.1111/rssb.12316", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Monte Carlo algorithm to sample from high dimensional\nprobability distributions that combines Markov chain Monte Carlo and importance\nsampling. We provide a careful theoretical analysis, including guarantees on\nrobustness to high dimensionality, explicit comparison with standard Markov\nchain Monte Carlo methods and illustrations of the potential improvements in\nefficiency. Simple and concrete intuition is provided for when the novel scheme\nis expected to outperform standard schemes. When applied to Bayesian\nvariable-selection problems, the novel algorithm is orders of magnitude more\nefficient than available alternative sampling schemes and enables fast and\nreliable fully Bayesian inferences with tens of thousand regressors.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 20:15:53 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 09:05:02 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Zanella", "Giacomo", ""], ["Roberts", "Gareth", ""]]}, {"id": "1805.00871", "submitter": "Janne Hakkarainen", "authors": "Janne Hakkarainen, Zenith Purisha, Antti Solonen, Samuli Siltanen", "title": "Undersampled dynamic X-ray tomography with dimension reduction Kalman\n  filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider prior-based dimension reduction Kalman filter for\nundersampled dynamic X-ray tomography. With this method, the X-ray\nreconstructions are parameterized by a low-dimensional basis. Thus, the\nproposed method is a) computationally very light; and b) extremely robust as\nall the computations can be done explicitly. With real and simulated\nmeasurement data, we show that the method provides accurate reconstructions\neven with very limited number of angular directions.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 15:42:03 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Hakkarainen", "Janne", ""], ["Purisha", "Zenith", ""], ["Solonen", "Antti", ""], ["Siltanen", "Samuli", ""]]}, {"id": "1805.01648", "submitter": "Niladri Chatterji", "authors": "Xiang Cheng, Niladri S. Chatterji, Yasin Abbasi-Yadkori, Peter L.\n  Bartlett, Michael I. Jordan", "title": "Sharp convergence rates for Langevin dynamics in the nonconvex setting", "comments": "78 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling from a distribution $p^*(x) \\propto\n\\exp\\left(-U(x)\\right)$, where the function $U$ is $L$-smooth everywhere and\n$m$-strongly convex outside a ball of radius $R$, but potentially nonconvex\ninside this ball. We study both overdamped and underdamped Langevin MCMC and\nestablish upper bounds on the number of steps required to obtain a sample from\na distribution that is within $\\epsilon$ of $p^*$ in $1$-Wasserstein distance.\nFor the first-order method (overdamped Langevin MCMC), the iteration complexity\nis $\\tilde{\\mathcal{O}}\\left(e^{cLR^2}d/\\epsilon^2\\right)$, where $d$ is the\ndimension of the underlying space. For the second-order method (underdamped\nLangevin MCMC), the iteration complexity is\n$\\tilde{\\mathcal{O}}\\left(e^{cLR^2}\\sqrt{d}/\\epsilon\\right)$ for an explicit\npositive constant $c$. Surprisingly, the iteration complexity for both these\nalgorithms is only polynomial in the dimension $d$ and the target accuracy\n$\\epsilon$. It is exponential, however, in the problem parameter $LR^2$, which\nis a measure of non-log-concavity of the target distribution.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 08:16:00 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 15:34:27 GMT"}, {"version": "v3", "created": "Sun, 3 Feb 2019 02:09:14 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 06:49:05 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Cheng", "Xiang", ""], ["Chatterji", "Niladri S.", ""], ["Abbasi-Yadkori", "Yasin", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1805.01852", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Sonja Greven", "title": "Inference for $L_2$-Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical inference framework for the component-wise\nfunctional gradient descent algorithm (CFGD) under normality assumption for\nmodel errors, also known as $L_2$-Boosting. The CFGD is one of the most\nversatile tools to analyze data, because it scales well to high-dimensional\ndata sets, allows for a very flexible definition of additive regression models\nand incorporates inbuilt variable selection. Due to the variable selection, we\nbuild on recent proposals for post-selection inference. However, the iterative\nnature of component-wise boosting, which can repeatedly select the same\ncomponent to update, necessitates adaptations and extensions to existing\napproaches. We propose tests and confidence intervals for linear, grouped and\npenalized additive model components selected by $L_2$-Boosting. Our concepts\nalso transfer to slow-learning algorithms more generally, and to other\nselection techniques which restrict the response space to more complex sets\nthan polyhedra. We apply our framework to an additive model for sales prices of\nresidential apartments and investigate the properties of our concepts in\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 16:51:38 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 10:00:44 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 14:44:10 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 20:17:26 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Greven", "Sonja", ""]]}, {"id": "1805.01870", "submitter": "Stephane Chretien", "authors": "Stephane Chretien, Alex Gibberd and Sandipan Roy", "title": "Hedging parameter selection for basis pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Compressed Sensing and high dimensional estimation, signal recovery often\nrelies on sparsity assumptions and estimation is performed via\n$\\ell_1$-penalized least-squares optimization, a.k.a. LASSO. The $\\ell_1$\npenalisation is usually controlled by a weight, also called \"relaxation\nparameter\", denoted by $\\lambda$. It is commonly thought that the practical\nefficiency of the LASSO for prediction crucially relies on accurate selection\nof $\\lambda$. In this short note, we propose to consider the hyper-parameter\nselection problem from a new perspective which combines the Hedge online\nlearning method by Freund and Shapire, with the stochastic Frank-Wolfe method\nfor the LASSO. Using the Hedge algorithm, we show that a our simple selection\nrule can achieve prediction results comparable to Cross Validation at a\npotentially much lower computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:33:27 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Chretien", "Stephane", ""], ["Gibberd", "Alex", ""], ["Roy", "Sandipan", ""]]}, {"id": "1805.01916", "submitter": "B{\\l}a\\.zej Miasojedow", "authors": "Szymon Majewski, B{\\l}a\\.zej Miasojedow, Eric Moulines", "title": "Analysis of nonsmooth stochastic approximation: the differential\n  inclusion approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the convergence of stochastic approximation when the\nfunctions to be minimized are not convex and nonsmooth. We show that the\n\"mean-limit\" approach to the convergence which leads, for smooth problems, to\nthe ODE approach can be adapted to the non-smooth case. The limiting dynamical\nsystem may be shown to be, under appropriate assumption, a differential\ninclusion. Our results expand earlier works in this direction by Benaim et al.\n(2005) and provide a general framework for proving convergence for\nunconstrained and constrained stochastic approximation problems, with either\nexplicit or implicit updates. In particular, our results allow us to establish\nthe convergence of stochastic subgradient and proximal stochastic gradient\ndescent algorithms arising in a large class of deep learning and\nhigh-dimensional statistical inference with sparsity inducing penalties.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 19:13:56 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Majewski", "Szymon", ""], ["Miasojedow", "B\u0142a\u017cej", ""], ["Moulines", "Eric", ""]]}, {"id": "1805.02075", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Decentralized Nonparametric Multiple Testing", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a big data multiple testing task, where, due to storage and\ncomputational bottlenecks, one is given a very large collection of p-values by\nsplitting into manageable chunks and distributing over thousands of computer\nnodes. This paper is concerned with the following question: How can we find the\nfull data multiple testing solution by operating completely independently on\nindividual machines in parallel, without any data exchange between nodes? This\nversion of the problem tends naturally to arise in a wide range of\ndata-intensive science and industry applications whose methodological solution\nhas not appeared in the literature to date; therefore, we feel it is necessary\nto undertake such analysis. Based on the nonparametric functional statistical\nviewpoint of large-scale inference, started in Mukhopadhyay (2016), this paper\nfurnishes a new computing model that brings unexpected simplicity to the design\nof the algorithm which might otherwise seem daunting using classical approach\nand notations.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 15:57:04 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1805.02257", "submitter": "Lingrui Gan", "authors": "Lingrui Gan, Naveen N. Narisetty, Feng Liang", "title": "Bayesian Regularization for Graphical Models with Unequal Shrinkage", "comments": "To appear in Journal of the American Statistical Association (Theory\n  & Methods)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bayesian framework for estimating a high-dimensional sparse\nprecision matrix, in which adaptive shrinkage and sparsity are induced by a\nmixture of Laplace priors. Besides discussing our formulation from the Bayesian\nstandpoint, we investigate the MAP (maximum a posteriori) estimator from a\npenalized likelihood perspective that gives rise to a new non-convex penalty\napproximating the $\\ell_0$ penalty. Optimal error rates for estimation\nconsistency in terms of various matrix norms along with selection consistency\nfor sparse structure recovery are shown for the unique MAP estimator under mild\nconditions. For fast and efficient computation, an EM algorithm is proposed to\ncompute the MAP estimator of the precision matrix and (approximate) posterior\nprobabilities on the edges of the underlying sparse structure. Through\nextensive simulation studies and a real application to a call center data, we\nhave demonstrated the fine performance of our method compared with existing\nalternatives.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 18:16:21 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:31:59 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Gan", "Lingrui", ""], ["Narisetty", "Naveen N.", ""], ["Liang", "Feng", ""]]}, {"id": "1805.02407", "submitter": "Adam Lund", "authors": "Adam Lund, S{\\o}ren Wengel Mogensen, Niels Richard Hansen", "title": "Soft Maximin Estimation for Heterogeneous Array Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of a common signal across many recordings is difficult when\neach recording -- in addition to the signal -- contains large, unique variation\ncomponents. Maximin estimation has previously been proposed as a robust\nestimation method in the presence of heterogeneous noise.\n  We propose soft maximin estimation as a computationally attractive\nmethodology for estimating a common signal from heterogeneous data. The soft\nmaximin loss is introduced as an aggregation, controlled by a parameter\n$\\zeta>0$, of explained variances and the estimator is obtained by minimizing\nthe penalized soft maximin loss.\n  By establishing statistical and computational properties we argue that the\nsoft maximin method is a statistically sensibel and computationally attractive\nalternative to existing methods. In particular we demonstrate, on simulated and\nreal data, that the soft maximin estimator can outperform existing methods both\nin terms of predictive performance and run time. We also provide a time and\nmemory efficient implementation for data with array-tensor structure in the R\npackage SMMA available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 09:02:40 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 10:33:57 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 18:46:47 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Lund", "Adam", ""], ["Mogensen", "S\u00f8ren Wengel", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "1805.03288", "submitter": "Robert Bassett", "authors": "Robert Bassett and James Sharpnack", "title": "Fused Density Estimation: Theory and Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a method for nonparametric density estimation on\ngeometric networks. We define fused density estimators as solutions to a total\nvariation regularized maximum-likelihood density estimation problem. We provide\ntheoretical support for fused density estimation by proving that the squared\nHellinger rate of convergence for the estimator achieves the minimax bound over\nunivariate densities of log-bounded variation. We reduce the original\nvariational formulation in order to transform it into a tractable,\nfinite-dimensional quadratic program. Because random variables on geometric\nnetworks are simple generalizations of the univariate case, this method also\nprovides a useful tool for univariate density estimation. Lastly, we apply this\nmethod and assess its performance on examples in the univariate and geometric\nnetwork setting. We compare the performance of different optimization\ntechniques to solve the problem, and use these results to inform\nrecommendations for the computation of fused density estimators.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 21:07:29 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 19:31:22 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Bassett", "Robert", ""], ["Sharpnack", "James", ""]]}, {"id": "1805.03304", "submitter": "Jonas Latz", "authors": "Christian Kahle, Kei Fong Lam, Jonas Latz, Elisabeth Ullmann", "title": "Bayesian parameter identification in Cahn-Hilliard models for biological\n  growth", "comments": null, "journal-ref": "SIAM/ASA J. Uncertain. Quantif. 7(2), p. 526-552, 2019", "doi": "10.1137/18M1210034", "report-no": null, "categories": "math.NA math.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the inverse problem of parameter estimation in a diffuse\ninterface model for tumour growth. The model consists of a fourth-order\nCahn-Hilliard system and contains three phenomenological parameters: the tumour\nproliferation rate, the nutrient consumption rate, and the chemotactic\nsensitivity. We study the inverse problem within the Bayesian framework and\nconstruct the likelihood and noise for two typical observation settings. One\nsetting involves an infinite-dimensional data space where we observe the full\ntumour. In the second setting we observe only the tumour volume, hence the data\nspace is finite-dimensional. We show the well-posedness of the posterior\nmeasure for both settings, building upon and improving the analytical results\nin [C. Kahle and K.F. Lam, Appl. Math. Optim. (2018)]. A numerical example\ninvolving synthetic data is presented in which the posterior measure is\nnumerically approximated by the sequential Monte Carlo approach with tempering.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 22:07:07 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 09:12:52 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 10:33:24 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Kahle", "Christian", ""], ["Lam", "Kei Fong", ""], ["Latz", "Jonas", ""], ["Ullmann", "Elisabeth", ""]]}, {"id": "1805.03309", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss, Joseph Guinness, Wenlong Gong, Daniel Zilber", "title": "Vecchia approximations of Gaussian-process predictions", "comments": null, "journal-ref": "Journal of Agricultural, Biological, and Environmental Statistics,\n  25(3), 383-414 (2020)", "doi": "10.1007/s13253-020-00401-7", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are highly flexible function estimators used for\ngeospatial analysis, nonparametric regression, and machine learning, but they\nare computationally infeasible for large datasets. Vecchia approximations of\nGPs have been used to enable fast evaluation of the likelihood for parameter\ninference. Here, we study Vecchia approximations of spatial predictions at\nobserved and unobserved locations, including obtaining joint predictive\ndistributions at large sets of locations. We consider a general Vecchia\nframework for GP predictions, which contains some novel and some existing\nspecial cases. We study the accuracy and computational properties of these\napproaches theoretically and numerically, proving that our new methods exhibit\nlinear computational complexity in the total number of spatial locations. We\nshow that certain choices within the framework can have a strong effect on\nuncertainty quantification and computational cost, which leads to specific\nrecommendations on which methods are most suitable for various settings. We\nalso apply our methods to a satellite dataset of chlorophyll fluorescence,\nshowing that the new methods are faster or more accurate than existing methods,\nand reduce unrealistic artifacts in prediction maps.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 22:25:40 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 13:36:40 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 15:55:32 GMT"}, {"version": "v4", "created": "Thu, 14 May 2020 22:26:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Guinness", "Joseph", ""], ["Gong", "Wenlong", ""], ["Zilber", "Daniel", ""]]}, {"id": "1805.03317", "submitter": "Matias Quiroz", "authors": "David Gunawan, Khue-Dung Dang, Matias Quiroz, Robert Kohn, Minh-Ngoc\n  Tran", "title": "Subsampling Sequential Monte Carlo for Static Bayesian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to speed up Sequential Monte Carlo (SMC) for Bayesian inference\nin large data problems by data subsampling. SMC sequentially updates a cloud of\nparticles through a sequence of distributions, beginning with a distribution\nthat is easy to sample from such as the prior and ending with the posterior\ndistribution. Each update of the particle cloud consists of three steps:\nreweighting, resampling, and moving. In the move step, each particle is moved\nusing a Markov kernel; this is typically the most computationally expensive\npart, particularly when the dataset is large. It is crucial to have an\nefficient move step to ensure particle diversity. Our article makes two\nimportant contributions. First, in order to speed up the SMC computation, we\nuse an approximately unbiased and efficient annealed likelihood estimator based\non data subsampling. The subsampling approach is more memory efficient than the\ncorresponding full data SMC, which is an advantage for parallel computation.\nSecond, we use a Metropolis within Gibbs kernel with two conditional updates. A\nHamiltonian Monte Carlo update makes distant moves for the model parameters,\nand a block pseudo-marginal proposal is used for the particles corresponding to\nthe auxiliary variables for the data subsampling. We demonstrate both the\nusefulness and limitations of the methodology for estimating four generalized\nlinear models and a generalized additive model with large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 23:17:01 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 08:12:12 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 10:36:25 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Gunawan", "David", ""], ["Dang", "Khue-Dung", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "1805.03326", "submitter": "Zdravko Botev", "authors": "Zdravko I. Botev and Pierre L'Ecuyer and Bruno Tuffin", "title": "Reliability Estimation for Networks with Minimal Flow Demand and Random\n  Link Capacities", "comments": null, "journal-ref": null, "doi": null, "report-no": "hal-01745187; G-2018-24", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a network whose links have random capacities and in which a\ncertain target amount of flow must be carried from some source nodes to some\ndestination nodes. Each destination node has a fixed demand that must be\nsatisfied and each source node has a given supply. We want to estimate the\nunreliability of the network, defined as the probability that the network\ncannot carry the required amount of flow to meet the demand at all destination\nnodes. When this unreliability is very small, which is our main interest in\nthis paper, standard Monte Carlo estimators become useless because failure to\nmeet the demand is a rare event. We propose and compare two different methods\nto handle this situation, one based on a conditional Monte Carlo approach and\nthe other based on generalized splitting. We find that the first is more\neffective when the network is highly reliable and not too large, whereas for a\nlarger network and/or moderate reliability, the second is more effective.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 00:14:32 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Botev", "Zdravko I.", ""], ["L'Ecuyer", "Pierre", ""], ["Tuffin", "Bruno", ""]]}, {"id": "1805.03924", "submitter": "Robert Salomone", "authors": "Robert Salomone, Leah F. South, Christopher C. Drovandi, Dirk P.\n  Kroese", "title": "Unbiased and Consistent Nested Sampling via Sequential Monte Carlo", "comments": "45 pages, some minor typographical errors fixed since last version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of sequential Monte Carlo methods called Nested\nSampling via Sequential Monte Carlo (NS-SMC), which reframes the Nested\nSampling method of Skilling (2006) in terms of sequential Monte Carlo\ntechniques. This new framework allows convergence results to be obtained in the\nsetting when Markov chain Monte Carlo (MCMC) is used to produce new samples. An\nadditional benefit is that marginal likelihood estimates are unbiased. In\ncontrast to NS, the analysis of NS-SMC does not require the (unrealistic)\nassumption that the simulated samples be independent. As the original NS\nalgorithm is a special case of NS-SMC, this provides insights as to why NS\nseems to produce accurate estimates despite a typical violation of its\nassumptions. For applications of NS-SMC, we give advice on tuning MCMC kernels\nin an automated manner via a preliminary pilot run, and present a new method\nfor appropriately choosing the number of MCMC repeats at each iteration.\nFinally, a numerical study is conducted where the performance of NS-SMC and\ntemperature-annealed SMC is compared on several challenging and realistic\nproblems. MATLAB code for our experiments is made available at\nhttps://github.com/LeahPrice/SMC-NS .\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 11:09:15 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 01:10:34 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 06:56:16 GMT"}, {"version": "v4", "created": "Mon, 12 Nov 2018 01:53:11 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Salomone", "Robert", ""], ["South", "Leah F.", ""], ["Drovandi", "Christopher C.", ""], ["Kroese", "Dirk P.", ""]]}, {"id": "1805.04924", "submitter": "Payam Siyari", "authors": "Payam Siyari, Bistra Dilkina, Constantine Dovrolis", "title": "Emergence and Evolution of Hierarchical Structure in Complex Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that many complex systems, both in technology and nature,\nexhibit hierarchical modularity: smaller modules, each of them providing a\ncertain function, are used within larger modules that perform more complex\nfunctions. What is not well understood however is how this hierarchical\nstructure (which is fundamentally a network property) emerges, and how it\nevolves over time. We propose a modeling framework, referred to as Evo-Lexis,\nthat provides insight to some fundamental questions about evolving hierarchical\nsystems. Evo-Lexis models the most elementary modules of the system as symbols\n(\"sources\") and the modules at the highest level of the hierarchy as sequences\nof those symbols (\"targets\"). Evo-Lexis computes the optimized adjustment of a\ngiven hierarchy when the set of targets changes over time by additions and\nremovals (a process referred to as \"incremental design\"). In this paper we use\ncomputation modeling to show that:\n  - Low-cost and deep hierarchies emerge when the population of target\nsequences evolves through tinkering and mutation. - Strong selection on the\ncost of new candidate targets results in reuse of more complex (longer) nodes\nin an optimized hierarchy. - The bias towards reuse of complex nodes results in\nan \"hourglass architecture\" (i.e., few intermediate nodes that cover almost all\nsource-target paths). - With such bias, the core nodes are conserved for\nrelatively long time periods although still being vulnerable to major\ntransitions and punctuated equilibria. - Finally, we analyze the differences in\nterms of cost and structure between incrementally designed hierarchies and the\ncorresponding \"clean-slate\" hierarchies which result when the system is\ndesigned from scratch after a change.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 18:38:51 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 21:40:05 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Siyari", "Payam", ""], ["Dilkina", "Bistra", ""], ["Dovrolis", "Constantine", ""]]}, {"id": "1805.05002", "submitter": "Natalie Karavarsamis", "authors": "N. Karavarsamis, G. Guillera-Arroita, RM Huggins, B J T Morgan", "title": "How can the score test be consistent?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The score test statistic using the observed information is easy to compute\nnumerically. Its large sample distribution under the null hypothesis is well\nknown and is equivalent to that of the score test based on the expected\ninformation, the likelihood-ratio test and the Wald test. However, several\nauthors have noted that under the alternative this no longer holds and in\nparticular the statistic can take negative values. Here we examine the score\ntest using the observed information in the context of comparing two binomial\nproportions under imperfect detection, a common problem in ecology when\nstudying occurrence of species. We demonstrate through a combination of\nsimulations and theoretical analysis that a new modified rule which we propose\nthat rejects the null hypothesis when the observed score statistic is larger\nthan the usual chi-square cut-off or is negative has power that is mostly\ngreater to any other test. In addition consistency is largely restored. Our new\ntest is easy to use and inference is always possible.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 03:35:02 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 06:05:40 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Karavarsamis", "N.", ""], ["Guillera-Arroita", "G.", ""], ["Huggins", "RM", ""], ["Morgan", "B J T", ""]]}, {"id": "1805.05054", "submitter": "Pierre Alquier", "authors": "Badr-Eddine Ch\\'erief-Abdellatif and Pierre Alquier", "title": "Consistency of Variational Bayes Inference for Estimation and Model\n  Selection in Mixtures", "comments": null, "journal-ref": "Electronic Journal of Statistics, 2018, vol. 12, no. 2, pp.\n  2995-3035", "doi": "10.1214/18-EJS1475", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are widely used in Bayesian statistics and machine learning,\nin particular in computational biology, natural language processing and many\nother fields. Variational inference, a technique for approximating intractable\nposteriors thanks to optimization algorithms, is extremely popular in practice\nwhen dealing with complex models such as mixtures. The contribution of this\npaper is two-fold. First, we study the concentration of variational\napproximations of posteriors, which is still an open problem for general\nmixtures, and we derive consistency and rates of convergence. We also tackle\nthe problem of model selection for the number of components: we study the\napproach already used in practice, which consists in maximizing a numerical\ncriterion (the Evidence Lower Bound). We prove that this strategy indeed leads\nto strong oracle inequalities. We illustrate our theoretical results by\napplications to Gaussian and multinomial mixtures.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 08:15:48 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 09:50:12 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""]]}, {"id": "1805.05168", "submitter": "Alastair Gregory", "authors": "Alastair Gregory", "title": "A streaming algorithm for bivariate empirical copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical copula functions can be used to model the dependence structure of\nmultivariate data. The Greenwald and Khanna algorithm is adapted in order to\nprovide a space-memory efficient approximation to the empirical copula function\nof a bivariate stream of data. A succinct space-memory efficient summary of\nvalues seen in the stream up to a certain time is maintained and can be queried\nat any point to return an approximation to the empirical bivariate copula\nfunction with guaranteed error bounds. An example then illustrates how these\nsummaries can be used as a tool to compute approximations to higher dimensional\ncopula decompositions containing bivariate copulas. The computational benefits\nand approximation error of the algorithm is theoretically and numerically\nassessed.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 13:35:25 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2019 10:19:33 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Gregory", "Alastair", ""]]}, {"id": "1805.05188", "submitter": "Shengxin Zhu", "authors": "Shengxin Zhu and Andrew J Wathen", "title": "Essential formulae for restricted maximum likelihood and its derivatives\n  associated with the linear mixed models", "comments": "arXiv admin note: substantial text overlap with arXiv:1608.07207", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted maximum likelihood method enhances popularity of maximum\nlikelihood methods for variance component analysis on large scale unbalanced\ndata. As the high throughput biological data sets and the emerged science on\nuncertainty quantification, such a method receives increasing attention.\nEstimating the unknown variance parameters with restricted maximum likelihood\nmethod usually requires an nonlinear iterative method. Therefore proper\nformulae for the log-likelihood function and its derivatives play an essential\nrole in practical algorithm design. It is our aim to provide a mathematical\nintroduction to this method, and supply a self-contained derivation on some\navailable formulae used in practical algorithms. Some new proof are supplied.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 03:59:11 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zhu", "Shengxin", ""], ["Wathen", "Andrew J", ""]]}, {"id": "1805.05289", "submitter": "Andrew Holbrook", "authors": "Andrew Holbrook", "title": "Note on the geodesic Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic Monte Carlo (gMC) is a powerful algorithm for Bayesian inference on\nnon-Euclidean manifolds. The original gMC algorithm was cleverly derived in\nterms of its progenitor, the Riemannian manifold Hamiltonian Monte Carlo\n(RMHMC). Here, it is shown that alternative and theoretically simpler\nderivations are available in which the original algorithm is a special case of\ntwo general classes of algorithms characterized by non-trivial mass matrices.\nThe proposed derivations work entirely in embedding coordinates and thus\nclarify the original algorithm as applied to manifolds embedded in Euclidean\nspace.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 17:00:23 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 23:26:43 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Holbrook", "Andrew", ""]]}, {"id": "1805.06328", "submitter": "Panpan Zhang", "authors": "Panpan Zhang and Dipak K. Dey", "title": "The degree profile and Gini index of random caterpillar trees", "comments": null, "journal-ref": "Prob. Eng. Inf. Sci. 33 (2019) 511-527", "doi": "10.1017/S0269964818000475", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the degree profile and Gini index of random\ncaterpillar trees (RCTs). We consider RCTs which evolve in two different\nmanners: uniform and nonuniform. The degrees of the vertices on the central\npath (i.e., the degree profile) of a uniform RCT follow a multinomial\ndistribution. For nonuniform RCTs, we focus on those growing in the fashion of\npreferential attachment. We develop methods based on stochastic recurrences to\ncompute the exact expectations and the dispersion matrix of the degree\nvariables. A generalized P\\'{o}lya urn model is exploited to determine the\nexact joint distribution of these degree variables. We apply the methods from\ncombinatorics to prove that the asymptotic distribution is Dirichlet. In\naddition, we propose a new type of Gini index to quantitatively distinguish the\nevolutionary characteristics of the two classes of RCTs. We present the results\nvia several numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 17:47:04 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhang", "Panpan", ""], ["Dey", "Dipak K.", ""]]}, {"id": "1805.06478", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio", "title": "Semi-parametric Bayesian change-point model based on the Dirichlet\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a semi-parametric Bayesian change-point model,\ndefining its time dynamic as a latent Markov process based on the Dirichlet\nprocess. We treat the number of change point as a random variable and we\nestimate it during model fitting. Posterior inference is carried out using a\nMarkov chain Monte Carlo algorithm based on a marginalized version of the\nproposed model. The model is illustrated using simulated examples and two real\ndatasets, namely the coal- mining disasters, that is a widely used dataset for\nillustrative purpose, and a dataset of indoor radon recordings. With the\nsimulated examples we show that the model is able to recover the parameters and\nnumber of change points, and we compare our results with the ones of the-state-\nof-the-art models, showing a clear improvement in terms of change points\nidentification. The results obtained on the coal-mining disasters and radon\ndata are coherent with previous literature.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 18:21:48 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 21:39:02 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 19:47:25 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Mastrantonio", "Gianluca", ""]]}, {"id": "1805.06639", "submitter": "Ze Jin", "authors": "Ze Jin, David S. Matteson", "title": "Independent Component Analysis via Energy-based and Kernel-based Mutual\n  Dependence Measures", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply both distance-based (Jin and Matteson, 2017) and kernel-based\n(Pfister et al., 2016) mutual dependence measures to independent component\nanalysis (ICA), and generalize dCovICA (Matteson and Tsay, 2017) to MDMICA,\nminimizing empirical dependence measures as an objective function in both\ndeflation and parallel manners. Solving this minimization problem, we introduce\nLatin hypercube sampling (LHS) (McKay et al., 2000), and a global optimization\nmethod, Bayesian optimization (BO) (Mockus, 1994) to improve the initialization\nof the Newton-type local optimization method. The performance of MDMICA is\nevaluated in various simulation studies and an image data example. When the ICA\nmodel is correct, MDMICA achieves competitive results compared to existing\napproaches. When the ICA model is misspecified, the estimated independent\ncomponents are less mutually dependent than the observed components using\nMDMICA, while they are prone to be even more mutually dependent than the\nobserved components using other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:53:09 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Matteson", "David S.", ""]]}, {"id": "1805.06640", "submitter": "Ze Jin", "authors": "Ze Jin, Xiaohan Yan, David S. Matteson", "title": "Testing for Conditional Mean Independence with Covariates through\n  Martingale Difference Divergence", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial problem in statistics is to decide whether additional variables\nare needed in a regression model. We propose a new multivariate test to\ninvestigate the conditional mean independence of Y given X conditioning on some\nknown effect Z, i.e., E(Y|X, Z) = E(Y|Z). Assuming that E(Y|Z) and Z are\nlinearly related, we reformulate an equivalent notion of conditional mean\nindependence through transformation, which is approximated in practice. We\napply the martingale difference divergence (Shao and Zhang, 2014) to measure\nconditional mean dependence, and show that the estimation error from\napproximation is negligible, as it has no impact on the asymptotic distribution\nof the test statistic under some regularity assumptions. The implementation of\nour test is demonstrated by both simulations and a financial data example.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:53:48 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Yan", "Xiaohan", ""], ["Matteson", "David S.", ""]]}, {"id": "1805.06855", "submitter": "Yinchu Zhu", "authors": "Yinchu Zhu", "title": "Learning non-smooth models: instrumental variable quantile regressions\n  and related problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes computationally efficient methods that can be used for\ninstrumental variable quantile regressions (IVQR) and related methods with\nstatistical guarantees. This is much needed when we investigate heterogenous\ntreatment effects since interactions between the endogenous treatment and\ncontrol variables lead to an increased number of endogenous covariates. We\nprove that the GMM formulation of IVQR is NP-hard and finding an approximate\nsolution is also NP-hard. Hence, solving the problem from a purely\ncomputational perspective seems unlikely. Instead, we aim to obtain an estimate\nthat has good statistical properties and is not necessarily the global solution\nof any optimization problem.\n  The proposal consists of employing $k$-step correction on an initial\nestimate. The initial estimate exploits the latest advances in mixed integer\nlinear programming and can be computed within seconds. One theoretical\ncontribution is that such initial estimators and Jacobian of the moment\ncondition used in the k-step correction need not be even consistent and merely\n$k=4\\log n$ fast iterations are needed to obtain an efficient estimator. The\noverall proposal scales well to handle extremely large sample sizes because\nlack of consistency requirement allows one to use a very small subsample to\nobtain the initial estimate and the k-step iterations on the full sample can be\nimplemented efficiently. Another contribution that is of independent interest\nis to propose a tuning-free estimation for the Jacobian matrix, whose\ndefinition nvolves conditional densities. This Jacobian estimator generalizes\nbootstrap quantile standard errors and can be efficiently computed via\nclosed-end solutions. We evaluate the performance of the proposal in\nsimulations and an empirical example on the heterogeneous treatment effect of\nJob Training Partnership Act.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 16:58:11 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 17:10:45 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 17:29:53 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 07:10:30 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Zhu", "Yinchu", ""]]}, {"id": "1805.06915", "submitter": "Martin Slawski", "authors": "Felicitas J. Detmer and Martin Slawski", "title": "A Note on Coding and Standardization of Categorical Variables in\n  (Sparse) Group Lasso Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical regressor variables are usually handled by introducing a set of\nindicator variables, and imposing a linear constraint to ensure identifiability\nin the presence of an intercept, or equivalently, using one of various coding\nschemes. As proposed in Yuan and Lin [J. R. Statist. Soc. B, 68 (2006), 49-67],\nthe group lasso is a natural and computationally convenient approach to perform\nvariable selection in settings with categorical covariates. As pointed out by\nSimon and Tibshirani [Stat. Sin., 22 (2011), 983-1001], \"standardization\" by\nmeans of block-wise orthonormalization of column submatrices each corresponding\nto one group of variables can substantially boost performance. In this note, we\nstudy the aspect of standardization for the special case of categorical\npredictors in detail. The main result is that orthonormalization is not\nrequired; column-wise scaling of the design matrix followed by re-scaling and\ncentering of the coefficients is shown to have exactly the same effect. Similar\nreductions can be achieved in the case of interactions. The extension to the\nso-called sparse group lasso, which additionally promotes within-group\nsparsity, is considered as well. The importance of proper standardization is\nillustrated via extensive simulations.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 18:21:58 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Detmer", "Felicitas J.", ""], ["Slawski", "Martin", ""]]}, {"id": "1805.07174", "submitter": "Bj\\\"orn Sprungk", "authors": "Daniel Rudolf and Bj\\\"orn Sprungk", "title": "On a Metropolis-Hastings importance sampling estimator", "comments": "33 pages, 4 figures, accepted for publication in Electron. J. Stat", "journal-ref": "Electron. J. Statist. 14(1) (2020) 857-889", "doi": "10.1214/20-EJS1680", "report-no": null, "categories": "stat.CO cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical approach for approximating expectations of functions w.r.t.\npartially known distributions is to compute the average of function values\nalong a trajectory of a Metropolis-Hastings (MH) Markov chain. A key part in\nthe MH algorithm is a suitable acceptance/rejection of a proposed state, which\nensures the correct stationary distribution of the resulting Markov chain.\nHowever, the rejection of proposals causes highly correlated samples. In\nparticular, when a state is rejected it is not taken any further into account.\nIn contrast to that we consider a MH importance sampling estimator which\nexplicitly incorporates all proposed states generated by the MH algorithm. The\nestimator satisfies a strong law of large numbers as well as a central limit\ntheorem, and, in addition to that, we provide an explicit mean squared error\nbound. Remarkably, the asymptotic variance of the MH importance sampling\nestimator does not involve any correlation term in contrast to its classical\ncounterpart. Moreover, although the analyzed estimator uses the same amount of\ninformation as the classical MH estimator, it can outperform the latter in\nscenarios of moderate dimensions as indicated by numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 12:39:33 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 13:14:25 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 14:54:13 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Rudolf", "Daniel", ""], ["Sprungk", "Bj\u00f6rn", ""]]}, {"id": "1805.07272", "submitter": "Fabian Rathke", "authors": "Fabian Rathke, Christoph Schn\\\"orr", "title": "Fast Multivariate Log-Concave Density Estimation", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel computational approach to log-concave density estimation is proposed.\nPrevious approaches utilize the piecewise-affine parametrization of the density\ninduced by the given sample set. The number of parameters as well as non-smooth\nsubgradient-based convex optimization for determining the maximum likelihood\ndensity estimate cause long runtimes for dimensions $d \\geq 2$ and large sample\nsets. The presented approach is based on mildly non-convex smooth\napproximations of the objective function and \\textit{sparse}, adaptive\npiecewise-affine density parametrization. Established memory-efficient\nnumerical optimization techniques enable to process larger data sets for\ndimensions $d \\geq 2$. While there is no guarantee that the algorithm returns\nthe maximum likelihood estimate for every problem instance, we provide\ncomprehensive numerical evidence that it does yield near-optimal results after\nsignificantly shorter runtimes. For example, 10000 samples in $\\mathbb{R}^2$\nare processed in two seconds, rather than in $\\approx 14$ hours required by the\nprevious approach to terminate. For higher dimensions, density estimation\nbecomes tractable as well: Processing $10000$ samples in $\\mathbb{R}^6$\nrequires 35 minutes. The software is publicly available as CRAN R package\nfmlogcondens.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:13:21 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 17:41:34 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Rathke", "Fabian", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1805.07427", "submitter": "Jan Hannig", "authors": "Randy C. S. Lai, J. Hannig and Thomas C. M. Lee", "title": "Method G: Uncertainty Quantification for Distributed Data Problems using\n  Generalized Fiducial Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not unusual for a data analyst to encounter data sets distributed\nacross several computers. This can happen for reasons such as privacy concerns,\nefficiency of likelihood evaluations, or just the sheer size of the whole data\nset. This presents new challenges to statisticians as even computing simple\nsummary statistics such as the median becomes computationally challenging.\nFurthermore, if other advanced statistical methods are desired, novel\ncomputational strategies are needed. In this paper we propose a new approach\nfor distributed analysis of massive data that is suitable for generalized\nfiducial inference and is based on a careful implementation of a \"divide and\nconquer\" strategy combined with importance sampling. The proposed approach\nrequires only small amount of communication between nodes, and is shown to be\nasymptotically equivalent to using the whole data set. Unlike most existing\nmethods, the proposed approach produces uncertainty measures (such as\nconfidence intervals) in addition to point estimates for parameters of\ninterest. The proposed approach is also applied to the analysis of a large set\nof solar images.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 20:15:42 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Lai", "Randy C. S.", ""], ["Hannig", "J.", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1805.08283", "submitter": "James M. Flegal", "authors": "Ying Liu and James M. Flegal", "title": "Weighted batch means estimators in Markov chain Monte Carlo", "comments": "52 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a family of weighted batch means variance estimators,\nwhich are computationally efficient and can be conveniently applied in\npractice. The focus is on Markov chain Monte Carlo simulations and estimation\nof the asymptotic covariance matrix in the Markov chain central limit theorem,\nwhere conditions ensuring strong consistency are provided. Finite sample\nperformance is evaluated through auto-regressive, Bayesian spatial-temporal,\nand Bayesian logistic regression examples, where the new estimators show\nsignificant computational gains with a minor sacrifice in variance compared\nwith existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 20:39:30 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Liu", "Ying", ""], ["Flegal", "James M.", ""]]}, {"id": "1805.08321", "submitter": "Tavor Baharav", "authors": "Vivek Bagaria, Tavor Z. Baharav, Govinda M. Kamath, David N. Tse", "title": "Bandit-Based Monte Carlo Optimization for Nearest Neighbors", "comments": "Accepted to the IEEE Journal on Selected Areas in Information Theory\n  (JSAIT) - Special Issue on Sequential, Active, and Reinforcement Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated Monte Carlo method estimates an expensive-to-compute quantity\nby random sampling. Bandit-based Monte Carlo optimization is a general\ntechnique for computing the minimum of many such expensive-to-compute\nquantities by adaptive random sampling. The technique converts an optimization\nproblem into a statistical estimation problem which is then solved via\nmulti-armed bandits. We apply this technique to solve the problem of\nhigh-dimensional $k$-nearest neighbors, developing an algorithm which we prove\nis able to identify exact nearest neighbors with high probability. We show that\nunder regularity assumptions on a dataset of $n$ points in $d$-dimensional\nspace, the complexity of our algorithm scales logarithmically with the\ndimension of the data as $O\\left((n+d)\\log^2\n\\left(\\frac{nd}{\\delta}\\right)\\right)$ for error probability $\\delta$, rather\nthan linearly as in exact computation requiring $O(nd)$. We corroborate our\ntheoretical results with numerical simulations, showing that our algorithm\noutperforms both exact computation and state-of-the-art algorithms such as\nkGraph, NGT, and LSH on real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 23:28:30 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 00:26:39 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 23:10:15 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 21:10:05 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bagaria", "Vivek", ""], ["Baharav", "Tavor Z.", ""], ["Kamath", "Govinda M.", ""], ["Tse", "David N.", ""]]}, {"id": "1805.08637", "submitter": "Robert J. Kunsch", "authors": "Robert J. Kunsch, Erich Novak, Daniel Rudolf", "title": "Solvable Integration Problems and Optimal Sample Size Selection", "comments": "38 pages, to appear in the Journal of Complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute the integral of a function or the expectation of a random variable\nwith minimal cost and use, for our new algorithm and for upper bounds of the\ncomplexity, i.i.d. samples. Under certain assumptions it is possible to select\na sample size based on a variance estimation, or -- more generally -- based on\nan estimation of a (central absolute) $p$-moment. That way one can guarantee a\nsmall absolute error with high probability, the problem is thus called\nsolvable. The expected cost of the method depends on the $p$-moment of the\nrandom variable, which can be arbitrarily large.\n  In order to prove the optimality of our algorithm we also provide lower\nbounds. These bounds apply not only to methods based on i.i.d. samples but also\nto general randomized algorithms. They show that -- up to constants -- the cost\nof the algorithm is optimal in terms of accuracy, confidence level, and norm of\nthe particular input random variable. Since the considered classes of random\nvariables or integrands are very large, the worst case cost would be infinite.\nNevertheless one can define adaptive stopping rules such that for each input\nthe expected cost is finite.\n  We contrast these positive results with examples of integration problems that\nare not solvable.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 14:39:12 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 15:07:20 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kunsch", "Robert J.", ""], ["Novak", "Erich", ""], ["Rudolf", "Daniel", ""]]}, {"id": "1805.08670", "submitter": "Colman Humphrey", "authors": "Colman Humphrey, Dan Swingley", "title": "Regression Analysis of Proportion Outcomes with Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regression method for proportional, or fractional, data with mixed effects\nis outlined, designed for analysis of datasets in which the outcomes have\nsubstantial weight at the bounds. In such cases a normal approximation is\nparticularly unsuitable as it can result in incorrect inference. To resolve\nthis problem, we employ a logistic regression model and then apply a bootstrap\nmethod to correct conservative confidence intervals. This paper outlines the\ntheory of the method, and demonstrates its utility using simulated data.\nWorking code for the R platform is provided through the package glmmboot,\navailable on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 15:44:08 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Humphrey", "Colman", ""], ["Swingley", "Dan", ""]]}, {"id": "1805.08719", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Parsimonious Bayesian deep networks", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining Bayesian nonparametrics and a forward model selection strategy, we\nconstruct parsimonious Bayesian deep networks (PBDNs) that infer\ncapacity-regularized network architectures from the data and require neither\ncross-validation nor fine-tuning when training the model. One of the two\nessential components of a PBDN is the development of a special infinite-wide\nsingle-hidden-layer neural network, whose number of active hidden units can be\ninferred from the data. The other one is the construction of a greedy\nlayer-wise learning algorithm that uses a forward model selection criterion to\ndetermine when to stop adding another hidden layer. We develop both Gibbs\nsampling and stochastic gradient descent based maximum a posteriori inference\nfor PBDNs, providing state-of-the-art classification accuracy and interpretable\ndata subtypes near the decision boundaries, while maintaining low computational\ncomplexity for out-of-sample prediction.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 16:26:42 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 15:11:43 GMT"}, {"version": "v3", "created": "Tue, 1 Jan 2019 00:45:05 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1805.08863", "submitter": "Charles Matthews", "authors": "Charles Matthews and Jonathan Weare", "title": "Langevin Markov Chain Monte Carlo with stochastic gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo sampling techniques have broad applications in machine learning,\nBayesian posterior inference, and parameter estimation. Often the target\ndistribution takes the form of a product distribution over a dataset with a\nlarge number of entries. For sampling schemes utilizing gradient information it\nis cheaper for the derivative to be approximated using a random small subset of\nthe data, introducing extra noise into the system. We present a new\ndiscretization scheme for underdamped Langevin dynamics when utilizing a\nstochastic (noisy) gradient. This scheme is shown to bias computed averages to\nsecond order in the stepsize while giving exact results in the special case of\nsampling a Gaussian distribution with a normally distributed stochastic\ngradient.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 20:54:44 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 22:22:57 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Matthews", "Charles", ""], ["Weare", "Jonathan", ""]]}, {"id": "1805.08931", "submitter": "Roel Ceballos", "authors": "Roel F. Ceballos, Fe F. Largo", "title": "On The Estimation of the Hurst Exponent Using Adjusted Rescaled Range\n  Analysis, Detrended Fluctuation Analysis and Variance Time Plot: A Case of\n  Exponential Distribution", "comments": null, "journal-ref": "Imperial Journal of Interdisciplinary Research 2017 (Volume 3,\n  Issue 8, pp. 424-434)", "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hurst Exponent has been widely used in different fields as a measure of long\nrange dependence in time series. It has been studied in hydrology and\ngeophysics, economics and finance, and recently, it is still a hot topic in the\ndifferent areas of research involving DNA sequences, cardiac dynamics, internet\ntraffic, meteorology and geology. Various methods in the estimation of Hurst\nExponent have been proposed such as Adjusted Rescaled Range Analysis, Detrended\nFluctuation Analysis and Variance Time Plot Analysis. This study explored the\nefficiency of the three methods: Adjusted Rescaled Range Analysis, Detrended\nFluctuation Analysis and Variance Time Plot Analysis in the estimation of Hurst\nExponent when data are generated from an exponential distribution. In addition,\nthe efficiency of the three methods was compared in different sample sizes of\n128, 256, 512, 1024 and varying {\\lambda} parameter values of 0.1, 0.5, 1.5,\n3.0, 5.0 and 7.0. The estimation process for each of the methods using\ndifferent sample sizes and {\\lambda} parameter values were repeated for 100,\n500 and 1000 times to verify the consistency of the result. A Scilab Program\ncontaining different functions was developed for the study to aid in the\nsimulation process and calculation. The Adjusted Rescaled Range Analysis was\nthe most efficient method with the smallest Mean Square Error for all {\\lambda}\nparameter values and different sample sizes.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 01:48:00 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Ceballos", "Roel F.", ""], ["Largo", "Fe F.", ""]]}, {"id": "1805.09108", "submitter": "Luciano Melodia", "authors": "Luciano Melodia", "title": "Deep Learning Estimation of Absorbed Dose for Nuclear Medicine\n  Diagnostics", "comments": "Master Thesis", "journal-ref": "Lib.Univ.Rgbg 2018 (1-36)", "doi": "10.31219/osf.io/zp6nv", "report-no": null, "categories": "stat.ML cs.LG nucl-ex physics.med-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of energy dose from Lu$^{177}$ radiotherapy can be estimated\nby convolving an image of a time-integrated activity distribution with a dose\nvoxel kernel (DVK) consisting of different types of tissues. This fast and\ninacurate approximation is inappropriate for personalized dosimetry as it\nneglects tissue heterogenity. The latter can be calculated using different\nimaging techniques such as CT and SPECT combined with a time consuming\nmonte-carlo simulation. The aim of this study is, for the first time, an\nestimation of DVKs from CT-derived density kernels (DK) via deep learning in\nconvolutional neural networks (CNNs). The proposed CNN achieved, on the test\nset, a mean intersection over union (IOU) of $= 0.86$ after $308$ epochs and a\ncorresponding mean squared error (MSE) $= 1.24 \\cdot 10^{-4}$. This\ngeneralization ability shows that the trained CNN can indeed learn the\ndifficult transfer function from DK to DVK. Future work will evaluate DVKs\nestimated by CNNs with full monte-carlo simulations of a whole body CT to\npredict patient specific voxel dose maps.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 13:54:00 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 11:45:48 GMT"}, {"version": "v3", "created": "Sun, 10 Jun 2018 12:45:11 GMT"}, {"version": "v4", "created": "Thu, 2 Jan 2020 10:26:13 GMT"}, {"version": "v5", "created": "Wed, 10 Jun 2020 11:59:14 GMT"}, {"version": "v6", "created": "Thu, 19 Nov 2020 14:02:22 GMT"}, {"version": "v7", "created": "Fri, 20 Nov 2020 09:40:38 GMT"}, {"version": "v8", "created": "Fri, 4 Dec 2020 14:37:57 GMT"}, {"version": "v9", "created": "Thu, 18 Mar 2021 12:59:38 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Melodia", "Luciano", ""]]}, {"id": "1805.09505", "submitter": "Ranjan Maitra", "authors": "Israel Almod\\'ovar-Rivera and Ranjan Maitra", "title": "Kernel-estimated Nonparametric Overlap-Based Syncytial Clustering", "comments": "32 pages, 22 figures, 9 tables: published in JMLR at:\n  http://jmlr.org/papers/v21/18-435.html", "journal-ref": "Journal of Machine Learning Research 21:122, 1-54 (2020)", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly-used clustering algorithms usually find ellipsoidal, spherical or\nother regular-structured clusters, but are more challenged when the underlying\ngroups lack formal structure or definition. Syncytial clustering is the name\nthat we introduce for methods that merge groups obtained from standard\nclustering algorithms in order to reveal complex group structure in the data.\nHere, we develop a distribution-free fully-automated syncytial clustering\nalgorithm that can be used with $k$-means and other algorithms. Our approach\nestimates the cumulative distribution function of the normed residuals from an\nappropriately fit $k$-groups model and calculates the estimated nonparametric\noverlap between each pair of clusters. Groups with high pairwise overlap are\nmerged as long as the estimated generalized overlap decreases. Our methodology\nis always a top performer in identifying groups with regular and irregular\nstructures in several datasets and can be applied to datasets with scatter or\nincomplete records. The approach is also used to identify the distinct kinds of\ngamma ray bursts in the Burst and Transient Source Experiment 4Br catalog and\nthe distinct kinds of activation in a functional Magnetic Resonance Imaging\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 04:50:10 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 01:40:05 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 02:38:06 GMT"}, {"version": "v4", "created": "Sat, 9 May 2020 17:10:38 GMT"}, {"version": "v5", "created": "Tue, 28 Jul 2020 18:09:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Almod\u00f3var-Rivera", "Israel", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1805.09674", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "A D-vine copula mixed model for joint meta-analysis and comparison of\n  diagnostic tests", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.03920,\n  arXiv:1502.07505", "journal-ref": "Statistical Methods in Medical Research, 2019, 28 (10-11),\n  3286-3300", "doi": "10.1177/0962280218796685", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a particular disease there may be two diagnostic tests developed, where\neach of the tests is subject to several studies. A quadrivariate generalized\nlinear mixed model (GLMM) has been recently proposed to joint meta-analyse and\ncompare two diagnostic tests. We propose a D-vine copula mixed model for joint\nmeta-analysis and comparison of two diagnostic tests. Our general model\nincludes the quadrivariate GLMM as a special case and can also operate on the\noriginal scale of sensitivities and specificities. The method allows the direct\ncalculation of sensitivity and specificity for each test, as well as, the\nparameters of the summary receiver operator characteristic (SROC) curve, along\nwith a comparison between the SROCs of each test. Our methodology is\ndemonstrated with an extensive simulation study and illustrated by\nmeta-analysing two examples where 2 tests for the diagnosis of a particular\ndisease are compared. Our study suggests that there can be an improvement on\nGLMM in fit to data since our model can also provide tail dependencies and\nasymmetries.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 19:49:41 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 17:15:43 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1805.10020", "submitter": "Gary Mirams", "authors": "Sanmitra Ghosh, David J. Gavaghan, Gary R. Mirams", "title": "Gaussian process emulation for discontinuous response surfaces with\n  applications for cardiac electrophysiology models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mathematical models of biological systems are beginning to be used for\nsafety-critical applications, where large numbers of repeated model evaluations\nare required to perform uncertainty quantification and sensitivity analysis.\nMost of these models are nonlinear both in variables and parameters/inputs\nwhich has two consequences. First, analytic solutions are rarely available so\nrepeated evaluation of these models by numerically solving differential\nequations incurs a significant computational burden. Second, many models\nundergo bifurcations in behaviour as parameters are varied. As a result,\nsimulation outputs often contain discontinuities as we change parameter values\nand move through parameter/input space.\n  Statistical emulators such as Gaussian processes are frequently used to\nreduce the computational cost of uncertainty quantification, but\ndiscontinuities render a standard Gaussian process emulation approach\nunsuitable as these emulators assume a smooth and continuous response to\nchanges in parameter values.\n  In this article, we propose a novel two-step method for building a Gaussian\nProcess emulator for models with discontinuous response surfaces. We first use\na Gaussian Process classifier to detect boundaries of discontinuities and then\nconstrain the Gaussian Process emulation of the response surface within these\nboundaries. We introduce a novel `certainty metric' to guide active learning\nfor a multi-class probabilistic classifier.\n  We apply the new classifier to simulations of drug action on a cardiac\nelectrophysiology model, to propagate our uncertainty in a drug's action\nthrough to predictions of changes to the cardiac action potential. The proposed\ntwo-step active learning method significantly reduces the computational cost of\nemulating models that undergo multiple bifurcations.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:01:53 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Ghosh", "Sanmitra", ""], ["Gavaghan", "David J.", ""], ["Mirams", "Gary R.", ""]]}, {"id": "1805.10036", "submitter": "Reza Hajargasht", "authors": "Gholamreza Hajargasht and Tomasz Wo\\'zniak", "title": "Accurate Computation of Marginal Data Densities Using Variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new marginal data density estimator (MDDE) that uses the\nvariational Bayes posterior density as a weighting density of the reciprocal\nimportance sampling (RIS) MDDE. This computationally convenient estimator is\nbased on variational Bayes posterior densities that are available for many\nmodels and requires simulated draws only from the posterior distribution. It\nprovides accurate estimates with a moderate number of posterior draws, has a\nfinite variance, and provides a minimum variance candidate for the class of RIS\nMDDEs. Its reciprocal is consistent, asymptotically normally distributed, and\nunbiased. These properties are obtained without truncating the weighting\ndensity, which is typical for other such estimators. Our proposed estimators\noutperform many existing MDDEs in terms of bias and numerical standard errors.\nIn particular, our RIS MDDE performs uniformly better than other estimators\nfrom this class.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:37:20 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 12:45:54 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hajargasht", "Gholamreza", ""], ["Wo\u017aniak", "Tomasz", ""]]}, {"id": "1805.10038", "submitter": "Ba Tuong Vo Prof", "authors": "Ba Tuong Vo, Ba Ngu Vo", "title": "A Multi-Scan Labeled Random Finite Set Model for Multi-object State\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State space models in which the system state is a finite set--called the\nmulti-object state--have generated considerable interest in recent years.\nSmoothing for state space models provides better estimation performance than\nfiltering by using the full posterior rather than the filtering density. In\nmulti-object state estimation, the Bayes multi-object filtering recursion\nadmits an analytic solution known as the Generalized Labeled Multi-Bernoulli\n(GLMB) filter. In this work, we extend the analytic GLMB recursion to propagate\nthe multi-object posterior. We also propose an implementation of this so-called\nmulti-scan GLMB posterior recursion using a similar approach to the GLMB filter\nimplementation.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:52:02 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Vo", "Ba Tuong", ""], ["Vo", "Ba Ngu", ""]]}, {"id": "1805.10157", "submitter": "Minh-Ngoc Tran", "authors": "Minh-Ngoc Tran, Nghia Nguyen, David Nott, Robert Kohn", "title": "Bayesian Deep Net GLM and GLMM", "comments": "35 pages, 7 figure, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep feedforward neural networks (DFNNs) are a powerful tool for functional\napproximation. We describe flexible versions of generalized linear and\ngeneralized linear mixed models incorporating basis functions formed by a DFNN.\nThe consideration of neural networks with random effects is not widely used in\nthe literature, perhaps because of the computational challenges of\nincorporating subject specific parameters into already complex models.\nEfficient computational methods for high-dimensional Bayesian inference are\ndeveloped using Gaussian variational approximation, with a parsimonious but\nflexible factor parametrization of the covariance matrix. We implement natural\ngradient methods for the optimization, exploiting the factor structure of the\nvariational covariance matrix in computation of the natural gradient. Our\nflexible DFNN models and Bayesian inference approach lead to a regression and\nclassification method that has a high prediction accuracy, and is able to\nquantify the prediction uncertainty in a principled and convenient way. We also\ndescribe how to perform variable selection in our deep learning method. The\nproposed methods are illustrated in a wide range of simulated and real-data\nexamples, and the results compare favourably to a state of the art flexible\nregression and classification method in the statistical literature, the\nBayesian additive regression trees (BART) method. User-friendly software\npackages in Matlab, R and Python implementing the proposed methods are\navailable at https://github.com/VBayesLab\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 13:48:40 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Nguyen", "Nghia", ""], ["Nott", "David", ""], ["Kohn", "Robert", ""]]}, {"id": "1805.10206", "submitter": "Paul Smith Mr.", "authors": "Elena Issoglio, Paul Smith, Jochen Voss", "title": "On the Estimation of Entropy in the FastICA Algorithm", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The fastICA method is a popular dimension reduction technique used to reveal\npatterns in data. Here we show both theoretically and in practice that the\napproximations used in fastICA can result in patterns not being successfully\nrecognised. We demonstrate this problem using a two-dimensional example where a\nclear structure is immediately visible to the naked eye, but where the\nprojection chosen by fastICA fails to reveal this structure. This implies that\ncare is needed when applying fastICA. We discuss how the problem arises and how\nit is intrinsically connected to the approximations that form the basis of the\ncomputational efficiency of fastICA.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 15:43:50 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 12:02:26 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 14:59:24 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2020 16:44:57 GMT"}, {"version": "v5", "created": "Tue, 8 Sep 2020 15:51:13 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Issoglio", "Elena", ""], ["Smith", "Paul", ""], ["Voss", "Jochen", ""]]}, {"id": "1805.10211", "submitter": "Laurent Risser", "authors": "Camille Champion (IMT), Anne-Claire Brunet (IMT), Jean-Michel Loubes\n  (IMT), Laurent Risser (IMT)", "title": "COREclust: a new package for a robust and scalable analysis of complex\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new R package COREclust dedicated to the\ndetection of representative variables in high dimensional spaces with a\npotentially limited number of observations. Variable sets detection is based on\nan original graph clustering strategy denoted CORE-clustering algorithm that\ndetects CORE-clusters, i.e. variable sets having a user defined size range and\nin which each variable is very similar to at least another variable.\nRepresentative variables are then robustely estimate as the CORE-cluster\ncenters. This strategy is entirely coded in C++ and wrapped by R using the Rcpp\npackage. A particular effort has been dedicated to keep its algorithmic cost\nreasonable so that it can be used on large datasets. After motivating our work,\nwe will explain the CORE-clustering algorithm as well as a greedy extension of\nthis algorithm. We will then present how to use it and results obtained on\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 15:50:15 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Champion", "Camille", "", "IMT"], ["Brunet", "Anne-Claire", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Risser", "Laurent", "", "IMT"]]}, {"id": "1805.10378", "submitter": "Zachary Charles", "authors": "Zachary Charles, Dimitris Papailiopoulos", "title": "Gradient Coding via the Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent and its many variants, including mini-batch stochastic\ngradient descent, form the algorithmic foundation of modern large-scale machine\nlearning. Due to the size and scale of modern data, gradient computations are\noften distributed across multiple compute nodes. Unfortunately, such\ndistributed implementations can face significant delays caused by straggler\nnodes, i.e., nodes that are much slower than average. Gradient coding is a new\ntechnique for mitigating the effect of stragglers via algorithmic redundancy.\nWhile effective, previously proposed gradient codes can be computationally\nexpensive to construct, inaccurate, or susceptible to adversarial stragglers.\nIn this work, we present the stochastic block code (SBC), a gradient code based\non the stochastic block model. We show that SBCs are efficient, accurate, and\nthat under certain settings, adversarial straggler selection becomes as hard as\ndetecting a community structure in the multiple community, block stochastic\ngraph model.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 22:02:30 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Charles", "Zachary", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "1805.10865", "submitter": "Xanthi Pedeli", "authors": "Xanthi Pedeli and Cristiano Varin", "title": "Pairwise likelihood estimation of latent autoregressive count models", "comments": "The final version of the paper has been published in Statistical\n  Methods in Medical Research", "journal-ref": null, "doi": "10.1177/0962280220924068", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent autoregressive models are useful time series models for the analysis\nof infectious disease data. Evaluation of the likelihood function of latent\nautoregressive models is intractable and its approximation through\nsimulation-based methods appears as a standard practice. Although simulation\nmethods may make the inferential problem feasible, they are often\ncomputationally intensive and the quality of the numerical approximation may be\ndifficult to assess. We consider instead a weighted pairwise likelihood\napproach and explore several computational and methodological aspects including\nestimation of robust standard errors and the role of numerical integration. The\nsuggested approach is illustrated using monthly data on invasive meningococcal\ndisease infection in Greece and Italy.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 11:13:35 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 12:53:17 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 08:13:39 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 08:18:44 GMT"}, {"version": "v5", "created": "Mon, 22 Jun 2020 07:03:20 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Pedeli", "Xanthi", ""], ["Varin", "Cristiano", ""]]}, {"id": "1805.11183", "submitter": "Mingyuan Zhou", "authors": "Mingzhang Yin and Mingyuan Zhou", "title": "Semi-Implicit Variational Inference", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-implicit variational inference (SIVI) is introduced to expand the\ncommonly used analytic variational distribution family, by mixing the\nvariational parameter with a flexible distribution. This mixing distribution\ncan assume any density function, explicit or not, as long as independent random\nsamples can be generated via reparameterization. Not only does SIVI expand the\nvariational family to incorporate highly flexible variational distributions,\nincluding implicit ones that have no analytic density functions, but also\nsandwiches the evidence lower bound (ELBO) between a lower bound and an upper\nbound, and further derives an asymptotically exact surrogate ELBO that is\namenable to optimization via stochastic gradient ascent. With a substantially\nexpanded variational family and a novel optimization algorithm, SIVI is shown\nto closely match the accuracy of MCMC in inferring the posterior in a variety\nof Bayesian inference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 21:55:02 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Yin", "Mingzhang", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1805.11238", "submitter": "David Gamarnik", "authors": "David Gamarnik", "title": "Explicit construction of RIP matrices is Ramsey-hard", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrices $\\Phi\\in\\R^{n\\times p}$ satisfying the Restricted Isometry Property\n(RIP) are an important ingredient of the compressive sensing methods. While it\nis known that random matrices satisfy the RIP with high probability even for\n$n=\\log^{O(1)}p$, the explicit construction of such matrices defied the\nrepeated efforts, and the most known approaches hit the so-called $\\sqrt{n}$\nsparsity bottleneck. The notable exception is the work by Bourgain et al\n\\cite{bourgain2011explicit} constructing an $n\\times p$ RIP matrix with\nsparsity $s=\\Theta(n^{{1\\over 2}+\\epsilon})$, but in the regime\n$n=\\Omega(p^{1-\\delta})$.\n  In this short note we resolve this open question in a sense by showing that\nan explicit construction of a matrix satisfying the RIP in the regime\n$n=O(\\log^2 p)$ and $s=\\Theta(n^{1\\over 2})$ implies an explicit construction\nof a three-colored Ramsey graph on $p$ nodes with clique sizes bounded by\n$O(\\log^2 p)$ -- a question in the extremal combinatorics which has been open\nfor decades.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 04:09:10 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 04:02:56 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Gamarnik", "David", ""]]}, {"id": "1805.11401", "submitter": "James Tucker", "authors": "J. Derek Tucker, John R. Lewis, Caleb King, and Sebastian Kurtek", "title": "A Geometric Approach for Computing Tolerance Bounds for Elastic\n  Functional Data", "comments": "25 pages", "journal-ref": null, "doi": "10.1080/02664763.2019.1645818", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for constructing tolerance bounds for functional data\nwith random warping variability. In particular, we define a generative,\nprobabilistic model for the amplitude and phase components of such\nobservations, which parsimoniously characterizes variability in the baseline\ndata. Based on the proposed model, we define two different types of tolerance\nbounds that are able to measure both types of variability, and as a result,\nidentify when the data has gone beyond the bounds of amplitude and/or phase.\nThe first functional tolerance bounds are computed via a bootstrap procedure on\nthe geometric space of amplitude and phase functions. The second functional\ntolerance bounds utilize functional Principal Component Analysis to construct a\ntolerance factor. This work is motivated by two main applications: process\ncontrol and disease monitoring. The problem of statistical analysis and\nmodeling of functional data in process control is important in determining when\na production has moved beyond a baseline. Similarly, in biomedical\napplications, doctors use long, approximately periodic signals (such as the\nelectrocardiogram) to diagnose and monitor diseases. In this context, it is\ndesirable to identify abnormalities in these signals. We additionally consider\na simulated example to assess our approach and compare it to two existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 12:53:48 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 16:45:24 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Tucker", "J. Derek", ""], ["Lewis", "John R.", ""], ["King", "Caleb", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1805.11793", "submitter": "Shouri Hu", "authors": "Hock Peng Chan and Shouri Hu", "title": "Infinite Arms Bandit: Optimality via Confidence Bounds", "comments": "Fourth version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Berry et al. (1997) initiated the development of the infinite arms bandit\nproblem. They derived a regret lower bound of all allocation strategies for\nBernoulli rewards with uniform priors, and proposed strategies based on success\nruns. Bonald and Prouti\\`{e}re (2013) proposed a two-target algorithm that\nachieves the regret lower bound, and extended optimality to Bernoulli rewards\nwith general priors. We present here a confidence bound target (CBT) algorithm\nthat achieves optimality for rewards that are bounded above. For each arm we\nconstruct a confidence bound and compare it against each other and a target\nvalue to determine if the arm should be sampled further. The target value\ndepends on the assumed priors of the arm means. In the absence of information\non the prior, the target value is determined empirically. Numerical studies\nhere show that CBT is versatile and outperforms its competitors.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 03:45:17 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 08:25:03 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 03:43:47 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2020 03:14:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chan", "Hock Peng", ""], ["Hu", "Shouri", ""]]}, {"id": "1805.11800", "submitter": "Kai Rothauge", "authors": "Alex Gittens, Kai Rothauge, Shusen Wang, Michael W. Mahoney, Lisa\n  Gerhardt, Prabhat, Jey Kottalam, Michael Ringenburg, Kristyn Maschhoff", "title": "Accelerating Large-Scale Data Analysis by Offloading to High-Performance\n  Computing Libraries using Alchemist", "comments": "Accepted for publication in Proceedings of the 24th ACM SIGKDD\n  International Conference on Knowledge Discovery and Data Mining, London, UK,\n  2018", "journal-ref": null, "doi": "10.1145/3219819.3219927", "report-no": null, "categories": "cs.DC cs.DB physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Spark is a popular system aimed at the analysis of large data sets,\nbut recent studies have shown that certain computations---in particular, many\nlinear algebra computations that are the basis for solving common machine\nlearning problems---are significantly slower in Spark than when done using\nlibraries written in a high-performance computing framework such as the\nMessage-Passing Interface (MPI).\n  To remedy this, we introduce Alchemist, a system designed to call MPI-based\nlibraries from Apache Spark. Using Alchemist with Spark helps accelerate linear\nalgebra, machine learning, and related computations, while still retaining the\nbenefits of working within the Spark environment. We discuss the motivation\nbehind the development of Alchemist, and we provide a brief overview of its\ndesign and implementation.\n  We also compare the performances of pure Spark implementations with those of\nSpark implementations that leverage MPI-based codes via Alchemist. To do so, we\nuse data science case studies: a large-scale application of the conjugate\ngradient method to solve very large linear systems arising in a speech\nclassification problem, where we see an improvement of an order of magnitude;\nand the truncated singular value decomposition (SVD) of a 400GB\nthree-dimensional ocean temperature data set, where we see a speedup of up to\n7.9x. We also illustrate that the truncated SVD computation is easily scalable\nto terabyte-sized data by applying it to data sets of sizes up to 17.6TB.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 04:23:41 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Gittens", "Alex", ""], ["Rothauge", "Kai", ""], ["Wang", "Shusen", ""], ["Mahoney", "Michael W.", ""], ["Gerhardt", "Lisa", ""], ["Prabhat", "", ""], ["Kottalam", "Jey", ""], ["Ringenburg", "Michael", ""], ["Maschhoff", "Kristyn", ""]]}, {"id": "1805.12525", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang, Michael D. Shields", "title": "On the quantification and efficient propagation of imprecise\n  probabilities with copula dependence", "comments": "41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of quantification and propagation of\nuncertainties associated with dependence modeling when data for characterizing\nprobability models are limited. Practically, the system inputs are often\nassumed to be mutually independent or correlated by a multivariate Gaussian\ndistribution. However, this subjective assumption may introduce bias in the\nresponse estimate if the real dependence structure deviates from this\nassumption. In this work, we overcome this limitation by introducing a flexible\ncopula dependence model to capture complex dependencies. A hierarchical\nBayesian multimodel approach is proposed to quantify uncertainty in dependence\nmodel-form and model parameters that result from small data sets. This approach\nbegins by identifying, through Bayesian multimodel inference, a set of\ncandidate marginal models and their corresponding model probabilities, and then\nestimating the uncertainty in the copula-based dependence structure, which is\nconditional on the marginals and their parameters. The overall uncertainties\nintegrating marginals and copulas are probabilistically represented by an\nensemble of multivariate candidate densities. A novel importance sampling\nreweighting approach is proposed to efficiently propagate the overall\nuncertainties through a computational model. Through an example studying the\ninfluence of constituent properties on the out-of-plane properties of\ntransversely isotropic E- glass fiber composites, we show that the composite\nproperty with copula-based dependence model converges to the true estimate as\ndata set size increases, while an independence or arbitrary Gaussian\ncorrelation assumption leads to a biased estimate.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 15:54:48 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 15:18:31 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 18:07:19 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 19:42:55 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Shields", "Michael D.", ""]]}]