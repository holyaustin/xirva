[{"id": "1710.00095", "submitter": "Arnak Dalalyan S.", "authors": "Arnak S. Dalalyan and Avetik G. Karagulyan", "title": "User-friendly guarantees for the Langevin Monte Carlo with inaccurate\n  gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of sampling from a given probability\ndensity function that is known to be smooth and strongly log-concave. We\nanalyze several methods of approximate sampling based on discretizations of the\n(highly overdamped) Langevin diffusion and establish guarantees on its error\nmeasured in the Wasserstein-2 distance. Our guarantees improve or extend the\nstate-of-the-art results in three directions. First, we provide an upper bound\non the error of the first-order Langevin Monte Carlo (LMC) algorithm with\noptimized varying step-size. This result has the advantage of being horizon\nfree (we do not need to know in advance the target precision) and to improve by\na logarithmic factor the corresponding result for the constant step-size.\nSecond, we study the case where accurate evaluations of the gradient of the\nlog-density are unavailable, but one can have access to approximations of the\naforementioned gradient. In such a situation, we consider both deterministic\nand stochastic approximations of the gradient and provide an upper bound on the\nsampling error of the first-order LMC that quantifies the impact of the\ngradient evaluation inaccuracies. Third, we establish upper bounds for two\nversions of the second-order LMC, which leverage the Hessian of the\nlog-density. We nonasymptotic guarantees on the sampling error of these\nsecond-order LMCs. These guarantees reveal that the second-order LMC algorithms\nimprove on the first-order LMC in ill-conditioned settings.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 21:15:03 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 21:01:23 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 11:46:52 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Dalalyan", "Arnak S.", ""], ["Karagulyan", "Avetik G.", ""]]}, {"id": "1710.00473", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Youngjun Choe", "title": "Importance Sampling and its Optimality for Stochastic Simulation Models", "comments": "37 pages, 6 figures, 2 tables. Accepted to the Electronic Journal of\n  Statistics", "journal-ref": null, "doi": "10.1214/19-EJS1604", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an expected outcome from a stochastic\nsimulation model. Our goal is to develop a theoretical framework on importance\nsampling for such estimation. By investigating the variance of an importance\nsampling estimator, we propose a two-stage procedure that involves a regression\nstage and a sampling stage to construct the final estimator. We introduce a\nparametric and a nonparametric regression estimator in the first stage and\nstudy how the allocation between the two stages affects the performance of the\nfinal estimator. We analyze the variance reduction rates and derive oracle\nproperties of both methods. We evaluate the empirical performances of the\nmethods using two numerical examples and a case study on wind turbine\nreliability evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 03:53:30 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 18:56:25 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Choe", "Youngjun", ""]]}, {"id": "1710.00578", "submitter": "Jack Baker", "authors": "Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth", "title": "sgmcmc: An R Package for Stochastic Gradient Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the R package sgmcmc; which can be used for Bayesian\ninference on problems with large datasets using stochastic gradient Markov\nchain Monte Carlo (SGMCMC). Traditional Markov chain Monte Carlo (MCMC)\nmethods, such as Metropolis-Hastings, are known to run prohibitively slowly as\nthe dataset size increases. SGMCMC solves this issue by only using a subset of\ndata at each iteration. SGMCMC requires calculating gradients of the log\nlikelihood and log priors, which can be time consuming and error prone to\nperform by hand. The sgmcmc package calculates these gradients itself using\nautomatic differentiation, making the implementation of these methods much\neasier. To do this, the package uses the software library TensorFlow, which has\na variety of statistical distributions and mathematical operations as standard,\nmeaning a wide class of models can be built using this framework. SGMCMC has\nbecome widely adopted in the machine learning literature, but less so in the\nstatistics community. We believe this may be partly due to lack of software;\nthis package aims to bridge this gap.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 11:01:53 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 10:13:42 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 12:01:23 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Baker", "Jack", ""], ["Fearnhead", "Paul", ""], ["Fox", "Emily B.", ""], ["Nemeth", "Christopher", ""]]}, {"id": "1710.00596", "submitter": "Konstantinos Perrakis", "authors": "Konstantinos Perrakis, Sach Mukherjee and the Alzheimers Disease\n  Neuroimaging Initiative", "title": "Scalable Bayesian regression in high dimensions with multiple data\n  sources", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2019.1624294", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of high-dimensional regression often involve multiple sources or\ntypes of covariates. We propose methodology for this setting, emphasizing the\n\"wide data\" regime with large total dimensionality p and sample size n<<p. We\nfocus on a flexible ridge-type prior with shrinkage levels that are specific to\neach data type or source and that are set automatically by empirical Bayes. All\nestimation, including setting of shrinkage levels, is formulated mainly in\nterms of inner product matrices of size n x n. This renders computation\nefficient in the wide data regime and allows scaling to problems with millions\nof features. Furthermore, the proposed procedures are free of user-set tuning\nparameters. We show how sparsity can be achieved by post-processing of the\nBayesian output via constrained minimization of a certain Kullback-Leibler\ndivergence. This yields sparse solutions with adaptive, source-specific\nshrinkage, including a closed-form variant that scales to very large p. We\npresent empirical results from a simulation study based on real data and a case\nstudy in Alzheimer's disease involving millions of features and multiple data\nsources.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 12:00:23 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 14:09:30 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 12:36:54 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 10:23:39 GMT"}, {"version": "v5", "created": "Fri, 19 Jan 2018 14:28:24 GMT"}, {"version": "v6", "created": "Wed, 5 Jun 2019 08:44:31 GMT"}, {"version": "v7", "created": "Wed, 21 Aug 2019 12:00:29 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Perrakis", "Konstantinos", ""], ["Mukherjee", "Sach", ""], ["Initiative", "the Alzheimers Disease Neuroimaging", ""]]}, {"id": "1710.00873", "submitter": "Andressa Cerqueira", "authors": "Andressa Cerqueira, Aur\\'elien Garivier and Florencia Leonardi", "title": "A note on perfect simulation for exponential random graph models", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a perfect simulation algorithm for the Exponential\nRandom Graph Model, based on the Coupling From The Past method of Propp &\nWilson (1996). We use a Glauber dynamics to construct the Markov Chain and we\nprove the monotonicity of the ERGM for a subset of the parametric space. We\nalso obtain an upper bound on the running time of the algorithm that depends on\nthe mixing time of the Markov chain.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 19:19:21 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Cerqueira", "Andressa", ""], ["Garivier", "Aur\u00e9lien", ""], ["Leonardi", "Florencia", ""]]}, {"id": "1710.00959", "submitter": "Yajuan Si", "authors": "Susanna Makela, Yajuan Si and Andrew Gelman", "title": "Bayesian Inference under Cluster Sampling with Probability Proportional\n  to Size", "comments": null, "journal-ref": null, "doi": "10.1002/sim.7892", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster sampling is common in survey practice, and the corresponding\ninference has been predominantly design-based. We develop a Bayesian framework\nfor cluster sampling and account for the design effect in the outcome modeling.\nWe consider a two-stage cluster sampling design where the clusters are first\nselected with probability proportional to cluster size, and then units are\nrandomly sampled inside selected clusters. Challenges arise when the sizes of\nnonsampled cluster are unknown. We propose nonparametric and parametric\nBayesian approaches for predicting the unknown cluster sizes, with this\ninference performed simultaneously with the model for survey outcome.\nSimulation studies show that the integrated Bayesian approach outperforms\nclassical methods with efficiency gains. We use Stan for computing and apply\nthe proposal to the Fragile Families and Child Wellbeing study as an\nillustration of complex survey inference in health surveys.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 02:06:55 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Makela", "Susanna", ""], ["Si", "Yajuan", ""], ["Gelman", "Andrew", ""]]}, {"id": "1710.01054", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta, Bastien Chopard, Jonas L\\\"att, Frank Dubois, Karim\n  Zouaoui Boudjeltia and Antonietta Mira", "title": "Parameter estimation of platelets deposition: Approximate Bayesian\n  computation with high performance computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.CB q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show the existing clinical tests to detect\nCardio/cerebrovascular diseases (CVD) are ineffectual as they do not consider\ndifferent stages of platelet activation or the molecular dynamics involved in\nplatelet interactions. Further they are also incapable to consider\ninter-individual variability. A physical description of platelets deposition\nwas introduced recently in Chopard et. al. [2017], by integrating fundamental\nunderstandings of how platelets interact in a numerical model, parameterized by\nfive parameters. These parameters specify the deposition process and are\nrelevant for a biomedical understanding of the phenomena. One of the main\nintuition is that these parameters are precisely the information needed for a\npathological test identifying CVD captured and that they capture the\ninter-individual variability. Following this intuition, here we devise a\nBayesian inferential scheme for estimation of these parameters. As the\nlikelihood function of the numerical model is intractable due to the complex\nstochastic nature of the model, we use a likelihood-free inference scheme\napproximate Bayesian computation (ABC) to calibrate the parameters in a\ndata-driven manner. As ABC requires the generation of many pseudo-data by\nexpensive simulation runs, we use a high performance computing (HPC) framework\nfor ABC to make the inference possible for this model. We illustrate that our\nmean posterior prediction of platelet deposition pattern matches the\nexperimental dataset closely with a tight posterior prediction error margin for\na collective dataset of 7 volunteers. The present approach can be used to build\na new generation of personalized platelet functionality tests for CVD\ndetection, using numerical modeling of platelet deposition, Bayesian\nuncertainty quantification and High performance computing.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 09:52:18 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 12:39:28 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Chopard", "Bastien", ""], ["L\u00e4tt", "Jonas", ""], ["Dubois", "Frank", ""], ["Boudjeltia", "Karim Zouaoui", ""], ["Mira", "Antonietta", ""]]}, {"id": "1710.01057", "submitter": "Alexander Buchholz", "authors": "Alexander Buchholz, Nicolas Chopin", "title": "Improving approximate Bayesian computation via quasi-Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ABC (approximate Bayesian computation) is a general approach for dealing with\nmodels with an intractable likelihood. In this work, we derive ABC algorithms\nbased on QMC (quasi- Monte Carlo) sequences. We show that the resulting ABC\nestimates have a lower variance than their Monte Carlo counter-parts. We also\ndevelop QMC variants of sequential ABC algorithms, which progressively adapt\nthe proposal distribution and the acceptance threshold. We illustrate our QMC\napproach through several examples taken from the ABC literature. Keywords:\nApproximate Bayesian computation, Likelihood-free inference, Quasi Monte Carlo,\nRandomized Quasi-Monte Carlo, Adaptive importance sampling\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 10:09:07 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 12:45:06 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 14:17:26 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Buchholz", "Alexander", ""], ["Chopin", "Nicolas", ""]]}, {"id": "1710.01227", "submitter": "Igor Halperin", "authors": "Igor Halperin", "title": "Keep It Real: Tail Probabilities of Compound Heavy-Tailed Distributions", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an analytical approach to the computation of tail probabilities of\ncompound distributions whose individual components have heavy tails. Our\napproach is based on the contour integration method, and gives rise to a\nrepresentation of the tail probability of a compound distribution in the form\nof a rapidly convergent one-dimensional integral involving a discontinuity of\nthe imaginary part of its moment generating function across a branch cut. The\nlatter integral can be evaluated in quadratures, or alternatively represented\nas an asymptotic expansion. Our approach thus offers a viable (especially at\nhigh percentile levels) alternative to more standard methods such as Monte\nCarlo or the Fast Fourier Transform, traditionally used for such problems. As a\npractical application, we use our method to compute the operational Value at\nRisk (VAR) of a financial institution, where individual losses are modeled as\nspliced distributions whose large loss components are given by power-law or\nlognormal distributions. Finally, we briefly discuss extensions of the present\nformalism for calculation of tail probabilities of compound distributions made\nof compound distributions with heavy tails.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 15:50:48 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Halperin", "Igor", ""]]}, {"id": "1710.01434", "submitter": "Ming Teng", "authors": "Ming Teng, Farouk S. Nathoo, Timothy D. Johnson", "title": "Bayesian Analysis of fMRI data with Spatially-Varying Autoregressive\n  Orders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling of fMRI data is challenging as the data are both\nspatially and temporally correlated. Spatially, measurements are taken at\nthousands of contiguous regions, called voxels, and temporally measurements are\ntaken at hundreds of time points at each voxel. Recent advances in Bayesian\nhierarchical modeling have addressed the challenges of spatiotemproal structure\nin fMRI data with models incorporating both spatial and temporal priors for\nsignal and noise. While there has been extensive research on modeling the fMRI\nsignal (i.e., the covolution of the experimental design with the functional\nchoice for the hemodynamic response function) and its spatial variability, less\nattention has been paid to realistic modeling of the temporal dependence that\ntypically exists within the fMRI noise, where a low order autoregressive\nprocess is typically adopted. Furthermore, the AR order is held constant across\nvoxels (e.g. AR(1) at each voxel). Motivated by an event-related fMRI\nexperiment, we propose a novel hierarchical Bayesian model with automatic\nselection of the autoregressive orders of the noise process that vary spatially\nover the brain. With simulation studies we show that our model has improved\naccuracy and apply it to our motivating example.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 01:32:10 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Teng", "Ming", ""], ["Nathoo", "Farouk S.", ""], ["Johnson", "Timothy D.", ""]]}, {"id": "1710.02223", "submitter": "Michael Crowther", "authors": "Michael J. Crowther", "title": "Extended multivariate generalised linear and non-linear mixed effects\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate data occurs in a wide range of fields, with ever more flexible\nmodel specifications being proposed, often within a multivariate generalised\nlinear mixed effects (MGLME) framework. In this article, we describe an\nextended framework, encompassing multiple outcomes of any type, each of which\ncould be repeatedly measured (longitudinal), with any number of levels, and\nwith any number of random effects at each level. Many standard distributions\nare described, as well as non-standard user-defined non-linear models. The\nextension focuses on a complex linear predictor for each outcome model,\nallowing sharing and linking between outcome models in an extremely flexible\nway, either by linking random effects directly, or the expected value of one\noutcome (or function of it) within the linear predictor of another. Non-linear\nand time-dependent effects are also seamlessly incorporated to the linear\npredictor through the use of splines or fractional polynomials. We further\npropose level-specific random effect distributions and numerical integration\ntechniques to improve usability, relaxing the normally distributed random\neffects assumption to allow multivariate $t$-distributed random effects. We\nconsider some special cases of the general framework, describing some new\nmodels in the fields of clustered survival data, joint longitudinal-survival\nmodels, and discuss various potential uses of the implementation. User\nfriendly, and easily extendable, software is provided.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 21:27:43 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Crowther", "Michael J.", ""]]}, {"id": "1710.02333", "submitter": "Bruno Ebner", "authors": "Bruno Ebner, Norbert Henze, Michael A. Klatt and Klaus Mecke", "title": "Goodness-of-fit tests for complete spatial randomness based on Minkowski\n  functionals of binary images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of goodness-of-fit tests for complete spatial randomness\n(CSR). In contrast to standard tests, our procedure utilizes a transformation\nof the data to a binary image, which is then characterized by geometric\nfunctionals. Under a suitable limiting regime, we derive the asymptotic\ndistribution of the test statistics under the null hypothesis and almost sure\nlimits under certain alternatives. The new tests are computationally efficient,\nand simulations show that they are strong competitors to other tests of CSR.\nThe tests are applied to a real data set in gamma-ray astronomy, and immediate\nextensions are presented to encourage further work.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 09:59:58 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Ebner", "Bruno", ""], ["Henze", "Norbert", ""], ["Klatt", "Michael A.", ""], ["Mecke", "Klaus", ""]]}, {"id": "1710.02385", "submitter": "Timo Adam", "authors": "Timo Adam, Andreas Mayr, Thomas Kneib", "title": "Gradient boosting in Markov-switching generalized additive models for\n  location, scale and shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel class of flexible latent-state time series regression\nmodels which we call Markov-switching generalized additive models for location,\nscale and shape. In contrast to conventional Markov-switching regression\nmodels, the presented methodology allows us to model different state-dependent\nparameters of the response distribution - not only the mean, but also variance,\nskewness and kurtosis parameters - as potentially smooth functions of a given\nset of explanatory variables. In addition, the set of possible distributions\nthat can be specified for the response is not limited to the exponential family\nbut additionally includes, for instance, a variety of Box-Cox-transformed,\nzero-inflated and mixture distributions. We propose an estimation approach\nbased on the EM algorithm, where we use the gradient boosting framework to\nprevent overfitting while simultaneously performing variable selection. The\nfeasibility of the suggested approach is assessed in simulation experiments and\nillustrated in a real-data setting, where we model the conditional distribution\nof the daily average price of energy in Spain over time.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 13:05:19 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 12:28:45 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Adam", "Timo", ""], ["Mayr", "Andreas", ""], ["Kneib", "Thomas", ""]]}, {"id": "1710.02588", "submitter": "Y. Samuel Wang", "authors": "Y. Samuel Wang, Mathias Drton", "title": "Empirical Likelihood for Linear Structural Equation Models with\n  Dependent Errors", "comments": "6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider linear structural equation models that are associated with mixed\ngraphs. The structural equations in these models only involve observed\nvariables, but their idiosyncratic error terms are allowed to be correlated and\nnon-Gaussian. We propose empirical likelihood (EL) procedures for inference,\nand suggest several modifications, including a profile likelihood, in order to\nimprove tractability and performance of the resulting methods. Through\nsimulations, we show that when the error distributions are non-Gaussian, the\nuse of EL and the proposed modifications may increase statistical efficiency\nand improve assessment of significance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 21:17:14 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Wang", "Y. Samuel", ""], ["Drton", "Mathias", ""]]}, {"id": "1710.02669", "submitter": "Jerzy Rydlewski", "authors": "Daniel Kosiorowski, Dominik Mielczarek, Jerzy P. Rydlewski", "title": "Aggregated moving functional median in robust prediction of hierarchical\n  functional time series - an application to forecasting web portal users\n  behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a new nonparametric and robust method of forecasting\nhierarchical functional time series is presented. The method is compared with\nHyndman and Shang's method with respect to their unbiasedness, effectiveness,\nrobustness, and computational complexity. Taking into account results of the\nanalytical, simulation and empirical studies, we come to the conclusion that\nour proposal is superior over the proposal of Hyndman and Shang with respect to\nsome statistical criteria and especially with respect to robustness and\ncomputational complexity. An empirical usefulness of our method is presented on\nexample of management of a certain web portal divided into four subservices. An\nextensive simulation study involving hierarchical systems consisted of FAR(1)\nprocesses and Wiener processes has been conducted as well.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 11:00:46 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 10:59:19 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Mielczarek", "Dominik", ""], ["Rydlewski", "Jerzy P.", ""]]}, {"id": "1710.02786", "submitter": "Carter Butts", "authors": "Carter T. Butts", "title": "A Perfect Sampling Method for Exponential Family Random Graph Models", "comments": "To appear in the Journal of Mathematical Sociology (accepted version)", "journal-ref": "Journal of Mathematical Sociology, 42(1), 17-36 (2018)", "doi": "10.1080/0022250X.2017.1396985", "report-no": null, "categories": "stat.CO cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generation of deviates from random graph models with non-trivial edge\ndependence is an increasingly important problem. Here, we introduce a method\nwhich allows perfect sampling from random graph models in exponential family\nform (\"exponential family random graph\" models), using a variant of Coupling\nFrom The Past. We illustrate the use of the method via an application to the\nMarkov graphs, a family that has been the subject of considerable research. We\nalso show how the method can be applied to a variant of the biased net models,\nwhich are not exponentially parameterized.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 06:00:49 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 12:17:27 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Butts", "Carter T.", ""]]}, {"id": "1710.02944", "submitter": "Zhongwen Liang", "authors": "Zhongwen Liang", "title": "A Unified Approach on the Local Power of Panel Unit Root Tests", "comments": "67 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a unified approach is proposed to derive the exact local\nasymptotic power for panel unit root tests, which is one of the most important\nissues in nonstationary panel data literature. Two most widely used panel unit\nroot tests known as Levin-Lin-Chu (LLC, Levin, Lin and Chu (2002)) and\nIm-Pesaran-Shin (IPS, Im, Pesaran and Shin (2003)) tests are systematically\nstudied for various situations to illustrate our method. Our approach is\ncharacteristic function based, and can be used directly in deriving the moments\nof the asymptotic distributions of these test statistics under the null and the\nlocal-to-unity alternatives. For the LLC test, the approach provides an\nalternative way to obtain the results that can be derived by the existing\nmethod. For the IPS test, the new results are obtained, which fills the gap in\nthe literature where few results exist, since the IPS test is non-admissible.\nMoreover, our approach has the advantage in deriving Edgeworth expansions of\nthese tests, which are also given in the paper. The simulations are presented\nto illustrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 05:57:06 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Liang", "Zhongwen", ""]]}, {"id": "1710.03127", "submitter": "Michael Grayling", "authors": "Michael Grayling, James Wason, Adrian Mander", "title": "Group Sequential Clinical Trial Designs for Normally Distributed Outcome\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a group sequential clinical trial, accumulated data are analysed at\nnumerous time-points in order to allow early decisions about a hypothesis of\ninterest. These designs have historically been recommended for their ethical,\nadministrative and economic benefits. In this work, we discuss a collection of\nnew Stata commands for computing the stopping boundaries and required group\nsize of various classical group sequential designs, assuming a normally\ndistributed outcome variable. Following this, we demonstrate how the\nperformance of several designs can be compared graphically.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 14:51:10 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 09:09:35 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Grayling", "Michael", ""], ["Wason", "James", ""], ["Mander", "Adrian", ""]]}, {"id": "1710.03133", "submitter": "Christopher Drovandi Dr", "authors": "Christopher C Drovandi, David J Nott and Daniel E Pagendam", "title": "A Semi-Automatic Method for History Matching using Sequential Monte\n  Carlo", "comments": "Accepted by SIAM/ASA Journal on Uncertainty Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the history matching method is to locate non-implausible regions\nof the parameter space of complex deterministic or stochastic models by\nmatching model outputs with data. It does this via a series of waves where at\neach wave an emulator is fitted to a small number of training samples. An\nimplausibility measure is defined which takes into account the closeness of\nsimulated and observed outputs as well as emulator uncertainty. As the waves\nprogress, the emulator becomes more accurate so that training samples are more\nconcentrated on promising regions of the space and poorer parts of the space\nare rejected with more confidence. Whilst history matching has proved to be\nuseful, existing implementations are not fully automated and some ad-hoc\nchoices are made during the process, which involves user intervention and is\ntime consuming. This occurs especially when the non-implausible region becomes\nsmall and it is difficult to sample this space uniformly to generate new\ntraining points. In this article we develop a sequential Monte Carlo (SMC)\nalgorithm for implementing history matching that is semi-automated. Our novel\nSMC approach reveals that the history matching method yields a non-implausible\nregion that can be multi-modal, highly irregular and very difficult to sample\nuniformly. Our SMC approach offers a much more reliable sampling of the\nnon-implausible space, which requires additional computation compared to other\napproaches used in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 15:02:01 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 12:42:50 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Drovandi", "Christopher C", ""], ["Nott", "David J", ""], ["Pagendam", "Daniel E", ""]]}, {"id": "1710.03157", "submitter": "Collin Erickson", "authors": "Collin B. Erickson and Bruce E. Ankenman and Susan M. Sanchez", "title": "Comparison of Gaussian process modeling software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process fitting, or kriging, is often used to create a model from a\nset of data. Many available software packages do this, but we show that very\ndifferent results can be obtained from different packages even when using the\nsame data and model. We describe the parameterization, features, and\noptimization used by eight different fitting packages that run on four\ndifferent platforms. We then compare these eight packages using various data\nfunctions and data sets, revealing that there are stark differences between the\npackages. In addition to comparing the prediction accuracy, the predictive\nvariance--which is important for evaluating precision of predictions and is\noften used in stopping criteria--is also evaluated.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 15:47:44 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Erickson", "Collin B.", ""], ["Ankenman", "Bruce E.", ""], ["Sanchez", "Susan M.", ""]]}, {"id": "1710.03206", "submitter": "Mickael Binois", "authors": "Mickael Binois and Jiangeng Huang and Robert B Gramacy and Mike\n  Ludkovski", "title": "Replication or exploration? Sequential design for stochastic simulation\n  experiments", "comments": "34 pages, 9 figures", "journal-ref": null, "doi": "10.1080/00401706.2018.1469433", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the merits of replication, and provide methods for optimal\ndesign (including replicates), with the goal of obtaining globally accurate\nemulation of noisy computer simulation experiments. We first show that\nreplication can be beneficial from both design and computational perspectives,\nin the context of Gaussian process surrogate modeling. We then develop a\nlookahead based sequential design scheme that can determine if a new run should\nbe at an existing input location (i.e., replicate) or at a new one (explore).\nWhen paired with a newly developed heteroskedastic Gaussian process model, our\ndynamic design scheme facilitates learning of signal and noise relationships\nwhich can vary throughout the input space. We show that it does so efficiently,\non both computational and statistical grounds. In addition to illustrative\nsynthetic examples, we demonstrate performance on two challenging real-data\nsimulation experiments, from inventory management and epidemiology.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 17:38:04 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 14:58:59 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Binois", "Mickael", ""], ["Huang", "Jiangeng", ""], ["Gramacy", "Robert B", ""], ["Ludkovski", "Mike", ""]]}, {"id": "1710.03266", "submitter": "Yun Yang", "authors": "Yun Yang and Debdeep Pati and Anirban Bhattacharya", "title": "$\\alpha$-Variational Inference with Statistical Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of variational approximations to Bayesian posterior\ndistributions, called $\\alpha$-VB, with provable statistical guarantees. The\nstandard variational approximation is a special case of $\\alpha$-VB with\n$\\alpha=1$. When $\\alpha \\in(0,1]$, a novel class of variational inequalities\nare developed for linking the Bayes risk under the variational approximation to\nthe objective function in the variational optimization problem, implying that\nmaximizing the evidence lower bound in variational inference has the effect of\nminimizing the Bayes risk within the variational density family. Operating in a\nfrequentist setup, the variational inequalities imply that point estimates\nconstructed from the $\\alpha$-VB procedure converge at an optimal rate to the\ntrue parameter in a wide range of problems. We illustrate our general theory\nwith a number of examples, including the mean-field variational approximation\nto (low)-high-dimensional Bayesian linear regression with spike and slab\npriors, mixture of Gaussian models, latent Dirichlet allocation, and (mixture\nof) Gaussian variational approximation in regular parametric models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 19:10:14 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 20:00:48 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Yang", "Yun", ""], ["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1710.03294", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang, Michael D. Shields", "title": "The effect of prior probabilities on quantification and propagation of\n  imprecise probabilities resulting from small datasets", "comments": "36 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.cma.2018.01.045", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines a methodology for Bayesian multimodel uncertainty\nquantification (UQ) and propagation and presents an investigation into the\neffect of prior probabilities on the resulting uncertainties. The UQ\nmethodology is adapted from the information-theoretic method previously\npresented by the authors (Zhang and Shields, 2018) to a fully Bayesian\nconstruction that enables greater flexibility in quantifying uncertainty in\nprobability model form. Being Bayesian in nature and rooted in UQ from small\ndatasets, prior probabilities in both probability model form and model\nparameters are shown to have a significant impact on quantified uncertainties\nand, consequently, on the uncertainties propagated through a physics-based\nmodel. These effects are specifically investigated for a simplified plate\nbuckling problem with uncertainties in material properties derived from a small\nnumber of experiments using noninformative priors and priors derived from past\nstudies of varying appropriateness. It is illustrated that prior probabilities\ncan have a significant impact on multimodel UQ for small datasets and\ninappropriate (but seemingly reasonable) priors may even have lingering effects\nthat bias probabilities even for large datasets. When applied to uncertainty\npropagation, this may result in probability bounds on response quantities that\ndo not include the true probabilities.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 20:08:48 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 16:16:12 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Shields", "Michael D.", ""]]}, {"id": "1710.04093", "submitter": "Florian Maire", "authors": "Aidan Boland, Nial Friel, Florian Maire", "title": "Efficient MCMC for Gibbs Random Fields using pre-computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference of Gibbs random fields (GRFs) is often referred to as a\ndoubly intractable problem, since the likelihood function is intractable. The\nexploration of the posterior distribution of such models is typically carried\nout with a sophisticated Markov chain Monte Carlo (MCMC) method, the exchange\nalgorithm (Murray et al., 2006), which requires simulations from the likelihood\nfunction at each iteration. The purpose of this paper is to consider an\napproach to dramatically reduce this computational overhead. To this end we\nintroduce a novel class of algorithms which use realizations of the GRF model,\nsimulated offline, at locations specified by a grid that spans the parameter\nspace. This strategy speeds up dramatically the posterior inference, as\nillustrated on several examples. However, using the pre-computed graphs\nintroduces a noise in the MCMC algorithm, which is no longer exact. We study\nthe theoretical behaviour of the resulting approximate MCMC algorithm and\nderive convergence bounds using a recent theoretical development on approximate\nMCMC methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 14:37:05 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 09:49:40 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Boland", "Aidan", ""], ["Friel", "Nial", ""], ["Maire", "Florian", ""]]}, {"id": "1710.04382", "submitter": "Richard Everitt", "authors": "Richard G. Everitt and Dennis Prangle and Philip Maybank and Mark Bell", "title": "Marginal sequential Monte Carlo for doubly intractable models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for models that have an intractable partition function is\nknown as a doubly intractable problem, where standard Monte Carlo methods are\nnot applicable. The past decade has seen the development of auxiliary variable\nMonte Carlo techniques (M{\\o}ller et al., 2006; Murray et al., 2006) for\ntackling this problem; these approaches being members of the more general class\nof pseudo-marginal, or exact-approximate, Monte Carlo algorithms (Andrieu and\nRoberts, 2009), which make use of unbiased estimates of intractable posteriors.\nEveritt et al. (2017) investigated the use of exact-approximate importance\nsampling (IS) and sequential Monte Carlo (SMC) in doubly intractable problems,\nbut focussed only on SMC algorithms that used data-point tempering. This paper\ndescribes SMC samplers that may use alternative sequences of distributions, and\ndescribes ways in which likelihood estimates may be improved adaptively as the\nalgorithm progresses, building on ideas from Moores et al. (2015). This\napproach is compared with a number of alternative algorithms for doubly\nintractable problems, including approximate Bayesian computation (ABC), which\nwe show is closely related to the method of M{\\o}ller et al. (2006).\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 06:36:14 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Everitt", "Richard G.", ""], ["Prangle", "Dennis", ""], ["Maybank", "Philip", ""], ["Bell", "Mark", ""]]}, {"id": "1710.04451", "submitter": "Yetkin Tua\\c{c}", "authors": "Yetkin Tua\\c{c}, Ye\\c{s}im G\\\"uney Birdal \\c{S}eno\\u{g}lu and Olcay\n  Arslan", "title": "Robust Parameter Estimation of Regression Model with AR(p) Error Terms", "comments": null, "journal-ref": null, "doi": "10.1080/03610918.2017.1343839", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a linear regression model with AR(p) error terms\nwith the assumption that the error terms have a t distribution as a heavy\ntailed alternative to the normal distribution. We obtain the estimators for the\nmodel parameters by using the conditional maximum likelihood (CML) method. We\nconduct an iteratively reweighting algorithm (IRA) to find the estimates for\nthe parameters of interest. We provide a simulation study and three real data\nexamples to illustrate the performance of the proposed robust estimators based\non t distribution.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 11:14:10 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Tua\u00e7", "Yetkin", ""], ["\u015eeno\u011flu", "Ye\u015fim G\u00fcney Birdal", ""], ["Arslan", "Olcay", ""]]}, {"id": "1710.04465", "submitter": "Claudio Fantacci", "authors": "Claudio Fantacci, Giulia Vezzani, Ugo Pattacini, Vadim Tikhanoff and\n  Lorenzo Natale", "title": "Markerless visual servoing on unknown objects for humanoid robot\n  platforms", "comments": null, "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA),\n  2018", "doi": "10.1109/ICRA.2018.8462914", "report-no": null, "categories": "cs.RO cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To precisely reach for an object with a humanoid robot, it is of central\nimportance to have good knowledge of both end-effector, object pose and shape.\nIn this work we propose a framework for markerless visual servoing on unknown\nobjects, which is divided in four main parts: I) a least-squares minimization\nproblem is formulated to find the volume of the object graspable by the robot's\nhand using its stereo vision; II) a recursive Bayesian filtering technique,\nbased on Sequential Monte Carlo (SMC) filtering, estimates the 6D pose\n(position and orientation) of the robot's end-effector without the use of\nmarkers; III) a nonlinear constrained optimization problem is formulated to\ncompute the desired graspable pose about the object; IV) an image-based visual\nservo control commands the robot's end-effector toward the desired pose. We\ndemonstrate effectiveness and robustness of our approach with extensive\nexperiments on the iCub humanoid robot platform, achieving real-time\ncomputation, smooth trajectories and sub-pixel precisions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 12:02:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Fantacci", "Claudio", ""], ["Vezzani", "Giulia", ""], ["Pattacini", "Ugo", ""], ["Tikhanoff", "Vadim", ""], ["Natale", "Lorenzo", ""]]}, {"id": "1710.04556", "submitter": "Alain Celisse", "authors": "Alain Celisse (LPP, MODAL), Guillemette Marot (MODAL, CERIM), Morgane\n  Pierre-Jean (LaMME), Guillem Rigaill (URGV)", "title": "New efficient algorithms for multiple change-point detection with\n  kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several statistical approaches based on reproducing kernels have been\nproposed to detect abrupt changes arising in the full distribution of the\nobservations and not only in the mean or variance. Some of these approaches\nenjoy good statistical properties (oracle inequality, \\ldots). Nonetheless,\nthey have a high computational cost both in terms of time and memory. This\nmakes their application difficult even for small and medium sample sizes ($n<\n10^4$). This computational issue is addressed by first describing a new\nefficient and exact algorithm for kernel multiple change-point detection with\nan improved worst-case complexity that is quadratic in time and linear in\nspace. It allows dealing with medium size signals (up to $n \\approx 10^5$).\nSecond, a faster but approximation algorithm is described. It is based on a\nlow-rank approximation to the Gram matrix. It is linear in time and space. This\napproximation algorithm can be applied to large-scale signals ($n \\geq 10^6$).\nThese exact and approximation algorithms have been implemented in \\texttt{R}\nand \\texttt{C} for various kernels. The computational and statistical\nperformances of these new algorithms have been assessed through empirical\nexperiments. The runtime of the new algorithms is observed to be faster than\nthat of other considered procedures. Finally, simulations confirmed the higher\nstatistical accuracy of kernel-based approaches to detect changes that are not\nonly in the mean. These simulations also illustrate the flexibility of\nkernel-based approaches to analyze complex biological profiles made of DNA copy\nnumber and allele B frequencies. An R package implementing the approach will be\nmade available on github.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 15:08:52 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Celisse", "Alain", "", "LPP, MODAL"], ["Marot", "Guillemette", "", "MODAL, CERIM"], ["Pierre-Jean", "Morgane", "", "LaMME"], ["Rigaill", "Guillem", "", "URGV"]]}, {"id": "1710.04586", "submitter": "Nikolas Kantas", "authors": "Francesc Pons Llopis, Nikolas Kantas, Alexandros Beskos, Ajay Jasra", "title": "Particle Filtering for Stochastic Navier-Stokes Signal Observed with\n  Linear Additive Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-linear filtering problem, whereby the signal obeys the\nstochastic Navier-Stokes equations and is observed through a linear mapping\nwith additive noise. The setup is relevant to data assimilation for numerical\nweather prediction and climate modelling, where similar models are used for\nunknown ocean or wind velocities. We present a particle filtering methodology\nthat uses likelihood informed importance proposals, adaptive tempering, and a\nsmall number of appropriate Markov Chain Monte Carlo steps. We provide a\ndetailed design for each of these steps and show in our numerical examples that\nthey are all crucial in terms of achieving good performance and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 16:11:41 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 11:08:59 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Llopis", "Francesc Pons", ""], ["Kantas", "Nikolas", ""], ["Beskos", "Alexandros", ""], ["Jasra", "Ajay", ""]]}, {"id": "1710.04948", "submitter": "Luca Martino", "authors": "Luca Martino", "title": "Parsimonious Adaptive Rejection Sampling", "comments": "Related Matlab code can be found at\n  http://www.lucamartino.altervista.org/PARS_CODE_v1.zip", "journal-ref": "IET Electronics Letters , Volume 53, Issue 16, Pages: 1115-1117,\n  2017", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) methods have become very popular in signal processing during\nthe past decades. The adaptive rejection sampling (ARS) algorithms are\nwell-known MC technique which draw efficiently independent samples from\nunivariate target densities. The ARS schemes yield a sequence of proposal\nfunctions that converge toward the target, so that the probability of accepting\na sample approaches one. However, sampling from the proposal pdf becomes more\ncomputationally demanding each time it is updated. We propose the Parsimonious\nAdaptive Rejection Sampling (PARS) method, where an efficient trade-off between\nacceptance rate and proposal complexity is obtained. Thus, the resulting\nalgorithm is faster than the standard ARS approach.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 14:50:54 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Martino", "Luca", ""]]}, {"id": "1710.04977", "submitter": "Theodore  Kypraios", "authors": "Muteb Alharthi, Theodore Kypraios and Philip D. O'Neill", "title": "Bayes factors for partially observed stochastic epidemic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of model choice for stochastic epidemic models given\npartial observation of a disease outbreak through time. Our main focus is on\nthe use of Bayes factors. Although Bayes factors have appeared in the epidemic\nmodelling literature before, they can be hard to compute and little attention\nhas been given to fundamental questions concerning their utility. In this paper\nwe derive analytic expressions for Bayes factors given complete observation\nthrough time, which suggest practical guidelines for model choice problems. We\nextend the power posterior method for computing Bayes factors so as to account\nfor missing data and apply this approach to partially observed epidemics. For\ncomparison, we also explore the use of a deviance information criterion for\nmissing data scenarios. The methods are illustrated via examples involving both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 15:55:21 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Alharthi", "Muteb", ""], ["Kypraios", "Theodore", ""], ["O'Neill", "Philip D.", ""]]}, {"id": "1710.05053", "submitter": "Trevor Campbell", "authors": "Trevor Campbell, Tamara Broderick", "title": "Automated Scalable Bayesian Inference via Hilbert Coresets", "comments": null, "journal-ref": "Journal of Machine Learning Research 20(15):1-38, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automation of posterior inference in Bayesian data analysis has enabled\nexperts and nonexperts alike to use more sophisticated models, engage in faster\nexploratory modeling and analysis, and ensure experimental reproducibility.\nHowever, standard automated posterior inference algorithms are not tractable at\nthe scale of massive modern datasets, and modifications to make them so are\ntypically model-specific, require expert tuning, and can break theoretical\nguarantees on inferential quality. Building on the Bayesian coresets framework,\nthis work instead takes advantage of data redundancy to shrink the dataset\nitself as a preprocessing step, providing fully-automated, scalable Bayesian\ninference with theoretical guarantees. We begin with an intuitive reformulation\nof Bayesian coreset construction as sparse vector sum approximation, and\ndemonstrate that its automation and performance-based shortcomings arise from\nthe use of the supremum norm. To address these shortcomings we develop Hilbert\ncoresets, i.e., Bayesian coresets constructed under a norm induced by an\ninner-product on the log-likelihood function space. We propose two Hilbert\ncoreset construction algorithms---one based on importance sampling, and one\nbased on the Frank-Wolfe algorithm---along with theoretical guarantees on\napproximation quality as a function of coreset size. Since the exact\ncomputation of the proposed inner-products is model-specific, we automate the\nconstruction with a random finite-dimensional projection of the log-likelihood\nfunctions. The resulting automated coreset construction algorithm is simple to\nimplement, and experiments on a variety of models with real and synthetic\ndatasets show that it provides high-quality posterior approximations and a\nsignificant reduction in the computational cost of inference.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 19:13:40 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 08:59:59 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "1710.05407", "submitter": "Yohan Petetin", "authors": "Roland Lamberti, Yohan Petetin, Fran\\c{c}ois Desbouvries and\n  Fran\\c{c}ois Septier", "title": "Semi-independent resampling for particle filtering", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2017.2775150", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among Sequential Monte Carlo (SMC) methods,Sampling Importance Resampling\n(SIR) algorithms are based on Importance Sampling (IS) and on some\nresampling-based)rejuvenation algorithm which aims at fighting against weight\ndegeneracy. However %whichever the resampling technique used this mechanism\ntends to be insufficient when applied to informative or high-dimensional\nmodels. In this paper we revisit the rejuvenation mechanism and propose a class\nof parameterized SIR-based solutions which enable to adjust the tradeoff\nbetween computational cost and statistical performances.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 21:46:04 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Lamberti", "Roland", ""], ["Petetin", "Yohan", ""], ["Desbouvries", "Fran\u00e7ois", ""], ["Septier", "Fran\u00e7ois", ""]]}, {"id": "1710.05513", "submitter": "Ziping Zhao", "authors": "Ziping Zhao and Daniel P. Palomar", "title": "Robust Maximum Likelihood Estimation of Sparse Vector Error Correction\n  Model", "comments": "5 pages, 3 figures, to appear in Proc. of the 2017 5th IEEE Global\n  Conference on Signal and Information Processing (GlobalSIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA q-fin.ST stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In econometrics and finance, the vector error correction model (VECM) is an\nimportant time series model for cointegration analysis, which is used to\nestimate the long-run equilibrium variable relationships. The traditional\nanalysis and estimation methodologies assume the underlying Gaussian\ndistribution but, in practice, heavy-tailed data and outliers can lead to the\ninapplicability of these methods. In this paper, we propose a robust model\nestimation method based on the Cauchy distribution to tackle this issue. In\naddition, sparse cointegration relations are considered to realize feature\nselection and dimension reduction. An efficient algorithm based on the\nmajorization-minimization (MM) method is applied to solve the proposed\nnonconvex problem. The performance of this algorithm is shown through numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 05:38:27 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhao", "Ziping", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1710.06033", "submitter": "Hiroshi Haramoto", "authors": "Hiroshi Haramoto, Makoto Matsumoto", "title": "Checking the Quality of Approximation of $p$-values in Statistical Tests\n  for Random Number Generators by Using a Three-Level Test", "comments": "18 pages", "journal-ref": null, "doi": "10.1016/j.matcom.2018.08.005", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical tests of pseudorandom number generators (PRNGs) are applicable to\nany type of random number generators and are indispensable for evaluation.\nWhile several practical packages for statistical tests of randomness exist,\nthey may suffer from a lack of reliability: for some tests, the amount of\napproximation error can be deemed significant. Reducing this error by finding a\nbetter approximation is necessary, but it generally requires an enormous amount\nof effort. In this paper, we introduce an experimental method for revealing\ndefects in statistical tests by using a three-level test proposed by Okutomi\nand Nakamura. In particular, we investigate the NIST test suite and the test\nbatteries in TestU01, which are widely used statistical packages. Furthermore,\nwe show the efficiency of several modifications for some tests.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 23:56:56 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 07:39:10 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Haramoto", "Hiroshi", ""], ["Matsumoto", "Makoto", ""]]}, {"id": "1710.06068", "submitter": "David W. Hogg", "authors": "David W. Hogg (Flatiron) (NYU) (MPIA) and Daniel Foreman-Mackey\n  (Flatiron) (UW)", "title": "Data analysis recipes: Using Markov Chain Monte Carlo", "comments": "A purely pedagogical contribution", "journal-ref": null, "doi": "10.3847/1538-4365/aab76e", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods for sampling probability density\nfunctions (combined with abundant computational resources) have transformed the\nsciences, especially in performing probabilistic inferences, or fitting models\nto data. In this primarily pedagogical contribution, we give a brief overview\nof the most basic MCMC method and some practical advice for the use of MCMC in\nreal inference problems. We give advice on method choice, tuning for\nperformance, methods for initialization, tests of convergence, troubleshooting,\nand use of the chain output to produce or report parameter estimates with\nassociated uncertainties. We argue that autocorrelation time is the most\nimportant test for convergence, as it directly connects to the uncertainty on\nthe sampling estimate of any quantity of interest. We emphasize that sampling\nis a method for doing integrals; this guides our thinking about how MCMC output\nis best used.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 03:03:13 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Hogg", "David W.", "", "Flatiron"], ["Foreman-Mackey", "Daniel", "", "Flatiron"]]}, {"id": "1710.06096", "submitter": "Ricky Fok", "authors": "Ricky Fok, Aijun An, and Xiaogang Wang", "title": "Spontaneous Symmetry Breaking in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework to understand the unprecedented performance and\nrobustness of deep neural networks using field theory. Correlations between the\nweights within the same layer can be described by symmetries in that layer, and\nnetworks generalize better if such symmetries are broken to reduce the\nredundancies of the weights. Using a two parameter field theory, we find that\nthe network can break such symmetries itself towards the end of training in a\nprocess commonly known in physics as spontaneous symmetry breaking. This\ncorresponds to a network generalizing itself without any user input layers to\nbreak the symmetry, but by communication with adjacent layers. In the layer\ndecoupling limit applicable to residual networks (He et al., 2015), we show\nthat the remnant symmetries that survive the non-linear layers are\nspontaneously broken. The Lagrangian for the non-linear and weight layers\ntogether has striking similarities with the one in quantum field theory of a\nscalar. Using results from quantum field theory we show that our framework is\nable to explain many experimentally observed phenomena,such as training on\nrandom labels with zero error (Zhang et al., 2017), the information bottleneck,\nthe phase transition out of it and gradient variance explosion (Shwartz-Ziv &\nTishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 04:55:14 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Fok", "Ricky", ""], ["An", "Aijun", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1710.06382", "submitter": "Jerry Chee", "authors": "Jerry Chee and Panos Toulis", "title": "Convergence diagnostics for stochastic gradient descent with constant\n  step size", "comments": "Accepted to Artificial Intelligence and Statistics, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many iterative procedures in stochastic optimization exhibit a transient\nphase followed by a stationary phase. During the transient phase the procedure\nconverges towards a region of interest, and during the stationary phase the\nprocedure oscillates in that region, commonly around a single point. In this\npaper, we develop a statistical diagnostic test to detect such phase transition\nin the context of stochastic gradient descent with constant learning rate. We\npresent theory and experiments suggesting that the region where the proposed\ndiagnostic is activated coincides with the convergence region. For a class of\nloss functions, we derive a closed-form solution describing such region.\nFinally, we suggest an application to speed up convergence of stochastic\ngradient descent by halving the learning rate each time stationarity is\ndetected. This leads to a new variant of stochastic gradient descent, which in\nmany settings is comparable to state-of-art.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 16:51:16 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 04:31:07 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Chee", "Jerry", ""], ["Toulis", "Panos", ""]]}, {"id": "1710.06591", "submitter": "Joshua Brown", "authors": "Joshua Brown, Terry Bossomaier, Lionel Barnett", "title": "Review of Data Structures for Computationally Efficient\n  Nearest-Neighbour Entropy Estimators for Large Systems with Periodic Boundary\n  Conditions", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theoretic quantities are extremely useful in discovering\nrelationships between two or more data sets. One popular method---particularly\nfor continuous systems---for estimating these quantities is the nearest\nneighbour estimators. When system sizes are very large or the systems have\nperiodic boundary conditions issues with performance and correctness surface,\nhowever solutions are known for each problem. Here we show that these solutions\nare inappropriate in systems that simultaneously contain both features and\ndiscuss a lesser known alternative solution involving Vantage Point trees that\nis capable of addressing both issues.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 06:22:15 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Brown", "Joshua", ""], ["Bossomaier", "Terry", ""], ["Barnett", "Lionel", ""]]}, {"id": "1710.06642", "submitter": "Frederik Beaujean", "authors": "Frederik Beaujean, Allen Caldwell and Olaf Reimann", "title": "Is the bump significant? An axion-search example", "comments": "18 pages, 8 figures. v2 fixes arxiv's parsing of the URL in the\n  abstract", "journal-ref": null, "doi": "10.1140/epjc/s10052-018-6217-y", "report-no": null, "categories": "hep-ex stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many experiments in physics involve searching for a localized excess over\nbackground expectations in an observed spectrum. If the background is known and\nthere is Gaussian noise, the amount of excess of successive observations can be\nquantified by the runs statistic taking care of the look-elsewhere effect. The\ndistribution of the runs statistic under the background model is known\nanalytically but the computation becomes too expensive for more than about a\nhundred observations. This work demonstrates a principled high-precision\nextrapolation from a few dozen up to millions of data points. It is most\nprecise in the interesting regime when an excess is present. The method is\nverified for benchmark cases and successfully applied to real data from an\naxion search. The code that implements our method is available at\nhttps://github.com/fredRos/runs .\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 09:31:36 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 12:56:00 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Beaujean", "Frederik", ""], ["Caldwell", "Allen", ""], ["Reimann", "Olaf", ""]]}, {"id": "1710.06965", "submitter": "Art Owen", "authors": "Art B. Owen and Yury Maximov and Michael Chertkov", "title": "Importance sampling the union of rare events with an application to\n  power systems analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider importance sampling to estimate the probability $\\mu$ of a union\nof $J$ rare events $H_j$ defined by a random variable $\\boldsymbol{x}$. The\nsampler we study has been used in spatial statistics, genomics and\ncombinatorics going back at least to Karp and Luby (1983). It works by sampling\none event at random, then sampling $\\boldsymbol{x}$ conditionally on that event\nhappening and it constructs an unbiased estimate of $\\mu$ by multiplying an\ninverse moment of the number of occuring events by the union bound. We prove\nsome variance bounds for this sampler. For a sample size of $n$, it has a\nvariance no larger than $\\mu(\\bar\\mu-\\mu)/n$ where $\\bar\\mu$ is the union\nbound. It also has a coefficient of variation no larger than\n$\\sqrt{(J+J^{-1}-2)/(4n)}$ regardless of the overlap pattern among the $J$\nevents. Our motivating problem comes from power system reliability, where the\nphase differences between connected nodes have a joint Gaussian distribution\nand the $J$ rare events arise from unacceptably large phase differences. In the\ngrid reliability problems even some events defined by $5772$ constraints in\n$326$ dimensions, with probability below $10^{-22}$, are estimated with a\ncoefficient of variation of about $0.0024$ with only $n=10{,}000$ sample\nvalues.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 00:09:36 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 01:35:32 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 09:39:26 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 00:41:21 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Owen", "Art B.", ""], ["Maximov", "Yury", ""], ["Chertkov", "Michael", ""]]}, {"id": "1710.07457", "submitter": "Remi Flamary", "authors": "Nicolas Courty, R\\'emi Flamary, M\\'elanie Ducoffe", "title": "Learning Wasserstein Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wasserstein distance received a lot of attention recently in the\ncommunity of machine learning, especially for its principled way of comparing\ndistributions. It has found numerous applications in several hard problems,\nsuch as domain adaptation, dimensionality reduction or generative models.\nHowever, its use is still limited by a heavy computational cost. Our goal is to\nalleviate this problem by providing an approximation mechanism that allows to\nbreak its inherent complexity. It relies on the search of an embedding where\nthe Euclidean distance mimics the Wasserstein distance. We show that such an\nembedding can be found with a siamese architecture associated with a decoder\nnetwork that allows to move from the embedding space back to the original input\nspace. Once this embedding has been found, computing optimization problems in\nthe Wasserstein space (e.g. barycenters, principal directions or even\narchetypes) can be conducted extremely fast. Numerical experiments supporting\nthis idea are conducted on image datasets, and show the wide potential benefits\nof our method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 09:09:34 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Courty", "Nicolas", ""], ["Flamary", "R\u00e9mi", ""], ["Ducoffe", "M\u00e9lanie", ""]]}, {"id": "1710.07693", "submitter": "Ricardo Ehlers", "authors": "Rafael S. Paix\\~ao, Ricardo S. Ehlers", "title": "Zero Variance and Hamiltonian Monte Carlo Methods in GARCH Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop Bayesian Hamiltonian Monte Carlo methods for\ninference in asymmetric GARCH models under different distributions for the\nerror term. We implemented Zero-variance and Hamiltonian Monte Carlo schemes\nfor parameter estimation to try and reduce the standard errors of the estimates\nthus obtaing more efficient results at the price of a small extra computational\ncost.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 20:02:15 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Paix\u00e3o", "Rafael S.", ""], ["Ehlers", "Ricardo S.", ""]]}, {"id": "1710.07702", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos, Zachary Kaplan, Thabo Samakhoana, Daniel\n  Sanz-Alonso", "title": "On the Consistency of Graph-based Bayesian Learning and the Scalability\n  of Sampling Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to semi-supervised learning proceeds by endowing the input\ndata with a graph structure in order to extract geometric information and\nincorporate it into a Bayesian framework. We introduce new theory that gives\nappropriate scalings of graph parameters that provably lead to a well-defined\nlimiting posterior as the size of the unlabeled data set grows. Furthermore, we\nshow that these consistency results have profound algorithmic implications.\nWhen consistency holds, carefully designed graph-based Markov chain Monte Carlo\nalgorithms are proved to have a uniform spectral gap, independent of the number\nof unlabeled inputs. Several numerical experiments corroborate both the\nstatistical consistency and the algorithmic scalability established by the\ntheory.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 20:57:14 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 16:26:30 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Kaplan", "Zachary", ""], ["Samakhoana", "Thabo", ""], ["Sanz-Alonso", "Daniel", ""]]}, {"id": "1710.07849", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Yanli Wang, Gurong Wu", "title": "Heat Kernel Smoothing in Irregular Image Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the discrete version of heat kernel smoothing on graph data\nstructure. The method is used to smooth data in an irregularly shaped domains\nin 3D images.\n  New statistical properties are derived. As an application, we show how to\nfilter out data in the lung blood vessel trees obtained from computed\ntomography. The method can be further used in representing the complex vessel\ntrees parametrically and extracting the skeleton representation of the trees.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 19:53:36 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chung", "Moo K.", ""], ["Wang", "Yanli", ""], ["Wu", "Gurong", ""]]}, {"id": "1710.08072", "submitter": "Pramudita Satria Palar Dr.", "authors": "Pramudita Satria Palar, Lavi Rizki Zuhal, Koji Shimoyama, Takeshi\n  Tsuchiya", "title": "Global Sensitivity Analysis via Multi-Fidelity Polynomial Chaos\n  Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of uncertainties are inevitable in engineering design and\nanalysis, where failure in understanding their effects might lead to the\nstructural or functional failure of the systems. The role of global sensitivity\nanalysis in this aspect is to quantify and rank the effects of input random\nvariables and their combinations to the variance of the random output. In\nproblems where the use of expensive computer simulations are required,\nmetamodels are widely used to speed up the process of global sensitivity\nanalysis. In this paper, a multi-fidelity framework for global sensitivity\nanalysis using polynomial chaos expansion (PCE) is presented. The goal is to\naccelerate the computation of Sobol sensitivity indices when the deterministic\nsimulation is expensive and simulations with multiple levels of fidelity are\navailable. This is especially useful in cases where a partial differential\nequation solver computer code is utilized to solve engineering problems. The\nmulti-fidelity PCE is constructed by combining the low-fidelity and correction\nPCE. Following this step, the Sobol indices are computed using this combined\nPCE. The PCE coefficients for both low-fidelity and correction PCE are computed\nwith spectral projection technique and sparse grid integration. In order to\ndemonstrate the capability of the proposed method for sensitivity analysis,\nseveral simulations are conducted. On the aerodynamic example, the\nmulti-fidelity approach is able to obtain an accurate value of Sobol indices\nwith 36.66% computational cost compared to the standard single-fidelity PCE for\na nearly similar accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 02:51:31 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Palar", "Pramudita Satria", ""], ["Zuhal", "Lavi Rizki", ""], ["Shimoyama", "Koji", ""], ["Tsuchiya", "Takeshi", ""]]}, {"id": "1710.08162", "submitter": "Quentin Frederik Gronau", "authors": "Quentin F. Gronau, Henrik Singmann, and Eric-Jan Wagenmakers", "title": "bridgesampling: An R Package for Estimating Normalizing Constants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical procedures such as Bayes factor model selection and Bayesian\nmodel averaging require the computation of normalizing constants (e.g.,\nmarginal likelihoods). These normalizing constants are notoriously difficult to\nobtain, as they usually involve high-dimensional integrals that cannot be\nsolved analytically. Here we introduce an R package that uses bridge sampling\n(Meng & Wong, 1996; Meng & Schilling, 2002) to estimate normalizing constants\nin a generic and easy-to-use fashion. For models implemented in Stan, the\nestimation procedure is automatic. We illustrate the functionality of the\npackage with three examples.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 09:24:26 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 11:28:42 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 09:01:32 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Gronau", "Quentin F.", ""], ["Singmann", "Henrik", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1710.08273", "submitter": "Jelle Goeman", "authors": "Rosa Meijer, Thijmen Krebs, Aldo Solari, Jelle Goeman", "title": "A shortcut for Hommel's procedure in linearithmic time", "comments": "arXiv admin note: text overlap with arXiv:1611.06739", "journal-ref": null, "doi": "10.1002/bimj.201700316", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hommel's and Hochberg's procedures for familywise error control are both\nderived as shortcuts in a closed testing procedure with the Simes local test.\nHommel's shortcut is exact but takes quadratic time in the number of\nhypotheses. Hochberg's shortcut takes only linearithmic time, but is\nconservative. In this paper we present an exact shortcut in linearithmic time,\ncombining the strengths of both procedures. The novel shortcut also applies to\na robust variant of Hommel's procedure that does not require the assumption of\nthe Simes inequality.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 13:53:58 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Meijer", "Rosa", ""], ["Krebs", "Thijmen", ""], ["Solari", "Aldo", ""], ["Goeman", "Jelle", ""]]}, {"id": "1710.08511", "submitter": "Denisa Roberts", "authors": "Lucas Roberts and Denisa Roberts", "title": "An Expectation Maximization Framework for Yule-Simon Preferential\n  Attachment Models", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop an Expectation Maximization(EM) algorithm to\nestimate the parameter of a Yule-Simon distribution. The Yule-Simon\ndistribution exhibits the \"rich get richer\" effect whereby an 80-20 type of\nrule tends to dominate. These distributions are ubiquitous in industrial\nsettings. The EM algorithm presented provides both frequentist and Bayesian\nestimates of the $\\lambda$ parameter. By placing the estimation method within\nthe EM framework we are able to derive Standard errors of the resulting\nestimate. Additionally, we prove convergence of the Yule-Simon EM algorithm and\nstudy the rate of convergence. An explicit, closed form solution for the rate\nof convergence of the algorithm is given. Applications including graph node\ndegree distribution estimation are listed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 21:33:31 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 23:11:55 GMT"}, {"version": "v3", "created": "Sun, 16 Sep 2018 14:10:43 GMT"}, {"version": "v4", "created": "Sat, 14 Nov 2020 20:09:41 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Roberts", "Lucas", ""], ["Roberts", "Denisa", ""]]}, {"id": "1710.08583", "submitter": "Abdollah Safari", "authors": "Abdollah Safari, Rachel MacKay Altman and Thomas M. Loughin", "title": "Display advertising: Estimating conversion probability efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of online display advertising is to entice users to \"convert\" (i.e.,\ntake a pre-defined action such as making a purchase) after clicking on the ad.\nAn important measure of the value of an ad is the probability of conversion.\nThe focus of this paper is the development of a computationally efficient,\naccurate, and precise estimator of conversion probability. The challenges\nassociated with this estimation problem are the delays in observing conversions\nand the size of the data set (both number of observations and number of\npredictors). Two models have previously been considered as a basis for\nestimation: A logistic regression model and a joint model for observed\nconversion statuses and delay times. Fitting the former is simple, but ignoring\nthe delays in conversion leads to an under-estimate of conversion probability.\nOn the other hand, the latter is less biased but computationally expensive to\nfit. Our proposed estimator is a compromise between these two estimators. We\napply our results to a data set from Criteo, a commerce marketing company that\npersonalizes online display advertisements for users.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 02:46:23 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Safari", "Abdollah", ""], ["Altman", "Rachel MacKay", ""], ["Loughin", "Thomas M.", ""]]}, {"id": "1710.08846", "submitter": "Yunchuan Kong", "authors": "Yunchuan Kong and Xiaodan Fan", "title": "A Bayesian Method for Joint Clustering of Vectorial Data and Network\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model-based integrative method for clustering objects given\nboth vectorial data, which describes the feature of each object, and network\ndata, which indicates the similarity of connected objects. The proposed general\nmodel is able to cluster the two types of data simultaneously within one\nintegrative probabilistic model, while traditional methods can only handle one\ndata type or depend on transforming one data type to another. Bayesian\ninference of the clustering is conducted based on a Markov chain Monte Carlo\nalgorithm. A special case of the general model combining the Gaussian mixture\nmodel and the stochastic block model is extensively studied. We used both\nsynthetic data and real data to evaluate this new method and compare it with\nalternative methods. The results show that our simultaneous clustering method\nperforms much better. This improvement is due to the power of the model-based\nprobabilistic approach for efficiently integrating information.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 15:26:46 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Kong", "Yunchuan", ""], ["Fan", "Xiaodan", ""]]}, {"id": "1710.08976", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss, Wenlong Gong", "title": "A class of multi-resolution approximations for large spatial datasets", "comments": null, "journal-ref": "Statistica Sinica, 30(4), 2203-2226 (2020)", "doi": "10.1007/s13253-020-00401-7", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are popular and flexible models for spatial, temporal, and\nfunctional data, but they are computationally infeasible for large datasets. We\ndiscuss Gaussian-process approximations that use basis functions at multiple\nresolutions to achieve fast inference and that can (approximately) represent\nany spatial covariance structure. We consider two special cases of this\nmulti-resolution-approximation framework, a taper version and a\ndomain-partitioning (block) version. We describe theoretical properties and\ninference procedures, and study the computational complexity of the methods.\nNumerical comparisons and an application to satellite data are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 20:17:45 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 17:45:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Gong", "Wenlong", ""]]}, {"id": "1710.09308", "submitter": "Frederik Vissing Mikkelsen", "authors": "Frederik Vissing Mikkelsen and Niels Richard Hansen", "title": "Learning Large Scale Ordinary Differential Equation Systems", "comments": "55 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning large scale nonlinear ordinary differential equation (ODE) systems\nfrom data is known to be computationally and statistically challenging. We\npresent a framework together with the adaptive integral matching (AIM)\nalgorithm for learning polynomial or rational ODE systems with a sparse network\nstructure. The framework allows for time course data sampled from multiple\nenvironments representing e.g. different interventions or perturbations of the\nsystem. The algorithm AIM combines an initial penalised integral matching step\nwith an adapted least squares step based on solving the ODE numerically. The R\npackage episode implements AIM together with several other algorithms and is\navailable from CRAN. It is shown that AIM achieves state-of-the-art network\nrecovery for the in silico phosphoprotein abundance data from the eighth DREAM\nchallenge with an AUROC of 0.74, and it is demonstrated via a range of\nnumerical examples that AIM has good statistical properties while being\ncomputationally feasible even for large systems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 15:38:43 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 06:13:15 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Mikkelsen", "Frederik Vissing", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "1710.09486", "submitter": "Tshilidzi Marwala", "authors": "M. Sherri, I. Boulkaibet, T. Marwala, M. I. Friswell", "title": "A Differential Evaluation Markov Chain Monte Carlo algorithm for\n  Bayesian Model Updating", "comments": "To be published in the IMAC XXXVI, Florida, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of the Bayesian tools in system identification and model updating\nparadigms has been increased in the last ten years. Usually, the Bayesian\ntechniques can be implemented to incorporate the uncertainties associated with\nmeasurements as well as the prediction made by the finite element model (FEM)\ninto the FEM updating procedure. In this case, the posterior distribution\nfunction describes the uncertainty in the FE model prediction and the\nexperimental data. Due to the complexity of the modeled systems, the analytical\nsolution for the posterior distribution function may not exist. This leads to\nthe use of numerical methods, such as Markov Chain Monte Carlo techniques, to\nobtain approximate solutions for the posterior distribution function. In this\npaper, a Differential Evaluation Markov Chain Monte Carlo (DE-MC) method is\nused to approximate the posterior function and update FEMs. The main idea of\nthe DE-MC approach is to combine the Differential Evolution, which is an\neffective global optimization algorithm over real parameter space, with Markov\nChain Monte Carlo (MCMC) techniques to generate samples from the posterior\ndistribution function. In this paper, the DE-MC method is discussed in detail\nwhile the performance and the accuracy of this algorithm are investigated by\nupdating two structural examples.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 22:42:57 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Sherri", "M.", ""], ["Boulkaibet", "I.", ""], ["Marwala", "T.", ""], ["Friswell", "M. I.", ""]]}, {"id": "1710.09663", "submitter": "Jiwoong Kim", "authors": "Jiwoong Kim", "title": "A Fast Algorithm for Solving Henderson's Mixed Model Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates a fast and stable method to solve Henderson's mixed\nmodel equation. The proposed algorithm is stable in that it avoids inverting a\nmatrix of a large dimension and hence is free from the curse of dimensionality.\nThis tactic is enabled through row operations performed on the design matrix.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 12:24:12 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Kim", "Jiwoong", ""]]}, {"id": "1710.09707", "submitter": "Francesca Molinari", "authors": "Hiroaki Kaido, Francesca Molinari, J\\\"org Stoye and Matthew Thirkettle", "title": "Calibrated Projection in MATLAB: Users' Manual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the calibrated-projection MATLAB package implementing the method\nto construct confidence intervals proposed by Kaido, Molinari and Stoye (2017).\nThis manual provides details on how to use the package for inference on\nprojections of partially identified parameters. It also explains how to use the\nMATLAB functions we developed to compute confidence intervals on solutions of\nnonlinear optimization problems with estimated constraints.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 21:20:19 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Kaido", "Hiroaki", ""], ["Molinari", "Francesca", ""], ["Stoye", "J\u00f6rg", ""], ["Thirkettle", "Matthew", ""]]}, {"id": "1710.09759", "submitter": "Abhirup Mallik", "authors": "Abhirup Mallik and Galin L. Jones", "title": "Directional Metropolis-Hastings", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new kernel for Metropolis Hastings called Directional Metropolis\nHastings (DMH) with multivariate update where the proposal kernel has state\ndependent covariance matrix. We use the derivative of the target distribution\nat the current state to change the orientation of the proposal distribution,\ntherefore producing a more plausible proposal. We study the conditions for\ngeometric ergodicity of our algorithm and provide necessary and sufficient\nconditions for convergence. We also suggest a scheme for adaptively update the\nvariance parameter and study the conditions of ergodicity of the adaptive\nalgorithm. We demonstrate the performance of our algorithm in a Bayesian\ngeneralized linear model problem.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 15:33:58 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Mallik", "Abhirup", ""], ["Jones", "Galin L.", ""]]}, {"id": "1710.09890", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou", "title": "Bayesian Nonparametric Models for Biomedical Data Analysis", "comments": "PhD thesis, UT Austin, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation, we develop nonparametric Bayesian models for biomedical\ndata analysis. In particular, we focus on inference for tumor heterogeneity and\ninference for missing data. First, we present a Bayesian feature allocation\nmodel for tumor subclone reconstruction using mutation pairs. The key\ninnovation lies in the use of short reads mapped to pairs of proximal single\nnucleotide variants (SNVs). In contrast, most existing methods use only\nmarginal reads for unpaired SNVs. In the same context of using mutation pairs,\nin order to recover the phylogenetic relationship of subclones, we then develop\na Bayesian treed feature allocation model. In contrast to commonly used feature\nallocation models, we allow the latent features to be dependent, using a tree\nstructure to introduce dependence. Finally, we propose a nonparametric Bayesian\napproach to monotone missing data in longitudinal studies with non-ignorable\nmissingness. In contrast to most existing methods, our method allows for\nincorporating information from auxiliary covariates and is able to capture\ncomplex structures among the response, missingness and auxiliary covariates.\nOur models are validated through simulation studies and are applied to\nreal-world biomedical datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 19:39:36 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Zhou", "Tianjian", ""]]}, {"id": "1710.10261", "submitter": "Marko Obradovi\\'c", "authors": "Bojana Milo\\v{s}evi\\'c and Marko Obradovi\\'c", "title": "Comparison of Efficiencies of Symmetry Tests around Unknown Center", "comments": "The final version will appear in Statistics", "journal-ref": null, "doi": "10.1080/02331888.2018.1526938", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, some recent and classical tests of symmetry are modified for\nthe case of an unknown center.\n  The unknown center is estimated with its $\\alpha$-trimmed mean estimator.\n  The asymptotic behavior of the new tests is explored.\n  The local approximate Bahadur efficiency is used to compare the tests to each\nother as well as to some other tests.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 11:02:28 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 10:19:08 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Milo\u0161evi\u0107", "Bojana", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "1710.10489", "submitter": "Bin Liu", "authors": "Bin Liu", "title": "ILAPF: Incremental Learning Assisted Particle Filtering", "comments": "5 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with dynamic system state estimation based on a\nseries of noisy measurement with the presence of outliers. An incremental\nlearning assisted particle filtering (ILAPF) method is presented, which can\nlearn the value range of outliers incrementally during the process of particle\nfiltering. The learned range of outliers is then used to improve subsequent\nfiltering of the future state. Convergence of the outlier range estimation\nprocedure is indicated by extensive empirical simulations using a set of\ndiffering outlier distribution models. The validity of the ILAPF algorithm is\nevaluated by illustrative simulations, and the result shows that ILAPF is more\naccurate and faster than a recently published state-ofthe-art robust particle\nfilter. It also shows that the incremental learning property of the ILAPF\nalgorithm provides an efficient way to implement transfer learning among\nrelated state filtering tasks.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 16:11:45 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 09:53:58 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Liu", "Bin", ""]]}, {"id": "1710.10728", "submitter": "Chengyu Liu", "authors": "Chengyu Liu, Wei Wang", "title": "Contextual Regression: An Accurate and Conveniently Interpretable\n  Nonlinear Model for Mining Discovery from Scientific Data", "comments": "18 pages of Main Article, 30 pages of Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms such as linear regression, SVM and neural network\nhave played an increasingly important role in the process of scientific\ndiscovery. However, none of them is both interpretable and accurate on\nnonlinear datasets. Here we present contextual regression, a method that joins\nthese two desirable properties together using a hybrid architecture of neural\nnetwork embedding and dot product layer. We demonstrate its high prediction\naccuracy and sensitivity through the task of predictive feature selection on a\nsimulated dataset and the application of predicting open chromatin sites in the\nhuman genome. On the simulated data, our method achieved high fidelity recovery\nof feature contributions under random noise levels up to 200%. On the open\nchromatin dataset, the application of our method not only outperformed the\nstate of the art method in terms of accuracy, but also unveiled two previously\nunfound open chromatin related histone marks. Our method can fill the blank of\naccurate and interpretable nonlinear modeling in scientific data mining tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 00:39:47 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Liu", "Chengyu", ""], ["Wang", "Wei", ""]]}, {"id": "1710.10951", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai", "title": "SGDLibrary: A MATLAB library for stochastic gradient descent algorithms", "comments": null, "journal-ref": "Journal of Machine Learning Research, vol.18, no.215, pp.1-5, 2018", "doi": null, "report-no": null, "categories": "cs.MS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the minimizer of a function $f:\n\\mathbb{R}^d \\rightarrow \\mathbb{R}$ of the finite-sum form $\\min f(w) =\n1/n\\sum_{i}^n f_i(w)$. This problem has been studied intensively in recent\nyears in the field of machine learning (ML). One promising approach for\nlarge-scale data is to use a stochastic optimization algorithm to solve the\nproblem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library\nof a collection of stochastic optimization algorithms. The purpose of the\nlibrary is to provide researchers and implementers a comprehensive evaluation\nenvironment for the use of these algorithms on various ML problems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 07:42:06 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 23:32:35 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Kasai", "Hiroyuki", ""]]}, {"id": "1710.11616", "submitter": "Chang-Han Rhee", "authors": "Chang-Han Rhee, Enlu Zhou, Peng Qiu", "title": "Space-filling design for nonlinear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing a computer experiment can be viewed as observing a mapping between\nthe model parameters and the corresponding model outputs predicted by the\ncomputer model. In view of this, experimental design for computer experiments\ncan be thought of as devising a reliable procedure for finding configurations\nof design points in the parameter space so that their images represent the\nmanifold parametrized by such a mapping (i.e., computer experiments).\nTraditional space-filling designs aim to achieve this goal by filling the\nparameter space with design points that are as \"uniform\" as possible in the\nparameter space. However, the resulting design points may be non-uniform in the\nmodel output space and hence fail to provide a reliable representation of the\nmanifold, becoming highly inefficient or even misleading in case the computer\nexperiments are non-linear. In this paper, we propose an iterative algorithm\nthat fills in the model output manifold uniformly---rather than the parameter\nspace uniformly---so that one could obtain a reliable understanding of the\nmodel behaviors with the minimal number of design points.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:49:36 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 22:45:11 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Rhee", "Chang-Han", ""], ["Zhou", "Enlu", ""], ["Qiu", "Peng", ""]]}]