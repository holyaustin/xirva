[{"id": "0811.0174", "submitter": "Yaming Yu", "authors": "Yaming Yu", "title": "A Bit of Information Theory, and the Data Augmentation Algorithm\n  Converges", "comments": null, "journal-ref": "IEEE Transactions on Information Theory 54 (2008) 5186--5188", "doi": "10.1109/TIT.2008.929918", "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data augmentation (DA) algorithm is a simple and powerful tool in\nstatistical computing. In this note basic information theory is used to prove a\nnontrivial convergence theorem for the DA algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2008 16:22:53 GMT"}], "update_date": "2009-09-12", "authors_parsed": [["Yu", "Yaming", ""]]}, {"id": "0811.0528", "submitter": "Art B. Owen", "authors": "Art B. Owen", "title": "Local antithetic sampling with scrambled nets", "comments": "Published in at http://dx.doi.org/10.1214/07-AOS548 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2008, Vol. 36, No. 5, 2319-2343", "doi": "10.1214/07-AOS548", "report-no": "IMS-AOS-AOS548", "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing an approximation to the integral\n$I=\\int_{[0,1]^d}f(x) dx$. Monte Carlo (MC) sampling typically attains a root\nmean squared error (RMSE) of $O(n^{-1/2})$ from $n$ independent random function\nevaluations. By contrast, quasi-Monte Carlo (QMC) sampling using carefully\nequispaced evaluation points can attain the rate $O(n^{-1+\\varepsilon})$ for\nany $\\varepsilon>0$ and randomized QMC (RQMC) can attain the RMSE\n$O(n^{-3/2+\\varepsilon})$, both under mild conditions on $f$. Classical\nvariance reduction methods for MC can be adapted to QMC. Published results\ncombining QMC with importance sampling and with control variates have found\nworthwhile improvements, but no change in the error rate. This paper extends\nthe classical variance reduction method of antithetic sampling and combines it\nwith RQMC. One such method is shown to bring a modest improvement in the RMSE\nrate, attaining $O(n^{-3/2-1/d+\\varepsilon})$ for any $\\varepsilon>0$, for\nsmooth enough $f$.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 14:42:34 GMT"}], "update_date": "2008-11-05", "authors_parsed": [["Owen", "Art B.", ""]]}, {"id": "0811.2678", "submitter": "Robb Muirhead", "authors": "Morris L. Eaton, Robb J. Muirhead", "title": "The \"north pole problem\" and random orthogonal matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by the following observation. Take a 3 x 3 random\n(Haar distributed) orthogonal matrix $\\Gamma$, and use it to \"rotate\" the north\npole, $x_0$ say, on the unit sphere in $R^3$. This then gives a point $u=\\Gamma\nx_0$ that is uniformly distributed on the unit sphere. Now use the same\northogonal matrix to transform u, giving $v=\\Gamma u=\\Gamma^2 x_0$. Simulations\nreported in Marzetta et al (2002) suggest that v is more likely to be in the\nnorthern hemisphere than in the southern hemisphere, and, morever, that\n$w=\\Gamma^3 x_0$ has higher probability of being closer to the poles $\\pm x_0$\nthan the uniformly distributed point u. In this paper we prove these results,\nin the general setting of dimension $p\\ge 3$, by deriving the exact\ndistributions of the relevant components of u and v. The essential questions\nanswered are the following. Let x be any fixed point on the unit sphere in\n$R^p$, where $p\\ge 3$. What are the distributions of $U_2=x'\\Gamma^2 x$ and\n$U_3=x'\\Gamma^3 x$? It is clear by orthogonal invariance that these\ndistribution do not depend on x, so that we can, without loss of generality,\ntake x to be $x_0=(1,0,...,0)'\\in R^p$. Call this the \"north pole\". Then\n$x_0'\\Gamma^ k x_0$ is the first component of the vector $\\Gamma^k x_0$. We\nderive stochastic representations for the exact distributions of $U_2$ and\n$U_3$ in terms of random variables with known distributions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2008 20:50:30 GMT"}], "update_date": "2008-11-18", "authors_parsed": [["Eaton", "Morris L.", ""], ["Muirhead", "Robb J.", ""]]}, {"id": "0811.2843", "submitter": "Steven P. Ellis", "authors": "Steven P. Ellis", "title": "An Algorithm for Unconstrained Quadratically Penalized Convex\n  Optimization", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2008_330", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A descent algorithm, \"Quasi-Quadratic Minimization with Memory\" (QQMM), is\nproposed for unconstrained minimization of the sum, $F$, of a non-negative\nconvex function, $V$, and a quadratic form. Such problems come up in\nregularized estimation in machine learning and statistics. In addition to\nvalues of $F$, QQMM requires the (sub)gradient of $V$. Two features of QQMM\nhelp keep low the number of evaluations of the objective function it needs.\nFirst, QQMM provides good control over stopping the iterative search. This\nfeature makes QQMM well adapted to statistical problems because in such\nproblems the objective function is based on random data and therefore stopping\nearly is sensible. Secondly, QQMM uses a complex method for determining trial\nminimizers of $F$. After a description of the problem and algorithm a\nsimulation study comparing QQMM to the popular BFGS optimization algorithm is\ndescribed. The simulation study and other experiments suggest that QQMM is\ngenerally substantially faster than BFGS in the problem domain for which it was\ndesigned. A QQMM-BFGS hybrid is also generally substantially faster than BFGS\nbut does better than QQMM when QQMM is very slow.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2008 08:57:00 GMT"}], "update_date": "2008-11-19", "authors_parsed": [["Ellis", "Steven P.", ""]]}, {"id": "0811.3342", "submitter": "Elvira Di Nardo Ph.D.", "authors": "E. Di Nardo, I. Oliva", "title": "On the computation of classical, boolean and free cumulants", "comments": "14 pages. in press, Applied Mathematics and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple and computationally efficient algorithm for\nconversion formulae between moments and cumulants. The algorithm provides just\none formula for classical, boolean and free cumulants. This is realized by\nusing a suitable polynomial representation of Abel polynomials. The algorithm\nrelies on the classical umbral calculus, a symbolic language introduced by Rota\nand Taylor in 1994, that is particularly suited to be implemented by using\nsoftware for symbolic computations. Here we give a MAPLE procedure. Comparisons\nwith existing procedures, especially for conversions between moments and free\ncumulants, as well as examples of applications to some well-known distributions\n(classical and free) end the paper.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2008 14:21:35 GMT"}], "update_date": "2008-11-21", "authors_parsed": [["Di Nardo", "E.", ""], ["Oliva", "I.", ""]]}, {"id": "0811.3355", "submitter": "Richard Wilkinson", "authors": "Richard D. Wilkinson", "title": "Approximate Bayesian computation (ABC) gives exact results under the\n  assumption of model error", "comments": "33 pages, 1 figure, to appear in Statistical Applications in Genetics\n  and Molecular Biology 2013", "journal-ref": "Stat.App.Gen.Mol.Bio. 12 (2013) 129-141", "doi": "10.1515/sagmb-2013-0010", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) or likelihood-free inference\nalgorithms are used to find approximations to posterior distributions without\nmaking explicit use of the likelihood function, depending instead on simulation\nof sample data sets from the model. In this paper we show that under the\nassumption of the existence of a uniform additive model error term, ABC\nalgorithms give exact results when sufficient summaries are used. This\ninterpretation allows the approximation made in many previous application\npapers to be understood, and should guide the choice of metric and tolerance in\nfuture work. ABC algorithms can be generalized by replacing the 0-1 cut-off\nwith an acceptance probability that varies with the distance of the simulated\ndata from the observed data. The acceptance density gives the distribution of\nthe error term, enabling the uniform error usually used to be replaced by a\ngeneral distribution. This generalization can also be applied to approximate\nMarkov chain Monte Carlo algorithms. In light of this work, ABC algorithms can\nbe seen as calibration techniques for implicit stochastic models, inferring\nparameter values in light of the computer model, data, prior beliefs about the\nparameter values, and any measurement or model errors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2008 15:36:00 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2013 11:38:03 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Wilkinson", "Richard D.", ""]]}, {"id": "0811.4095", "submitter": "Matti Vihola", "authors": "Matti Vihola", "title": "Grapham: Graphical Models with Adaptive Random Walk Metropolis\n  Algorithms", "comments": "9 pages, 3 figures; added references, revised language, other minor\n  changes", "journal-ref": "Computational Statistics & Data Analysis 54 (2010) 49-54", "doi": "10.1016/j.csda.2009.09.001", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently developed adaptive Markov chain Monte Carlo (MCMC) methods have been\napplied successfully to many problems in Bayesian statistics. Grapham is a new\nopen source implementation covering several such methods, with emphasis on\ngraphical models for directed acyclic graphs. The implemented algorithms\ninclude the seminal Adaptive Metropolis algorithm adjusting the proposal\ncovariance according to the history of the chain and a Metropolis algorithm\nadjusting the proposal scale based on the observed acceptance probability.\nDifferent variants of the algorithms allow one, for example, to use these two\nalgorithms together, employ delayed rejection and adjust several parameters of\nthe algorithms. The implemented Metropolis-within-Gibbs update allows arbitrary\nsampling blocks. The software is written in C and uses a simple extension\nlanguage Lua in configuration.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2008 14:27:06 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2009 08:14:23 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2009 09:02:37 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Vihola", "Matti", ""]]}, {"id": "0811.4447", "submitter": "Nancy Zhang", "authors": "Hock Peng Chan, Nancy R. Zhang, and Louis H.Y. Chen", "title": "Importance Sampling of Word Patterns in DNA and Protein Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods can provide accurate p-value estimates of word counting\ntest statistics and are easy to implement. They are especially attractive when\nan asymptotic theory is absent or when either the search sequence or the word\npattern is too short for the application of asymptotic formulae. Naive direct\nMonte Carlo is undesirable for the estimation of small probabilities because\nthe associated rare events of interest are seldom generated. We propose instead\nefficient importance sampling algorithms that use controlled insertion of the\ndesired word patterns on randomly generated sequences. The implementation is\nillustrated on word patterns of biological interest: Palindromes and inverted\nrepeats, patterns arising from position specific weight matrices and\nco-occurrences of pairs of motifs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2008 22:36:19 GMT"}], "update_date": "2008-12-01", "authors_parsed": [["Chan", "Hock Peng", ""], ["Zhang", "Nancy R.", ""], ["Chen", "Louis H. Y.", ""]]}]