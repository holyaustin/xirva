[{"id": "1508.00219", "submitter": "Ali Akbar Jafari", "authors": "Ali Akbar Jafari and Rasool Roozegar", "title": "On Bivariate Generalized Exponential-Power Series Class of Distributions", "comments": "arXiv admin note: text overlap with arXiv:1507.07535", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new class of bivariate distributions by\ncompounding the bivariate generalized exponential and power-series\ndistributions.\n  This new class contains some new sub-models such as the bivariate generalized\nexponential distribution, the bivariate generalized exponential-poisson,\n-logarithmic, -binomial and -negative binomial distributions. We derive\ndifferent properties of the new class of distributions. The EM algorithm is\nused to determine the maximum likelihood estimates of the parameters. We\nillustrate the usefulness of the new distributions by means of an application\nto a real data set.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 11:06:33 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Jafari", "Ali Akbar", ""], ["Roozegar", "Rasool", ""]]}, {"id": "1508.00279", "submitter": "Florian Heinrichs", "authors": "Holger Dette, Roman Guchenko, Viatcheslav B. Melas", "title": "Efficient computation of Bayesian optimal discriminating designs", "comments": "Keyword and Phrases: Design of experiment; Bayesian optimal design;\n  model discrimination; gradient methods; model uncertainty; Kullback-Leibler\n  distance. arXiv admin note: text overlap with arXiv:1412.2548", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient algorithm for the determination of Bayesian optimal\ndiscriminating designs for competing regression models is developed, where the\nmain focus is on models with general distributional assumptions beyond the\n\"classical\" case of normally distributed homoscedastic errors. For this purpose\nwe consider a Bayesian version of the Kullback- Leibler (KL) optimality\ncriterion introduced by L\\'opez-Fidalgo et al. (2007). Discretizing the prior\ndistribution leads to local KL-optimal discriminating design problems for a\nlarge number of competing models. All currently available methods either\nrequire a large computation time or fail to calculate the optimal\ndiscriminating design, because they can only deal efficiently with a few model\ncomparisons. In this paper we develop a new algorithm for the determination of\nBayesian optimal discriminating designs with respect to the Kullback-Leibler\ncriterion. It is demonstrated that the new algorithm is able to calculate the\noptimal discriminating designs with reasonable accuracy and computational time\nin situations where all currently available procedures are either slow or fail.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 20:08:49 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Dette", "Holger", ""], ["Guchenko", "Roman", ""], ["Melas", "Viatcheslav B.", ""]]}, {"id": "1508.00286", "submitter": "Pierre Latouche", "authors": "Pierre Latouche, St\\'ephane Robin, Sarah Ouadah", "title": "Goodness of fit of logistic models for random graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is a natural and simple tool to understand how covariates\ncontribute to explain the topology of a binary network. Once the model fitted,\nthe practitioner is interested in the goodness-of-fit of the regression in\norder to check if the covariates are sufficient to explain the whole topology\nof the network and, if they are not, to analyze the residual structure. To\naddress this problem, we introduce a generic model that combines logistic\nregression with a network-oriented residual term. This residual term takes the\nform of the graphon function of a W-graph. Using a variational Bayes framework,\nwe infer the residual graphon by averaging over a series of blockwise constant\nfunctions. This approach allows us to define a generic goodness-of-fit\ncriterion, which corresponds to the posterior probability for the residual\ngraphon to be constant. Experiments on toy data are carried out to assess the\naccuracy of the procedure. Several networks from social sciences and ecology\nare studied to illustrate the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 21:02:32 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 14:21:58 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Latouche", "Pierre", ""], ["Robin", "St\u00e9phane", ""], ["Ouadah", "Sarah", ""]]}, {"id": "1508.00468", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "Evolutionary Algorithms: Concepts, Designs, and Applications in\n  Bioinformatics: Evolutionary Algorithms for Bioinformatics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.GN q-bio.QM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since genetic algorithm was proposed by John Holland (Holland J. H., 1975) in\nthe early 1970s, the study of evolutionary algorithm has emerged as a popular\nresearch field (Civicioglu & Besdok, 2013). Researchers from various scientific\nand engineering disciplines have been digging into this field, exploring the\nunique power of evolutionary algorithms (Hadka & Reed, 2013). Many applications\nhave been successfully proposed in the past twenty years. For example,\nmechanical design (Lampinen & Zelinka, 1999), electromagnetic optimization\n(Rahmat-Samii & Michielssen, 1999), environmental protection (Bertini, Felice,\nMoretti, & Pizzuti, 2010), finance (Larkin & Ryan, 2010), musical orchestration\n(Esling, Carpentier, & Agon, 2010), pipe routing (Furuholmen, Glette, Hovin, &\nTorresen, 2010), and nuclear reactor core design (Sacco, Henderson,\nRios-Coelho, Ali, & Pereira, 2009). In particular, its function optimization\ncapability was highlighted (Goldberg & Richardson, 1987) because of its high\nadaptability to different function landscapes, to which we cannot apply\ntraditional optimization techniques (Wong, Leung, & Wong, 2009). Here we review\nthe applications of evolutionary algorithms in bioinformatics.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 16:05:34 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Wong", "Ka-Chun", ""]]}, {"id": "1508.00635", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Bayesian mixtures of spatial spline regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work relates the framework of model-based clustering for spatial\nfunctional data where the data are surfaces. We first introduce a Bayesian\nspatial spline regression model with mixed-effects (BSSR) for modeling spatial\nfunction data. The BSSR model is based on Nodal basis functions for spatial\nregression and accommodates both common mean behavior for the data through a\nfixed-effects part, and variability inter-individuals thanks to a\nrandom-effects part. Then, in order to model populations of spatial functional\ndata issued from heterogeneous groups, we integrate the BSSR model into a\nmixture framework. The resulting model is a Bayesian mixture of spatial spline\nregressions with mixed-effects (BMSSR) used for density estimation and\nmodel-based surface clustering. The models, through their Bayesian formulation,\nallow to integrate possible prior knowledge on the data structure and\nconstitute a good alternative to recent mixture of spatial spline regressions\nmodel estimated in a maximum likelihood framework via the\nexpectation-maximization (EM) algorithm. The Bayesian model inference is\nperformed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbs\nsampler to infer the BSSR and the BMSSR models and apply them on simulated\nsurfaces and a real problem of handwritten digit recognition using the MNIST\ndata set. The obtained results highlight the potential benefit of the proposed\nBayesian approaches for modeling surfaces possibly dispersed in particular in\nclusters.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 01:29:49 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1508.00793", "submitter": "Konstantinos Perrakis", "authors": "Dimitris Fouskakis, Ioannis Ntzoufras, Konstantinos Perrakis", "title": "Power-Expected-Posterior Priors for Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power-expected-posterior (PEP) prior provides an objective, automatic,\nconsistent and parsimonious model selection procedure. At the same time it\nresolves the conceptual and computational problems due to the use of imaginary\ndata. Namely, (i) it dispenses with the need to select and average across all\npossible minimal imaginary samples, and (ii) it diminishes the effect that the\nimaginary data have upon the posterior distribution. These attributes allow for\nlarge sample approximations, when needed, in order to reduce the computational\nburden under more complex models. In this work we generalize the applicability\nof the PEP methodology, focusing on the framework of generalized linear models\n(GLMs), by introducing two new PEP definitions which are in effect applicable\nto any general model setting. Hyper-prior extensions for the power parameter\nthat regulates the contribution of the imaginary data are introduced. We\nfurther study the validity of the predictive matching and of the model\nselection consistency, providing analytical proofs for the former and empirical\nevidence supporting the latter. For estimation of posterior model and inclusion\nprobabilities we introduce a tuning-free Gibbs-based variable selection\nsampler. Several simulation scenarios and one real life example are considered\nin order to evaluate the performance of the proposed methods compared to other\ncommonly used approaches based on mixtures of g-priors. Results indicate that\nthe GLM-PEP priors are more effective in the identification of sparse and\nparsimonious model formulations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 15:05:23 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 12:34:22 GMT"}, {"version": "v3", "created": "Mon, 15 Aug 2016 13:54:40 GMT"}, {"version": "v4", "created": "Fri, 29 Sep 2017 16:58:46 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Fouskakis", "Dimitris", ""], ["Ntzoufras", "Ioannis", ""], ["Perrakis", "Konstantinos", ""]]}, {"id": "1508.01050", "submitter": "Maurizio Filippone", "authors": "Xiaoyu Xiong and V\\'aclav \\v{S}m\\'idl and Maurizio Filippone", "title": "Adaptive Multiple Importance Sampling for Gaussian Processes", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of Gaussian processes where quantification of uncertainty is\na strict requirement, it is necessary to accurately characterize the posterior\ndistribution over Gaussian process covariance parameters. Normally, this is\ndone by means of standard Markov chain Monte Carlo (MCMC) algorithms. Motivated\nby the issues related to the complexity of calculating the marginal likelihood\nthat can make MCMC algorithms inefficient, this paper develops an alternative\ninference framework based on Adaptive Multiple Importance Sampling (AMIS). This\npaper studies the application of AMIS in the case of a Gaussian likelihood, and\nproposes the Pseudo-Marginal AMIS for non-Gaussian likelihoods, where the\nmarginal likelihood is unbiasedly estimated. The results suggest that the\nproposed framework outperforms MCMC-based inference of covariance parameters in\na wide range of scenarios and remains competitive for moderately large\ndimensional parameter spaces.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 12:28:03 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 13:09:31 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Xiong", "Xiaoyu", ""], ["\u0160m\u00eddl", "V\u00e1clav", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1508.01126", "submitter": "Stanislav Volgushev", "authors": "Srijan Sengupta, Stanislav Volgushev, and Xiaofeng Shao", "title": "A subsampled double bootstrap for massive data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bootstrap is a popular and powerful method for assessing precision of\nestimators and inferential methods. However, for massive datasets which are\nincreasingly prevalent, the bootstrap becomes prohibitively costly in\ncomputation and its feasibility is questionable even with modern parallel\ncomputing platforms. Recently Kleiner, Talwalkar, Sarkar, and Jordan (2014)\nproposed a method called BLB (Bag of Little Bootstraps) for massive data which\nis more computationally scalable with little sacrifice of statistical accuracy.\nBuilding on BLB and the idea of fast double bootstrap, we propose a new\nresampling method, the subsampled double bootstrap, for both independent data\nand time series data. We establish consistency of the subsampled double\nbootstrap under mild conditions for both independent and dependent cases.\nMethodologically, the subsampled double bootstrap is superior to BLB in terms\nof running time, more sample coverage and automatic implementation with less\ntuning parameters for a given time budget. Its advantage relative to BLB and\nbootstrap is also demonstrated in numerical simulations and a data\nillustration.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 16:44:49 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Sengupta", "Srijan", ""], ["Volgushev", "Stanislav", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "1508.01132", "submitter": "Simon Cotter", "authors": "Colin Cotter, Simon Cotter, Paul Russell", "title": "Ensemble Transport Adaptive Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo methods are a powerful and commonly used family of\nnumerical methods for sampling from complex probability distributions. As\napplications of these methods increase in size and complexity, the need for\nefficient methods increases. In this paper, we present a particle ensemble\nalgorithm. At each iteration, an importance sampling proposal distribution is\nformed using an ensemble of particles. A stratified sample is taken from this\ndistribution and weighted under the posterior, a state-of-the-art ensemble\ntransport resampling method is then used to create an evenly weighted sample\nready for the next iteration. We demonstrate that this ensemble transport\nadaptive importance sampling (ETAIS) method outperforms MCMC methods with\nequivalent proposal distributions for low dimensional problems, and in fact\nshows better than linear improvements in convergence rates with respect to the\nnumber of ensemble members. We also introduce a new resampling strategy,\nmultinomial transformation (MT), which while not as accurate as the ensemble\ntransport resampler, is substantially less costly for large ensemble sizes, and\ncan then be used in conjunction with ETAIS for complex problems. We also focus\non how algorithmic parameters regarding the mixture proposal can be quickly\ntuned to optimise performance. In particular, we demonstrate this methodology's\nsuperior sampling for multimodal problems, such as those arising from inference\nfor mixture models, and for problems with expensive likelihoods requiring the\nsolution of a differential equation, for which speed-ups of orders of magnitude\nare demonstrated. Likelihood evaluations of the ensemble could be computed in a\ndistributed manner, suggesting that this methodology is a good candidate for\nparallel Bayesian computations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 17:08:59 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 17:05:31 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 15:09:40 GMT"}, {"version": "v4", "created": "Wed, 30 Jan 2019 15:47:33 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Cotter", "Colin", ""], ["Cotter", "Simon", ""], ["Russell", "Paul", ""]]}, {"id": "1508.01280", "submitter": "Zhou Fan", "authors": "Zhou Fan and Lester Mackey", "title": "Empirical Bayesian analysis of simultaneous changepoints in multiple\n  data sequences", "comments": "31 pages, 11 figures v3: Modify synthetic data comparisons based on\n  reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy number variations in cancer cells and volatility fluctuations in stock\nprices are commonly manifested as changepoints occurring at the same positions\nacross related data sequences. We introduce a Bayesian modeling framework,\nBASIC, that employs a changepoint prior to capture the co-occurrence tendency\nin data of this type. We design efficient algorithms to sample from and\nmaximize over the BASIC changepoint posterior and develop a Monte Carlo\nexpectation-maximization procedure to select prior hyperparameters in an\nempirical Bayes fashion. We use the resulting BASIC framework to analyze DNA\ncopy number variations in the NCI-60 cancer cell lines and to identify\nimportant events that affected the price volatility of S&P 500 stocks from 2000\nto 2009.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 04:42:37 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2016 00:48:27 GMT"}, {"version": "v3", "created": "Fri, 14 Apr 2017 03:08:27 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Fan", "Zhou", ""], ["Mackey", "Lester", ""]]}, {"id": "1508.01341", "submitter": "Aaron Spettl", "authors": "Aaron Spettl, Tim Brereton, Qibin Duan, Thomas Werz, Carl E. Krill\n  III, Dirk P. Kroese, Volker Schmidt", "title": "Fitting Laguerre tessellation approximations to tomographic image data", "comments": "27 pages, 10 figures, 2 tables", "journal-ref": "Philosophical Magazine 96 (2016), pp. 166-189", "doi": "10.1080/14786435.2015.1125540", "report-no": null, "categories": "cond-mat.mtrl-sci stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of polycrystalline materials benefits greatly from accurate\nquantitative descriptions of their grain structures. Laguerre tessellations\napproximate such grain structures very well. However, it is a quite challenging\nproblem to fit a Laguerre tessellation to tomographic data, as a\nhigh-dimensional optimization problem with many local minima must be solved. In\nthis paper, we formulate a version of this optimization problem that can be\nsolved quickly using the cross-entropy method, a robust stochastic optimization\ntechnique that can avoid becoming trapped in local minima. We demonstrate the\neffectiveness of our approach by applying it to both artificially generated and\nexperimentally produced tomographic data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 09:50:37 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 08:54:55 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Spettl", "Aaron", ""], ["Brereton", "Tim", ""], ["Duan", "Qibin", ""], ["Werz", "Thomas", ""], ["Krill", "Carl E.", "III"], ["Kroese", "Dirk P.", ""], ["Schmidt", "Volker", ""]]}, {"id": "1508.01681", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien, Tianwen Wei and Basad Ali Hussain Al-sarray", "title": "Joint estimation and model order selection for one dimensional ARMA\n  models via convex optimization: a nuclear norm penalization approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating ARMA models is computationally interesting due to\nthe nonconcavity of the log-likelihood function. Recent results were based on\nthe convex minimization. Joint model selection using penalization by a convex\nnorm, e.g. the nuclear norm of a certain matrix related to the state space\nformulation was extensively studied from a computational viewpoint. The goal of\nthe present short note is to present a theoretical study of a nuclear norm\npenalization based variant of the method of\n\\cite{Bauer:Automatica05,Bauer:EconTh05} under the assumption of a Gaussian\nnoise process.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 13:21:15 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Wei", "Tianwen", ""], ["Al-sarray", "Basad Ali Hussain", ""]]}, {"id": "1508.01922", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder and Peter Radchenko", "title": "The Discrete Dantzig Selector: Estimating Sparse Linear Models via Mixed\n  Integer Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel high-dimensional linear regression estimator: the Discrete\nDantzig Selector, which minimizes the number of nonzero regression coefficients\nsubject to a budget on the maximal absolute correlation between the features\nand residuals. Motivated by the significant advances in integer optimization\nover the past 10-15 years, we present a Mixed Integer Linear Optimization\n(MILO) approach to obtain certifiably optimal global solutions to this\nnonconvex optimization problem. The current state of algorithmics in integer\noptimization makes our proposal substantially more computationally attractive\nthan the least squares subset selection framework based on integer quadratic\noptimization, recently proposed in [8] and the continuous nonconvex quadratic\noptimization framework of [33]. We propose new discrete first-order methods,\nwhich when paired with state-of-the-art MILO solvers, lead to good solutions\nfor the Discrete Dantzig Selector problem for a given computational budget. We\nillustrate that our integrated approach provides globally optimal solutions in\nsignificantly shorter computation times, when compared to off-the-shelf MILO\nsolvers. We demonstrate both theoretically and empirically that in a wide range\nof regimes the statistical properties of the Discrete Dantzig Selector are\nsuperior to those of popular $\\ell_{1}$-based approaches. We illustrate that\nour approach can handle problem instances with p = 10,000 features with\ncertifiable optimality making it a highly scalable combinatorial variable\nselection approach in sparse linear modeling.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 16:13:07 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 00:52:56 GMT"}, {"version": "v3", "created": "Thu, 19 Jan 2017 05:48:12 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""]]}, {"id": "1508.02087", "submitter": "Philipp Moritz", "authors": "Philipp Moritz, Robert Nishihara, Michael I. Jordan", "title": "A Linearly-Convergent Stochastic L-BFGS Algorithm", "comments": "10 pages, 3 figures in International Conference on Artificial\n  Intelligence and Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new stochastic L-BFGS algorithm and prove a linear convergence\nrate for strongly convex and smooth functions. Our algorithm draws heavily from\na recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as\na recent approach to variance reduction for stochastic gradient descent from\nJohnson and Zhang (2013). We demonstrate experimentally that our algorithm\nperforms well on large-scale convex and non-convex optimization problems,\nexhibiting linear convergence and rapidly solving the optimization problems to\nhigh levels of precision. Furthermore, we show that our algorithm performs well\nfor a wide-range of step sizes, often differing by several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 21:40:33 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 23:36:06 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Moritz", "Philipp", ""], ["Nishihara", "Robert", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1508.02651", "submitter": "Konstantinos Spiliopoulos", "authors": "Alexandra Chronopoulou and Konstantinos Spiliopoulos", "title": "Sequential Monte Carlo for fractional Stochastic Volatility Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a fractional stochastic volatility model, that is a\nmodel in which the volatility may exhibit a long-range dependent or a\nrough/antipersistent behavior. We propose a dynamic sequential Monte Carlo\nmethodology that is applicable to both long memory and antipersistent processes\nin order to estimate the volatility as well as the unknown parameters of the\nmodel. We establish a central limit theorem for the state and parameter filters\nand we study asymptotic properties (consistency and asymptotic normality) for\nthe filter. We illustrate our results with a simulation study and we apply our\nmethod to estimating the volatility and the parameters of a long-range\ndependent model for S&P 500 data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 16:36:53 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 20:14:32 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Chronopoulou", "Alexandra", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1508.02663", "submitter": "Andrew Roth", "authors": "Alexandre Bouchard-C\\^ot\\'e, Arnaud Doucet, Andrew Roth", "title": "Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Markov chain Monte Carlo method to sample from the\nposterior distribution of conjugate mixture models. This algorithm relies on a\nflexible split-merge procedure built using the particle Gibbs sampler. Contrary\nto available split-merge procedures, the resulting so-called Particle Gibbs\nSplit-Merge sampler does not require the computation of a complex acceptance\nratio, is simple to implement using existing sequential Monte Carlo libraries\nand can be parallelized. We investigate its performance experimentally on\nsynthetic problems as well as on geolocation and cancer genomics data. In all\nthese examples, the particle Gibbs split-merge sampler outperforms\nstate-of-the-art split-merge methods by up to an order of magnitude for a fixed\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 17:26:13 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 13:35:49 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Doucet", "Arnaud", ""], ["Roth", "Andrew", ""]]}, {"id": "1508.02749", "submitter": "Georg Mainik", "authors": "Georg Mainik", "title": "Risk aggregation with empirical margins: Latin hypercubes, empirical\n  copulas, and convergence of sum distributions", "comments": "Manuscript accepted in the Journal of Multivariate Analysis", "journal-ref": null, "doi": "10.1016/j.jmva.2015.07.008", "report-no": null, "categories": "q-fin.RM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies convergence properties of multivariate distributions\nconstructed by endowing empirical margins with a copula. This setting includes\nLatin Hypercube Sampling with dependence, also known as the Iman--Conover\nmethod. The primary question addressed here is the convergence of the component\nsum, which is relevant to risk aggregation in insurance and finance.\n  This paper shows that a CLT for the aggregated risk distribution is not\navailable, so that the underlying mathematical problem goes beyond classic\nfunctional CLTs for empirical copulas. This issue is relevant to Monte-Carlo\nbased risk aggregation in all multivariate models generated by plugging\nempirical margins into a copula.\n  Instead of a functional CLT, this paper establishes strong uniform\nconsistency of the estimated sum distribution function and provides a\nsufficient criterion for the convergence rate $O(n^{-1/2})$ in probability.\nThese convergence results hold for all copulas with bounded densities. Examples\nwith unbounded densities include bivariate Clayton and Gauss copulas. The\nconvergence results are not specific to the component sum and hold also for any\nother componentwise non-decreasing aggregation function. On the other hand,\nconvergence of estimates for the joint distribution is much easier to prove,\nincluding CLTs.\n  Beyond Iman--Conover estimates, the results of this paper apply to\nmultivariate distributions obtained by plugging empirical margins into an exact\ncopula or by plugging exact margins into an empirical copula.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 21:14:49 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Mainik", "Georg", ""]]}, {"id": "1508.02766", "submitter": "Artur Gramacki", "authors": "Artur Gramacki, Jaros{\\l}aw Gramacki", "title": "FFT-Based Fast Computation of Multivariate Kernel Estimators with\n  Unconstrained Bandwidth Matrices", "comments": "10 pages, 1 figure, R source codes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of fast computation of multivariate kernel density estimation\n(KDE) is still an open research problem. In our view, the existing solutions do\nnot resolve this matter in a satisfactory way. One of the most elegant and\nefficient approach utilizes the fast Fourier transform. Unfortunately, the\nexisting FFT-based solution suffers from a serious limitation, as it can\naccurately operate only with the constrained (i.e., diagonal) multivariate\nbandwidth matrices. In this paper we describe the problem and give a\nsatisfactory solution. The proposed solution may be successfully used also in\nother research problems, for example for the fast computation of the optimal\nbandwidth for KDE.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 22:39:32 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2015 10:49:23 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2015 15:58:36 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2015 18:54:26 GMT"}, {"version": "v5", "created": "Wed, 7 Oct 2015 19:46:32 GMT"}, {"version": "v6", "created": "Wed, 7 Sep 2016 08:11:19 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Gramacki", "Artur", ""], ["Gramacki", "Jaros\u0142aw", ""]]}, {"id": "1508.02818", "submitter": "Damon McDougall", "authors": "Sylvain Plessis, Damon McDougall, Kathy Mandt, Thomas Greathouse,\n  Adrienn Luspay-Kuti", "title": "Uncertainty for calculating transport on Titan: a probabilistic\n  description of bimolecular diffusion parameters", "comments": null, "journal-ref": null, "doi": "10.1016/j.pss.2015.08.004", "report-no": null, "categories": "astro-ph.EP astro-ph.IM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bimolecular diffusion coefficients are important parameters used by\natmospheric models to calculate altitude profiles of minor constituents in an\natmosphere. Unfortunately, laboratory measurements of these coefficients were\nnever conducted at temperature conditions relevant to the atmosphere of Titan.\nHere we conduct a detailed uncertainty analysis of the bimolecular diffusion\ncoefficient parameters as applied to Titan's upper atmosphere to provide a\nbetter understanding of the impact of uncertainty for this parameter on models.\nBecause temperature and pressure conditions are much lower than the laboratory\nconditions in which bimolecular diffusion parameters were measured, we apply a\nBayesian framework, a problem-agnostic framework, to determine parameter\nestimates and associated uncertainties. We solve the Bayesian calibration\nproblem using the open-source QUESO library which also performs a propagation\nof uncertainties in the calibrated parameters to temperature and pressure\nconditions observed in Titan's upper atmosphere. Our results show that, after\npropagating uncertainty through the Massman model, the uncertainty in molecular\ndiffusion is highly correlated to temperature and we observe no noticeable\ncorrelation with pressure. We propagate the calibrated molecular diffusion\nestimate and associated uncertainty to obtain an estimate with uncertainty due\nto bimolecular diffusion for the methane molar fraction as a function of\naltitude. Results show that the uncertainty in methane abundance due to\nmolecular diffusion is in general small compared to eddy diffusion and the\nchemical kinetics description. However, methane abundance is most sensitive to\nuncertainty in molecular diffusion above 1200 km where the errors are\nnontrivial and could have important implications for scientific research based\non diffusion models in this altitude range.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 05:56:41 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Plessis", "Sylvain", ""], ["McDougall", "Damon", ""], ["Mandt", "Kathy", ""], ["Greathouse", "Thomas", ""], ["Luspay-Kuti", "Adrienn", ""]]}, {"id": "1508.02958", "submitter": "Madison McGaffin", "authors": "Madison G. McGaffin and Jeffrey A. Fessler", "title": "Algorithmic Design of Majorizers for Large-Scale Inverse Problems", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative majorize-minimize (MM) (also called optimization transfer)\nalgorithms solve challenging numerical optimization problems by solving a\nseries of \"easier\" optimization problems that are constructed to guarantee\nmonotonic descent of the cost function. Many MM algorithms replace a\ncomputationally expensive Hessian matrix with another more computationally\nconvenient majorizing matrix. These majorizing matrices are often generated\nusing various matrix inequalities, and consequently the set of available\nmajorizers is limited to structures for which these matrix inequalities can be\nefficiently applied. In this paper, we present a technique to algorithmically\ndesign matrix majorizers with wide varieties of structures. We use a novel\nduality-based approach to avoid the high computational and memory costs of\nstandard semidefinite programming techniques. We present some preliminary\nresults for 2D X-ray CT reconstruction that indicate these more exotic\nregularizers may significantly accelerate MM algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 15:35:02 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2015 01:47:16 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["McGaffin", "Madison G.", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1508.03283", "submitter": "JInglai Li", "authors": "Zhe Feng, Jinglai Li", "title": "An adaptive independence sampler MCMC algorithm for infinite dimensional\n  Bayesian inferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering problems require to perform Bayesian\ninferences in function spaces, in which the unknowns are of infinite dimension.\nIn such problems, many standard Markov Chain Monte Carlo (MCMC) algorithms\nbecome arbitrary slow under the mesh refinement, which is referred to as being\ndimension dependent. In this work we develop an independence sampler based MCMC\nmethod for the infinite dimensional Bayesian inferences. We represent the\nproposal distribution as a mixture of a finite number of specially parametrized\nGaussian measures. We show that under the chosen parametrization, the resulting\nMCMC algorithm is dimension independent. We also design an efficient adaptive\nalgorithm to adjust the parameter values of the mixtures from the previous\nsamples. Finally we provide numerical examples to demonstrate the efficiency\nand robustness of the proposed method, even for problems with multimodal\nposterior distributions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 17:50:05 GMT"}, {"version": "v2", "created": "Sun, 10 Apr 2016 13:38:06 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Feng", "Zhe", ""], ["Li", "Jinglai", ""]]}, {"id": "1508.03387", "submitter": "James Johndrow", "authors": "James E. Johndrow, Jonathan C. Mattingly, Sayan Mukherjee, David\n  Dunson", "title": "Optimal approximating Markov chains for Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markov Chain Monte Carlo method is the dominant paradigm for posterior\ncomputation in Bayesian analysis. It is common to control computation time by\nmaking approximations to the Markov transition kernel. Comparatively little\nattention has been paid to computational optimality in these approximating\nMarkov Chains, or when such approximations are justified relative to obtaining\nshorter paths from the exact kernel. We give simple, sharp bounds for uniform\napproximations of uniformly mixing Markov chains. We then suggest a notion of\noptimality that incorporates computation time and approximation error, and use\nour bounds to make generalizations about properties of good approximations in\nthe uniformly mixing setting. The relevance of these properties is demonstrated\nin applications to a minibatching-based approximate MCMC algorithm for large\n$n$ logistic regression and low-rank approximations for Gaussian processes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 23:50:46 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 22:37:46 GMT"}, {"version": "v3", "created": "Tue, 29 Aug 2017 04:28:10 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Johndrow", "James E.", ""], ["Mattingly", "Jonathan C.", ""], ["Mukherjee", "Sayan", ""], ["Dunson", "David", ""]]}, {"id": "1508.03483", "submitter": "Marius Hofert", "authors": "Mathieu Cambou, Marius Hofert, Christiane Lemieux", "title": "Quasi-random numbers for copula models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work addresses the question how sampling algorithms for commonly\napplied copula models can be adapted to account for quasi-random numbers.\nBesides sampling methods such as the conditional distribution method (based on\na one-to-one transformation), it is also shown that typically faster sampling\nmethods (based on stochastic representations) can be used to improve upon\nclassical Monte Carlo methods when pseudo-random number generators are replaced\nby quasi-random number generators. This opens the door to quasi-random numbers\nfor models well beyond independent margins or the multivariate normal\ndistribution. Detailed examples (in the context of finance and insurance),\nillustrations and simulations are given and software has been developed and\nprovided in the R packages copula and qrng.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 12:57:51 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 03:18:27 GMT"}, {"version": "v3", "created": "Sat, 12 Mar 2016 11:33:43 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Cambou", "Mathieu", ""], ["Hofert", "Marius", ""], ["Lemieux", "Christiane", ""]]}, {"id": "1508.03747", "submitter": "Subhadeep Mukhopadhyay", "authors": "Scott Bruce, Zeda Li, Hsiang-Chieh Yang, and Subhadeep Mukhopadhyay", "title": "Nonparametric Distributed Learning Architecture for Big Data: Algorithm\n  and Applications", "comments": "The purpose of this paper is to answer the question: What is the\n  relevance of small-data-ideas in this big-data world? The bigger question is:\n  Should we make difficult things easy or easy things look difficult? The first\n  option will probably make some impact in the long-run, but the second one\n  will surely earn prestigious journal publications in short-run, IEEE\n  Transactions on Big Data (forthcoming). The first report came out in 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dramatic increases in the size and complexity of modern datasets have made\ntraditional \"centralized\" statistical inference prohibitive. In addition to\ncomputational challenges associated with big data learning, the presence of\nnumerous data types (e.g. discrete, continuous, categorical, etc.) makes\nautomation and scalability difficult. A question of immediate concern is how to\ndesign a data-intensive statistical inference architecture without changing the\nbasic statistical modeling principles developed for \"small\" data over the last\ncentury. To address this problem, we present MetaLP, a flexible, distributed\nstatistical modeling framework.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 16:13:29 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 21:05:41 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2016 03:10:19 GMT"}, {"version": "v4", "created": "Sat, 2 Apr 2016 16:14:33 GMT"}, {"version": "v5", "created": "Mon, 26 Feb 2018 16:24:30 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Bruce", "Scott", ""], ["Li", "Zeda", ""], ["Yang", "Hsiang-Chieh", ""], ["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1508.03884", "submitter": "Enes Makalic", "authors": "Enes Makalic and Daniel F. Schmidt", "title": "A simple sampler for the horseshoe estimator", "comments": null, "journal-ref": "IEEE Signal Processing Letters, Vol. 23(1), pp. 179-182, 2016", "doi": "10.1109/LSP.2015.2503725", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we derive a simple Bayesian sampler for linear regression with\nthe horseshoe hierarchy. A new interpretation of the horseshoe model is\npresented, and extensions to logistic regression and alternative hierarchies,\nsuch as horseshoe$+$, are discussed. Due to the conjugacy of the proposed\nhierarchy, Chib's algorithm may be used to easily compute the marginal\nlikelihood of the model.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 00:28:54 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 01:56:51 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2015 02:23:01 GMT"}, {"version": "v4", "created": "Mon, 28 Sep 2015 06:31:00 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Makalic", "Enes", ""], ["Schmidt", "Daniel F.", ""]]}, {"id": "1508.04253", "submitter": "Luca Martino", "authors": "L. Martino, F. Louzada", "title": "Issues in the Multiple Try Metropolis mixing", "comments": null, "journal-ref": "Computational Statistics, 2016", "doi": "10.1007/s00180-016-0643-9", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiple Try Metropolis (MTM) algorithm is an advanced MCMC technique\nbased on drawing and testing several candidates at each iteration of the\nalgorithm. One of them is selected according to certain weights and then it is\ntested according to a suitable acceptance probability. Clearly, since the\ncomputational cost increases as the employed number of tries grows, one expects\nthat the performance of an MTM scheme improves as the number of tries\nincreases, as well. However, there are scenarios where the increase of number\nof tries does not produce a corresponding enhancement of the performance. In\nthis work, we describe these scenarios and then we introduce possible solutions\nfor solving these issues.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 09:18:53 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 19:32:32 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Martino", "L.", ""], ["Louzada", "F.", ""]]}, {"id": "1508.04409", "submitter": "Marvin N Wright", "authors": "Marvin N. Wright and Andreas Ziegler", "title": "ranger: A Fast Implementation of Random Forests for High Dimensional\n  Data in C++ and R", "comments": null, "journal-ref": "Wright, M. N. & Ziegler, A. (2017). ranger: A fast implementation\n  of random forests for high dimensional data in C++ and R. Journal of\n  Statistical Software 77:1-17", "doi": "10.18637/jss.v077.i01", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the C++ application and R package ranger. The software is a fast\nimplementation of random forests for high dimensional data. Ensembles of\nclassification, regression and survival trees are supported. We describe the\nimplementation, provide examples, validate the package with a reference\nimplementation, and compare runtime and memory usage with other\nimplementations. The new software proves to scale best with the number of\nfeatures, samples, trees, and features tried for splitting. Finally, we show\nthat ranger is the fastest and most memory efficient implementation of random\nforests to analyze data on the scale of a genome-wide association study.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 18:47:10 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 09:04:34 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Wright", "Marvin N.", ""], ["Ziegler", "Andreas", ""]]}, {"id": "1508.04876", "submitter": "Georgios Karagiannis", "authors": "Georgios Karagiannis, Bledar A. Konomi, Guang Lin, Faming Liang", "title": "Parallel and Interacting Stochastic Approximation Annealing algorithms\n  for global optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the parallel and interacting stochastic approximation annealing\n(PISAA) algorithm, a stochastic simulation procedure for global optimisation,\nthat extends and improves the stochastic approximation annealing (SAA) by using\npopulation Monte Carlo ideas. The standard SAA algorithm guarantees convergence\nto the global minimum when a square-root cooling schedule is used; however the\nefficiency of its performance depends crucially on its self-adjusting\nmechanism. Because its mechanism is based on information obtained from only a\nsingle chain, SAA may present slow convergence in complex optimisation\nproblems. The proposed algorithm involves simulating a population of SAA chains\nthat interact each other in a manner that ensures significant improvement of\nthe self-adjusting mechanism and better exploration of the sampling space.\nCentral to the proposed algorithm are the ideas of (i) recycling information\nfrom the whole population of Markov chains to design a more accurate/stable\nself-adjusting mechanism and (ii) incorporating more advanced proposals, such\nas crossover operations, for the exploration of the sampling space. PISAA\npresents a significantly improved performance in terms of convergence. PISAA\ncan be implemented in parallel computing environments if available. We\ndemonstrate the good performance of the proposed algorithm on challenging\napplications including Bayesian network learning and protein folding. Our\nnumerical comparisons suggest that PISAA outperforms the simulated annealing,\nstochastic approximation annealing, and annealing evolutionary stochastic\napproximation Monte Carlo especially in high dimensional or rugged scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 05:01:52 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Karagiannis", "Georgios", ""], ["Konomi", "Bledar A.", ""], ["Lin", "Guang", ""], ["Liang", "Faming", ""]]}, {"id": "1508.05047", "submitter": "Konstantin Zuev M", "authors": "James L. Beck and Konstantin M. Zuev", "title": "Rare Event Simulation", "comments": "Contribution to the Springer Handbook on Uncertainty Quantification.\n  13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare events are events that are expected to occur infrequently, or more\ntechnically, those that have low probabilities (say, order of $10^{-3}$ or\nless) of occurring according to a probability model. In the context of\nuncertainty quantification, the rare events often correspond to failure of\nsystems designed for high reliability, meaning that the system performance\nfails to meet some design or operation specifications. As reviewed in this\nsection, computation of such rare-event probabilities is challenging.\nAnalytical solutions are usually not available for non-trivial problems and\nstandard Monte Carlo simulation is computationally inefficient. Therefore, much\nresearch effort has focused on developing advanced stochastic simulation\nmethods that are more efficient. In this section, we address the problem of\nestimating rare-event probabilities by Monte Carlo simulation, Importance\nSampling and Subset Simulation for highly reliable dynamic systems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 17:06:55 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Beck", "James L.", ""], ["Zuev", "Konstantin M.", ""]]}, {"id": "1508.05178", "submitter": "Gael Martin Prof", "authors": "David T. Frazier, Gael M. Martin and Christian P. Robert", "title": "On Consistency of Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods have become increasingly\nprevalent of late, facilitating as they do the analysis of intractable, or\nchallenging, statistical problems. With the initial focus being primarily on\nthe practical import of ABC, exploration of its formal statistical properties\nhas begun to attract more attention. The aim of this paper is to establish\ngeneral conditions under which ABC methods are Bayesian consistent, in the\nsense of producing draws that yield a degenerate posterior distribution at the\ntrue parameter (vector) asymptotically (in the sample size). We derive\nconditions under which arbitrary summary statistics yield consistent inference\nin the Bayesian sense, with these conditions linked to identification of the\ntrue parameters. Using simple illustrative examples that have featured in the\nliterature, we demonstrate that identification, and hence consistency, is\nunlikely to be achieved in many cases, and propose a simple diagnostic\nprocedure that can indicate the presence of this problem. We also formally\nexplore the link between consistency and the use of auxiliary models within\nABC, and illustrate the subsequent results in the Lotka-Volterra predator-prey\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 05:41:41 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Frazier", "David T.", ""], ["Martin", "Gael M.", ""], ["Robert", "Christian P.", ""]]}, {"id": "1508.05918", "submitter": "Olanrewaju Akande", "authors": "Olanrewaju Akande, Fan Li and Jerome Reiter", "title": "An Empirical Comparison of Multiple Imputation Methods for Categorical\n  Data", "comments": null, "journal-ref": null, "doi": "10.1080/00031305.2016.1277158", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation is a common approach for dealing with missing values in\nstatistical databases. The imputer fills in missing values with draws from\npredictive models estimated from the observed data, resulting in multiple,\ncompleted versions of the database. Researchers have developed a variety of\ndefault routines to implement multiple imputation; however, there has been\nlimited research comparing the performance of these methods, particularly for\ncategorical data. We use simulation studies to compare repeated sampling\nproperties of three default multiple imputation methods for categorical data,\nincluding chained equations using generalized linear models, chained equations\nusing classification and regression trees, and a fully Bayesian joint\ndistribution based on Dirichlet Process mixture models. We base the simulations\non categorical data from the American Community Survey. In the circumstances of\nthis study, the results suggest that default chained equations approaches based\non generalized linear models are dominated by the default regression tree and\nBayesian mixture model approaches. They also suggest competing advantages for\nthe regression tree and Bayesian mixture model approaches, making both\nreasonable default engines for multiple imputation of categorical data. A\nsupplementary material for this article is available online.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 19:14:07 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 01:55:17 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Akande", "Olanrewaju", ""], ["Li", "Fan", ""], ["Reiter", "Jerome", ""]]}, {"id": "1508.06235", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang", "title": "Clustering With Side Information: From a Probabilistic Model to a\n  Deterministic Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a model-based clustering method (TVClust) that\nrobustly incorporates noisy side information as soft-constraints and aims to\nseek a consensus between side information and the observed data. Our method is\nbased on a nonparametric Bayesian hierarchical model that combines the\nprobabilistic model for the data instance and the one for the side-information.\nAn efficient Gibbs sampling algorithm is proposed for posterior inference.\nUsing the small-variance asymptotics of our probabilistic model, we then derive\na new deterministic clustering algorithm (RDP-means). It can be viewed as an\nextension of K-means that allows for the inclusion of side information and has\nthe additional property that the number of clusters does not need to be\nspecified a priori. Empirical studies have been carried out to compare our work\nwith many constrained clustering algorithms from the literature on both a\nvariety of data sets and under a variety of conditions such as using noisy side\ninformation and erroneous k values. The results of our experiments show strong\nresults for our probabilistic and deterministic approaches under these\nconditions when compared to other algorithms in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 18:13:27 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 17:46:36 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 17:48:54 GMT"}, {"version": "v4", "created": "Sat, 31 Oct 2015 05:38:15 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Khashabi", "Daniel", ""], ["Wieting", "John", ""], ["Liu", "Jeffrey Yufei", ""], ["Liang", "Feng", ""]]}, {"id": "1508.06452", "submitter": "Janne Hakkarainen", "authors": "Antti Solonen, Tiangang Cui, Janne Hakkarainen, and Youssef Marzouk", "title": "On dimension reduction in Gaussian filters", "comments": null, "journal-ref": null, "doi": "10.1088/0266-5611/32/4/045003", "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A priori dimension reduction is a widely adopted technique for reducing the\ncomputational complexity of stationary inverse problems. In this setting, the\nsolution of an inverse problem is parameterized by a low-dimensional basis that\nis often obtained from the truncated Karhunen-Loeve expansion of the prior\ndistribution. For high-dimensional inverse problems equipped with smoothing\npriors, this technique can lead to drastic reductions in parameter dimension\nand significant computational savings.\n  In this paper, we extend the concept of a priori dimension reduction to\nnon-stationary inverse problems, in which the goal is to sequentially infer the\nstate of a dynamical system. Our approach proceeds in an offline-online\nfashion. We first identify a low-dimensional subspace in the state space before\nsolving the inverse problem (the offline phase), using either the method of\n\"snapshots\" or regularized covariance estimation. Then this subspace is used to\nreduce the computational complexity of various filtering algorithms - including\nthe Kalman filter, extended Kalman filter, and ensemble Kalman filter - within\na novel subspace-constrained Bayesian prediction-and-update procedure (the\nonline phase). We demonstrate the performance of our new dimension reduction\napproach on various numerical examples. In some test cases, our approach\nreduces the dimensionality of the original problem by orders of magnitude and\nyields up to two orders of magnitude in computational savings.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 11:41:59 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 14:04:04 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 08:37:39 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Solonen", "Antti", ""], ["Cui", "Tiangang", ""], ["Hakkarainen", "Janne", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1508.06700", "submitter": "JInglai Li", "authors": "Keyi Wu, Jinglai Li", "title": "A surrogate accelerated multicanonical Monte Carlo method for\n  uncertainty quantification", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2016.06.020", "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider a class of uncertainty quantification problems where\nthe system performance or reliability is characterized by a scalar parameter\n$y$. The performance parameter $y$ is random due to the presence of various\nsources of uncertainty in the system, and our goal is to estimate the\nprobability density function (PDF) of $y$. We propose to use the multicanonical\nMonte Carlo (MMC) method, a special type of adaptive importance sampling\nalgorithm, to compute the PDF of interest. Moreover, we develop an adaptive\nalgorithm to construct local Gaussian process surrogates to further accelerate\nthe MMC iterations. With numerical examples we demonstrate that the proposed\nmethod can achieve several orders of magnitudes of speedup over the standard\nMonte Carlo method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 02:11:37 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 13:29:26 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Wu", "Keyi", ""], ["Li", "Jinglai", ""]]}, {"id": "1508.07643", "submitter": "Marc Goessling", "authors": "Marc Goessling and Shan Kang", "title": "Directional Decision Lists", "comments": "IEEE Big Data for Advanced Manufacturing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel family of decision lists consisting of\nhighly interpretable models which can be learned efficiently in a greedy\nmanner. The defining property is that all rules are oriented in the same\ndirection. Particular examples of this family are decision lists with\nmonotonically decreasing (or increasing) probabilities. On simulated data we\nempirically confirm that the proposed model family is easier to train than\ngeneral decision lists. We exemplify the practical usability of our approach by\nidentifying problem symptoms in a manufacturing process.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 23:00:35 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 16:41:16 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2016 19:38:53 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Goessling", "Marc", ""], ["Kang", "Shan", ""]]}, {"id": "1508.07937", "submitter": "Vahed Maroufy", "authors": "Vahed Maroufy and Paul Marriott", "title": "Local and global robustness in conjugate Bayesian analysis", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the influence of perturbations of conjugate priors in\nBayesian inference. A perturbed prior is defined inside a larger family, local\nmixture models, and the effect on posterior inference is studied. The\nperturbation, in some sense, generalizes the linear perturbation studied in\n\\cite{Gustafson1996}. It is intuitive, naturally normalized and is flexible for\nstatistical applications. Both global and local sensitivity analyses are\nconsidered. A geometric approach is employed for optimizing the sensitivity\ndirection function, the difference between posterior means and the divergence\nfunction between posterior predictive models. All the sensitivity measure\nfunctions are defined on a convex space with non-trivial boundary which is\nshown to be a smooth manifold.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 18:09:05 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Maroufy", "Vahed", ""], ["Marriott", "Paul", ""]]}]