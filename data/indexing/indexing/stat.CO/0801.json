[{"id": "0801.0499", "submitter": "Daniel Yekutieli Dr.", "authors": "Daniel Yekutieli", "title": "Adjusted Bayesian inference for selected parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of providing inference from a Bayesian perspective for\nparameters selected after viewing the data. We present a Bayesian framework for\nproviding inference for selected parameters, based on the observation that\nproviding Bayesian inference for selected parameters is a truncated data\nproblem. We show that if the prior for the parameter is non-informative, or if\nthe parameter is a \"fixed\" unknown constant, then it is necessary to adjust the\nBayesian inference for selection. Our second contribution is the introduction\nof Bayesian False Discovery Rate controlling methodology,which generalizes\nexisting Bayesian FDR methods that are only defined in the two-group mixture\nmodel.We illustrate our results by applying them to simulated data and data\nfroma microarray experiment.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2008 10:29:09 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2009 14:12:17 GMT"}, {"version": "v3", "created": "Mon, 26 Jan 2009 08:57:26 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2009 11:04:47 GMT"}, {"version": "v5", "created": "Sun, 20 Jun 2010 13:21:43 GMT"}, {"version": "v6", "created": "Sun, 27 Mar 2011 17:51:16 GMT"}, {"version": "v7", "created": "Thu, 15 Sep 2011 03:51:07 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Yekutieli", "Daniel", ""]]}, {"id": "0801.1864", "submitter": "Robert Kohn", "authors": "P. Giordani and R. Kohn", "title": "Adaptive Independent Metropolis-Hastings by Fast Estimation of Mixtures\n  of Normals", "comments": "35 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": null, "abstract": "  We construct an adaptive independent Metropolis-Hastings sampler that uses a\nmixture of normals as a proposal distribution. To take full advantage of the\npotential of adaptive sampling our algorithm updates the mixture of normals\nfrequently, starting early in the chain. The algorithm is built for speed and\nreliability and its sampling performance is evaluated with real and simulated\nexamples. Our article outlines conditions for adaptive sampling to hold and\ngives a readily accessible proof that under these conditions the sampling\nscheme generates iterates that converge to the target distribution.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2008 22:53:12 GMT"}], "update_date": "2008-01-15", "authors_parsed": [["Giordani", "P.", ""], ["Kohn", "R.", ""]]}, {"id": "0801.2555", "submitter": "Ping Ma", "authors": "Ping Ma and Wenxuan Zhong", "title": "Penalized Clustering of Large Scale Functional Data with Multiple\n  Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  In this article, we propose a penalized clustering method for large scale\ndata with multiple covariates through a functional data approach. In the\nproposed method, responses and covariates are linked together through\nnonparametric multivariate functions (fixed effects), which have great\nflexibility in modeling a variety of function features, such as jump points,\nbranching, and periodicity. Functional ANOVA is employed to further decompose\nmultivariate functions in a reproducing kernel Hilbert space and provide\nassociated notions of main effect and interaction. Parsimonious random effects\nare used to capture various correlation structures. The mixed-effect models are\nnested under a general mixture model, in which the heterogeneity of functional\ndata is characterized. We propose a penalized Henderson's likelihood approach\nfor model-fitting and design a rejection-controlled EM algorithm for the\nestimation. Our method selects smoothing parameters through generalized\ncross-validation. Furthermore, the Bayesian confidence intervals are used to\nmeasure the clustering uncertainty. Simulation studies and real-data examples\nare presented to investigate the empirical performance of the proposed method.\nOpen-source code is available in the R package MFDA.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2008 19:34:50 GMT"}], "update_date": "2008-01-17", "authors_parsed": [["Ma", "Ping", ""], ["Zhong", "Wenxuan", ""]]}, {"id": "0801.2748", "submitter": "Mark Kliger", "authors": "Ami Wiesel, Mark Kliger and Alfred O. Hero III", "title": "A greedy approach to sparse canonical correlation analysis", "comments": "5 pages, 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": null, "abstract": "  We consider the problem of sparse canonical correlation analysis (CCA), i.e.,\nthe search for two linear combinations, one for each multivariate, that yield\nmaximum correlation using a specified number of variables. We propose an\nefficient numerical approximation based on a direct greedy approach which\nbounds the correlation at each stage. The method is specifically designed to\ncope with large data sets and its computational complexity depends only on the\nsparsity levels. We analyze the algorithm's performance through the tradeoff\nbetween correlation and parsimony. The results of numerical simulation suggest\nthat a significant portion of the correlation may be captured using a\nrelatively small number of variables. In addition, we examine the use of sparse\nCCA as a regularization method when the number of available samples is small\ncompared to the dimensions of the multivariates.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2008 18:53:07 GMT"}], "update_date": "2008-01-18", "authors_parsed": [["Wiesel", "Ami", ""], ["Kliger", "Mark", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "0801.3352", "submitter": "Piero Barone", "authors": "Piero Barone", "title": "On the condensed density of the generalized eigenvalues of pencils of\n  Hankel Gaussian random matrices and applications", "comments": "30 pages, 16 figures, better approximations provided", "journal-ref": "Journal of Multivariate Analysis 111 (2012) 160-173", "doi": "10.1016/j.jmva.2012.05.009", "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pencils of Hankel matrices whose elements have a joint Gaussian distribution\nwith nonzero mean and not identical covariance are considered. An approximation\nto the distribution of the squared modulus of their determinant is computed\nwhich allows to get a closed form approximation of the condensed density of the\ngeneralized eigenvalues of the pencils. Implications of this result for solving\nseveral moments problems are discussed and some numerical examples are\nprovided.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2008 16:29:20 GMT"}, {"version": "v2", "created": "Tue, 7 Sep 2010 08:53:36 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Barone", "Piero", ""]]}, {"id": "0801.3513", "submitter": "Christian Robert", "authors": "Christian Robert (CEREMADE), Jean-Michel Marin (INRIA Futurs)", "title": "On some difficulties with a posterior probability approximation\n  technique", "comments": "Second version, resubmitted", "journal-ref": "Bayesian Analysis(2008), 3(2), 427-442", "doi": "10.1214/08-BA316", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Scott (2002) and Congdon (2006), a new method is advanced to compute\nposterior probabilities of models under consideration. It is based solely on\nMCMC outputs restricted to single models, i.e., it is bypassing reversible jump\nand other model exploration techniques. While it is indeed possible to\napproximate posterior probabilities based solely on MCMC outputs from single\nmodels, as demonstrated by Gelfand and Dey (1994) and Bartolucci et al. (2006),\nwe show that the proposals of Scott (2002) and Congdon (2006) are biased and\nadvance several arguments towards this thesis, the primary one being the\nconfusion between model-based posteriors and joint pseudo-posteriors. From a\npractical point of view, the bias in Scott's (2002) approximation appears to be\nmuch more severe than the one in Congdon's (2006), the later being often of the\nsame magnitude as the posterior probability it approximates, although we also\nexhibit an example where the divergence from the true posterior probability is\nextreme.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2008 07:26:00 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2008 12:13:28 GMT"}, {"version": "v3", "created": "Tue, 18 Mar 2008 21:23:13 GMT"}, {"version": "v4", "created": "Mon, 21 Apr 2008 14:32:18 GMT"}, {"version": "v5", "created": "Thu, 5 Jun 2008 07:04:06 GMT"}], "update_date": "2010-10-11", "authors_parsed": [["Robert", "Christian", "", "CEREMADE"], ["Marin", "Jean-Michel", "", "INRIA Futurs"]]}, {"id": "0801.3552", "submitter": "Ioana Cosma", "authors": "Peter Clifford and Ioana A. Cosma", "title": "A statistical analysis of probabilistic counting algorithms", "comments": "19 pages, 0 figures", "journal-ref": "Scandinavian Journal of Statistics, 39, 1, 1-14, 2012", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of cardinality estimation in data stream\napplications. We present a statistical analysis of probabilistic counting\nalgorithms, focusing on two techniques that use pseudo-random variates to form\nlow-dimensional data sketches. We apply conventional statistical methods to\ncompare probabilistic algorithms based on storing either selected order\nstatistics, or random projections. We derive estimators of the cardinality in\nboth cases, and show that the maximal-term estimator is recursively computable\nand has exponentially decreasing error bounds. Furthermore, we show that the\nestimators have comparable asymptotic efficiency, and explain this result by\ndemonstrating an unexpected connection between the two approaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2008 12:55:29 GMT"}, {"version": "v2", "created": "Sun, 16 May 2010 12:06:25 GMT"}, {"version": "v3", "created": "Sun, 7 Nov 2010 14:19:40 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Clifford", "Peter", ""], ["Cosma", "Ioana A.", ""]]}, {"id": "0801.3559", "submitter": "Ioana Cosma", "authors": "Peter Clifford and Ioana A. Cosma", "title": "Efficient l_{alpha} Distance Approximation for High Dimensional Data\n  Using alpha-Stable Projection", "comments": "8 pages, 3 figures, submitted to COMPSTAT2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": null, "abstract": "  In recent years, large high-dimensional data sets have become commonplace in\na wide range of applications in science and commerce. Techniques for dimension\nreduction are of primary concern in statistical analysis. Projection methods\nplay an important role. We investigate the use of projection algorithms that\nexploit properties of the alpha-stable distributions. We show that l_{alpha}\ndistances and quasi-distances can be recovered from random projections with\nfull statistical efficiency by L-estimation. The computational requirements of\nour algorithm are modest; after a once-and-for-all calculation to determine an\narray of length k, the algorithm runs in O(k) time for each distance, where k\nis the reduced dimension of the projection.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2008 12:00:10 GMT"}], "update_date": "2008-01-24", "authors_parsed": [["Clifford", "Peter", ""], ["Cosma", "Ioana A.", ""]]}, {"id": "0801.3887", "submitter": "Christian Robert", "authors": "Nicolas Chopin (CREST), Christian Robert (CREST, Ceremade)", "title": "Properties of Nested Sampling", "comments": "Revision submitted to Biometrika", "journal-ref": "Biometrika 97(3):741-755, 2010", "doi": "10.1093/biomet/asq021", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested sampling is a simulation method for approximating marginal likelihoods\nproposed by Skilling (2006). We establish that nested sampling has an\napproximation error that vanishes at the standard Monte Carlo rate and that\nthis error is asymptotically Gaussian. We show that the asymptotic variance of\nthe nested sampling approximation typically grows linearly with the dimension\nof the parameter. We discuss the applicability and efficiency of nested\nsampling in realistic problems, and we compare it with two current methods for\ncomputing marginal likelihood. We propose an extension that avoids resorting to\nMarkov chain Monte Carlo to obtain the simulated points.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2008 07:25:25 GMT"}, {"version": "v2", "created": "Sat, 25 Oct 2008 06:47:43 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2009 18:50:46 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2009 20:40:26 GMT"}], "update_date": "2010-10-11", "authors_parsed": [["Chopin", "Nicolas", "", "CREST"], ["Robert", "Christian", "", "CREST, Ceremade"]]}, {"id": "0801.4172", "submitter": "Piero Barone", "authors": "Piero Barone", "title": "Computational aspects and applications of a new transform for solving\n  the complex exponentials approximation problem", "comments": "28 pages, 20 figures", "journal-ref": "Digital Signal processing 20 (2010) 724-735", "doi": "10.1016/j.dsp.2009.10.003", "report-no": null, "categories": "math.NA stat.AP stat.CO", "license": null, "abstract": "  Many real life problems can be reduced to the solution of a complex\nexponentials approximation problem which is usually ill posed. Recently a new\ntransform for solving this problem, formulated as a specific moments problem in\nthe plane, has been proposed in a theoretical framework. In this work some\ncomputational issues are addressed to make this new tool useful in practice. An\nalgorithm is developed and used to solve a Nuclear Magnetic Resonance\nspectrometry problem, two time series interpolation and extrapolation problems\nand a shape from moments problem.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2008 16:37:16 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Barone", "Piero", ""]]}]