[{"id": "1604.00872", "submitter": "Akihiko Nishimura", "authors": "Akihiko Nishimura and David Dunson", "title": "Geometrically Tempered Hamiltonian Monte Carlo", "comments": "44 papges, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) has become routinely used for sampling from\nposterior distributions. Its extension Riemann manifold HMC (RMHMC) modifies\nthe proposal kernel through distortion of local distances by a Riemannian\nmetric. The performance depends critically on the choice of metric, with the\nFisher information providing the standard choice. In this article, we propose a\nnew class of metrics aimed at improving HMC's performance on multi-modal target\ndistributions. We refer to the proposed approach as geometrically tempered HMC\n(GTHMC) due to its connection to other tempering methods. We establish a\ngeometric theory behind RMHMC to motivate GTHMC and characterize its\ntheoretical properties. Moreover, we develop a novel variable step size\nintegrator for simulating Hamiltonian dynamics to improve on the usual\nSt\\\"{o}rmer-Verlet integrator which suffers from numerical instability in GTHMC\nsettings. We illustrate GTHMC through simulations, demonstrating generality and\nsubstantial gains over standard HMC implementations in terms of effective\nsample sizes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 14:24:38 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 02:58:34 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Nishimura", "Akihiko", ""], ["Dunson", "David", ""]]}, {"id": "1604.00889", "submitter": "Akihiko Nishimura", "authors": "Akihiko Nishimura and David Dunson", "title": "Variable length trajectory compressible hybrid Monte Carlo", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid Monte Carlo (HMC) generates samples from a prescribed probability\ndistribution in a configuration space by simulating Hamiltonian dynamics,\nfollowed by the Metropolis (-Hastings) acceptance/rejection step. Compressible\nHMC (CHMC) generalizes HMC to a situation in which the dynamics is reversible\nbut not necessarily Hamiltonian. This article presents a framework to further\nextend the algorithm. Within the existing framework, each trajectory of the\ndynamics must be integrated for the same amount of (random) time to generate a\nvalid Metropolis proposal. Our generalized acceptance/rejection mechanism\nallows a more deliberate choice of the integration time for each trajectory.\nThe proposed algorithm in particular enables an effective application of\nvariable step size integrators to HMC-type sampling algorithms based on\nreversible dynamics. The potential of our framework is further demonstrated by\nanother extension of HMC which reduces the wasted computations due to unstable\nnumerical approximations and corresponding rejected proposals.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 14:56:50 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Nishimura", "Akihiko", ""], ["Dunson", "David", ""]]}, {"id": "1604.01067", "submitter": "Bubacarr Bah", "authors": "Bubacarr Bah", "title": "Sparse matrices for weighted sparse recovery", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CC cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derived the first sparse recovery guarantees for weighted $\\ell_1$\nminimization with sparse random matrices and the class of weighted sparse\nsignals, using a weighted versions of the null space property to derive these\nguarantees. These sparse matrices from expender graphs can be applied very fast\nand have other better computational complexities than their dense counterparts.\nIn addition we show that, using such sparse matrices, weighted sparse recovery\nwith weighted $\\ell_1$ minimization leads to sample complexities that are\nlinear in the weighted sparsity of the signal and these sampling rates can be\nsmaller than those of standard sparse recovery. Moreover, these results reduce\nto known results in standard sparse recovery and sparse recovery with prior\ninformation and the results are supported by numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 21:23:46 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 07:26:39 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Bah", "Bubacarr", ""]]}, {"id": "1604.01120", "submitter": "Takashi Goda", "authors": "Takashi Goda", "title": "Unbiased Monte Carlo estimation for the expected value of partial\n  perfect information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected value of partial perfect information (EVPPI) denotes the value\nof eliminating uncertainty on a subset of unknown parameters involved in a\ndecision model. The EVPPI can be regarded as a decision-theoretic sensitivity\nindex, and has been widely used for identifying relatively important unknown\nparameters. It follows from Jensen's inequality, however, that the standard\nnested Monte Carlo computation of the EVPPI results in biased estimates. In\nthis paper we introduce two unbiased Monte Carlo estimators for the EVPPI based\non multilevel Monte Carlo method, introduced by Heinrich (1998) and Giles\n(2008), and its extension by Rhee and Glynn (2012, 2015). Our unbiased\nestimators are simple and straightforward to implement, and thus are of highly\npractical use. Numerical experiments show that even the convergence behaviors\nof our unbiased estimators are superior to that of the standard nested Monte\nCarlo estimator.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 03:01:15 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Goda", "Takashi", ""]]}, {"id": "1604.01250", "submitter": "Alvin Chua", "authors": "Christopher J. Moore, Alvin J. K. Chua, Christopher P. L. Berry,\n  Jonathan R. Gair", "title": "Fast methods for training Gaussian processes on large data sets", "comments": "Fixed missing references", "journal-ref": "R. Soc. Open Sci. 3, 160125 (2016)", "doi": "10.1098/rsos.160125", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression (GPR) is a non-parametric Bayesian technique for\ninterpolating or fitting data. The main barrier to further uptake of this\npowerful tool rests in the computational costs associated with the matrices\nwhich arise when dealing with large data sets. Here, we derive some simple\nresults which we have found useful for speeding up the learning stage in the\nGPR algorithm, and especially for performing Bayesian model comparison between\ndifferent covariance functions. We apply our techniques to both synthetic and\nreal data and quantify the speed-up relative to using nested sampling to\nnumerically evaluate model evidences.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 13:29:15 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 10:38:28 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Moore", "Christopher J.", ""], ["Chua", "Alvin J. K.", ""], ["Berry", "Christopher P. L.", ""], ["Gair", "Jonathan R.", ""]]}, {"id": "1604.01443", "submitter": "Li Ma", "authors": "Li Ma and Jacopo Soriano", "title": "Analysis of distributional variation through multi-scale Beta-Binomial\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical analyses involve the comparison of multiple data sets\ncollected under different conditions in order to identify the difference in the\nunderlying distributions. A common challenge in multi-sample comparison is the\npresence of various confounders, or extraneous causes other than the conditions\nof interest that also contribute to the difference across the distributions.\nThey result in false findings, i.e., identified differences that are not\nreplicable in follow-up investigations. We consider an ANOVA approach to\naddressing this issue in multi-sample comparison---by collecting replicate data\nsets under each condition, thereby allowing the identification of the\ninteresting distributional variation from the extraneous ones. We introduce a\nmulti-scale Bayesian hierarchical model for the analysis of distributional\nvariation (ANDOVA) under this design, based on a collection of Beta-Binomial\ntests targeting variations of different scales at different locations across\nthe sample space. Instead treating the tests independently, the model employs a\ngraphical structure to introduce dependency among the individual tests thereby\nallowing borrowing of strength among them. We derive efficient inference recipe\nthrough a combination of numerical integration and message passing, and\nevaluate the ability of our method to effectively address ANDOVA through\nextensive simulation. We utilize our method to analyze a DNase-seq data set for\nidentifying differences in transcriptional factor binding.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 22:35:22 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Ma", "Li", ""], ["Soriano", "Jacopo", ""]]}, {"id": "1604.02724", "submitter": "Gregory Rice", "authors": "Gregory Rice and Han Lin Shang", "title": "A plug-in bandwidth selection procedure for long run covariance\n  estimation with stationary functional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In arenas of application including environmental science, economics, and\nmedicine, it is increasingly common to consider time series of curves or\nfunctions. Many inferential procedures employed in the analysis of such data\ninvolve the long run covariance function or operator, which is analogous to the\nlong run covariance matrix familiar to finite dimensional time series analysis\nand econometrics. This function may be naturally estimated using a smoothed\nperiodogram type estimator evaluated at frequency zero that relies crucially on\nthe choice of a bandwidth parameter. Motivated by a number of prior\ncontributions in the finite dimensional setting, we propose a bandwidth\nselection method that aims to minimize the estimator's asymptotic mean squared\nnormed error (AMSNE) in $L^2[0,1]^2$. As the AMSNE depends on unknown\npopulation quantities including the long run covariance function itself,\nestimates for these are plugged in in an initial step after which the estimated\nAMSNE can be minimized to produce an empirical optimal bandwidth. We show that\nthe bandwidth produced in this way is asymptotically consistent with the AMSNE\noptimal bandwidth, with quantifiable rates, under mild stationarity and moment\nconditions. These results and the efficacy of the proposed methodology are\nevaluated by means of a comprehensive simulation study, from which we can offer\npractical advice on how to select the bandwidth parameter in this setting.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 19:00:02 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Rice", "Gregory", ""], ["Shang", "Han Lin", ""]]}, {"id": "1604.03802", "submitter": "Chang-Yun Lin", "authors": "Chang-Yun Lin", "title": "Robust designs to model uncertainty with high estimation and prediction\n  efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alphabetic optimality criteria, such as the $D$, $A$, and $I$ criteria,\nrequire specifying a model to select optimal designs. They are not model free\nand the optimal designs selected by them are not robust to model uncertainty.\nRecently, many extensions of the $D$ and $A$ criteria have been proposed for\nselecting robust designs with high estimation efficiency. However, approaches\nfor finding robust designs with high prediction efficiency are rarely studied\nin the literature. In this paper, we propose the $P_\\alpha$ criterion and\ndevelop its approximation version for two-level designs, called the ${\\tilde\nP_\\alpha}$ criterion. They are useful for selecting robust designs with high\nestimation, high prediction, or balanced estimation and prediction efficiency\nfor projective submodels. Computational studies show that the ${\\tilde\nP}_\\alpha$ criterion is a good approximation of the $P_\\alpha$ criterion and\ncan reduce great computation time when we search designs over a wide range of\nmodels. The connection between the ${\\tilde P_\\alpha}$ criterion and the\ngeneralized minimum aberration (GMA) criterion is studied. Result shows that\n${\\tilde P_\\alpha}$ plays a great role to link the alphabetic optimality\ncriteria and the aberration-based criteria.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 14:31:10 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Lin", "Chang-Yun", ""]]}, {"id": "1604.04582", "submitter": "Fabio Rapallo", "authors": "Roberto Fontana and Fabio Rapallo", "title": "Simulations on the combinatorial structure of D-optimal designs", "comments": "9 pages. Submitted for the Proceedings volume of the 8th IWS -\n  International Workshop on Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present the results of several simulations on main-effect\nfactorial designs. The goal of such simulations is to investigate the\nconnections between the $D$-optimality of a design and its geometrical\nstructure. By means of a combinatorial object, namely the circuit basis of the\ndesign matrix, we show that it is possible to define a simple index that\nexhibits strong connections with the $D$-optimality.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 17:41:37 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Fontana", "Roberto", ""], ["Rapallo", "Fabio", ""]]}, {"id": "1604.04980", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung, Robert B. Gramacy, Benjamin Haaland", "title": "Potentially Predictive Variance Reducing Subsample Locations in Local\n  Gaussian Process Regression", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": "10.5705/ss.202016.0138", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process models are commonly used as emulators for computer\nexperiments. However, developing a Gaussian process emulator can be\ncomputationally prohibitive when the number of experimental samples is even\nmoderately large. Local Gaussian process approximation (Gramacy and Apley,\n2015) was proposed as an accurate and computationally feasible emulation\nalternative. However, constructing local sub-designs specific to predictions at\na particular location of interest remains a substantial computational\nbottleneck to the technique. In this paper, two computationally efficient\nneighborhood search limiting techniques are proposed, a maximum distance method\nand a feature approximation method. Two examples demonstrate that the proposed\nmethods indeed save substantial computation while retaining emulation accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 04:05:59 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 20:05:41 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Sung", "Chih-Li", ""], ["Gramacy", "Robert B.", ""], ["Haaland", "Benjamin", ""]]}, {"id": "1604.05478", "submitter": "Mattia Molinaro", "authors": "Mattia Molinaro, Reinhard Furrer", "title": "Valid parameter space of a bivariate Gaussian Markov random field with a\n  generalized block-Toeplitz precision matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Markov random fields (GMRFs) are extensively used in statistics to\nmodel area-based data and usually depend on several parameters in order to\ncapture complex spatial correlations. In this context, it is important to\ndetermine the valid parameter space, namely the domain ensuring (semi)\npositive-definiteness of the precision matrix. Depending on the structure of\nthe latter, this task can be challenging. While univari- ate GMRFs with\nblock-Toeplitz precision are well studied in the literature, not much is\nanalytically known about bivariate GMRFs. So far, only restrictive sufficient\nconditions and brute-force approaches were proposed, which are computationally\nexpensive for the size of modern datasets. In this paper, we consider a\nbivariate GMRF, which is part of a hierarchical model used in spatial\nstatistics to analyze data coming from projec- tions of regional climate\nchange. By extending classical convergence results of univariate fields with\ntoroidal boundary conditions to fields without boundary conditions, we pro-\nvide asymptotically closed-form expressions of the valid parameter space. We\ndevelop a general methodology that can be used to determine the valid parameter\nspace of bivariate GMRFs whose precision matrix has a generalized\nblock-Toeplitz structure and for which classical convergence results are not\ndirectly applicable. Finally, we quantify the rate of convergence of our\napproach through a numerical study in R.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 09:01:20 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Molinaro", "Mattia", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1604.05643", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos and Emmanouil Mentzakis", "title": "A copula-based model for multivariate ordinal panel data: application to\n  well-being composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel copula-based multivariate panel ordinal model is developed to\nestimate structural relations among components of well-being. Each ordinal\ntime-series is modelled using a copula-based Markov model to relate the\nmarginal distributions of the response at each time of observation and then, at\neach observation time, the conditional distributions of each ordinal\ntime-series are joined using a multivariate t copula. Maximum simulated\nlikelihood based on evaluating the multidimensional integrals of the likelihood\nwith randomized quasi Monte Carlo methods is used for the estimation.\nAsymptotic calculations show that our method is nearly as efficient as maximum\nlikelihood for fully specified multivariate copula models. Our findings\nhighlight the importance of one's relative position in evaluating their\nwell-being with no direct effects of socio-economic characteristics on\nwell-being but strong indirect effects through their impact on components of\nwell-being. Temporal resilience, habit formation and behavioural traits can\nexplain the dependence in the joint tails over time and across well-being\ncomponents.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 16:31:25 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 20:11:28 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""], ["Mentzakis", "Emmanouil", ""]]}, {"id": "1604.05658", "submitter": "Jonathan Stroud", "authors": "Biao Yang, Jonathan R. Stroud, Gabriel Huerta", "title": "Sequential Monte Carlo Smoothing with Parameter Estimation", "comments": "23 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two new Bayesian smoothing methods for general state-space models\nwith unknown parameters. The first approach is based on the particle learning\nand smoothing algorithm, but with an adjustment in the backward resampling\nweights. The second is a new method combining sequential parameter learning and\nsmoothing algorithms for general state-space models. This method is\nstraightforward but effective, and we find it is the best existing Sequential\nMonte Carlo algorithm to solve the joint Bayesian smoothing problem. We first\nillustrate the methods on three benchmark models using simulated data, and then\napply them to a stochastic volatility model for daily S&P 500 index returns\nduring the financial crisis.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 17:28:07 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Yang", "Biao", ""], ["Stroud", "Jonathan R.", ""], ["Huerta", "Gabriel", ""]]}, {"id": "1604.05661", "submitter": "Luca Rossini", "authors": "Fabrizio Leisen and Luca Rossini and Cristiano Villa", "title": "Objective Bayesian Analysis of the Yule-Simon Distribution with\n  Applications", "comments": "24 pages, 11 Figures, 7 Tables", "journal-ref": null, "doi": "10.1007/s00180-017-0735-1", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Yule-Simon distribution is usually employed in the analysis of frequency\ndata. As the Bayesian literature, so far, ignored this distribution, here we\nshow the derivation of two objective priors for the parameter of the Yule-Simon\ndistribution. In particular, we discuss the Jeffreys prior and a loss-based\nprior, which has recently appeared in the literature. We illustrate the\nperformance of the derived priors through a simulation study and the analysis\nof real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 17:30:45 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""], ["Villa", "Cristiano", ""]]}, {"id": "1604.06383", "submitter": "Adam Jaeger", "authors": "Adam Jaeger and Nicole Lazar", "title": "Piecewise Empirical Likelihood", "comments": "Significant changes have been made to article, renamed split sample\n  empirical likelihood. Replacement article is titled Split Sample Empirical\n  Likelihood arXiv:1703.03312", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric methods avoid the problem of having to specify a particular\ndata generating mechanism, but can be computationally intensive, reducing their\naccessibility for large data problems. Empirical likelihood, a non-parametric\napproach to the likelihood function, is also limited in application due to the\ncomputational demands necessary. We propose a new approach that combines\nmultiple non-parametric likelihood-type components to build a data-driven\napproximation of the true function. We will examine the theoretical properties\nof this piecewise empirical likelihood and demonstrate the computational gains\nof this methodology.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 16:52:55 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 16:19:06 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 17:29:55 GMT"}, {"version": "v4", "created": "Thu, 14 Dec 2017 18:40:13 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Jaeger", "Adam", ""], ["Lazar", "Nicole", ""]]}, {"id": "1604.06398", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin and Geir Storvik", "title": "Mode jumping MCMC for Bayesian variable selection in GLMM", "comments": "46 pages, 6 figures, 16 tables", "journal-ref": null, "doi": "10.1016/j.csda.2018.05.020", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear mixed models (GLMM) are used for inference and prediction\nin a wide range of different applications providing a powerful scientific tool.\nAn increasing number of sources of data are becoming available, introducing a\nvariety of candidate explanatory variables for these models. Selection of an\noptimal combination of variables is thus becoming crucial. In a Bayesian\nsetting, the posterior distribution of the models, based on the observed data,\ncan be viewed as a relevant measure for the model evidence. The number of\npossible models increases exponentially in the number of candidate variables.\nMoreover, the space of models has numerous local extrema in terms of posterior\nmodel probabilities. To resolve these issues a novel MCMC algorithm for the\nsearch through the model space via efficient mode jumping for GLMMs is\nintroduced. The algorithm is based on that marginal likelihoods can be\nefficiently calculated within each model. It is recommended that either exact\nexpressions or precise approximations of marginal likelihoods are applied. The\nsuggested algorithm is applied to simulated data, the famous U.S. crime data,\nprotein activity data and epigenetic data and is compared to several existing\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 17:47:23 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 16:16:04 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 17:04:41 GMT"}, {"version": "v4", "created": "Thu, 7 Jun 2018 09:51:26 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""]]}, {"id": "1604.06716", "submitter": "Guy Nason Prof.", "authors": "Guy P. Nason, Ben Powell, Duncan Elliott and Paul A. Smith", "title": "Supplementary Material for \"Should we sample a time series more\n  frequently? Decision support via multirate spectrum estimation (with\n  discussion)\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report includes an assortment of technical details and\nextended discussions related to paper \"Should we sample a time series more\nfrequently? Decision support via multirate spectrum estimation (with\ndiscussion)\", which introduces a model for estimating the log-spectral density\nof a stationary discrete time process given systematically missing data and\nmodels the cost implication for changing the sampling rate.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 15:42:25 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Nason", "Guy P.", ""], ["Powell", "Ben", ""], ["Elliott", "Duncan", ""], ["Smith", "Paul A.", ""]]}, {"id": "1604.06815", "submitter": "Christian M\\\"uller", "authors": "Jacob Bien, Irina Gaynanova, Johannes Lederer, Christian M\\\"uller", "title": "Non-convex Global Minimization and False Discovery Rate Control for the\n  TREX", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 2017, Vol. 27,\n  No. 1, 23-33", "doi": "10.1080/10618600.2017.1341414", "report-no": null, "categories": "stat.ML cs.OH stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TREX is a recently introduced method for performing sparse\nhigh-dimensional regression. Despite its statistical promise as an alternative\nto the lasso, square-root lasso, and scaled lasso, the TREX is computationally\nchallenging in that it requires solving a non-convex optimization problem. This\npaper shows a remarkable result: despite the non-convexity of the TREX problem,\nthere exists a polynomial-time algorithm that is guaranteed to find the global\nminimum. This result adds the TREX to a very short list of non-convex\noptimization problems that can be globally optimized (principal components\nanalysis being a famous example). After deriving and developing this new\napproach, we demonstrate that (i) the ability of the preexisting TREX heuristic\nto reach the global minimum is strongly dependent on the difficulty of the\nunderlying statistical problem, (ii) the new polynomial-time algorithm for TREX\npermits a novel variable ranking and selection scheme, (iii) this scheme can be\nincorporated into a rule that controls the false discovery rate (FDR) of\nincluded features in the model. To achieve this last aim, we provide an\nextension of the results of Barber & Candes (2015) to establish that the\nknockoff filter framework can be applied to the TREX. This investigation thus\nprovides both a rare case study of a heuristic for non-convex optimization and\na novel way of exploiting non-convexity for statistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 20:28:55 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 20:07:35 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bien", "Jacob", ""], ["Gaynanova", "Irina", ""], ["Lederer", "Johannes", ""], ["M\u00fcller", "Christian", ""]]}, {"id": "1604.06837", "submitter": "Martin Copenhaver", "authors": "Dimitris Bertsimas, Martin S. Copenhaver, Rahul Mazumder", "title": "Certifiably Optimal Low Rank Factor Analysis", "comments": null, "journal-ref": "JMLR 18(29) (2017)", "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor Analysis (FA) is a technique of fundamental importance that is widely\nused in classical and modern multivariate statistics, psychometrics and\neconometrics. In this paper, we revisit the classical rank-constrained FA\nproblem, which seeks to approximate an observed covariance matrix\n($\\boldsymbol\\Sigma$), by the sum of a Positive Semidefinite (PSD) low-rank\ncomponent ($\\boldsymbol\\Theta$) and a diagonal matrix ($\\boldsymbol\\Phi$) (with\nnonnegative entries) subject to $\\boldsymbol\\Sigma - \\boldsymbol\\Phi$ being\nPSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite\nOptimization based formulations for this task. We introduce a reformulation of\nthe problem as a smooth optimization problem with convex compact constraints\nand propose a unified algorithmic framework, utilizing state of the art\ntechniques in nonlinear optimization to obtain high-quality feasible solutions\nfor our proposed formulation. At the same time, by using a variety of\ntechniques from discrete and global optimization, we show that these solutions\nare certifiably optimal in many cases, even for problems with thousands of\nvariables. Our techniques are general and make no assumption on the underlying\nproblem data. The estimator proposed herein, aids statistical interpretability,\nprovides computational scalability and significantly improved accuracy when\ncompared to current, publicly available popular methods for rank-constrained\nFA. We demonstrate the effectiveness of our proposal on an array of synthetic\nand real-life datasets. To our knowledge, this is the first paper that\ndemonstrates how a previously intractable rank-constrained optimization problem\ncan be solved to provable optimality by coupling developments in convex\nanalysis and in discrete optimization.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 00:24:14 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Copenhaver", "Martin S.", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1604.07027", "submitter": "Shinichiro Shirota", "authors": "Shinichiro Shirota and Alan. E. Gelfand", "title": "Approximate Bayesian Computation and Model Validation for Repulsive\n  Spatial Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications involving spatial point patterns, we find evidence of\ninhibition or repulsion. The most commonly used class of models for such\nsettings are the Gibbs point processes. A recent alternative, at least to the\nstatistical community, is the determinantal point process. Here, we examine\nmodel fitting and inference for both of these classes of processes in a\nBayesian framework. While usual MCMC model fitting can be available, the\nalgorithms are complex and are not always well behaved. We propose using\napproximate Bayesian computation (ABC) for such fitting. This approach becomes\nattractive because, though likelihoods are very challenging to work with for\nthese processes, generation of realizations given parameter values is\nrelatively straightforward. As a result, the ABC fitting approach is\nwell-suited for these models. In addition, such simulation makes them\nwell-suited for posterior predictive inference as well as for model assessment.\nWe provide details for all of the above along with some simulation\ninvestigation and an illustrative analysis of a point pattern of tree data\nexhibiting repulsion. R-code and datasets are included in the supplementary\nmaterial.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 13:20:23 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 19:23:55 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Gelfand", "Alan. E.", ""]]}, {"id": "1604.07080", "submitter": "Josmar Mazucheli", "authors": "K. V. P. Barco, J. Mazucheli, and V. Janeiro", "title": "The Power Inverse Lindley Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several probability distributions have been proposed in the literature,\nespecially with the aim of obtaining models that are more flexible relative to\nthe behaviors of the density and hazard rate functions. Recently, a new\ngeneralization of the Lindley distribution was proposed by Ghitany et al.\n(2013), called power Lindley distribution. Another generaliza- tion was\nproposed by Sharma et al. (2015), known as inverse Lindley distribution. In\nthis paper, a new distribution is proposed, which is obtained from these two\ngeneralizations and named power inverse Lindley distribution. Some properties\nof this new distribution and study of the behavior of maximum likelihood\nestimators are presented and discussed. It is also applied considering real\ndata and compared with the fits obtained for already- known distributions. When\napplied, the power inverse Lindley distribution was found to be a good\nalternative for modeling survival data.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 20:40:09 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Barco", "K. V. P.", ""], ["Mazucheli", "J.", ""], ["Janeiro", "V.", ""]]}, {"id": "1604.07177", "submitter": "Sinan Y{\\i}ld{\\i}r{\\i}m", "authors": "Sinan Y{\\i}ld{\\i}r{\\i}m", "title": "On the Use of Penalty MCMC for Differential Privacy", "comments": "15 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We view the penalty algorithm of Ceperley and Dewing (1999), a Markov chain\nMonte Carlo (MCMC) algorithm for Bayesian inference, in the context of data\nprivacy. Specifically, we study differential privacy of the penalty algorithm\nand advocate its use for data privacy. We show that in the simple model of\nindependent observations the algorithm has desirable convergence and privacy\nproperties that scale with data size. Two special cases are also investigated\nand privacy preserving schemes are proposed for those cases: (i) Data are\ndistributed among several data owners who are interested in the inference of a\ncommon parameter while preserving their data privacy. (ii) The data likelihood\nbelongs to an exponential family.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 09:19:22 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Y\u0131ld\u0131r\u0131m", "Sinan", ""]]}, {"id": "1604.07299", "submitter": "Matthew Moores", "authors": "Matthew Moores, Kirsten Gracie, Jake Carson, Karen Faulds, Duncan\n  Graham, Mark Girolami", "title": "Bayesian modelling and quantification of Raman spectroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raman spectroscopy can be used to identify molecules such as DNA by the\ncharacteristic scattering of light from a laser. It is sensitive at very low\nconcentrations and can accurately quantify the amount of a given molecule in a\nsample. The presence of a large, nonuniform background presents a major\nchallenge to analysis of these spectra. To overcome this challenge, we\nintroduce a sequential Monte Carlo (SMC) algorithm to separate each observed\nspectrum into a series of peaks plus a smoothly-varying baseline, corrupted by\nadditive white noise. The peaks are modelled as Lorentzian, Gaussian, or\npseudo-Voigt functions, while the baseline is estimated using a penalised cubic\nspline. This latent continuous representation accounts for differences in\nresolution between measurements. The posterior distribution can be\nincrementally updated as more data becomes available, resulting in a scalable\nalgorithm that is robust to local maxima. By incorporating this representation\nin a Bayesian hierarchical regression model, we can quantify the relationship\nbetween molecular concentration and peak intensity, thereby providing an\nimproved estimate of the limit of detection, which is of major importance to\nanalytical chemistry.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 15:07:29 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 22:22:30 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Moores", "Matthew", ""], ["Gracie", "Kirsten", ""], ["Carson", "Jake", ""], ["Faulds", "Karen", ""], ["Graham", "Duncan", ""], ["Girolami", "Mark", ""]]}, {"id": "1604.07451", "submitter": "Guo Yu", "authors": "Guo Yu and Jacob Bien", "title": "Learning Local Dependence In Ordered Data", "comments": null, "journal-ref": "Journal of Machine Learning (2017) 18(42) 1-60", "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, data come with a natural ordering. This ordering can\noften induce local dependence among nearby variables. However, in complex data,\nthe width of this dependence may vary, making simple assumptions such as a\nconstant neighborhood size unrealistic. We propose a framework for learning\nthis local dependence based on estimating the inverse of the Cholesky factor of\nthe covariance matrix. Penalized maximum likelihood estimation of this matrix\nyields a simple regression interpretation for local dependence in which\nvariables are predicted by their neighbors. Our proposed method involves\nsolving a convex, penalized Gaussian likelihood problem with a hierarchical\ngroup lasso penalty. The problem decomposes into independent subproblems which\ncan be solved efficiently in parallel using first-order methods. Our method\nyields a sparse, symmetric, positive definite estimator of the precision\nmatrix, encoding a Gaussian graphical model. We derive theoretical results not\nfound in existing methods attaining this structure. In particular, our\nconditions for signed support recovery and estimation consistency rates in\nmultiple norms are as mild as those in a regression problem. Empirical results\nshow our method performing favorably compared to existing methods. We apply our\nmethod to genomic data to flexibly model linkage disequilibrium. Our method is\nalso applied to improve the performance of discriminant analysis in sound\nrecording classification.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 21:20:51 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 21:42:25 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 03:15:08 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Yu", "Guo", ""], ["Bien", "Jacob", ""]]}, {"id": "1604.07949", "submitter": "Worapree Ole Maneesoonthorn", "authors": "Gael M. Martin, Brendan P.M. McCabe, David T. Frazier, Worapree\n  Maneesoonthorn and Christian P. Robert", "title": "Auxiliary Likelihood-Based Approximate Bayesian Computation in State\n  Space Models", "comments": "This paper is forthcoming at the Journal of Computational and\n  Graphical Statistics. It also supersedes the earlier arXiv paper \"Approximate\n  Bayesian Computation in State Space Models\" (arXiv:1409.8363)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computationally simple approach to inference in state space models is\nproposed, using approximate Bayesian computation (ABC). ABC avoids evaluation\nof an intractable likelihood by matching summary statistics for the observed\ndata with statistics computed from data simulated from the true process, based\non parameter draws from the prior. Draws that produce a 'match' between\nobserved and simulated summaries are retained, and used to estimate the\ninaccessible posterior. With no reduction to a low-dimensional set of\nsufficient statistics being possible in the state space setting, we define the\nsummaries as the maximum of an auxiliary likelihood function, and thereby\nexploit the asymptotic sufficiency of this estimator for the auxiliary\nparameter vector. We derive conditions under which this approach - including a\ncomputationally efficient version based on the auxiliary score - achieves\nBayesian consistency. To reduce the well-documented inaccuracy of ABC in\nmulti-parameter settings, we propose the separate treatment of each parameter\ndimension using an integrated likelihood technique. Three stochastic volatility\nmodels for which exact Bayesian inference is either computationally\nchallenging, or infeasible, are used for illustration. We demonstrate that our\napproach compares favorably against an extensive set of approximate and exact\ncomparators. An empirical illustration completes the paper.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 06:55:26 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 05:46:58 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2018 22:53:41 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Martin", "Gael M.", ""], ["McCabe", "Brendan P. M.", ""], ["Frazier", "David T.", ""], ["Maneesoonthorn", "Worapree", ""], ["Robert", "Christian P.", ""]]}, {"id": "1604.08016", "submitter": "Florian Maire", "authors": "Florian Maire, Nial Friel, Antonietta Mira, Adrian Raftery", "title": "Adaptive Incremental Mixture Markov chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Adaptive Incremental Mixture Markov chain Monte Carlo (AIMM), a\nnovel approach to sample from challenging probability distributions defined on\na general state-space. While adaptive MCMC methods usually update a parametric\nproposal kernel with a global rule, AIMM locally adapts a semiparametric\nkernel. AIMM is based on an independent Metropolis-Hastings proposal\ndistribution which takes the form of a finite mixture of Gaussian\ndistributions. Central to this approach is the idea that the proposal\ndistribution adapts to the target by locally adding a mixture component when\nthe discrepancy between the proposal mixture and the target is deemed to be too\nlarge. As a result, the number of components in the mixture proposal is not\nfixed in advance. Theoretically, we prove that there exists a process that can\nbe made arbitrarily close to AIMM and that converges to the correct target\ndistribution. We also illustrate that it performs well in practice in a variety\nof challenging situations, including high-dimensional and multimodal target\ndistributions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 10:59:56 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 09:50:06 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Maire", "Florian", ""], ["Friel", "Nial", ""], ["Mira", "Antonietta", ""], ["Raftery", "Adrian", ""]]}, {"id": "1604.08098", "submitter": "Lei Han", "authors": "Lei Han, Kean Ming Tan, Ting Yang and Tong Zhang", "title": "Local Uncertainty Sampling for Large-Scale Multi-Class Logistic\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for building statistical models in the big data era is that\nthe available data volume far exceeds the computational capability. A common\napproach for solving this problem is to employ a subsampled dataset that can be\nhandled by available computational resources. In this paper, we propose a\ngeneral subsampling scheme for large-scale multi-class logistic regression and\nexamine the variance of the resulting estimator. We show that asymptotically,\nthe proposed method always achieves a smaller variance than that of the uniform\nrandom sampling. Moreover, when the classes are conditionally imbalanced,\nsignificant improvement over uniform sampling can be achieved. Empirical\nperformance of the proposed method is compared to other methods on both\nsimulated and real-world datasets, and these results match and confirm our\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 15:00:12 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 09:05:18 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 05:08:08 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Han", "Lei", ""], ["Tan", "Kean Ming", ""], ["Yang", "Ting", ""], ["Zhang", "Tong", ""]]}, {"id": "1604.08102", "submitter": "Dennis Prangle", "authors": "Dennis Prangle and Richard G. Everitt", "title": "An ABC interpretation of the multiple auxiliary variable method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the auxiliary variable method (M{\\o}ller et al., 2006; Murray et\nal., 2006) for inference of Markov random fields can be viewed as an\napproximate Bayesian computation method for likelihood estimation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 15:07:48 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Prangle", "Dennis", ""], ["Everitt", "Richard G.", ""]]}, {"id": "1604.08320", "submitter": "Xun Huan", "authors": "Xun Huan, Youssef M. Marzouk", "title": "Sequential Bayesian optimal experimental design via approximate dynamic\n  programming", "comments": "Preprint 34 pages, 12 figures (36 small figures). v1 submitted to the\n  SIAM/ASA Journal on Uncertainty Quantification on April 27, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of multiple experiments is commonly undertaken via suboptimal\nstrategies, such as batch (open-loop) design that omits feedback or greedy\n(myopic) design that does not account for future effects. This paper introduces\nnew strategies for the optimal design of sequential experiments. First, we\nrigorously formulate the general sequential optimal experimental design (sOED)\nproblem as a dynamic program. Batch and greedy designs are shown to result from\nspecial cases of this formulation. We then focus on sOED for parameter\ninference, adopting a Bayesian formulation with an information theoretic design\nobjective. To make the problem tractable, we develop new numerical approaches\nfor nonlinear design with continuous parameter, design, and observation spaces.\nWe approximate the optimal policy by using backward induction with regression\nto construct and refine value function approximations in the dynamic program.\nThe proposed algorithm iteratively generates trajectories via exploration and\nexploitation to improve approximation accuracy in frequently visited regions of\nthe state space. Numerical results are verified against analytical solutions in\na linear-Gaussian setting. Advantages over batch and greedy design are then\ndemonstrated on a nonlinear source inversion problem where we seek an optimal\npolicy for sequential sensing.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 06:32:27 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Huan", "Xun", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1604.08616", "submitter": "Priyam Das", "authors": "Priyam Das", "title": "Black-box optimization on hyper-rectangle using Recursive Modified\n  Pattern Search and application to ROC-based Classification Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Statistics, multi-modal and non-smooth likelihood (or, objective function)\nmaximization problems often arise with known upper and lower bound of the\nparameters. A novel derivative-free global optimization technique is developed\nto optimize any black-box function on a hyper-rectangular euclidean space. In\nliterature, pattern search technique has been shown to be a powerful tool for\nblackbox optimization. The proposed algorithm follows the principle of pattern\nsearch technique where new updated solution is obtained from the current\nsolution making movements (within the constrained sample space) along the\ncoordinates. Before making a jump from the current solution point to a new\nsolution point, objective function is evaluated in several neighborhood points\naround the current solution and the best solution point is chosen based on the\nobjective function values at those points. Parallel threading can be used to\nmake the algorithm more scalable. Performance of the proposed method is\nevaluated based on optimization of upto 5000 dimensional multi-modal benchmark\nfunctions. The proposed algorithm is shown to perform upto 40 and 368 times\nfaster compared to Genetic Algorithm (GA) and Simulated Annealing (SA)\nrespectively. The proposed method is used to estimate the optimal biomarker\ncombination from Alzheimer data by maximizing the empirical estimates of area\nunder ROC curve.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 20:59:31 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 05:43:53 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 05:49:22 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Das", "Priyam", ""]]}]