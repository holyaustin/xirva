[{"id": "1512.00205", "submitter": "Simon Barthelm\\'e", "authors": "Simon Barthelm\\'e, Nicolas Chopin, Vincent Cottet", "title": "Divide and conquer in ABC: Expectation-Progagation algorithms for\n  likelihood-free inference", "comments": "To appear in the forthcoming Handbook of Approximate Bayesian\n  Computation (ABC), edited by S. Sisson, L. Fan, and M. Beaumont", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ABC algorithms are notoriously expensive in computing time, as they require\nsimulating many complete artificial datasets from the model. We advocate in\nthis paper a \"divide and conquer\" approach to ABC, where we split the\nlikelihood into n factors, and combine in some way n \"local\" ABC approximations\nof each factor. This has two advantages: (a) such an approach is typically much\nfaster than standard ABC and (b) it makes it possible to use local summary\nstatistics (i.e. summary statistics that depend only on the data-points that\ncorrespond to a single factor), rather than global summary statistics (that\ndepend on the complete dataset). This greatly alleviates the bias introduced by\nsummary statistics, and even removes it entirely in situations where local\nsummary statistics are simply the identity function.\n  We focus on EP (Expectation-Propagation), a convenient and powerful way to\ncombine n local approximations into a global approximation. Compared to the EP-\nABC approach of Barthelm\\'e and Chopin (2014), we present two variations, one\nbased on the parallel EP algorithm of Cseke and Heskes (2011), which has the\nadvantage of being implementable on a parallel architecture, and one version\nwhich bridges the gap between standard EP and parallel EP. We illustrate our\napproach with an expensive application of ABC, namely inference on spatial\nextremes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 10:16:46 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Barthelm\u00e9", "Simon", ""], ["Chopin", "Nicolas", ""], ["Cottet", "Vincent", ""]]}, {"id": "1512.00379", "submitter": "Lakshmi Roychowdhury", "authors": "Lakshmi Roychowdhury", "title": "Optimal quantization for nonuniform Cantor distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a Borel probability measure on $\\mathbb R$ such that $P=\\frac 1 4\nP\\circ S_1^{-1} +\\frac 3 4 P\\circ S_2^{-1}$, where $S_1$ and $S_2$ are two\nsimilarity mappings on $\\mathbb R$ such that $S_1(x)=\\frac 1 4 x $ and\n$S_2(x)=\\frac 1 2 x +\\frac 12$ for all $x\\in \\mathbb R$. Such a probability\nmeasure $P$ has support the Cantor set generated by $S_1$ and $S_2$. For this\nprobability measure, in this paper, we give an induction formula to determine\nthe optimal sets of $n$-means and the $n$th quantization errors for all $n\\geq\n2$. We have shown that the same induction formula also works for the Cantor\ndistribution $P:=\\psi^2 P\\circ S_1^{-1} +\\psi^4 P\\circ S_2^{-1}$ supported by\nthe Cantor set generated by $S_1(x)=\\frac 13x$ and $S_2(x)=\\frac 13 x+\\frac 23$\nfor all $x\\in \\mathbb R$, where $\\psi$ is the square root of the Golden ratio\n$\\frac 12(\\sqrt 5-1)$. In addition, we give a counter example to show that the\ninduction formula does not work for all Cantor distributions. Using the\ninduction formula we obtain some results and observations which are also given\nin this paper.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 18:34:30 GMT"}, {"version": "v10", "created": "Mon, 18 Nov 2019 16:31:32 GMT"}, {"version": "v11", "created": "Tue, 19 Nov 2019 05:10:14 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 18:54:22 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2016 19:07:31 GMT"}, {"version": "v4", "created": "Wed, 2 Mar 2016 15:02:38 GMT"}, {"version": "v5", "created": "Thu, 7 Apr 2016 05:42:25 GMT"}, {"version": "v6", "created": "Mon, 13 Jun 2016 18:42:38 GMT"}, {"version": "v7", "created": "Sun, 20 Nov 2016 18:57:41 GMT"}, {"version": "v8", "created": "Sun, 28 Jan 2018 20:57:46 GMT"}, {"version": "v9", "created": "Thu, 14 Nov 2019 06:49:14 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Roychowdhury", "Lakshmi", ""]]}, {"id": "1512.00792", "submitter": "Rebecca Steorts", "authors": "Jeffrey Miller, Brenda Betancourt, Abbas Zaidi, Hanna Wallach, and\n  Rebecca C. Steorts", "title": "Microclustering: When the Cluster Sizes Grow Sublinearly with the Size\n  of the Data Set", "comments": "8 pages, 3 figures, NIPS Bayesian Nonparametrics: The Next Generation\n  Workshop Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some tasks, this\nassumption is undesirable. For example, when performing entity resolution, the\nsize of each cluster is often unrelated to the size of the data set.\nConsequently, each cluster contains a negligible fraction of the total number\nof data points. Such tasks therefore require models that yield clusters whose\nsizes grow sublinearly with the size of the data set. We address this\nrequirement by defining the \\emph{microclustering property} and introducing a\nnew model that exhibits this property. We compare this model to several\ncommonly used clustering models by checking model fit using real and simulated\ndata sets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 18:08:48 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Miller", "Jeffrey", ""], ["Betancourt", "Brenda", ""], ["Zaidi", "Abbas", ""], ["Wallach", "Hanna", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1512.00825", "submitter": "Anne van Delft Dr.", "authors": "Anne van Delft and Michael Eichler", "title": "Data-adaptive estimation of time-varying spectral densities", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 28:2, 244-255,\n  2019", "doi": "10.1080/10618600.2018.1512866", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a data-adaptive non-parametric approach for the\nestimation of time-varying spectral densities from nonstationary time series.\nTime-varying spectral densities are commonly estimated by local kernel\nsmoothing. The performance of these nonparametric estimators, however, depends\ncrucially on the smoothing bandwidths that need to be specified in both time\nand frequency direction. As an alternative and extension to traditional\nbandwidth selection methods, we propose an iterative algorithm for constructing\nlocalized smoothing kernels data-adaptively. The main idea, inspired by the\nconcept of propagation-separation (Polzehl and Spokoiny 2006), is to determine\nfor a point in the time-frequency plane the largest local vicinity over which\nsmoothing is justified by the data. By shaping the smoothing kernels\nnonparametrically, our method not only avoids the problem of bandwidth\nselection in the strict sense but also becomes more flexible. It not only\nadapts to changing curvature in smoothly varying spectra but also adjusts for\nstructural breaks in the time-varying spectrum.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 19:56:38 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 16:18:22 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 22:32:45 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["van Delft", "Anne", ""], ["Eichler", "Michael", ""]]}, {"id": "1512.00933", "submitter": "Francois-Xavier Briol", "authors": "Fran\\c{c}ois-Xavier Briol, Chris. J. Oates, Mark Girolami, Michael A.\n  Osborne and Dino Sejdinovic", "title": "Probabilistic Integration: A Role in Statistical Computation?", "comments": "Several improvements suggested by reviewers, including additional\n  experiments on uncertainty quantification properties. Change of title:\n  previously \"Probabilistic Integration: A Role for Statisticians in Numerical\n  Analysis?\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A research frontier has emerged in scientific computation, wherein numerical\nerror is regarded as a source of epistemic uncertainty that can be modelled.\nThis raises several statistical challenges, including the design of statistical\nmethods that enable the coherent propagation of probabilities through a\n(possibly deterministic) computational work-flow. This paper examines the case\nfor probabilistic numerical methods in routine statistical computation. Our\nfocus is on numerical integration, where a probabilistic integrator is equipped\nwith a full distribution over its output that reflects the presence of an\nunknown numerical error. Our main technical contribution is to establish, for\nthe first time, rates of posterior contraction for these methods. These show\nthat probabilistic integrators can in principle enjoy the \"best of both\nworlds\", leveraging the sampling efficiency of Monte Carlo methods whilst\nproviding a principled route to assess the impact of numerical error on\nscientific conclusions. Several substantial applications are provided for\nillustration and critical evaluation, including examples from statistical\nmodelling, computer graphics and a computer model for an oil reservoir.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 02:52:33 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 22:29:14 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 06:09:18 GMT"}, {"version": "v4", "created": "Mon, 11 Apr 2016 09:15:20 GMT"}, {"version": "v5", "created": "Thu, 20 Oct 2016 08:44:17 GMT"}, {"version": "v6", "created": "Wed, 18 Oct 2017 14:15:40 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Briol", "Fran\u00e7ois-Xavier", ""], ["Oates", "Chris. J.", ""], ["Girolami", "Mark", ""], ["Osborne", "Michael A.", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1512.00982", "submitter": "Jere Koskela", "authors": "Jere Koskela and Paul A. Jenkins and Dario Span\\`o", "title": "Bayesian non-parametric inference for $\\Lambda$-coalescents: consistency\n  and a parametric method", "comments": "28 pages, 3 figures", "journal-ref": "Bernoulli 24(3):2122-2153, 2018", "doi": "10.3150/16-BEJ923", "report-no": null, "categories": "stat.ME math.PR math.ST q-bio.PE stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate Bayesian non-parametric inference of the $\\Lambda$-measure of\n$\\Lambda$-coalescent processes with recurrent mutation, parametrised by\nprobability measures on the unit interval. We give verifiable criteria on the\nprior for posterior consistency when observations form a time series, and prove\nthat any non-trivial prior is inconsistent when all observations are\ncontemporaneous. We then show that the likelihood given a data set of size $n\n\\in \\mathbb{N}$ is constant across $\\Lambda$-measures whose leading $n - 2$\nmoments agree, and focus on inferring truncated sequences of moments. We\nprovide a large class of functionals which can be extremised using finite\ncomputation given a credible region of posterior truncated moment sequences,\nand a pseudo-marginal Metropolis-Hastings algorithm for sampling the posterior.\nFinally, we compare the efficiency of the exact and noisy pseudo-marginal\nalgorithms with and without delayed acceptance acceleration using a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 08:03:36 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 15:32:37 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2016 11:03:21 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2016 16:19:58 GMT"}, {"version": "v5", "created": "Mon, 23 Jan 2017 18:21:10 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Koskela", "Jere", ""], ["Jenkins", "Paul A.", ""], ["Span\u00f2", "Dario", ""]]}, {"id": "1512.01022", "submitter": "Matti Vihola", "authors": "Matti Vihola", "title": "Unbiased estimators and multilevel Monte Carlo", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel Monte Carlo (MLMC) and unbiased estimators recently proposed by\nMcLeish (Monte Carlo Methods Appl., 2011) and Rhee and Glynn (Oper. Res., 2015)\nare closely related. This connection is elaborated by presenting a new general\nclass of unbiased estimators, which admits previous debiasing schemes as\nspecial cases. New lower variance estimators are proposed, which are stratified\nversions of earlier unbiased schemes. Under general conditions, essentially\nwhen MLMC admits the canonical square root Monte Carlo error rate, the proposed\nnew schemes are shown to be asymptotically as efficient as MLMC, both in terms\nof variance and cost. The experiments demonstrate that the variance reduction\nprovided by the new schemes can be substantial.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 10:11:59 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 16:32:28 GMT"}, {"version": "v3", "created": "Tue, 29 Nov 2016 17:24:24 GMT"}, {"version": "v4", "created": "Thu, 11 May 2017 13:50:42 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Vihola", "Matti", ""]]}, {"id": "1512.01027", "submitter": "Firas Hamze", "authors": "Firas Hamze, Evgeny Andryash", "title": "Discrete Equilibrium Sampling with Arbitrary Nonequilibrium Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cond-mat.dis-nn cond-mat.stat-mech cs.AI physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for performing statistical sampling, expectation\nestimation, and partition function approximation using \\emph{arbitrary}\nheuristic stochastic processes defined over discrete state spaces. Using a\nhighly parallel construction we call the \\emph{sequential constraining\nprocess}, we are able to simultaneously generate states with the heuristic\nprocess and accurately estimate their probabilities, even when they are far too\nsmall to be realistically inferred by direct counting. After showing that both\ntheoretically correct importance sampling and Markov chain Monte Carlo are\npossible using the sequential constraining process, we integrate it into a\nmethodology called \\emph{state space sampling}, extending the ideas of state\nspace search from computer science to the sampling context. The methodology\ncomprises a dynamic data structure that constructs a robust Bayesian model of\nthe statistics generated by the heuristic process subject to an accuracy\nconstraint, the posterior Kullback-Leibler divergence. Sampling from the\ndynamic structure will generally yield partial states, which are completed by\nrecursively calling the heuristic to refine the structure and resuming the\nsampling. Our experiments on various Ising models suggest that state space\nsampling enables heuristic state generation with accurate probability\nestimates, demonstrated by illustrating the convergence of a simulated\nannealing process to the Boltzmann distribution with increasing run length.\nConsequently, heretofore unprecedented direct importance sampling using the\n\\emph{final} (marginal) distribution of a generic stochastic process is\nallowed, potentially augmenting the range of algorithms at the Monte Carlo\npractitioner's disposal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 10:28:58 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Hamze", "Firas", ""], ["Andryash", "Evgeny", ""]]}, {"id": "1512.01139", "submitter": "Vivak Patel", "authors": "Vivak Patel", "title": "Kalman-based Stochastic Gradient Method with Stop Condition and\n  Insensitivity to Conditioning", "comments": null, "journal-ref": "SIAM J. Optim. 26 (2016) 2620-2648", "doi": "10.1137/15M1048239", "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern proximal and stochastic gradient descent (SGD) methods are believed to\nefficiently minimize large composite objective functions, but such methods have\ntwo algorithmic challenges: (1) a lack of fast or justified stop conditions,\nand (2) sensitivity to the objective function's conditioning. In response to\nthe first challenge, modern proximal and SGD methods guarantee convergence only\nafter multiple epochs, but such a guarantee renders proximal and SGD methods\ninfeasible when the number of component functions is very large or infinite. In\nresponse to the second challenge, second order SGD methods have been developed,\nbut they are marred by the complexity of their analysis. In this work, we\naddress these challenges on the limited, but important, linear regression\nproblem by introducing and analyzing a second order proximal/SGD method based\non Kalman Filtering (kSGD). Through our analysis, we show kSGD is\nasymptotically optimal, develop a fast algorithm for very large, infinite or\nstreaming data sources with a justified stop condition, prove that kSGD is\ninsensitive to the problem's conditioning, and develop a unique approach for\nanalyzing the complex second order dynamics. Our theoretical results are\nsupported by numerical experiments on three regression problems (linear,\nnonparametric wavelet, and logistic) using three large publicly available\ndatasets. Moreover, our analysis and experiments lay a foundation for embedding\nkSGD in multiple epoch algorithms, extending kSGD to other problem classes, and\ndeveloping parallel and low memory kSGD implementations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 16:13:33 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 21:51:38 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Patel", "Vivak", ""]]}, {"id": "1512.01272", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka, Patrick Shafto, Eric Jonas, Cap Petschulat, Max\n  Gasner, Joshua B. Tenenbaum", "title": "CrossCat: A Fully Bayesian Nonparametric Method for Analyzing\n  Heterogeneous, High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a widespread need for statistical methods that can analyze\nhigh-dimensional datasets with- out imposing restrictive or opaque modeling\nassumptions. This paper describes a domain-general data analysis method called\nCrossCat. CrossCat infers multiple non-overlapping views of the data, each\nconsisting of a subset of the variables, and uses a separate nonparametric\nmixture to model each view. CrossCat is based on approximately Bayesian\ninference in a hierarchical, nonparamet- ric model for data tables. This model\nconsists of a Dirichlet process mixture over the columns of a data table in\nwhich each mixture component is itself an independent Dirichlet process mixture\nover the rows; the inner mixture components are simple parametric models whose\nform depends on the types of data in the table. CrossCat combines strengths of\nmixture modeling and Bayesian net- work structure learning. Like mixture\nmodeling, CrossCat can model a broad class of distributions by positing latent\nvariables, and produces representations that can be efficiently conditioned and\nsampled from for prediction. Like Bayesian networks, CrossCat represents the\ndependencies and independencies between variables, and thus remains accurate\nwhen there are multiple statistical signals. Inference is done via a scalable\nGibbs sampling scheme; this paper shows that it works well in practice. This\npaper also includes empirical results on heterogeneous tabular data of up to 10\nmillion cells, such as hospital cost and quality measures, voting records,\nunemployment rates, gene expression measurements, and images of handwritten\ndigits. CrossCat infers structure that is consistent with accepted findings and\ncommon-sense knowledge in multiple domains and yields predictive accuracy\ncompetitive with generative, discriminative, and model-free alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 22:39:37 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Shafto", "Patrick", ""], ["Jonas", "Eric", ""], ["Petschulat", "Cap", ""], ["Gasner", "Max", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1512.01496", "submitter": "Radu Craiu", "authors": "Roberto Casarin, Radu V. Craiu and Fabrizio Leisen", "title": "Embarrassingly Parallel Sequential Markov-chain Monte Carlo for Large\n  Sets of Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian computation crucially relies on Markov chain Monte Carlo (MCMC)\nalgorithms. In the case of massive data sets, running the Metropolis-Hastings\nsampler to draw from the posterior distribution becomes prohibitive due to the\nlarge number of likelihood terms that need to be calculated at each iteration.\nIn order to perform Bayesian inference for a large set of time series, we\nconsider an algorithm that combines 'divide and conquer\" ideas previously used\nto design MCMC algorithms for big data with a sequential MCMC strategy. The\nperformance of the method is illustrated using a large set of financial data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 17:56:41 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Casarin", "Roberto", ""], ["Craiu", "Radu V.", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1512.01631", "submitter": "Xiaohan Yan", "authors": "Xiaohan Yan and Jacob Bien", "title": "Hierarchical Sparse Modeling: A Choice of Two Group Lasso Formulations", "comments": "30 pages, 13 figures", "journal-ref": "Statist. Sci. 32 (2017), no. 4, 531--560", "doi": "10.1214/17-STS622", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demanding sparsity in estimated models has become a routine practice in\nstatistics. In many situations, we wish to require that the sparsity patterns\nattained honor certain problem-specific constraints. Hierarchical sparse\nmodeling (HSM) refers to situations in which these constraints specify that one\nset of parameters be set to zero whenever another is set to zero. In recent\nyears, numerous papers have developed convex regularizers for this form of\nsparsity structure, which arises in many areas of statistics including\ninteraction modeling, time series analysis, and covariance estimation. In this\npaper, we observe that these methods fall into two frameworks, the group lasso\n(GL) and latent overlapping group lasso (LOG), which have not been\nsystematically compared in the context of HSM. The purpose of this paper is to\nprovide a side-by-side comparison of these two frameworks for HSM in terms of\ntheir statistical properties and computational efficiency. We call special\nattention to GL's more aggressive shrinkage of parameters deep in the\nhierarchy, a property not shared by LOG. In terms of computation, we introduce\na finite-step algorithm that exactly solves the proximal operator of LOG for a\ncertain simple HSM structure; we later exploit this to develop a novel\npath-based block coordinate descent scheme for general HSM structures. Both\nalgorithms greatly improve the computational performance of LOG. Finally, we\ncompare the two methods in the context of covariance estimation, where we\nintroduce a new sparsely-banded estimator using LOG, which we show achieves the\nstatistical advantages of an existing GL-based method but is simpler to express\nand more efficient to compute.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 07:00:54 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 02:13:49 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 04:03:45 GMT"}, {"version": "v4", "created": "Wed, 29 Nov 2017 20:05:56 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yan", "Xiaohan", ""], ["Bien", "Jacob", ""]]}, {"id": "1512.01633", "submitter": "Claudio Agostinelli", "authors": "Claudio Agostinelli, Alfio Marazzi, Victor J. Yohai and Alex\n  Randriamiharisoa", "title": "Robust Estimation of the Generalized Loggamma Model. The R Package\n  robustloggamma", "comments": "Accepted in Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  robustloggamma is an R package for robust estimation and inference in the\ngeneralized loggamma model. We briefly introduce the model, the estimation\nprocedures and the computational algorithms. Then, we illustrate the use of the\npackage with the help of a real data set.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 07:11:26 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Agostinelli", "Claudio", ""], ["Marazzi", "Alfio", ""], ["Yohai", "Victor J.", ""], ["Randriamiharisoa", "Alex", ""]]}, {"id": "1512.01779", "submitter": "Zaid Sawlan", "authors": "Ivo Babuska, Zaid Sawlan, Marco Scavino, Barna Szab\\'o, Ra\\'ul Tempone", "title": "Bayesian inference and model comparison for metallic fatigue data", "comments": null, "journal-ref": "Computer Methods in Applied Mechanics and Engineering 304 (2016)\n  171--196", "doi": "10.1016/j.cma.2016.02.013", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a statistical treatment of stress-life (S-N) data\ndrawn from a collection of records of fatigue experiments that were performed\non 75S-T6 aluminum alloys. Our main objective is to predict the fatigue life of\nmaterials by providing a systematic approach to model calibration, model\nselection and model ranking with reference to S-N data. To this purpose, we\nconsider fatigue-limit models and random fatigue-limit models that are\nspecially designed to allow the treatment of the run-outs (right-censored\ndata). We first fit the models to the data by maximum likelihood methods and\nestimate the quantiles of the life distribution of the alloy specimen. To\nassess the robustness of the estimation of the quantile functions, we obtain\nbootstrap confidence bands by stratified resampling with respect to the cycle\nratio. We then compare and rank the models by classical measures of fit based\non information criteria. We also consider a Bayesian approach that provides,\nunder the prior distribution of the model parameters selected by the user,\ntheir simulation-based posterior distributions. We implement and apply Bayesian\nmodel comparison methods, such as Bayes factor ranking and predictive\ninformation criteria based on cross-validation techniques under various a\npriori scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 11:30:34 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Babuska", "Ivo", ""], ["Sawlan", "Zaid", ""], ["Scavino", "Marco", ""], ["Szab\u00f3", "Barna", ""], ["Tempone", "Ra\u00fal", ""]]}, {"id": "1512.02097", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based\n  Parameter-Insensitive Clustering Method", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most density-based clustering methods largely rely on how well the underlying\ndensity is estimated. However, density estimation itself is also a challenging\nproblem, especially the determination of the kernel bandwidth. A large\nbandwidth could lead to the over-smoothed density estimation in which the\nnumber of density peaks could be less than the true clusters, while a small\nbandwidth could lead to the under-smoothed density estimation in which spurious\ndensity peaks, or called the \"ripple noise\", would be generated in the\nestimated density. In this paper, we propose a density-based hierarchical\nclustering method, called the Deep Nearest Neighbor Descent (D-NND), which\ncould learn the underlying density structure layer by layer and capture the\ncluster structure at the same time. The over-smoothed density estimation could\nbe largely avoided and the negative effect of the under-estimated cases could\nbe also largely reduced. Overall, D-NND presents not only the strong capability\nof discovering the underlying cluster structure but also the remarkable\nreliability due to its insensitivity to parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 15:47:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1512.02271", "submitter": "Damon McDougall", "authors": "Damon McDougall and Richard Moore", "title": "Optimal strategies for the control of autonomous vehicles in data\n  assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to compute optimal control paths for autonomous vehicles\ndeployed for the purpose of inferring a velocity field. In addition to being\nadvected by the flow, the vehicles are able to effect a fixed relative speed\nwith arbitrary control over direction. It is this direction that is used as the\nbasis for the locally optimal control algorithm presented here, with objective\nformed from the variance trace of the expected posterior distribution. We\npresent results for linear flows near hyperbolic fixed points.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 22:31:40 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["McDougall", "Damon", ""], ["Moore", "Richard", ""]]}, {"id": "1512.02452", "submitter": "Allan De Freitas", "authors": "Allan De Freitas, Fran\\c{c}ois Septier, Lyudmila Mihaylova", "title": "Sequential Markov Chain Monte Carlo for Bayesian Filtering with Massive\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in digital sensors, digital data storage and communications have\nresulted in systems being capable of accumulating large collections of data. In\nthe light of dealing with the challenges that massive data present, this work\nproposes solutions to inference and filtering problems within the Bayesian\nframework. Two novel Bayesian inference algorithms are developed for non-linear\nand non-Gaussian state space models, able to deal with large volumes of data\n(or observations). These are sequential Markov chain Monte Carlo (MCMC)\napproaches relying on two key ideas: 1) subsample the massive data and utilise\na smaller subset for filtering and inference, and 2) a divide and conquer type\napproach computing local filtering distributions each using a subset of the\nmeasurements. Simulation results highlight the accuracy and the large\ncomputational savings, that can reach 90% by the proposed algorithms when\ncompared with standard techniques.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 13:20:03 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["De Freitas", "Allan", ""], ["Septier", "Fran\u00e7ois", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1512.02713", "submitter": "Art Owen", "authors": "Kinjal Basu and Art B. Owen", "title": "Transformations and Hardy-Krause variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a multivariable Faa di Bruno formula we give conditions on\ntransformations $\\tau:[0,1]^m\\to\\mathcal{X}$ where $\\mathcal{X}$ is a closed\nand bounded subset of $\\mathbb{R}^d$ such that $f\\circ\\tau$ is of bounded\nvariation in the sense of Hardy and Krause for all $f\\in C^d(\\mathcal{x})$. We\ngive similar conditions for $f\\circ\\tau$ to be smooth enough for scrambled net\nsampling to attain $O(n^{-3/2+\\epsilon})$ accuracy. Some popular symmetric\ntransformations to the simplex and sphere are shown to satisfy neither\ncondition. Some other transformations due to Fang and Wang (1993) satisfy the\nfirst but not the second condition. We provide transformations for the simplex\nthat makes $f\\circ\\tau$ smooth enough to fully benefit from scrambled net\nsampling for all $f$ in a class of generalized polynomials. We also find\nsufficient conditions for the Rosenblatt-Hlawka-M\\\"uck transformation in\n$\\mathbb{R}^2$ and for importance sampling to be of bounded variation in the\nsense of Hardy and Krause.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 01:28:00 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Basu", "Kinjal", ""], ["Owen", "Art B.", ""]]}, {"id": "1512.02863", "submitter": "Daniel Vogel", "authors": "Alexander D\\\"urre, David E. Tyler, Daniel Vogel", "title": "On the eigenvalues of the spatial sign covariance matrix in more than\n  two dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We gather several results on the eigenvalues of the spatial sign covariance\nmatrix of an elliptical distribution. It is shown that the eigenvalues are a\none-to-one function of the eigenvalues of the shape matrix and that they are\ncloser together than the latter. We further provide a one-dimensional integral\nrepresentation of the eigenvalues, which facilitates their numerical\ncomputation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 14:14:56 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 14:01:17 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["D\u00fcrre", "Alexander", ""], ["Tyler", "David E.", ""], ["Vogel", "Daniel", ""]]}, {"id": "1512.03251", "submitter": "Evgeny Nikulchev", "authors": "V.N. Petrushin, E.V. Nikulchev, D.A. Korolev", "title": "Histogram Arithmetic under Uncertainty of Probability Density Function", "comments": "10 pages", "journal-ref": "Applied Mathematical Sciences 9(2015) 7043-7052", "doi": "10.12988/ams.2015.510644", "report-no": null, "categories": "cs.NA stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article we propose a method of performing arithmetic operations on\nvaria-bles with unknown distribution. The approach to the evaluation results of\narithme-tic operations can select probability intervals of the algebraic\nequations and their systems solutions, of differential equations and their\nsystems in case of histogram evaluation of the empirical density distributions\nof random parameters.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 13:42:35 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Petrushin", "V. N.", ""], ["Nikulchev", "E. V.", ""], ["Korolev", "D. A.", ""]]}, {"id": "1512.03307", "submitter": "Nicolas Jung", "authors": "Jung Nicolas and Fr\\'ed\\'eric Bertrand and Myriam Maumy-Bertrand", "title": "AcSel: selecting variables with accuracy in correlated datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of high-throughput technologies, it is possible to measure\nlarge amounts of data relatively at low cost. Such situations arise in many\nfields from sciences to humanities, and variable selection may be of great help\nto answer challenges that are specific to each of them. Variable selection may\nallow to know, among all measured variables, which are of interest and which\nare not. A lot of methods have been proposed to handle this issue, with the\nLasso and other penalized regression as special cases. These methods fail in\nsome cases and linear correlation between explanatory variables is the most\ncommon of these, especially in big datasets. In this article, we introduce\nAcSel, a wrapping algorithm able to enhance the accuracy of any variable\nselection method. To achieve this result, we use intensive computational\nsimulations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 16:15:16 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Nicolas", "Jung", ""], ["Bertrand", "Fr\u00e9d\u00e9ric", ""], ["Maumy-Bertrand", "Myriam", ""]]}, {"id": "1512.03350", "submitter": "Michael Fop", "authors": "Michael Fop, Keith Smart, Thomas Brendan Murphy", "title": "Variable Selection for Latent Class Analysis with Application to Low\n  Back Pain Diagnosis", "comments": "Published in The Annals of Applied Statistics by the Institute of\n  Mathematical Statistics", "journal-ref": "The Annals of Applied Statistics 2017, Vol. 11, No. 4, 2085-2115", "doi": "10.1214/17-AOAS1061", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of most relevant clinical criteria related to low back\npain disorders may aid the evaluation of the nature of pain suffered in a way\nthat usefully informs patient assessment and treatment. Data concerning low\nback pain can be of categorical nature, in the form of a check-list in which\neach item denotes presence or absence of a clinical condition. Latent class\nanalysis is a model-based clustering method for multivariate categorical\nresponses, which can be applied to such data for a preliminary diagnosis of the\ntype of pain. In this work, we propose a variable selection method for latent\nclass analysis applied to the selection of the most useful variables in\ndetecting the group structure in the data. The method is based on the\ncomparison of two different models and allows the discarding of those variables\nwith no group information and those variables carrying the same information as\nthe already selected ones. We consider a swap-stepwise algorithm where at each\nstep the models are compared through an approximation to their Bayes factor.\nThe method is applied to the selection of the clinical criteria most useful for\nthe clustering of patients in different classes. It is shown to perform a\nparsimonious variable selection and to give a clustering performance comparable\nto the expert-based classification of patients into three classes of pain.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 18:03:54 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 18:38:52 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Fop", "Michael", ""], ["Smart", "Keith", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1512.03720", "submitter": "Matthias Morzfeld", "authors": "Matthias Morzfeld and Daniel Hodyss and Chris Snyder", "title": "What the collapse of the ensemble Kalman filter tells us about particle\n  filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ensemble Kalman filter (EnKF) is a reliable data assimilation tool for\nhigh-dimensional meteorological problems. On the other hand, the EnKF can be\ninterpreted as a particle filter, and particle filters collapse in\nhigh-dimensional problems. We explain that these seemingly contradictory\nstatements offer insights about how particle filters function in certain\nhigh-dimensional problems, and in particular support recent efforts in\nmeteorology to \"localize\" particle filters, i.e., to restrict the influence of\nan observation to its neighborhood.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 17:16:16 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 19:18:54 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Morzfeld", "Matthias", ""], ["Hodyss", "Daniel", ""], ["Snyder", "Chris", ""]]}, {"id": "1512.03883", "submitter": "Yiyuan She", "authors": "Qiaoya Zhang, Yiyuan She", "title": "Sparse Generalized Principal Component Analysis for Large-scale\n  Applications beyond Gaussianity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a dimension reduction technique. It\nproduces inconsistent estimators when the dimensionality is moderate to high,\nwhich is often the problem in modern large-scale applications where algorithm\nscalability and model interpretability are difficult to achieve, not to mention\nthe prevalence of missing values. While existing sparse PCA methods alleviate\ninconsistency, they are constrained to the Gaussian assumption of classical PCA\nand fail to address algorithm scalability issues. We generalize sparse PCA to\nthe broad exponential family distributions under high-dimensional setup, with\nbuilt-in treatment for missing values. Meanwhile we propose a family of\niterative sparse generalized PCA (SG-PCA) algorithms such that despite the\nnon-convexity and non-smoothness of the optimization task, the loss function\ndecreases in every iteration. In terms of ease and intuitive parameter tuning,\nour sparsity-inducing regularization is far superior to the popular Lasso.\nFurthermore, to promote overall scalability, accelerated gradient is integrated\nfor fast convergence, while a progressive screening technique gradually\nsqueezes out nuisance dimensions of a large-scale problem for feasible\noptimization. High-dimensional simulation and real data experiments demonstrate\nthe efficiency and efficacy of SG-PCA.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 06:45:05 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 02:36:12 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Zhang", "Qiaoya", ""], ["She", "Yiyuan", ""]]}, {"id": "1512.03976", "submitter": "Joaquin Miguez", "authors": "In\\'es P. Mari\\~no, Joaquin Miguez, Alexey Zaikin", "title": "An iterative importance sampler for Bayesian parameter estimation in\n  stochastic models of multicellular clocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a stochastic version of the synthetic multicellular clock\nmodel proposed by Garcia-Ojalvo, Elowitz and Strogatz. By introducing dynamical\nnoise in the model and assuming that the partial observations of the system can\nbe contaminated by additive noise, we enable a principled mechanism to\nrepresent experimental uncertainties in the synthesis of the multicellular\nsystem and pave the way for the design of probabilistic methods for the\nestimation of any unknowns in the model. Within this setup, we investigate the\nuse of an iterative importance sampling scheme, termed nonlinear population\nMonte Carlo (NPMC), for the Bayesian estimation of the model parameters. The\nalgorithm yields a stochastic approximation of the posterior probability\ndistribution of the unknown parameters given the available data (partial and\npossibly noisy observations). We prove a new theoretical result for this\nalgorithm, which indicates that the approximations converge almost surely to\nthe actual distributions, even when the weights in the importance sampling\nscheme cannot be computed exactly. We also provide a detailed numerical\nassessment of the stochastic multicellular model and the accuracy of the\nproposed NPMC algorithm, including a comparison with the popular particle\nMetropolis-Hastings algorithm of Andrieu {\\em et al.}, 2010, applied to the\nsame model and an approximate Bayesian computation sequential Monte Carlo\nmethod introduced by Mari\\~no {\\em et al.}, 2013.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 23:37:51 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Mari\u00f1o", "In\u00e9s P.", ""], ["Miguez", "Joaquin", ""], ["Zaikin", "Alexey", ""]]}, {"id": "1512.04395", "submitter": "Claudio Agostinelli", "authors": "Claudio Agostinelli", "title": "Local Half-Region Depth for Functional Data", "comments": "46 pages. 19 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data depth proves successful in the analysis of multivariate data sets, in\nparticular deriving an overall center and assigning ranks to the observed\nunits. Two key features are: the directions of the ordering, from the center\ntowards the outside, and the recognition of a unique center irrespective of the\ndistribution being unimodal or multimodal. This behaviour is a consequence of\nthe monotonicity of the ranks that decrease along any ray from the deepest\npoint. Recently, a wider framework allowing identification of partial centers\nwas suggested in Agostinelli and Romanazzi [2011]. The corresponding\ngeneralized depth functions, called local depth functions are able to record\nlocal fluctuations and can be used in mode detection, identification of\ncomponents in mixture models and in cluster analysis. Functional data [Ramsay\nand Silverman, 2006] are become common nowadays. Recently, Lopez-Pintado and\nRomo [2011] has proposed the half-region depth suited for functional data and\nfor high dimensional data. Here, following their work we propose a local\nversion of this data depth, we study its theoretical properties and illustrate\nits behaviour with examples based on real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 16:29:22 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 09:15:06 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Agostinelli", "Claudio", ""]]}, {"id": "1512.04468", "submitter": "Basil Bayati", "authors": "Basil S. Bayati", "title": "A Method to Calculate the Exit Time in Stochastic Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method is presented to compute the exit time for the stochastic\nsimulation algorithm. The method is based on the addition of a series of random\nvariables and is derived using the convolution theorem. The final distribution\nis derived and approximated in the frequency domain. The distribution for the\nfinal time is transformed back to the real domain and can be sampled from in a\nsimulation. The result is an approximation of the classical stochastic\nsimulation algorithm that requires fewer random variates. An analysis of the\nerror and speedup compared to the stochastic simulation algorithm is presented.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 19:40:17 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Bayati", "Basil S.", ""]]}, {"id": "1512.04481", "submitter": "Phaedon-Stelios Koutsourelakis", "authors": "Isabell M. Franck, P.S. Koutsourelakis", "title": "Multimodal, high-dimensional, model-based, Bayesian inverse problems\n  with applications in biomechanics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the numerical solution of model-based, Bayesian\ninverse problems. We are particularly interested in cases where the cost of\neach likelihood evaluation (forward-model call) is expensive and the number of\nun- known (latent) variables is high. This is the setting in many problems in\ncom- putational physics where forward models with nonlinear PDEs are used and\nthe parameters to be calibrated involve spatio-temporarily varying\ncoefficients, which upon discretization give rise to a high-dimensional vector\nof unknowns. One of the consequences of the well-documented ill-posedness of\ninverse prob- lems is the possibility of multiple solutions. While such\ninformation is contained in the posterior density in Bayesian formulations, the\ndiscovery of a single mode, let alone multiple, is a formidable task. The goal\nof the present paper is two- fold. On one hand, we propose approximate,\nadaptive inference strategies using mixture densities to capture multi-modal\nposteriors, and on the other, to ex- tend our work in [1] with regards to\neffective dimensionality reduction techniques that reveal low-dimensional\nsubspaces where the posterior variance is mostly concentrated. We validate the\nmodel proposed by employing Importance Sam- pling which confirms that the bias\nintroduced is small and can be efficiently corrected if the analyst wishes to\ndo so. We demonstrate the performance of the proposed strategy in nonlinear\nelastography where the identification of the mechanical properties of\nbiological materials can inform non-invasive, medical di- agnosis. The\ndiscovery of multiple modes (solutions) in such problems is critical in\nachieving the diagnostic objectives.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 19:37:29 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 12:10:55 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 20:27:50 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Franck", "Isabell M.", ""], ["Koutsourelakis", "P. S.", ""]]}, {"id": "1512.04743", "submitter": "Peter Neal Dr", "authors": "Panayiota Touloupou, Naif Alzahrani, Peter Neal, Simon E.F. Spencer\n  and Trevelyan J. McKinley", "title": "Model comparison with missing data using MCMC and importance sampling", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting between competing statistical models is a challenging problem\nespecially when the competing models are non-nested. In this paper we offer a\nsimple solution by devising an algorithm which combines MCMC and importance\nsampling to obtain computationally efficient estimates of the marginal\nlikelihood which can then be used to compare the models. The algorithm is\nsuccessfully applied to longitudinal epidemic and time series data sets and\nshown to outperform existing methods for computing the marginal likelihood.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 11:52:14 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Touloupou", "Panayiota", ""], ["Alzahrani", "Naif", ""], ["Neal", "Peter", ""], ["Spencer", "Simon E. F.", ""], ["McKinley", "Trevelyan J.", ""]]}, {"id": "1512.04831", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Adeline Samson", "title": "Coupling stochastic EM and Approximate Bayesian Computation for\n  parameter inference in state-space models", "comments": "29 pages. Made a small fix to equation (8). Added the doi of the\n  published version, doi: 10.1007/s00180-017-0770-y", "journal-ref": null, "doi": "10.1007/s00180-017-0770-y", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the class of state-space models and perform maximum likelihood\nestimation for the model parameters. We consider a stochastic approximation\nexpectation-maximization (SAEM) algorithm to maximize the likelihood function\nwith the novelty of using approximate Bayesian computation (ABC) within SAEM.\nThe task is to provide each iteration of SAEM with a filtered state of the\nsystem, and this is achieved using an ABC sampler for the hidden state, based\non sequential Monte Carlo (SMC) methodology. It is shown that the resulting\nSAEM-ABC algorithm can be calibrated to return accurate inference, and in some\nsituations it can outperform a version of SAEM incorporating the bootstrap\nfilter. Two simulation studies are presented, first a nonlinear Gaussian\nstate-space model then a state-space model having dynamics expressed by a\nstochastic differential equation. Comparisons with iterated filtering for\nmaximum likelihood inference, and Gibbs sampling and particle marginal methods\nfor Bayesian inference are presented.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:59:56 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 23:52:58 GMT"}, {"version": "v3", "created": "Fri, 16 Jun 2017 21:12:50 GMT"}, {"version": "v4", "created": "Thu, 28 Sep 2017 11:59:45 GMT"}, {"version": "v5", "created": "Mon, 16 Oct 2017 13:07:32 GMT"}, {"version": "v6", "created": "Tue, 24 Oct 2017 09:30:57 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Picchini", "Umberto", ""], ["Samson", "Adeline", ""]]}, {"id": "1512.04834", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber and Nick Whiteley", "title": "Stability with respect to initial conditions in V-norm for nonlinear\n  filters with ergodic observations", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish conditions for an exponential rate of forgetting of the initial\ndistribution of nonlinear filters in $V$-norm, path-wise along almost all\nobservation sequences. In contrast to previous works, our results allow for\nunbounded test functions. The analysis is conducted in an general setup\ninvolving nonnegative kernels in a random environment which allows treatment of\nfilters and prediction filters in a single framework. The main result is\nillustrated on two examples, the first showing that a total variation norm\nstability result obtained by Douc et al. (2009) can be extended to $V$-norm\nwithout any additional assumptions, the second concerning a situation in which\nforgetting of the initial condition holds in $V$-norm for the filters, but the\n$V$-norm of each prediction filter is infinite.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 16:07:48 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Gerber", "Mathieu", ""], ["Whiteley", "Nick", ""]]}, {"id": "1512.05153", "submitter": "Ines Wilms", "authors": "Ines Wilms and Christophe Croux", "title": "An algorithm for the multivariate group lasso with covariance estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a group lasso estimator for the multivariate linear regression model\nthat accounts for correlated error terms. A block coordinate descent algorithm\nis used to compute this estimator. We perform a simulation study with\ncategorical data and multivariate time series data, typical settings with a\nnatural grouping among the predictor variables. Our simulation studies show the\ngood performance of the proposed group lasso estimator compared to alternative\nestimators. We illustrate the method on a time series data set of gene\nexpressions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 12:50:30 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Wilms", "Ines", ""], ["Croux", "Christophe", ""]]}, {"id": "1512.05633", "submitter": "Dennis Prangle", "authors": "Dennis Prangle", "title": "Summary Statistics in Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is due to appear as a chapter of the forthcoming Handbook of\nApproximate Bayesian Computation (ABC) edited by S. Sisson, Y. Fan, and M.\nBeaumont.\n  Since the earliest work on ABC, it has been recognised that using summary\nstatistics is essential to produce useful inference results. This is because\nABC suffers from a curse of dimensionality effect, whereby using high\ndimensional inputs causes large approximation errors in the output. It is\ntherefore crucial to find low dimensional summaries which are informative about\nthe parameter inference or model choice task at hand. This chapter reviews the\nmethods which have been proposed to select such summaries, extending the\nprevious review paper of Blum et al. (2013) with recent developments. Related\ntheoretical results on the ABC curse of dimensionality and sufficiency are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 15:38:34 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Prangle", "Dennis", ""]]}, {"id": "1512.05913", "submitter": "Phaedon-Stelios Koutsourelakis", "authors": "P.S. Koutsourelakis", "title": "A novel Bayesian strategy for the identification of spatially-varying\n  material properties and model validation: an application to static\n  elastography", "comments": "in International Journal for Numerical Methods in Engineering 2012", "journal-ref": null, "doi": "10.1002/nme.4261", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper proposes a novel Bayesian, computational strategy in the\ncontext of model-based inverse problems in elastostatics. On one hand we\nattempt to provide probabilistic estimates of the material properties and their\nspatial variability that account for the various sources of uncertainty. On the\nother hand we attempt to address the question of model fidelity in relation to\nthe experimental reality and particularly in the context of the material\nconstitutive law adopted. This is especially important in biomedical settings\nwhen the inferred material properties will be used to make decisions/diagnoses.\nWe propose an expanded parametrization that enables the quantification of model\ndiscrepancies in addition to the constitutive parameters. We propose scalable\ncomputational strategies for carrying out inference and learning tasks and\ndemonstrate their effectiveness in numerical examples with noiseless and noisy\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 11:53:11 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Koutsourelakis", "P. S.", ""]]}, {"id": "1512.06171", "submitter": "Alex Gibberd Mr", "authors": "Alexander J. Gibberd and James D. B. Nelson", "title": "Regularized Estimation of Piecewise Constant Gaussian Graphical Models:\n  The Group-Fused Graphical Lasso", "comments": "32 pages, 9 figures", "journal-ref": "Journal of Computational and Graphical Statistics, 2017, Volume\n  26, Number 3, pp 623--634", "doi": "10.1080/10618600.2017.1302340", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time-evolving precision matrix of a piecewise-constant Gaussian graphical\nmodel encodes the dynamic conditional dependency structure of a multivariate\ntime-series. Traditionally, graphical models are estimated under the assumption\nthat data is drawn identically from a generating distribution. Introducing\nsparsity and sparse-difference inducing priors we relax these assumptions and\npropose a novel regularized M-estimator to jointly estimate both the graph and\nchangepoint structure. The resulting estimator possesses the ability to\ntherefore favor sparse dependency structures and/or smoothly evolving graph\nstructures, as required. Moreover, our approach extends current methods to\nallow estimation of changepoints that are grouped across multiple dependencies\nin a system. An efficient algorithm for estimating structure is proposed. We\nstudy the empirical recovery properties in a synthetic setting. The qualitative\neffect of grouped changepoint estimation is then demonstrated by applying the\nmethod on two real-world data-sets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 00:53:58 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:28:38 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Gibberd", "Alexander J.", ""], ["Nelson", "James D. B.", ""]]}, {"id": "1512.06412", "submitter": "Benjamin Frot", "authors": "Benjamin Frot, Luke Jostins, Gil McVean", "title": "Latent variable model selection for Gaussian conditional random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a conditional Gaussian graphical model in\nthe presence of latent variables. Building on recent advances in this field, we\nsuggest a method that decomposes the parameters of a conditional Markov random\nfield into the sum of a sparse and a low-rank matrix. We derive convergence\nbounds for this estimator and show that it is well-behaved in the\nhigh-dimensional regime as well as \"sparsistent\" (i.e. capable of recovering\nthe graph structure). We then show how proximal gradient algorithms and\nsemi-definite programming techniques can be employed to fit the model to\nthousands of variables. Through extensive simulations, we illustrate the\nconditions required for identifiability and show that there is a wide range of\nsituations in which this model performs significantly better than its\ncounterparts, for example, by accommodating more latent variables. Finally, the\nsuggested method is applied to two datasets comprising individual level data on\ngenetic variants and metabolites levels. We show our results replicate better\nthan alternative approaches and show enriched biological signal.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 17:44:49 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 00:24:33 GMT"}, {"version": "v3", "created": "Sat, 4 Mar 2017 21:49:35 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Frot", "Benjamin", ""], ["Jostins", "Luke", ""], ["McVean", "Gil", ""]]}, {"id": "1512.06564", "submitter": "Tamio Koyama", "authors": "Tamio Koyama", "title": "Holonomic gradient method for the probability content of a simplex\n  region with a multivariate normal distribution", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the holonomic gradient method to evaluate the probability content of a\nsimplex region under a multivariate normal distribution. This probability\nequals to the integral of the probability density function of the multivariate\nGaussian distribution on the simplex region. For this purpose, we generalize\nthe inclusion--exclusion identity which was given for polyhedra, to the faces\nof a polyhedron. This extended inclusion--exclusion identity enables us to\ncalculate the derivatives of the function associated with the probability\ncontent of a polyhedron in general position. We show that these derivatives can\nbe written as integrals of the faces of the polyhedron.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 10:25:16 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 06:22:40 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Koyama", "Tamio", ""]]}, {"id": "1512.06999", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (PARIETAL), Michael Eickenberg (PARIETAL), Elvis\n  Dohmatob (PARIETAL), Bertand Thirion (PARIETAL)", "title": "FAASTA: A fast solver for total-variation regularization of\n  ill-conditioned problems with application to brain imaging", "comments": null, "journal-ref": "Colloque GRETSI, Sep 2015, Lyon, France. Gretsi, 2015,\n  http://www.gretsi.fr/colloque2015/myGretsi/programme.php", "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total variation (TV) penalty, as many other analysis-sparsity problems,\ndoes not lead to separable factors or a proximal operatorwith a closed-form\nexpression, such as soft thresholding for the $\\ell\\_1$ penalty. As a result,\nin a variational formulation of an inverse problem or statisticallearning\nestimation, it leads to challenging non-smooth optimization problemsthat are\noften solved with elaborate single-step first-order methods. When thedata-fit\nterm arises from empirical measurements, as in brain imaging, it isoften very\nill-conditioned and without simple structure. In this situation, in proximal\nsplitting methods, the computation cost of thegradient step can easily dominate\neach iteration. Thus it is beneficialto minimize the number of gradient\nsteps.We present fAASTA, a variant of FISTA, that relies on an internal solver\nforthe TV proximal operator, and refines its tolerance to balance\ncomputationalcost of the gradient and the proximal steps. We give benchmarks\nandillustrations on \"brain decoding\": recovering brain maps from\nnoisymeasurements to predict observed behavior. The algorithm as well as\ntheempirical study of convergence speed are valuable for any non-exact\nproximaloperator, in particular analysis-sparsity problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 09:35:55 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["Eickenberg", "Michael", "", "PARIETAL"], ["Dohmatob", "Elvis", "", "PARIETAL"], ["Thirion", "Bertand", "", "PARIETAL"]]}, {"id": "1512.07246", "submitter": "Bryan Lewis", "authors": "James Baglama, Michael Kane, Bryan Lewis, Alex Poliakov", "title": "Efficient Thresholded Correlation using Truncated Singular Value\n  Decomposition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently computing a subset of a correlation matrix consisting of values\nabove a specified threshold is important to many practical applications.\nReal-world problems in genomics, machine learning, finance other applications\ncan produce correlation matrices too large to explicitly form and tractably\ncompute. Often, only values corresponding to highly-correlated vectors are of\ninterest, and those values typically make up a small fraction of the overall\ncorrelation matrix. We present a method based on the singular value\ndecomposition (SVD) and its relationship to the data covariance structure that\ncan efficiently compute thresholded subsets of very large correlation matrices.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 20:51:41 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 21:59:39 GMT"}, {"version": "v3", "created": "Sat, 12 Mar 2016 00:38:49 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Baglama", "James", ""], ["Kane", "Michael", ""], ["Lewis", "Bryan", ""], ["Poliakov", "Alex", ""]]}, {"id": "1512.07678", "submitter": "Alexis Roche", "authors": "Alexis Roche", "title": "Composite Bayesian inference", "comments": "Working paper: 4th version (v4), significantly improved wrt previous\n  version (v3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit and generalize the concept of composite likelihood as a method to\nmake a probabilistic inference by aggregation of multiple Bayesian agents,\nthereby defining a class of predictive models which we call composite Bayesian.\nThis perspective gives insight to choose the weights associated with composite\nlikelihood, either a priori or via learning; in the latter case, they may be\ntuned so as to minimize prediction cross-entropy, yielding an easy-to-solve\nconvex problem. We argue that composite Bayesian inference is a middle way\nbetween generative and discriminative models that trades off between\ninterpretability and prediction performance, both of which are crucial to many\nartificial intelligence tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 01:28:30 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 08:40:37 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 09:31:17 GMT"}, {"version": "v4", "created": "Wed, 17 Apr 2019 15:34:30 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Roche", "Alexis", ""]]}, {"id": "1512.07713", "submitter": "Dootika Vats", "authors": "Dootika Vats, James M. Flegal, and Galin L. Jones", "title": "Multivariate Output Analysis for Markov chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) produces a correlated sample for estimating\nexpectations with respect to a target distribution. A fundamental question is\nwhen should sampling stop so that we have good estimates of the desired\nquantities? The key to answering this question lies in assessing the Monte\nCarlo error through a multivariate Markov chain central limit theorem (CLT).\nThe multivariate nature of this Monte Carlo error largely has been ignored in\nthe MCMC literature. We present a multivariate framework for terminating\nsimulation in MCMC. We define a multivariate effective sample size, estimating\nwhich requires strongly consistent estimators of the covariance matrix in the\nMarkov chain CLT; a property we show for the multivariate batch means\nestimator. We then provide a lower bound on the number of minimum effective\nsamples required for a desired level of precision. This lower bound depends on\nthe problem only in the dimension of the expectation being estimated, and not\non the underlying stochastic process. This result is obtained by drawing a\nconnection between terminating simulation via effective sample size and\nterminating simulation using a relative standard deviation fixed-volume\nsequential stopping rule; which we demonstrate is an asymptotically valid\nprocedure. The finite sample properties of the proposed method are demonstrated\nin a variety of examples.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 04:43:47 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 22:25:35 GMT"}, {"version": "v3", "created": "Sun, 14 May 2017 11:55:08 GMT"}, {"version": "v4", "created": "Fri, 29 Sep 2017 15:13:46 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Vats", "Dootika", ""], ["Flegal", "James M.", ""], ["Jones", "Galin L.", ""]]}, {"id": "1512.08650", "submitter": "Mehdi Molkaraie", "authors": "Mehdi Molkaraie", "title": "An Importance Sampling Scheme for Models in a Strong External Field", "comments": "Proc. IEEE Int. Symp. on Information Theory (ISIT), Hong Kong, June\n  14-19, 2015. arXiv admin note: text overlap with arXiv:1401.4912", "journal-ref": null, "doi": "10.1109/ISIT.2015.7282641", "report-no": null, "categories": "physics.comp-ph cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Monte Carlo methods to estimate the partition function of the\ntwo-dimensional Ising model in the presence of an external magnetic field. The\nestimation is done in the dual of the Forney factor graph representing the\nmodel. The proposed methods can efficiently compute an estimate of the\npartition function in a wide range of model parameters. As an example, we\nconsider models that are in a strong external field.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 11:01:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Molkaraie", "Mehdi", ""]]}]