[{"id": "1407.0738", "submitter": "Vikram Krishnamurthy", "authors": "Vikram Krishnamurthy and Cristian Rojas", "title": "Reduced Complexity Filtering with Stochastic Dominance Bounds: A Convex\n  Optimization Approach", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2362886", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses stochastic dominance principles to construct upper and lower\nsample path bounds for Hidden Markov Model (HMM) filters. Given a HMM, by using\nconvex optimization methods for nuclear norm minimization with copositive\nconstraints, we construct low rank stochastic marices so that the optimal\nfilters using these matrices provably lower and upper bound (with respect to a\npartially ordered set) the true filtered distribution at each time instant.\nSince these matrices are low rank (say R), the computational cost of evaluating\nthe filtering bounds is O(XR) instead of O(X2). A Monte-Carlo importance\nsampling filter is presented that exploits these upper and lower bounds to\nestimate the optimal posterior. Finally, using the Dobrushin coefficient,\nexplicit bounds are given on the variational norm between the true posterior\nand the upper and lower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 22:31:51 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Krishnamurthy", "Vikram", ""], ["Rojas", "Cristian", ""]]}, {"id": "1407.1697", "submitter": "Masaaki Nagahara", "authors": "Masaaki Nagahara and Clyde F. Martin", "title": "L1 Control Theoretic Smoothing Splines", "comments": "Accepted for publication in IEEE Signal Processing Letters. 4 pages\n  (twocolumn), 5 figures", "journal-ref": null, "doi": "10.1109/LSP.2014.2337017", "report-no": null, "categories": "cs.IT cs.SY math.IT math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose control theoretic smoothing splines with L1\noptimality for reducing the number of parameters that describes the fitted\ncurve as well as removing outlier data. A control theoretic spline is a\nsmoothing spline that is generated as an output of a given linear dynamical\nsystem. Conventional design requires exactly the same number of base functions\nas given data, and the result is not robust against outliers. To solve these\nproblems, we propose to use L1 optimality, that is, we use the L1 norm for the\nregularization term and/or the empirical risk term. The optimization is\ndescribed by a convex optimization, which can be efficiently solved via a\nnumerical optimization software. A numerical example shows the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 12:58:30 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 21:26:42 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Nagahara", "Masaaki", ""], ["Martin", "Clyde F.", ""]]}, {"id": "1407.1751", "submitter": "Marco Enea", "authors": "Marco Enea and Gianfranco Lovison", "title": "A penalized approach to the bivariate logistic regression model for the\n  association between ordinal responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bivariate ordered logistic models (BOLMs) are appealing to jointly model the\nmarginal distribution of two ordered responses and their association, given a\nset of covariates. When the number of categories of the responses increases,\nthe number of global odds ratios (or their re-parametrizations) to be estimated\nalso increases and estimating the association structure becomes crucial for\nthis type of data. In fact, such data could be too \"rich\" to be fully modelled\nwith an ordinary BOLM while, sometimes, the well-known Dale's model could be\ntoo parsimonious to provide a good fit. In addition, when the cross-tabulation\nof the responses contains some zeros, for a number of model configurations,\nincluding the bivariate version of the partial proportional odds model (PPOM),\nestimation of a BOLM by the Fisher-scoring algorithm may either fail or\nestimate a too \"irregular\" association structure. In this work, we propose to\nuse a nonparametric approach for the maximum likelihood estimation of a BOLM.\nWe apply penalties to the differences between adjacent row and column effects.\nAs a result, estimation is less demanding than an ordinary BOLM, permitting the\nfit of PPOMs and/or the smoothing of the marginal and association parameters by\npolynomial curves and surfaces, with scores chosen by the data. Model selection\nis based on the penalized log-likelihood ratio, whose limiting distribution has\nbeen studied through simulations, and AIC. Our proposal is compared to the\nGoodman's model and the Dale's model, in terms of goodness-of-fit and\nparsimony, on a literature data set. Finally, an application on an original\ndata set of liver disease patients is proposed.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 15:52:29 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Enea", "Marco", ""], ["Lovison", "Gianfranco", ""]]}, {"id": "1407.1774", "submitter": "Benjamin Hofner", "authors": "Benjamin Hofner, Andreas Mayr, Matthias Schmid", "title": "gamboostLSS: An R Package for Model Building and Variable Selection in\n  the GAMLSS Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized additive models for location, scale and shape (GAMLSS) are a\nflexible class of regression models that allow to model multiple parameters of\na distribution function, such as the mean and the standard deviation,\nsimultaneously. With the R package gamboostLSS, we provide a boosting method to\nfit these models. Variable selection and model choice are naturally available\nwithin this regularized regression framework. To introduce and illustrate the R\npackage gamboostLSS and its infrastructure, we use a data set on stunted growth\nin India. In addition to the specification and application of the model itself,\nwe present a variety of convenience functions, including methods for tuning\nparameter selection, prediction and visualization of results. The package\ngamboostLSS is available from CRAN\n(http://cran.r-project.org/package=gamboostLSS).\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 17:11:09 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Hofner", "Benjamin", ""], ["Mayr", "Andreas", ""], ["Schmid", "Matthias", ""]]}, {"id": "1407.2864", "submitter": "Brooks Paige", "authors": "Brooks Paige, Frank Wood, Arnaud Doucet, Yee Whye Teh", "title": "Asynchronous Anytime Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new sequential Monte Carlo algorithm we call the particle\ncascade. The particle cascade is an asynchronous, anytime alternative to\ntraditional particle filtering algorithms. It uses no barrier synchronizations\nwhich leads to improved particle throughput and memory efficiency. It is an\nanytime algorithm in the sense that it can be run forever to emit an unbounded\nnumber of particles while keeping within a fixed memory budget. We prove that\nthe particle cascade is an unbiased marginal likelihood estimator which means\nthat it can be straightforwardly plugged into existing pseudomarginal methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 17:04:38 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Paige", "Brooks", ""], ["Wood", "Frank", ""], ["Doucet", "Arnaud", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1407.2954", "submitter": "Henry Scharf", "authors": "Henry Scharf, Ryan Elmore, Kenny Gruchalla", "title": "Prioritized Data Compression using Wavelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume of data and the velocity with which it is being generated by com-\nputational experiments on high performance computing (HPC) systems is quickly\noutpacing our ability to effectively store this information in its full\nfidelity. There- fore, it is critically important to identify and study\ncompression methodologies that retain as much information as possible,\nparticularly in the most salient regions of the simulation space. In this\npaper, we cast this in terms of a general decision-theoretic problem and\ndiscuss a wavelet-based compression strategy for its solution. We pro- vide a\nheuristic argument as justification and illustrate our methodology on several\nexamples. Finally, we will discuss how our proposed methodology may be utilized\nin an HPC environment on large-scale computational experiments.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 20:11:23 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Scharf", "Henry", ""], ["Elmore", "Ryan", ""], ["Gruchalla", "Kenny", ""]]}, {"id": "1407.3334", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Offline to Online Conversion", "comments": "20 LaTeX pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of converting offline estimators into an online\npredictor or estimator with small extra regret. Formally this is the problem of\nmerging a collection of probability measures over strings of length 1,2,3,...\ninto a single probability measure over infinite sequences. We describe various\napproaches and their pros and cons on various examples. As a side-result we\ngive an elementary non-heuristic purely combinatoric derivation of Turing's\nfamous estimator. Our main technical contribution is to determine the\ncomputational complexity of online estimators with good guarantees in general.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 01:30:59 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "1407.3336", "submitter": "Jiangjiang Zhang", "authors": "Jiangjiang Zhang, Weixuan Li", "title": "An Inverse Gaussian Process Monte Carlo algorithm for estimation and\n  uncertainty assessment of hydrologic model parameters", "comments": "45 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving hydrologic inverse problems usually requires repetitive forward\nsimulations. One approach to mitigate the computational cost is to build a\nsurrogate model, i.e., an approximate mapping from model parameters (input) to\nobservable quantities (output), so the forward simulations can be done quickly.\nAlternatively, if the surrogate is constructed to approximate the inverse\nmapping from model outputs to parameters, the parameter estimates can be\nobtained directly by treating measurements as inputs to this inverse surrogate.\nMoreover, the uncertainties of parameters can be quantified by propagating the\nmeasurement uncertainties in a straightforward Monte Carlo manner. Based on\nthis idea, we proposed a novel surrogate-based approach for parameter\nestimation and uncertainty assessment, i.e., the Inverse Gaussian Process Monte\nCarlo (IGPMC) algorithm. The Gaussian Process (GP) regression is used to\ndirectly approximate the inverse function of the model output-input\nrelationship. For ill-posed problems, i.e., when there exist non-unique sets of\ninput parameters that all produce an identical system output, multiple inverse\nGP systems are constructed and multiple parameter estimates can be obtained\naccordingly. The accuracy and efficiency of this IGPMC algorithm were\ndemonstrated through four numerical case studies. Results obtained from the\nMarkov Chain Monte Carlo (MCMC) are used as references to assess our new\nproposed method. It was shown that, the IGPMC algorithm can generally obtain\nreliable parameter estimates with an affordable computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 02:02:42 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 04:30:45 GMT"}, {"version": "v3", "created": "Tue, 25 Nov 2014 08:58:07 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2015 02:53:27 GMT"}, {"version": "v5", "created": "Fri, 15 May 2015 00:42:56 GMT"}, {"version": "v6", "created": "Tue, 16 Jun 2015 04:52:23 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Zhang", "Jiangjiang", ""], ["Li", "Weixuan", ""]]}, {"id": "1407.3463", "submitter": "Alessio Spantini", "authors": "Alessio Spantini, Antti Solonen, Tiangang Cui, James Martin, Luis\n  Tenorio, and Youssef Marzouk", "title": "Optimal low-rank approximations of Bayesian linear inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bayesian approach to inverse problems, data are often informative,\nrelative to the prior, only on a low-dimensional subspace of the parameter\nspace. Significant computational savings can be achieved by using this subspace\nto characterize and approximate the posterior distribution of the parameters.\nWe first investigate approximation of the posterior covariance matrix as a\nlow-rank update of the prior covariance matrix. We prove optimality of a\nparticular update, based on the leading eigendirections of the matrix pencil\ndefined by the Hessian of the negative log-likelihood and the prior precision,\nfor a broad class of loss functions. This class includes the F\\\"{o}rstner\nmetric for symmetric positive definite matrices, as well as the\nKullback-Leibler divergence and the Hellinger distance between the associated\ndistributions. We also propose two fast approximations of the posterior mean\nand prove their optimality with respect to a weighted Bayes risk under\nsquared-error loss. These approximations are deployed in an offline-online\nmanner, where a more costly but data-independent offline calculation is\nfollowed by fast online evaluations. As a result, these approximations are\nparticularly useful when repeated posterior mean evaluations are required for\nmultiple data sets. We demonstrate our theoretical results with several\nnumerical examples, including high-dimensional X-ray tomography and an inverse\nheat conduction problem. In both of these examples, the intrinsic\nlow-dimensional structure of the inference problem can be exploited while\nproducing results that are essentially indistinguishable from solutions\ncomputed in the full space.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jul 2014 13:23:50 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 14:54:58 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Spantini", "Alessio", ""], ["Solonen", "Antti", ""], ["Cui", "Tiangang", ""], ["Martin", "James", ""], ["Tenorio", "Luis", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1407.3492", "submitter": "Colin  Gillespie", "authors": "Colin S Gillespie", "title": "Fitting heavy tailed distributions: the poweRlaw package", "comments": "The code for this paper can be found at\n  https://github.com/csgillespie/poweRlaw", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, the power law distribution has been used as the data\ngenerating mechanism in many disparate fields. However, at times the techniques\nused to fit the power law distribution have been inappropriate. This paper\ndescribes the poweRlaw R package, which makes fitting power laws and other\nheavy-tailed distributions straightforward. This package contains R functions\nfor fitting, comparing and visualising heavy tailed distributions. Overall, it\nprovides a principled approach to power law fitting.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jul 2014 17:30:19 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Gillespie", "Colin S", ""]]}, {"id": "1407.4211", "submitter": "Maria Lomeli Miss", "authors": "Mar\\'ia Lomel\\'i, Stefano Favaro, Yee Whye Teh", "title": "A marginal sampler for $\\sigma$-Stable Poisson-Kingman mixture models", "comments": "New algorithmic performance comparisons were added", "journal-ref": null, "doi": "10.1080/10618600.2015.1110526", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the class of $\\sigma$-stable Poisson-Kingman random\nprobability measures (RPMs) in the context of Bayesian nonparametric mixture\nmodeling. This is a large class of discrete RPMs which encompasses most of the\nthe popular discrete RPMs used in Bayesian nonparametrics, such as the\nDirichlet process, Pitman-Yor process, the normalized inverse Gaussian process\nand the normalized generalized Gamma process. We show how certain sampling\nproperties and marginal characterizations of $\\sigma$-stable Poisson-Kingman\nRPMs can be usefully exploited for devising a Markov chain Monte Carlo (MCMC)\nalgorithm for making inference in Bayesian nonparametric mixture modeling.\nSpecifically, we introduce a novel and efficient MCMC sampling scheme in an\naugmented space that has a fixed number of auxiliary variables per iteration.\nWe apply our sampling scheme for a density estimation and clustering tasks with\nunidimensional and multidimensional datasets, and we compare it against\ncompeting sampling schemes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 07:06:10 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2015 23:53:20 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 14:37:43 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Lomel\u00ed", "Mar\u00eda", ""], ["Favaro", "Stefano", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1407.4414", "submitter": "Jem Corcoran", "authors": "J.N. Corcoran and D. Jennings", "title": "Particle Filtering and Smoothing Using Windowed Rejection Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Particle methods\" are sequential Monte Carlo algorithms, typically involving\nimportance sampling, that are used to estimate and sample from joint and\nmarginal densities from a collection of a, presumably increasing, number of\nrandom variables. In particular, a particle filter aims to estimate the current\nstate $X_{n}$ of a stochastic system that is not directly observable by\nestimating a posterior distribution $\\pi(x_{n}|y_{1},y_{2}, \\ldots, y_{n})$\nwhere the $\\{Y_{n}\\}$ are observations related to the $\\{X_{n}\\}$ through some\nmeasurement model $\\pi(y_{n}|x_{n})$. A particle smoother aims to estimate a\nmarginal distribution $\\pi(x_{i}|y_{1},y_{2}, \\ldots, y_{n})$ for $1 \\leq i <\nn$. Particle methods are used extensively for hidden Markov models where\n$\\{X_{n}\\}$ is a Markov chain as well as for more general state space models.\n  Existing particle filtering algorithms are extremely fast and easy to\nimplement. Although they suffer from issues of degeneracy and \"sample\nimpoverishment\", steps can be taken to minimize these problems and overall they\nare excellent tools for inference. However, if one wishes to sample from a\nposterior distribution of interest, a particle filter is only able to produce\ndependent draws. Particle smoothing algorithms are complicated and far less\nrobust, often requiring cumbersome post-processing, \"forward-backward\"\nrecursions, and multiple passes through subroutines. In this paper we introduce\nan alternative algorithm for both filtering and smoothing that is based on\nrejection sampling \"in windows\" . We compare both speed and accuracy of the\ntraditional particle filter and this \"windowed rejection sampler\" (WRS) for\nseveral examples and show that good estimates for smoothing distributions are\nobtained at no extra cost.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:25:53 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Corcoran", "J. N.", ""], ["Jennings", "D.", ""]]}, {"id": "1407.4416", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "In Defense of MinHash Over SimHash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MinHash and SimHash are the two widely adopted Locality Sensitive Hashing\n(LSH) algorithms for large-scale data processing applications. Deciding which\nLSH to use for a particular problem at hand is an important question, which has\nno clear answer in the existing literature. In this study, we provide a\ntheoretical answer (validated by experiments) that MinHash virtually always\noutperforms SimHash when the data are binary, as common in practice such as\nsearch.\n  The collision probability of MinHash is a function of resemblance similarity\n($\\mathcal{R}$), while the collision probability of SimHash is a function of\ncosine similarity ($\\mathcal{S}$). To provide a common basis for comparison, we\nevaluate retrieval results in terms of $\\mathcal{S}$ for both MinHash and\nSimHash. This evaluation is valid as we can prove that MinHash is a valid LSH\nwith respect to $\\mathcal{S}$, by using a general inequality $\\mathcal{S}^2\\leq\n\\mathcal{R}\\leq \\frac{\\mathcal{S}}{2-\\mathcal{S}}$. Our worst case analysis can\nshow that MinHash significantly outperforms SimHash in high similarity region.\n  Interestingly, our intensive experiments reveal that MinHash is also\nsubstantially better than SimHash even in datasets where most of the data\npoints are not too similar to each other. This is partly because, in practical\ndata, often $\\mathcal{R}\\geq \\frac{\\mathcal{S}}{z-\\mathcal{S}}$ holds where $z$\nis only slightly larger than 2 (e.g., $z\\leq 2.1$). Our restricted worst case\nanalysis by assuming $\\frac{\\mathcal{S}}{z-\\mathcal{S}}\\leq \\mathcal{R}\\leq\n\\frac{\\mathcal{S}}{2-\\mathcal{S}}$ shows that MinHash indeed significantly\noutperforms SimHash even in low similarity region.\n  We believe the results in this paper will provide valuable guidelines for\nsearch in practice, especially when the data are sparse.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:27:02 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1407.4543", "submitter": "Ya Le", "authors": "Ya Le, Trevor Hastie", "title": "Sparse Quadratic Discriminant Analysis and Community Bayes", "comments": "Revised version (adding more experiments)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a class of rules spanning the range between quadratic discriminant\nanalysis and naive Bayes, through a path of sparse graphical models. A group\nlasso penalty is used to introduce shrinkage and encourage a similar pattern of\nsparsity across precision matrices. It gives sparse estimates of interactions\nand produces interpretable models. Inspired by the connected-components\nstructure of the estimated precision matrices, we propose the community Bayes\nmodel, which partitions features into several conditional independent\ncommunities and splits the classification problem into separate smaller ones.\nThe community Bayes idea is quite general and can be applied to non-Gaussian\ndata and likelihood-based classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 03:11:19 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 06:13:35 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Le", "Ya", ""], ["Hastie", "Trevor", ""]]}, {"id": "1407.4916", "submitter": "Andre Beinrucker", "authors": "Andre Beinrucker, \\\"Ur\\\"un Dogan, Gilles Blanchard", "title": "Extensions of stability selection using subsamples of observations and\n  covariates", "comments": "accepted for publication in Statistics and Computing", "journal-ref": "Statistics and Computing 26 (5): 1059-1077 (2016)", "doi": "10.1007/s11222-015-9589-y", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce extensions of stability selection, a method to stabilise\nvariable selection methods introduced by Meinshausen and B\\\"uhlmann (J R Stat\nSoc 72:417-473, 2010). We propose to apply a base selection method repeatedly\nto random observation subsamples and covariate subsets under scrutiny, and to\nselect covariates based on their selection frequency. We analyse the effects\nand benefits of these extensions. Our analysis generalizes the theoretical\nresults of Meinshausen and B\\\"uhlmann (J R Stat Soc 72:417-473, 2010) from the\ncase of half-samples to subsamples of arbitrary size. We study, in a\ntheoretical manner, the effect of taking random covariate subsets using a\nsimplified score model. Finally we validate these extensions on numerical\nexperiments on both synthetic and real datasets, and compare the obtained\nresults in detail to the original stability selection method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 08:52:41 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 16:53:44 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2015 08:21:08 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Beinrucker", "Andre", ""], ["Dogan", "\u00dcr\u00fcn", ""], ["Blanchard", "Gilles", ""]]}, {"id": "1407.4981", "submitter": "Michael Gutmann", "authors": "Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, Jukka Corander", "title": "Likelihood-free inference via classification", "comments": "Accepted for publication in Statistics and Computing (Feb 13, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly complex generative models are being used across disciplines as\nthey allow for realistic characterization of data, but a common difficulty with\nthem is the prohibitively large computational cost to evaluate the likelihood\nfunction and thus to perform likelihood-based statistical inference. A\nlikelihood-free inference framework has emerged where the parameters are\nidentified by finding values that yield simulated data resembling the observed\ndata. While widely applicable, a major difficulty in this framework is how to\nmeasure the discrepancy between the simulated and observed data. Transforming\nthe original problem into a problem of classifying the data into simulated\nversus observed, we find that classification accuracy can be used to assess the\ndiscrepancy. The complete arsenal of classification methods becomes thereby\navailable for inference of intractable generative models. We validate our\napproach using theory and simulations for both point estimation and Bayesian\ninference, and demonstrate its use on real data by inferring an\nindividual-based epidemiological model for bacterial infections in child care\ncenters.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 13:17:30 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2015 12:09:27 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 11:14:02 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Gutmann", "Michael U.", ""], ["Dutta", "Ritabrata", ""], ["Kaski", "Samuel", ""], ["Corander", "Jukka", ""]]}, {"id": "1407.5341", "submitter": "In\\'es del Puerto", "authors": "M. Gonzalez, C. Minuesa, I. del Puerto", "title": "Maximum likelihood estimation and Expectation-Maximization algorithm for\n  controlled branching processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The controlled branching process is a generalization of the classical\nBienaym\\'e-Galton-Watson branching process. It is a useful model for describing\nthe evolution of populations in which the population size at each generation\nneeds to be controlled. The maximum likelihood estimation of the parameters of\ninterest for this process is addressed under various sample schemes. Firstly,\nassuming that the entire family tree can be observed, the corresponding\nestimators are obtained and their asymptotic properties investigated. Secondly,\nsince in practice it is not usual to observe such a sample, the maximum\nlikelihood estimation is initially considered using the sample given by the\ntotal number of individuals and progenitors of each generation, and then using\nthe sample given by only the generation sizes. Expectation-maximization\nalgorithms are developed to address these problems as incomplete data\nestimation problems. The accuracy of the procedures is illustrated by means of\na simulated example.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 21:54:25 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 21:29:29 GMT"}, {"version": "v3", "created": "Thu, 5 Feb 2015 22:18:24 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Gonzalez", "M.", ""], ["Minuesa", "C.", ""], ["del Puerto", "I.", ""]]}, {"id": "1407.5459", "submitter": "Johannes Buchner", "authors": "Johannes Buchner", "title": "A statistical test for Nested Sampling algorithms", "comments": "11 pages, 7 figures. Published in Statistics and Computing, Springer,\n  September 2014", "journal-ref": null, "doi": "10.1007/s11222-014-9512-y", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested sampling is an iterative integration procedure that shrinks the prior\nvolume towards higher likelihoods by removing a \"live\" point at a time. A\nreplacement point is drawn uniformly from the prior above an ever-increasing\nlikelihood threshold. Thus, the problem of drawing from a space above a certain\nlikelihood value arises naturally in nested sampling, making algorithms that\nsolve this problem a key ingredient to the nested sampling framework. If the\ndrawn points are distributed uniformly, the removal of a point shrinks the\nvolume in a well-understood way, and the integration of nested sampling is\nunbiased. In this work, I develop a statistical test to check whether this is\nthe case. This \"Shrinkage Test\" is useful to verify nested sampling algorithms\nin a controlled environment. I apply the shrinkage test to a test-problem, and\nshow that some existing algorithms fail to pass it due to over-optimisation. I\nthen demonstrate that a simple algorithm can be constructed which is robust\nagainst this type of problem. This RADFRIENDS algorithm is, however,\ninefficient in comparison to MULTINEST.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 11:18:21 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 00:03:57 GMT"}, {"version": "v3", "created": "Tue, 2 Dec 2014 19:13:36 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Buchner", "Johannes", ""]]}, {"id": "1407.5770", "submitter": "Anthony Lee", "authors": "Anthony Lee, Arnaud Doucet, Krzysztof {\\L}atuszy\\'nski", "title": "Perfect simulation using atomic regeneration with application to\n  Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an irreducible, Harris recurrent Markov chain of transition kernel\n{\\Pi} and invariant probability measure {\\pi}. If {\\Pi} satisfies a\nminorization condition, then the split chain allows the identification of\nregeneration times which may be exploited to obtain perfect samples from {\\pi}.\nUnfortunately, many transition kernels associated with complex Markov chain\nMonte Carlo algorithms are analytically intractable, so establishing a\nminorization condition and simulating the split chain is challenging, if not\nimpossible. For uniformly ergodic Markov chains with intractable transition\nkernels, we propose two efficient perfect simulation procedures of similar\nexpected running time which are instances of the multigamma coupler and an\nimputation scheme. These algorithms overcome the intractability of the kernel\nby introducing an artificial atom and using a Bernoulli factory. We detail an\napplication of these procedures when {\\Pi} is the recently introduced iterated\nconditional Sequential Monte Carlo kernel. We additionally provide results on\nthe general applicability of the methodology, and how Sequential Monte Carlo\nmethods may be used to facilitate perfect simulation and/or unbiased estimation\nof expectations with respect to the stationary distribution of a non-uniformly\nergodic Markov chain.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 07:51:05 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Lee", "Anthony", ""], ["Doucet", "Arnaud", ""], ["\u0141atuszy\u0144ski", "Krzysztof", ""]]}, {"id": "1407.5915", "submitter": "Julien  Chiquet Dr.", "authors": "Julien Chiquet and Pierre Gutierrez and Guillem Rigaill", "title": "Fast tree inference with weighted fusion penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a data set with many features observed in a large number of conditions,\nit is desirable to fuse and aggregate conditions which are similar to ease the\ninterpretation and extract the main characteristics of the data. This paper\npresents a multidimensional fusion penalty framework to address this question\nwhen the number of conditions is large. If the fusion penalty is encoded by an\n$\\ell_q$-norm, we prove for uniform weights that the path of solutions is a\ntree which is suitable for interpretability. For the $\\ell_1$ and\n$\\ell_\\infty$-norms, the path is piecewise linear and we derive a homotopy\nalgorithm to recover exactly the whole tree structure. For weighted\n$\\ell_1$-fusion penalties, we demonstrate that distance-decreasing weights lead\nto balanced tree structures. For a subclass of these weights that we call\n\"exponentially adaptive\", we derive an $\\mathcal{O}(n\\log(n))$ homotopy\nalgorithm and we prove an asymptotic oracle property. This guarantees that we\nrecover the underlying structure of the data efficiently both from a\nstatistical and a computational point of view. We provide a fast implementation\nof the homotopy algorithm for the single feature case, as well as an efficient\nembedded cross-validation procedure that takes advantage of the tree structure\nof the path of solutions. Our proposal outperforms its competing procedures on\nsimulations both in terms of timings and prediction accuracy. As an example we\nconsider phenotypic data: given one or several traits, we reconstruct a\nbalanced tree structure and assess its agreement with the known taxonomy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 15:57:04 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 20:09:58 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Chiquet", "Julien", ""], ["Gutierrez", "Pierre", ""], ["Rigaill", "Guillem", ""]]}, {"id": "1407.6484", "submitter": "Dominik Liebl", "authors": "Oualid Bada and Dominik Liebl", "title": "The R-package phtt: Panel Data Analysis with Heterogeneous Time Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R-package phtt provides estimation procedures for panel data with large\ndimensions n, T, and general forms of unobservable heterogeneous effects.\nParticularly, the estimation procedures are those of Bai (2009) and Kneip,\nSickles, and Song (2012), which complement one another very well: both models\nassume the unobservable heterogeneous effects to have a factor structure. Kneip\net al. (2012) considers the case in which the time varying common factors have\nrelatively smooth patterns including strongly positive auto-correlated\nstationary as well as non-stationary factors, whereas the method of Bai (2009)\nfocuses on stochastic bounded factors such as ARMA processes. Additionally, the\nphtt package provides a wide range of dimensionality criteria in order to\nestimate the number of the unobserved factors simultaneously with the remaining\nmodel parameters.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 08:30:32 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Bada", "Oualid", ""], ["Liebl", "Dominik", ""]]}, {"id": "1407.6518", "submitter": "Salvador Pueyo", "authors": "Salvador Pueyo", "title": "Algorithm for the maximum likelihood estimation of the parameters of the\n  truncated normal and lognormal distributions", "comments": "4 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simple procedure to estimate the parameters of the\nunivariate truncated normal and lognormal distributions by maximum likelihood.\nIt starts from a reparameterization of the lognormal that was previously\nintroduced by the author and is especially useful when the lognormal is close\nto a power law, which is a limiting case of the first distribution. One of the\nnew parameters quantifies the distance from the power law, and vanishes when\nthe power law gives a sufficient description of the data. At this point, the\nother parameter equals the exponent of the power law. In contrast, when using\nthe standard parameterization, the parameters of the lognormal diverge in the\nneighborhood of the power law. Whether or not we are in this neighborhood, the\nnew parameters have properties that ease the process of estimation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 10:29:24 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Pueyo", "Salvador", ""]]}, {"id": "1407.8071", "submitter": "Joaquin Miguez", "authors": "Dan Crisan, Joaquin Miguez and Gonzalo Rios", "title": "A simple scheme for the parallelization of particle filters and its\n  application to the tracking of complex stochastic systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of possibly the simplest scheme for the\nparallelisation of the standard particle filter, that consists in splitting the\ncomputational budget into $M$ fully independent particle filters with $N$\nparticles each, and then obtaining the desired estimators by averaging over the\n$M$ independent outcomes of the filters. This approach minimises the\nparallelisation overhead yet displays highly desirable theoretical properties.\nUnder very mild assumptions, we analyse the mean square error (MSE) of the\nestimators of 1-dimensional statistics of the optimal filtering distribution\nand show explicitly the effect of parallelisation scheme on the convergence\nrate. Specifically, we study the decomposition of the MSE into variance and\nbias components, to show that the former decays as $\\frac{1}{MN}$, i.e.,\nlinearly with the total number of particles, while the latter converges towards\n$0$ as $\\frac{1}{N^2}$. Parallelisation, therefore, has the obvious advantage\nof dividing the running times while preserving the (asymptotic) performance of\nthe particle filter. Following this lead, we propose a time-error index to\ncompare schemes with different degrees of parallelisation. Finally, we provide\ntwo numerical examples. The first one deals with the tracking of a Lorenz 63\nchaotic system with dynamical noise and partial (noisy) observations, while the\nsecond example involves a dynamical network of modified FitzHugh-Nagumo (FH-N)\nstochastic nodes. The latter is a large dimensional system ($\\approx3,000$\nstate variables in our computer experiments) designed to numerically reproduce\ntypical electrical phenomena observed in the atria of the human heart. In both\nexamples, we show how the proposed parallelisation scheme attains the same\napproximation accuracy as a centralised particle filter with only a small\nfraction of the running time, using a standard multicore computer.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 14:59:25 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 16:02:51 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Crisan", "Dan", ""], ["Miguez", "Joaquin", ""], ["Rios", "Gonzalo", ""]]}, {"id": "1407.8253", "submitter": "Hua Zhou", "authors": "Hua Zhou and John Blangero and Thomas D. Dyer and Kei-hang K. Chan and\n  Kenneth Lange and Eric M. Sobel", "title": "Fast Genome-Wide QTL Association Mapping on Pedigree and Population Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.PE stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Since most analysis software for genome-wide association studies (GWAS)\ncurrently exploit only unrelated individuals, there is a need for efficient\napplications that can handle general pedigree data or mixtures of both\npopulation and pedigree data. Even data sets thought to consist of only\nunrelated individuals may include cryptic relationships that can lead to false\npositives if not discovered and controlled for. In addition, family designs\npossess compelling advantages. They are better equipped to detect rare\nvariants, control for population stratification, and facilitate the study of\nparent-of-origin effects. Pedigrees selected for extreme trait values often\nsegregate a single gene with strong effect. Finally, many pedigrees are\navailable as an important legacy from the era of linkage analysis.\nUnfortunately, pedigree likelihoods are notoriously hard to compute. In this\npaper we re-examine the computational bottlenecks and implement ultra-fast\npedigree-based GWAS analysis. Kinship coefficients can either be based on\nexplicitly provided pedigrees or automatically estimated from dense markers.\nOur strategy (a) works for random sample data, pedigree data, or a mix of both;\n(b) entails no loss of power; (c) allows for any number of covariate\nadjustments, including correction for population stratification; (d) allows for\ntesting SNPs under additive, dominant, and recessive models; and (e)\naccommodates both univariate and multivariate quantitative traits. On a typical\npersonal computer (6 CPU cores at 2.67 GHz), analyzing a univariate HDL\n(high-density lipoprotein) trait from the San Antonio Family Heart Study\n(935,392 SNPs on 1357 individuals in 124 pedigrees) takes less than 2 minutes\nand 1.5 GB of memory. Complete multivariate QTL analysis of the three\ntime-points of the longitudinal HDL multivariate trait takes less than 5\nminutes and 1.5 GB of memory.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 01:42:21 GMT"}, {"version": "v2", "created": "Sat, 20 Dec 2014 03:31:15 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Zhou", "Hua", ""], ["Blangero", "John", ""], ["Dyer", "Thomas D.", ""], ["Chan", "Kei-hang K.", ""], ["Lange", "Kenneth", ""], ["Sobel", "Eric M.", ""]]}]