[{"id": "0710.2912", "submitter": "Adom Giffin", "authors": "Adom Giffin", "title": "Updating Probabilities: An Econometric Example", "comments": "Presented at the 3rd Econophysics Colloquium, Ancona, Italy, Sept\n  27-29, 2007. 12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  We demonstrate how information in the form of observable data and moment\nconstraints are introduced into the method of Maximum relative Entropy (ME). A\ngeneral example of updating with data and moments is shown. A specific\neconometric example is solved in detail which can then be used as a template\nfor real world problems. A numerical example is compared to a large deviation\nsolution which illustrates some of the advantages of the ME method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2007 20:59:55 GMT"}], "update_date": "2007-10-17", "authors_parsed": [["Giffin", "Adom", ""]]}, {"id": "0710.4228", "submitter": "Omiros Papaspiliopoulos", "authors": "Omiros Papaspiliopoulos and Gareth Roberts", "title": "Retrospective Markov chain Monte Carlo methods for Dirichlet process\n  hierarchical model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  Inference for Dirichlet process hierarchical models is typically performed\nusing Markov chain Monte Carlo methods, which can be roughly categorised into\nmarginal and conditional methods. The former integrate out analytically the\ninfinite-dimensional component of the hierarchical model and sample from the\nmarginal distribution of the remaining variables using the Gibbs sampler.\nConditional methods impute the Dirichlet process and update it as a component\nof the Gibbs sampler. Since this requires imputation of an infinite-dimensional\nprocess, implementation of the conditional method has relied on finite\napproximations. In this paper we show how to avoid such approximations by\ndesigning two novel Markov chain Monte Carlo algorithms which sample from the\nexact posterior distribution of quantities of interest. The approximations are\navoided by the new technique of retrospective sampling. We also show how the\nalgorithms can obtain samples from functionals of the Dirichlet process. The\nmarginal and the conditional methods are compared and a careful simulation\nstudy is included, which involves a non-conjugate model, different datasets and\nprior specifications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2007 10:27:36 GMT"}], "update_date": "2007-10-24", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Roberts", "Gareth", ""]]}, {"id": "0710.4234", "submitter": "Omiros Papaspiliopoulos", "authors": "Omiros Papaspiliopoulos and Gareth Roberts", "title": "Stability of the Gibbs Sampler for Bayesian Hierarchical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  We characterise the convergence of the Gibbs sampler which samples from the\njoint posterior distribution of parameters and missing data in hierarchical\nlinear models with arbitrary symmetric error distributions. We show that the\nconvergence can be uniform, geometric or sub-geometric depending on the\nrelative tail behaviour of the error distributions, and on the parametrisation\nchosen. Our theory is applied to characterise the convergence of the Gibbs\nsampler on latent Gaussian process models. We indicate how the theoretical\nframework we introduce will be useful in analyzing more complex models.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2007 10:57:15 GMT"}], "update_date": "2007-10-24", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Roberts", "Gareth", ""]]}, {"id": "0710.4242", "submitter": "Olivier Cappe", "authors": "Olivier Capp\\'e (LTCI), Randal Douc (CMAP), Arnaud Guillin (LATP),\n  Jean-Michel Marin (INRIA Futurs), Christian P. Robert (CEREMADE)", "title": "Adaptive Importance Sampling in General Mixture Classes", "comments": "Removed misleading comment in Section 2", "journal-ref": "Statistics and Computing 18, 4 (2008) 447-459", "doi": "10.1007/s11222-008-9059-x", "report-no": null, "categories": "stat.CO", "license": null, "abstract": "  In this paper, we propose an adaptive algorithm that iteratively updates both\nthe weights and component parameters of a mixture importance sampling density\nso as to optimise the importance sampling performances, as measured by an\nentropy criterion. The method is shown to be applicable to a wide class of\nimportance sampling densities, which includes in particular mixtures of\nmultivariate Student t distributions. The performances of the proposed scheme\nare studied on both artificial and real examples, highlighting in particular\nthe benefit of a novel Rao-Blackwellisation device which can be easily\nincorporated in the updating scheme.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2007 11:23:46 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2008 14:09:26 GMT"}, {"version": "v3", "created": "Wed, 30 Apr 2008 09:46:29 GMT"}, {"version": "v4", "created": "Fri, 30 May 2008 08:38:00 GMT"}], "update_date": "2009-08-18", "authors_parsed": [["Capp\u00e9", "Olivier", "", "LTCI"], ["Douc", "Randal", "", "CMAP"], ["Guillin", "Arnaud", "", "LATP"], ["Marin", "Jean-Michel", "", "INRIA Futurs"], ["Robert", "Christian P.", "", "CEREMADE"]]}, {"id": "0710.4245", "submitter": "Omiros Papaspiliopoulos", "authors": "Paul Fearnhead, Omiros Papaspiliopoulos and Gareth Roberts", "title": "Particle Filters for Partially Observed Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  In this paper we introduce a novel particle filter scheme for a class of\npartially-observed multivariate diffusions. %continuous-time dynamic models\nwhere the %signal is given by a multivariate diffusion process. We consider a\nvariety of observation schemes, including diffusion observed with error,\nobservation of a subset of the components of the multivariate diffusion and\narrival times of a Poisson process whose intensity is a known function of the\ndiffusion (Cox process). Unlike currently available methods, our particle\nfilters do not require approximations of the transition and/or the observation\ndensity using time-discretisations. Instead, they build on recent methodology\nfor the exact simulation of the diffusion process and the unbiased estimation\nof the transition density as described in \\cite{besk:papa:robe:fear:2006}. %In\nparticular, w We introduce the Generalised Poisson Estimator, which generalises\nthe Poisson Estimator of \\cite{besk:papa:robe:fear:2006}. %Thus, our filters\navoid the systematic biases caused by %time-discretisations and they have\nsignificant computational %advantages over alternative continuous-time filters.\nThese %advantages are supported theoretically by a A central limit theorem is\ngiven for our particle filter scheme.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2007 11:57:43 GMT"}], "update_date": "2007-10-24", "authors_parsed": [["Fearnhead", "Paul", ""], ["Papaspiliopoulos", "Omiros", ""], ["Roberts", "Gareth", ""]]}, {"id": "0710.4536", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Herbert K. H. Lee", "title": "Bayesian treed Gaussian process models with an application to computer\n  modeling", "comments": "32 pages, 9 figures, to appear in the Journal of the American\n  Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a computer experiment for the design of a rocket booster, this\npaper explores nonstationary modeling methodologies that couple stationary\nGaussian processes with treed partitioning. Partitioning is a simple but\neffective method for dealing with nonstationarity. The methodological\ndevelopments and statistical computing details which make this approach\nefficient are described in detail. In addition to providing an analysis of the\nrocket booster simulator, our approach is demonstrated to be effective in other\narenas.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2007 19:10:12 GMT"}, {"version": "v10", "created": "Tue, 17 Mar 2009 15:46:13 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2008 14:58:55 GMT"}, {"version": "v3", "created": "Tue, 12 Feb 2008 17:46:37 GMT"}, {"version": "v4", "created": "Sat, 16 Feb 2008 13:14:40 GMT"}, {"version": "v5", "created": "Thu, 28 Feb 2008 18:06:34 GMT"}, {"version": "v6", "created": "Mon, 3 Mar 2008 18:31:35 GMT"}, {"version": "v7", "created": "Fri, 4 Apr 2008 08:28:48 GMT"}, {"version": "v8", "created": "Mon, 3 Nov 2008 18:35:42 GMT"}, {"version": "v9", "created": "Mon, 24 Nov 2008 12:55:18 GMT"}], "update_date": "2009-03-17", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lee", "Herbert K. H.", ""]]}, {"id": "0710.5098", "submitter": "Anastasia Papavasiliou", "authors": "Anastasia Papavasiliou", "title": "Particle Filters for Multiscale Diffusions", "comments": "to appear in ESAIM Proceedings (Workshop on Sequential Monte Carlo\n  Methods: filtering and other applications, Oxford, 2006)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": null, "abstract": "  We consider multiscale stochastic systems that are partially observed at\ndiscrete points of the slow time scale. We introduce a particle filter that\ntakes advantage of the multiscale structure of the system to efficiently\napproximate the optimal filter.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2007 14:28:08 GMT"}], "update_date": "2007-10-29", "authors_parsed": [["Papavasiliou", "Anastasia", ""]]}, {"id": "0710.5343", "submitter": "Jie Peng", "authors": "Jie Peng and Debashis Paul", "title": "A geometric approach to maximum likelihood estimation of the functional\n  principal components from sparse longitudinal data", "comments": "88 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  In this paper, we consider the problem of estimating the eigenvalues and\neigenfunctions of the covariance kernel (i.e., the functional principal\ncomponents) from sparse and irregularly observed longitudinal data. We approach\nthis problem through a maximum likelihood method assuming that the covariance\nkernel is smooth and finite dimensional. We exploit the smoothness of the\neigenfunctions to reduce dimensionality by restricting them to a lower\ndimensional space of smooth functions. The estimation scheme is developed based\non a Newton-Raphson procedure using the fact that the basis coefficients\nrepresenting the eigenfunctions lie on a Stiefel manifold. We also address the\nselection of the right number of basis functions, as well as that of the\ndimension of the covariance kernel by a second order approximation to the\nleave-one-curve-out cross-validation score that is computationally very\nefficient. The effectiveness of our procedure is demonstrated by simulation\nstudies and an application to a CD4 counts data set. In the simulation studies,\nour method performs well on both estimation and model selection. It also\noutperforms two existing approaches: one based on a local polynomial smoothing\nof the empirical covariances, and another using an EM algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2007 06:13:21 GMT"}], "update_date": "2007-10-30", "authors_parsed": [["Peng", "Jie", ""], ["Paul", "Debashis", ""]]}, {"id": "0710.5670", "submitter": "Inbal Yahav", "authors": "Inbal Yahav and Galit Shmueli", "title": "An Elegant Method for Generating Multivariate Poisson Random Variable", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": null, "abstract": "  Generating multivariate Poisson data is essential in many applications.\nCurrent simulation methods suffer from limitations ranging from computational\ncomplexity to restrictions on the structure of the correlation matrix. We\npropose a computationally efficient and conceptually appealing method for\ngenerating multivariate Poisson data. The method is based on simulating\nmultivariate Normal data and converting them to achieve a specific correlation\nmatrix and Poisson rate vector. This allows for generating data that have\npositive or negative correlations as well as different rates.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2007 15:05:58 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2008 23:38:20 GMT"}], "update_date": "2008-03-13", "authors_parsed": [["Yahav", "Inbal", ""], ["Shmueli", "Galit", ""]]}, {"id": "0710.5837", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy, Joo Hee Lee, and Ricardo Silva", "title": "On estimating covariances between many assets with histories of highly\n  variable length", "comments": "39 pages, 5 figures, 2 tables, submitted to JCGS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative portfolio allocation requires the accurate and tractable\nestimation of covariances between a large number of assets, whose histories can\ngreatly vary in length. Such data are said to follow a monotone missingness\npattern, under which the likelihood has a convenient factorization. Upon\nfurther assuming that asset returns are multivariate normally distributed, with\nhistories at least as long as the total asset count, maximum likelihood (ML)\nestimates are easily obtained by performing repeated ordinary least squares\n(OLS) regressions, one for each asset. Things get more interesting when there\nare more assets than historical returns. OLS becomes unstable due to\nrank--deficient design matrices, which is called a \"big p small n\" problem. We\nexplore remedies that involve making a change of basis, as in principal\ncomponents or partial least squares regression, or by applying shrinkage\nmethods like ridge regression or the lasso. This enables the estimation of\ncovariances between large sets of assets with histories of essentially\narbitrary length, and offers improvements in accuracy and interpretation. We\nfurther extend the method by showing how external factors can be incorporated.\nThis allows for the adaptive use of factors without the restrictive assumptions\ncommon in factor models. Our methods are demonstrated on randomly generated\ndata, and then benchmarked by the performance of balanced portfolios using real\nhistorical financial returns. An accompanying R package called monomvn,\ncontaining code implementing the estimators described herein, has been made\nfreely available on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2007 15:36:20 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2007 10:44:14 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2007 16:53:16 GMT"}, {"version": "v4", "created": "Tue, 22 Jan 2008 20:49:15 GMT"}, {"version": "v5", "created": "Mon, 10 Mar 2008 09:19:16 GMT"}, {"version": "v6", "created": "Tue, 7 Oct 2008 02:24:19 GMT"}, {"version": "v7", "created": "Tue, 24 Feb 2009 17:28:34 GMT"}], "update_date": "2009-02-24", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lee", "Joo Hee", ""], ["Silva", "Ricardo", ""]]}, {"id": "0710.5872", "submitter": "Piergiacomo Sabino", "authors": "Piergiacomo Sabino (Dipartimento di Matematica Universit\\`a degli\n  Studi di Bari)", "title": "Implementing Quasi-Monte Carlo Simulations with Linear Transformations", "comments": "17 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": null, "abstract": "  Pricing exotic multi-asset path-dependent options requires extensive Monte\nCarlo simulations. In the recent years the interest to the Quasi-monte Carlo\ntechnique has been renewed and several results have been proposed in order to\nimprove its efficiency with the notion of effective dimension. To this aim,\nImai and Tan introduced a general variance reduction technique in order to\nminimize the nominal dimension of the Monte Carlo method. Taking into account\nthese advantages, we investigate this approach in detail in order to make it\nfaster from the computational point of view. Indeed, we realize the linear\ntransformation decomposition relying on a fast ad hoc QR decomposition that\nconsiderably reduces the computational burden. This setting makes the linear\ntransformation method even more convenient from the computational point of\nview. We implement a high-dimensional (2500) Quasi-Monte Carlo simulation\ncombined with the linear transformation in order to price Asian basket options\nwith same set of parameters published by Imai and Tan. For the simulation of\nthe high-dimensional random sample, we use a 50-dimensional scrambled Sobol\nsequence for the first 50 components, determined by the linear transformation\nmethod, and pad the remaining ones out by the Latin Hypercube Sampling. The aim\nof this numerical setting is to investigate the accuracy of the estimation by\ngiving a higher convergence rate only to those components selected by the\nlinear transformation technique. We launch our simulation experiment also using\nthe standard Cholesky and the principal component decomposition methods with\npseudo-random and Latin Hypercube sampling generators. Finally, we compare our\nresults and computational times, with those presented in Imai and Tan.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2007 14:57:12 GMT"}], "update_date": "2007-11-01", "authors_parsed": [["Sabino", "Piergiacomo", "", "Dipartimento di Matematica Universit\u00e0 degli\n  Studi di Bari"]]}]