[{"id": "1203.0106", "submitter": "Francois Caron", "authors": "Fran\\c{c}ois Caron (INRIA Bordeaux - Sud-Ouest, IMB), Luke Bornn\n  (Statistics), Arnaud Doucet", "title": "Sparsity-Promoting Bayesian Dynamic Linear Models", "comments": null, "journal-ref": "N&deg; RR-7895 (2012)", "doi": null, "report-no": "RR-7895", "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-promoting priors have become increasingly popular over recent years\ndue to an increased number of regression and classification applications\ninvolving a large number of predictors. In time series applications where\nobservations are collected over time, it is often unrealistic to assume that\nthe underlying sparsity pattern is fixed. We propose here an original class of\nflexible Bayesian linear models for dynamic sparsity modelling. The proposed\nclass of models expands upon the existing Bayesian literature on sparse\nregression using generalized multivariate hyperbolic distributions. The\nproperties of the models are explored through both analytic results and\nsimulation studies. We demonstrate the model on a financial application where\nit is shown that it accurately represents the patterns seen in the analysis of\nstock and derivative data, and is able to detect major events by filtering an\nartificial portfolio of assets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 07:39:21 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Caron", "Fran\u00e7ois", "", "INRIA Bordeaux - Sud-Ouest, IMB"], ["Bornn", "Luke", "", "Statistics"], ["Doucet", "Arnaud", ""]]}, {"id": "1203.0919", "submitter": "Matthias Troffaes", "authors": "Matthias C. M. Troffaes", "title": "Finite approximations to coherent choice", "comments": "15 pages, 1 figure, 2 tables", "journal-ref": "International Journal of Approximate Reasoning 50 (2009) 655-665", "doi": "10.1016/j.ijar.2008.07.001", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies and bounds the effects of approximating loss functions and\ncredal sets on choice functions, under very weak assumptions. In particular,\nthe credal set is assumed to be neither convex nor closed. The main result is\nthat the effects of approximation can be bounded, although in general,\napproximation of the credal set may not always be practically possible. In case\nof pairwise choice, I demonstrate how the situation can be improved by showing\nthat only approximations of the extreme points of the closure of the convex\nhull of the credal set need to be taken into account, as expected.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 13:58:04 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Troffaes", "Matthias C. M.", ""]]}, {"id": "1203.1269", "submitter": "Pritam Ranjan", "authors": "Mark Franey, Pritam Ranjan, and Hugh Chipman", "title": "A Short Note on Gaussian Process Modeling for Large Datasets using\n  Graphics Processing Units", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graphics processing unit (GPU) has emerged as a powerful and cost\neffective processor for general performance computing. GPUs are capable of an\norder of magnitude more floating-point operations per second as compared to\nmodern central processing units (CPUs), and thus provide a great deal of\npromise for computationally intensive statistical applications. Fitting complex\nstatistical models with a large number of parameters and/or for large datasets\nis often very computationally expensive. In this study, we focus on Gaussian\nprocess (GP) models -- statistical models commonly used for emulating expensive\ncomputer simulators. We demonstrate that the computational cost of implementing\nGP models can be significantly reduced by using a CPU+GPU heterogeneous\ncomputing system over an analogous implementation on a traditional computing\nsystem with no GPU acceleration. Our small study suggests that GP models are\nfertile ground for further implementation on CPU+GPU systems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 18:19:10 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2012 21:13:38 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Franey", "Mark", ""], ["Ranjan", "Pritam", ""], ["Chipman", "Hugh", ""]]}, {"id": "1203.1547", "submitter": "Gregory Schott", "authors": "Gr\\'egory Schott (for the RooStats Team)", "title": "RooStats for Searches", "comments": "Contributed to \"PHYSTAT 2011 Workshop on Statistical Issues Related\n  to Discovery Claims in Search Experiments and Unfolding\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an hep-ex stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RooStats toolkit, which is distributed with the ROOT software package,\nprovides a large collection of software tools that implement statistical\nmethods commonly used by the High Energy Physics community. The toolkit is\nbased on RooFit, a high-level data analysis modeling package that implements\nvarious methods of statistical data analysis. RooStats enforces a clear mapping\nof statistical concepts to C++ classes and methods and emphasizes the ability\nto easily combine analyses within and across experiments. We present an\noverview of the RooStats toolkit, describe some of the methods used for\nhypothesis testing and estimation of confidence intervals and finally discuss\nsome of the latest developments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 17:30:55 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Schott", "Gr\u00e9gory", "", "for the RooStats Team"]]}, {"id": "1203.2295", "submitter": "Eric Chi", "authors": "Eric C. Chi and Kenneth Lange", "title": "Techniques for Solving Sudoku Puzzles", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving Sudoku puzzles is one of the most popular pastimes in the world.\nPuzzles range in difficulty from easy to very challenging; the hardest puzzles\ntend to have the most empty cells. The current paper explains and compares\nthree algorithms for solving Sudoku puzzles. Backtracking, simulated annealing,\nand alternating projections are generic methods for attacking combinatorial\noptimization problems. Our results favor backtracking. It infallibly solves a\nSudoku puzzle or deduces that a unique solution does not exist. However,\nbacktracking does not scale well in high-dimensional combinatorial\noptimization. Hence, it is useful to expose students in the mathematical\nsciences to the other two solution techniques in a concrete setting. Simulated\nannealing shares a common structure with MCMC (Markov chain Monte Carlo) and\nenjoys wide applicability. The method of alternating projections solves the\nfeasibility problem in convex programming. Converting a discrete optimization\nproblem into a continuous optimization problem opens up the possibility of\nhandling combinatorial problems of much higher dimensionality.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 00:12:05 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2012 16:48:45 GMT"}, {"version": "v3", "created": "Thu, 16 May 2013 17:55:03 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Chi", "Eric C.", ""], ["Lange", "Kenneth", ""]]}, {"id": "1203.2394", "submitter": "Mohamed Ahmed", "authors": "Mohamed Osama Ahmed, Pouyan T. Bibalan, Nando de Freitas and Simon\n  Fauvel", "title": "Decentralized, Adaptive, Look-Ahead Particle Filtering", "comments": "16 pages, 11 figures, Authorship in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decentralized particle filter (DPF) was proposed recently to increase the\nlevel of parallelism of particle filtering. Given a decomposition of the state\nspace into two nested sets of variables, the DPF uses a particle filter to\nsample the first set and then conditions on this sample to generate a set of\nsamples for the second set of variables. The DPF can be understood as a variant\nof the popular Rao-Blackwellized particle filter (RBPF), where the second step\nis carried out using Monte Carlo approximations instead of analytical\ninference. As a result, the range of applications of the DPF is broader than\nthe one for the RBPF. In this paper, we improve the DPF in two ways. First, we\nderive a Monte Carlo approximation of the optimal proposal distribution and,\nconsequently, design and implement a more efficient look-ahead DPF. Although\nthe decentralized filters were initially designed to capitalize on parallel\nimplementation, we show that the look-ahead DPF can outperform the standard\nparticle filter even on a single machine. Second, we propose the use of bandit\nalgorithms to automatically configure the state space decomposition of the DPF.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 02:09:32 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Ahmed", "Mohamed Osama", ""], ["Bibalan", "Pouyan T.", ""], ["de Freitas", "Nando", ""], ["Fauvel", "Simon", ""]]}, {"id": "1203.3083", "submitter": "Aaron Francis McDaid", "authors": "Aaron F. McDaid, Thomas Brendan Murphy, Nial Friel and Neil J Hurley", "title": "Clustering in networks with the collapsed Stochastic Block Model", "comments": "A later version, called \"Improved Bayesian inference for the\n  Stochastic Block Model with application to large networks\" has been accepted\n  by 'Computational Statistics and Data Analysis'. Publication date to be\n  confirmed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  An efficient MCMC algorithm is presented to cluster the nodes of a network\nsuch that nodes with similar role in the network are clustered together. This\nis known as block-modelling or block-clustering. The model is the stochastic\nblockmodel (SBM) with block parameters integrated out. The resulting marginal\ndistribution defines a posterior over the number of clusters and cluster\nmemberships. Sampling from this posterior is simpler than from the original SBM\nas transdimensional MCMC can be avoided. The algorithm is based on the\nallocation sampler. It requires a prior to be placed on the number of clusters,\nthereby allowing the number of clusters to be directly estimated by the\nalgorithm, rather than being given as an input parameter. Synthetic and real\ndata are used to test the speed and accuracy of the model and algorithm,\nincluding the ability to estimate the number of clusters. The algorithm can\nscale to networks with up to ten thousand nodes and tens of millions of edges.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 13:44:00 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2012 14:37:40 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2012 16:28:01 GMT"}, {"version": "v4", "created": "Thu, 8 Nov 2012 13:55:50 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["McDaid", "Aaron F.", ""], ["Murphy", "Thomas Brendan", ""], ["Friel", "Nial", ""], ["Hurley", "Neil J", ""]]}, {"id": "1203.3484", "submitter": "Firas Hamze", "authors": "Firas Hamze, Nando de Freitas", "title": "Intracluster Moves for Constrained Discrete-Space MCMC", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-236-243", "categories": "stat.CO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of sampling from binary distributions with\nconstraints. In particular, it proposes an MCMC method to draw samples from a\ndistribution of the set of all states at a specified distance from some\nreference state. For example, when the reference state is the vector of zeros,\nthe algorithm can draw samples from a binary distribution with a constraint on\nthe number of active variables, say the number of 1's. We motivate the need for\nthis algorithm with examples from statistical physics and probabilistic\ninference. Unlike previous algorithms proposed to sample from binary\ndistributions with these constraints, the new algorithm allows for large moves\nin state space and tends to propose them such that they are energetically\nfavourable. The algorithm is demonstrated on three Boltzmann machines of\nvarying difficulty: A ferromagnetic Ising model (with positive potentials), a\nrestricted Boltzmann machine with learned Gabor-like filters as potentials, and\na challenging three-dimensional spin-glass (with positive and negative\npotentials).\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Hamze", "Firas", ""], ["de Freitas", "Nando", ""]]}, {"id": "1203.3725", "submitter": "Richard Everitt", "authors": "Richard G. Everitt", "title": "Bayesian Parameter Estimation for Latent Markov Random Fields and Social\n  Networks", "comments": "26 pages, 2 figures, accepted in Journal of Computational and\n  Graphical Statistics (http://www.amstat.org/publications/jcgs.cfm)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cond-mat.stat-mech cs.AI cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models are widely used in statistics, physics and\nmachine vision. However Bayesian parameter estimation for undirected models is\nextremely challenging, since evaluation of the posterior typically involves the\ncalculation of an intractable normalising constant. This problem has received\nmuch attention, but very little of this has focussed on the important practical\ncase where the data consists of noisy or incomplete observations of the\nunderlying hidden structure. This paper specifically addresses this problem,\ncomparing two alternative methodologies. In the first of these approaches\nparticle Markov chain Monte Carlo (Andrieu et al., 2010) is used to efficiently\nexplore the parameter space, combined with the exchange algorithm (Murray et\nal., 2006) for avoiding the calculation of the intractable normalising constant\n(a proof showing that this combination targets the correct distribution in\nfound in a supplementary appendix online). This approach is compared with\napproximate Bayesian computation (Pritchard et al., 1999). Applications to\nestimating the parameters of Ising models and exponential random graphs from\nnoisy data are presented. Each algorithm used in the paper targets an\napproximation to the true posterior due to the use of MCMC to simulate from the\nlatent graphical model, in lieu of being able to do this exactly in general.\nThe supplementary appendix also describes the nature of the resulting\napproximation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 07:31:15 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Everitt", "Richard G.", ""]]}, {"id": "1203.3880", "submitter": "Chanseok Park", "authors": "Chanseok Park and Seong Beom Lee", "title": "Parameter Estimation from Censored Samples using the\n  Expectation-Maximization Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report TR2003-05-PL, Department of Mathematical Sciences,\n  Clemson University, Clemson, SC, USA", "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with parameter estimation when the data are randomly right\ncensored. The maximum likelihood estimates from censored samples are obtained\nby using the expectation-maximization (EM) and Monte Carlo EM (MCEM)\nalgorithms. We introduce the concept of the EM and MCEM algorithms and develop\nparameter estimation methods for a variety of distributions such as normal,\nLaplace and Rayleigh distributions. These proposed methods are illustrated with\nthree examples.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 18:06:25 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Park", "Chanseok", ""], ["Lee", "Seong Beom", ""]]}, {"id": "1203.3896", "submitter": "Xi Luo", "authors": "Weidong Liu and Xi Luo", "title": "Fast and Adaptive Sparse Precision Matrix Estimation in High Dimensions", "comments": "Maintext: 24 pages. Supplement: 13 pages. R package scio implementing\n  the proposed method is available on CRAN at\n  https://cran.r-project.org/package=scio . Published in J of Multivariate\n  Analysis at\n  http://www.sciencedirect.com/science/article/pii/S0047259X14002607", "journal-ref": "Journal of Multivariate Analysis. 2015; 135:153 -62", "doi": "10.1016/J.Jmva.2014.11.005", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for estimating sparse precision matrices in\nthe high dimensional setting. It has been popular to study fast computation and\nadaptive procedures for this problem. We propose a novel approach, called\nSparse Column-wise Inverse Operator, to address these two issues. We analyze an\nadaptive procedure based on cross validation, and establish its convergence\nrate under the Frobenius norm. The convergence rates under other matrix norms\nare also established. This method also enjoys the advantage of fast computation\nfor large-scale problems, via a coordinate descent algorithm. Numerical merits\nare illustrated using both simulated and real datasets. In particular, it\nperforms favorably on an HIV brain tissue dataset and an ADHD resting-state\nfMRI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 21:58:02 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 07:09:46 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Liu", "Weidong", ""], ["Luo", "Xi", ""]]}, {"id": "1203.4468", "submitter": "Chanseok Park", "authors": "Chanseok Park", "title": "A Quantile Variant of the EM Algorithm and Its Applications to Parameter\n  Estimation with Interval Data", "comments": null, "journal-ref": null, "doi": "10.1177/1748301818779007", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm is a powerful computational\ntechnique for finding the maximum likelihood estimates for parametric models\nwhen the data are not fully observed. The EM is best suited for situations\nwhere the expectation in each E-step and the maximization in each M-step are\nstraightforward. A difficulty with the implementation of the EM algorithm is\nthat each E-step requires the integration of the log-likelihood function in\nclosed form. The explicit integration can be avoided by using what is known as\nthe Monte Carlo EM (MCEM) algorithm. The MCEM uses a random sample to estimate\nthe integral at each E-step. However, the problem with the MCEM is that it\noften converges to the integral quite slowly and the convergence behavior can\nalso be unstable, which causes a computational burden. In this paper, we\npropose what we refer to as the quantile variant of the EM (QEM) algorithm. We\nprove that the proposed QEM method has an accuracy of $O(1/K^2)$ while the MCEM\nmethod has an accuracy of $O_p(1/\\sqrt{K})$. Thus, the proposed QEM method\npossesses faster and more stable convergence properties when compared with the\nMCEM algorithm. The improved performance is illustrated through the numerical\nstudies. Several practical examples illustrating its use in interval-censored\ndata problems are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 15:23:27 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 10:34:13 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Park", "Chanseok", ""]]}, {"id": "1203.5475", "submitter": "Alexander Jung", "authors": "Alexander Jung and Georg Taub\\\"ock and Franz Hlawatsch", "title": "Compressive Spectral Estimation for Nonstationary Random Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the spectral characteristics of a nonstationary random process is\nan important but challenging task, which can be facilitated by exploiting\nstructural properties of the process. In certain applications, the observed\nprocesses are underspread, i.e., their time and frequency correlations exhibit\na reasonably fast decay, and approximately time-frequency sparse, i.e., a\nreasonably large percentage of the spectral values is small. For this class of\nprocesses, we propose a compressive estimator of the discrete Rihaczek spectrum\n(RS). This estimator combines a minimum variance unbiased estimator of the RS\n(which is a smoothed Rihaczek distribution using an appropriately designed\nsmoothing kernel) with a compressed sensing technique that exploits the\napproximate time-frequency sparsity. As a result of the compression stage, the\nnumber of measurements required for good estimation performance can be\nsignificantly reduced. The measurements are values of the discrete ambiguity\nfunction of the observed signal at randomly chosen time and frequency lag\npositions. We provide bounds on the mean-square estimation error of both the\nminimum variance unbiased RS estimator and the compressive RS estimator, and we\ndemonstrate the performance of the compressive estimator by means of simulation\nresults. The proposed compressive RS estimator can also be used for estimating\nother time-dependent spectra (e.g., the Wigner-Ville spectrum) since for an\nunderspread process most spectra are almost equal.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2012 07:35:12 GMT"}, {"version": "v2", "created": "Sat, 21 Apr 2012 16:58:09 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2013 15:16:06 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Jung", "Alexander", ""], ["Taub\u00f6ck", "Georg", ""], ["Hlawatsch", "Franz", ""]]}, {"id": "1203.5483", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani, Bhiksha Raj, and Petros Boufounos", "title": "Greedy Sparsity-Constrained Optimization", "comments": null, "journal-ref": "Journal of Machine Learning Research, 14(3):807--841, 2013", "doi": null, "report-no": null, "categories": "stat.ML math.NA math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-constrained optimization has wide applicability in machine learning,\nstatistics, and signal processing problems such as feature selection and\ncompressive Sensing. A vast body of work has studied the sparsity-constrained\noptimization from theoretical, algorithmic, and application aspects in the\ncontext of sparse estimation in linear models where the fidelity of the\nestimate is measured by the squared error. In contrast, relatively less effort\nhas been made in the study of sparsity-constrained optimization in cases where\nnonlinear models are involved or the cost function is not quadratic. In this\npaper we propose a greedy algorithm, Gradient Support Pursuit (GraSP), to\napproximate sparse minima of cost functions of arbitrary form. Should a cost\nfunction have a Stable Restricted Hessian (SRH) or a Stable Restricted\nLinearization (SRL), both of which are introduced in this paper, our algorithm\nis guaranteed to produce a sparse vector within a bounded distance from the\ntrue sparse optimum. Our approach generalizes known results for quadratic cost\nfunctions that arise in sparse linear regression and Compressive Sensing. We\nalso evaluate the performance of GraSP through numerical simulations on\nsynthetic data, where the algorithm is employed for sparse logistic regression\nwith and without $\\ell_2$-regularization.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2012 10:01:01 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2012 14:54:12 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2013 15:18:28 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Bahmani", "Sohail", ""], ["Raj", "Bhiksha", ""], ["Boufounos", "Petros", ""]]}, {"id": "1203.5950", "submitter": "Joseph Dureau", "authors": "Joseph Dureau, Konstantinos Kalogeropoulos and Marc Baguelin", "title": "Capturing the time-varying drivers of an epidemic using stochastic\n  dynamical systems", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemics are often modelled using non-linear dynamical systems observed\nthrough partial and noisy data. In this paper, we consider stochastic\nextensions in order to capture unknown influences (changing behaviors, public\ninterventions, seasonal effects etc). These models assign diffusion processes\nto the time-varying parameters, and our inferential procedure is based on a\nsuitably adjusted adaptive particle MCMC algorithm. The performance of the\nproposed computational methods is validated on simulated data and the adopted\nmodel is applied to the 2009 H1N1 pandemic in England. In addition to\nestimating the effective contact rate trajectories, the methodology is applied\nin real time to provide evidence in related public health decisions. Diffusion\ndriven SEIR-type models with age structure are also introduced.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 12:25:00 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2012 14:18:44 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Dureau", "Joseph", ""], ["Kalogeropoulos", "Konstantinos", ""], ["Baguelin", "Marc", ""]]}, {"id": "1203.6216", "submitter": "Konstantinos Kalogeropoulos", "authors": "Alexandros Beskos, Konstantinos Kalogeropoulos and Erik Pazos", "title": "Advanced MCMC Methods for Sampling on Diffusion Pathspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to calibrate increasingly complex statistical models requires a\npersistent effort for further advances on available, computationally intensive\nMonte Carlo methods. We study here an advanced version of familiar Markov Chain\nMonte Carlo (MCMC) algorithms that sample from target distributions defined as\nchange of measures from Gaussian laws on general Hilbert spaces. Such a model\nstructure arises in several contexts: we focus here at the important class of\nstatistical models driven by diffusion paths whence the Wiener process\nconstitutes the reference Gaussian law. Particular emphasis is given on\nadvanced Hybrid Monte-Carlo (HMC) which makes large, derivative-driven steps in\nthe state space (in contrast with local-move Random-walk-type algorithms) with\nanalytical and experimental results. We illustrate it's computational\nadvantages in various diffusion processes and observation regimes; examples\ninclude stochastic volatility and latent survival models. In contrast with\ntheir standard MCMC counterparts, the advanced versions have mesh-free mixing\ntimes, as these will not deteriorate upon refinement of the approximation of\nthe inherently infinite-dimensional diffusion paths by finite-dimensional ones\nused in practice when applying the algorithms on a computer.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 10:25:07 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 17:51:06 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Beskos", "Alexandros", ""], ["Kalogeropoulos", "Konstantinos", ""], ["Pazos", "Erik", ""]]}, {"id": "1203.6276", "submitter": "Pekka Malo", "authors": "Ankur Sinha, Pekka Malo, Timo Kuosmanen", "title": "A Multi-objective Exploratory Procedure for Regression Model Selection", "comments": "in Journal of Computational and Graphical Statistics, Vol. 24, Iss.\n  1, 2015", "journal-ref": null, "doi": "10.1080/10618600.2014.899236", "report-no": null, "categories": "stat.CO cs.NE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is recognized as one of the most critical steps in\nstatistical modeling. The problems encountered in engineering and social\nsciences are commonly characterized by over-abundance of explanatory variables,\nnon-linearities and unknown interdependencies between the regressors. An added\ndifficulty is that the analysts may have little or no prior knowledge on the\nrelative importance of the variables. To provide a robust method for model\nselection, this paper introduces the Multi-objective Genetic Algorithm for\nVariable Selection (MOGA-VS) that provides the user with an optimal set of\nregression models for a given data-set. The algorithm considers the regression\nproblem as a two objective task, and explores the Pareto-optimal (best subset)\nmodels by preferring those models over the other which have less number of\nregression coefficients and better goodness of fit. The model exploration can\nbe performed based on in-sample or generalization error minimization. The model\nselection is proposed to be performed in two steps. First, we generate the\nfrontier of Pareto-optimal regression models by eliminating the dominated\nmodels without any user intervention. Second, a decision making process is\nexecuted which allows the user to choose the most preferred model using\nvisualisations and simple metrics. The method has been evaluated on a recently\npublished real dataset on Communities and Crime within United States.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 14:15:24 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 15:54:33 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2013 22:34:01 GMT"}, {"version": "v4", "created": "Wed, 13 Jul 2016 07:53:01 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Sinha", "Ankur", ""], ["Malo", "Pekka", ""], ["Kuosmanen", "Timo", ""]]}, {"id": "1203.6452", "submitter": "David Ginsbourger", "authors": "Cl\\'ement Chevalier (IRSN-SEC), David Ginsbourger", "title": "Corrected Kriging update formulae for batch-sequential data assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a lot of effort has been paid to the efficient computation of\nKriging predictors when observations are assimilated sequentially. In\nparticular, Kriging update formulae enabling significant computational savings\nwere derived in Barnes and Watson (1992), Gao et al. (1996), and Emery (2009).\nTaking advantage of the previous Kriging mean and variance calculations helps\navoiding a costly $(n+1) \\times (n+1)$ matrix inversion when adding one\nobservation to the $n$ already available ones. In addition to traditional\nupdate formulae taking into account a single new observation, Emery (2009) also\nproposed formulae for the batch-sequential case, i.e. when $r > 1$ new\nobservations are simultaneously assimilated. However, the Kriging variance and\ncovariance formulae given without proof in Emery (2009) for the\nbatch-sequential case are not correct. In this paper we fix this issue and\nestablish corrected expressions for updated Kriging variances and covariances\nwhen assimilating several observations in parallel.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 07:41:52 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Chevalier", "Cl\u00e9ment", "", "IRSN-SEC"], ["Ginsbourger", "David", ""]]}, {"id": "1203.6750", "submitter": "Marco Huber", "authors": "Marco F. Huber", "title": "Adaptive Gaussian Mixture Filter Based on Statistical Linearization", "comments": "8 pages, appeared in the proceedings of the 14th International\n  Conference on Information Fusion, Chicago, Illinois, USA, July 2011.\n  Correction of an error in formula (22).\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5977694&isnumber=5977431", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixtures are a common density representation in nonlinear,\nnon-Gaussian Bayesian state estimation. Selecting an appropriate number of\nGaussian components, however, is difficult as one has to trade of computational\ncomplexity against estimation accuracy. In this paper, an adaptive Gaussian\nmixture filter based on statistical linearization is proposed. Depending on the\nnonlinearity of the considered estimation problem, this filter dynamically\nincreases the number of components via splitting. For this purpose, a measure\nis introduced that allows for quantifying the locally induced linearization\nerror at each Gaussian mixture component. The deviation between the nonlinear\nand the linearized state space model is evaluated for determining the splitting\ndirection. The proposed approach is not restricted to a specific statistical\nlinearization method. Simulations show the superior estimation performance\ncompared to related approaches and common filtering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 09:05:35 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Huber", "Marco F.", ""]]}]