[{"id": "1810.00117", "submitter": "Toby Hocking", "authors": "Toby Dylan Hocking, Guillem Rigaill, Paul Fearnhead, Guillaume Bourque", "title": "Generalized Functional Pruning Optimal Partitioning (GFPOP) for\n  Constrained Changepoint Detection in Genomic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new algorithm and R package for peak detection in genomic data\nsets using constrained changepoint algorithms. These detect changes from\nbackground to peak regions by imposing the constraint that the mean should\nalternately increase then decrease. An existing algorithm for this problem\nexists, and gives state-of-the-art accuracy results, but it is computationally\nexpensive when the number of changes is large. We propose the GFPOP algorithm\nthat jointly estimates the number of peaks and their locations by minimizing a\ncost function which consists of a data fitting term and a penalty for each\nchangepoint. Empirically this algorithm has a cost that is $O(N \\log(N))$ for\nanalysing data of length $N$. We also propose a sequential search algorithm\nthat finds the best solution with $K$ segments in $O(\\log(K)N \\log(N))$ time,\nwhich is much faster than the previous $O(KN \\log(N))$ algorithm. We show that\nour disk-based implementation in the PeakSegDisk R package can be used to\nquickly compute constrained optimal models with many changepoints, which are\nneeded to analyze typical genomic data sets that have tens of millions of\nobservations.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 00:10:30 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Hocking", "Toby Dylan", ""], ["Rigaill", "Guillem", ""], ["Fearnhead", "Paul", ""], ["Bourque", "Guillaume", ""]]}, {"id": "1810.00118", "submitter": "Jialin Liu", "authors": "Jialin Liu, Wotao Yin, Wuchen Li, Yat Tin Chow", "title": "Multilevel Optimal Transport: a Fast Approximation of Wasserstein-1\n  distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast algorithm for the calculation of the Wasserstein-1\ndistance, which is a particular type of optimal transport distance with\nhomogeneous of degree one transport cost.\n  Our algorithm is built on multilevel primal-dual algorithms. Several\nnumerical examples and a complexity analysis are provided to demonstrate its\ncomputational speed. On some commonly used image examples of size\n$512\\times512$, the proposed algorithm gives solutions within $0.2\\sim 1.5$\nseconds on a single CPU, which is much faster than the state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 00:29:05 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 03:41:38 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 04:07:04 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Liu", "Jialin", ""], ["Yin", "Wotao", ""], ["Li", "Wuchen", ""], ["Chow", "Yat Tin", ""]]}, {"id": "1810.00289", "submitter": "Fr\\'ed\\'eric Bertrand", "authors": "F. Bertrand and M. Maumy-Bertrand", "title": "A Sheet of Maple to Compute Second-Order Edgeworth Expansions and\n  Related Quantities of any Function of the Mean of an iid Sample of an\n  Absolutely Continuous Distribution", "comments": "20 pages, 8 figures, code snippet", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We designed a completely automated Maple ($\\geqslant 15$) worksheet for\nderiving Edgeworth and Cornish-Fisher expansions as well as the acceleration\nconstant of the bootstrap bias-corrected and accelerated technique. It is valid\nfor non-parametric or parametric bootstrap, of any (studentized) statistics\nthat is -a regular enough- function of the mean of an iid sample of an\nabsolutely continuous distribution.\n  This worksheet allowed us to point out one error in the second-order\nCornish-Fisher expansion of the studentized mean stated in Theorem 13.5 by Das\nGupta in [8, p. 194] as well as lay the stress on the influence of the slight\nchange of the normalizing constant when computing the second-order Edgeworth\nand Cornish-Fisher expansions of the t-distribution as stated in Theorem 11.4.2\nby Lehman and Romano in [14, p. 460].\n  In addition, we successfully applied the worksheet to a complex maximum\nlikelihood estimator as a first step to derive more accurate confidence\nintervals in order to enhance quality controls. The worksheet also features\nexport of Maple results into R code. In addition, we provide R code to plot\nthese expansions as well as their increasing rearrangements. All these\nsupplemental materials are available upon request.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 01:26:19 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bertrand", "F.", ""], ["Maumy-Bertrand", "M.", ""]]}, {"id": "1810.00412", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Yue Sheng", "title": "Distributed linear regression by averaging", "comments": "V2 adds a new section on iterative averaging methods, adds\n  applications of the calculus of deterministic equivalents, and reorganizes\n  the paper", "journal-ref": "Annals of Statistics, 2020+", "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed statistical learning problems arise commonly when dealing with\nlarge datasets. In this setup, datasets are partitioned over machines, which\ncompute locally, and communicate short messages. Communication is often the\nbottleneck. In this paper, we study one-step and iterative weighted parameter\naveraging in statistical linear models under data parallelism. We do linear\nregression on each machine, send the results to a central server, and take a\nweighted average of the parameters. Optionally, we iterate, sending back the\nweighted average and doing local ridge regressions centered at it. How does\nthis work compared to doing linear regression on the full data? Here we study\nthe performance loss in estimation, test error, and confidence interval length\nin high dimensions, where the number of parameters is comparable to the\ntraining data size. We find the performance loss in one-step weighted\naveraging, and also give results for iterative averaging. We also find that\ndifferent problems are affected differently by the distributed framework.\nEstimation error and confidence interval length increase a lot, while\nprediction error increases much less. We rely on recent results from random\nmatrix theory, where we develop a new calculus of deterministic equivalents as\na tool of broader interest.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 15:59:03 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 04:33:28 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Dobriban", "Edgar", ""], ["Sheng", "Yue", ""]]}, {"id": "1810.00810", "submitter": "Sebastian Matera", "authors": "Sandra D\\\"opking and Sebastian Matera", "title": "Multilevel Adaptive Sparse Grid Quadrature for Monte Carlo models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems require to approximate an expected value by some kind of Monte\nCarlo (MC) sampling, e.g. molecular dynamics (MD) or simulation of stochastic\nreaction models (also termed kinetic Monte Carlo (kMC)). Often, we are\nfurthermore interested in some integral of the MC model's output over the input\nparameters. We present a Multilevel Adaptive Sparse Grid strategy for the\nnumerical integration of such problems where the integrand is implicitly\ndefined by a Monte Carlo model. In this approach, we exploit different levels\nof sampling accuracy in the Monte Carlo model to reduce the overall\ncomputational costs compared to a single level approach. Unlike existing\napproaches for Multilevel Numerical Quadrature, our approach is not based on a\ntelescoping sum, but we rather utilize the intrinsic multilevel structure of\nthe sparse grids and the employed locally supported, piecewise linear basis\nfunctions. Besides illustrative toy models, we demonstrate the methodology on a\nrealistic kMC model for CO oxidation. We find significant savings compared to\nthe single level approach - often orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 16:47:39 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 16:59:51 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["D\u00f6pking", "Sandra", ""], ["Matera", "Sebastian", ""]]}, {"id": "1810.01005", "submitter": "Fr\\'ed\\'eric Bertrand", "authors": "F. Bertrand, and M. Maumy-Bertrand", "title": "plsRglm: Partial least squares linear and generalized linear regression\n  for processing incomplete datasets by cross-validation and bootstrap\n  techniques with R", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the plsRglm package is to deal with complete and incomplete\ndatasets through several new techniques or, at least, some which were not yet\nimplemented in R. Indeed, not only does it make available the extension of the\nPLS regression to the generalized linear regression models, but also bootstrap\ntechniques, leave-one-out and repeated $k$-fold cross-validation. In addition,\ngraphical displays help the user to assess the significance of the predictors\nwhen using bootstrap techniques. Biplots (Fig. 4) can be used to delve into the\nrelationship between individuals and variables.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 22:59:38 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bertrand", "F.", ""], ["Maumy-Bertrand", "M.", ""]]}, {"id": "1810.01072", "submitter": "Kenyon Ng", "authors": "Kenyon Ng, Berwin A. Turlach, Kevin Murray", "title": "A flexible sequential Monte Carlo algorithm for parametric constrained\n  regression", "comments": "Typo corrections. Code available on\n  https://github.com/weiyaw/blackbox", "journal-ref": "Computational Statistics & Data Analysis 138 (2019) 13-26", "doi": "10.1016/j.csda.2019.03.011", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is proposed that enables the imposition of shape constraints on\nregression curves, without requiring the constraints to be written as\nclosed-form expressions, nor assuming the functional form of the loss function.\nThis algorithm is based on Sequential Monte Carlo-Simulated Annealing and only\nrelies on an indicator function that assesses whether or not the constraints\nare fulfilled, thus allowing the enforcement of various complex constraints by\nspecifying an appropriate indicator function without altering other parts of\nthe algorithm. The algorithm is illustrated by fitting rational function and\nB-spline regression models subject to a monotonicity constraint. An\nimplementation of the algorithm using R is freely available on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 05:18:44 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 12:26:58 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 05:26:21 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ng", "Kenyon", ""], ["Turlach", "Berwin A.", ""], ["Murray", "Kevin", ""]]}, {"id": "1810.01116", "submitter": "Jaehyuk Choi", "authors": "Jaehyuk Choi, Yeda Du, Qingshuo Song", "title": "Inverse Gaussian quadrature and finite normal-mixture approximation of\n  the generalized hyperbolic distribution", "comments": null, "journal-ref": "Journal of Computational and Applied Mathematics, 388:113302, 2021", "doi": "10.1016/j.cam.2020.113302", "report-no": null, "categories": "stat.CO q-fin.CP q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a numerical quadrature for the generalized inverse Gaussian\ndistribution is derived from the Gauss-Hermite quadrature by exploiting its\nrelationship with the normal distribution. The proposed quadrature is not\nGaussian, but it exactly integrates the polynomials of both positive and\nnegative orders. Using the quadrature, the generalized hyperbolic distribution\nis efficiently approximated as a finite normal variance-mean mixture.\nTherefore, the expectations under the distribution, such as cumulative\ndistribution function and European option price, are accurately computed as\nweighted sums of those under normal distributions. The generalized hyperbolic\nrandom variates are also sampled in a straightforward manner. The accuracy of\nthe methods is illustrated with numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 08:39:05 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 07:41:45 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 17:09:04 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Choi", "Jaehyuk", ""], ["Du", "Yeda", ""], ["Song", "Qingshuo", ""]]}, {"id": "1810.01382", "submitter": "Maxime Rischard", "authors": "Maxime Rischard, Pierre E. Jacob, Natesh Pillai", "title": "Unbiased estimation of log normalizing constants with applications to\n  Bayesian cross-validation", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posterior distributions often feature intractable normalizing constants,\ncalled marginal likelihoods or evidence, that are useful for model comparison\nvia Bayes factors. This has motivated a number of methods for estimating ratios\nof normalizing constants in statistics. In computational physics the logarithm\nof these ratios correspond to free energy differences. Combining unbiased\nMarkov chain Monte Carlo estimators with path sampling, also called\nthermodynamic integration, we propose new unbiased estimators of the logarithm\nof ratios of normalizing constants. As a by-product, we propose unbiased\nestimators of the Bayesian cross-validation criterion. The proposed estimators\nare consistent, asymptotically Normal and can easily benefit from parallel\nprocessing devices. Various examples are considered for illustration.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 17:26:18 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Rischard", "Maxime", ""], ["Jacob", "Pierre E.", ""], ["Pillai", "Natesh", ""]]}, {"id": "1810.01675", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri, Subhro Ghosh, David J. Nott, Kim Cuc Pham", "title": "An easy-to-use empirical likelihood ABC method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientifically well-motivated statistical models in natural, engineering\nand environmental sciences are specified through a generative process, but in\nsome cases it may not be possible to write down a likelihood for these models\nanalytically. Approximate Bayesian computation (ABC) methods, which allow\nBayesian inference in these situations, are typically computationally\nintensive. Recently, computationally attractive empirical likelihood based ABC\nmethods have been suggested in the literature. These methods heavily rely on\nthe availability of a set of suitable analytically tractable estimating\nequations. We propose an easy-to-use empirical likelihood ABC method, where the\nonly inputs required are a choice of summary statistic, it's observed value,\nand the ability to simulate summary statistics for any parameter value under\nthe model. It is shown that the posterior obtained using the proposed method is\nconsistent, and its performance is explored using various examples.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 10:37:31 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 11:20:19 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Chaudhuri", "Sanjay", ""], ["Ghosh", "Subhro", ""], ["Nott", "David J.", ""], ["Pham", "Kim Cuc", ""]]}, {"id": "1810.01710", "submitter": "Joakim Beck", "authors": "Marco Ballesio, Joakim Beck, Anamika Pandey, Laura Parisi, Erik von\n  Schwerin, Raul Tempone", "title": "Multilevel Monte Carlo Acceleration of Seismic Wave Propagation under\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We interpret uncertainty in a model for seismic wave propagation by treating\nthe model parameters as random variables, and apply the Multilevel Monte Carlo\n(MLMC) method to reduce the cost of approximating expected values of selected,\nphysically relevant, quantities of interest (QoI) with respect to the random\nvariables. Targeting source inversion problems, where the source of an\nearthquake is inferred from ground motion recordings on the Earth's surface, we\nconsider two QoI that measure the discrepancies between computed seismic\nsignals and given reference signals: one QoI, $\\hbox{QoI}_E$, is defined in\nterms of the $L^2$-misfit, which is directly related to maximum likelihood\nestimates of the source parameters; the other, $\\hbox{QoI}_W$, is based on the\nquadratic Wasserstein distance between probability distributions, and\nrepresents one possible choice in a class of such misfit functions that have\nbecome increasingly popular to solve seismic inversion in recent years. We\nsimulate seismic wave propagation, including seismic attenuation, using a\npublicly available code in widespread use, based on the spectral element\nmethod. Using random coefficients and deterministic initial and boundary data,\nwe present benchmark numerical experiments with synthetic data in a\ntwo-dimensional physical domain and a one-dimensional velocity model where the\nassumed parameter uncertainty is motivated by realistic Earth models. Here, the\ncomputational cost of the standard Monte Carlo method was reduced by up to 97%\nfor $\\hbox{QoI}_E$, and up to 78% for $\\hbox{QoI}_W$, using a relevant range of\ntolerances. Shifting to three-dimensional domains is straight-forward and will\nfurther increase the relative computational work reduction.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 12:12:46 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 10:35:53 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 13:18:58 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Ballesio", "Marco", ""], ["Beck", "Joakim", ""], ["Pandey", "Anamika", ""], ["Parisi", "Laura", ""], ["von Schwerin", "Erik", ""], ["Tempone", "Raul", ""]]}, {"id": "1810.01761", "submitter": "Moritz Schauer", "authors": "Joris Bierkens, Frank van der Meulen, Moritz Schauer", "title": "Simulation of elliptic and hypo-elliptic conditional diffusions", "comments": null, "journal-ref": "Adv. Appl. Probab. 52 (2020) 173-212", "doi": "10.1017/apr.2019.54", "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose $X$ is a multidimensional diffusion process. Assume that at time zero\nthe state of $X$ is fully observed, but at time $T>0$ only linear combinations\nof its components are observed. That is, one only observes the vector $L X_T$\nfor a given matrix $L$. In this paper we show how samples from the conditioned\nprocess can be generated. The main contribution of this paper is to prove that\nguided proposals, introduced in Schauer et al. (2017), can be used in a unified\nway for both uniformly and hypo-elliptic diffusions, also when $L$ is not the\nidentity matrix. This is illustrated by excellent performance in two\nchallenging cases: a partially observed twice integrated diffusion with\nmultiple wells and the partially observed FitzHugh-Nagumo model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 14:28:05 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 07:23:51 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Bierkens", "Joris", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""]]}, {"id": "1810.01903", "submitter": "Jiangeng Huang", "authors": "Jiangeng Huang, Robert B. Gramacy, Mickael Binois, Mirko Libraschi", "title": "On-site surrogates for large-scale calibration", "comments": "31 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a computer model calibration problem from the oil and gas\nindustry, involving the design of a honeycomb seal, we develop a new Bayesian\nmethodology to cope with limitations in the canonical apparatus stemming from\nseveral factors. We propose a new strategy of on-site design and surrogate\nmodeling for a computer simulator acting on a high-dimensional input space\nthat, although relatively speedy, is prone to numerical instabilities, missing\ndata, and nonstationary dynamics. Our aim is to strike a balance between\ndata-faithful modeling and computational tractability in a calibration\nframework--tailoring the computer model to a limited field experiment.\nSituating our on-site surrogates within the canonical calibration apparatus\nrequires updates to that framework. We describe a novel yet intuitive Bayesian\nsetup that carefully decomposes otherwise prohibitively large matrices by\nexploiting the sparse blockwise structure. Empirical illustrations demonstrate\nthat this approach performs well on toy data and our motivating honeycomb\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 18:25:08 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 15:55:05 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Huang", "Jiangeng", ""], ["Gramacy", "Robert B.", ""], ["Binois", "Mickael", ""], ["Libraschi", "Mirko", ""]]}, {"id": "1810.02030", "submitter": "Chao Gao", "authors": "Chao Gao, Jiyi Liu, Yuan Yao, Weizhi Zhu", "title": "Robust Estimation and Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation under Huber's $\\epsilon$-contamination model has become an\nimportant topic in statistics and theoretical computer science. Statistically\noptimal procedures such as Tukey's median and other estimators based on depth\nfunctions are impractical because of their computational intractability. In\nthis paper, we establish an intriguing connection between $f$-GANs and various\ndepth functions through the lens of $f$-Learning. Similar to the derivation of\n$f$-GANs, we show that these depth functions that lead to statistically optimal\nrobust estimators can all be viewed as variational lower bounds of the total\nvariation distance in the framework of $f$-Learning. This connection opens the\ndoor of computing robust estimators using tools developed for training GANs. In\nparticular, we show in both theory and experiments that some appropriate\nstructures of discriminator networks with hidden layers in GANs lead to\nstatistically optimal robust location estimators for both Gaussian distribution\nand general elliptical distributions where first moment may not exist.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 02:37:16 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 01:47:46 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 20:09:43 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Gao", "Chao", ""], ["Liu", "Jiyi", ""], ["Yao", "Yuan", ""], ["Zhu", "Weizhi", ""]]}, {"id": "1810.03136", "submitter": "Tom Reynkens", "authors": "Sander Devriendt, Katrien Antonio, Tom Reynkens, Roel Verbelen", "title": "Sparse Regression with Multi-type Regularized Feature Modeling", "comments": null, "journal-ref": "Insurance: Mathematics and Economics (2020)", "doi": "10.1016/j.insmatheco.2020.11.010", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the statistical and machine learning literature, regularization\ntechniques are often used to construct sparse (predictive) models. Most\nregularization strategies only work for data where all predictors are treated\nidentically, such as Lasso regression for (continuous) predictors treated as\nlinear effects. However, many predictive problems involve different types of\npredictors and require a tailored regularization term. We propose a multi-type\nLasso penalty that acts on the objective function as a sum of subpenalties, one\nfor each type of predictor. As such, we allow for predictor selection and level\nfusion within a predictor in a data-driven way, simultaneous with the parameter\nestimation process. We develop a new estimation strategy for convex predictive\nmodels with this multi-type penalty. Using the theory of proximal operators,\nour estimation procedure is computationally efficient, partitioning the overall\noptimization problem into easier to solve subproblems, specific for each\npredictor type and its associated penalty. Earlier research applies\napproximations to non-differentiable penalties to solve the optimization\nproblem. The proposed SMuRF algorithm removes the need for approximations and\nachieves a higher accuracy and computational efficiency. This is demonstrated\nwith an extensive simulation study and the analysis of a case-study on\ninsurance pricing analytics.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 12:42:23 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 17:10:06 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Devriendt", "Sander", ""], ["Antonio", "Katrien", ""], ["Reynkens", "Tom", ""], ["Verbelen", "Roel", ""]]}, {"id": "1810.03398", "submitter": "Simon Bartels", "authors": "Simon Bartels, Jon Cockayne, Ilse C. F. Ipsen and Philipp Hennig", "title": "Probabilistic Linear Solvers: A Unifying View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have developed a new, probabilistic interpretation for\nnumerical algorithms solving linear systems in which the solution is inferred\nin a Bayesian framework, either directly or by inferring the unknown action of\nthe matrix inverse. These approaches have typically focused on replicating the\nbehavior of the conjugate gradient method as a prototypical iterative method.\nIn this work surprisingly general conditions for equivalence of these disparate\nmethods are presented. We also describe connections between probabilistic\nlinear solvers and projection methods for linear systems, providing a\nprobabilistic interpretation of a far more general class of iterative methods.\nIn particular, this provides such an interpretation of the generalised minimum\nresidual method. A probabilistic view of preconditioning is also introduced.\nThese developments unify the literature on probabilistic linear solvers, and\nprovide foundational connections to the literature on iterative solvers for\nlinear systems.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 12:19:53 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 12:25:36 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Bartels", "Simon", ""], ["Cockayne", "Jon", ""], ["Ipsen", "Ilse C. F.", ""], ["Hennig", "Philipp", ""]]}, {"id": "1810.03440", "submitter": "Filip Tronarp", "authors": "Filip Tronarp, Hans Kersting, Simo S\\\"arkk\\\"a, Philipp Hennig", "title": "Probabilistic Solutions To Ordinary Differential Equations As Non-Linear\n  Bayesian Filtering: A New Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate probabilistic numerical approximations to solutions of ordinary\ndifferential equations (ODEs) as problems in Gaussian process (GP) regression\nwith non-linear measurement functions. This is achieved by defining the\nmeasurement sequence to consist of the observations of the difference between\nthe derivative of the GP and the vector field evaluated at the GP---which are\nall identically zero at the solution of the ODE. When the GP has a state-space\nrepresentation, the problem can be reduced to a non-linear Bayesian filtering\nproblem and all widely-used approximations to the Bayesian filtering and\nsmoothing problems become applicable. Furthermore, all previous GP-based ODE\nsolvers that are formulated in terms of generating synthetic measurements of\nthe gradient field come out as specific approximations. Based on the non-linear\nBayesian filtering problem posed in this paper, we develop novel Gaussian\nsolvers for which we establish favourable stability properties. Additionally,\nnon-Gaussian approximations to the filtering problem are derived by the\nparticle filter approach. The resulting solvers are compared with other\nprobabilistic solvers in illustrative experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 13:36:24 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 16:30:22 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 12:20:07 GMT"}, {"version": "v4", "created": "Wed, 24 Apr 2019 09:13:11 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Tronarp", "Filip", ""], ["Kersting", "Hans", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Hennig", "Philipp", ""]]}, {"id": "1810.03688", "submitter": "Vadim Sokolov", "authors": "Laura Schultz and Vadim Sokolov", "title": "Practical Bayesian Optimization for Transportation Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a method to solve optimization problem when objective function is\na complex stochastic simulator of an urban transportation system. To reach this\ngoal, a Bayesian optimization framework is introduced. We show how the choice\nof prior and inference algorithm effect the outcome of our optimization\nprocedure. We develop dimensionality reduction techniques that allow for our\noptimization techniques to be applicable for real-life problems. We develop a\ndistributed, Gaussian Process Bayesian regression and active learning models\nthat allow parallel execution of our algorithms and enable usage of high\nperformance computing. We present a fully Bayesian approach that is more sample\nefficient and reduces computational budget. Our framework is supported by\ntheoretical analysis and an empirical study. We demonstrate our framework on\nthe problem of calibrating a multi-modal transportation network of city of\nBloomington, Illinois. Finally, we discuss directions for further research.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 20:31:22 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 06:29:12 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Schultz", "Laura", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1810.03814", "submitter": "Yueyong Shi", "authors": "Jian Huang, Yuling Jiao, Xiliang Lu, Yueyong Shi, Qinglong Yang", "title": "SNAP: A semismooth Newton algorithm for pathwise optimization with\n  optimal local convergence rate and oracle properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semismooth Newton algorithm for pathwise optimization (SNAP) for\nthe LASSO and Enet in sparse, high-dimensional linear regression. SNAP is\nderived from a suitable formulation of the KKT conditions based on Newton\nderivatives. It solves the semismooth KKT equations efficiently by actively and\ncontinuously seeking the support of the regression coefficients along the\nsolution path with warm start. At each knot in the path, SNAP converges locally\nsuperlinearly for the Enet criterion and achieves an optimal local convergence\nrate for the LASSO criterion, i.e., SNAP converges in one step at the cost of\ntwo matrix-vector multiplication per iteration. Under certain regularity\nconditions on the design matrix and the minimum magnitude of the nonzero\nelements of the target regression coefficients, we show that SNAP hits a\nsolution with the same signs as the regression coefficients and achieves a\nsharp estimation error bound in finite steps with high probability. The\ncomputational complexity of SNAP is shown to be the same as that of LARS and\ncoordinate descent algorithms per iteration. Simulation studies and real data\nanalysis support our theoretical results and demonstrate that SNAP is faster\nand accurate than LARS and coordinate descent algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 04:44:42 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Lu", "Xiliang", ""], ["Shi", "Yueyong", ""], ["Yang", "Qinglong", ""]]}, {"id": "1810.04200", "submitter": "Marcin Jurek", "authors": "Marcin Jurek and Matthias Katzfuss", "title": "Multi-resolution filters for massive spatio-temporal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal data sets are rapidly growing in size. For example,\nenvironmental variables are measured with ever-higher resolution by increasing\nnumbers of automated sensors mounted on satellites and aircraft. Using such\ndata, which are typically noisy and incomplete, the goal is to obtain complete\nmaps of the spatio-temporal process, together with proper uncertainty\nquantification. We focus here on real-time filtering inference in linear\nGaussian state-space models. At each time point, the state is a spatial field\nevaluated on a very large spatial grid, making exact inference using the Kalman\nfilter computationally infeasible. Instead, we propose a multi-resolution\nfilter (MRF), a highly scalable and fully probabilistic filtering method that\nresolves spatial features at all scales. We prove that the MRF matrices exhibit\na particular block-sparse multi-resolution structure that is preserved under\nfiltering operations through time. We also discuss inference on time-varying\nparameters using an approximate Rao-Blackwellized particle filter, in which the\nintegrated likelihood is computed using the MRF. We compare the MRF to existing\napproaches in a simulation study and a real satellite-data application.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 18:24:49 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 16:25:27 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Jurek", "Marcin", ""], ["Katzfuss", "Matthias", ""]]}, {"id": "1810.04350", "submitter": "Oliver Maclaren", "authors": "Oliver J. Maclaren, Ruanui Nicholson, Elvar K. Bjarkason, John P.\n  O'Sullivan and Michael J. O'Sullivan", "title": "Incorporating Posterior-Informed Approximation Errors into a\n  Hierarchical Framework to Facilitate Out-of-the-Box MCMC Sampling for\n  Geothermal Inverse Problems and Uncertainty Quantification", "comments": "48 pages (double spaced, draft mode, including references and\n  appendices). 8 figures (main text), 6 figures (appendix). Minor revision\n  submitted to WRR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider geothermal inverse problems and uncertainty quantification from a\nBayesian perspective. Our main goal is to make standard, `out-of-the-box'\nMarkov chain Monte Carlo (MCMC) sampling more feasible for complex simulation\nmodels by using suitable approximations. To do this, we first show how to pose\nboth the inverse and prediction problems in a hierarchical Bayesian framework.\nWe then show how to incorporate so-called posterior-informed model\napproximation error into this hierarchical framework, using a modified form of\nthe Bayesian approximation error (BAE) approach. This enables the use of a\n`coarse', approximate model in place of a finer, more expensive model, while\naccounting for the additional uncertainty and potential bias that this can\nintroduce. Our method requires only simple probability modelling, a relatively\nsmall number of fine model simulations, and only modifies the target posterior\n-- any standard MCMC sampling algorithm can be used to sample the new\nposterior. These corrections can also be used in methods that are not based on\nMCMC sampling. We show that our approach can achieve significant computational\nspeed-ups on two geothermal test problems. We also demonstrate the dangers of\nnaively using coarse, approximate models in place of finer models, without\naccounting for the induced approximation errors. The naive approach tends to\ngive overly confident and biased posteriors while incorporating BAE into our\nhierarchical framework corrects for this while maintaining computational\nefficiency and ease-of-use.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 03:33:04 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 00:26:28 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 23:49:31 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Maclaren", "Oliver J.", ""], ["Nicholson", "Ruanui", ""], ["Bjarkason", "Elvar K.", ""], ["O'Sullivan", "John P.", ""], ["O'Sullivan", "Michael J.", ""]]}, {"id": "1810.04443", "submitter": "Samuel Orso", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "On the Properties of Simulation-based Estimators in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Considering the increasing size of available data, the need for statistical\nmethods that control the finite sample bias is growing. This is mainly due to\nthe frequent settings where the number of variables is large and allowed to\nincrease with the sample size bringing standard inferential procedures to incur\nsignificant loss in terms of performance. Moreover, the complexity of\nstatistical models is also increasing thereby entailing important computational\nchallenges in constructing new estimators or in implementing classical ones. A\ntrade-off between numerical complexity and statistical properties is often\naccepted. However, numerically efficient estimators that are altogether\nunbiased, consistent and asymptotically normal in high dimensional problems\nwould generally be ideal. In this paper, we set a general framework from which\nsuch estimators can easily be derived for wide classes of models. This\nframework is based on the concepts that underlie simulation-based estimation\nmethods such as indirect inference. The approach allows various extensions\ncompared to previous results as it is adapted to possibly inconsistent\nestimators and is applicable to discrete models and/or models with a large\nnumber of parameters. We consider an algorithm, namely the Iterative Bootstrap\n(IB), to efficiently compute simulation-based estimators by showing its\nconvergence properties. Within this framework we also prove the properties of\nsimulation-based estimators, more specifically the unbiasedness, consistency\nand asymptotic normality when the number of parameters is allowed to increase\nwith the sample size. Therefore, an important implication of the proposed\napproach is that it allows to obtain unbiased estimators in finite samples.\nFinally, we study this approach when applied to three common models, namely\nlogistic regression, negative binomial regression and lasso regression.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 10:12:54 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 07:37:42 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1810.04448", "submitter": "Chuan-Long Xie", "authors": "Heng Peng, Chuanlong Xie, Jingxin Zhao", "title": "Fast Inference Procedures for Semivarying Coefficient Models via Local\n  Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semivarying coefficient models are widely used in the application of\nfinance, economics, medical science and many other areas. The functional\ncoefficients are commonly estimated by local smoothing methods, e.g. local\nlinear estimator. This implies that one should implement the estimation\nprocedure for hundreds of times to obtain an estimate of one function. So the\ncomputation cost is very severe. In this paper, we give an insight to the\ntrade-off between statistical efficiency and computation simplicity, and\nproposes a fast inference procedure for semivarying coefficient model. In our\nmethod, the coefficient functions are approximated by piecewise constants,\nwhich is a simple and rough approximation. This makes our estimators easy to\nimplement and avoid repeat estimation. In this work, we shall show that though\nthese estimators are not asymptotically optimal, they are efficient enough for\nbuilding further inference procedure. Furthermore, three tests are brought out\nto check whether certain coefficient is constant. Our results clearly show that\nwhen the room for improving the asymptotic efficiency is limited, a proper\ntrade-off between statistical efficiency and computation simplicity can be\ntaken into consideration to improve the performance of the inference procedure.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 10:33:51 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 15:25:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Peng", "Heng", ""], ["Xie", "Chuanlong", ""], ["Zhao", "Jingxin", ""]]}, {"id": "1810.04449", "submitter": "Christian P. Robert", "authors": "Changye Wu (U Paris Dauphine), Julien Stoehr (U Paris Dauphine), and\n  Christian P. Robert (U Paris Dauphine & U Warwick)", "title": "Faster Hamiltonian Monte Carlo by Learning Leapfrog Scale", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo samplers have become standard algorithms for MCMC\nimplementations, as opposed to more basic versions, but they still require some\namount of tuning and calibration. Exploiting the U-turn criterion of the NUTS\nalgorithm (Hoffman and Gelman, 2014), we propose a version of HMC that relies\non the distribution of the integration time of the associated leapfrog\nintegrator. Using in addition the primal-dual averaging method for tuning the\nstep size of the integrator, we achieve an essentially calibration free version\nof HMC. When compared with the original NUTS on several benchmarks, this\nalgorithm exhibits a significantly improved efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 10:34:48 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 09:46:38 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Wu", "Changye", "", "U Paris Dauphine"], ["Stoehr", "Julien", "", "U Paris Dauphine"], ["Robert", "Christian P.", "", "U Paris Dauphine & U Warwick"]]}, {"id": "1810.04744", "submitter": "Morteza Jalalvand", "authors": "Morteza Jalalvand (1), Mohammad A. Charsooghi (1) ((1) Institute for\n  Advanced Studies in Basic Sciences)", "title": "Generalized Ziggurat Algorithm for Unimodal and Unbounded Probability\n  Density Functions with Zest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a modified Ziggurat algorithm that could generate a random number\nfrom all unimodal and unbounded PDFs. For PDFs that have unbounded density\nand/or unbounded support we use a combination of nonlinear mapping function and\nrejection sampling to generate a random number from the peak and/or the tail\ndistribution. A family of mapping functions and their corresponding acceptance\nprobability functions are presented (along with the criteria for their use and\ntheir efficiency) that could be used to generate random numbers from infinite\ntails and unbounded densities.\n  The Zest library which is a C++ implementation of this algorithm is also\npresented. Zest can efficiently generate normal, exponential, cauchy, gamma,\nWeibull, log-normal, chi-squared, student's t and Fisher's f variates. The user\ncan also define their custom PDF as a class and supply it as a template\nargument to our library's class without modifying any part of the library.\nPerformance of Zest is compared against performance of random modules of (GCC's\nimplementation of) Standard Template Library (STL) and Boost. The presented\nresults show that Zest is faster than both in most cases, sometimes by a factor\nof more than 10.\n  We also present a C++ implementation of a uniform floating-point random\nnumber generator (RNG) which is capable of producing all representable\nfloating-point numbers in $[0,1)$ which will be used in the Ziggurat algorithm\nnear unbounded peaks and tails. The common method of dividing a random integer\nby the range of the RNG can not produce random floating-point numbers with\nfully random fraction bits and very small random numbers. The presented uniform\nfloating-point RNG is very efficient and in the case of producing double\nprecision floating-point numbers it's even faster than simply multiplying a\n64-bit integer by $2^{-64}$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 20:41:37 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Jalalvand", "Morteza", ""], ["Charsooghi", "Mohammad A.", ""]]}, {"id": "1810.04811", "submitter": "Ick Hoon Jin", "authors": "Jonghyun Yun, Minsuk Shin, Ick Hoon Jin, Faming Liang", "title": "Stochastic Approximation Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Hamilton Monte Carlo (HMC) has become widespread as one of the\nmore reliable approaches to efficient sample generation processes. However, HMC\nis difficult to sample in a multimodal posterior distribution because the HMC\nchain cannot cross energy barrier between modes due to the energy conservation\nproperty. In this paper, we propose a Stochastic Approximate Hamilton Monte\nCarlo (SAHMC) algorithm for generating samples from multimodal density under\nthe Hamiltonian Monte Carlo (HMC) framework. SAHMC can adaptively lower the\nenergy barrier to move the Hamiltonian trajectory more frequently and more\neasily between modes. Our simulation studies show that the potential for SAHMC\nto explore a multimodal target distribution more efficiently than HMC based\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 01:15:50 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 05:01:15 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Yun", "Jonghyun", ""], ["Shin", "Minsuk", ""], ["Jin", "Ick Hoon", ""], ["Liang", "Faming", ""]]}, {"id": "1810.05079", "submitter": "Markus Wallerberger", "authors": "Markus Wallerberger", "title": "Efficient estimation of autocorrelation spectra", "comments": "9 pages, 5 figures; submitted to Phys. Rev. E", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The performance of Markov chain Monte Carlo calculations is determined by\nboth ensemble variance of the Monte Carlo estimator and autocorrelation of the\nMarkov process. In order to study autocorrelation, binning analysis is commonly\nused, where the autocorrelation is estimated from results grouped into bins of\nlogarithmically increasing sizes.\n  In this paper, we show that binning analysis comes with a bias that can be\neliminated by combining bin sizes. We then show binning analysis can be\nperformed on-the-fly with linear overhead in time and logarithmic overhead in\nmemory with respect to the sample size. We then show that binning analysis\ncontains information not only about the integrated effect of autocorrelation,\nbut can be used to estimate the spectrum of autocorrelation lengths, yielding\nthe height of phase space barriers in the system. Finally, we revisit the Ising\nmodel and apply the proposed method to recover its autocorrelation spectra.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 15:22:46 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 12:00:04 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Wallerberger", "Markus", ""]]}, {"id": "1810.05189", "submitter": "E. G. Patrick Bos", "authors": "E. G. Patrick Bos, Francisco-Shu Kitaura, Rien van de Weygaert", "title": "Bayesian cosmic density field inference from redshift space dark matter\n  maps", "comments": "34 pages, 25 figures, 1 table. Submitted to MNRAS. Accompanying code\n  at https://github.com/egpbos/barcode", "journal-ref": null, "doi": "10.1093/mnras/stz1864", "report-no": null, "categories": "astro-ph.CO stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-consistent Bayesian formalism to sample the primordial\ndensity fields compatible with a set of dark matter density tracers after\ncosmic evolution observed in redshift space. Previous works on density\nreconstruction did not self-consistently consider redshift space distortions or\nincluded an additional iterative distortion correction step. We present here\nthe analytic solution of coherent flows within a Hamiltonian Monte Carlo\nposterior sampling of the primordial density field. We test our method within\nthe Zel'dovich approximation, presenting also an analytic solution including\ntidal fields and spherical collapse on small scales using augmented Lagrangian\nperturbation theory. Our resulting reconstructed fields are isotropic and their\npower spectra are unbiased compared to the true one defined by our mock\nobservations. Novel algorithmic implementations are introduced regarding the\nmass assignment kernels when defining the dark matter density field and\noptimization of the time step in the Hamiltonian equations of motions. Our\nalgorithm, dubbed barcode, promises to be specially suited for analysis of the\ndark matter cosmic web down to scales of a few Megaparsecs. This large scale\nstructure is implied by the observed spatial distribution of galaxy clusters\n--- such as obtained from X-ray, SZ or weak lensing surveys --- as well as that\nof the intergalactic medium sampled by the Lyman alpha forest or perhaps even\nby deep hydrogen intensity mapping. In these cases, virialized motions are\nnegligible, and the tracers cannot be modeled as point-like objects. It could\nbe used in all of these contexts as a baryon acoustic oscillation\nreconstruction algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 18:07:54 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 18:33:25 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 12:16:00 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Bos", "E. G. Patrick", ""], ["Kitaura", "Francisco-Shu", ""], ["van de Weygaert", "Rien", ""]]}, {"id": "1810.05204", "submitter": "Mehran Yarahmadi", "authors": "Mehran Yarahmadi and J. Robert Mahan", "title": "Verification of Two-Dimensional Monte Carlo Ray-Trace Methodology in\n  Radiation Heat Transfer Analysis", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.12632.14089", "report-no": null, "categories": "physics.comp-ph stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the frequent appearance in the radiation heat transfer literature of\narticles describing Monte Carlo ray-trace (MCRT) applications to\ntwo-dimensional enclosures, no formal verification may be found of the method\ncommonly used to determine the directional distribution of diffuse emission and\nreflection when estimating two-dimensional radiation distribution factors.\nConsidered are two methods for determining the direction cosines in this\nsituation. The results are shown to be in agreement with those obtained in the\nlimiting case of a three-dimensional enclosure as one of its dimensions is\nincreased.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 18:58:30 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 19:56:43 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 17:44:33 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Yarahmadi", "Mehran", ""], ["Mahan", "J. Robert", ""]]}, {"id": "1810.05374", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Daniel P. Simpson, Yuling Yao, Andrew Gelman", "title": "Limitations of \"Limitations of Bayesian leave-one-out cross-validation\n  for model selection\"", "comments": "To appear in Computational Brain & Behavior", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is an invited discussion of the article by Gronau and\nWagenmakers (2018) that can be found at\nhttps://dx.doi.org/10.1007/s42113-018-0011-7.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 06:45:46 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Vehtari", "Aki", ""], ["Simpson", "Daniel P.", ""], ["Yao", "Yuling", ""], ["Gelman", "Andrew", ""]]}, {"id": "1810.05620", "submitter": "Xiaoxian Tang", "authors": "Xiaoxian Tang, Timo De Wolff, Rukai Zhao", "title": "Computing Elimination Ideals and Discriminants of Likelihood Equations", "comments": "31 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC math.AG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a probabilistic algorithm for computing elimination ideals of\nlikelihood equations, which is for larger models by far more efficient than\ndirectly computing Groebner bases or the interpolation method proposed in the\nfirst author's previous work. The efficiency is improved by a theoretical\nresult showing that the sum of data variables appears in most coefficients of\nthe generator polynomial of elimination ideal. Furthermore, applying the known\nstructures of Newton polytopes of discriminants, we can also efficiently deduce\ndiscriminants of the elimination ideals. For instance, the discriminants of 3\nby 3 matrix model and one Jukes-Cantor model in phylogenetics (with sizes over\n30 GB and 8 GB text files, respectively) can be computed by our methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 17:30:01 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Tang", "Xiaoxian", ""], ["De Wolff", "Timo", ""], ["Zhao", "Rukai", ""]]}, {"id": "1810.05759", "submitter": "Yuan Wang", "authors": "Yuan Wang and Bei Wang", "title": "Topological Inference of Manifolds with Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of data points sampled from some underlying space, there are two\nimportant challenges in geometric and topological data analysis when dealing\nwith sampled data: reconstruction -- how to assemble discrete samples into\nglobal structures, and inference -- how to extract geometric and topological\ninformation from data that are high-dimensional, incomplete and noisy. Niyogi\net al. (2008) have shown that by constructing an offset of the samples using a\nsuitable offset parameter could provide reconstructions that preserve homotopy\ntypes therefore homology for densely sampled smooth submanifolds of Euclidean\nspace without boundary. Chazal et al. (2009) and Attali et al. (2013) have\nintroduced a parameterized set of sampling conditions that extend the results\nof Niyogi et al. to a large class of compact subsets of Euclidean space. Our\nwork tackles data problems that fill a gap between the work of Niyogi et al.\nand Chazal et al. In particular, we give a probabilistic notion of sampling\nconditions for manifolds with boundary that could not be handled by existing\ntheories. We also give stronger results that relate topological equivalence\nbetween the offset and the manifold as a deformation retract.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 23:25:33 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Wang", "Yuan", ""], ["Wang", "Bei", ""]]}, {"id": "1810.05845", "submitter": "Nicholas Tawn", "authors": "Nicholas G. Tawn and Gareth O. Roberts", "title": "Optimal Temperature Spacing for Regionally Weight-preserving Tempering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel tempering is popular method for allowing MCMC algorithms to properly\nexplore a $d$-dimensional multimodal target density. One problem with\ntraditional power-based parallel tempering for multimodal targets is that the\nproportion of probability mass associated to the modes can change for different\ninverse-temperature values, sometimes dramatically so. Complementary work by\nthe authors proposes a novel solution involving auxiliary targets that preserve\nregional weight upon powering up the density. This paper attempts to address\nthe question of how to choose the temperature spacings in an optimal way when\nusing this type of weight-preserving approach.The problem is analysed in a\ntractable setting for computation of the expected squared jumping distance\nwhich can then be optimised with regards to a tuning parameter. The conclusion\nis that for an appropriately constructed regionally weight-preserved tempering\nalgorithm targeting a $d$-dimensional target distribution, the consecutive\ntemperature spacings should behave as $\\mathcal{O}\\left(d^{-1/2}\\right)$ and\nthis induces an optimal acceptance rate for temperature swap moves that lies in\nthe interval $[0,0.234]$.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 11:50:28 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Tawn", "Nicholas G.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1810.06433", "submitter": "Geoff Nicholls", "authors": "Jeong Eun Lee, Geoff K. Nicholls, Robin J. Ryder", "title": "Calibration procedures for approximate Bayesian credible sets", "comments": "28 pages, 6 Figures, 1 Table, 4 Algorithm boxes. Revision improves\n  clarity of presentation and adds relevant citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and apply two calibration procedures for checking the coverage of\napproximate Bayesian credible sets including intervals estimated using Monte\nCarlo methods. The user has an ideal prior and likelihood, but generates a\ncredible set for an approximate posterior which is not proportional to the\nproduct of ideal likelihood and prior. We estimate the realised posterior\ncoverage achieved by the approximate credible set. This is the coverage of the\nunknown ``true'' parameter if the data are a realisation of the user's ideal\nobservation model conditioned on the parameter, and the parameter is a draw\nfrom the user's ideal prior. In one approach we estimate the posterior coverage\nat the data by making a semi-parametric logistic regression of binary coverage\noutcomes on simulated data against summary statistics evaluated on simulated\ndata. In another we use Importance Sampling from the approximate posterior,\nwindowing simulated data to fall close to the observed data. We illustrate our\nmethods on four examples.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:56:17 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 14:16:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Lee", "Jeong Eun", ""], ["Nicholls", "Geoff K.", ""], ["Ryder", "Robin J.", ""]]}, {"id": "1810.06940", "submitter": "Philipp Otto", "authors": "Philipp Otto and Rick Steinert", "title": "Estimation of the Spatial Weighting Matrix for Spatiotemporal Data under\n  the Presence of Structural Breaks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a two-step lasso estimation approach to estimate\nthe full spatial weights matrix of spatiotemporal autoregressive models. In\naddition, we allow for an unknown number of structural breaks in the local\nmeans of each spatial locations. The proposed approach jointly estimates the\nspatial dependence, all structural breaks, and the local mean levels. In\naddition, it is easy to compute the suggested estimators, because of a convex\nobjective function resulting from a slight simplification. Via simulation\nstudies, we show the finite-sample performance of the estimators and provide a\npractical guidance, when the approach could be applied. Eventually, the\ninvented method is illustrated by an empirical example of regional monthly\nreal-estate prices in Berlin from 1995 to 2014. The spatial units are defined\nby the respective ZIP codes. In particular, we can estimate local mean levels\nand quantify the deviation of the observed prices from these levels due to\nspatial spill over effects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 11:53:51 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Otto", "Philipp", ""], ["Steinert", "Rick", ""]]}, {"id": "1810.06966", "submitter": "Victor Minden", "authors": "Victor Minden, Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "Biologically Plausible Online Principal Component Analysis Without\n  Recurrent Neural Dynamics", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.DS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks that learn to perform Principal Component Analysis\n(PCA) and related tasks using strictly local learning rules have been\npreviously derived based on the principle of similarity matching: similar pairs\nof inputs should map to similar pairs of outputs. However, the operation of\nthese networks (and of similar networks) requires a fixed-point iteration to\ndetermine the output corresponding to a given input, which means that dynamics\nmust operate on a faster time scale than the variation of the input. Further,\nduring these fast dynamics such networks typically \"disable\" learning, updating\nsynaptic weights only once the fixed-point iteration has been resolved. Here,\nwe derive a network for PCA-based dimensionality reduction that avoids this\nfast fixed-point iteration. The key novelty of our approach is a modification\nof the similarity matching objective to encourage near-diagonality of a\nsynaptic weight matrix. We then approximately invert this matrix using a Taylor\nseries approximation, replacing the previous fast iterations. In the offline\nsetting, our algorithm corresponds to a dynamical system, the stability of\nwhich we rigorously analyze. In the online setting (i.e., with stochastic\ngradients), we map our algorithm to a familiar neural network architecture and\ngive numerical results showing that our method converges at a competitive rate.\nThe computational complexity per iteration of our online algorithm is linear in\nthe total degrees of freedom, which is in some sense optimal.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:06:38 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 02:28:03 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Minden", "Victor", ""], ["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1810.06999", "submitter": "Sai Praneeth Karimireddy", "authors": "Sai Praneeth Karimireddy, Anastasia Koloskova, Sebastian U. Stich,\n  Martin Jaggi", "title": "Efficient Greedy Coordinate Descent for Composite Problems", "comments": "44 pages, 17 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinate descent with random coordinate selection is the current state of\nthe art for many large scale optimization problems. However, greedy selection\nof the steepest coordinate on smooth problems can yield convergence rates\nindependent of the dimension $n$, and requiring upto $n$ times fewer\niterations.\n  In this paper, we consider greedy updates that are based on subgradients for\na class of non-smooth composite problems, which includes $L1$-regularized\nproblems, SVMs and related applications. For these problems we provide (i) the\nfirst linear rates of convergence independent of $n$, and show that our greedy\nupdate rule provides speedups similar to those obtained in the smooth case.\nThis was previously conjectured to be true for a stronger greedy coordinate\nselection strategy.\n  Furthermore, we show that (ii) our new selection rule can be mapped to\ninstances of maximum inner product search, allowing to leverage standard\nnearest neighbor algorithms to speed up the implementation. We demonstrate the\nvalidity of the approach through extensive numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:54:59 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Karimireddy", "Sai Praneeth", ""], ["Koloskova", "Anastasia", ""], ["Stich", "Sebastian U.", ""], ["Jaggi", "Martin", ""]]}, {"id": "1810.07128", "submitter": "Sen Na", "authors": "Sen Na, Zhuoran Yang, Zhaoran Wang, Mladen Kolar", "title": "High-dimensional Varying Index Coefficient Models via Stein's Identity", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameter estimation problem for a varying index coefficient\nmodel in high dimensions. Unlike the most existing works that iteratively\nestimate the parameters and link functions, based on the generalized Stein's\nidentity, we propose computationally efficient estimators for the\nhigh-dimensional parameters without estimating the link functions. We consider\ntwo different setups where we either estimate each sparse parameter vector\nindividually or estimate the parameters simultaneously as a sparse or low-rank\nmatrix. For all these cases, our estimators are shown to achieve optimal\nstatistical rates of convergence (up to logarithmic terms in the low-rank\nsetting). Moreover, throughout our analysis, we only require the covariate to\nsatisfy certain moment conditions, which is significantly weaker than the\nGaussian or elliptically symmetric assumptions that are commonly made in the\nexisting literature. Finally, we conduct extensive numerical experiments to\ncorroborate the theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 16:51:28 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 03:00:26 GMT"}, {"version": "v3", "created": "Sun, 21 Oct 2018 21:56:15 GMT"}, {"version": "v4", "created": "Fri, 25 Oct 2019 18:33:53 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Na", "Sen", ""], ["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""], ["Kolar", "Mladen", ""]]}, {"id": "1810.08316", "submitter": "Anru R. Zhang", "authors": "Anru R. Zhang and T. Tony Cai and Yihong Wu", "title": "Heteroskedastic PCA: Algorithm, Optimality, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A general framework for principal component analysis (PCA) in the presence of\nheteroskedastic noise is introduced. We propose an algorithm called HeteroPCA,\nwhich involves iteratively imputing the diagonal entries of the sample\ncovariance matrix to remove estimation bias due to heteroskedasticity. This\nprocedure is computationally efficient and provably optimal under the\ngeneralized spiked covariance model. A key technical step is a deterministic\nrobust perturbation analysis on singular subspaces, which can be of independent\ninterest. The effectiveness of the proposed algorithm is demonstrated in a\nsuite of problems in high-dimensional statistics, including singular value\ndecomposition (SVD) under heteroskedastic noise, Poisson PCA, and SVD for\nheteroskedastic and incomplete data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 00:22:25 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 01:29:04 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 14:00:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Anru R.", ""], ["Cai", "T. Tony", ""], ["Wu", "Yihong", ""]]}, {"id": "1810.08727", "submitter": "Paul Grigas", "authors": "Robert M. Freund, Paul Grigas, Rahul Mazumder", "title": "Condition Number Analysis of Logistic Regression, and its Implications\n  for Standard First-Order Solution Methods", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is one of the most popular methods in binary\nclassification, wherein estimation of model parameters is carried out by\nsolving the maximum likelihood (ML) optimization problem, and the ML estimator\nis defined to be the optimal solution of this problem. It is well known that\nthe ML estimator exists when the data is non-separable, but fails to exist when\nthe data is separable. First-order methods are the algorithms of choice for\nsolving large-scale instances of the logistic regression problem. In this\npaper, we introduce a pair of condition numbers that measure the degree of\nnon-separability or separability of a given dataset in the setting of binary\nclassification, and we study how these condition numbers relate to and inform\nthe properties and the convergence guarantees of first-order methods. When the\ntraining data is non-separable, we show that the degree of non-separability\nnaturally enters the analysis and informs the properties and convergence\nguarantees of two standard first-order methods: steepest descent (for any given\nnorm) and stochastic gradient descent. Expanding on the work of Bach, we also\nshow how the degree of non-separability enters into the analysis of linear\nconvergence of steepest descent (without needing strong convexity), as well as\nthe adaptive convergence of stochastic gradient descent. When the training data\nis separable, first-order methods rather curiously have good empirical success,\nwhich is not well understood in theory. In the case of separable data, we\ndemonstrate how the degree of separability enters into the analysis of $\\ell_2$\nsteepest descent and stochastic gradient descent for delivering\napproximate-maximum-margin solutions with associated computational guarantees\nas well. This suggests that first-order methods can lead to statistically\nmeaningful solutions in the separable case, even though the ML solution does\nnot exist.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 01:37:20 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Freund", "Robert M.", ""], ["Grigas", "Paul", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1810.08791", "submitter": "JInglai Li", "authors": "Linjie Wen, Jiangqi Wu, Linjun Lu and Jinglai Li", "title": "A defensive marginal particle filtering method for data assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filtering (PF) is an often used method to estimate the states of\ndynamical systems. A major limitation of the standard PF method is that the\ndimensionality of the state space increases as the time proceeds and eventually\nmay cause degeneracy of the algorithm. A possible approach to alleviate the\ndegeneracy issue is to compute the marginal posterior distribution at each time\nstep, which leads to the so-called marginal PF method. A key issue in the\nmarginal PF method is to construct a good sampling distribution in the marginal\nspace. When the posterior distribution is close to Gaussian, the Ensemble\nKalman filter (EnKF) method can usually provide a good sampling distribution;\nhowever the EnKF approximation may fail completely when the posterior is\nstrongly non-Gaussian. In this work we propose a defensive marginal PF (DMPF)\nalgorithm which constructs a sampling distribution in the marginal space by\ncombining the standard PF and the EnKF approximation using a multiple\nimportance sampling (MIS) scheme. An important feature of the proposed\nalgorithm is that it can automatically adjust the relative weight of the PF and\nthe EnKF components in the MIS scheme in each step, according to how\nnon-Gaussian the posterior is. With numerical examples we demonstrate that the\nproposed method can perform well regardless of whether the posteriors can be\nwell approximated by Gaussian.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 11:49:52 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 16:56:14 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 13:35:28 GMT"}, {"version": "v4", "created": "Thu, 29 Aug 2019 10:20:36 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Wen", "Linjie", ""], ["Wu", "Jiangqi", ""], ["Lu", "Linjun", ""], ["Li", "Jinglai", ""]]}, {"id": "1810.08826", "submitter": "Qian Qin", "authors": "Qian Qin and James P. Hobert", "title": "Wasserstein-based methods for convergence complexity analysis of MCMC\n  with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last 25 years, techniques based on drift and minorization (d&m) have\nbeen mainstays in the convergence analysis of MCMC algorithms. However, results\npresented herein suggest that d&m may be less useful in the emerging area of\nconvergence complexity analysis, which is the study of how the convergence\nbehavior of Monte Carlo Markov chains scale with sample size, $n$, and/or\nnumber of covariates, $p$. The problem appears to be that minorization can\nbecome a serious liability as dimension increases. Alternative methods for\nconstructing convergence rate bounds (with respect to total variation distance)\nthat do not require minorization are investigated. Based on Wasserstein\ndistances and random mappings, these methods can produce bounds that are\nsubstantially more robust to increasing dimension than those based on d&m. The\nWasserstein-based bounds are used to develop strong convergence complexity\nresults for MCMC algorithms used in Bayesian probit regression and random\neffects models in the challenging asymptotic regime where $n$ and $p$ are both\nlarge.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 16:56:28 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 02:01:33 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 05:52:42 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Qin", "Qian", ""], ["Hobert", "James P.", ""]]}, {"id": "1810.08829", "submitter": "Yin Xian", "authors": "Yin Xian, Hanlin Gu, Wei Wang, Xuhui Huang, Yuan Yao, Yang Wang,\n  Jian-Feng Cai", "title": "Data-Driven Tight Frame for Cryo-EM Image Denoising and Conformational\n  Classification", "comments": "2018 IEEE Global Signal and Information Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cryo-electron microscope (cryo-EM) is increasingly popular these years.\nIt helps to uncover the biological structures and functions of macromolecules.\nIn this paper, we address image denoising problem in cryo-EM. Denoising the\ncryo-EM images can help to distinguish different molecular conformations and\nimprove three dimensional reconstruction resolution. We introduce the use of\ndata-driven tight frame (DDTF) algorithm for cryo-EM image denoising. The DDTF\nalgorithm is closely related to the dictionary learning. The advantage of DDTF\nalgorithm is that it is computationally efficient, and can well identify the\ntexture and shape of images without using large data samples. Experimental\nresults on cryo-EM image denoising and conformational classification\ndemonstrate the power of DDTF algorithm for cryo-EM image denoising and\nclassification.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 17:07:40 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 08:56:58 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Xian", "Yin", ""], ["Gu", "Hanlin", ""], ["Wang", "Wei", ""], ["Huang", "Xuhui", ""], ["Yao", "Yuan", ""], ["Wang", "Yang", ""], ["Cai", "Jian-Feng", ""]]}, {"id": "1810.08918", "submitter": "Antonio Punzo", "authors": "Antonio Punzo and Cristina Tortora", "title": "Multiple Scaled Contaminated Normal Distribution and Its Application in\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate contaminated normal (MCN) distribution represents a simple\nheavy-tailed generalization of the multivariate normal (MN) distribution to\nmodel elliptical contoured scatters in the presence of mild outliers, referred\nto as \"bad\" points. The MCN can also automatically detect bad points. The price\nof these advantages is two additional parameters, both with specific and useful\ninterpretations: proportion of good observations and degree of contamination.\nHowever, points may be bad in some dimensions but good in others. The use of an\noverall proportion of good observations and of an overall degree of\ncontamination is limiting. To overcome this limitation, we propose a multiple\nscaled contaminated normal (MSCN) distribution with a proportion of good\nobservations and a degree of contamination for each dimension. Once the model\nis fitted, each observation has a posterior probability of being good with\nrespect to each dimension. Thanks to this probability, we have a method for\nsimultaneous directional robust estimation of the parameters of the MN\ndistribution based on down-weighting and for the automatic directional\ndetection of bad points by means of maximum a posteriori probabilities. The\nterm \"directional\" is added to specify that the method works separately for\neach dimension. Mixtures of MSCN distributions are also proposed as an\napplication of the proposed model for robust clustering. An extension of the EM\nalgorithm is used for parameter estimation based on the maximum likelihood\napproach. Real and simulated data are used to show the usefulness of our\nmixture with respect to well-established mixtures of symmetric distributions\nwith heavy tails.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 09:37:44 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Punzo", "Antonio", ""], ["Tortora", "Cristina", ""]]}, {"id": "1810.09098", "submitter": "Christopher Aicher", "authors": "Christopher Aicher, Yi-An Ma, Nicholas J. Foti, and Emily B. Fox", "title": "Stochastic Gradient MCMC for State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State space models (SSMs) are a flexible approach to modeling complex time\nseries. However, inference in SSMs is often computationally prohibitive for\nlong time series. Stochastic gradient MCMC (SGMCMC) is a popular method for\nscalable Bayesian inference for large independent data. Unfortunately when\napplied to dependent data, such as in SSMs, SGMCMC's stochastic gradient\nestimates are biased as they break crucial temporal dependencies. To alleviate\nthis, we propose stochastic gradient estimators that control this bias by\nperforming additional computation in a `buffer' to reduce breaking\ndependencies. Furthermore, we derive error bounds for this bias and show a\ngeometric decay under mild conditions. Using these estimators, we develop novel\nSGMCMC samplers for discrete, continuous and mixed-type SSMs with analytic\nmessage passing. Our experiments on real and synthetic data demonstrate the\neffectiveness of our SGMCMC algorithms compared to batch MCMC, allowing us to\nscale inference to long time series with millions of time points.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 05:53:22 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 18:09:39 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Aicher", "Christopher", ""], ["Ma", "Yi-An", ""], ["Foti", "Nicholas J.", ""], ["Fox", "Emily B.", ""]]}, {"id": "1810.09116", "submitter": "Zicheng Liu", "authors": "Z.Liu, D. Lesselier, B. Sudret and J. Wiart", "title": "Surrogate modeling based on resampled polynomial chaos expansions", "comments": null, "journal-ref": null, "doi": "10.1016/j.ress.2020.107008", "report-no": "RSUQ-2018-006", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In surrogate modeling, polynomial chaos expansion (PCE) is popularly utilized\nto represent the random model responses, which are computationally expensive\nand usually obtained by deterministic numerical modeling approaches including\nfinite element and finite-difference time-domain methods. Recently, efforts\nhave been made on improving the prediction performance of the PCE-based model\nand building efficiency by only selecting the influential basis polynomials\n(e.g., via the approach of least angle regression). This paper proposes an\napproach, named as resampled PCE (rPCE), to further optimize the selection by\nmaking use of the knowledge that the true model is fixed despite the\nstatistical uncertainty inherent to sampling in the training. By simulating\ndata variation via resampling ($k$-fold division utilized here) and collecting\nthe selected polynomials with respect to all resamples, polynomials are ranked\nmainly according to the selection frequency. The resampling scheme (the value\nof $k$ here) matters much and various configurations are considered and\ncompared. The proposed resampled PCE is implemented with two popular selection\ntechniques, namely least angle regression and orthogonal matching pursuit, and\na combination thereof. The performance of the proposed algorithm is\ndemonstrated on two analytical examples, a benchmark problem in structural\nmechanics, as well as a realistic case study in computational dosimetry.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 07:14:09 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 17:05:09 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Liu", "Z.", ""], ["Lesselier", "D.", ""], ["Sudret", "B.", ""], ["Wiart", "J.", ""]]}, {"id": "1810.09291", "submitter": "Bin Liu", "authors": "Bin Liu", "title": "Robust Particle Filtering via Bayesian Nonparametric Outlier Modeling", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the online estimation of a nonlinear dynamic\nsystem from a series of noisy measurements. The focus is on cases wherein\noutliers are present in-between normal noises. We assume that the outliers\nfollow an unknown generating mechanism which deviates from that of normal\nnoises, and then model the outliers using a Bayesian nonparametric model called\nDirichlet process mixture (DPM). A sequential particle-based algorithm is\nderived for posterior inference for the outlier model as well as the state of\nthe system to be estimated. The resulting algorithm is termed DPM based robust\nPF (DPM-RPF). The nonparametric feature makes this algorithm allow the data to\n\"speak for itself\" to determine the complexity and structure of the outlier\nmodel. Simulation results show that it performs remarkably better than two\nstate-of-the-art methods especially when outliers appear frequently along time.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 13:53:37 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 09:17:48 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 03:40:29 GMT"}, {"version": "v4", "created": "Sun, 18 Nov 2018 09:34:58 GMT"}, {"version": "v5", "created": "Tue, 20 Nov 2018 02:34:01 GMT"}, {"version": "v6", "created": "Tue, 12 Feb 2019 12:41:12 GMT"}, {"version": "v7", "created": "Sat, 11 May 2019 07:23:47 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Liu", "Bin", ""]]}, {"id": "1810.09624", "submitter": "Earo Wang", "authors": "Earo Wang, Dianne Cook, Rob J Hyndman", "title": "Calendar-based graphics for visualizing people's daily schedules", "comments": "31 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calendars are broadly used in society to display temporal information, and\nevents. This paper describes a new R package with functionality to organize and\ndisplay temporal data, collected on sub-daily resolution, into a calendar\nlayout. The function `frame_calendar` uses linear algebra on the date variable\nto restructure data into a format lending itself to calendar layouts. The user\ncan apply the grammar of graphics to create plots inside each calendar cell,\nand thus the displays synchronize neatly with ggplot2 graphics. The motivating\napplication is studying pedestrian behavior in Melbourne, Australia, based on\ncounts which are captured at hourly intervals by sensors scattered around the\ncity. Faceting by the usual features such as day and month, was insufficient to\nexamine the behavior. Making displays on a monthly calendar format helps to\nunderstand pedestrian patterns relative to events such as work days, weekends,\nholidays, and special events. The layout algorithm has several format options\nand variations. It is implemented in the R package sugrrants.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 01:35:25 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Wang", "Earo", ""], ["Cook", "Dianne", ""], ["Hyndman", "Rob J", ""]]}, {"id": "1810.09753", "submitter": "Taras Lazariv", "authors": "Taras Lazariv and Christoph Lehmann", "title": "Goodness-of-Fit Tests for Large Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, data analysis in the world of Big Data is connected typically to\ndata mining, descriptive or exploratory statistics, e.~g.\\ cluster analysis,\nclassification or regression analysis. Aside these techniques there is a huge\narea of methods from inferential statistics that are rarely considered in\nconnection with Big Data. Nevertheless, inferential methods are also of use for\nBig Data analysis, especially for quantifying uncertainty. The article at hand\nwill provide some insights to methodological and technical issues referring\ninferential methods in the Big Data area in order to bring together Big Data\nand inferential statistics, as it comes along with its difficulties. We present\nan approach that allows testing goodness-of-fit without model assumptions and\nrelying on the empirical distribution. Especially, the method is able to\nutilize information from large datasets. Thereby, the approach is based on a\nclear theoretical background. We concentrate on the widely-used\nKolmogorov-Smirnov test that is applied for testing goodness-of-fit in\nstatistics. Our approach can be parallelized easily, which makes it applicable\nto distributed datasets particularly on a compute cluster. By this\ncontribution, we turn to an audience that is interested in the technical and\nmethodological backgrounds while implementing especially inferential\nstatistical methods with Big Data tools as e. g. Spark.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 10:03:38 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Lazariv", "Taras", ""], ["Lehmann", "Christoph", ""]]}, {"id": "1810.09810", "submitter": "Simon Taylor", "authors": "Simon A. C. Taylor, Timothy Park and Idris A. Eckley", "title": "Multivariate Locally Stationary Wavelet Process Analysis with the mvLSW\n  R Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the R package mvLSW. The package contains a suite of\ntools for the analysis of multivariate locally stationary wavelet (LSW) time\nseries. Key elements include: (i) the simulation of multivariate LSW time\nseries for a given multivariate evolutionary wavelet spectrum (EWS); (ii)\nestimation of the time-dependent multivariate EWS for a given time series;\n(iii) estimation of the time-dependent coherence and partial coherence between\ntime series channels; and, (iv) estimation of approximate confidence intervals\nfor multivariate EWS estimates. A demonstration of the package is presented via\nboth a simulated example and a case study with EuStockMarkets from the datasets\npackage. This paper has been accepted by the Journal of Statistical Software.\nPresented code extracts demonstrating the mvLSW package is performed under\nversion 1.2.1.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 12:22:37 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Taylor", "Simon A. C.", ""], ["Park", "Timothy", ""], ["Eckley", "Idris A.", ""]]}, {"id": "1810.09899", "submitter": "Traiko Dinev", "authors": "Traiko Dinev and Michael U. Gutmann", "title": "Dynamic Likelihood-free Inference via Ratio Estimation (DIRE)", "comments": "For a demo, see https://traiko.com/pages/research/lfire/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric statistical models that are implicitly defined in terms of a\nstochastic data generating process are used in a wide range of scientific\ndisciplines because they enable accurate modeling. However, learning the\nparameters from observed data is generally very difficult because their\nlikelihood function is typically intractable. Likelihood-free Bayesian\ninference methods have been proposed which include the frameworks of\napproximate Bayesian computation (ABC), synthetic likelihood, and its recent\ngeneralization that performs likelihood-free inference by ratio estimation\n(LFIRE). A major difficulty in all these methods is choosing summary statistics\nthat reduce the dimensionality of the data to facilitate inference. While\nseveral methods for choosing summary statistics have been proposed for ABC, the\nliterature for synthetic likelihood and LFIRE is very thin to date. We here\naddress this gap in the literature, focusing on the important special case of\ntime-series models. We show that convolutional neural networks trained to\npredict the input parameters from the data provide suitable summary statistics\nfor LFIRE. On a wide range of time-series models, a single neural network\narchitecture produced equally or more accurate posteriors than alternative\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 15:02:47 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Dinev", "Traiko", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1810.09912", "submitter": "Steven Kleinegesse", "authors": "Steven Kleinegesse and Michael Gutmann", "title": "Efficient Bayesian Experimental Design for Implicit Models", "comments": "Added references and fixed typos. Results and figures remain\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian experimental design involves the optimal allocation of resources in\nan experiment, with the aim of optimising cost and performance. For implicit\nmodels, where the likelihood is intractable but sampling from the model is\npossible, this task is particularly difficult and therefore largely unexplored.\nThis is mainly due to technical difficulties associated with approximating\nposterior distributions and utility functions. We devise a novel experimental\ndesign framework for implicit models that improves upon previous work in two\nways. First, we use the mutual information between parameters and data as the\nutility function, which has previously not been feasible. We achieve this by\nutilising Likelihood-Free Inference by Ratio Estimation (LFIRE) to approximate\nposterior distributions, instead of the traditional approximate Bayesian\ncomputation or synthetic likelihood methods. Secondly, we use Bayesian\noptimisation in order to solve the optimal design problem, as opposed to the\ntypically used grid search or sampling-based methods. We find that this\nincreases efficiency and allows us to consider higher design dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 15:24:29 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 13:48:19 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Kleinegesse", "Steven", ""], ["Gutmann", "Michael", ""]]}, {"id": "1810.09920", "submitter": "Alexander Lin", "authors": "Alexander Lin, Yingzhuo Zhang, Jeremy Heng, Stephen A. Allsop, Kay M.\n  Tye, Pierre E. Jacob, and Demba Ba", "title": "Clustering Time Series with Nonlinear Dynamics: A Bayesian\n  Non-Parametric and Particle-Based Approach", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS 2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a general statistical framework for clustering multiple time\nseries that exhibit nonlinear dynamics into an a-priori-unknown number of\nsub-groups. Our motivation comes from neuroscience, where an important problem\nis to identify, within a large assembly of neurons, subsets that respond\nsimilarly to a stimulus or contingency. Upon modeling the multiple time series\nas the output of a Dirichlet process mixture of nonlinear state-space models,\nwe derive a Metropolis-within-Gibbs algorithm for full Bayesian inference that\nalternates between sampling cluster assignments and sampling parameter values\nthat form the basis of the clustering. The Metropolis step employs recent\ninnovations in particle-based methods. We apply the framework to clustering\ntime series acquired from the prefrontal cortex of mice in an experiment\ndesigned to characterize the neural underpinnings of fear.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 15:40:25 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 18:44:19 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 21:28:11 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 18:40:29 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Lin", "Alexander", ""], ["Zhang", "Yingzhuo", ""], ["Heng", "Jeremy", ""], ["Allsop", "Stephen A.", ""], ["Tye", "Kay M.", ""], ["Jacob", "Pierre E.", ""], ["Ba", "Demba", ""]]}, {"id": "1810.10883", "submitter": "Tim van Erven", "authors": "Tim van Erven and Botond Szabo", "title": "Fast Exact Bayesian Inference for Sparse Signals in the Normal Sequence\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider exact algorithms for Bayesian inference with model selection\npriors (including spike-and-slab priors) in the sparse normal sequence model.\nBecause the best existing exact algorithm becomes numerically unstable for\nsample sizes over n=500, there has been much attention for alternative\napproaches like approximate algorithms (Gibbs sampling, variational Bayes,\netc.), shrinkage priors (e.g. the Horseshoe prior and the Spike-and-Slab LASSO)\nor empirical Bayesian methods. However, by introducing algorithmic ideas from\nonline sequential prediction, we show that exact calculations are feasible for\nmuch larger sample sizes: for general model selection priors we reach n=25000,\nand for certain spike-and-slab priors we can easily reach n=100000. We further\nprove a de Finetti-like result for finite sample sizes that characterizes\nexactly which model selection priors can be expressed as spike-and-slab priors.\nThe computational speed and numerical accuracy of the proposed methods are\ndemonstrated in experiments on simulated data, on a differential gene\nexpression data set, and to compare the effect of multiple hyper-parameter\nsettings in the beta-binomial prior. In our experimental evaluation we compute\nguaranteed bounds on the numerical accuracy of all new algorithms, which shows\nthat the proposed methods are numerically reliable whereas an alternative based\non long division is not.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 13:58:27 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 14:24:30 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["van Erven", "Tim", ""], ["Szabo", "Botond", ""]]}, {"id": "1810.10985", "submitter": "Philip Stark", "authors": "Philip B. Stark and Kellie Ottoboni", "title": "Random Sampling: Practice Makes Imperfect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The pseudo-random number generators (PRNGs), sampling algorithms, and\nalgorithms for generating random integers in some common statistical packages\nand programming languages are unnecessarily inaccurate, by an amount that may\nmatter for statistical inference. Most use PRNGs with state spaces that are too\nsmall for contemporary sampling problems and methods such as the bootstrap and\npermutation tests. The random sampling algorithms in many packages rely on the\nfalse assumption that PRNGs produce IID $U[0, 1)$ outputs. The discreteness of\nPRNG outputs and the limited state space of common PRNGs cause those algorithms\nto perform poorly in practice. Statistics packages and scientific programming\nlanguages should use cryptographically secure PRNGs by default (not for their\nsecurity properties, but for their statistical ones), and offer weaker PRNGs\nonly as an option. Software should not use methods that assume PRNG outputs are\nIID $U[0,1)$ random variables, such as generating a random sample by permuting\nthe population and taking the first $k$ items or generating random integers by\nmultiplying a pseudo-random binary fraction or float by a constant and rounding\nthe result. More accurate methods are available.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 17:18:47 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 16:27:21 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Stark", "Philip B.", ""], ["Ottoboni", "Kellie", ""]]}, {"id": "1810.11130", "submitter": "Paulo Orenstein", "authors": "Paulo Orenstein", "title": "Robust Importance Sampling with Adaptive Winsorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is a widely used technique to estimate properties of a\ndistribution. This paper investigates trading-off some bias for variance by\nadaptively winsorizing the importance sampling estimator. The novel winsorizing\nprocedure, based on the Balancing Principle (or Lepskii's Method), chooses a\nthreshold level among a pre-defined set by roughly balancing the bias and\nvariance of the estimator when winsorized at different levels. As a\nconsequence, it provides a principled way to perform winsorization with\nfinite-sample optimality guarantees under minimal assumptions. In various\nexamples, the proposed estimator is shown to have smaller mean squared error\nand mean absolute deviation than leading alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 22:45:45 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 20:30:19 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Orenstein", "Paulo", ""]]}, {"id": "1810.11163", "submitter": "Yu Du", "authors": "Yu Du and Ravi Varadhan", "title": "SQUAREM: An R Package for Off-the-Shelf Acceleration of EM, MM and Other\n  EM-like Monotone Algorithms", "comments": "Accepted by Journal of Statistical Software and waiting to be\n  published", "journal-ref": null, "doi": "10.18637/jss.v092.i07", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss R package SQUAREM for accelerating iterative algorithms which\nexhibit slow, monotone convergence. These include the well-known\nexpectation-maximization algorithm, majorize-minimize (MM), and other EM-like\nalgorithms such as expectation conditional maximization, and generalized EM\nalgorithms. We demonstrate the simplicity, generality, and power of SQUAREM\nthrough a wide array of applications of EM/MM problems, including binary\nPoisson mixture, factor analysis, interval censoring, genetics admixture, and\nlogistic regression maximum likelihood estimation (an MM problem). We show that\nSQUAREM is easy to apply, and can accelerate any fixed-point, smooth,\ncontraction mapping with linear convergence rate. Squared iterative scheme\n(Squarem) algorithm provides significant speed-up of EM-like algorithms. The\nmargin of the advantage for Squarem is especially huge for high-dimensional\nproblems or when EM step is relatively time-consuming to evaluate. Squarem can\nbe used off-the-shelf since there is no need for the user to tweak any control\nparameters to optimize performance. Given its remarkable ease of use, Squarem\nmay be considered as a default accelerator for slowly converging EM-like\nalgorithms. All the comparisons of CPU computing time in the paper are made on\na quad-core 2.3 GHz Intel Core i7 Mac computer. R Package SQUAREM can be\ndownloaded at https://cran.r-project.org/web/packages/SQUAREM/index.html.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 01:46:36 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 01:00:05 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Du", "Yu", ""], ["Varadhan", "Ravi", ""]]}, {"id": "1810.11209", "submitter": "Mingyuan Zhou", "authors": "Dandan Guo, Bo Chen, Hao Zhang, Mingyuan Zhou", "title": "Deep Poisson gamma dynamical systems", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially\nobserved multivariate count data, improving previously proposed models by not\nonly mining deep hierarchical latent structure from the data, but also\ncapturing both first-order and long-range temporal dependencies. Using\nsophisticated but simple-to-implement data augmentation techniques, we derived\nclosed-form Gibbs sampling update equations by first backward and upward\npropagating auxiliary latent counts, and then forward and downward sampling\nlatent variables. Moreover, we develop stochastic gradient MCMC inference that\nis scalable to very long multivariate count time series. Experiments on both\nsynthetic and a variety of real-world data demonstrate that the proposed model\nnot only has excellent predictive performance, but also provides highly\ninterpretable multilayer latent structure to represent hierarchical and\ntemporal information propagation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 07:28:20 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 01:01:02 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Guo", "Dandan", ""], ["Chen", "Bo", ""], ["Zhang", "Hao", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1810.11332", "submitter": "Arin Chaudhuri", "authors": "Arin Chaudhuri, Wenhao Hu", "title": "A fast algorithm for computing distance correlation", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2019.01.016", "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical dependence measures such as Pearson correlation, Spearman's $\\rho$,\nand Kendall's $\\tau$ can detect only monotonic or linear dependence. To\novercome these limitations, Szekely et al.(2007) proposed distance covariance\nas a weighted $L_2$ distance between the joint characteristic function and the\nproduct of marginal distributions. The distance covariance is $0$ if and only\nif two random vectors ${X}$ and ${Y}$ are independent. This measure has the\npower to detect the presence of a dependence structure when the sample size is\nlarge enough. They further showed that the sample distance covariance can be\ncalculated simply from modified Euclidean distances, which typically requires\n$\\mathcal{O}(n^2)$ cost. The quadratic computing time greatly limits the\napplication of distance covariance to large data. In this paper, we present a\nsimple exact $\\mathcal{O}(n\\log(n))$ algorithm to calculate the sample distance\ncovariance between two univariate random variables. The proposed method\nessentially consists of two sorting steps, so it is easy to implement.\nEmpirical results show that the proposed algorithm is significantly faster than\nstate-of-the-art methods. The algorithm's speed will enable researchers to\nexplore complicated dependence structures in large datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 13:57:23 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 18:40:56 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Chaudhuri", "Arin", ""], ["Hu", "Wenhao", ""]]}, {"id": "1810.11557", "submitter": "Simon Demers", "authors": "Simon Demers", "title": "The Duration of Optimal Stopping Problems", "comments": "37 pages, 2 figures, 4 tables. This version contains important\n  corrections and additional extensions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal stopping problems give rise to random distributions describing how\nmany applicants the decision-maker will sample or interview before choosing\none, a quantity sometimes referred to as the search time or process duration.\nThis research note surveys several variants of optimal stopping problems,\nextends earlier results in various directions, and shows how many interviews\nare expected to be conducted in various settings. The focus is on problems that\nrequire a decision-maker to choose a candidate from a pool of sequential\napplicants with no recall, in the vein of previously studied Cayley-Moser,\nSecretary and Sultan's Dowry problems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 23:49:34 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 04:21:30 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 21:44:04 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Demers", "Simon", ""]]}, {"id": "1810.12068", "submitter": "Heather Turner", "authors": "Heather L. Turner, Jacob van Etten, David Firth and Ioannis Kosmidis", "title": "Modelling rankings in R: the PlackettLuce package", "comments": "In v2: review of software implementing alternative models to\n  Plackett-Luce; comparison of algorithms provided by the PlackettLuce package;\n  further examples of rankings where the underlying win-loss network is not\n  strongly connected. In addition, general editing to improve organisation and\n  clarity. In v3: corrected headings Table 4, minor edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the R package PlackettLuce, which implements a\ngeneralization of the Plackett-Luce model for rankings data. The generalization\naccommodates both ties (of arbitrary order) and partial rankings (complete\nrankings of subsets of items). By default, the implementation adds a set of\npseudo-comparisons with a hypothetical item, ensuring that the underlying\nnetwork of wins and losses between items is always strongly connected. In this\nway, the worth of each item always has a finite maximum likelihood estimate,\nwith finite standard error. The use of pseudo-comparisons also has a\nregularization effect, shrinking the estimated parameters towards equal item\nworth. In addition to standard methods for model summary, PlackettLuce provides\na method to compute quasi standard errors for the item parameters. This\nprovides the basis for comparison intervals that do not change with the choice\nof identifiability constraint placed on the item parameters. Finally, the\npackage provides a method for model-based partitioning using covariates whose\nvalues vary between rankings, enabling the identification of subgroups of\njudges or settings that have different item worths. The features of the package\nare demonstrated through application to classic and novel data sets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 12:01:07 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 12:07:48 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 16:42:31 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Turner", "Heather L.", ""], ["van Etten", "Jacob", ""], ["Firth", "David", ""], ["Kosmidis", "Ioannis", ""]]}, {"id": "1810.12161", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi and Bao-Tuyen Huynh", "title": "Regularized Maximum Likelihood Estimation and Feature Selection in\n  Mixtures-of-Experts Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) are successful models for modeling heterogeneous\ndata in many statistical learning problems including regression, clustering and\nclassification. Generally fitted by maximum likelihood estimation via the\nwell-known EM algorithm, their application to high-dimensional problems is\nstill therefore challenging. We consider the problem of fitting and feature\nselection in MoE models, and propose a regularized maximum likelihood\nestimation approach that encourages sparse solutions for heterogeneous\nregression data models with potentially high-dimensional predictors. Unlike\nstate-of-the art regularized MLE for MoE, the proposed modelings do not require\nan approximate of the penalty function. We develop two hybrid EM algorithms: an\nExpectation-Majorization-Maximization (EM/MM) algorithm, and an EM algorithm\nwith coordinate ascent algorithm. The proposed algorithms allow to\nautomatically obtaining sparse solutions without thresholding, and avoid matrix\ninversion by allowing univariate parameter updates. An experimental study shows\nthe good performance of the algorithms in terms of recovering the actual sparse\nsolutions, parameter estimation, and clustering of heterogeneous regression\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:42:04 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Huynh", "Bao-Tuyen", ""]]}, {"id": "1810.12184", "submitter": "Elyas Heidari", "authors": "Elyas Heidari, Vahid Balazadeh-Meresht, Ali Sharifi-Zarchi", "title": "Multivariate Analysis and Visualization using R Package muvis", "comments": "online documentation: https://baio-lab.github.io/muvis/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased application of multivariate data in many scientific areas has\nconsiderably raised the complexity of analysis and interpretation. Although\nquite a few approaches have been put forward to address this issue, there is\nstill a gap between the most efficient proposed methods and available software.\nmuvis is an R package (core team (2017)) which is a toolkit for analyzing\nmultivariate datasets. Several tools are implemented for common analyses of\nmultivariate datasets, including preprocessing, dimensionality reduction,\nstatistical analysis, Probabilistic Graphical Modeling, hypothesis testing, and\nvisualization. Furthermore, we have implemented two novel\nmethods--Variable-wise Kullback-Leibler Divergence (VKL) and Violating\nVariable-wise Kullback-Leibler Divergence (VVKL)--which are proposed to find\nthe features with most different probability distributions between two specific\ngroups of samples. The main aim of the package is to provide a wide range of\nusers with different levels of expertise in R with a set of tools for\ncomprehensive analysis of multivariate datasets. We exploited the NHANES\ndataset to declare the functionality of muvis in practice.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 15:14:56 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Heidari", "Elyas", ""], ["Balazadeh-Meresht", "Vahid", ""], ["Sharifi-Zarchi", "Ali", ""]]}, {"id": "1810.12361", "submitter": "Murat A. Erdogdu", "authors": "Murat A. Erdogdu, Lester Mackey, Ohad Shamir", "title": "Global Non-convex Optimization with Discretized Diffusions", "comments": "19 pages, NeurIPS 2018 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Euler discretization of the Langevin diffusion is known to converge to the\nglobal minimizers of certain convex and non-convex optimization problems. We\nshow that this property holds for any suitably smooth diffusion and that\ndifferent diffusions are suitable for optimizing different classes of convex\nand non-convex functions. This allows us to design diffusions suitable for\nglobally optimizing convex and non-convex functions not covered by the existing\nLangevin theory. Our non-asymptotic analysis delivers computable optimization\nand integration error bounds based on easily accessed properties of the\nobjective and chosen diffusion. Central to our approach are new explicit Stein\nfactor bounds on the solutions of Poisson equations. We complement these\nresults with improved optimization guarantees for targets other than the\nstandard Gibbs measure.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 19:09:23 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 08:28:28 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Erdogdu", "Murat A.", ""], ["Mackey", "Lester", ""], ["Shamir", "Ohad", ""]]}, {"id": "1810.12437", "submitter": "Akihiko Nishimura", "authors": "Akihiko Nishimura and Marc A. Suchard", "title": "Prior-preconditioned conjugate gradient method for accelerated Gibbs\n  sampling in \"large $n$ & large $p$\" Bayesian sparse regression", "comments": "35 pages, 7 figures + Supplement (42 pages, 18 figures); Software\n  package available --- see documentation at\n  https://bayes-bridge.readthedocs.io and source code at\n  https://github.com/aki-nishimura/bayes-bridge", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a modern observational study based on healthcare databases, the number of\nobservations and of predictors typically range in the order of $10^5$ ~ $10^6$\nand of $10^4$ ~ $10^5$. Despite the large sample size, data rarely provide\nsufficient information to reliably estimate such a large number of parameters.\nSparse regression techniques provide potential solutions, one notable approach\nbeing the Bayesian methods based on shrinkage priors. In the \"large n & large\np\" setting, however, posterior computation encounters a major bottleneck at\nrepeated sampling from a high-dimensional Gaussian distribution, whose\nprecision matrix $\\Phi$ is expensive to compute and factorize. In this article,\nwe present a novel algorithm to speed up this bottleneck based on the following\nobservation: we can cheaply generate a random vector $b$ such that the solution\nto the linear system $\\Phi \\beta = b$ has the desired Gaussian distribution. We\ncan then solve the linear system by the conjugate gradient (CG) algorithm\nthrough matrix-vector multiplications by $\\Phi$; this involves no explicit\nfactorization or calculation of $\\bPhi$ itself. Rapid convergence of CG in this\ncontext is guaranteed by the theory of prior-preconditioning we develop. We\napply our algorithm to a clinically relevant large-scale observational study\nwith n = 72,489 patients and p = 22,175 clinical covariates, designed to assess\nthe relative risk of adverse events from two alternative blood anti-coagulants.\nOur algorithm demonstrates an order of magnitude speed-up in the posterior\ncomputation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 22:21:56 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 15:24:01 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 16:44:40 GMT"}, {"version": "v4", "created": "Fri, 17 Jan 2020 17:35:58 GMT"}, {"version": "v5", "created": "Tue, 20 Jul 2021 18:01:22 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Nishimura", "Akihiko", ""], ["Suchard", "Marc A.", ""]]}]