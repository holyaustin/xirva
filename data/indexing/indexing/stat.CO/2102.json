[{"id": "2102.00218", "submitter": "Ari Pakman", "authors": "Ari Pakman, Amin Nejatbakhsh, Dar Gilboa, Abdullah Makkeh, Luca\n  Mazzucato, Michael Wibral, Elad Schneidman", "title": "Estimating the Unique Information of Continuous Variables in Recurrent\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The integration and transfer of information from multiple sources to multiple\ntargets is a core motive of neural systems. The emerging field of partial\ninformation decomposition (PID) provides a novel information-theoretic lens\ninto these mechanisms by identifying synergistic, redundant, and unique\ncontributions to the mutual information between one and several variables.\nWhile many works have studied aspects of PID for Gaussian and discrete\ndistributions, the case of general continuous distributions is still uncharted\nterritory. In this work we present a method for estimating the unique\ninformation in continuous distributions, for the case of one versus two\nvariables. Our method solves the associated optimization problem over the space\nof distributions with fixed bivariate marginals by combining copula\ndecompositions and techniques developed to optimize variational autoencoders.\nWe obtain excellent agreement with known analytic results for Gaussians, and\nillustrate the power of our new approach in several brain-inspired neural\nmodels. Our method is capable of recovering the effective connectivity of a\nchaotic network of rate neurons, and uncovers a complex trade-off between\nredundancy, synergy and unique information in recurrent networks trained to\nsolve a generalized XOR task.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 12:34:42 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 19:41:17 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 09:34:39 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 07:33:21 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Pakman", "Ari", ""], ["Nejatbakhsh", "Amin", ""], ["Gilboa", "Dar", ""], ["Makkeh", "Abdullah", ""], ["Mazzucato", "Luca", ""], ["Wibral", "Michael", ""], ["Schneidman", "Elad", ""]]}, {"id": "2102.00249", "submitter": "Evandro Konzen", "authors": "Evandro Konzen, Yafeng Cheng, Jian Qing Shi", "title": "Gaussian Process for Functional Data Analysis: The GPFDA Package for R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and describe the GPFDA package for R. The package provides\nflexible functionalities for dealing with Gaussian process regression (GPR)\nmodels for functional data. Multivariate functional data, functional data with\nmultidimensional inputs, and nonseparable and/or nonstationary covariance\nstructures can be modeled. In addition, the package fits functional regression\nmodels where the mean function depends on scalar and/or functional covariates\nand the covariance structure is modeled by a GPR model. In this paper, we\npresent the versatility of GPFDA with respect to mean function and covariance\nfunction specifications and illustrate the implementation of estimation and\nprediction of some models through reproducible numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 15:45:07 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Konzen", "Evandro", ""], ["Cheng", "Yafeng", ""], ["Shi", "Jian Qing", ""]]}, {"id": "2102.00366", "submitter": "John O'Leary", "authors": "John O'Leary, Guanyang Wang", "title": "Transition kernel couplings of the Metropolis-Hastings algorithm", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Couplings play a central role in the analysis of Markov chain convergence to\nstationarity and in the construction of novel Markov chain Monte Carlo\ndiagnostics, estimators, and variance reduction techniques. The quality of the\nresulting bounds or methods typically depends on how quickly the coupling\ninduces meeting between chains, a property sometimes referred to as its\nefficiency. The design of efficient Markovian couplings remains a difficult\nopen question, especially for discrete time processes. In pursuit of this goal,\nin this paper we fully characterize the couplings of the Metropolis--Hastings\n(MH) transition kernel, providing necessary and sufficient conditions in terms\nof the underlying proposal and acceptance distributions. We apply these results\nto characterize the set of maximal couplings of the MH kernel, resolving open\nquestions posed in O'Leary et al. [2020] on the structure and properties of\nthese couplings. These results represent an advance in the understanding of the\nMH kernel and a step toward the formulation of efficient couplings for this\npopular family of algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 03:45:57 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 18:45:05 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["O'Leary", "John", ""], ["Wang", "Guanyang", ""]]}, {"id": "2102.00514", "submitter": "Fatemeh Yaghoobi", "authors": "Fatemeh Yaghoobi, Adrien Corenflos, Sakira Hassan, Simo S\\\"arkk\\\"a", "title": "Parallel Iterated Extended and Sigma-point Kalman Smoothers", "comments": "Accepted to be published in IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of Bayesian filtering and smoothing in nonlinear models with\nadditive noise is an active area of research. Classical Taylor series as well\nas more recent sigma-point based methods are two well-known strategies to deal\nwith these problems. However, these methods are inherently sequential and do\nnot in their standard formulation allow for parallelization in the time domain.\nIn this paper, we present a set of parallel formulas that replace the existing\nsequential ones in order to achieve lower time (span) complexity. Our\nexperimental results done with a graphics processing unit (GPU) illustrate the\nefficiency of the proposed methods over their sequential counterparts.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 19:09:45 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yaghoobi", "Fatemeh", ""], ["Corenflos", "Adrien", ""], ["Hassan", "Sakira", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2102.00574", "submitter": "Sahand Farhoodi", "authors": "Sahand Farhoodi, Uri Eden", "title": "The problem of perfect predictors in statistical spike train models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Linear Models (GLMs) have been used extensively in statistical\nmodels of spike train data. However, the IRLS algorithm, which is often used to\nfit such models, can fail to converge in situations where response and\nnon-response can be separated by a single predictor or a linear combination of\nmultiple predictors. Such situations are likely to arise in many neural systems\ndue to properties such as refractoriness and incomplete sampling of the signals\nthat influence spiking. In this paper, we describe multiple classes of\napproaches to address this problem: Standard IRLS with a fixed iteration limit,\ncomputing the maximum likelihood solution in the limit, Bayesian estimation,\nregularization, change of basis, and modifying the search parameters. We\ndemonstrate a specific application of each of these methods to spiking data\nfrom rat somatosensory cortex and discuss the advantages and disadvantages of\neach. We also provide an example of a roadmap for selecting a method based on\nthe problem's particular analysis issues and scientific goals.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 00:38:31 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Farhoodi", "Sahand", ""], ["Eden", "Uri", ""]]}, {"id": "2102.00733", "submitter": "Krzysztof Podgorski", "authors": "Krzysztof Podg\\'orski", "title": "Splinets -- splines through the Taylor expansion, their support sets and\n  orthogonal bases", "comments": "27 pages, 8 figures, the R-package to be submitted to CRAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new representation of splines that targets efficiency in the analysis of\nfunctional data is implemented. The efficiency is achieved through two novel\nfeatures: using the recently introduced orthonormal spline bases, the so-called\n{\\it splinets} and accounting for the spline support sets in the proposed\nspline object representation. The recently-introduced orthogonal splinets are\nevaluated by {\\it dyadic orthogonalization} of the $B$-splines. The package is\nbuilt around the {\\it Splinets}-object that represents a collection of splines.\nIt treats splines as mathematical functions and contains information about the\nsupport sets and the values of the derivatives at the knots that uniquely\ndefine these functions. Algebra and calculus of splines utilize the Taylor\nexpansions at the knots within the support sets. Several orthonormalization\nprocedures of the $B$-splines are implemented including the recommended dyadic\nmethod leading to the splinets. The method bases on a dyadic algorithm that can\nbe also viewed as the efficient method of diagonalizing a band matrix. The\nlocality of the $B$-splines in terms of the support sets is, to a great extend,\npreserved in the corresponding splinet. This together with implemented\nalgorithms utilizing locality of the supports provides a valuable computational\ntool for functional data analysis. The benefits are particularly evident when\nthe sparsity in the data plays an important role. Various diagnostic tools are\nprovided allowing to maintain the stability of the computations. Finally, the\nprojection operation to the space of splines is implemented that facilitates\nfunctional data analysis. An example of a simple functional analysis of the\ndata using the tools in the package is presented. The functionality of the\npackage extends beyond the splines to piecewise polynomial functions, although\nthe splines are its focus.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 09:57:45 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Podg\u00f3rski", "Krzysztof", ""]]}, {"id": "2102.01144", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Double bootstrapping for visualising the distribution of descriptive\n  statistics of functional data", "comments": "22 pages, 9 figures, 1 table, to appear at the Journal of Statistical\n  Computation and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a double bootstrap procedure for reducing coverage error in the\nconfidence intervals of descriptive statistics for independent and identically\ndistributed functional data. Through a series of Monte Carlo simulations, we\ncompare the finite sample performance of single and double bootstrap procedures\nfor estimating the distribution of descriptive statistics for independent and\nidentically distributed functional data. At the cost of longer computational\ntime, the double bootstrap with the same bootstrap method reduces confidence\nlevel error and provides improved coverage accuracy than the single bootstrap.\nIllustrated by a Canadian weather station data set, the double bootstrap\nprocedure presents a tool for visualising the distribution of the descriptive\nstatistics for the functional data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:22:43 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "2102.01221", "submitter": "Jiatong Sui", "authors": "Hongjun Li, Jiatong Sui, Shengpeng Mu, Xing Qiu", "title": "Stochastic Properties of Minimal Arc Distance and Cosine Similarity\n  between a Random Point and Prespecified Sites on Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In applications such as wireless communication, it is important to study the\nstatistical properties of $L_{2}$, the minimal arc distance between a random\npoint (e.g., a cellphone user) uniformly distributed on a sphere to a set of\npre-defined seeds (e.g., wireless towers) on that sphere. In this study, we\nfirst derive the distribution (CDF) and density (PDF) functions of the arc\ndistance between a selected vertex of a spherical triangle to a random point\nuniformly distributed within this triangle. Next, using computational\ntechniques based on spherical Voronoi diagram and triangular partition of\nVoronoi cells, we derive moments of $L_{2}$ and $\\cos L_{2}$. These results are\nverified by extensive Monte Carlo simulations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 23:01:23 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Li", "Hongjun", ""], ["Sui", "Jiatong", ""], ["Mu", "Shengpeng", ""], ["Qiu", "Xing", ""]]}, {"id": "2102.01742", "submitter": "Juan B\\'ogalo Rom\\'an", "authors": "Juan B\\'ogalo, Pilar Poncela, Eva Senra", "title": "cissa(): A MATLAB Function for Signal Extraction", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  cissa() is a MATLAB function for signal extraction by Circulant Singular\nSpectrum Analysis, a procedure proposed in Bogalo et al (2021). cissa()\nextracts the underlying signals in a time series identifying their frequency of\noscillation in an automated way, by just introducing the data and the window\nlength. This solution can be applied to stationary as well as to non-stationary\nand non-linear time series. Additionally, in this paper, we solve some\ntechnical issues regarding the beginning and end of sample data points. We also\nintroduce novel criteria in order to reconstruct the underlying signals\ngrouping some of the extracted components. The output of cissa() is the input\nof the function group() to reconstruct the desired signals by further grouping\nthe extracted components. group() allows a novel user to create standard\nsignals by automated grouping options while an expert user can decide on the\nnumber of groups and their composition. To illustrate its versatility and\nperformance in several fields we include 3 examples: an AM-FM synthetic signal,\nan example of the physical world given by a voiced speech signal and an\neconomic time series. Possible applications include de-noising,\nde-seasonalizing, de-trending and extracting business cycles, among others.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 20:26:25 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["B\u00f3galo", "Juan", ""], ["Poncela", "Pilar", ""], ["Senra", "Eva", ""]]}, {"id": "2102.01784", "submitter": "Scott Bruce", "authors": "Pramita Bagchi and Scott A. Bruce", "title": "Adaptive Frequency Band Analysis for Functional Time Series", "comments": "33 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The frequency-domain properties of nonstationary functional time series often\ncontain valuable information. These properties are characterized through its\ntime-varying power spectrum. Practitioners seeking low-dimensional summary\nmeasures of the power spectrum often partition frequencies into bands and\ncreate collapsed measures of power within bands. However, standard frequency\nbands have largely been developed through manual inspection of time series data\nand may not adequately summarize power spectra. In this article, we propose a\nframework for adaptive frequency band estimation of nonstationary functional\ntime series that optimally summarizes the time-varying dynamics of the series.\nWe develop a scan statistic and search algorithm to detect changes in the\nfrequency domain. We establish theoretical properties of this framework and\ndevelop a computationally-efficient implementation. The validity of our method\nis also justified through numerous simulation studies and an application to\nanalyzing electroencephalogram data in participants alternating between eyes\nopen and eyes closed conditions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 22:33:37 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 21:19:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Bagchi", "Pramita", ""], ["Bruce", "Scott A.", ""]]}, {"id": "2102.01790", "submitter": "John O'Leary", "authors": "John O'Leary", "title": "Couplings of the Random-Walk Metropolis algorithm", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Couplings play a central role in contemporary Markov chain Monte Carlo\nmethods and in the analysis of their convergence to stationarity. In most\ncases, a coupling must induce relatively fast meeting between chains to ensure\ngood performance. In this paper we fix attention on the random walk Metropolis\nalgorithm and examine a range of coupling design choices. We introduce proposal\nand acceptance step couplings based on geometric, optimal transport, and\nmaximality considerations. We consider the theoretical properties of these\nchoices and examine their implication for the meeting time of the chains. We\nconclude by extracting a few general principles and hypotheses on the design of\neffective couplings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 22:46:26 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["O'Leary", "John", ""]]}, {"id": "2102.01803", "submitter": "Ganchao Wei", "authors": "Ganchao Wei, Ian H. Stevenson", "title": "Tracking fast and slow changes in synaptic weights from simultaneously\n  observed pre- and postsynaptic spiking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Synapses change on multiple timescales, ranging from milliseconds to minutes,\ndue to a combination of both short- and long-term plasticity. Here we develop\nan extension of the common Generalized Linear Model to infer both short- and\nlong-term changes in the coupling between a pre- and post-synaptic neuron based\non observed spiking activity. We model short-term synaptic plasticity using\nadditive effects that depend on the presynaptic spike timing, and we model\nlong-term changes in both synaptic weight and baseline firing rate using point\nprocess adaptive smoothing. Using simulations, we first show that this model\ncan accurately recover time-varying synaptic weights 1) for both depressing and\nfacilitating synapses, 2) with a variety of long-term changes (including\nrealistic changes, such as due to STDP), 3) with a range of pre- and\npost-synaptic firing rates, and 4) for both excitatory and inhibitory synapses.\nWe then apply our model to two experimentally recorded putative synaptic\nconnections. We find that simultaneously tracking fast changes in synaptic\nweights, slow changes in synaptic weights, and unexplained variations in\nbaseline firing is essential. Omitting any one of these factors can lead to\nspurious inferences for the others. Altogether, this model provides a flexible\nframework for tracking short- and long-term variation in spike transmission.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 23:54:00 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 02:12:41 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wei", "Ganchao", ""], ["Stevenson", "Ian H.", ""]]}, {"id": "2102.01982", "submitter": "Michael Fop", "authors": "Michael Fop, Pierre-Alexandre Mattei, Charles Bouveyron, Thomas\n  Brendan Murphy", "title": "Unobserved classes and extra variables in high-dimensional discriminant\n  analysis", "comments": "29 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In supervised classification problems, the test set may contain data points\nbelonging to classes not observed in the learning phase. Moreover, the same\nunits in the test data may be measured on a set of additional variables\nrecorded at a subsequent stage with respect to when the learning sample was\ncollected. In this situation, the classifier built in the learning phase needs\nto adapt to handle potential unknown classes and the extra dimensions. We\nintroduce a model-based discriminant approach, Dimension-Adaptive Mixture\nDiscriminant Analysis (D-AMDA), which can detect unobserved classes and adapt\nto the increasing dimensionality. Model estimation is carried out via a full\ninductive approach based on an EM algorithm. The method is then embedded in a\nmore general framework for adaptive variable selection and classification\nsuitable for data of large dimensions. A simulation study and an artificial\nexperiment related to classification of adulterated honey samples are used to\nvalidate the ability of the proposed framework to deal with complex situations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 10:01:52 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fop", "Michael", ""], ["Mattei", "Pierre-Alexandre", ""], ["Bouveyron", "Charles", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "2102.02297", "submitter": "Benjamin Bolker", "authors": "Steve Cygu, Jonathan Dushoff, and Benjamin M. Bolker", "title": "pcoxtime: Penalized Cox Proportional Hazard Model for Time-dependent\n  Covariates", "comments": "Submitted to Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The penalized Cox proportional hazard model is a popular analytical approach\nfor survival data with a large number of covariates. Such problems are\nespecially challenging when covariates vary over follow-up time (i.e., the\ncovariates are time-dependent). The standard R packages for fully penalized Cox\nmodels cannot currently incorporate time-dependent covariates. To address this\ngap, we implement a variant of gradient descent algorithm (proximal gradient\ndescent) for fitting penalized Cox models. We apply our implementation to real\nand simulated data sets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 21:09:12 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 16:46:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cygu", "Steve", ""], ["Dushoff", "Jonathan", ""], ["Bolker", "Benjamin M.", ""]]}, {"id": "2102.02421", "submitter": "Paul Atzberger", "authors": "B. J. Gross, P. Kuberry, P. J. Atzberger", "title": "First-Passage Time Statistics on Surfaces of General Shape: Surface PDE\n  Solvers using Generalized Moving Least Squares (GMLS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.data-an q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop numerical methods for computing statistics of stochastic processes\non surfaces of general shape with drift-diffusion dynamics $d{X}_t = a({X}_t)dt\n+ {b}({X}_t)d{W}_t$. We consider on a surface domain $\\Omega$ the statistics\n$u(\\mathbf{x}) = \\mathbb{E}^{\\mathbf{x}}\\left[\\int_0^\\tau g(X_t)dt \\right] +\n\\mathbb{E}^{\\mathbf{x}}\\left[f(X_\\tau)\\right]$ with the exit stopping time\n$\\tau = \\inf_t \\{t > 0 \\; |\\; X_t \\not\\in \\Omega\\}$. Using Dynkin's formula, we\ncompute statistics by developing high-order Generalized Moving Least Squares\n(GMLS) solvers for the associated surface PDE boundary-value problems. We focus\nparticularly on the mean First Passage Times (FPTs) given by the special case\n$f = 0,\\, g = 1$ with $u(\\mathbf{x}) =\n\\mathbb{E}^{\\mathbf{x}}\\left[\\tau\\right]$. We perform studies for a variety of\nshapes showing our methods converge with high-order accuracy both in capturing\nthe geometry and the surface PDE solutions. We then perform studies showing how\nFPTs are influenced by the surface geometry, drift dynamics, and spatially\ndependent diffusivities.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 05:30:32 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Gross", "B. J.", ""], ["Kuberry", "P.", ""], ["Atzberger", "P. J.", ""]]}, {"id": "2102.02504", "submitter": "Pierre Alquier", "authors": "Dimitri Meunier and Pierre Alquier", "title": "Meta-strategy for Learning Tuning Parameters with Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online gradient methods, like the online gradient algorithm (OGA), often\ndepend on tuning parameters that are difficult to set in practice. We consider\nan online meta-learning scenario, and we propose a meta-strategy to learn these\nparameters from past tasks. Our strategy is based on the minimization of a\nregret bound. It allows to learn the initialization and the step size in OGA\nwith guarantees. We provide a regret analysis of the strategy in the case of\nconvex losses. It suggests that, when there are parameters\n$\\theta_1,\\dots,\\theta_T$ solving well tasks $1,\\dots,T$ respectively and that\nare close enough one to each other, our strategy indeed improves on learning\neach task in isolation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 09:32:22 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 09:03:58 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Meunier", "Dimitri", ""], ["Alquier", "Pierre", ""]]}, {"id": "2102.02642", "submitter": "Benjamin Christoffersen", "authors": "Benjamin Christoffersen, Mark Clements, Keith Humphreys, Hedvig\n  Kjellstr\\\"om", "title": "Asymptotically Exact and Fast Gaussian Copula Models for Imputation of\n  Mixed Data Types", "comments": "20 pages, 1 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing values with mixed data types is a common problem in a large number of\nmachine learning applications such as processing of surveys and in different\nmedical applications. Recently, Gaussian copula models have been suggested as a\nmeans of performing imputation of missing values using a probabilistic\nframework. While the present Gaussian copula models have shown to yield state\nof the art performance, they have two limitations: they are based on an\napproximation that is fast but may be imprecise and they do not support\nunordered multinomial variables. We address the first limitation using direct\nand arbitrarily precise approximations both for model estimation and imputation\nby using randomized quasi-Monte Carlo procedures. The method we provide has\nlower errors for the estimated model parameters and the imputed values,\ncompared to previously proposed methods. We also extend the previous Gaussian\ncopula models to include unordered multinomial variables in addition to the\npresent support of ordinal, binary, and continuous variables.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 14:42:29 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 10:15:31 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Christoffersen", "Benjamin", ""], ["Clements", "Mark", ""], ["Humphreys", "Keith", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2102.02691", "submitter": "Tomasz Nowicki", "authors": "Soumyadip Ghosh, Yingdong Lu, Tomasz Nowicki", "title": "HMC, an Algorithms in Data Mining, the Functional Analysis approach", "comments": "arXiv admin note: text overlap with arXiv:2101.08688", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to facilitate the communication between the\nAnalytic, Probabilistic and Algorithmic communities.\n  We present a proof of convergence of the Hamiltonian (Hybrid) Monte Carlo\nalgorithm from the point of view of the\n  Dynamical Systems, where the evolving objects are densities of probability\ndistributions and the tool are derived from the Functional Analysis.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:39:00 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Ghosh", "Soumyadip", ""], ["Lu", "Yingdong", ""], ["Nowicki", "Tomasz", ""]]}, {"id": "2102.02794", "submitter": "Scott Bruce", "authors": "Zeda Li, Scott A. Bruce, and Tian Cai", "title": "Classification of Categorical Time Series Using the Spectral Envelope\n  and Optimal Scalings", "comments": "29 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces a novel approach to the classification of categorical\ntime series under the supervised learning paradigm. To construct meaningful\nfeatures for categorical time series classification, we consider two relevant\nquantities: the spectral envelope and its corresponding set of optimal\nscalings. These quantities characterize oscillatory patterns in a categorical\ntime series as the largest possible power at each frequency, or spectral\nenvelope, obtained by assigning numerical values, or scalings, to categories\nthat optimally emphasize oscillations at each frequency. Our procedure combines\nthese two quantities to produce an interpretable and parsimonious feature-based\nclassifier that can be used to accurately determine group membership for\ncategorical time series. Classification consistency of the proposed method is\ninvestigated, and simulation studies are used to demonstrate accuracy in\nclassifying categorical time series with various underlying group structures.\nFinally, we use the proposed method to explore key differences in oscillatory\npatterns of sleep stage time series for patients with different sleep disorders\nand accurately classify patients accordingly.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:29:27 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Zeda", ""], ["Bruce", "Scott A.", ""], ["Cai", "Tian", ""]]}, {"id": "2102.02852", "submitter": "Bj\\\"orn Holzhauer", "authors": "Bj\\\"orn Holzhauer (1), Lisa V. Hampson (1), John Paul Gosling (2),\n  Bj\\\"orn Bornkamp (1), Joseph Kahn (3), Markus R. Lange (1), Wen-Lin Luo (3),\n  Caterina Brindicci (1), David Lawrence (1), Steffen Ballerstedt (1), Anthony\n  O'Hagan (4) ((1) Novartis Pharma AG, Basel, Switzerland, (2) JBA Risk\n  Management Ltd, Skipton, United Kingdom, (3) Novartis Pharmaceuticals\n  Corporation, East Hanover, USA, (4) The University of Sheffield, School of\n  Mathematics and Statistics, Sheffield, United Kingdom)", "title": "Eliciting judgements about dependent quantities of interest: The SHELF\n  extension and copula methods illustrated using an asthma case study", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pharmaceutical companies regularly need to make decisions about drug\ndevelopment programs based on the limited knowledge from early stage clinical\ntrials. In this situation, eliciting the judgements of experts is an attractive\napproach for synthesising evidence on the unknown quantities of interest. When\ncalculating the probability of success for a drug development program, multiple\nquantities of interest - such as the effect of a drug on different endpoints -\nshould not be treated as unrelated.\n  We discuss two approaches for establishing a multivariate distribution for\nseveral related quantities within the SHeffield ELicitation Framework (SHELF).\nThe first approach elicits experts' judgements about a quantity of interest\nconditional on knowledge about another one. For the second approach, we first\nelicit marginal distributions for each quantity of interest. Then, for each\npair of quantities, we elicit the concordance probability that both lie on the\nsame side of their respective elicited medians. This allows us to specify a\ncopula to obtain the joint distribution of the quantities of interest.\n  We show how these approaches were used in an elicitation workshop that was\nperformed to assess the probability of success of the registrational program of\nan asthma drug. The judgements of the experts, which were obtained prior to\ncompletion of the pivotal studies, were well aligned with the final trial\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 19:37:14 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 16:10:53 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Holzhauer", "Bj\u00f6rn", ""], ["Hampson", "Lisa V.", ""], ["Gosling", "John Paul", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Kahn", "Joseph", ""], ["Lange", "Markus R.", ""], ["Luo", "Wen-Lin", ""], ["Brindicci", "Caterina", ""], ["Lawrence", "David", ""], ["Ballerstedt", "Steffen", ""], ["O'Hagan", "Anthony", ""]]}, {"id": "2102.02984", "submitter": "Edward Acheampong Dr", "authors": "Edward Acheampong, Eric Okyere, Samuel Iddi, Joseph H. K. Bonney,\n  Jonathan A. D. Wattis, Rachel L. Gomes", "title": "Modelling COVID-19 Transmission Dynamics in Ghana", "comments": "Submitted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.DS stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In late 2019, a novel coronavirus, the SARS-CoV-2 outbreak was identified in\nWuhan, China and later spread to every corner of the globe. Whilst the number\nof infection-induced deaths in Ghana, West Africa are minimal when compared\nwith the rest of the world, the impact on the local health service is still\nsignificant. Compartmental models are a useful framework for investigating\ntransmission of diseases in societies. To understand how the infection will\nspread and how to limit the outbreak. We have developed a modified SEIR\ncompartmental model with nine compartments (CoVCom9) to describe the dynamics\nof SARS-CoV-2 transmission in Ghana. We have carried out a detailed\nmathematical analysis of the CoVCom9, including the derivation of the basic\nreproduction number, $\\mathcal{R}_{0}$. In particular, we have shown that the\ndisease-free equilibrium is globally asymptotically stable when\n$\\mathcal{R}_{0}<1$ via a candidate Lyapunov function. Using the SARS-CoV-2\nreported data for confirmed-positive cases and deaths from March 13 to August\n10, 2020, we have parametrised the CoVCom9 model. The results of this fit show\ngood agreement with data. We used Latin hypercube sampling-rank correlation\ncoefficient (LHS-PRCC) to investigate the uncertainty and sensitivity of\n$\\mathcal{R}_{0}$ since the results derived are significant in controlling the\nspread of SARS-CoV-2. We estimate that over this five month period, the basic\nreproduction number is given by $\\mathcal{R}_{0} = 3.110$, with the 95\\%\nconfidence interval being $2.042 \\leq \\mathcal{R}_0 \\leq 3.240$, and the mean\nvalue being $\\mathcal{R}_{0}=2.623$. Of the 32 parameters in the model, we find\nthat just six have a significant influence on $\\mathcal{R}_{0}$, these include\nthe rate of testing, where an increasing testing rate contributes to the\nreduction of $\\mathcal{R}_{0}$.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 03:32:54 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Acheampong", "Edward", ""], ["Okyere", "Eric", ""], ["Iddi", "Samuel", ""], ["Bonney", "Joseph H. K.", ""], ["Wattis", "Jonathan A. D.", ""], ["Gomes", "Rachel L.", ""]]}, {"id": "2102.03106", "submitter": "Valeria Policastro", "authors": "Valeria Policastro, Dario Righelli, Annamaria Carissimo, Luisa Cutillo\n  and Italia De Feis", "title": "ROBustness In Network (robin): an R package for Comparison and\n  Validation of communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In network analysis, many community detection algorithms have been developed,\nhowever, their implementation leaves unaddressed the question of the\nstatistical validation of the results. Here we present robin(ROBustness In\nNetwork), an R package to assess the robustness of the community structure of a\nnetwork found by one or more methods to give indications about their\nreliability. The procedure initially detects if the community structure found\nby a set of algorithms is statistically significant and then compares two\nselected detection algorithms on the same graph to choose the one that better\nfits the network of interest. We demonstrate the use of our package on the\nAmerican College Football benchmark dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 11:12:18 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Policastro", "Valeria", ""], ["Righelli", "Dario", ""], ["Carissimo", "Annamaria", ""], ["Cutillo", "Luisa", ""], ["De Feis", "Italia", ""]]}, {"id": "2102.03411", "submitter": "Moo K. Chung", "authors": "Moo K. Chung", "title": "Cosine Series Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This short paper is based on Chung et al. (2010), where the cosine series\nrepresentation (CSR) is used in modeling the shape of white matter fiber tracts\nin diffusion tensor imaging(DTI) and Wang et al. (2018), where the method is\nused to denoise EEG. The proposed explicit analytic approach offers far\nsuperior flexibility in statistical modeling compared to the usual implicit\nFourier transform methods such as the discrete cosine transforms often used in\nsignal processing. The MATLAB codes and sample data can be obtained from\nhttp://brainimaging.waisman.wisc.edu/~chung/tracts.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 20:22:12 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chung", "Moo K.", ""]]}, {"id": "2102.03585", "submitter": "Salar Fattahi", "authors": "Salar Fattahi and Andres Gomez", "title": "Scalable Inference of Sparsely-changing Markov Random Fields with Strong\n  Statistical Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of inferring time-varying Markov random\nfields (MRF), where the underlying graphical model is both sparse and changes\nsparsely over time. Most of the existing methods for the inference of\ntime-varying MRFs rely on the regularized maximum likelihood estimation (MLE),\nthat typically suffer from weak statistical guarantees and high computational\ntime. Instead, we introduce a new class of constrained optimization problems\nfor the inference of sparsely-changing MRFs. The proposed optimization problem\nis formulated based on the exact $\\ell_0$ regularization, and can be solved in\nnear-linear time and memory. Moreover, we show that the proposed estimator\nenjoys a provably small estimation error. As a special case, we derive sharp\nstatistical guarantees for the inference of sparsely-changing Gaussian MRFs\n(GMRF) in the high-dimensional regime, showing that such problems can be\nlearned with as few as one sample per time. Our proposed method is extremely\nefficient in practice: it can accurately estimate sparsely-changing graphical\nmodels with more than 500 million variables in less than one hour.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 13:53:00 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Fattahi", "Salar", ""], ["Gomez", "Andres", ""]]}, {"id": "2102.03639", "submitter": "Ranjan Maitra", "authors": "Wei-Chen Chen and Ranjan Maitra", "title": "A Practical Model-based Segmentation Approach for Accurate Activation\n  Detection in Single-Subject functional Magnetic Resonance Imaging Studies", "comments": "20 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Magnetic Resonance Imaging (fMRI) maps cerebral activation in\nresponse to stimuli but this activation is often difficult to detect,\nespecially in low-signal contexts and single-subject studies. Accurate\nactivation detection can be guided by the fact that very few voxels are, in\nreality, truly activated and that activated voxels are spatially localized, but\nit is challenging to incorporate both these facts. We provide a computationally\nfeasible and methodologically sound model-based approach, implemented in the R\npackage MixfMRI, that bounds the a priori expected proportion of activated\nvoxels while also incorporating spatial context. Results on simulation\nexperiments for different levels of activation detection difficulty are\nuniformly encouraging. The value of the methodology in low-signal and\nsingle-subject fMRI studies is illustrated on a sports imagination experiment.\nConcurrently, we also extend the potential use of fMRI as a clinical tool to,\nfor example, detect awareness and improve treatment in individual patients in\npersistent vegetative state, such as traumatic brain injury survivors.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 18:46:33 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chen", "Wei-Chen", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2102.03681", "submitter": "James Yang", "authors": "James Yang", "title": "FastAD: Expression Template-Based C++ Library for Fast and\n  Memory-Efficient Automatic Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic differentiation is a set of techniques to efficiently and\naccurately compute the derivative of a function represented by a computer\nprogram. Existing C++ libraries for automatic differentiation (e.g. Adept, Stan\nMath Library), however, exhibit large memory consumptions and runtime\nperformance issues. This paper introduces FastAD, a new C++ template library\nfor automatic differentiation, that overcomes all of these challenges in\nexisting libraries by using vectorization, simpler memory management using a\nfully expression-template-based design, and other compile-time optimizations to\nremove some run-time overhead. Benchmarks show that FastAD performs 2-10 times\nfaster than Adept and 2-19 times faster than Stan across various test cases\nincluding a few real-world examples.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 23:17:10 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Yang", "James", ""]]}, {"id": "2102.03954", "submitter": "Serge Vicente Vicente", "authors": "Serge Vicente, Alejandro Murua", "title": "Large-data determinantal clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal consensus clustering is a promising and attractive alternative\nto partitioning about medoids and k-means for ensemble clustering. Based on a\ndeterminantal point process or DPP sampling, it ensures that subsets of similar\npoints are less likely to be selected as centroids. It favors more diverse\nsubsets of points. The sampling algorithm of the determinantal point process\nrequires the eigendecomposition of a Gram matrix. This becomes computationally\nintensive when the data size is very large. This is particularly an issue in\nconsensus clustering, where a given clustering algorithm is run several times\nin order to produce a final consolidated clustering. We propose two efficient\nalternatives to carry out determinantal consensus clustering on large datasets.\nThey consist in DPP sampling based on sparse and small kernel matrices whose\neigenvalue distributions are close to that of the original Gram matrix.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 00:31:05 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Vicente", "Serge", ""], ["Murua", "Alejandro", ""]]}, {"id": "2102.04687", "submitter": "Sudeep Bapat", "authors": "Sudeep R. Bapat and Rohit Bhardwaj", "title": "On an Inflated Unit-Lindley Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling fractional data in various real life scenarios is a challenging\ntask. This paper consider situations where fractional data is observed on the\ninterval [0,1]. The unit-Lindley distribution has been discussed in the\nliterature where its support lies between 0 and 1. In this paper, we focus on\nan inflated variant of the unit-Lindley distribution, where the inflation\noccurs at both 0 and 1. Various properties of the inflated unit-Lindley\ndistribution are discussed and examined, including point estimation based on\nthe maximum likelihood method and interval estimation. Finally, extensive Monte\nCarlo simulation and real-data analyses are carried out to compare the fit of\nour proposed distribution along with some of the existing ones such as the\ninflated beta and the inflated Kumaraswamy distributions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 07:30:07 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bapat", "Sudeep R.", ""], ["Bhardwaj", "Rohit", ""]]}, {"id": "2102.05103", "submitter": "Thomas Maullin-Sapey", "authors": "Thomas Maullin-Sapey, Thomas E. Nichols", "title": "Fisher Scoring for crossed factor Linear Mixed Models", "comments": "For supplementary material see\n  https://www.overleaf.com/read/bvscgqrvqnjh . For code and notebooks, see\n  https://github.com/TomMaullin/LMMPaper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of longitudinal, heterogeneous or unbalanced clustered data is\nof primary importance to a wide range of applications. The Linear Mixed Model\n(LMM) is a popular and flexible extension of the linear model specifically\ndesigned for such purposes. Historically, a large proportion of material\npublished on the LMM concerns the application of popular numerical optimization\nalgorithms, such as Newton-Raphson, Fisher Scoring and Expectation Maximization\nto single-factor LMMs (i.e. LMMs that only contain one \"factor\" by which\nobservations are grouped). However, in recent years, the focus of the LMM\nliterature has moved towards the development of estimation and inference\nmethods for more complex, multi-factored designs. In this paper, we present and\nderive new expressions for the extension of an algorithm classically used for\nsingle-factor LMM parameter estimation, Fisher Scoring, to multiple,\ncrossed-factor designs. Through simulation and real data examples, we compare\nfive variants of the Fisher Scoring algorithm with one another, as well as\nagainst a baseline established by the R package lmer, and find evidence of\ncorrectness and strong computational efficiency for four of the five proposed\napproaches. Additionally, we provide a new method for LMM Satterthwaite degrees\nof freedom estimation based on analytical results, which does not require\niterative gradient estimation. Via simulation, we find that this approach\nproduces estimates with both lower bias and lower variance than the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 20:00:11 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 20:35:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Maullin-Sapey", "Thomas", ""], ["Nichols", "Thomas E.", ""]]}, {"id": "2102.05320", "submitter": "Sen Na", "authors": "Sen Na, Mihai Anitescu, Mladen Kolar", "title": "An Adaptive Stochastic Sequential Quadratic Programming with\n  Differentiable Exact Augmented Lagrangians", "comments": "59 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA math.NA stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of solving nonlinear optimization programs with\nstochastic objective and deterministic equality constraints. We assume for the\nobjective that the function evaluation, the gradient, and the Hessian are\ninaccessible, while one can compute their stochastic estimates by, for example,\nsubsampling. We propose a stochastic algorithm based on sequential quadratic\nprogramming (SQP) that uses a differentiable exact augmented Lagrangian as the\nmerit function. To motivate our algorithm, we revisit an old SQP method\n\\citep{Lucidi1990Recursive} developed for deterministic programs. We simplify\nthat method and derive an adaptive SQP, which serves as the skeleton of our\nstochastic algorithm. Based on the derived algorithm, we then propose a\nnon-adaptive SQP for optimizing stochastic objectives, where the gradient and\nthe Hessian are replaced by stochastic estimates but the stepsize is\ndeterministic and prespecified. Finally, we incorporate a recent stochastic\nline search procedure \\citep{Paquette2020Stochastic} into our non-adaptive\nstochastic SQP to arrive at an adaptive stochastic SQP. To our knowledge, the\nproposed algorithm is the first stochastic SQP that allows a line search\nprocedure and the first stochastic line search procedure that allows the\nconstraints. The global convergence for all proposed SQP methods is\nestablished, while numerical experiments on nonlinear problems in the CUTEst\ntest set demonstrate the superiority of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 08:40:55 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Na", "Sen", ""], ["Anitescu", "Mihai", ""], ["Kolar", "Mladen", ""]]}, {"id": "2102.05407", "submitter": "V\\'ictor Elvira", "authors": "V\\'ictor Elvira and Luca Martino", "title": "Advances in Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling (IS) is a Monte Carlo technique for the approximation of\nintractable distributions and integrals with respect to them. The origin of IS\ndates from the early 1950s. In the last decades, the rise of the Bayesian\nparadigm and the increase of the available computational resources have\npropelled the interest in this theoretically sound methodology. In this paper,\nwe first describe the basic IS algorithm and then revisit the recent advances\nin this methodology. We pay particular attention to two sophisticated lines.\nFirst, we focus on multiple IS (MIS), the case where more than one proposal is\navailable. Second, we describe adaptive IS (AIS), the generic methodology for\nadapting one or more proposals.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 12:50:38 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 09:40:29 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Elvira", "V\u00edctor", ""], ["Martino", "Luca", ""]]}, {"id": "2102.05801", "submitter": "Hana Sevcikova", "authors": "Adrian E. Raftery, Hana \\v{S}ev\\v{c}\\'ikov\\'a, Bernard W. Silverman", "title": "The vote Package: Single Transferable Vote and Other Electoral Systems\n  in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the vote package in R, which implements the plurality (or\nfirst-past-the-post), two-round runoff, score, approval and single transferable\nvote (STV) electoral systems, as well as methods for selecting the Condorcet\nwinner and loser. We emphasize the STV system, which we have found to work well\nin practice for multi-winner elections with small electorates, such as\ncommittee and council elections, and the selection of multiple job candidates.\nFor single-winner elections, the STV is also called instant runoff voting\n(IRV), ranked choice voting (RCV), or the alternative vote (AV) system. The\npackage also implements the STV system with equal preferences, for the first\ntime in a software package, to our knowledge. It also implements a new variant\nof STV, in which a minimum number of candidates from a specified group are\nrequired to be elected. We illustrate the package with several real examples.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 01:50:46 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Raftery", "Adrian E.", ""], ["\u0160ev\u010d\u00edkov\u00e1", "Hana", ""], ["Silverman", "Bernard W.", ""]]}, {"id": "2102.06058", "submitter": "S\\'ebastien Bourguignon", "authors": "Ramzi Ben Mhenni and S\\'ebastien Bourguignon and J\\'er\\^ome Idier", "title": "SLS (Single $\\ell_1$ Selection): a new greedy algorithm with an\n  $\\ell_1$-norm selection rule", "comments": "in Proceedings of iTWIST'20, Paper-ID: 24, Nantes, France, December,\n  2-4, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new greedy algorithm for sparse approximation,\ncalled SLS for Single L_1 Selection. SLS essentially consists of a greedy\nforward strategy, where the selection rule of a new component at each iteration\nis based on solving a least-squares optimization problem, penalized by the L_1\nnorm of the remaining variables. Then, the component with maximum amplitude is\nselected. Simulation results on difficult sparse deconvolution problems\ninvolving a highly correlated dictionary reveal the efficiency of the method,\nwhich outperforms popular greedy algorithms and Basis Pursuit Denoising when\nthe solution is sparse.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 15:05:17 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Mhenni", "Ramzi Ben", ""], ["Bourguignon", "S\u00e9bastien", ""], ["Idier", "J\u00e9r\u00f4me", ""]]}, {"id": "2102.06851", "submitter": "Juan Domingo Gonzalez", "authors": "Juan D. Gonzalez, Ricardo Maronna, Victor J. Yohai and Ruben H.Zamar", "title": "Robust Model-Based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of robust and Fisher-consistent estimators for mixture\nmodels. These estimators can be used to construct robust model-based clustering\nprocedures. We study in detail the case of multivariate normal mixtures and\npropose a procedure that uses S estimators of multivariate location and\nscatter. We develop an algorithm to compute the estimators and to build the\nclusters which is quite similar to the EM algorithm. An extensive Monte Carlo\nsimulation study shows that our proposal compares favorably with other robust\nand non robust model-based clustering procedures. We apply ours and alternative\nprocedures to a real data set and again find that the best results are obtained\nusing our proposal.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 02:51:42 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 21:17:49 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 16:30:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gonzalez", "Juan D.", ""], ["Maronna", "Ricardo", ""], ["Yohai", "Victor J.", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "2102.07139", "submitter": "James Brofos", "authors": "James A. Brofos, Roy R. Lederman", "title": "Evaluating the Implicit Midpoint Integrator for Riemannian Manifold\n  Hamiltonian Monte Carlo", "comments": "This version has incorporated additional results and improvements to\n  the implementation; see the GitHub\n  (https://github.com/JamesBrofos/Evaluating-the-Implicit-Midpoint-Integrator)\n  for details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Riemannian manifold Hamiltonian Monte Carlo is traditionally carried out\nusing the generalized leapfrog integrator. However, this integrator is not the\nonly choice and other integrators yielding valid Markov chain transition\noperators may be considered. In this work, we examine the implicit midpoint\nintegrator as an alternative to the generalized leapfrog integrator. We discuss\nadvantages and disadvantages of the implicit midpoint integrator for\nHamiltonian Monte Carlo, its theoretical properties, and an empirical\nassessment of the critical attributes of such an integrator for Hamiltonian\nMonte Carlo: energy conservation, volume preservation, and reversibility.\nEmpirically, we find that while leapfrog iterations are faster, the implicit\nmidpoint integrator has better energy conservation, leading to higher\nacceptance rates, as well as better conservation of volume and better\nreversibility, arguably yielding a more accurate sampling procedure.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 12:28:27 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 07:09:23 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Brofos", "James A.", ""], ["Lederman", "Roy R.", ""]]}, {"id": "2102.07432", "submitter": "Pierre Ablin", "authors": "Pierre Ablin and Gabriel Peyr\\'e", "title": "Fast and accurate optimization on the orthogonal manifold without\n  retraction", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of minimizing a function over the manifold of\northogonal matrices. The majority of algorithms for this problem compute a\ndirection in the tangent space, and then use a retraction to move in that\ndirection while staying on the manifold. Unfortunately, the numerical\ncomputation of retractions on the orthogonal manifold always involves some\nexpensive linear algebra operation, such as matrix inversion or matrix\nsquare-root. These operations quickly become expensive as the dimension of the\nmatrices grows. To bypass this limitation, we propose the landing algorithm\nwhich does not involve retractions. The algorithm is not constrained to stay on\nthe manifold but its evolution is driven by a potential energy which\nprogressively attracts it towards the manifold. One iteration of the landing\nalgorithm only involves matrix multiplications, which makes it cheap compared\nto its retraction counterparts. We provide an analysis of the convergence of\nthe algorithm, and demonstrate its promises on large-scale problems, where it\nis faster and less prone to numerical errors than retraction-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 10:12:05 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Ablin", "Pierre", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "2102.07556", "submitter": "Cyrus Mostajeran Dr", "authors": "Simon Heuveline, Salem Said, Cyrus Mostajeran", "title": "Gaussian distributions on Riemannian symmetric spaces in the large N\n  limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Gaussian distributions on certain Riemannian symmetric spaces. In\ncontrast to the Euclidean case, it is challenging to compute the normalization\nfactors of such distributions, which we refer to as partition functions. In\nsome cases, such as the space of Hermitian positive definite matrices or\nhyperbolic space, it is possible to compute them exactly using techniques from\nrandom matrix theory. However, in most cases which are important to\napplications, such as the space of symmetric positive definite (SPD) matrices\nor the Siegel domain, this is only possible numerically. Moreover, when we\nconsider, for instance, high-dimensional SPD matrices, the known algorithms for\ncomputing partition functions can become exceedingly slow. Motivated by notions\nfrom theoretical physics, we will discuss how to approximate the partition\nfunctions in the large $N$ limit: an approximation that gets increasingly\nbetter as the dimension of the underlying symmetric space (more precisely, its\nrank) gets larger. We will give formulas for leading order terms in the case of\nSPD matrices and related spaces. Furthermore, we will characterize the large\n$N$ limit of the Siegel domain through a singular integral equation arising as\na saddle-point equation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 13:52:12 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 16:27:05 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Heuveline", "Simon", ""], ["Said", "Salem", ""], ["Mostajeran", "Cyrus", ""]]}, {"id": "2102.07579", "submitter": "The Tien Mai", "authors": "The Tien Mai", "title": "Efficient Bayesian reduced rank regression using Langevin Monte Carlo\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of Bayesian reduced rank regression is considered in this paper.\nWe propose, for the first time, to use Langevin Monte Carlo method in this\nproblem. A spectral scaled Student prior distrbution is used to exploit the\nunderlying low-rank structure of the coefficient matrix. We show that our\nalgorithms are significantly faster than the Gibbs sampler in high-dimensional\nsetting. Simulation results show that our proposed algorithms for Bayesian\nreduced rank regression are comparable to the state-of-the-art method where the\nrank is chosen by cross validation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 14:39:51 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Mai", "The Tien", ""]]}, {"id": "2102.07612", "submitter": "Ath\\'ena\\\"is Gautier", "authors": "Ath\\'ena\\\"is Gautier, David Ginsbourger, Guillaume Pirot", "title": "Goal-oriented adaptive sampling under random field modelling of response\n  probability distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the study of natural and artificial complex systems, responses that are\nnot completely determined by the considered decision variables are commonly\nmodelled probabilistically, resulting in response distributions varying across\ndecision space. We consider cases where the spatial variation of these response\ndistributions does not only concern their mean and/or variance but also other\nfeatures including for instance shape or uni-modality versus multi-modality.\nOur contributions build upon a non-parametric Bayesian approach to modelling\nthe thereby induced fields of probability distributions, and in particular to a\nspatial extension of the logistic Gaussian model.\n  The considered models deliver probabilistic predictions of response\ndistributions at candidate points, allowing for instance to perform\n(approximate) posterior simulations of probability density functions, to\njointly predict multiple moments and other functionals of target distributions,\nas well as to quantify the impact of collecting new samples on the state of\nknowledge of the distribution field of interest. In particular, we introduce\nadaptive sampling strategies leveraging the potential of the considered random\ndistribution field models to guide system evaluations in a goal-oriented way,\nwith a view towards parsimoniously addressing calibration and related problems\nfrom non-linear (stochastic) inversion and global optimisation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 15:55:23 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:42:47 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Gautier", "Ath\u00e9na\u00efs", ""], ["Ginsbourger", "David", ""], ["Pirot", "Guillaume", ""]]}, {"id": "2102.07720", "submitter": "Vittorio Romaniello", "authors": "Saifuddin Syed, Vittorio Romaniello, Trevor Campbell, Alexandre\n  Bouchard-C\\^ot\\'e", "title": "Parallel Tempering on Optimized Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel tempering (PT) is a class of Markov chain Monte Carlo algorithms\nthat constructs a path of distributions annealing between a tractable reference\nand an intractable target, and then interchanges states along the path to\nimprove mixing in the target. The performance of PT depends on how quickly a\nsample from the reference distribution makes its way to the target, which in\nturn depends on the particular path of annealing distributions. However, past\nwork on PT has used only simple paths constructed from convex combinations of\nthe reference and target log-densities. This paper begins by demonstrating that\nthis path performs poorly in the setting where the reference and target are\nnearly mutually singular. To address this issue, we expand the framework of PT\nto general families of paths, formulate the choice of path as an optimization\nproblem that admits tractable gradient estimates, and propose a flexible new\nfamily of spline interpolation paths for use in practice. Theoretical and\nempirical results both demonstrate that our proposed methodology breaks\npreviously-established upper performance limits for traditional paths.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:15:45 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 17:04:23 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Syed", "Saifuddin", ""], ["Romaniello", "Vittorio", ""], ["Campbell", "Trevor", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""]]}, {"id": "2102.07771", "submitter": "Cyrus Mostajeran Dr", "authors": "Quinten Tupker, Salem Said, Cyrus Mostajeran", "title": "Online learning of Riemannian hidden Markov models in homogeneous\n  Hadamard spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models with observations in a Euclidean space play an important\nrole in signal and image processing. Previous work extending to models where\nobservations lie in Riemannian manifolds based on the Baum-Welch algorithm\nsuffered from high memory usage and slow speed. Here we present an algorithm\nthat is online, more accurate, and offers dramatic improvements in speed and\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 17:30:11 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 16:30:10 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tupker", "Quinten", ""], ["Said", "Salem", ""], ["Mostajeran", "Cyrus", ""]]}, {"id": "2102.07850", "submitter": "James Thornton Mr", "authors": "Adrien Corenflos, James Thornton, George Deligiannidis, Arnaud Doucet", "title": "Differentiable Particle Filtering via Entropy-Regularized Optimal\n  Transport", "comments": "9 pages of content + 11 pages supplementary, accepted for oral at\n  ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Particle Filtering (PF) methods are an established class of procedures for\nperforming inference in non-linear state-space models. Resampling is a key\ningredient of PF, necessary to obtain low variance likelihood and states\nestimates. However, traditional resampling methods result in PF-based loss\nfunctions being non-differentiable with respect to model and PF parameters. In\na variational inference context, resampling also yields high variance gradient\nestimates of the PF-based evidence lower bound. By leveraging optimal transport\nideas, we introduce a principled differentiable particle filter and provide\nconvergence results. We demonstrate this novel method on a variety of\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 21:05:33 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 10:23:44 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 12:25:38 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Corenflos", "Adrien", ""], ["Thornton", "James", ""], ["Deligiannidis", "George", ""], ["Doucet", "Arnaud", ""]]}, {"id": "2102.07951", "submitter": "Boulbaba Ben Amor Prof.", "authors": "Boulbaba Ben Amor, Sylvain Arguill\\`ere and Ling Shao", "title": "ResNet-LDDMM: Advancing the LDDMM Framework Using Deep Residual Networks", "comments": "Submitted to T-PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In deformable registration, the geometric framework - large deformation\ndiffeomorphic metric mapping or LDDMM, in short - has inspired numerous\ntechniques for comparing, deforming, averaging and analyzing shapes or images.\nGrounded in flows, which are akin to the equations of motion used in fluid\ndynamics, LDDMM algorithms solve the flow equation in the space of plausible\ndeformations, i.e. diffeomorphisms. In this work, we make use of deep residual\nneural networks to solve the non-stationary ODE (flow equation) based on a\nEuler's discretization scheme. The central idea is to represent time-dependent\nvelocity fields as fully connected ReLU neural networks (building blocks) and\nderive optimal weights by minimizing a regularized loss function. Computing\nminimizing paths between deformations, thus between shapes, turns to find\noptimal network parameters by back-propagating over the intermediate building\nblocks. Geometrically, at each time step, ResNet-LDDMM searches for an optimal\npartition of the space into multiple polytopes, and then computes optimal\nvelocity vectors as affine transformations on each of these polytopes. As a\nresult, different parts of the shape, even if they are close (such as two\nfingers of a hand), can be made to belong to different polytopes, and therefore\nbe moved in different directions without costing too much energy. Importantly,\nwe show how diffeomorphic transformations, or more precisely bilipshitz\ntransformations, are predicted by our algorithm. We illustrate these ideas on\ndiverse registration problems of 3D shapes under complex topology-preserving\ntransformations. We thus provide essential foundations for more advanced shape\nvariability analysis under a novel joint geometric-neural networks\nRiemannian-like framework, i.e. ResNet-LDDMM.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 04:07:13 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Amor", "Boulbaba Ben", ""], ["Arguill\u00e8re", "Sylvain", ""], ["Shao", "Ling", ""]]}, {"id": "2102.08037", "submitter": "Thomas Viehmann", "authors": "Thomas Viehmann", "title": "Numerically more stable computation of the p-values for the two-sample\n  Kolmogorov-Smirnov test", "comments": null, "journal-ref": null, "doi": null, "report-no": "2021-1", "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The two-sample Kolmogorov-Smirnov test is a widely used statistical test for\ndetecting whether two samples are likely to come from the same distribution.\nImplementations typically recur on an article of Hodges from 1957. The advances\nin computation speed make it feasible to compute exact p-values for a much\nlarger range of problem sizes, but these run into numerical stability problems\nfrom floating point operations. We provide a simple transformation of the\ndefining recurrence for the two-side two-sample KS test that avoids this.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 09:20:22 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Viehmann", "Thomas", ""]]}, {"id": "2102.08057", "submitter": "James Hodgson", "authors": "James Hodgson, Adam M. Johansen, Murray Pollock", "title": "Unbiased simulation of rare events in continuous time", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For rare events described in terms of Markov processes, truly unbiased\nestimation of the rare event probability generally requires the avoidance of\nnumerical approximations of the Markov process. Recent work in the exact and\n$\\varepsilon$-strong simulation of diffusions, which can be used to almost\nsurely constrain sample paths to a given tolerance, suggests one way to do\nthis. We specify how such algorithms can be combined with the classical\nmultilevel splitting method for rare event simulation. This provides unbiased\nestimations of the probability in question. We discuss the practical\nfeasibility of the algorithm with reference to existing $\\varepsilon$-strong\nmethods and provide proof-of-concept numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 10:10:53 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Hodgson", "James", ""], ["Johansen", "Adam M.", ""], ["Pollock", "Murray", ""]]}, {"id": "2102.08232", "submitter": "Mark de Rooij", "authors": "Mark de Rooij and Patrick J. F. Groenen", "title": "The MELODIC family for simultaneous binary logistic regression in a\n  reduced space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logistic regression is a commonly used method for binary classification.\nResearchers often have more than a single binary response variable and\nsimultaneous analysis is beneficial because it provides insight into the\ndependencies among response variables as well as between the predictor\nvariables and the responses. Moreover, in such a simultaneous analysis the\nequations can lend each other strength, which might increase predictive\naccuracy. In this paper, we propose the MELODIC family for simultaneous binary\nlogistic regression modeling. In this family, the regression models are defined\nin a Euclidean space of reduced dimension, based on a distance rule. The model\nmay be interpreted in terms of logistic regression coefficients or in terms of\na biplot. We discuss a fast iterative majorization (or MM) algorithm for\nparameter estimation. Two applications are shown in detail: one relating\npersonality characteristics to drug consumption profiles and one relating\npersonality characteristics to depressive and anxiety disorders. We present a\nthorough comparison of our MELODIC family with alternative approaches for\nmultivariate binary data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 15:47:20 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["de Rooij", "Mark", ""], ["Groenen", "Patrick J. F.", ""]]}, {"id": "2102.08877", "submitter": "Suchit Mehrotra", "authors": "Suchit Mehrotra, Arnab Maity", "title": "Variational Inference for Shrinkage Priors: The R package vir", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present vir, an R package for variational inference with shrinkage priors.\nOur package implements variational and stochastic variational algorithms for\nlinear and probit regression models, the use of which is a common first step in\nmany applied analyses. We review variational inference and show how the\nderivation for a Gibbs sampler can be easily modified to derive a corresponding\nvariational or stochastic variational algorithm. We provide simulations showing\nthat, at least for a normal linear model, variational inference can lead to\nsimilar uncertainty quantification as the corresponding Gibbs samplers, while\nestimating the model parameters at a fraction of the computational cost. Our\ntiming experiments show situations in which our algorithms converge faster than\nthe frequentist LASSO implementations in glmnet while simultaneously providing\nsuperior parameter estimation and variable selection. Hence, our package can be\nutilized to quickly explore different combinations of predictors in a linear\nmodel, while providing accurate uncertainty quantification in many applied\nsituations. The package is implemented natively in R and RcppEigen, which has\nthe benefit of bypassing the substantial operating system specific overhead of\nlinking external libraries to work efficiently with R.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:15:34 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Mehrotra", "Suchit", ""], ["Maity", "Arnab", ""]]}, {"id": "2102.09008", "submitter": "Suchit Mehrotra", "authors": "Suchit Mehrotra, Halley Brantley, Peter Onglao, Patricia Bata, Roland\n  Romero, Jacob Westman, Lauren Bangerter, Arnab Maity", "title": "Divide-and-Conquer MCMC for Multivariate Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of large scale medical claims data has the potential to improve\nquality of care by generating insights which can be used to create tailored\nmedical programs. In particular, the multivariate probit model can be used to\ninvestigate the correlation between multiple binary responses of interest in\nsuch data, e.g. the presence of multiple chronic conditions. Bayesian modeling\nis well suited to such analyses because of the automatic uncertainty\nquantification provided by the posterior distribution. A complicating factor is\nthat large medical claims datasets often do not fit in memory, which renders\nthe estimation of the posterior using traditional Markov Chain Monte Carlo\n(MCMC) methods computationally infeasible. To address this challenge, we extend\nexisting divide-and-conquer MCMC algorithms to the multivariate probit model,\ndemonstrating, via simulation, that they should be preferred over mean-field\nvariational inference when the estimation of the latent correlation structure\nbetween binary responses is of primary interest. We apply this algorithm to a\nlarge database of de-identified Medicare Advantage claims from a single large\nUS health insurance provider, where we find medically meaningful groupings of\ncommon chronic conditions and asses the impact of the urban-rural health gap by\nidentifying underutilized provider specialties in rural areas.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 20:02:17 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 20:36:37 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 17:19:34 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Mehrotra", "Suchit", ""], ["Brantley", "Halley", ""], ["Onglao", "Peter", ""], ["Bata", "Patricia", ""], ["Romero", "Roland", ""], ["Westman", "Jacob", ""], ["Bangerter", "Lauren", ""], ["Maity", "Arnab", ""]]}, {"id": "2102.09042", "submitter": "Ali Hasan", "authors": "Ali Hasan, Khalil Elkhalil, Joao M. Pereira, Sina Farsiu, Jose H.\n  Blanchet, Vahid Tarokh", "title": "Deep Extreme Value Copulas for Estimation and Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new method for modeling the distribution function of high\ndimensional extreme value distributions. The Pickands dependence function\nmodels the relationship between the covariates in the tails, and we learn this\nfunction using a neural network that is designed to satisfy its required\nproperties. Moreover, we present new methods for recovering the spectral\nrepresentation of extreme distributions and propose a generative model for\nsampling from extreme copulas. Numerical examples are provided demonstrating\nthe efficacy and promise of our proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 22:02:47 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Hasan", "Ali", ""], ["Elkhalil", "Khalil", ""], ["Pereira", "Joao M.", ""], ["Farsiu", "Sina", ""], ["Blanchet", "Jose H.", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2102.09299", "submitter": "Pavel Vesel\\'y", "authors": "Graham Cormode, Abhinav Mishra, Joseph Ross, Pavel Vesel\\'y", "title": "Theory meets Practice at the Median: a worst case comparison of relative\n  error quantile algorithms", "comments": "Updated experiments, improved presentation. To appear in KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the distribution and quantiles of data is a foundational task in\ndata mining and data science. We study algorithms which provide accurate\nresults for extreme quantile queries using a small amount of space, thus\nhelping to understand the tails of the input distribution. Namely, we focus on\ntwo recent state-of-the-art solutions: $t$-digest and ReqSketch. While\n$t$-digest is a popular compact summary which works well in a variety of\nsettings, ReqSketch comes with formal accuracy guarantees at the cost of its\nsize growing as new observations are inserted. In this work, we provide insight\ninto which conditions make one preferable to the other. Namely, we show how to\nconstruct inputs for $t$-digest that induce an almost arbitrarily large error\nand demonstrate that it fails to provide accurate results even on i.i.d.\nsamples from a highly non-uniform distribution. We propose practical\nimprovements to ReqSketch, making it faster than $t$-digest, while its error\nstays bounded on any instance. Still, our results confirm that $t$-digest\nremains more accurate on the ``non-adversarial'' data encountered in practice.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:18:44 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:13:02 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Cormode", "Graham", ""], ["Mishra", "Abhinav", ""], ["Ross", "Joseph", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "2102.09634", "submitter": "Lifeth \\'Alvarez", "authors": "Alvarez Lifeth", "title": "Modeling epigenetic evolutionary algorithms: An approach based on the\n  epigenetic regulation process", "comments": "Master in Computer Systems Engineering. Universidad Nacional de\n  Colombia. Thesis, 132 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological processes have been the source of inspiration for heuristic\nmethods that generate high-quality solutions to solve optimization and search\nproblems. This thesis presents an epigenetic technique for Evolutionary\nAlgorithms, inspired by the epigenetic regulation process, a mechanism to\nbetter understand the ability of individuals to adapt and learn from the\nenvironment. Epigenetic regulation comprises biological mechanisms by which\nsmall molecules, also known as epigenetic tags, are attached to or removed from\na particular gene, affecting the phenotype. Five fundamental elements form the\nbasis of the designed technique: first, a metaphorical representation of\nEpigenetic Tags as binary strings; second, a layer on chromosome top structure\nused to bind the tags (the Epigenotype layer); third, a Marking Function to\nadd, remove, and modify tags; fourth, an Epigenetic Growing Function that acts\nlike an interpreter, or decoder of the tags located over the alleles, in such a\nway that the phenotypic variations can be reflected when evaluating the\nindividuals; and fifth, a tags inheritance mechanism. A set of experiments are\nperformed for determining the applicability of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 21:51:50 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Lifeth", "Alvarez", ""]]}, {"id": "2102.09964", "submitter": "Adrien Corenflos", "authors": "Adrien Corenflos, Zheng Zhao, Simo S\\\"arkk\\\"a", "title": "Temporal Gaussian Process Regression in Logarithmic Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this article is to present a novel parallelization method for\ntemporal Gaussian process (GP) regression problems. The method allows for\nsolving GP regression problems in logarithmic O(log N) time, where N is the\nnumber of time steps. Our approach uses the state-space representation of GPs\nwhich in its original form allows for linear O(N) time GP regression by\nleveraging the Kalman filtering and smoothing methods. By using a recently\nproposed parallelization method for Bayesian filters and smoothers, we are able\nto reduce the linear computational complexity of the temporal GP regression\nproblems into logarithmic span complexity. This ensures logarithmic time\ncomplexity when run on parallel hardware such as a graphics processing unit\n(GPU). We experimentally demonstrate the computational benefits on simulated\nand real datasets via our open-source implementation leveraging the GPflow\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 14:57:17 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 07:30:40 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 19:36:16 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 07:23:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Corenflos", "Adrien", ""], ["Zhao", "Zheng", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2102.10388", "submitter": "Kris Sankaran", "authors": "Kris Sankaran", "title": "Measuring the Stability of Learned Features", "comments": "39 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many modern datasets don't fit neatly into $n \\times p$ matrices, but most\ntechniques for measuring statistical stability expect rectangular data. We\nstudy methods for stability assessment on non-rectangular data, using\nstatistical learning algorithms to extract rectangular latent features. We\ndesign controlled simulations to characterize the power and practicality of\ncompeting approaches. This motivates new strategies for visualizing feature\nstability. Our stability curves supplement the direct analysis, providing\ninformation about the reliability of inferences based on learned features.\nFinally, we illustrate our approach using a spatial proteomics dataset, where\nmachine learning tools can augment the scientist's workflow, but where\nguarantees of statistical reproducibility are still central. Our raw data,\npackaged code, and experimental outputs are publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 16:57:19 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Sankaran", "Kris", ""]]}, {"id": "2102.10583", "submitter": "Jinglai Li", "authors": "Hongqiao Wang, Ziqiao Ao, Tengchao Yu and Jinglai Li", "title": "Inverse Gaussian Process regression for likelihood-free inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider Bayesian inference problems with intractable\nlikelihood functions. We present a method to compute an approximate of the\nposterior with a limited number of model simulations. The method features an\ninverse Gaussian Process regression (IGPR), i.e., one from the output of a\nsimulation model to the input of it. Within the method, we provide an adaptive\nalgorithm with a tempering procedure to construct the approximations of the\nmarginal posterior distributions. With examples we demonstrate that IGPR has a\ncompetitive performance compared to some commonly used algorithms, especially\nin terms of statistical stability and computational efficiency, while the price\nto pay is that it can only compute a weighted Gaussian approximation of the\nmarginal posteriors.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 11:04:10 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wang", "Hongqiao", ""], ["Ao", "Ziqiao", ""], ["Yu", "Tengchao", ""], ["Li", "Jinglai", ""]]}, {"id": "2102.10773", "submitter": "Vassilis Digalakis Jr.", "authors": "Dimitris Bertsimas, Vassilis Digalakis Jr., Michael Linghzi Li, Omar\n  Skali Lami", "title": "Slowly Varying Regression under Sparsity", "comments": "Submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of parameter estimation in slowly varying regression\nmodels with sparsity constraints. We formulate the problem as a mixed integer\noptimization problem and demonstrate that it can be reformulated exactly as a\nbinary convex optimization problem through a novel exact relaxation. The\nrelaxation utilizes a new equality on Moore-Penrose inverses that convexifies\nthe non-convex objective function while coinciding with the original objective\non all feasible binary points. This allows us to solve the problem\nsignificantly more efficiently and to provable optimality using a cutting\nplane-type algorithm. We develop a highly optimized implementation of such\nalgorithm, which substantially improves upon the asymptotic computational\ncomplexity of a straightforward implementation. We further develop a heuristic\nmethod that is guaranteed to produce a feasible solution and, as we empirically\nillustrate, generates high quality warm-start solutions for the binary\noptimization problem. We show, on both synthetic and real-world datasets, that\nthe resulting algorithm outperforms competing formulations in comparable times\nacross a variety of metrics including out-of-sample predictive performance,\nsupport recovery accuracy, and false positive rate. The algorithm enables us to\ntrain models with 10,000s of parameters, is robust to noise, and able to\neffectively capture the underlying slowly changing support of the data\ngenerating process.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 04:51:44 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Digalakis", "Vassilis", "Jr."], ["Li", "Michael Linghzi", ""], ["Lami", "Omar Skali", ""]]}, {"id": "2102.11086", "submitter": "Chris J. Maddison", "authors": "Yangjun Ruan, Karen Ullrich, Daniel Severo, James Townsend, Ashish\n  Khisti, Arnaud Doucet, Alireza Makhzani, Chris J. Maddison", "title": "Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models have been successfully applied in lossless compression\nwith the bits-back coding algorithm. However, bits-back suffers from an\nincrease in the bitrate equal to the KL divergence between the approximate\nposterior and the true posterior. In this paper, we show how to remove this gap\nasymptotically by deriving bits-back coding algorithms from tighter variational\nbounds. The key idea is to exploit extended space representations of Monte\nCarlo estimators of the marginal likelihood. Naively applied, our schemes would\nrequire more initial bits than the standard bits-back coder, but we show how to\ndrastically reduce this additional cost with couplings in the latent space.\nWhen parallel architectures can be exploited, our coders can achieve better\nrates than bits-back with little additional cost. We demonstrate improved\nlossless compression rates in a variety of settings, especially in\nout-of-distribution or sequential data compression.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 14:58:01 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 01:38:37 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ruan", "Yangjun", ""], ["Ullrich", "Karen", ""], ["Severo", "Daniel", ""], ["Townsend", "James", ""], ["Khisti", "Ashish", ""], ["Doucet", "Arnaud", ""], ["Makhzani", "Alireza", ""], ["Maddison", "Chris J.", ""]]}, {"id": "2102.11297", "submitter": "Jeffrey Wong", "authors": "Jeffrey Wong, Eskil Forsell, Randall Lewis, Tobias Mao and Matthew\n  Wardrop", "title": "You Only Compress Once: Optimal Data Compression for Estimating Linear\n  Models", "comments": "v2: Further reduce matrix algebra and fix typo in Section 5.3.3.\n  Improve the relationships across Section 5.3.1, 5.3.2, and 5.3.3. v3: Change\n  citation styles and update Section 5.3.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear models are used in online decision making, such as in machine\nlearning, policy algorithms, and experimentation platforms. Many engineering\nsystems that use linear models achieve computational efficiency through\ndistributed systems and expert configuration. While there are strengths to this\napproach, it is still difficult to have an environment that enables researchers\nto interactively iterate and explore data and models, as well as leverage\nanalytics solutions from the open source community. Consequently, innovation\ncan be blocked.\n  Conditionally sufficient statistics is a unified data compression and\nestimation strategy that is useful for the model development process, as well\nas the engineering deployment process. The strategy estimates linear models\nfrom compressed data without loss on the estimated parameters and their\ncovariances, even when errors are autocorrelated within clusters of\nobservations. Additionally, the compression preserves almost all interactions\nwith the the original data, unlocking better productivity for both researchers\nand engineering systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 19:00:18 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 18:07:14 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 17:34:31 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wong", "Jeffrey", ""], ["Forsell", "Eskil", ""], ["Lewis", "Randall", ""], ["Mao", "Tobias", ""], ["Wardrop", "Matthew", ""]]}, {"id": "2102.11425", "submitter": "Francesco Denti", "authors": "Francesco Denti", "title": "intRinsic: an R package for model-based estimation of the intrinsic\n  dimension of a dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The estimation of the intrinsic dimension of a dataset is a fundamental step\nin most dimensionality reduction techniques. This article illustrates\nintRinsic, an R package that implements novel state-of-the-art likelihood-based\nestimators of the intrinsic dimension of a dataset. In detail, the methods\nincluded in this package are the TWO-NN, Gride, and Hidalgo models. To allow\nthese novel estimators to be easily accessible, the package contains a few\nhigh-level, intuitive functions that rely on a broader set of efficient,\nlow-level routines. intRinsic encompasses models that fall into two categories:\nhomogeneous and heterogeneous intrinsic dimension estimators. The first\ncategory contains the TWO-NN and Gride models. The functions dedicated to these\ntwo methods carry out inference under both the frequentist and Bayesian\nframeworks. In the second category we find Hidalgo, a Bayesian mixture model,\nfor which an efficient Gibbs sampler is implemented. After discussing the\ntheoretical background, we demonstrate the performance of the models on\nsimulated datasets. This way, we can assess the results by comparing them with\nthe ground truth. Then, we employ the package to study the intrinsic dimension\nof the Alon dataset, obtained from a famous microarray experiment. We show how\nthe estimation of homogeneous and heterogeneous intrinsic dimensions allows us\nto gain valuable insights about the topological structure of a dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 00:09:22 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Denti", "Francesco", ""]]}, {"id": "2102.11575", "submitter": "Juan Kuntz", "authors": "Juan Kuntz, Francesca R. Crucinio, Adam M. Johansen", "title": "Product-form estimators: exploiting independence to scale up Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of Monte Carlo estimators for product-form target\ndistributions that aim to overcome the rapid growth of variance with dimension\noften observed for standard estimators. We identify them with a class of\ngeneralized U-Statistics, and thus establish their unbiasedness, consistency,\nand asymptotic normality. Moreover, we show that they achieve lower variances\nthan their conventional counterparts given the same number of samples drawn\nfrom the target, investigate the gap in variance via several examples, and\nidentify the situations in which the difference is most, and least, pronounced.\nWe further study the estimators' computational cost and delineate the settings\nin which they are most efficient. We illustrate their utility beyond the\nsetting of product-form distributions by detailing two simple extensions (one\nto targets that are mixtures of product-form distributions and another to\ntargets that are absolutely continuous with respect to product-form\ndistributions) and conclude by discussing further possible uses.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:27:30 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:54:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kuntz", "Juan", ""], ["Crucinio", "Francesca R.", ""], ["Johansen", "Adam M.", ""]]}, {"id": "2102.12230", "submitter": "Kody Law", "authors": "Jeremy Heng, Ajay Jasra, Kody J. H. Law, Alexander Tarakanov", "title": "On Unbiased Estimation for Discretized Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider computing expectations w.r.t. probability\nmeasures which are subject to discretization error. Examples include partially\nobserved diffusion processes or inverse problems, where one may have to\ndiscretize time and/or space, in order to practically work with the probability\nof interest. Given access only to these discretizations, we consider the\nconstruction of unbiased Monte Carlo estimators of expectations w.r.t. such\ntarget probability distributions. It is shown how to obtain such estimators\nusing a novel adaptation of randomization schemes and Markov simulation\nmethods. Under appropriate assumptions, these estimators possess finite\nvariance and finite expected cost. There are two important consequences of this\napproach: (i) unbiased inference is achieved at the canonical complexity rate,\nand (ii) the resulting estimators can be generated independently, thereby\nallowing strong scaling to arbitrarily many parallel processors. Several\nalgorithms are presented, and applied to some examples of Bayesian inference\nproblems, with both simulated and real observed data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:48:07 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Heng", "Jeremy", ""], ["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Tarakanov", "Alexander", ""]]}, {"id": "2102.12261", "submitter": "Kody Law", "authors": "Kody J. H. Law and Vitaly Zankin", "title": "Sparse online variational Bayesian regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers variational Bayesian inference as an inexpensive and\nscalable alternative to a fully Bayesian approach in the context of\nsparsity-promoting priors. In particular, the priors considered arise from\nscale mixtures of Normal distributions with a generalized inverse Gaussian\nmixing distribution. This includes the variational Bayesian LASSO as an\ninexpensive and scalable alternative to the Bayesian LASSO introduced in [56].\nIt also includes priors which more strongly promote sparsity. For linear models\nthe method requires only the iterative solution of deterministic least squares\nproblems. Furthermore, for $n\\rightarrow \\infty$ data points and p unknown\ncovariates the method can be implemented exactly online with a cost of O(p$^3$)\nin computation and O(p$^2$) in memory. For large p an approximation is able to\nachieve promising results for a cost of O(p) in both computation and memory.\nStrategies for hyper-parameter tuning are also considered. The method is\nimplemented for real and simulated data. It is shown that the performance in\nterms of variable selection and uncertainty quantification of the variational\nBayesian LASSO can be comparable to the Bayesian LASSO for problems which are\ntractable with that method, and for a fraction of the cost. The present method\ncomfortably handles n = p = 131,073 on a laptop in minutes, and n = 10$^5$, p =\n10$^6$ overnight.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 12:49:42 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Law", "Kody J. H.", ""], ["Zankin", "Vitaly", ""]]}, {"id": "2102.12290", "submitter": "Anass El Yaagoubi Bourakna", "authors": "Anass El Yaagoubi Bourakna, Marco Pinto, Norbert Fortin, Hernando\n  Ombao", "title": "Smooth Online Parameter Estimation for time varying VAR models with\n  application to rat's LFP data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series data appear often as realizations of non-stationary\nprocesses where the covariance matrix or spectral matrix smoothly evolve over\ntime. Most of the current approaches estimate the time-varying spectral\nproperties only retrospectively - that is, after the entire data has been\nobserved. Retrospective estimation is a major limitation in many adaptive\ncontrol applications where it is important to estimate these properties and\ndetect changes in the system as they happen in real-time. One major obstacle in\nonline estimation is the computational cost due to the high-dimensionality of\nthe parameters. Existing methods such as the Kalman filter or local least\nsquares are feasible. However, they are not always suitable because they\nprovide noisy estimates and can become prohibitively costly as the dimension of\nthe time series increases. In our brain signal application, it is critical to\ndevelop a robust method that can estimate, in real-time, the properties of the\nunderlying stochastic process, in particular, the spectral brain connectivity\nmeasures. For these reasons we propose a new smooth online parameter estimation\napproach (SOPE) that has the ability to control for the smoothness of the\nestimates with a reasonable computational complexity. Consequently, the models\nare fit in real-time even for high dimensional time series. We demonstrate that\nour proposed SOPE approach is as good as the Kalman filter in terms of\nmean-squared error for small dimensions. However, unlike the Kalman filter, the\nSOPE has lower computational cost and hence scalable for higher dimensions.\nFinally, we apply the SOPE method to a rat's local field potential data during\na hippocampus-dependent sequence-memory task. As demonstrated in the video, the\nproposed SOPE method is able to capture the dynamics of the connectivity as the\nrat performs the sequence of non-spatial working memory tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 13:55:30 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 16:52:12 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bourakna", "Anass El Yaagoubi", ""], ["Pinto", "Marco", ""], ["Fortin", "Norbert", ""], ["Ombao", "Hernando", ""]]}, {"id": "2102.12478", "submitter": "W.J. Handley", "authors": "Justin Alsing and Will Handley", "title": "Nested sampling with any prior you like", "comments": "5 pages, 2 figures, Published as an MNRAS letter", "journal-ref": "MNRAS 505, L95-L99 (2021)", "doi": "10.1093/mnrasl/slab057", "report-no": null, "categories": "astro-ph.IM astro-ph.CO physics.data-an stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested sampling is an important tool for conducting Bayesian analysis in\nAstronomy and other fields, both for sampling complicated posterior\ndistributions for parameter inference, and for computing marginal likelihoods\nfor model comparison. One technical obstacle to using nested sampling in\npractice is the requirement (for most common implementations) that prior\ndistributions be provided in the form of transformations from the unit\nhyper-cube to the target prior density. For many applications - particularly\nwhen using the posterior from one experiment as the prior for another - such a\ntransformation is not readily available. In this letter we show that parametric\nbijectors trained on samples from a desired prior density provide a\ngeneral-purpose method for constructing transformations from the uniform base\ndensity to a target prior, enabling the practical use of nested sampling under\narbitrary priors. We demonstrate the use of trained bijectors in conjunction\nwith nested sampling on a number of examples from cosmology.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:45:13 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 21:02:29 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 15:37:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Alsing", "Justin", ""], ["Handley", "Will", ""]]}, {"id": "2102.12535", "submitter": "Panpan Zhang", "authors": "Panpan Zhang and Xiaojing Wang", "title": "Several topological indices of random caterpillars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In chemical graph theory, caterpillar trees have been an appealing model to\nrepresent the molecular structures of benzenoid hydrocarbon. Meanwhile,\ntopological index has been thought of as a powerful tool for modeling\nquantitative structure-property relationship and quantitative\nstructure-activity between molecules in chemical compounds. In this article, we\nconsider a class of caterpillar trees that are incorporated with randomness,\ncalled random caterpillars, and investigate several popular topological indices\nof this random class, including Zagreb index, Randi\\'{c} index and Wiener\nindex, etc. Especially, a central limit theorem is developed for the asymptotic\ndistribution of the Zagreb index of random caterpillars.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 20:03:06 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zhang", "Panpan", ""], ["Wang", "Xiaojing", ""]]}, {"id": "2102.12976", "submitter": "Eric Chuu", "authors": "Eric Chuu, Debdeep Pati, Anirban Bhattacharya", "title": "A Hybrid Approximation to the Marginal Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the marginal likelihood or evidence is one of the core challenges\nin Bayesian analysis. While there are many established methods for estimating\nthis quantity, they predominantly rely on using a large number of posterior\nsamples obtained from a Markov Chain Monte Carlo (MCMC) algorithm. As the\ndimension of the parameter space increases, however, many of these methods\nbecome prohibitively slow and potentially inaccurate. In this paper, we propose\na novel method in which we use the MCMC samples to learn a high probability\npartition of the parameter space and then form a deterministic approximation\nover each of these partition sets. This two-step procedure, which constitutes\nboth a probabilistic and a deterministic component, is termed a Hybrid\napproximation to the marginal likelihood. We demonstrate its versatility in a\nplethora of examples with varying dimension and sample size, and we also\nhighlight the Hybrid approximation's effectiveness in situations where there is\neither a limited number or only approximate MCMC samples available.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 05:09:41 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Chuu", "Eric", ""], ["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "2102.13209", "submitter": "Fotios Petropoulos", "authors": "Fotios Petropoulos and Yael Grushka-Cockayne", "title": "Fast and frugal time series forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the years, families of forecasting models, such as the exponential\nsmoothing family and Autoregressive Integrated Moving Average, have expanded to\ncontain multiple possible forms and forecasting profiles. In this paper, we\nquestion the need to consider such large families of models. We argue that\nparsimoniously identifying suitable subsets of models will not decrease the\nforecasting accuracy nor will it reduce the ability to estimate the forecast\nuncertainty. We propose a framework that balances forecasting performance\nversus computational cost, resulting in a set of reduced families of models and\nempirically demonstrate this trade-offs. We translate computational benefits to\nmonetary cost savings and discuss the implications of our results in the\ncontext of large retailers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 22:00:55 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Petropoulos", "Fotios", ""], ["Grushka-Cockayne", "Yael", ""]]}, {"id": "2102.13245", "submitter": "Tiangang Cui", "authors": "Tiangang Cui and Olivier Zahm", "title": "Data-Free Likelihood-Informed Dimension Reduction of Bayesian Inverse\n  Problems", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/abeafb", "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Identifying a low-dimensional informed parameter subspace offers a viable\npath to alleviating the dimensionality challenge in the sampled-based solution\nto large-scale Bayesian inverse problems. This paper introduces a novel\ngradient-based dimension reduction method in which the informed subspace does\nnot depend on the data. This permits an online-offline computational strategy\nwhere the expensive low-dimensional structure of the problem is detected in an\noffline phase, meaning before observing the data. This strategy is particularly\nrelevant for multiple inversion problems as the same informed subspace can be\nreused. The proposed approach allows controlling the approximation error (in\nexpectation over the data) of the posterior distribution. We also present\nsampling strategies that exploit the informed subspace to draw efficiently\nsamples from the exact posterior distribution. The method is successfully\nillustrated on two numerical examples: a PDE-based inverse problem with a\nGaussian process prior and a tomography problem with Poisson data and a\nBesov-$\\mathcal{B}^2_{11}$ prior.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 00:31:19 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Cui", "Tiangang", ""], ["Zahm", "Olivier", ""]]}, {"id": "2102.13287", "submitter": "Xiaoping Shi", "authors": "Xiaoping Shi, Meiqian Chen and Yucheng Dong", "title": "Exploring the space-time pattern of log-transformed infectious count of\n  COVID-19: a clustering-segmented autoregressive sigmoid model", "comments": "29 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  At the end of April 20, 2020, there were only a few new COVID-19 cases\nremaining in China, whereas the rest of the world had shown increases in the\nnumber of new cases. It is of extreme importance to develop an efficient\nstatistical model of COVID-19 spread, which could help in the global fight\nagainst the virus. We propose a clustering-segmented autoregressive sigmoid\n(CSAS) model to explore the space-time pattern of the log-transformed\ninfectious count. Four key characteristics are included in this CSAS model,\nincluding unknown clusters, change points, stretched S-curves, and\nautoregressive terms, in order to understand how this outbreak is spreading in\ntime and in space, to understand how the spread is affected by epidemic control\nstrategies, and to apply the model to updated data from an extended period of\ntime. We propose a nonparametric graph-based clustering method for discovering\ndissimilarity of the curve time series in space, which is justified with\ntheoretical support to demonstrate how the model works under mild and easily\nverified conditions. We propose a very strict purity score that penalizes\noverestimation of clusters. Simulations show that our nonparametric graph-based\nclustering method is faster and more accurate than the parametric clustering\nmethod regardless of the size of data sets. We provide a Bayesian information\ncriterion (BIC) to identify multiple change points and calculate a confidence\ninterval for a mean response. By applying the CSAS model to the collected data,\nwe can explain the differences between prevention and control policies in China\nand selected countries.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 03:08:50 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Shi", "Xiaoping", ""], ["Chen", "Meiqian", ""], ["Dong", "Yucheng", ""]]}, {"id": "2102.13393", "submitter": "Michael Pfarrhofer", "authors": "Manfred M. Fischer, Niko Hauzenberger, Florian Huber, Michael\n  Pfarrhofer", "title": "General Bayesian time-varying parameter VARs for predicting government\n  bond yields", "comments": "JEL: C11, C30, E37, E43; KEYWORDS: Bayesian shrinkage, interest rate\n  forecasting, latent effect modifiers, MCMC sampling, time-varying parameter\n  regression", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-varying parameter (TVP) regressions commonly assume that time-variation\nin the coefficients is determined by a simple stochastic process such as a\nrandom walk. While such models are capable of capturing a wide range of dynamic\npatterns, the true nature of time variation might stem from other sources, or\narise from different laws of motion. In this paper, we propose a flexible TVP\nVAR that assumes the TVPs to depend on a panel of partially latent covariates.\nThe latent part of these covariates differ in their state dynamics and thus\ncapture smoothly evolving or abruptly changing coefficients. To determine which\nof these covariates are important, and thus to decide on the appropriate state\nevolution, we introduce Bayesian shrinkage priors to perform model selection.\nAs an empirical application, we forecast the US term structure of interest\nrates and show that our approach performs well relative to a set of competing\nmodels. We then show how the model can be used to explain structural breaks in\ncoefficients related to the US yield curve.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:02:58 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Fischer", "Manfred M.", ""], ["Hauzenberger", "Niko", ""], ["Huber", "Florian", ""], ["Pfarrhofer", "Michael", ""]]}, {"id": "2102.13518", "submitter": "Thorsten Simon", "authors": "Thomas Muschinski, Georg J. Mayr, Thorsten Simon, Achim Zeileis", "title": "Cholesky-based multivariate Gaussian regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multivariate Gaussian regression is embedded into a general distributional\nregression framework using flexible additive predictors determining all\ndistributional parameters. While this is relatively straightforward for the\nmeans of the multivariate dependent variable, it is more challenging for the\nfull covariance matrix {\\Sigma} due to two main difficulties: (i) ensuring\npositive-definiteness of {\\Sigma} and (ii) regularizing the high model\ncomplexity. Both challenges are addressed by adopting a parameterization of\n{\\Sigma} based on its basic or modified Cholesky decomposition, respectively.\nUnlike the decomposition into variances and a correlation matrix, the Cholesky\ndecomposition guarantees positive-definiteness for any predictor values\nregardless of the distributional dimension. Thus, this enables linking all\ndistributional parameters to flexible predictors without any joint constraints\nthat would substantially complicate other parameterizations. Moreover, this\napproach enables regularization of the flexible additive predictors through\npenalized maximum likelihood or Bayesian estimation as for other distributional\nregression models. Finally, the Cholesky decomposition allows to reduce the\nnumber of parameters when the components of the multivariate dependent variable\nhave a natural order (typically time) and a maximum lag can be assumed for the\ndependencies among the components.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 14:46:35 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Muschinski", "Thomas", ""], ["Mayr", "Georg J.", ""], ["Simon", "Thorsten", ""], ["Zeileis", "Achim", ""]]}, {"id": "2102.13548", "submitter": "Ronaldo Dias", "authors": "Larissa Alves and Ronaldo Dias and Helio S. Migon", "title": "Variational Full Bayes Lasso: Knots Selection in Regression Splines", "comments": "The authors contributed equally to the design and implementation of\n  the research, to the analysis of the results and to the writing of the\n  manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We develop a fully automatic Bayesian Lasso via variational inference. This\nis a scalable procedure for approximating the posterior distribution. Special\nattention is driven to the knot selection in regression spline. In order to\ncarry through our proposal, a full automatic variational Bayesian Lasso, a\nJefferey's prior is proposed for the hyperparameters and a decision theoretical\napproach is introduced to decide if a knot is selected or not. Extensive\nsimulation studies were developed to ensure the effectiveness of the proposed\nalgorithms. The performance of the algorithms were also tested in some real\ndata sets, including data from the world pandemic Covid-19. Again, the\nalgorithms showed a very good performance in capturing the data structure.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 15:41:28 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Alves", "Larissa", ""], ["Dias", "Ronaldo", ""], ["Migon", "Helio S.", ""]]}]