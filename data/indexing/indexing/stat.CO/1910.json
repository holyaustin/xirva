[{"id": "1910.00011", "submitter": "Bin Liu", "authors": "Bin Liu", "title": "Data-Driven Model Set Design for Model Averaged Particle Filter", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with sequential state filtering in the presence of\nnonlinearity, non-Gaussianity and model uncertainty. For this problem, the\nBayesian model averaged particle filter (BMAPF) is perhaps one of the most\nefficient solutions. Major advances of BMAPF have been made, while it still\nlacks a generic and practical approach to design the model set. This paper\nfills in this gap by proposing a generic data-driven method for BMAPF model set\ndesign. Unlike existent methods, the proposed solution does not require any\nprior knowledge on the parameter value of the true model; it only assumes that\na small number of noisy observations are pre-obtained. The Bayesian\noptimization (BO) method is adapted to search the model components, each of\nwhich is associated with a specific segment of the pre-obtained dataset.The\naverage performance of these model components is guaranteed since each one's\nparameter value is elaborately tuned via BO to maximize the marginal\nlikelihood. The diversity in the model components is also ensured, as different\ncomponents match the different segments of the pre-obtained dataset,\nrespectively. Computer simulations are used to demonstrate the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 14:27:51 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 10:21:20 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 08:23:27 GMT"}, {"version": "v4", "created": "Sun, 13 Oct 2019 10:40:43 GMT"}, {"version": "v5", "created": "Mon, 27 Jan 2020 15:05:14 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Liu", "Bin", ""]]}, {"id": "1910.00152", "submitter": "Tianyi Lin", "authors": "Tianyi Lin, Nhat Ho, Marco Cuturi and Michael I. Jordan", "title": "On the Complexity of Approximating Multimarginal Optimal Transport", "comments": "Improve the paper significantly; 39 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximating the multimarginal optimal transport\n(MOT) distance, a generalization of the classical optimal transport distance,\nconsidered here between $m$ discrete probability distributions supported each\non $n$ support points. First, we show that the standard linear programming (LP)\nrepresentation of the MOT problem is not a minimum-cost flow problem when $m\n\\geq 3$. This negative result implies that some combinatorial algorithms, e.g.,\nnetwork simplex method, are not suitable for approximating the MOT problem,\nwhile the worst-case complexity bound for the deterministic interior-point\nalgorithm remains a quantity of $\\tilde{O}(n^{3m})$. We then propose two simple\nand \\textit{deterministic} algorithms for approximating the MOT problem. The\nfirst algorithm, which we refer to as \\textit{multimarginal Sinkhorn}\nalgorithm, is a provably efficient multimarginal generalization of the Sinkhorn\nalgorithm. We show that it achieves a complexity bound of\n$\\tilde{O}(m^3n^m\\varepsilon^{-2})$ for a tolerance $\\varepsilon \\in (0, 1)$.\nThis provides a first \\textit{near-linear time} complexity bound guarantee for\napproximating the MOT problem and matches the best known complexity bound for\nthe Sinkhorn algorithm in the classical OT setting when $m = 2$. The second\nalgorithm, which we refer to as \\textit{accelerated multimarginal Sinkhorn}\nalgorithm, achieves the acceleration by incorporating an estimate sequence and\nthe complexity bound is $\\tilde{O}(m^3n^{m+1/3}\\varepsilon^{-4/3})$. This bound\nis better than that of the first algorithm in terms of $1/\\varepsilon$, and\naccelerated alternating minimization\nalgorithm~\\citep{Tupitsa-2020-Multimarginal} in terms of $n$. Finally, we\ncompare our new algorithms with the commercial LP solver \\textsc{Gurobi}.\nPreliminary results on synthetic data and real images demonstrate the\neffectiveness and efficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 23:43:43 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 23:00:46 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Lin", "Tianyi", ""], ["Ho", "Nhat", ""], ["Cuturi", "Marco", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1910.00213", "submitter": "Masaaki Inoue", "authors": "Masaaki Inoue, Thong Pham, Hidetoshi Shimodaira", "title": "Joint Estimation of the Non-parametric Transitivity and Preferential\n  Attachment Functions in Scientific Co-authorship Networks", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical method to estimate simultaneously the non-parametric\ntransitivity and preferential attachment functions in a growing network, in\ncontrast to conventional methods that either estimate each function in\nisolation or assume some functional form for them. Our model is shown to be a\ngood fit to two real-world co-authorship networks and be able to bring to light\nintriguing details of the preferential attachment and transitivity phenomena\nthat would be unavailable under traditional methods. We also introduce a method\nto quantify the amount of contributions of those phenomena in the growth\nprocess of a network based on the probabilistic dynamic process induced by the\nmodel formula. Applying this method, we found that transitivity dominated PA in\nboth co-authorship networks. This suggests the importance of indirect relations\nin scientific creative processes. The proposed methods are implemented in the R\npackage FoFaF.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 06:17:59 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Inoue", "Masaaki", ""], ["Pham", "Thong", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1910.00551", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Nicolas Flammarion, Martin J. Wainwright, Peter L.\n  Bartlett", "title": "An Efficient Sampling Algorithm for Non-smooth Composite Potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling from a density of the form $p(x) \\propto\n\\exp(-f(x)- g(x))$, where $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a smooth\nand strongly convex function and $g: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a\nconvex and Lipschitz function. We propose a new algorithm based on the\nMetropolis-Hastings framework, and prove that it mixes to within TV distance\n$\\varepsilon$ of the target density in at most $O(d \\log (d/\\varepsilon))$\niterations. This guarantee extends previous results on sampling from\ndistributions with smooth log densities ($g = 0$) to the more general composite\nnon-smooth case, with the same mixing time up to a multiple of the condition\nnumber. Our method is based on a novel proximal-based proposal distribution\nthat can be efficiently computed for a large class of non-smooth functions $g$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:25:55 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Mou", "Wenlong", ""], ["Flammarion", "Nicolas", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1910.00699", "submitter": "Yugandhar Sarkale", "authors": "Yugandhar Sarkale, Saeed Nozhati, Edwin K. P. Chong, Bruce R.\n  Ellingwood", "title": "Decision Automation for Electric Power Network Recovery", "comments": "Submitted to IEEE Transactions on Automation Science and Engineering\n  (13 pages and 6 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY eess.SY math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical infrastructure systems such as electric power networks, water\nnetworks, and transportation systems play a major role in the welfare of any\ncommunity. In the aftermath of disasters, their recovery is of paramount\nimportance; orderly and efficient recovery involves the assignment of limited\nresources (a combination of human repair workers and machines) to repair\ndamaged infrastructure components. The decision maker must also deal with\nuncertainty in the outcome of the resource-allocation actions during recovery.\nThe manual assignment of resources seldom is optimal despite the expertise of\nthe decision maker because of the large number of choices and uncertainties in\nconsequences of sequential decisions. This combinatorial assignment problem\nunder uncertainty is known to be \\mbox{NP-hard}. We propose a novel decision\ntechnique that addresses the massive number of decision choices for large-scale\nreal-world problems; in addition, our method also features an experiential\nlearning component that adaptively determines the utilization of the\ncomputational resources based on the performance of a small number of choices.\nOur framework is closed-loop, and naturally incorporates all the attractive\nfeatures of such a decision-making system. In contrast to myopic approaches,\nwhich do not account for the future effects of the current choices, our\nmethodology has an anticipatory learning component that effectively\nincorporates \\emph{lookahead} into the solutions. To this end, we leverage the\ntheory of regression analysis, Markov decision processes (MDPs), multi-armed\nbandits, and stochastic models of community damage from natural disasters to\ndevelop a method for near-optimal recovery of communities. Our method\ncontributes to the general problem of MDPs with massive action spaces with\napplication to recovery of communities affected by hazards.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:30:02 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 16:59:33 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 22:53:35 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Sarkale", "Yugandhar", ""], ["Nozhati", "Saeed", ""], ["Chong", "Edwin K. P.", ""], ["Ellingwood", "Bruce R.", ""]]}, {"id": "1910.01031", "submitter": "H{\\aa}vard Heitlo Holm", "authors": "H{\\aa}vard Heitlo Holm, Martin Lilleeng S{\\ae}tra and Peter Jan van\n  Leeuwen", "title": "Massively Parallel Implicit Equal-Weights Particle Filter for Ocean\n  Drift Trajectory Forecasting", "comments": "41 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.ao-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting ocean drift trajectories are important for many applications,\nincluding search and rescue operations, oil spill cleanup and iceberg risk\nmitigation. In an operational setting, forecasts of drift trajectories are\nproduced based on computationally demanding forecasts of three-dimensional\nocean currents. Herein, we investigate a complementary approach for shorter\ntime scales by using a recent state-of-the-art implicit equal-weights particle\nfilter applied to a simplified ocean model. To achieve this, we present a new\nalgorithmic design for a data-assimilation system in which all components -\nincluding the model, model errors, and particle filter - take advantage of\nmassively parallel compute architectures, such as graphical processing units.\nFaster computations can enable in-situ and ad-hoc model runs for emergency\nmanagement, and larger ensembles for better uncertainty quantification. Using a\nchallenging test case with near-realistic chaotic instabilities, we run\ndata-assimilation experiments based on synthetic observations from drifting and\nmoored buoys, and analyse the trajectory forecasts for the drifters. Our\nresults show that even sparse drifter observations are sufficient to\nsignificantly improve short-term drift forecasts up to twelve hours. With\nequidistant moored buoys observing only 0.1% of the state space, the ensemble\ngives an accurate description of the true state after data assimilation\nfollowed by a high-quality probabilistic forecast.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 15:35:07 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Holm", "H\u00e5vard Heitlo", ""], ["S\u00e6tra", "Martin Lilleeng", ""], ["van Leeuwen", "Peter Jan", ""]]}, {"id": "1910.01305", "submitter": "Jeffrey Wong", "authors": "Jeffrey Wong, Randall Lewis, Matthew Wardrop", "title": "Efficient Computation of Linear Model Treatment Effects in an\n  Experimentation Platform", "comments": "Revised some grammar", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear models are a core component for statistical software that analyzes\ntreatment effects. They are used in experimentation platforms where analysis is\nautomated, as well as scientific studies where analysis is done locally and\nmanually. However, the implementations of linear models for online platforms\nand for local analysis are often different due to differing demands in\nscalability. To improve leverage, we developed one library that powers both\nmodes of analysis, consolidates many forms of treatment effects from linear\nmodels, and optimizes the computational performance of the software. The highly\nperformant algorithms allow the library to run linear models at Netflix's scale\nand reply to live queries for treatment effects from an online service without\nusing distributed systems in seconds. Wrapping these algorithms in a library\nthat can be used interactively make large scale linear models accessible for\nboth online platforms and scientists to use locally.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 05:16:59 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 03:09:36 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Wong", "Jeffrey", ""], ["Lewis", "Randall", ""], ["Wardrop", "Matthew", ""]]}, {"id": "1910.01312", "submitter": "Chengjing Wang", "authors": "Dunbiao Niu, Chengjing Wang, Peipei Tang, Qingsong Wang, and Enbin\n  Song", "title": "A sparse semismooth Newton based augmented Lagrangian method for\n  large-scale support vector machines", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) are successful modeling and prediction tools\nwith a variety of applications. Previous work has demonstrated the superiority\nof the SVMs in dealing with the high dimensional, low sample size problems.\nHowever, the numerical difficulties of the SVMs will become severe with the\nincrease of the sample size. Although there exist many solvers for the SVMs,\nonly few of them are designed by exploiting the special structures of the SVMs.\nIn this paper, we propose a highly efficient sparse semismooth Newton based\naugmented Lagrangian method for solving a large-scale convex quadratic\nprogramming problem with a linear equality constraint and a simple box\nconstraint, which is generated from the dual problems of the SVMs. By\nleveraging the primal-dual error bound result, the fast local convergence rate\nof the augmented Lagrangian method can be guaranteed. Furthermore, by\nexploiting the second-order sparsity of the problem when using the semismooth\nNewton method,the algorithm can efficiently solve the aforementioned difficult\nproblems. Finally, numerical comparisons demonstrate that the proposed\nalgorithm outperforms the current state-of-the-art solvers for the large-scale\nSVMs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 05:51:47 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 06:08:02 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Niu", "Dunbiao", ""], ["Wang", "Chengjing", ""], ["Tang", "Peipei", ""], ["Wang", "Qingsong", ""], ["Song", "Enbin", ""]]}, {"id": "1910.01399", "submitter": "Mario Teixeira Parente", "authors": "Mario Teixeira Parente, Jonas Wallin, Barbara Wohlmuth", "title": "Generalized bounds for active subspaces", "comments": "27 pages, 6 figures", "journal-ref": "Electronic Journal of Statistics 14 (1), 917-943, 2020", "doi": "10.1214/20-EJS1684", "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider scenarios in which traditional estimates for the\nactive subspace method based on probabilistic Poincar\\'e inequalities are not\nvalid due to unbounded Poincar\\'e constants. Consequently, we propose a\nframework that allows to derive generalized estimates in the sense that it\nenables to control the trade-off between the size of the Poincar\\'e constant\nand a weaker order of the final error bound. In particular, we investigate\nindependently exponentially distributed random variables in dimension two or\nlarger and give explicit expressions for corresponding Poincar\\'e constants\nshowing their dependence on the dimension of the problem. Finally, we suggest\npossibilities for future work that aim for extending the class of distributions\napplicable to the active subspace method as we regard this as an opportunity to\nenlarge its usability.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:53:33 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 13:31:08 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 19:13:45 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Parente", "Mario Teixeira", ""], ["Wallin", "Jonas", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1910.01964", "submitter": "Feriel Bouhadjera", "authors": "Bouhadjera Feriel (ULCO, Facult\\'e des Sciences Universit\\'e Badji\n  Mokhtar), Elias Ould Said (ULCO)", "title": "On the strong uniform consistency for relative error of the regression\n  function estimator for censoring times series model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random vector (X, T), where X is d-dimensional and T is\none-dimensional. We suppose that the random variable T is subject to random\nright censoring and satisfies the $\\alpha$-mixing property. The aim of this\npaper is to study the behavior of the kernel estimator of the relative error\nregression and to establish its uniform almost sure consistency with rate.\nFurthermore, we have highlighted the covariance term which measures the\ndependency. The simulation study shows that the proposed estimator performs\nwell for a finite sample size in different cases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 14:15:15 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Feriel", "Bouhadjera", "", "ULCO, Facult\u00e9 des Sciences Universit\u00e9 Badji\n  Mokhtar"], ["Said", "Elias Ould", "", "ULCO"]]}, {"id": "1910.02277", "submitter": "Pierre Marion", "authors": "Pierre Marion, Maxime Godin, Pierre L'Ecuyer", "title": "An algorithm to compute the $t$-value of a digital net and of its\n  projections", "comments": "24 pages, 2 figures, 1 table", "journal-ref": "Journal of Computational and Applied Mathematics, 371 (2020)", "doi": "10.1016/j.cam.2019.112669", "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital nets are among the most successful methods to construct\nlow-discrepancy point sets for quasi-Monte Carlo integration. Their quality is\ntraditionally assessed by a measure called the $t$-value. A refinement computes\nthe $t$-value of the projections over subsets of coordinates and takes a\nweighted average (or some other function) of these values. It is also of\ninterest to compute the $t$-values of embedded nets obtained by taking subsets\nof the points. In this paper, we propose an efficient algorithm to compute such\nmeasures and we compare our approach with previously proposed methods both\nempirically and in terms of computational complexity.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 14:31:38 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 16:20:45 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Marion", "Pierre", ""], ["Godin", "Maxime", ""], ["L'Ecuyer", "Pierre", ""]]}, {"id": "1910.02301", "submitter": "Isuru Hewapathirana", "authors": "Isuru Udayangani Hewapathirana, Dominic Lee, Elena Moltchanova and\n  Jeanette McLeod", "title": "Change Detection in Noisy Dynamic Networks: A Spectral Embedding\n  Approach", "comments": "44 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection in dynamic networks is an important problem in many areas,\nsuch as fraud detection, cyber intrusion detection and health care monitoring.\nIt is a challenging problem because it involves a time sequence of graphs, each\nof which is usually very large and sparse with heterogeneous vertex degrees,\nresulting in a complex, high dimensional mathematical object. Spectral\nembedding methods provide an effective way to transform a graph to a lower\ndimensional latent Euclidean space that preserves the underlying structure of\nthe network. Although change detection methods that use spectral embedding are\navailable, they do not address sparsity and degree heterogeneity that usually\noccur in noisy real-world graphs and a majority of these methods focus on\nchanges in the behaviour of the overall network.\n  In this paper, we adapt previously developed techniques in spectral graph\ntheory and propose a novel concept of applying Procrustes techniques to\nembedded points for vertices in a graph to detect changes in entity behaviour.\nOur spectral embedding approach not only addresses sparsity and degree\nheterogeneity issues, but also obtains an estimate of the appropriate embedding\ndimension. We call this method CDP (change detection using Procrustes\nanalysis). We demonstrate the performance of CDP through extensive simulation\nexperiments and a real-world application. CDP successfully detects various\ntypes of vertex-based changes including (i) changes in vertex degree, (ii)\nchanges in community membership of vertices, and (iii) unusual increase or\ndecrease in edge weight between vertices. The change detection performance of\nCDP is compared with two other baseline methods that employ alternative\nspectral embedding approaches. In both cases, CDP generally shows superior\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 18:02:18 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Hewapathirana", "Isuru Udayangani", ""], ["Lee", "Dominic", ""], ["Moltchanova", "Elena", ""], ["McLeod", "Jeanette", ""]]}, {"id": "1910.02497", "submitter": "Anirban Chaudhuri", "authors": "Anirban Chaudhuri, Alexandre N. Marques, Karen E. Willcox", "title": "mfEGRA: Multifidelity Efficient Global Reliability Analysis through\n  Active Learning for Failure Boundary Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops mfEGRA, a multifidelity active learning method using\ndata-driven adaptively refined surrogates for failure boundary location in\nreliability analysis. This work addresses the issue of prohibitive cost of\nreliability analysis using Monte Carlo sampling for expensive-to-evaluate\nhigh-fidelity models by using cheaper-to-evaluate approximations of the\nhigh-fidelity model. The method builds on the Efficient Global Reliability\nAnalysis (EGRA) method, which is a surrogate-based method that uses adaptive\nsampling for refining Gaussian process surrogates for failure boundary location\nusing a single-fidelity model. Our method introduces a two-stage adaptive\nsampling criterion that uses a multifidelity Gaussian process surrogate to\nleverage multiple information sources with different fidelities. The method\ncombines expected feasibility criterion from EGRA with one-step lookahead\ninformation gain to refine the surrogate around the failure boundary. The\ncomputational savings from mfEGRA depends on the discrepancy between the\ndifferent models, and the relative cost of evaluating the different models as\ncompared to the high-fidelity model. We show that accurate estimation of\nreliability using mfEGRA leads to computational savings of $\\sim$46% for an\nanalytic multimodal test problem and 24% for a three-dimensional acoustic horn\nproblem, when compared to single-fidelity EGRA. We also show the effect of\nusing a priori drawn Monte Carlo samples in the implementation for the acoustic\nhorn problem, where mfEGRA leads to computational savings of 45% for the\nthree-dimensional case and 48% for a rarer event four-dimensional case as\ncompared to single-fidelity EGRA.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 18:37:12 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 22:08:27 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 15:32:10 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2020 19:35:03 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Chaudhuri", "Anirban", ""], ["Marques", "Alexandre N.", ""], ["Willcox", "Karen E.", ""]]}, {"id": "1910.02506", "submitter": "Subharup Guha", "authors": "Subharup Guha, Rex Jung and David Dunson", "title": "Predicting Phenotypes from Brain Connection Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the problem of predicting a response variable based\non a network-valued predictor. Our particular motivation is developing\ninterpretable and accurate predictive models for cognitive traits and\nneuro-psychiatric disorders based on an individual's brain connection network\n(connectome). Current methods focus on reducing the complex and\nhigh-dimensional brain network into a low-dimensional set of pre-specified\nfeatures prior to applying standard predictive algorithms. Such methods are\nsensitive to feature choice and inevitably discard information. We instead\npropose a nonparametric Bayes class of models that utilize information from the\nentire adjacency matrix defining connections among brain regions in adaptively\ndefining flexible predictive algorithms, while maintaining interpretability.\nThe proposed Bayesian Connectomics (BaCon) model class utilizes\nPoisson-Dirichlet processes to detect a lower-dimensional, bidirectional\n(covariate, subject) pattern in the adjacency matrix. The small n, large p\nproblem is transformed into a \"small n, small q\" problem, facilitating an\neffective stochastic search of the predictors. A spike-and-slab prior for the\ncluster predictors strikes a balance between regression model parsimony and\nflexibility, resulting in improved inferences and test case predictions. We\ndescribe basic properties of the BaCon model class and develop efficient\nalgorithms for posterior computation. The resulting methods are shown to\noutperform existing approaches in simulations and applied to a creative\nreasoning data set.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 19:19:05 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 16:23:18 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Guha", "Subharup", ""], ["Jung", "Rex", ""], ["Dunson", "David", ""]]}, {"id": "1910.02721", "submitter": "Eliana Duarte", "authors": "Lamprini Ananiadi, Eliana Duarte", "title": "Gr\\\"obner bases for staged trees", "comments": "17 pages, 3 figures", "journal-ref": "Alg. Stat. 12 (2021) 1-20", "doi": "10.2140/astat.2021.12.1", "report-no": null, "categories": "math.AC math.AG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the problem of finding generators of the toric\nideal associated to a combinatorial object called a staged tree. Our main\ntheorem states that toric ideals of staged trees that are balanced and\nstratified are generated by a quadratic Gr\\\"obner basis whose intial terms are\nsquarefree. The proof of this result is based on Sullivant's toric fiber\nproduct construction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 10:58:23 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ananiadi", "Lamprini", ""], ["Duarte", "Eliana", ""]]}, {"id": "1910.02995", "submitter": "Matthew Fisher Mr", "authors": "Matthew A Fisher, Chris J Oates, Catherine Powell, Aretha Teckentrup", "title": "A Locally Adaptive Bayesian Cubature Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian cubature (BC) is a popular inferential perspective on the cubature\nof expensive integrands, wherein the integrand is emulated using a stochastic\nprocess model. Several approaches have been put forward to encode sequential\nadaptation (i.e. dependence on previous integrand evaluations) into this\nframework. However, these proposals have been limited to either estimating the\nparameters of a stationary covariance model or focusing computational resources\non regions where large values are taken by the integrand. In contrast, many\nclassical adaptive cubature methods focus computational resources on spatial\nregions in which local error estimates are largest. The contributions of this\nwork are three-fold: First, we present a theoretical result that suggests there\ndoes not exist a direct Bayesian analogue of the classical adaptive trapezoidal\nmethod. Then we put forward a novel BC method that has empirically similar\nbehaviour to the adaptive trapezoidal method. Finally we present evidence that\nthe novel method provides improved cubature performance, relative to standard\nBC, in a detailed empirical assessment.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:26:08 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Fisher", "Matthew A", ""], ["Oates", "Chris J", ""], ["Powell", "Catherine", ""], ["Teckentrup", "Aretha", ""]]}, {"id": "1910.03632", "submitter": "Dennis Prangle", "authors": "Dennis Prangle", "title": "Distilling importance sampling", "comments": "This version adds a second application, and fixes some minor errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two main approaches to Bayesian inference are sampling and optimisation\nmethods. However many complicated posteriors are difficult to approximate by\neither. Therefore we propose a novel approach combining features of both. We\nuse a flexible parameterised family of densities, such as a normalising flow.\nGiven a density from this family approximating the posterior, we use importance\nsampling to produce a weighted sample from a more accurate posterior\napproximation. This sample is then used in optimisation to update the\nparameters of the approximate density, which we view as distilling the\nimportance sampling results. We iterate these steps and gradually improve the\nquality of the posterior approximation. We illustrate our method in two\nchallenging examples: a queueing model and a stochastic differential equation\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:38:50 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 14:15:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Prangle", "Dennis", ""]]}, {"id": "1910.03643", "submitter": "Denis Belomestny", "authors": "D. Belomestny, L. Iosipoi, E. Moulines, A. Naumov and S. Samsonov", "title": "Variance reduction for Markov chains with application to MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel variance reduction approach for additive\nfunctionals of Markov chains based on minimization of an estimate for the\nasymptotic variance of these functionals over suitable classes of control\nvariates. A distinctive feature of the proposed approach is its ability to\nsignificantly reduce the overall finite sample variance. This feature is\ntheoretically demonstrated by means of a deep non asymptotic analysis of a\nvariance reduced functional as well as by a thorough simulation study. In\nparticular we apply our method to various MCMC Bayesian estimation problems\nwhere it favourably compares to the existing variance reduction approaches.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 19:05:36 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 08:37:54 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Belomestny", "D.", ""], ["Iosipoi", "L.", ""], ["Moulines", "E.", ""], ["Naumov", "A.", ""], ["Samsonov", "S.", ""]]}, {"id": "1910.03906", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Akyildiz, Gerrit J.J. van den Burg, Theodoros Damoulas,\n  Mark F. J. Steel", "title": "Probabilistic sequential matrix factorization", "comments": "Accepted for publication at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the probabilistic sequential matrix factorization (PSMF) method\nfor factorizing time-varying and non-stationary datasets consisting of\nhigh-dimensional time-series. In particular, we consider nonlinear Gaussian\nstate-space models where sequential approximate inference results in the\nfactorization of a data matrix into a dictionary and time-varying coefficients\nwith potentially nonlinear Markovian dependencies. The assumed Markovian\nstructure on the coefficients enables us to encode temporal dependencies into a\nlow-dimensional feature space. The proposed inference method is solely based on\nan approximate extended Kalman filtering scheme, which makes the resulting\nmethod particularly efficient. PSMF can account for temporal nonlinearities\nand, more importantly, can be used to calibrate and estimate generic\ndifferentiable nonlinear subspace models. We also introduce a robust version of\nPSMF, called rPSMF, which uses Student-t filters to handle model\nmisspecification. We show that PSMF can be used in multiple contexts: modeling\ntime series with a periodic subspace, robustifying changepoint detection\nmethods, and imputing missing data in several high-dimensional time-series,\nsuch as measurements of pollutants across London.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 11:30:29 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 19:01:46 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 17:08:19 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Akyildiz", "\u00d6mer Deniz", ""], ["Burg", "Gerrit J. J. van den", ""], ["Damoulas", "Theodoros", ""], ["Steel", "Mark F. J.", ""]]}, {"id": "1910.04364", "submitter": "arXiv Admin", "authors": "Ruize Gao, Ying Zhao", "title": "Network Entropy based on Cluster Expansion on Motifs for Undirected\n  Graphs", "comments": "arXiv admin note: This submission has been removed by arXiv\n  administrators as the submitter did not have the right to agree to the\n  license at the time of submission. Version 3 was an inappropriate replacement", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GN stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of the network can be described by motifs, which are subgraphs\nthat often repeat themselves. In order to understand the structure of network\nmotifs, it is of great importance to study subgraphs from the perspective of\nstatistical mechanics. In this paper, we use clustering extensions in\nstatistical physics to solve the problem of using motifs as network primitives.\nBy projecting the network motifs to clusters in the gas model, we develop the\npartition function of the network, which enables us to calculate global\nthermodynamic quantities, such as energy, entropy, and vice versa. Then, we\ngive the analytic expressions of the number of specific types of motifs and\ncalculate their correlated entropy. We conduct algebraic experiments on\ndatasets, both synthetic and in real life, and evaluate the qualitative and\nquantitative characterization of motif entropy derived from the partition\nfunction. Our findings show that the motif entropy of networks in real life,\nfor instance, financial and stock market networks, is of high correlation to\nthe change of network structure. Hence, our findings are consistent with recent\nstudies about the similar topic that network motifs can be represented as basic\nelements of well-defined information processing functions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 04:49:42 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 20:38:08 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 03:58:47 GMT"}, {"version": "v4", "created": "Wed, 23 Oct 2019 16:13:42 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gao", "Ruize", ""], ["Zhao", "Ying", ""]]}, {"id": "1910.04379", "submitter": "Feroz Ali T M", "authors": "T M Feroz Ali", "title": "Maneuvering, Multi-Target Tracking using Particle Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop tracking and estimation techniques relevant to\nunderwater targets. Particularly, we explore particle filtering techniques for\ntarget tracking. It is a numerical approximation method for implementing a\nrecursive Bayesian estimation procedure. It does not require the assumptions of\nlinearity and Guassianity like the traditional Kalman filter based techniques\nand is capable of handling non-Gaussian noise distributions and non-linearities\nin the measurements as well as target dynamics. The performance of particle\nfilters is verified using simulations and compared with Extended Kalman Filter.\nParticle filters can track multi-targets and highly maneuvering targets.\nHowever, it has higher computational load. The efficient use of particle\nfilters for multi-target tracking using Independent Partition Particle Filter\n(IPPF) and tracking highly maneuvering targets using Multiple Model Particle\nFilter(MMPF) are also explored in this work. These techniques require only\nsmaller number of particles and help in reducing the computational cost. Data\nassociation problem exists in multi-target tracking due to lack of information\nat the observer about the proper association between the targets and the\nreceived measurements. The problem becomes more involved when the targets move\nmuch closer and there are clutter and missed target detections at the observer.\nMonte Carlo Joint Probabilistic Data Association Filter (MCJPDAF) efficiently\nsolves data association for the mentioned situation. Due to the inability of\nthe standard MCJPDAF to track highly maneuvering targets, Monte Carlo Multiple\nModel Joint Probabilistic Data Association Filter (MC-MMJPDAF) which combines\nthe technique of Multiple Model Particle Filter(MMPF) in the framework of\nMC-JPDAF has been proposed. The simulation results shows the superiority of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 06:07:26 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Ali", "T M Feroz", ""]]}, {"id": "1910.04574", "submitter": "Roi Naveiro", "authors": "Tahir Ekin, Roi Naveiro, Alberto Torres-Barr\\'an, David R\\'ios-Insua", "title": "Augmented Probability Simulation Methods for Non-cooperative Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust decision support framework with computational algorithms\nfor decision makers in non-cooperative sequential setups. Existing simulation\nbased approaches can be inefficient when there is a large number of feasible\ndecisions and uncertain outcomes. Hence, we provide a novel alternative to\nsolve non-cooperative sequential games based on augmented probability\nsimulation. We propose approaches to approximate subgame perfect equilibria\nunder complete information, assess the robustness of such solutions and,\nfinally, approximate adversarial risk analysis solutions when lacking complete\ninformation. This framework could be especially beneficial in application\ndomains such as cybersecurity and counter-terrorism.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 13:54:57 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 16:53:28 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Ekin", "Tahir", ""], ["Naveiro", "Roi", ""], ["Torres-Barr\u00e1n", "Alberto", ""], ["R\u00edos-Insua", "David", ""]]}, {"id": "1910.04672", "submitter": "Alexander Buchholz", "authors": "Alexander Buchholz, Daniel Ahfock, Sylvia Richardson", "title": "Distributed Computation for Marginal Likelihood based Model Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a general method for distributed Bayesian model choice, using the\nmarginal likelihood, where each worker has access only to non-overlapping\nsubsets of the data. Our approach approximates the model evidence for the full\ndata set through Monte Carlo sampling from the posterior on every subset\ngenerating a model evidence per subset. The model evidences per worker are then\nconsistently combined using a novel approach which corrects for the splitting\nusing summary statistics of the generated samples. This divide-and-conquer\napproach allows Bayesian model choice in the large data setting, exploiting all\navailable information but limiting communication between workers. Our work\nthereby complements the work on consensus Monte Carlo (Scott et al., 2016) by\nexplicitly enabling model choice. In addition, we show how the suggested\napproach can be extended to model choice within a reversible jump setting that\nexplores multiple feature combinations within one run.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:23:34 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 15:58:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Buchholz", "Alexander", ""], ["Ahfock", "Daniel", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1910.05212", "submitter": "Jan Povala", "authors": "Jan Povala, Seppo Virtanen, Mark Girolami", "title": "Burglary in London: Insights from Statistical Heterogeneous Spatial\n  Point Processes", "comments": "Accepted at the Journal of the Royal Statistical Society Series C:\n  Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain operational insights regarding the crime of burglary in London we\nconsider the estimation of effects of covariates on the intensity of spatial\npoint patterns. By taking into account localised properties of criminal\nbehaviour, we propose a spatial extension to model-based clustering methods\nfrom the mixture modelling literature. The proposed Bayesian model is a finite\nmixture of Poisson generalised linear models such that each location is\nprobabilistically assigned to one of the clusters. Each cluster is\ncharacterised by the regression coefficients which we subsequently use to\ninterpret the localised effects of the covariates. Using a blocking structure\nof the study region, our approach allows specifying spatial dependence between\nnearby locations. We estimate the proposed model using Markov Chain Monte Carlo\nmethods and provide a Python implementation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 14:27:52 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 08:04:36 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 08:12:09 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Povala", "Jan", ""], ["Virtanen", "Seppo", ""], ["Girolami", "Mark", ""]]}, {"id": "1910.05275", "submitter": "JInglai Li", "authors": "Tengchao Yu, Hongqiao Wang, Jinglai Li", "title": "Maximum conditional entropy Hamiltonian Monte Carlo sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Hamiltonian Monte Carlo (HMC) sampler depends critically\non some algorithm parameters such as the total integration time and the\nnumerical integration stepsize. The parameter tuning is particularly\nchallenging when the mass matrix of the HMC sampler is adapted. We propose in\nthis work a Kolmogorov-Sinai entropy (KSE) based design criterion to optimize\nthese algorithm parameters, which can avoid some potential issues in the often\nused jumping-distance based measures. For near-Gaussian distributions, we are\nable to derive the optimal algorithm parameters with respect to the KSE\ncriterion analytically. As a byproduct the KSE criterion also provides a\ntheoretical justification for the need to adapt the mass matrix in HMC sampler.\nBased on the results, we propose an adaptive HMC algorithm, and we then\ndemonstrate the performance of the proposed algorithm with numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 16:09:08 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 17:24:51 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yu", "Tengchao", ""], ["Wang", "Hongqiao", ""], ["Li", "Jinglai", ""]]}, {"id": "1910.05509", "submitter": "Mahroo Bahreinian", "authors": "Mahroo Bahreinian, Roberto Tron", "title": "A Computational Theory of Robust Localization Verifiability in the\n  Presence of Pure Outlier Measurements", "comments": "Accepted for publication in the proceedings of Conference on Decision\n  and Control 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of localizing a set of nodes from relative pairwise measurements\nis at the core of many applications such as Structure from Motion (SfM), sensor\nnetworks, and Simultaneous Localization And Mapping (SLAM). In practical\nsituations, the accuracy of the relative measurements is marred by noise and\noutliers; hence, we have the problem of quantifying how much we should trust\nthe solution returned by some given localization solver. In this work, we focus\non the question of whether an L1-norm robust optimization formulation can\nrecover a solution that is identical to the ground truth, under the scenario of\ntranslation-only measurements corrupted exclusively by outliers and no noise;\nwe call this concept verifiability. On the theoretical side, we prove that the\nverifiability of a problem depends only on the topology of the graph of\nmeasurements, the edge support of the outliers, and their signs, while it is\nindependent of ground truth locations of the nodes, and of any positive scaling\nof the outliers. On the computational side, we present a novel approach based\non the dual simplex algorithm that can check the verifiability of a problem,\ncompletely characterize the space of equivalent solutions if they exist, and\nidentify subgraphs that are verifiable. As an application of our theory, we\nprovide a procedure to compute a priori probability of recovering a solution\ncongruent or equivalent to the ground truth given a measurement graph and the\nprobabilities of each edge containing an outlier.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 07:02:24 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Bahreinian", "Mahroo", ""], ["Tron", "Roberto", ""]]}, {"id": "1910.05692", "submitter": "Babak Shahbaba", "authors": "Babak Shahbaba, Luis Martinez Lomeli, Tian Chen, and Shiwei Lan", "title": "Deep Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new computationally efficient sampling scheme for Bayesian\ninference involving high dimensional probability distributions. Our method maps\nthe original parameter space into a low-dimensional latent space, explores the\nlatent space to generate samples, and maps these samples back to the original\nspace for inference. While our method can be used in conjunction with any\ndimension reduction technique to obtain the latent space, and any standard\nsampling algorithm to explore the low-dimensional space, here we specifically\nuse a combination of auto-encoders (for dimensionality reduction) and\nHamiltonian Monte Carlo (HMC, for sampling). To this end, we first run an HMC\nto generate some initial samples from the original parameter space, and then\nuse these samples to train an auto-encoder. Next, starting with an initial\nstate, we use the encoding part of the autoencoder to map the initial state to\na point in the low-dimensional latent space. Using another HMC, this point is\nthen treated as an initial state in the latent space to generate a new state,\nwhich is then mapped to the original space using the decoding part of the\nauto-encoder. The resulting point can be treated as a Metropolis-Hasting (MH)\nproposal, which is either accepted or rejected. While the induced dynamics in\nthe parameter space is no longer Hamiltonian, it remains time reversible, and\nthe Markov chain could still converge to the canonical distribution using a\nvolume correction term. Dropping the volume correction step results in\nconvergence to an approximate but reasonably accurate distribution. The\nempirical results based on several high-dimensional problems show that our\nmethod could substantially reduce the computational cost of Bayesian inference.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 05:59:56 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Shahbaba", "Babak", ""], ["Lomeli", "Luis Martinez", ""], ["Chen", "Tian", ""], ["Lan", "Shiwei", ""]]}, {"id": "1910.05846", "submitter": "Emmanuel Afolabi Bakare E. A.", "authors": "Bakare E.A., Are E.B., Abolarin O.E., Osanyinlusi S.A., Ngwu Benitho,\n  and Ubaka Obiaderi N", "title": "Mathematical Modelling and Analysis of Transmission Dynamics of Lassa\n  fever", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, a periodically-forced seasonal non-autonomous system of a\nnon-linear ordinary differential equation is developed that captures the\ndynamics of Lassa fever transmission and seasonal variation in the birth of\nmastomys rodents where time was measured in days to capture seasonality. It was\nshown that the model is epidemiologically meaningful and mathematically\nwell-posed by using the results from the qualitative properties of the solution\nof the model. It was established that in order to eliminate Lassa fever\ndisease, treatments with Ribavirin must be provided early to reduce mortality\nand other preventive measures like an educational campaign, community hygiene,\nIsolation of infected humans, and culling/destruction of rodents must be\napplied to also reduce the morbidity of the disease. Finally, the obtained\nresults gave a primer framework for planning and designing cost-effective\nstrategies for good interventions in eliminating Lassa fever.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:20:12 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["A.", "Bakare E.", ""], ["B.", "Are E.", ""], ["E.", "Abolarin O.", ""], ["A.", "Osanyinlusi S.", ""], ["Benitho", "Ngwu", ""], ["N", "Ubaka Obiaderi", ""]]}, {"id": "1910.05904", "submitter": "Jun Yang", "authors": "Robert M. Anderson and Haosui Duanmu and Aaron Smith and Jun Yang", "title": "Drift, Minorization, and Hitting Times", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"drift-and-minorization\" method, introduced and popularized in\n(Rosenthal, 1995; Meyn and Tweedie, 1994; Meyn and Tweedie, 2012), remains the\nmost popular approach for bounding the convergence rates of Markov chains used\nin statistical computation. This approach requires estimates of two quantities:\nthe rate at which a single copy of the Markov chain \"drifts\" towards a fixed\n\"small set\", and a \"minorization condition\" which gives the worst-case time for\ntwo Markov chains started within the small set to couple with moderately large\nprobability. In this paper, we build on (Oliveira, 2012; Peres and Sousi, 2015)\nand our work (Anderson, Duanmu, Smith, 2019a; Anderson, Duanmu, Smith, 2019b)\nto replace the \"minorization condition\" with an alternative \"hitting condition\"\nthat is stated in terms of only one Markov chain, and illustrate how this can\nbe used to obtain similar bounds that can be easier to use.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 03:21:42 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 19:03:47 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 08:09:11 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Anderson", "Robert M.", ""], ["Duanmu", "Haosui", ""], ["Smith", "Aaron", ""], ["Yang", "Jun", ""]]}, {"id": "1910.05956", "submitter": "Stanislav Nagy", "authors": "Stanislav Nagy, Rainer Dyckerhoff and Pavlo Mozharovskyi", "title": "Uniform convergence rates for the approximated halfspace and projection\n  depth", "comments": null, "journal-ref": "Electron. J. Statist. 14 (2) 3939 - 3975, 2020", "doi": "10.1214/20-EJS1759", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of some depths that satisfy the projection\nproperty, such as the halfspace depth or the projection depth, is known to be\nhigh, especially for data of higher dimensionality. In such scenarios, the\nexact depth is frequently approximated using a randomized approach: The data\nare projected into a finite number of directions uniformly distributed on the\nunit sphere, and the minimal depth of these univariate projections is used to\napproximate the true depth. We provide a theoretical background for this\napproximation procedure. Several uniform consistency results are established,\nand the corresponding uniform convergence rates are provided. For elliptically\nsymmetric distributions and the halfspace depth it is shown that the obtained\nuniform convergence rates are sharp. In particular, guidelines for the choice\nof the number of random projections in order to achieve a given precision of\nthe depths are stated.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 07:25:41 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Nagy", "Stanislav", ""], ["Dyckerhoff", "Rainer", ""], ["Mozharovskyi", "Pavlo", ""]]}, {"id": "1910.06008", "submitter": "Alan Huang", "authors": "Alan Huang, Andy Sang Il Kim", "title": "Bayesian generalized linear model for over and under dispersed counts", "comments": "10 pages, 2 figures, 1 table; 2 page supplement. Accepted October\n  2019", "journal-ref": null, "doi": "10.1080/03610926.2019.1682162", "report-no": "Accepted October 2019", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models that can handle both over and under dispersed counts are rare\nin the literature, perhaps because full probability distributions for dispersed\ncounts are rather difficult to construct. This note takes a first look at\nBayesian Conway-Maxwell-Poisson generalized linear models that can handle both\nover and under dispersion yet retain the parsimony and interpretability of\nclassical count regression models. The focus is on providing an explicit\ndemonstration of Bayesian regression inferences for dispersed counts via a\nMetropolis-Hastings algorithm. We illustrate the approach on two data analysis\nexamples and demonstrate some favourable frequentist properties via a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:38:51 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 13:59:07 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Huang", "Alan", ""], ["Kim", "Andy Sang Il", ""]]}, {"id": "1910.06121", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Aki Vehtari, Pekka Marttinen", "title": "Batch simulations and uncertainty quantification in Gaussian process\n  surrogate approximate Bayesian computation", "comments": "Minor improvements and clarifications to the text over the previous\n  version. 20 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational efficiency of approximate Bayesian computation (ABC) has\nbeen improved by using surrogate models such as Gaussian processes (GP). In one\nsuch promising framework the discrepancy between the simulated and observed\ndata is modelled with a GP which is further used to form a model-based\nestimator for the intractable posterior. In this article we improve this\napproach in several ways. We develop batch-sequential Bayesian experimental\ndesign strategies to parallellise the expensive simulations. In earlier work\nonly sequential strategies have been used. Current surrogate-based ABC methods\nalso do not fully account the uncertainty due to the limited budget of\nsimulations as they output only a point estimate of the ABC posterior. We\npropose a numerical method to fully quantify the uncertainty in, for example,\nABC posterior moments. We also provide some new analysis on the GP modelling\nassumptions in the resulting improved framework called Bayesian ABC and discuss\nits connection to Bayesian quadrature (BQ) and Bayesian optimisation (BO).\nExperiments with toy and real-world simulation models demonstrate advantages of\nthe proposed techniques.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 13:01:32 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 12:35:02 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 12:46:52 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1910.06539", "submitter": "Theodore Papamarkou", "authors": "Theodore Papamarkou and Jacob Hinkle and M. Todd Young and David\n  Womble", "title": "Challenges in Markov chain Monte Carlo for Bayesian neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods have not been broadly adopted in\nBayesian neural networks (BNNs). This paper initially reviews the main\nchallenges in sampling from the parameter posterior of a neural network via\nMCMC. Such challenges culminate to lack of convergence to the parameter\nposterior. Nevertheless, this paper shows that a non-converged Markov chain,\ngenerated via MCMC sampling from the parameter space of a neural network, can\nyield via Bayesian marginalization a valuable predictive posterior of the\noutput of the neural network. Classification examples based on multilayer\nperceptrons showcase highly accurate predictive posteriors. The postulate of\nlimited scope for MCMC developments in BNNs is partially valid; an\nasymptotically exact parameter posterior seems less plausible, yet an accurate\npredictive posterior is a tenable research avenue.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 05:43:45 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 18:02:21 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 03:37:55 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2021 10:39:02 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Papamarkou", "Theodore", ""], ["Hinkle", "Jacob", ""], ["Young", "M. Todd", ""], ["Womble", "David", ""]]}, {"id": "1910.06593", "submitter": "Gloria Cecchini", "authors": "Gloria Cecchini, Bjoern Schelter", "title": "Iterative procedure for network inference", "comments": null, "journal-ref": "Communications in Nonlinear Science and Numerical Simulation,\n  105286 (2020)", "doi": "10.1016/j.cnsns.2020.105286", "report-no": null, "categories": "physics.data-an physics.soc-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a network is reconstructed from data, two types of errors can occur:\nfalse positive and false negative errors about the presence or absence of\nlinks. In this paper, the vertex degree distribution of the true underlying\nnetwork is analytically reconstructed using an iterative procedure. Such\nprocedure is based on the inferred network and estimates for the probabilities\n$\\alpha$ and $\\beta$ of type I and type II errors, respectively. The iteration\nprocedure consists of choosing various values for $\\alpha$ to perform the\niteration steps of the network reconstruction. For the first step, the standard\nvalue for $\\alpha$ of 0.05 can be chosen as an example. The result of this\nfirst step gives a first estimate of the network topology of interest. For the\nsecond iteration step the value for $\\alpha$ is adjusted according to the\nfindings of the first step. This procedure is iterated, ultimately leading to a\nreconstruction of the vertex degree distribution tailored to its previously\nunknown network topology.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 08:45:33 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 07:52:31 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Cecchini", "Gloria", ""], ["Schelter", "Bjoern", ""]]}, {"id": "1910.06870", "submitter": "Guanyu Hu", "authors": "Guanyu Hu, Fred Huffer, Ming-Hui Chen", "title": "New Development of Bayesian Variable Selection Criteria for Spatial\n  Point Process with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Selecting important spatial-dependent variables under the nonhomogeneous\nspatial Poisson process model is an important topic of great current interest.\nIn this paper, we use the Deviance Information Criterion (DIC) and Logarithm of\nthe Pseudo Marginal Likelihood (LPML) for Bayesian variable selection under the\nnonhomogeneous spatial Poisson process model. We further derive the new Monte\nCarlo estimation formula for LPML in the spatial Poisson process setting.\nExtensive simulation studies are carried out to evaluate the empirical\nperformance of the proposed criteria. The proposed methodology is further\napplied to the analysis of two large data sets, the Earthquake Hazards Program\nof United States Geological Survey (USGS) earthquake data and the Forest of\nBarro Colorado Island (BCI) data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:30:59 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Hu", "Guanyu", ""], ["Huffer", "Fred", ""], ["Chen", "Ming-Hui", ""]]}, {"id": "1910.06897", "submitter": "Philip White", "authors": "Philip A. White and Alan E. Gelfand", "title": "Generalized Evolutionary Point Processes: Model Specifications and Model\n  Comparison", "comments": null, "journal-ref": "Methodology and Computing in Applied Probability (2020+)", "doi": "10.1007/s11009-020-09797-8", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized evolutionary point processes offer a class of point process\nmodels that allows for either excitation or inhibition based upon the history\nof the process. In this regard, we propose modeling which comprises\ngeneralization of the nonlinear Hawkes process. Working within a Bayesian\nframework, model fitting is implemented through Markov chain Monte Carlo. This\nentails discussion of computation of the likelihood for such point patterns.\nFurthermore, for this class of models, we discuss strategies for model\ncomparison. Using simulation, we illustrate how well we can distinguish these\nmodels from point pattern specifications with conditionally independent event\ntimes, e.g., Poisson processes. Specifically, we demonstrate that these models\ncan correctly identify true relationships (i.e., excitation or\ninhibition/control). Then, we consider a novel extension of the log Gaussian\nCox process that incorporates evolutionary behavior and illustrate that our\nmodel comparison approach prefers the evolutionary log Gaussian Cox process\ncompared to simpler models. We also examine a real dataset consisting of\nviolent crime events from the 11th police district in Chicago from the year\n2018. This data exhibits strong daily seasonality and changes across the year.\nAfter we account for these data attributes, we find significant but mild\nself-excitation, implying that event occurrence increases the intensity of\nfuture events.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 16:12:13 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["White", "Philip A.", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1910.07712", "submitter": "Jie Peng", "authors": "Jilei Yang and Jie Peng", "title": "Estimating Spatially-Smoothed Fiber Orientation Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-weighted magnetic resonance imaging (D-MRI) is an in-vivo and\nnon-invasive imaging technology to probe anatomical architectures of biological\nsamples. The anatomy of white matter fiber tracts in the brain can be revealed\nto help understanding of the connectivity patterns among different brain\nregions. In this paper, we propose a novel Nearest-neighbor Adaptive Regression\nModel (NARM) for adaptive estimation of the fiber orientation distribution\n(FOD) function based on D-MRI data, where spatial homogeneity is used to\nimprove FOD estimation by incorporating neighborhood information. Specifically,\nwe formulate the FOD estimation problem as a weighted linear regression\nproblem, where the weights are chosen to account for spatial proximity and\npotential heterogeneity due to different fiber configurations. The weights are\nadaptively updated and a stopping rule based on nearest neighbor distance is\ndesigned to prevent over-smoothing. NARM is further extended to accommodate\nD-MRI data with multiple bvalues.\n  Comprehensive simulation results demonstrate that NARM leads to satisfactory\nFOD reconstructions and performs better than voxel-wise estimation as well as\ncompeting smoothing methods. By applying NARM to real 3T D-MRI datasets, we\ndemonstrate the effectiveness of NARM in recovering more realistic crossing\nfiber patterns and producing more coherent fiber tracking results, establishing\nthe practical value of NARM for analyzing D-MRI data and providing reliable\ninformation on brain structural connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 05:13:49 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Yang", "Jilei", ""], ["Peng", "Jie", ""]]}, {"id": "1910.08091", "submitter": "Yura Perov N", "authors": "Yura Perov, Logan Graham, Kostis Gourgoulias, Jonathan G. Richens,\n  Ciar\\'an M. Lee, Adam Baker, Saurabh Johri", "title": "MultiVerse: Causal Reasoning using Importance Sampling in Probabilistic\n  Programming", "comments": "Logan and Yura have made equal contributions to the paper. Accepted\n  to the 2nd Symposium on Advances in Approximate Bayesian Inference\n  (Vancouver, Canada, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We elaborate on using importance sampling for causal reasoning, in particular\nfor counterfactual inference. We show how this can be implemented natively in\nprobabilistic programming. By considering the structure of the counterfactual\nquery, one can significantly optimise the inference process. We also consider\ndesign choices to enable further optimisations. We introduce MultiVerse, a\nprobabilistic programming prototype engine for approximate causal reasoning. We\nprovide experimental results and compare with Pyro, an existing probabilistic\nprogramming framework with some of causal reasoning tools.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:00:24 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 10:05:13 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Perov", "Yura", ""], ["Graham", "Logan", ""], ["Gourgoulias", "Kostis", ""], ["Richens", "Jonathan G.", ""], ["Lee", "Ciar\u00e1n M.", ""], ["Baker", "Adam", ""], ["Johri", "Saurabh", ""]]}, {"id": "1910.09408", "submitter": "Sibo Cheng", "authors": "Sibo Cheng (EDF R&D PERICLES, LIMSI), Jean-Philippe Argaud (EDF R&D\n  PERICLES), Bertrand Iooss (EDF R&D PRISME, IMT), Didier Lucor (LIMSI),\n  Ang\\'elique Pon\\c{c}ot (EDF R&D PERICLES)", "title": "Background Error Covariance Iterative Updating with Invariant\n  Observation Measures for Data Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to leverage the information embedded in the background state and\nobservations, covariance matrices modelling is a pivotal point in data\nassimilation algorithms. These matrices are often estimated from an ensemble of\nobservations or forecast differences. Nevertheless, for many industrial\napplications the modelling still remains empirical based on some form of\nexpertise and physical constraints enforcement in the absence of historical\nobservations or predictions. We have developed two novel robust adaptive\nassimilation methods named CUTE (Covariance Updating iTerativE) and PUB\n(Partially Updating BLUE). These two non-parametric methods are based on\ndifferent optimization objectives, both capable of sequentially adapting\nbackground error covariance matrices in order to improve assimilation results\nunder the assumption of a good knowledge of the observation error covariances.\nWe have compared these two methods with the standard approach using a\nmisspecified background matrix in a shallow water twin experiments framework\nwith a linear observation operator. Numerical experiments have shown that the\nproposed methods bear a real advantage both in terms of posterior error\ncorrelation identification and assimilation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:35:05 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cheng", "Sibo", "", "EDF R&D PERICLES, LIMSI"], ["Argaud", "Jean-Philippe", "", "EDF R&D\n  PERICLES"], ["Iooss", "Bertrand", "", "EDF R&D PRISME, IMT"], ["Lucor", "Didier", "", "LIMSI"], ["Pon\u00e7ot", "Ang\u00e9lique", "", "EDF R&D PERICLES"]]}, {"id": "1910.09527", "submitter": "Jan Kudlicka", "authors": "Jan Kudlicka and Lawrence M. Murray and Thomas B. Sch\\\"on and Fredrik\n  Lindsten", "title": "Particle filter with rejection control and unbiased estimator of the\n  marginal likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the combined use of resampling and partial rejection control in\nsequential Monte Carlo methods, also known as particle filters. While the\nvariance reducing properties of rejection control are known, there has not been\n(to the best of our knowledge) any work on unbiased estimation of the marginal\nlikelihood (also known as the model evidence or the normalizing constant) in\nthis type of particle filter. Being able to estimate the marginal likelihood\nwithout bias is highly relevant for model comparison, computation of\ninterpretable and reliable confidence intervals, and in exact approximation\nmethods, such as particle Markov chain Monte Carlo. In the paper we present a\nparticle filter with rejection control that enables unbiased estimation of the\nmarginal likelihood.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 17:47:22 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 15:54:31 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Kudlicka", "Jan", ""], ["Murray", "Lawrence M.", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1910.09711", "submitter": "Jaewoo Park", "authors": "Jaewoo Park and Murali Haran", "title": "Reduced-dimensional Monte Carlo Maximum Likelihood for Latent Gaussian\n  Random Field Models", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo maximum likelihood (MCML) provides an elegant approach to find\nmaximum likelihood estimators (MLEs) for latent variable models. However, MCML\nalgorithms are computationally expensive when the latent variables are\nhigh-dimensional and correlated, as is the case for latent Gaussian random\nfield models. Latent Gaussian random field models are widely used, for example\nin building flexible regression models and in the interpolation of spatially\ndependent data in many research areas such as analyzing count data in disease\nmodeling and presence-absence satellite images of ice sheets. We propose a\ncomputationally efficient MCML algorithm by using a projection-based approach\nto reduce the dimensions of the random effects. We develop an iterative method\nfor finding an effective importance function; this is generally a challenging\nproblem and is crucial for the MCML algorithm to be computationally feasible.\nWe find that our method is applicable to both continuous (latent Gaussian\nprocess) and discrete domain (latent Gaussian Markov random field) models. We\nillustrate the application of our methods to challenging simulated and real\ndata examples for which maximum likelihood estimation would otherwise be very\nchallenging. Furthermore, we study an often overlooked challenge in MCML\napproaches to latent variable models: practical issues in calculating standard\nerrors of the resulting estimates, and assessing whether resulting confidence\nintervals provide nominal coverage. Our study therefore provides useful\ninsights into the details of implementing MCML algorithms for high-dimensional\nlatent variable models.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:37:02 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 08:53:05 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Park", "Jaewoo", ""], ["Haran", "Murali", ""]]}, {"id": "1910.10133", "submitter": "Arthur Charpentier", "authors": "Charpentier, Arthur and Mussard, Stephane and Tea Ouraga", "title": "Principal Component Analysis: A Generalized Gini Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A principal component analysis based on the generalized Gini correlation\nindex is proposed (Gini PCA). The Gini PCA generalizes the standard PCA based\non the variance. It is shown, in the Gaussian case, that the standard PCA is\nequivalent to the Gini PCA. It is also proven that the dimensionality reduction\nbased on the generalized Gini correlation matrix, that relies on city-block\ndistances, is robust to outliers. Monte Carlo simulations and an application on\ncars data (with outliers) show the robustness of the Gini PCA and provide\ndifferent interpretations of the results compared with the variance PCA.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 17:32:33 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Charpentier", "", ""], ["Arthur", "", ""], ["Mussard", "", ""], ["Stephane", "", ""], ["Ouraga", "Tea", ""]]}, {"id": "1910.10225", "submitter": "Pulong Ma", "authors": "Pulong Ma", "title": "Objective Bayesian Analysis of a Cokriging Model for Hierarchical\n  Multifidelity Codes", "comments": "26 pages, published", "journal-ref": null, "doi": "10.1137/19M1289893", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive cokriging models have been widely used to emulate multiple\ncomputer models with different levels of fidelity. The dependence structures\nare modeled via Gaussian processes at each level of fidelity, where covariance\nstructures are often parameterized up to a few parameters. The predictive\ndistributions typically require intensive Monte Carlo approximations in\nprevious works. This article derives new closed-form formulas to compute the\nmeans and variances of predictive distributions in autoregressive cokriging\nmodels that only depend on correlation parameters. For parameter estimation, we\nconsider objective Bayesian analysis of such autoregressive cokriging models.\nWe show that common choices of prior distributions, such as the constant prior\nand inverse correlation prior, typically lead to improper posteriors. We also\ndevelop several objective priors such as the independent reference prior and\nthe independent Jeffreys prior that are shown to yield proper posterior\ndistributions. This development is illustrated with a borehole function in an\neight-dimensional input space and applied to an engineering application in a\nsix-dimensional input space.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 20:50:31 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 16:21:11 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 18:22:19 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 17:56:42 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ma", "Pulong", ""]]}, {"id": "1910.10576", "submitter": "Tien Cuong Phi", "authors": "Tien Cuong Phi, Alexandre Muzy and Patricia Reynaud-Bouret", "title": "Event-scheduling algorithms with Kalikow decomposition for simulating\n  potentially infinite neuronal networks", "comments": null, "journal-ref": "Springer Nature Computer Science, 2020", "doi": "10.1007/s42979-019-0039-3", "report-no": null, "categories": "stat.CO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-scheduling algorithms can compute in continuous time the next\noccurrence of points (as events) of a counting process based on their current\nconditional intensity. In particular event-scheduling algorithms can be adapted\nto perform the simulation of finite neuronal networks activity. These\nalgorithms are based on Ogata's thinning strategy \\cite{Oga81}, which always\nneeds to simulate the whole network to access the behaviour of one particular\nneuron of the network. On the other hand, for discrete time models, theoretical\nalgorithms based on Kalikow decomposition can pick at random influencing\nneurons and perform a perfect simulation (meaning without approximations) of\nthe behaviour of one given neuron embedded in an infinite network, at every\ntime step. These algorithms are currently not computationally tractable in\ncontinuous time. To solve this problem, an event-scheduling algorithm with\nKalikow decomposition is proposed here for the sequential simulation of point\nprocesses neuronal models satisfying this decomposition. This new algorithm is\napplied to infinite neuronal networks whose finite time simulation is a\nprerequisite to realistic brain modeling.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:23:16 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Phi", "Tien Cuong", ""], ["Muzy", "Alexandre", ""], ["Reynaud-Bouret", "Patricia", ""]]}, {"id": "1910.10606", "submitter": "Anindya Goswami Mr.", "authors": "Milan Kumar Das and Anindya Goswami and Sharan Rajani", "title": "Inference of Binary Regime Models with Jump Discontinuities", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the instances of jumps in a discrete time series sample of a jump\ndiffusion model is a challenging task. We have developed a novel statistical\ntechnique for jump detection and volatility estimation in a return time series\ndata using a threshold method. Since we derive the threshold and the volatility\nestimator simultaneously by solving an implicit equation, we obtain\nunprecedented accuracy across a wide range of parameter values. Using this\nmethod, the increments attributed to jumps have been removed from a large\ncollection of historical data of Indian sectorial indices. Subsequently, we\ntest the presence of regime switching dynamics in the volatility coefficient\nusing a new discriminating statistic. The statistic is shown to be sensitive to\nthe transition kernel of the regime switching model. We perform the testing\nusing bootstrap method and find a clear indication of presence of multiple\nregimes of volatility in the data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 15:23:42 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 06:58:36 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 10:38:36 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Das", "Milan Kumar", ""], ["Goswami", "Anindya", ""], ["Rajani", "Sharan", ""]]}, {"id": "1910.10748", "submitter": "Koray Kachar", "authors": "Koray G. Kachar and Alex A. Gorodetsky", "title": "Dynamic multi-agent assignment via discrete optimal transport", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimal solution to a deterministic dynamic assignment problem\nby leveraging connections to the theory of discrete optimal transport to\nconvert the combinatorial assignment problem into a tractable linear program.\nWe seek to allow a multi-vehicle swarm to accomplish a dynamically changing\ntask, for example tracking a multi-target swarm. Our approach simultaneously\ndetermines the optimal assignment and the control of the individual agents. As\na result, the assignment policy accounts for the dynamics and capabilities of a\nheterogeneous set of agents and targets. In contrast to a majority of existing\nassignment schemes, this approach improves upon distance-based metrics for\nassignments by considering cost metrics that account for the underlying\ndynamics manifold. We provide a theoretical justification for the reformulation\nof this problem, and show that the minimizer of the dynamic assignment problem\nis equivalent to the minimizer of the associated Monge problem arising in\noptimal transport. We prove that by accounting for dynamics, we only require\ncomputing an assignment once over the operating lifetime --- significantly\ndecreasing computational expense. Furthermore, we show that the cost benefits\nachieved by our approach increase as the swarm size increases, achieving almost\n50\\% cost reduction compared with distance-based metrics. We demonstrate our\napproach through simulation on several linear and linearized problems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 18:08:39 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Kachar", "Koray G.", ""], ["Gorodetsky", "Alex A.", ""]]}, {"id": "1910.10779", "submitter": "Niko Hauzenberger", "authors": "Niko Hauzenberger, Florian Huber, Gary Koop, Luca Onorante", "title": "Fast and Flexible Bayesian Inference in Time-varying Parameter\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we write the time-varying parameter (TVP) regression model\ninvolving K explanatory variables and T observations as a constant coefficient\nregression model with TK explanatory variables. In contrast with much of the\nexisting literature which assumes coefficients to evolve according to a random\nwalk, a hierarchical mixture model on the TVPs is introduced. The resulting\nmodel closely mimics a random coefficients specification which groups the TVPs\ninto several regimes. These flexible mixtures allow for TVPs that feature a\nsmall, moderate or large number of structural breaks. We develop\ncomputationally efficient Bayesian econometric methods based on the singular\nvalue decomposition of the TK regressors. In artificial data, we find our\nmethods to be accurate and much faster than standard approaches in terms of\ncomputation time. In an empirical exercise involving inflation forecasting\nusing a large number of predictors, we find our methods to forecast better than\nalternative approaches and document different patterns of parameter change than\nare found with approaches which assume random walk evolution of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 19:49:55 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 10:08:14 GMT"}, {"version": "v3", "created": "Sun, 25 Apr 2021 17:56:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Hauzenberger", "Niko", ""], ["Huber", "Florian", ""], ["Koop", "Gary", ""], ["Onorante", "Luca", ""]]}, {"id": "1910.10854", "submitter": "Ursula Laa", "authors": "Ursula Laa, Dianne Cook, German Valencia", "title": "A slice tour for finding hollowness in high-dimensional data", "comments": "13 pages, 6 figures", "journal-ref": "Journal of Computational and Graphical Statistics 29 (2020)\n  681-687", "doi": "10.1080/10618600.2020.1777140", "report-no": null, "categories": "stat.CO cs.HC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking projections of high-dimensional data is a common analytical and\nvisualisation technique in statistics for working with high-dimensional\nproblems. Sectioning, or slicing, through high dimensions is less common, but\ncan be useful for visualising data with concavities, or non-linear structure.\nIt is associated with conditional distributions in statistics, and also linked\nbrushing between plots in interactive data visualisation. This short technical\nnote describes a simple approach for slicing in the orthogonal space of\nprojections obtained when running a tour, thus presenting the viewer with an\ninterpolated sequence of sliced projections. The method has been implemented in\nR as an extension to the tourr package, and can be used to explore for concave\nand non-linear structures in multivariate distributions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:27:27 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Laa", "Ursula", ""], ["Cook", "Dianne", ""], ["Valencia", "German", ""]]}, {"id": "1910.10898", "submitter": "Abdul-Nasah Soale", "authors": "Abdul-Nasah Soale, Yuexiao Dong", "title": "On expectile-assisted inverse regression estimation for sufficient\n  dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moment-based sufficient dimension reduction methods such as sliced inverse\nregression may not work well in the presence of heteroscedasticity. We propose\nto first estimate the expectiles through kernel expectile regression, and then\ncarry out dimension reduction based on random projections of the regression\nexpectiles. Several popular inverse regression methods in the literature are\nextended under this general framework. The proposed expectile-assisted methods\noutperform existing moment-based dimension reduction methods in both numerical\nstudies and an analysis of the Big Mac data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 03:22:18 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:13:09 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Soale", "Abdul-Nasah", ""], ["Dong", "Yuexiao", ""]]}, {"id": "1910.11219", "submitter": "Onur Teymur", "authors": "Onur Teymur and Sarah Filippi", "title": "A Bayesian nonparametric test for conditional independence", "comments": null, "journal-ref": "Foundations of Data Science (2020) 2(2):155-172", "doi": "10.3934/fods.2020009", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a Bayesian nonparametric method for quantifying the\nrelative evidence in a dataset in favour of the dependence or independence of\ntwo variables conditional on a third. The approach uses Polya tree priors on\nspaces of conditional probability densities, accounting for uncertainty in the\nform of the underlying distributions in a nonparametric way. The Bayesian\nperspective provides an inherently symmetric probability measure of conditional\ndependence or independence, a feature particularly advantageous in causal\ndiscovery and not employed in existing procedures of this type.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:23:49 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 19:33:53 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Teymur", "Onur", ""], ["Filippi", "Sarah", ""]]}, {"id": "1910.11429", "submitter": "Peter Holderrieth", "authors": "Peter Holderrieth", "title": "Cores for Piecewise-Deterministic Markov Processes used in Markov Chain\n  Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show fundamental properties of the Markov semigroup of recently proposed\nMCMC algorithms based on piecewise-deterministic Markov processes (PDMPs) such\nas the Bouncy Particle Sampler, the Zig Zag Process or the Randomized\nHamiltonian Monte Carlo method. Under assumptions typically satisfied in MCMC\nsettings, we prove that PDMPs are Feller processes and the space of infinitely\ndifferentiable functions with compact support forms a core of their generator.\nAs we illustrate via martingale problems and a simplified proof of the\ninvariance of target distributions, these results provide a fundamental tool\nfor the rigorous analysis of these algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 17:15:07 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 21:07:20 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 21:34:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Holderrieth", "Peter", ""]]}, {"id": "1910.11445", "submitter": "Fan Yin", "authors": "Fan Yin, Weining Shen, Carter T. Butts", "title": "Finite Mixtures of ERGMs for Modeling Ensembles of Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of networks arise in many scientific fields, but there are few\nstatistical tools for inferring their generative processes, particularly in the\npresence of both dyadic dependence and cross-graph heterogeneity. To fill in\nthis gap, we propose characterizing network ensembles via finite mixtures of\nexponential family random graph models, a framework for parametric statistical\nmodeling of graphs that has been successful in explicitly modeling the complex\nstochastic processes that govern the structure of edges in a network. Our\nproposed modeling framework can also be used for applications such as\nmodel-based clustering of ensembles of networks and density estimation for\ncomplex graph distributions. We develop a Metropolis-within-Gibbs algorithm to\nconduct fully Bayesian inference and adapt a version of deviance information\ncriterion for missing data models to choose the number of latent heterogeneous\ngenerative mechanisms. Simulation studies show that the proposed procedure can\nrecover the true number of latent heterogeneous generative processes and\ncorresponding parameters. We demonstrate the utility of the proposed approach\nusing an ensemble of political co-voting networks among U.S. Senators.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 22:37:10 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 05:07:52 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 04:14:20 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Yin", "Fan", ""], ["Shen", "Weining", ""], ["Butts", "Carter T.", ""]]}, {"id": "1910.11720", "submitter": "Stefan Engblom", "authors": "Stefan Engblom and Robin Eriksson and Stefan Widgren", "title": "Bayesian epidemiological modeling over high-resolution network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.soc-ph q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical epidemiological models have a broad use, including both\nqualitative and quantitative applications. With the increasing availability of\ndata, large-scale quantitative disease spread models can nowadays be\nformulated. Such models have a great potential, e.g., in risk assessments in\npublic health. Their main challenge is model parameterization given\nsurveillance data, a problem which often limits their practical usage.\n  We offer a solution to this problem by developing a Bayesian methodology\nsuitable to epidemiological models driven by network data. The greatest\ndifficulty in obtaining a concentrated parameter posterior is the quality of\nsurveillance data; disease measurements are often scarce and carry little\ninformation about the parameters. The often overlooked problem of the model's\nidentifiability therefore needs to be addressed, and we do so using a hierarchy\nof increasingly realistic known truth experiments.\n  Our proposed Bayesian approach performs convincingly across all our synthetic\ntests. From pathogen measurements of shiga toxin-producing Escherichia coli\nO157 in Swedish cattle, we are able to produce an accurate statistical model of\nfirst-principles confronted with data. Within this model we explore the\npotential of a Bayesian public health framework by assessing the efficiency of\ndisease detection and -intervention scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 13:36:08 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 12:06:22 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Engblom", "Stefan", ""], ["Eriksson", "Robin", ""], ["Widgren", "Stefan", ""]]}, {"id": "1910.11953", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob, Ruobin Gong, Paul T. Edlefsen, Arthur P. Dempster", "title": "A Gibbs sampler for a class of random convex polytopes", "comments": "23 pages including the references and appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Gibbs sampler for the Dempster-Shafer (DS) approach to\nstatistical inference for Categorical distributions. The DS framework extends\nthe Bayesian approach, allows in particular the use of partial prior\ninformation, and yields three-valued uncertainty assessments representing\nprobabilities \"for\", \"against\", and \"don't know\" about formal assertions of\ninterest. The proposed algorithm targets the distribution of a class of random\nconvex polytopes which encapsulate the DS inference. The sampler relies on an\nequivalence between the iterative constraints of the vertex configuration and\nthe non-negativity of cycles in a fully connected directed graph. Illustrations\ninclude the testing of independence in 2x2 contingency tables and parameter\nestimation of the linkage model.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 22:04:07 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:29:48 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 20:34:07 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Jacob", "Pierre E.", ""], ["Gong", "Ruobin", ""], ["Edlefsen", "Paul T.", ""], ["Dempster", "Arthur P.", ""]]}, {"id": "1910.12126", "submitter": "Zeda Li", "authors": "Zeda Li, Ori Rosen, Fabio Ferrarelli, and Robert T. Krafty", "title": "Adaptive Bayesian Spectral Analysis of High-dimensional Nonstationary\n  Time Series", "comments": "7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a nonparametric approach to spectral analysis of a\nhigh-dimensional multivariate nonstationary time series. The procedure is based\non a novel frequency-domain factor model that provides a flexible yet\nparsimonious representation of spectral matrices from a large number of\nsimultaneously observed time series. Real and imaginary parts of the factor\nloading matrices are modeled independently using a prior that is formulated\nfrom the tensor product of penalized splines and multiplicative gamma process\nshrinkage priors, allowing for infinitely many factors with loadings\nincreasingly shrunk towards zero as the column index increases. Formulated in a\nfully Bayesian framework, the time series is adaptively partitioned into\napproximately stationary segments, where both the number and location of\npartition points are assumed unknown. Stochastic approximation Monte Carlo\n(SAMC) techniques are used to accommodate the unknown number of segments, and a\nconditional Whittle likelihood-based Gibbs sampler is developed for efficient\nsampling within segments. By averaging over the distribution of partitions, the\nproposed method can approximate both abrupt and slowly varying changes in\nspectral matrices. Performance of the proposed model is evaluated by extensive\nsimulations and demonstrated through the analysis of high-density\nelectroencephalography.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 19:59:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Li", "Zeda", ""], ["Rosen", "Ori", ""], ["Ferrarelli", "Fabio", ""], ["Krafty", "Robert T.", ""]]}, {"id": "1910.12431", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, Gianluca Detommaso, Robert Scheichl", "title": "Multilevel Dimension-Independent Likelihood-Informed MCMC for\n  Large-Scale Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a non-trivial integration of dimension-independent\nlikelihood-informed (DILI) MCMC (Cui, Law, Marzouk, 2016) and the multilevel\nMCMC (Dodwell et al., 2015) to explore the hierarchy of posterior\ndistributions. This integration offers several advantages: First, DILI-MCMC\nemploys an intrinsic likelihood-informed subspace (LIS) (Cui et al., 2014) --\nwhich involves a number of forward and adjoint model simulations -- to design\naccelerated operator-weighted proposals. By exploiting the multilevel structure\nof the discretised parameters and discretised forward models, we design a\nRayleigh-Ritz procedure to significantly reduce the computational effort in\nbuilding the LIS and operating with DILI proposals. Second, the resulting\nDILI-MCMC can drastically improve the sampling efficiency of MCMC at each\nlevel, and hence reduce the integration error of the multilevel algorithm for\nfixed CPU time. To be able to fully exploit the power of multilevel MCMC and to\nreduce the dependencies of samples on different levels for a parallel\nimplementation, we also suggest a new pooling strategy for allocating\ncomputational resources across different levels and constructing Markov chains\nat higher levels conditioned on those simulated on lower levels. Numerical\nresults confirm the improved computational efficiency of the multilevel DILI\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 04:10:30 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Cui", "Tiangang", ""], ["Detommaso", "Gianluca", ""], ["Scheichl", "Robert", ""]]}, {"id": "1910.12815", "submitter": "Kimia Nadjahi", "authors": "Kimia Nadjahi, Valentin De Bortoli, Alain Durmus, Roland Badeau, Umut\n  \\c{S}im\\c{s}ekli", "title": "Approximate Bayesian Computation with the Sliced-Wasserstein Distance", "comments": "Accepted at ICASSP 2020 (publication and oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a popular method for approximate\ninference in generative models with intractable but easy-to-sample likelihood.\nIt constructs an approximate posterior distribution by finding parameters for\nwhich the simulated data are close to the observations in terms of summary\nstatistics. These statistics are defined beforehand and might induce a loss of\ninformation, which has been shown to deteriorate the quality of the\napproximation. To overcome this problem, Wasserstein-ABC has been recently\nproposed, and compares the datasets via the Wasserstein distance between their\nempirical distributions, but does not scale well to the dimension or the number\nof samples. We propose a new ABC technique, called Sliced-Wasserstein ABC and\nbased on the Sliced-Wasserstein distance, which has better computational and\nstatistical properties. We derive two theoretical results showing the\nasymptotical consistency of our approach, and we illustrate its advantages on\nsynthetic data and an image denoising task.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:18:25 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 10:09:02 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Nadjahi", "Kimia", ""], ["De Bortoli", "Valentin", ""], ["Durmus", "Alain", ""], ["Badeau", "Roland", ""], ["\u015eim\u015fekli", "Umut", ""]]}, {"id": "1910.12925", "submitter": "Metin Bulus", "authors": "Metin Bulus", "title": "Minimum Detectable Effect Size Computations for Cluster-Level Regression\n  Discontinuity: Quadratic Functional Form and Beyond", "comments": "Please do not cite this draft without author's permission. It\n  includes many typos and errors (final derivations, conclusions and\n  implications do not change). It also criticizes rdpower R and Stata commands\n  developed by Cattaneo, Titiunik, and Vazquez-Bare (2019) in the wrong context\n  (rdpower allows ex-ante power computations). Issues are fixed in a\n  peer-reviewed draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study extends power formulas proposed by Schochet (2008) assuming that\nthe cluster-level score variable follows quadratic functional form. Results\nreveal that we need not be concerned with treatment by linear term interaction,\nand polynomial degree up to second order for symmetric truncation intervals. In\ncomparison, every slight change in the functional form alters sample size\nrequirements for asymmetric truncation intervals. Finally, an empirical\nframework beyond quadratic functional form is provided when the asymptotic\nvariance of the treatment effect is untraceable. In this case, the CRD design\neffect is either computed from moments of the sample or approximate population\nmoments via simulation. Formulas for quadratic functional form and the extended\nempirical framework are implemented in the cosa R package and companion Shiny\nweb application.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 19:32:00 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 07:59:00 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Bulus", "Metin", ""]]}, {"id": "1910.13013", "submitter": "Simon Tindemans", "authors": "Simon Tindemans and Goran Strbac", "title": "Accelerating System Adequacy Assessment using the Multilevel Monte Carlo\n  Approach", "comments": null, "journal-ref": "Electric Power Systems Research, Volume 189, 106740, (2020)", "doi": "10.1016/j.epsr.2020.106740", "report-no": null, "categories": "stat.CO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately and efficiently estimating system performance under uncertainty is\nparamount in power system planning and operation. Monte Carlo simulation is\noften used for this purpose, but convergence may be slow, especially when\ndetailed models are used. Previously published methods to speed up computations\nmay severely constrain model complexity, limiting their real-world\neffectiveness. This paper uses the recently proposed Multilevel Monte Carlo\n(MLMC) framework, which combines outputs from a hierarchy of simulators to\nboost computational efficiency without sacrificing accuracy. It explains which\nrequirements the MLMC framework imposes on the model hierarchy, and how these\nnaturally occur in power system adequacy assessment problems. Two adequacy\nassessment examples are studied in detail: a composite system and a system with\nheterogeneous storage units. An intuitive speed metric is introduced for easy\ncomparison of simulation setups. Depending on the problem and metric of\ninterest, large speedups can be obtained.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 00:07:35 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 21:20:11 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Tindemans", "Simon", ""], ["Strbac", "Goran", ""]]}, {"id": "1910.13048", "submitter": "Jeffrey Wong", "authors": "Jeffrey Wong", "title": "Efficient Computation for Centered Linear Regression with Sparse Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression with sparse inputs is a common theme for large scale models.\nOptimizing the underlying linear algebra for sparse inputs allows such models\nto be estimated faster. At the same time, centering the inputs has benefits in\nimproving the interpretation and convergence of the model. However, centering\nthe data naturally makes sparse data become dense, limiting opportunities for\noptimization. We propose an efficient strategy that estimates centered\nregression while taking advantage of sparse structure in data, improving\ncomputational performance and decreasing the memory footprint of the estimator.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 02:34:22 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Wong", "Jeffrey", ""]]}, {"id": "1910.13273", "submitter": "Jorge Ignacio Gonz\\'alez C\\'azares", "authors": "Jorge Ignacio Gonz\\'alez C\\'azares, Aleksandar Mijatovi\\'c, Ger\\'onimo\n  Uribe Bravo", "title": "$\\varepsilon$-strong simulation of the convex minorants of stable\n  processes and meanders", "comments": "38 pages, 8 figures", "journal-ref": "Electron. J. Probab. 25 (2020) 1-33", "doi": "10.1214/20-EJP503", "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using marked Dirichlet processes we characterise the law of the convex\nminorant of the meander for a certain class of L\\'evy processes, which includes\nsubordinated stable and symmetric L\\'evy processes. We apply this\ncharacterisaiton to construct $\\varepsilon$-strong simulation ($\\varepsilon$SS)\nalgorithms for the convex minorant of stable meanders, the finite dimensional\ndistributions of stable meanders and the convex minorants of weakly stable\nprocesses. We prove that the running times of our $\\varepsilon$SS algorithms\nhave finite exponential moments. We implement the algorithms in Julia 1.0\n(available on GitHub) and present numerical examples supporting our convergence\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 13:54:25 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["C\u00e1zares", "Jorge Ignacio Gonz\u00e1lez", ""], ["Mijatovi\u0107", "Aleksandar", ""], ["Bravo", "Ger\u00f3nimo Uribe", ""]]}, {"id": "1910.13627", "submitter": "Matias Quiroz", "authors": "Robert Salomone, Matias Quiroz, Robert Kohn, Mattias Villani,\n  Minh-Ngoc Tran", "title": "Spectral Subsampling MCMC for Stationary Time Series", "comments": "Empirical section significantly revised and extended", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference using Markov Chain Monte Carlo (MCMC) on large datasets\nhas developed rapidly in recent years. However, the underlying methods are\ngenerally limited to relatively simple settings where the data have specific\nforms of independence. We propose a novel technique for speeding up MCMC for\ntime series data by efficient data subsampling in the frequency domain. For\nseveral challenging time series models, we demonstrate a speedup of up to two\norders of magnitude while incurring negligible bias compared to MCMC on the\nfull dataset. We also propose alternative control variates for variance\nreduction based on data grouping and coreset constructions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 02:31:56 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 00:27:50 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Salomone", "Robert", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Villani", "Mattias", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "1910.13674", "submitter": "Aleksandr Aravkin", "authors": "Jonathan Jonker, Peng Zheng, and Aleksandr Y. Aravkin", "title": "Efficient Robust Parameter Identification in Generalized Kalman\n  Smoothing Models", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic inference problems in autoregressive (AR/ARMA/ARIMA), exponential\nsmoothing, and navigation are often formulated and solved using state-space\nmodels (SSM), which allow a range of statistical distributions to inform\ninnovations and errors. In many applications the main goal is to identify not\nonly the hidden state, but also additional unknown model parameters (e.g. AR\ncoefficients or unknown dynamics).\n  We show how to efficiently optimize over model parameters in SSM that use\nsmooth process and measurement losses. Our approach is to project out state\nvariables, obtaining a value function that only depends on the parameters of\ninterest, and derive analytical formulas for first and second derivatives that\ncan be used by many types of optimization methods.\n  The approach can be used with smooth robust penalties such as Hybrid and the\nStudent's t, in addition to classic least squares. We use the approach to\nestimate robust AR models and long-run unemployment rates with sudden changes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:09:51 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Jonker", "Jonathan", ""], ["Zheng", "Peng", ""], ["Aravkin", "Aleksandr Y.", ""]]}, {"id": "1910.13848", "submitter": "Antonio Forcina", "authors": "Antonio Forcina, Maria Kateri", "title": "An extended class of RC association models: estimation and main\n  properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extended class of multiplicative row-column (RC) association models,\nintroduced in this paper for two-way contingency tables, allows users to select\nboth the type of logit (local, global, continuation, reverse continuation)\nsuitable for the row and column classification variables and the scale on which\ninteractions are measured. As in \\cite{Kateri95} for the case of local logits,\nour extended class of bivariate interactions is linked to divergence measures\nand, by means of a representation theorem, we provide reconstruction formulas\nfor the joint probabilities depending on pairs of logit types. These results\nare the key to show that, given marginal logits, our extended interactions\ndetermine uniquely the bivariate distribution. We also determine the kind of\npositive association which is implied by our extended interactions being non\nnegative. Quick model selection within this wide class can be performed by an\nefficient algorithm for computing maximum likelihood estimates which exploits\nthe properties of a reduced rank constraint imposed on the matrix of extended\ninteractions and allows for additional linear constraint on marginal logits. An\napplication to social mobility data is presented and discussed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:44:44 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 10:35:57 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 20:26:47 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2020 06:29:28 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Forcina", "Antonio", ""], ["Kateri", "Maria", ""]]}, {"id": "1910.14078", "submitter": "Subharup Guha", "authors": "Subharup Guha and Sujit K. Ghosh", "title": "Probabilistic Detection and Estimation of Conic Sections from Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring unknown conic sections on the basis of noisy data is a challenging\nproblem with applications in computer vision. A major limitation of the\ncurrently available methods for conic sections is that estimation methods rely\non the underlying shape of the conics (being known to be ellipse, parabola or\nhyperbola). A general purpose Bayesian hierarchical model is proposed for conic\nsections and corresponding estimation method based on noisy data is shown to\nwork even when the specific nature of the conic section is unknown. The model,\nthus, provides probabilistic detection of the underlying conic section and\ninference about the associated parameters of the conic section. Through\nextensive simulation studies where the true conics may not be known, the\nmethodology is demonstrated to have practical and methodological advantages\nrelative to many existing techniques. In addition, the proposed method provides\nprobabilistic measures of uncertainty of the estimated parameters. Furthermore,\nwe observe high fidelity to the true conics even in challenging situations,\nsuch as data arising from partial conics in arbitrarily rotated and\nnon-standard form, and where a visual inspection is unable to correctly\nidentify the type of conic section underlying the data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:42:54 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 19:16:19 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Guha", "Subharup", ""], ["Ghosh", "Sujit K.", ""]]}, {"id": "1910.14101", "submitter": "Mark Risser", "authors": "Mark D. Risser and Daniel Turek", "title": "Bayesian inference for high-dimensional nonstationary Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the diverse literature on nonstationary spatial modeling and\napproximate Gaussian process (GP) methods, there are no general approaches for\nconducting fully Bayesian inference for moderately sized nonstationary spatial\ndata sets on a personal laptop. For statisticians and data scientists who wish\nto learn about spatially-referenced data and conduct posterior inference and\nprediction with appropriate uncertainty quantification, the lack of such\napproaches and corresponding software is a significant limitation. In this\npaper, we develop methodology for implementing formal Bayesian inference for a\ngeneral class of nonstationary GPs. Our novel approach uses pre-existing\nframeworks for characterizing nonstationarity in a new way that is applicable\nfor small to moderately sized data sets via modern GP likelihood\napproximations. Posterior sampling is implemented using flexible MCMC methods,\nwith nonstationary posterior prediction conducted as a post-processing step. We\ndemonstrate our novel methods on two data sets, ranging from several hundred to\nseveral thousand locations, and compare our methodology with related\nstatistical methods that provide off-the-shelf software. All of our methods are\nimplemented in the freely available BayesNSGP software package for R.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 19:41:57 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 16:42:04 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Risser", "Mark D.", ""], ["Turek", "Daniel", ""]]}, {"id": "1910.14145", "submitter": "Anna Wigren", "authors": "Anna Wigren, Riccardo Sven Risuleo, Lawrence Murray, Fredrik Lindsten", "title": "Parameter elimination in particle Gibbs sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference in state-space models is challenging due to\nhigh-dimensional state trajectories. A viable approach is particle Markov chain\nMonte Carlo, combining MCMC and sequential Monte Carlo to form \"exact\napproximations\" to otherwise intractable MCMC methods. The performance of the\napproximation is limited to that of the exact method. We focus on particle\nGibbs and particle Gibbs with ancestor sampling, improving their performance\nbeyond that of the underlying Gibbs sampler (which they approximate) by\nmarginalizing out one or more parameters. This is possible when the parameter\nprior is conjugate to the complete data likelihood. Marginalization yields a\nnon-Markovian model for inference, but we show that, in contrast to the general\ncase, this method still scales linearly in time. While marginalization can be\ncumbersome to implement, recent advances in probabilistic programming have\nenabled its automation. We demonstrate how the marginalized methods are viable\nas efficient inference backends in probabilistic programming, and demonstrate\nwith examples in ecology and epidemiology.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 21:23:05 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Wigren", "Anna", ""], ["Risuleo", "Riccardo Sven", ""], ["Murray", "Lawrence", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1910.14227", "submitter": "Anthony Ebert", "authors": "Anthony Ebert, Pierre Pudlo, Kerrie Mengersen, Paul Wu, Christopher\n  Drovandi", "title": "Combined parameter and state inference with automatically calibrated ABC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State space models contain time-indexed parameters, termed states, as well as\nstatic parameters, simply termed parameters. The problem of inferring both\nstatic parameters as well as states simultaneously, based on time-indexed\nobservations, is the subject of much recent literature. This problem is\ncompounded once we consider models with intractable likelihoods. In these\nsituations, some emerging approaches have incorporated existing likelihood-free\ntechniques for static parameters, such as approximate Bayesian computation\n(ABC) into likelihood-based algorithms for combined inference of parameters and\nstates. These emerging approaches currently require extensive manual\ncalibration of a time-indexed tuning parameter: the acceptance threshold.\n  We design an SMC$^2$ algorithm (Chopin et al., 2013, JRSS B) for\nlikelihood-free approximation with automatically tuned thresholds. We prove\nconsistency of the algorithm and discuss the proposed calibration. We\ndemonstrate this algorithm's performance with three examples. We begin with two\nexamples of state space models. The first example is a toy example, with an\nemission distribution that is a skew normal distribution. The second example is\na stochastic volatility model involving an intractable stable distribution. The\nlast example is the most challenging; it deals with an inhomogeneous Hawkes\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 03:03:44 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 22:58:27 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ebert", "Anthony", ""], ["Pudlo", "Pierre", ""], ["Mengersen", "Kerrie", ""], ["Wu", "Paul", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1910.14290", "submitter": "Dimitris Kugiumtzis", "authors": "Elsa Siggiridou, Christos Koutlis, Alkiviadis Tsimpiris and Dimitris\n  Kugiumtzis", "title": "Evaluation of Granger causality measures for constructing networks from\n  multivariate time series", "comments": "24 pages, 5 figures, to be published in Entropy", "journal-ref": null, "doi": "10.3390/e21111080", "report-no": null, "categories": "stat.CO cs.IT math.IT nlin.CD physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality and variants of this concept allow the study of complex\ndynamical systems as networks constructed from multivariate time series. In\nthis work, a large number of Granger causality measures used to form causality\nnetworks from multivariate time series are assessed. These measures are in the\ntime domain, such as model-based and information measures, the frequency domain\nand the phase domain. The study aims also to compare bivariate and multivariate\nmeasures, linear and nonlinear measures, as well as the use of dimension\nreduction in linear model-based measures and information measures. The latter\nis particular relevant in the study of high-dimensional time series. For the\nperformance of the multivariate causality measures, low and high dimensional\ncoupled dynamical systems are considered in discrete and continuous time, as\nwell as deterministic and stochastic. The measures are evaluated and ranked\naccording to their ability to provide causality networks that match the\noriginal coupling structure. The simulation study concludes that the Granger\ncausality measures using dimension reduction are superior and should be\npreferred particularly in studies involving many observed variables, such as\nmulti-channel electroencephalograms and financial markets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 07:58:19 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Siggiridou", "Elsa", ""], ["Koutlis", "Christos", ""], ["Tsimpiris", "Alkiviadis", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1910.14590", "submitter": "Rom\\'an Salmer\\'on", "authors": "Rom\\'an Salmer\\'on and Catalina Garc\\'ia and Jos\\'e Garc\\'ia", "title": "\"multiColl\": An R package to detect multicollinearity", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a guide for the use of some of the functions of the R\npackage \"multiColl\" for the detection of near multicollinearity. The main\ncontribution, in comparison to other existing packages in R or other\neconometric software, is the treatment of qualitative independent variables and\nthe intercept in the simple/multiple linear regression model.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:43:02 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Salmer\u00f3n", "Rom\u00e1n", ""], ["Garc\u00eda", "Catalina", ""], ["Garc\u00eda", "Jos\u00e9", ""]]}]