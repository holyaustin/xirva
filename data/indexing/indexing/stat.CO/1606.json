[{"id": "1606.00142", "submitter": "Ning Xu", "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher", "title": "Model selection consistency from the perspective of generalization\n  ability and VC theory with an application to Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.EC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is difficult to analyse yet theoretically and empirically\nimportant, especially for high-dimensional data analysis. Recently the least\nabsolute shrinkage and selection operator (Lasso) has been applied in the\nstatistical and econometric literature. Consis- tency of Lasso has been\nestablished under various conditions, some of which are difficult to verify in\npractice. In this paper, we study model selection from the perspective of\ngeneralization ability, under the framework of structural risk minimization\n(SRM) and Vapnik-Chervonenkis (VC) theory. The approach emphasizes the balance\nbetween the in-sample and out-of-sample fit, which can be achieved by using\ncross-validation to select a penalty on model complexity. We show that an exact\nrelationship exists between the generalization ability of a model and model\nselection consistency. By implementing SRM and the VC inequality, we show that\nLasso is L2-consistent for model selection under assumptions similar to those\nimposed on OLS. Furthermore, we derive a probabilistic bound for the distance\nbetween the penalized extremum estimator and the extremum estimator without\npenalty, which is dominated by overfitting. We also propose a new measurement\nof overfitting, GR2, based on generalization ability, that converges to zero if\nmodel selection is consistent. Using simulations, we demonstrate that the\nproposed CV-Lasso algorithm performs well in terms of model selection and\noverfitting control.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 07:22:01 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Xu", "Ning", ""], ["Hong", "Jian", ""], ["Fisher", "Timothy C. G.", ""]]}, {"id": "1606.00242", "submitter": "Rune Juhl", "authors": "Rune Juhl, Jan Kloppenborg M{\\o}ller, Henrik Madsen", "title": "ctsmr - Continuous Time Stochastic Modeling in R", "comments": "11 pages, including R code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ctsmr is an R package providing a general framework for identifying and\nestimating partially observed continuous-discrete time gray-box models. The\nestimation is based on maximum likelihood principles and Kalman filtering\nefficiently implemented in Fortran. This paper briefly demonstrates how to\nconstruct a Continuous Time Stochastic Model using multivariate time series\ndata, and how to estimate the embedded parameters. The setup provides a unique\nframework for statistical modeling of physical phenomena, and the approach is\noften called grey box modeling. Finally three examples are provided to\ndemonstrate the capabilities of ctsmr.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 11:51:49 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Juhl", "Rune", ""], ["M\u00f8ller", "Jan Kloppenborg", ""], ["Madsen", "Henrik", ""]]}, {"id": "1606.00451", "submitter": "Jacob Bien", "authors": "Jacob Bien", "title": "Graph-Guided Banding of the Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization has become a primary tool for developing reliable estimators\nof the covariance matrix in high-dimensional settings. To curb the curse of\ndimensionality, numerous methods assume that the population covariance (or\ninverse covariance) matrix is sparse, while making no particular structural\nassumptions on the desired pattern of sparsity. A highly-related, yet\ncomplementary, literature studies the specific setting in which the measured\nvariables have a known ordering, in which case a banded population matrix is\noften assumed. While the banded approach is conceptually and computationally\neasier than asking for \"patternless sparsity,\" it is only applicable in very\nspecific situations (such as when data are measured over time or\none-dimensional space). This work proposes a generalization of the notion of\nbandedness that greatly expands the range of problems in which banded\nestimators apply.\n  We develop convex regularizers occupying the broad middle ground between the\nformer approach of \"patternless sparsity\" and the latter reliance on having a\nknown ordering. Our framework defines bandedness with respect to a known graph\non the measured variables. Such a graph is available in diverse situations, and\nwe provide a theoretical, computational, and applied treatment of two new\nestimators. An R package, called ggb, implements these new methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 20:01:02 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 20:27:43 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Bien", "Jacob", ""]]}, {"id": "1606.00464", "submitter": "Christian Panse", "authors": "Christian Panse", "title": "Rectangular Statistical Cartograms in R: The recmap Package", "comments": "26 pages, 13 figures, two tables. Accepted at the Journal of\n  Statistical Software (#2711). The source code is available on CRAN, see\n  https://CRAN.R-project.org/package=recmap", "journal-ref": "Journal of Statistical Software, Code Snippets, 86(1), 1-27.\n  (2018)", "doi": "10.18637/jss.v086.c01", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartogram drawing is a technique for showing geography-related statistical\ninformation, such as demographic and epidemiological data. The idea is to\ndistort a map by resizing its regions according to a statistical parameter by\nkeeping the map recognizable. This article describes an R package implementing\nan algorithm called RecMap which approximates every map region by a rectangle\nwhere the area corresponds to the given statistical value (maintain zero\ncartographic error). The package implements the computationally intensive tasks\nin C++. This paper's contribution is that it demonstrates on real and synthetic\nmaps how recmap can be used, how it is implemented and used with other\nstatistical packages.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 20:48:23 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 09:35:08 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Panse", "Christian", ""]]}, {"id": "1606.00546", "submitter": "Florian Ziel", "authors": "Florian Ziel, Carsten Croonenbroeck, Daniel Ambach", "title": "Forecasting wind power - Modeling periodic and non-linear effects under\n  conditional heteroscedasticity", "comments": null, "journal-ref": "Applied Energy, 177 (2016) 285-297", "doi": "10.1016/j.apenergy.2016.05.111", "report-no": null, "categories": "stat.AP stat.CO stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present an approach that enables joint wind speed and wind\npower forecasts for a wind park. We combine a multivariate seasonal time\nvarying threshold autoregressive moving average (TVARMA) model with a power\nthreshold generalized autoregressive conditional heteroscedastic (power-TGARCH)\nmodel. The modeling framework incorporates diurnal and annual periodicity\nmodeling by periodic B-splines, conditional heteroscedasticity and a complex\nautoregressive structure with non-linear impacts. In contrast to usually\ntime-consuming estimation approaches as likelihood estimation, we apply a\nhigh-dimensional shrinkage technique. We utilize an iteratively re-weighted\nleast absolute shrinkage and selection operator (lasso) technique. It allows\nfor conditional heteroscedasticity, provides fast computing times and\nguarantees a parsimonious and regularized specification, even though the\nparameter space may be vast. We are able to show that our approach provides\naccurate forecasts of wind power at a turbine-specific level for forecasting\nhorizons of up to 48 h (short- to medium-term forecasts).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 06:01:14 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Ziel", "Florian", ""], ["Croonenbroeck", "Carsten", ""], ["Ambach", "Daniel", ""]]}, {"id": "1606.00787", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Eric Xing", "title": "Post-Inference Prior Swapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Bayesian methods are praised for their ability to incorporate useful\nprior knowledge, in practice, convenient priors that allow for computationally\ncheap or tractable inference are commonly used. In this paper, we investigate\nthe following question: for a given model, is it possible to compute an\ninference result with any convenient false prior, and afterwards, given any\ntarget prior of interest, quickly transform this result into the target\nposterior? A potential solution is to use importance sampling (IS). However, we\ndemonstrate that IS will fail for many choices of the target prior, depending\non its parametric form and similarity to the false prior. Instead, we propose\nprior swapping, a method that leverages the pre-inferred false posterior to\nefficiently generate accurate posterior samples under arbitrary target priors.\nPrior swapping lets us apply less-costly inference algorithms to certain\nmodels, and incorporate new or updated prior information \"post-inference\". We\ngive theoretical guarantees about our method, and demonstrate it empirically on\na number of models and priors.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 18:20:35 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 18:01:17 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Neiswanger", "Willie", ""], ["Xing", "Eric", ""]]}, {"id": "1606.00980", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en, Anders Eklund, David Bolin and Mattias Villani", "title": "Fast Bayesian whole-brain fMRI analysis with spatial 3D priors", "comments": null, "journal-ref": "NeuroImage (2017), vol. 146, 211-225", "doi": "10.1016/j.neuroimage.2016.11.040", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial whole-brain Bayesian modeling of task-related functional magnetic\nresonance imaging (fMRI) is a great computational challenge. Most of the\ncurrently proposed methods therefore do inference in subregions of the brain\nseparately or do approximate inference without comparison to the true posterior\ndistribution. A popular such method, which is now the standard method for\nBayesian single subject analysis in the SPM software, is introduced in Penny et\nal. (2005b). The method processes the data slice-by-slice and uses an\napproximate variational Bayes (VB) estimation algorithm that enforces posterior\nindependence between activity coefficients in different voxels. We introduce a\nfast and practical Markov chain Monte Carlo (MCMC) scheme for exact inference\nin the same model, both slice-wise and for the whole brain using a 3D prior on\nactivity coefficients. The algorithm exploits sparsity and uses modern\ntechniques for efficient sampling from high-dimensional Gaussian distributions,\nleading to speed-ups without which MCMC would not be a practical option. Using\nMCMC, we are for the first time able to evaluate the approximate VB posterior\nagainst the exact MCMC posterior, and show that VB can lead to spurious\nactivation. In addition, we develop an improved VB method that drops the\nassumption of independent voxels a posteriori. This algorithm is shown to be\nmuch faster than both MCMC and the original VB for large datasets, with\nnegligible error compared to the MCMC posterior.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 06:40:19 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 08:40:58 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Eklund", "Anders", ""], ["Bolin", "David", ""], ["Villani", "Mattias", ""]]}, {"id": "1606.01016", "submitter": "Alexandre Thiery", "authors": "Deborshee Sen, Alexandre Thiery and Ajay Jasra", "title": "On Coupling Particle Filter Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters are a powerful and flexible tool for performing inference on\nstate-space models. They involve a collection of samples evolving over time\nthrough a combination of sampling and re-sampling steps. The re-sampling step\nis necessary to ensure that weight degeneracy is avoided. In several situations\nof statistical interest, it is important to be able to compare the estimates\nproduced by two different particle filters; consequently, being able to\nefficiently couple two particle filter trajectories is often of paramount\nimportance. In this text, we propose several ways to do so. In particular, we\nleverage ideas from the optimal transportation literature. In general, though,\ncomputing the optimal transport map is extremely computationally expensive; to\ndeal with this, we introduce computationally tractable approximations to\noptimal transport couplings. We demonstrate that our resulting algorithms for\ncoupling two particle filter trajectories often perform orders of magnitude\nmore efficiently than more standard approaches.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 09:31:54 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 04:36:32 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Sen", "Deborshee", ""], ["Thiery", "Alexandre", ""], ["Jasra", "Ajay", ""]]}, {"id": "1606.01156", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Coupling of Particle Filters", "comments": "Technical report, 24 pages for the main document + 18 pages of\n  appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters provide Monte Carlo approximations of intractable quantities\nsuch as point-wise evaluations of the likelihood in state space models. In many\nscenarios, the interest lies in the comparison of these quantities as some\nparameter or input varies. To facilitate such comparisons, we introduce and\nstudy methods to couple two particle filters in such a way that the correlation\nbetween the two underlying particle systems is increased. The motivation stems\nfrom the classic variance reduction technique of positively correlating two\nestimators. The key challenge in constructing such a coupling stems from the\ndiscontinuity of the resampling step of the particle filter. As our first\ncontribution, we consider coupled resampling algorithms. Within bootstrap\nparticle filters, they improve the precision of finite-difference estimators of\nthe score vector and boost the performance of particle marginal\nMetropolis--Hastings algorithms for parameter inference. The second\ncontribution arises from the use of these coupled resampling schemes within\nconditional particle filters, allowing for unbiased estimators of smoothing\nfunctionals. The result is a new smoothing strategy that operates by averaging\na number of independent and unbiased estimators, which allows for 1)\nstraightforward parallelization and 2) the construction of accurate error\nestimates. Neither of the above is possible with existing particle smoothers.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 15:54:52 GMT"}, {"version": "v2", "created": "Sat, 16 Jul 2016 12:55:51 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Jacob", "Pierre E.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1606.01528", "submitter": "Giacomo Zanella", "authors": "Giacomo Zanella, Wilfrid S. Kendall, Myl\\`ene B\\'edard", "title": "A Dirichlet Form approach to MCMC Optimal Scaling", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops the use of Dirichlet forms to deliver proofs of optimal\nscaling results for Markov chain Monte Carlo algorithms (specifically,\nMetropolis-Hastings random walk samplers) under regularity conditions which are\nsubstantially weaker than those required by the original approach (based on the\nuse of infinitesimal generators). The Dirichlet form methods have the added\nadvantage of providing an explicit construction of the underlying\ninfinite-dimensional context. In particular, this enables us directly to\nestablish weak convergence to the relevant infinite-dimensional distributions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 16:15:38 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 16:14:39 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Zanella", "Giacomo", ""], ["Kendall", "Wilfrid S.", ""], ["B\u00e9dard", "Myl\u00e8ne", ""]]}, {"id": "1606.01662", "submitter": "Bruno Sudret", "authors": "V. Yaghoubi and S. Marelli and B. Sudret and T. Abrahamsson", "title": "Sparse polynomial chaos expansions of frequency response functions using\n  stochastic frequency transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2016-006-V2", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequency response functions (FRFs) are important for assessing the behavior\nof stochastic linear dynamic systems. For large systems, their evaluations are\ntime-consuming even for a single simulation. In such cases, uncertainty\nquantification by crude Monte-Carlo simulation is not feasible. In this paper,\nwe propose the use of sparse adaptive polynomial chaos expansions (PCE) as a\nsurrogate of the full model. To overcome known limitations of PCE when applied\nto FRF simulation, we propose a frequency transformation strategy that\nmaximizes the similarity between FRFs prior to the calculation of the PCE\nsurrogate. This strategy results in lower-order PCEs for each frequency.\nPrincipal component analysis is then employed to reduce the number of random\noutputs. The proposed approach is applied to two case studies: a simple 2-DOF\nsystem and a 6-DOF system with 16 random inputs. The accuracy assessment of the\nresults indicates that the proposed approach can predict single FRFs\naccurately. Besides, it is shown that the first two moments of the FRFs\nobtained by the PCE converge to the reference results faster than with the\nMonte-Carlo (MC) methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 09:08:02 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 17:32:05 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Yaghoubi", "V.", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""], ["Abrahamsson", "T.", ""]]}, {"id": "1606.02054", "submitter": "Geoffrey McLachlan", "authors": "Sharon X Lee, Kaleb L Lee, and Geoffrey J McLachlan", "title": "A simple multithreaded implementation of the EM algorithm for mixture\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models have been widely used for the modelling and analysis of\ndata from heterogeneous populations. Maximum likelihood estimation of the\nparameters is typically carried out via the Expectation-Maximization (EM)\nalgorithm. The complexity of the implementation of the algorithm depends on the\nparametric distribution that is adopted as the component densities of the\nmixture model. In the case of the skew normal and skew t-distributions, for\nexample, the E-step would involve complicated expressions that are\ncomputationally expensive to evaluate. This can become quite time-consuming for\nlarge and/or high-dimensional datasets. In this paper, we develop a\nmultithreaded version of the EM algorithm for the fitting of finite mixture\nmodels. Due to the structure of the algorithm for these models, the E- and\nM-steps can be easily reformulated to be executed in parallel across multiple\nthreads to take advantage of the processing power available in modern-day\nmulticore machines. Our approach is simple and easy to implement, requiring\nonly small changes to standard code. To illustrate the approach, we focus on a\nfairly general mixture model that includes as special or limiting cases some of\nthe most commonly used mixture models including the normal, t-, skew normal,\nand skew t-mixture models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 08:04:11 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Lee", "Sharon X", ""], ["Lee", "Kaleb L", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1606.02275", "submitter": "Roger Grosse", "authors": "Roger B. Grosse and Siddharth Ancha and Daniel M. Roy", "title": "Measuring the reliability of MCMC inference with bidirectional Monte\n  Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is one of the main workhorses of\nprobabilistic inference, but it is notoriously hard to measure the quality of\napproximate posterior samples. This challenge is particularly salient in black\nbox inference methods, which can hide details and obscure inference failures.\nIn this work, we extend the recently introduced bidirectional Monte Carlo\ntechnique to evaluate MCMC-based posterior inference algorithms. By running\nannealed importance sampling (AIS) chains both from prior to posterior and vice\nversa on simulated data, we upper bound in expectation the symmetrized KL\ndivergence between the true posterior distribution and the distribution of\napproximate samples. We present Bounding Divergences with REverse Annealing\n(BREAD), a protocol for validating the relevance of simulated data experiments\nto real datasets, and integrate it into two probabilistic programming\nlanguages: WebPPL and Stan. As an example of how BREAD can be used to guide the\ndesign of inference algorithms, we apply it to study the effectiveness of\ndifferent model representations in both WebPPL and Stan.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:39:02 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Grosse", "Roger B.", ""], ["Ancha", "Siddharth", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1606.02566", "submitter": "Julyan Arbel", "authors": "Julyan Arbel, Igor Pr\\\"unster", "title": "A moment-matching Ferguson and Klass algorithm", "comments": "24 pages, 6 figures, 5 tables", "journal-ref": "Statistics and Computing, 27(1):3--17, 2017", "doi": "10.1007/s11222-016-9676-8", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completely random measures (CRM) represent the key building block of a wide\nvariety of popular stochastic models and play a pivotal role in modern Bayesian\nNonparametrics. A popular representation of CRMs as a random series with\ndecreasing jumps is due to Ferguson and Klass (1972). This can immediately be\nturned into an algorithm for sampling realizations of CRMs or more elaborate\nmodels involving transformed CRMs. However, concrete implementation requires to\ntruncate the random series at some threshold resulting in an approximation\nerror. The goal of this paper is to quantify the quality of the approximation\nby a moment-matching criterion, which consists in evaluating a measure of\ndiscrepancy between actual moments and moments based on the simulation output.\nSeen as a function of the truncation level, the methodology can be used to\ndetermine the truncation level needed to reach a certain level of precision.\nThe resulting moment-matching \\FK algorithm is then implemented and illustrated\non several popular Bayesian nonparametric models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:18:21 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Arbel", "Julyan", ""], ["Pr\u00fcnster", "Igor", ""]]}, {"id": "1606.03245", "submitter": "Deniz Yenigun", "authors": "Deniz Yenigun, Gunes Ertan, Michael Siciliano", "title": "Omission and Commission Errors in Network Cognition and Network\n  Estimation using ROC Curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive Social Structure (CSS) network studies collect relational data on\nrespondents' direct ties and their perception of ties among all other\nindividuals in the network. When reporting their perception networks,\nrespondents commit two types of errors, namely, omission (false negatives) and\ncommission (false positives) errors. We first assess the relationship between\nthese two error types, and their contributions on the overall respondent\naccuracy. Next we propose a method for estimating networks based on perceptions\nof a random sample of respondents from a bounded social network, which utilizes\nthe Receiving Operator Characteristic (ROC) curve for balancing the tradeoffs\nbetween omission and commission errors. A comparative numerical study shows\nthat the proposed estimation method performs well. This new method can be\neasily integrated to organization studies that use randomized surveys to study\nmultiple organizations. The burgeoning field of multilevel analysis of\ninter-organizational networks can also immensely benefit from this approach.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 09:31:52 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Yenigun", "Deniz", ""], ["Ertan", "Gunes", ""], ["Siciliano", "Michael", ""]]}, {"id": "1606.03749", "submitter": "Omiros Papaspiliopoulos", "authors": "Omiros Papaspiliopoulos and David Rossell", "title": "Scalable Bayesian variable selection and model averaging under block\n  orthogonal design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable algorithmic framework for exact Bayesian variable\nselection and model averaging in linear models under the assumption that the\nGram matrix is block-diagonal, and as a heuristic for exploring the model space\nfor general designs. In block-diagonal designs our approach returns the most\nprobable model of any given size without resorting to numerical integration.\nThe algorithm also provides a novel and efficient solution to the frequentist\nbest subset selection problem for block-diagonal designs. Posterior\nprobabilities for any number of models are obtained by evaluating a single\none-dimensional integral that can be computed upfront, and other quantities of\ninterest such as variable inclusion probabilities and model averaged regression\nestimates by carrying out an adaptive, deterministic one-dimensional numerical\nintegration. The overall computational cost scales linearly with the number of\nblocks, which can be processed in parallel, and exponentially with the block\nsize, rendering it most adequate in situations where predictors are organized\nin many moderately-sized blocks. For general designs, we approximate the Gram\nmatrix by a block-diagonal using spectral clustering and propose an iterative\nalgorithm that capitalizes on the block-diagonal algorithms to explore\nefficiently the model space. All methods proposed in this article are\nimplemented in the R library mombf.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 18:35:38 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 09:22:17 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Rossell", "David", ""]]}, {"id": "1606.03757", "submitter": "Brendon Brewer", "authors": "Brendon J. Brewer and Daniel Foreman-Mackey", "title": "DNest4: Diffusive Nested Sampling in C++ and Python", "comments": "Submitted. 33 pages, 9 figures. v2 removed a duplicated figure, v3\n  added a comparison to other packages, v4 fixed a few minor issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In probabilistic (Bayesian) inferences, we typically want to compute\nproperties of the posterior distribution, describing knowledge of unknown\nquantities in the context of a particular dataset and the assumed prior\ninformation. The marginal likelihood, also known as the \"evidence\", is a key\nquantity in Bayesian model selection. The Diffusive Nested Sampling algorithm,\na variant of Nested Sampling, is a powerful tool for generating posterior\nsamples and estimating marginal likelihoods. It is effective at solving complex\nproblems including many where the posterior distribution is multimodal or has\nstrong dependencies between variables. DNest4 is an open source (MIT licensed),\nmulti-threaded implementation of this algorithm in C++11, along with associated\nutilities including: i) RJObject, a class template for finite mixture models,\n(ii) A Python package allowing basic use without C++ coding, and iii)\nExperimental support for models implemented in Julia. In this paper we\ndemonstrate DNest4 usage through examples including simple Bayesian data\nanalysis, finite mixture models, and Approximate Bayesian Computation.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 19:21:30 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 00:57:29 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 02:22:54 GMT"}, {"version": "v4", "created": "Wed, 2 Nov 2016 01:15:31 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Brewer", "Brendon J.", ""], ["Foreman-Mackey", "Daniel", ""]]}, {"id": "1606.03766", "submitter": "Antonio Punzo", "authors": "Antonio Punzo, Angelo Mazza, Paul D. McNicholas", "title": "ContaminatedMixt: An R Package for Fitting Parsimonious Mixtures of\n  Multivariate Contaminated Normal Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the R package ContaminatedMixt, conceived to disseminate the use\nof mixtures of multivariate contaminated normal distributions as a tool for\nrobust clustering and classification under the common assumption of\nelliptically contoured groups. Thirteen variants of the model are also\nimplemented to introduce parsimony. The expectation-conditional maximization\nalgorithm is adopted to obtain maximum likelihood parameter estimates, and\nlikelihood-based model selection criteria are used to select the model and the\nnumber of groups. Parallel computation can be used on multicore PCs and\ncomputer clusters, when several models have to be fitted. Differently from the\nmore popular mixtures of multivariate normal and t distributions, this approach\nalso allows for automatic detection of mild outliers via the maximum a\nposteriori probabilities procedure. To exemplify the use of the package,\napplications to artificial and real data are presented.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 21:43:14 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Punzo", "Antonio", ""], ["Mazza", "Angelo", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1606.04182", "submitter": "Jiwoong Kim", "authors": "Jiwoong Kim", "title": "KoulMde: An R Package for Koul's Minimum Distance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a full description of the R package KoulMde which is\ndesigned for Koul's minimum distance estimation method. When we encounter\nestimation problems in the linear regression and autogressive models, this\npackage provides more efficient estimators than other R packages.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 00:50:30 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Kim", "Jiwoong", ""]]}, {"id": "1606.04273", "submitter": "Bruno Sudret", "authors": "L. Le Gratiet and S. Marelli and B. Sudret", "title": "Metamodel-based sensitivity analysis: Polynomial chaos expansions and\n  Gaussian processes", "comments": "Handbook of Uncertainty Quantification, Ghanem, R., Higdon, D.,\n  Owhadi, H. (Eds.), 2016", "journal-ref": null, "doi": "10.1007/978-3-319-11259-6_38-1", "report-no": "RSUQ-2016-007", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis is now established as a powerful approach for\ndetermining the key random input parameters that drive the uncertainty of model\noutput predictions. Yet the classical computation of the so-called Sobol'\nindices is based on Monte Carlo simulation, which is not affordable when\ncomputationally expensive models are used, as it is the case in most\napplications in engineering and applied sciences. In this respect metamodels\nsuch as polynomial chaos expansions (PCE) and Gaussian processes (GP) have\nreceived tremendous attention in the last few years, as they allow one to\nreplace the original, taxing model by a surrogate which is built from an\nexperimental design of limited size. Then the surrogate can be used to compute\nthe sensitivity indices in negligible time. In this chapter an introduction to\neach technique is given, with an emphasis on their strengths and limitations in\nthe context of global sensitivity analysis. In particular, Sobol' (resp. total\nSobol') indices can be computed analytically from the PCE coefficients. In\ncontrast, confidence intervals on sensitivity indices can be derived\nstraightforwardly from the properties of GPs. The performance of the two\ntechniques is finally compared on three well-known analytical benchmarks\n(Ishigami, G-Sobol and Morris functions) as well as on a realistic engineering\napplication (deflection of a truss structure).\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:33:21 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Gratiet", "L. Le", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "1606.04478", "submitter": "Andrew Holbrook", "authors": "Andrew Holbrook, Alexander Vandenberg-Rodes, Babak Shahbaba", "title": "Bayesian Inference on Matrix Manifolds for Linear Dimensionality\n  Reduction", "comments": "All datasets and computer programs are publicly available at\n  http://www.ics.uci.edu/~babaks/Site/Codes.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reframe linear dimensionality reduction as a problem of Bayesian inference\non matrix manifolds. This natural paradigm extends the Bayesian framework to\ndimensionality reduction tasks in higher dimensions with simpler models at\ngreater speeds. Here an orthogonal basis is treated as a single point on a\nmanifold and is associated with a linear subspace on which observations vary\nmaximally. Throughout this paper, we employ the Grassmann and Stiefel manifolds\nfor various dimensionality reduction problems, explore the connection between\nthe two manifolds, and use Hybrid Monte Carlo for posterior sampling on the\nGrassmannian for the first time. We delineate in which situations either\nmanifold should be considered. Further, matrix manifold models are used to\nyield scientific insight in the context of cognitive neuroscience, and we\nconclude that our methods are suitable for basic inference as well as accurate\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 17:58:49 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Holbrook", "Andrew", ""], ["Vandenberg-Rodes", "Alexander", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1606.05030", "submitter": "Hayato Waki", "authors": "Keiji Kimura and Hayato Waki", "title": "Minimization of Akaike's Information Criterion in Linear Regression\n  Analysis via Mixed Integer Nonlinear Program", "comments": "We modified abstract, introduction and the beginning of section 3\n  from the first version", "journal-ref": "Optimization Methods and Software 33 (3), 633-649, 2018", "doi": "10.1080/10556788.2017.1333611", "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Akaike's information criterion (AIC) is a measure of the quality of a\nstatistical model for a given set of data. We can determine the best\nstatistical model for a particular data set by the minimization of the AIC.\nSince we need to evaluate exponentially many candidates of the model by the\nminimization of the AIC, the minimization is unreasonable. Instead, stepwise\nmethods, which are local search algorithms, are commonly used to find a better\nstatistical model though it may not be the best.\n  We propose a branch and bound search algorithm for a mixed integer nonlinear\nprogramming formulation of the AIC minimization by Miyashiro and Takano (2015).\nMore concretely, we propose methods to find lower and upper bounds, and\nbranching rules for this minimization. We then combine them with SCIP, which is\na mathematical optimization software and a branch-and-bound framework. We show\nthat the proposed method can provide the best statistical model based on AIC\nfor small-sized or medium-sized benchmark data sets in UCI Machine Learning\nRepository. Furthermore, we show that this method finds good quality solutions\nfor large-sized benchmark data sets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 02:55:11 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 10:39:24 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Kimura", "Keiji", ""], ["Waki", "Hayato", ""]]}, {"id": "1606.05578", "submitter": "Alec Koppel", "authors": "Alec Koppel, Brian M. Sadler, and Alejandro Ribeiro", "title": "Proximity Without Consensus in Online Multi-Agent Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2686368", "report-no": null, "categories": "cs.MA cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic optimization problems in multi-agent settings, where a\nnetwork of agents aims to learn parameters which are optimal in terms of a\nglobal objective, while giving preference to locally observed streaming\ninformation. To do so, we depart from the canonical decentralized optimization\nframework where agreement constraints are enforced, and instead formulate a\nproblem where each agent minimizes a global objective while enforcing network\nproximity constraints. This formulation includes online consensus optimization\nas a special case, but allows for the more general hypothesis that there is\ndata heterogeneity across the network. To solve this problem, we propose using\na stochastic saddle point algorithm inspired by Arrow and Hurwicz. This method\nyields a decentralized algorithm for processing observations sequentially\nreceived at each node of the network. Using Lagrange multipliers to penalize\nthe discrepancy between them, only neighboring nodes exchange model\ninformation. We establish that under a constant step-size regime the\ntime-average suboptimality and constraint violation are contained in a\nneighborhood whose radius vanishes with increasing number of iterations. As a\nconsequence, we prove that the time-average primal vectors converge to the\noptimal objective while satisfying the network proximity constraints. We apply\nthis method to the problem of sequentially estimating a correlated random field\nin a sensor network, as well as an online source localization problem, both of\nwhich demonstrate the empirical validity of the aforementioned convergence\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 16:17:19 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Koppel", "Alec", ""], ["Sadler", "Brian M.", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1606.05584", "submitter": "M.I. Borrajo", "authors": "Mar\\'ia Isabel Borrajo, Wenceslao Gonz\\'alez-Manteiga and Mar\\'ia\n  Dolores Mart\\'inez-Miranda", "title": "Bandwidth selection for kernel density estimation with length-biased\n  data", "comments": "35 pages", "journal-ref": null, "doi": "10.1080/10485252.2017.1339309", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Length-biased data are a particular case of weighted data, which arise in\nmany situations: biomedicine, quality control or epidemiology among others. In\nthis paper we study the theoretical properties of kernel density estimation in\nthe context of length-biased data, proposing two consistent bootstrap methods\nthat we use for bandwidth selection. Apart from the bootstrap bandwidth\nselectors we suggest a rule-of-thumb. These bandwidth selection proposals are\ncompared with a least-squares cross-validation method. A simulation study is\naccomplished to understand the behaviour of the procedures in finite samples.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 16:41:21 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 15:39:57 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Borrajo", "Mar\u00eda Isabel", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Mart\u00ednez-Miranda", "Mar\u00eda Dolores", ""]]}, {"id": "1606.05656", "submitter": "Leopoldo Catania", "authors": "Leopoldo Catania and Nima Nonejad", "title": "Dynamic Model Averaging for Practitioners in Economics and Finance: The\n  eDMA Package", "comments": "21 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raftery, Karny, and Ettler (2010) introduce an estimation technique, which\nthey refer to as Dynamic Model Averaging (DMA). In their application, DMA is\nused to predict the output strip thickness for a cold rolling mill, where the\noutput is measured with a time delay. Recently, DMA has also shown to be useful\nin macroeconomic and financial applications. In this paper, we present the eDMA\npackage for DMA estimation implemented in R. The eDMA package is especially\nsuited for practitioners in economics and finance, where typically a large\nnumber of predictors are available. Our implementation is up to 133 times\nfaster then a standard implementation using a single-core CPU. Thus, with the\nhelp of this package, practitioners are able to perform DMA on a standard PC\nwithout resorting to large clusters, which are not easily available to all\nresearchers. We demonstrate the usefulness of this package through simulation\nexperiments and an empirical application using quarterly U.S. inflation data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 20:00:56 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 14:30:52 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 19:34:04 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Catania", "Leopoldo", ""], ["Nonejad", "Nima", ""]]}, {"id": "1606.06351", "submitter": "Shiwei Lan", "authors": "Alexandros Beskos, Mark Girolami, Shiwei Lan, Patrick E. Farrell and\n  Andrew M. Stuart", "title": "Geometric MCMC for Infinite-Dimensional Inverse Problems", "comments": "37 pages, 15 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2016.12.041", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inverse problems often involve sampling posterior distributions on\ninfinite-dimensional function spaces. Traditional Markov chain Monte Carlo\n(MCMC) algorithms are characterized by deteriorating mixing times upon\nmesh-refinement, when the finite-dimensional approximations become more\naccurate. Such methods are typically forced to reduce step-sizes as the\ndiscretization gets finer, and thus are expensive as a function of dimension.\nRecently, a new class of MCMC methods with mesh-independent convergence times\nhas emerged. However, few of them take into account the geometry of the\nposterior informed by the data. At the same time, recently developed geometric\nMCMC algorithms have been found to be powerful in exploring complicated\ndistributions that deviate significantly from elliptic Gaussian laws, but are\nin general computationally intractable for models defined in infinite\ndimensions. In this work, we combine geometric methods on a finite-dimensional\nsubspace with mesh-independent infinite-dimensional approaches. Our objective\nis to speed up MCMC mixing times, without significantly increasing the\ncomputational cost per step (for instance, in comparison with the vanilla\npreconditioned Crank-Nicolson (pCN) method). This is achieved by using ideas\nfrom geometric MCMC to probe the complex structure of an intrinsic\nfinite-dimensional subspace where most data information concentrates, while\nretaining robust mixing times as the dimension grows by using pCN-like methods\nin the complementary subspace. The resulting algorithms are demonstrated in the\ncontext of three challenging inverse problems arising in subsurface flow, heat\nconduction and incompressible flow control. The algorithms exhibit up to two\norders of magnitude improvement in sampling efficiency when compared with the\npCN method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 22:30:14 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 10:52:17 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Beskos", "Alexandros", ""], ["Girolami", "Mark", ""], ["Lan", "Shiwei", ""], ["Farrell", "Patrick E.", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1606.06611", "submitter": "Hadi Meidani", "authors": "Negin Alemazkoor, Hadi Meidani", "title": "Divide and Conquer: An Incremental Sparsity Promoting Compressive\n  Sampling Approach for Polynomial Chaos Expansions", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2017.01.039", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an efficient sparse recovery approach for Polynomial\nChaos (PC) expansions, which promotes the sparsity by breaking the\ndimensionality of the problem. The proposed algorithm incrementally explores\nsub-dimensional expansions for a sparser recovery, and shows success when\nremoval of uninfluential parameters that results in a lower coherence for\nmeasurement matrix, allows for a higher order and/or sparser expansion to be\nrecovered. The incremental algorithm effectively searches for the sparsest PC\napproximation, and not only can it decrease the prediction error, it can also\nreduce the dimensionality of PCE model. Four numerical examples are provided to\ndemonstrate the validity of the proposed approach. The results from these\nexamples show that the incremental algorithm substantially outperforms\nconventional compressive sampling approaches for PCE, in terms of both solution\nsparsity and prediction error.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:08:41 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 19:14:52 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Alemazkoor", "Negin", ""], ["Meidani", "Hadi", ""]]}, {"id": "1606.06659", "submitter": "Jarad Niemi", "authors": "Will Landau and Jarad Niemi", "title": "A fully Bayesian strategy for high-dimensional hierarchical modeling\n  using massively parallel computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is the predominant tool used in Bayesian\nparameter estimation for hierarchical models. When the model expands due to an\nincreasing number of hierarchical levels, number of groups at a particular\nlevel, or number of observations in each group, a fully Bayesian analysis via\nMCMC can easily become computationally demanding, even intractable. We\nillustrate how the steps in an MCMC for hierarchical models are predominantly\none of two types: conditionally independent draws or low-dimensional draws\nbased on summary statistics of parameters at higher levels of the hierarchy.\nParallel computing can increase efficiency by performing embarrassingly\nparallel computations for conditionally independent draws and calculating the\nsummary statistics using parallel reductions. During the MCMC algorithm, we\nrecord running means and means of squared parameter values to allow convergence\ndiagnosis and posterior inference while avoiding the costly memory transfer\nbottleneck. We demonstrate the effectiveness of the algorithm on a model\nmotivated by next generation sequencing data, and we release our implementation\nin R packages fbseq and fbseqCUDA.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 17:00:41 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Landau", "Will", ""], ["Niemi", "Jarad", ""]]}, {"id": "1606.06956", "submitter": "Thomas Li", "authors": "Thomas J. X. Li and Christian M. Reidys", "title": "Statistics of topological RNA structures", "comments": "29 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO q-bio.BM q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study properties of topological RNA structures, i.e.~RNA\ncontact structures with cross-serial interactions that are filtered by their\ntopological genus. RNA secondary structures within this framework are\ntopological structures having genus zero. We derive a new bivariate generating\nfunction whose singular expansion allows us to analyze the distributions of\narcs, stacks, hairpin- , interior- and multi-loops. We then extend this\nanalysis to H-type pseudoknots, kissing hairpins as well as $3$-knots and\ncompute their respective expectation values. Finally we discuss our results and\nput them into context with data obtained by uniform sampling structures of\nfixed genus.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 14:03:02 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Li", "Thomas J. X.", ""], ["Reidys", "Christian M.", ""]]}, {"id": "1606.07300", "submitter": "Nirmal Baran Chakrabarti", "authors": "N B Chakrabarti", "title": "A Note on a Sum of Lognormals", "comments": "16 pages, 6 figures, to be submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note considers the applicability of Gauss-Hermite quadrature and direct\nnumerical quadrature for computation of moment generating function (mgf) and\nthe derivatives. A preprocessing using the asymptotic technique is employed\nwhile computing the characteristic function (chf) using Gauss Hermite\nquadrature while this is optional for mgf. The mgf of the low and high\namplitude regions of a single lognormal variable and the derivatives is\nexamined and attention is drawn to the effect of variance. The problem of\ninversion of the mgf/chf of a sum of lognormals to obtain the CDF/pdf is\nconsidered with special reference to methods related to Post Widder technique,\nGaussian quadrature and the Fourier series method. The method based on the\ncomplex exponential integral which makes use of the derivative of the cumulant\nis an alternative. Segmentation of the mgf/chf on the basis of the derivative\nstructure which indicates activity rate is shown to be useful.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:59:56 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 11:16:51 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Chakrabarti", "N B", ""]]}, {"id": "1606.07620", "submitter": "Daniel Fischer", "authors": "Daniel Fischer, Karl Mosler, Jyrki M\\\"ott\\\"onen, Klaus Nordhausen,\n  Oleksii Pokotylo, Daniel Vogel", "title": "Computing the Oja Median in R: The Package OjaNP", "comments": null, "journal-ref": "Journal of Statistical Software, 92: 1-36 (2020)", "doi": "10.18637/jss.v092.i08", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Oja median is one of several extensions of the univariate median to the\nmultivariate case. It has many nice properties, but is computationally\ndemanding. In this paper, we first review the properties of the Oja median and\ncompare it to other multivariate medians. Afterwards we discuss four algorithms\nto compute the Oja median, which are implemented in our R-package OjaNP.\nBesides these algorithms, the package contains also functions to compute Oja\nsigns, Oja signed ranks, Oja ranks, and the related scatter concepts. To\nillustrate their use, the corresponding multivariate one- and $C$-sample\nlocation tests are implemented.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 09:47:11 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Fischer", "Daniel", ""], ["Mosler", "Karl", ""], ["M\u00f6tt\u00f6nen", "Jyrki", ""], ["Nordhausen", "Klaus", ""], ["Pokotylo", "Oleksii", ""], ["Vogel", "Daniel", ""]]}, {"id": "1606.07667", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Oli Pall Geirsson, Birgir Hrafnkelsson, Olafur\n  Birgir Davidsson and Sigurdur Magnus Gardarsson", "title": "A Bayesian hierarchical model for monthly maxima of instantaneous flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a comprehensive Bayesian hierarchical model for monthly maxima of\ninstantaneous flow in river catchments. The Gumbel distribution is used as the\nprobabilistic model for the observations, which are assumed to come from\nseveral catchments. Our suggested latent model is Gaussian and designed for\nmonthly maxima, making better use of the data than the standard approach using\nannual maxima. At the latent level, linear mixed models are used for both the\nlocation and scale parameters of the Gumbel distribution, accounting for\nseasonal dependence and covariates from the catchments. The specification of\nprior distributions makes use of penalised complexity (PC) priors, to ensure\nrobust inference for the latent parameters. The main idea behind the PC priors\nis to shrink toward a base model, thus avoiding overfitting. PC priors also\nprovide a convenient framework for prior elicitation based on simple notions of\nscale. Prior distributions for regression coefficients are also elicited based\non hydrological and meteorological knowledge. Posterior inference was done\nusing the MCMC split sampler, an efficient Gibbs blocking scheme tailored to\nlatent Gaussian models. The proposed model was applied to observed data from\neight river catchments in Iceland. A cross-validation study demonstrates good\npredictive performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 12:50:11 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Geirsson", "Oli Pall", ""], ["Hrafnkelsson", "Birgir", ""], ["Davidsson", "Olafur Birgir", ""], ["Gardarsson", "Sigurdur Magnus", ""]]}, {"id": "1606.07845", "submitter": "Kamiar Rahnama Rad", "authors": "Kamiar Rahnama Rad and Timothy A. Machado and Liam Paninski", "title": "Robust and scalable Bayesian analysis of spatial neural tuning function\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common analytical problem in neuroscience is the interpretation of neural\nactivity with respect to sensory input or behavioral output. This is typically\nachieved by regressing measured neural activity against known stimuli or\nbehavioral variables to produce a \"tuning function\" for each neuron.\nUnfortunately, because this approach handles neurons individually, it cannot\ntake advantage of simultaneous measurements from spatially adjacent neurons\nthat often have similar tuning properties. On the other hand, sharing\ninformation between adjacent neurons can errantly degrade estimates of tuning\nfunctions across space if there are sharp discontinuities in tuning between\nnearby neurons. In this paper, we develop a computationally efficient block\nGibbs sampler that effectively pools information between neurons to de-noise\ntuning function estimates while simultaneously preserving sharp discontinuities\nthat might exist in the organization of tuning across space. This method is\nfully Bayesian and its computational cost per iteration scales\nsub-quadratically with total parameter dimensionality. We demonstrate the\nrobustness and scalability of this approach by applying it to both real and\nsynthetic datasets. In particular, an application to data from the spinal cord\nillustrates that the proposed methods can dramatically decrease the\nexperimental time required to accurately estimate tuning functions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 22:15:19 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Rad", "Kamiar Rahnama", ""], ["Machado", "Timothy A.", ""], ["Paninski", "Liam", ""]]}, {"id": "1606.07868", "submitter": "Razieh Nabi", "authors": "Razieh Nabi and Xiaogang Su", "title": "coxphMIC: An R Package for Sparse Estimation of Cox Proportional Hazards\n  Models", "comments": "10 pages and 3 figures", "journal-ref": "The R Journal, 9, (2017) 229-238", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe an R package named coxphMIC, which implements the\nsparse estimation method for Cox proportional hazards models via approximated\ninformation criterion (Su et al., 2016 Biometrics). The developed methodology\nis named MIC which stands for \"Minimizing approximated Information Criteria\". A\nreparameterization step is introduced to enforce sparsity while at the same\ntime keeping the objective function smooth. As a result, MIC is computationally\nfast with a superior performance in sparse estimation. Furthermore, the\nreparameterization tactic yields an additional advantage in terms of\ncircumventing post-selection inference. The MIC method and its R implementation\nare introduced and illustrated with the PBC data.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 03:32:47 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 19:20:48 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Nabi", "Razieh", ""], ["Su", "Xiaogang", ""]]}, {"id": "1606.07892", "submitter": "Qinyi Zhang", "authors": "Qinyi Zhang and Sarah Filippi and Arthur Gretton and Dino Sejdinovic", "title": "Large-Scale Kernel Methods for Independence Testing", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s11222-016-9721-7", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations of probability measures in reproducing kernel Hilbert spaces\nprovide a flexible framework for fully nonparametric hypothesis tests of\nindependence, which can capture any type of departure from independence,\nincluding nonlinear associations and multivariate interactions. However, these\napproaches come with an at least quadratic computational cost in the number of\nobservations, which can be prohibitive in many applications. Arguably, it is\nexactly in such large-scale datasets that capturing any type of dependence is\nof interest, so striking a favourable tradeoff between computational efficiency\nand test performance for kernel independence tests would have a direct impact\non their applicability in practice. In this contribution, we provide an\nextensive study of the use of large-scale kernel approximations in the context\nof independence testing, contrasting block-based, Nystrom and random Fourier\nfeature approaches. Through a variety of synthetic data experiments, it is\ndemonstrated that our novel large scale methods give comparable performance\nwith existing methods whilst using significantly less computation time and\nmemory.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 10:09:03 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zhang", "Qinyi", ""], ["Filippi", "Sarah", ""], ["Gretton", "Arthur", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1606.07984", "submitter": "Shinichiro Shirota Mr", "authors": "Shinichiro Shirota, Alan. E. Gelfand", "title": "Approximate Marginal Posterior for Log Gaussian Cox Processes", "comments": "The title of updated version is \"Inference for log Gaussian Cox\n  processes using an approximate marginal posterior\" [arXiv:1611.10359]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log Gaussian Cox process is a flexible class of Cox processes, whose\nintensity surface is stochastic, for incorporating complex spatial and time\nstructure of point patterns. The straightforward inference based on Markov\nchain Monte Carlo is computationally heavy because the computational cost of\ninverse or Cholesky decomposition of high dimensional covariance matrices of\nGaussian latent variables is cubic order of their dimension. Furthermore, since\nhyperparameters for Gaussian latent variables have high correlations with\nsampled Gaussian latent processes themselves, standard Markov chain Monte Carlo\nstrategies are inefficient. In this paper, we propose an efficient and scalable\ncomputational strategy for spatial log Gaussian Cox processes. The proposed\nalgorithm is based on pseudo-marginal Markov chain Monte Carlo approach. Based\non this approach, we propose estimation of approximate marginal posterior for\nparameters and comprehensive model validation strategies. We provide details\nfor all of the above along with some simulation investigation for univariate\nand multivariate settings and analysis of a point pattern of tree data\nexhibiting positive and negative interaction between different species.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 01:39:05 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 21:01:05 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Gelfand", "Alan. E.", ""]]}, {"id": "1606.07995", "submitter": "Vladimir Minin", "authors": "Jonathan Fintzi, Xiang Cui, Jon Wakefield, Vladimir N. Minin", "title": "Efficient data augmentation for fitting stochastic epidemic models to\n  prevalence data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic epidemic models describe the dynamics of an epidemic as a disease\nspreads through a population. Typically, only a fraction of cases are observed\nat a set of discrete times. The absence of complete information about the time\nevolution of an epidemic gives rise to a complicated latent variable problem in\nwhich the state space size of the epidemic grows large as the population size\nincreases. This makes analytically integrating over the missing data infeasible\nfor populations of even moderate size. We present a data augmentation Markov\nchain Monte Carlo (MCMC) framework for Bayesian estimation of stochastic\nepidemic model parameters, in which measurements are augmented with\nsubject-level disease histories. In our MCMC algorithm, we propose each new\nsubject-level path, conditional on the data, using a time-inhomogeneous\ncontinuous-time Markov process with rates determined by the infection histories\nof other individuals. The method is general, and may be applied, with minimal\nmodifications, to a broad class of stochastic epidemic models. We present our\nalgorithm in the context of multiple stochastic epidemic models in which the\ndata are binomially sampled prevalence counts, and apply our method to data\nfrom an outbreak of influenza in a British boarding school.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 05:23:23 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 22:04:33 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Fintzi", "Jonathan", ""], ["Cui", "Xiang", ""], ["Wakefield", "Jon", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1606.08160", "submitter": "B{\\l}a\\.zej Miasojedow", "authors": "B{\\l}a\\.zej Miasojedow and Wojcieh Niemiro", "title": "Geometric ergodicity of Rao and Teh's algorithm for Markov jump\n  processes and CTBNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rao and Teh (2012, 2013) introduced an efficient MCMC algorithm for sampling\nfrom the posterior distribution of a hidden Markov jump process. The algorithm\nis based on the idea of sampling virtual jumps. In the present paper we show\nthat the Markov chain generated by Rao and Teh's algorithm is geometrically\nergodic. To this end we establish a geometric drift condition towards a small\nset. A similar result is also proved for a special version of the algorithm,\nused for probabilistic inference in Continuous Time Bayesian Networks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 08:20:46 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Miasojedow", "B\u0142a\u017cej", ""], ["Niemiro", "Wojcieh", ""]]}, {"id": "1606.08350", "submitter": "Ba Tuong Vo Prof", "authors": "Ba Ngu Vo, Ba Tuong Vo, Hung Gia Hoang", "title": "An Efficient Implementation of the Generalized Labeled Multi-Bernoulli\n  Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient implementation of the generalized labeled\nmulti-Bernoulli (GLMB) filter by combining the prediction and update into a\nsingle step. In contrast to an earlier implementation that involves separate\ntruncations in the prediction and update steps, the proposed implementation\nrequires only one truncation procedure for each iteration. Furthermore, we\npropose an efficient algorithm for truncating the GLMB filtering density based\non Gibbs sampling. The resulting implementation has a linear complexity in the\nnumber of measurements and quadratic in the number of hypothesized objects.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 16:24:03 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 09:54:15 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Vo", "Ba Ngu", ""], ["Vo", "Ba Tuong", ""], ["Hoang", "Hung Gia", ""]]}, {"id": "1606.08373", "submitter": "Anthony Lee", "authors": "George Deligiannidis and Anthony Lee", "title": "Which ergodic averages have finite asymptotic variance?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the class of $L^2$ functions for which ergodic averages of a\nreversible Markov chain have finite asymptotic variance is determined by the\nclass of $L^2$ functions for which ergodic averages of its associated jump\nchain have finite asymptotic variance. This allows us to characterize\ncompletely which ergodic averages have finite asymptotic variance when the\nMarkov chain is an independence sampler. In addition, we obtain a simple\nsufficient condition for all ergodic averages of $L^2$ functions of the primary\nvariable in a pseudo-marginal Markov chain to have finite asymptotic variance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 17:19:28 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 19:03:41 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Deligiannidis", "George", ""], ["Lee", "Anthony", ""]]}, {"id": "1606.08577", "submitter": "Bruno Sudret", "authors": "K. Konakli and B. Sudret", "title": "Reliability analysis of high-dimensional models using low-rank tensor\n  approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2016-003", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineering and applied sciences use models of increasing complexity to\nsimulate the behaviour of manufactured and physical systems. Propagation of\nuncertainties from the input to a response quantity of interest through such\nmodels may become intractable in cases when a single simulation is time\ndemanding. Particularly challenging is the reliability analysis of systems\nrepresented by computationally costly models, because of the large number of\nmodel evaluations that are typically required to estimate small probabilities\nof failure. In this paper, we demonstrate the potential of a newly emerged\nmeta-modelling technique known as low-rank tensor approximations to address\nthis limitation. This technique is especially promising for high-dimensional\nproblems because: (i) the number of unknowns in the generic functional form of\nthe meta-model grows only linearly with the input dimension and (ii) such\napproximations can be constructed by relying on a series of minimization\nproblems of small size independent of the input dimension. In example\napplications involving finite-element models pertinent to structural mechanics\nand heat conduction, low-rank tensor approximations built with polynomial bases\nare found to outperform the popular sparse polynomial chaos expansions in the\nestimation of tail probabilities when small experimental designs are used. It\nshould be emphasized that contrary to methods particularly targeted to\nreliability analysis, the meta-modelling approach also provides a full\nprobabilistic description of the model response, which can be used to estimate\nany statistical measure of interest.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 07:07:54 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Konakli", "K.", ""], ["Sudret", "B.", ""]]}, {"id": "1606.08650", "submitter": "Axel Finke", "authors": "Axel Finke and Sumeetpal S. Singh", "title": "Approximate Smoothing and Parameter Estimation in High-Dimensional\n  State-Space Models", "comments": "Includes supplementary materials", "journal-ref": "IEEE Transactions on Signal Processing, 65(22), 5982-5994, 2017", "doi": "10.1109/TSP.2017.2733504", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present approximate algorithms for performing smoothing in a class of\nhigh-dimensional state-space models via sequential Monte Carlo methods\n(\"particle filters\"). In high dimensions, a prohibitively large number of Monte\nCarlo samples (\"particles\") -- growing exponentially in the dimension of the\nstate space -- is usually required to obtain a useful smoother. Using blocking\nstrategies as in Rebeschini and Van Handel (2015) (and earlier pioneering work\non blocking), we exploit the spatial ergodicity properties of the model to\ncircumvent this curse of dimensionality. We thus obtain approximate smoothers\nthat can be computed recursively in time and in parallel in space. First, we\nshow that the bias of our blocked smoother is bounded uniformly in the time\nhorizon and in the model dimension. We then approximate the blocked smoother\nwith particles and derive the asymptotic variance of idealised versions of our\nblocked particle smoother to show that variance is no longer adversely effected\nby the dimension of the model. Finally, we employ our method to successfully\nperform maximum-likelihood estimation via stochastic gradient-ascent and\nstochastic expectation--maximisation algorithms in a 100-dimensional\nstate-space model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 11:09:51 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 16:27:15 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 11:55:54 GMT"}, {"version": "v4", "created": "Wed, 20 Sep 2017 15:01:25 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Finke", "Axel", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1606.09419", "submitter": "Alan Benson", "authors": "Alan Benson, Nial Friel", "title": "An adaptive MCMC method for multiple changepoint analysis with\n  applications to large datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Bayesian inference for changepoints where the\nnumber and position of the changepoints are both unknown. In particular, we\nconsider product partition models where it is possible to integrate out model\nparameters for the regime between each changepoint, leaving a posterior\ndistribution over a latent vector indicating the presence or not of a\nchangepoint at each observation. The same problem setting has been considered\nby Fearnhead (2006) where one can use filtering recursions to make exact\ninference. However the complexity of this algorithm depends quadratically on\nthe number of observations. Our approach relies on an adaptive Markov Chain\nMonte Carlo (MCMC) method for finite discrete state spaces. We develop an\nadaptive algorithm which can learn from the past states of the Markov chain in\norder to build proposal distributions which can quickly discover where\nchangepoint are likely to be located. We prove that our algorithm leaves the\nposterior distribution ergodic. Crucially, we demonstrate that our adaptive\nMCMC algorithm is viable for large datasets for which the filtering recursions\napproach is not. Moreover, we show that inference is possible in a reasonable\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 10:29:37 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 19:40:18 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Benson", "Alan", ""], ["Friel", "Nial", ""]]}]