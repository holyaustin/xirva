[{"id": "2005.00386", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss, Joseph Guinness, Earl Lawrence", "title": "Scaled Vecchia approximation for fast computer-model emulation", "comments": "R code available at https://github.com/katzfuss-group/scaledVecchia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific phenomena are studied using computer experiments consisting\nof multiple runs of a computer model while varying the input settings. Gaussian\nprocesses (GPs) are a popular tool for the analysis of computer experiments,\nenabling interpolation between input settings, but direct GP inference is\ncomputationally infeasible for large datasets. We adapt and extend a powerful\nclass of GP methods from spatial statistics to enable the scalable analysis and\nemulation of large computer experiments. Specifically, we apply Vecchia's\nordered conditional approximation in a transformed input space, with each input\nscaled according to how strongly it relates to the computer-model response. The\nscaling is learned from the data, by estimating parameters in the GP covariance\nfunction using Fisher scoring. Our methods are highly scalable, enabling\nestimation, joint prediction and simulation in near-linear time in the number\nof model runs. In several numerical examples, our approach substantially\noutperformed existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:08:31 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 16:03:08 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 21:07:03 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 15:43:56 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Guinness", "Joseph", ""], ["Lawrence", "Earl", ""]]}, {"id": "2005.00597", "submitter": "Irina Gaynanova", "authors": "Benjamin Risk and Irina Gaynanova", "title": "Simultaneous Non-Gaussian Component Analysis (SING) for Data Integration\n  in Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As advances in technology allow the acquisition of complementary information,\nit is increasingly common for scientific studies to collect multiple datasets.\nLarge-scale neuroimaging studies often include multiple modalities (e.g., task\nfunctional MRI, resting-state fMRI, diffusion MRI, and/or structural MRI), with\nthe aim to understand the relationships between datasets. In this study, we\nseek to understand whether regions of the brain activated in a working memory\ntask relate to resting-state correlations. In neuroimaging, a popular approach\nuses principal component analysis for dimension reduction prior to canonical\ncorrelation analysis with joint independent component analysis, but this may\ndiscard biological features with low variance and/or spuriously associate\nstructure unique to a dataset with joint structure. We introduce Simultaneous\nNon-Gaussian component analysis (SING) in which dimension reduction and feature\nextraction are achieved simultaneously, and shared information is captured via\nsubject scores. We apply our method to a working memory task and resting-state\ncorrelations from the Human Connectome Project. We find joint structure as\nevident from joint scores whose loadings highlight resting-state correlations\ninvolving regions associated with working memory. Moreover, some of the subject\nscores are related to fluid intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:35:00 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 00:54:51 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Risk", "Benjamin", ""], ["Gaynanova", "Irina", ""]]}, {"id": "2005.00605", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir Storvik, Florian Frommlet", "title": "Rejoinder for the discussion of the paper \"A novel algorithmic approach\n  to Bayesian Logic Regression\"", "comments": "published in Bayesian Analysis, Volume 15, Number 1 (2020)", "journal-ref": "Bayesian Analysis, Volume 15, Number 1 (2020)", "doi": null, "report-no": null, "categories": "stat.ME math.LO stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this rejoinder we summarize the comments, questions and remarks on the\npaper \"A novel algorithmic approach to Bayesian Logic Regression\" from the\ndiscussants. We then respond to those comments, questions and remarks, provide\nseveral extensions of the original model and give a tutorial on our R-package\nEMJMCMC (http://aliaksah.github.io/EMJMCMC2016/)\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:59:56 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""], ["Frommlet", "Florian", ""]]}, {"id": "2005.00905", "submitter": "Hong Zhang", "authors": "Hong Zhang, Judong Shen and Zheyang Wu", "title": "An efficient and accurate approximation to the distribution of quadratic\n  forms of Gaussian variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational and applied statistics, it is of great interest to get fast\nand accurate calculation for the distributions of the quadratic forms of\nGaussian random variables. This paper presents a novel approximation strategy\nthat contains two developments. First, we propose a faster numerical procedure\nin computing the moments of the quadratic forms. Second, we establish a general\nmoment-matching framework for distribution approximation, which covers existing\napproximation methods for the distributions of the quadratic forms of Gaussian\nvariables. Under this framework, a novel moment-ratio method (MR) is proposed\nto match the ratio of skewness and kurtosis based on the gamma distribution.\nOur extensive simulations show that 1) MR is almost as accurate as the exact\ndistribution calculation and is much more efficient; 2) comparing with existing\napproximation methods, MR significantly improves the accuracy of approximating\nfar right tail probabilities. The proposed method has wide applications. For\nexample, it is a better choice than existing methods for facilitating\nhypothesis testing in big data analysis, where efficient and accurate\ncalculation of very small $p$-values is desired. An R package Qapprox that\nimplements related methods is available on CRAN.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 19:07:19 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 13:26:42 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhang", "Hong", ""], ["Shen", "Judong", ""], ["Wu", "Zheyang", ""]]}, {"id": "2005.00952", "submitter": "Michael Dumelle", "authors": "Michael Dumelle and Jay M. Ver Hoef and Claudio Fuentes and Alix\n  Gitelman", "title": "A Linear Mixed Model Formulation for Spatio-Temporal Random Processes\n  with Computational Advances for the Separable and Product-Sum Covariances", "comments": "43 pages (including an Appendix) and 8 figures", "journal-ref": "Spatial Staistics, Volume 43, 2021", "doi": "10.1016/j.spasta.2021.100510", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe spatio-temporal random processes using linear mixed models. We\nshow how many commonly used models can be viewed as special cases of this\ngeneral framework and pay close attention to models with separable or\nproduct-sum covariances. The proposed linear mixed model formulation\nfacilitates the implementation of a novel algorithm using Stegle\neigendecompositions, a recursive application of the Sherman-Morrison-Woodbury\nformula, and Helmert-Wolf blocking to efficiently invert separable and\nproduct-sum covariance matrices, even when every spatial location is not\nobserved at every time point. We show our algorithm provides noticeable\nimprovements over the standard Cholesky decomposition approach. Via\nsimulations, we assess the performance of the separable and product-sum\ncovariances and identify scenarios where separable covariances are noticeably\ninferior to product-sum covariances. We also compare likelihood-based and\nsemivariogram-based estimation and discuss benefits and drawbacks of both. We\nuse the proposed approach to analyze daily maximum temperature data in Oregon,\nUSA, during the 2019 summer. We end by offering guidelines for choosing among\nthese covariances and estimation methods based on properties of observed data.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 00:11:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Dumelle", "Michael", ""], ["Hoef", "Jay M. Ver", ""], ["Fuentes", "Claudio", ""], ["Gitelman", "Alix", ""]]}, {"id": "2005.00971", "submitter": "Esam Mahdi", "authors": "Esam Mahdi", "title": "A Powerful Portmanteau Test for Detecting Nonlinearity in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new portmanteau test statistic is proposed for detecting nonlinearity in\ntime series data. In this paper, we elaborate on the Toeplitz autocorrelation\nmatrix to the autocorrelation and cross-correlation of residuals and squared\nresiduals block matrix. We derive a new portmanteau test statistic using the\nlog of the determinant of the mth autocorrelations and cross-correlations block\nmatrix. The asymptotic distribution of the proposed test statistic is derived\nas a linear combination of chi-squared distributions and can be approximated by\na gamma distribution. This test is applied to identify the linearity and\nnonlinearity dependency of some stationary time series models. It is shown that\nthe convergence of the new test to its asymptotic distribution is reasonable\nwith higher power than other tests in many situations. We demonstrate the\nefficiency of the proposed test by investigating linear and nonlinear effects\nin Vodafone Qatar and Nikkei-300 daily returns.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 02:56:08 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mahdi", "Esam", ""]]}, {"id": "2005.01262", "submitter": "Yijun Zuo", "authors": "Yijun Zuo", "title": "Exact computation of projection regression depth and fast computation of\n  its induced median and other estimators", "comments": "21 pages, 1 figure, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1905.11846", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zuo (2019) (Z19) addressed the computation of the projection regression depth\n(PRD) and its induced median (the maximum depth estimator). Z19 achieved the\nexact computation of PRD via a modified version of regular univariate sample\nmedian, which resulted in the loss of invariance of PRD and the equivariance of\ndepth induced median. This article achieves the exact computation without\nscarifying the invariance of PRD and the equivariance of the regression median.\nZ19 also addressed the approximate computation of PRD induced median, the naive\nalgorithm in Z19 is very slow. This article modifies the approximation in Z19\nand adopts Rcpp package and consequently obtains a much (could be $100$ times)\nfaster algorithm with an even better level of accuracy meanwhile. Furthermore,\nas the third major contribution, this article introduces three new depth\ninduced estimators which can run $300$ times faster than that of Z19 meanwhile\nmaintaining the same (or a bit better) level of accuracy. Real as well as\nsimulated data examples are presented to illustrate the difference between the\nalgorithms of Z19 and the ones proposed in this article. Findings support the\nstatements above and manifest the major contributions of the article.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 04:21:33 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zuo", "Yijun", ""]]}, {"id": "2005.01285", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe", "title": "Connecting the Dots: Towards Continuous Time Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous time Hamiltonian Monte Carlo is introduced, as a powerful\nalternative to Markov chain Monte Carlo methods for continuous target\ndistributions. The method is constructed in two steps: First Hamiltonian\ndynamics are chosen as the deterministic dynamics in a continuous time\npiecewise deterministic Markov process. Under very mild restrictions, such a\nprocess will have the desired target distribution as an invariant distribution.\nSecondly, the numerical implementation of such processes, based on adaptive\nnumerical integration of second order ordinary differential equations is\nconsidered. The numerical implementation yields an approximate, yet highly\nrobust algorithm that, unlike conventional Hamiltonian Monte Carlo, enables the\nexploitation of the complete Hamiltonian trajectories (hence the title). The\nproposed algorithm may yield large speedups and improvements in stability\nrelative to relevant benchmarks, while incurring numerical errors that are\nnegligible relative to the overall Monte Carlo errors.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 06:23:13 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 14:00:54 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kleppe", "Tore Selland", ""]]}, {"id": "2005.01309", "submitter": "Bruno Sudret", "authors": "X. Zhu and B. Sudret", "title": "Global sensitivity analysis for stochastic simulators based on\n  generalized lambda surrogate models", "comments": null, "journal-ref": "Reliability Engineering and System Safety, #107815, 2021", "doi": "10.1016/j.ress.2021.107815", "report-no": "RSUQ-2020-004D", "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis aims at quantifying the impact of input\nvariability onto the variation of the response of a computational model. It has\nbeen widely applied to deterministic simulators, for which a set of input\nparameters has a unique corresponding output value. Stochastic simulators,\nhowever, have intrinsic randomness due to their use of (pseudo)random numbers,\nso they give different results when run twice with the same input parameters\nbut non-common random numbers. Due to this random nature, conventional Sobol'\nindices, used in global sensitivity analysis, can be extended to stochastic\nsimulators in different ways. In this paper, we discuss three possible\nextensions and focus on those that depend only on the statistical dependence\nbetween input and output. This choice ignores the detailed data generating\nprocess involving the internal randomness, and can thus be applied to a wider\nclass of problems. We propose to use the generalized lambda model to emulate\nthe response distribution of stochastic simulators. Such a surrogate can be\nconstructed without the need for replications. The proposed method is applied\nto three examples including two case studies in finance and epidemiology. The\nresults confirm the convergence of the approach for estimating the sensitivity\nindices even with the presence of strong heteroskedasticity and small\nsignal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 08:03:31 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 16:45:43 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 13:32:39 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 09:56:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhu", "X.", ""], ["Sudret", "B.", ""]]}, {"id": "2005.01336", "submitter": "Jesus Maria Sanz-Serna", "authors": "J.M. Sanz-Serna", "title": "Is the NUTS algorithm correct?", "comments": "Some statements in the paper are misleading. It is possible to think\n  of NUTS at not being a slice/Gibbs sampler and, with an alternative\n  interpretation, it may be be possible to prove that the algorithm is correct.\n  In addition the experiment reported in Figure 2 should have had many initial\n  states drawn from the target rather than using a single value", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to investigate whether the popular No U-turn (NUTS)\nsampling algorithm is correct, i.e.\\ whether the target probability\ndistribution is \\emph{exactly} conserved by the algorithm. It turns out that\none of the Gibbs substeps used in the algorithm cannot always be guaranteed to\nbe correct.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:21:54 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 08:21:25 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Sanz-Serna", "J. M.", ""]]}, {"id": "2005.01379", "submitter": "Gaetano Romano", "authors": "Gaetano Romano, Guillem Rigaill, Vincent Runge, Paul Fearnhead", "title": "Detecting Abrupt Changes in the Presence of Local Fluctuations and\n  Autocorrelated Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst there are a plethora of algorithms for detecting changes in mean in\nunivariate time-series, almost all struggle in real applications where there is\nautocorrelated noise or where the mean fluctuates locally between the abrupt\nchanges that one wishes to detect. In these cases, default implementations,\nwhich are often based on assumptions of a constant mean between changes and\nindependent noise, can lead to substantial over-estimation of the number of\nchanges. We propose a principled approach to detect such abrupt changes that\nmodels local fluctuations as a random walk process and autocorrelated noise via\nan AR(1) process. We then estimate the number and location of changepoints by\nminimising a penalised cost based on this model. We develop a novel and\nefficient dynamic programming algorithm, DeCAFS, that can solve this\nminimisation problem; despite the additional challenge of dependence across\nsegments, due to the autocorrelated noise, which makes existing algorithms\ninapplicable. Theory and empirical results show that our approach has greater\npower at detecting abrupt changes than existing approaches. We apply our method\nto measuring gene expression levels in bacteria.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:51:19 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Romano", "Gaetano", ""], ["Rigaill", "Guillem", ""], ["Runge", "Vincent", ""], ["Fearnhead", "Paul", ""]]}, {"id": "2005.01457", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Tomohiro Shinozaki, Katsuhiro Iba, Satoshi Teramukai and\n  Toshi A. Furukawa", "title": "Confidence intervals of prediction accuracy measures for multivariable\n  prediction models based on the bootstrap-based optimism correction methods", "comments": null, "journal-ref": "Stat Med. 2021", "doi": "10.1002/sim.9148", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In assessing prediction accuracy of multivariable prediction models, optimism\ncorrections are essential for preventing biased results. However, in most\npublished papers of clinical prediction models, the point estimates of the\nprediction accuracy measures are corrected by adequate bootstrap-based\ncorrection methods, but their confidence intervals are not corrected, e.g., the\nDeLong's confidence interval is usually used for assessing the C-statistic.\nThese naive methods do not adjust for the optimism bias and do not account for\nstatistical variability in the estimation of parameters in the prediction\nmodels. Therefore, their coverage probabilities of the true value of the\nprediction accuracy measure can be seriously below the nominal level (e.g.,\n95%). In this article, we provide two generic bootstrap methods, namely (1)\nlocation-shifted bootstrap confidence intervals and (2) two-stage bootstrap\nconfidence intervals, that can be generally applied to the bootstrap-based\noptimism correction methods, i.e., the Harrell's bias correction, 0.632, and\n0.632+ methods. In addition, they can be widely applied to various methods for\nprediction model development involving modern shrinkage methods such as the\nridge and lasso regressions. Through numerical evaluations by simulations, the\nproposed confidence intervals showed favourable coverage performances. Besides,\nthe current standard practices based on the optimism-uncorrected methods showed\nserious undercoverage properties. To avoid erroneous results, the\noptimism-uncorrected confidence intervals should not be used in practice, and\nthe adjusted methods are recommended instead. We also developed the R package\npredboot for implementing these methods (https://github.com/nomahi/predboot).\nThe effectiveness of the proposed methods are illustrated via applications to\nthe GUSTO-I clinical trial.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:15:54 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 11:33:59 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 08:40:44 GMT"}, {"version": "v4", "created": "Wed, 30 Jun 2021 10:18:57 GMT"}, {"version": "v5", "created": "Sun, 25 Jul 2021 12:56:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Noma", "Hisashi", ""], ["Shinozaki", "Tomohiro", ""], ["Iba", "Katsuhiro", ""], ["Teramukai", "Satoshi", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "2005.01814", "submitter": "Hossein Mohammadi", "authors": "Hossein Mohammadi, Peter Challenor, Daniel Williamson, Marc Goodfellow", "title": "Cross-validation based adaptive sampling for Gaussian process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications, we are interested in approximating\nblack-box, costly functions as accurately as possible with the smallest number\nof function evaluations. A complex computer code is an example of such a\nfunction. In this work, a Gaussian process (GP) emulator is used to approximate\nthe output of complex computer code. We consider the problem of extending an\ninitial experiment (set of model runs) sequentially to improve the emulator.\n  A sequential sampling approach based on leave-one-out (LOO) cross-validation\nis proposed that can be easily extended to a batch mode. This is a desirable\nproperty since it saves the user time when parallel computing is available.\nAfter fitting a GP to training data points, the expected squared LOO (ES-LOO)\nerror is calculated at each design point. ES-LOO is used as a measure to\nidentify important data points. More precisely, when this quantity is large at\na point it means that the quality of prediction depends a great deal on that\npoint and adding more samples nearby could improve the accuracy of the GP. As a\nresult, it is reasonable to select the next sample where ES-LOO is maximised.\nHowever, ES-LOO is only known at the experimental design and needs to be\nestimated at unobserved points. To do this, a second GP is fitted to the ES-LOO\nerrors and where the maximum of the modified expected improvement (EI)\ncriterion occurs is chosen as the next sample. EI is a popular acquisition\nfunction in Bayesian optimisation and is used to trade-off between local/global\nsearch. However, it has a tendency towards exploitation, meaning that its\nmaximum is close to the (current) \"best\" sample. To avoid clustering, a\nmodified version of EI, called pseudo expected improvement, is employed which\nis more explorative than EI yet allows us to discover unexplored regions. Our\nresults show that the proposed sampling method is promising.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 19:51:01 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 13:33:53 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 11:09:43 GMT"}, {"version": "v4", "created": "Sun, 26 Jul 2020 08:44:55 GMT"}, {"version": "v5", "created": "Sat, 6 Mar 2021 11:27:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Mohammadi", "Hossein", ""], ["Challenor", "Peter", ""], ["Williamson", "Daniel", ""], ["Goodfellow", "Marc", ""]]}, {"id": "2005.01895", "submitter": "Ping-Shou Zhong", "authors": "Shawn Santo and Ping-Shou Zhong", "title": "Homogeneity Tests of Covariance and Change-Points Identification for\n  High-Dimensional Functional Data", "comments": "The paper has 32 pages with 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference problems for high-dimensional (HD) functional data with\na dense number (T) of repeated measurements taken for a large number of p\nvariables from a small number of n experimental units. The spatial and temporal\ndependence, high dimensionality, and the dense number of repeated measurements\nall make theoretical studies and computation challenging. This paper has two\naims; our first aim is to solve the theoretical and computational challenges in\ndetecting and identifying change points among covariance matrices from HD\nfunctional data. The second aim is to provide computationally efficient and\ntuning-free tools with a guaranteed stochastic error control. The change point\ndetection procedure is developed in the form of testing the homogeneity of\ncovariance matrices. The weak convergence of the stochastic process formed by\nthe test statistics is established under the \"large p, large T and small n\"\nsetting. Under a mild set of conditions, our change point identification\nestimator is proven to be consistent for change points in any location of a\nsequence. Its rate of convergence depends on the data dimension, sample size,\nnumber of repeated measurements, and signal-to-noise ratio. We also show that\nour proposed computation algorithms can significantly reduce the computation\ntime and are applicable to real-world data such as fMRI data with a large\nnumber of HD repeated measurements. Simulation results demonstrate both finite\nsample performance and computational effectiveness of our proposed procedures.\nWe observe that the empirical size of the test is well controlled at the\nnominal level, and the locations of multiple change points can accurately be\nidentified. An application to fMRI data demonstrates that our proposed methods\ncan identify event boundaries in the preface of the movie Sherlock. Our\nproposed procedures are implemented in an R package TechPhD.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 01:04:58 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Santo", "Shawn", ""], ["Zhong", "Ping-Shou", ""]]}, {"id": "2005.02585", "submitter": "Sanjeena Subedi", "authors": "Yuan Fang, Dimitris Karlis, and Sanjeena Subedi", "title": "A Bayesian approach for clustering skewed data using mixtures of\n  multivariate normal-inverse Gaussian distributions", "comments": "40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Gaussian mixture models are gaining increasing attention for mixture\nmodel-based clustering particularly when dealing with data that exhibit\nfeatures such as skewness and heavy tails. Here, such a mixture distribution is\npresented, based on the multivariate normal inverse Gaussian (MNIG)\ndistribution. For parameter estimation of the mixture, a Bayesian approach via\nGibbs sampler is used; for this, a novel approach to simulate univariate\ngeneralized inverse Gaussian random variables and matrix generalized inverse\nGaussian random matrices is provided. The proposed algorithm will be applied to\nboth simulated and real data. Through simulation studies and real data\nanalysis, we show parameter recovery and that our approach provides competitive\nclustering results compared to other clustering approaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 03:56:14 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Fang", "Yuan", ""], ["Karlis", "Dimitris", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "2005.02773", "submitter": "Topi Paananen", "authors": "Topi Paananen, Alejandro Catalina, Paul-Christian B\\\"urkner, Aki\n  Vehtari", "title": "Group Heterogeneity Assessment for Multilevel Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sets contain an inherent multilevel structure, for example, because\nof repeated measurements of the same observational units. Taking this structure\ninto account is critical for the accuracy and calibration of any statistical\nanalysis performed on such data. However, the large number of possible model\nconfigurations hinders the use of multilevel models in practice. In this work,\nwe propose a flexible framework for efficiently assessing differences between\nthe levels of given grouping variables in the data. The assessed group\nheterogeneity is valuable in choosing the relevant group coefficients to\nconsider in a multilevel model. Our empirical evaluations demonstrate that the\nframework can reliably identify relevant multilevel components in both\nsimulated and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 12:42:04 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Paananen", "Topi", ""], ["Catalina", "Alejandro", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "2005.02793", "submitter": "Dr. Wolfgang A. Rolke", "authors": "Wolfgang Rolke and Cristian Gutierrez Gongora", "title": "A Chi-square Goodness-of-Fit Test for Continuous Distributions against a\n  known Alternative", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": "10.1007/s00180-020-00997-x", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chi square goodness-of-fit test is among the oldest known statistical\ntests, first proposed by Pearson in 1900 for the multinomial distribution. It\nhas been in use in many fields ever since. However, various studies have shown\nthat when applied to data from a continuous distribution it is generally\ninferior to other methods such as the Kolmogorov-Smirnov or Anderson-Darling\ntests. However, the performance, that is the power, of the chi square test\ndepends crucially on the way the data is binned. In this paper we describe a\nmethod that automatically finds a binning that is very good against a specific\nalternative. We show that then the chi square test is generally competitive and\nsometimes even superior to other standard tests.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:08:59 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Rolke", "Wolfgang", ""], ["Gongora", "Cristian Gutierrez", ""]]}, {"id": "2005.02952", "submitter": "Mickael Albertus", "authors": "Mickael Albertus", "title": "Exponential increase of the power of the independence and homogeneity\n  chi-square tests with auxiliary information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an extension of the work about the exponential increase of the\npower of two non-parametric tests: the $ Z $-test and the chi-square\ngoodness-of-fit test. Subject to having auxiliary information, it is possible\nto improve exponentially relative to the size of the sample the power of the\nfamous chi-square tests of independence and homogeneity. Improving the power of\nthese statistical tests by using auxiliary information makes it possible either\nto reduce the probability of accepting the null hypothesis under the\nalternative hypothesis, or to reduce the size of the sample necessary to reach\na predefined power. The suggested method is computational and some simple\nstatistical applications are presented to illustrate these results. The\nframework of this work is non-parametric, so it can be applied to any kind of\ndata and any area using statistics.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:47:46 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Albertus", "Mickael", ""]]}, {"id": "2005.03221", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai, Juliet Biggs, Krisztina Kelevitz, Zahra\n  Sadeghi, Tim Wright, James Thompson, Alin Achim, David Bull", "title": "Deep Learning Framework for Detecting Ground Deformation in the Built\n  Environment using Satellite InSAR data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large volumes of Sentinel-1 data produced over Europe are being used to\ndevelop pan-national ground motion services. However, simple analysis\ntechniques like thresholding cannot detect and classify complex deformation\nsignals reliably making providing usable information to a broad range of\nnon-expert stakeholders a challenge. Here we explore the applicability of deep\nlearning approaches by adapting a pre-trained convolutional neural network\n(CNN) to detect deformation in a national-scale velocity field. For our\nproof-of-concept, we focus on the UK where previously identified deformation is\nassociated with coal-mining, ground water withdrawal, landslides and\ntunnelling. The sparsity of measurement points and the presence of spike noise\nmake this a challenging application for deep learning networks, which involve\ncalculations of the spatial convolution between images. Moreover, insufficient\nground truth data exists to construct a balanced training data set, and the\ndeformation signals are slower and more localised than in previous\napplications. We propose three enhancement methods to tackle these problems: i)\nspatial interpolation with modified matrix completion, ii) a synthetic training\ndataset based on the characteristics of real UK velocity map, and iii) enhanced\nover-wrapping techniques. Using velocity maps spanning 2015-2019, our framework\ndetects several areas of coal mining subsidence, uplift due to dewatering,\nslate quarries, landslides and tunnel engineering works. The results\ndemonstrate the potential applicability of the proposed framework to the\ndevelopment of automated ground motion analysis systems.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 03:14:00 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 03:20:00 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Biggs", "Juliet", ""], ["Kelevitz", "Krisztina", ""], ["Sadeghi", "Zahra", ""], ["Wright", "Tim", ""], ["Thompson", "James", ""], ["Achim", "Alin", ""], ["Bull", "David", ""]]}, {"id": "2005.03246", "submitter": "Nicolas Langren\\'e", "authors": "Nicolas Langren\\'e, Xavier Warin", "title": "Fast multivariate empirical cumulative distribution function with\n  connection to kernel density estimation", "comments": "26 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the problem of computing empirical cumulative\ndistribution functions (ECDF) efficiently on large, multivariate datasets.\nComputing an ECDF at one evaluation point requires $\\mathcal{O}(N)$ operations\non a dataset composed of $N$ data points. Therefore, a direct evaluation of\nECDFs at $N$ evaluation points requires a quadratic $\\mathcal{O}(N^2)$\noperations, which is prohibitive for large-scale problems. Two fast and exact\nmethods are proposed and compared. The first one is based on fast summation in\nlexicographical order, with a $\\mathcal{O}(N{\\log}N)$ complexity and requires\nthe evaluation points to lie on a regular grid. The second one is based on the\ndivide-and-conquer principle, with a $\\mathcal{O}(N\\log(N)^{(d-1){\\vee}1})$\ncomplexity and requires the evaluation points to coincide with the input\npoints. The two fast algorithms are described and detailed in the general\n$d$-dimensional case, and numerical experiments validate their speed and\naccuracy. Secondly, the paper establishes a direct connection between\ncumulative distribution functions and kernel density estimation (KDE) for a\nlarge class of kernels. This connection paves the way for fast exact algorithms\nfor multivariate kernel density estimation and kernel regression. Numerical\ntests with the Laplacian kernel validate the speed and accuracy of the proposed\nalgorithms. A broad range of large-scale multivariate density estimation,\ncumulative distribution estimation, survival function estimation and regression\nproblems can benefit from the proposed numerical methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 04:38:42 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 13:14:34 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Langren\u00e9", "Nicolas", ""], ["Warin", "Xavier", ""]]}, {"id": "2005.03253", "submitter": "Illia Horenko Dr.", "authors": "Horenko Illia and Marchenko Ganna and Gagliardini Patrick", "title": "On a computationally-scalable sparse formulation of the multidimensional\n  and non-stationary maximum entropy principle", "comments": null, "journal-ref": "Commun. Appl. Math. Comput. Sci. 15 (2020) 15-32", "doi": "10.2140/camcos.2020.15.15", "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven modelling and computational predictions based on maximum entropy\nprinciple (MaxEnt-principle) aim at finding as-simple-as-possible - but not\nsimpler then necessary - models that allow to avoid the data overfitting\nproblem. We derive a multivariate non-parametric and non-stationary formulation\nof the MaxEnt-principle and show that its solution can be approximated through\na numerical maximisation of the sparse constrained optimization problem with\nregularization. Application of the resulting algorithm to popular financial\nbenchmarks reveals memoryless models allowing for simple and qualitative\ndescriptions of the major stock market indexes data. We compare the obtained\nMaxEnt-models to the heteroschedastic models from the computational\neconometrics (GARCH, GARCH-GJR, MS-GARCH, GARCH-PML4) in terms of the model\nfit, complexity and prediction quality. We compare the resulting model\nlog-likelihoods, the values of the Bayesian Information Criterion, posterior\nmodel probabilities, the quality of the data autocorrelation function fits as\nwell as the Value-at-Risk prediction quality. We show that all of the\nconsidered seven major financial benchmark time series (DJI, SPX, FTSE, STOXX,\nSMI, HSI and N225) are better described by conditionally memoryless\nMaxEnt-models with nonstationary regime-switching than by the common\neconometric models with finite memory. This analysis also reveals a sparse\nnetwork of statistically-significant temporal relations for the positive and\nnegative latent variance changes among different markets. The code is provided\nfor open access.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 05:22:46 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Illia", "Horenko", ""], ["Ganna", "Marchenko", ""], ["Patrick", "Gagliardini", ""]]}, {"id": "2005.03730", "submitter": "Johan Larsson", "authors": "Johan Larsson, Ma{\\l}gorzata Bogdan, Jonas Wallin", "title": "The Strong Screening Rule for SLOPE", "comments": "15 pages, 5 figures", "journal-ref": "Advances in Neural Information Processing Systems 33, 2020, p.\n  14592-14603", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting relevant features from data sets where the number of observations\n($n$) is much smaller then the number of predictors ($p$) is a major challenge\nin modern statistics. Sorted L-One Penalized Estimation (SLOPE), a\ngeneralization of the lasso, is a promising method within this setting. Current\nnumerical procedures for SLOPE, however, lack the efficiency that respective\ntools for the lasso enjoy, particularly in the context of estimating a complete\nregularization path. A key component in the efficiency of the lasso is\npredictor screening rules: rules that allow predictors to be discarded before\nestimating the model. This is the first paper to establish such a rule for\nSLOPE. We develop a screening rule for SLOPE by examining its subdifferential\nand show that this rule is a generalization of the strong rule for the lasso.\nOur rule is heuristic, which means that it may discard predictors erroneously.\nWe present conditions under which this may happen and show that such situations\nare rare and easily safeguarded against by a simple check of the optimality\nconditions. Our numerical experiments show that the rule performs well in\npractice, leading to improvements by orders of magnitude for data in the $p \\gg\nn$ domain, as well as incurring no additional computational overhead when $n\n\\gg p$. We also examine the effect of correlation structures in the design\nmatrix on the rule and discuss algorithmic strategies for employing the rule.\nFinally, we provide an efficient implementation of the rule in our R package\nSLOPE.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 20:14:20 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 18:17:19 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Larsson", "Johan", ""], ["Bogdan", "Ma\u0142gorzata", ""], ["Wallin", "Jonas", ""]]}, {"id": "2005.03952", "submitter": "Chris Oates", "authors": "Marina Riabiz, Wilson Chen, Jon Cockayne, Pawel Swietach, Steven A.\n  Niederer, Lester Mackey, Chris. J. Oates", "title": "Optimal Thinning of MCMC Output", "comments": "To appear in the Journal of the Royal Statistical Society, Series B,\n  2021+", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of heuristics to assess the convergence and compress the output of\nMarkov chain Monte Carlo can be sub-optimal in terms of the empirical\napproximations that are produced. Typically a number of the initial states are\nattributed to \"burn in\" and removed, whilst the remainder of the chain is\n\"thinned\" if compression is also required. In this paper we consider the\nproblem of retrospectively selecting a subset of states, of fixed cardinality,\nfrom the sample path such that the approximation provided by their empirical\ndistribution is close to optimal. A novel method is proposed, based on greedy\nminimisation of a kernel Stein discrepancy, that is suitable for problems where\nheavy compression is required. Theoretical results guarantee consistency of the\nmethod and its effectiveness is demonstrated in the challenging context of\nparameter inference for ordinary differential equations. Software is available\nin the Stein Thinning package in Python, R and MATLAB.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 10:54:25 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:48:04 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 14:58:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Riabiz", "Marina", ""], ["Chen", "Wilson", ""], ["Cockayne", "Jon", ""], ["Swietach", "Pawel", ""], ["Niederer", "Steven A.", ""], ["Mackey", "Lester", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2005.04050", "submitter": "Mark van der Loo", "authors": "Mark P.J. van der Loo", "title": "Monitoring data in R with the lumberjack package", "comments": "Accepted for publication in the Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring data while it is processed and transformed can yield detailed\ninsight into the dynamics of a (running) production system. The lumberjack\npackage is a lightweight package allowing users to follow how an R object is\ntransformed as it is manipulated by R code. The package abstracts all logging\ncode from the user, who only needs to specify which objects are logged and what\ninformation should be logged. A few default loggers are included with the\npackage but the package is extensible through user-defined logger objects.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 13:58:03 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["van der Loo", "Mark P. J.", ""]]}, {"id": "2005.04711", "submitter": "Federico Marotta", "authors": "Federico Marotta (Universit\\`a degli Studi di Torino)", "title": "fplyr: the split-apply-combine strategy for big data in R", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present fplyr, a new package for the R language to deal with big files. It\nallows users to easily implement the split-apply-combine strategy for files\nthat are too big to fit into the available memory, without relying on data\nbases nor introducing non-native R classes. A custom function can be applied\nindependently to each group of observations, and the results may be either\nreturned or directly printed to one or more output files.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 16:34:35 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 16:18:59 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Marotta", "Federico", "", "Universit\u00e0 degli Studi di Torino"]]}, {"id": "2005.05195", "submitter": "Ryan Cory-Wright", "authors": "Dimitris Bertsimas, Ryan Cory-Wright, Jean Pauphilet", "title": "Solving Large-Scale Sparse PCA to Certifiable (Near) Optimality", "comments": "Revision submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (PCA) is a popular dimensionality\nreduction technique for obtaining principal components which are linear\ncombinations of a small subset of the original features. Existing approaches\ncannot supply certifiably optimal principal components with more than $p=100s$\nof variables. By reformulating sparse PCA as a convex mixed-integer\nsemidefinite optimization problem, we design a cutting-plane method which\nsolves the problem to certifiable optimality at the scale of selecting k=5\ncovariates from p=300 variables, and provides small bound gaps at a larger\nscale. We also propose a convex relaxation and greedy rounding scheme that\nprovides bound gaps of $1-2\\%$ in practice within minutes for $p=100$s or hours\nfor $p=1,000$s and is therefore a viable alternative to the exact method at\nscale. Using real-world financial and medical datasets, we illustrate our\napproach's ability to derive interpretable principal components tractably at\nscale.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 15:39:23 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 21:11:28 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 21:04:39 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Cory-Wright", "Ryan", ""], ["Pauphilet", "Jean", ""]]}, {"id": "2005.05584", "submitter": "Kengo Kamatani", "authors": "Kengo Kamatani and Xiaolin Song", "title": "Non-reversible guided Metropolis kernel", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a class of non-reversible Metropolis kernels as a multivariate\nextension of the guided-walk kernel proposed by Gustafson 1998. The main idea\nof our method is to introduce a projection that maps a state space to a totally\nordered group. By using Haar measure, we construct a novel Markov kernel termed\nHaar-mixture kernel, which is of interest in its own right. This is achieved by\ninducing a topological structure to the totally ordered group. Our proposed\nmethod, the Delta-guided Metropolis--Haar kernel, is constructed by using the\nHaar-mixture kernel as a proposal kernel. The proposed non-reversible kernel is\nat least 10 times better than the random-walk Metropolis kernel and Hamiltonian\nMonte Carlo kernel for the logistic regression and a discretely observed\nstochastic process in terms of effective sample size per second.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 07:25:26 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 22:57:03 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kamatani", "Kengo", ""], ["Song", "Xiaolin", ""]]}, {"id": "2005.06334", "submitter": "Stefan Lenz", "authors": "Stefan Lenz, Maren Hackenberg, Harald Binder", "title": "The JuliaConnectoR: a functionally oriented interface for integrating\n  Julia in R", "comments": "23 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like many groups considering the new programming language Julia, we faced the\nchallenge of accessing the algorithms that we develop in Julia from R.\nTherefore, we developed the R package JuliaConnectoR, available from the CRAN\nrepository and GitHub (https://github.com/stefan-m-lenz/JuliaConnectoR), in\nparticular for making advanced deep learning tools available. For\nmaintainability and stability, we decided to base communication between R and\nJulia on TCP, using an optimized binary format for exchanging data. Our package\nalso specifically contains features that allow for a convenient interactive use\nin R. This makes it easy to develop R extensions with Julia or to simply call\nfunctionality from Julia packages in R. Interacting with Julia objects and\ncalling Julia functions becomes user-friendly, as Julia functions and variables\nare made directly available as objects in the R workspace. We illustrate the\nfurther features of our package with code examples, and also discuss advantages\nover the two alternative packages JuliaCall and XRJulia. Finally, we\ndemonstrate the usage of the package with a more extensive example for\nemploying neural ordinary differential equations, a recent deep learning\ntechnique that has received much attention. This example also provides more\ngeneral guidance for integrating deep learning techniques from Julia into R.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 14:18:34 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:14:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lenz", "Stefan", ""], ["Hackenberg", "Maren", ""], ["Binder", "Harald", ""]]}, {"id": "2005.06553", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein", "title": "Two equalities expressing the determinant of a matrix in terms of\n  expectations over matrix-vector products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two equations expressing the inverse determinant of a full rank\nmatrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ in terms of expectations over\nmatrix-vector products. The first relationship is $|\\mathrm{det}\n(\\mathbf{A})|^{-1} = \\mathbb{E}_{\\mathbf{s} \\sim \\mathcal{S}^{n-1}}\\bigl[\\,\n\\Vert \\mathbf{As}\\Vert^{-n} \\bigr]$, where expectations are over vectors drawn\nuniformly on the surface of an $n$-dimensional radius one hypersphere. The\nsecond relationship is $|\\mathrm{det}(\\mathbf{A})|^{-1} =\n\\mathbb{E}_{\\mathbf{x} \\sim q}[\\,p(\\mathbf{Ax}) /\\, q(\\mathbf{x})]$, where $p$\nand $q$ are smooth distributions, and $q$ has full support.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 19:42:40 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 17:35:27 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""]]}, {"id": "2005.06848", "submitter": "Geoffrey McLachlan", "authors": "Sharon X. Lee, Geoffrey J. McLachlan, and Kaleb L. Leemaqz", "title": "Multi-Node EM Algorithm for Finite Mixture Models", "comments": "12 Pages,1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models are powerful tools for modelling and analyzing\nheterogeneous data. Parameter estimation is typically carried out using maximum\nlikelihood estimation via the Expectation-Maximization (EM) algorithm.\nRecently, the adoption of flexible distributions as component densities has\nbecome increasingly popular. Often, the EM algorithm for these models involves\ncomplicated expressions that are time-consuming to evaluate numerically. In\nthis paper, we describe a parallel implementation of the EM-algorithm suitable\nfor both single-threaded and multi-threaded processors and for both single\nmachine and multiple-node systems. Numerical experiments are performed to\ndemonstrate the potential performance gain n different settings. Comparison is\nalso made across two commonly used platforms - R and MATLAB. For illustration,\na fairly general mixture model is used in the comparison.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 10:05:30 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""], ["Leemaqz", "Kaleb L.", ""]]}, {"id": "2005.07380", "submitter": "Bruno Sudret", "authors": "P.-R. Wagner, S. Marelli and B. Sudret", "title": "Bayesian model inversion using stochastic spectral embedding", "comments": null, "journal-ref": "Journal of Computational Physics, Vol. 436, 110141 (2021)", "doi": "10.1016/j.jcp.2021.110141", "report-no": "RSUQ-2020-005B", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new sampling-free approach to solve Bayesian model\ninversion problems that is an extension of the previously proposed spectral\nlikelihood expansions (SLE) method. Our approach, called stochastic spectral\nlikelihood embedding (SSLE), uses the recently presented stochastic spectral\nembedding (SSE) method for local spectral expansion refinement to approximate\nthe likelihood function at the core of Bayesian inversion problems. We show\nthat, similar to SLE, this approach results in analytical expressions for key\nstatistics of the Bayesian posterior distribution, such as evidence, posterior\nmoments and posterior marginals, by direct post-processing of the expansion\ncoefficients. Because SSLE and SSE rely on the direct approximation of the\nlikelihood function, they are in a way independent of the\ncomputational/mathematical complexity of the forward model. We further enhance\nthe efficiency of SSLE by introducing a likelihood specific adaptive sample\nenrichment scheme. To showcase the performance of the proposed SSLE, we solve\nthree problems that exhibit different kinds of complexity in the likelihood\nfunction: multimodality, high posterior concentration and high nominal\ndimensionality. We demonstrate how SSLE significantly improves on SLE, and\npresent it as a promising alternative to existing inversion frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 07:06:55 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:22:44 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wagner", "P. -R.", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "2005.07587", "submitter": "Yufei Yi", "authors": "Yufei Yi, Matey Neykov", "title": "Non-Sparse PCA in High Dimensions via Cone Projected Power Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a cone projected power iteration algorithm to\nrecover the first principal eigenvector from a noisy positive semidefinite\nmatrix. When the true principal eigenvector is assumed to belong to a convex\ncone, the proposed algorithm is fast and has a tractable error. Specifically,\nthe method achieves polynomial time complexity for certain convex cones\nequipped with fast projection such as the monotone cone. It attains a small\nerror when the noisy matrix has a small cone-restricted operator norm. We\nsupplement the above results with a minimax lower bound of the error under the\nspiked covariance model. Our numerical experiments on simulated and real data,\nshow that our method achieves shorter run time and smaller error in comparison\nto the ordinary power iteration and some sparse principal component analysis\nalgorithms if the principal eigenvector is in a convex cone.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 15:02:24 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 17:22:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yi", "Yufei", ""], ["Neykov", "Matey", ""]]}, {"id": "2005.08027", "submitter": "Aijun Zhang", "authors": "Zebin Yang, Hengtao Zhang, Agus Sudjianto, Aijun Zhang", "title": "An Effective and Efficient Initialization Scheme for Training\n  Multi-layer Feedforward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network initialization is the first and critical step for training neural\nnetworks. In this paper, we propose a novel network initialization scheme based\non the celebrated Stein's identity. By viewing multi-layer feedforward neural\nnetworks as cascades of multi-index models, the projection weights to the first\nhidden layer are initialized using eigenvectors of the cross-moment matrix\nbetween the input's second-order score function and the response. The input\ndata is then forward propagated to the next layer and such a procedure can be\nrepeated until all the hidden layers are initialized. Finally, the weights for\nthe output layer are initialized by generalized linear modeling. Such a\nproposed SteinGLM method is shown through extensive numerical results to be\nmuch faster and more accurate than other popular methods commonly used for\ntraining neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 16:17:37 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 05:36:21 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 12:51:00 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Yang", "Zebin", ""], ["Zhang", "Hengtao", ""], ["Sudjianto", "Agus", ""], ["Zhang", "Aijun", ""]]}, {"id": "2005.08057", "submitter": "Yang Feng", "authors": "Yang Feng and Qingfeng Liu", "title": "Nested Model Averaging on Solution Path for High-dimensional Linear\n  Regression", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the nested model averaging method on the solution path for a\nhigh-dimensional linear regression problem. In particular, we propose to\ncombine model averaging with regularized estimators (e.g., lasso and SLOPE) on\nthe solution path for high-dimensional linear regression. In simulation\nstudies, we first conduct a systematic investigation on the impact of predictor\nordering on the behavior of nested model averaging, then show that nested model\naveraging with lasso and SLOPE compares favorably with other competing methods,\nincluding the infeasible lasso and SLOPE with the tuning parameter optimally\nselected. A real data analysis on predicting the per capita violent crime in\nthe United States shows an outstanding performance of the nested model\naveraging with lasso.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 18:09:57 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Feng", "Yang", ""], ["Liu", "Qingfeng", ""]]}, {"id": "2005.08159", "submitter": "Zhiqiang Tan", "authors": "Zexi Song and Zhiqiang Tan", "title": "Hamiltonian Assisted Metropolis Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various Markov chain Monte Carlo (MCMC) methods are studied to improve upon\nrandom walk Metropolis sampling, for simulation from complex distributions.\nExamples include Metropolis-adjusted Langevin algorithms, Hamiltonian Monte\nCarlo, and other recent algorithms related to underdamped Langevin dynamics. We\npropose a broad class of irreversible sampling algorithms, called Hamiltonian\nassisted Metropolis sampling (HAMS), and develop two specific algorithms with\nappropriate tuning and preconditioning strategies. Our HAMS algorithms are\ndesigned to achieve two distinctive properties, while using an augmented target\ndensity with momentum as an auxiliary variable. One is generalized detailed\nbalance, which induces an irreversible exploration of the target. The other is\na rejection-free property, which allows our algorithms to perform\nsatisfactorily with relatively large step sizes. Furthermore, we formulate a\nframework of generalized Metropolis--Hastings sampling, which not only\nhighlights our construction of HAMS at a more abstract level, but also\nfacilitates possible further development of irreversible MCMC algorithms. We\npresent several numerical experiments, where the proposed algorithms are found\nto consistently yield superior results among existing ones.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 03:47:53 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Song", "Zexi", ""], ["Tan", "Zhiqiang", ""]]}, {"id": "2005.08334", "submitter": "Fernando Llorente Fern\\'andez", "authors": "Fernando Llorente, Luca Martino, David Delgado, Javier Lopez-Santiago", "title": "Marginal likelihood computation for model selection and hypothesis\n  testing: an extensive review", "comments": "Keywords: Marginal likelihood, Bayesian evidence, numerical\n  integration, model selection, hypothesis testing, quadrature rules,\n  double-intractable posteriors, partition functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an up-to-date introduction to, and overview of, marginal likelihood\ncomputation for model selection and hypothesis testing. Computing normalizing\nconstants of probability models (or ratio of constants) is a fundamental issue\nin many applications in statistics, applied mathematics, signal processing and\nmachine learning. This article provides a comprehensive study of the\nstate-of-the-art of the topic. We highlight limitations, benefits, connections\nand differences among the different techniques. Problems and possible solutions\nwith the use of improper priors are also described. Some of the most relevant\nmethodologies are compared through theoretical comparisons and numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 18:31:58 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 11:29:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Llorente", "Fernando", ""], ["Martino", "Luca", ""], ["Delgado", "David", ""], ["Lopez-Santiago", "Javier", ""]]}, {"id": "2005.08373", "submitter": "Kevin Smith", "authors": "Kevin D. Smith", "title": "A Tutorial on Multivariate $k$-Statistics and their Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document aims to provide an accessible tutorial on the unbiased\nestimation of multivariate cumulants, using $k$-statistics. We offer an\nexplicit and general formula for multivariate $k$-statistics of arbitrary\norder. We also prove that the $k$-statistics are unbiased, using M\\\"obius\ninversion and rudimentary combinatorics. Many detailed examples are considered\nthroughout the paper. We conclude with a discussion of $k$-statistics\ncomputation, including the challenge of time complexity, and we examine a\ncouple of possible avenues to improve the efficiency of this computation. The\npurpose of this document is threefold: to provide a clear introduction to\n$k$-statistics without relying on specialized tools like the umbral calculus;\nto construct an explicit formula for $k$-statistics that might facilitate\nfuture approximations and faster algorithms; and to serve as a companion paper\nto our Python library PyMoments, which implements this formula.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 21:20:40 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Smith", "Kevin D.", ""]]}, {"id": "2005.08414", "submitter": "Takashi Goda", "authors": "Takashi Goda, Tomohiko Hironaka, Wataru Kitade, Adam Foster", "title": "Unbiased MLMC stochastic gradient-based optimization of Bayesian\n  experimental designs", "comments": "major revision, 26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose an efficient stochastic optimization algorithm to\nsearch for Bayesian experimental designs such that the expected information\ngain is maximized. The gradient of the expected information gain with respect\nto experimental design parameters is given by a nested expectation, for which\nthe standard Monte Carlo method using a fixed number of inner samples yields a\nbiased estimator. In this paper, applying the idea of randomized multilevel\nMonte Carlo (MLMC) methods, we introduce an unbiased Monte Carlo estimator for\nthe gradient of the expected information gain with finite expected squared\n$\\ell_2$-norm and finite expected computational cost per sample. Our unbiased\nestimator can be combined well with stochastic gradient descent algorithms,\nwhich results in our proposal of an optimization algorithm to search for an\noptimal Bayesian experimental design. Numerical experiments confirm that our\nproposed algorithm works well not only for a simple test problem but also for a\nmore realistic pharmacokinetic problem.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 01:02:31 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 02:21:46 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 05:13:20 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Goda", "Takashi", ""], ["Hironaka", "Tomohiko", ""], ["Kitade", "Wataru", ""], ["Foster", "Adam", ""]]}, {"id": "2005.08583", "submitter": "Ben Moews", "authors": "Ben Moews, Morgan A. Schmitz, Andrew J. Lawler, Joe Zuntz, Alex I.\n  Malz, Rafael S. de Souza, Ricardo Vilalta, Alberto Krone-Martins, Emille E.\n  O. Ishida (for the COIN Collaboration)", "title": "Ridges in the Dark Energy Survey for cosmic trough identification", "comments": "12 pages, 5 figures, preprint submitted to MNRAS", "journal-ref": null, "doi": "10.1093/mnras/staa3204", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosmic voids and their corresponding redshift-aggregated projections of mass\ndensities, known as troughs, play an important role in our attempt to model the\nlarge-scale structure of the Universe. Understanding these structures leads to\ntests comparing the standard model with alternative cosmologies, constraints on\nthe dark energy equation of state, and provides evidence to differentiate among\ngravitational theories. In this paper, we extend the subspace-constrained mean\nshift algorithm, a recently introduced method to estimate density ridges, and\napply it to 2D weak-lensing mass density maps from the Dark Energy Survey Y1\ndata release to identify curvilinear filamentary structures. We compare the\nobtained ridges with previous approaches to extract trough structure in the\nsame data, and apply curvelets as an alternative wavelet-based method to\nconstrain densities. We then invoke the Wasserstein distance between noisy and\nnoiseless simulations to validate the denoising capabilities of our method. Our\nresults demonstrate the viability of ridge estimation as a precursor for\ndenoising weak lensing quantities to recover the large-scale structure, paving\nthe way for a more versatile and effective search for troughs.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 10:48:55 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Moews", "Ben", "", "for the COIN Collaboration"], ["Schmitz", "Morgan A.", "", "for the COIN Collaboration"], ["Lawler", "Andrew J.", "", "for the COIN Collaboration"], ["Zuntz", "Joe", "", "for the COIN Collaboration"], ["Malz", "Alex I.", "", "for the COIN Collaboration"], ["de Souza", "Rafael S.", "", "for the COIN Collaboration"], ["Vilalta", "Ricardo", "", "for the COIN Collaboration"], ["Krone-Martins", "Alberto", "", "for the COIN Collaboration"], ["Ishida", "Emille E. O.", "", "for the COIN Collaboration"]]}, {"id": "2005.08794", "submitter": "Min Xu", "authors": "Harry Crane and Min Xu", "title": "Inference on the History of a Randomly Growing Tree", "comments": "36 pages; 7 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of infectious disease in a human community or the proliferation of\nfake news on social media can be modeled as a randomly growing tree-shaped\ngraph. The history of the random growth process is often unobserved but\ncontains important information such as the source of the infection. We consider\nthe problem of statistical inference on aspects of the latent history using\nonly a single snapshot of the final tree. Our approach is to apply random\nlabels to the observed unlabeled tree and analyze the resulting distribution of\nthe growth process, conditional on the final outcome. We show that this\nconditional distribution is tractable under a shape-exchangeability condition,\nwhich we introduce here, and that this condition is satisfied for many popular\nmodels for randomly growing trees such as uniform attachment, linear\npreferential attachment and uniform attachment on a $D$-regular tree. For\ninference of the root under shape-exchangeability, we propose O(n log n) time\nalgorithms for constructing confidence sets with valid frequentist coverage as\nwell as bounds on the expected size of the confidence sets. We also provide\nefficient sampling algorithms that extend our methods to a wide class of\ninference problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:15:15 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 00:28:22 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 19:24:59 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Crane", "Harry", ""], ["Xu", "Min", ""]]}, {"id": "2005.09152", "submitter": "Anqi Dong", "authors": "Anqi Dong, Amirhossein Taghvaei, Tryphon T. Georgiou", "title": "Lasso formulation of the shortest path problem", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The shortest path problem is formulated as an $l_1$-regularized regression\nproblem, known as lasso. Based on this formulation, a connection is established\nbetween Dijkstra's shortest path algorithm and the least angle regression\n(LARS) for the lasso problem. Specifically, the solution path of the lasso\nproblem, obtained by varying the regularization parameter from infinity to zero\n(the regularization path), corresponds to shortest path trees that appear in\nthe bi-directional Dijkstra algorithm. Although Dijkstra's algorithm and the\nLARS formulation provide exact solutions, they become impractical when the size\nof the graph is exceedingly large. To overcome this issue, the alternating\ndirection method of multipliers (ADMM) is proposed to solve the lasso\nformulation. The resulting algorithm produces good and fast approximations of\nthe shortest path by sacrificing exactness that may not be absolutely essential\nin many applications. Numerical experiments are provided to illustrate the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:16:01 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 18:04:43 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Dong", "Anqi", ""], ["Taghvaei", "Amirhossein", ""], ["Georgiou", "Tryphon T.", ""]]}, {"id": "2005.09235", "submitter": "Guanyang Wang", "authors": "Guanyang Wang", "title": "On the Theoretical Properties of the Exchange Algorithm", "comments": "33 pages, 2 figures, typos fixed, include more examples, add\n  literature review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchange algorithm is one of the most popular extensions of\nMetropolis-Hastings algorithm to sample from doubly-intractable distributions.\nHowever, theoretical exploration of exchange algorithm is very limited. For\nexample, natural questions like `Does exchange algorithm converge at a\ngeometric rate?' or `Does the exchange algorithm admit a Central Limit\nTheorem?' have not been answered. In this paper, we study the theoretical\nproperties of exchange algorithm, in terms of asymptotic variance and\nconvergence speed. We compare the exchange algorithm with the original\nMetropolis-Hastings algorithm and provide both necessary and sufficient\nconditions for geometric ergodicity of the exchange algorithm, which can be\napplied to various practical applications such as exponential random graph\nmodels and Ising models. A central limit theorem for the exchange algorithm is\nalso established. Meanwhile, a concrete example, involving the Binomial model\nwith conjugate and non-conjugate priors, is treated in detail with sharp\nconvergence rates. Our results justify the theoretical usefulness of the\nexchange algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 06:16:43 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 22:15:34 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 02:39:41 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wang", "Guanyang", ""]]}, {"id": "2005.09301", "submitter": "Mark van de Wiel", "authors": "Mark A. van de Wiel, Mirrelijn M. van Nee, Armin Rauschenberger", "title": "Fast cross-validation for multi-penalty ridge regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional prediction with multiple data types needs to account for\npotentially strong differences in predictive signal. Ridge regression is a\nsimple model for high-dimensional data that has challenged the predictive\nperformance of many more complex models and learners, and that allows inclusion\nof data type specific penalties. The largest challenge for multi-penalty ridge\nis to optimize these penalties efficiently in a cross-validation (CV) setting,\nin particular for GLM and Cox ridge regression, which require an additional\nestimation loop by iterative weighted least squares (IWLS). Our main\ncontribution is a computationally very efficient formula for the multi-penalty,\nsample-weighted hat-matrix, as used in the IWLS algorithm. As a result, nearly\nall computations are in low-dimensional space, rendering a speed-up of several\norders of magnitude. We developed a flexible framework that facilitates\nmultiple types of response, unpenalized covariates, several performance\ncriteria and repeated CV. Extensions to paired and preferential data types are\nincluded and illustrated on several cancer genomics survival prediction\nproblems. Moreover, we present similar computational shortcuts for maximum\nmarginal likelihood and Bayesian probit regression. The corresponding\nR-package, multiridge, serves as a versatile standalone tool, but also as a\nfast benchmark for other more complex models and multi-view learners.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 09:13:43 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 07:52:28 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["van de Wiel", "Mark A.", ""], ["van Nee", "Mirrelijn M.", ""], ["Rauschenberger", "Armin", ""]]}, {"id": "2005.09710", "submitter": "Boian Lazov", "authors": "Boian Lazov and Tsvetan Vetsov", "title": "Sum of Three Cubes via Optimisation", "comments": "21 pages without the appendices. Any comments will be greatly\n  appreciated!", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By first solving the equation $x^3+y^3+z^3=k$ with fixed $k$ for $z$ and then\nconsidering the distance to the nearest integer function of the result, we turn\nthe sum of three cubes problem into an optimisation one. We then apply three\nstochastic optimisation algorithms to this function in the case with $k=2$,\nwhere there are many known solutions. The goal is to test the effectiveness of\nthe method in searching for integer solutions. The algorithms are a\nmodification of particle swarm optimisation and two implementations of\nsimulated annealing. We want to compare their effectiveness as measured by the\nrunning times of the algorithms. To this end, we model the time data by\nassuming two underlying probability distributions -- exponential and\nlog-normal, and calculate some numerical characteristics for them. Finally, we\nevaluate the statistical distinguishability of our models with respect to the\ngeodesic distance in the manifold with the corresponding Fisher information\nmetric.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 19:14:57 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Lazov", "Boian", ""], ["Vetsov", "Tsvetan", ""]]}, {"id": "2005.10361", "submitter": "Izhar Asael Alonzo Matamoros", "authors": "Izhar Asael Alonzo Matamoros, Cristian Andres Cruz Torres", "title": "varstan: An R package for Bayesian analysis of structured time series\n  models with Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  varstan is an \\proglang{R} package for Bayesian analysis of time series\nmodels using \\proglang{Stan}. The package offers a dynamic way to choose a\nmodel, define priors in a wide range of distributions, check model's fit, and\nforecast with the m-steps ahead predictive distribution. The users can widely\nchoose between implemented models such as \\textit{multiplicative seasonal\nARIMA, dynamic regression, random walks, GARCH, dynamic harmonic\nregressions,VARMA, stochastic Volatility Models, and generalized t-student with\nunknown degree freedom GARCH models}. Every model constructor in \\pkg{varstan}\ndefines weakly informative priors, but prior specifications can be changed in a\ndynamic and flexible way, so the prior distributions reflect the parameter's\ninitial beliefs. For model selection, the package offers the classical\ninformation criteria: AIC, AICc, BIC, DIC, Bayes factor. And more recent\ncriteria such as Widely-applicable information criteria (\\textit{WAIC}), and\nthe Bayesian leave one out cross-validation (\\textit{loo}). In addition, a\nBayesian version for automatic order selection in seasonal ARIMA and dynamic\nregression models can be used as an initial step for the time series analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 21:18:55 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Matamoros", "Izhar Asael Alonzo", ""], ["Torres", "Cristian Andres Cruz", ""]]}, {"id": "2005.10435", "submitter": "HaiYing Wang", "authors": "Jun Yu, HaiYing Wang, Mingyao Ai and Huiming Zhang", "title": "Optimal Distributed Subsampling for Maximum Quasi-Likelihood Estimators\n  with Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonuniform subsampling methods are effective to reduce computational burden\nand maintain estimation efficiency for massive data. Existing methods mostly\nfocus on subsampling with replacement due to its high computational efficiency.\nIf the data volume is so large that nonuniform subsampling probabilities cannot\nbe calculated all at once, then subsampling with replacement is infeasible to\nimplement. This paper solves this problem using Poisson subsampling. We first\nderive optimal Poisson subsampling probabilities in the context of\nquasi-likelihood estimation under the A- and L-optimality criteria. For a\npractically implementable algorithm with approximated optimal subsampling\nprobabilities, we establish the consistency and asymptotic normality of the\nresultant estimators. To deal with the situation that the full data are stored\nin different blocks or at multiple locations, we develop a distributed\nsubsampling framework, in which statistics are computed simultaneously on\nsmaller partitions of the full data. Asymptotic properties of the resultant\naggregated estimator are investigated. We illustrate and evaluate the proposed\nstrategies through numerical experiments on simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 02:46:56 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 13:45:32 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 15:32:22 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yu", "Jun", ""], ["Wang", "HaiYing", ""], ["Ai", "Mingyao", ""], ["Zhang", "Huiming", ""]]}, {"id": "2005.10483", "submitter": "Gherardo Varando", "authors": "Gherardo Varando and Niels Richard Hansen", "title": "Graphical continuous Lyapunov models", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear Lyapunov equation of a covariance matrix parametrizes the\nequilibrium covariance matrix of a stochastic process. This parametrization can\nbe interpreted as a new graphical model class, and we show how the model class\nbehaves under marginalization and introduce a method for structure learning via\n$\\ell_1$-penalized loss minimization. Our proposed method is demonstrated to\noutperform alternative structure learning algorithms in a simulation study, and\nwe illustrate its application for protein phosphorylation network\nreconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 06:50:27 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Varando", "Gherardo", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "2005.10494", "submitter": "Xuekui Zhang", "authors": "Yitao Lu, Julie Zhou, Li Xing, Xuekui Zhang", "title": "The Optimal Design of Clinical Trials with Potential Biomarker Effects,\n  A Novel Computational Approach", "comments": "18 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a future trend of healthcare, personalized medicine tailors medical\ntreatments to individual patients. It requires to identify a subset of patients\nwith the best response to treatment. The subset can be defined by a biomarker\n(e.g. expression of a gene) and its cutoff value. Topics on subset\nidentification have received massive attention. There are over 2 million hits\nby keyword searches on Google Scholar. However, how to properly incorporate the\nidentified subsets/biomarkers to design clinical trials is not trivial and\nrarely discussed in the literature, which leads to a gap between research\nresults and real-world drug development.\n  To fill in this gap, we formulate the problem of clinical trial design into\nan optimization problem involving high-dimensional integration, and propose a\nnovel computational solution based on Monte-Carlo and smoothing methods. Our\nmethod utilizes the modern techniques of General-Purpose computing on Graphics\nProcessing Units for large-scale parallel computing. Compared to the standard\nmethod in three-dimensional problems, our approach is more accurate and 133\ntimes faster. This advantage increases when dimensionality increases. Our\nmethod is scalable to higher-dimensional problems since the precision bound is\na finite number not affected by dimensionality.\n  Our software will be available on GitHub and CRAN, which can be applied to\nguide the design of clinical trials to incorporate the biomarker better.\nAlthough our research is motivated by the design of clinical trials, the method\ncan be used widely to solve other optimization problems involving\nhigh-dimensional integration.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 07:19:07 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Lu", "Yitao", ""], ["Zhou", "Julie", ""], ["Xing", "Li", ""], ["Zhang", "Xuekui", ""]]}, {"id": "2005.10518", "submitter": "Mariia Nosova", "authors": "Mariia Nosova", "title": "A Mathematical Model of Population Growth as a Queuing System", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a new mathematical model of human population growth as an\nautonomous non-Markov queuing system with an unlimited number of servers and\ntwo types of applications is proposed. The research of this system was carried\nout a virtual phase method and a modified method of asymptotic analysis of a\nstochastic age-specific density for a number of applications served in the\nsystem at time t and was proofed that the asymptotic distribution is Gaussian.\nThe main probabilistic characteristics of this distribution are found. The\nmathematical model and methods of its research can be applied to the analysis\nof the population growth both in a single country and around the world.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 08:46:12 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Nosova", "Mariia", ""]]}, {"id": "2005.11300", "submitter": "Thomas Foster", "authors": "Thomas Foster, Chon Lok Lei, Martin Robinson, David Gavaghan, Ben\n  Lambert", "title": "Model Evidence with Fast Tree Based Quadrature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional integration is essential to many areas of science, ranging\nfrom particle physics to Bayesian inference. Approximating these integrals is\nhard, due in part to the difficulty of locating and sampling from regions of\nthe integration domain that make significant contributions to the overall\nintegral. Here, we present a new algorithm called Tree Quadrature (TQ) that\nseparates this sampling problem from the problem of using those samples to\nproduce an approximation of the integral. TQ places no qualifications on how\nthe samples provided to it are obtained, allowing it to use state-of-the-art\nsampling algorithms that are largely ignored by existing integration\nalgorithms. Given a set of samples, TQ constructs a surrogate model of the\nintegrand in the form of a regression tree, with a structure optimised to\nmaximise integral precision. The tree divides the integration domain into\nsmaller containers, which are individually integrated and aggregated to\nestimate the overall integral. Any method can be used to integrate each\nindividual container, so existing integration methods, like Bayesian Monte\nCarlo, can be combined with TQ to boost their performance. On a set of\nbenchmark problems, we show that TQ provides accurate approximations to\nintegrals in up to 15 dimensions; and in dimensions 4 and above, it outperforms\nsimple Monte Carlo and the popular Vegas method.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 17:48:06 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Foster", "Thomas", ""], ["Lei", "Chon Lok", ""], ["Robinson", "Martin", ""], ["Gavaghan", "David", ""], ["Lambert", "Ben", ""]]}, {"id": "2005.11461", "submitter": "HaiYing Wang", "authors": "Guanyu Hu and HaiYing Wang", "title": "Most Likely Optimal Subsampled Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) requires to evaluate the full data likelihood\nat different parameter values iteratively and is often computationally\ninfeasible for large data sets. In this paper, we propose to approximate the\nlog-likelihood with subsamples taken according to nonuniform subsampling\nprobabilities, and derive the most likely optimal (MLO) subsampling\nprobabilities for better approximation. Compared with existing subsampled MCMC\nalgorithm with equal subsampling probabilities, our MLO subsampled MCMC has a\nhigher estimation efficiency with the same subsampling ratio. We also derive a\nformula using the asymptotic distribution of the subsampled log-likelihood to\ndetermine the required subsample size in each MCMC iteration for a given level\nof precision. This formula is used to develop an adaptive version of the MLO\nsubsampled MCMC algorithm. Numerical experiments demonstrate that the proposed\nmethod outperforms the uniform subsampled MCMC.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 04:13:12 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Hu", "Guanyu", ""], ["Wang", "HaiYing", ""]]}, {"id": "2005.11588", "submitter": "Wenyu Chen", "authors": "Wenyu Chen, Rahul Mazumder", "title": "Multivariate Convex Regression at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present new large-scale algorithms for fitting a subgradient regularized\nmultivariate convex regression function to $n$ samples in $d$ dimensions -- a\nkey problem in shape constrained nonparametric regression with widespread\napplications in statistics, engineering and the applied sciences. The\ninfinite-dimensional learning task can be expressed via a convex quadratic\nprogram (QP) with $O(nd)$ decision variables and $O(n^2)$ constraints. While\ninstances with $n$ in the lower thousands can be addressed with current\nalgorithms within reasonable runtimes, solving larger problems (e.g., $n\\approx\n10^4$ or $10^5$) is computationally challenging. To this end, we present an\nactive set type algorithm on the dual QP. For computational scalability, we\nperform approximate optimization of the reduced sub-problems; and propose\nrandomized augmentation rules for expanding the active set. Although the dual\nis not strongly convex, we present a novel linear convergence rate of our\nalgorithm on the dual. We demonstrate that our framework can approximately\nsolve instances of the convex regression problem with $n=10^5$ and $d=10$\nwithin minutes; and offers significant computational gains compared to earlier\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 19:08:39 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 21:20:12 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chen", "Wenyu", ""], ["Mazumder", "Rahul", ""]]}, {"id": "2005.11805", "submitter": "John Paige", "authors": "John Paige, Geir-Arne Fuglstad, Andrea Riebler, and Jon Wakefield", "title": "Bayesian Multiresolution Modeling Of Georeferenced Data", "comments": "main manuscript: 33 pages, 7 figures, 2 tables; supplemental\n  materials: 9 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current implementations of multiresolution methods are limited in terms of\npossible types of responses and approaches to inference. We provide a\nmultiresolution approach for spatial analysis of non-Gaussian responses using\nlatent Gaussian models and Bayesian inference via integrated nested Laplace\napproximation (INLA). The approach builds on `LatticeKrig', but uses a\nreparameterization of the model parameters that is intuitive and interpretable\nso that modeling and prior selection can be guided by expert knowledge about\nthe different spatial scales at which dependence acts. The priors can be used\nto make inference robust and integration over model parameters allows for more\naccurate posterior estimates of uncertainty. The extended LatticeKrig (ELK)\nmodel is compared to a standard implementation of LatticeKrig (LK), and a\nstandard Mat\\'ern model, and we find modest improvement in spatial\noversmoothing and prediction for the ELK model for counts of secondary\neducation completion for women in Kenya collected in the 2014 Kenya demographic\nhealth survey. Through a simulation study with Gaussian responses and a\nrealistic mix of short and long scale dependencies, we demonstrate that the\ndifferences between the three approaches for prediction increases with distance\nto nearest observation.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 17:22:49 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 01:39:43 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Paige", "John", ""], ["Fuglstad", "Geir-Arne", ""], ["Riebler", "Andrea", ""], ["Wakefield", "Jon", ""]]}, {"id": "2005.11890", "submitter": "Ronan Perry", "authors": "Ronan Perry, Gavin Mischler, Richard Guo, Theodore Lee, Alexander\n  Chang, Arman Koul, Cameron Franz, Hugo Richard, Iain Carmichael, Pierre\n  Ablin, Alexandre Gramfort, Joshua T. Vogelstein", "title": "mvlearn: Multiview Machine Learning in Python", "comments": "6 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As data are generated more and more from multiple disparate sources,\nmultiview data sets, where each sample has features in distinct views, have\nballooned in recent years. However, no comprehensive package exists that\nenables non-specialists to use these methods easily. mvlearn is a Python\nlibrary which implements the leading multiview machine learning methods. Its\nsimple API closely follows that of scikit-learn for increased ease-of-use. The\npackage can be installed from Python Package Index (PyPI) and the conda package\nmanager and is released under the MIT open-source license. The documentation,\ndetailed examples, and all releases are available at\nhttps://mvlearn.github.io/.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 02:35:35 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 18:20:34 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 15:13:37 GMT"}, {"version": "v4", "created": "Tue, 25 May 2021 18:16:18 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Perry", "Ronan", ""], ["Mischler", "Gavin", ""], ["Guo", "Richard", ""], ["Lee", "Theodore", ""], ["Chang", "Alexander", ""], ["Koul", "Arman", ""], ["Franz", "Cameron", ""], ["Richard", "Hugo", ""], ["Carmichael", "Iain", ""], ["Ablin", "Pierre", ""], ["Gramfort", "Alexandre", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "2005.12840", "submitter": "Michelangelo Misuraca", "authors": "Michelangelo Misuraca, Alessia Forciniti, Germana Scepi, Maria Spano", "title": "Sentiment Analysis for Education with R: packages, methods and practical\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sentiment Analysis (SA) refers to a family of techniques at the crossroads of\nstatistics, natural language processing, and computational linguistics. The\nprimary goal is to detect the semantic orientation of individual opinions and\ncomments expressed in written texts. There are several practical applications\nof SA in several domains. In an educational context, the use of this approach\nallows processing students' feedback, aiming at monitoring the teaching\neffectiveness of instructors and enhancing the learning experience. This paper\nwants to review the different R packages that can be used to carry on SA,\ncomparing the implemented methods, discussing their characteristics, and\nshowing how they perform by considering a simple example.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:10:43 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Misuraca", "Michelangelo", ""], ["Forciniti", "Alessia", ""], ["Scepi", "Germana", ""], ["Spano", "Maria", ""]]}, {"id": "2005.13097", "submitter": "Rasa Hosseinzadeh", "authors": "Murat A. Erdogdu, Rasa Hosseinzadeh", "title": "On the Convergence of Langevin Monte Carlo: The Interplay between Tail\n  Growth and Smoothness", "comments": "51 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sampling from a target distribution ${\\nu_* = e^{-f}}$ using the\nunadjusted Langevin Monte Carlo (LMC) algorithm. For any potential function $f$\nwhose tails behave like ${\\|x\\|^\\alpha}$ for ${\\alpha \\in [1,2]}$, and has\n$\\beta$-H\\\"older continuous gradient, we prove that ${\\widetilde{\\mathcal{O}}\n\\Big(d^{\\frac{1}{\\beta}+\\frac{1+\\beta}{\\beta}(\\frac{2}{\\alpha} -\n\\boldsymbol{1}_{\\{\\alpha \\neq 1\\}})} \\epsilon^{-\\frac{1}{\\beta}}\\Big)}$ steps\nare sufficient to reach the $\\epsilon $-neighborhood of a $d$-dimensional\ntarget distribution $\\nu_*$ in KL-divergence. This convergence rate, in terms\nof $\\epsilon$ dependency, is not directly influenced by the tail growth rate\n$\\alpha$ of the potential function as long as its growth is at least linear,\nand it only relies on the order of smoothness $\\beta$. One notable consequence\nof this result is that for potentials with Lipschitz gradient, i.e. $\\beta=1$,\nour rate recovers the best known rate\n${\\widetilde{\\mathcal{O}}(d\\epsilon^{-1})}$ which was established for strongly\nconvex potentials in terms of $\\epsilon$ dependency, but we show that the same\nrate is achievable for a wider class of potentials that are degenerately convex\nat infinity. The growth rate $\\alpha$ starts to have an effect on the\nestablished rate in high dimensions where $d$ is large; furthermore, it\nrecovers the best-known dimension dependency when the tail growth of the\npotential is quadratic, i.e. ${\\alpha = 2}$, in the current setup. Our\nframework allows for finite perturbations, and any order of smoothness\n${\\beta\\in(0,1]}$; consequently, our results are applicable to a wide class of\nnon-convex potentials that are weakly smooth and exhibit at least linear tail\ngrowth.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 00:26:20 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Erdogdu", "Murat A.", ""], ["Hosseinzadeh", "Rasa", ""]]}, {"id": "2005.13199", "submitter": "Riko Kelter", "authors": "Riko Kelter", "title": "Bayesian model selection in the $\\mathcal{M}$-open setting --\n  Approximate posterior inference and probability-proportional-to-size\n  subsampling for efficient large-scale leave-one-out cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison of competing statistical models is an essential part of\npsychological research. From a Bayesian perspective, various approaches to\nmodel comparison and selection have been proposed in the literature. However,\nthe applicability of these approaches strongly depends on the assumptions about\nthe model space $\\mathcal{M}$, the so-called model view. Furthermore,\ntraditional methods like leave-one-out cross-validation (LOO-CV) estimate the\nexpected log predictive density (ELPD) of a model to investigate how the model\ngeneralises out-of-sample, which quickly becomes computationally inefficient\nwhen sample size becomes large. Here, we provide a tutorial on approximate\nPareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO),\na computationally efficient method for Bayesian model comparison. First, we\ndiscuss several model views and the available Bayesian model comparison methods\nin each. We then use Bayesian logistic regression as a running example how to\napply the method in practice, and show that it outperforms other methods like\nLOO-CV or information criteria in terms of computational effort while providing\nsimilarly accurate ELPD estimates. In a second step, we show how even\nlarge-scale models can be compared efficiently by using posterior\napproximations in combination with probability-proportional-to-size\nsubsampling. We show how to compare competing models based on the ELPD\nestimates provided, and how to conduct posterior predictive checks to safeguard\nagainst overconfidence in one of the models under consideration. We conclude\nthat the method is attractive for mathematical psychologists who aim at\ncomparing several competing statistical models, which are possibly\nhigh-dimensional and in the big-data regime.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 06:57:27 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Kelter", "Riko", ""]]}, {"id": "2005.13375", "submitter": "Adam Edwards", "authors": "Adam M. Edwards, Robert B. Gramacy", "title": "Precision Aggregated Local Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale Gaussian process (GP) regression is infeasible for larger data\nsets due to cubic scaling of flops and quadratic storage involved in working\nwith covariance matrices. Remedies in recent literature focus on\ndivide-and-conquer, e.g., partitioning into sub-problems and inducing\nfunctional (and thus computational) independence. Such approximations can be\nspeedy, accurate, and sometimes even more flexible than an ordinary GPs.\nHowever, a big downside is loss of continuity at partition boundaries. Modern\nmethods like local approximate GPs (LAGPs) imply effectively infinite\npartitioning and are thus pathologically good and bad in this regard. Model\naveraging, an alternative to divide-and-conquer, can maintain absolute\ncontinuity but often over-smooths, diminishing accuracy. Here we propose\nputting LAGP-like methods into a local experts-like framework, blending\npartition-based speed with model-averaging continuity, as a flagship example of\nwhat we call precision aggregated local models (PALM). Using $K$ LAGPs, each\nselecting $n$ from $N$ total data pairs, we illustrate a scheme that is at most\ncubic in $n$, quadratic in $K$, and linear in $N$, drastically reducing\ncomputational and storage demands. Extensive empirical illustration shows how\nPALM is at least as accurate as LAGP, can be much faster in terms of speed, and\nfurnishes continuous predictive surfaces. Finally, we propose sequential\nupdating scheme which greedily refines a PALM predictor up to a computational\nbudget.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 14:12:13 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Edwards", "Adam M.", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "2005.13483", "submitter": "Pradeep Reddy Raamana", "authors": "Pradeep Reddy Raamana", "title": "Kernel methods library for pattern analysis and machine learning in\n  python", "comments": "6 pages, 3 code examples, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel methods have proven to be powerful techniques for pattern analysis and\nmachine learning (ML) in a variety of domains. However, many of their original\nor advanced implementations remain in Matlab. With the incredible rise and\nadoption of Python in the ML and data science world, there is a clear need for\na well-defined library that enables not only the use of popular kernels, but\nalso allows easy definition of customized kernels to fine-tune them for diverse\napplications. The kernelmethods library fills that important void in the python\nML ecosystem in a domain-agnostic fashion, allowing the sample data type to be\nanything from numerical, categorical, graphs or a combination of them. In\naddition, this library provides a number of well-defined classes to make\nvarious kernel-based operations efficient (for large scale datasets), modular\n(for ease of domain adaptation), and inter-operable (across different\necosystems). The library is available at\nhttps://github.com/raamana/kernelmethods.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:44:42 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Raamana", "Pradeep Reddy", ""]]}, {"id": "2005.14025", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "copent: Estimating Copula Entropy and Transfer Entropy in R", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT cs.LG cs.MS math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical independence and conditional independence are two fundamental\nconcepts in statistics and machine learning. Copula Entropy is a mathematical\nconcept defined by Ma and Sun for multivariate statistical independence\nmeasuring and testing, and also proved to be closely related to conditional\nindependence (or transfer entropy). As the unified framework for measuring both\nindependence and causality, CE has been applied to solve several related\nstatistical or machine learning problems, including association discovery,\nstructure learning, variable selection, and causal discovery. The nonparametric\nmethods for estimating copula entropy and transfer entropy were also proposed\npreviously. This paper introduces copent, the R package which implements these\nproposed methods for estimating copula entropy and transfer entropy. The\nimplementation detail of the package is introduced. Three examples with\nsimulated data and real-world data on variable selection and causal discovery\nare also presented to demonstrate the usage of this package. The examples on\nvariable selection and causal discovery show the strong ability of copent on\ntesting (conditional) independence compared with the related packages. The\ncopent package is available on the Comprehensive R Archive Network (CRAN) and\nalso on GitHub at https://github.com/majianthu/copent.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 10:01:12 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 00:14:25 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 00:41:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "2005.14073", "submitter": "Banghua Zhu", "authors": "Banghua Zhu, Jiantao Jiao and Jacob Steinhardt", "title": "Robust estimation via generalized quasi-gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore why many recently proposed robust estimation problems are\nefficiently solvable, even though the underlying optimization problems are\nnon-convex. We study the loss landscape of these robust estimation problems,\nand identify the existence of \"generalized quasi-gradients\". Whenever these\nquasi-gradients exist, a large family of low-regret algorithms are guaranteed\nto approximate the global minimum; this includes the commonly-used filtering\nalgorithm.\n  For robust mean estimation of distributions under bounded covariance, we show\nthat any first-order stationary point of the associated optimization problem is\nan {approximate global minimum} if and only if the corruption level $\\epsilon <\n1/3$. Consequently, any optimization algorithm that aproaches a stationary\npoint yields an efficient robust estimator with breakdown point $1/3$. With\ncareful initialization and step size, we improve this to $1/2$, which is\noptimal.\n  For other tasks, including linear regression and joint mean and covariance\nestimation, the loss landscape is more rugged: there are stationary points\narbitrarily far from the global minimum. Nevertheless, we show that generalized\nquasi-gradients exist and construct efficient algorithms. These algorithms are\nsimpler than previous ones in the literature, and for linear regression we\nimprove the estimation error from $O(\\sqrt{\\epsilon})$ to the optimal rate of\n$O(\\epsilon)$ for small $\\epsilon$ assuming certified hypercontractivity. For\nmean estimation with near-identity covariance, we show that a simple gradient\ndescent algorithm achieves breakdown point $1/3$ and iteration complexity\n$\\tilde{O}(d/\\epsilon^2)$.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:14:33 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhu", "Banghua", ""], ["Jiao", "Jiantao", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2005.14199", "submitter": "David W. Hogg", "authors": "David W. Hogg (NYU) (MPIA) (Flatiron), Adrian M. Price-Whelan\n  (Flatiron), Boris Leistedt (Imperial) (NYU)", "title": "Data Analysis Recipes: Products of multivariate Gaussians in Bayesian\n  inferences", "comments": "a chapter of a book we will never write", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A product of two Gaussians (or normal distributions) is another Gaussian.\nThat's a valuable and useful fact! Here we use it to derive a refactoring of a\ncommon product of multivariate Gaussians: The product of a Gaussian likelihood\ntimes a Gaussian prior, where some or all of those parameters enter the\nlikelihood only in the mean and only linearly. That is, a linear, Gaussian,\nBayesian model. This product of a likelihood times a prior pdf can be\nrefactored into a product of a marginalized likelihood (or a Bayesian evidence)\ntimes a posterior pdf, where (in this case) both of these are also Gaussian.\nThe means and variance tensors of the refactored Gaussians are straightforward\nto obtain as closed-form expressions; here we deliver these expressions, with\ndiscussion. The closed-form expressions can be used to speed up and improve the\nprecision of inferences that contain linear parameters with Gaussian priors. We\nconnect these methods to inferences that arise frequently in physics and\nastronomy.\n  If all you want is the answer, the question is posed and answered at the\nbeginning of Section 3. We show two toy examples, in the form of worked\nexercises, in Section 4. The solutions, discussion, and exercises in this Note\nare aimed at someone who is already familiar with the basic ideas of Bayesian\ninference and probability.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 18:00:00 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Hogg", "David W.", "", "NYU"], ["Price-Whelan", "Adrian M.", "", "Flatiron"], ["Leistedt", "Boris", "", "Imperial"]]}, {"id": "2005.14281", "submitter": "Philip Maybank", "authors": "Philip Maybank, Patrick Peltzer, Uwe Naumann, Ingo Bojak", "title": "MCMC for Bayesian uncertainty quantification from time-series data", "comments": null, "journal-ref": "LNCS, volume 12143, Coputational Science - ICCS 2020", "doi": "10.1007/978-3-030-50436-6", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in science and engineering require uncertainty quantification\nthat accounts for observed data. For example, in computational neuroscience,\nNeural Population Models (NPMs) are mechanistic models that describe brain\nphysiology in a range of different states. Within computational neuroscience\nthere is growing interest in the inverse problem of inferring NPM parameters\nfrom recordings such as the EEG (Electroencephalogram). Uncertainty\nquantification is essential in this application area in order to infer the\nmechanistic effect of interventions such as anaesthesia. This paper presents\nC++ software for Bayesian uncertainty quantification in the parameters of NPMs\nfrom approximately stationary data using Markov Chain Monte Carlo (MCMC).\nModern MCMC methods require first order (and in some cases higher order)\nderivatives of the posterior density. The software presented offers two\ndistinct methods of evaluating derivatives: finite differences and exact\nderivatives obtained through Algorithmic Differentiation (AD). For AD, two\ndifferent implementations are used: the open source Stan Math Library and the\ncommercially licenced dco/c++ tool distributed by NAG (Numerical Algorithms\nGroup). The use of derivative information in MCMC sampling is demonstrated\nthrough a simple example, the noise-driven harmonic oscillator. And different\nmethods for computing derivatives are compared. The software is written in a\nmodular object-oriented way such that it can be extended to derivative based\nMCMC for other scientific domains.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 20:35:19 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 10:08:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Maybank", "Philip", ""], ["Peltzer", "Patrick", ""], ["Naumann", "Uwe", ""], ["Bojak", "Ingo", ""]]}, {"id": "2005.14372", "submitter": "James Tucker", "authors": "J. Derek Tucker, Lyndsay Shand, and Kenny Chowdhary", "title": "Multimodal Bayesian Registration of Noisy Functions using Hamiltonian\n  Monte Carlo", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2021.107298", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data registration is a necessary processing step for many\napplications. The observed data can be inherently noisy, often due to\nmeasurement error or natural process uncertainty, which most functional\nalignment methods cannot handle. A pair of functions can also have multiple\noptimal alignment solutions, which is not addressed in current literature. In\nthis paper, a flexible Bayesian approach to functional alignment is presented,\nwhich appropriately accounts for noise in the data without any pre-smoothing\nrequired. Additionally, by running parallel MCMC chains, the method can account\nfor multiple optimal alignments via the multi-modal posterior distribution of\nthe warping functions. To most efficiently sample the warping functions, the\napproach relies on a modification of the standard Hamiltonian Monte Carlo to be\nwell-defined on the infinite-dimensional Hilbert space. This flexible Bayesian\nalignment method is applied to both simulated data and real data sets to show\nits efficiency in handling noisy functions and successfully accounting for\nmultiple optimal alignments in the posterior; characterizing the uncertainty\nsurrounding the warping functions.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 03:12:09 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 04:16:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Tucker", "J. Derek", ""], ["Shand", "Lyndsay", ""], ["Chowdhary", "Kenny", ""]]}, {"id": "2005.14398", "submitter": "Sarouyeh Khoshkholgh", "authors": "Sarouyeh Khoshkholgh, Andrea Zunino, Klaus Mosegaard", "title": "Informed Proposal Monte Carlo", "comments": null, "journal-ref": null, "doi": "10.1093/gji/ggab173", "report-no": null, "categories": "physics.geo-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any search or sampling algorithm for solution of inverse problems needs\nguidance to be efficient. Many algorithms collect and apply information about\nthe problem on the fly, and much improvement has been made in this way.\nHowever, as a consequence of the the No-Free-Lunch Theorem, the only way we can\nensure a significantly better performance of search and sampling algorithms is\nto build in as much information about the problem as possible. In the special\ncase of Markov Chain Monte Carlo sampling (MCMC) we review how this is done\nthrough the choice of proposal distribution, and we show how this way of adding\nmore information about the problem can be made particularly efficient when\nbased on an approximate physics model of the problem. A highly nonlinear\ninverse scattering problem with a high-dimensional model space serves as an\nillustration of the gain of efficiency through this approach.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 05:42:45 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Khoshkholgh", "Sarouyeh", ""], ["Zunino", "Andrea", ""], ["Mosegaard", "Klaus", ""]]}, {"id": "2005.14605", "submitter": "Alexander Borisenko", "authors": "Oleksandr Borysenko and Maksym Byshkin", "title": "CoolMomentum: A Method for Stochastic Optimization by Langevin Dynamics\n  with Simulated Annealing", "comments": "9 pages, 2 figures", "journal-ref": "Borysenko, O., Byshkin, M. CoolMomentum: a method for stochastic\n  optimization by Langevin dynamics with simulated annealing. Sci Rep 11, 10705\n  (2021)", "doi": "10.1038/s41598-021-90144-3", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning applications require global optimization of non-convex\nobjective functions, which have multiple local minima. The same problem is\noften found in physical simulations and may be resolved by the methods of\nLangevin dynamics with Simulated Annealing, which is a well-established\napproach for minimization of many-particle potentials. This analogy provides\nuseful insights for non-convex stochastic optimization in machine learning.\nHere we find that integration of the discretized Langevin equation gives a\ncoordinate updating rule equivalent to the famous Momentum optimization\nalgorithm. As a main result, we show that a gradual decrease of the momentum\ncoefficient from the initial value close to unity until zero is equivalent to\napplication of Simulated Annealing or slow cooling, in physical terms. Making\nuse of this novel approach, we propose CoolMomentum -- a new stochastic\noptimization method. Applying Coolmomentum to optimization of Resnet-20 on\nCifar-10 dataset and Efficientnet-B0 on Imagenet, we demonstrate that it is\nable to achieve high accuracies.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 14:44:24 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 15:26:37 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Borysenko", "Oleksandr", ""], ["Byshkin", "Maksym", ""]]}]