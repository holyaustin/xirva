[{"id": "1809.00301", "submitter": "Joaqu\\'in M\\'iguez", "authors": "Dan Crisan, Alberto Lopez-Yela, Joaquin Miguez", "title": "Stable approximation schemes for optimal filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stable filter has the property that it asymptotically `forgets' initial\nperturbations. As a result of this property, it is possible to construct\napproximations of such filters whose errors remain small in time, in other\nwords approximations that are uniformly convergent in the time variable. As\nuniform approximations are ideal from a practical perspective, finding criteria\nfor filter stability has been the subject of many papers. In this paper we seek\nto construct approximate filters that stay close to a given (possibly) unstable\nfilter. Such filters are obtained through a general truncation scheme and,\nunder certain constraints, are stable. The construction enables us to give a\ncharacterisation of the topological properties of the set of optimal filters.\nIn particular, we introduce a natural topology on this set, under which the\nsubset of stable filters is dense.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 05:59:56 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 16:26:35 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 14:36:16 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Crisan", "Dan", ""], ["Lopez-Yela", "Alberto", ""], ["Miguez", "Joaquin", ""]]}, {"id": "1809.00351", "submitter": "Irene C\\'ordoba", "authors": "Irene C\\'ordoba, Gherardo Varando, Concha Bielza, Pedro Larra\\~naga", "title": "A fast Metropolis-Hastings method for generating random correlation\n  matrices", "comments": "8 pages, 3 figures, 2018 conference", "journal-ref": "Lecture Notes in Computer Science (IDEAL 2018), 11314:117-124,\n  2018", "doi": "10.1007/978-3-030-03493-1_13", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Metropolis-Hastings algorithm to sample uniformly from the\nspace of correlation matrices. Existing methods in the literature are based on\nelaborated representations of a correlation matrix, or on complex\nparametrizations of it. By contrast, our method is intuitive and simple, based\nthe classical Cholesky factorization of a positive definite matrix and Markov\nchain Monte Carlo theory. We perform a detailed convergence analysis of the\nresulting Markov chain, and show how it benefits from fast convergence, both\ntheoretically and empirically. Furthermore, in numerical experiments our\nalgorithm is shown to be significantly faster than the current alternative\napproaches, thanks to its simple yet principled approach.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 14:47:40 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["C\u00f3rdoba", "Irene", ""], ["Varando", "Gherardo", ""], ["Bielza", "Concha", ""], ["Larra\u00f1aga", "Pedro", ""]]}, {"id": "1809.00669", "submitter": "Maikol Sol\\'is", "authors": "Alberto Hern\\'andez, Maikol Sol\\'is, Ronald Z\\'u\\~niga", "title": "Geometric goodness of fit measure to detect patterns in data point\n  clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We derived a geometric goodness-of-fit index, similar to $R^2$ using\ntopological data analysis techniques. We build the Vietoris-Rips complex from\nthe data-cloud projected onto each variable. Estimating the area of the complex\nand their domain, we create an index that measures the emptiness of the space\nwith respect to the data. We made the analysis with an own package called TopSA\n(Topological Sensitivy Analysis).\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 17:47:39 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 00:03:48 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 21:49:00 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 22:26:16 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hern\u00e1ndez", "Alberto", ""], ["Sol\u00eds", "Maikol", ""], ["Z\u00fa\u00f1iga", "Ronald", ""]]}, {"id": "1809.01046", "submitter": "Bingjing Tang", "authors": "Aditi Iyer, Bingjing Tang, Vinayak Rao, Nan Kong", "title": "Group-Representative Functional Network Estimation from Multi-Subject\n  fMRI Data via MRF-based Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel two-phase approach to functional network estimation of\nmulti-subject functional Magnetic Resonance Imaging (fMRI) data, which applies\nmodel-based image segmentation to determine a group-representative connectivity\nmap. In our approach, we first improve clustering-based Independent Component\nAnalysis (ICA) to generate maps of components occurring consistently across\nsubjects, and then estimate the group-representative map through MAP-MRF\n(Maximum a priori - Markov random field) labeling. For the latter, we provide a\nnovel and efficient variational Bayes algorithm. We study the performance of\nthe proposed method using synthesized data following a theoretical model, and\ndemonstrate its viability in blind extraction of group-representative\nfunctional networks using simulated fMRI data. We anticipate the proposed\nmethod will be applied in identifying common neuronal characteristics in a\npopulation, and could be further extended to real-world clinical diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 16:32:22 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Iyer", "Aditi", ""], ["Tang", "Bingjing", ""], ["Rao", "Vinayak", ""], ["Kong", "Nan", ""]]}, {"id": "1809.01903", "submitter": "Chris Sherlock Dr.", "authors": "Chris Sherlock", "title": "Reversible Markov chains: variational representations and ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This pedagogical document explains three variational representations that are\nuseful when comparing the efficiencies of reversible Markov chains: (i) the\nDirichlet form and the associated variational representations of the spectral\ngaps; (ii) a variational representation of the asymptotic variance of an\nergodic average; and (iii) the conductance, and the equivalence of a non-zero\nconductance to a non-zero right spectral gap.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 09:35:07 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Sherlock", "Chris", ""]]}, {"id": "1809.02264", "submitter": "Nicholas Tierney", "authors": "Nicholas J Tierney, Dianne H Cook", "title": "Expanding tidy data principles to facilitate missing data exploration,\n  visualization and assessment of imputations", "comments": "30 pages, 16 figures, 7 tables, package available at\n  github.com/njtierney/naniar", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the large body of research on missing value distributions and\nimputation, there is comparatively little literature with a focus on how to\nmake it easy to handle, explore, and impute missing values in data. This paper\naddresses this gap. The new methodology builds upon tidy data principles, with\nthe goal of integrating missing value handling as a key part of data analysis\nworkflows. We define a new data structure, and a suite of new operations.\nTogether, these provide a connected framework for handling, exploring, and\nimputing missing values. These methods are available in the R package `naniar`.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 01:01:38 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 00:50:37 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Tierney", "Nicholas J", ""], ["Cook", "Dianne H", ""]]}, {"id": "1809.02385", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Mixtures of Skewed Matrix Variate Bilinear Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, data have become increasingly higher dimensional and,\ntherefore, an increased need has arisen for dimension reduction techniques for\nclustering. Although such techniques are firmly established in the literature\nfor multivariate data, there is a relative paucity in the area of matrix\nvariate, or three-way, data. Furthermore, the few methods that are available\nall assume matrix variate normality, which is not always sensible if cluster\nskewness or excess kurtosis is present. Mixtures of bilinear factor analyzers\nusing skewed matrix variate distributions are proposed. In all, four such\nmixture models are presented, based on matrix variate skew-t, generalized\nhyperbolic, variance-gamma, and normal inverse Gaussian distributions,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 10:04:39 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 23:50:33 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 17:05:05 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1809.02857", "submitter": "Vincent Vu", "authors": "Vincent Q. Vu", "title": "Computational Sufficiency, Reflection Groups, and Generalized Lasso\n  Penalties", "comments": "28 page, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimators with generalized lasso penalties within the computational\nsufficiency framework introduced by Vu (2018, arXiv:1807.05985). By\nrepresenting these penalties as support functions of zonotopes and more\ngenerally Minkowski sums of line segments and rays, we show that there is a\nnatural reflection group associated with the underlying optimization problem. A\nconsequence of this point of view is that for large classes of estimators\nsharing the same penalty, the penalized least squares estimator is\ncomputationally minimal sufficient. This means that all such estimators can be\ncomputed by refining the output of any algorithm for the least squares case. An\ninteresting technical component is our analysis of coordinate descent on the\ndual problem. A key insight is that the iterates are obtained by reflecting and\naveraging, so they converge to an element of the dual feasible set that is\nminimal with respect to a ordering induced by the group associated with the\npenalty. Our main application is fused lasso/total variation denoising and\nisotonic regression on arbitrary graphs. In those cases the associated group is\na permutation group.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 19:31:27 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Vu", "Vincent Q.", ""]]}, {"id": "1809.02959", "submitter": "Mahdi Teimouri Yanesari", "authors": "Mahdi Teimouri", "title": "MPS: An R package for modelling new families of distributions", "comments": "35 pages; 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an \\verb|R| package, called \\verb|MPS|, for computing the\nprobability density function, computing the cumulative distribution function,\ncomputing the quantile function, simulating random variables, and estimating\nthe parameters of 24 new shifted families of distributions. By considering an\nextra shift (location) parameter for each family more flexibility yields. Under\nsome situations, since the maximum likelihood estimators may fail to exist, we\nadopt the well-known maximum product spacings approach to estimate the\nparameters of shifted 24 new families of distributions. The performance of the\n\\verb|MPS| package for computing the cdf, pdf, and simulating random samples\nwill be checked by examples. The performance of the maximum product spacings\napproach is demonstrated by executing \\verb|MPS| package for three sets of real\ndata. As it will be shown, for the first set, the maximum likelihood estimators\nbreak down but \\verb|MPS| package find them. For the second set, adding the\nlocation parameter leads to acceptance the model while absence of the location\nparameter makes the model quite inappropriate. For the third set, presence of\nthe location parameter yields a better fit.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 11:51:55 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Teimouri", "Mahdi", ""]]}, {"id": "1809.03031", "submitter": "Dimitris Korobilis Prof", "authors": "Gary Koop, Dimitris Korobilis", "title": "Bayesian dynamic variable selection in high dimensions", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a variational Bayes algorithm for computationally\nefficient posterior and predictive inference in time-varying parameter (TVP)\nmodels. Within this context we specify a new dynamic variable/model selection\nstrategy for TVP dynamic regression models in the presence of a large number of\npredictors. This strategy allows for assessing in individual time periods which\npredictors are relevant (or not) for forecasting the dependent variable. The\nnew algorithm is evaluated numerically using synthetic data and its\ncomputational advantages are established. Using macroeconomic data for the US\nwe find that regression models that combine time-varying parameters with the\ninformation in many predictors have the potential to improve forecasts of price\ninflation over a number of alternative forecasting models.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 19:52:05 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 09:43:16 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Koop", "Gary", ""], ["Korobilis", "Dimitris", ""]]}, {"id": "1809.03122", "submitter": "Shin Harase", "authors": "Shin Harase", "title": "Comparison of Sobol' sequences in financial applications", "comments": "20 pages", "journal-ref": "Monte Carlo Methods and Applications, Volume 25, Issue 1 (March\n  2019), Pages 61-74", "doi": "10.1515/mcma-2019-2029", "report-no": null, "categories": "math.NA cs.CE cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sobol' sequences are widely used for quasi-Monte Carlo methods that arise in\nfinancial applications. Sobol' sequences have parameter values called direction\nnumbers, which are freely chosen by the user, so there are several\nimplementations of Sobol' sequence generators. The aim of this paper is to\nprovide a comparative study of (non-commercial) high-dimensional Sobol'\nsequences by calculating financial models. Additionally, we implement the\nNiederreiter sequence (in base 2) with a slight modification, that is, we\nreorder the rows of the generating matrices, and analyze and compare it with\nthe Sobol' sequences.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 03:46:45 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 12:47:34 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 11:45:57 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Harase", "Shin", ""]]}, {"id": "1809.03176", "submitter": "Colin Fox", "authors": "Tiangang Cui, Colin Fox, Michael J O'Sullivan", "title": "A posteriori stochastic correction of reduced models in delayed\n  acceptance MCMC, with application to multiphase subsurface inverse problems", "comments": "Submitted to IJNME", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample-based Bayesian inference provides a route to uncertainty\nquantification in the geosciences, and inverse problems in general, though is\nvery computationally demanding in the naive form that requires simulating an\naccurate computer model at each iteration. We present a new approach that\nconstructs a stochastic correction to the error induced by a reduced model,\nwith the correction improving as the algorithm proceeds. This enables sampling\nfrom the correct target distribution at reduced computational cost per\niteration, as in existing delayed-acceptance schemes, while avoiding\nappreciable loss of statistical efficiency that necessarily occurs when using a\nreduced model. Use of the stochastic correction significantly reduces the\ncomputational cost of estimating quantities of interest within desired\nuncertainty bounds. In contrast, existing schemes that use a reduced model\ndirectly as a surrogate do not actually improve computational efficiency in our\ntarget applications. We build on recent simplified conditions for adaptive\nMarkov chain Monte Carlo algorithms to give practical approximation schemes and\nalgorithms with guaranteed convergence. The efficacy of this new approach is\ndemonstrated in two computational examples, including calibration of a\nlarge-scale numerical model of a real geothermal reservoir, that show good\ncomputational and statistical efficiencies on both synthetic and measured data\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 08:33:47 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 09:33:27 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Cui", "Tiangang", ""], ["Fox", "Colin", ""], ["O'Sullivan", "Michael J", ""]]}, {"id": "1809.03388", "submitter": "Christian P. Robert", "authors": "Changye Wu (CEREMADE, UNiversity Paris Dauphine) and Christian P.\n  Robert (CEREMADE, UNiversity Paris Dauphine and Dept of Statistics,\n  University of Warwick)", "title": "The Coordinate Sampler: A Non-Reversible Gibbs-like MCMC Sampler", "comments": "5 figures, 26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this article, we derive a novel non-reversible, continuous-time Markov\nchain Monte Carlo (MCMC) sampler, called Coordinate Sampler, based on a\npiecewise deterministic Markov process (PDMP), which can be seen as a variant\nof the Zigzag sampler. In addition to proving a theoretical validation for this\nnew sampling algorithm, we show that the Markov chain it induces exhibits\ngeometrical ergodicity convergence, for distributions whose tails decay at\nleast as fast as an exponential distribution and at most as fast as a Gaussian\ndistribution. Several numerical examples highlight that our coordinate sampler\nis more efficient than the Zigzag sampler, in terms of effective sample size.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:21:11 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 08:20:08 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Wu", "Changye", "", "CEREMADE, UNiversity Paris Dauphine"], ["Robert", "Christian P.", "", "CEREMADE, UNiversity Paris Dauphine and Dept of Statistics,\n  University of Warwick"]]}, {"id": "1809.03561", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Quantile Regression for Qualifying Match of GEFCom2017 Probabilistic\n  Load Forecasting", "comments": "accepted for International Journal of Forecasting", "journal-ref": "International Journal of Forecasting, 35.4 (2019) 1400-1408", "doi": "10.1016/j.ijforecast.2018.07.004", "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple quantile regression-based forecasting method that was\napplied in a probabilistic load forecasting framework of the Global Energy\nForecasting Competition 2017 (GEFCom2017). The hourly load data is log\ntransformed and split into a long-term trend component and a remainder term.\nThe key forecasting element is the quantile regression approach for the\nremainder term that takes into account weekly and annual seasonalities such as\ntheir interactions. Temperature information is only used to stabilize the\nforecast of the long-term trend component. Public holidays information is\nignored. Still, the forecasting method placed second in the open data track and\nfourth in the definite data track with our forecasting method, which is\nremarkable given simplicity of the model. The method also outperforms the\nVanilla benchmark consistently.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 19:31:15 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "1809.03659", "submitter": "Boris Beranger", "authors": "Boris Beranger, Huan Lin, and Scott A. Sisson", "title": "New models for symbolic data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic data analysis (SDA) is an emerging area of statistics concerned with\nunderstanding and modelling data that takes distributional form (i.e. symbols),\nsuch as random lists, intervals and histograms. It was developed under the\npremise that the statistical unit of interest is the symbol, and that inference\nis required at this level. Here we consider a different perspective, which\nopens a new research direction in the field of SDA. We assume that, as with a\nstandard statistical analysis, inference is required at the level of\nindividual-level data. However, the individual-level data are aggregated into\nsymbols - group-based distributional-valued summaries - prior to the analysis.\nIn this way, large and complex datasets can be reduced to a smaller number of\ndistributional summaries, that may be analysed more efficiently than the\noriginal dataset. As such, we develop SDA techniques as a new approach for the\nanalysis of big data. In particular we introduce a new general method for\nconstructing likelihood functions for symbolic data based on a desired\nprobability model for the underlying measurement-level data, while only\nobserving the distributional summaries. This approach opens the door for new\nclasses of symbol design and construction, in addition to developing SDA as a\nviable tool to enable and improve upon classical data analyses, particularly\nfor very large and complex datasets. We illustrate this new direction for SDA\nresearch through several real and simulated data analyses.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 02:36:10 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 03:13:04 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Beranger", "Boris", ""], ["Lin", "Huan", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1809.03986", "submitter": "Emmanouil Zampetakis", "authors": "Constantinos Daskalakis and Themis Gouleakis and Christos Tzamos and\n  Manolis Zampetakis", "title": "Efficient Statistics, in High Dimensions, from Truncated Samples", "comments": "Appeared at 59th Annual IEEE Symposium on Foundations of Computer\n  Science (FOCS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an efficient algorithm for the classical problem, going back to\nGalton, Pearson, and Fisher, of estimating, with arbitrary accuracy the\nparameters of a multivariate normal distribution from truncated samples.\nTruncated samples from a $d$-variate normal ${\\cal\nN}(\\mathbf{\\mu},\\mathbf{\\Sigma})$ means a samples is only revealed if it falls\nin some subset $S \\subseteq \\mathbb{R}^d$; otherwise the samples are hidden and\ntheir count in proportion to the revealed samples is also hidden. We show that\nthe mean $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$ can be\nestimated with arbitrary accuracy in polynomial-time, as long as we have oracle\naccess to $S$, and $S$ has non-trivial measure under the unknown $d$-variate\nnormal distribution. Additionally we show that without oracle access to $S$,\nany non-trivial estimation is impossible.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 15:42:43 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:39:09 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Gouleakis", "Themis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1809.04000", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, Stephan Hemri and Mehrez El Ayari", "title": "Statistical post-processing of hydrological forecasts using Bayesian\n  model averaging", "comments": "19 pages, 6 figures", "journal-ref": "Water Resources Research 55 (2019), no. 5, 3997-4013", "doi": "10.1029/2018WR024028", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable probabilistic forecasts of hydrological quantities like\nrunoff or water level are beneficial to various areas of society. Probabilistic\nstate-of-the-art hydrological ensemble prediction models are usually driven\nwith meteorological ensemble forecasts. Hence, biases and dispersion errors of\nthe meteorological forecasts cascade down to the hydrological predictions and\nadd to the errors of the hydrological models. The systematic parts of these\nerrors can be reduced by applying statistical post-processing. For a sound\nestimation of predictive uncertainty and an optimal correction of systematic\nerrors, statistical post-processing methods should be tailored to the\nparticular forecast variable at hand. Former studies have shown that it can\nmake sense to treat hydrological quantities as bounded variables. In this\npaper, a doubly truncated Bayesian model averaging (BMA) method, which allows\nfor flexible post-processing of (multi-model) ensemble forecasts of water\nlevel, is introduced. A case study based on water level for a gauge of river\nRhine, reveals a good predictive skill of doubly truncated BMA compared both to\nthe raw ensemble and the reference ensemble model output statistics approach.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 13:06:16 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Hemri", "Stephan", ""], ["Ayari", "Mehrez El", ""]]}, {"id": "1809.04068", "submitter": "Balu Nadiga", "authors": "B.T. Nadiga, N.M. Urban", "title": "Dependence of Inferred Climate Sensitivity on the Discrepancy Model", "comments": "4 pages, 1 figure", "journal-ref": "NCAR Technical Notes NCAR/TN-529+PROC, 2016", "doi": "10.5065/D6K072N", "report-no": "NCAR Technical Notes NCAR/TN-529+PROC", "categories": "physics.ao-ph physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the effect of different temporal error structures on the\ninference of equilibrium climate sensitivity\\footnote{ECS is defined as the\nrealized equilibrium surface warming---globally-averaged surface air\ntemperature---for a doubling of CO$_2$}(ECS), in the context of an energy\nbalance model (EBM) that is commonly employed in analyzing earth system models\n(ESM) and observations. We consider error structures ranging from uncorrelated\n(IID normal) to AR(1) to Gaussian correlation (Gaussian Process GP) to analyze\nthe abrupt 4xCO$_2$ CMIP5 experiment in twenty-one different ESMs. For seven of\nthe ESMs, the posterior distribution of ECS is seen to depend rather weakly on\nthe discrepancy model used suggesting that the discrepancies were largely\nuncorrelated. However, large differences for four, and moderate differences for\nthe rest of the ESMs, leads us to suggest that AR(1) is an appropriate\ndiscrepancy correlation structure to use in situations such as the one\nconsidered in this article. Other significant findings include: (a) When\nestimates of ECS (mode) were differrent, estimates using IID were higher (b)\nFor four of the ESMs, uncertainty in the inference of ECS was higher with the\nIID discrepancy structure than with the other correlated structures, and (c)\nUncertainty in the estimation of GP parameters were much higher than with the\nestimation of IID or AR(1) parameters, possibly due to identifiability issues.\nThey need to be investigated further.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 21:56:17 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Nadiga", "B. T.", ""], ["Urban", "N. M.", ""]]}, {"id": "1809.04129", "submitter": "V\\'ictor Elvira", "authors": "V\\'ictor Elvira, Luca Martino, Christian P. Robert", "title": "Rethinking the Effective Sample Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective sample size (ESS) is widely used in sample-based simulation\nmethods for assessing the quality of a Monte Carlo approximation of a given\ndistribution and of related integrals. In this paper, we revisit and complete\nthe approximation of the ESS in the specific context of importance sampling\n(IS). The derivation of this approximation, that we will denote as\n$\\widehat{\\text{ESS}}$, is only partially available in Kong [1992]. This\napproximation has been widely used in the last 25 years due to its simplicity\nas a practical rule of thumb in a wide variety of importance sampling methods.\nHowever, we show that the multiple assumptions and approximations in the\nderivation of $\\widehat{\\text{ESS}}$, makes it difficult to be considered even\nas a reasonable approximation of the ESS. We extend the discussion of the ESS\nin the multiple importance sampling (MIS) setting, and we display numerical\nexamples. This paper does not cover the use of ESS for MCMC algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 19:54:32 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Elvira", "V\u00edctor", ""], ["Martino", "Luca", ""], ["Robert", "Christian P.", ""]]}, {"id": "1809.04389", "submitter": "Pulong Ma", "authors": "Pulong Ma, Emily L. Kang", "title": "Spatio-Temporal Data Fusion for Massive Sea Surface Temperature Data\n  from MODIS and AMSR-E Instruments", "comments": "Accepted in Environmetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing data have been widely used to study various geophysical\nprocesses. With the advances in remote-sensing technology, massive amount of\nremote sensing data are collected in space over time. Different satellite\ninstruments typically have different footprints, measurement-error\ncharacteristics, and data coverages. To combine datasets from different\nsatellite instruments, we propose a dynamic fused Gaussian process (DFGP) model\nthat enables fast statistical inference such as filtering and smoothing for\nmassive spatio-temporal datasets in a data-fusion context. Based upon a\nspatio-temporal-random-effects model, the DFGP methodology represents the\nunderlying true process with two components: a linear combination of a small\nnumber of basis functions and random coefficients with a general covariance\nmatrix, together with a linear combination of a large number of basis functions\nand Markov random coefficients. To model the underlying geophysical process at\ndifferent spatial resolutions, we rely on the change-of-support property, which\nalso allows efficient computations in the DFGP model. To estimate model\nparameters, we devise a computationally efficient stochastic\nexpectation-maximization (SEM) algorithm to ensure its scalability for massive\ndatasets. The DFGP model is applied to a total of 3.7 million sea surface\ntemperature datasets in the tropical Pacific Ocean for a one-week time period\nin 2010 from MODIS and AMSR-E instruments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 12:54:06 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 17:01:34 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Ma", "Pulong", ""], ["Kang", "Emily L.", ""]]}, {"id": "1809.04494", "submitter": "Dennis Becker", "authors": "Dennis Becker", "title": "Automatic structure estimation of predictive models for symptom\n  development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Online mental health treatment has the premise to meet the increasing demand\nfor mental health treatment at a lower cost than traditional treatment.\nHowever, online treatment suffers from high drop-out rates, which might negate\ntheir cost effectiveness. Predictive models might aid in early identification\nof deviating clients which allows to target them directly to prevent drop-out\nand improve treatment outcomes. We propose a two-staged multi-objective\noptimization process to automatically infer model structures based on\necological momentary assessment for prediction of future symptom development.\nThe proposed multi-objective optimization approach results in a temporal-causal\nnetwork model with the best prediction performance for each concept. This\nallows for a selection of a disorder-specific model structure based on the\nenvisioned field of application.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 14:50:19 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Becker", "Dennis", ""]]}, {"id": "1809.04541", "submitter": "Dootika Vats", "authors": "Dootika Vats and James M. Flegal", "title": "Lugsail lag windows for estimating time-average covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lag windows are commonly used in time series, econometrics, steady-state\nsimulation, and Markov chain Monte Carlo to estimate time-average covariance\nmatrices. In the presence of positive correlation of the underlying process,\nestimators of this matrix almost always exhibit significant negative bias,\nleading to undesirable finite-sample properties. We propose a new family of lag\nwindows specifically designed to improve finite-sample performance by\noffsetting this negative bias. Any existing lag window can be adapted into a\nlugsail equivalent with no additional assumptions. We use these lag windows\nwithin spectral variance estimators and demonstrate its advantages in a linear\nregression model with autocorrelated and heteroskedastic residuals. We further\nemploy the lugsail lag windows in weighted batch means estimators due to their\ncomputational efficiency on large simulation output. We obtain bias and\nvariance results for these multivariate estimators and significantly weaken the\nmixing condition on the process. Superior finite-sample properties are\nillustrated in a vector autoregressive process and a Bayesian logistic\nregression model.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:19:01 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 17:52:15 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 03:56:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Vats", "Dootika", ""], ["Flegal", "James M.", ""]]}, {"id": "1809.04571", "submitter": "J. Ricardo G. Mendon\\c{c}a", "authors": "J. R. G. Mendon\\c{c}a", "title": "Efficient generation of random derangements with the expected\n  distribution of cycle lengths", "comments": "This version corrected and updated; 14 pages, 2 algorithms, 2 tables,\n  4 figures", "journal-ref": "Computational and Applied Mathematics 39 (3), 244 (2020)", "doi": "10.1007/s40314-020-01295-4", "report-no": null, "categories": "stat.CO cond-mat.stat-mech math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to generate random derangements efficiently by two different\ntechniques: random restricted transpositions and sequential importance\nsampling. The algorithm employing restricted transpositions can also be used to\ngenerate random fixed-point-free involutions only, a.k.a. random perfect\nmatchings on the complete graph. Our data indicate that the algorithms generate\nrandom samples with the expected distribution of cycle lengths, which we\nderive, and for relatively small samples, which can actually be very large in\nabsolute numbers, we argue that they generate samples indistinguishable from\nthe uniform distribution. Both algorithms are simple to understand and\nimplement and possess a performance comparable to or better than those of\ncurrently known methods. Simulations suggest that the mixing time of the\nalgorithm based on random restricted transpositions (in the total variance\ndistance with respect to the distribution of cycle lengths) is\n$O(n^{a}\\log{n}^{2})$ with $a \\simeq \\frac{1}{2}$ and $n$ the length of the\nderangement. We prove that the sequential importance sampling algorithm\ngenerates random derangements in $O(n)$ time with probability $O(1/n)$ of\nfailing.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 17:25:19 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 11:19:28 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 16:25:03 GMT"}, {"version": "v4", "created": "Sun, 5 Jan 2020 16:49:52 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Mendon\u00e7a", "J. R. G.", ""]]}, {"id": "1809.04746", "submitter": "Zhenxun Wang", "authors": "Zhenxun Wang, Yunan Wu, Haitao Chu", "title": "On equivalence of the LKJ distribution and the restricted Wishart\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we want to show the Restricted Wishart distribution is\nequivalent to the LKJ distribution, which is one way to specify a uniform\ndistribution from the space of positive definite correlation matrices. Based on\nthis theorem, we propose a new method to generate random correlation matrices\nfrom the LKJ distribution. This new method is faster than the original onion\nmethod for generating random matrices, especially in the low dimension\n($T<120$) situation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 02:34:01 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wang", "Zhenxun", ""], ["Wu", "Yunan", ""], ["Chu", "Haitao", ""]]}, {"id": "1809.04885", "submitter": "Anneke Weide", "authors": "Anneke Cleopatra Weide and Andr\\'e Beauducel", "title": "Varimax rotation based on gradient projection needs between 10 and more\n  than 500 random start loading matrices for optimal performance", "comments": "19 pages, 8 figures, 2 tables, 4 figures in the Supplement", "journal-ref": null, "doi": "10.3389/fpsyg.2019.00645", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient projection rotation (GPR) is a promising method to rotate factor or\ncomponent loadings by different criteria. Since the conditions for optimal\nperformance of GPR-Varimax are widely unknown, this simulation study\ninvestigates GPR towards the Varimax criterion in principal component analysis.\nThe conditions of the simulation study comprise two sample sizes (n = 100, n =\n300), with orthogonal simple structure population models based on four numbers\nof components (3, 6, 9, 12), with- and without Kaiser-normalization, and six\nnumbers of random start loading matrices for GPR-Varimax rotation (1, 10, 50,\n100, 500, 1,000). GPR-Varimax rotation always performed better when at least 10\nrandom matrices were used for start loadings instead of the identity matrix.\nGPR-Varimax worked better for a small number of components, larger (n = 300) as\ncompared to smaller (n = 100) samples, and when loadings were Kaiser-normalized\nbefore rotation. To ensure optimal (stationary) performance of GPR-Varimax in\nrecovering orthogonal simple structure, we recommend using at least 10\niterations of start loading matrices for the rotation of up to three components\nand 50 iterations for up to six components. For up to nine components, rotation\nshould be based on a sample size of at least 300 cases, Kaiser-normalization,\nand more than 50 different start loading matrices. For more than nine\ncomponents, GPR-Varimax rotation should be based on at least 300 cases,\nKaiser-normalization, and at least 500 different start loading matrices.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 10:59:10 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Weide", "Anneke Cleopatra", ""], ["Beauducel", "Andr\u00e9", ""]]}, {"id": "1809.05066", "submitter": "Jianfeng Lu", "authors": "Anton Martinsson, Jianfeng Lu, Benedict Leimkuhler and Eric\n  Vanden-Eijnden", "title": "Simulated Tempering Method in the Infinite Switch Limit with Adaptive\n  Weight Learning", "comments": null, "journal-ref": null, "doi": "10.1088/1742-5468/aaf323", "report-no": null, "categories": "physics.chem-ph math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the theoretical foundations of the simulated tempering method\nand use our findings to design efficient algorithms. Employing a large\ndeviation argument first used for replica exchange molecular dynamics [Plattner\net al., J. Chem. Phys. 135:134111 (2011)], we demonstrate that the most\nefficient approach to simulated tempering is to vary the temperature infinitely\nrapidly. In this limit, we can replace the equations of motion for the\ntemperature and physical variables by averaged equations for the latter alone,\nwith the forces rescaled according to a position-dependent function defined in\nterms of temperature weights. The averaged equations are similar to those used\nin Gao's integrated-over-temperature method, except that we show that it is\nbetter to use a continuous rather than a discrete set of temperatures. We give\na theoretical argument for the choice of the temperature weights as the\nreciprocal partition function, thereby relating simulated tempering to\nWang-Landau sampling. Finally, we describe a self-consistent algorithm for\nsimultaneously sampling the canonical ensemble and learning the weights during\nsimulation. This algorithm is tested on a system of harmonic oscillators as\nwell as a continuous variant of the Curie-Weiss model, where it is shown to\nperform well and to accurately capture the second-order phase transition\nobserved in this model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:22:31 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 14:00:28 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Martinsson", "Anton", ""], ["Lu", "Jianfeng", ""], ["Leimkuhler", "Benedict", ""], ["Vanden-Eijnden", "Eric", ""]]}, {"id": "1809.05212", "submitter": "Enes Makalic", "authors": "Enes Makalic and Daniel F. Schmidt", "title": "An efficient algorithm for sampling from $\\sin^k(x)$ for generating\n  random correlation matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we develop a novel algorithm for generating random numbers from\na distribution with a probability density function proportional to $\\sin^k(x)$,\n$x \\in (0,\\pi)$ and $k \\geq 1$. Our algorithm is highly efficient and is based\non rejection sampling where the envelope distribution is an appropriately\nchosen beta distribution. An example application illustrating how the new\nalgorithm can be used to generate random correlation matrices is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 00:27:58 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 23:58:20 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Makalic", "Enes", ""], ["Schmidt", "Daniel F.", ""]]}, {"id": "1809.05301", "submitter": "Markus Hainy", "authors": "Markus Hainy, David J. Price, Olivier Restif, Christopher Drovandi", "title": "Optimal Bayesian design for model discrimination via classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Performing optimal Bayesian design for discriminating between competing\nmodels is computationally intensive as it involves estimating posterior model\nprobabilities for thousands of simulated datasets. This issue is compounded\nfurther when the likelihood functions for the rival models are computationally\nexpensive. A new approach using supervised classification methods is developed\nto perform Bayesian optimal model discrimination design. This approach requires\nconsiderably fewer simulations from the candidate models than previous\napproaches using approximate Bayesian computation. Further, it is easy to\nassess the performance of the optimal design through the misclassification\nerror rate. The approach is particularly useful in the presence of models with\nintractable likelihoods but can also provide computational advantages when the\nlikelihoods are manageable.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 08:28:49 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 08:38:00 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Hainy", "Markus", ""], ["Price", "David J.", ""], ["Restif", "Olivier", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1809.05800", "submitter": "Ziwen An", "authors": "Ziwen An, David J. Nott and Christopher Drovandi", "title": "Robust Bayesian Synthetic Likelihood via a Semi-Parametric Approach", "comments": "37 pages Latex; the paper has been re-organised, moved section 4 and\n  5 to appendices, moved less important example figures to appendices, added\n  \"sensitivity to n\" section to appendices, added a shrinkage example to\n  appendices, typos and references corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian synthetic likelihood (BSL) is now a well established method for\nperforming approximate Bayesian parameter estimation for simulation-based\nmodels that do not possess a tractable likelihood function. BSL approximates an\nintractable likelihood function of a carefully chosen summary statistic at a\nparameter value with a multivariate normal distribution. The mean and\ncovariance matrix of this normal distribution are estimated from independent\nsimulations of the model. Due to the parametric assumption implicit in BSL, it\ncan be preferred to its non-parametric competitor, approximate Bayesian\ncomputation, in certain applications where a high-dimensional summary statistic\nis of interest. However, despite several successful applications of BSL, its\nwidespread use in scientific fields may be hindered by the strong normality\nassumption. In this paper, we develop a semi-parametric approach to relax this\nassumption to an extent and maintain the computational advantages of BSL\nwithout any additional tuning. We test our new method, semiBSL, on several\nchallenging examples involving simulated and real data and demonstrate that\nsemiBSL can be significantly more robust than BSL and another approach in the\nliterature.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 03:28:16 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 07:15:12 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["An", "Ziwen", ""], ["Nott", "David J.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1809.06052", "submitter": "Arabin Kumar Dey", "authors": "Biplab Paul, Arabin Kumar Dey, Arjun K Gupta and Debasis Kundu", "title": "Parameter Estimation of absolute continuous four parameter Geometric\n  Marshall-Olkin bivariate Pareto Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formulate a four parameter absolute continuous Geometric\nMarshall-Olkin bivariate Pareto distribution and study its parameter estimation\nthrough EM algorithm and also explore the bayesian analysis through slice cum\nGibbs sampler approach. Numerical results are shown to verify the performance\nof the algorithms. We illustrate the procedures through a real life data\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:42:21 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Paul", "Biplab", ""], ["Dey", "Arabin Kumar", ""], ["Gupta", "Arjun K", ""], ["Kundu", "Debasis", ""]]}, {"id": "1809.06405", "submitter": "Arabin Kumar Dey", "authors": "Biplab Paul, Arabin Kumar Dey and Sanku Dey", "title": "Bayesian analysis of absolute continuous Marshall-Olkin bivariate Pareto\n  distribution with location and scale parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides two different novel approaches of slice sampling to\nestimate the parameters of absolute continuous Marshall-Olkin bivariate Pareto\ndistribution with location and scale parameters. We carry out the bayesian\nanalysis taking gamma prior for shape and scale parameters and truncated normal\nfor location parameters. Credible intervals and coverage probabilities are also\nprovided for all methods. A real-life data analysis is shown for illustrative\npurpose.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 18:48:30 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Paul", "Biplab", ""], ["Dey", "Arabin Kumar", ""], ["Dey", "Sanku", ""]]}, {"id": "1809.06520", "submitter": "Kellie Ottoboni", "authors": "Kellie Ottoboni and Philip B. Stark", "title": "Random problems with R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  R (Version 3.5.1 patched) has an issue with its random sampling\nfunctionality. R generates random integers between $1$ and $m$ by multiplying\nrandom floats by $m$, taking the floor, and adding $1$ to the result.\nWell-known quantization effects in this approach result in a non-uniform\ndistribution on $\\{ 1, \\ldots, m\\}$. The difference, which depends on $m$, can\nbe substantial. Because the sample function in R relies on generating random\nintegers, random sampling in R is biased. There is an easy fix: construct\nrandom integers directly from random bits, rather than multiplying a random\nfloat by $m$. That is the strategy taken in Python's numpy.random.randint()\nfunction, among others. Example source code in Python is available at\nhttps://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py\n(see functions getrandbits() and randbelow_from_randbits()).\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 03:46:47 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 17:16:04 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 17:25:07 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Ottoboni", "Kellie", ""], ["Stark", "Philip B.", ""]]}, {"id": "1809.06594", "submitter": "Patrick J. Laub", "authors": "Thomas Taimre and Patrick J. Laub", "title": "Rare tail approximation using asymptotics and $L^1$ polar coordinates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a class of importance sampling (IS) estimators for\nestimating the right tail probability of a sum of continuous random variables\nbased on a change of variables to $L^1$ polar coordinates in which the radial\nand angular components of the IS distribution are considered separately.\n  When the asymptotic behaviour of the sum is known we exploit it for the\nradial change of measure, and the resulting estimator has the appealing form of\nthe (known) asymptotic multiplied by a random multiplicative correction factor.\nGiven we assume knowledge of the asymptotic behaviour of the sum in this\nframework, traditional notions of efficiency that appear in the rare-event\nliterature hold little practical meaning here. Instead, we focus on the\npractical behaviour of the proposed estimator in the pre-asymptotic regime for\nright tail probabilities between roughly $10^{-3}$ and $10^{-7}$.\n  The proposed estimator and procedure are applicable in both the heavy- and\nlight-tailed settings, as well as for independent and dependent summands. In\nthe case of independent summands, we find that our estimator compares\nfavourably with exponential tilting (iid light-tailed summands) and the\nAsmussen--Kroese method (independent subexponential summands).\n  However, for dependent subexponential summands using the same simple angular\ndistribution as for the independent case, the performance of our estimator\nrapidly degenerates with increasing dimension, suggesting an open avenue for\nfurther research.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 08:47:29 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Taimre", "Thomas", ""], ["Laub", "Patrick J.", ""]]}, {"id": "1809.06758", "submitter": "James Scott Mr", "authors": "James Scott, Axel Gandy", "title": "State-Dependent Kernel Selection for Conditional Sampling of Graphs", "comments": "Package implementing the samplers can be found at\n  https://github.com/jscott6/cgsampr", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces new efficient algorithms for two problems: sampling\nconditional on vertex degrees in unweighted graphs, and sampling conditional on\nvertex strengths in weighted graphs. The algorithms can sample conditional on\nthe presence or absence of an arbitrary number of edges. The resulting\nconditional distributions provide the basis for exact tests. Existing samplers\nbased on MCMC or sequential importance sampling are generally not scalable;\ntheir efficiency degrades in sparse graphs. MCMC methods usually require\nexplicit computation of a Markov basis to navigate the complex state space;\nthis is computationally intensive even for small graphs. We use state-dependent\nkernel selection to develop new MCMC samplers. These do not require a Markov\nbasis, and are efficient both in sparse and dense graphs. The key idea is to\nintelligently select a Markov kernel on the basis of the current state of the\nchain. We apply our methods to testing hypotheses on a real network and\ncontingency table. The algorithms appear orders of magnitude more efficient\nthan existing methods in the test cases considered.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 14:17:30 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Scott", "James", ""], ["Gandy", "Axel", ""]]}, {"id": "1809.06904", "submitter": "Amanda Muyskens", "authors": "Amanda Muyskens, Joseph Guinness, and Montserrat Fuentes", "title": "Non-Stationary Covariance Estimation using the Stochastic Score\n  Approximation for Large Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce computational methods that allow for effective estimation of a\nflexible, parametric non-stationary spatial model when the field size is too\nlarge to compute the multivariate normal likelihood directly. In this method,\nthe field is defined as a weighted spatially varying linear combination of a\nglobally stationary process and locally stationary processes. Often in such a\nmodel, the difficulty in its practical use is in the definition of the\nboundaries for the local processes, and therefore we describe one such\nselection procedure that generally captures complex non-stationary\nrelationships. We generalize the use of stochastic approximation to the score\nequations for data on a partial grid in this non-stationary case and provide\ntools for evaluating the approximate score in $O(n\\log n)$ operations and\n$O(n)$ storage. We perform various simulations to explore the effectiveness and\nspeed of the proposed methods and conclude by making inference on the\naccumulation behavior of arsenic applied to a sand grain.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 19:41:29 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Muyskens", "Amanda", ""], ["Guinness", "Joseph", ""], ["Fuentes", "Montserrat", ""]]}, {"id": "1809.07139", "submitter": "Andrew Golightly", "authors": "Andrew Golightly and Chris Sherlock", "title": "Efficient sampling of conditioned Markov jump processes", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of generating draws from a Markov jump process (MJP)\nbetween two time-points at which the process is known. Resulting draws are\ntypically termed bridges and the generation of such bridges plays a key role in\nsimulation-based inference algorithms for MJPs. The problem is challenging due\nto the intractability of the conditioned process, necessitating the use of\ncomputationally intensive methods such as weighted resampling or Markov chain\nMonte Carlo. An efficient implementation of such schemes requires an\napproximation of the intractable conditioned hazard/propensity function that is\nboth cheap and accurate. In this paper, we review some existing approaches to\nthis problem before outlining our novel contribution. Essentially, we leverage\nthe tractability of a Gaussian approximation of the MJP and suggest a\ncomputationally efficient implementation of the resulting conditioned hazard\napproximation. We compare and contrast our approach with existing methods using\nthree examples.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 12:14:45 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 17:39:16 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Golightly", "Andrew", ""], ["Sherlock", "Chris", ""]]}, {"id": "1809.07694", "submitter": "Xin Cao", "authors": "Xin Cao", "title": "Improved Online Wilson Score Interval Method for Community Answer\n  Quality Ranking", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a fast and easy-to-deploy method with a strong\ninterpretability for community answer quality ranking is proposed. This method\nis improved based on the Wilson score interval method [Wilson, 1927], which\nretains its advantages and simultaneously improve the degree of satisfaction\nwith the ranking of the high-quality answers. The improved answer quality score\nconsiders both Wilson score interval and the spotlight index, the latter of\nwhich will be introduced in the article. This method could significantly\nimprove the ranking of the best answers with high attention in diverse\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 21:54:13 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Cao", "Xin", ""]]}, {"id": "1809.07763", "submitter": "Alicja Gosiewska", "authors": "Alicja Gosiewska, Przemyslaw Biecek", "title": "auditor: an R Package for Model-Agnostic Visual Validation and\n  Diagnostics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have spread to almost every area of life. They are\nsuccessfully applied in biology, medicine, finance, physics, and other fields.\nWith modern software it is easy to train even a~complex model that fits the\ntraining data and results in high accuracy on the test set. The problem arises\nwhen models fail confronted with real-world data.\n  This paper describes methodology and tools for model-agnostic audit.\nIntroduced techniques facilitate assessing and comparing the goodness of fit\nand performance of models. In~addition, they may be used for the analysis of\nthe similarity of residuals and for identification of~outliers and influential\nobservations. The examination is carried out by diagnostic scores and visual\nverification.\n  Presented methods were implemented in the auditor package for R. Due to\nflexible and~consistent grammar, it is simple to validate models of any\nclasses.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 19:14:46 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 06:28:36 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 18:01:43 GMT"}, {"version": "v4", "created": "Tue, 26 May 2020 15:15:19 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Gosiewska", "Alicja", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1809.07905", "submitter": "Jared Huling", "authors": "Jared D. Huling, Menggang Yu", "title": "Subgroup Identification Using the personalized Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of disparate statistical methods have been proposed for subgroup\nidentification to help tailor treatment decisions for patients. However a\nmajority of them do not have corresponding R packages and the few that do\npertain to particular statistical methods or provide little means of evaluating\nwhether meaningful subgroups have been found. Recently, the work of Chen, Tian,\nCai, and Yu (2017) unified many of these subgroup identification methods into\none general, consistent framework. The goal of the personalized package is to\nprovide a corresponding unified software framework for subgroup identification\nanalyses that provides not only estimation of subgroups, but evaluation of\ntreatment effects within estimated subgroups. The personalized package allows\nfor a variety of subgroup identification methods for many types of outcomes\ncommonly encountered in medical settings. The package is built to incorporate\nthe entire subgroup identification analysis pipeline including propensity score\ndiagnostics, subgroup estimation, analysis of the treatment effects within\nsubgroups, and evaluation of identified subgroups. In this framework, different\nmethods can be accessed with little change in the analysis code. Similarly, new\nmethods can easily be incorporated into the package. Besides familiar\nstatistical models, the package also allows flexible machine learning tools to\nbe leveraged in subgroup identification. Further estimation improvements can be\nobtained via efficiency augmentation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 01:07:25 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 18:52:04 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Huling", "Jared D.", ""], ["Yu", "Menggang", ""]]}, {"id": "1809.08190", "submitter": "Marc H\\\"ark\\\"onen", "authors": "Marc H\\\"ark\\\"onen, Tomonari Sei, Yoshihiro Hirose", "title": "Holonomic extended least angle regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main problems studied in statistics is the fitting of models.\nIdeally, we would like to explain a large dataset with as few parameters as\npossible. There have been numerous attempts at automatizing this process. Most\nnotably, the Least Angle Regression algorithm, or LARS, is a computationally\nefficient algorithm that ranks the covariates of a linear model. The algorithm\nis further extended to a class of distributions in the generalized linear model\nby using properties of the manifold of exponential families as dually flat\nmanifolds. However this extension assumes that the normalizing constant of the\njoint distribution of observations is easy to compute. This is often not the\ncase, for example the normalizing constant may contain a complicated integral.\nWe circumvent this issue if the normalizing constant satisfies a holonomic\nsystem, a system of linear partial differential equations with a\nfinite-dimensional space of solutions. In this paper we present a modification\nof the holonomic gradient method and add it to the extended LARS algorithm. We\ncall this the holonomic extended least angle regression algorithm, or HELARS.\nThe algorithm was implemented using the statistical software R, and was tested\nwith real and simulated datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 16:15:31 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["H\u00e4rk\u00f6nen", "Marc", ""], ["Sei", "Tomonari", ""], ["Hirose", "Yoshihiro", ""]]}, {"id": "1809.09238", "submitter": "Putu Ayu Sudyanti", "authors": "Putu Ayu Sudyanti and Vinayak Rao", "title": "Flexible Mixture Modeling on Constrained Spaces", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses challenges in flexibly modeling multimodal data that lie\non constrained spaces. Such data are commonly found in spatial applications,\nsuch as climatology and criminology, where measurements are restricted to a\ngeographical area. Other settings include domains where unsuitable recordings\nare discarded, such as flow-cytometry measurements. A simple approach to\nmodeling such data is through the use of mixture models, especially\nnonparametric mixture models. Mixture models, while flexible and theoretically\nwell-understood, are unsuitable for settings involving complicated constraints,\nleading to difficulties in specifying the component distributions and in\nevaluating normalization constants. Bayesian inference over the parameters of\nthese models results in posterior distributions that are doubly-intractable. We\naddress this problem via an algorithm based on rejection sampling and data\naugmentation. We view samples from a truncated distribution as outcomes of a\nrejection sampling scheme, where proposals are made from a simple mixture model\nand are rejected if they violate the constraints. Our scheme proceeds by\nimputing the rejected samples given mixture parameters and then resampling\nparameters given all samples. We study two modeling approaches: mixtures of\ntruncated Gaussians and truncated mixtures of Gaussians, along with their\nassociated Markov chain Monte Carlo sampling algorithms. We also discuss\nvariations of the models, as well as approximations to improve mixing, reduce\ncomputational cost, and lower variance. We present results on simulated data\nand apply our algorithms to two problems; one involving flow-cytometry data,\nand the other, crime recorded in the city of Chicago.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 22:13:03 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 02:41:51 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sudyanti", "Putu Ayu", ""], ["Rao", "Vinayak", ""]]}, {"id": "1809.09367", "submitter": "Edgar Steiger", "authors": "Edgar Steiger, Martin Vingron", "title": "Sparse-Group Bayesian Feature Selection Using Expectation Propagation\n  for Signal Recovery and Network Reconstruction", "comments": "44 pages, 15 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian method for feature selection in the presence of\ngrouping information with sparsity on the between- and within group level.\nInstead of using a stochastic algorithm for parameter inference, we employ\nexpectation propagation, which is a deterministic and fast algorithm. Available\nmethods for feature selection in the presence of grouping information have a\nnumber of short-comings: on one hand, lasso methods, while being fast,\nunderestimate the regression coefficients and do not make good use of the\ngrouping information, and on the other hand, Bayesian approaches, while\naccurate in parameter estimation, often rely on the stochastic and slow Gibbs\nsampling procedure to recover the parameters, rendering them infeasible e.g.\nfor gene network reconstruction. Our approach of a Bayesian sparse-group\nframework with expectation propagation enables us to not only recover accurate\nparameter estimates in signal recovery problems, but also makes it possible to\napply this Bayesian framework to large-scale network reconstruction problems.\nThe presented method is generic but in terms of application we focus on gene\nregulatory networks. We show on simulated and experimental data that the method\nconstitutes a good choice for network reconstruction regarding the number of\ncorrectly selected features, prediction on new data and reasonable computing\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 09:08:26 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Steiger", "Edgar", ""], ["Vingron", "Martin", ""]]}, {"id": "1809.09445", "submitter": "Yousra El-Bachir", "authors": "Yousra El-Bachir and Anthony C. Davison", "title": "Fast Automatic Smoothing for Generalized Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple generalized additive models (GAMs) are a type of distributional\nregression wherein parameters of probability distributions depend on predictors\nthrough smooth functions, with selection of the degree of smoothness via $L_2$\nregularization. Multiple GAMs allow finer statistical inference by\nincorporating explanatory information in any or all of the parameters of the\ndistribution. Owing to their nonlinearity, flexibility and interpretability,\nGAMs are widely used, but reliable and fast methods for automatic smoothing in\nlarge datasets are still lacking, despite recent advances. We develop a general\nmethodology for automatically learning the optimal degree of $L_2$\nregularization for multiple GAMs using an empirical Bayes approach. The smooth\nfunctions are penalized by different amounts, which are learned simultaneously\nby maximization of a marginal likelihood through an approximate\nexpectation-maximization algorithm that involves a double Laplace approximation\nat the E-step, and leads to an efficient M-step. Empirical analysis shows that\nthe resulting algorithm is numerically stable, faster than all existing methods\nand achieves state-of-the-art accuracy. For illustration, we apply it to an\nimportant and challenging problem in the analysis of extremal data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 12:59:01 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["El-Bachir", "Yousra", ""], ["Davison", "Anthony C.", ""]]}, {"id": "1809.09505", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Trevor Campbell, Miko{\\l}aj Kasprzak, Tamara\n  Broderick", "title": "Practical bounds on the error of Bayesian posterior approximations: A\n  nonasymptotic approach", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference typically requires the computation of an approximation to\nthe posterior distribution. An important requirement for an approximate\nBayesian inference algorithm is to output high-accuracy posterior mean and\nuncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain\nMonte Carlo, remain the gold standard for approximate Bayesian inference\nbecause they have a robust finite-sample theory and reliable convergence\ndiagnostics. However, alternative methods, which are more scalable or apply to\nproblems where Markov Chain Monte Carlo cannot be used, lack the same\nfinite-data approximation theory and tools for evaluating their accuracy. In\nthis work, we develop a flexible new approach to bounding the error of mean and\nuncertainty estimates of scalable inference algorithms. Our strategy is to\ncontrol the estimation errors in terms of Wasserstein distance, then bound the\nWasserstein distance via a generalized notion of Fisher distance. Unlike\ncomputing the Wasserstein distance, which requires access to the normalized\nposterior distribution, the Fisher distance is tractable to compute because it\nrequires access only to the gradient of the log posterior density. We\ndemonstrate the usefulness of our Fisher distance approach by deriving bounds\non the Wasserstein error of the Laplace approximation and Hilbert coresets. We\nanticipate that our approach will be applicable to many other approximate\ninference methods such as the integrated Laplace approximation, variational\ninference, and approximate Bayesian computation\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 14:11:32 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 21:33:00 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Campbell", "Trevor", ""], ["Kasprzak", "Miko\u0142aj", ""], ["Broderick", "Tamara", ""]]}, {"id": "1809.09546", "submitter": "Mahdi Teimouri Yanesari", "authors": "Mahdi Teimouri, Mahdi Torshizi, Adel Mohammadpour, and Saralees\n  Nadarajah", "title": "alphastable: An R Package for Modelling Multivariate Stable and Mixture\n  of Symmetric Stable Distributions", "comments": "35 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The family of stable distributions received extensive applications in many\nfields of studies since it incorporates both the skewness and heavy tails. In\nthis paper, we introduce a package written in the R language called\nalphastable. The alphastable performs a variety of tasks including: 1-\ngenerating random numbers from univariate, truncated, and multivariate stable\ndistributions. 2- computing the probability density function of univariate and\nmultivariate elliptically contoured stable distributions, 3- computing the\ndistribution function of univariate stable distributions, 4- estimating the\nparameters of univariate symmetric stable, univariate Cauchy, mixture of\nCauchy, mixture of univariate symmetric stable, multivariate elliptically\ncontoured stable, and multivariate strictly stable distributions. This package,\nas it will be shown, is very useful for modelling data in univariate and\nmultivariate cases that arise in the fields of finance and economics.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 15:29:00 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Teimouri", "Mahdi", ""], ["Torshizi", "Mahdi", ""], ["Mohammadpour", "Adel", ""], ["Nadarajah", "Saralees", ""]]}, {"id": "1809.09547", "submitter": "Felipe Medina-Aguayo", "authors": "Felipe Medina-Aguayo, Daniel Rudolf and Nikolaus Schweizer", "title": "Perturbation Bounds for Monte Carlo within Metropolis via Restricted\n  Approximations", "comments": null, "journal-ref": null, "doi": "10.1016/j.spa.2019.06.015", "report-no": null, "categories": "stat.CO cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Monte Carlo within Metropolis (MCwM) algorithm, interpreted as a\nperturbed Metropolis-Hastings (MH) algorithm, provides an approach for\napproximate sampling when the target distribution is intractable. Assuming the\nunperturbed Markov chain is geometrically ergodic, we show explicit estimates\nof the difference between the n-th step distributions of the perturbed MCwM and\nthe unperturbed MH chains. These bounds are based on novel perturbation results\nfor Markov chains which are of interest beyond the MCwM setting. To apply the\nbounds, we need to control the difference between the transition probabilities\nof the two chains and to verify stability of the perturbed chain.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 15:29:02 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 15:42:30 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Medina-Aguayo", "Felipe", ""], ["Rudolf", "Daniel", ""], ["Schweizer", "Nikolaus", ""]]}, {"id": "1809.09803", "submitter": "Rathinavel Jagadeeswaran", "authors": "R. Jagadeeswaran and Fred J. Hickernell", "title": "Fast Automatic Bayesian Cubature Using Lattice Sampling", "comments": "Revised manuscript", "journal-ref": "Statistics and Computing, 29(6), 1215-1229, 2019", "doi": "10.1007/s11222-019-09895-9", "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic cubatures approximate multidimensional integrals to user-specified\nerror tolerances. For high dimensional problems, it makes sense to fix the\nsampling density but determine the sample size, $n$, automatically. Bayesian\ncubature postulates that the integrand is an instance of a stochastic process.\nHere we assume a Gaussian process parameterized by a constant mean and a\ncovariance function defined by a scale parameter times a parameterized function\nspecifying how the integrand values at two different points in the domain are\nrelated. These parameters are estimated from integrand values or are given\nnon-informative priors. The sample size, $n$, is chosen to make the half-width\nof the credible interval for the Bayesian posterior mean no greater than the\nerror tolerance.\n  The process just outlined typically requires vector-matrix operations with a\ncomputational cost of $O(n^3)$. Our innovation is to pair low discrepancy nodes\nwith matching kernels that lower the computational cost to $O(n \\log n)$. This\napproach is demonstrated using rank-1 lattice sequences and shift-invariant\nkernels. Our algorithm is implemented in the Guaranteed Automatic Integration\nLibrary (GAIL).\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 04:38:34 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 04:30:16 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jagadeeswaran", "R.", ""], ["Hickernell", "Fred J.", ""]]}, {"id": "1809.09928", "submitter": "Yasuhiro Omori", "authors": "Yuta Yamauchi and Yasuhiro Omori", "title": "Multivariate Stochastic Volatility Model with Realized Volatilities and\n  Pairwise Realized Correlations", "comments": null, "journal-ref": "Journal of Business and Economic Statistics 38-4 (2020) 839-855", "doi": "10.1080/07350015.2019.1602048", "report-no": null, "categories": "econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although stochastic volatility and GARCH (generalized autoregressive\nconditional heteroscedasticity) models have successfully described the\nvolatility dynamics of univariate asset returns, extending them to the\nmultivariate models with dynamic correlations has been difficult due to several\nmajor problems. First, there are too many parameters to estimate if available\ndata are only daily returns, which results in unstable estimates. One solution\nto this problem is to incorporate additional observations based on intraday\nasset returns, such as realized covariances. Second, since multivariate asset\nreturns are not synchronously traded, we have to use the largest time intervals\nsuch that all asset returns are observed in order to compute the realized\ncovariance matrices. However, in this study, we fail to make full use of the\navailable intraday informations when there are less frequently traded assets.\nThird, it is not straightforward to guarantee that the estimated (and the\nrealized) covariance matrices are positive definite. Our contributions are the\nfollowing: (1) we obtain the stable parameter estimates for the dynamic\ncorrelation models using the realized measures, (2) we make full use of\nintraday informations by using pairwise realized correlations, (3) the\ncovariance matrices are guaranteed to be positive definite, (4) we avoid the\narbitrariness of the ordering of asset returns, (5) we propose the flexible\ncorrelation structure model (e.g., such as setting some correlations to be zero\nif necessary), and (6) the parsimonious specification for the leverage effect\nis proposed. Our proposed models are applied to the daily returns of nine U.S.\nstocks with their realized volatilities and pairwise realized correlations and\nare shown to outperform the existing models with respect to portfolio\nperformances.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 12:03:32 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 06:14:43 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Yamauchi", "Yuta", ""], ["Omori", "Yasuhiro", ""]]}, {"id": "1809.10227", "submitter": "Toni Karvonen", "authors": "Toni Karvonen, Simo S\\\"arkk\\\"a, Chris. J. Oates", "title": "Symmetry Exploits for Bayesian Cubature Methods", "comments": "Accepted for publication in Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian cubature provides a flexible framework for numerical integration, in\nwhich a priori knowledge on the integrand can be encoded and exploited. This\nadditional flexibility, compared to many classical cubature methods, comes at a\ncomputational cost which is cubic in the number of evaluations of the\nintegrand. It has been recently observed that fully symmetric point sets can be\nexploited in order to reduce - in some cases substantially - the computational\ncost of the standard Bayesian cubature method. This work identifies several\nadditional symmetry exploits within the Bayesian cubature framework. In\nparticular, we go beyond earlier work in considering non-symmetric measures\nand, in addition to the standard Bayesian cubature method, present exploits for\nthe Bayes-Sard cubature method and the multi-output Bayesian cubature method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 20:47:40 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 21:27:28 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Karvonen", "Toni", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Oates", "Chris. J.", ""]]}, {"id": "1809.10330", "submitter": "Matias Quiroz", "authors": "Ming Xu, Matias Quiroz, Robert Kohn, Scott A. Sisson", "title": "Variance reduction properties of the reparameterization trick", "comments": "Accepted for publication by AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reparameterization trick is widely used in variational inference as it\nyields more accurate estimates of the gradient of the variational objective\nthan alternative approaches such as the score function method. Although there\nis overwhelming empirical evidence in the literature showing its success, there\nis relatively little research exploring why the reparameterization trick is so\neffective. We explore this under the idealized assumptions that the variational\napproximation is a mean-field Gaussian density and that the log of the joint\ndensity of the model parameters and the data is a quadratic function that\ndepends on the variational mean. From this, we show that the marginal variances\nof the reparameterization gradient estimator are smaller than those of the\nscore function gradient estimator. We apply the result of our idealized\nanalysis to real-world examples.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 03:33:20 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 04:35:12 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 05:20:23 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Xu", "Ming", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1809.10728", "submitter": "John Hughes", "authors": "John Hughes", "title": "sklarsomega: An R Package for Measuring Agreement Using Sklar's Omega\n  Coefficient", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.02734", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  R package sklarsomega provides tools for measuring agreement using Sklar's\nomega coefficient, which subsumes Krippendorff's alpha coefficient, which in\nturn subsumes a number of other well-known agreement coefficients. The package\npermits users to apply the omega methodology to nominal, ordinal, interval, or\nratio scores; can accommodate any number of units, any number of coders, and\nmissingness; and can measure intra-coder agreement, inter-coder agreement, and\nagreement relative to a gold standard. Classical inference is available for all\nlevels of measurement while Bayesian inference is available for interval data\nand ratio data only.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 20:36:40 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Hughes", "John", ""]]}, {"id": "1809.11064", "submitter": "Eufrasio Lima Neto Mr.", "authors": "Eufr\\'asio de A. Lima Neto, Alu\\'isio de S. Pinheiro and Adenice G. O.\n  Ferreira", "title": "On wavelets to select the parametric form of a regression model", "comments": null, "journal-ref": "Communications in Statistics - Simulation and Computation, 2019", "doi": "10.1080/03610918.2019.1610441", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let Y be a response variable related with a set of explanatory variables and\nlet f1, f2, ..., fk be a set of the parametric forms representing a set of\ncandidate's model. Let f* be the true model among the set of k plausible\nmodels. We discuss in this paper the use of wavelet regression method as\nauxiliary for the choice of the \"true\" parametric form of a regression model,\nparticularly, for the cases of nonlinear regression and generalized linear\nmodels. The use of a non-parametric method for the choice of the more\nappropriate parametric equation in regression problems would be interesting in\npractice due to the simplicity and because the probabilistic assumptions are\nnot required. We evaluate the performance of the proposed wavelet procedure\nbased on the true classification rate of the correct parametric form among a\nrange of k candidate models, taking into account a wide ranges of scenarios and\nconfigurations as well as in real data set applications.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:39:39 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Neto", "Eufr\u00e1sio de A. Lima", ""], ["Pinheiro", "Alu\u00edsio de S.", ""], ["Ferreira", "Adenice G. O.", ""]]}]