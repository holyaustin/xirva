[{"id": "1011.0057", "submitter": "Julien Cornebise", "authors": "Luke Bornn, Julien Cornebise, Gareth W. Peters", "title": "Discussion of \"Riemann manifold Langevin and Hamiltonian Monte Carlo\n  methods'' by M. Girolami and B. Calderhead", "comments": "To appear in Journal of the Royal Statitistical Society Series B, in\n  the discussion of the read paper by Calderhead and Girolami 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report is the union of two contributions to the discussion of\nthe Read Paper \"Riemann manifold Langevin and Hamiltonian Monte Carlo methods\"\nby B. Calderhead and M. Girolami, presented in front of the Royal Statistical\nSociety on October 13th 2010 and to appear in the Journal of the Royal\nStatistical Society Series B. The first comment establishes a parallel and\npossible interactions with Adaptive Monte Carlo methods. The second comment\nexposes a detailed study of Riemannian Manifold Hamiltonian Monte Carlo (RMHMC)\nfor a weakly identifiable model presenting a strong ridge in its geometry.\n", "versions": [{"version": "v1", "created": "Sat, 30 Oct 2010 08:38:15 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Bornn", "Luke", ""], ["Cornebise", "Julien", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1011.0175", "submitter": "Madeleine Thompson", "authors": "Madeleine B. Thompson", "title": "A Comparison of Methods for Computing Autocorrelation Time", "comments": null, "journal-ref": null, "doi": null, "report-no": "University of Toronto Statistics Department Technical Report 1007", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes four methods for estimating autocorrelation time and\nevaluates these methods with a test set of seven series. Fitting an\nautoregressive process appears to be the most accurate method of the four. An R\npackage is provided for extending the comparison to more methods and test\nseries.\n", "versions": [{"version": "v1", "created": "Sun, 31 Oct 2010 16:16:30 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Thompson", "Madeleine B.", ""]]}, {"id": "1011.0506", "submitter": "Suren Rathnayake", "authors": "Vladimir Nikulin, Tian-Hsiang Huang, Shu-Kay Ng, Suren I Rathnayake,\n  and Geoffrey J McLachlan", "title": "A Very Fast Algorithm for Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IR physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a very fast algorithm for general matrix factorization of a data\nmatrix for use in the statistical analysis of high-dimensional data via latent\nfactors. Such data are prevalent across many application areas and generate an\never-increasing demand for methods of dimension reduction in order to undertake\nthe statistical analysis of interest. Our algorithm uses a gradient-based\napproach which can be used with an arbitrary loss function provided the latter\nis differentiable. The speed and effectiveness of our algorithm for dimension\nreduction is demonstrated in the context of supervised classification of some\nreal high-dimensional data sets from the bioinformatics literature.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 03:36:52 GMT"}], "update_date": "2010-11-03", "authors_parsed": [["Nikulin", "Vladimir", ""], ["Huang", "Tian-Hsiang", ""], ["Ng", "Shu-Kay", ""], ["Rathnayake", "Suren I", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1011.0834", "submitter": "Christian P. Robert", "authors": "Simon Barthelme, Magali Beffy, Nicolas Chopin, Arnaud Doucet, Pierre\n  Jacob, Adam M. Johansen, Jean-Michel Marin, and Christian P. Robert", "title": "Discussions on \"Riemann manifold Langevin and Hamiltonian Monte Carlo\n  methods\"", "comments": "6 pages, one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a collection of discussions of `Riemann manifold Langevin and\nHamiltonian Monte Carlo methods\" by Girolami and Calderhead, to appear in the\nJournal of the Royal Statistical Society, Series B.\n", "versions": [{"version": "v1", "created": "Wed, 3 Nov 2010 09:56:21 GMT"}], "update_date": "2010-11-04", "authors_parsed": [["Barthelme", "Simon", ""], ["Beffy", "Magali", ""], ["Chopin", "Nicolas", ""], ["Doucet", "Arnaud", ""], ["Jacob", "Pierre", ""], ["Johansen", "Adam M.", ""], ["Marin", "Jean-Michel", ""], ["Robert", "Christian P.", ""]]}, {"id": "1011.1170", "submitter": "Radu Craiu Dr", "authors": "Roberto Casarin, Radu V. Craiu and Fabrizio Leisen", "title": "Interacting Multiple Try Algorithms with Different Proposal\n  Distributions", "comments": null, "journal-ref": "Statistics and Computing, 23, No. 2, 185--200, 2013", "doi": "10.1007/s11222-011-9301-9", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of interacting Markov chain Monte Carlo (MCMC)\nalgorithms designed for increasing the efficiency of a modified multiple-try\nMetropolis (MTM) algorithm. The extension with respect to the existing MCMC\nliterature is twofold. The sampler proposed extends the basic MTM algorithm by\nallowing different proposal distributions in the multiple-try generation step.\nWe exploit the structure of the MTM algorithm with different proposal\ndistributions to naturally introduce an interacting MTM mechanism (IMTM) that\nexpands the class of population Monte Carlo methods. We show the validity of\nthe algorithm and discuss the choice of the selection weights and of the\ndifferent proposals. We provide numerical studies which show that the new\nalgorithm can perform better than the basic MTM algorithm and that the\ninteraction mechanism allows the IMTM to efficiently explore the state space.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 14:26:11 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Casarin", "Roberto", ""], ["Craiu", "Radu V.", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1011.1745", "submitter": "Olivier Cappe", "authors": "Olivier Capp\\'e (LTCI)", "title": "Online Expectation-Maximisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tutorial chapter on the Online EM algorithm to appear in the volume\n'Mixtures' edited by Kerrie Mengersen, Mike Titterington and Christian P.\nRobert.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 10:01:57 GMT"}], "update_date": "2010-11-09", "authors_parsed": [["Capp\u00e9", "Olivier", "", "LTCI"]]}, {"id": "1011.1761", "submitter": "Francois Caron", "authors": "Francois Caron (INRIA Bordeaux - Sud-Ouest, IMB), Arnaud Doucet", "title": "Efficient Bayesian Inference for Generalized Bradley-Terry Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bradley-Terry model is a popular approach to describe probabilities of\nthe possible outcomes when elements of a set are repeatedly compared with one\nanother in pairs. It has found many applications including animal behaviour,\nchess ranking and multiclass classification. Numerous extensions of the basic\nmodel have also been proposed in the literature including models with ties,\nmultiple comparisons, group comparisons and random graphs. From a computational\npoint of view, Hunter (2004) has proposed efficient iterative MM\n(minorization-maximization) algorithms to perform maximum likelihood estimation\nfor these generalized Bradley-Terry models whereas Bayesian inference is\ntypically performed using MCMC (Markov chain Monte Carlo) algorithms based on\ntailored Metropolis-Hastings (M-H) proposals. We show here that these MM\\\nalgorithms can be reinterpreted as special instances of\nExpectation-Maximization (EM) algorithms associated to suitable sets of latent\nvariables and propose some original extensions. These latent variables allow us\nto derive simple Gibbs samplers for Bayesian inference. We demonstrate\nexperimentally the efficiency of these algorithms on a variety of applications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 10:40:19 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Caron", "Francois", "", "INRIA Bordeaux - Sud-Ouest, IMB"], ["Doucet", "Arnaud", ""]]}, {"id": "1011.2153", "submitter": "Jimmy Olsson Dr", "authors": "Jimmy Olsson and Tobias Ryd\\'en", "title": "Metropolising forward particle filtering backward sampling and\n  Rao-Blackwellisation of Metropolised particle smoothers", "comments": "This work has been submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothing in state-space models amounts to computing the conditional\ndistribution of the latent state trajectory, given observations, or\nexpectations of functionals of the state trajectory with respect to this\ndistributions. For models that are not linear Gaussian or possess finite state\nspace, smoothing distributions are in general infeasible to compute as they\ninvolve intergrals over a space of dimensionality at least equal to the number\nof observations. Recent years have seen an increased interest in Monte\nCarlo-based methods for smoothing, often involving particle filters. One such\nmethod is to approximate filter distributions with a particle filter, and then\nto simulate backwards on the trellis of particles using a backward kernel. We\nshow that by supplementing this procedure with a Metropolis-Hastings step\ndeciding whether to accept a proposed trajectory or not, one obtains a Markov\nchain Monte Carlo scheme whose stationary distribution is the exact smoothing\ndistribution. We also show that in this procedure, backward sampling can be\nreplaced by backward smoothing, which effectively means averaging over all\npossible trajectories. In an example we compare these approaches to a similar\none recently proposed by Andrieu, Doucet and Holenstein, and show that the new\nmethods can be more efficient in terms of precision (inverse variance) per\ncomputation time.\n", "versions": [{"version": "v1", "created": "Tue, 9 Nov 2010 17:20:44 GMT"}], "update_date": "2010-11-10", "authors_parsed": [["Olsson", "Jimmy", ""], ["Ryd\u00e9n", "Tobias", ""]]}, {"id": "1011.2416", "submitter": "Phaedon-Stelios Koutsourelakis", "authors": "I. Bilionis, P.S. Koutsourelakis", "title": "Free energy computations by minimization of Kullback-Leibler divergence:\n  an efficient adaptive biasing potential method for sparse representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP physics.comp-ph stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The present paper proposes an adaptive biasing potential for the computation\nof free energy landscapes. It is motivated by statistical learning arguments\nand unifies the tasks of biasing the molecular dynamics to escape free energy\nwells and estimating the free energy function, under the same objective. It\noffers rigorous convergence diagnostics even though history dependent,\nnon-Markovian dynamics are employed. It makes use of a greedy optimization\nscheme in order to obtain sparse representations of the free energy function\nwhich can be particularly useful in multidimensional cases. It employs\nembarrassingly parallelizable sampling schemes that are based on adaptive\nSequential Monte Carlo and can be readily coupled with legacy molecular\ndynamics simulators. The sequential nature of the learning and sampling scheme\nenables the efficient calculation of free energy functions parametrized by the\ntemperature. The characteristics and capabilities of the proposed method are\ndemonstrated in three numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 10 Nov 2010 16:08:02 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Bilionis", "I.", ""], ["Koutsourelakis", "P. S.", ""]]}, {"id": "1011.2437", "submitter": "Nick Whiteley Dr", "authors": "Nick Whiteley, Christophe Andrieu and Arnaud Doucet", "title": "Efficient Bayesian Inference for Switching State-Space Models using\n  Discrete Particle Markov Chain Monte Carlo Methods", "comments": "Bristol University Statistics Research Report 10:04. See:\n  http://www.maths.bris.ac.uk/research/stats/reports/2010/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Switching state-space models (SSSM) are a very popular class of time series\nmodels that have found many applications in statistics, econometrics and\nadvanced signal processing. Bayesian inference for these models typically\nrelies on Markov chain Monte Carlo (MCMC) techniques. However, even\nsophisticated MCMC methods dedicated to SSSM can prove quite inefficient as\nthey update potentially strongly correlated discrete-valued latent variables\none-at-a-time (Carter and Kohn, 1996; Gerlach et al., 2000; Giordani and Kohn,\n2008). Particle Markov chain Monte Carlo (PMCMC) methods are a recently\ndeveloped class of MCMC algorithms which use particle filters to build\nefficient proposal distributions in high-dimensions (Andrieu et al., 2010). The\nexisting PMCMC methods of Andrieu et al. (2010) are applicable to SSSM, but are\nrestricted to employing standard particle filtering techniques. Yet, in the\ncontext of discrete-valued latent variables, specialised particle techniques\nhave been developed which can outperform by up to an order of magnitude\nstandard methods (Fearnhead, 1998; Fearnhead and Clifford, 2003; Fearnhead,\n2004). In this paper we develop a novel class of PMCMC methods relying on these\nvery efficient particle algorithms. We establish the theoretical validy of this\nnew generic methodology referred to as discrete PMCMC and demonstrate it on a\nvariety of examples including a multiple change-points model for well-log data\nand a model for U.S./U.K. exchange rate data. Discrete PMCMC algorithms are\nshown to outperform experimentally state-of-the-art MCMC techniques for a fixed\ncomputational complexity. Additionally they can be easily parallelized (Lee et\nal., 2010) which allows further substantial gains.\n", "versions": [{"version": "v1", "created": "Wed, 10 Nov 2010 17:24:31 GMT"}], "update_date": "2010-11-11", "authors_parsed": [["Whiteley", "Nick", ""], ["Andrieu", "Christophe", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1011.2624", "submitter": "Marcela Svarc", "authors": "Ricardo Fraiman, Badih Ghattas and Marcela Svarc", "title": "Clustering using Unsupervised Binary Trees: CUBT", "comments": "This paper has been withdrawn by the author due to an involuntary\n  double submission to the arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We herein introduce a new method of interpretable clustering that uses\nunsupervised binary trees. It is a three-stage procedure, the first stage of\nwhich entails a series of recursive binary splits to reduce the heterogeneity\nof the data within the new subsamples. During the second stage (pruning),\nconsideration is given to whether adjacent nodes can be aggregated. Finally,\nduring the third stage (joining), similar clusters are joined together, even if\nthey do not share the same parent originally. Consistency results are obtained,\nand the procedure is used on simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Nov 2010 12:12:56 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2011 14:00:46 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Fraiman", "Ricardo", ""], ["Ghattas", "Badih", ""], ["Svarc", "Marcela", ""]]}, {"id": "1011.2932", "submitter": "Jason Wyse", "authors": "Jason Wyse and Nial Friel", "title": "Simulation-based Bayesian analysis for multiple changepoints", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Markov chain Monte Carlo method to generate approximate\nposterior samples in retrospective multiple changepoint problems where the\nnumber of changes is not known in advance. The method uses conjugate models\nwhereby the marginal likelihood for the data between consecutive changepoints\nis tractable. Inclusion of hyperpriors gives a near automatic algorithm\nproviding a robust alternative to popular filtering recursions approaches in\ncases which may be sensitive to prior information. Three real examples are used\nto demonstrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Nov 2010 14:58:32 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Wyse", "Jason", ""], ["Friel", "Nial", ""]]}, {"id": "1011.2948", "submitter": "Jason Wyse", "authors": "Jason Wyse and Nial Friel", "title": "Block clustering with collapsed latent block models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian extension of the latent block model for model-based\nblock clustering of data matrices. Our approach considers a block model where\nblock parameters may be integrated out. The result is a posterior defined over\nthe number of clusters in rows and columns and cluster memberships. The number\nof row and column clusters need not be known in advance as these are sampled\nalong with cluster memberhips using Markov chain Monte Carlo. This differs from\nexisting work on latent block models, where the number of clusters is assumed\nknown or is chosen using some information criteria. We analyze both simulated\nand real data to validate the technique.\n", "versions": [{"version": "v1", "created": "Fri, 12 Nov 2010 15:29:40 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Wyse", "Jason", ""], ["Friel", "Nial", ""]]}, {"id": "1011.4381", "submitter": "Matti Vihola", "authors": "Matti Vihola", "title": "Robust adaptive Metropolis algorithm with coerced acceptance rate", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive Metropolis (AM) algorithm of Haario, Saksman and Tamminen\n[Bernoulli 7 (2001) 223-242] uses the estimated covariance of the target\ndistribution in the proposal distribution. This paper introduces a new robust\nadaptive Metropolis algorithm estimating the shape of the target distribution\nand simultaneously coercing the acceptance rate. The adaptation rule is\ncomputationally simple adding no extra cost compared with the AM algorithm. The\nadaptation strategy can be seen as a multidimensional extension of the\npreviously proposed method adapting the scale of the proposal distribution in\norder to attain a given acceptance rate. The empirical results show promising\nbehaviour of the new algorithm in an example with Student target distribution\nhaving no finite second moment, where the AM covariance estimate is unstable.\nIn the examples with finite second moments, the performance of the new approach\nseems to be competitive with the AM algorithm combined with scale adaptation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Nov 2010 10:23:30 GMT"}, {"version": "v2", "created": "Fri, 27 May 2011 13:03:51 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Vihola", "Matti", ""]]}, {"id": "1011.4457", "submitter": "Madeleine Thompson", "authors": "Madeleine B. Thompson", "title": "Graphical Comparison of MCMC Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": "University of Toronto Statistics Department Technical Report 1010", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a graphical method for comparing performance of Markov\nChain Monte Carlo methods. Most researchers present comparisons of MCMC methods\nusing tables of figures of merit; this paper presents a graphical alternative.\nIt first discusses the computation of autocorrelation time, then uses this to\nconstruct a figure of merit, log density function evaluations per independent\nobservation. Then, it demonstrates how one can plot this figure of merit\nagainst a tuning parameter in a grid of plots where columns represent sampling\nmethods and rows represent distributions. This type of visualization makes it\npossible to convey a greater depth of information without overwhelming the user\nwith numbers, allowing researchers to put their contributions into a broader\ncontext than is possible with a textual presentation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Nov 2010 16:11:27 GMT"}], "update_date": "2010-11-22", "authors_parsed": [["Thompson", "Madeleine B.", ""]]}, {"id": "1011.4604", "submitter": "Ting Kei Pong", "authors": "Zhaosong Lu, Ting Kei Pong and Yong Zhang", "title": "An Alternating Direction Method for Finding Dantzig Selectors", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the alternating direction method for finding the\nDantzig selectors, which are first introduced in [8]. In particular, at each\niteration we apply the nonmonotone gradient method proposed in [17] to\napproximately solve one subproblem of this method. We compare our approach with\na first-order method proposed in [3]. The computational results show that our\napproach usually outperforms that method in terms of CPU time while producing\nsolutions of comparable quality.\n", "versions": [{"version": "v1", "created": "Sat, 20 Nov 2010 19:49:58 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["Lu", "Zhaosong", ""], ["Pong", "Ting Kei", ""], ["Zhang", "Yong", ""]]}, {"id": "1011.4722", "submitter": "Madeleine Thompson", "authors": "Madeleine B. Thompson and Radford M. Neal", "title": "Slice Sampling with Adaptive Multivariate Steps: The Shrinking-Rank\n  Method", "comments": null, "journal-ref": "Proceedings of the 2010 Joint Statistical Meetings, Section on\n  Statistical Computing, pages 3890-3896", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shrinking rank method is a variation of slice sampling that is efficient\nat sampling from multivariate distributions with highly correlated parameters.\nIt requires that the gradient of the log-density be computable. At each\nindividual step, it approximates the current slice with a Gaussian occupying a\nshrinking-dimension subspace. The dimension of the approximation is shrunk\northogonally to the gradient at rejected proposals, since the gradients at\npoints outside the current slice tend to point towards the slice. This causes\nthe proposal distribution to converge rapidly to an estimate of the longest\naxis of the slice, resulting in states that are less correlated than those\ngenerated by related methods. After describing the method, we compare it to two\nother methods on several distributions and obtain favorable results.\n", "versions": [{"version": "v1", "created": "Mon, 22 Nov 2010 03:34:35 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["Thompson", "Madeleine B.", ""], ["Neal", "Radford M.", ""]]}, {"id": "1011.5038", "submitter": "Jason Wyse", "authors": "Jason Wyse, Nial Friel, H{\\aa}vard Rue", "title": "Approximate simulation-free Bayesian inference for multiple changepoint\n  models with dependence within segments", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes approaches for the analysis of multiple changepoint\nmodels when dependency in the data is modelled through a hierarchical Gaussian\nMarkov random field. Integrated nested Laplace approximations are used to\napproximate data quantities, and an approximate filtering recursions approach\nis proposed for savings in compuational cost when detecting changepoints. All\nof these methods are simulation free. Analysis of real data demonstrates the\nusefulness of the approach in general. The new models which allow for data\ndependence are compared with conventional models where data within segments is\nassumed independent.\n", "versions": [{"version": "v1", "created": "Tue, 23 Nov 2010 09:32:20 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2011 12:47:57 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Wyse", "Jason", ""], ["Friel", "Nial", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1011.5631", "submitter": "Valderio Reisen VAR", "authors": "Valderio A. Reisen, Wilfredo Palma, Josu Arteche, Bartolomeu Zamprogno", "title": "Seasonal fractional long-memory processes. A semiparametric estimation\n  approach", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores seasonal and long-memory time series properties by using\nthe seasonal fractional ARIMA model when the seasonal data has one and two\nseasonal periods and short-memory counterparts. The stationarity and\ninvertibility parameter conditions are established for the model studied. To\nestimate the memory parameters, the method given in Reisen, Rodrigues and Palma\n(2006 a,b) is generalized here to deal with a time series with two seasonal\nfractional long-memory parameters. The asymptotic properties are established\nand the accuracy of the method is investigated through Monte Carlo experiments.\nThe good performance of the estimator indicates that it can be an alternative\ncompetitive procedure to estimate seasonal long-memory time series data.\nArtificial and PM10 series were considered as examples of applications of the\nproposed estimation method.\n", "versions": [{"version": "v1", "created": "Thu, 25 Nov 2010 14:15:19 GMT"}], "update_date": "2010-11-29", "authors_parsed": [["Reisen", "Valderio A.", ""], ["Palma", "Wilfredo", ""], ["Arteche", "Josu", ""], ["Zamprogno", "Bartolomeu", ""]]}, {"id": "1011.6216", "submitter": "Hong-Li Zeng", "authors": "Hong-Li Zeng, Erik Aurell, Mikko Alava and Hamed Mahmoudi", "title": "Network inference using asynchronously updated kinetic Ising Model", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.83.041135", "report-no": null, "categories": "stat.CO cond-mat.dis-nn physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network structures are reconstructed from dynamical data by respectively\nnaive mean field (nMF) and Thouless-Anderson-Palmer (TAP) approximations. For\nTAP approximation, we use two methods to reconstruct the network: a) iteration\nmethod; b) casting the inference formula to a set of cubic equations and\nsolving it directly. We investigate inference of the asymmetric Sherrington-\nKirkpatrick (S-K) model using asynchronous update. The solutions of the sets\ncubic equation depend of temperature T in the S-K model, and a critical\ntemperature Tc is found around 2.1. For T < Tc, the solutions of the cubic\nequation sets are composed of 1 real root and two conjugate complex roots while\nfor T > Tc there are three real roots. The iteration method is convergent only\nif the cubic equations have three real solutions. The two methods give same\nresults when the iteration method is convergent. Compared to nMF, TAP is\nsomewhat better at low temperatures, but approaches the same performance as\ntemperature increase. Both methods behave better for longer data length, but\nfor improvement arises, TAP is well pronounced.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 12:37:31 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Zeng", "Hong-Li", ""], ["Aurell", "Erik", ""], ["Alava", "Mikko", ""], ["Mahmoudi", "Hamed", ""]]}, {"id": "1011.6409", "submitter": "Holger Hoefling", "authors": "Holger H\\\"ofling, Harald Binder, Martin Schumacher", "title": "A coordinate-wise optimization algorithm for the Fused Lasso", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L1 -penalized regression methods such as the Lasso (Tibshirani 1996) that\nachieve both variable selection and shrinkage have been very popular. An\nextension of this method is the Fused Lasso (Tibshirani and Wang 2007), which\nallows for the incorporation of external information into the model. In this\narticle, we develop new and fast algorithms for solving the Fused Lasso which\nare based on coordinate-wise optimization. This class of algorithms has\nrecently been applied very successfully to solve L1 -penalized problems very\nquickly (Friedman et al. 2007). As a straightforward coordinate-wise procedure\ndoes not converge to the global optimum in general, we adapt it in two ways,\nusing maximum-flow algorithms and a Huber penalty based approximation to the\nloss function. In a simulation study, we evaluate the speed of these algorithms\nand compare them to other standard methods. As the Huber-penalty based method\nis only approximate, we also evaluate its accuracy. Apart from this, we also\nextend the Fused Lasso to logistic as well as proportional hazards models and\nallow for a more flexible penalty structure.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 23:02:53 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["H\u00f6fling", "Holger", ""], ["Binder", "Harald", ""], ["Schumacher", "Martin", ""]]}]