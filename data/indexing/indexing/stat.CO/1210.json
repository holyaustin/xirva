[{"id": "1210.0066", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Iterative Reweighted Minimization Methods for $l_p$ Regularized\n  Unconstrained Nonlinear Programming", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study general $l_p$ regularized unconstrained minimization\nproblems. In particular, we derive lower bounds for nonzero entries of first-\nand second-order stationary points, and hence also of local minimizers of the\n$l_p$ minimization problems. We extend some existing iterative reweighted $l_1$\n(IRL1) and $l_2$ (IRL2) minimization methods to solve these problems and\nproposed new variants for them in which each subproblem has a closed form\nsolution. Also, we provide a unified convergence analysis for these methods. In\naddition, we propose a novel Lipschitz continuous $\\epsilon$-approximation to\n$\\|x\\|^p_p$. Using this result, we develop new IRL1 methods for the $l_p$\nminimization problems and showed that any accumulation point of the sequence\ngenerated by these methods is a first-order stationary point, provided that the\napproximation parameter $\\epsilon$ is below a computable threshold value. This\nis a remarkable result since all existing iterative reweighted minimization\nmethods require that $\\epsilon$ be dynamically updated and approach zero. Our\ncomputational results demonstrate that the new IRL1 method is generally more\nstable than the existing IRL1 methods [21,18] in terms of objective function\nvalue and CPU time.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2012 01:42:57 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1210.0137", "submitter": "Pierre Deville Pierre", "authors": "Vincent D. Blondel, Markus Esch, Connie Chan, Fabrice Clerot, Pierre\n  Deville, Etienne Huens, Fr\\'ed\\'eric Morlot, Zbigniew Smoreda and Cezary\n  Ziemlicki", "title": "Data for Development: the D4D Challenge on Mobile Phone Data", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI physics.soc-ph stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Orange \"Data for Development\" (D4D) challenge is an open data challenge\non anonymous call patterns of Orange's mobile phone users in Ivory Coast. The\ngoal of the challenge is to help address society development questions in novel\nways by contributing to the socio-economic development and well-being of the\nIvory Coast population. Participants to the challenge are given access to four\nmobile phone datasets and the purpose of this paper is to describe the four\ndatasets. The website http://www.d4d.orange.com contains more information about\nthe participation rules. The datasets are based on anonymized Call Detail\nRecords (CDR) of phone calls and SMS exchanges between five million of Orange's\ncustomers in Ivory Coast between December 1, 2011 and April 28, 2012. The\ndatasets are: (a) antenna-to-antenna traffic on an hourly basis, (b) individual\ntrajectories for 50,000 customers for two week time windows with antenna\nlocation information, (3) individual trajectories for 500,000 customers over\nthe entire observation period with sub-prefecture location information, and (4)\na sample of communication graphs for 5,000 customers\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2012 17:39:16 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2013 12:56:55 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Blondel", "Vincent D.", ""], ["Esch", "Markus", ""], ["Chan", "Connie", ""], ["Clerot", "Fabrice", ""], ["Deville", "Pierre", ""], ["Huens", "Etienne", ""], ["Morlot", "Fr\u00e9d\u00e9ric", ""], ["Smoreda", "Zbigniew", ""], ["Ziemlicki", "Cezary", ""]]}, {"id": "1210.0198", "submitter": "Bernd Sturmfels", "authors": "Jonathan Hauenstein, Jose Rodriguez, Bernd Sturmfels", "title": "Maximum Likelihood for Matrices with Rank Constraints", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation is a fundamental optimization problem in\nstatistics. We study this problem on manifolds of matrices with bounded rank.\nThese represent mixtures of distributions of two independent discrete random\nvariables. We determine the maximum likelihood degree for a range of\ndeterminantal varieties, and we apply numerical algebraic geometry to compute\nall critical points of their likelihood functions. This led to the discovery of\nmaximum likelihood duality between matrices of complementary ranks, a result\nproved subsequently by Draisma and Rodriguez.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2012 13:40:48 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2013 14:05:21 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Hauenstein", "Jonathan", ""], ["Rodriguez", "Jose", ""], ["Sturmfels", "Bernd", ""]]}, {"id": "1210.0220", "submitter": "Nick Whiteley", "authors": "Nick Whiteley, Anthony Lee", "title": "Twisted particle filters", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1167 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 1, 115-141", "doi": "10.1214/13-AOS1167", "report-no": "IMS-AOS-AOS1167", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate sampling laws for particle algorithms and the influence of\nthese laws on the efficiency of particle approximations of marginal likelihoods\nin hidden Markov models. Among a broad class of candidates we characterize the\nessentially unique family of particle system transition kernels which is\noptimal with respect to an asymptotic-in-time variance growth rate criterion.\nThe sampling structure of the algorithm defined by these optimal transitions\nturns out to be only subtly different from standard algorithms and yet the\nfluctuation properties of the estimates it provides can be dramatically\ndifferent. The structure of the optimal transition suggests a new class of\nalgorithms, which we term \"twisted\" particle filters and which we validate with\nasymptotic analysis of a more traditional nature, in the regime where the\nnumber of particles tends to infinity.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2012 17:04:38 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 15:22:28 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2013 15:45:16 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2014 13:58:34 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Whiteley", "Nick", ""], ["Lee", "Anthony", ""]]}, {"id": "1210.0333", "submitter": "Thiago Martins", "authors": "Thiago G. Martins, Daniel Simpson, Finn Lindgren and H{\\aa}vard Rue", "title": "Bayesian computing with INLA: new features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The INLA approach for approximate Bayesian inference for latent Gaussian\nmodels has been shown to give fast and accurate estimates of posterior\nmarginals and also to be a valuable tool in practice via the R-package R-INLA.\nIn this paper we formalize new developments in the R-INLA package and show how\nthese features greatly extend the scope of models that can be analyzed by this\ninterface. We also discuss the current default method in R-INLA to approximate\nposterior marginals of the hyperparameters using only a modest number of\nevaluations of the joint posterior distribution of the hyperparameters, without\nany need for numerical integration.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 10:06:54 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2013 12:33:22 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Martins", "Thiago G.", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1210.1180", "submitter": "Andreas Eberle", "authors": "Andreas Eberle", "title": "Error bounds for Metropolis-Hastings algorithms applied to perturbations\n  of Gaussian measures in high dimensions", "comments": "Published in at http://dx.doi.org/10.1214/13-AAP926 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2014, Vol. 24, No. 1, 337-377", "doi": "10.1214/13-AAP926", "report-no": "IMS-AAP-AAP926", "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metropolis-adjusted Langevin algorithm (MALA) is a Metropolis-Hastings\nmethod for approximate sampling from continuous distributions. We derive upper\nbounds for the contraction rate in Kantorovich-Rubinstein-Wasserstein distance\nof the MALA chain with semi-implicit Euler proposals applied to log-concave\nprobability measures that have a density w.r.t. a Gaussian reference measure.\nFor sufficiently \"regular\" densities, the estimates are dimension-independent,\nand they hold for sufficiently small step sizes $h$ that do not depend on the\ndimension either. In the limit $h\\downarrow0$, the bounds approach the known\noptimal contraction rates for overdamped Langevin diffusions in a convex\npotential. A similar approach also applies to Metropolis-Hastings chains with\nOrnstein-Uhlenbeck proposals. In this case, the resulting estimates are still\nindependent of the dimension but less optimal, reflecting the fact that MALA is\na higher order approximation of the diffusion limit than Metropolis-Hastings\nwith Ornstein-Uhlenbeck proposals.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 17:48:12 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 08:22:42 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Eberle", "Andreas", ""]]}, {"id": "1210.1388", "submitter": "Jean-Michel Marin", "authors": "Mohammed Sedki and Pierre Pudlo and Jean-Michel Marin and Christian P.\n  Robert and Jean-Marie Cornuet", "title": "Efficient learning in ABC algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation has been successfully used in population\ngenetics to bypass the calculation of the likelihood. These methods provide\naccurate estimates of the posterior distribution by comparing the observed\ndataset to a sample of datasets simulated from the model. Although\nparallelization is easily achieved, computation times for ensuring a suitable\napproximation quality of the posterior distribution are still high. To\nalleviate the computational burden, we propose an adaptive, sequential\nalgorithm that runs faster than other ABC algorithms but maintains accuracy of\nthe approximation. This proposal relies on the sequential Monte Carlo sampler\nof Del Moral et al. (2012) but is calibrated to reduce the number of\nsimulations from the model. The paper concludes with numerical experiments on a\ntoy example and on a population genetic study of Apis mellifera, where our\nalgorithm was shown to be faster than traditional ABC schemes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 11:44:52 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2013 18:13:34 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Sedki", "Mohammed", ""], ["Pudlo", "Pierre", ""], ["Marin", "Jean-Michel", ""], ["Robert", "Christian P.", ""], ["Cornuet", "Jean-Marie", ""]]}, {"id": "1210.1434", "submitter": "Thiago Martins", "authors": "Thiago G. Martins and H{\\aa}vard Rue", "title": "Extending INLA to a class of near-Gaussian latent models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work extends the Integrated Nested Laplace Approximation (INLA) method\nto latent models outside the scope of latent Gaussian models, where independent\ncomponents of the latent field can have a near-Gaussian distribution. The\nproposed methodology is an essential component of a bigger project that aim to\nextend the R package INLA (R-INLA) in order to allow the user to add\nflexibility and challenge the Gaussian assumptions of some of the model\ncomponents in a straightforward and intuitive way. Our approach is applied to\ntwo examples and the results are compared with that obtained by Markov Chain\nMonte Carlo (MCMC), showing similar accuracy with only a small fraction of\ncomputational time. Implementation of the proposed extension is available in\nthe R-INLA package.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 13:38:41 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2013 14:37:10 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Martins", "Thiago G.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1210.1484", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu, Matti Vihola", "title": "Convergence properties of pseudo-marginal Markov chain Monte Carlo\n  algorithms", "comments": "Published at http://dx.doi.org/10.1214/14-AAP1022 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2015, Vol. 25, No. 2, 1030-1077", "doi": "10.1214/14-AAP1022", "report-no": "IMS-AAP-AAP1022", "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study convergence properties of pseudo-marginal Markov chain Monte Carlo\nalgorithms (Andrieu and Roberts [Ann. Statist. 37 (2009) 697-725]). We find\nthat the asymptotic variance of the pseudo-marginal algorithm is always at\nleast as large as that of the marginal algorithm. We show that if the marginal\nchain admits a (right) spectral gap and the weights (normalised estimates of\nthe target density) are uniformly bounded, then the pseudo-marginal chain has a\nspectral gap. In many cases, a similar result holds for the absolute spectral\ngap, which is equivalent to geometric ergodicity. We consider also unbounded\nweight distributions and recover polynomial convergence rates in more specific\ncases, when the marginal algorithm is uniformly ergodic or an independent\nMetropolis-Hastings or a random-walk Metropolis targeting a super-exponential\ndensity with regular contours. Our results on geometric and polynomial\nconvergence rates imply central limit theorems. We also prove that under\ngeneral conditions, the asymptotic variance of the pseudo-marginal algorithm\nconverges to the asymptotic variance of the marginal algorithm if the accuracy\nof the estimators is increased.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 15:18:14 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 13:44:09 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 13:20:03 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Andrieu", "Christophe", ""], ["Vihola", "Matti", ""]]}, {"id": "1210.1773", "submitter": "Mikkel Meyer Andersen", "authors": "Mikkel Meyer Andersen, Poul Svante Eriksen", "title": "Efficient Forward Simulation of Fisher-Wright Populations with\n  Stochastic Population Size and Neutral Single Step Mutations in Haplotypes", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both population genetics and forensic genetics it is important to know how\nhaplotypes are distributed in a population. Simulation of population dynamics\nhelps facilitating research on the distribution of haplotypes. In forensic\ngenetics, the haplotypes can for example consist of lineage markers such as\nshort tandem repeat loci on the Y chromosome (Y-STR). A dominating model for\ndescribing population dynamics is the simple, yet powerful, Fisher-Wright\nmodel. We describe an efficient algorithm for exact forward simulation of exact\nFisher-Wright populations (and not approximative such as the coalescent model).\nThe efficiency comes from convenient data structures by changing the\ntraditional view from individuals to haplotypes. The algorithm is implemented\nin the open-source R package 'fwsim' and is able to simulate very large\npopulations. We focus on a haploid model and assume stochastic population size\nwith flexible growth specification, no selection, a neutral single step\nmutation process, and self-reproducing individuals. These assumptions make the\nalgorithm ideal for studying lineage markers such as Y-STR.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 14:38:00 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Andersen", "Mikkel Meyer", ""], ["Eriksen", "Poul Svante", ""]]}, {"id": "1210.2077", "submitter": "Julien  Chiquet Dr.", "authors": "Yves Grandvalet, Julien Chiquet and Christophe Ambroise", "title": "Sparsity by Worst-Case Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new interpretation of sparse penalties such as the\nelastic-net and the group-lasso. Beyond providing a new viewpoint on these\npenalization schemes, our approach results in a unified optimization strategy.\nOur experiments demonstrate that this strategy, implemented on the elastic-net,\nis computationally extremely efficient for small to medium size problems. Our\naccompanying software solves problems very accurately, at machine precision, in\nthe time required to get a rough estimate with competing state-of-the-art\nalgorithms. We illustrate on real and artificial datasets that this accuracy is\nrequired to for the correctness of the support of the solution, which is an\nimportant element for the interpretability of sparsity-inducing penalties.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 17:06:45 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 16:24:52 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Grandvalet", "Yves", ""], ["Chiquet", "Julien", ""], ["Ambroise", "Christophe", ""]]}, {"id": "1210.2254", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "A weighted least squares procedure to approximate least absolute\n  deviation estimation in time series with specific reference to infinite\n  variance unit root problems", "comments": null, "journal-ref": "South African Statistical Journal, 2013, 47(1), pp. 61-70", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weighted regression procedure is proposed for regression type problems\nwhere the innovations are heavy-tailed. This method approximates the least\nabsolute regression method in large samples, and the main advantage will be if\nthe sample is large and for problems with many independent variables. In such\nproblems bootstrap methods must often be utilized to test hypotheses and\nespecially in such a case this procedure has an advantage over least absolute\nregression. The procedure will be illustrated on first-order autoregressive\nproblems, including the random walk. A bootstrap procedure is used to test the\nunit root hypothesis and good results were found.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 12:13:34 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["van Zyl", "J. Martin", ""]]}, {"id": "1210.2601", "submitter": "R\\'{e}mi Bardenet", "authors": "R\\'emi Bardenet, Olivier Capp\\'e, Gersende Fort, Bal\\'azs K\\'egl", "title": "Adaptive MCMC with online relabeling", "comments": "Published at http://dx.doi.org/10.3150/13-BEJ578 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 3, 1304-1340", "doi": "10.3150/13-BEJ578", "report-no": "IMS-BEJ-BEJ578", "categories": "stat.CO math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When targeting a distribution that is artificially invariant under some\npermutations, Markov chain Monte Carlo (MCMC) algorithms face the\nlabel-switching problem, rendering marginal inference particularly cumbersome.\nSuch a situation arises, for example, in the Bayesian analysis of finite\nmixture models. Adaptive MCMC algorithms such as adaptive Metropolis (AM),\nwhich self-calibrates its proposal distribution using an online estimate of the\ncovariance matrix of the target, are no exception. To address the\nlabel-switching issue, relabeling algorithms associate a permutation to each\nMCMC sample, trying to obtain reasonable marginals. In the case of adaptive\nMetropolis (Bernoulli 7 (2001) 223-242), an online relabeling strategy is\nrequired. This paper is devoted to the AMOR algorithm, a provably consistent\nvariant of AM that can cope with the label-switching problem. The idea is to\nnest relabeling steps within the MCMC algorithm based on the estimation of a\nsingle covariance matrix that is used both for adapting the covariance of the\nproposal distribution in the Metropolis algorithm step and for online\nrelabeling. We compare the behavior of AMOR to similar relabeling methods. In\nthe case of compactly supported target distributions, we prove a strong law of\nlarge numbers for AMOR and its ergodicity. These are the first results on the\nconsistency of an online relabeling algorithm to our knowledge. The proof\nunderlines latent relations between relabeling and vector quantization.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 14:05:38 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2013 15:17:07 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2015 11:54:09 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Capp\u00e9", "Olivier", ""], ["Fort", "Gersende", ""], ["K\u00e9gl", "Bal\u00e1zs", ""]]}, {"id": "1210.3039", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Sequential Convex Programming Methods for A Class of Structured\n  Nonlinear Programming", "comments": "This paper has been withdrawn by the author due to major revision and\n  corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a broad class of structured nonlinear programming\n(SNLP) problems. In particular, we first establish the first-order optimality\nconditions for them. Then we propose sequential convex programming (SCP)\nmethods for solving them in which each iteration is obtained by solving a\nconvex programming problem exactly or inexactly. Under some suitable\nassumptions, we establish that any accumulation point of the sequence generated\nby the methods is a KKT point of the SNLP problems. In addition, we propose a\nvariant of the exact SCP method for SNLP in which nonmonotone scheme and\n\"local\" Lipschitz constants of the associated functions are used. And a similar\nconvergence result as mentioned above is established.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 20:01:29 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 17:31:12 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1210.3296", "submitter": "Sarah Filippi", "authors": "Daniel Silk, Saran Filippi and Michael P.H. Stumpf", "title": "Optimizing Threshold - Schedules for Approximate Bayesian Computation\n  Sequential Monte Carlo Samplers: Applications to Molecular Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood-free sequential Approximate Bayesian Computation (ABC)\nalgorithms, are increasingly popular inference tools for complex biological\nmodels. Such algorithms proceed by constructing a succession of probability\ndistributions over the parameter space conditional upon the simulated data\nlying in an $\\epsilon$--ball around the observed data, for decreasing values of\nthe threshold $\\epsilon$. While in theory, the distributions (starting from a\nsuitably defined prior) will converge towards the unknown posterior as\n$\\epsilon$ tends to zero, the exact sequence of thresholds can impact upon the\ncomputational efficiency and success of a particular application. In\nparticular, we show here that the current preferred method of choosing\nthresholds as a pre-determined quantile of the distances between simulated and\nobserved data from the previous population, can lead to the inferred posterior\ndistribution being very different to the true posterior. Threshold selection\nthus remains an important challenge. Here we propose an automated and adaptive\nmethod that allows us to balance the need to minimise the threshold with\ncomputational efficiency. Moreover, our method which centres around predicting\nthe threshold - acceptance rate curve using the unscented transform, enables us\nto avoid local minima - a problem that has plagued previous threshold schemes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 16:46:39 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Silk", "Daniel", ""], ["Filippi", "Saran", ""], ["Stumpf", "Michael P. H.", ""]]}, {"id": "1210.3405", "submitter": "Scott Sisson", "authors": "P. Menendez, Y. Fan, P. H. Garthwaite, S. A. Sisson", "title": "Simultaneous adjustment of bias and coverage probabilities for\n  confidence intervals", "comments": "To appear in Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed for the correction of confidence intervals when the\noriginal interval does not have the correct nominal coverage probabilities in\nthe frequentist sense. The proposed method is general and does not require any\ndistributional assumptions. It can be applied to both frequentist and Bayesian\ninference where interval estimates are desired. We provide theoretical results\nfor the consistency of the proposed estimator, and give two complex examples,\non confidence interval correction for composite likelihood estimators and in\napproximate Bayesian computation (ABC), to demonstrate the wide applicability\nof the new method. Comparison is made with the double-bootstrap and other\nmethods of improving confidence interval coverage.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2012 00:48:34 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2013 03:40:58 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Menendez", "P.", ""], ["Fan", "Y.", ""], ["Garthwaite", "P. H.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1210.3849", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Alice X. D. Dong, Robert Kohn", "title": "A Copula Based Bayesian Approach for Paid-Incurred Claims Models for\n  Non-Life Insurance Reserving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article considers the class of recently developed stochastic models that\ncombine claims payments and incurred losses information into a coherent\nreserving methodology. In particular, we develop a family of Heirarchical\nBayesian Paid-Incurred-Claims models, combining the claims reserving models of\nHertig et al. (1985) and Gogol et al. (1993). In the process we extend the\nindependent log-normal model of Merz et al. (2010) by incorporating different\ndependence structures using a Data-Augmented mixture Copula Paid-Incurred\nclaims model.\n  The utility and influence of incorporating both payment and incurred losses\ninto estimating of the full predictive distribution of the outstanding loss\nliabilities and the resulting reserves is demonstrated in the following cases:\n(i) an independent payment (P) data model; (ii) the independent\nPayment-Incurred Claims (PIC) data model of Merz et al. (2010); (iii) a novel\ndependent lag-year telescoping block diagonal Gaussian Copula PIC data model\nincorporating conjugacy via transformation; (iv) a novel data-augmented mixture\nArchimedean copula dependent PIC data model.\n  Inference in such models is developed via a class of adaptive Markov chain\nMonte Carlo sampling algorithms. These incorporate a data-augmentation\nframework utilized to efficiently evaluate the likelihood for the copula based\nPIC model in the loss reserving triangles. The adaptation strategy is based on\nrepresenting a positive definite covariance matrix by the exponential of a\nsymmetric matrix as proposed by Leonard et al. (1992).\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2012 22:12:41 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2012 20:36:38 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2012 23:02:22 GMT"}, {"version": "v4", "created": "Sun, 9 Dec 2012 16:36:55 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Peters", "Gareth W.", ""], ["Dong", "Alice X. D.", ""], ["Kohn", "Robert", ""]]}, {"id": "1210.4683", "submitter": "Ajay Jasra", "authors": "Elena Ehrlich, Ajay Jasra, Nikolas Kantas", "title": "Static Parameter Estimation for ABC Approximations of Hidden Markov\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we focus on Maximum Likelihood estimation (MLE) for the\nstatic parameters of hidden Markov models (HMMs). We will consider the case\nwhere one cannot or does not want to compute the conditional likelihood density\nof the observation given the hidden state because of increased computational\ncomplexity or analytical intractability. Instead we will assume that one may\nobtain samples from this conditional likelihood and hence use approximate\nBayesian computation (ABC) approximations of the original HMM. ABC\napproximations are biased, but the bias can be controlled to arbitrary\nprecision via a parameter \\epsilon>0; the bias typically goes to zero as\n\\epsilon \\searrow 0. We first establish that the bias in the log-likelihood and\ngradient of the log-likelihood of the ABC approximation, for a fixed batch of\ndata, is no worse than \\mathcal{O}(n\\epsilon), n being the number of data;\nhence, for computational reasons, one might expect reasonable parameter\nestimates using such an ABC approximation. Turning to the computational problem\nof estimating $\\theta$, we propose, using the ABC-sequential Monte Carlo (SMC)\nalgorithm in Jasra et al. (2012), an approach based upon simultaneous\nperturbation stochastic approximation (SPSA). Our method is investigated on two\nnumerical examples\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 09:57:20 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Ehrlich", "Elena", ""], ["Jasra", "Ajay", ""], ["Kantas", "Nikolas", ""]]}, {"id": "1210.4844", "submitter": "Cedric Archambeau", "authors": "Cedric Archambeau, Francois Caron", "title": "Plackett-Luce regression: A new Bayesian model for polychotomous data", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-84-92", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multinomial logistic regression is one of the most popular models for\nmodelling the effect of explanatory variables on a subject choice between a set\nof specified options. This model has found numerous applications in machine\nlearning, psychology or economy. Bayesian inference in this model is non\ntrivial and requires, either to resort to a MetropolisHastings algorithm, or\nrejection sampling within a Gibbs sampler. In this paper, we propose an\nalternative model to multinomial logistic regression. The model builds on the\nPlackett-Luce model, a popular model for multiple comparisons. We show that the\nintroduction of a suitable set of auxiliary variables leads to an\nExpectation-Maximization algorithm to find Maximum A Posteriori estimates of\nthe parameters. We further provide a full Bayesian treatment by deriving a\nGibbs sampler, which only requires to sample from highly standard\ndistributions. We also propose a variational approximate inference scheme. All\nare very simple to implement. One property of our Plackett-Luce regression\nmodel is that it learns a sparse set of feature weights. We compare our method\nto sparse Bayesian multinomial logistic regression and show that it is\ncompetitive, especially in presence of polychotomous data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:34:18 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Archambeau", "Cedric", ""], ["Caron", "Francois", ""]]}, {"id": "1210.4963", "submitter": "Nikolai Krivulin", "authors": "Nikolai Krivulin", "title": "An analysis of the least median of squares regression problem", "comments": "10th Symposium on Computational Statistics (COMPSTAT), Neuchatel,\n  Switzerland, August 1992", "journal-ref": "Computational Statistics: Proceedings of the 10th Symposium on\n  Computational Statistics, COMPSTAT (Neuchatel, Switzerland, August 1992),\n  Vol. 1, pp. 471-476", "doi": "10.1007/978-3-662-26811-7_65", "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization problem that arises out of the least median of squared\nresiduals method in linear regression is analyzed. To simplify the analysis,\nthe problem is replaced by an equivalent one of minimizing the median of\nabsolute residuals. A useful representation of the last problem is given to\nexamine properties of the objective function and estimate the number of its\nlocal minima. It is shown that the exact number of local minima is equal to $\n{p+\\lfloor (n-1)/2 \\rfloor \\choose{p}} $, where $ p $ is the dimension of the\nregression model and $ n $ is the number of observations. As applications of\nthe results, three algorithms are also outlined.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 21:35:06 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Krivulin", "Nikolai", ""]]}, {"id": "1210.5267", "submitter": "Silvia Bacci Dr", "authors": "Francesco Bartolucci, Silvia Bacci, Michela Gnaldi", "title": "MultiLCIRT: An R package for multidimensional latent class item response\n  models", "comments": "36 pages, 1 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1201.4667", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We illustrate a class of Item Response Theory (IRT) models for binary and\nordinal polythomous items and we describe an R package for dealing with these\nmodels, which is named MultiLCIRT. The models at issue extend traditional IRT\nmodels allowing for (i) multidimensionality and (ii) discreteness of latent\ntraits. This class of models also allows for different parameterizations for\nthe conditional distribution of the response variables given the latent traits,\ndepending on both the type of link function and the constraints imposed on the\ndiscriminating and the difficulty item parameters. We illustrate how the\nproposed class of models may be estimated by the maximum likelihood approach\nvia an Expectation-Maximization algorithm, which is implemented in the\nMultiLCIRT package, and we discuss in detail issues related to model selection.\nIn order to illustrate this package, we analyze two datasets: one concerning\nbinary items and referred to the measurement of ability in mathematics and the\nother one coming from the administration of ordinal polythomous items for the\nassessment of anxiety and depression. In the first application, we illustrate\nhow aggregating items in homogeneous groups through a model-based hierarchical\nclustering procedure which is implemented in the proposed package. In the\nsecond application, we describe the steps to select a specific model having the\nbest fit in our class of IRT models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 21:40:33 GMT"}], "update_date": "2012-10-22", "authors_parsed": [["Bartolucci", "Francesco", ""], ["Bacci", "Silvia", ""], ["Gnaldi", "Michela", ""]]}, {"id": "1210.5277", "submitter": "Fran\\c{c}ois Desbouvries", "authors": "Yohan Petetin and Fran\\c{c}ois Desbouvries", "title": "Bayesian Conditional Monte Carlo Algorithms for Sequential Single and\n  Multi-Object filtering", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian filtering aims at tracking sequentially a hidden process from an\nobserved one. In particular, sequential Monte Carlo (SMC) techniques propagate\nin time weighted trajectories which represent the posterior probability density\nfunction (pdf) of the hidden process given the available observations. On the\nother hand, Conditional Monte Carlo (CMC) is a variance reduction technique\nwhich replaces the estimator of a moment of interest by its conditional\nexpectation given another variable. In this paper we show that up to some\nadaptations, one can make use of the time recursive nature of SMC algorithms in\norder to propose natural temporal CMC estimators of some point estimates of the\nhidden process, which outperform the associated crude Monte Carlo (MC)\nestimator whatever the number of samples. We next show that our Bayesian CMC\nestimators can be computed exactly, or approximated efficiently, in some hidden\nMarkov chain (HMC) models; in some jump Markov state-space systems (JMSS); as\nwell as in multitarget filtering. Finally our algorithms are validated via\nsimulations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 23:08:24 GMT"}], "update_date": "2012-10-22", "authors_parsed": [["Petetin", "Yohan", ""], ["Desbouvries", "Fran\u00e7ois", ""]]}, {"id": "1210.5418", "submitter": "Nikolai Krivulin", "authors": "Nikolai Krivulin", "title": "Unbiased estimates for gradients of stochastic network performance\n  measures", "comments": null, "journal-ref": "Acta Applicandae Mathematicae, October 1993, Volume 33, Issue 1,\n  pp. 21-43. ISSN: 0167-8019", "doi": "10.1007/BF00995493", "report-no": null, "categories": "math.OC stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three classes of stochastic networks and their performance measures are\nconsidered. These performance measures are defined as the expected value of\nsome random variables and cannot normally be obtained analytically as functions\nof network parameters in a closed form. We give similar representations for the\nrandom variables to provide a useful way of analytical study of these functions\nand their gradients. The representations are used to obtain sufficient\nconditions for the gradient estimates to be unbiased. The conditions are rather\ngeneral and usually met in simulation study of the stochastic networks.\nApplications of the results are discussed and some practical algorithms of\ncalculating unbiased estimates of the gradients are also presented.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 13:38:06 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Krivulin", "Nikolai", ""]]}, {"id": "1210.5992", "submitter": "Jianqing Fan", "authors": "Jianqing Fan, Lingzhou Xue, Hui Zou", "title": "Strong oracle optimality of folded concave penalized estimation", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1198 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). With Corrections", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 3, 819-849", "doi": "10.1214/13-AOS1198", "report-no": "IMS-AOS-AOS1198", "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Folded concave penalization methods have been shown to enjoy the strong\noracle property for high-dimensional sparse estimation. However, a folded\nconcave penalization problem usually has multiple local solutions and the\noracle property is established only for one of the unknown local solutions. A\nchallenging fundamental issue still remains that it is not clear whether the\nlocal optimum computed by a given optimization algorithm possesses those nice\ntheoretical properties. To close this important theoretical gap in over a\ndecade, we provide a unified theory to show explicitly how to obtain the oracle\nsolution via the local linear approximation algorithm. For a folded concave\npenalized estimation problem, we show that as long as the problem is\nlocalizable and the oracle estimator is well behaved, we can obtain the oracle\nestimator by using the one-step local linear approximation. In addition, once\nthe oracle estimator is obtained, the local linear approximation algorithm\nconverges, namely it produces the same estimator in the next iteration. The\ngeneral theory is demonstrated by using four classical sparse estimation\nproblems, that is, sparse linear regression, sparse logistic regression, sparse\nprecision matrix estimation and sparse quantile regression.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 18:39:03 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 03:16:12 GMT"}, {"version": "v3", "created": "Tue, 27 May 2014 05:30:35 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 11:36:04 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Fan", "Jianqing", ""], ["Xue", "Lingzhou", ""], ["Zou", "Hui", ""]]}, {"id": "1210.6059", "submitter": "Sergiy Nesterko", "authors": "Sergiy Nesterko and Joseph Blitzstein", "title": "Bias-Variance and Breadth-Depth Tradeoffs in Respondent-Driven Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a link-tracing network sampling strategy\nfor collecting data from hard-to-reach populations, such as injection drug\nusers or individuals at high risk of being infected with HIV. The mechanism is\nto find initial participants (seeds), and give each of them a fixed number of\ncoupons allowing them to recruit people they know from the population of\ninterest, with a mutual financial incentive. The new participants are given\ncoupons again and the process repeats. Currently, the standard RDS estimator\nused in practice is known as the Volz-Heckathorn (VH) estimator. It relies on\nstrong assumptions about the underlying social network and the RDS process. Via\nsimulation, we study the relative performance of the plain mean and VH\nestimator when assumptions of the latter are not satisfied, under different\nnetwork types (including homophily and rich-get-richer networks), participant\nreferral patterns, and varying number of coupons. The analysis demonstrates\nthat the plain mean outperforms the VH estimator in many but not all of the\nsimulated settings, including homophily networks. Also, we highlight the\nimplications of multiple recruitment and varying referral patterns on the depth\nof RDS process. We develop interactive visualizations of the findings and RDS\nprocess to further build insight into the various factors contributing to the\nperformance of current RDS estimation techniques.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 20:30:03 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Nesterko", "Sergiy", ""], ["Blitzstein", "Joseph", ""]]}, {"id": "1210.6703", "submitter": "Anthony Lee", "authors": "Anthony Lee and Krzysztof Latuszynski", "title": "Variance bounding and geometric ergodicity of Markov chain Monte Carlo\n  kernels for approximate Bayesian computation", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": "10.1093/biomet/asu027", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation has emerged as a standard computational tool\nwhen dealing with the increasingly common scenario of completely intractable\nlikelihood functions in Bayesian inference. We show that many common Markov\nchain Monte Carlo kernels used to facilitate inference in this setting can fail\nto be variance bounding, and hence geometrically ergodic, which can have\nconsequences for the reliability of estimates in practice. This phenomenon is\ntypically independent of the choice of tolerance in the approximation. We then\nprove that a recently introduced Markov kernel in this setting can inherit\nvariance bounding and geometric ergodicity from its intractable\nMetropolis--Hastings counterpart, under reasonably weak and manageable\nconditions. We show that the computational cost of this alternative kernel is\nbounded whenever the prior is proper, and present indicative results on an\nexample where spectral gaps and asymptotic variances can be computed, as well\nas an example involving inference for a partially and discretely observed,\ntime-homogeneous, pure jump Markov process. We also supply two general\ntheorems, one of which provides a simple sufficient condition for lack of\nvariance bounding for reversible kernels and the other provides a positive\nresult concerning inheritance of variance bounding and geometric ergodicity for\nmixtures of reversible kernels.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 23:14:51 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 13:16:10 GMT"}, {"version": "v3", "created": "Tue, 25 Mar 2014 21:57:19 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Lee", "Anthony", ""], ["Latuszynski", "Krzysztof", ""]]}, {"id": "1210.6911", "submitter": "Fredrik Lindsten", "authors": "Fredrik Lindsten, Michael I. Jordan and Thomas B. Sch\\\"on", "title": "Ancestor Sampling for Particle Gibbs", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems (NIPS) 25,\n  (2012) 2600-2608", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method in the family of particle MCMC methods that we\nrefer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the\nexisting PG with backward simulation (PG-BS) procedure, we use backward\nsampling to (considerably) improve the mixing of the PG kernel. Instead of\nusing separate forward and backward sweeps as in PG-BS, however, we achieve the\nsame effect in a single forward sweep. We apply the PG-AS framework to the\nchallenging class of non-Markovian state-space models. We develop a truncation\nstrategy of these models that is applicable in principle to any\nbackward-simulation-based method, but which is particularly well suited to the\nPG-AS framework. In particular, as we show in a simulation study, PG-AS can\nyield an order-of-magnitude improved accuracy relative to PG-BS due to its\nrobustness to the truncation error. Several application examples are discussed,\nincluding Rao-Blackwellized particle smoothing and inference in degenerate\nstate-space models.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 17:12:36 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Lindsten", "Fredrik", ""], ["Jordan", "Michael I.", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1210.7463", "submitter": "Cesar Roberto de Souza", "authors": "C\\'esar Roberto de Souza", "title": "A Tutorial on Principal Component Analysis with the Accord.NET Framework", "comments": "35 pages, Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document aims to clarify frequent questions on using the Accord.NET\nFramework to perform statistical analyses. Here, we reproduce all steps of the\nfamous Lindsay's Tutorial on Principal Component Analysis, in an attempt to\ngive the reader a complete hands-on overview on the framework's basics while\nalso discussing some of the results and sources of divergence between the\nresults generated by Accord.NET and by other software packages.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2012 14:04:11 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["de Souza", "C\u00e9sar Roberto", ""]]}, {"id": "1210.7477", "submitter": "Robert Nishihara", "authors": "Robert Nishihara, Iain Murray, Ryan P. Adams", "title": "Parallel MCMC with Generalized Elliptical Slice Sampling", "comments": "19 pages, 8 figures, 3 algorithms", "journal-ref": "Journal of Machine Learning Research 15:2087-2112, 2014", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models are conceptually powerful tools for finding structure in\ndata, but their practical effectiveness is often limited by our ability to\nperform inference in them. Exact inference is frequently intractable, so\napproximate inference is often performed using Markov chain Monte Carlo (MCMC).\nTo achieve the best possible results from MCMC, we want to efficiently simulate\nmany steps of a rapidly mixing Markov chain which leaves the target\ndistribution invariant. Of particular interest in this regard is how to take\nadvantage of multi-core computing to speed up MCMC-based inference, both to\nimprove mixing and to distribute the computational load. In this paper, we\npresent a parallelizable Markov chain Monte Carlo algorithm for efficiently\nsampling from continuous probability distributions that can take advantage of\nhundreds of cores. This method shares information between parallel Markov\nchains to build a scale-mixture of Gaussians approximation to the density\nfunction of the target distribution. We combine this approximation with a\nrecent method known as elliptical slice sampling to create a Markov chain with\nno step-size parameters that can mix rapidly without requiring gradient or\ncurvature computations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2012 17:10:29 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2014 01:05:43 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Nishihara", "Robert", ""], ["Murray", "Iain", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1210.7485", "submitter": "Alireza Daneshkhah Alireza Daneshkhah", "authors": "Alireza Daneshkhah, Golamali Parham, Omid Chatrabgoun, M. Jokar", "title": "Approximation Multivariate Distribution with pair copula Using the\n  Orthonormal Polynomial and Legendre Multiwavelets basis functions", "comments": "31 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1206.2700 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we concentrate on new methodologies for copulas introduced and\ndeveloped by Joe, Cooke, Bedford, Kurowica, Daneshkhah and others on the new\nclass of graphical models called vines as a way of constructing higher\ndimensional distributions. We develop the approximation method presented by\nBedford et al (2012) at which they show that any $n$-dimensional copula density\ncan be approximated arbitrarily well pointwise using a finite parameter set of\n2-dimensional copulas in a vine or pair-copula construction. Our constructive\napproach involves the use of minimum information copulas that can be specified\nto any required degree of precision based on the available data or experts'\njudgements. By using this method, we are able to use a fixed finite dimensional\nfamily of copulas to be employed in a vine construction, with the promise of a\nuniform level of approximation.\n  The basic idea behind this method is to use a two-dimensional ordinary\npolynomial series to approximate any log-density of a bivariate copula function\nby truncating the series at an appropriate point. We present an alternative\napproximation of the multivariate distribution of interest by considering\northonormal polynomial and Legendre multiwavelets as the basis functions. We\nshow the derived approximations are more precise and computationally faster\nwith better properties than the one proposed by Bedford et al. (2012). We then\napply our method to modelling a dataset of Norwegian financial data that was\npreviously analysed in the series of papers, and finally compare our results by\nthem.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2012 18:22:47 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Daneshkhah", "Alireza", ""], ["Parham", "Golamali", ""], ["Chatrabgoun", "Omid", ""], ["Jokar", "M.", ""]]}, {"id": "1210.7726", "submitter": "Ashkan Panahi", "authors": "Ashkan Panahi and Mats Viberg", "title": "Performance Analysis of Parameter Estimation Using LASSO", "comments": "This paper is to be submitted for review to IEEE transactions on\n  signal processing (TSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Least Absolute Shrinkage and Selection Operator (LASSO) has gained\nattention in a wide class of continuous parametric estimation problems with\npromising results. It has been a subject of research for more than a decade.\nDue to the nature of LASSO, the previous analyses have been non-parametric.\nThis ignores useful information and makes it difficult to compare LASSO to\ntraditional estimators. In particular, the role of the regularization parameter\nand super-resolution properties of LASSO have not been well-understood yet. The\nobjective of this work is to provide a new insight into this context by\nintroducing LASSO as a parametric technique of a varying order. This provides\nus theoretical expressions for the LASSO-based estimation error and false alarm\nrate in the asymptotic case of high SNR and dense grids. For this case, LASSO\nis compared to maximum likelihood and conventional beamforming. It is found\nthat LASSO loses performance due to the regularization term, but the amount of\nloss is practically negligible with a proper choice of the regularization\nparameter. Thus, we provide suggestions on the selection of the regularization\nparameter. Without loss of generality, we present the comparative numerical\nresults in the context of Direction of Arrival (DOA) estimation using a sensor\narray.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 16:48:07 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 11:23:19 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 10:52:41 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Panahi", "Ashkan", ""], ["Viberg", "Mats", ""]]}, {"id": "1210.8268", "submitter": "Ioannis Papastathopoulos", "authors": "Ioannis Papastathopoulos and Jonathan A. Tawn", "title": "Dependence Properties of Multivariate Max-Stable Distributions", "comments": "13 double spaced pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an m-dimensional multivariate extreme value distribution there exist\n2^{m}-1 exponent measures which are linked and completely characterise the\ndependence of the distribution and all of its lower dimensional margins. In\nthis paper we generalise the inequalities of Schlather and Tawn (2002) for the\nsets of extremal coefficients and construct bounds that higher order exponent\nmeasures need to satisfy to be consistent with lower order exponent measures.\nSubsequently we construct nonparametric estimators of the exponent measures\nwhich impose, through a likelihood-based procedure, the new dependence\nconstraints and provide an improvement on the unconstrained estimators.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 08:46:30 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Papastathopoulos", "Ioannis", ""], ["Tawn", "Jonathan A.", ""]]}]