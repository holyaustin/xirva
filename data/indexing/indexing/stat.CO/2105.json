[{"id": "2105.00244", "submitter": "Aleksandr Aravkin", "authors": "Metin Vural, Aleksandr Y. Aravkin, and S{\\l}awomir Stan'czak", "title": "l1-Norm Minimization with Regula Falsi Type Root Finding Methods", "comments": "l1 -norm minimization, nonconvex models, Regula-Falsi, root-finding", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse level-set formulations allow practitioners to find the minimum 1-norm\nsolution subject to likelihood constraints. Prior art requires this constraint\nto be convex. In this letter, we develop an efficient approach for nonconvex\nlikelihoods, using Regula Falsi root-finding techniques to solve the level-set\nformulation. Regula Falsi methods are simple, derivative-free, and efficient,\nand the approach provably extends level-set methods to the broader class of\nnonconvex inverse problems. Practical performance is illustrated using\nl1-regularized Student's t inversion, which is a nonconvex approach used to\ndevelop outlier-robust formulations.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:24:38 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Vural", "Metin", ""], ["Aravkin", "Aleksandr Y.", ""], ["Stan'czak", "S\u0142awomir", ""]]}, {"id": "2105.00488", "submitter": "Polina Suter", "authors": "Polina Suter and Jack Kuipers and Giusi Moffa and Niko Beerenwinkel", "title": "Bayesian structure learning and sampling of Bayesian networks with the R\n  package BiDAG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R package BiDAG implements Markov chain Monte Carlo (MCMC) methods for\nstructure learning and sampling of Bayesian networks. The package includes\ntools to search for a maximum a posteriori (MAP) graph and to sample graphs\nfrom the posterior distribution given the data. A new hybrid approach to\nstructure learning enables inference in large graphs. In the first step, we\ndefine a reduced search space by means of the PC algorithm or based on prior\nknowledge. In the second step, an iterative order MCMC scheme proceeds to\noptimize within the restricted search space and estimate the MAP graph.\nSampling from the posterior distribution is implemented using either order or\npartition MCMC. The models and algorithms can handle both discrete and\ncontinuous data. The BiDAG package also provides an implementation of MCMC\nschemes for structure learning and sampling of dynamic Bayesian networks.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 14:42:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Suter", "Polina", ""], ["Kuipers", "Jack", ""], ["Moffa", "Giusi", ""], ["Beerenwinkel", "Niko", ""]]}, {"id": "2105.00520", "submitter": "Ameer Dharamshi", "authors": "Ameer Dharamshi, Vivian Ngo, and Jeffrey S. Rosenthal", "title": "Sampling by Divergence Minimization", "comments": "33 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of Markov Chain Monte Carlo (MCMC) methods designed to\nsample from target distributions with irregular geometry using an adaptive\nscheme. In cases where targets exhibit non-Gaussian behaviour, we propose that\nadaption should be regional in nature as opposed to global. Our algorithms\nminimize the information projection side of the Kullback-Leibler (KL)\ndivergence between the proposal distribution class and the target to encourage\nproposals distributed similarly to the regional geometry of the target. Unlike\ntraditional adaptive MCMC, this procedure rapidly adapts to the geometry of the\ncurrent position as it explores the space without the need for a large batch of\nsamples. We extend this approach to multimodal targets by introducing a heavily\ntempered chain to enable faster mixing between regions of interest. The\ndivergence minimization algorithms are tested on target distributions with\nmultiple irregularly shaped modes and we provide results demonstrating the\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 18:00:19 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dharamshi", "Ameer", ""], ["Ngo", "Vivian", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "2105.00553", "submitter": "Xu Wu", "authors": "Ziyu Xie, Farah Alsafadi, Xu Wu", "title": "Towards Improving the Predictive Capability of Computer Simulations by\n  Integrating Inverse Uncertainty Quantification and Quantitative Validation\n  with Bayesian Hypothesis Testing", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Best Estimate plus Uncertainty (BEPU) approach for nuclear systems\nmodeling and simulation requires that the prediction uncertainty must be\nquantified in order to prove that the investigated design stays within\nacceptance criteria. A rigorous Uncertainty Quantification (UQ) process should\nsimultaneously consider multiple sources of quantifiable uncertainties: (1)\nparameter uncertainty due to randomness or lack of knowledge; (2) experimental\nuncertainty due to measurement noise; (3) model uncertainty caused by\nmissing/incomplete physics and numerical approximation errors, and (4) code\nuncertainty when surrogate models are used. In this paper, we propose a\ncomprehensive framework to integrate results from inverse UQ and quantitative\nvalidation to provide robust predictions so that all these sources of\nuncertainties can be taken into consideration.\n  Inverse UQ quantifies the parameter uncertainties based on experimental data\nwhile taking into account uncertainties from model, code and measurement. In\nthe validation step, we use a quantitative validation metric based on Bayesian\nhypothesis testing. The resulting metric, called the Bayes factor, is then used\nto form weighting factors to combine the prior and posterior knowledge of the\nparameter uncertainties in a Bayesian model averaging process. In this way,\nmodel predictions will be able to integrate the results from inverse UQ and\nvalidation to account for all available sources of uncertainties. This\nframework is a step towards addressing the ANS Nuclear Grand Challenge on\n\"Simulation/Experimentation\" by bridging the gap between models and data.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 21:30:24 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xie", "Ziyu", ""], ["Alsafadi", "Farah", ""], ["Wu", "Xu", ""]]}, {"id": "2105.00700", "submitter": "David Mori\\~na Prof.", "authors": "David Mori\\~na, Pedro Puig and Albert Navarro", "title": "Analysis of zero inflated dichotomous variables from a Bayesian\n  perspective: Application to occupational health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work proposes a new methodology to fit zero inflated Bernoulli data from\na Bayesian approach, able to distinguish between two potential sources of zeros\n(structurals and non-structurals). Its usage is illustrated by means of a real\nexample from the field of occupational health as the phenomenon of sickness\npresenteeism, in which it is reasonable to think that some individuals will\nnever be at risk of suffering it because they have not been sick in the period\nof study (structural zeros). Without separating structural and non-structural\nzeros one would one would be studying jointly the general health status and the\npresenteeism itself, and therefore obtaining potentially biased estimates as\nthe phenomenon is being implicitly underestimated by diluting it into the\ngeneral health status. The proposed methodology performance has been evaluated\nthrough a comprehensive simulation study, and it has been compiled as an R\npackage freely available to the community.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:13:08 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mori\u00f1a", "David", ""], ["Puig", "Pedro", ""], ["Navarro", "Albert", ""]]}, {"id": "2105.00887", "submitter": "Nawaf Bou-Rabee", "authors": "Nawaf Bou-Rabee and Andreas Eberle", "title": "Mixing Time Guarantees for Unadjusted Hamiltonian Monte Carlo", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide quantitative upper bounds on the total variation mixing time of\nthe Markov chain corresponding to the unadjusted Hamiltonian Monte Carlo (uHMC)\nalgorithm. For two general classes of models and fixed time discretization step\nsize $h$, the mixing time is shown to depend only logarithmically on the\ndimension. Moreover, we provide quantitative upper bounds on the total\nvariation distance between the invariant measure of the uHMC chain and the true\ntarget measure. As a consequence, we show that an $\\varepsilon$-accurate\napproximation of the target distribution $\\mu$ in total variation distance can\nbe achieved by uHMC for a broad class of models with\n$O\\left(d^{3/4}\\varepsilon^{-1/2}\\log (d/\\varepsilon )\\right)$ gradient\nevaluations, and for mean field models with weak interactions with\n$O\\left(d^{1/2}\\varepsilon^{-1/2}\\log (d/\\varepsilon )\\right)$ gradient\nevaluations. The proofs are based on the construction of successful couplings\nfor uHMC that realize the upper bounds.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:13:47 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bou-Rabee", "Nawaf", ""], ["Eberle", "Andreas", ""]]}, {"id": "2105.00991", "submitter": "Yingwei Xin", "authors": "Yunwen Chen, Zuotao Liu, Daqi Ji, Yingwei Xin, Wenguang Wang, Lu Yao,\n  Yi Zou", "title": "Context-aware Ensemble of Multifaceted Factorization Models for\n  Recommendation Prediction in Social Networks", "comments": "KDD 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the solution of Shanda Innovations team to Task 1 of\nKDD-Cup 2012. A novel approach called Multifaceted Factorization Models is\nproposed to incorporate a great variety of features in social networks. Social\nrelationships and actions between users are integrated as implicit feedbacks to\nimprove the recommendation accuracy. Keywords, tags, profiles, time and some\nother features are also utilized for modeling user interests. In addition, user\nbehaviors are modeled from the durations of recommendation records. A\ncontext-aware ensemble framework is then applied to combine multiple predictors\nand produce final recommendation results. The proposed approach obtained\n0.43959 (public score) / 0.41874 (private score) on the testing dataset, which\nachieved the 2nd place in the KDD-Cup competition.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:42:50 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Yunwen", ""], ["Liu", "Zuotao", ""], ["Ji", "Daqi", ""], ["Xin", "Yingwei", ""], ["Wang", "Wenguang", ""], ["Yao", "Lu", ""], ["Zou", "Yi", ""]]}, {"id": "2105.01039", "submitter": "Christian Staerk", "authors": "Christian Staerk, Maria Kateri, Ioannis Ntzoufras", "title": "A Metropolized adaptive subspace algorithm for high-dimensional Bayesian\n  variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple and efficient adaptive Markov Chain Monte Carlo (MCMC) method,\ncalled the Metropolized Adaptive Subspace (MAdaSub) algorithm, is proposed for\nsampling from high-dimensional posterior model distributions in Bayesian\nvariable selection. The MAdaSub algorithm is based on an independent\nMetropolis-Hastings sampler, where the individual proposal probabilities of the\nexplanatory variables are updated after each iteration using a form of Bayesian\nadaptive learning, in a way that they finally converge to the respective\ncovariates' posterior inclusion probabilities. We prove the ergodicity of the\nalgorithm and present a parallel version of MAdaSub with an adaptation scheme\nfor the proposal probabilities based on the combination of information from\nmultiple chains. The effectiveness of the algorithm is demonstrated via various\nsimulated and real data examples, including a high-dimensional problem with\nmore than 20,000 covariates.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:37:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Staerk", "Christian", ""], ["Kateri", "Maria", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "2105.01185", "submitter": "Jo\\~ao Ribeiro Medeiros", "authors": "Jo\\~ao R. Medeiros and S\\'ilvio M. Duarte Queir\\'os", "title": "Effective temperatures for single particle system under dichotomous\n  noise", "comments": null, "journal-ref": "J. Stat. Mech. (2021) 063205", "doi": "10.1088/1742-5468/ac014e", "report-no": null, "categories": "cond-mat.stat-mech physics.class-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three different definitions of effective temperature -- $\\mathcal{T}_{\\rm\nk}$, $\\mathcal{T}_{\\rm i}$ and $\\mathcal{T}_{\\rm r}$ related to kinetic theory,\nsystem entropy and response theory, respectively -- are applied in the\ndescription of a non-equilibrium generalised massive Langevin model in contact\nwith dichotomous noise. The differences between the definitions of\n$\\mathcal{T}$ naturally wade out as the reservoir reaches its white-noise\nlimit, approaching Gaussian features. The same framework is employed in its\noverdamped version as well, showing the loss of inertial contributions to the\ndynamics of the system also makes the three mentioned approaches for effective\ntemperature equivalent.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:39:10 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Medeiros", "Jo\u00e3o R.", ""], ["Queir\u00f3s", "S\u00edlvio M. Duarte", ""]]}, {"id": "2105.01557", "submitter": "Jordi Tur", "authors": "Jordi Tur (Centre de Recerca Matem\\`atica), David Mori\\~na (Department\n  of Econometrics, Statistics and Applied Economics, Riskcenter-IREA,\n  Universitat de Barcelona), Pedro Puig (Barcelona Graduate School of\n  Mathematics (BGSMath), Departament de Matem\\`atiques, Universitat Aut\\`onoma\n  de Barcelona), Alejandra Caba\\~na (Barcelona Graduate School of Mathematics\n  (BGSMath), Departament de Matem\\`atiques, Universitat Aut\\`onoma de\n  Barcelona), Argimiro Arratia (Department of Computer Science, Universitat\n  Polit\\`ecnica de Catalunya), Amanda Fern\\'andez-Fontelo (Chair of Statistics,\n  School of Business and Economics, Humboldt-Universit\\\"at zu Berlin)", "title": "Good distribution modelling with the R package good", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although models for count data with over-dispersion have been widely\nconsidered in the literature, models for under-dispersion -- the opposite\nphenomenon -- have received less attention as it is only relatively common in\nparticular research fields such as biodosimetry and ecology. The Good\ndistribution is a flexible alternative for modelling count data showing either\nover-dispersion or under-dispersion, although no R packages are still available\nto the best of our knowledge. We aim to present in the following the R package\ngood that computes the standard probabilistic functions (i.e., probability\ndensity function, cumulative distribution function, and quantile function) and\ngenerates random samples from a population following a Good distribution. The\npackage also considers a function for Good regression, including covariates in\na similar way to that of the standard glm function. We finally show the use of\nsuch a package with some real-world data examples addressing both\nover-dispersion and especially under-dispersion.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:14:03 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tur", "Jordi", "", "Centre de Recerca Matem\u00e0tica"], ["Mori\u00f1a", "David", "", "Department\n  of Econometrics, Statistics and Applied Economics, Riskcenter-IREA,\n  Universitat de Barcelona"], ["Puig", "Pedro", "", "Barcelona Graduate School of\n  Mathematics"], ["Caba\u00f1a", "Alejandra", "", "Barcelona Graduate School of Mathematics"], ["Arratia", "Argimiro", "", "Department of Computer Science, Universitat\n  Polit\u00e8cnica de Catalunya"], ["Fern\u00e1ndez-Fontelo", "Amanda", "", "Chair of Statistics,\n  School of Business and Economics, Humboldt-Universit\u00e4t zu Berlin"]]}, {"id": "2105.01576", "submitter": "Jorge Ramirez", "authors": "Juan M. Restrepo, Jorge M. Ramirez", "title": "Homotopy Sampling, with an Application to Particle Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a homotopy sampling procedure, loosely based on importance\nsampling. Starting from a known probability distribution, the homotopy\nprocedure generates the unknown normalization of a target distribution. In the\ncontext of stationary distributions that are associated with physical systems\nthe method is an alternative way to estimate an unknown microcanonical\nensemble. The process is iterative and also generates samples from the target\ndistribution. In practice, the homotopy procedure does not circumvent using\nsample averages in the estimation of the normalization constant. The error in\nthe procedure depends on the errors incurred in sample averaging and the number\nof stages used in the computational implementation of the process. However, we\nshow that it is possible to exchange the number of homotopy stages and the\ntotal number of samples needed at each stage in order to enhance the\ncomputational efficiency of the implemented algorithm. Estimates of the error\nas a function of stages and sample averages are derived. These could guide\ncomputational efficiency decisions on how the calculation would be mapped to a\ngiven computer architecture. Consideration is given to how the procedure can be\nadapted to Bayesian estimation problems, both stationary and non-stationary.\nEmphasis is placed on the non-stationary problems, and in particular, on a\nsequential estimation technique known as particle filtering. It is shown that a\nmodification of the particle filter framework to include the homotopy process\ncan improve the computational robustness of particle filters. The homotopy\nprocess can ameliorate particle filter collapse, a common challenge to using\nparticle filters when the sample dimension is small compared with the state\nspace dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:47:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Restrepo", "Juan M.", ""], ["Ramirez", "Jorge M.", ""]]}, {"id": "2105.02140", "submitter": "Luiza Piancastelli", "authors": "Luiza Piancastelli, Nial Friel, Julie Vercelloni, Kerrie Mengersen,\n  Antonietta Mira", "title": "A Bayesian latent allocation model for clustering compositional data\n  with application to the Great Barrier Reef", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative abundance is a common metric to estimate the composition of species\nin ecological surveys reflecting patterns of commonness and rarity of\nbiological assemblages. Measurements of coral reef compositions formed by four\ncommunities along Australia's Great Barrier Reef (GBR) gathered between 2012\nand 2017 are the focus of this paper. We undertake the task of finding clusters\nof transect locations with similar community composition and investigate\nchanges in clustering dynamics over time. During these years, an unprecedented\nsequence of extreme weather events (cyclones and coral bleaching) impacted the\n58 surveyed locations. The dependence between constituent parts of a\ncomposition presents a challenge for existing multivariate clustering\napproaches. In this paper, we introduce a finite mixture of Dirichlet\ndistributions with group-specific parameters, where cluster memberships are\ndictated by unobserved latent variables. The inference is carried in a Bayesian\nframework, where MCMC strategies are outlined to sample from the posterior\nmodel. Simulation studies are presented to illustrate the performance of the\nmodel in a controlled setting. The application of the model to the 2012 coral\nreef data reveals that clusters were spatially distributed in similar ways\nacross reefs which indicates a potential influence of wave exposure at the\norigin of coral reef community composition. The number of clusters estimated by\nthe model decreased from four in 2012 to two from 2014 until 2017. Posterior\nprobabilities of transect allocations to the same cluster substantially\nincrease through time showing a potential homogenization of community\ncomposition across the whole GBR. The Bayesian model highlights the diversity\nof coral reef community composition within a coral reef and rapid changes\nacross large spatial scales that may contribute to undermining the future of\nthe GBR's biodiversity.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:47:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Piancastelli", "Luiza", ""], ["Friel", "Nial", ""], ["Vercelloni", "Julie", ""], ["Mengersen", "Kerrie", ""], ["Mira", "Antonietta", ""]]}, {"id": "2105.02211", "submitter": "Ivan Jericevich", "authors": "Ivan Jericevich and Patrick Chang and Tim Gebbie", "title": "Simulation and estimation of a point-process market-model with a\n  matching engine", "comments": "19 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR q-fin.CP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The extent to which a matching engine can cloud the modelling of underlying\norder submission and management processes in a financial market remains an\nunanswered concern with regards to market models. Here we consider a 10-variate\nHawkes process with simple rules to simulate common order types which are\nsubmitted to a matching engine. Hawkes processes can be used to model the time\nand order of events, and how these events relate to each other. However, they\nprovide a freedom with regards to implementation mechanics relating to the\nprices and volumes of injected orders. This allows us to consider a reference\nHawkes model and two additional models which have rules that change the\nbehaviour of limit orders. The resulting trade and quote data from the\nsimulations are then calibrated and compared with the original order generating\nprocess to determine the extent with which implementation rules can distort\nmodel parameters. Evidence from validation and hypothesis tests suggest that\nthe true model specification can be significantly distorted by market\nmechanics, and that practical considerations not directly due to model\nspecification can be important with regards to model identification within an\ninherently asynchronous trading environment.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:38:27 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Jericevich", "Ivan", ""], ["Chang", "Patrick", ""], ["Gebbie", "Tim", ""]]}, {"id": "2105.02499", "submitter": "Xavier de Luna", "authors": "Mohammad Ghasempour and Xavier de Luna", "title": "SDRcausal: an R package for causal inference based on sufficient\n  dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SDRcausal is a package that implements sufficient dimension reduction methods\nfor causal inference as proposed in Ghosh, Ma, and de Luna (2021). The package\nimplements (augmented) inverse probability weighting and outcome regression\n(imputation) estimators of an average treatment effect (ATE) parameter.\nNuisance models, both treatment assignment probability given the covariates\n(propensity score) and outcome regression models, are fitted by using\nsemiparametric locally efficient dimension reduction estimators, thereby\nallowing for large sets of confounding covariates. Techniques including linear\nextrapolation, numerical differentiation, and truncation have been used to\nobtain a practicable implementation of the methods. Finding the suitable\ndimension reduction map (central mean subspace) requires solving an\noptimization problem, and several optimization algorithms are given as choices\nto the user. The package also provides estimators of the asymptotic variances\nof the causal effect estimators implemented. Plotting options are provided. The\ncore of the methods are implemented in C language, and parallelization is\nallowed for. The user-friendly and freeware R language is used as interface.\nThe package can be downloaded from Github repository:\nhttps://github.com/stat4reg.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:05:15 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ghasempour", "Mohammad", ""], ["de Luna", "Xavier", ""]]}, {"id": "2105.02579", "submitter": "Fernando Llorente Fern\\'andez", "authors": "F. Llorente, E. Curbelo, L. Martino, V. Elvira, D. Delgado", "title": "MCMC-driven importance samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods are the standard procedure for estimating complicated\nintegrals of multidimensional Bayesian posterior distributions. In this work,\nwe focus on LAIS, a class of adaptive importance samplers where Markov chain\nMonte Carlo (MCMC) algorithms are employed to drive an underlying multiple\nimportance sampling (IS) scheme. Its power lies in the simplicity of the\nlayered framework: the upper layer locates proposal densities by means of MCMC\nalgorithms; while the lower layer handles the multiple IS scheme, in order to\ncompute the final estimators. The modular nature of LAIS allows for different\npossible choices in the upper and lower layers, that will have different\nperformance and computational costs. In this work, we propose different\nenhancements in order to increase the efficiency and reduce the computational\ncost, of both upper and lower layers. The different variants are essential if\nwe aim to address computational challenges arising in real-world applications,\nsuch as highly concentrated posterior distributions (due to large amounts of\ndata, etc.). Hamiltonian-driven importance samplers are presented and tested.\nFurthermore, we introduce different strategies for designing cheaper schemes,\nfor instance, recycling samples generated in the upper layer and using them in\nthe final estimators in the lower layer. Numerical experiments show the\nbenefits of the proposed schemes as compared to the vanilla version of LAIS and\nother benchmark methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:59:02 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 14:15:51 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 07:50:28 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Llorente", "F.", ""], ["Curbelo", "E.", ""], ["Martino", "L.", ""], ["Elvira", "V.", ""], ["Delgado", "D.", ""]]}, {"id": "2105.02952", "submitter": "Kentaro Hoffman", "authors": "Kentaro Hoffman, Jan Hannig, Kai Zhang", "title": "Comments on: A Gibbs sampler for a class of random convex polytopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this comment we discuss relative strengths and weaknesses of simplex and\nDirichlet Dempster-Shafer inference as applied to multi-resolution tests of\nindependence.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:39:34 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hoffman", "Kentaro", ""], ["Hannig", "Jan", ""], ["Zhang", "Kai", ""]]}, {"id": "2105.03228", "submitter": "Jocelyn Chi", "authors": "Jocelyn T. Chi, Ilse C. F. Ipsen, Tzu-Hung Hsiao, Ching-Heng Lin,\n  Li-San Wang, Wan-Ping Lee, Tzu-Pin Lu, Jung-Ying Tzeng", "title": "SEAGLE: A Scalable Exact Algorithm for Large-Scale Set-Based GxE Tests\n  in Biobank Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The explosion of biobank data offers immediate opportunities for\ngene-environment (GxE) interaction studies of complex diseases because of the\nlarge sample sizes and the rich collection in genetic and non-genetic\ninformation. However, the extremely large sample size also introduces new\ncomputational challenges in GxE assessment, especially for set-based GxE\nvariance component (VC) tests, which are a widely used strategy to boost\noverall GxE signals and to evaluate the joint GxE effect of multiple variants\nfrom a biologically meaningful unit (e.g., gene). In this work, we focus on\ncontinuous traits and present SEAGLE, a Scalable Exact AlGorithm for\nLarge-scale set-based GxE tests, to permit GxE VC tests for biobank-scale data.\nSEAGLE employs modern matrix computations to achieve the same \"exact\" results\nas the original GxE VC tests without imposing additional assumptions or relying\non approximations. SEAGLE can easily accommodate sample sizes in the order of\n$10^5$, is implementable on standard laptops, and does not require specialized\ncomputing equipment. We demonstrate SEAGLE's performance through extensive\nsimulations. We illustrate its utility by conducting genome-wide gene-based GxE\nanalysis on the Taiwan Biobank data to explore the interaction of gene and\nphysical activity status on body mass index.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:01:12 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 02:39:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chi", "Jocelyn T.", ""], ["Ipsen", "Ilse C. F.", ""], ["Hsiao", "Tzu-Hung", ""], ["Lin", "Ching-Heng", ""], ["Wang", "Li-San", ""], ["Lee", "Wan-Ping", ""], ["Lu", "Tzu-Pin", ""], ["Tzeng", "Jung-Ying", ""]]}, {"id": "2105.03309", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i", "title": "Estimating latent linear correlations from fuzzy frequency tables", "comments": "27 pages, 5 figures, 9 tables, 2 supplementary figures, 2\n  supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research concerns the estimation of latent linear or polychoric\ncorrelations from fuzzy frequency tables. Fuzzy counts are of particular\ninterest to many disciplines including social and behavioral sciences, and are\nespecially relevant when observed data are classified using fuzzy categories -\nas for socio-economic studies, clinical evaluations, content analysis,\ninter-rater reliability analysis - or when imprecise observations are\nclassified into either precise or imprecise categories - as for the analysis of\nratings data or fuzzy coded variables. In these cases, the space of count\nmatrices is no longer defined over naturals and, consequently, the polychoric\nestimator cannot be used to accurately estimate latent linear correlations. The\naim of this contribution is twofold. First, we illustrate a computational\nprocedure based on generalized natural numbers for computing fuzzy frequencies.\nSecond, we reformulate the problem of estimating latent linear correlations\nfrom fuzzy counts in the context of Expectation-Maximization based maximum\nlikelihood estimation. A simulation study and two applications are used to\ninvestigate the characteristics of the proposed method. Overall, the results\nshow that the fuzzy EM-based polychoric estimator is more efficient to deal\nwith imprecise count data as opposed to standard polychoric estimators that may\nbe used in this context.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:00:57 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""]]}, {"id": "2105.03481", "submitter": "Christophe Ley", "authors": "Andreas Anastasiou, Alessandro Barp, Fran\\c{c}ois-Xavier Briol, Bruno\n  Ebner, Robert E. Gaunt, Fatemeh Ghaderinezhad, Jackson Gorham, Arthur\n  Gretton, Christophe Ley, Qiang Liu, Lester Mackey, Chris. J. Oates, Gesine\n  Reinert, Yvik Swan", "title": "Stein's Method Meets Statistics: A Review of Some Recent Developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein's method is a collection of tools for analysing distributional\ncomparisons through the study of a class of linear operators called Stein\noperators. Originally studied in probability, Stein's method has also enabled\nsome important developments in statistics. This early success has led to a high\nresearch activity in this area in recent years. The goal of this survey is to\nbring together some of these developments in theoretical statistics as well as\nin computational statistics and, in doing so, to stimulate further research\ninto the successful field of Stein's method and statistics. The topics we\ndiscuss include: explicit error bounds for asymptotic approximations of\nestimators and test statistics, a measure of prior sensitivity in Bayesian\nstatistics, tools to benchmark and compare sampling methods such as approximate\nMarkov chain Monte Carlo, deterministic alternatives to sampling methods,\ncontrol variate techniques, and goodness-of-fit testing.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 19:55:14 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Anastasiou", "Andreas", ""], ["Barp", "Alessandro", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Ebner", "Bruno", ""], ["Gaunt", "Robert E.", ""], ["Ghaderinezhad", "Fatemeh", ""], ["Gorham", "Jackson", ""], ["Gretton", "Arthur", ""], ["Ley", "Christophe", ""], ["Liu", "Qiang", ""], ["Mackey", "Lester", ""], ["Oates", "Chris. J.", ""], ["Reinert", "Gesine", ""], ["Swan", "Yvik", ""]]}, {"id": "2105.03651", "submitter": "Anirban Mondal", "authors": "Anirban Mondal and Bani Mallick", "title": "Spline-Based Bayesian Emulators for Large Scale Spatial Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian approach to nonlinear inverse problems is considered where the\nunknown quantity (input) is a random spatial field. The forward model is\ncomplex and non-linear, therefore computationally expensive. An emulator-based\nmethodology is developed, where the Bayesian multivariate adaptive regression\nsplines (BMARS) are used to model the function that maps the inputs to the\noutputs. Discrete cosine transformation (DCT) is used for dimension reduction\nof the input spatial field. The posterior sampling is carried out using\ntrans-dimensional Markov Chain Monte Carlo (MCMC) methods. Numerical results\nare presented by analyzing simulated as well as real data on hydrocarbon\nreservoir characterization.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:28:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Mondal", "Anirban", ""], ["Mallick", "Bani", ""]]}, {"id": "2105.04134", "submitter": "Daniel Barreiro Ures", "authors": "D. Barreiro-Ures, R. Cao and M. Francisco-Fern\\'andez", "title": "Bagging cross-validated bandwidth selection in nonparametric regression\n  estimation with applications to large-sized samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-validation is a well-known and widely used bandwidth selection method\nin nonparametric regression estimation. However, this technique has two\nremarkable drawbacks: (i) the large variability of the selected bandwidths, and\n(ii) the inability to provide results in a reasonable time for very large\nsample sizes. To overcome these problems, bagging cross-validation bandwidths\nare analyzed in this paper. This approach consists in computing the\ncross-validation bandwidths for a finite number of subsamples and then\nrescaling the averaged smoothing parameters to the original sample size. Under\na random-design regression model, asymptotic expressions up to a second-order\nfor the bias and variance of the leave-one-out cross-validation bandwidth for\nthe Nadaraya--Watson estimator are obtained. Subsequently, the asymptotic bias\nand variance and the limit distribution are derived for the bagged\ncross-validation selector. Suitable choices of the number of subsamples and the\nsubsample size lead to an $n^{-1/2}$ rate for the convergence in distribution\nof the bagging cross-validation selector, outperforming the rate $n^{-3/10}$ of\nleave-one-out cross-validation. Several simulations and an illustration on a\nreal dataset related to the COVID-19 pandemic show the behavior of our proposal\nand its better performance, in terms of statistical efficiency and computing\ntime, when compared to leave-one-out cross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:31:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Barreiro-Ures", "D.", ""], ["Cao", "R.", ""], ["Francisco-Fern\u00e1ndez", "M.", ""]]}, {"id": "2105.04374", "submitter": "Quan Vu", "authors": "Quan Vu, Matthew T. Moores, Andrew Zammit-Mangion", "title": "Warped Gradient-Enhanced Gaussian Process Surrogate Models for Inference\n  with Intractable Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo methods for intractable likelihoods, such as the\nexchange algorithm, require simulations of the sufficient statistics at every\niteration of the Markov chain, which often result in expensive computations.\nSurrogate models for the likelihood function have been developed to accelerate\ninference algorithms in this context. However, these surrogate models tend to\nbe relatively inflexible, and often provide a poor approximation to the true\nlikelihood function. In this article, we propose the use of a warped,\ngradient-enhanced, Gaussian process surrogate model for the likelihood\nfunction, which jointly models the sample means and variances of the sufficient\nstatistics, and uses warping functions to capture covariance nonstationarity in\nthe input parameter space. We show that both the consideration of\nnonstationarity and the inclusion of gradient information can be leveraged to\nobtain a surrogate model that outperforms the conventional stationary Gaussian\nprocess surrogate model when making inference, particularly in regions where\nthe likelihood function exhibits a phase transition. We also show that the\nproposed surrogate model can be used to improve the effective sample size per\nunit time when embedded in exact inferential algorithms. The utility of our\napproach in speeding up inferential algorithms is demonstrated on simulated and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:55:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Vu", "Quan", ""], ["Moores", "Matthew T.", ""], ["Zammit-Mangion", "Andrew", ""]]}, {"id": "2105.04379", "submitter": "Steven Kleinegesse", "authors": "Steven Kleinegesse and Michael U. Gutmann", "title": "Gradient-based Bayesian Experimental Design for Implicit Models using\n  Mutual Information Lower Bounds", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a framework for Bayesian experimental design (BED) with implicit\nmodels, where the data-generating distribution is intractable but sampling from\nit is still possible. In order to find optimal experimental designs for such\nmodels, our approach maximises mutual information lower bounds that are\nparametrised by neural networks. By training a neural network on sampled data,\nwe simultaneously update network parameters and designs using stochastic\ngradient-ascent. The framework enables experimental design with a variety of\nprominent lower bounds and can be applied to a wide range of scientific tasks,\nsuch as parameter estimation, model discrimination and improving future\npredictions. Using a set of intractable toy models, we provide a comprehensive\nempirical comparison of prominent lower bounds applied to the aforementioned\ntasks. We further validate our framework on a challenging system of stochastic\ndifferential equations from epidemiology.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:59:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kleinegesse", "Steven", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "2105.04427", "submitter": "Martina Han\\v{c}ov\\'a", "authors": "Martina Han\\v{c}ov\\'a, Andrej Gajdo\\v{s} and Jozef Han\\v{c}", "title": "A practical, effective calculation of gamma difference distributions\n  with open data science tools", "comments": "26 pages, 4 figures, 2 tables, 1 box, corrected typos in text and\n  tables, more precise runtimes for MMA and MTB, added references, correct\n  links for online materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, there is still no officially accepted and extensively verified\nimplementation of computing the gamma difference distribution allowing unequal\nshape parameters. We explore four computational ways of the gamma difference\ndistribution with the different shape parameters resulting from time series\nkriging, a forecasting approach based on the best linear unbiased prediction,\nand linear mixed models. The results of our numerical study, with emphasis on\nusing open data science tools, demonstrate that our open tool implemented in\nhigh-performance Python(with Numba) is exponentially fast, highly accurate, and\nvery reliable. It combines numerical inversion of the characteristic function\nand the trapezoidal rule with the double exponential oscillatory transformation\n(DE quadrature). At the double 53-bit precision, our tool outperformed the\nspeed of the analytical computation based on Tricomi's $U(a, b, z)$ function in\nCAS software (commercial Mathematica, open SageMath) by 1.5-2 orders. At the\nprecision of scientific numerical computational tools, it exceeded open SciPy,\nNumPy, and commercial MATLAB 5-10 times. The potential future application of\nour tool for a mixture of characteristic functions could open new possibilities\nfor fast data analysis based on exact probability distributions in areas like\nmultidimensional statistics, measurement uncertainty analysis in metrology as\nwell as in financial mathematics and risk analysis.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:38:07 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 15:51:17 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Han\u010dov\u00e1", "Martina", ""], ["Gajdo\u0161", "Andrej", ""], ["Han\u010d", "Jozef", ""]]}, {"id": "2105.04599", "submitter": "Yiming Xu", "authors": "Yiming Xu, Akil Narayan", "title": "Budget-limited distribution learning in multifidelity problems", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multifidelity methods are widely used for statistical estimation of\nquantities of interest (QoIs) in uncertainty quantification using simulation\ncodes of differing costs and accuracies. Many methods approximate\nnumerical-valued statistics that represent only limited information of the\nQoIs. In this paper, we introduce a semi-parametric approach that aims to\neffectively describe the distribution of a scalar-valued QoI in the\nmultifidelity setup. Under a linear model hypothesis, we propose an\nexploration-exploitation strategy to reconstruct the full distribution of a\nscalar-valued QoI using samples from a subset of low-fidelity regressors. We\nderive an informative asymptotic bound for the mean 1-Wasserstein distance\nbetween the estimator and the true distribution, and use it to adaptively\nallocate computational budget for parametric estimation and non-parametric\nreconstruction. Assuming the linear model is correct, we prove that such a\nprocedure is consistent, and converges to the optimal policy (and hence optimal\ncomputational budget allocation) under an upper bound criterion as the budget\ngoes to infinity. A major advantage of our approach compared to several other\nmultifidelity methods is that it is automatic, and its implementation does not\nrequire a hierarchical model setup, cross-model information, or \\textit{a\npriori} known model statistics. Numerical experiments are provided in the end\nto support our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:29:43 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Xu", "Yiming", ""], ["Narayan", "Akil", ""]]}, {"id": "2105.04789", "submitter": "Mohammad Javad Davoudabadi Mr", "authors": "Mohammad Javad Davoudabadi, Daniel Pagendam, Christopher Drovandi,\n  Jeff Baldock, Gentry White", "title": "Modelling and predicting soil carbon sequestration: is current model\n  structure fit for purpose?", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soil carbon accounting and prediction play a key role in building decision\nsupport systems for land managers selling carbon credits, in the spirit of the\nParis and Kyoto protocol agreements. Land managers typically rely on\ncomputationally complex models fit using sparse datasets to make these\naccountings and predictions. The model complexity and sparsity of the data can\nlead to over-fitting, leading to inaccurate results using new data or making\npredictions. Modellers address over-fitting by simplifying their models,\nneglecting some soil organic carbon (SOC) components. In this study, we\nintroduce two novel SOC models and a new RothC-like model and investigate how\nthe SOC components and complexity of the SOC models affect the SOC prediction\nin the presence of small and sparse time series data. We develop model\nselection methods that can identify the soil carbon model with the best\npredictive performance, in light of the available data. Through this analysis\nwe reveal that commonly used complex soil carbon models can over-fit in the\npresence of sparse time series data, and our simpler models can produce more\naccurate predictions.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 05:22:05 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Davoudabadi", "Mohammad Javad", ""], ["Pagendam", "Daniel", ""], ["Drovandi", "Christopher", ""], ["Baldock", "Jeff", ""], ["White", "Gentry", ""]]}, {"id": "2105.04829", "submitter": "Rose Baker", "authors": "Rose Baker", "title": "Estimating accurate covariance matrices on fitted model parameters", "comments": "14 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The accurate computation of the covariance matrix of fitted model parameters\nis a somewhat neglected task in Statistics. Algorithms are given for computing\naccurate covariance matrices derived from computing the Hessian matrix by\nnumerical differentiation, and also for the covariance matrix of the posterior\ndistribution of model parameters. Evaluations on two datasets where the Hessian\ncould be computed analytically show that the numerical differentiation\nalgorithm is very accurate.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:33:50 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Baker", "Rose", ""]]}, {"id": "2105.04912", "submitter": "Jeremie Houssineau", "authors": "Jeremy Heng, Jeremie Houssineau and Ajay Jasra", "title": "On Unbiased Score Estimation for Partially Observed Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of statistical inference for a class of\npartially-observed diffusion processes, with discretely-observed data and\nfinite-dimensional parameters. We construct unbiased estimators of the score\nfunction, i.e. the gradient of the log-likelihood function with respect to\nparameters, with no time-discretization bias. These estimators can be\nstraightforwardly employed within stochastic gradient methods to perform\nmaximum likelihood estimation or Bayesian inference. As our proposed\nmethodology only requires access to a time-discretization scheme such as the\nEuler-Maruyama method, it is applicable to a wide class of diffusion processes\nand observation models. Our approach is based on a representation of the score\nas a smoothing expectation using Girsanov theorem, and a novel adaptation of\nthe randomization schemes developed in Mcleish [2011], Rhee and Glynn [2015],\nJacob et al. [2020a]. This allows one to remove the time-discretization bias\nand burn-in bias when computing smoothing expectations using the conditional\nparticle filter of Andrieu et al. [2010]. Central to our approach is the\ndevelopment of new couplings of multiple conditional particle filters. We prove\nunder assumptions that our estimators are unbiased and have finite variance.\nThe methodology is illustrated on several challenging applications from\npopulation ecology and neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:58:12 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Heng", "Jeremy", ""], ["Houssineau", "Jeremie", ""], ["Jasra", "Ajay", ""]]}, {"id": "2105.05031", "submitter": "Kyriakos Flouris", "authors": "Kyriakos Flouris, Anna Volokitin, Gustav Bredell, Ender Konukoglu", "title": "Gradient flow encoding with distance optimization adaptive step size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The autoencoder model uses an encoder to map data samples to a lower\ndimensional latent space and then a decoder to map the latent space\nrepresentations back to the data space. Implicitly, it relies on the encoder to\napproximate the inverse of the decoder network, so that samples can be mapped\nto and back from the latent space faithfully. This approximation may lead to\nsub-optimal latent space representations. In this work, we investigate a\ndecoder-only method that uses gradient flow to encode data samples in the\nlatent space. The gradient flow is defined based on a given decoder and aims to\nfind the optimal latent space representation for any given sample through\noptimisation, eliminating the need of an approximate inversion through an\nencoder. Implementing gradient flow through ordinary differential equations\n(ODE), we leverage the adjoint method to train a given decoder. We further show\nempirically that the costly integrals in the adjoint method may not be entirely\nnecessary. Additionally, we propose a $2^{nd}$ order ODE variant to the method,\nwhich approximates Nesterov's accelerated gradient descent, with faster\nconvergence per iteration. Commonly used ODE solvers can be quite sensitive to\nthe integration step-size depending on the stiffness of the ODE. To overcome\nthe sensitivity for gradient flow encoding, we use an adaptive solver that\nprioritises minimising loss at each integration step. We assess the proposed\nmethod in comparison to the autoencoding model. In our experiments, GFE showed\na much higher data-efficiency than the autoencoding model, which can be crucial\nfor data scarce applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:38:23 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Flouris", "Kyriakos", ""], ["Volokitin", "Anna", ""], ["Bredell", "Gustav", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2105.05213", "submitter": "Oluwasegun Ojo", "authors": "Oluwasegun Ojo, Rosa E. Lillo, Antonio Fern\\'andez Anta", "title": "Outlier Detection for Functional Data with R Package fdaoutlier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Outlier detection is one of the standard exploratory analysis tasks in\nfunctional data analysis. We present the R package fdaoutlier which contains\nimplementations of some of the latest techniques for detecting functional\noutliers. The package makes it easy to detect different types of outliers\n(magnitude, shape, and amplitude) in functional data, and some of the\nimplemented methods can be applied to both univariate and multivariate\nfunctional data. We illustrate the main functionality of the R package with\ncommon functional datasets in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:28:53 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ojo", "Oluwasegun", ""], ["Lillo", "Rosa E.", ""], ["Anta", "Antonio Fern\u00e1ndez", ""]]}, {"id": "2105.05334", "submitter": "Jem Corcoran", "authors": "J. N. Mueller, J. N. Corcoran", "title": "Coupling from the Past for the Stochastic Simulation of Chemical\n  Reaction Networks", "comments": "27 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chemical reaction networks (CRNs) are fundamental computational models used\nto study the behavior of chemical reactions in well-mixed solutions. They have\nbeen used extensively to model a broad range of biological systems, and are\nprimarily used when the more traditional model of deterministic continuous mass\naction kinetics is invalid due to small molecular counts. We present a perfect\nsampling algorithm to draw error-free samples from the stationary distributions\nof stochastic models for coupled, linear chemical reaction networks. The state\nspaces of such networks are given by all permissible combinations of molecular\ncounts for each chemical species, and thereby grow exponentially with the\nnumbers of species in the network. To avoid simulations involving large numbers\nof states, we propose a subset of chemical species such that coupling of paths\nstarted from these states guarantee coupling of paths started from all states\nin the state space and we show for the well-known Reversible Michaelis-Menten\nmodel that the subset does in fact guarantee perfect draws from the stationary\ndistribution of interest. We compare solutions computed in two ways with this\nalgorithm to those found analytically using the chemical master equation and we\ncompare the distribution of coupling times for the two simulation approaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 20:20:02 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Mueller", "J. N.", ""], ["Corcoran", "J. N.", ""]]}, {"id": "2105.05352", "submitter": "Carson Kent", "authors": "Carson Kent, Jose Blanchet, Peter Glynn", "title": "Frank-Wolfe Methods in Probability Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of Frank-Wolfe algorithms for minimizing\ndifferentiable functionals over probability measures. This framework can be\nshown to encompass a diverse range of tasks in areas such as artificial\nintelligence, reinforcement learning, and optimization. Concrete computational\ncomplexities for these algorithms are established and demonstrate that these\nmethods enjoy convergence in regimes that go beyond convexity and require\nminimal regularity of the underlying functional. Novel techniques used to\nobtain these results also lead to the development of new complexity bounds and\nduality theorems for a family of distributionally robust optimization problems.\nThe performance of our method is demonstrated on several nonparametric\nestimation problems.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 21:58:17 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kent", "Carson", ""], ["Blanchet", "Jose", ""], ["Glynn", "Peter", ""]]}, {"id": "2105.05489", "submitter": "Shumao Zhang", "authors": "Shumao Zhang, Pengchuan Zhang, Thomas Y. Hou", "title": "Multiscale Invertible Generative Networks for High-Dimensional Bayesian\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Multiscale Invertible Generative Network (MsIGN) and associated\ntraining algorithm that leverages multiscale structure to solve\nhigh-dimensional Bayesian inference. To address the curse of dimensionality,\nMsIGN exploits the low-dimensional nature of the posterior, and generates\nsamples from coarse to fine scale (low to high dimension) by iteratively\nupsampling and refining samples. MsIGN is trained in a multi-stage manner to\nminimize the Jeffreys divergence, which avoids mode dropping in\nhigh-dimensional cases. On two high-dimensional Bayesian inverse problems, we\nshow superior performance of MsIGN over previous approaches in posterior\napproximation and multiple mode capture. On the natural image synthesis task,\nMsIGN achieves superior performance in bits-per-dimension over baseline models\nand yields great interpret-ability of its neurons in intermediate layers.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 07:51:47 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhang", "Shumao", ""], ["Zhang", "Pengchuan", ""], ["Hou", "Thomas Y.", ""]]}, {"id": "2105.05648", "submitter": "Johan Larsson", "authors": "Johan Larsson", "title": "Look-Ahead Screening Rules for the Lasso", "comments": "EYSM 2021 short paper; 6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso is a popular method to induce shrinkage and sparsity in the\nsolution vector (coefficients) of regression problems, particularly when there\nare many predictors relative to the number of observations. Solving the lasso\nin this high-dimensional setting can, however, be computationally demanding.\nFortunately, this demand can be alleviated via the use of screening rules that\ndiscard predictors prior to fitting the model, leading to a reduced problem to\nbe solved. In this paper, we present a new screening strategy: look-ahead\nscreening. Our method uses safe screening rules to find a range of penalty\nvalues for which a given predictor cannot enter the model, thereby screening\npredictors along the remainder of the path. In experiments we show that these\nlook-ahead screening rules outperform the active warm-start version of the Gap\nSafe rules.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:27:40 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 10:05:19 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Larsson", "Johan", ""]]}, {"id": "2105.05719", "submitter": "Quan Zhou", "authors": "Quan Zhou, Jun Yang, Dootika Vats, Gareth O. Roberts and Jeffrey S.\n  Rosenthal", "title": "Dimension-free Mixing for High-dimensional Bayesian Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yang et al. (2016) proved that the symmetric random walk Metropolis--Hastings\nalgorithm for Bayesian variable selection is rapidly mixing under mild\nhigh-dimensional assumptions. We propose a novel MCMC sampler using an informed\nproposal scheme, which we prove achieves a much faster mixing time that is\nindependent of the number of covariates, under the same assumptions. To the\nbest of our knowledge, this is the first high-dimensional result which\nrigorously shows that the mixing rate of informed MCMC methods can be fast\nenough to offset the computational cost of local posterior evaluation.\nMotivated by the theoretical analysis of our sampler, we further propose a new\napproach called \"two-stage drift condition\" to studying convergence rates of\nMarkov chains on general state spaces, which can be useful for obtaining tight\ncomplexity bounds in high-dimensional settings. The practical advantages of our\nalgorithm are illustrated by both simulation studies and real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:12:44 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 23:17:48 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zhou", "Quan", ""], ["Yang", "Jun", ""], ["Vats", "Dootika", ""], ["Roberts", "Gareth O.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "2105.05842", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Lester Mackey", "title": "Kernel Thinning", "comments": "Accepted for presentation as an extended abstract at the Conference\n  on Learning Theory (COLT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error in\nthe associated reproducing kernel Hilbert space. With high probability, the\nmaximum discrepancy in integration error is\n$\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported\n$\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log\nn})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an\nequal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$\nintegration error. Our sub-exponential guarantees resemble the classical\nquasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply\nto general distributions on $\\mathbb{R}^d$ and a wide range of common kernels.\nWe use our results to derive explicit non-asymptotic maximum mean discrepancy\nbounds for Gaussian, Mat\\'ern, and B-spline kernels and present two vignettes\nillustrating the practical benefits of kernel thinning over i.i.d. sampling and\nstandard Markov chain Monte Carlo thinning.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:56:42 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 17:59:23 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 20:57:55 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Mackey", "Lester", ""]]}, {"id": "2105.05969", "submitter": "Jonas \\v{S}ukys", "authors": "Jonas \\v{S}ukys and Marco Bacci", "title": "SPUX Framework: a Scalable Package for Bayesian Uncertainty\n  Quantification and Propagation", "comments": "Supplementary Material available as a PDF in submission package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SPUX - a modular framework for Bayesian inference enabling\nuncertainty quantification and propagation in linear and nonlinear,\ndeterministic and stochastic models, and supporting Bayesian model selection.\nSPUX can be coupled to any serial or parallel application written in any\nprogramming language, (e.g. including Python, R, Julia, C/C++, Fortran, Java,\nor a binary executable), scales effortlessly from serial runs on a personal\ncomputer to parallel high performance computing clusters, and aims to provide a\nplatform particularly suited to support and foster reproducibility in\ncomputational science. We illustrate SPUX capabilities for a simple yet\nrepresentative random walk model, describe how to couple different types of\nuser applications, and showcase several readily available examples from\nenvironmental sciences. In addition to available state-of-the-art numerical\ninference algorithms including EMCEE, PMCMC (PF) and SABC, the open source\nnature of the SPUX framework and the explicit description of the hierarchical\nparallel SPUX executors should also greatly simplify the implementation and\nusage of other inference and optimization techniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:16:24 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["\u0160ukys", "Jonas", ""], ["Bacci", "Marco", ""]]}, {"id": "2105.06036", "submitter": "Americo Cunha Jr", "authors": "Eber Dantas, Michel Tosin, Americo Cunha Jr", "title": "Calibration of a SEIR-SEI epidemic model to describe the Zika virus\n  outbreak in Brazil", "comments": null, "journal-ref": "Applied Mathematics and Computation, vol. 338, pp. 249-259, 2018", "doi": "10.1016/j.amc.2018.06.024", "report-no": null, "categories": "q-bio.PE cs.NA math.DS math.NA math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instances of Zika virus epidemic have been reported around the world\nin the last two decades, turning the related illness into an international\nconcern. In this context the use of mathematical models for epidemics is of\ngreat importance, since they are useful tools to study the underlying outbreak\nnumbers and allow one to test the effectiveness of different strategies used to\ncombat the associated diseases. This work deals with the development and\ncalibration of an epidemic model to describe the 2016 outbreak of Zika virus in\nBrazil. A system of 8 differential equations with 8 parameters is employed to\nmodel the evolution of the infection through two populations. Nominal values\nfor the model parameters are estimated from the literature. An inverse problem\nis formulated and solved by comparing the system response to real data from the\noutbreak. The calibrated results presents realistic parameters and returns\nreasonable descriptions, with the curve shape similar to the outbreak evolution\nand peak value close to the highest number of infected people during 2016.\nConsiderations about the lack of data for some initial conditions are also made\nthrough an analysis over the response behavior according to their change in\nvalue.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:51:20 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Dantas", "Eber", ""], ["Tosin", "Michel", ""], ["Cunha", "Americo", "Jr"]]}, {"id": "2105.06618", "submitter": "Mahdi Abolghasemi", "authors": "Mahdi Abolghasemi, Babak Abbasi, Toktam Babaei, Zahra HosseiniFard", "title": "How to effectively use machine learning models to predict the solutions\n  for optimization problems: lessons from loss function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Using machine learning in solving constraint optimization and combinatorial\nproblems is becoming an active research area in both computer science and\noperations research communities. This paper aims to predict a good solution for\nconstraint optimization problems using advanced machine learning techniques. It\nextends the work of \\cite{abbasi2020predicting} to use machine learning models\nfor predicting the solution of large-scaled stochastic optimization models by\nexamining more advanced algorithms and various costs associated with the\npredicted values of decision variables. It also investigates the importance of\nloss function and error criterion in machine learning models where they are\nused for predicting solutions of optimization problems. We use a blood\ntransshipment problem as the case study. The results for the case study show\nthat LightGBM provides promising solutions and outperforms other machine\nlearning models used by \\cite{abbasi2020predicting} specially when mean\nabsolute deviation criterion is used.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 02:14:00 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Abolghasemi", "Mahdi", ""], ["Abbasi", "Babak", ""], ["Babaei", "Toktam", ""], ["HosseiniFard", "Zahra", ""]]}, {"id": "2105.06995", "submitter": "Nathan Hara", "authors": "Nathan C. Hara, Nicolas Unger, Jean-Baptiste Delisle, Rodrigo D\\'iaz,\n  Damien S\\'egransan", "title": "Improving exoplanet detection capabilities with the false inclusion\n  probability. Comparison with other detection criteria in the context of\n  radial velocities", "comments": "Accepted for publication in Astronomy & Astrophysics", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP astro-ph.IM stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Context. In exoplanet searches with radial velocity data, the most common\nstatistical significance metrics are the Bayes factor and the false alarm\nprobability (FAP). Both have proved useful, but do not directly address whether\nan exoplanet detection should be claimed. Furthermore, it is unclear which\ndetection threshold should be taken and how robust the detections are to model\nmisspecification. Aims. The present work aims at defining a detection criterion\nwhich conveys as precisely as possible the information needed to claim an\nexoplanet detection. We compare this new criterion to existing ones in terms of\nsensitivity and robustness. Methods. We define a significance metric called the\nfalse inclusion probability (FIP) based on the posterior probability of\npresence of a planet. Posterior distributions are computed with the nested\nsampling package Polychord. We show that for FIP and Bayes factor calculations,\ndefining priors on linear parameters as Gaussian mixture models allows to\nsignificantly speed up computations. The performances of the FAP, Bayes factor\nand FIP are studied with simulations as well as analytical arguments. We\ncompare the methods assuming the model is correct, then evaluate their\nsensitivity to the prior and likelihood choices. Results. Among other\nproperties, the FIP offers ways to test the reliability of the significance\nlevels, it is particularly efficient to account for aliasing and allows to\nexclude the presence of planets with a certain confidence. We find that, in our\nsimulations, the FIP outperforms existing detection metrics. We show that\nplanet detections are sensitive to priors on period and semi-amplitude and that\nletting free the noise parameters offers better performances than fixing a\nnoise model based on a fit to ancillary indicators.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:59:04 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Hara", "Nathan C.", ""], ["Unger", "Nicolas", ""], ["Delisle", "Jean-Baptiste", ""], ["D\u00edaz", "Rodrigo", ""], ["S\u00e9gransan", "Damien", ""]]}, {"id": "2105.07027", "submitter": "Andrea Scarinci", "authors": "Andrea Scarinci, Michael Fehler and Youssef Marzouk", "title": "Bayesian inference under model misspecification using\n  transport-Lagrangian distances: an application to seismic inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model misspecification constitutes a major obstacle to reliable inference in\nmany inverse problems. Inverse problems in seismology, for example, are\nparticularly affected by misspecification of wave propagation velocities. In\nthis paper, we focus on a specific seismic inverse problem - full-waveform\nmoment tensor inversion - and develop a Bayesian framework that seeks\nrobustness to velocity misspecification. A novel element of our framework is\nthe use of transport-Lagrangian (TL) distances between observed and model\npredicted waveforms to specify a loss function, and the use of this loss to\ndefine a generalized belief update via a Gibbs posterior. The TL distance\nnaturally disregards certain features of the data that are more sensitive to\nmodel misspecification, and therefore produces less biased or dispersed\nposterior distributions in this setting. To make the latter notion precise, we\nuse several diagnostics to assess the quality of inference and uncertainty\nquantification, i.e., continuous rank probability scores and rank histograms.\nWe interpret these diagnostics in the Bayesian setting and compare the results\nto those obtained using more typical Gaussian noise models and squared-error\nloss, under various scenarios of misspecification. Finally, we discuss\npotential generalizability of the proposed framework to a broader class of\ninverse problems affected by model misspecification.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:41:45 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Scarinci", "Andrea", ""], ["Fehler", "Michael", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2105.07900", "submitter": "Kazuma Tsuji", "authors": "Kazuma Tsuji and Ken'ichiro Tanaka", "title": "Acceleration of the kernel herding algorithm by improved gradient\n  approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel herding is a method used to construct quadrature formulas in a\nreproducing kernel Hilbert space. Although there are some advantages of kernel\nherding, such as numerical stability of quadrature and effective outputs of\nnodes and weights, the convergence speed of worst-case integration error is\nslow in comparison to other quadrature methods. To address this problem, we\npropose two improved versions of the kernel herding algorithm. The fundamental\nconcept of both algorithms involves approximating negative gradients with a\npositive linear combination of vertex directions. We analyzed the convergence\nand validity of both algorithms theoretically; in particular, we showed that\nthe approximation of negative gradients directly influences the convergence\nspeed. In addition, we confirmed the accelerated convergence of the worst-case\nintegration error with respect to the number of nodes and computational time\nthrough numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:32:45 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tsuji", "Kazuma", ""], ["Tanaka", "Ken'ichiro", ""]]}, {"id": "2105.07935", "submitter": "Alessandro Casa", "authors": "Alessandro Casa, Andrea Cappozzo, Michael Fop", "title": "Group-wise shrinkage for multiclass Gaussian Graphical Models", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian Graphical Models are widely employed for modelling dependence among\nvariables. Likewise, finite Gaussian mixtures are often the standard way to go\nfor model-based clustering of continuous features. With the increasing\navailability of high-dimensional datasets, a methodological link between these\ntwo approaches has been established in order to provide a framework for\nperforming penalized model-based clustering in the presence of large precision\nmatrices. Notwithstanding, current methodologies do not account for the fact\nthat groups may possess different degrees of association among the variables,\nthus implicitly assuming similar levels of sparsity across the classes. We\novercome this limitation by deriving group-wise penalty factors, automatically\nenforcing under or over-connectivity in the estimated graphs. The approach is\nentirely data-driven and does not require any additional hyper-parameter\nspecification. Simulated data experiments showcase the validity of our\nproposal.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:21:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Casa", "Alessandro", ""], ["Cappozzo", "Andrea", ""], ["Fop", "Michael", ""]]}, {"id": "2105.08026", "submitter": "Thomas Loredo", "authors": "J\\'anos M. Szalai-Gindl, Thomas J. Loredo, Brandon C. Kelly, Istv\\'an\n  Csabai, Tam\\'as Budav\\'ari, L\\'aszl\\'o Dobos", "title": "GPU-Accelerated Hierarchical Bayesian Inference with Application to\n  Modeling Cosmic Populations: CUDAHM", "comments": "28 pages, 7 figures, 2 appendices", "journal-ref": "Abridged version published as \"GPU-accelerated hierarchical\n  Bayesian estimation of luminosity functions using flux-limited observations\n  with photometric noise\" in Astronomy & Computing (2018)", "doi": "10.1016/j.ascom.2018.10.004", "report-no": null, "categories": "astro-ph.IM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a computational framework for hierarchical Bayesian inference\nwith simple (typically single-plate) parametric graphical models that uses\ngraphics processing units (GPUs) to accelerate computations, enabling\ndeployment on very large datasets. Its C++ implementation, CUDAHM (CUDA for\nHierarchical Models) exploits conditional independence between instances of a\nplate, facilitating massively parallel exploration of the replication parameter\nspace using the single instruction, multiple data architecture of GPUs. It\nprovides support for constructing Metropolis-within-Gibbs samplers that iterate\nbetween GPU-accelerated robust adaptive Metropolis sampling of plate-level\nparameters conditional on upper-level parameters, and Metropolis-Hastings\nsampling of upper-level parameters on the host processor conditional on the GPU\nresults. CUDAHM is motivated by demographic problems in astronomy, where\ndensity estimation and linear and nonlinear regression problems must be\naddressed for populations of thousands to millions of objects whose features\nare measured with possibly complex uncertainties. We describe a thinned latent\npoint process framework for modeling such demographic data. We demonstrate\naccurate GPU-accelerated parametric conditional density deconvolution for\nsimulated populations of up to 300,000 objects in ~1 hour using a single NVIDIA\nTesla K40c GPU. Supplementary material provides details about the CUDAHM API\nand the demonstration problem.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:27:03 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Szalai-Gindl", "J\u00e1nos M.", ""], ["Loredo", "Thomas J.", ""], ["Kelly", "Brandon C.", ""], ["Csabai", "Istv\u00e1n", ""], ["Budav\u00e1ri", "Tam\u00e1s", ""], ["Dobos", "L\u00e1szl\u00f3", ""]]}, {"id": "2105.08451", "submitter": "Sourabh Bhattacharya", "authors": "Sourabh Bhattacharya", "title": "Bayesian Levy-Dynamic Spatio-Temporal Process: Towards Big Data Analysis", "comments": "Feedback welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of big data, all scientific disciplines are evolving fast to cope\nup with the enormity of the available information. So is statistics, the queen\nof science. Big data are particularly relevant to spatio-temporal statistics,\nthanks to much-improved technology in satellite based remote sensing and\nGeographical Information Systems. However, none of the existing approaches seem\nto meet the simultaneous demand of reality emulation and cheap computation. In\nthis article, with the Levy random fields as the starting point, e construct a\nnew Bayesian nonparametric, nonstationary and nonseparable dynamic spatio-\ntemporal model with the additional realistic property that the lagged\nspatio-temporal correlations converge to zero as the lag tends to infinity.\nAlthough our Bayesian model seems to be intricately structured and is\nvariable-dimensional with respect to each time index, we are able to devise a\nfast and efficient parallel Markov Chain Monte Carlo (MCMC) algorithm for\nBayesian inference. Our simulation experiment brings out quite encouraging\nperformance from our Bayesian Levy-dynamic approach. We finally apply our\nBayesian Levy-dynamic model and methods to a sea surface temperature dataset\nconsisting of 139,300 data points in space and time. Although not big data in\nthe true sense, this is a large and highly structured data by any standard.\nEven for this large and complex data, our parallel MCMC algorithm, implemented\non 80 processors, generated 110,000 MCMC realizations from the Levy-dynamic\nposterior within a single day, and the resultant Bayesian posterior predictive\nanalysis turned out to be encouraging. Thus, it is not unreasonable to expect\nthat with significantly more computing resources, it is feasible to analyse\nterabytes of spatio-temporal data with our new model and methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 11:45:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bhattacharya", "Sourabh", ""]]}, {"id": "2105.08814", "submitter": "Geoff Boeing", "authors": "Shiqin Liu, Carl Higgs, Jonathan Arundel, Geoff Boeing, Nicholas\n  Cerdera, David Moctezuma, Ester Cerin, Deepti Adlakha, Melanie Lowe, and\n  Billie Giles-Corti", "title": "A Generalized Framework for Measuring Pedestrian Accessibility around\n  the World Using Open Data", "comments": null, "journal-ref": "Geographical Analysis, 2021", "doi": "10.1111/gean.12290", "report-no": null, "categories": "cs.CY econ.GN physics.soc-ph q-fin.EC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian accessibility is an important factor in urban transport and land\nuse policy and critical for creating healthy, sustainable cities. Developing\nand evaluating indicators measuring inequalities in pedestrian accessibility\ncan help planners and policymakers benchmark and monitor the progress of city\nplanning interventions. However, measuring and assessing indicators of urban\ndesign and transport features at high resolution worldwide to enable city\ncomparisons is challenging due to limited availability of official, high\nquality, and comparable spatial data, as well as spatial analysis tools\noffering customizable frameworks for indicator construction and analysis. To\naddress these challenges, this study develops an open source software framework\nto construct pedestrian accessibility indicators for cities using open and\nconsistent data. It presents a generalized method to consistently measure\npedestrian accessibility at high resolution and spatially aggregated scale, to\nallow for both within- and between-city analyses. The open source and open data\nmethods developed in this study can be extended to other cities worldwide to\nsupport local planning and policymaking. The software is made publicly\navailable for reuse in an open repository.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:22:58 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Liu", "Shiqin", ""], ["Higgs", "Carl", ""], ["Arundel", "Jonathan", ""], ["Boeing", "Geoff", ""], ["Cerdera", "Nicholas", ""], ["Moctezuma", "David", ""], ["Cerin", "Ester", ""], ["Adlakha", "Deepti", ""], ["Lowe", "Melanie", ""], ["Giles-Corti", "Billie", ""]]}, {"id": "2105.08966", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist", "title": "Latent Gaussian Model Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Gaussian models and boosting are widely used techniques in statistics\nand machine learning. Tree-boosting shows excellent predictive accuracy on many\ndata sets, but potential drawbacks are that it assumes conditional independence\nof samples, produces discontinuous predictions for, e.g., spatial data, and it\ncan have difficulty with high-cardinality categorical variables. Latent\nGaussian models, such as Gaussian process and grouped random effects models,\nare flexible prior models that allow for making probabilistic predictions.\nHowever, existing latent Gaussian models usually assume either a zero or a\nlinear prior mean function which can be an unrealistic assumption. This article\nintroduces a novel approach that combines boosting and latent Gaussian models\nin order to remedy the above-mentioned drawbacks and to leverage the advantages\nof both techniques. We obtain increased predictive accuracy compared to\nexisting approaches in both simulated and real-world data experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:36:30 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 13:42:12 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sigrist", "Fabio", ""]]}, {"id": "2105.09477", "submitter": "Ehsan Haghighat", "authors": "Ehsan Haghighat, Ali Can Bekar, Erdogan Madenci, Ruben Juanes", "title": "Deep learning for solution and inversion of structural mechanics and\n  vibrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has been the most popular machine learning method in the last\nfew years. In this chapter, we present the application of deep learning and\nphysics-informed neural networks concerning structural mechanics and vibration\nproblems. Demonstration problems involve de-noising data, solution to\ntime-dependent ordinary and partial differential equations, and characterizing\nthe system's response for a given data.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:26:06 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Haghighat", "Ehsan", ""], ["Bekar", "Ali Can", ""], ["Madenci", "Erdogan", ""], ["Juanes", "Ruben", ""]]}, {"id": "2105.09495", "submitter": "Motonori Oka", "authors": "Motonori Oka, Kensuke Okada", "title": "Scalable Estimation Algorithm for the DINA Q-matrix Combining Stochastic\n  Optimization and Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic classification models (DCMs) enable finer-grained inspection of\nthe latent states of respondents' strengths and weaknesses. However, the\naccuracy of diagnosis deteriorates when misspecification occurs in the\npredefined item-attribute relationship, which is defined by a Q-matrix. To\nforestall misdiagnosis, several Q-matrix estimation methods have been developed\nin recent years; however, their scalability to large-scale assessment is\nextremely limited. In this study, we focus on the deterministic inputs, noisy\n\"and\" gate (DINA) model and propose a new framework for Q-matrix estimation in\nwhich the goal is to find the Q-matrix with the maximized marginal likelihood.\nBased on this framework, we developed a scalable estimation algorithm for the\nDINA Q-matrix by constructing an iteration algorithm utilizing stochastic\noptimization and variational inference. The simulation and empirical studies\nreveal that the proposed method achieves high-speed computation and good\naccuracy. Our method can be a useful tool for estimating a Q-matrix in\nlarge-scale settings.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:43:13 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Oka", "Motonori", ""], ["Okada", "Kensuke", ""]]}, {"id": "2105.09512", "submitter": "Americo Cunha Jr", "authors": "A. Cunha Jr, R. Nasser, R. Sampaio, H. Lopes, and K. Breitman", "title": "Uncertainty quantification through Monte Carlo method in a cloud\n  computing setting", "comments": null, "journal-ref": "Computer Physics Communications, vol. 185, pp. 1355-1363, 2014", "doi": "10.1016/j.cpc.2014.01.006", "report-no": null, "categories": "stat.CO cs.MS math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Monte Carlo (MC) method is the most common technique used for uncertainty\nquantification, due to its simplicity and good statistical results. However,\nits computational cost is extremely high, and, in many cases, prohibitive.\nFortunately, the MC algorithm is easily parallelizable, which allows its use in\nsimulations where the computation of a single realization is very costly. This\nwork presents a methodology for the parallelization of the MC method, in the\ncontext of cloud computing. This strategy is based on the MapReduce paradigm,\nand allows an efficient distribution of tasks in the cloud. This methodology is\nillustrated on a problem of structural dynamics that is subject to\nuncertainties. The results show that the technique is capable of producing good\nresults concerning statistical moments of low order. It is shown that even a\nsimple problem may require many realizations for convergence of histograms,\nwhich makes the cloud computing strategy very attractive (due to its high\nscalability capacity and low-cost). Additionally, the results regarding the\ntime of processing and storage space usage allow one to qualify this new\nmethodology as a solution for simulations that require a number of MC\nrealizations beyond the standard.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 04:52:40 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cunha", "A.", "Jr"], ["Nasser", "R.", ""], ["Sampaio", "R.", ""], ["Lopes", "H.", ""], ["Breitman", "K.", ""]]}, {"id": "2105.09695", "submitter": "Zheng Zhao", "authors": "Zheng Zhao, Rui Gao, Simo S\\\"arkk\\\"a", "title": "Hierarchical Non-Stationary Temporal Gaussian Processes With\n  $L^1$-Regularization", "comments": "20 pages. Submitted to Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with regularized extensions of hierarchical\nnon-stationary temporal Gaussian processes (NSGPs) in which the parameters\n(e.g., length-scale) are modeled as GPs. In particular, we consider two\ncommonly used NSGP constructions which are based on explicitly constructed\nnon-stationary covariance functions and stochastic differential equations,\nrespectively. We extend these NSGPs by including $L^1$-regularization on the\nprocesses in order to induce sparseness. To solve the resulting regularized\nNSGP (R-NSGP) regression problem we develop a method based on the alternating\ndirection method of multipliers (ADMM) and we also analyze its convergence\nproperties theoretically. We also evaluate the performance of the proposed\nmethods in simulated and real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:15:33 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhao", "Zheng", ""], ["Gao", "Rui", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2105.09712", "submitter": "Ingeborg Gullikstad Hem", "authors": "Ingeborg Gullikstad Hem, Geir-Arne Fuglstad and Andrea Riebler", "title": "makemyprior: Intuitive Construction of Joint Priors for Variance\n  Parameters in R", "comments": "40 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Priors allow us to robustify inference and to incorporate expert knowledge in\nBayesian hierarchical models. This is particularly important when there are\nrandom effects that are hard to identify based on observed data. The challenge\nlies in understanding and controlling the joint influence of the priors for the\nvariance parameters, and makemyprior is an R package that guides the\nformulation of joint prior distributions for variance parameters. A joint prior\ndistribution is constructed based on a hierarchical decomposition of the total\nvariance in the model along a tree, and takes the entire model structure into\naccount. Users input their prior beliefs or express ignorance at each level of\nthe tree. Prior beliefs can be general ideas about reasonable ranges of\nvariance values and need not be detailed expert knowledge. The constructed\npriors lead to robust inference and guarantee proper posteriors. A graphical\nuser interface facilitates construction and assessment of different choices of\npriors through visualization of the tree and joint prior. The package aims to\nexpand the toolbox of applied researchers and make priors an active component\nin their Bayesian workflow.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:52:17 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hem", "Ingeborg Gullikstad", ""], ["Fuglstad", "Geir-Arne", ""], ["Riebler", "Andrea", ""]]}, {"id": "2105.09824", "submitter": "S. Ashwin Renganathan", "authors": "S. Ashwin Renganathan, Jeffrey Larson, and Stefan M. Wild", "title": "Lookahead Acquisition Functions for Finite-Horizon Time-Dependent\n  Bayesian Optimization and Application to Quantum Optimal Control", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel Bayesian method to solve the maximization of a\ntime-dependent expensive-to-evaluate stochastic oracle. We are interested in\nthe decision that maximizes the oracle at a finite time horizon, given a\nlimited budget of noisy evaluations of the oracle that can be performed before\nthe horizon. Our recursive two-step lookahead acquisition function for Bayesian\noptimization makes nonmyopic decisions at every stage by maximizing the\nexpected utility at the specified time horizon. Specifically, we propose a\ngeneralized two-step lookahead framework with a customizable \\emph{value}\nfunction that allows users to define the utility. We illustrate how lookahead\nversions of classic acquisition functions such as the expected improvement,\nprobability of improvement, and upper confidence bound can be obtained with\nthis framework. We demonstrate the utility of our proposed approach on several\ncarefully constructed synthetic cases and a real-world quantum optimal control\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:17:27 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Renganathan", "S. Ashwin", ""], ["Larson", "Jeffrey", ""], ["Wild", "Stefan M.", ""]]}, {"id": "2105.10084", "submitter": "Americo Cunha Jr", "authors": "Americo Cunha Jr, Rubens Sampaio", "title": "On the nonlinear stochastic dynamics of a continuous system with\n  discrete attached elements", "comments": null, "journal-ref": "Applied Mathematical Modelling, vol. 39, pp. 809-819, 2015", "doi": "10.1016/j.apm.2014.07.012", "report-no": null, "categories": "cond-mat.stat-mech cs.CE math.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a theoretical study on the influence of a discrete\nelement in the nonlinear dynamics of a continuous mechanical system subject to\nrandomness in the model parameters. This system is composed by an elastic bar,\nattached to springs and a lumped mass, with a random elastic modulus and\nsubjected to a Gaussian white-noise distributed external force. One can note\nthat the dynamic behavior of the bar is significantly altered when the lumped\nmass is varied, becoming, on the right extreme and for large values of the\nconcentrated mass, similar to a mass-spring system. It is also observed that\nthe system response is more influenced by the randomness for small values of\nthe lumped mass. The study conducted also show an irregular distribution of\nenergy through the spectrum of frequencies, asymmetries and multimodal behavior\nin the probability distributions of the lumped mass velocity.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 01:26:57 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Cunha", "Americo", "Jr"], ["Sampaio", "Rubens", ""]]}, {"id": "2105.10168", "submitter": "Itziar Fern\\'andez", "authors": "I. Fern\\'andez, A. Rodr\\'iguez-Collado, Y. Larriba, A. Lamela, C.\n  Canedo, C. Rueda", "title": "FMM: An R Package for Modeling Rhythmic Patterns in Oscillatory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is dedicated to the R package FMM which implements a novel\napproach to describe rhythmic patterns in oscillatory signals. The frequency\nmodulated M\\\"obius (FMM) model is defined as a parametric signal plus a\ngaussian noise, where the signal can be described as a single or a sum of\nwaves. The FMM approach is flexible enough to describe a great variety of\nrhythmic patterns. The FMM package includes all required functions to fit and\nexplore single and multi-wave FMM models, as well as a restricted version that\nallows equality constraints between parameters representing a priori knowledge\nabout the shape to be included. Moreover, the FMM package can generate\nsynthetic data and visualize the results of the fitting process. The potential\nof this methodology is illustrated with examples of such biological\noscillations as the circadian rhythm in gene expression, the electrical\nactivity of the heartbeat and neuronal activity.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:28:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Fern\u00e1ndez", "I.", ""], ["Rodr\u00edguez-Collado", "A.", ""], ["Larriba", "Y.", ""], ["Lamela", "A.", ""], ["Canedo", "C.", ""], ["Rueda", "C.", ""]]}, {"id": "2105.10392", "submitter": "Tim Verdonck", "authors": "Robin Van Oirbeek and Jolien Ponnet and Tim Verdonck", "title": "Computational Efficient Approximations of the Concordance Probability in\n  a Big Data Setting", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance measurement is an essential task once a statistical model is\ncreated. The Area Under the receiving operating characteristics Curve (AUC) is\nthe most popular measure for evaluating the quality of a binary classifier. In\nthis case, AUC is equal to the concordance probability, a frequently used\nmeasure to evaluate the discriminatory power of the model. Contrary to AUC, the\nconcordance probability can also be extended to the situation with a continuous\nresponse variable. Due to the staggering size of data sets nowadays,\ndetermining this discriminatory measure requires a tremendous amount of costly\ncomputations and is hence immensely time consuming, certainly in case of a\ncontinuous response variable. Therefore, we propose two estimation methods that\ncalculate the concordance probability in a fast and accurate way and that can\nbe applied to both the discrete and continuous setting. Extensive simulation\nstudies show the excellent performance and fast computing times of both\nestimators. Finally, experiments on two real-life data sets confirm the\nconclusions of the artificial simulations.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:09:53 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Van Oirbeek", "Robin", ""], ["Ponnet", "Jolien", ""], ["Verdonck", "Tim", ""]]}, {"id": "2105.10890", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein and Jorge Mateu", "title": "Bayesian Effect Selection for Additive Quantile Regression with an\n  Analysis to Air Pollution Thresholds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical techniques used in air pollution modelling usually lack the\npossibility to understand which predictors affect air pollution in which\nfunctional form; and are not able to regress on exceedances over certain\nthresholds imposed by authorities directly. The latter naturally induce\nconditional quantiles and reflect the seriousness of particular events. In the\npresent paper we focus on this important aspect by developing quantile\nregression models further. We propose a general Bayesian effect selection\napproach for additive quantile regression within a highly interpretable\nframework. We place separate normal beta prime spike and slab priors on the\nscalar importance parameters of effect parts and implement a fast Gibbs\nsampling scheme. Specifically, it enables to study quantile-specific covariate\neffects, allows these covariates to be of general functional form using\nadditive predictors, and facilitates the analysts' decision whether an effect\nshould be included linearly, non-linearly or not at all in the quantiles of\ninterest. In a detailed analysis on air pollution data in Madrid (Spain) we\nfind the added value of modelling extreme nitrogen dioxide (NO2) concentrations\nand how thresholds are driven differently by several climatological variables\nand traffic as a spatial proxy. Our results underpin the need of enhanced\nstatistical models to support short-term decisions and enable local authorities\nto mitigate or even prevent exceedances of NO2 concentration limits.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 09:02:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Klein", "Nadja", ""], ["Mateu", "Jorge", ""]]}, {"id": "2105.11004", "submitter": "Aleksandros Sobczyk", "authors": "Aleksandros Sobczyk (1) and Efstratios Gallopoulos (2) ((1) IBM\n  Research Europe, Zurich, Switzerland (2) Computer Engineering and Informatics\n  Department, University of Patras, Greece)", "title": "Estimating leverage scores via rank revealing methods and randomization", "comments": "To appear in SIAM Journal on Matrix Analysis and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study algorithms for estimating the statistical leverage scores of\nrectangular dense or sparse matrices of arbitrary rank. Our approach is based\non combining rank revealing methods with compositions of dense and sparse\nrandomized dimensionality reduction transforms. We first develop a set of fast\nnovel algorithms for rank estimation, column subset selection and least squares\npreconditioning. We then describe the design and implementation of leverage\nscore estimators based on these primitives. These estimators are also effective\nfor rank deficient input, which is frequently the case in data analytics\napplications. We provide detailed complexity analyses for all algorithms as\nwell as meaningful approximation bounds and comparisons with the\nstate-of-the-art. We conduct extensive numerical experiments to evaluate our\nalgorithms and to illustrate their properties and performance using synthetic\nand real world data sets.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 19:21:55 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sobczyk", "Aleksandros", ""], ["Gallopoulos", "Efstratios", ""]]}, {"id": "2105.11007", "submitter": "Yue Bai", "authors": "Peiliang Bai, Yue Bai, Abolfazl Safikhani, George Michailidis", "title": "Multiple Change Point Detection in Structured VAR Models: the VARDetect\n  R Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vector Auto-Regressive (VAR) models capture lead-lag temporal dynamics of\nmultivariate time series data. They have been widely used in macroeconomics,\nfinancial econometrics, neuroscience and functional genomics. In many\napplications, the data exhibit structural changes in their autoregressive\ndynamics, which correspond to changes in the transition matrices of the VAR\nmodel that specify such dynamics. We present the R package VARDetect that\nimplements two classes of algorithms to detect multiple change points in\npiecewise stationary VAR models. The first exhibits sublinear computational\ncomplexity in the number of time points and is best suited for structured\nsparse models, while the second exhibits linear time complexity and is designed\nfor models whose transition matrices are assumed to have a low rank plus sparse\ndecomposition. The package also has functions to generate data from the various\nvariants of VAR models discussed, which is useful in simulation studies, as\nwell as to visualize the results through network layouts.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 19:45:11 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 15:00:45 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Bai", "Peiliang", ""], ["Bai", "Yue", ""], ["Safikhani", "Abolfazl", ""], ["Michailidis", "George", ""]]}, {"id": "2105.11387", "submitter": "Wenyu Chen", "authors": "Wenyu Chen, Rahul Mazumder, Richard J. Samworth", "title": "A new computational framework for log-concave density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Statistics, log-concave density estimation is a central problem within the\nfield of nonparametric inference under shape constraints. Despite great\nprogress in recent years on the statistical theory of the canonical estimator,\nnamely the log-concave maximum likelihood estimator, adoption of this method\nhas been hampered by the complexities of the non-smooth convex optimization\nproblem that underpins its computation. We provide enhanced understanding of\nthe structural properties of this optimization problem, which motivates the\nproposal of new algorithms, based on both randomized and Nesterov smoothing,\ncombined with an appropriate integral discretization of increasing accuracy. We\nprove that these methods enjoy, both with high probability and in expectation,\na convergence rate of order $1/T$ up to logarithmic factors on the objective\nfunction scale, where $T$ denotes the number of iterations. The benefits of our\nnew computational framework are demonstrated on both synthetic and real data,\nand our implementation is available in a github repository \\texttt{LogConcComp}\n(Log-Concave Computation).\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:25:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chen", "Wenyu", ""], ["Mazumder", "Rahul", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2105.12271", "submitter": "Yu Wang", "authors": "Yu Wang and Alfred Hero", "title": "SG-PALM: a Fast Physically Interpretable Tensor Graphical Model", "comments": "Accepted in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new graphical model inference procedure, called SG-PALM, for\nlearning conditional dependency structure of high-dimensional tensor-variate\ndata. Unlike most other tensor graphical models the proposed model is\ninterpretable and computationally scalable to high dimension. Physical\ninterpretability follows from the Sylvester generative (SG) model on which\nSG-PALM is based: the model is exact for any observation process that is a\nsolution of a partial differential equation of Poisson type. Scalability\nfollows from the fast proximal alternating linearized minimization (PALM)\nprocedure that SG-PALM uses during training. We establish that SG-PALM\nconverges linearly (i.e., geometric convergence rate) to a global optimum of\nits objective function. We demonstrate the scalability and accuracy of SG-PALM\nfor an important but challenging climate prediction problem: spatio-temporal\nforecasting of solar flares from multimodal imaging data.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 00:24:25 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Wang", "Yu", ""], ["Hero", "Alfred", ""]]}, {"id": "2105.12286", "submitter": "Amadou Barry", "authors": "Amadou Barry, Nikhil Bhagwat, Bratislav Misic, Jean-Baptiste Poline\n  and Celia M. T. Greenwood", "title": "An algorithm-based multiple detection influence measure for high\n  dimensional regression using expectile", "comments": "38 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of influential observations is an important part of data\nanalysis that can prevent erroneous conclusions drawn from biased estimators.\nHowever, in high dimensional data, this identification is challenging.\nClassical and recently-developed methods often perform poorly when there are\nmultiple influential observations in the same dataset. In particular, current\nmethods can fail when there is masking several influential observations with\nsimilar characteristics, or swamping when the influential observations are near\nthe boundary of the space spanned by well-behaved observations. Therefore, we\npropose an algorithm-based, multi-step, multiple detection procedure to\nidentify influential observations that addresses current limitations. Our\nthree-step algorithm to identify and capture undesirable variability in the\ndata, $\\asymMIP,$ is based on two complementary statistics, inspired by\nasymmetric correlations, and built on expectiles. Simulations demonstrate\nhigher detection power than competing methods. Use of the resulting asymptotic\ndistribution leads to detection of influential observations without the need\nfor computationally demanding procedures such as the bootstrap. The application\nof our method to the Autism Brain Imaging Data Exchange neuroimaging dataset\nresulted in a more balanced and accurate prediction of brain maturity based on\ncortical thickness. See our GitHub for a free R package that implements our\nalgorithm: \\texttt{asymMIP} (\\url{github.com/AmBarry/hidetify}).\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 01:16:24 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Barry", "Amadou", ""], ["Bhagwat", "Nikhil", ""], ["Misic", "Bratislav", ""], ["Poline", "Jean-Baptiste", ""], ["Greenwood", "Celia M. T.", ""]]}, {"id": "2105.12488", "submitter": "Neil Chada", "authors": "Neil K. Chada, Lassi Roininen, Jarkko Suuronen", "title": "Cauchy Markov Random Field Priors for Bayesian Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Cauchy Markov random field priors in statistical inverse problems\ncan potentially lead to posterior distributions which are non-Gaussian,\nhigh-dimensional, multimodal and heavy-tailed. In order to use such priors\nsuccessfully, sophisticated optimization and Markov chain Monte Carlo (MCMC)\nmethods are usually required. In this paper, our focus is largely on reviewing\nrecently developed Cauchy difference priors, while introducing interesting new\nvariants, whilst providing a comparison. We firstly propose a one-dimensional\nsecond order Cauchy difference prior, and construct new first and second order\ntwo-dimensional isotropic Cauchy difference priors. Another new Cauchy prior is\nbased on the stochastic partial differential equation approach, derived from\nMat\\'{e}rn type Gaussian presentation. The comparison also includes Cauchy\nsheets. Our numerical computations are based on both maximum a posteriori and\nconditional mean estimation.We exploit state-of-the-art MCMC methodologies such\nas Metropolis-within-Gibbs, Repelling-Attracting Metropolis, and No-U-Turn\nsampler variant of Hamiltonian Monte Carlo. We demonstrate the models and\nmethods constructed for one-dimensional and two-dimensional deconvolution\nproblems. Thorough MCMC statistics are provided for all test cases, including\npotential scale reduction factors.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:46:48 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chada", "Neil K.", ""], ["Roininen", "Lassi", ""], ["Suuronen", "Jarkko", ""]]}, {"id": "2105.12704", "submitter": "Juan Nelson Mart\\'inez Dahbura", "authors": "Juan Nelson Mart\\'inez Dahbura, Shota Komatsu, Takanori Nishida and\n  Angelo Mele", "title": "A Structural Model of Business Card Exchange Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.SI stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social and professional networks affect labor market dynamics, knowledge\ndiffusion and new business creation. To understand the determinants of how\nthese networks are formed in the first place, we analyze a unique dataset of\nbusiness cards exchanges among a sample of over 240,000 users of the\nmulti-platform contact management and professional social networking tool for\nindividuals Eight. We develop a structural model of network formation with\nstrategic interactions, and we estimate users' payoffs that depend on the\ncomposition of business relationships, as well as indirect business\ninteractions. We allow heterogeneity of users in both observable and\nunobservable characteristics to affect how relationships form and are\nmaintained. The model's stationary equilibrium delivers a likelihood that is a\nmixture of exponential random graph models that we can characterize in\nclosed-form. We overcome several econometric and computational challenges in\nestimation, by exploiting a two-step estimation procedure, variational\napproximations and minorization-maximization methods. Our algorithm is\nscalable, highly parallelizable and makes efficient use of computer memory to\nallow estimation in massive networks. We show that users payoffs display\nhomophily in several dimensions, e.g. location; furthermore, users unobservable\ncharacteristics also display homophily.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:33:50 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 09:50:46 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dahbura", "Juan Nelson Mart\u00ednez", ""], ["Komatsu", "Shota", ""], ["Nishida", "Takanori", ""], ["Mele", "Angelo", ""]]}, {"id": "2105.12815", "submitter": "Anna Grim", "authors": "Anna Grim and Pedro Felzenszwalb", "title": "Convex Combination Belief Propagation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new message passing algorithms for inference with graphical\nmodels. The standard min-sum and sum-product belief propagation algorithms are\nguaranteed to converge when the graph is tree-structured, but may not converge\nand can be sensitive to the initialization when the graph contains cycles. This\npaper describes modifications to the standard belief propagation algorithms\nthat are guaranteed to converge to a unique solution regardless of the topology\nof the graph.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 20:06:57 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Grim", "Anna", ""], ["Felzenszwalb", "Pedro", ""]]}, {"id": "2105.12894", "submitter": "Chaofan Huang", "authors": "Chaofan Huang, Simin Ma, Shihao Yang", "title": "MAGI-X: Manifold-Constrained Gaussian Process Inference for Unknown\n  System Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equations (ODEs), commonly used to characterize the\ndynamic systems, are difficult to propose in closed-form for many complicated\nscientific applications, even with the help of domain expert. We propose a fast\nand accurate data-driven method, MAGI-X, to learn the unknown dynamic from the\nobservation data in a non-parametric fashion, without the need of any domain\nknowledge. Unlike the existing methods that mainly rely on the costly numerical\nintegration, MAGI-X utilizes the powerful functional approximator of neural\nnetwork to learn the unknown nonlinear dynamic within the MAnifold-constrained\nGaussian process Inference (MAGI) framework that completely circumvents the\nnumerical integration. Comparing against the state-of-the-art methods on three\nrealistic examples, MAGI-X achieves competitive accuracy in both fitting and\nforecasting while only taking a fraction of computational time. Moreover,\nMAGI-X provides practical solution for the inference of partial observed\nsystems, which no previous method is able to handle.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 01:01:40 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:28:33 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Huang", "Chaofan", ""], ["Ma", "Simin", ""], ["Yang", "Shihao", ""]]}, {"id": "2105.13059", "submitter": "Jeremie Coullon", "authors": "Jeremie Coullon, Leah South, Christopher Nemeth", "title": "Stochastic Gradient MCMC with Multi-Armed Bandit Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic gradient Markov chain Monte Carlo (SGMCMC) is a popular class of\nalgorithms for scalable Bayesian inference. However, these algorithms include\nhyperparameters such as step size or batch size that influence the accuracy of\nestimators based on the obtained samples. As a result, these hyperparameters\nmust be tuned by the practitioner and currently no principled and automated way\nto tune them exists. Standard MCMC tuning methods based on acceptance rates\ncannot be used for SGMCMC, thus requiring alternative tools and diagnostics. We\npropose a novel bandit-based algorithm that tunes SGMCMC hyperparameters to\nmaximize the accuracy of the posterior approximation by minimizing the kernel\nStein discrepancy (KSD). We provide theoretical results supporting this\napproach and assess alternative metrics to KSD. We support our results with\nexperiments on both simulated and real datasets, and find that this method is\npractical for a wide range of application areas.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:00:31 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 13:49:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Coullon", "Jeremie", ""], ["South", "Leah", ""], ["Nemeth", "Christopher", ""]]}, {"id": "2105.13081", "submitter": "Wagner Barreto-Souza", "authors": "Raanju R. Sundararajan and Wagner Barreto-Souza", "title": "Student-t Stochastic Volatility Model With Composite Likelihood\n  EM-Algorithm", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new robust stochastic volatility (SV) model having Student-t marginals is\nproposed. Our process is defined through a linear normal regression model\ndriven by a latent gamma process that controls temporal dependence. This gamma\nprocess is strategically chosen to enable us to find an explicit expression for\nthe pairwise joint density function of the Student-t response process. With\nthis at hand, we propose a composite likelihood (CL) based inference for our\nmodel, which can be straightforwardly implemented with a low computational\ncost. This is a remarkable feature of our Student-t SV process over existing SV\nmodels in the literature that involve computationally heavy algorithms for\nestimating parameters. Aiming at a precise estimation of the parameters related\nto the latent process, we propose a CL Expectation-Maximization algorithm and\ndiscuss a bootstrap approach to obtain standard errors. The finite-sample\nperformance of our composite likelihood methods is assessed through Monte Carlo\nsimulations. The methodology is motivated by an empirical application in the\nfinancial market. We analyze the relationship, across multiple time periods,\nbetween various US sector Exchange-Traded Funds returns and individual\ncompanies' stock price returns based on our novel Student-t model. This\nrelationship is further utilized in selecting optimal financial portfolios.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:09:11 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Sundararajan", "Raanju R.", ""], ["Barreto-Souza", "Wagner", ""]]}, {"id": "2105.13402", "submitter": "John Russo", "authors": "John D. Russo, Jeremy Copperman, David Aristoff, Gideon Simpson,\n  Daniel M. Zuckerman", "title": "Unbiased estimation of equilibrium, rates, and committors from Markov\n  state model analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov state models (MSMs) have been broadly adopted for analyzing molecular\ndynamics trajectories, but the approximate nature of the models that results\nfrom coarse-graining into discrete states is a long-known limitation. We show\ntheoretically that, despite the coarse graining, in principle MSM-like analysis\ncan yield unbiased estimation of key observables. We describe unbiased\nestimators for equilibrium state populations, for the mean first-passage time\n(MFPT) of an arbitrary process, and for state committors - i.e., splitting\nprobabilities. Generically, the estimators are only asymptotically unbiased but\nwe describe how extension of a recently proposed reweighting scheme can\naccelerate relaxation to unbiased values. Exactly accounting for 'sliding\nwindow' averaging over finite-length trajectories is a key, novel element of\nour analysis. In general, our analysis indicates that coarse-grained MSMs are\nasymptotically unbiased for steady-state properties only when appropriate\nboundary conditions (e.g., source-sink for MFPT estimation) are applied\ndirectly to trajectories, prior to calculation of the appropriate transition\nmatrix.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:08:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Russo", "John D.", ""], ["Copperman", "Jeremy", ""], ["Aristoff", "David", ""], ["Simpson", "Gideon", ""], ["Zuckerman", "Daniel M.", ""]]}, {"id": "2105.13440", "submitter": "Peter Carbonetto", "authors": "Peter Carbonetto, Abhishek Sarkar, Zihao Wang and Matthew Stephens", "title": "Non-negative matrix factorization algorithms greatly improve topic model\n  fits", "comments": "Submitted to Advances in Neural Information Processing Systems 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We report on the potential for using algorithms for non-negative matrix\nfactorization (NMF) to improve parameter estimation in topic models. While\nseveral papers have studied connections between NMF and topic models, none have\nsuggested leveraging these connections to develop new algorithms for fitting\ntopic models. Importantly, NMF avoids the \"sum-to-one\" constraints on the topic\nmodel parameters, resulting in an optimization problem with simpler structure\nand more efficient computations. Building on recent advances in optimization\nalgorithms for NMF, we show that first solving the NMF problem then recovering\nthe topic model fit can produce remarkably better fits, and in less time, than\nstandard algorithms for topic models. While we focus primarily on maximum\nlikelihood estimation, we show that this approach also has the potential to\nimprove variational inference for topic models. Our methods are implemented in\nthe R package fastTopics.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:34:46 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Carbonetto", "Peter", ""], ["Sarkar", "Abhishek", ""], ["Wang", "Zihao", ""], ["Stephens", "Matthew", ""]]}, {"id": "2105.13566", "submitter": "Miguel Biron-Lattes", "authors": "Miguel Biron-Lattes, Alexandre Bouchard-C\\^ot\\'e, Trevor Campbell", "title": "Pseudo-marginal Inference for CTMCs on Infinite Spaces via Monotonic\n  Likelihood Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for Continuous-Time Markov Chains (CTMCs) on countably\ninfinite spaces is notoriously difficult because evaluating the likelihood\nexactly is intractable. One way to address this challenge is to first build a\nnon-negative and unbiased estimate of the likelihood -- involving the matrix\nexponential of finite truncations of the true rate matrix -- and then to use\nthe estimates in a pseudo-marginal inference method. In this work, we show that\nwe can dramatically increase the efficiency of this approach by avoiding the\ncomputation of exact matrix exponentials. In particular, we develop a general\nmethodology for constructing an unbiased, non-negative estimate of the\nlikelihood using doubly-monotone matrix exponential approximations. We further\ndevelop a novel approximation in this family -- the skeletoid -- as well as\ntheory regarding its approximation error and how that relates to the variance\nof the estimates used in pseudo-marginal inference. Experimental results show\nthat our approach yields more efficient posterior inference for a wide variety\nof CTMCs.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 03:20:57 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Biron-Lattes", "Miguel", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Campbell", "Trevor", ""]]}, {"id": "2105.13747", "submitter": "Swarnadip Ghosh", "authors": "Swarnadip Ghosh, Trevor Hastie and Art B. Owen", "title": "Scalable logistic regression with crossed random effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The cost of both generalized least squares (GLS) and Gibbs sampling in a\ncrossed random effects model can easily grow faster than $N^{3/2}$ for $N$\nobservations. Ghosh et al. (2020) develop a backfitting algorithm that reduces\nthe cost to $O(N)$. Here we extend that method to a generalized linear mixed\nmodel for logistic regression. We use backfitting within an iteratively\nreweighted penalized least square algorithm. The specific approach is a version\nof penalized quasi-likelihood due to Schall (1991). A straightforward version\nof Schall's algorithm would also cost more than $N^{3/2}$ because it requires\nthe trace of the inverse of a large matrix. We approximate that quantity at\ncost $O(N)$ and prove that this substitution makes an asymptotically negligible\ndifference. Our backfitting algorithm also collapses the fixed effect with one\nrandom effect at a time in a way that is analogous to the collapsed Gibbs\nsampler of Papaspiliopoulos et al. (2020). We use a symmetric operator that\nfacilitates efficient covariance computation. We illustrate our method on a\nreal dataset from Stitch Fix. By properly accounting for crossed random effects\nwe show that a naive logistic regression could underestimate sampling variances\nby several hundred fold.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 11:32:00 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ghosh", "Swarnadip", ""], ["Hastie", "Trevor", ""], ["Owen", "Art B.", ""]]}, {"id": "2105.13850", "submitter": "Michael Kirchhof", "authors": "Michael Kirchhof and Lena Schmid and Christopher Reining and Michael\n  ten Hompel and Markus Pauly", "title": "pRSL: Interpretable Multi-label Stacking by Learning Probabilistic Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key task in multi-label classification is modeling the structure between\nthe involved classes. Modeling this structure by probabilistic and\ninterpretable means enables application in a broad variety of tasks such as\nzero-shot learning or learning from incomplete data. In this paper, we present\nthe probabilistic rule stacking learner (pRSL) which uses probabilistic\npropositional logic rules and belief propagation to combine the predictions of\nseveral underlying classifiers. We derive algorithms for exact and approximate\ninference and learning, and show that pRSL reaches state-of-the-art performance\non various benchmark datasets.\n  In the process, we introduce a novel multicategorical generalization of the\nnoisy-or gate. Additionally, we report simulation results on the quality of\nloopy belief propagation algorithms for approximate inference in bipartite\nnoisy-or networks.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:06:21 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kirchhof", "Michael", ""], ["Schmid", "Lena", ""], ["Reining", "Christopher", ""], ["Hompel", "Michael ten", ""], ["Pauly", "Markus", ""]]}, {"id": "2105.13923", "submitter": "Andrew Fowlie Assoc. Prof.", "authors": "Andrew Fowlie, Sebastian Hoof, Will Handley", "title": "Nested sampling for frequentist computation: fast estimation of small\n  $p$-values", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex hep-ph stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel method for computing $p$-values based on nested sampling\n(NS) applied to the sampling space rather than the parameter space of the\nproblem, in contrast to its usage in Bayesian computation. The computational\ncost of NS scales as $\\log^2{1/p}$, which compares favorably to the $1/p$\nscaling for Monte Carlo (MC) simulations. For significances greater than about\n$4\\sigma$ in both a toy problem and a simplified resonance search, we show that\nNS requires orders of magnitude fewer simulations than ordinary MC estimates.\nThis is particularly relevant for high-energy physics, which adopts a $5\\sigma$\ngold standard for discovery. We conclude with remarks on new connections\nbetween Bayesian and frequentist computation and possibilities for tuning NS\nimplementations for still better performance in this setting.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:06:03 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Fowlie", "Andrew", ""], ["Hoof", "Sebastian", ""], ["Handley", "Will", ""]]}, {"id": "2105.14035", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang and Johannes Lederer", "title": "DeepMoM: Robust Deep Learning With Median-of-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data used in deep learning is notoriously problematic. For example, data are\nusually combined from diverse sources, rarely cleaned and vetted thoroughly,\nand sometimes corrupted on purpose. Intentional corruption that targets the\nweak spots of algorithms has been studied extensively under the label of\n\"adversarial attacks.\" In contrast, the arguably much more common case of\ncorruption that reflects the limited quality of data has been studied much\nless. Such \"random\" corruptions are due to measurement errors, unreliable\nsources, convenience sampling, and so forth. These kinds of corruption are\ncommon in deep learning, because data are rarely collected according to strict\nprotocols -- in strong contrast to the formalized data collection in some parts\nof classical statistics. This paper concerns such corruption. We introduce an\napproach motivated by very recent insights into median-of-means and Le Cam's\nprinciple, we show that the approach can be readily implemented, and we\ndemonstrate that it performs very well in practice. In conclusion, we believe\nthat our approach is a very promising alternative to standard parameter\ntraining based on least-squares and cross-entropy loss.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:07:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Shih-Ting", ""], ["Lederer", "Johannes", ""]]}, {"id": "2105.14052", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang and Johannes Lederer", "title": "Targeted Deep Learning: Framework, Methods, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning systems are typically designed to perform for a wide range of\ntest inputs. For example, deep learning systems in autonomous cars are supposed\nto deal with traffic situations for which they were not specifically trained.\nIn general, the ability to cope with a broad spectrum of unseen test inputs is\ncalled generalization. Generalization is definitely important in applications\nwhere the possible test inputs are known but plentiful or simply unknown, but\nthere are also cases where the possible inputs are few and unlabeled but known\nbeforehand. For example, medicine is currently interested in targeting\ntreatments to individual patients; the number of patients at any given time is\nusually small (typically one), their diagnoses/responses/... are still unknown,\nbut their general characteristics (such as genome information, protein levels\nin the blood, and so forth) are known before the treatment. We propose to call\ndeep learning in such applications targeted deep learning. In this paper, we\nintroduce a framework for targeted deep learning, and we devise and test an\napproach for adapting standard pipelines to the requirements of targeted deep\nlearning. The approach is very general yet easy to use: it can be implemented\nas a simple data-preprocessing step. We demonstrate on a variety of real-world\ndata that our approach can indeed render standard deep learning faster and more\naccurate when the test inputs are known beforehand.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:37:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Shih-Ting", ""], ["Lederer", "Johannes", ""]]}, {"id": "2105.14395", "submitter": "Sanvesh Srivastava", "authors": "Chunlei Wang and Sanvesh Srivastava", "title": "Divide-and-Conquer Bayesian Inference in Hidden Markov Models", "comments": "51 pages and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Divide-and-conquer Bayesian methods consist of three steps: dividing the data\ninto smaller computationally manageable subsets, running a sampling algorithm\nin parallel on all the subsets, and combining parameter draws from all the\nsubsets. The combined parameter draws are used for efficient posterior\ninference in massive data settings. A major restriction of existing\ndivide-and-conquer methods is that their first two steps assume that the\nobservations are independent. We address this problem by developing a\ndivide-and-conquer method for Bayesian inference in parametric hidden Markov\nmodels, where the state space is known and finite. Our main contributions are\ntwo-fold. First, after partitioning the data into smaller blocks of consecutive\nobservations, we modify the likelihood for performing posterior computations on\nthe subsets such that the posterior variances of the subset and true posterior\ndistributions have the same asymptotic order. Second, if the number of subsets\nis chosen appropriately depending on the mixing properties of the hidden Markov\nchain, then we show that the subset posterior distributions defined using the\nmodified likelihood are asymptotically normal as the subset sample size tends\nto infinity. The latter result also implies that we can use any existing\ncombination algorithm in the third step. We show that the combined posterior\ndistribution obtained using one such algorithm is close to the true posterior\ndistribution in 1-Wasserstein distance under widely used regularity\nassumptions. Our numerical results show that the proposed method provides an\naccurate approximation of the true posterior distribution than its competitors\nin diverse simulation studies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:16:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Chunlei", ""], ["Srivastava", "Sanvesh", ""]]}, {"id": "2105.14577", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla, Sivaraman Balakrishnan, Larry Wasserman", "title": "The HulC: Confidence Regions from Convex Hulls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze the HulC, an intuitive and general method for\nconstructing confidence sets using the convex hull of estimates constructed\nfrom subsets of the data. Unlike classical methods which are based on\nestimating the (limiting) distribution of an estimator, the HulC is often\nsimpler to use and effectively bypasses this step. In comparison to the\nbootstrap, the HulC requires fewer regularity conditions and succeeds in many\nexamples where the bootstrap provably fails. Unlike subsampling, the HulC does\nnot require knowledge of the rate of convergence of the estimators on which it\nis based. The validity of the HulC requires knowledge of the (asymptotic)\nmedian-bias of the estimators. We further analyze a variant of our basic\nmethod, called the Adaptive HulC, which is fully data-driven and estimates the\nmedian-bias using subsampling. We show that the Adaptive HulC retains the\naforementioned strengths of the HulC. In certain cases where the underlying\nestimators are pathologically asymmetric the HulC and Adaptive HulC can fail to\nprovide useful confidence sets. We propose a final variant, the Unimodal HulC,\nwhich can salvage the situation in cases where the distribution of the\nunderlying estimator is (asymptotically) unimodal. We discuss these methods in\nthe context of several challenging inferential problems which arise in\nparametric, semi-parametric, and non-parametric inference. Although our focus\nis on validity under weak regularity conditions, we also provide some general\nresults on the width of the HulC confidence sets, showing that in many cases\nthe HulC confidence sets have near-optimal width.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 16:21:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "2105.15036", "submitter": "Khue-Dung Dang", "authors": "Khue-Dung Dang and Luca Maestrini", "title": "Fitting Structural Equation Models via Variational Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models are commonly used to capture the structural\nrelationship between sets of observed and unobservable variables. In Bayesian\nsettings, fitting and inference for these models are typically performed via\nMarkov chain Monte Carlo methods that may be computationally intensive,\nespecially for models with a large number of manifest variables or complex\nstructures. Variational approximations can be a fast alternative; however they\nhave not been adequately explored for this class of models. We develop a mean\nfield variational Bayes approach for fitting basic structural equation models.\nWe show that this variational approximation method can provide reliable\ninference while being significantly faster than Markov chain Monte Carlo.\nClassical mean field variational Bayes may underestimate the true posterior\nvariance, therefore we propose and study bootstrap to overcome this issue. We\ndiscuss different inference strategies based on bootstrap and demonstrate how\nthese can considerably improve the accuracy of the variational approximation\nthrough real and simulated examples.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:18:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Dang", "Khue-Dung", ""], ["Maestrini", "Luca", ""]]}]