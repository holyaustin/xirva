[{"id": "1410.0215", "submitter": "Joakim Beck", "authors": "Joakim Beck and Serge Guillas", "title": "Sequential Design with Mutual Information for Computer Experiments\n  (MICE): Emulation of a Tsunami Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulators can be computationally intensive to run over a large\nnumber of input values, as required for optimization and various uncertainty\nquantification tasks. The standard paradigm for the design and analysis of\ncomputer experiments is to employ Gaussian random fields to model computer\nsimulators. Gaussian process models are trained on input-output data obtained\nfrom simulation runs at various input values. Following this approach, we\npropose a sequential design algorithm, MICE (Mutual Information for Computer\nExperiments), that adaptively selects the input values at which to run the\ncomputer simulator, in order to maximize the expected information gain (mutual\ninformation) over the input space. The superior computational efficiency of the\nMICE algorithm compared to other algorithms is demonstrated by test functions,\nand a tsunami simulator with overall gains of up to 20% in that case.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 13:31:56 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 13:09:01 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Beck", "Joakim", ""], ["Guillas", "Serge", ""]]}, {"id": "1410.0524", "submitter": "Jamie Owen", "authors": "Jamie Owen and Darren J. Wilkinson and Colin S. Gillespie", "title": "Likelihood free inference for Markov processes: a comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches to Bayesian inference for problems with intractable likelihoods\nhave become increasingly important in recent years. Approximate Bayesian\ncomputation (ABC) and \"likelihood free\" Markov chain Monte Carlo techniques are\npopular methods for tackling inference in these scenarios but such techniques\nare computationally expensive. In this paper we compare the two approaches to\ninference, with a particular focus on parameter inference for stochastic\nkinetic models, widely used in systems biology. Discrete time transition\nkernels for models of this type are intractable for all but the most trivial\nsystems yet forward simulation is usually straightforward. We discuss the\nrelative merits and drawbacks of each approach whilst considering the\ncomputational cost implications and efficiency of these techniques. In order to\nexplore the properties of each approach we examine a range of observation\nregimes using two example models. We use a Lotka--Volterra predator prey model\nto explore the impact of full or partial species observations using various\ntime course observations under the assumption of known and unknown measurement\nerror. Further investigation into the impact of observation error is then made\nusing a Schl\\\"ogl system, a test case which exhibits bi-modal state stability\nin some regions of parameter space.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 12:04:42 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Owen", "Jamie", ""], ["Wilkinson", "Darren J.", ""], ["Gillespie", "Colin S.", ""]]}, {"id": "1410.0726", "submitter": "Kun  Yang", "authors": "Kun Yang, Hao Su, Wing Hung Wong", "title": "co-BPM: a Bayesian Model for Divergence Estimation", "comments": "Key Words: coupled binary partition, divergence, MCMC, clustering,\n  classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divergence is not only an important mathematical concept in information\ntheory, but also applied to machine learning problems such as low-dimensional\nembedding, manifold learning, clustering, classification, and anomaly\ndetection. We proposed a bayesian model---co-BPM---to characterize the\ndiscrepancy of two sample sets, i.e., to estimate the divergence of their\nunderlying distributions. In order to avoid the pitfalls of plug-in methods\nthat estimate each density independently, our bayesian model attempts to learn\na coupled binary partition of the sample space that best captures the\nlandscapes of both distributions, then make direct inference on their\ndivergences. The prior is constructed by leveraging the sequential buildup of\nthe coupled binary partitions and the posterior is sampled via our specialized\nMCMC. Our model provides a unified way to estimate various types of divergences\nand enjoys convincing accuracy. We demonstrate its effectiveness through\nsimulations, comparisons with the \\emph{state-of-the-art} and a real data\nexample.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 22:46:28 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 06:13:42 GMT"}, {"version": "v3", "created": "Sun, 12 Jun 2016 03:34:17 GMT"}, {"version": "v4", "created": "Sun, 20 Nov 2016 05:45:25 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Yang", "Kun", ""], ["Su", "Hao", ""], ["Wong", "Wing Hung", ""]]}, {"id": "1410.0793", "submitter": "Marco Giordan", "authors": "Marco Giordan and Federico Vaggi and Ron Wehrens", "title": "On the maximization of likelihoods belonging to the exponential family\n  using ideas related to the Levenberg-Marquardt approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Levenberg-Marquardt algorithm is a flexible iterative procedure used to\nsolve non-linear least squares problems. In this work we study how a class of\npossible adaptations of this procedure can be used to solve maximum likelihood\nproblems when the underlying distributions are in the exponential family. We\nformally demonstrate a local convergence property and we discuss a possible\nimplementation of the penalization involved in this class of algorithms.\nApplications to real and simulated compositional data show the stability and\nefficiency of this approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 09:36:21 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Giordan", "Marco", ""], ["Vaggi", "Federico", ""], ["Wehrens", "Ron", ""]]}, {"id": "1410.0870", "submitter": "Jaakko Luttinen", "authors": "Jaakko Luttinen", "title": "BayesPy: Variational Bayesian Inference in Python", "comments": "Submitted to Journal of Machine Learning Research - Machine Learning\n  Open Source Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  BayesPy is an open-source Python software package for performing variational\nBayesian inference. It is based on the variational message passing framework\nand supports conjugate exponential family models. By removing the tedious task\nof implementing the variational Bayesian update equations, the user can\nconstruct models faster and in a less error-prone way. Simple syntax, flexible\nmodel construction and efficient inference make BayesPy suitable for both\naverage and expert Bayesian users. It also supports some advanced methods such\nas stochastic and collapsed variational inference.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 14:51:09 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 12:07:09 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 14:55:19 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Luttinen", "Jaakko", ""]]}, {"id": "1410.1101", "submitter": "Rodrigo S. Targino", "authors": "Rodrigo S. Targino, Gareth W. Peters, Pavel V. Shevchenko", "title": "Sequential Monte Carlo Samplers for capital allocation under\n  copula-dependent risk models", "comments": null, "journal-ref": "Insurance: Mathematics and Economics 61 (2015) 206-226", "doi": "10.1016/j.insmatheco.2015.01.007", "report-no": null, "categories": "stat.CO q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we assume a multivariate risk model has been developed for a\nportfolio and its capital derived as a homogeneous risk measure. The Euler (or\ngradient) principle, then, states that the capital to be allocated to each\ncomponent of the portfolio has to be calculated as an expectation conditional\nto a rare event, which can be challenging to evaluate in practice. We exploit\nthe copula-dependence within the portfolio risks to design a Sequential Monte\nCarlo Samplers based estimate to the marginal conditional expectations involved\nin the problem, showing its efficiency through a series of computational\nexamples.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 00:10:43 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 18:46:26 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Targino", "Rodrigo S.", ""], ["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "1410.1173", "submitter": "Yiyuan She", "authors": "Yiyuan She, Shijie Li, and Dapeng Wu", "title": "Robust Orthogonal Complement Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the robustification of principal component analysis has attracted\nlots of attention from statisticians, engineers and computer scientists. In\nthis work we study the type of outliers that are not necessarily apparent in\nthe original observation space but can seriously affect the principal subspace\nestimation. Based on a mathematical formulation of such transformed outliers, a\nnovel robust orthogonal complement principal component analysis (ROC-PCA) is\nproposed. The framework combines the popular sparsity-enforcing and low rank\nregularization techniques to deal with row-wise outliers as well as\nelement-wise outliers. A non-asymptotic oracle inequality guarantees the\naccuracy and high breakdown performance of ROC-PCA in finite samples. To tackle\nthe computational challenges, an efficient algorithm is developed on the basis\nof Stiefel manifold optimization and iterative thresholding. Furthermore, a\nbatch variant is proposed to significantly reduce the cost in ultra high\ndimensions. The paper also points out a pitfall of a common practice of SVD\nreduction in robust PCA. Experiments show the effectiveness and efficiency of\nROC-PCA in both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 16:20:34 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 19:08:29 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 02:41:55 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["She", "Yiyuan", ""], ["Li", "Shijie", ""], ["Wu", "Dapeng", ""]]}, {"id": "1410.1174", "submitter": "Yiyuan She", "authors": "Yiyuan She, Yuejia He, and Dapeng Wu", "title": "Learning Topology and Dynamics of Large Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2358956", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale recurrent networks have drawn increasing attention recently\nbecause of their capabilities in modeling a large variety of real-world\nphenomena and physical mechanisms. This paper studies how to identify all\nauthentic connections and estimate system parameters of a recurrent network,\ngiven a sequence of node observations. This task becomes extremely challenging\nin modern network applications, because the available observations are usually\nvery noisy and limited, and the associated dynamical system is strongly\nnonlinear. By formulating the problem as multivariate sparse sigmoidal\nregression, we develop simple-to-implement network learning algorithms, with\nrigorous convergence guarantee in theory, for a variety of sparsity-promoting\npenalty forms. A quantile variant of progressive recurrent network screening is\nproposed for efficient computation and allows for direct cardinality control of\nnetwork topology in estimation. Moreover, we investigate recurrent network\nstability conditions in Lyapunov's sense, and integrate such stability\nconstraints into sparse network learning. Experiments show excellent\nperformance of the proposed algorithms in network topology identification and\nforecasting.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 16:46:04 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["She", "Yiyuan", ""], ["He", "Yuejia", ""], ["Wu", "Dapeng", ""]]}, {"id": "1410.1221", "submitter": "Noemi Petra", "authors": "Tobin Isaac, Noemi Petra, Georg Stadler, Omar Ghattas", "title": "Scalable and efficient algorithms for the propagation of uncertainty\n  from data through inference to prediction for large-scale problems, with\n  application to flow of the Antarctic ice sheet", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2015.04.047", "report-no": null, "categories": "math.OC math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of research on efficient and scalable algorithms in\ncomputational science and engineering has focused on the forward problem: given\nparameter inputs, solve the governing equations to determine output quantities\nof interest. In contrast, here we consider the broader question: given a\n(large-scale) model containing uncertain parameters, (possibly) noisy\nobservational data, and a prediction quantity of interest, how do we construct\nefficient and scalable algorithms to (1) infer the model parameters from the\ndata (the deterministic inverse problem), (2) quantify the uncertainty in the\ninferred parameters (the Bayesian inference problem), and (3) propagate the\nresulting uncertain parameters through the model to issue predictions with\nquantified uncertainties (the forward uncertainty propagation problem)? We\npresent efficient and scalable algorithms for this end-to-end,\ndata-to-prediction process under the Gaussian approximation and in the context\nof modeling the flow of the Antarctic ice sheet and its effect on sea level.\nThe ice is modeled as a viscous, incompressible, creeping, shear-thinning\nfluid. The observational data come from InSAR satellite measurements of surface\nice flow velocity, and the uncertain parameter field to be inferred is the\nbasal sliding parameter. The prediction quantity of interest is the present-day\nice mass flux from the Antarctic continent to the ocean. We show that the work\nrequired for executing this data-to-prediction process is independent of the\nstate dimension, parameter dimension, data dimension, and number of processor\ncores. The key to achieving this dimension independence is to exploit the fact\nthat the observational data typically provide only sparse information on model\nparameters. This property can be exploited to construct a low rank\napproximation of the linearized parameter-to-observable map.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 22:55:16 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 03:25:10 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Isaac", "Tobin", ""], ["Petra", "Noemi", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1410.1503", "submitter": "Xiaoming Huo", "authors": "Xiaoming Huo, Gabor J. Szekely", "title": "Fast Computing for Distance Covariance", "comments": "38 pages, 6 tables, 5 figures. arXiv admin note: text overlap with\n  arXiv:1205.4701 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance covariance and distance correlation have been widely adopted in\nmeasuring dependence of a pair of random variables or random vectors. If the\ncomputation of distance covariance and distance correlation is implemented\ndirectly accordingly to its definition then its computational complexity is\nO($n^2$) which is a disadvantage compared to other faster methods. In this\npaper we show that the computation of distance covariance and distance\ncorrelation of real valued random variables can be implemented by an O(n log n)\nalgorithm and this is comparable to other computationally efficient algorithms.\nThe new formula we derive for an unbiased estimator for squared distance\ncovariance turns out to be a U-statistic. This fact implies some nice\nasymptotic properties that were derived before via more complex methods. We\napply the fast computing algorithm to some synthetic data. Our work will make\ndistance correlation applicable to a much wider class of applications.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 19:40:44 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Huo", "Xiaoming", ""], ["Szekely", "Gabor J.", ""]]}, {"id": "1410.1771", "submitter": "Nicolas Chopin", "authors": "James Ridgway and Pierre Alquier and Nicolas Chopin and Feng Liang", "title": "PAC-Bayesian AUC classification and scoring", "comments": "Accepted at NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a scoring and classification procedure based on the PAC-Bayesian\napproach and the AUC (Area Under Curve) criterion. We focus initially on the\nclass of linear score functions. We derive PAC-Bayesian non-asymptotic bounds\nfor two types of prior for the score parameters: a Gaussian prior, and a\nspike-and-slab prior; the latter makes it possible to perform feature\nselection. One important advantage of our approach is that it is amenable to\npowerful Bayesian computational tools. We derive in particular a Sequential\nMonte Carlo algorithm, as an efficient method which may be used as a gold\nstandard, and an Expectation-Propagation algorithm, as a much faster but\napproximate method. We also extend our method to a class of non-linear score\nfunctions, essentially leading to a nonparametric procedure, by considering a\nGaussian process prior.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 15:27:56 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 09:13:13 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Ridgway", "James", ""], ["Alquier", "Pierre", ""], ["Chopin", "Nicolas", ""], ["Liang", "Feng", ""]]}, {"id": "1410.2046", "submitter": "Lan Jiang", "authors": "Lan Jiang, Sumeetpal S. Singh, Sinan Y{\\i}ld{\\i}r{\\i}m", "title": "Bayesian tracking and parameter learning for non-linear multiple target\n  tracking models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2454474", "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Bayesian tracking and parameter learning algorithm for\nnon-linear non-Gaussian multiple target tracking (MTT) models. We design a\nMarkov chain Monte Carlo (MCMC) algorithm to sample from the posterior\ndistribution of the target states, birth and death times, and association of\nobservations to targets, which constitutes the solution to the tracking\nproblem, as well as the model parameters. In the numerical section, we present\nperformance comparisons with several competing techniques and demonstrate\nsignificant performance improvements in all cases.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 10:02:06 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Jiang", "Lan", ""], ["Singh", "Sumeetpal S.", ""], ["Y\u0131ld\u0131r\u0131m", "Sinan", ""]]}, {"id": "1410.2109", "submitter": "Gabriel Stoltz", "authors": "G. Fort (LTCI, CNRS and Telecom Paris Tech), B. Jourdain, T. Lelievre\n  and G. Stoltz (CERMICS, Ecole des Ponts and INRIA Rocquencourt)", "title": "Self-Healing Umbrella Sampling: Convergence and efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Self-Healing Umbrella Sampling (SHUS) algorithm is an adaptive biasing\nalgorithm which has been proposed to efficiently sample a multimodal\nprobability measure. We show that this method can be seen as a variant of the\nwell-known Wang-Landau algorithm. Adapting results on the convergence of the\nWang-Landau algorithm, we prove the convergence of the SHUS algorithm. We also\ncompare the two methods in terms of efficiency. We finally propose a\nmodification of the SHUS algorithm in order to increase its efficiency, and\nexhibit some similarities of SHUS with the well-tempered metadynamics method.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 13:34:45 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Fort", "G.", "", "LTCI, CNRS and Telecom Paris Tech"], ["Jourdain", "B.", "", "CERMICS, Ecole des Ponts and INRIA Rocquencourt"], ["Lelievre", "T.", "", "CERMICS, Ecole des Ponts and INRIA Rocquencourt"], ["Stoltz", "G.", "", "CERMICS, Ecole des Ponts and INRIA Rocquencourt"]]}, {"id": "1410.3234", "submitter": "Ao Kong", "authors": "Ao Kong and Robert Azencott", "title": "Markov Random Fields and Mass Spectra Discrimination", "comments": "43pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For mass spectra acquired from cancer patients by MALDI or SELDI techniques,\nautomated discrimination between cancer types or stages has often been\nimplemented by machine learnings. These techniques typically generate\n\"black-box\" classifiers, which are difficult to interpret biologically. We\ndevelop new and efficient signature discovery algorithms leading to\ninterpretable signatures combining the discriminating power of explicitly\nselected small groups of biomarkers, identified by their m/z ratios. Our\napproach is based on rigorous stochastic modeling of \"homogeneous\" datasets of\nmass spectra by a versatile class of parameterized Markov Random Fields. We\npresent detailed algorithms validated by precise theoretical results. We also\noutline the successful tests of our approach to generate efficient explicit\nsignatures for six benchmark discrimination tasks, based on mass spectra\nacquired from colorectal cancer patients, as well as from ovarian cancer\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 09:31:36 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Kong", "Ao", ""], ["Azencott", "Robert", ""]]}, {"id": "1410.4009", "submitter": "Dean Eckles", "authors": "Dean Eckles and Maurits Kaptein", "title": "Thompson sampling with the online bootstrap", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling provides a solution to bandit problems in which new\nobservations are allocated to arms with the posterior probability that an arm\nis optimal. While sometimes easy to implement and asymptotically optimal,\nThompson sampling can be computationally demanding in large scale bandit\nproblems, and its performance is dependent on the model fit to the observed\ndata. We introduce bootstrap Thompson sampling (BTS), a heuristic method for\nsolving bandit problems which modifies Thompson sampling by replacing the\nposterior distribution used in Thompson sampling by a bootstrap distribution.\nWe first explain BTS and show that the performance of BTS is competitive to\nThompson sampling in the well-studied Bernoulli bandit case. Subsequently, we\ndetail why BTS using the online bootstrap is more scalable than regular\nThompson sampling, and we show through simulation that BTS is more robust to a\nmisspecified error distribution. BTS is an appealing modification of Thompson\nsampling, especially when samples from the posterior are otherwise not\navailable or are costly.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 11:01:52 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Eckles", "Dean", ""], ["Kaptein", "Maurits", ""]]}, {"id": "1410.4217", "submitter": "Jing Xi", "authors": "Jing Xi and Seth Sullivant", "title": "Sequential Importance Sampling for Two-dimensional Ising Models", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cond-mat.stat-mech math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, sequential importance sampling (SIS) has been well developed\nfor sampling contingency tables with linear constraints. In this paper, we\napply SIS procedure to 2-dimensional Ising models, which give observations of\n0-1 tables and include both linear and quadratic constraints. We show how to\ncompute bounds for specific cells by solving linear programming (LP) problems\nover cut polytopes to reduce rejections. The computational results, which\nincludes both simulations and real data analysis, suggest that our method\nperforms very well for sparse tables and when the 1's are spread out: the\ncomputational times are short, the acceptance rates are high, and if proper\ntests are used then in most cases our conclusions are theoretically reasonable.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 20:21:50 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Xi", "Jing", ""], ["Sullivant", "Seth", ""]]}, {"id": "1410.4231", "submitter": "Jimmy Olsson Dr", "authors": "Pierre Del Moral, Eric Moulines, Jimmy Olsson, and Christelle Verg\\'e", "title": "Convergence properties of weighted particle islands with application to\n  the double bootstrap algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle island models (Verg\\'e et al., 2013) provide a means of\nparallelization of sequential Monte Carlo methods, and in this paper we present\nnovel convergence results for algorithms of this sort. In particular we\nestablish a central limit theorem - as the number of islands and the common\nsize of the islands tend jointly to infinity - of the double bootstrap\nalgorithm with possibly adaptive selection on the island level. For this\npurpose we introduce a notion of archipelagos of weighted islands and find\nconditions under which a set of convergence properties are preserved by\ndifferent operations on such archipelagos. This theory allows arbitrary\ncompositions of these operations to be straightforwardly analyzed, providing a\nvery flexible framework covering the double bootstrap algorithm as a special\ncase. Finally, we establish the long-term numerical stability of the double\nbootstrap algorithm by bounding its asymptotic variance under weak and easily\nchecked assumptions satisfied for a wide range of models with possibly\nnon-compact state space.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 21:10:05 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 17:28:07 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Del Moral", "Pierre", ""], ["Moulines", "Eric", ""], ["Olsson", "Jimmy", ""], ["Verg\u00e9", "Christelle", ""]]}, {"id": "1410.4534", "submitter": "Ricardo Ehlers", "authors": "Marcelo Hartmann and Ricardo Ehlers", "title": "Bayesian Inference for Generalized Extreme Value Distributions via\n  Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": "10.1080/03610918.2016.1152365", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to evaluate and compare Markov chain Monte Carlo\n(MCMC) methods to estimate the parameters in a generalized extreme value model.\nWe employed the Bayesian approach using traditional Metropolis-Hastings\nmethods, Hamiltonian Monte Carlo (HMC) and Riemann manifold HMC (RMHMC) methods\nto obtain the approximations to the posterior marginal distributions of\ninterest. Applications to real datasets of maxima illustrate illustrate how HMC\ncan be much more efficient computationally than traditional MCMC and simulation\nstudies are conducted to compare the algorithms in terms of how fast they get\nclose enough to the stationary distribution so as to provide good estimates\nwith a smaller number of iterations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 18:53:20 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 17:40:10 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Hartmann", "Marcelo", ""], ["Ehlers", "Ricardo", ""]]}, {"id": "1410.4755", "submitter": "Hans-Werner van Wyk", "authors": "Hans-Werner van Wyk, Max Gunzburger, John Burkardt, and Miroslav\n  Stoyanov", "title": "Power-Law Noises over General Spatial Domains and on Non-Standard Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power-law noises abound in nature and have been observed extensively in both\ntime series and spatially varying environmental parameters. Although, recent\nyears have seen the extension of traditional stochastic partial differential\nequations to include systems driven by fractional Brownian motion, spatially\ndistributed scale-invariance has received comparatively little attention,\nespecially for parameters defined over non-standard spatial domains. This paper\ndiscusses the generalization of power-law noises to general spatial domains by\noutlining their theoretical underpinnings as well as addressing their numerical\nsimulation on arbitrary meshes. Three computational algorithms are presented\nfor efficiently generating their sample paths, accompanied by numerous\nnumerical illustrations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 15:19:00 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["van Wyk", "Hans-Werner", ""], ["Gunzburger", "Max", ""], ["Burkardt", "John", ""], ["Stoyanov", "Miroslav", ""]]}, {"id": "1410.4773", "submitter": "Shahrouz Khalili", "authors": "Shahrouz Khalili, Osvaldo Simeone, Alexander M. Haimovich", "title": "Cloud Radio-Multistatic Radar: Joint Optimization of Code Vector and\n  Backhaul Quantization", "comments": "To be published in IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2014.2363939", "report-no": null, "categories": "math.ST cs.IT math.IT stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multistatic radar set-up is considered in which distributed receive\nantennas are connected to a Fusion Center (FC) via limited-capacity backhaul\nlinks. Similar to cloud radio access networks in communications, the receive\nantennas quantize the received baseband signal before transmitting it to the\nFC. The problem of maximizing the detection performance at the FC jointly over\nthe code vector used by the transmitting antenna and over the statistics of the\nnoise introduced by backhaul quantization is investigated. Specifically,\nadopting the information-theoretic criterion of the Bhattacharyya distance to\nevaluate the detection performance at the FC and information-theoretic measures\nof the quantization rate, the problem at hand is addressed via a Block\nCoordinate Descent (BCD) method coupled with Majorization-Minimization (MM).\nNumerical results demonstrate the advantages of the proposed joint optimization\napproach over more conventional solutions that perform separate optimization.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 15:52:03 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 15:31:16 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Khalili", "Shahrouz", ""], ["Simeone", "Osvaldo", ""], ["Haimovich", "Alexander M.", ""]]}, {"id": "1410.4803", "submitter": "Christopher C. Chang", "authors": "Christopher C. Chang and Carson C. Chow and Laurent C.A.M. Tellier and\n  Shashaank Vattikuti and Shaun M. Purcell and James J. Lee", "title": "Second-generation PLINK: rising to the challenge of larger and richer\n  datasets", "comments": "2 figures, 1 additional file", "journal-ref": "GigaScience 2015, 4:7", "doi": "10.1186/s13742-015-0047-8", "report-no": null, "categories": "q-bio.GN stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PLINK 1 is a widely used open-source C/C++ toolset for genome-wide\nassociation studies (GWAS) and research in population genetics. However, the\nsteady accumulation of data from imputation and whole-genome sequencing studies\nhas exposed a strong need for even faster and more scalable implementations of\nkey functions. In addition, GWAS and population-genetic data now frequently\ncontain probabilistic calls, phase information, and/or multiallelic variants,\nnone of which can be represented by PLINK 1's primary data format.\n  To address these issues, we are developing a second-generation codebase for\nPLINK. The first major release from this codebase, PLINK 1.9, introduces\nextensive use of bit-level parallelism, O(sqrt(n))-time/constant-space\nHardy-Weinberg equilibrium and Fisher's exact tests, and many other algorithmic\nimprovements. In combination, these changes accelerate most operations by 1-4\norders of magnitude, and allow the program to handle datasets too large to fit\nin RAM. This will be followed by PLINK 2.0, which will introduce (a) a new data\nformat capable of efficiently representing probabilities, phase, and\nmultiallelic variants, and (b) extensions of many functions to account for the\nnew types of information.\n  The second-generation versions of PLINK will offer dramatic improvements in\nperformance and compatibility. For the first time, users without access to\nhigh-end computing resources can perform several essential analyses of the\nfeature-rich and very large genetic datasets coming into use.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 17:37:02 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Chang", "Christopher C.", ""], ["Chow", "Carson C.", ""], ["Tellier", "Laurent C. A. M.", ""], ["Vattikuti", "Shashaank", ""], ["Purcell", "Shaun M.", ""], ["Lee", "James J.", ""]]}, {"id": "1410.4812", "submitter": "Reshad Hosseini", "authors": "Reshad Hosseini, Suvrit Sra, Lucas Theis, Matthias Bethge", "title": "Inference and Mixture Modeling with the Elliptical Gamma Distribution", "comments": "23 pages, 11 figures", "journal-ref": "Computational Statistics & Data Analysis 2016, Vol. 101, 29-43", "doi": "10.1016/j.csda.2016.02.009", "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study modeling and inference with the Elliptical Gamma Distribution (EGD).\nWe consider maximum likelihood (ML) estimation for EGD scatter matrices, a task\nfor which we develop new fixed-point algorithms. Our algorithms are efficient\nand converge to global optima despite nonconvexity. Moreover, they turn out to\nbe much faster than both a well-known iterative algorithm of Kent & Tyler\n(1991) and sophisticated manifold optimization algorithms. Subsequently, we\ninvoke our ML algorithms as subroutines for estimating parameters of a mixture\nof EGDs. We illustrate our methods by applying them to model natural image\nstatistics---the proposed EGD mixture model yields the most parsimonious model\namong several competing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 18:19:27 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 07:51:26 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Hosseini", "Reshad", ""], ["Sra", "Suvrit", ""], ["Theis", "Lucas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1410.5232", "submitter": "Anestis Touloumis", "authors": "Anestis Touloumis", "title": "R Package multgee: A Generalized Estimating Equations Solver for\n  Multinomial Responses", "comments": "To appear in Journal of Statistical Software", "journal-ref": "Journal of Statistical Software 64 (2015), pp. 1--14", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package multgee implements the local odds ratios generalized estimating\nequations (GEE) approach proposed by Touloumis et al. (2013), a GEE approach\nfor correlated multinomial responses that circumvents theoretical and practical\nlimitations of the GEE method. A main strength of multgee is that it provides\nGEE routines for both ordinal (ordLORgee) and nominal (nomLORgee) responses,\nwhile relevant softwares in R and SAS are restricted to ordinal responses under\na marginal cumulative link model specification. In addition, multgee offers a\nmarginal adjacent categories logit model for ordinal responses and a marginal\nbaseline category logit model for nominal. Further, utility functions are\navailable to ease the local odds ratios structure selection (intrinsic.pars)\nand to perform a Wald type goodness-of-fit test between two nested GEE models\n(waldts). We demonstrate the application of multgee through a clinical trial\nwith clustered ordinal multinomial responses.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 11:35:13 GMT"}, {"version": "v2", "created": "Tue, 21 Oct 2014 10:37:23 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 11:31:25 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Touloumis", "Anestis", ""]]}, {"id": "1410.5392", "submitter": "Yu Cheng", "authors": "Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng and Shang-Hua Teng", "title": "Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling\n  for Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a sampling problem basic to computational statistical inference,\nwe develop a nearly optimal algorithm for a fundamental problem in spectral\ngraph theory and numerical analysis. Given an $n\\times n$ SDDM matrix ${\\bf\n\\mathbf{M}}$, and a constant $-1 \\leq p \\leq 1$, our algorithm gives efficient\naccess to a sparse $n\\times n$ linear operator $\\tilde{\\mathbf{C}}$ such that\n$${\\mathbf{M}}^{p} \\approx \\tilde{\\mathbf{C}} \\tilde{\\mathbf{C}}^\\top.$$ The\nsolution is based on factoring ${\\bf \\mathbf{M}}$ into a product of simple and\nsparse matrices using squaring and spectral sparsification. For ${\\mathbf{M}}$\nwith $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and\npolylogarithmic depth on a parallel machine with $m$ processors. This gives the\nfirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.\nrandom univariate Gaussian samples to generate i.i.d. random samples for\n$n$-dimensional Gaussian random fields with SDDM precision matrices. For\nsampling this natural subclass of Gaussian random fields, it is optimal in the\nrandomness and nearly optimal in the work and parallel complexity. In addition,\nour sampling algorithm can be directly extended to Gaussian random fields with\nSDD precision matrices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 18:59:58 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Cheng", "Dehua", ""], ["Cheng", "Yu", ""], ["Liu", "Yan", ""], ["Peng", "Richard", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1410.5851", "submitter": "Yong Kong", "authors": "Yong Kong", "title": "Calculating complexity of large randomized libraries", "comments": "14 pages, 4 figures", "journal-ref": "Journal of Theoretical Biology 259 (3), 641-645, 2009", "doi": "10.1016/j.jtbi.2009.04.008", "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized libraries are increasingly popular in protein engineering and\nother biomedical research fields. Statistics of the libraries are useful to\nguide and evaluate randomized library construction. Previous works only give\nthe mean of the number of unique sequences in the library, and they can only\nhandle equal molar ratio of the four nucleotides at a small number of mutation\nsites. We derive formulas to calculate the mean and variance of the number of\nunique sequences in libraries generated by cassette mutagenesis with mixtures\nof arbitrary nucleotide ratios. Computer program was developed which utilizes\narbitrary numerical precision software package to calculate the statistics of\nlarge libraries. The statistics of library with mutations in more than $20$\namino acids can be calculated easily. Results show that the nucleotide ratios\nhave significant effects on these statistics. The more skewed the ratio, the\nlarger the library size is needed to obtain the same expected number of unique\nsequences. The program is freely available at\n\\url{http://graphics.med.yale.edu/cgi-bin/lib_comp.pl}.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 20:43:32 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Kong", "Yong", ""]]}, {"id": "1410.5899", "submitter": "Alen Alexanderian", "authors": "Alen Alexanderian, Noemi Petra, Georg Stadler, and Omar Ghattas", "title": "A Fast and Scalable Method for A-Optimal Design of Experiments for\n  Infinite-dimensional Bayesian Nonlinear Inverse Problems", "comments": "30 pages; minor revisions; accepted for publication in SIAM Journal\n  on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of optimal experimental design (OED) for Bayesian\nnonlinear inverse problems governed by PDEs. The goal is to find a placement of\nsensors, at which experimental data are collected, so as to minimize the\nuncertainty in the inferred parameter field. We formulate the OED objective\nfunction by generalizing the classical A-optimal experimental design criterion\nusing the expected value of the trace of the posterior covariance. We seek a\nmethod that solves the OED problem at a cost (measured in the number of forward\nPDE solves) that is independent of both the parameter and sensor dimensions. To\nfacilitate this, we construct a Gaussian approximation to the posterior at the\nmaximum a posteriori probability (MAP) point, and use the resulting covariance\noperator to define the OED objective function. We use randomized trace\nestimation to compute the trace of this (implicitly defined) covariance\noperator. The resulting OED problem includes as constraints the PDEs\ncharacterizing the MAP point, and the PDEs describing the action of the\ncovariance operator to vectors. The sparsity of the sensor configurations is\ncontrolled using sparsifying penalty functions. We elaborate our OED method for\nthe problem of determining the sensor placement to best infer the coefficient\nof an elliptic PDE. Adjoint methods are used to compute the gradient of the\nPDE-constrained OED objective function. We provide numerical results for\ninference of the permeability field in a porous medium flow problem, and\ndemonstrate that the number of PDE solves required for the evaluation of the\nOED objective function and its gradient is essentially independent of both the\nparameter and sensor dimensions. The number of quasi-Newton iterations for\ncomputing an OED also exhibits the same dimension invariance properties.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 02:31:07 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 05:01:12 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Alexanderian", "Alen", ""], ["Petra", "Noemi", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1410.6131", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova, James Booth and Martin T. Wells", "title": "Penalized versus constrained generalized eigenvalue problems", "comments": "18 pages, 8 figures", "journal-ref": "Journal of Computational and Graphical Statistics 2017, Vol. 26,\n  No. 2, 379-387", "doi": "10.1080/10618600.2016.1172017", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the difference between using an $\\ell_1$ penalty versus an\n$\\ell_1$ constraint in generalized eigenvalue problems, such as principal\ncomponent analysis and discriminant analysis. Our main finding is that an\n$\\ell_1$ penalty may fail to provide very sparse solutions; a severe\ndisadvantage for variable selection that can be remedied by using an $\\ell_1$\nconstraint. Our claims are supported both by empirical evidence and theoretical\nanalysis. Finally, we illustrate the advantages of an $\\ell_1$ constraint in\nthe context of discriminant analysis and principal component analysis.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 18:30:38 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 14:24:10 GMT"}, {"version": "v3", "created": "Mon, 4 May 2015 15:54:42 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""], ["Booth", "James", ""], ["Wells", "Martin T.", ""]]}, {"id": "1410.6151", "submitter": "Matthias Morzfeld", "authors": "Jonathan Goodman, Kevin K. Lin, Matthias Morzfeld", "title": "Small-noise analysis and symmetrization of implicit Monte Carlo samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit samplers are algorithms for producing independent, weighted samples\nfrom multi-variate probability distributions. These are often applied in\nBayesian data assimilation algorithms. We use Laplace asymptotic expansions to\nanalyze two implicit samplers in the small noise regime. Our analysis suggests\na symmetrization of the algo- rithms that leads to improved (implicit) sampling\nschemes at a rel- atively small additional cost. Computational experiments\nconfirm the theory and show that symmetrization is effective for small noise\nsampling problems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 19:49:43 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Goodman", "Jonathan", ""], ["Lin", "Kevin K.", ""], ["Morzfeld", "Matthias", ""]]}, {"id": "1410.6460", "submitter": "Tim Salimans", "authors": "Tim Salimans, Diederik P. Kingma, Max Welling", "title": "Markov Chain Monte Carlo and Variational Inference: Bridging the Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in stochastic gradient variational inference have made it\npossible to perform variational Bayesian inference with posterior\napproximations containing auxiliary random variables. This enables us to\nexplore a new synthesis of variational inference and Monte Carlo methods where\nwe incorporate one or more steps of MCMC into our variational approximation. By\ndoing so we obtain a rich class of inference algorithms bridging the gap\nbetween variational methods and MCMC, and offering the best of both worlds:\nfast posterior approximation through the maximization of an explicit objective,\nwith the option of trading off additional computation for additional accuracy.\nWe describe the theoretical foundations that make this possible and show some\npromising first results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 19:23:53 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 12:21:53 GMT"}, {"version": "v3", "created": "Tue, 2 Dec 2014 18:03:43 GMT"}, {"version": "v4", "created": "Tue, 19 May 2015 13:53:13 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Salimans", "Tim", ""], ["Kingma", "Diederik P.", ""], ["Welling", "Max", ""]]}, {"id": "1410.6466", "submitter": "Dehua Cheng", "authors": "Dehua Cheng, Xinran He, Yan Liu", "title": "Model Selection for Topic Models via Spectral Decomposition", "comments": "accepted in AISTATS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have achieved significant successes in analyzing large-scale\ntext corpus. In practical applications, we are always confronted with the\nchallenge of model selection, i.e., how to appropriately set the number of\ntopics. Following recent advances in topic model inference via tensor\ndecomposition, we make a first attempt to provide theoretical analysis on model\nselection in latent Dirichlet allocation. Under mild conditions, we derive the\nupper bound and lower bound on the number of topics given a text collection of\nfinite size. Experimental results demonstrate that our bounds are accurate and\ntight. Furthermore, using Gaussian mixture model as an example, we show that\nour methodology can be easily generalized to model selection analysis for other\nlatent models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 19:38:44 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 01:39:14 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Cheng", "Dehua", ""], ["He", "Xinran", ""], ["Liu", "Yan", ""]]}, {"id": "1410.6604", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, Peichao Peng, David Dunson", "title": "Median Selection Subset Aggregation for Parallel Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For massive data sets, efficient computation commonly relies on distributed\nalgorithms that store and process subsets of the data on different machines,\nminimizing communication costs. Our focus is on regression and classification\nproblems involving many features. A variety of distributed algorithms have been\nproposed in this context, but challenges arise in defining an algorithm with\nlow communication, theoretical guarantees and excellent practical performance\nin general settings. We propose a MEdian Selection Subset AGgregation Estimator\n(message) algorithm, which attempts to solve these problems. The algorithm\napplies feature selection in parallel for each subset using Lasso or another\nmethod, calculates the `median' feature inclusion index, estimates coefficients\nfor the selected features in parallel for each subset, and then averages these\nestimates. The algorithm is simple, involves very minimal communication, scales\nefficiently in both sample and feature size, and has theoretical guarantees. In\nparticular, we show model selection consistency and coefficient estimation\nefficiency. Extensive experiments show excellent performance in variable\nselection, estimation, prediction, and computation time relative to usual\ncompetitors.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 07:52:55 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Wang", "Xiangyu", ""], ["Peng", "Peichao", ""], ["Dunson", "David", ""]]}, {"id": "1410.6948", "submitter": "Jan Mandel", "authors": "Jan Mandel, Adam K. Kochanski, Martin Vejmelka, and Jonathan D.\n  Beezley", "title": "Data Assimilation of Satellite Fire Detection in Coupled Atmosphere-Fire\n  Simulation by WRF-SFIRE", "comments": "9 pages, VII International Conference on Forest Fire Research,\n  Coimbra, Portugal, November 17-20, 2014", "journal-ref": "Advances in Forest Fire Research, D. X. Viegas (ed), 2014, 716-724", "doi": "10.14195/978-989-26-0884-6_80", "report-no": "UCD CCM Report 324", "categories": "physics.data-an physics.ao-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently available satellite active fire detection products from the VIIRS\nand MODIS instruments on polar-orbiting satellites produce detection squares in\narbitrary locations. There is no global fire/no fire map, no detection under\ncloud cover, false negatives are common, and the detection squares are much\ncoarser than the resolution of a fire behavior model. Consequently, current\nactive fire satellite detection products should be used to improve fire\nmodeling in a statistical sense only, rather than as a direct input. We\ndescribe a new data assimilation method for active fire detection, based on a\nmodification of the fire arrival time to simultaneously minimize the difference\nfrom the forecast fire arrival time and maximize the likelihood of the fire\ndetection data. This method is inspired by contour detection methods used in\ncomputer vision, and it can be cast as a Bayesian inverse problem technique, or\na generalized Tikhonov regularization. After the new fire arrival time on the\nwhole simulation domain is found, the model can be re-run from a time in the\npast using the new fire arrival time to generate the heat fluxes and to spin up\nthe atmospheric model until the satellite overpass time, when the coupled\nsimulation continues from the modified state.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 18:45:26 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Mandel", "Jan", ""], ["Kochanski", "Adam K.", ""], ["Vejmelka", "Martin", ""], ["Beezley", "Jonathan D.", ""]]}, {"id": "1410.7348", "submitter": "Mehrdad Abolbashari", "authors": "Mehrdad Abolbashari, Gelareh Babaie, Jonathan Babaie, and Faramarz\n  Farahi", "title": "Fractional Bi-Spectrum", "comments": "3 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A signal with discrete frequency components, has a zero bispectrum if no\nlinear combination of the frequencies equals one of the frequency components.\nWe introduce fractional bispectrum in which for such signals the fractional\nbispectrum is nonzero. It is shown that fractional bispectrum has the same\nproperty as bispectrum for Gaussian signals: the fractional bispectrum of a\nzero mean Gaussian signal is zero; therefore it can be used to eliminate or\nreduce the Gaussian noise.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 18:39:20 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Abolbashari", "Mehrdad", ""], ["Babaie", "Gelareh", ""], ["Babaie", "Jonathan", ""], ["Farahi", "Faramarz", ""]]}, {"id": "1410.7357", "submitter": "Sonja Petrovic", "authors": "Vishesh Karwa, Michael J. Pelsmajer, Sonja Petrovi\\'c, Despina Stasi,\n  Dane Wilburne", "title": "Statistical models for cores decomposition of an undirected random graph", "comments": "Subsection 3.1 is new: `Sample space restriction and degeneracy of\n  real-world networks'. Several clarifying comments have been added. Discussion\n  now mentions 2 additional specific open problems. Bibliography updated. 25\n  pages (including appendix), ~10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI physics.soc-ph stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-core decomposition is a widely studied summary statistic that\ndescribes a graph's global connectivity structure. In this paper, we move\nbeyond using $k$-core decomposition as a tool to summarize a graph and propose\nusing $k$-core decomposition as a tool to model random graphs. We propose using\nthe shell distribution vector, a way of summarizing the decomposition, as a\nsufficient statistic for a family of exponential random graph models. We study\nthe properties and behavior of the model family, implement a Markov chain Monte\nCarlo algorithm for simulating graphs from the model, implement a direct\nsampler from the set of graphs with a given shell distribution, and explore the\nsampling distributions of some of the commonly used complementary statistics as\ngood candidates for heuristic model fitting. These algorithms provide first\nfundamental steps necessary for solving the following problems: parameter\nestimation in this ERGM, extending the model to its Bayesian relative, and\ndeveloping a rigorous methodology for testing goodness of fit of the model and\nmodel selection. The methods are applied to a synthetic network as well as the\nwell-known Sampson monks dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 19:08:50 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2015 19:59:15 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 15:59:31 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Karwa", "Vishesh", ""], ["Pelsmajer", "Michael J.", ""], ["Petrovi\u0107", "Sonja", ""], ["Stasi", "Despina", ""], ["Wilburne", "Dane", ""]]}, {"id": "1410.7659", "submitter": "Guy Bresler", "authors": "Guy Bresler and David Gamarnik and Devavrat Shah", "title": "Learning graphical models from the Glauber dynamics", "comments": "9 pages. Appeared in Allerton Conference 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of learning undirected graphical models\nfrom data generated according to the Glauber dynamics. The Glauber dynamics is\na Markov chain that sequentially updates individual nodes (variables) in a\ngraphical model and it is frequently used to sample from the stationary\ndistribution (to which it converges given sufficient time). Additionally, the\nGlauber dynamics is a natural dynamical model in a variety of settings. This\nwork deviates from the standard formulation of graphical model learning in the\nliterature, where one assumes access to i.i.d. samples from the distribution.\n  Much of the research on graphical model learning has been directed towards\nfinding algorithms with low computational cost. As the main result of this\nwork, we establish that the problem of reconstructing binary pairwise graphical\nmodels is computationally tractable when we observe the Glauber dynamics.\nSpecifically, we show that a binary pairwise graphical model on $p$ nodes with\nmaximum degree $d$ can be learned in time $f(d)p^2\\log p$, for a function\n$f(d)$, using nearly the information-theoretic minimum number of samples.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 15:32:09 GMT"}, {"version": "v2", "created": "Sat, 29 Nov 2014 02:31:20 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Bresler", "Guy", ""], ["Gamarnik", "David", ""], ["Shah", "Devavrat", ""]]}, {"id": "1410.7799", "submitter": "Adrian Raftery", "authors": "Luca Onorante and Adrian E. Raftery", "title": "Dynamic Model Averaging in Large Model Spaces Using Dynamic Occam's\n  Window", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model averaging has become a widely used approach to accounting for\nuncertainty about the structural form of the model generating the data. When\ndata arrive sequentially and the generating model can change over time, Dynamic\nModel Averaging (DMA) extends model averaging to deal with this situation.\nOften in macroeconomics, however, many candidate explanatory variables are\navailable and the number of possible models becomes too large for DMA to be\napplied in its original form. We propose a new method for this situation which\nallows us to perform DMA without considering the whole model space, but using a\nsubset of models and dynamically optimizing the choice of models at each point\nin time. This yields a dynamic form of Occam's window. We evaluate the method\nin the context of the problem of nowcasting GDP in the Euro area. We find that\nits forecasting performance compares well that of other methods.\n  Keywords: Bayesian model averaging; Model uncertainty; Nowcasting; Occam's\nwindow.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 20:43:05 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Onorante", "Luca", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1410.8050", "submitter": "Emanuele Taufer", "authors": "Emanuele Taufer", "title": "A note on the empirical process of strongly dependent stable random\n  variables", "comments": "16 pages, 3 tables", "journal-ref": null, "doi": "10.1016/j.spl.2015.07.032", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the limit properties of the empirical process of\n$\\alpha$-stable random variables with long range dependence. The\n$\\alpha$-stable random variables are constructed by non-linear transformations\nof bivariate sequences of strongly dependent gaussian processes. The approach\nfollowed allows an analysis of the empirical process by means of expansions in\nterms of bivariate Hermite polynomials for the full range $0<\\alpha<2$. A weak\nuniform reduction principle is provided and it is shown that the limiting\nprocess is gaussian. The results of the paper different substantailly from\nthose available for empirical processes obtained by stable moving averages with\nlong memory. An application to goodness-of-fit testing is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 16:37:19 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Taufer", "Emanuele", ""]]}, {"id": "1410.8276", "submitter": "Guilherme Rodrigues", "authors": "G. S. Rodrigues, David J. Nott and S. A. Sisson", "title": "Functional regression approximate Bayesian computation for Gaussian\n  process density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Bayesian nonparametric method for hierarchical modelling\non a set of related density functions, where grouped data in the form of\nsamples from each density function are available. Borrowing strength across the\ngroups is a major challenge in this context. To address this problem, we\nintroduce a hierarchically structured prior, defined over a set of univariate\ndensity functions, using convenient transformations of Gaussian processes.\nInference is performed through approximate Bayesian computation (ABC), via a\nnovel functional regression adjustment. The performance of the proposed method\nis illustrated via a simulation study and an analysis of rural high school exam\nperformance in Brazil.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 07:23:33 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Rodrigues", "G. S.", ""], ["Nott", "David J.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1410.8504", "submitter": "Leopoldo Catania", "authors": "Mauro Bernardi and Leopoldo Catania", "title": "The Model Confidence Set package for R", "comments": "20 pages, 2 tables, 15 code chunk", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the R package MCS which implements the Model Confidence\nSet (MCS) procedure recently developed by Hansen et al. (2011). The Hansen's\nprocedure consists on a sequence of tests which permits to construct a set of\n'superior' models, where the null hypothesis of Equal Predictive Ability (EPA)\nis not rejected at a certain confidence level. The EPA statistic tests is\ncalculated for an arbitrary loss function, meaning that we could test models on\nvarious aspects, for example punctual forecasts. The relevance of the package\nis shown using an example which aims at illustrating in details the use of the\nfunctions provided by the package. The example compares the ability of\ndifferent models belonging to the ARCH family to predict large financial\nlosses. We also discuss the implementation of the ARCH--type models and their\nmaximum likelihood estimation using the popular R package rugarch developed by\nGhalanos (2014).\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 19:24:45 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Bernardi", "Mauro", ""], ["Catania", "Leopoldo", ""]]}]