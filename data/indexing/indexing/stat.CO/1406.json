[{"id": "1406.0182", "submitter": "Javier Contreras-Reyes JCR", "authors": "Reinaldo B. Arellano-Valle and Javier E. Contreras-Reyes", "title": "Discriminant functions arising from selection distributions: theory and\n  simulation", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of normality in data has been considered in the field of\nstatistical analysis for a long time. However, in many practical situations,\nthis assumption is clearly unrealistic. It has recently been suggested that the\nuse of distributions indexed by skewness/shape parameters produce more\nexibility in the modelling of different applications. Consequently, the results\nshow a more realistic interpretation for these problems. For these reasons, the\naim of this paper is to investigate the effects of the generalisation of a\ndiscrimination function method through the class of multivariate extended\nskew-elliptical distributions, study in detail the multivariate extended\nskew-normal case and develop a quadratic approximation function for this family\nof distributions. A simulation study is reported to evaluate the adequacy of\nthe proposed classification rule as well as the performance of the EM algorithm\nto estimate the model parameters.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 17:32:08 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Arellano-Valle", "Reinaldo B.", ""], ["Contreras-Reyes", "Javier E.", ""]]}, {"id": "1406.0225", "submitter": "Paul Kabaila", "authors": "Paul Kabaila", "title": "A new method of randomization of lattice rules for multiple integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cranley and Patterson put forward the following randomization as the basis\nfor the estimation of the error of a lattice rule for an integral of a\none-periodic function over the unit cube in s dimensions. The lattice rule is\nrandomized using independent random shifts in each coordinate direction that\nare uniformly distributed in the interval [0,1]. This randomized lattice rule\nresults in an unbiased estimator of the multiple integral. However, in\npractice, random variables that are independent and uniformly distributed on\n[0,1] are not available, since this would require an infinite number of random\nindependent bits. A more realistic practical implementation of the Cranley and\nPatterson randomization uses rs independent random bits, in the following way.\nThe lattice rule is randomized using independent random shifts in each\ncoordinate direction that are uniformly distributed on {0, 1/2^r, ...\n,(2^r-1)/2^r}, where r may be large. For a rank-1 lattice rule with 2^m\nquadrature points and r >= m, we show that this randomized lattice rule leads\nto an estimator of the multiple integral that typically has a large bias. We\ntherefore propose that these rs independent random bits be used to perform a\nnew randomization that employs an extension, in the number of quadrature\npoints, to a lattice rule with 2^(m+sr) quadrature points (leading to embedded\nlattice rules).This new randomization is shown to lead to an estimator of the\nmultiple integral that has much smaller bias.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 00:58:55 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Kabaila", "Paul", ""]]}, {"id": "1406.0808", "submitter": "Pietro Coretto", "authors": "Pietro Coretto and Christian Hennig", "title": "Robust improper maximum likelihood: tuning, computation, and a\n  comparison with other methods for robust Gaussian clustering", "comments": null, "journal-ref": "Journal of the American Statistical Association 111(516), pp.\n  1648--1659 (2016)", "doi": "10.1080/01621459.2015.1100996", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two main topics of this paper are the introduction of the \"optimally\ntuned improper maximum likelihood estimator\" (OTRIMLE) for robust clustering\nbased on the multivariate Gaussian model for clusters, and a comprehensive\nsimulation study comparing the OTRIMLE to Maximum Likelihood in Gaussian\nmixtures with and without noise component, mixtures of t-distributions, and the\nTCLUST approach for trimmed clustering. The OTRIMLE uses an improper constant\ndensity for modelling outliers and noise. This can be chosen optimally so that\nthe non-noise part of the data looks as close to a Gaussian mixture as\npossible. Some deviation from Gaussianity can be traded in for lowering the\nestimated noise proportion. Covariance matrix constraints and computation of\nthe OTRIMLE are also treated. In the simulation study, all methods are\nconfronted with setups in which their model assumptions are not exactly\nfulfilled, and in order to evaluate the experiments in a standardized way by\nmisclassification rates, a new model-based definition of \"true clusters\" is\nintroduced that deviates from the usual identification of mixture components\nwith clusters. In the study, every method turns out to be superior for one or\nmore setups, but the OTRIMLE achieves the most satisfactory overall\nperformance. The methods are also applied to two real datasets, one without and\none with known \"true\" clusters.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 13:25:25 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 16:48:18 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 09:58:15 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2015 17:43:02 GMT"}, {"version": "v5", "created": "Sat, 28 Jan 2017 18:51:12 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Coretto", "Pietro", ""], ["Hennig", "Christian", ""]]}, {"id": "1406.1102", "submitter": "Pinghua Gong", "authors": "Pinghua Gong and Jieping Ye", "title": "Linear Convergence of Variance-Reduced Stochastic Gradient without\n  Strong Convexity", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient algorithms estimate the gradient based on only one or a\nfew samples and enjoy low computational cost per iteration. They have been\nwidely used in large-scale optimization problems. However, stochastic gradient\nalgorithms are usually slow to converge and achieve sub-linear convergence\nrates, due to the inherent variance in the gradient computation. To accelerate\nthe convergence, some variance-reduced stochastic gradient algorithms, e.g.,\nproximal stochastic variance-reduced gradient (Prox-SVRG) algorithm, have\nrecently been proposed to solve strongly convex problems. Under the strongly\nconvex condition, these variance-reduced stochastic gradient algorithms achieve\na linear convergence rate. However, many machine learning problems are convex\nbut not strongly convex. In this paper, we introduce Prox-SVRG and its\nprojected variant called Variance-Reduced Projected Stochastic Gradient (VRPSG)\nto solve a class of non-strongly convex optimization problems widely used in\nmachine learning. As the main technical contribution of this paper, we show\nthat both VRPSG and Prox-SVRG achieve a linear convergence rate without strong\nconvexity. A key ingredient in our proof is a Semi-Strongly Convex (SSC)\ninequality which is the first to be rigorously proved for a class of\nnon-strongly convex problems in both constrained and regularized settings.\nMoreover, the SSC inequality is independent of algorithms and may be applied to\nanalyze other stochastic gradient algorithms besides VRPSG and Prox-SVRG, which\nmay be of independent interest. To the best of our knowledge, this is the first\nwork that establishes the linear convergence rate for the variance-reduced\nstochastic gradient algorithms on solving both constrained and regularized\nproblems without strong convexity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 16:37:33 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2015 14:44:37 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Gong", "Pinghua", ""], ["Ye", "Jieping", ""]]}, {"id": "1406.1234", "submitter": "Lijiang Chen", "authors": "Chen Lijiang", "title": "A Geometric Method to Obtain the Generation Probability of a Sentence", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"How to generate a sentence\" is the most critical and difficult problem in\nall the natural language processing technologies. In this paper, we present a\nnew approach to explain the generation process of a sentence from the\nperspective of mathematics. Our method is based on the premise that in our\nbrain a sentence is a part of a word network which is formed by many word\nnodes. Experiments show that the probability of the entire sentence can be\nobtained by the probabilities of single words and the probabilities of the\nco-occurrence of word pairs, which indicate that human use the synthesis method\nto generate a sentence.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 23:28:51 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Lijiang", "Chen", ""]]}, {"id": "1406.1245", "submitter": "Paul McNicholas", "authors": "Amay Cheam and Paul D. McNicholas", "title": "Modelling Receiver Operating Characteristic Curves Using Gaussian\n  Mixtures", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.04.010", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The receiver operating characteristic curve is widely applied in measuring\nthe performance of diagnostic tests. Many direct and indirect approaches have\nbeen proposed for modelling the ROC curve, and because of its tractability, the\nGaussian distribution has typically been used to model both populations. We\npropose using a Gaussian mixture model, leading to a more flexible approach\nthat better accounts for atypical data. Monte Carlo simulation is used to\ncircumvent the issue of absence of a closed-form. We show that our method\nperforms favourably when compared to the crude binormal curve and to the\nsemi-parametric frequentist binormal ROC using the famous LABROC procedure.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 00:12:23 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Cheam", "Amay", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1406.1440", "submitter": "Pierre Alquier", "authors": "Pierre Alquier, Vincent Cottet, Nicolas Chopin, Judith Rousseau", "title": "Bayesian matrix completion: prior specification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix estimation from incomplete measurements recently received\nincreased attention due to the emergence of several challenging applications,\nsuch as recommender systems; see in particular the famous Netflix challenge.\nWhile the behaviour of algorithms based on nuclear norm minimization is now\nwell understood, an as yet unexplored avenue of research is the behaviour of\nBayesian algorithms in this context. In this paper, we briefly review the\npriors used in the Bayesian literature for matrix completion. A standard\napproach is to assign an inverse gamma prior to the singular values of a\ncertain singular value decomposition of the matrix of interest; this prior is\nconjugate. However, we show that two other types of priors (again for the\nsingular values) may be conjugate for this model: a gamma prior, and a discrete\nprior. Conjugacy is very convenient, as it makes it possible to implement\neither Gibbs sampling or Variational Bayes. Interestingly enough, the maximum a\nposteriori for these different priors is related to the nuclear norm\nminimization problems. We also compare all these priors on simulated datasets,\nand on the classical MovieLens and Netflix datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 16:46:46 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 15:32:24 GMT"}, {"version": "v3", "created": "Wed, 22 Oct 2014 15:34:33 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Alquier", "Pierre", ""], ["Cottet", "Vincent", ""], ["Chopin", "Nicolas", ""], ["Rousseau", "Judith", ""]]}, {"id": "1406.2098", "submitter": "Jie Peng", "authors": "Ru Wang and Jie Peng", "title": "Learning directed acyclic graphs via bootstrap aggregating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models are graphical representations of probability\ndistributions. Graphical models have applications in many fields including\nbiology, social sciences, linguistic, neuroscience. In this paper, we propose\ndirected acyclic graphs (DAGs) learning via bootstrap aggregating. The proposed\nprocedure is named as DAGBag. Specifically, an ensemble of DAGs is first\nlearned based on bootstrap resamples of the data and then an aggregated DAG is\nderived by minimizing the overall distance to the entire ensemble. A family of\nmetrics based on the structural hamming distance is defined for the space of\nDAGs (of a given node set) and is used for aggregation. Under the\nhigh-dimensional-low-sample size setting, the graph learned on one data set\noften has excessive number of false positive edges due to over-fitting of the\nnoise. Aggregation overcomes over-fitting through variance reduction and thus\ngreatly reduces false positives. We also develop an efficient implementation of\nthe hill climbing search algorithm of DAG learning which makes the proposed\nmethod computationally competitive for the high-dimensional regime. The DAGBag\nprocedure is implemented in the R package dagbag.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 07:43:22 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Wang", "Ru", ""], ["Peng", "Jie", ""]]}, {"id": "1406.2660", "submitter": "Christian P. Robert", "authors": "Marco Banterle (CEREMADE, Universite Paris-Dauphine), Clara Grazian\n  (CEREMADE, Universite Paris-Dauphine), Christian P. Robert (CEREMADE,\n  Universite Paris-Dauphine, and University of Warwick)", "title": "Accelerating Metropolis-Hastings algorithms: Delayed acceptance with\n  prefetching", "comments": "20 pages, 12 figures, 2 tables, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MCMC algorithms such as Metropolis-Hastings algorithms are slowed down by the\ncomputation of complex target distributions as exemplified by huge datasets. We\noffer in this paper an approach to reduce the computational costs of such\nalgorithms by a simple and universal divide-and-conquer strategy. The idea\nbehind the generic acceleration is to divide the acceptance step into several\nparts, aiming at a major reduction in computing time that outranks the\ncorresponding reduction in acceptance probability. The division decomposes the\n\"prior x likelihood\" term into a product such that some of its components are\nmuch cheaper to compute than others. Each of the components can be sequentially\ncompared with a uniform variate, the first rejection signalling that the\nproposed value is considered no further, This approach can in turn be\naccelerated as part of a prefetching algorithm taking advantage of the parallel\nabilities of the computer at hand. We illustrate those accelerating features on\na series of toy and realistic examples.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 18:48:32 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Banterle", "Marco", "", "CEREMADE, Universite Paris-Dauphine"], ["Grazian", "Clara", "", "CEREMADE, Universite Paris-Dauphine"], ["Robert", "Christian P.", "", "CEREMADE,\n  Universite Paris-Dauphine, and University of Warwick"]]}, {"id": "1406.2780", "submitter": "Conrad Burden", "authors": "Conrad J. Burden", "title": "An R Implementation of the Polya-Aeppli Distribution", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient implementation of the Polya-Aeppli, or geometirc compound\nPoisson, distribution in the statistical programming language R is presented.\nThe implementation is available as the package polyaAeppli and consists of\nfunctions for the mass function, cumulative distribution function, quantile\nfunction and random variate generation with those parameters conventionally\nprovided for standard univatiate probability distributions in the stats package\nin R\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 05:15:59 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Burden", "Conrad J.", ""]]}, {"id": "1406.2839", "submitter": "Nicolas Chopin", "authors": "Simon Barthelm\\'e and Nicolas Chopin", "title": "The Poisson transform for unnormalised statistical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to standard statistical models, unnormalised statistical models only\nspecify the likelihood function up to a constant. While such models are natural\nand popular, the lack of normalisation makes inference much more difficult.\nHere we show that inferring the parameters of a unnormalised model on a space\n$\\Omega$ can be mapped onto an equivalent problem of estimating the intensity\nof a Poisson point process on $\\Omega$. The unnormalised statistical model now\nspecifies an intensity function that does not need to be normalised.\nEffectively, the normalisation constant may now be inferred as just another\nparameter, at no loss of information. The result can be extended to cover\nnon-IID models, which includes for example unnormalised models for sequences of\ngraphs (dynamical graphs), or for sequences of binary vectors. As a\nconsequence, we prove that unnormalised parameteric inference in non-IID models\ncan be turned into a semi-parametric estimation problem. Moreover, we show that\nthe noise-contrastive divergence of Gutmann & Hyv\\\"arinen (2012) can be\nunderstood as an approximation of the Poisson transform, and extended to\nnon-IID settings. We use our results to fit spatial Markov chain models of eye\nmovements, where the Poisson transform allows us to turn a highly non-standard\nmodel into vanilla semi-parametric logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 09:23:21 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 15:05:57 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Barthelm\u00e9", "Simon", ""], ["Chopin", "Nicolas", ""]]}, {"id": "1406.3183", "submitter": "Pete Bunch", "authors": "Pete Bunch, Simon Godsill", "title": "Approximations of the Optimal Importance Density using Gaussian Particle\n  Flow Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently developed particle flow algorithms provide an alternative to\nimportance sampling for drawing particles from a posterior distribution, and a\nnumber of particle filters based on this principle have been proposed. Samples\nare drawn from the prior and then moved according to some dynamics over an\ninterval of pseudo-time such that their final values are distributed according\nto the desired posterior. In practice, implementing a particle flow sampler\nrequires multiple layers of approximation, with the result that the final\nsamples do not in general have the correct posterior distribution. In this\npaper we consider using an approximate Gaussian flow for sampling with a class\nof nonlinear Gaussian models. We use the particle flow within an importance\nsampler, correcting for the discrepancy between the target and actual densities\nwith importance weights. We present a suitable numerical integration procedure\nfor use with this flow and an accompanying step-size control algorithm. In a\nfiltering context, we use the particle flow to sample from the optimal\nimportance density, rather than the filtering density itself, avoiding the need\nto make analytical or numerical approximations of the predictive density.\nSimulations using particle flow importance sampling within a particle filter\ndemonstrate significant improvement over standard approximations of the optimal\nimportance density, and the algorithm falls within the standard sequential\nMonte Carlo framework.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 10:19:07 GMT"}, {"version": "v2", "created": "Fri, 25 Jul 2014 16:36:48 GMT"}, {"version": "v3", "created": "Thu, 27 Nov 2014 11:18:45 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Bunch", "Pete", ""], ["Godsill", "Simon", ""]]}, {"id": "1406.3218", "submitter": "B{\\l}a\\.zej Miasojedow", "authors": "Mateusz Krzysztof {\\L}\\k{a}cki, B{\\l}a\\.zej Miasojedow", "title": "State dependent swap strategies and adaptive adjusting of number of\n  temperatures in Parallel Tempering algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present extensions to the original adaptive parallel\ntempering algorithm. Two different approaches are presented. In the first one\nwe introduce state-dependent strategies using current information to perform a\nswap step. It encompasses a wide family of potential moves including the\nstandard one and Equi Energy type move, without any loss in tractability. In\nthe second one, we introduce online adjustment of the number of temperatures.\nNumerical experiments demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 12:44:12 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 11:12:40 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["\u0141\u0105cki", "Mateusz Krzysztof", ""], ["Miasojedow", "B\u0142a\u017cej", ""]]}, {"id": "1406.3774", "submitter": "Roland Langrock", "authors": "Roland Langrock and Thomas Kneib and Richard Glennie and Th\\'eo\n  Michelot", "title": "Markov-switching generalized additive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Markov-switching regression models, i.e. models for time series\nregression analyses where the functional relationship between covariates and\nresponse is subject to regime switching controlled by an unobservable Markov\nchain. Building on the powerful hidden Markov model machinery and the methods\nfor penalized B-splines routinely used in regression analyses, we develop a\nframework for nonparametrically estimating the functional form of the effect of\nthe covariates in such a regression model, assuming an additive structure of\nthe predictor. The resulting class of Markov-switching generalized additive\nmodels is immensely flexible, and contains as special cases the common\nparametric Markov-switching regression models and also generalized additive and\ngeneralized linear models. The feasibility of the suggested maximum penalized\nlikelihood approach is demonstrated by simulation and further illustrated by\nmodelling how energy price in Spain depends on the Euro/Dollar exchange rate.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 21:24:47 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 20:29:33 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Langrock", "Roland", ""], ["Kneib", "Thomas", ""], ["Glennie", "Richard", ""], ["Michelot", "Th\u00e9o", ""]]}, {"id": "1406.3843", "submitter": "Yichuan Zhang", "authors": "Yichuan Zhang, Charles Sutton", "title": "Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian\n  Hierarchical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from hierarchical Bayesian models is often difficult for MCMC\nmethods, because of the strong correlations between the model parameters and\nthe hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC)\nmethods have significant potential advantages in this setting, but are\ncomputationally expensive. We introduce a new RMHMC method, which we call\nsemi-separable Hamiltonian Monte Carlo, which uses a specially designed mass\nmatrix that allows the joint Hamiltonian over model parameters and\nhyperparameters to decompose into two simpler Hamiltonians. This structure is\nexploited by a new integrator which we call the alternating blockwise leapfrog\nalgorithm. The resulting method can mix faster than simpler Gibbs sampling\nwhile being simpler and more efficient than previous instances of RMHMC.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 19:03:46 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Zhang", "Yichuan", ""], ["Sutton", "Charles", ""]]}, {"id": "1406.3852", "submitter": "Matthew Blaschko", "authors": "Wacha Bounliphone, Arthur Gretton, Arthur Tenenhaus (E3S), Matthew\n  Blaschko (INRIA Saclay - Ile de France, CVN)", "title": "A low variance consistent test of relative dependency", "comments": "International Conference on Machine Learning, Jul 2015, Lille, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel non-parametric statistical hypothesis test of relative\ndependence between a source variable and two candidate target variables. Such a\ntest enables us to determine whether one source variable is significantly more\ndependent on a first target variable or a second. Dependence is measured via\nthe Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of\nempirical dependence measures (source-target 1, source-target 2). We test\nwhether the first dependence measure is significantly larger than the second.\nModeling the covariance between these HSIC statistics leads to a provably more\npowerful test than the construction of independent HSIC statistics by\nsub-sampling. The resulting test is consistent and unbiased, and (being based\non U-statistics) has favorable convergence properties. The test can be computed\nin quadratic time, matching the computational complexity of standard empirical\nHSIC estimators. The effectiveness of the test is demonstrated on several\nreal-world problems: we identify language groups from a multilingual corpus,\nand we prove that tumor location is more dependent on gene expression than\nchromosomal imbalances. Source code is available for download at\nhttps://github.com/wbounliphone/reldep.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 19:23:11 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 17:12:58 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 08:25:19 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Bounliphone", "Wacha", "", "E3S"], ["Gretton", "Arthur", "", "E3S"], ["Tenenhaus", "Arthur", "", "E3S"], ["Blaschko", "Matthew", "", "INRIA Saclay - Ile de France, CVN"]]}, {"id": "1406.4068", "submitter": "Jeffrey Morris", "authors": "Jeffrey S. Morris", "title": "Functional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis (FDA) involves the analysis of data whose ideal\nunits of observation are functions defined on some continuous domain, and the\nobserved data consist of a sample of functions taken from some population,\nsampled on a discrete grid. Ramsay and Silverman's 1997 textbook sparked the\ndevelopment of this field, which has accelerated in the past 10 years to become\none of the fastest growing areas of statistics, fueled by the growing number of\napplications yielding this type of data. One unique characteristic of FDA is\nthe need to combine information both across and within functions, which Ramsay\nand Silverman called replication and regularization, respectively. This article\nwill focus on functional regression, the area of FDA that has received the most\nattention in applications and methodological development. First will be an\nintroduction to basis functions, key building blocks for regularization in\nfunctional regression methods, followed by an overview of functional regression\nmethods, split into three types: [1] functional predictor regression\n(scalar-on-function), [2] functional response regression (function-on-scalar)\nand [3] function-on-function regression. For each, the role of replication and\nregularization will be discussed and the methodological development described\nin a roughly chronological manner, at times deviating from the historical\ntimeline to group together similar methods. The primary focus is on modeling\nand methodology, highlighting the modeling structures that have been developed\nand the various regularization approaches employed. At the end is a brief\ndiscussion describing potential areas of future development in this field.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 16:56:35 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Morris", "Jeffrey S.", ""]]}, {"id": "1406.4306", "submitter": "Xiaodong Luo", "authors": "Xiaodong Luo, Rolf J. Lorentzen, Andreas S. Stordal and Geir\n  N{\\ae}vdal", "title": "Toward an enhanced Bayesian estimation framework for multiphase flow\n  soft-sensing", "comments": "To appear in Inverse Problems", "journal-ref": null, "doi": "10.1088/0266-5611/30/11/114012", "report-no": null, "categories": "stat.ME math.OC physics.data-an physics.geo-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work the authors study the multiphase flow soft-sensing problem based\non a previously established framework. There are three functional modules in\nthis framework, namely, a transient well flow model that describes the response\nof certain physical variables in a well, for instance, temperature, velocity\nand pressure, to the flow rates entering and leaving the well zones; a Markov\njump process that is designed to capture the potential abrupt changes in the\nflow rates; and an estimation method that is adopted to estimate the underlying\nflow rates based on the measurements from the physical sensors installed in the\nwell.\n  In the previous studies, the variances of the flow rates in the Markov jump\nprocess are chosen manually. To fill this gap, in the current work two\nautomatic approaches are proposed in order to optimize the variance estimation.\nThrough a numerical example, we show that, when the estimation framework is\nused in conjunction with these two proposed variance-estimation approaches, it\ncan achieve reasonable performance in terms of matching both the measurements\nof the physical sensors and the true underlying flow rates.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 10:32:30 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Luo", "Xiaodong", ""], ["Lorentzen", "Rolf J.", ""], ["Stordal", "Andreas S.", ""], ["N\u00e6vdal", "Geir", ""]]}, {"id": "1406.4307", "submitter": "Xiaodong Luo", "authors": "Xiaodong Luo and Ibrahim Hoteit", "title": "Ensemble Kalman filtering with residual nudging: an extension to state\n  estimation problems with nonlinear observation operators", "comments": "To appear in Monthly Weather Review", "journal-ref": null, "doi": "10.1175/MWR-D-13-00328.1", "report-no": null, "categories": "physics.ao-ph math.OC nlin.CD physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ensemble Kalman filter (EnKF) is an efficient algorithm for many data\nassimilation problems. In certain circumstances, however, divergence of the\nEnKF might be spotted. In previous studies, the authors proposed an\nobservation-space-based strategy, called residual nudging, to improve the\nstability of the EnKF when dealing with linear observation operators. The main\nidea behind residual nudging is to monitor and, if necessary, adjust the\ndistances (misfits) between the real observations and the simulated ones of the\nstate estimates, in the hope that by doing so one may be able to obtain better\nestimation accuracy.\n  In the present study, residual nudging is extended and modified in order to\nhandle nonlinear observation operators. Such extension and modification result\nin an iterative filtering framework that, under suitable conditions, is able to\nachieve the objective of residual nudging for data assimilation problems with\nnonlinear observation operators. The 40 dimensional Lorenz 96 model is used to\nillustrate the performance of the iterative filter. Numerical results show\nthat, while a normal EnKF may diverge with nonlinear observation operators, the\nproposed iterative filter remains stable and leads to reasonable estimation\naccuracy under various experimental settings.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 10:33:46 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Luo", "Xiaodong", ""], ["Hoteit", "Ibrahim", ""]]}, {"id": "1406.4406", "submitter": "Sophie Donnet", "authors": "Sophie Donnet, Vincent Rivoirard, Judith Rousseau and Catia Scricciolo", "title": "Posterior concentration rates for empirical Bayes procedures, with\n  applications to Dirichlet Process mixtures", "comments": "With supplementary material", "journal-ref": null, "doi": null, "report-no": "1001815", "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide general conditions to check on the model and the\nprior to derive posterior concentration rates for data-dependent priors (or\nempirical Bayes approaches). We aim at providing conditions that are close to\nthe conditions provided in the seminal paper by Ghosal and van der Vaart\n(2007a). We then apply the general theorem to two different settings: the\nestimation of a density using Dirichlet process mixtures of Gaussian random\nvariables with base measure depending on some empirical quantities and the\nestimation of the intensity of a counting process under the Aalen model. A\nsimulation study for inhomogeneous Poisson processes also illustrates our\nresults. In the former case we also derive some results on the estimation of\nthe mixing density and on the deconvolution problem. In the latter, we provide\na general theorem on posterior concentration rates for counting processes with\nAalen multiplicative intensity with priors not depending on the data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 15:46:44 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Donnet", "Sophie", ""], ["Rivoirard", "Vincent", ""], ["Rousseau", "Judith", ""], ["Scricciolo", "Catia", ""]]}, {"id": "1406.4704", "submitter": "Moritz Schauer", "authors": "Frank van der Meulen and Moritz Schauer", "title": "Bayesian estimation of discretely observed multi-dimensional diffusion\n  processes using guided proposals", "comments": null, "journal-ref": "Electron. J. Statist. Volume 11, Number 1 (2017), 2358-2396", "doi": "10.1214/17-EJS1290", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of parameters of a diffusion based on discrete time observations\nposes a difficult problem due to the lack of a closed form expression for the\nlikelihood. From a Bayesian computational perspective it can be casted as a\nmissing data problem where the diffusion bridges in between discrete-time\nobservations are missing. The computational problem can then be dealt with\nusing a Markov-chain Monte-Carlo method known as data-augmentation. If unknown\nparameters appear in the diffusion coefficient, direct implementation of\ndata-augmentation results in a Markov chain that is reducible. Furthermore,\ndata-augmentation requires efficient sampling of diffusion bridges, which can\nbe difficult, especially in the multidimensional case.\n  We present a general framework to deal with with these problems that does not\nrely on discretisation. The construction generalises previous approaches and\nsheds light on the assumptions necessary to make these approaches work. We\ndefine a random-walk type Metropolis-Hastings sampler for updating diffusion\nbridges. Our methods are illustrated using guided proposals for sampling\ndiffusion bridges. These are Markov processes obtained by adding a guiding term\nto the drift of the diffusion. We give general guidelines on the construction\nof these proposals and introduce a time change and scaling of the guided\nproposal that reduces discretisation error. Numerical examples demonstrate the\nperformance of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 13:07:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 18:50:17 GMT"}, {"version": "v3", "created": "Mon, 11 Jul 2016 16:04:53 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""]]}, {"id": "1406.4806", "submitter": "Jeroen Ooms", "authors": "Jeroen Ooms", "title": "The OpenCPU System: Towards a Universal Interface for Scientific\n  Computing through Separation of Concerns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications integrating analysis components require a programmable interface\nwhich defines statistical operations independently of any programming language.\nBy separating concerns of scientific computing from application and\nimplementation details we can derive an interoperable API for data analysis.\nBut what exactly are the concerns of scientific computing? To answer this\nquestion, the paper starts with an exploration of the purpose, problems,\ncharacteristics, struggles, culture, and community of this unique branch of\ncomputing. By mapping out the domain logic, we try to unveil the fundamental\nprinciples and concepts behind statistical software. Along the way we highlight\nimportant problems and bottlenecks that need to be addressed by the system in\norder to facilitate reliable and scalable analysis units. Finally, the OpenCPU\nsoftware is introduced as an example implementation that builds on HTTP and R\nto expose a simple, abstracted interface for scientific computing.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 00:03:52 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Ooms", "Jeroen", ""]]}, {"id": "1406.4993", "submitter": "Fredrik Lindsten", "authors": "Fredrik Lindsten, Adam M. Johansen, Christian A. Naesseth, Bonnie\n  Kirkpatrick, Thomas B. Sch\\\"on, John Aston, Alexandre Bouchard-C\\^ot\\'e", "title": "Divide-and-Conquer with Sequential Monte Carlo", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 26(2):445-458,\n  2017", "doi": "10.1080/10618600.2016.1237363", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel class of Sequential Monte Carlo (SMC) algorithms,\nappropriate for inference in probabilistic graphical models. This class of\nalgorithms adopts a divide-and-conquer approach based upon an auxiliary\ntree-structured decomposition of the model of interest, turning the overall\ninferential task into a collection of recursively solved sub-problems. The\nproposed method is applicable to a broad class of probabilistic graphical\nmodels, including models with loops. Unlike a standard SMC sampler, the\nproposed Divide-and-Conquer SMC employs multiple independent populations of\nweighted particles, which are resampled, merged, and propagated as the method\nprogresses. We illustrate empirically that this approach can outperform\nstandard methods in terms of the accuracy of the posterior expectation and\nmarginal likelihood approximations. Divide-and-Conquer SMC also opens up novel\nparallel implementation options and the possibility of concentrating the\ncomputational effort on the most challenging sub-problems. We demonstrate its\nperformance on a Markov random field and on a hierarchical logistic regression\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 10:01:15 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 10:59:00 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Lindsten", "Fredrik", ""], ["Johansen", "Adam M.", ""], ["Naesseth", "Christian A.", ""], ["Kirkpatrick", "Bonnie", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Aston", "John", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""]]}, {"id": "1406.5005", "submitter": "Jonathan Rougier", "authors": "Jonathan Rougier and Andrew Zammit Mangion and Nana Schoen", "title": "Computation and Visualisation for large-scale Gaussian updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In geostatistics, and also in other applications in science and engineering,\nwe are now performing updates on Gaussian process models with many thousands or\neven millions of components. These large-scale inferences involve computational\nchallenges, because the updating equations cannot be solved as written, owing\nto the size and cost of the matrix operations. They also involve\nrepresentational challenges, to account for judgements of heterogeneity\nconcerning the underlying fields, and diverse sources of observations.\n  Diagnostics are particularly valuable in this situation. We present a\ndiagnostic and visualisation tool for large-scale Gaussian updates, the `medal\nplot'. This shows the updated uncertainty for each observation, and also\nsummarises the sharing of information across observations, as a proxy for the\nsharing of information across the state vector. It allows us to `sanity-check'\nthe code implementing the update, but it can also reveal unexpected features in\nour modelling. We discuss computational issues for large-scale updates, and we\nillustrate with an application to assess mass trends in the Antarctic Ice\nSheet.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 11:00:50 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Rougier", "Jonathan", ""], ["Mangion", "Andrew Zammit", ""], ["Schoen", "Nana", ""]]}, {"id": "1406.5062", "submitter": "Akram Shalabi", "authors": "A Shalabi, A C C Coolen, and E de Rinaldis", "title": "Overcoming computational inability to predict clinical outcome from\n  high-dimensional patient data using Bayesian methods", "comments": "Conference Paper 3rd ICCSISCT 2014 - Sydney, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical outcome prediction from high-dimensional data is problematic in the\ncommon setting where there is only a relatively small number of samples. The\nimbalance causes data overfitting, and outcome prediction becomes\ncomputationally expensive or even impossible. We propose a Bayesian outcome\nprediction method that can be applied to data of arbitrary dimension d, from 2\noutcome classes, and reduces overfitting without any approximations at\nparameter level. This is achieved by avoiding numerical integration or\napproximation, and solving the Bayesian integrals analytically. We thereby\nreduce the dimension of numerical integrals from 2d dimensions to 4, for any d.\nFor large d, this is reduced further to 3, and we obtain a simple outcome\nprediction formula without integrals in leading order for very large d. We\ncompare our method to the mclustDA method (Fraley and Raftery 2002), using\nsimulated and real data sets. Our method perform as well as or better than\nmclustDA in low dimensions d. In large dimensions d, mclustDA breaks down due\nto computational limitations, while our method provides a feasible and\ncomputationally efficient alternative.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 14:32:48 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Shalabi", "A", ""], ["Coolen", "A C C", ""], ["de Rinaldis", "E", ""]]}, {"id": "1406.5550", "submitter": "Andrei Zinovyev Dr.", "authors": "Alexander N. Gorban, Alexander Pitenko, Andrei Zinovyev", "title": "ViDaExpert: user-friendly tool for nonlinear visualization and analysis\n  of multidimensional vectorial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  ViDaExpert is a tool for visualization and analysis of multidimensional\nvectorial data. ViDaExpert is able to work with data tables of \"object-feature\"\ntype that might contain numerical feature values as well as textual labels for\nrows (objects) and columns (features). ViDaExpert implements several\nstatistical methods such as standard and weighted Principal Component Analysis\n(PCA) and the method of elastic maps (non-linear version of PCA), Linear\nDiscriminant Analysis (LDA), multilinear regression, K-Means clustering, a\nvariant of decision tree construction algorithm. Equipped with several\nuser-friendly dialogs for configuring data point representations (size, shape,\ncolor) and fast 3D viewer, ViDaExpert is a handy tool allowing to construct an\ninteractive 3D-scene representing a table of data in multidimensional space and\nperform its quick and insightfull statistical analysis, from basic to advanced\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 22:31:25 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 14:40:04 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Gorban", "Alexander N.", ""], ["Pitenko", "Alexander", ""], ["Zinovyev", "Andrei", ""]]}, {"id": "1406.5823", "submitter": "Martin M\\\"achler", "authors": "Douglas Bates, Martin M\\\"achler, Ben Bolker, Steve Walker", "title": "Fitting Linear Mixed-Effects Models using lme4", "comments": "51 pages, including R code, and an appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood or restricted maximum likelihood (REML) estimates of the\nparameters in linear mixed-effects models can be determined using the lmer\nfunction in the lme4 package for R. As for most model-fitting functions in R,\nthe model is described in an lmer call by a formula, in this case including\nboth fixed- and random-effects terms. The formula and data together determine a\nnumerical representation of the model from which the profiled deviance or the\nprofiled REML criterion can be evaluated as a function of some of the model\nparameters. The appropriate criterion is optimized, using one of the\nconstrained optimization functions in R, to provide the parameter estimates. We\ndescribe the structure of the model, the steps in evaluating the profiled\ndeviance or REML criterion, and the structure of classes or types that\nrepresents such a model. Sufficient detail is included to allow specialization\nof these structures by users who wish to write functions to fit specialized\nlinear mixed models, such as models incorporating pedigrees or smoothing\nsplines, that are not easily expressible in the formula language used by lmer.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 07:54:48 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Bates", "Douglas", ""], ["M\u00e4chler", "Martin", ""], ["Bolker", "Ben", ""], ["Walker", "Steve", ""]]}, {"id": "1406.6010", "submitter": "Anthony Lee", "authors": "Anthony Lee and Nick Whiteley", "title": "Forest resampling for distributed sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper brings explicit considerations of distributed computing\narchitectures and data structures into the rigorous design of Sequential Monte\nCarlo (SMC) methods. A theoretical result established recently by the authors\nshows that adapting interaction between particles to suitably control the\nEffective Sample Size (ESS) is sufficient to guarantee stability of SMC\nalgorithms. Our objective is to leverage this result and devise algorithms\nwhich are thus guaranteed to work well in a distributed setting. We make three\nmain contributions to achieve this. Firstly, we study mathematical properties\nof the ESS as a function of matrices and graphs that parameterize the\ninteraction amongst particles. Secondly, we show how these graphs can be\ninduced by tree data structures which model the logical network topology of an\nabstract distributed computing environment. Thirdly, we present efficient\ndistributed algorithms that achieve the desired ESS control, perform resampling\nand operate on forests associated with these trees.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 18:15:27 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Lee", "Anthony", ""], ["Whiteley", "Nick", ""]]}, {"id": "1406.6288", "submitter": "Jean-Michel Marin", "authors": "Pierre Pudlo, Jean-Michel Marin (IMAG and IBC, Universite de\n  Montpellier), Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier (CBGP, INRA,\n  Montpellier), Christian P. Robert (Universite Paris-Dauphine and University\n  of Warwick)", "title": "Reliable ABC model choice via random forests", "comments": "39 pages, 15 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.PE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods provide an elaborate approach\nto Bayesian inference on complex models, including model choice. Both\ntheoretical arguments and simulation experiments indicate, however, that model\nposterior probabilities may be poorly evaluated by standard ABC techniques. We\npropose a novel approach based on a machine learning tool named random forests\nto conduct selection among the highly complex models covered by ABC algorithms.\nWe thus modify the way Bayesian model selection is both understood and\noperated, in that we rephrase the inferential goal as a classification problem,\nfirst predicting the model that best fits the data with random forests and\npostponing the approximation of the posterior probability of the predicted MAP\nfor a second stage also relying on random forests. Compared with earlier\nimplementations of ABC model choice, the ABC random forest approach offers\nseveral potential improvements: (i) it often has a larger discriminative power\namong the competing models, (ii) it is more robust against the number and\nchoice of statistics summarizing the data, (iii) the computing effort is\ndrastically reduced (with a gain in computation efficiency of at least fifty),\nand (iv) it includes an approximation of the posterior probability of the\nselected model. The call to random forests will undoubtedly extend the range of\nsize of datasets and complexity of models that ABC can handle. We illustrate\nthe power of this novel methodology by analyzing controlled experiments as well\nas genuine population genetics datasets. The proposed methodologies are\nimplemented in the R package abcrf available on the CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 16:03:32 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 09:45:03 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2015 15:19:41 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Pudlo", "Pierre", "", "IMAG and IBC, Universite de\n  Montpellier"], ["Marin", "Jean-Michel", "", "IMAG and IBC, Universite de\n  Montpellier"], ["Estoup", "Arnaud", "", "CBGP, INRA,\n  Montpellier"], ["Cornuet", "Jean-Marie", "", "CBGP, INRA,\n  Montpellier"], ["Gautier", "Mathieu", "", "CBGP, INRA,\n  Montpellier"], ["Robert", "Christian P.", "", "Universite Paris-Dauphine and University\n  of Warwick"]]}, {"id": "1406.6652", "submitter": "Vinayak Rao", "authors": "Vinayak Rao, Lizhen Lin, David Dunson", "title": "Data augmentation for models based on rejection sampling", "comments": "6 figures. arXiv admin note: text overlap with arXiv:1311.0907", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data augmentation scheme to perform Markov chain Monte Carlo\ninference for models where data generation involves a rejection sampling\nalgorithm. Our idea, which seems to be missing in the literature, is a simple\nscheme to instantiate the rejected proposals preceding each data point. The\nresulting joint probability over observed and rejected variables can be much\nsimpler than the marginal distribution over the observed variables, which often\ninvolves intractable integrals. We consider three problems, the first being the\nmodeling of flow-cytometry measurements subject to truncation. The second is a\nBayesian analysis of the matrix Langevin distribution on the Stiefel manifold,\nand the third, Bayesian inference for a nonparametric Gaussian process density\nmodel. The latter two are instances of problems where Markov chain Monte Carlo\ninference is doubly-intractable. Our experiments demonstrate superior\nperformance over state-of-the-art sampling algorithms for such problems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 17:48:01 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 15:42:14 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Rao", "Vinayak", ""], ["Lin", "Lizhen", ""], ["Dunson", "David", ""]]}, {"id": "1406.7536", "submitter": "Giuseppe Vinci", "authors": "Giuseppe Vinci, Peter Freeman, Jeffrey Newman, Larry Wasserman and\n  Christopher Genovese", "title": "Estimating the distribution of Galaxy Morphologies on a continuous space", "comments": "4 pages, 3 figures, Statistical Challenges in 21st Century Cosmology,\n  Proceedings IAU Symposium No. 306, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA astro-ph.CO stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incredible variety of galaxy shapes cannot be summarized by human defined\ndiscrete classes of shapes without causing a possibly large loss of\ninformation. Dictionary learning and sparse coding allow us to reduce the high\ndimensional space of shapes into a manageable low dimensional continuous vector\nspace. Statistical inference can be done in the reduced space via probability\ndistribution estimation and manifold estimation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 18:47:18 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Vinci", "Giuseppe", ""], ["Freeman", "Peter", ""], ["Newman", "Jeffrey", ""], ["Wasserman", "Larry", ""], ["Genovese", "Christopher", ""]]}, {"id": "1406.7648", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Bayesian Network Constraint-Based Structure Learning Algorithms:\n  Parallel and Optimised Implementations in the bnlearn R Package", "comments": "20 pages, 4 figures", "journal-ref": "Journal of Statistical Software (2017), 77(2), 1-20", "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.MS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known in the literature that the problem of learning the structure\nof Bayesian networks is very hard to tackle: its computational complexity is\nsuper-exponential in the number of nodes in the worst case and polynomial in\nmost real-world scenarios.\n  Efficient implementations of score-based structure learning benefit from past\nand current research in optimisation theory, which can be adapted to the task\nby using the network score as the objective function to maximise. This is not\ntrue for approaches based on conditional independence tests, called\nconstraint-based learning algorithms. The only optimisation in widespread use,\nbacktracking, leverages the symmetries implied by the definitions of\nneighbourhood and Markov blanket.\n  In this paper we illustrate how backtracking is implemented in recent\nversions of the bnlearn R package, and how it degrades the stability of\nBayesian network structure learning for little gain in terms of speed. As an\nalternative, we describe a software architecture and framework that can be used\nto parallelise constraint-based structure learning algorithms (also implemented\nin bnlearn) and we demonstrate its performance using four reference networks\nand two real-world data sets from genetics and systems biology. We show that on\nmodern multi-core or multiprocessor hardware parallel implementations are\npreferable over backtracking, which was developed when single-processor\nmachines were the norm.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 09:56:20 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 10:27:23 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1406.7863", "submitter": "Derick Rivers", "authors": "Derick L. Rivers and Edward L. Boone", "title": "A Dynamic Approach to Linear Statistical Calibration with an Application\n  in Microwave Radiometry", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of statistical calibration of a measuring instrument can be\nframed both in a statistical context as well as in an engineering context. In\nthe first, the problem is dealt with by distinguishing between the 'classical'\napproach and the 'inverse' regression approach. Both of these models are static\nmodels and are used to estimate exact measurements from measurements that are\naffected by error. In the engineering context, the variables of interest are\nconsidered to be taken at the time at which you observe it. The Bayesian time\nseries analysis method of Dynamic Linear Models (DLM) can be used to monitor\nthe evolution of the measures, thus introducing an dynamic approach to\nstatistical calibration. The research presented employs the use of Bayesian\nmethodology to perform statistical calibration. The DLM's framework is used to\ncapture the time-varying parameters that maybe changing or drifting over time.\nTwo separate DLM based models are presented in this paper. A simulation study\nis conducted where the two models are compared to some well known 'static'\ncalibration approaches in the literature from both the frequentist and Bayesian\nperspectives. The focus of the study is to understand how well the dynamic\nstatistical calibration methods performs under various signal-to-noise ratios,\nr. The posterior distributions of the estimated calibration points as well as\nthe 95% coverage intervals are compared by statistical summaries. These dynamic\nmethods are applied to a microwave radiometry data set.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 19:29:46 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 15:32:50 GMT"}, {"version": "v3", "created": "Wed, 9 Jul 2014 20:18:59 GMT"}, {"version": "v4", "created": "Sun, 13 Jul 2014 11:07:25 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Rivers", "Derick L.", ""], ["Boone", "Edward L.", ""]]}]