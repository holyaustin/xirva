[{"id": "1302.0261", "submitter": "Yiyuan She", "authors": "Florentina Bunea, Johannes Lederer, Yiyuan She", "title": "The Group Square-Root Lasso: Theoretical Properties and Fast Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the Group Square-Root Lasso (GSRL) method for\nestimation in high dimensional sparse regression models with group structure.\nThe new estimator minimizes the square root of the residual sum of squares plus\na penalty term proportional to the sum of the Euclidean norms of groups of the\nregression parameter vector. The net advantage of the method over the existing\nGroup Lasso (GL)-type procedures consists in the form of the proportionality\nfactor used in the penalty term, which for GSRL is independent of the variance\nof the error terms. This is of crucial importance in models with more\nparameters than the sample size, when estimating the variance of the noise\nbecomes as difficult as the original problem. We show that the GSRL estimator\nadapts to the unknown sparsity of the regression vector, and has the same\noptimal estimation and prediction accuracy as the GL estimators, under the same\nminimal conditions on the model. This extends the results recently established\nfor the Square-Root Lasso, for sparse regression without group structure.\nMoreover, as a new type of result for Square-Root Lasso methods, with or\nwithout groups, we study correct pattern recovery, and show that it can be\nachieved under conditions similar to those needed by the Lasso or\nGroup-Lasso-type methods, but with a simplified tuning strategy. We implement\nour method via a new algorithm, with proved convergence properties, which,\nunlike existing methods, scales well with the dimension of the problem. Our\nsimulation studies support strongly our theoretical findings.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 19:48:01 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 15:16:28 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Bunea", "Florentina", ""], ["Lederer", "Johannes", ""], ["She", "Yiyuan", ""]]}, {"id": "1302.1095", "submitter": "Gianluca Campanella", "authors": "Gianluca Campanella, Maria De Iorio, Ajay Jasra and Marc Chadeau-Hyam", "title": "The TimeMachine for Inference on Stochastic Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of genealogical trees backwards in time, from observations up\nto the most recent common ancestor (MRCA), is hindered by the fact that, while\napproaching the root of the tree, coalescent events become rarer, with a\ncorresponding increase in computation time. The recently proposed \"Time\nMachine\" tackles this issue by stopping the simulation of the tree before\nreaching the MRCA and correcting for the induced bias. We present a\ncomputationally efficient implementation of this approach that exploits\nmultithreading.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 16:26:24 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Campanella", "Gianluca", ""], ["De Iorio", "Maria", ""], ["Jasra", "Ajay", ""], ["Chadeau-Hyam", "Marc", ""]]}, {"id": "1302.1154", "submitter": "Zuofeng Shang", "authors": "Zuofeng Shang and Ping Li", "title": "Bayesian Ultrahigh-Dimensional Screening Via MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the theoretical and numerical property of a fully Bayesian model\nselection method in sparse ultrahigh-dimensional settings, i.e., $p\\gg n$,\nwhere $p$ is the number of covariates and $n$ is the sample size. Our method\nconsists of (1) a hierarchical Bayesian model with a novel prior placed over\nthe model space which includes a hyperparameter $t_n$ controlling the model\nsize, and (2) an efficient MCMC algorithm for automatic and stochastic search\nof the models. Our theory shows that, when specifying $t_n$ correctly, the\nproposed method yields selection consistency, i.e., the posterior probability\nof the true model asymptotically approaches one; when $t_n$ is misspecified,\nthe selected model is still asymptotically nested in the true model. The theory\nalso reveals insensitivity of the selection result with respect to the choice\nof $t_n$. In implementations, a reasonable prior is further assumed on $t_n$\nwhich allows us to draw its samples stochastically. Our approach conducts\nselection, estimation and even inference in a unified framework. No additional\nprescreening or dimension reduction step is needed. Two novel $g$-priors are\nproposed to make our approach more flexible. A simulation study is given to\ndisplay the numerical advantage of our method.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 19:11:46 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 22:23:44 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2013 15:05:39 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2013 14:47:51 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Shang", "Zuofeng", ""], ["Li", "Ping", ""]]}, {"id": "1302.1884", "submitter": "Ryan Martin", "authors": "Chuanhai Liu, Ryan Martin, Nick Syring", "title": "Simulating from a gamma distribution with small shape parameter", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulating from a gamma distribution with small shape parameter is a\nchallenging problem. Towards an efficient method, we obtain a limiting\ndistribution for a suitably normalized gamma distribution when the shape\nparameter tends to zero. Then this limiting distribution provides insight to\nthe construction of a new, simple, and highly efficient acceptance--rejection\nalgorithm. Comparisons based on acceptance rates show that the proposed\nprocedure is more efficient than existing acceptance--rejection methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2013 21:22:44 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2013 14:08:01 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 17:34:17 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Liu", "Chuanhai", ""], ["Martin", "Ryan", ""], ["Syring", "Nick", ""]]}, {"id": "1302.2102", "submitter": "Paul McNicholas", "authors": "Ryan P. Browne and Paul D. McNicholas", "title": "Estimating Common Principal Components in High Dimensions", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-013-0139-1", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing an objective function that depends on\nan orthonormal matrix. This situation is encountered when looking for common\nprincipal components, for example, and the Flury method is a popular approach.\nHowever, the Flury method is not effective for higher dimensional problems. We\nobtain several simple majorization-minizmation (MM) algorithms that provide\nsolutions to this problem and are effective in higher dimensions. We then use\nsimulated data to compare them with other approaches in terms of convergence\nand computational time.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 18:05:16 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1302.2126", "submitter": "Leif Ellingson", "authors": "Leif Ellingson, Vic Patrangenaru, Frits Ruymgaart", "title": "Nonparametric Estimation of Means on Hilbert Manifolds and Extrinsic\n  Analysis of Mean Shapes of Contours", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of nonparametric inference in high level digital\nimage analysis, we introduce a general extrinsic approach for data analysis on\nHilbert manifolds with a focus on means of probability distributions on such\nsample spaces. To perform inference on these means, we appeal to the concept of\nneighborhood hypotheses from functional data analysis and derive a one-sample\ntest. We then consider analysis of shapes of contours lying in the plane. By\nembedding the corresponding sample space of such shapes, which is a Hilbert\nmanifold, into a space of Hilbert-Schmidt operators, we can define extrinsic\nmean shapes of planar contours and their sample analogues. We apply the general\nmethods to this problem while considering the computational restrictions faced\nwhen utilizing digital imaging data. Comparisons of computational cost are\nprovided to another method for analyzing shapes of contours.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 19:40:08 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Ellingson", "Leif", ""], ["Patrangenaru", "Vic", ""], ["Ruymgaart", "Frits", ""]]}, {"id": "1302.2129", "submitter": "Nima Noorshams", "authors": "Nima Noorshams, Martin Wainwright", "title": "Non-Asymptotic Analysis of an Optimal Algorithm for Network-Constrained\n  Averaging with Noisy Links", "comments": null, "journal-ref": "N. Noorshams, M. J. Wainwright, \"Non-Asymptotic Analysis of an\n  Optimal Algorithm for Network-Constrained Averaging with Noisy Links\", IEEE\n  journal of selected topics in signal processing, vol. 5, no. 4, pp. 833-844,\n  Aug. 2011", "doi": "10.1109/JSTSP.2011.2122241", "report-no": null, "categories": "cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of network-constrained averaging is to compute the average of a\nset of values distributed throughout a graph G using an algorithm that can pass\nmessages only along graph edges. We study this problem in the noisy setting, in\nwhich the communication along each link is modeled by an additive white\nGaussian noise channel. We propose a two-phase decentralized algorithm, and we\nuse stochastic approximation methods in conjunction with the spectral graph\ntheory to provide concrete (non-asymptotic) bounds on the mean-squared error.\nHaving found such bounds, we analyze how the number of iterations T_G(n;\n\\delta) required to achieve mean-squared error \\delta\\ scales as a function of\nthe graph topology and the number of nodes n. Previous work provided guarantees\nwith the number of iterations scaling inversely with the second smallest\neigenvalue of the Laplacian. This paper gives an algorithm that reduces this\ngraph dependence to the graph diameter, which is the best scaling possible.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 19:46:51 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Noorshams", "Nima", ""], ["Wainwright", "Martin", ""]]}, {"id": "1302.2142", "submitter": "Ying Liu", "authors": "Ying Liu, Andrew Gelman and Tian Zheng", "title": "Simulation-efficient shortest probability intervals", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian highest posterior density (HPD) intervals can be estimated directly\nfrom simulations via empirical shortest intervals. Unfortunately, these can be\nnoisy (that is, have a high Monte Carlo error). We derive an optimal weighting\nstrategy using bootstrap and quadratic programming to obtain a more compu-\ntationally stable HPD, or in general, shortest probability interval (Spin). We\nprove the consistency of our method. Simulation studies on a range of theoret-\nical and real-data examples, some with symmetric and some with asymmetric\nposterior densities, show that intervals constructed using Spin have better\ncov- erage (relative to the posterior distribution) and lower Monte Carlo error\nthan empirical shortest intervals. We implement the new method in an R package\n(SPIn) so it can be routinely used in post-processing of Bayesian simulations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 20:59:13 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Liu", "Ying", ""], ["Gelman", "Andrew", ""], ["Zheng", "Tian", ""]]}, {"id": "1302.2325", "submitter": "Anatoli Juditsky B.", "authors": "Zaid Harchaoui, Anatoli Juditsky, Arkadi Nemirovski", "title": "Conditional Gradient Algorithms for Norm-Regularized Smooth Convex\n  Optimization", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by some applications in signal processing and machine learning, we\nconsider two convex optimization problems where, given a cone $K$, a norm\n$\\|\\cdot\\|$ and a smooth convex function $f$, we want either 1) to minimize the\nnorm over the intersection of the cone and a level set of $f$, or 2) to\nminimize over the cone the sum of $f$ and a multiple of the norm. We focus on\nthe case where (a) the dimension of the problem is too large to allow for\ninterior point algorithms, (b) $\\|\\cdot\\|$ is \"too complicated\" to allow for\ncomputationally cheap Bregman projections required in the first-order proximal\ngradient algorithms. On the other hand, we assume that {it is relatively easy\nto minimize linear forms over the intersection of $K$ and the unit\n$\\|\\cdot\\|$-ball}. Motivating examples are given by the nuclear norm with $K$\nbeing the entire space of matrices, or the positive semidefinite cone in the\nspace of symmetric matrices, and the Total Variation norm on the space of 2D\nimages. We discuss versions of the Conditional Gradient algorithm capable to\nhandle our problems of interest, provide the related theoretical efficiency\nestimates and outline some applications.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2013 12:24:44 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 15:30:42 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2013 12:57:54 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2013 16:54:05 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Harchaoui", "Zaid", ""], ["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1302.5324", "submitter": "Simon Lyons", "authors": "Simon Lyons, Simo S\\\"arkk\\\"a, Amos Storkey", "title": "Series Expansion Approximations of Brownian Motion for Non-Linear Kalman\n  Filtering of Diffusion Processes", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2303430", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a novel application of sigma-point methods to\ncontinuous-discrete filtering. In principle, the nonlinear continuous- discrete\nfiltering problem can be solved exactly. In practice, the solution contains\nterms that are computationally intractible. Assumed density filtering methods\nattempt to match statistics of the filtering distribution to some set of more\ntractible probability distributions. We describe a novel method that decomposes\nthe Brownian motion driving the signal in a generalised Fourier series, which\nis truncated after a number of terms. This approximation to Brownian can be\ndescribed using a relatively small number of Fourier coefficients, and allows\nus to compute statistics of the filtering distribution with a single\napplication of a sigma-point method. Assumed density filters that exist in the\nliterature usually rely on discretisation of the signal dynamics followed by\niterated application of a sigma point transform (or a limiting case thereof).\nIterating the transform in this manner can lead to loss of information about\nthe filtering distri- bution in highly nonlinear settings. We demonstrate that\nour method is better equipped to cope with such problems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 16:26:32 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 17:51:27 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2014 18:55:43 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Lyons", "Simon", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Storkey", "Amos", ""]]}, {"id": "1302.5475", "submitter": "Kei Hirose", "authors": "Kei Hirose and Michio Yamamoto", "title": "Estimation of oblique structure via penalized likelihood factor analysis", "comments": "19 pages. arXiv admin note: substantial text overlap with\n  arXiv:1205.5868", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse estimation via a lasso-type penalized\nlikelihood procedure in a factor analysis model. Typically, the model\nestimation is done under the assumption that the common factors are orthogonal\n(uncorrelated). However, the lasso-type penalization method based on the\northogonal model can often estimate a completely different model from that with\nthe true factor structure when the common factors are correlated. In order to\novercome this problem, we propose to incorporate a factor correlation into the\nmodel, and estimate the factor correlation along with parameters included in\nthe orthogonal model by maximum penalized likelihood procedure. An entire\nsolution path is computed by the EM algorithm with coordinate descent, which\npermits the application to a wide variety of convex and nonconvex penalties.\nThe proposed method can provide sufficiently sparse solutions, and be applied\nto the data where the number of variables is larger than the number of\nobservations. Monte Carlo simulations are conducted to investigate the\neffectiveness of our modeling strategies. The results show that the lasso-type\npenalization based on the orthogonal model cannot often approximate the true\nfactor structure, whereas our approach performs well in various situations. The\nusefulness of the proposed procedure is also illustrated through the analysis\nof real data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 03:54:40 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Hirose", "Kei", ""], ["Yamamoto", "Michio", ""]]}, {"id": "1302.5624", "submitter": "Dennis Prangle", "authors": "Dennis Prangle, Paul Fearnhead, Murray P. Cox, Patrick J. Biggs, Nigel\n  P. French", "title": "Semi-automatic selection of summary statistics for ABC model choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central statistical goal is to choose between alternative explanatory\nmodels of data. In many modern applications, such as population genetics, it is\nnot possible to apply standard methods based on evaluating the likelihood\nfunctions of the models, as these are numerically intractable. Approximate\nBayesian computation (ABC) is a commonly used alternative for such situations.\nABC simulates data x for many parameter values under each model, which is\ncompared to the observed data xobs. More weight is placed on models under which\nS(x) is close to S(xobs), where S maps data to a vector of summary statistics.\nPrevious work has shown the choice of S is crucial to the efficiency and\naccuracy of ABC. This paper provides a method to select good summary statistics\nfor model choice. It uses a preliminary step, simulating many x values from all\nmodels and fitting regressions to this with the model as response. The\nresulting model weight estimators are used as S in an ABC analysis. Theoretical\nresults are given to justify this as approximating low dimensional sufficient\nstatistics. A substantive application is presented: choosing between competing\ncoalescent models of demographic growth for Campylobacter jejuni in New Zealand\nusing multi-locus sequence typing data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 15:48:31 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Prangle", "Dennis", ""], ["Fearnhead", "Paul", ""], ["Cox", "Murray P.", ""], ["Biggs", "Patrick J.", ""], ["French", "Nigel P.", ""]]}, {"id": "1302.5762", "submitter": "Yue Wu", "authors": "Yue Wu and Brian Tracey and Premkumar Natarajan and Joseph P. Noonan", "title": "Probabilistic Non-Local Means", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": "10.1109/LSP.2013.2263135", "report-no": null, "categories": "cs.CV stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a so-called probabilistic non-local means (PNLM)\nmethod for image denoising. Our main contributions are: 1) we point out defects\nof the weight function used in the classic NLM; 2) we successfully derive all\ntheoretical statistics of patch-wise differences for Gaussian noise; and 3) we\nemploy this prior information and formulate the probabilistic weights truly\nreflecting the similarity between two noisy patches. The probabilistic nature\nof the new weight function also provides a theoretical basis to choose\nthresholds rejecting dissimilar patches for fast computations. Our simulation\nresults indicate the PNLM outperforms the classic NLM and many NLM recent\nvariants in terms of peak signal noise ratio (PSNR) and structural similarity\n(SSIM) index. Encouraging improvements are also found when we replace the NLM\nweights with the probabilistic weights in tested NLM variants.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 04:48:14 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Wu", "Yue", ""], ["Tracey", "Brian", ""], ["Natarajan", "Premkumar", ""], ["Noonan", "Joseph P.", ""]]}, {"id": "1302.6182", "submitter": "Ziyu Wang", "authors": "ziyu wang, Shakir Mohamed, Nando de Freitas", "title": "Adaptive Hamiltonian and Riemann Manifold Monte Carlo Samplers", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the widely-experienced difficulty in tuning\nHamiltonian-based Monte Carlo samplers. We develop an algorithm that allows for\nthe adaptation of Hamiltonian and Riemann manifold Hamiltonian Monte Carlo\nsamplers using Bayesian optimization that allows for infinite adaptation of the\nparameters of these samplers. We show that the resulting sampling algorithms\nare ergodic, and that the use of our adaptive algorithms makes it easy to\nobtain more efficient samplers, in some cases precluding the need for more\ncomplex solutions. Hamiltonian-based Monte Carlo samplers are widely known to\nbe an excellent choice of MCMC method, and we aim with this paper to remove a\nkey obstacle towards the more widespread use of these samplers in practice.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 18:23:43 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["wang", "ziyu", ""], ["Mohamed", "Shakir", ""], ["de Freitas", "Nando", ""]]}, {"id": "1302.6964", "submitter": "Murray Pollock", "authors": "Murray Pollock, Adam M. Johansen, Gareth O. Roberts", "title": "On the exact and $\\varepsilon$-strong simulation of (jump) diffusions", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ676 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 794-856", "doi": "10.3150/14-BEJ676", "report-no": "IMS-BEJ-BEJ676", "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a framework for simulating finite dimensional\nrepresentations of (jump) diffusion sample paths over finite intervals, without\ndiscretisation error (exactly), in such a way that the sample path can be\nrestored at any finite collection of time points. Within this framework we\nextend existing exact algorithms and introduce novel adaptive approaches. We\nconsider an application of the methodology developed within this paper which\nallows the simulation of upper and lower bounding processes which almost surely\nconstrain (jump) diffusion sample paths to any specified tolerance. We\ndemonstrate the efficacy of our approach by showing that with finite\ncomputation it is possible to determine whether or not sample paths cross\nvarious irregular barriers, simulate to any specified tolerance the first\nhitting time of the irregular barrier and simulate killed diffusion sample\npaths.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 19:21:12 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 15:09:36 GMT"}, {"version": "v3", "created": "Tue, 9 Sep 2014 12:14:16 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2016 10:53:15 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Pollock", "Murray", ""], ["Johansen", "Adam M.", ""], ["Roberts", "Gareth O.", ""]]}]