[{"id": "1103.0365", "submitter": "Pradeep", "authors": "J. Pradeep, E. Srinivasan and S. Himavathi", "title": "Diagonal Based Feature Extraction for Handwritten Alphabets Recognition\n  System using Neural Network", "comments": null, "journal-ref": null, "doi": "10.5121/ijcsit.2011.3103", "report-no": null, "categories": "stat.CO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An off-line handwritten alphabetical character recognition system using\nmultilayer feed forward neural network is described in the paper. A new method,\ncalled, diagonal based feature extraction is introduced for extracting the\nfeatures of the handwritten alphabets. Fifty data sets, each containing 26\nalphabets written by various people, are used for training the neural network\nand 570 different handwritten alphabetical characters are used for testing. The\nproposed recognition system performs quite well yielding higher levels of\nrecognition accuracy compared to the systems employing the conventional\nhorizontal and vertical methods of feature extraction. This system will be\nsuitable for converting handwritten documents into structural text form and\nrecognizing handwritten names.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 08:48:21 GMT"}], "update_date": "2011-03-03", "authors_parsed": [["Pradeep", "J.", ""], ["Srinivasan", "E.", ""], ["Himavathi", "S.", ""]]}, {"id": "1103.0542", "submitter": "Natesh S. Pillai", "authors": "Natesh S. Pillai, Andrew M. Stuart, Alexandre H. Thi\\'ery", "title": "Optimal scaling and diffusion limits for the Langevin algorithm in high\n  dimensions", "comments": "Published in at http://dx.doi.org/10.1214/11-AAP828 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2012, Vol. 22, No. 6, 2320-2356", "doi": "10.1214/11-AAP828", "report-no": "IMS-AAP-AAP828", "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metropolis-adjusted Langevin (MALA) algorithm is a sampling algorithm\nwhich makes local moves by incorporating information about the gradient of the\nlogarithm of the target density. In this paper we study the efficiency of MALA\non a natural class of target measures supported on an infinite dimensional\nHilbert space. These natural measures have density with respect to a Gaussian\nrandom field measure and arise in many applications such as Bayesian\nnonparametric statistics and the theory of conditioned diffusions. We prove\nthat, started in stationarity, a suitably interpolated and scaled version of\nthe Markov chain corresponding to MALA converges to an infinite dimensional\ndiffusion process. Our results imply that, in stationarity, the MALA algorithm\napplied to an N-dimensional approximation of the target will take\n$\\mathcal{O}(N^{1/3})$ steps to explore the invariant measure, comparing\nfavorably with the Random Walk Metropolis which was recently shown to require\n$\\mathcal{O}(N)$ steps when applied to the same class of problems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 20:54:37 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2011 17:44:48 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2012 11:26:35 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Pillai", "Natesh S.", ""], ["Stuart", "Andrew M.", ""], ["Thi\u00e9ry", "Alexandre H.", ""]]}, {"id": "1103.1805", "submitter": "Matthias Troffaes", "authors": "Matthias C. M. Troffaes and Sebastien Destercke", "title": "Probability boxes on totally preordered spaces for multivariate\n  modelling", "comments": "33 pages, 8 figures; v2: added keywords, added doi link", "journal-ref": "International Journal of Approximate Reasoning 52 (2011) 767-791", "doi": "10.1016/j.ijar.2011.02.001", "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pair of lower and upper cumulative distribution functions, also called\nprobability box or p-box, is among the most popular models used in imprecise\nprobability theory. They arise naturally in expert elicitation, for instance in\ncases where bounds are specified on the quantiles of a random variable, or when\nquantiles are specified only at a finite number of points. Many practical and\nformal results concerning p-boxes already exist in the literature. In this\npaper, we provide new efficient tools to construct multivariate p-boxes and\ndevelop algorithms to draw inferences from them. For this purpose, we formalise\nand extend the theory of p-boxes using Walley's behavioural theory of imprecise\nprobabilities, and heavily rely on its notion of natural extension and existing\nresults about independence modeling. In particular, we allow p-boxes to be\ndefined on arbitrary totally preordered spaces, hence thereby also admitting\nmultivariate p-boxes via probability bounds over any collection of nested sets.\nWe focus on the cases of independence (using the factorization property), and\nof unknown dependence (using the Fr\\'echet bounds), and we show that our\napproach extends the probabilistic arithmetic of Williamson and Downs. Two\ndesign problems---a damped oscillator, and a river dike---demonstrate the\npractical feasibility of our results.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 15:26:51 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2011 10:02:33 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Troffaes", "Matthias C. M.", ""], ["Destercke", "Sebastien", ""]]}, {"id": "1103.2876", "submitter": "Charlotte Soneson", "authors": "Charlotte Soneson and Magnus Fontes", "title": "A framework for list representation, enabling list stabilization through\n  incorporation of gene exchangeabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of multivariate data sets from e.g. microarray studies frequently\nresults in lists of genes which are associated with some response of interest.\nThe biological interpretation is often complicated by the statistical\ninstability of the obtained gene lists with respect to sampling variations,\nwhich may partly be due to the functional redundancy among genes, implying that\nmultiple genes can play exchangeable roles in the cell. In this paper we use\nthe concept of exchangeability of random variables to model this functional\nredundancy and thereby account for the instability attributable to sampling\nvariations. We present a flexible framework to incorporate the exchangeability\ninto the representation of lists. The proposed framework supports\nstraightforward robust comparison between any two lists. It can also be used to\ngenerate new, more stable gene rankings incorporating more information from the\nexperimental data. Using a microarray data set from lung cancer patients we\nshow that the proposed method provides more robust gene rankings than existing\nmethods with respect to sampling variations, without compromising the\nbiological significance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2011 10:37:10 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Soneson", "Charlotte", ""], ["Fontes", "Magnus", ""]]}, {"id": "1103.3508", "submitter": "Bjoern Bornkamp", "authors": "Bj\\\"orn Bornkamp", "title": "Approximating Probability Densities by Iterated Laplace Approximations", "comments": "to appear in Journal of Computational and Graphical Statistics,\n  http://pubs.amstat.org/loi/jcgs", "journal-ref": "updated version published in Journal of Computational and\n  Graphical Statistics, 2011, 20, 656--669", "doi": "10.1198/jcgs.2011.10099", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Laplace approximation is an old, but frequently used method to\napproximate integrals for Bayesian calculations. In this paper we develop an\nextension of the Laplace approximation, by applying it iteratively to the\nresidual, i.e., the difference between the current approximation and the true\nfunction. The final approximation is thus a linear combination of multivariate\nnormal densities, where the coefficients are chosen to achieve a good fit to\nthe target distribution. We illustrate on real and artificial examples that the\nproposed procedure is a computationally efficient alternative to current\napproaches for approximation of multivariate probability densities. The\nR-package iterLap implementing the methods described in this article is\navailable from the CRAN servers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2011 20:05:28 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Bornkamp", "Bj\u00f6rn", ""]]}, {"id": "1103.3738", "submitter": "Hua Zhou", "authors": "Hua Zhou and Kenneth Lange", "title": "A Path Algorithm for Constrained Estimation", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": "10.1080/10618600.2012.681248", "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many least squares problems involve affine equality and inequality\nconstraints. Although there are variety of methods for solving such problems,\nmost statisticians find constrained estimation challenging. The current paper\nproposes a new path following algorithm for quadratic programming based on\nexact penalization. Similar penalties arise in $l_1$ regularization in model\nselection. Classical penalty methods solve a sequence of unconstrained problems\nthat put greater and greater stress on meeting the constraints. In the limit as\nthe penalty constant tends to $\\infty$, one recovers the constrained solution.\nIn the exact penalty method, squared penalties are replaced by absolute value\npenalties, and the solution is recovered for a finite value of the penalty\nconstant. The exact path following method starts at the unconstrained solution\nand follows the solution path as the penalty constant increases. In the\nprocess, the solution path hits, slides along, and exits from the various\nconstraints. Path following in lasso penalized regression, in contrast, starts\nwith a large value of the penalty constant and works its way downward. In both\nsettings, inspection of the entire solution path is revealing. Just as with the\nlasso and generalized lasso, it is possible to plot the effective degrees of\nfreedom along the solution path. For a strictly convex quadratic program, the\nexact penalty algorithm can be framed entirely in terms of the sweep operator\nof regression analysis. A few well chosen examples illustrate the mechanics and\npotential of path following.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2011 01:27:52 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Zhou", "Hua", ""], ["Lange", "Kenneth", ""]]}, {"id": "1103.3965", "submitter": "Ajay Jasra", "authors": "Alexandros Beskos, Dan Crisan and Ajay Jasra", "title": "On the Stability of Sequential Monte Carlo Methods in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the stability of a Sequential Monte Carlo (SMC) method applied\nto the problem of sampling from a target distribution on $\\mathbb{R}^d$ for\nlarge $d$. It is well known that using a single importance sampling step one\nproduces an approximation for the target that deteriorates as the dimension $d$\nincreases, unless the number of Monte Carlo samples $N$ increases at an\nexponential rate in $d$. We show that this degeneracy can be avoided by\nintroducing a sequence of artificial targets, starting from a `simple' density\nand moving to the one of interest, using an SMC method to sample from the\nsequence. Using this class of SMC methods with a fixed number of samples, one\ncan produce an approximation for which the effective sample size (ESS)\nconverges to a random variable $\\varepsilon_N$ as $d\\rightarrow\\infty$ with\n$1<\\varepsilon_{N}<N$. The convergence is achieved with a computational cost\nproportional to $Nd^2$. If $\\varepsilon_N\\ll N$, we can raise its value by\nintroducing a number of resampling steps, say $m$ (where $m$ is independent of\n$d$). In this case, ESS converges to a random variable $\\varepsilon_{N,m}$ as\n$d\\rightarrow\\infty$ and $\\lim_{m\\to\\infty}\\varepsilon_{N,m}=N$. Also, we show\nthat the Monte Carlo error for estimating a fixed dimensional marginal\nexpectation is of order $\\frac{1}{\\sqrt{N}}$ uniformly in $d$. The results\nimply that, in high dimensions, SMC algorithms can efficiently control the\nvariability of the importance sampling weights and estimate fixed dimensional\nmarginals at a cost which is less than exponential in $d$ and indicate that, in\nhigh dimensions, resampling leads to a reduction in the Monte Carlo error and\nincrease in the ESS.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 10:41:30 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2012 09:25:17 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Beskos", "Alexandros", ""], ["Crisan", "Dan", ""], ["Jasra", "Ajay", ""]]}, {"id": "1103.3970", "submitter": "Nick Whiteley Dr", "authors": "Nick Whiteley", "title": "Sequential Monte Carlo samplers: error bounds and insensitivity to\n  initial conditions", "comments": "36 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses finite sample stability properties of sequential Monte\nCarlo methods for approximating sequences of probability distributions. The\nresults presented herein are applicable in the scenario where the start and end\ndistributions in the sequence are fixed and the number of intermediate steps is\na parameter of the algorithm. Under assumptions which hold on non-compact\nspaces, it is shown that the effect of the initial distribution decays\nexponentially fast in the number of intermediate steps and the corresponding\nstochastic error is stable in \\mathbb{L}_{p} norm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 11:06:58 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Whiteley", "Nick", ""]]}, {"id": "1103.4891", "submitter": "Adrian Dobra", "authors": "Adrian Dobra", "title": "Dynamic Markov Bases", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational approach for generating Markov bases for multi-way\ncontingency tables whose cells counts might be constrained by fixed marginals\nand by lower and upper bounds. Our framework includes tables with structural\nzeros as a particular case. In- stead of computing the entire Markov basis in\nan initial step, our framework finds sets of local moves that connect each\ntable in the reference set with a set of neighbor tables. We construct a Markov\nchain on the reference set of tables that requires only a set of local moves at\neach iteration. The union of these sets of local moves forms a dynamic Markov\nbasis. We illustrate the practicality of our algorithms in the estimation of\nexact p-values for a three-way table with structural zeros and a sparse\neight-way table. Computer code implementing the methods de- scribed in the\narticle as well as the two datasets used in the numerical examples are\navailable as supplemental material.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2011 01:16:08 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Dobra", "Adrian", ""]]}, {"id": "1103.5407", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott", "title": "Data augmentation for non-Gaussian regression models using variance-mean\n  mixtures", "comments": "Added a discussion of quasi-Newton acceleration", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the theory of normal variance-mean mixtures to derive a\ndata-augmentation scheme for a class of common regularization problems. This\ngeneralizes existing theory on normal variance mixtures for priors in\nregression and classification. It also allows variants of the\nexpectation-maximization algorithm to be brought to bear on a wider range of\nmodels than previously appreciated. We demonstrate the method on several\nexamples, including sparse quantile regression and binary logistic regression.\nWe also show that quasi-Newton acceleration can substantially improve the speed\nof the algorithm without compromising its robustness.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 16:28:32 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2011 22:37:08 GMT"}, {"version": "v3", "created": "Sun, 26 Feb 2012 04:51:17 GMT"}, {"version": "v4", "created": "Sat, 22 Sep 2012 21:19:23 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1103.5986", "submitter": "Todd Graves", "authors": "Todd L. Graves", "title": "Automatic Step Size Selection in Random Walk Metropolis Algorithms", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR 11-01936", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practitioners of Markov chain Monte Carlo (MCMC) may hesitate to use random\nwalk Metropolis-Hastings algorithms, especially variable-at-a-time algorithms\nwith many parameters, because these algorithms require users to select values\nof tuning parameters (step sizes). These algorithms perform poorly if the step\nsizes are set to be too low or too high. We show in this paper that it is not\ndifficult for an algorithm to tune these step sizes automatically to obtain a\ndesired acceptance probability, since the logit of the acceptance probability\nis very nearly linear in the log of the step size, with known slope\ncoefficient. These ideas work in most applications, including single parameter\nor block moves on the linear, log, or logit scales. We discuss the\nimplementation of this algorithm in the software package YADAS.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 16:32:16 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Graves", "Todd L.", ""]]}, {"id": "1103.6096", "submitter": "Ad Ridder", "authors": "Paul Dupuis, Bahar Kaynar, Ad Ridder, Reuven Rubinstein, Radislav\n  Vaisman", "title": "Counting with Combined Splitting and Capture-Recapture Methods", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the splitting method to three well-known counting problems, namely\n3-SAT, random graphs with prescribed degrees, and binary contingency tables. We\npresent an enhanced version of the splitting method based on the\ncapture-recapture technique, and show by experiments the superiority of this\ntechnique for SAT problems in terms of variance of the associated estimators,\nand speed of the algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 06:42:26 GMT"}], "update_date": "2011-04-01", "authors_parsed": [["Dupuis", "Paul", ""], ["Kaynar", "Bahar", ""], ["Ridder", "Ad", ""], ["Rubinstein", "Reuven", ""], ["Vaisman", "Radislav", ""]]}]