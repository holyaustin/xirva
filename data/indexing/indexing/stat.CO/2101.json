[{"id": "2101.00064", "submitter": "Stefanie Schwaar", "authors": "J\\\"urgen Franke, Mario Hefter, Andr\\'e Herzwurm, Klaus Ritter,\n  Stefanie Schwaar", "title": "Adaptive Quantile Computation for Brownian Bridge in Change-Point\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an example for the fast calculation of distributional parameters of\nGaussian processes, we propose a new Monte Carlo algorithm for the computation\nof quantiles of the supremum norm of weighted Brownian bridges. As it is known,\nthe corresponding distributions arise asymptotically for weighted CUSUM\nstatistics for change-point detection. The new algorithm employs an adaptive\n(sequential) time discretization for the trajectories of the Brownian bridge. A\nsimulation study shows that the new algorithm by far outperforms the standard\napproach, which employs a uniform time discretization.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 20:40:51 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Franke", "J\u00fcrgen", ""], ["Hefter", "Mario", ""], ["Herzwurm", "Andr\u00e9", ""], ["Ritter", "Klaus", ""], ["Schwaar", "Stefanie", ""]]}, {"id": "2101.00118", "submitter": "Anirban Mondal", "authors": "Anirban Mondal, Kai Yin, Abhijit Mandal", "title": "A Two Stage Adaptive Metropolis Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sampling algorithm combining two quite powerful ideas in the\nMarkov chain Monte Carlo literature -- adaptive Metropolis sampler and\ntwo-stage Metropolis-Hastings sampler. The proposed sampling method will be\nparticularly very useful for high-dimensional posterior sampling in Bayesian\nmodels with expensive likelihoods. In the first stage of the proposed\nalgorithm, an adaptive proposal is used based on the previously sampled states\nand the corresponding acceptance probability is computed based on an\napproximated inexpensive target density. The true expensive target density is\nevaluated while computing the second stage acceptance probability only if the\nproposal is accepted in the first stage. The adaptive nature of the algorithm\nguarantees faster convergence of the chain and very good mixing properties. On\nthe other hand, the two-stage approach helps in rejecting the bad proposals in\nthe inexpensive first stage, making the algorithm computationally efficient. As\nthe proposals are dependent on the previous states the chain loses its Markov\nproperty, but we prove that it retains the desired ergodicity property. The\nperformance of the proposed algorithm is compared with the existing algorithms\nin two simulated and two real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 00:16:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mondal", "Anirban", ""], ["Yin", "Kai", ""], ["Mandal", "Abhijit", ""]]}, {"id": "2101.00351", "submitter": "Geoff Boeing", "authors": "Geoff Boeing and Dani Arribas-Bel", "title": "GIS and Computational Notebooks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers and practitioners across many disciplines have recently adopted\ncomputational notebooks to develop, document, and share their scientific\nworkflows - and the GIS community is no exception. This chapter introduces\ncomputational notebooks in the geographical context. It begins by explaining\nthe computational paradigm and philosophy that underlie notebooks. Next it\nunpacks their architecture to illustrate a notebook user's typical workflow.\nThen it discusses the main benefits notebooks offer GIS researchers and\npractitioners, including better integration with modern software, more natural\naccess to new forms of data, and better alignment with the principles and\nbenefits of open science. In this context, it identifies notebooks as the glue\nthat binds together a broader ecosystem of open source packages and\ntransferable platforms for computational geography. The chapter concludes with\na brief illustration of using notebooks for a set of basic GIS operations.\nCompared to traditional desktop GIS, notebooks can make spatial analysis more\nnimble, extensible, and reproducible and have thus evolved into an important\ncomponent of the geospatial science toolkit.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 01:59:14 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Boeing", "Geoff", ""], ["Arribas-Bel", "Dani", ""]]}, {"id": "2101.00503", "submitter": "Florian Klimm", "authors": "Florian Klimm, Nick S. Jones and Michael T. Schaub", "title": "Modularity maximisation for graphons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.SI nlin.AO physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Networks are a widely-used tool to investigate the large-scale connectivity\nstructure in complex systems and graphons have been proposed as an infinite\nsize limit of dense networks. The detection of communities or other meso-scale\nstructures is a prominent topic in network science as it allows the\nidentification of functional building blocks in complex systems. When such\nbuilding blocks may be present in graphons is an open question. In this paper,\nwe define a graphon-modularity and demonstrate that it can be maximised to\ndetect communities in graphons. We then investigate specific synthetic graphons\nand show that they may show a wide range of different community structures. We\nalso reformulate the graphon-modularity maximisation as a continuous\noptimisation problem and so prove the optimal community structure or lack\nthereof for some graphons, something that is usually not possible for networks.\nFurthermore, we demonstrate that estimating a graphon from network data as an\nintermediate step can improve the detection of communities, in comparison with\nexclusively maximising the modularity of the network. While the choice of\ngraphon-estimator may strongly influence the accord between the community\nstructure of a network and its estimated graphon, we find that there is a\nsubstantial overlap if an appropriate estimator is used. Our study demonstrates\nthat community detection for graphons is possible and may serve as a\nprivacy-preserving way to cluster network data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 19:44:44 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Klimm", "Florian", ""], ["Jones", "Nick S.", ""], ["Schaub", "Michael T.", ""]]}, {"id": "2101.01011", "submitter": "Christian P. Robert", "authors": "Christian P. Robert and Gareth O. Roberts (University of Warwick)", "title": "Rao-Blackwellization in the MCMC era", "comments": "This paper is to appear in The International Statistical Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rao-Blackwellization is a notion often occurring in the MCMC literature, with\npossibly different meanings and connections with the original Rao--Blackwell\ntheorem (Rao, 1945 and Blackwell,1947), including a reduction of the variance\nof the resulting Monte Carlo approximations. This survey reviews some of the\nmeanings of the term.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 14:44:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Robert", "Christian P.", "", "University of Warwick"], ["Roberts", "Gareth O.", "", "University of Warwick"]]}, {"id": "2101.01157", "submitter": "Kidus Asfaw", "authors": "Kidus Asfaw, Joonha Park, Allister Ho, Aaron A. King, Edward Ionides", "title": "Partially observed Markov processes with spatial structure via the R\n  package spatPomp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address inference for a partially observed nonlinear non-Gaussian latent\nstochastic system comprised of interacting units. Each unit has a state, which\nmay be discrete or continuous, scalar or vector valued. In biological\napplications, the state may represent a structured population or the abundances\nof a collection of species at a single location. Units can have spatial\nlocations, allowing the description of spatially distributed interacting\npopulations arising in ecology, epidemiology and elsewhere. We consider models\nwhere the collection of states is a latent Markov process, and a time series of\nnoisy or incomplete measurements is made on each unit. A model of this form is\ncalled a spatiotemporal partially observed Markov process (SpatPOMP). The R\npackage spatPomp provides an environment for implementing SpatPOMP models,\nanalyzing data, and developing new inference approaches. We describe the\nspatPomp implementations of some methods with scaling properties suited to\nSpatPOMP models. We demonstrate the package on a simple Gaussian system and on\na nontrivial epidemiological model for measles transmission within and between\ncities. We show how to construct user-specified SpatPOMP models within\nspatPomp.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:40:28 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 22:39:13 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Asfaw", "Kidus", ""], ["Park", "Joonha", ""], ["Ho", "Allister", ""], ["King", "Aaron A.", ""], ["Ionides", "Edward", ""]]}, {"id": "2101.01253", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu and Sinan Y{\\i}ld{\\i}r{\\i}m and Arnaud Doucet and\n  Nicolas Chopin", "title": "Metropolis-Hastings with Averaged Acceptance Ratios", "comments": "arXiv admin note: text overlap with arXiv:1803.09527", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods to sample from a probability\ndistribution $\\pi$ defined on a space $(\\Theta,\\mathcal{T})$ consist of the\nsimulation of realisations of Markov chains $\\{\\theta_{n},n\\geq1\\}$ of\ninvariant distribution $\\pi$ and such that the distribution of $\\theta_{i}$\nconverges to $\\pi$ as $i\\rightarrow\\infty$. In practice one is typically\ninterested in the computation of expectations of functions, say $f$, with\nrespect to $\\pi$ and it is also required that averages\n$M^{-1}\\sum_{n=1}^{M}f(\\theta_{n})$ converge to the expectation of interest.\nThe iterative nature of MCMC makes it difficult to develop generic methods to\ntake advantage of parallel computing environments when interested in reducing\ntime to convergence. While numerous approaches have been proposed to reduce the\nvariance of ergodic averages, including averaging over independent realisations\nof $\\{\\theta_{n},n\\geq1\\}$ simulated on several computers, techniques to reduce\nthe \"burn-in\" of MCMC are scarce. In this paper we explore a simple and generic\napproach to improve convergence to equilibrium of existing algorithms which\nrely on the Metropolis-Hastings (MH) update, the main building block of MCMC.\nThe main idea is to use averages of the acceptance ratio w.r.t. multiple\nrealisations of random variables involved, while preserving $\\pi$ as invariant\ndistribution. The methodology requires limited change to existing code, is\nnaturally suited to parallel computing and is shown on our examples to provide\nsubstantial performance improvements both in terms of convergence to\nequilibrium and variance of ergodic averages. In some scenarios gains are\nobserved even on a serial machine.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 17:10:46 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Andrieu", "Christophe", ""], ["Y\u0131ld\u0131r\u0131m", "Sinan", ""], ["Doucet", "Arnaud", ""], ["Chopin", "Nicolas", ""]]}, {"id": "2101.01871", "submitter": "Sanjeena Subedi", "authors": "Wangshu Tu and Sanjeena Subedi", "title": "Logistic Normal Multinomial Factor Analyzers for Clustering Microbiome\n  Data", "comments": "50 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human microbiome plays an important role in human health and disease\nstatus. Next generating sequencing technologies allow for quantifying the\ncomposition of the human microbiome. Clustering these microbiome data can\nprovide valuable information by identifying underlying patterns across samples.\nRecently, Fang and Subedi (2020) proposed a logistic normal multinomial mixture\nmodel (LNM-MM) for clustering microbiome data. As microbiome data tends to be\nhigh dimensional, here, we develop a family of logistic normal multinomial\nfactor analyzers (LNM-FA) by incorporating a factor analyzer structure in the\nLNM-MM. This family of models is more suitable for high-dimensional data as the\nnumber of parameters in LNM-FA can be greatly reduced by assuming that the\nnumber of latent factors is small. Parameter estimation is done using a\ncomputationally efficient variant of the alternating expectation conditional\nmaximization algorithm that utilizes variational Gaussian approximations. The\nproposed method is illustrated using simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:01:29 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tu", "Wangshu", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "2101.01960", "submitter": "Rebecca Killick", "authors": "Xueheng Shi, Colin Gallagher, Robert Lund and Rebecca Killick", "title": "A Comparison of Single and Multiple Changepoint Techniques for Time\n  Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes and compares several prominent single and multiple\nchangepoint techniques for time series data. Due to their importance in\ninferential matters, changepoint research on correlated data has accelerated\nrecently. Unfortunately, small perturbations in model assumptions can\ndrastically alter changepoint conclusions; for example, heavy positive\ncorrelation in a time series can be misattributed to a mean shift should\ncorrelation be ignored. This paper considers both single and multiple\nchangepoint techniques. The paper begins by examining cumulative sum (CUSUM)\nand likelihood ratio tests and their variants for the single changepoint\nproblem; here, various statistics, boundary cropping scenarios, and scaling\nmethods (e.g., scaling to an extreme value or Brownian Bridge limit) are\ncompared. A recently developed test based on summing squared CUSUM statistics\nover all times is shown to have realistic Type I errors and superior detection\npower. The paper then turns to the multiple changepoint setting. Here,\npenalized likelihoods drive the discourse, with AIC, BIC, mBIC, and MDL\npenalties being considered. Binary and wild binary segmentation techniques are\nalso compared. We introduce a new distance metric specifically designed to\ncompare two multiple changepoint segmentations. Algorithmic and computational\nconcerns are discussed and simulations are provided to support all conclusions.\nIn the end, the multiple changepoint setting admits no clear methodological\nwinner, performance depending on the particular scenario. Nonetheless, some\npractical guidance will emerge.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 10:36:57 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Shi", "Xueheng", ""], ["Gallagher", "Colin", ""], ["Lund", "Robert", ""], ["Killick", "Rebecca", ""]]}, {"id": "2101.02028", "submitter": "Ye Tian", "authors": "Ye Tian", "title": "A Multilayer Correlated Topic Model", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We proposed a novel multilayer correlated topic model (MCTM) to analyze how\nthe main ideas inherit and vary between a document and its different segments,\nwhich helps understand an article's structure. The variational\nexpectation-maximization (EM) algorithm was derived to estimate the posterior\nand parameters in MCTM. We introduced two potential applications of MCTM,\nincluding the paragraph-level document analysis and market basket data\nanalysis. The effectiveness of MCTM in understanding the document structure has\nbeen verified by the great predictive performance on held-out documents and\nintuitive visualization. We also showed that MCTM could successfully capture\ncustomers' popular shopping patterns in the market basket analysis.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 21:50:36 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tian", "Ye", ""]]}, {"id": "2101.02035", "submitter": "David Degras", "authors": "David Degras", "title": "Scalable Feature Matching Across Large Data Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with matching feature vectors in a one-to-one fashion\nacross large collections of datasets. Formulating this task as a\nmultidimensional assignment problem with decomposable costs (MDADC), we develop\nextremely fast algorithms with time complexity linear in the number $n$ of\ndatasets and space complexity a small fraction of the data size. These\nremarkable properties hinge on using the squared Euclidean distance as\ndissimilarity function, which can reduce ${n \\choose 2}$ matching problems\nbetween pairs of datasets to $n$ problems and enable calculating assignment\ncosts on the fly. To our knowledge, no other method applicable to the MDADC\npossesses these linear scaling and low-storage properties necessary to\nlarge-scale applications. In numerical experiments, the novel algorithms\noutperform competing methods and show excellent computational and optimization\nperformances. An application of feature matching to a large neuroimaging\ndatabase is presented. The algorithms of this paper are implemented in the R\npackage matchFeat available at https://github.com/ddegras/matchFeat.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 13:52:30 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Degras", "David", ""]]}, {"id": "2101.02148", "submitter": "Solt Kov\\'acs", "authors": "Solt Kov\\'acs, Tobias Ruckstuhl, Helena Obrist, Peter B\\\"uhlmann", "title": "Graphical Elastic Net and Target Matrices: Fast Algorithms and Software\n  for Sparse Precision Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of undirected Gaussian graphical models and inverse\ncovariances in high-dimensional scenarios by penalizing the corresponding\nprecision matrix. While single $L_1$ (Graphical Lasso) and $L_2$ (Graphical\nRidge) penalties for the precision matrix have already been studied, we propose\nthe combination of both, yielding an Elastic Net type penalty. We enable\nadditional flexibility by allowing to include diagonal target matrices for the\nprecision matrix. We generalize existing algorithms for the Graphical Lasso and\nprovide corresponding software with an efficient implementation to facilitate\nusage for practitioners. Our software borrows computationally favorable parts\nfrom a number of existing packages for the Graphical Lasso, leading to an\noverall fast(er) implementation and at the same time yielding also much more\nmethodological flexibility.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:28:30 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Kov\u00e1cs", "Solt", ""], ["Ruckstuhl", "Tobias", ""], ["Obrist", "Helena", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2101.02417", "submitter": "Xin Tong Thomson", "authors": "Tiangang Cui and Xin T. Tong", "title": "A unified performance analysis of likelihood-informed subspace methods", "comments": "48 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood-informed subspace (LIS) method offers a viable route to\nreducing the dimensionality of high-dimensional probability distributions\narisen in Bayesian inference. LIS identifies an intrinsic low-dimensional\nlinear subspace where the target distribution differs the most from some\ntractable reference distribution. Such a subspace can be identified using the\nleading eigenvectors of a Gram matrix of the gradient of the log-likelihood\nfunction. Then, the original high-dimensional target distribution is\napproximated through various forms of ridge approximations of the likelihood\nfunction, in which the approximated likelihood only has support on the\nintrinsic low-dimensional subspace. This approximation enables the design of\ninference algorithms that can scale sub-linearly with the apparent\ndimensionality of the problem. Intuitively, the accuracy of the approximation,\nand hence the performance of the inference algorithms, are influenced by three\nfactors -- the dimension truncation error in identifying the subspace, Monte\nCarlo error in estimating the Gram matrices, and Monte Carlo error in\nconstructing ridge approximations. This work establishes a unified framework to\nanalysis each of these three factors and their interplay. Under mild technical\nassumptions, we establish error bounds for a range of existing dimension\nreduction techniques based on the principle of LIS. Our error bounds also\nprovide useful insights into the accuracy comparison of these methods. In\naddition, we analyze the integration of LIS with sampling methods such as\nMarkov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC). We also\ndemonstrate our analyses on a linear inverse problem with Gaussian prior, which\nshows that all the estimates can be dimension-independent if the prior\ncovariance is a trace-class operator.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:48:42 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Cui", "Tiangang", ""], ["Tong", "Xin T.", ""]]}, {"id": "2101.02506", "submitter": "Gregor Zens", "authors": "Gregor Zens, Sylvia Fr\\\"uhwirth-Schnatter, Helga Wagner", "title": "Efficient Bayesian Modeling of Binary and Categorical Data in R: The UPG\n  Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the UPG package for highly efficient Bayesian inference in\nprobit, logit, multinomial logit and binomial logit models. UPG offers a\nconvenient estimation framework for balanced and imbalanced data settings where\nsampling efficiency is ensured through Markov chain Monte Carlo boosting\nmethods. All sampling algorithms are implemented in C++, allowing for rapid\nparameter estimation. In addition, UPG provides several methods for fast\nproduction of output tables and summary plots that are easily accessible to a\nbroad range of users.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:55:55 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zens", "Gregor", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Wagner", "Helga", ""]]}, {"id": "2101.02786", "submitter": "Trung Pham", "authors": "Trung Pham, Alex A. Gorodetsky", "title": "Ensemble approximate control variate estimators: Applications to\n  multi-fidelity importance sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The recent growth in multi-fidelity uncertainty quantification has given rise\nto a large set of variance reduction techniques that leverage information from\nmodel ensembles to provide variance reduction for estimates of the statistics\nof a high-fidelity model. In this paper we provide two contributions: (1) we\nutilize an ensemble estimator to account for uncertainties in the optimal\nweights of approximate control variate (ACV) approaches and derive lower bounds\non the number of samples required to guarantee variance reduction; and (2) we\nextend an existing multi-fidelity importance sampling (MFIS) scheme to leverage\ncontrol variates. As such we make significant progress towards both increasing\nthe practicality of approximate control variates$-$for instance, by accounting\nfor the effect of pilot samples$-$and using multi-fidelity approaches more\neffectively for estimating low-probability events. The numerical results\nindicate our hybrid MFIS-ACV estimator achieves up to 50% improvement in\nvariance reduction over the existing state-of-the-art MFIS estimator, which had\nalready shown outstanding convergence rate compared to the Monte Carlo method,\non several problems of computational mechanics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 22:05:23 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Pham", "Trung", ""], ["Gorodetsky", "Alex A.", ""]]}, {"id": "2101.02912", "submitter": "Rahul Bhadani", "authors": "Rahul Bhadani", "title": "Nonlinear Optimization in R using nlopt", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we present a problem of nonlinear constraint optimization\nwith equality and inequality constraints. Objective functions are defined to be\nnonlinear and optimizers may have a lower and upper bound. We solve the\noptimization problem using the open-source R package nloptr. Several examples\nhave been presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:48:24 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 01:49:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bhadani", "Rahul", ""]]}, {"id": "2101.03079", "submitter": "Jacob Vorstrup Goldman", "authors": "Jacob Vorstrup Goldman and Sumeetpal Sidhu Singh", "title": "Spatiotemporal blocking of the bouncy particle sampler for efficient\n  inference in state space models", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel blocked version of the continuous-time bouncy particle\nsampler of [Bouchard-C\\^ot\\'e et al., 2018] which is applicable to any\ndifferentiable probability density. This alternative implementation is\nmotivated by blocked Gibbs sampling for state space models [Singh et al., 2017]\nand leads to significant improvement in terms of effective sample size per\nsecond, and furthermore, allows for significant parallelization of the\nresulting algorithm. The new algorithms are particularly efficient for latent\nstate inference in high-dimensional state space models, where blocking in both\nspace and time is necessary to avoid degeneracy of MCMC. The efficiency of our\nblocked bouncy particle sampler, in comparison with both the standard\nimplementation of the bouncy particle sampler and the particle Gibbs algorithm\nof Andrieu et al. [2010], is illustrated numerically for both simulated data\nand a challenging real-world financial dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:14:23 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 07:48:04 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Goldman", "Jacob Vorstrup", ""], ["Singh", "Sumeetpal Sidhu", ""]]}, {"id": "2101.03093", "submitter": "Ricardo Baptista", "authors": "Ricardo Baptista, Youssef Marzouk, Rebecca E. Morrison, Olivier Zahm", "title": "Learning non-Gaussian graphical models via Hessian scores and triangular\n  transport", "comments": "40 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Undirected probabilistic graphical models represent the conditional\ndependencies, or Markov properties, of a collection of random variables.\nKnowing the sparsity of such a graphical model is valuable for modeling\nmultivariate distributions and for efficiently performing inference. While the\nproblem of learning graph structure from data has been studied extensively for\ncertain parametric families of distributions, most existing methods fail to\nconsistently recover the graph structure for non-Gaussian data. Here we propose\nan algorithm for learning the Markov structure of continuous and non-Gaussian\ndistributions. To characterize conditional independence, we introduce a score\nbased on integrated Hessian information from the joint log-density, and we\nprove that this score upper bounds the conditional mutual information for a\ngeneral class of distributions. To compute the score, our algorithm SING\nestimates the density using a deterministic coupling, induced by a triangular\ntransport map, and iteratively exploits sparse structure in the map to reveal\nsparsity in the graph. For certain non-Gaussian datasets, we show that our\nalgorithm recovers the graph structure even with a biased approximation to the\ndensity. Among other examples, we apply sing to learn the dependencies between\nthe states of a chaotic dynamical system with local interactions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:42:42 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Baptista", "Ricardo", ""], ["Marzouk", "Youssef", ""], ["Morrison", "Rebecca E.", ""], ["Zahm", "Olivier", ""]]}, {"id": "2101.03108", "submitter": "David Ginsbourger", "authors": "David Ginsbourger and Cedric Sch\\\"arer", "title": "Fast calculation of Gaussian Process multiple-fold cross-validation\n  residuals and their covariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize fast Gaussian process leave-one-out formulae to multiple-fold\ncross-validation, highlighting in turn in broad settings the covariance\nstructure of cross-validation residuals. The employed approach, that relies on\nblock matrix inversion via Schur complements, is applied to both Simple and\nUniversal Kriging frameworks. We illustrate how resulting covariances affect\nmodel diagnostics and how to properly transform residuals in the first place.\nBeyond that, we examine how accounting for dependency between such residuals\naffect cross-validation-based estimation of the scale parameter. It is found in\ntwo distinct cases, namely in scale estimation and in broader covariance\nparameter estimation via pseudo-likelihood, that correcting for covariances\nbetween cross-validation residuals leads back to maximum likelihood estimation\nor to an original variation thereof. The proposed fast calculation of Gaussian\nProcess multiple-fold cross-validation residuals is implemented and benchmarked\nagainst a naive implementation, all in R language. Numerical experiments\nhighlight the accuracy of our approach as well as the substantial speed-ups\nthat it enables. It is noticeable however, as supported by a discussion on the\nmain drivers of computational costs and by a dedicated numerical benchmark,\nthat speed-ups steeply decline as the number of folds (say, all sharing the\nsame size) decreases. Overall, our results enable fast multiple-fold\ncross-validation, have direct consequences in GP model diagnostics, and pave\nthe way to future work on hyperparameter fitting as well as on the promising\nfield of goal-oriented fold design.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 17:02:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ginsbourger", "David", ""], ["Sch\u00e4rer", "Cedric", ""]]}, {"id": "2101.03323", "submitter": "Jingwei Liu", "authors": "Jingwei Liu", "title": "SARS-Cov-2 RNA Sequence Classification Based on Territory Information", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CovID-19 genetics analysis is critical to determine virus type,virus variant\nand evaluate vaccines. In this paper, SARS-Cov-2 RNA sequence analysis relative\nto region or territory is investigated. A uniform framework of sequence SVM\nmodel with various genetics length from short to long and mixed-bases is\ndeveloped by projecting SARS-Cov-2 RNA sequence to different dimensional space,\nthen scoring it according to the output probability of pre-trained SVM models\nto explore the territory or origin information of SARS-Cov-2. Different sample\nsize ratio of training set and test set is also discussed in the data analysis.\nTwo SARS-Cov-2 RNA classification tasks are constructed based on GISAID\ndatabase, one is for mainland, Hongkong and Taiwan of China, and the other is a\n6-class classification task (Africa, Asia, Europe, North American, South\nAmerican\\& Central American, Ocean) of 7 continents. For 3-class classification\nof China, the Top-1 accuracy rate can reach 82.45\\% (train 60\\%, test=40\\%);\nFor 2-class classification of China, the Top-1 accuracy rate can reach 97.35\\%\n(train 80\\%, test 20\\%); For 6-class classification task of world, when the\nratio of training set and test set is 20\\% : 80\\% , the Top-1 accuracy rate can\nachieve 30.30\\%. And, some Top-N results are also given.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 09:12:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Liu", "Jingwei", ""]]}, {"id": "2101.03491", "submitter": "Narumasa Tsutsumida", "authors": "Joseph Emile Honour Percival, Narumasa Tsutsumida, Daisuke Murakami,\n  Takahiro Yoshida, Tomoki Nakaya", "title": "gwpcorMapper: an interactive mapping tool for exploring geographically\n  weighted correlation and partial correlation in high-dimensional geospatial\n  datasets", "comments": "18 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploratory spatial data analysis (ESDA) plays a key role in research that\nincludes geographic data. In ESDA, analysts often want to be able to visualize\nobservations and local relationships on a map. However, software dedicated to\nvisualizing local spatial relations be-tween multiple variables in high\ndimensional datasets remains undeveloped. This paper introduces gwpcorMapper, a\nnewly developed software application for mapping geographically weighted\ncorrelation and partial correlation in large multivariate datasets.\ngwpcorMap-per facilitates ESDA by giving researchers the ability to interact\nwith map components that describe local correlative relationships. We built\ngwpcorMapper using the R Shiny framework. The software inherits its core\nalgorithm from GWpcor, an R library for calculating the geographically weighted\ncorrelation and partial correlation statistics. We demonstrate the application\nof gwpcorMapper by using it to explore census data in order to find meaningful\nrelationships that describe the work-life environment in the 23 special wards\nof Tokyo, Japan. We show that gwpcorMapper is useful in both variable selection\nand parameter tuning for geographically weighted statistics. gwpcorMapper\nhighlights that there are strong statistically clear local variations in the\nrelationship between the number of commuters and the total number of hours\nworked when considering the total population in each district across the 23\nspecial wards of Tokyo. Our application demonstrates that the ESDA process with\nhigh-dimensional geospatial data using gwpcorMapper has applications across\nmultiple fields.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 07:16:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Percival", "Joseph Emile Honour", ""], ["Tsutsumida", "Narumasa", ""], ["Murakami", "Daisuke", ""], ["Yoshida", "Takahiro", ""], ["Nakaya", "Tomoki", ""]]}, {"id": "2101.03579", "submitter": "Michele Peruzzi", "authors": "Michele Peruzzi, Sudipto Banerjee, David B. Dunson, Andrew O. Finley", "title": "Grid-Parametrize-Split (GriPS) for Improved Scalable Inference in\n  Spatial Big Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid advancements in spatial technologies including Geographic Information\nSystems (GIS) and remote sensing have generated massive amounts of spatially\nreferenced data in a variety of scientific and data-driven industrial\napplications. These advancements have led to a substantial, and still\nexpanding, literature on the modeling and analysis of spatially oriented big\ndata. In particular, Bayesian inferences for high-dimensional spatial processes\nare being sought in a variety of remote-sensing applications including, but not\nlimited to, modeling next generation Light Detection and Ranging (LiDAR)\nsystems and other remotely sensed data. Massively scalable spatial processes,\nin particular Gaussian processes (GPs), are being explored extensively for the\nincreasingly encountered big data settings. Recent developments include GPs\nconstructed from sparse Directed Acyclic Graphs (DAGs) with a limited number of\nneighbors (parents) to characterize dependence across the spatial domain. The\nDAG can be used to devise fast algorithms for posterior sampling of the latent\nprocess, but these may exhibit pathological behavior in estimating covariance\nparameters. While these issues are mitigated by considering marginalized\nsamplers that exploit the underlying sparse precision matrix, these algorithms\nare slower, less flexible, and oblivious of structure in the data. The current\narticle introduces the Grid-Parametrize-Split (GriPS) approach for conducting\nBayesian inference in spatially oriented big data settings by a combination of\ncareful model construction and algorithm design to effectuate substantial\nimprovements in MCMC convergence. We demonstrate the effectiveness of our\nproposed methods through simulation experiments and subsequently undertake the\nmodeling of LiDAR outcomes and production of their predictive maps using G-LiHT\nand other remotely sensed variables.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 16:51:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Peruzzi", "Michele", ""], ["Banerjee", "Sudipto", ""], ["Dunson", "David B.", ""], ["Finley", "Andrew O.", ""]]}, {"id": "2101.03657", "submitter": "Sedigheh Zolaktaf", "authors": "Sedigheh Zolaktaf, Frits Dannenberg, Mark Schmidt, Anne Condon, Erik\n  Winfree", "title": "The pathway elaboration method for mean first passage time estimation in\n  large continuous-time Markov chains with applications to nucleic acid\n  kinetics", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time Markov chains (CTMCs) are widely used in many applications,\nincluding in modeling nucleic acid kinetics (non-equilibrium dynamics). A\ntypical issue in CTMCs is that the number of states could be large, making mean\nfirst passage time (MFPT) estimation challenging, particularly for events that\nhappen on a long time scale (rare events).We propose the pathway elaboration\nmethod, a time-efficient probabilistic truncation-based approach for\ndetailed-balance CTMCs. It can be used for estimating the MFPT for rare events\nin addition to rapidly evaluating perturbed parameters without expensive\nrecomputations. We demonstrate that pathway elaboration is suitable for\npredicting nucleic acid kinetics by conducting computational experiments on 267\nmeasurements that cover a wide range of rates for different types of reactions.\nWe utilize pathway elaboration to gain insight on the kinetics of two\ncontrasting reactions, one being a rare event. We compare the performance of\npathway elaboration with the stochastic simulation algorithm (SSA) for MFPT\nestimation on 237 of the reactions for which SSA is feasible. We further use\npathway elaboration to rapidly evaluate perturbed model parameters during\noptimization with respect to experimentally measured rates for these 237\nreactions. The testing error on the remaining 30 reactions, which involved rare\nevents and were not feasible to simulate with SSA, improved comparably with the\ntraining error. Our framework and dataset are available at https://github.com/\nDNA-and-Natural-Algorithms-Group/PathwayElaboration.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 01:45:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zolaktaf", "Sedigheh", ""], ["Dannenberg", "Frits", ""], ["Schmidt", "Mark", ""], ["Condon", "Anne", ""], ["Winfree", "Erik", ""]]}, {"id": "2101.03906", "submitter": "Shiwei Lan", "authors": "Shiwei Lan, Shuyi Li, Babak Shahbaba", "title": "Scaling Up Bayesian Uncertainty Quantification for Inverse Problems\n  using Deep Neural Networks", "comments": "40 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the importance of uncertainty quantification (UQ), Bayesian approach\nto inverse problems has recently gained popularity in applied mathematics,\nphysics, and engineering. However, traditional Bayesian inference methods based\non Markov Chain Monte Carlo (MCMC) tend to be computationally intensive and\ninefficient for such high dimensional problems. To address this issue, several\nmethods based on surrogate models have been proposed to speed up the inference\nprocess. More specifically, the calibration-emulation-sampling (CES) scheme has\nbeen proven to be successful in large dimensional UQ problems. In this work, we\npropose a novel CES approach for Bayesian inference based on deep neural\nnetwork (DNN) models for the emulation phase. The resulting algorithm is not\nonly computationally more efficient, but also less sensitive to the training\nset. Further, by using an Autoencoder (AE) for dimension reduction, we have\nbeen able to speed up our Bayesian inference method up to three orders of\nmagnitude. Overall, our method, henceforth called \\emph{Dimension-Reduced\nEmulative Autoencoder Monte Carlo (DREAM)} algorithm, is able to scale Bayesian\nUQ up to thousands of dimensions in physics-constrained inverse problems. Using\ntwo low-dimensional (linear and nonlinear) inverse problems we illustrate the\nvalidity this approach. Next, we apply our method to two high-dimensional\nnumerical examples (elliptic and advection-diffussion) to demonstrate its\ncomputational advantage over existing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 14:18:38 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Lan", "Shiwei", ""], ["Li", "Shuyi", ""], ["Shahbaba", "Babak", ""]]}, {"id": "2101.04043", "submitter": "Yiming Xu", "authors": "Yiming Xu and Akil Narayan", "title": "Randomized weakly admissible meshes", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weakly admissible mesh (WAM) on a continuum real-valued domain is a\nsequence of discrete grids such that the discrete maximum norm of polynomials\non the grid is comparable to the supremum norm of polynomials on the domain.\nThe asymptotic rate of growth of the grid sizes and of the comparability\nconstant must grow in a controlled manner. In this paper we generalize the\nnotion of a WAM to a hierarchical subspaces of not necessarily polynomial\nfunctions, and we analyze particular strategies for random sampling as a\ntechnique for generating WAMs. Our main results show that WAM's and their\nstronger variant, admissible meshes, can be generated by random sampling, and\nour analysis provides concrete estimates for growth of both the meshes and the\ndiscrete-continuum comparability constants.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:26:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Xu", "Yiming", ""], ["Narayan", "Akil", ""]]}, {"id": "2101.04084", "submitter": "Quan Zhou", "authors": "Quan Zhou, Hyunwoong Chang", "title": "Complexity analysis of Bayesian learning of high-dimensional DAG models\n  and their equivalence classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider MCMC methods for learning equivalence classes of sparse Gaussian\nDAG models when $p = e^{o(n)}$. The main contribution of this work is a rapid\nmixing result for a random walk Metropolis-Hastings algorithm, which we prove\nusing a canonical path method. It reveals that the complexity of Bayesian\nlearning of sparse equivalence classes grows only polynomially in $n$ and $p$,\nunder some common high-dimensional assumptions. Further, a series of\nhigh-dimensional consistency results is obtained by the path method, including\nthe strong selection consistency of an empirical Bayes model for structure\nlearning and the consistency of a greedy local search on the restricted search\nspace. Rapid mixing and slow mixing results for other structure-learning MCMC\nmethods are also derived. Our path method and mixing time results yield crucial\ninsights into the computational aspects of high-dimensional structure learning,\nwhich may be used to develop more efficient MCMC algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:27:59 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhou", "Quan", ""], ["Chang", "Hyunwoong", ""]]}, {"id": "2101.04426", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Pietro Spitali, Cristina Al-Khalili Szigyarto, The\n  MARK-MD Consortium, Roula Tsonaka", "title": "Penalized regression calibration: a method for the prediction of\n  survival outcomes using complex longitudinal and high-dimensional data", "comments": "Minor changes from version 1 (typos in legends of figures,\n  acknowledgements, etc.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Longitudinal and high-dimensional measurements have become increasingly\ncommon in biomedical research. However, methods to predict survival outcomes\nusing covariates that are both longitudinal and high-dimensional are currently\nmissing. In this article we propose penalized regression calibration (PRC), a\nmethod that can be employed to predict survival in such situations.\n  PRC comprises three modelling steps: first, the trajectories described by the\nlongitudinal predictors are flexibly modelled through the specification of\nmultivariate latent process mixed models. Second, subject-specific summaries of\nthe longitudinal trajectories are derived from the fitted mixed effects models.\nThird, the time to event outcome is predicted using the subject-specific\nsummaries as covariates in a penalized Cox model.\n  To ensure a proper internal validation of the fitted PRC models, we\nfurthermore develop a cluster bootstrap optimism correction procedure (CBOCP)\nthat allows to correct for the optimistic bias of apparent measures of\npredictiveness.\n  After studying the behaviour of PRC via simulations, we conclude by\nillustrating an application of PRC to data from an observational study that\ninvolved patients affected by Duchenne muscular dystrophy (DMD), where the goal\nis predict time to loss of ambulation using longitudinal blood biomarkers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:56:18 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 16:49:15 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Signorelli", "Mirko", ""], ["Spitali", "Pietro", ""], ["Szigyarto", "Cristina Al-Khalili", ""], ["Consortium", "The MARK-MD", ""], ["Tsonaka", "Roula", ""]]}, {"id": "2101.04437", "submitter": "Dootika Vats", "authors": "Kushagra Gupta, Dootika Vats, Snigdhansu Chatterjee", "title": "Bayesian equation selection on sparse data for discovery of stochastic\n  dynamical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often the underlying system of differential equations driving a stochastic\ndynamical system is assumed to be known, with inference conditioned on this\nassumption. We present a Bayesian framework for discovering this system of\ndifferential equations under assumptions that align with real-life scenarios,\nincluding the availability of relatively sparse data. Further, we discuss\ncomputational strategies that are critical in teasing out the important details\nabout the dynamical system and algorithmic innovations to solve for acute\nparameter interdependence in the absence of rich data. This gives a complete\nBayesian pathway for model identification via a variable selection paradigm and\nparameter estimation of the corresponding model using only the observed data.\nWe present detailed computations and analysis of the Lorenz-96, Lorenz-63, and\nthe Orstein-Uhlenbeck system using the Bayesian framework we propose.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:21:57 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Gupta", "Kushagra", ""], ["Vats", "Dootika", ""], ["Chatterjee", "Snigdhansu", ""]]}, {"id": "2101.04468", "submitter": "Alex Stringer", "authors": "Alex Stringer", "title": "Implementing Approximate Bayesian Inference using Adaptive Quadrature:\n  the aghq Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aghq package for implementing approximate Bayesian inference using\nadaptive quadrature is introduced. The method and software are described, and\nuse of the package in making approximate Bayesian inferences in several\nchallenging low- and high-dimensional models is illustrated. Examples include\nan infectious disease model; an astrostatistical model for estimating the mass\nof the Milky Way; two examples in non-Gaussian model-based geostatistics\nincluding one incorporating zero-inflation which is not easily fit using other\nmethods; and a model for zero-inflated, overdispersed count data. The aghq\npackage is especially compatible with the popular TMB interface for automatic\ndifferentiation and Laplace approximation, and existing users of that software\ncan make approximate Bayesian inferences with aghq using very little additional\ncode. The aghq package is available from CRAN and complete code for all\nexamples in this paper can be found at\nhttps://github.com/awstringer1/aghq-software-paper-code.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 13:31:55 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 02:12:45 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 17:26:41 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Stringer", "Alex", ""]]}, {"id": "2101.04805", "submitter": "Albert Vexler", "authors": "Ablert Vexler, Gregory Gurevich and Li Zou", "title": "Exact Multivariate Two-Sample Density-Based Empirical Likelihood Ratio\n  Tests Applicable to Retrospective and Group Sequential Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric tests for equality of multivariate distributions are frequently\ndesired in research. It is commonly required that test-procedures based on\nrelatively small samples of vectors accurately control the corresponding Type I\nError (TIE) rates. Often, in the multivariate testing, extensions of\nnull-distribution-free univariate methods, e.g., Kolmogorov-Smirnov and\nCramer-von Mises type schemes, are not exact, since their null distributions\ndepend on underlying data distributions. The present paper extends the\ndensity-based empirical likelihood technique in order to nonparametrically\napproximate the most powerful test for the multivariate two-sample (MTS)\nproblem, yielding an exact finite-sample test statistic. We rigorously\nestablish and apply one-to-one-mapping between the equality of vectors\ndistributions and the equality of distributions of relevant univariate linear\nprojections. In this framework, we prove an algorithm that simplifies the use\nof projection pursuit, employing only a few of the infinitely many linear\ncombinations of observed vectors components. The displayed distribution-free\nstrategy is employed in retrospective and group sequential manners. The\nasymptotic consistency of the proposed technique is shown. Monte Carlo studies\ndemonstrate that the proposed procedures exhibit extremely high and stable\npower characteristics across a variety of settings. Supplementary materials for\nthis article are available online.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:54:56 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Vexler", "Ablert", ""], ["Gurevich", "Gregory", ""], ["Zou", "Li", ""]]}, {"id": "2101.04890", "submitter": "Xiu Yang", "authors": "Mengqi Hu, Yifei Lou, Xiu Yang", "title": "The l1-l2 minimization with rotation for sparse approximation in\n  uncertainty quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a combination of rotational compressive sensing with the\nl1-l2 minimization to estimate coefficients of generalized polynomial chaos\n(gPC) used in uncertainty quantification. In particular, we aim to identify a\nrotation matrix such that the gPC of a set of random variables after the\nrotation has a sparser representation. However, this rotational approach alters\nthe underlying linear system to be solved, which makes finding the sparse\ncoefficients much more difficult than the case without rotation. We further\nadopt the l1-l2 minimization that is more suited for such ill-posed problems in\ncompressive sensing (CS) than the classic l1 approach. We conduct extensive\nexperiments on standard gPC problem settings, showing superior performance of\nthe proposed combination of rotation and l1-l2 minimization over the ones\nwithout rotation and with rotation but using the l1 minimization.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:47:07 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hu", "Mengqi", ""], ["Lou", "Yifei", ""], ["Yang", "Xiu", ""]]}, {"id": "2101.05129", "submitter": "Anirban Chaudhuri", "authors": "Anirban Chaudhuri, Boris Kramer, Matthew Norton, Johannes O. Royset,\n  Karen Willcox", "title": "Certifiable Risk-Based Engineering Design Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable, risk-averse design of complex engineering systems with optimized\nperformance requires dealing with uncertainties. A conventional approach is to\nadd safety margins to a design that was obtained from deterministic\noptimization. Safer engineering designs require appropriate cost and constraint\nfunction definitions that capture the \\textit{risk} associated with unwanted\nsystem behavior in the presence of uncertainties. The paper proposes two\nnotions of certifiability. The first is based on accounting for the magnitude\nof failure to ensure data-informed conservativeness. The second is the ability\nto provide optimization convergence guarantees by preserving convexity.\nSatisfying these notions leads to \\textit{certifiable} risk-based design\noptimization (CRiBDO). In the context of CRiBDO, risk measures based on\nsuperquantile (a.k.a.\\ conditional value-at-risk) and buffered probability of\nfailure are analyzed. CRiBDO is contrasted with reliability-based design\noptimization (RBDO), where uncertainties are accounted for via the probability\nof failure, through a structural and a thermal design problem. A reformulation\nof the short column structural design problem leading to a convex CRiBDO\nproblem is presented. The CRiBDO formulations capture more information about\nthe problem to assign the appropriate conservativeness, exhibit superior\noptimization convergence by preserving properties of underlying functions, and\nalleviate the adverse effects of choosing hard failure thresholds required in\nRBDO.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:23:15 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 18:30:17 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Chaudhuri", "Anirban", ""], ["Kramer", "Boris", ""], ["Norton", "Matthew", ""], ["Royset", "Johannes O.", ""], ["Willcox", "Karen", ""]]}, {"id": "2101.05365", "submitter": "Mike Lindow", "authors": "Mike Lindow, David DeFranza, Arul Mishra, Himanshu Mishra", "title": "Scared into Action: How Partisanship and Fear are Associated with\n  Reactions to Public Health Directives", "comments": "54 pages, 11 figures", "journal-ref": null, "doi": "10.31234/osf.io/8me7q", "report-no": null, "categories": "econ.GN cs.CL q-fin.EC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differences in political ideology are increasingly appearing as an impediment\nto successful bipartisan communication from local leadership. For example,\nrecent empirical findings have shown that conservatives are less likely to\nadhere to COVID-19 health directives. This behavior is in direct contradiction\nto past research which indicates that conservatives are more rule abiding,\nprefer to avoid loss, and are more prevention-motivated than liberals. We\nreconcile this disconnect between recent empirical findings and past research\nby using insights gathered from press releases, millions of tweets, and\nmobility data capturing local movement in retail, grocery, workplace, parks,\nand transit domains during COVID-19 shelter-in-place orders. We find that\nconservatives adhere to health directives when they express more fear of the\nvirus. In order to better understand this phenomenon, we analyze both official\nand citizen communications and find that press releases from local and federal\ngovernment, along with the number of confirmed COVID-19 cases, lead to an\nincrease in expressions of fear on Twitter.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 17:29:10 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Lindow", "Mike", ""], ["DeFranza", "David", ""], ["Mishra", "Arul", ""], ["Mishra", "Himanshu", ""]]}, {"id": "2101.05928", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Qian Wen", "title": "A practical test for a planted community in heterogeneous networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the fundamental task in graph data mining is to find a planted\ncommunity(dense subgraph), which has wide application in biology, finance, spam\ndetection and so on. For a real network data, the existence of a dense subgraph\nis generally unknown. Statistical tests have been devised to testing the\nexistence of dense subgraph in a homogeneous random graph. However, many\nnetworks present extreme heterogeneity, that is, the degrees of nodes or\nvertexes don't concentrate on a typical value. The existing tests designed for\nhomogeneous random graph are not straightforwardly applicable to the\nheterogeneous case. Recently, scan test was proposed for detecting a dense\nsubgraph in heterogeneous(inhomogeneous) graph(\\cite{BCHV19}). However, the\ncomputational complexity of the scan test is generally not polynomial in the\ngraph size, which makes the test impractical for large or moderate networks. In\nthis paper, we propose a polynomial-time test that has the standard normal\ndistribution as the null limiting distribution. The power of the test is\ntheoretically investigated and we evaluate the performance of the test by\nsimulation and real data example.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 01:34:14 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Yuan", "Mingao", ""], ["Wen", "Qian", ""]]}, {"id": "2101.06034", "submitter": "Julian Wagner", "authors": "Julian Wagner and G\\\"oran Kauermann and Ralf M\\\"unnich", "title": "Matrix-free Penalized Spline Smoothing with Multiple Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper motivates high dimensional smoothing with penalized splines and its\nnumerical calculation in an efficient way. If smoothing is carried out over\nthree or more covariates the classical tensor product spline bases explode in\ntheir dimension bringing the estimation to its numerical limits. A recent\napproach by Siebenborn and Wagner(2019) circumvents storage expensive\nimplementations by proposing matrix-free calculations which allows to smooth\nover several covariates. We extend their approach here by linking penalized\nsmoothing and its Bayesian formulation as mixed model which provides a\nmatrix-free calculation of the smoothing parameter to avoid the use of\nhigh-computational cross validation. Further, we show how to extend the ideas\ntowards generalized regression models. The extended approach is applied to\nremote sensing satellite data in combination with spatial smoothing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 09:52:30 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Wagner", "Julian", ""], ["Kauermann", "G\u00f6ran", ""], ["M\u00fcnnich", "Ralf", ""]]}, {"id": "2101.06369", "submitter": "Dao Nguyen", "authors": "Dao Nguyen, Xin Dang, Yixin Chen", "title": "Unadjusted Langevin algorithm for non-convex weakly smooth potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discretization of continuous-time diffusion processes is a widely recognized\nmethod for sampling. However, the canonical Euler Maruyama discretization of\nthe Langevin diffusion process, referred as Unadjusted Langevin Algorithm\n(ULA), studied mostly in the context of smooth (gradient Lipschitz) and\nstrongly log-concave densities, is a considerable hindrance for its deployment\nin many sciences, including statistics and machine learning. In this paper, we\nestablish several theoretical contributions to the literature on such sampling\nmethods for non-convex distributions. Particularly, we introduce a new mixture\nweakly smooth condition, under which we prove that ULA will converge with\nadditional log-Sobolev inequality. We also show that ULA for smoothing\npotential will converge in $L_{2}$-Wasserstein distance. Moreover, using\nconvexification of nonconvex domain \\citep{ma2019sampling} in combination with\nregularization, we establish the convergence in Kullback-Leibler (KL)\ndivergence with the number of iterations to reach $\\epsilon$-neighborhood of a\ntarget distribution in only polynomial dependence on the dimension. We relax\nthe conditions of \\citep{vempala2019rapid} and prove convergence guarantees\nunder isoperimetry, and non-strongly convex at infinity.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 04:47:50 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 21:07:55 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 04:44:08 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Nguyen", "Dao", ""], ["Dang", "Xin", ""], ["Chen", "Yixin", ""]]}, {"id": "2101.06453", "submitter": "Anand Jerry George", "authors": "Anand Jerry George, Navin Kashyap", "title": "An MCMC Method to Sample from Lattice Distributions", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Markov Chain Monte Carlo (MCMC) algorithm to generate samples\nfrom probability distributions supported on a $d$-dimensional lattice $\\Lambda\n= \\mathbf{B}\\mathbb{Z}^d$, where $\\mathbf{B}$ is a full-rank matrix.\nSpecifically, we consider lattice distributions $P_\\Lambda$ in which the\nprobability at a lattice point is proportional to a given probability density\nfunction, $f$, evaluated at that point. To generate samples from $P_\\Lambda$,\nit suffices to draw samples from a pull-back measure $P_{\\mathbb{Z}^d}$ defined\non the integer lattice. The probability of an integer lattice point under\n$P_{\\mathbb{Z}^d}$ is proportional to the density function $\\pi =\n|\\det(\\mathbf{B})|f\\circ \\mathbf{B}$. The algorithm we present in this paper\nfor sampling from $P_{\\mathbb{Z}^d}$ is based on the Metropolis-Hastings\nframework. In particular, we use $\\pi$ as the proposal distribution and\ncalculate the Metropolis-Hastings acceptance ratio for a well-chosen target\ndistribution. We can use any method, denoted by ALG, that ideally draws samples\nfrom the probability density $\\pi$, to generate a proposed state. The target\ndistribution is a piecewise sigmoidal distribution, chosen such that the\ncoordinate-wise rounding of a sample drawn from the target distribution gives a\nsample from $P_{\\mathbb{Z}^d}$. When ALG is ideal, we show that our algorithm\nis uniformly ergodic if $-\\log(\\pi)$ satisfies a gradient Lipschitz condition.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:01:53 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 12:23:37 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["George", "Anand Jerry", ""], ["Kashyap", "Navin", ""]]}, {"id": "2101.06494", "submitter": "Debashis Chatterjee", "authors": "Debashis Chatterjee", "title": "Novel Bayesian Procrustes Variance-based Inferences in Geometric\n  Morphometrics & Novel R package: BPviGM1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Compared to abundant classical statistics-based literature, to date, very\nlittle Bayesian literature exists on Procrustes shape analysis in Geometric\nMorphometrics, probably because of being a relatively new branch of statistical\nresearch and because of inherent computational difficulty associated with\nBayesian analysis. Moreover, we may obtain a plethora of novel inferences from\nBayesian Procrustes analysis of shape parameter distributions. In this paper,\nwe propose to regard the posterior of Procrustes shape variance as\nmorphological variability indicators. Here we propose novel Bayesian\nmethodologies for Procrustes shape analysis based on landmark data's isotropic\nvariance assumption and propose a Bayesian statistical test for model\nvalidation of new species discovery using morphological variation reflected in\nthe posterior distribution of landmark-variance of objects studied under\nGeometric Morphometrics. We will consider Gaussian distribution-based and\nheavy-tailed t distribution-based models for Procrustes analysis.\n  To date, we are not aware of any direct R package for Bayesian Procrustes\nanalysis for landmark-based Geometric Morphometrics. Hence, we introduce a\nnovel, simple R package \\textbf{BPviGM1} (\"Bayesian Procrustes Variance-based\ninferences in Geometric Morphometrics 1\"), which essentially contains the R\ncode implementations of the computations for proposed models and methodologies,\nsuch as R function for Markov Chain Monte Carlo (MCMC) run for drawing samples\nfrom posterior of parameters of concern and R function for the proposed\nBayesian test of model validation based on significance morphological\nvariation. As an application, we can quantitatively show that primate male-face\nmay be genetically viable to more shape-variation than the same for females.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 18:12:21 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 13:25:14 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Chatterjee", "Debashis", ""]]}, {"id": "2101.06763", "submitter": "Theodoulos Rodosthenous", "authors": "Theodoulos Rodosthenous, Vahid Shahrezaei and Marina Evangelou", "title": "Multi-view Data Visualisation via Manifold Learning", "comments": "27 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Non-linear dimensionality reduction can be performed by \\textit{manifold\nlearning} approaches, such as Stochastic Neighbour Embedding (SNE), Locally\nLinear Embedding (LLE) and Isometric Feature Mapping (ISOMAP). These methods\naim to produce two or three latent embeddings, primarily to visualise the data\nin intelligible representations. This manuscript proposes extensions of\nStudent's t-distributed SNE (t-SNE), LLE and ISOMAP, for dimensionality\nreduction and visualisation of multi-view data. Multi-view data refers to\nmultiple types of data generated from the same samples. The proposed multi-view\napproaches provide more comprehensible projections of the samples compared to\nthe ones obtained by visualising each data-view separately. Commonly\nvisualisation is used for identifying underlying patterns within the samples.\nBy incorporating the obtained low-dimensional embeddings from the multi-view\nmanifold approaches into the K-means clustering algorithm, it is shown that\nclusters of the samples are accurately identified. Through the analysis of real\nand synthetic data the proposed multi-SNE approach is found to have the best\nperformance. We further illustrate the applicability of the multi-SNE approach\nfor the analysis of multi-omics single-cell data, where the aim is to visualise\nand identify cell heterogeneity and cell types in biological tissues relevant\nto health and disease.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:54:36 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 11:22:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Rodosthenous", "Theodoulos", ""], ["Shahrezaei", "Vahid", ""], ["Evangelou", "Marina", ""]]}, {"id": "2101.07141", "submitter": "Achim Zeileis", "authors": "Susanne K\\\"oll, Ioannis Kosmidis, Christian Kleiber, Achim Zeileis", "title": "Bias Reduction as a Remedy to the Consequences of Infinite Estimates in\n  Poisson and Tobit Regression", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data separation is a well-studied phenomenon that can cause problems in the\nestimation and inference from binary response models. Complete or\nquasi-complete separation occurs when there is a combination of regressors in\nthe model whose value can perfectly predict one or both outcomes. In such\ncases, and such cases only, the maximum likelihood estimates and the\ncorresponding standard errors are infinite. It is less widely known that the\nsame can happen in further microeconometric models. One of the few works in the\narea is Santos Silva and Tenreyro (2010) who note that the finiteness of the\nmaximum likelihood estimates in Poisson regression depends on the data\nconfiguration and propose a strategy to detect and overcome the consequences of\ndata separation. However, their approach can lead to notable bias on the\nparameter estimates when the regressors are correlated. We illustrate how\nbias-reducing adjustments to the maximum likelihood score equations can\novercome the consequences of separation in Poisson and Tobit regression models.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:07:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["K\u00f6ll", "Susanne", ""], ["Kosmidis", "Ioannis", ""], ["Kleiber", "Christian", ""], ["Zeileis", "Achim", ""]]}, {"id": "2101.07330", "submitter": "Benjamin Zhang", "authors": "Benjamin Zhang, Tuhin Sahai, and Youssef Marzouk", "title": "A Koopman framework for rare event simulation in stochastic differential\n  equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit the relationship between the stochastic Koopman operator and the\nKolmogorov backward equation to construct importance sampling schemes for\nstochastic differential equations. Specifically, we propose using\neigenfunctions of the stochastic Koopman operator to approximate the Doob\ntransform for an observable of interest (e.g., associated with a rare event)\nwhich in turn yields an approximation of the corresponding zero-variance\nimportance sampling estimator. Our approach is broadly applicable and\nsystematic, treating non-normal systems, non-gradient systems, and systems with\noscillatory dynamics or rank-deficient noise in a common framework. In\nnonlinear settings where the stochastic Koopman eigenfunctions cannot be\nderived analytically, we use dynamic mode decomposition (DMD) methods to\ncompute them numerically, but the framework is agnostic to the particular\nnumerical method employed. Numerical experiments demonstrate that even coarse\napproximations of a few eigenfunctions, where the latter are built from\nnon-rare trajectories, can produce effective importance sampling schemes for\nrare events.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 21:23:11 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Zhang", "Benjamin", ""], ["Sahai", "Tuhin", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2101.07359", "submitter": "Zeyu Bian", "authors": "Zeyu Bian, Erica EM Moodie, Susan M Shortreed and Sahir Bhatnagar", "title": "Variable Selection in Regression-based Estimation of Dynamic Treatment\n  Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic treatment regimes (DTRs) consist of a sequence of decision rules, one\nper stage of intervention, that finds effective treatments for individual\npatients according to patient information history. DTRs can be estimated from\nmodels which include the interaction between treatment and a small number of\ncovariates which are often chosen a priori. However, with increasingly large\nand complex data being collected, it is difficult to know which prognostic\nfactors might be relevant in the treatment rule. Therefore, a more data-driven\napproach of selecting these covariates might improve the estimated decision\nrules and simplify models to make them easier to interpret. We propose a\nvariable selection method for DTR estimation using penalized dynamic weighted\nleast squares. Our method has the strong heredity property, that is, an\ninteraction term can be included in the model only if the corresponding main\nterms have also been selected. Through simulations, we show our method has both\nthe double robustness property and the oracle property, and the newly proposed\nmethods compare favorably with other variable selection approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:53:55 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bian", "Zeyu", ""], ["Moodie", "Erica EM", ""], ["Shortreed", "Susan M", ""], ["Bhatnagar", "Sahir", ""]]}, {"id": "2101.07464", "submitter": "Yue Lu", "authors": "Yue M. Lu", "title": "Householder Dice: A Matrix-Free Algorithm for Simulating Dynamics on\n  Gaussian and Random Orthogonal Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT physics.data-an stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new algorithm, named Householder Dice (HD), for\nsimulating dynamics on dense random matrix ensembles with translation-invariant\nproperties. Examples include the Gaussian ensemble, the Haar-distributed random\northogonal ensemble, and their complex-valued counterparts. A \"direct\" approach\nto the simulation, where one first generates a dense $n \\times n$ matrix from\nthe ensemble, requires at least $\\mathcal{O}(n^2)$ resource in space and time.\nThe HD algorithm overcomes this $\\mathcal{O}(n^2)$ bottleneck by using the\nprinciple of deferred decisions: rather than fixing the entire random matrix in\nadvance, it lets the randomness unfold with the dynamics. At the heart of this\nmatrix-free algorithm is an adaptive and recursive construction of (random)\nHouseholder reflectors. These orthogonal transformations exploit the group\nsymmetry of the matrix ensembles, while simultaneously maintaining the\nstatistical correlations induced by the dynamics. The memory and computation\ncosts of the HD algorithm are $\\mathcal{O}(nT)$ and $\\mathcal{O}(nT^2)$,\nrespectively, with $T$ being the number of iterations. When $T \\ll n$, which is\nnearly always the case in practice, the new algorithm leads to significant\nreductions in runtime and memory footprint. Numerical results demonstrate the\npromise of the HD algorithm as a new computational tool in the study of\nhigh-dimensional random systems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 04:50:53 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 00:15:14 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lu", "Yue M.", ""]]}, {"id": "2101.07564", "submitter": "Luc Pronzato", "authors": "Luc Pronzato", "title": "Performance analysis of greedy algorithms for minimising a Maximum Mean\n  Discrepancy", "comments": "34 pages, 7 figures, preprint submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyse the performance of several iterative algorithms for the\nquantisation of a probability measure $\\mu$, based on the minimisation of a\nMaximum Mean Discrepancy (MMD). Our analysis includes kernel herding, greedy\nMMD minimisation and Sequential Bayesian Quadrature (SBQ). We show that the\nfinite-sample-size approximation error, measured by the MMD, decreases as $1/n$\nfor SBQ and also for kernel herding and greedy MMD minimisation when using a\nsuitable step-size sequence. The upper bound on the approximation error is\nslightly better for SBQ, but the other methods are significantly faster, with a\ncomputational cost that increases only linearly with the number of points\nselected. This is illustrated by two numerical examples, with the target\nmeasure $\\mu$ being uniform (a space-filling design application) and with $\\mu$\na Gaussian mixture.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:18:51 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Pronzato", "Luc", ""]]}, {"id": "2101.07709", "submitter": "Nicholas Marshall", "authors": "Tamir Bendory, Ti-Yen Lan, Nicholas F. Marshall, Iris Rukshin, Amit\n  Singer", "title": "Multi-target detection with rotations", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi-target detection problem of estimating a\ntwo-dimensional target image from a large noisy measurement image that contains\nmany randomly rotated and translated copies of the target image. Motivated by\nsingle-particle cryo-electron microscopy, we focus on the low signal-to-noise\nregime, where it is difficult to estimate the locations and orientations of the\ntarget images in the measurement. Our approach uses autocorrelation analysis to\nestimate rotationally and translationally invariant features of the target\nimage. We demonstrate that, regardless of the level of noise, our technique can\nbe used to recover the target image when the measurement is sufficiently large.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:20:01 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bendory", "Tamir", ""], ["Lan", "Ti-Yen", ""], ["Marshall", "Nicholas F.", ""], ["Rukshin", "Iris", ""], ["Singer", "Amit", ""]]}, {"id": "2101.07710", "submitter": "Mohammad Fayaz", "authors": "Mohammad Fayaz, Alireza Abadi, Soheila Khodakarim", "title": "The effect of Hybrid Principal Components Analysis on the Signal\n  Compression Functional Regression: With EEG-fMRI Application", "comments": "It has 11 pages with 3 tables and 3 figures. It presented at \"The\n  13th International Conference of the ERCIM WG on Computational and\n  Methodological Statistics (CMStatistics 2020) (Virtual), 19-21 December 2020\"\n  (http://www.cmstatistics.org/CMStatistics2020/index.php). We plan to publish\n  it in statistical journals, especially in the conference's recommended\n  journals", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Objective: In some situations that exist both scalar and functional data,\ncalled mixed and hybrid data, the hybrid PCA (HPCA) was introduced. Among the\nregression models for the hybrid data, we can count covariate-adjusted HPCA,\nthe Semi-functional partial linear regression, function-on-function (FOF)\nregression with signal compression, and functional additive regression, models.\nIn this article, we study the effects of HPCA decomposition of hybrid data on\nthe prediction accuracy of the FOF regression with signal compressions. Method:\nWe stated a two-step procedure for incorporating the HPCA in the functional\nregressions. The first step is reconstructing the data based on the HPCAs and\nthe second step is merging data on the other dimensions and calculate the\npoint-wise average of the desired functional dimension. We also choose the\nnumber of HPCA based on Mean Squared Perdition Error (MSPE). Result: In the two\nsimulations, we show that the regression models with the first HPCA have the\nbest accuracy prediction and model fit summaries among no HPCA and all HPCAs\nwith a training/testing approach. Finally, we applied our methodology to the\nEEG-fMRI dataset. Conclusions: We conclude that our methodology improves the\nprediction of the experiments with the EEG datasets. And we recommend that\ninstead of using the functional PCA on the desired dimension, reconstruct the\ndata with HPCA and average it on the other two dimensions for functional\nregression models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:26:47 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Fayaz", "Mohammad", ""], ["Abadi", "Alireza", ""], ["Khodakarim", "Soheila", ""]]}, {"id": "2101.07718", "submitter": "Zhu Wang", "authors": "Zhu Wang", "title": "Unified Robust Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Boosting is a popular machine learning algorithm in regression and\nclassification problems. Boosting can combine a sequence of regression trees to\nobtain accurate prediction. In the presence of outliers, traditional boosting,\nbased on optimizing convex loss functions, may show inferior results. In this\narticle, a unified robust boosting is proposed for more resistant estimation.\nThe method utilizes a recently developed concave-convex family for robust\nestimation, composite optimization by conjugation operator, and functional\ndecent boosting. As a result, an iteratively reweighted boosting algorithm can\nbe conveniently constructed with existing software. Applications in robust\nregression, classification and Poisson regression are demonstrated in the R\npackage ccboost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:48:32 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Wang", "Zhu", ""]]}, {"id": "2101.07766", "submitter": "Louis Raynal", "authors": "Louis Raynal and Jukka-Pekka Onnela", "title": "Selection of Summary Statistics for Network Model Choice with\n  Approximate Bayesian Computation", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) now serves as one of the major\nstrategies to perform model choice and parameter inference on models with\nintractable likelihoods. An essential component of ABC involves comparing a\nlarge amount of simulated data with the observed data through summary\nstatistics. To avoid the curse of dimensionality, summary statistic selection\nis of prime importance, and becomes even more critical when applying ABC to\nmechanistic network models. Indeed, while many summary statistics can be used\nto encode network structures, their computational complexity can be highly\nvariable. For large networks, computation of summary statistics can quickly\ncreate a bottleneck, making the use of ABC difficult. To reduce this\ncomputational burden and make the analysis of mechanistic network models more\npractical, we investigated two questions in a model choice framework. First, we\nstudied the utility of cost-based filter selection methods to account for\ndifferent summary costs during the selection process. Second, we performed\nselection using networks generated with a smaller number of nodes to reduce the\ntime required for the selection step. Our findings show that computationally\ninexpensive summary statistics can be efficiently selected with minimal impact\non classification accuracy. Furthermore, we found that networks with a smaller\nnumber of nodes can only be employed to eliminate a moderate number of\nsummaries. While this latter finding is network specific, the former is general\nand can be adapted to any ABC application.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:21:06 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Raynal", "Louis", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "2101.07987", "submitter": "Martin Bladt", "authors": "Martin Bladt and Jorge Yslas", "title": "matrixdist: An R Package for Inhomogeneous Phase-Type Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inhomogeneous phase-type distributions (IPH) are a broad class of laws which\narise from the absorption times of Markov jump processes. In the\ntime-homogeneous particular case, we recover phase-type (PH) distributions. In\nmatrix notation, various functionals corresponding to their distributional\nproperties are explicitly available and succinctly described. As the number of\nparameters increases, IPH distributions may converge weakly to any probability\nmeasure on the positive real line, making them particularly attractive\ncandidates for statistical modelling purposes. Contrary to PH distributions,\nthe IPH class allows for a wide range of tail behaviours, which often leads to\nadequate estimation with a moderate number of parameters. One of the main\ndifficulties in estimating PH and IPH distributions is their large number of\nmatrix parameters. This drawback is best handled through the\nexpectation-maximisation (EM) algorithm, exploiting the underlying and\nunobserved Markov structure. The matrixdist package presents tools for IPH\ndistributions to efficiently evaluate functionals, simulate, and carry out\nmaximum likelihood estimation through a three-step EM algorithm. Aggregated and\nright-censored data are supported by the fitting routines, and in particular,\none may estimate time-to-event data, histograms, or discretised theoretical\ndistributions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:20:41 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 10:23:39 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Bladt", "Martin", ""], ["Yslas", "Jorge", ""]]}, {"id": "2101.08049", "submitter": "Luka \\v{Z}nidari\\v{c}", "authors": "Luka \\v{Z}nidari\\v{c}, Gjorgji Nusev, Bertrand Morel, Julie Mougin,\n  {\\DJ}ani Juri\\v{c}i\\'c and Pavle Bo\\v{s}koski", "title": "Evaluating uncertainties in electrochemical impedance spectra of solid\n  oxide fuel cells", "comments": "28 pages, 18 figures. Submitted to: Applied Energy", "journal-ref": null, "doi": "10.1016/j.apenergy.2021.117101", "report-no": null, "categories": "stat.CO cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electrochemical impedance spectroscopy (EIS) is a widely used tool for\ncharacterization of fuel cells and other electrochemical conversion systems.\nWhen applied to the on-line monitoring in the context of in-field applications,\nthe disturbances, drifts and sensor noise may cause severe distortions in the\nevaluated spectra, especially in the low-frequency part. Failure to ignore the\nrandom effects can result in misinterpreted spectra and, consequently, in\nmisleading diagnostic reasoning. This fact has not been often addressed in the\nresearch so far. In this paper, we propose an approach to the quantification of\nthe spectral uncertainty, which relies on evaluating the uncertainty of the\nequivalent circuit model (ECM). We apply the computationally efficient\nvariational Bayes (VB) method and compare the quality of the results with those\nobtained with the Markov chain Monte Carlo (MCMC) algorithm. Namely, MCMC\nalgorithm returns accurate distributions of the estimated model parameters,\nwhile VB approach provides the approximate distributions. By using simulated\nand real data we show that approximate results provided by VB approach,\nalthough slightly over-optimistic, are still close to the more realistic MCMC\nestimates. A great advantage of the VB method for online monitoring is low\ncomputational load, which is several orders of magnitude lower compared to\nMCMC. The performance of VB algorithm is demonstrated on a case of ECM\nparameters estimation in a 6 cell solid oxide fuel cell (SOFC) stack. The\ncomplete numerical implementation for recreating the results can be found at\nhttps://repo.ijs.si/lznidaric/variational-bayes-supplementary-material.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:07:32 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 12:22:32 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 11:35:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["\u017dnidari\u010d", "Luka", ""], ["Nusev", "Gjorgji", ""], ["Morel", "Bertrand", ""], ["Mougin", "Julie", ""], ["Juri\u010di\u0107", "\u0110ani", ""], ["Bo\u0161koski", "Pavle", ""]]}, {"id": "2101.08436", "submitter": "Karl Oskar Ekvall", "authors": "Karl Oskar Ekvall and Aaron J. Molstad", "title": "Mixed-type multivariate response regression with covariance estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for multivariate response regressions where the\nelements of the response vector can be of mixed types, for example some\ncontinuous and some discrete. Our method is based on a model which assumes the\nobservable mixed-type response vector is connected to a latent multivariate\nnormal response linear regression through a link function. We explore the\nproperties of this model and show its parameters are identifiable under\nreasonable conditions. We impose no parametric restrictions on the covariance\nof the latent normal other than positive definiteness, thereby avoiding\nassumptions about unobservable variables which can be difficult to verify. To\naccommodate this generality, we propose a novel algorithm for approximate\nmaximum likelihood estimation that works \"off-the-shelf\" with many different\ncombinations of response types, and which scales well in the dimension of the\nresponse vector. Our method typically gives better predictions and parameter\nestimates than fitting separate models for the different response types and\nallows for approximate likelihood ratio testing of relevant hypotheses such as\nindependence of responses. The usefulness of the proposed method is illustrated\nin simulations; and one biomedical and one genomic data example.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 04:48:44 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 07:42:45 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ekvall", "Karl Oskar", ""], ["Molstad", "Aaron J.", ""]]}, {"id": "2101.08492", "submitter": "Jouni Helske", "authors": "Jouni Helske and Matti Vihola", "title": "bssm: Bayesian Inference of Non-linear and Non-Gaussian State Space\n  Models in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an R package bssm for Bayesian non-linear/non-Gaussian state space\nmodelling. Unlike the existing packages, bssm allows for easy-to-use\napproximate inference based on Gaussian approximations such as the Laplace\napproximation and the extended Kalman filter. The package accommodates also\ndiscretely observed latent diffusion processes. The inference is based on fully\nautomatic, adaptive Markov chain Monte Carlo (MCMC) on the hyperparameters,\nwith optional importance sampling post-correction to eliminate any\napproximation bias. The package implements also a direct pseudo-marginal MCMC\nand a delayed acceptance pseudo-marginal MCMC using intermediate\napproximations. The package offers an easy-to-use interface to define models\nwith linear-Gaussian state dynamics with non-Gaussian observation models, and\nhas an Rcpp interface for specifying custom non-linear and diffusion models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:22:47 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 06:38:31 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Helske", "Jouni", ""], ["Vihola", "Matti", ""]]}, {"id": "2101.08872", "submitter": "Andrea Arnold", "authors": "Anna Fitzpatrick, Molly Folino, Andrea Arnold", "title": "Fourier Series-Based Approximation of Time-Varying Parameters Using the\n  Ensemble Kalman Filter", "comments": "13 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a Fourier series-based approximation method using\nensemble Kalman filtering to estimate time-varying parameters in deterministic\ndynamical systems. We demonstrate the capability of this approach in estimating\nboth sinusoidal and polynomial forcing parameters in a mass-spring system.\nResults emphasize the importance of the choice of frequencies in the\napproximation model terms on the corresponding time-varying parameter\nestimates.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:26:03 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Fitzpatrick", "Anna", ""], ["Folino", "Molly", ""], ["Arnold", "Andrea", ""]]}, {"id": "2101.09514", "submitter": "Nadhir Ben Rached", "authors": "Nadhir Ben Rached and Abdul-Lateef Haji-Ali and Gerardo Rubino and\n  Raul Tempone", "title": "Efficient Importance Sampling for Large Sums of Independent and\n  Identically Distributed Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to estimate the probability that the sum of nonnegative independent\nand identically distributed random variables falls below a given threshold,\ni.e., $\\mathbb{P}(\\sum_{i=1}^{N}{X_i} \\leq \\gamma)$, via importance sampling\n(IS). We are particularly interested in the rare event regime when $N$ is large\nand/or $\\gamma$ is small. The exponential twisting is a popular technique that,\nin most of the cases, compares favorably to existing estimators. However, it\nhas several limitations: i) it assumes the knowledge of the moment generating\nfunction of $X_i$ and ii) sampling under the new measure is not straightforward\nand might be expensive. The aim of this work is to propose an alternative\nchange of measure that yields, in the rare event regime corresponding to large\n$N$ and/or small $\\gamma$, at least the same performance as the exponential\ntwisting technique and, at the same time, does not introduce serious\nlimitations. For distributions whose probability density functions (PDFs) are\n$\\mathcal{O}(x^{d})$, as $x \\rightarrow 0$ and $d>-1$, we prove that the Gamma\nIS PDF with appropriately chosen parameters retrieves asymptotically, in the\nrare event regime, the same performance of the estimator based on the use of\nthe exponential twisting technique. Moreover, in the Log-normal setting, where\nthe PDF at zero vanishes faster than any polynomial, we numerically show that a\nGamma IS PDF with optimized parameters clearly outperforms the exponential\ntwisting change of measure. Numerical experiments validate the efficiency of\nthe proposed estimator in delivering a highly accurate estimate in the regime\nof large $N$ and/or small $\\gamma$.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 14:42:14 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rached", "Nadhir Ben", ""], ["Haji-Ali", "Abdul-Lateef", ""], ["Rubino", "Gerardo", ""], ["Tempone", "Raul", ""]]}, {"id": "2101.09604", "submitter": "Johannes Buchner", "authors": "Johannes Buchner", "title": "UltraNest -- a robust, general purpose Bayesian inference engine", "comments": "Longer version of the paper published in JOSS. UltraNest can be found\n  at https://johannesbuchner.github.io/UltraNest/", "journal-ref": null, "doi": null, "report-no": "10.21105/joss.03001", "categories": "stat.CO astro-ph.IM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  UltraNest is a general-purpose Bayesian inference package for parameter\nestimation and model comparison. It allows fitting arbitrary models specified\nas likelihood functions written in Python, C, C++, Fortran, Julia or R. With a\nfocus on correctness and speed (in that order), UltraNest is especially useful\nfor multi-modal or non-Gaussian parameter spaces, computational expensive\nmodels, in robust pipelines. Parallelisation to computing clusters and resuming\nincomplete runs is available.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 23:32:43 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 13:27:40 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Buchner", "Johannes", ""]]}, {"id": "2101.09675", "submitter": "Johannes Buchner", "authors": "Johannes Buchner", "title": "Nested Sampling Methods", "comments": "Updated version incorporating constructive input from four(!)\n  positive reports (two referees, assistant editor and editor). The open-source\n  UltraNest package and astrostatistics tutorials can be found at\n  https://johannesbuchner.github.io/UltraNest/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Nested sampling (NS) computes parameter posterior distributions and makes\nBayesian model comparison computationally feasible. Its strengths are the\nunsupervised navigation of complex, potentially multi-modal posteriors until a\nwell-defined termination point. A systematic literature review of nested\nsampling algorithms and variants is presented. We focus on complete algorithms,\nincluding solutions to likelihood-restricted prior sampling, parallelisation,\ntermination and diagnostics. The relation between number of live points,\ndimensionality and computational cost is studied for two complete algorithms. A\nnew formulation of NS is presented, which casts the parameter space exploration\nas a search on a tree. Previously published ways of obtaining robust error\nestimates and dynamic variations of the number of live points are presented as\nspecial cases of this formulation. A new on-line diagnostic test is presented\nbased on previous insertion rank order work. The survey of nested sampling\nmethods concludes with outlooks for future research.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 09:20:12 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 13:46:31 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Buchner", "Johannes", ""]]}, {"id": "2101.09747", "submitter": "Emmanuel Vazquez", "authors": "Subhasish Basak, S\\'ebastien Petit, Julien Bect, Emmanuel Vazquez", "title": "Numerical issues in maximum likelihood parameter estimation for Gaussian\n  process interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article investigates the origin of numerical issues in maximum\nlikelihood parameter estimation for Gaussian process (GP) interpolation and\ninvestigates simple but effective strategies for improving commonly used\nopen-source software implementations. This work targets a basic problem but a\nhost of studies, particularly in the literature of Bayesian optimization, rely\non off-the-shelf GP implementations. For the conclusions of these studies to be\nreliable and reproducible, robust GP implementations are critical.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 16:30:55 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 19:31:13 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Basak", "Subhasish", ""], ["Petit", "S\u00e9bastien", ""], ["Bect", "Julien", ""], ["Vazquez", "Emmanuel", ""]]}, {"id": "2101.10103", "submitter": "Arnald Puy", "authors": "Arnald Puy, Samuele Lo Piano, Andrea Saltelli, Simon A. Levin", "title": "sensobol: an R package to compute variance-based sensitivity indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R package \"sensobol\" provides several functions to conduct variance-based\nuncertainty and sensitivity analysis, from the estimation of sensitivity\nindices to the visual representation of the results. It implements several\nstate-of-the-art first and total-order estimators and allows the computation of\nup to third-order effects, as well as of the approximation error, in a swift\nand user-friendly way. Its flexibility makes it also appropriate for models\nwith either a scalar or a multivariate output. We illustrate its functionality\nby conducting a variance-based sensitivity analysis of three classic models:\nthe Sobol' (1998) G function, the logistic population growth model of Verhulst\n(1845), and the spruce budworm and forest model of Ludwig, Jones and Holling\n(1976).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:36:39 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 14:09:38 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Puy", "Arnald", ""], ["Piano", "Samuele Lo", ""], ["Saltelli", "Andrea", ""], ["Levin", "Simon A.", ""]]}, {"id": "2101.10583", "submitter": "Elvira Di Nardo Prof.", "authors": "E. Di Nardo", "title": "On the connection between orthant probabilities and the first passage\n  time problem", "comments": "2 tables", "journal-ref": "Journal of Statistical Computation & Simulation (2005), vol. 75,\n  437--445", "doi": "10.1080/0094965042000221664", "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a new Monte Carlo method for the evaluation of the\northant probabilities by sampling first passage times of a non-singular\nGaussian discrete time-series across an absorbing boundary. This procedure\nmakes use of a simulation of several time-series sample paths, aiming to record\ntheir first crossing instants. Thus, the computation of the orthant\nprobabilities is traced back to the accurate simulation of a non-singular\nGaussian discrete-time series. Moreover, if the simulation is also efficient,\nthis method is shown to be more speedy than the others proposed in the\nliterature. As example, we make use of the Davies-Harte algorithm in the\nevaluation of the orthant probabilities associated to the ARFIMA$(0,d,0)$\nmodel. Test results are presented that compare this method with currently\navailable software.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:21:30 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Di Nardo", "E.", ""]]}, {"id": "2101.10859", "submitter": "Martha Frysztacki M.Sc.", "authors": "Martha Maria Frysztacki and Jonas H\\\"orsch and Veit Hagenmeyer and Tom\n  Brown", "title": "The strong effect of network resolution on electricity system models\n  with high shares of wind and solar", "comments": "22 pages, 16 figures, 7 tables, pre-print submitted to Elsevier\n  (Applied Energy) updated version: texts and graphics updated to published\n  version + journal reference", "journal-ref": "Applied Energy, Volume 291, June 2021", "doi": "10.1016/j.apenergy.2021.116726", "report-no": null, "categories": "physics.soc-ph stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Energy system modellers typically choose a low spatial resolution for their\nmodels based on administrative boundaries such as countries, which eases data\ncollection and reduces computation times. However, a low spatial resolution can\nlead to sub-optimal investment decisions for wind and solar generation.\nIgnoring power grid bottlenecks within regions tends to underestimate system\ncosts, while combining locations with different wind and solar capacity factors\nin the same resource class tends to overestimate costs. We investigate these\ntwo competing effects in a capacity expansion model for Europe's power system\nwith a high share of renewables, taking advantage of newly-available\nhigh-resolution datasets as well as computational advances. We vary the number\nof nodes, interpolating between a 37-node model based on country and\nsynchronous zone boundaries, and a 1024-node model based on the location of\nelectricity substations. If we focus on the effect of renewable resource\nresolution and ignore network restrictions, we find that a higher resolution\nallows the optimal solution to concentrate wind and solar capacity at sites\nwith better capacity factors and thus reduces system costs by up to 10%\ncompared to a low resolution model. This results in a big swing from offshore\nto onshore wind investment. However, if we introduce grid bottlenecks by\nraising the network resolution, costs increase by up to 23% as generation has\nto be sourced more locally at sites with worse capacity factors. These effects\nare most pronounced in scenarios where grid expansion is limited, for example,\nby low local acceptance. We show that allowing grid expansion mitigates some of\nthe effects of the low grid resolution, and lowers overall costs by around 16%.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:17:11 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 15:49:42 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 07:35:29 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Frysztacki", "Martha Maria", ""], ["H\u00f6rsch", "Jonas", ""], ["Hagenmeyer", "Veit", ""], ["Brown", "Tom", ""]]}, {"id": "2101.10948", "submitter": "Peter Kasprzak Mr", "authors": "Peter Kasprzak, Lachlan Mitchell, Olena Kravchuk and Andy Timmins", "title": "Six Years of Shiny in Research -- Collaborative Development of Web Tools\n  in R", "comments": "48 pages with references, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Shiny in research publications is investigated. From the\nappearance of this popular web application framework for R through to 2018, it\nhas been utilised in many diverse research areas. While it can be shown that\nthe complexity of Shiny applications is limited by the background architecture,\nand real security concerns exist for novice app developers, the collaborative\nbenefits are worth attention from the wider research community. Shiny\nsimplifies the use of complex methodologies for users of different\nspecialities, at the level of proficiency appropriate for the end user. This\nenables a diverse community of users to interact efficiently, utilising\ncutting-edge methodologies. The literature reviewed demonstrates that complex\nmethodologies can be put into practice without the necessity for investment in\nprofessional training. It would appear that Shiny opens up concurrent benefits\nin communication between those who analyse data and those in other disciplines,\nthereby potentially enriching research through this technology.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 03:55:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kasprzak", "Peter", ""], ["Mitchell", "Lachlan", ""], ["Kravchuk", "Olena", ""], ["Timmins", "Andy", ""]]}, {"id": "2101.11003", "submitter": "Steven Golovkine", "authors": "Steven Golovkine", "title": "FDApy: a Python package for functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the Python package, FDApy, as an implementation of functional\ndata. This package provide modules for the analysis of such data. It includes\nclasses for different dimensional data as well as irregularly sampled\nfunctional data. A simulation toolbox is also provided. It might be used to\nsimulate different clusters of functional data. Some methodologies to handle\nthese data are implemented, such as dimension reduction and clustering. New\nmethods can be easily added. The package is publicly available on the Python\nPackage Index and Github.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:07:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Golovkine", "Steven", ""]]}, {"id": "2101.11048", "submitter": "Johannes Brust", "authors": "Johannes J. Brust, Roummel F. Marcia, Cosmin G. Petra, and Michael A.\n  Saunders", "title": "Large-scale Optimization with Linear Equality Constraints using Reduced\n  Compact Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": "ANL/MCS-P9430-0121", "categories": "math.OC cs.NA math.NA stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For optimization problems with sparse linear equality constraints, we observe\nthat the (1,1) block of the inverse KKT matrix remains unchanged when projected\nonto the nullspace of the constraints. We develop reduced compact\nrepresentations of the limited-memory BFGS Hessian to compute search directions\nefficiently. Orthogonal projections are implemented by sparse QR factorization\nor preconditioned LSQR iteration. In numerical experiments two proposed\ntrust-region algorithms improve in computation times, often significantly,\ncompared to previous implementations and compared to IPOPT.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:40:35 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Brust", "Johannes J.", ""], ["Marcia", "Roummel F.", ""], ["Petra", "Cosmin G.", ""], ["Saunders", "Michael A.", ""]]}, {"id": "2101.11083", "submitter": "Naoki Awaya", "authors": "Naoki Awaya and Li Ma", "title": "Tree boosting for learning probability measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning probability measures based on an i.i.d. sample is a fundamental\ninference task, but is challenging when the sample space is high-dimensional.\nInspired by the success of tree boosting in high-dimensional classification and\nregression, we propose a tree boosting method for learning high-dimensional\nprobability distributions. We formulate concepts of \"addition\" and \"residuals\"\non probability distributions in terms of compositions of a new, more general\nnotion of multivariate cumulative distribution functions (CDFs) than classical\nCDFs. This then gives rise to a simple boosting algorithm based on\nforward-stagewise (FS) fitting of an additive ensemble of measures, which\nsequentially minimizes the entropy loss. The output of the FS algorithm allows\nanalytic computation of the probability density function for the fitted\ndistribution. It also provides an exact simulator for drawing independent Monte\nCarlo samples from the fitted measure. Typical considerations in applying\nboosting--namely choosing the number of trees, setting the appropriate level of\nshrinkage/regularization in the weak learner, and the evaluation of variable\nimportance--can all be accomplished in an analogous fashion to traditional\nboosting in supervised learning. Numerical experiments confirm that boosting\ncan substantially improve the fit to multivariate distributions compared to the\nstate-of-the-art single-tree learner and is computationally efficient. We\nillustrate through an application to a data set from mass cytometry how the\nsimulator can be used to investigate various aspects of the underlying\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 21:03:27 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 19:41:48 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 23:28:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Awaya", "Naoki", ""], ["Ma", "Li", ""]]}, {"id": "2101.11159", "submitter": "Shanshan Xie", "authors": "Shanshan Xie (1), Tim Hillel (2), Ying Jin (1) ((1) Department of\n  Architecture, University of Cambridge, (2) Transportation and Mobility\n  Laboratory, EPFL)", "title": "An Early Stopping Bayesian Data Assimilation Approach for Mixed-Logit\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixed-logit model is a flexible tool in transportation choice analysis,\nwhich provides valuable insights into inter and intra-individual behavioural\nheterogeneity. However, applications of mixed-logit models are limited by the\nhigh computational and data requirements for model estimation. When estimating\non small samples, the Bayesian estimation approach becomes vulnerable to over\nand under-fitting. This is problematic for investigating the behaviour of\nspecific population sub-groups or market segments with low data availability.\nSimilar challenges arise when transferring an existing model to a new location\nor time period, e.g., when estimating post-pandemic travel behaviour. We\npropose an Early Stopping Bayesian Data Assimilation (ESBDA) simulator for\nestimation of mixed-logit which combines a Bayesian statistical approach with\nMachine Learning methodologies. The aim is to improve the transferability of\nmixed-logit models and to enable the estimation of robust choice models with\nlow data availability. This approach can provide new insights into choice\nbehaviour where the traditional estimation of mixed-logit models was not\npossible due to low data availability, and open up new opportunities for\ninvestment and planning decisions support. The ESBDA estimator is benchmarked\nagainst the Direct Application approach, a basic Bayesian simulator with random\nstarting parameter values and a Bayesian Data Assimilation (BDA) simulator\nwithout early stopping. The ESBDA approach is found to effectively overcome\nunder and over-fitting and non-convergence issues in simulation. Its resulting\nmodels clearly outperform those of the reference simulators in predictive\naccuracy. Furthermore, models estimated with ESBDA tend to be more robust, with\nsignificant parameters with signs and values consistent with behavioural\ntheory, even when estimated on small samples.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 01:35:25 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Xie", "Shanshan", ""], ["Hillel", "Tim", ""], ["Jin", "Ying", ""]]}, {"id": "2101.11370", "submitter": "Francesco Finazzi", "authors": "Yaqiong Wang, Francesco Finazzi, Alessandro Fass\\`o", "title": "D-STEM v2: A Software for Modelling Functional Spatio-Temporal Data", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional spatio-temporal data naturally arise in many environmental and\nclimate applications where data are collected in a three-dimensional space over\ntime. The MATLAB D-STEM v1 software package was first introduced for modelling\nmultivariate space-time data and has been recently extended to D-STEM v2 to\nhandle functional data indexed across space and over time. This paper\nintroduces the new modelling capabilities of D-STEM v2 as well as the\ncomplexity reduction techniques required when dealing with large data sets.\nModel estimation, validation and dynamic kriging are demonstrated in two case\nstudies, one related to ground-level air quality data in Beijing, China, and\nthe other one related to atmospheric profile data collected globally through\nradio sounding.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 13:06:06 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Wang", "Yaqiong", ""], ["Finazzi", "Francesco", ""], ["Fass\u00f2", "Alessandro", ""]]}, {"id": "2101.11460", "submitter": "Hamza M. Ruzayqat", "authors": "Dan Crisan, Pierre Del Moral, Ajay Jasra, Hamza Ruzayqat", "title": "Log-Normalization Constant Estimation using the Ensemble Kalman-Bucy\n  Filter with Application to High-Dimensional Models", "comments": "25 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we consider the estimation of the log-normalization constant\nassociated to a class of continuous-time filtering models. In particular, we\nconsider ensemble Kalman-Bucy filter based estimates based upon several\nnonlinear Kalman-Bucy diffusions. Based upon new conditional bias results for\nthe mean of the afore-mentioned methods, we analyze the empirical log-scale\nnormalization constants in terms of their $\\mathbb{L}_n-$errors and conditional\nbias. Depending on the type of nonlinear Kalman-Bucy diffusion, we show that\nthese are of order $(\\sqrt{t/N}) + t/N$ or $1/\\sqrt{N}$ ($\\mathbb{L}_n-$errors)\nand of order $[t+\\sqrt{t}]/N$ or $1/N$ (conditional bias), where $t$ is the\ntime horizon and $N$ is the ensemble size. Finally, we use these results for\nonline static parameter estimation for above filtering models and implement the\nmethodology for both linear and nonlinear models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:46:26 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Crisan", "Dan", ""], ["Del Moral", "Pierre", ""], ["Jasra", "Ajay", ""], ["Ruzayqat", "Hamza", ""]]}, {"id": "2101.11740", "submitter": "Johannes Brust", "authors": "Johannes J. Brust and Mihai Anitescu", "title": "Convergence Analysis of Fixed Point Chance Constrained Optimal Power\n  Flow Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": "ANL/MCS-P9431-0121", "categories": "math.OC cs.SY eess.SY stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For optimal power flow problems with chance constraints, a particularly\neffective method is based on a fixed point iteration applied to a sequence of\ndeterministic power flow problems. However, a priori, the convergence of such\nan approach is not necessarily guaranteed. This article analyses the\nconvergence conditions for this fixed point approach, and reports numerical\nexperiments including for large IEEE networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 23:31:56 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Brust", "Johannes J.", ""], ["Anitescu", "Mihai", ""]]}, {"id": "2101.11857", "submitter": "Beth Ann Griffin PhD", "authors": "Ricardo Sanchez, Beth Ann Griffin, Daniel McCaffrey", "title": "Best Practices in Scientific Computing", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is becoming increasingly complex, both in terms of the rich sources\nof data we have access to as well as in terms of the statistical and\ncomputational methods we can use on those data. These factors create an\never-increasing risk for errors in our code and sensitivity in our findings to\ndata preparation and execution of complex statistical and computing methods.\nThe consequences of coding and data mistakes can be substantial. Openness\n(e.g., providing others with data code) and transparency (e.g., requiring that\ndata processing and code follow standards) are two key solutions to help\nalleviate concerns about replicability and errors. In this paper, we describe\nthe key steps for implementing a code quality assurance (QA) process for\nresearchers to follow to improve their coding practices throughout a project to\nassure the quality of the final data, code, analyses and ultimately the\nresults. These steps include: (i) adherence to principles for code writing and\nstyle that follow best practices, (ii) clear written documentation that\ndescribes code, workflow and key analytic decisions; (iii) careful version\ncontrol, (iv) good data management; and (iv) regular testing and review.\nFollowing all these steps will greatly improve the ability of a study to assure\nresults are accurate and reproducible. The responsibility for code QA falls not\nonly on individual researchers but institutions, journals, and funding agencies\nas well.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:02:51 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 18:13:33 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Sanchez", "Ricardo", ""], ["Griffin", "Beth Ann", ""], ["McCaffrey", "Daniel", ""]]}, {"id": "2101.12156", "submitter": "Nianqiao Ju", "authors": "Nianqiao Ju, Jeremy Heng, and Pierre E. Jacob", "title": "Sequential Monte Carlo algorithms for agent-based models of disease\n  transmission", "comments": "39 pages, 7 figures, 2. tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO nlin.CG q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-based models of disease transmission involve stochastic rules that\nspecify how a number of individuals would infect one another, recover or be\nremoved from the population. Common yet stringent assumptions stipulate\ninterchangeability of agents and that all pairwise contact are equally likely.\nUnder these assumptions, the population can be summarized by counting the\nnumber of susceptible and infected individuals, which greatly facilitates\nstatistical inference. We consider the task of inference without such\nsimplifying assumptions, in which case, the population cannot be summarized by\nlow-dimensional counts. We design improved particle filters, where each\nparticle corresponds to a specific configuration of the population of agents,\nthat take either the next or all future observations into account when\nproposing population configurations. Using simulated data sets, we illustrate\nthat orders of magnitude improvements are possible over bootstrap particle\nfilters. We also provide theoretical support for the approximations employed to\nmake the algorithms practical.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:08:20 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Ju", "Nianqiao", ""], ["Heng", "Jeremy", ""], ["Jacob", "Pierre E.", ""]]}]